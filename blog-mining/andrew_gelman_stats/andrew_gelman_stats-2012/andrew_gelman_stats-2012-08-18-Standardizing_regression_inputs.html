<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1462 andrew gelman stats-2012-08-18-Standardizing regression inputs</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2012" href="../home/andrew_gelman_stats-2012_home.html">andrew_gelman_stats-2012</a> <a title="andrew_gelman_stats-2012-1462" href="#">andrew_gelman_stats-2012-1462</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1462 andrew gelman stats-2012-08-18-Standardizing regression inputs</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2012-1462-html" href="http://andrewgelman.com/2012/08/18/standardizing-regression-inputs/">html</a></p><p>Introduction: Andy Flies, Ph.D. candidate in zoology, writes:
  
After reading your paper about scaling regression inputs by two standard deviations I found your  blog post  stating that you wished you had scaled by 1 sd and coded the binary inputs as -1 and 1.  Here is my question:


If you code the binary input as -1 and 1, do you then standardize it?  This makes sense to me because the mean of the standardized input is then zero and the sd is 1, which is what the mean and sd are for all of the other standardized inputs.  I know that if you code the binary input as 0 and 1 it should not be standardized.


Also, I am not interested in the actual units (i.e. mg/ml) of my response variable and I would like to compare a couple of different response variables that are on different scales.  Would it make sense to standardize the response variable also?
  
My reply:  No, I donâ&euro;&trade;t standardize the binary input.  The point of standardizing inputs is to make the coefs directly interpretable, but with binary i</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 candidate in zoology, writes:    After reading your paper about scaling regression inputs by two standard deviations I found your  blog post  stating that you wished you had scaled by 1 sd and coded the binary inputs as -1 and 1. [sent-3, score-2.047]
</p><p>2 Here is my question:   If you code the binary input as -1 and 1, do you then standardize it? [sent-4, score-1.21]
</p><p>3 This makes sense to me because the mean of the standardized input is then zero and the sd is 1, which is what the mean and sd are for all of the other standardized inputs. [sent-5, score-1.383]
</p><p>4 I know that if you code the binary input as 0 and 1 it should not be standardized. [sent-6, score-0.701]
</p><p>5 Also, I am not interested in the actual units (i. [sent-7, score-0.173]
</p><p>6 mg/ml) of my response variable and I would like to compare a couple of different response variables that are on different scales. [sent-9, score-0.608]
</p><p>7 Would it make sense to standardize the response variable also? [sent-10, score-0.842]
</p><p>8 My reply:  No, I donâ&euro;&trade;t standardize the binary input. [sent-11, score-0.865]
</p><p>9 The point of standardizing inputs is to make the coefs directly interpretable, but with binary inputs the interpretation is already clear, since there is only one possible comparison. [sent-12, score-1.768]
</p><p>10 Unless the analysis is on the log scale (as in an elasticity model), in which case, again, the coefs are already directly interpretable. [sent-14, score-0.65]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('standardize', 0.509), ('binary', 0.356), ('inputs', 0.355), ('sd', 0.263), ('input', 0.234), ('coefs', 0.229), ('interpretable', 0.209), ('standardized', 0.184), ('response', 0.129), ('zoology', 0.127), ('standardizing', 0.12), ('elasticity', 0.115), ('code', 0.111), ('variable', 0.103), ('scaled', 0.1), ('directly', 0.096), ('coded', 0.095), ('scaling', 0.094), ('andy', 0.092), ('deviations', 0.088), ('stating', 0.087), ('units', 0.084), ('already', 0.081), ('log', 0.075), ('mean', 0.072), ('candidate', 0.064), ('continuous', 0.064), ('unless', 0.061), ('sense', 0.06), ('interpretation', 0.06), ('compare', 0.058), ('scale', 0.054), ('actual', 0.051), ('zero', 0.051), ('different', 0.05), ('also', 0.05), ('variables', 0.046), ('couple', 0.043), ('yes', 0.042), ('make', 0.041), ('reading', 0.041), ('regression', 0.041), ('standard', 0.04), ('possible', 0.039), ('clear', 0.038), ('interested', 0.038), ('reply', 0.037), ('since', 0.036), ('found', 0.035), ('post', 0.033)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="1462-tfidf-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-18-Standardizing_regression_inputs.html">1462 andrew gelman stats-2012-08-18-Standardizing regression inputs</a></p>
<p>Introduction: Andy Flies, Ph.D. candidate in zoology, writes:
  
After reading your paper about scaling regression inputs by two standard deviations I found your  blog post  stating that you wished you had scaled by 1 sd and coded the binary inputs as -1 and 1.  Here is my question:


If you code the binary input as -1 and 1, do you then standardize it?  This makes sense to me because the mean of the standardized input is then zero and the sd is 1, which is what the mean and sd are for all of the other standardized inputs.  I know that if you code the binary input as 0 and 1 it should not be standardized.


Also, I am not interested in the actual units (i.e. mg/ml) of my response variable and I would like to compare a couple of different response variables that are on different scales.  Would it make sense to standardize the response variable also?
  
My reply:  No, I donâ&euro;&trade;t standardize the binary input.  The point of standardizing inputs is to make the coefs directly interpretable, but with binary i</p><p>2 0.17525181 <a title="1462-tfidf-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-21-Finite-population_Anova_calculations_for_models_with_interactions.html">1686 andrew gelman stats-2013-01-21-Finite-population Anova calculations for models with interactions</a></p>
<p>Introduction: Jim Thomson writes:
  
I wonder if you could provide some clarification on the correct way to calculate the finite-population standard deviations for interaction terms in your Bayesian approach to ANOVA (as explained in your 2005 paper, and Gelman and Hill 2007).


I understand that it is the SD of the constrained batch coefficients that is of interest, but in most WinBUGS examples I have seen, the SDs are all calculated directly as sd.fin<-sd(beta.main[]) for main effects and sd(beta.int[,]) for interaction effects,  where beta.main and beta.int are the unconstrained coefficients, e.g. beta.int[i,j]~dnorm(0,tau).


For main effects, I can see that it makes no difference, since the constrained value is calculated by subtracting the mean, and sd(B[]) = sd(B[]-mean(B[])).


But the conventional sum-to-zero constraint for interaction terms in linear models is more complicated than subtracting the mean (there are only (n1-1)*(n2-1) free coefficients for an interaction b/w factors with n1 a</p><p>3 0.13450186 <a title="1462-tfidf-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-07-An_%28impressive%29_increase_in_survival_rate_from_50%25_to_60%25_corresponds_to_an_R-squared_of_%28only%29_1%25.__Counterintuitive%2C_huh%3F.html">1524 andrew gelman stats-2012-10-07-An (impressive) increase in survival rate from 50% to 60% corresponds to an R-squared of (only) 1%.  Counterintuitive, huh?</a></p>
<p>Introduction: I was just reading  an old post  and came across this example which I’d like to share with you again:
  
Here’s a story of R-squared = 1%. Consider a 0/1 outcome with about half the people in each category. For.example, half the people with some disease die in a year and half live. Now suppose there’s a treatment that increases survival rate from 50% to 60%. The unexplained sd is 0.5 and the explained sd is 0.05, hence R-squared is 0.01.</p><p>4 0.1321805 <a title="1462-tfidf-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-08-Displaying_a_fitted_multilevel_model.html">328 andrew gelman stats-2010-10-08-Displaying a fitted multilevel model</a></p>
<p>Introduction: Elissa Brown writes:
  
I’m working on some data using a multinomial model (3 categories for the response &  2 predictors-1 continuous and 1 binary), and I’ve been looking and looking for some sort of nice graphical way to show my model at work. Something like a predicted probabilities plot. I know you can do this for the levels of Y with just one covariate, but is this still a valid way to describe the multinomial model (just doing a pred plot for each covariate)? What’s the deal, is there really no way to graphically represent a successful multinomial model? Also, is it unreasonable to break down your model into a binary response just to get some ROC curves? This seems like cheating. From what I’ve found so far, it seems that people just avoid graphical support when discussing their fitted multinomial models.
  
My reply:
 
It’s hard for me to think about this sort of thing in the abstract with no context.  We do have one example in chapter 6 of ARM where we display data and fitted m</p><p>5 0.12855703 <a title="1462-tfidf-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-17-Graphical_tools_for_understanding_multilevel_models.html">772 andrew gelman stats-2011-06-17-Graphical tools for understanding multilevel models</a></p>
<p>Introduction: There are a few things I want to do:
 
1.  Understand a fitted model using tools such as  average predictive comparisons ,  R-squared, and partial pooling factors .  In defining these concepts, Iain and I came up with some clever tricks, including (but not limited to):
 
- Separating the inputs and averaging over all possible values of the input not being altered (for average predictive comparisons);
 
- Defining partial pooling  without  referring to a raw-data or maximum-likelihood or no-pooling estimate (these don’t necessarily exist when you’re fitting logistic regression with sparse data);
 
- Defining an R-squared for each level of a multilevel model.
 
The methods get pretty complicated, though, and they have some loose ends–in particular, for average predictive comparisons with continuous input variables.
 
So now we want to implement these in R and put them into arm along with bglmer etc.
 
2.  Setting up coefplot so it works more generally (that is, so the graphics look nice</p><p>6 0.11326243 <a title="1462-tfidf-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-27-Average_predictive_comparisons_when_changing_a_pair_of_variables.html">1346 andrew gelman stats-2012-05-27-Average predictive comparisons when changing a pair of variables</a></p>
<p>7 0.11135457 <a title="1462-tfidf-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-24-Estimating_and_summarizing_inference_for_hierarchical_variance_parameters_when_the_number_of_groups_is_small.html">2145 andrew gelman stats-2013-12-24-Estimating and summarizing inference for hierarchical variance parameters when the number of groups is small</a></p>
<p>8 0.10696086 <a title="1462-tfidf-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-25-Continuous_variables_in_Bayesian_networks.html">1228 andrew gelman stats-2012-03-25-Continuous variables in Bayesian networks</a></p>
<p>9 0.10504217 <a title="1462-tfidf-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-03-A_psychology_researcher_asks%3A__Is_Anova_dead%3F.html">888 andrew gelman stats-2011-09-03-A psychology researcher asks:  Is Anova dead?</a></p>
<p>10 0.096885934 <a title="1462-tfidf-10" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-08-How_to_display_multinominal_logit_results_graphically%3F.html">2163 andrew gelman stats-2014-01-08-How to display multinominal logit results graphically?</a></p>
<p>11 0.092817254 <a title="1462-tfidf-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-21-Avoiding_boundary_estimates_in_linear_mixed_models.html">918 andrew gelman stats-2011-09-21-Avoiding boundary estimates in linear mixed models</a></p>
<p>12 0.086421706 <a title="1462-tfidf-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-08-I_Am_Too_Absolutely_Heteroskedastic_for_This_Probit_Model.html">1047 andrew gelman stats-2011-12-08-I Am Too Absolutely Heteroskedastic for This Probit Model</a></p>
<p>13 0.085405029 <a title="1462-tfidf-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-01-Imputing_count_data.html">14 andrew gelman stats-2010-05-01-Imputing count data</a></p>
<p>14 0.084697433 <a title="1462-tfidf-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-15-R-squared_for_multilevel_models.html">1121 andrew gelman stats-2012-01-15-R-squared for multilevel models</a></p>
<p>15 0.084681734 <a title="1462-tfidf-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-22-The_scaled_inverse_Wishart_prior_distribution_for_a_covariance_matrix_in_a_hierarchical_model.html">1466 andrew gelman stats-2012-08-22-The scaled inverse Wishart prior distribution for a covariance matrix in a hierarchical model</a></p>
<p>16 0.083810858 <a title="1462-tfidf-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-15-Exploratory_multilevel_analysis_when_group-level_variables_are_of_importance.html">1900 andrew gelman stats-2013-06-15-Exploratory multilevel analysis when group-level variables are of importance</a></p>
<p>17 0.083556592 <a title="1462-tfidf-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-21-Building_a_regression_model_._._._with_only_27_data_points.html">1506 andrew gelman stats-2012-09-21-Building a regression model . . . with only 27 data points</a></p>
<p>18 0.083101951 <a title="1462-tfidf-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-14-Questions_about_a_study_of_charter_schools.html">957 andrew gelman stats-2011-10-14-Questions about a study of charter schools</a></p>
<p>19 0.08159741 <a title="1462-tfidf-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-28-Simplify_until_your_fake-data_check_works%2C_then_add_complications_until_you_can_figure_out_where_the_problem_is_coming_from.html">1875 andrew gelman stats-2013-05-28-Simplify until your fake-data check works, then add complications until you can figure out where the problem is coming from</a></p>
<p>20 0.080939993 <a title="1462-tfidf-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Allowing_interaction_terms_to_vary.html">753 andrew gelman stats-2011-06-09-Allowing interaction terms to vary</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.093), (1, 0.054), (2, 0.04), (3, -0.002), (4, 0.074), (5, -0.01), (6, 0.023), (7, -0.037), (8, 0.033), (9, 0.045), (10, 0.023), (11, 0.024), (12, -0.003), (13, -0.034), (14, 0.008), (15, 0.039), (16, -0.002), (17, -0.014), (18, -0.006), (19, 0.019), (20, 0.02), (21, 0.046), (22, 0.017), (23, -0.016), (24, -0.008), (25, 0.007), (26, 0.016), (27, -0.031), (28, -0.012), (29, -0.038), (30, 0.032), (31, 0.047), (32, 0.012), (33, 0.026), (34, 0.024), (35, -0.04), (36, 0.024), (37, 0.012), (38, -0.023), (39, 0.003), (40, 0.003), (41, -0.034), (42, 0.012), (43, 0.001), (44, -0.024), (45, 0.026), (46, 0.003), (47, -0.002), (48, 0.019), (49, 0.036)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96011138 <a title="1462-lsi-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-18-Standardizing_regression_inputs.html">1462 andrew gelman stats-2012-08-18-Standardizing regression inputs</a></p>
<p>Introduction: Andy Flies, Ph.D. candidate in zoology, writes:
  
After reading your paper about scaling regression inputs by two standard deviations I found your  blog post  stating that you wished you had scaled by 1 sd and coded the binary inputs as -1 and 1.  Here is my question:


If you code the binary input as -1 and 1, do you then standardize it?  This makes sense to me because the mean of the standardized input is then zero and the sd is 1, which is what the mean and sd are for all of the other standardized inputs.  I know that if you code the binary input as 0 and 1 it should not be standardized.


Also, I am not interested in the actual units (i.e. mg/ml) of my response variable and I would like to compare a couple of different response variables that are on different scales.  Would it make sense to standardize the response variable also?
  
My reply:  No, I donâ&euro;&trade;t standardize the binary input.  The point of standardizing inputs is to make the coefs directly interpretable, but with binary i</p><p>2 0.78160429 <a title="1462-lsi-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-01-Imputing_count_data.html">14 andrew gelman stats-2010-05-01-Imputing count data</a></p>
<p>Introduction: Guy asks:
  
I am analyzing an original survey of farmers in Uganda. I am hoping to use a battery of welfare proxy variables to create a single welfare index using PCA. I have quick question which I hope you can find time to address:


How do you recommend treating count data? (for example # of rooms, # of chickens, # of cows, # of radios)? In my dataset these variables are highly skewed with many responses at zero (which makes taking the natural log problematic). In the case of # of cows or chickens several obs have values in the hundreds.
  
My response:  Hereâ&euro;&trade;s what we do in our mi package in R.  We split a variable into two parts:  an indicator for whether it is positive, and the positive part.  That is, y = u*v.  Then u is binary and can be modeled using logisitc regression, and v can be modeled on the log scale.  At the end you can round to the nearest integer  if you want to avoid fractional values.</p><p>3 0.7695545 <a title="1462-lsi-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-04-Question_about_standard_range_for_social_science_correlations.html">257 andrew gelman stats-2010-09-04-Question about standard range for social science correlations</a></p>
<p>Introduction: Andrew Eppig writes:
  
I’m a physicist by training who is transitioning to the social sciences. I recently came across a  reference  in the Economist to a paper on IQ and parasites which I read as I have more than a passing interest in IQ research (having read much that you and others (e.g., Shalizi, Wicherts) have written). In this paper I note that the authors find a very high correlation between national IQ and parasite prevalence. The strength of the correlation (-0.76 to -0.82) surprised me, as I’m used to much weaker correlations in the social sciences. To me, it’s a bit too high, suggesting that there are other factors at play or that one of the variables is merely a proxy for a large number of other variables. But I have no basis for this other than a gut feeling and a memory of a plot on  Language Log  about the distribution of correlation coefficients in social psychology.


So my question is this: Is a correlation in the range of (-0.82,-0.76) more likely to be a correlatio</p><p>4 0.76511937 <a title="1462-lsi-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-15-R-squared_for_multilevel_models.html">1121 andrew gelman stats-2012-01-15-R-squared for multilevel models</a></p>
<p>Introduction: Fred Schiff writes: 
  
  
I’m writing to you to ask about the “R-squared” approximation procedure you suggest in your 2004 book with Dr. Hill.  [See also  this paper  with Pardoe---ed.]


I’m a media sociologist at the University of Houston.  I’ve been using HLM3 for about two years.  


Briefly about my data.  It’s a content analysis of news stories with a continuous scale dependent variable, story prominence.  I have 6090 news stories, 114 newspapers, and 59 newspaper group owners.  All the Level-1, Level-2 and dependent variables have been standardized. Since the means were zero anyway, we left the variables uncentered.  All the Level-3 ownership groups and characteristics are dichotomous scales that were left uncentered.  


PROBLEM:  The single most important result I am looking for is to compare the strength of nine competing Level-1 variables in their ability to predict and explain the outcome variable, story prominence.  We are trying to use the residuals to calculate a “R-squ</p><p>5 0.76362628 <a title="1462-lsi-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-21-Finite-population_Anova_calculations_for_models_with_interactions.html">1686 andrew gelman stats-2013-01-21-Finite-population Anova calculations for models with interactions</a></p>
<p>Introduction: Jim Thomson writes:
  
I wonder if you could provide some clarification on the correct way to calculate the finite-population standard deviations for interaction terms in your Bayesian approach to ANOVA (as explained in your 2005 paper, and Gelman and Hill 2007).


I understand that it is the SD of the constrained batch coefficients that is of interest, but in most WinBUGS examples I have seen, the SDs are all calculated directly as sd.fin<-sd(beta.main[]) for main effects and sd(beta.int[,]) for interaction effects,  where beta.main and beta.int are the unconstrained coefficients, e.g. beta.int[i,j]~dnorm(0,tau).


For main effects, I can see that it makes no difference, since the constrained value is calculated by subtracting the mean, and sd(B[]) = sd(B[]-mean(B[])).


But the conventional sum-to-zero constraint for interaction terms in linear models is more complicated than subtracting the mean (there are only (n1-1)*(n2-1) free coefficients for an interaction b/w factors with n1 a</p><p>6 0.76269859 <a title="1462-lsi-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-21-Interpreting_interactions_in_discrete-data_regression.html">1908 andrew gelman stats-2013-06-21-Interpreting interactions in discrete-data regression</a></p>
<p>7 0.75031149 <a title="1462-lsi-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-15-Still_more_Mr._P_in_public_health.html">770 andrew gelman stats-2011-06-15-Still more Mr. P in public health</a></p>
<p>8 0.74381483 <a title="1462-lsi-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-04-What_are_the_key_assumptions_of_linear_regression%3F.html">1967 andrew gelman stats-2013-08-04-What are the key assumptions of linear regression?</a></p>
<p>9 0.74210322 <a title="1462-lsi-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-31-Using_factor_analysis_or_principal_components_analysis_or_measurement-error_models_for_biological_measurements_in_archaeology%3F.html">1094 andrew gelman stats-2011-12-31-Using factor analysis or principal components analysis or measurement-error models for biological measurements in archaeology?</a></p>
<p>10 0.73767054 <a title="1462-lsi-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-03-A_psychology_researcher_asks%3A__Is_Anova_dead%3F.html">888 andrew gelman stats-2011-09-03-A psychology researcher asks:  Is Anova dead?</a></p>
<p>11 0.73163581 <a title="1462-lsi-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-18-Check_your_missing-data_imputations_using_cross-validation.html">1218 andrew gelman stats-2012-03-18-Check your missing-data imputations using cross-validation</a></p>
<p>12 0.71241832 <a title="1462-lsi-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-14-The_robust_beauty_of_improper_linear_models_in_decision_making.html">1981 andrew gelman stats-2013-08-14-The robust beauty of improper linear models in decision making</a></p>
<p>13 0.70537353 <a title="1462-lsi-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-02-Interaction-based_feature_selection_and_classification_for_high-dimensional_biological_data.html">1703 andrew gelman stats-2013-02-02-Interaction-based feature selection and classification for high-dimensional biological data</a></p>
<p>14 0.70158958 <a title="1462-lsi-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-28-Hierarchical_ordered_logit_or_probit.html">684 andrew gelman stats-2011-04-28-Hierarchical ordered logit or probit</a></p>
<p>15 0.69358057 <a title="1462-lsi-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-02-Interactions_of_predictors_in_a_causal_model.html">251 andrew gelman stats-2010-09-02-Interactions of predictors in a causal model</a></p>
<p>16 0.69163126 <a title="1462-lsi-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-07-There_are_never_70_distinct_parameters.html">327 andrew gelman stats-2010-10-07-There are never 70 distinct parameters</a></p>
<p>17 0.68904328 <a title="1462-lsi-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Allowing_interaction_terms_to_vary.html">753 andrew gelman stats-2011-06-09-Allowing interaction terms to vary</a></p>
<p>18 0.68496603 <a title="1462-lsi-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-28-Matching_for_preprocessing_data_for_causal_inference.html">375 andrew gelman stats-2010-10-28-Matching for preprocessing data for causal inference</a></p>
<p>19 0.681431 <a title="1462-lsi-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-05-Understanding_regression_models_and_regression_coefficients.html">1656 andrew gelman stats-2013-01-05-Understanding regression models and regression coefficients</a></p>
<p>20 0.68128645 <a title="1462-lsi-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-01-Modeling_y_%3D_a_%2B_b_%2B_c.html">1294 andrew gelman stats-2012-05-01-Modeling y = a + b + c</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(9, 0.025), (11, 0.162), (16, 0.048), (18, 0.043), (24, 0.08), (52, 0.042), (76, 0.03), (84, 0.034), (87, 0.016), (89, 0.023), (93, 0.021), (99, 0.341)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96062142 <a title="1462-lda-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-18-Standardizing_regression_inputs.html">1462 andrew gelman stats-2012-08-18-Standardizing regression inputs</a></p>
<p>Introduction: Andy Flies, Ph.D. candidate in zoology, writes:
  
After reading your paper about scaling regression inputs by two standard deviations I found your  blog post  stating that you wished you had scaled by 1 sd and coded the binary inputs as -1 and 1.  Here is my question:


If you code the binary input as -1 and 1, do you then standardize it?  This makes sense to me because the mean of the standardized input is then zero and the sd is 1, which is what the mean and sd are for all of the other standardized inputs.  I know that if you code the binary input as 0 and 1 it should not be standardized.


Also, I am not interested in the actual units (i.e. mg/ml) of my response variable and I would like to compare a couple of different response variables that are on different scales.  Would it make sense to standardize the response variable also?
  
My reply:  No, I donâ&euro;&trade;t standardize the binary input.  The point of standardizing inputs is to make the coefs directly interpretable, but with binary i</p><p>2 0.93575096 <a title="1462-lda-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-10-My_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1311 andrew gelman stats-2012-05-10-My final exam for Design and Analysis of Sample Surveys</a></p>
<p>Introduction: We had 28 class periods, so I wrote an exam with an approximate correspondence of one question per class.  Rather than dumping the exam in your lap all at once, I’ll post the questions once per day.  Then each day I’ll post the answer to yesterday’s questions.  So it will be 29 days in all.  I’ll post them to appear late in the day so as not to interfere with our main daily posts (which are currently backed up to early June).
 
The course was offered in the political science department and covered a mix of statistical and political topics.  Followers of our  recent discussion  on test questions won’t be surprised to learn that some of the questions are ambiguous.  This wasn’t on purpose.  I tried my best, but good questions are hard to write.
 
Question 1 will appear tomorrow.</p><p>3 0.93572199 <a title="1462-lda-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-22-The_scaled_inverse_Wishart_prior_distribution_for_a_covariance_matrix_in_a_hierarchical_model.html">1466 andrew gelman stats-2012-08-22-The scaled inverse Wishart prior distribution for a covariance matrix in a hierarchical model</a></p>
<p>Introduction: Since we’re  talking  about the scaled inverse Wishart . . . here’s a recent message from Chris Chatham:
 
I have been reading your book on Bayesian Hierarchical/Multilevel Modeling but have been struggling a bit with deciding whether to model my multivariate normal distribution using the scaled inverse Wishart approach you advocate given the arguments at  this blog post  [entitled "Why an inverse-Wishart prior may not be such a good idea"].
 
My reply:  We discuss this in our book.  We know the inverse-Wishart has problems, that’s why we recommend the  scaled  inverse-Wishart, which is a more general class of models.   Here ‘s an old blog post on the topic.  And also of course there’s the description in our book.
 
Chris pointed me to the following  comment  by Simon Barthelmé:
  
Using the scaled inverse Wishart doesn’t change anything, the standard deviations of the invidual coefficients and their covariance are still dependent. My answer would be to use a prior that models the stan</p><p>4 0.93137366 <a title="1462-lda-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-08-Blogging%3A__Is_it_%E2%80%9Cfair_use%E2%80%9D%3F.html">458 andrew gelman stats-2010-12-08-Blogging:  Is it “fair use”?</a></p>
<p>Introduction: Dave Kane writes:
  
I [Kane] am involved in a dispute relating to whether or not a blog can be considered part of one’s academic writing.  Williams College restricts the use of undergraduate theses as follows:

 
Non-commercial, academic use within the scope of “Fair Use” standards is acceptable. Otherwise, you may not copy or distribute any content without the permission of the copyright holder.
 

Seems obvious enough. Yet some folks think that my use of thesis material in a blog post fails this test because it is not “academic.” See  this post  for the gory details.
      
Parenthetically, your readers might be interested in the substantive discovery here, the details of the Williams admissions process (which is probably very similar to Columbia’s). Williams places students into academic rating (AR) categories as follows:


      verbal   math   composite SAT II   ACT    AP 
AR 1: 770-800 750-800 1520-1600 750-800 35-36 mostly 5s 
AR 2: 730-770 720-750 1450-1520 720-770 33-34 4s an</p><p>5 0.91992766 <a title="1462-lda-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-30-%E2%80%9CPresidential_Election_Outcomes_Directly_Influence_Suicide_Rates%E2%80%9D.html">382 andrew gelman stats-2010-10-30-“Presidential Election Outcomes Directly Influence Suicide Rates”</a></p>
<p>Introduction: This came in the spam the other day:
  
College Station, TX–August 16, 2010–Change and hope were central themes to the November 2008 U.S. presidential election. A new longitudinal study published in the September issue of  Social Science Quarterly   analyzes suicide rates at a state level from 1981ï¼2005 and determines that presidential election outcomes directly influence suicide rates among voters.


In states where the majority of voters supported the national election winner suicide rates decreased. However, counter-intuitively, suicide rates decreased even more dramatically in states where the majority of voters supported the election loser (4.6 percent lower for males and 5.3 lower for females). This article is the first in its field to focus on candidate and state-specific outcomes in relation to suicide rates. Prior research on this topic focused on whether the election process itself influenced suicide rates, and found that suicide rates fell during the election season.


Ric</p><p>6 0.91753978 <a title="1462-lda-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-28-World_Economic_Forum_Data_Visualization_Challenge.html">378 andrew gelman stats-2010-10-28-World Economic Forum Data Visualization Challenge</a></p>
<p>7 0.91737944 <a title="1462-lda-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-21-Belief_in_hell_is_associated_with_lower_crime_rates.html">1386 andrew gelman stats-2012-06-21-Belief in hell is associated with lower crime rates</a></p>
<p>8 0.91250134 <a title="1462-lda-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-12-%E2%80%9CTeaching_effectiveness%E2%80%9D_as_another_dimension_in_cognitive_ability.html">1620 andrew gelman stats-2012-12-12-“Teaching effectiveness” as another dimension in cognitive ability</a></p>
<p>9 0.9117201 <a title="1462-lda-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-22-%E2%80%9CGross_misuse_of_statistics%E2%80%9D_can_be_a_good_thing%2C_if_it_indicates_the_acceptance_of_the_importance_of_statistical_reasoning.html">1276 andrew gelman stats-2012-04-22-“Gross misuse of statistics” can be a good thing, if it indicates the acceptance of the importance of statistical reasoning</a></p>
<p>10 0.90732062 <a title="1462-lda-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-03-Is_Harvard_hurting_poor_kids_by_cutting_tuition_for_the_upper_middle_class%3F.html">598 andrew gelman stats-2011-03-03-Is Harvard hurting poor kids by cutting tuition for the upper middle class?</a></p>
<p>11 0.90395224 <a title="1462-lda-11" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-18-A_course_in_sample_surveys_for_political_science.html">2175 andrew gelman stats-2014-01-18-A course in sample surveys for political science</a></p>
<p>12 0.89861906 <a title="1462-lda-12" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-10-What_property_is_important_in_a_risk_prediction_model%3F_Discrimination_or_calibration%3F.html">2328 andrew gelman stats-2014-05-10-What property is important in a risk prediction model? Discrimination or calibration?</a></p>
<p>13 0.8975867 <a title="1462-lda-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-21-Will_Tiger_Woods_catch_Jack_Nicklaus%3F__And_a_discussion_of_the_virtues_of_using_continuous_data_even_if_your_goal_is_discrete_prediction.html">1387 andrew gelman stats-2012-06-21-Will Tiger Woods catch Jack Nicklaus?  And a discussion of the virtues of using continuous data even if your goal is discrete prediction</a></p>
<p>14 0.89488125 <a title="1462-lda-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-06-Yes%2C_checking_calibration_of_probability_forecasts_is_part_of_Bayesian_statistics.html">1610 andrew gelman stats-2012-12-06-Yes, checking calibration of probability forecasts is part of Bayesian statistics</a></p>
<p>15 0.89369684 <a title="1462-lda-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-03-%E2%80%9CDo_you_guys_pay_your_bills%3F%E2%80%9D.html">1193 andrew gelman stats-2012-03-03-“Do you guys pay your bills?”</a></p>
<p>16 0.8936196 <a title="1462-lda-16" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-05-On_deck_this_month.html">2320 andrew gelman stats-2014-05-05-On deck this month</a></p>
<p>17 0.89285541 <a title="1462-lda-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-20-Displaying_inferences_from_complex_models.html">1815 andrew gelman stats-2013-04-20-Displaying inferences from complex models</a></p>
<p>18 0.89194989 <a title="1462-lda-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-17-G%2B_hangout_for_test_run_of_BDA_course.html">2066 andrew gelman stats-2013-10-17-G+ hangout for test run of BDA course</a></p>
<p>19 0.89167833 <a title="1462-lda-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-12-UnConMax_%E2%80%93_uncertainty_consideration_maxims_7_%2B--_2.html">82 andrew gelman stats-2010-06-12-UnConMax – uncertainty consideration maxims 7 +-- 2</a></p>
<p>20 0.8908304 <a title="1462-lda-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-28-Econ_coauthorship_update.html">1917 andrew gelman stats-2013-06-28-Econ coauthorship update</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
