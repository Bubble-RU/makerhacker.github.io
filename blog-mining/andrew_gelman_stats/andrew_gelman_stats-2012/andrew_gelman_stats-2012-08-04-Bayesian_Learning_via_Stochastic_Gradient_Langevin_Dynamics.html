<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1443 andrew gelman stats-2012-08-04-Bayesian Learning via Stochastic Gradient Langevin Dynamics</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2012" href="../home/andrew_gelman_stats-2012_home.html">andrew_gelman_stats-2012</a> <a title="andrew_gelman_stats-2012-1443" href="#">andrew_gelman_stats-2012-1443</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1443 andrew gelman stats-2012-08-04-Bayesian Learning via Stochastic Gradient Langevin Dynamics</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2012-1443-html" href="http://andrewgelman.com/2012/08/04/bayesian-learning-via-stochastic-gradient-langevin-dynamics/">html</a></p><p>Introduction: Burak Bayramli writes:
  
In  this paper  by Sunjin Ahn, Anoop Korattikara, and Max Welling and  this paper  by Welling and Yee Whye The, there are some arguments on big data and the use of MCMC. Both papers have suggested improvements to speed up MCMC computations. I was wondering what your thoughts were, especially on this paragraph:

 
When a dataset has a billion data-cases (as is not uncommon these days) MCMC algorithms will not even have generated a single (burn-in) sample when a clever learning algorithm based on stochastic gradients may already be making fairly good predictions. In fact, the intriguing results of Bottou and Bousquet (2008) seem to indicate that in terms of “number of bits learned per unit of computation”, an algorithm as simple as stochastic gradient descent is almost optimally efficient. We therefore argue that for Bayesian methods to remain useful in an age when the datasets grow at an exponential rate, they need to embrace the ideas of the stochastic optimiz</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Burak Bayramli writes:    In  this paper  by Sunjin Ahn, Anoop Korattikara, and Max Welling and  this paper  by Welling and Yee Whye The, there are some arguments on big data and the use of MCMC. [sent-1, score-0.15]
</p><p>2 Both papers have suggested improvements to speed up MCMC computations. [sent-2, score-0.273]
</p><p>3 In fact, the intriguing results of Bottou and Bousquet (2008) seem to indicate that in terms of “number of bits learned per unit of computation”, an algorithm as simple as stochastic gradient descent is almost optimally efficient. [sent-4, score-1.083]
</p><p>4 We therefore argue that for Bayesian methods to remain useful in an age when the datasets grow at an exponential rate, they need to embrace the ideas of the stochastic optimization literature. [sent-5, score-0.836]
</p><p>5 ”    My [Bayramli's] argument against this is that Bayesian models are more expressive, and coupled with MCMC they allowed researchers to solve previously impossible problems. [sent-6, score-0.17]
</p><p>6 My reply:   I glanced at the papers only quickly but the general idea makes sense. [sent-8, score-0.235]
</p><p>7 I’ve thought for awhile that the Bayesian central limit theorem should allow efficient inference via data partitioning, but my only attempt was not particularly successful (which is why  this  2005 paper with Zaiying Huang is unpublished; in fact I don’t even recall if we submitted it anywhere). [sent-9, score-0.153]
</p><p>8 I also feel warmly about ideas of combining stochastic optimization with Hamiltonian dynamics and MCMC sampling, as this is what we are doing with  Nuts . [sent-12, score-0.641]
</p><p>9 Finally, it often seems that methodological advances come from solving applied problems that are in our way. [sent-13, score-0.248]
</p><p>10 I like the papers you link to because they appear to be motivated by new applications rather than being new methods applied to benchmark problems. [sent-14, score-0.296]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('stochastic', 0.324), ('mcmc', 0.288), ('ahn', 0.264), ('welling', 0.264), ('bayramli', 0.24), ('optimization', 0.151), ('algorithm', 0.127), ('papers', 0.122), ('burak', 0.12), ('yee', 0.12), ('zaiying', 0.113), ('huang', 0.113), ('glanced', 0.113), ('partitioning', 0.113), ('expressive', 0.108), ('optimally', 0.105), ('descent', 0.099), ('uncommon', 0.099), ('embrace', 0.099), ('coupled', 0.097), ('gradients', 0.097), ('exponential', 0.095), ('terms', 0.094), ('bayesian', 0.094), ('benchmark', 0.093), ('gradient', 0.091), ('unpublished', 0.09), ('intriguing', 0.09), ('grow', 0.088), ('advances', 0.087), ('max', 0.087), ('dynamics', 0.087), ('billion', 0.083), ('nuts', 0.083), ('generated', 0.081), ('applied', 0.081), ('solving', 0.08), ('ideas', 0.079), ('bits', 0.079), ('hamiltonian', 0.079), ('anywhere', 0.079), ('fact', 0.078), ('solved', 0.077), ('improvements', 0.077), ('paper', 0.075), ('unit', 0.074), ('clever', 0.074), ('speed', 0.074), ('previously', 0.073), ('algorithms', 0.072)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="1443-tfidf-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-04-Bayesian_Learning_via_Stochastic_Gradient_Langevin_Dynamics.html">1443 andrew gelman stats-2012-08-04-Bayesian Learning via Stochastic Gradient Langevin Dynamics</a></p>
<p>Introduction: Burak Bayramli writes:
  
In  this paper  by Sunjin Ahn, Anoop Korattikara, and Max Welling and  this paper  by Welling and Yee Whye The, there are some arguments on big data and the use of MCMC. Both papers have suggested improvements to speed up MCMC computations. I was wondering what your thoughts were, especially on this paragraph:

 
When a dataset has a billion data-cases (as is not uncommon these days) MCMC algorithms will not even have generated a single (burn-in) sample when a clever learning algorithm based on stochastic gradients may already be making fairly good predictions. In fact, the intriguing results of Bottou and Bousquet (2008) seem to indicate that in terms of “number of bits learned per unit of computation”, an algorithm as simple as stochastic gradient descent is almost optimally efficient. We therefore argue that for Bayesian methods to remain useful in an age when the datasets grow at an exponential rate, they need to embrace the ideas of the stochastic optimiz</p><p>2 0.13424832 <a title="1443-tfidf-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-26-A_statistician%E2%80%99s_rants_and_raves.html">1185 andrew gelman stats-2012-02-26-A statistician’s rants and raves</a></p>
<p>Introduction: Not from me, from  Dean Foster , who maybe was in the same stochastic processes course with me, thirty years ago.</p><p>3 0.13338956 <a title="1443-tfidf-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-24-F-f-f-fake_data.html">1735 andrew gelman stats-2013-02-24-F-f-f-fake data</a></p>
<p>Introduction: Tiago Fragoso writes:
  
Suppose I fit a two stage regression model


Y = a + bx + e 
a = cw + d + e1


I could fit it all in one step by using MCMC for example (my model is more complicated than that, so I’ll have to do it by MCMC). However, I could fit the first regression only using MCMC because those estimates are hard to obtain and perform the second regression using least squares or a separate MCMC. 


So there’s an ‘one step’ inference based on doing it all at the same time and a ‘two step’ inference by fitting one and using the estimates on the further steps. What is gained or lost between both? Is anything done in this question?
  
My response:
 
Rather than answering your particular question, I’ll give you my generic answer, which is to simulate fake data from your model, then fit your model both ways and see how the results differ.  Repeat the simulation a few thousand times and you can make all the statistical comparisons you like.</p><p>4 0.12405703 <a title="1443-tfidf-4" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-03-Running_into_a_Stan_Reference_by_Accident.html">2231 andrew gelman stats-2014-03-03-Running into a Stan Reference by Accident</a></p>
<p>Introduction: We were talking about parallelizing MCMC and I came up with what I thought was a neat idea for parallelizing MCMC (sample with fractional prior, average samples on a per-draw basis).  But then I realized this approach could get the right posterior mean or right posterior variance, but not both, depending on how the prior was divided (for a beta-binomial example).  Then  Aki  told me it had already been done in a more general form in a paper of Scott et al.,  Bayes and Big Data , which was then used as the baseline in: 
 
Willie Neiswanger, Chong Wang, and Eric Xing. 2013.   Asymptotically Exact, Embarrassingly Parallel MCMC .  arXiv  1311.4780. 
 
It’s a neat paper, which Xi’an  already blogged  about months ago.  But what really struck me was the following quote:
  

We use Stan, an automated Hamiltonian Monte Carlo (HMC) software package, to perform sampling for both the true posterior (for groundtruth and comparison methods) and for the subposteriors on each machine. One advantage o</p><p>5 0.12043784 <a title="1443-tfidf-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-18-Derivative-based_MCMC_as_a_breakthrough_technique_for_implementing_Bayesian_statistics.html">419 andrew gelman stats-2010-11-18-Derivative-based MCMC as a breakthrough technique for implementing Bayesian statistics</a></p>
<p>Introduction: John Salvatier pointed me to  this blog  on derivative based MCMC algorithms (also sometimes called “hybrid” or “Hamiltonian” Monte Carlo) and automatic differentiation as the future of MCMC.
 
This all makes sense to me and is consistent both with my mathematical intuition from studying Metropolis algorithms and my experience with Matt using hybrid MCMC when fitting hierarchical spline models. In particular, I agree with Salvatier’s point about the potential for computation of analytic derivatives of the log-density function.  As long as we’re mostly snapping together our models using analytically-simple pieces, the same part of the program that handles the computation of log-posterior densities should also be able to compute derivatives analytically.
 
I’ve been a big fan of automatic derivative-based MCMC methods since I started hearing about them a couple years ago (I’m thinking of the DREAM project and of Mark Girolami’s paper), and I too wonder why they haven’t been used more.  I</p><p>6 0.11399738 <a title="1443-tfidf-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-30-Stan_uses_Nuts%21.html">1036 andrew gelman stats-2011-11-30-Stan uses Nuts!</a></p>
<p>7 0.11395516 <a title="1443-tfidf-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-19-Will_Stan_work_well_with_40%C3%9740_matrices%3F.html">861 andrew gelman stats-2011-08-19-Will Stan work well with 40×40 matrices?</a></p>
<p>8 0.10897515 <a title="1443-tfidf-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-09-Commercial_Bayesian_inference_software_is_popping_up_all_over.html">1489 andrew gelman stats-2012-09-09-Commercial Bayesian inference software is popping up all over</a></p>
<p>9 0.10468104 <a title="1443-tfidf-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-09-Special_journal_issue_on_statistical_methods_for_the_social_sciences.html">24 andrew gelman stats-2010-05-09-Special journal issue on statistical methods for the social sciences</a></p>
<p>10 0.10248343 <a title="1443-tfidf-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-27-Bridges_between_deterministic_and_probabilistic_models_for_binary_data.html">780 andrew gelman stats-2011-06-27-Bridges between deterministic and probabilistic models for binary data</a></p>
<p>11 0.10197467 <a title="1443-tfidf-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-25-Ways_of_knowing.html">1469 andrew gelman stats-2012-08-25-Ways of knowing</a></p>
<p>12 0.10039257 <a title="1443-tfidf-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-15-What_are_the_trickiest_models_to_fit%3F.html">575 andrew gelman stats-2011-02-15-What are the trickiest models to fit?</a></p>
<p>13 0.10018 <a title="1443-tfidf-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-17-Probability-processing_hardware.html">214 andrew gelman stats-2010-08-17-Probability-processing hardware</a></p>
<p>14 0.091388561 <a title="1443-tfidf-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-22-My_talks_that_were_scheduled_for_Tues_at_the_Data_Skeptics_meetup_and_Wed_at_the_Open_Statistical_Programming_meetup.html">1950 andrew gelman stats-2013-07-22-My talks that were scheduled for Tues at the Data Skeptics meetup and Wed at the Open Statistical Programming meetup</a></p>
<p>15 0.090905428 <a title="1443-tfidf-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-28-Economists_argue_about_Bayes.html">1695 andrew gelman stats-2013-01-28-Economists argue about Bayes</a></p>
<p>16 0.088639006 <a title="1443-tfidf-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-20-The_pervasive_twoishness_of_statistics%3B_in_particular%2C_the_%E2%80%9Csampling_distribution%E2%80%9D_and_the_%E2%80%9Clikelihood%E2%80%9D_are_two_different_models%2C_and_that%E2%80%99s_a_good_thing.html">774 andrew gelman stats-2011-06-20-The pervasive twoishness of statistics; in particular, the “sampling distribution” and the “likelihood” are two different models, and that’s a good thing</a></p>
<p>17 0.088613056 <a title="1443-tfidf-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-11-Convergence_Monitoring_for_Non-Identifiable_and_Non-Parametric_Models.html">1374 andrew gelman stats-2012-06-11-Convergence Monitoring for Non-Identifiable and Non-Parametric Models</a></p>
<p>18 0.088175438 <a title="1443-tfidf-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-30-Computational_problems_with_glm_etc..html">1516 andrew gelman stats-2012-09-30-Computational problems with glm etc.</a></p>
<p>19 0.087148272 <a title="1443-tfidf-19" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-12-More_on_publishing_in_journals.html">2245 andrew gelman stats-2014-03-12-More on publishing in journals</a></p>
<p>20 0.08607547 <a title="1443-tfidf-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-24-Yet_another_Bayesian_job_opportunity.html">231 andrew gelman stats-2010-08-24-Yet another Bayesian job opportunity</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.153), (1, 0.069), (2, -0.057), (3, -0.006), (4, -0.007), (5, 0.039), (6, -0.046), (7, -0.063), (8, 0.001), (9, -0.024), (10, 0.016), (11, -0.029), (12, -0.059), (13, 0.001), (14, 0.016), (15, 0.003), (16, 0.044), (17, 0.015), (18, -0.041), (19, -0.004), (20, 0.01), (21, 0.025), (22, -0.014), (23, 0.003), (24, 0.028), (25, 0.017), (26, -0.033), (27, 0.054), (28, 0.075), (29, 0.001), (30, 0.013), (31, 0.005), (32, 0.028), (33, -0.012), (34, 0.024), (35, -0.023), (36, -0.026), (37, 0.006), (38, -0.034), (39, 0.028), (40, -0.012), (41, 0.031), (42, -0.01), (43, -0.01), (44, 0.029), (45, -0.028), (46, 0.003), (47, 0.021), (48, 0.045), (49, -0.02)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96121585 <a title="1443-lsi-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-04-Bayesian_Learning_via_Stochastic_Gradient_Langevin_Dynamics.html">1443 andrew gelman stats-2012-08-04-Bayesian Learning via Stochastic Gradient Langevin Dynamics</a></p>
<p>Introduction: Burak Bayramli writes:
  
In  this paper  by Sunjin Ahn, Anoop Korattikara, and Max Welling and  this paper  by Welling and Yee Whye The, there are some arguments on big data and the use of MCMC. Both papers have suggested improvements to speed up MCMC computations. I was wondering what your thoughts were, especially on this paragraph:

 
When a dataset has a billion data-cases (as is not uncommon these days) MCMC algorithms will not even have generated a single (burn-in) sample when a clever learning algorithm based on stochastic gradients may already be making fairly good predictions. In fact, the intriguing results of Bottou and Bousquet (2008) seem to indicate that in terms of “number of bits learned per unit of computation”, an algorithm as simple as stochastic gradient descent is almost optimally efficient. We therefore argue that for Bayesian methods to remain useful in an age when the datasets grow at an exponential rate, they need to embrace the ideas of the stochastic optimiz</p><p>2 0.75040025 <a title="1443-lsi-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-15-What_are_the_trickiest_models_to_fit%3F.html">575 andrew gelman stats-2011-02-15-What are the trickiest models to fit?</a></p>
<p>Introduction: John Salvatier writes:
  
What do you and your readers think are the trickiest models to fit? If I had an algorithm that I claimed could fit many models with little fuss, what kinds of models would really impress you? I am interested in testing different MCMC sampling methods to evaluate their performance and I want to stretch the bounds of their abilities.
  
I don’t know what’s the trickiest, but just about anything I work on in a serious way gives me some troubles.  This reminds me that we should finish our Bayesian Benchmarks paper already.</p><p>3 0.74243331 <a title="1443-lsi-3" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-31-The_most-cited_statistics_papers_ever.html">2277 andrew gelman stats-2014-03-31-The most-cited statistics papers ever</a></p>
<p>Introduction: Robert Grant has a  list .  I’ll just give the ones with more than 10,000 Google Scholar cites:
  

Cox (1972) Regression and life tables: 35,512 citations. 


Dempster, Laird, Rubin (1977) Maximum likelihood from incomplete data via the EM algorithm: 34,988


Bland & Altman (1986) Statistical methods for assessing agreement between two methods of clinical measurement: 27,181


Geman & Geman (1984) Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images: 15,106
  
We can find some more via searching Google scholar for familiar names and topics; thus:
  

Metropolis et al. (1953) Equation of state calculations by fast computing machines: 26,000


Benjamini and Hochberg (1995) Controlling the false discovery rate: a practical and powerful approach to multiple testing: 21,000


White (1980) A heteroskedasticity-consistent covariance matrix estimator and a direct test for heteroskedasticity: 18,000


Heckman (1977) Sample selection bias as a specification error:</p><p>4 0.73535281 <a title="1443-lsi-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-24-Bleg%3A_Automatic_Differentiation_for_Log_Prob_Gradients%3F.html">535 andrew gelman stats-2011-01-24-Bleg: Automatic Differentiation for Log Prob Gradients?</a></p>
<p>Introduction: We need help picking out an automatic differentiation package for Hamiltonian Monte Carlo sampling from the posterior of a  generalized linear model with deep interactions.  Specifically, we need to compute gradients for log probability functions with thousands of parameters that involve matrix (determinants, eigenvalues, inverses), stats (distributions), and math (log gamma) functions.  Any suggestions?
    
 The Application: Hybrid Monte Carlo for Posteriors 
 
We’re getting serious about implementing posterior sampling using Hamiltonian Monte Carlo.  HMC speeds up mixing by including gradient information to help guide the Metropolis proposals toward areas  high probability.  In practice, the algorithm requires a handful or  of gradient calculations per sample, but there are many dimensions and the functions are hairy enough we don’t want to compute derivaties by hand.
 

 Auto Diff: Perhaps not What you Think 
 
It may not have been clear to readers of this blog that automatic diffe</p><p>5 0.72395951 <a title="1443-lsi-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-29-Bayes_in_astronomy.html">1091 andrew gelman stats-2011-12-29-Bayes in astronomy</a></p>
<p>Introduction: David Schminovich points me to  this paper  by Yu Lu, H. Mo, Martin Weinberg, and Neal Katz:
  
We believe that a wide range of physical processes conspire to shape the observed galaxy population but we remain unsure of their detailed interactions. The semi-analytic model (SAM) of galaxy formation uses multi-dimensional parameterisations of the physical processes of galaxy formation and provides a tool to constrain these underlying physical interactions. Because of the high dimensionality, the parametric problem of galaxy formation may be profitably tackled with a Bayesian-inference based approach, which allows one to constrain theory with data in a statistically rigorous way. In this paper we develop a SAM in the framework of Bayesian inference. . . .
  
And here’s  another  from the same authors, this time on “Bayesian inference of galaxy formation from the K-band luminosity function of galaxies: tensions between theory and observation.”
 
I haven’t actually looked at the papers but</p><p>6 0.71610016 <a title="1443-lsi-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-11-Convergence_Monitoring_for_Non-Identifiable_and_Non-Parametric_Models.html">1374 andrew gelman stats-2012-06-11-Convergence Monitoring for Non-Identifiable and Non-Parametric Models</a></p>
<p>7 0.71543074 <a title="1443-lsi-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-18-Derivative-based_MCMC_as_a_breakthrough_technique_for_implementing_Bayesian_statistics.html">419 andrew gelman stats-2010-11-18-Derivative-based MCMC as a breakthrough technique for implementing Bayesian statistics</a></p>
<p>8 0.71102458 <a title="1443-lsi-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-09-The_first_version_of_my_%E2%80%9Cinference_from_iterative_simulation_using_parallel_sequences%E2%80%9D_paper%21.html">1309 andrew gelman stats-2012-05-09-The first version of my “inference from iterative simulation using parallel sequences” paper!</a></p>
<p>9 0.70978785 <a title="1443-lsi-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-26-An_AI_can_build_and_try_out_statistical_models_using_an_open-ended_generative_grammar.html">1739 andrew gelman stats-2013-02-26-An AI can build and try out statistical models using an open-ended generative grammar</a></p>
<p>10 0.70365542 <a title="1443-lsi-10" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-29-Bayesian_Uncertainty_Quantification_for_Differential_Equations%21.html">2311 andrew gelman stats-2014-04-29-Bayesian Uncertainty Quantification for Differential Equations!</a></p>
<p>11 0.69551313 <a title="1443-lsi-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-21-Handbook_of_Markov_Chain_Monte_Carlo.html">674 andrew gelman stats-2011-04-21-Handbook of Markov Chain Monte Carlo</a></p>
<p>12 0.69505638 <a title="1443-lsi-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-19-An_interweaving-transformation_strategy_for_boosting_MCMC_efficiency.html">964 andrew gelman stats-2011-10-19-An interweaving-transformation strategy for boosting MCMC efficiency</a></p>
<p>13 0.69358945 <a title="1443-lsi-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-19-Validation_of_Software_for_Bayesian_Models_Using_Posterior_Quantiles.html">1019 andrew gelman stats-2011-11-19-Validation of Software for Bayesian Models Using Posterior Quantiles</a></p>
<p>14 0.69036031 <a title="1443-lsi-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-25-Ways_of_knowing.html">1469 andrew gelman stats-2012-08-25-Ways of knowing</a></p>
<p>15 0.6901871 <a title="1443-lsi-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-18-Those_wacky_anti-Bayesians_used_to_be_intimidating%2C_but_now_they%E2%80%99re_just_pathetic.html">2254 andrew gelman stats-2014-03-18-Those wacky anti-Bayesians used to be intimidating, but now they’re just pathetic</a></p>
<p>16 0.68950576 <a title="1443-lsi-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-17-Probability-processing_hardware.html">214 andrew gelman stats-2010-08-17-Probability-processing hardware</a></p>
<p>17 0.68588817 <a title="1443-lsi-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-18-EP_and_ABC.html">2067 andrew gelman stats-2013-10-18-EP and ABC</a></p>
<p>18 0.68285 <a title="1443-lsi-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-30-Stan_Project%3A__Continuous_Relaxations_for_Discrete_MRFs.html">2003 andrew gelman stats-2013-08-30-Stan Project:  Continuous Relaxations for Discrete MRFs</a></p>
<p>19 0.68155575 <a title="1443-lsi-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-29-Hamiltonian_Monte_Carlo_stories.html">931 andrew gelman stats-2011-09-29-Hamiltonian Monte Carlo stories</a></p>
<p>20 0.67797601 <a title="1443-lsi-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-24-New_ideas_on_DIC_from_Martyn_Plummer_and_Sumio_Watanabe.html">778 andrew gelman stats-2011-06-24-New ideas on DIC from Martyn Plummer and Sumio Watanabe</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(15, 0.018), (16, 0.047), (21, 0.046), (24, 0.138), (35, 0.21), (42, 0.014), (51, 0.01), (55, 0.011), (63, 0.015), (68, 0.012), (74, 0.011), (76, 0.019), (77, 0.036), (82, 0.015), (86, 0.02), (89, 0.022), (99, 0.236)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.9439348 <a title="1443-lda-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-17-Why_a_bonobo_won%E2%80%99t_play_poker_with_you.html">473 andrew gelman stats-2010-12-17-Why a bonobo won’t play poker with you</a></p>
<p>Introduction: Sciencedaily has posted an article titled  Apes Unwilling to Gamble When Odds Are Uncertain :
  

The apes readily distinguished between the different probabilities of winning: they gambled a lot when there was a 100 percent chance, less when there was a 50 percent chance, and only rarely when there was no chance In some trials, however, the experimenter didn’t remove a lid from the bowl, so the apes couldn’t assess the likelihood of winning a banana The odds from the covered bowl were identical to those from the risky option: a 50 percent chance of getting the much sought-after banana. But apes of both species were less likely to choose this ambiguous option.

   

Like humans, they showed “ambiguity aversion” — preferring to gamble more when they knew the odds than when they didn’t. Given some of the other differences between chimps and bonobos, Hare and Rosati had expected to find the bonobos to be more averse to ambiguity, but that didn’t turn out to be the case.

  
Thanks to  Sta</p><p>2 0.92047679 <a title="1443-lda-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-04-Is_it_rational_to_vote%3F.html">837 andrew gelman stats-2011-08-04-Is it rational to vote?</a></p>
<p>Introduction: Hear me interviewed on the topic  here .
 
P.S.  The interview was fine but I donâ&euro;&trade;t agree with everything on the linked website.  For example,  this  bit:
  
Global warming is not the first case of a widespread fear based on incomplete knowledge turned out to be false or at least greatly exaggerated.  Global warming has many of the characteristics of a popular delusion, an irrational fear or cause that is embraced by millions of people because, well, it is believed by millions of people!
  
All right, then.</p><p>same-blog 3 0.9183532 <a title="1443-lda-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-04-Bayesian_Learning_via_Stochastic_Gradient_Langevin_Dynamics.html">1443 andrew gelman stats-2012-08-04-Bayesian Learning via Stochastic Gradient Langevin Dynamics</a></p>
<p>Introduction: Burak Bayramli writes:
  
In  this paper  by Sunjin Ahn, Anoop Korattikara, and Max Welling and  this paper  by Welling and Yee Whye The, there are some arguments on big data and the use of MCMC. Both papers have suggested improvements to speed up MCMC computations. I was wondering what your thoughts were, especially on this paragraph:

 
When a dataset has a billion data-cases (as is not uncommon these days) MCMC algorithms will not even have generated a single (burn-in) sample when a clever learning algorithm based on stochastic gradients may already be making fairly good predictions. In fact, the intriguing results of Bottou and Bousquet (2008) seem to indicate that in terms of “number of bits learned per unit of computation”, an algorithm as simple as stochastic gradient descent is almost optimally efficient. We therefore argue that for Bayesian methods to remain useful in an age when the datasets grow at an exponential rate, they need to embrace the ideas of the stochastic optimiz</p><p>4 0.90378511 <a title="1443-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-04-45%25_hitting%2C_25%25_fielding%2C_25%25_pitching%2C_and_100%25_not_telling_us_how_they_did_it.html">942 andrew gelman stats-2011-10-04-45% hitting, 25% fielding, 25% pitching, and 100% not telling us how they did it</a></p>
<p>Introduction: A University of Delaware press release  reports :
  
This month, the Journal of Quantitative Analysis in Sports will feature the article “An Estimate of How Hitting, Pitching, Fielding, and Base-stealing Impact Team Winning Percentages in Baseball.”  In it, University of Delaware Prof. Charles Pavitt of the Department of Communication defines the perfect “formula” for Major League Baseball (MLB) teams to use to build the ultimate winning team.


Pavitt found hitting accounts for more than 45 percent of teams’ winning records, fielding for 25 percent and pitching for 25 percent. And that the impact of stolen bases is greatly overestimated.


He crunched hitting, pitching, fielding and base-stealing records for every MLB team over a 48-year period from 1951-1998 with a method no other researcher has used in this area. In statistical parlance, he used a conceptual decomposition of offense and defense into its component parts and then analyzed recombinations of the parts in intuitively mea</p><p>5 0.89658284 <a title="1443-lda-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-30-Rickey_Henderson_and_Peter_Angelos%2C_together_again.html">881 andrew gelman stats-2011-08-30-Rickey Henderson and Peter Angelos, together again</a></p>
<p>Introduction: Today I was reminded of a riddle from junior high: 
Q:  What do you get when you cross an elephant with peanut butter? 
A:  Peanut butter that never forgets, or an elephant that sticks to the roof of your mouth.
 
The occasion was a  link  from Tyler Cowen to a new book by Garry Kasparov and . . . Peter Thiel.
 
Kasparov we all know about.  I still remember how he pulled out a victory in  the last game of his tournament with Karpov.  Just amazing:  he had to win the game, a draw would not be enough.  Both players knew that Kasparov had to win.  And he did it.  A feat as impressive as Kirk Gibson’s off-the-bench game-winning home run in the 1987 Series.
 
Peter Theil is a more obscure figure.  He’s been featured a  couple  of  times  on this blog and comes across as your typical overconfident rich dude.
 
It’s an odd combination, sort of like what you might get if Rickey Henderson and Peter Angelos were to write a book about how to reform baseball.  Cowen writes, “How can I not pre-orde</p><p>6 0.89116305 <a title="1443-lda-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-03-On_house_arrest_for_p-hacking.html">2049 andrew gelman stats-2013-10-03-On house arrest for p-hacking</a></p>
<p>7 0.89070344 <a title="1443-lda-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-25-Quantitative_Methods_in_the_Social_Sciences_M.A.%3A_Innovative%2C_interdisciplinary_social_science_research_program_for_a_data-rich_world.html">591 andrew gelman stats-2011-02-25-Quantitative Methods in the Social Sciences M.A.: Innovative, interdisciplinary social science research program for a data-rich world</a></p>
<p>8 0.88119793 <a title="1443-lda-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-26-A_simple_semigraphic_display.html">296 andrew gelman stats-2010-09-26-A simple semigraphic display</a></p>
<p>9 0.87951094 <a title="1443-lda-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-30-Computational_problems_with_glm_etc..html">1516 andrew gelman stats-2012-09-30-Computational problems with glm etc.</a></p>
<p>10 0.872177 <a title="1443-lda-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-08-How_to_solve_the_Post_Office%E2%80%99s_problems%3F.html">895 andrew gelman stats-2011-09-08-How to solve the Post Office’s problems?</a></p>
<p>11 0.87033683 <a title="1443-lda-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-05-More_plain_old_everyday_Bayesianism.html">1926 andrew gelman stats-2013-07-05-More plain old everyday Bayesianism</a></p>
<p>12 0.85953677 <a title="1443-lda-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-01-The_placebo_effect_in_pharma.html">388 andrew gelman stats-2010-11-01-The placebo effect in pharma</a></p>
<p>13 0.85885358 <a title="1443-lda-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-20-Prior_beliefs_about_locations_of_decision_boundaries.html">1130 andrew gelman stats-2012-01-20-Prior beliefs about locations of decision boundaries</a></p>
<p>14 0.84424901 <a title="1443-lda-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-14-Learning_from_failure.html">1264 andrew gelman stats-2012-04-14-Learning from failure</a></p>
<p>15 0.82928973 <a title="1443-lda-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-03-Taleb_%2B_3.5_years.html">392 andrew gelman stats-2010-11-03-Taleb + 3.5 years</a></p>
<p>16 0.81219393 <a title="1443-lda-16" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-30-Adjudicating_between_alternative_interpretations_of_a_statistical_interaction%3F.html">2274 andrew gelman stats-2014-03-30-Adjudicating between alternative interpretations of a statistical interaction?</a></p>
<p>17 0.80967271 <a title="1443-lda-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-17-Macro_causality.html">807 andrew gelman stats-2011-07-17-Macro causality</a></p>
<p>18 0.80597115 <a title="1443-lda-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-12-Stan_1.3.0_and_RStan_1.3.0_Ready_for_Action.html">1799 andrew gelman stats-2013-04-12-Stan 1.3.0 and RStan 1.3.0 Ready for Action</a></p>
<p>19 0.80146658 <a title="1443-lda-19" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-17-On_deck_this_week%3A__Revisitings.html">2253 andrew gelman stats-2014-03-17-On deck this week:  Revisitings</a></p>
<p>20 0.79987842 <a title="1443-lda-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-25-Xihong_Lin_on_sparsity_and_density.html">2185 andrew gelman stats-2014-01-25-Xihong Lin on sparsity and density</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
