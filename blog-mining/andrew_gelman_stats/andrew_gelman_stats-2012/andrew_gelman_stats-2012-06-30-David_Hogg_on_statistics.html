<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1401 andrew gelman stats-2012-06-30-David Hogg on statistics</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2012" href="../home/andrew_gelman_stats-2012_home.html">andrew_gelman_stats-2012</a> <a title="andrew_gelman_stats-2012-1401" href="#">andrew_gelman_stats-2012-1401</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1401 andrew gelman stats-2012-06-30-David Hogg on statistics</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2012-1401-html" href="http://andrewgelman.com/2012/06/30/david-hogg-on-statistics/">html</a></p><p>Introduction: Data analysis recipes: Fitting a model to data :
  
We go through the many considerations involved in fitting a model to data, using as an example the fit of a straight line to a set of points in a two-dimensional plane. Standard weighted least-squares fitting is only appropriate when there is a dimension along which the data points have negligible uncertainties, and another along which all the uncertainties can be described by Gaussians of known variance; these conditions are rarely met in practice. We consider cases of general, heterogeneous, and arbitrarily covariant two-dimensional uncertainties, and situations in which there are bad data (large outliers), unknown uncertainties, and unknown but expected intrinsic scatter in the linear relationship being fit. Above all we emphasize the importance of having a “generative model” for the data, even an approximate one. Once there is a generative model, the subsequent fitting is non-arbitrary because the model permits direct computation</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Data analysis recipes: Fitting a model to data :    We go through the many considerations involved in fitting a model to data, using as an example the fit of a straight line to a set of points in a two-dimensional plane. [sent-1, score-0.787]
</p><p>2 Standard weighted least-squares fitting is only appropriate when there is a dimension along which the data points have negligible uncertainties, and another along which all the uncertainties can be described by Gaussians of known variance; these conditions are rarely met in practice. [sent-2, score-1.255]
</p><p>3 We consider cases of general, heterogeneous, and arbitrarily covariant two-dimensional uncertainties, and situations in which there are bad data (large outliers), unknown uncertainties, and unknown but expected intrinsic scatter in the linear relationship being fit. [sent-3, score-0.88]
</p><p>4 Above all we emphasize the importance of having a “generative model” for the data, even an approximate one. [sent-4, score-0.071]
</p><p>5 Once there is a generative model, the subsequent fitting is non-arbitrary because the model permits direct computation of the likelihood of the parameters or the posterior probability distribution. [sent-5, score-1.311]
</p><p>6 Construction of a posterior probability distribution is indispensible if there are “nuisance parameters” to marginalize away. [sent-6, score-0.613]
</p><p>7 Data analysis recipes: Probability calculus for inference :    In this pedagogical text aimed at those wanting to start thinking about or brush up on probabilistic inference, I review the rules by which probability distribution functions can (and cannot) be combined. [sent-7, score-1.379]
</p><p>8 I connect these rules to the operations performed in probabilistic data analysis. [sent-8, score-0.653]
</p><p>9 Dimensional analysis is emphasized as a valuable tool for helping to construct non-wrong probabilistic statements. [sent-9, score-0.623]
</p><p>10 The applications of probability calculus in constructing likelihoods, marginalized likelihoods, posterior probabilities, and posterior predictions are all discussed. [sent-10, score-1.001]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('uncertainties', 0.378), ('probabilistic', 0.234), ('fitting', 0.233), ('recipes', 0.225), ('posterior', 0.205), ('probability', 0.197), ('generative', 0.189), ('calculus', 0.183), ('likelihoods', 0.183), ('unknown', 0.168), ('data', 0.135), ('intrinsic', 0.125), ('brush', 0.125), ('gaussians', 0.125), ('indispensible', 0.125), ('marginalized', 0.125), ('model', 0.123), ('rules', 0.119), ('pedagogical', 0.112), ('negligible', 0.112), ('scatter', 0.108), ('permits', 0.105), ('heterogeneous', 0.105), ('nuisance', 0.105), ('arbitrarily', 0.105), ('parameters', 0.1), ('dimensional', 0.098), ('considerations', 0.093), ('outliers', 0.09), ('operations', 0.089), ('aimed', 0.088), ('subsequent', 0.088), ('distribution', 0.086), ('constructing', 0.086), ('weighted', 0.084), ('construction', 0.083), ('emphasized', 0.082), ('along', 0.082), ('analysis', 0.08), ('construct', 0.079), ('inference', 0.079), ('dimension', 0.078), ('helping', 0.077), ('wanting', 0.076), ('connect', 0.076), ('rarely', 0.071), ('approximate', 0.071), ('valuable', 0.071), ('computation', 0.071), ('situations', 0.071)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="1401-tfidf-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-30-David_Hogg_on_statistics.html">1401 andrew gelman stats-2012-06-30-David Hogg on statistics</a></p>
<p>Introduction: Data analysis recipes: Fitting a model to data :
  
We go through the many considerations involved in fitting a model to data, using as an example the fit of a straight line to a set of points in a two-dimensional plane. Standard weighted least-squares fitting is only appropriate when there is a dimension along which the data points have negligible uncertainties, and another along which all the uncertainties can be described by Gaussians of known variance; these conditions are rarely met in practice. We consider cases of general, heterogeneous, and arbitrarily covariant two-dimensional uncertainties, and situations in which there are bad data (large outliers), unknown uncertainties, and unknown but expected intrinsic scatter in the linear relationship being fit. Above all we emphasize the importance of having a “generative model” for the data, even an approximate one. Once there is a generative model, the subsequent fitting is non-arbitrary because the model permits direct computation</p><p>2 0.16064805 <a title="1401-tfidf-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-29-Postdocs_in_probabilistic_modeling%21__With_David_Blei%21__And_Stan%21.html">1961 andrew gelman stats-2013-07-29-Postdocs in probabilistic modeling!  With David Blei!  And Stan!</a></p>
<p>Introduction: David Blei writes:
  
I have  two postdoc openings for basic research in probabilistic modeling .


The thrusts are (a) scalable inference and (b) model checking.  We 
will be developing new methods and implementing them in probabilistic 
programming systems.  I am open to applicants interested in many kinds 
of applications and from any field.
  
“Scalable inference” means black-box VB and related ideas, and “probabilistic programming systems” means Stan!  (You might be familiar with Stan as an implementation of Nuts for posterior sampling, but Stan is also an efficient program for computing probability densities and their gradients, and as such is an ideal platform for developing scalable implementations of variational inference and related algorithms.)
 
And you know I like model checking.
 
Here’s the full ad: 
  
  
===== POSTDOC POSITIONS IN PROBABILISTIC MODELING =====


We expect to have two postdoctoral positions available for January 2014 (or later).  These positions are in D</p><p>3 0.15138176 <a title="1401-tfidf-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-27-Bridges_between_deterministic_and_probabilistic_models_for_binary_data.html">780 andrew gelman stats-2011-06-27-Bridges between deterministic and probabilistic models for binary data</a></p>
<p>Introduction: For the analysis of binary data, various deterministic models have been proposed, which are generally simpler to fit and easier to understand than probabilistic models. We claim that corresponding to any deterministic model is an implicit stochastic model in which the deterministic model fits imperfectly, with errors occurring at random. In the context of binary data, we consider a model in which the probability of error depends on the model prediction. We show how to fit this model using a stochastic modification of deterministic optimization schemes.
 
The advantages of fitting the stochastic model explicitly (rather than implicitly, by simply fitting a deterministic model and accepting the occurrence of errors) include quantification of uncertainty in the deterministic model’s parameter estimates, better estimation of the true model error rate, and the ability to check the fit of the model nontrivially. We illustrate this with a simple theoretical example of item response data and w</p><p>4 0.14276941 <a title="1401-tfidf-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-04-Model_checking_and_model_understanding_in_machine_learning.html">1482 andrew gelman stats-2012-09-04-Model checking and model understanding in machine learning</a></p>
<p>Introduction: Last month I  wrote :
  
Computer scientists are often brilliant but they can be unfamiliar with what is done in the worlds of data collection and analysis. This goes the other way too: statisticians such as myself can look pretty awkward, reinventing (or failing to reinvent) various wheels when we write computer programs or, even worse, try to design software.Andrew MacNamara writes:
  
Andrew MacNamara followed up with some thoughts:
  
I [MacNamara] had some basic statistics training through my MBA program, after having completed an undergrad degree in computer science. Since then I’ve been very interested in learning more about statistical techniques, including things like GLM and censored data analyses as well as machine learning topics like neural nets, SVMs, etc. I began following your blog after some research into Bayesian analysis topics and I am trying to dig deeper on that side of things.


One thing I have noticed is that there seems to be a distinction between data analysi</p><p>5 0.13990189 <a title="1401-tfidf-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-01-Martin_and_Liu%3A__Probabilistic_inference_based_on_consistency_of_model_with_data.html">1095 andrew gelman stats-2012-01-01-Martin and Liu:  Probabilistic inference based on consistency of model with data</a></p>
<p>Introduction: What better way to start then new year than with some hard-core statistical theory?
 
Ryan Martin and Chuanhai Liu send along  a new paper  on inferential models:
  
Probability is a useful tool for describing uncertainty, so it is natural to strive for a system of statistical inference based on probabilities for or against various hypotheses. But existing probabilistic inference methods struggle to provide a meaningful interpretation of the probabilities across experiments in sufficient generality. In this paper we further develop a promising new approach based on what are called inferential models (IMs). The fundamental idea behind IMs is that there is an unobservable auxiliary variable that itself describes the inherent uncertainty about the parameter of interest, and that posterior probabilistic inference can be accomplished by predicting this unobserved quantity. We describe a simple and intuitive three-step construction of a random set of candidate parameter values, each being co</p><p>6 0.1383169 <a title="1401-tfidf-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<p>7 0.12780546 <a title="1401-tfidf-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-03-Question_about_predictive_checks.html">1363 andrew gelman stats-2012-06-03-Question about predictive checks</a></p>
<p>8 0.12648593 <a title="1401-tfidf-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-07-Philosophy_and_the_practice_of_Bayesian_statistics_%28with_all_the_discussions%21%29.html">1712 andrew gelman stats-2013-02-07-Philosophy and the practice of Bayesian statistics (with all the discussions!)</a></p>
<p>9 0.12624162 <a title="1401-tfidf-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-20-The_pervasive_twoishness_of_statistics%3B_in_particular%2C_the_%E2%80%9Csampling_distribution%E2%80%9D_and_the_%E2%80%9Clikelihood%E2%80%9D_are_two_different_models%2C_and_that%E2%80%99s_a_good_thing.html">774 andrew gelman stats-2011-06-20-The pervasive twoishness of statistics; in particular, the “sampling distribution” and the “likelihood” are two different models, and that’s a good thing</a></p>
<p>10 0.12385833 <a title="1401-tfidf-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Difficulties_with_Bayesian_model_averaging.html">754 andrew gelman stats-2011-06-09-Difficulties with Bayesian model averaging</a></p>
<p>11 0.12365258 <a title="1401-tfidf-11" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-05-Prior_distribution_for_a_predicted_probability.html">2200 andrew gelman stats-2014-02-05-Prior distribution for a predicted probability</a></p>
<p>12 0.12332181 <a title="1401-tfidf-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>13 0.11778508 <a title="1401-tfidf-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-11-Gelman_on_Hennig_on_Gelman_on_Bayes.html">1208 andrew gelman stats-2012-03-11-Gelman on Hennig on Gelman on Bayes</a></p>
<p>14 0.11719774 <a title="1401-tfidf-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-20-Likelihood_thresholds_and_decisions.html">1422 andrew gelman stats-2012-07-20-Likelihood thresholds and decisions</a></p>
<p>15 0.11710822 <a title="1401-tfidf-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-07-16-Priors.html">1941 andrew gelman stats-2013-07-16-Priors</a></p>
<p>16 0.11698711 <a title="1401-tfidf-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-07-When_you%E2%80%99re_planning_on_fitting_a_model%2C_build_up_to_it_by_fitting_simpler_models_first.__Then%2C_once_you_have_a_model_you_like%2C_check_the_hell_out_of_it.html">1972 andrew gelman stats-2013-08-07-When you’re planning on fitting a model, build up to it by fitting simpler models first.  Then, once you have a model you like, check the hell out of it</a></p>
<p>17 0.11519995 <a title="1401-tfidf-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-27-Bayesian_model_averaging_or_fitting_a_larger_model.html">1999 andrew gelman stats-2013-08-27-Bayesian model averaging or fitting a larger model</a></p>
<p>18 0.11207186 <a title="1401-tfidf-18" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-12-How_to_think_about_%E2%80%9Cidentifiability%E2%80%9D_in_Bayesian_inference%3F.html">2208 andrew gelman stats-2014-02-12-How to think about “identifiability” in Bayesian inference?</a></p>
<p>19 0.11131135 <a title="1401-tfidf-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-25-Avoiding_boundary_estimates_using_a_prior_distribution_as_regularization.html">779 andrew gelman stats-2011-06-25-Avoiding boundary estimates using a prior distribution as regularization</a></p>
<p>20 0.10824936 <a title="1401-tfidf-20" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-08-P-values_and_statistical_practice.html">1713 andrew gelman stats-2013-02-08-P-values and statistical practice</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.142), (1, 0.182), (2, 0.009), (3, 0.042), (4, 0.006), (5, -0.002), (6, -0.002), (7, -0.017), (8, 0.008), (9, -0.015), (10, 0.001), (11, 0.015), (12, -0.074), (13, -0.043), (14, -0.098), (15, -0.013), (16, 0.042), (17, -0.006), (18, 0.008), (19, -0.044), (20, 0.027), (21, -0.017), (22, -0.001), (23, -0.036), (24, -0.001), (25, 0.057), (26, -0.021), (27, 0.035), (28, 0.043), (29, -0.027), (30, -0.063), (31, 0.01), (32, -0.044), (33, 0.021), (34, -0.021), (35, 0.016), (36, 0.007), (37, -0.066), (38, -0.011), (39, 0.014), (40, 0.01), (41, -0.017), (42, 0.033), (43, -0.028), (44, 0.036), (45, -0.011), (46, -0.0), (47, 0.012), (48, 0.039), (49, -0.018)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94448608 <a title="1401-lsi-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-30-David_Hogg_on_statistics.html">1401 andrew gelman stats-2012-06-30-David Hogg on statistics</a></p>
<p>Introduction: Data analysis recipes: Fitting a model to data :
  
We go through the many considerations involved in fitting a model to data, using as an example the fit of a straight line to a set of points in a two-dimensional plane. Standard weighted least-squares fitting is only appropriate when there is a dimension along which the data points have negligible uncertainties, and another along which all the uncertainties can be described by Gaussians of known variance; these conditions are rarely met in practice. We consider cases of general, heterogeneous, and arbitrarily covariant two-dimensional uncertainties, and situations in which there are bad data (large outliers), unknown uncertainties, and unknown but expected intrinsic scatter in the linear relationship being fit. Above all we emphasize the importance of having a “generative model” for the data, even an approximate one. Once there is a generative model, the subsequent fitting is non-arbitrary because the model permits direct computation</p><p>2 0.85915762 <a title="1401-lsi-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-03-Question_about_predictive_checks.html">1363 andrew gelman stats-2012-06-03-Question about predictive checks</a></p>
<p>Introduction: Klaas Metselaar writes: 
  
  
I [Metselaar] am currently involved in a discussion about the use of the notion “predictive” as used in “posterior predictive check”. I would argue that the notion “predictive” should be reserved for posterior checks using information not used in the determination of the posterior. 
I quote from the discussion: 
“However, the predictive uncertainty in a Bayesian calculation requires sampling from all the random variables, and this includes both the model parameters and the residual error”.


My [Metselaar's] comment:


This may be exactly the point I am worried about: shouldn’t the predictive uncertainty be defined as sampling from the posterior parameter distribution +  residual error + sampling from the prediction error distribution? 
Residual error reduces to measurement error in the case of a  model which is perfect for the sample of experiments. Measurement error could be reduced to almost zero by ideal and perfect measurement instruments. 
I would h</p><p>3 0.85032344 <a title="1401-lsi-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-18-Understanding_posterior_p-values.html">2029 andrew gelman stats-2013-09-18-Understanding posterior p-values</a></p>
<p>Introduction: David Kaplan writes:
  
I came across your  paper  “Understanding Posterior Predictive P-values”, and I have a question regarding your statement “If a posterior predictive p-value is 0.4, say, that means that, if we believe the model, we think there is a 40% chance that tomorrow’s value of T(y_rep) will exceed today’s T(y).” This is perfectly understandable to me and represents the idea of calibration.  However, I am unsure how this relates to statements about fit.  If T is the LR chi-square or Pearson chi-square, then your statement that there is a 40% chance that tomorrows value exceeds today’s value indicates bad fit, I think.  Yet, some literature indicates that high p-values suggest good fit.  Could you clarify this?
  
My reply:
 
I think that “fit” depends on the question being asked.  In this case, I’d say the model fits for this particular purpose, even though it might not fit for other purposes.
 
And here’s the abstract of the paper:
  
Posterior predictive p-values do not i</p><p>4 0.84636527 <a title="1401-lsi-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-26-Modeling_probability_data.html">1284 andrew gelman stats-2012-04-26-Modeling probability data</a></p>
<p>Introduction: Rafael Huber writes: 
  
  
I conducted an experiment in which subjects where asked to estimate the probability of a certain event given a number of information (like a wheater forecaster or a stockmarket trader). These probability estimates are the dependent variable of my experiment. My goal is to model the data with a (hierarchical) Bayesian regression. A linear equation with all the presented information (quantified as log odds) defines the mu of a normal likelihood. The tau as precision is another free parameter.


y[r] ~ dnorm( mu[r] , tau[ subj[r] ] ) 
mu[r] <- b0[ subj[r] ] + b1[ subj[r] ] * x1[r] + b2[ subj[r] ] * x2[r] + b3[ subj[r] ] * x3[r]


My problem is that I do not believe that the normal is the correct probability distribution to model probability data (â&euro;Ś because the error is limited). However, until now nobody was able to tell me how I can correctly model probability data.
  
My reply:  You can take the logit of the data before analyzing them.  That is assuming there</p><p>5 0.8063491 <a title="1401-lsi-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-20-The_pervasive_twoishness_of_statistics%3B_in_particular%2C_the_%E2%80%9Csampling_distribution%E2%80%9D_and_the_%E2%80%9Clikelihood%E2%80%9D_are_two_different_models%2C_and_that%E2%80%99s_a_good_thing.html">774 andrew gelman stats-2011-06-20-The pervasive twoishness of statistics; in particular, the “sampling distribution” and the “likelihood” are two different models, and that’s a good thing</a></p>
<p>Introduction: Lots of good statistical methods make use of two models.  For example:
 
- Classical statistics:  estimates and standard errors using the likelihood function; tests and p-values using the sampling distribution.  (The sampling distribution is  not  equivalent to the likelihood, as has been much discussed, for example in sequential stopping problems.)
 
- Bayesian data analysis:  inference using the posterior distribution; model checking using the predictive distribution (which, again, depends on the data-generating process in a way that the likelihood does not).
 
- Machine learning:  estimation using the data; evaluation using cross-validation (which requires some rule for partitioning the data, a rule that stands outside of the data themselves).
 
- Bootstrap, jackknife, etc:  estimation using an “estimator” (which, I would argue, is based in some sense on a model for the data), uncertainties using resampling (which, I would argue, is close to the idea of a “sampling distribution” in</p><p>6 0.79280293 <a title="1401-lsi-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-15-More_on_AIC%2C_WAIC%2C_etc.html">1983 andrew gelman stats-2013-08-15-More on AIC, WAIC, etc</a></p>
<p>7 0.79068398 <a title="1401-lsi-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-19-Whassup_with_deviance_having_a_high_posterior_correlation_with_a_parameter_in_the_model%3F.html">1221 andrew gelman stats-2012-03-19-Whassup with deviance having a high posterior correlation with a parameter in the model?</a></p>
<p>8 0.78976327 <a title="1401-lsi-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-21-More_on_Bayesian_model_selection_in_high-dimensional_settings.html">1817 andrew gelman stats-2013-04-21-More on Bayesian model selection in high-dimensional settings</a></p>
<p>9 0.78650337 <a title="1401-lsi-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Difficulties_with_Bayesian_model_averaging.html">754 andrew gelman stats-2011-06-09-Difficulties with Bayesian model averaging</a></p>
<p>10 0.78618336 <a title="1401-lsi-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-27-Bayesian_model_averaging_or_fitting_a_larger_model.html">1999 andrew gelman stats-2013-08-27-Bayesian model averaging or fitting a larger model</a></p>
<p>11 0.78401637 <a title="1401-lsi-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-16-%E2%80%9CReal_data_can_be_a_pain%E2%80%9D.html">1460 andrew gelman stats-2012-08-16-“Real data can be a pain”</a></p>
<p>12 0.78309536 <a title="1401-lsi-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-06-Quote_of_the_day.html">398 andrew gelman stats-2010-11-06-Quote of the day</a></p>
<p>13 0.77526653 <a title="1401-lsi-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-12-UnConMax_%E2%80%93_uncertainty_consideration_maxims_7_%2B--_2.html">82 andrew gelman stats-2010-06-12-UnConMax – uncertainty consideration maxims 7 +-- 2</a></p>
<p>14 0.77085751 <a title="1401-lsi-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-07-Chi-square_FAIL_when_many_cells_have_small_expected_values.html">996 andrew gelman stats-2011-11-07-Chi-square FAIL when many cells have small expected values</a></p>
<p>15 0.75942057 <a title="1401-lsi-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-28-Understanding_simulations_in_terms_of_predictive_inference%3F.html">1287 andrew gelman stats-2012-04-28-Understanding simulations in terms of predictive inference?</a></p>
<p>16 0.75632131 <a title="1401-lsi-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-28-Using_predator-prey_models_on_the_Canadian_lynx_series.html">1141 andrew gelman stats-2012-01-28-Using predator-prey models on the Canadian lynx series</a></p>
<p>17 0.74990368 <a title="1401-lsi-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-11-Adding_an_error_model_to_a_deterministic_model.html">1162 andrew gelman stats-2012-02-11-Adding an error model to a deterministic model</a></p>
<p>18 0.74844033 <a title="1401-lsi-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<p>19 0.74822599 <a title="1401-lsi-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-15-How_I_think_about_mixture_models.html">1459 andrew gelman stats-2012-08-15-How I think about mixture models</a></p>
<p>20 0.74751508 <a title="1401-lsi-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-10-Another_reason_why_you_can_get_good_inferences_from_a_bad_model.html">1527 andrew gelman stats-2012-10-10-Another reason why you can get good inferences from a bad model</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(6, 0.021), (16, 0.028), (21, 0.292), (24, 0.198), (26, 0.013), (27, 0.029), (42, 0.021), (47, 0.011), (53, 0.01), (57, 0.012), (59, 0.011), (73, 0.034), (99, 0.21)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.97231573 <a title="1401-lda-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-20-The_R_code_for_those_time-use_graphs.html">672 andrew gelman stats-2011-04-20-The R code for those time-use graphs</a></p>
<p>Introduction: By popular demand, hereâ&euro;&trade;s my R script for the  time-use graphs :
  

 
 
# The data
a1 <- c(4.2,3.2,11.1,1.3,2.2,2.0)
a2 <- c(3.9,3.2,10.0,0.8,3.1,3.1)
a3 <- c(6.3,2.5,9.8,0.9,2.2,2.4)
a4 <- c(4.4,3.1,9.8,0.8,3.3,2.7)
a5 <- c(4.8,3.0,9.9,0.7,3.3,2.4)
a6 <- c(4.0,3.4,10.5,0.7,3.3,2.1)
a <- rbind(a1,a2,a3,a4,a5,a6)
avg <- colMeans (a)
avg.array <- t (array (avg, rev(dim(a))))
diff <- a - avg.array
country.name <- c("France", "Germany", "Japan", "Britain", "USA", "Turkey")

# The line plots

par (mfrow=c(2,3), mar=c(4,4,2,.5), mgp=c(2,.7,0), tck=-.02, oma=c(3,0,4,0),
  bg="gray96", fg="gray30")
for (i in 1:6){
  plot (c(1,6), c(-1,1.7), xlab="", ylab="", xaxt="n", yaxt="n",
    bty="l", type="n")
  lines (1:6, diff[i,], col="blue")
  points (1:6, diff[i,], pch=19, col="black")
  if (i>3){
    axis (1, c(1,3,5), c ("Work,\nstudy", "Eat,\nsleep",
      "Leisure"), mgp=c(2,1.5,0), tck=0, cex.axis=1.2)
    axis (1, c(2,4,6), c ("Unpaid\nwork",
      "Personal\nCare", "Other"), mgp=c(2,1.5,0),</p><p>2 0.95633698 <a title="1401-lda-2" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-21-On_deck_this_week.html">2298 andrew gelman stats-2014-04-21-On deck this week</a></p>
<p>Introduction: Mon :  Ticket to Baaaath
 
 Tues :  Ticket to Baaaaarf
 
 Wed :  Thinking of doing a list experiment?  Here’s a list of reasons why you should think again
 
 Thurs :  An open site for researchers to post and share papers
 
 Fri :  Questions about “Too Good to Be True”
 
 Sat :  Sleazy sock puppet can’t stop spamming our discussion of compressed sensing and promoting the work of Xiteng Liu
 
 Sun :  White stripes and dead armadillos</p><p>3 0.92606825 <a title="1401-lda-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-10-A_defense_of_Tom_Wolfe_based_on_the_impossibility_of_the_law_of_small_numbers_in_network_structure.html">1615 andrew gelman stats-2012-12-10-A defense of Tom Wolfe based on the impossibility of the law of small numbers in network structure</a></p>
<p>Introduction: A tall thin young man came to my office today to talk about one of my current pet topics:  stories and social science.  I brought up Tom Wolfe and his goal of compressing an entire city into a single novel, and how this reminded me of the psychologists Kahneman and Tversky’s concept of “the law of small numbers,” the idea that we expect any small sample to replicate all the properties of the larger population that it represents.  Strictly speaking, the law of small numbers is impossible—any small sample necessarily has its own unique features—but this is even more true if we consider network properties.
 
The average American knows about 700 people (depending on how you define “know”) and this defines a social network over the population.  Now suppose you look at a few hundred people and all their connections.  This mini-network will almost necessarily look much much sparser than the national network, as we’re removing the connections to the people not in the sample.
 
Now consider how</p><p>same-blog 4 0.92504644 <a title="1401-lda-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-30-David_Hogg_on_statistics.html">1401 andrew gelman stats-2012-06-30-David Hogg on statistics</a></p>
<p>Introduction: Data analysis recipes: Fitting a model to data :
  
We go through the many considerations involved in fitting a model to data, using as an example the fit of a straight line to a set of points in a two-dimensional plane. Standard weighted least-squares fitting is only appropriate when there is a dimension along which the data points have negligible uncertainties, and another along which all the uncertainties can be described by Gaussians of known variance; these conditions are rarely met in practice. We consider cases of general, heterogeneous, and arbitrarily covariant two-dimensional uncertainties, and situations in which there are bad data (large outliers), unknown uncertainties, and unknown but expected intrinsic scatter in the linear relationship being fit. Above all we emphasize the importance of having a “generative model” for the data, even an approximate one. Once there is a generative model, the subsequent fitting is non-arbitrary because the model permits direct computation</p><p>5 0.92152917 <a title="1401-lda-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-16-Wanted%3A__Probability_distributions_for_rank_orderings.html">151 andrew gelman stats-2010-07-16-Wanted:  Probability distributions for rank orderings</a></p>
<p>Introduction: Dietrich Stoyan writes:
  
 
I asked the IMS people for an expert in statistics of voting/elections and they wrote me your name. I am a statistician, but never worked in the field voting/elections. It was my son-in-law who asked me for statistical theories in that field.


He posed in particular the following problem:


The aim of the voting is to come to a ranking of c candidates. Every vote is a permutation of these c candidates. The problem is to have probability distributions in the set of all permutations of c elements.


Are there theories for such distributions?


I should be very grateful for a fast answer with hints to literature. (I confess that I do not know your books.) 
 

 
My reply:  Rather than trying to model the ranks directly, Iâ&euro;&trade;d recommend modeling a latent continuous outcome which then implies a distribution on ranks, if the ranks are of interest. There are lots of distributions of c-dimensional continuous outcomes.  In political science, the usual way to start is</p><p>6 0.91484094 <a title="1401-lda-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-27-Banned_in_NYC_school_tests.html">1232 andrew gelman stats-2012-03-27-Banned in NYC school tests</a></p>
<p>7 0.9058122 <a title="1401-lda-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-27-Neumann_update.html">432 andrew gelman stats-2010-11-27-Neumann update</a></p>
<p>8 0.90340447 <a title="1401-lda-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-07-Hipmunk_FAIL%3A__Graphics_without_content_is_not_enough.html">894 andrew gelman stats-2011-09-07-Hipmunk FAIL:  Graphics without content is not enough</a></p>
<p>9 0.89456356 <a title="1401-lda-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-26-%E2%80%9CA_Vast_Graveyard_of_Undead_Theories%3A__Publication_Bias_and_Psychological_Science%E2%80%99s_Aversion_to_the_Null%E2%80%9D.html">1826 andrew gelman stats-2013-04-26-“A Vast Graveyard of Undead Theories:  Publication Bias and Psychological Science’s Aversion to the Null”</a></p>
<p>10 0.88652122 <a title="1401-lda-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-15-%E2%80%9C10_Things_You_Need_to_Know_About_Causal_Effects%E2%80%9D.html">1675 andrew gelman stats-2013-01-15-“10 Things You Need to Know About Causal Effects”</a></p>
<p>11 0.87959599 <a title="1401-lda-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-13-News_coverage_of_statistical_issues%E2%80%A6how_did_I_do%3F.html">514 andrew gelman stats-2011-01-13-News coverage of statistical issues…how did I do?</a></p>
<p>12 0.87536991 <a title="1401-lda-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-22-Please_stop_me_before_I_barf_again.html">1275 andrew gelman stats-2012-04-22-Please stop me before I barf again</a></p>
<p>13 0.86945462 <a title="1401-lda-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-01-Two_Postdoc_Positions_Available_on_Bayesian_Hierarchical_Modeling.html">62 andrew gelman stats-2010-06-01-Two Postdoc Positions Available on Bayesian Hierarchical Modeling</a></p>
<p>14 0.86012065 <a title="1401-lda-14" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-26-Sleazy_sock_puppet_can%E2%80%99t_stop_spamming_our_discussion_of_compressed_sensing_and_promoting_the_work_of_Xiteng_Liu.html">2306 andrew gelman stats-2014-04-26-Sleazy sock puppet can’t stop spamming our discussion of compressed sensing and promoting the work of Xiteng Liu</a></p>
<p>15 0.85945421 <a title="1401-lda-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-20-Adding_more_information_can_make_the_variance_go_up_%28depending_on_your_model%29.html">810 andrew gelman stats-2011-07-20-Adding more information can make the variance go up (depending on your model)</a></p>
<p>16 0.85368371 <a title="1401-lda-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-15-A_silly_paper_that_tries_to_make_fun_of_multilevel_models.html">854 andrew gelman stats-2011-08-15-A silly paper that tries to make fun of multilevel models</a></p>
<p>17 0.84772688 <a title="1401-lda-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-15-Does_quantum_uncertainty_have_a_place_in_everyday_applied_statistics%3F.html">1857 andrew gelman stats-2013-05-15-Does quantum uncertainty have a place in everyday applied statistics?</a></p>
<p>18 0.8381052 <a title="1401-lda-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-19-The_grasshopper_wins%2C_and_Greg_Mankiw%E2%80%99s_grandmother_would_be_%E2%80%9Cshocked_and_appalled%E2%80%9D_all_over_again.html">1728 andrew gelman stats-2013-02-19-The grasshopper wins, and Greg Mankiw’s grandmother would be “shocked and appalled” all over again</a></p>
<p>19 0.83754468 <a title="1401-lda-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-27-One_way_that_psychology_research_is_different_than_medical_research.html">433 andrew gelman stats-2010-11-27-One way that psychology research is different than medical research</a></p>
<p>20 0.83053064 <a title="1401-lda-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-13-Jim_Campbell_argues_that_Larry_Bartels%E2%80%99s_%E2%80%9CUnequal_Democracy%E2%80%9D_findings_are_not_robust.html">659 andrew gelman stats-2011-04-13-Jim Campbell argues that Larry Bartels’s “Unequal Democracy” findings are not robust</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
