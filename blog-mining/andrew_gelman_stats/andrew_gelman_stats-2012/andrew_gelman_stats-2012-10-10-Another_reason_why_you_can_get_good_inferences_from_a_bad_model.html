<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1527 andrew gelman stats-2012-10-10-Another reason why you can get good inferences from a bad model</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2012" href="../home/andrew_gelman_stats-2012_home.html">andrew_gelman_stats-2012</a> <a title="andrew_gelman_stats-2012-1527" href="#">andrew_gelman_stats-2012-1527</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1527 andrew gelman stats-2012-10-10-Another reason why you can get good inferences from a bad model</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2012-1527-html" href="http://andrewgelman.com/2012/10/10/another-reason-why-you-can-get-good-inferences-from-a-bad-model/">html</a></p><p>Introduction: John Cook  considers  how people justify probability distribution assumptions:
  
Sometimes distribution assumptions are not justified.


Sometimes distributions can be derived from fundamental principles [or] . . . on theoretical grounds. For example, large samples and the central limit theorem together may justify assuming that something is normally distributed.


Often the choice of distribution is somewhat arbitrary, chosen by intuition or for convenience, and then empirically shown to work well enough.


Sometimes a distribution can be a bad fit and still work well, depending on what you’re asking of it.
  
Cook continues:
  
The last point is particularly interesting. It’s not hard to imagine that a poor fit would produce poor results. It’s surprising when a poor fit produces good results.
  
And then he gives an example of an effective but inaccurate model used to model survival times in a clinical trial.  Cook explains:
  
The [poorly-fitting] method works well because of the q</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 John Cook  considers  how people justify probability distribution assumptions:    Sometimes distribution assumptions are not justified. [sent-1, score-0.861]
</p><p>2 For example, large samples and the central limit theorem together may justify assuming that something is normally distributed. [sent-6, score-0.805]
</p><p>3 Often the choice of distribution is somewhat arbitrary, chosen by intuition or for convenience, and then empirically shown to work well enough. [sent-7, score-0.439]
</p><p>4 Sometimes a distribution can be a bad fit and still work well, depending on what you’re asking of it. [sent-8, score-0.477]
</p><p>5 It’s not hard to imagine that a poor fit would produce poor results. [sent-10, score-0.397]
</p><p>6 And then he gives an example of an effective but inaccurate model used to model survival times in a clinical trial. [sent-12, score-0.782]
</p><p>7 Cook explains:    The [poorly-fitting] method works well because of the question being asked. [sent-13, score-0.398]
</p><p>8 The method is not being asked to accurately model the distribution of survival times for patients in the trial. [sent-14, score-1.013]
</p><p>9 As the simulations in this paper show, the method makes the right decision with high probability, even when the actual survival times are not exponentially distributed. [sent-16, score-0.526]
</p><p>10 An example where a bad model works well because of its implicit assumptions    In  Section 9. [sent-18, score-0.807]
</p><p>11 3 of Bayesian Data Analysis (second edition) , we compare several different methods for estimating a population total from a random sample in an artificial problem in which the population is the set of all cities and towns in a state. [sent-19, score-0.567]
</p><p>12 The data are skewed—some cities have much more population than others—but if you use standard survey-sampling estimates and standard errors, you get OK inferences. [sent-20, score-0.395]
</p><p>13 The inferences are not perfect—in particular, the confidence interval can include negative values because the brute-force approach doesn’t “know” that the data (city populations) are all positive—but the intervals make sense and have reasonable coverage properties. [sent-21, score-0.619]
</p><p>14 In contrast, as Don Rubin showed when he first considered this example, comparable analyses applying the normal distribution to log or power-transformed data can give horrible answers. [sent-22, score-0.449]
</p><p>15 How come the interval estimates based on these skewed data have reasonable coverage we use the normal distribution, while inferences based on the much more sensible lognormal or power-transformed models are so disastrous? [sent-24, score-1.081]
</p><p>16 A quick answer is that the normal-theory method makes implicit use of the central limit theorem, but then this just pushes the question back one step:  Why should the central limit theorem apply here? [sent-25, score-1.136]
</p><p>17 The theorem applies for this finite sample (n=100, in this case) because, although the underlying distribution is skewed, there are no extreme outliers. [sent-27, score-0.619]
</p><p>18 By using the normal-based interval, we are implicitly assuming a reasonable upper bound in the population. [sent-28, score-0.435]
</p><p>19 And, in fact, if we put an upper bound into the power-transformed model, it works even better. [sent-29, score-0.397]
</p><p>20 Sometimes an ill-fitting model works well because, although it doesn’t fit much of the data, it includes some assumption that is relevant to inferences, some aspect of the model that would be difficult to ascertain from the data alone. [sent-31, score-1.078]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('distribution', 0.267), ('skewed', 0.224), ('theorem', 0.219), ('survival', 0.205), ('cook', 0.185), ('model', 0.171), ('limit', 0.166), ('interval', 0.165), ('works', 0.156), ('inferences', 0.141), ('method', 0.141), ('poor', 0.136), ('justify', 0.133), ('bound', 0.13), ('central', 0.13), ('cities', 0.128), ('assumptions', 0.126), ('sometimes', 0.125), ('fit', 0.125), ('coverage', 0.12), ('upper', 0.111), ('population', 0.109), ('implicit', 0.104), ('well', 0.101), ('reasonable', 0.101), ('assumption', 0.097), ('ascertain', 0.097), ('disastrous', 0.097), ('assuming', 0.093), ('times', 0.093), ('data', 0.092), ('normal', 0.09), ('towns', 0.087), ('exponentially', 0.087), ('bad', 0.085), ('lognormal', 0.082), ('pushes', 0.08), ('inaccurate', 0.078), ('asked', 0.072), ('empirically', 0.071), ('artificial', 0.069), ('although', 0.068), ('considers', 0.068), ('estimates', 0.066), ('populations', 0.065), ('convenience', 0.065), ('sample', 0.065), ('example', 0.064), ('normally', 0.064), ('patients', 0.064)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000002 <a title="1527-tfidf-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-10-Another_reason_why_you_can_get_good_inferences_from_a_bad_model.html">1527 andrew gelman stats-2012-10-10-Another reason why you can get good inferences from a bad model</a></p>
<p>Introduction: John Cook  considers  how people justify probability distribution assumptions:
  
Sometimes distribution assumptions are not justified.


Sometimes distributions can be derived from fundamental principles [or] . . . on theoretical grounds. For example, large samples and the central limit theorem together may justify assuming that something is normally distributed.


Often the choice of distribution is somewhat arbitrary, chosen by intuition or for convenience, and then empirically shown to work well enough.


Sometimes a distribution can be a bad fit and still work well, depending on what you’re asking of it.
  
Cook continues:
  
The last point is particularly interesting. It’s not hard to imagine that a poor fit would produce poor results. It’s surprising when a poor fit produces good results.
  
And then he gives an example of an effective but inaccurate model used to model survival times in a clinical trial.  Cook explains:
  
The [poorly-fitting] method works well because of the q</p><p>2 0.20369205 <a title="1527-tfidf-2" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-19-Transformations_for_non-normal_data.html">2176 andrew gelman stats-2014-01-19-Transformations for non-normal data</a></p>
<p>Introduction: Steve Peterson writes:
  
I recently submitted a proposal on applying a Bayesian analysis to gender comparisons on motivational constructs. I had an idea on how to improve the model I used and was hoping you could give me some feedback.


The data come from a survey based on 5-point Likert scales. Different constructs are measured for each student as scores derived from averaging a student’s responses on particular subsets of survey questions. (I suppose it is not uncontroversial to treat these scores as interval measures and would be interested to hear if you have any objections.) I am comparing genders on each construct. Researchers typically use t-tests to do so.


To use a Bayesian approach I applied the programs written in R and JAGS by John Kruschke for estimating the difference of means:


http://www.indiana.edu/~kruschke/BEST/


An issue in that analysis is that the distributions of student scores are not normal. There was skewness in some of the distributions and not always in</p><p>3 0.1965611 <a title="1527-tfidf-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-24-Why_it_doesn%E2%80%99t_make_sense_in_general_to_form_confidence_intervals_by_inverting_hypothesis_tests.html">1913 andrew gelman stats-2013-06-24-Why it doesn’t make sense in general to form confidence intervals by inverting hypothesis tests</a></p>
<p>Introduction: I’m reposing  this  classic from 2011 . . . Peter Bergman pointed me to  this discussion  from Cyrus of  a presentation  by Guido Imbens on design of randomized experiments.
 
Cyrus writes:
  
The standard analysis that Imbens proposes includes (1) a Fisher-type permutation test of the sharp null hypothesis–what Imbens referred to as “testing”–along with a (2) Neyman-type point estimate of the sample average treatment effect and confidence interval–what Imbens referred to as “estimation.” . . .


Imbens claimed that testing and estimation are separate enterprises with separate goals and that the two should not be confused. I [Cyrus] took it as a warning against proposals that use “inverted” tests in order to produce point estimates and confidence intervals. There is no reason that such confidence intervals will have accurate coverage except under rather dire assumptions, meaning that they are not “confidence intervals” in the way that we usually think of them.
  
I agree completely.  T</p><p>4 0.19645916 <a title="1527-tfidf-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-30-Works_well_versus_well_understood.html">738 andrew gelman stats-2011-05-30-Works well versus well understood</a></p>
<p>Introduction: John Cook  discusses  the John Tukey quote, “The test of a good procedure is how well it works, not how well it is understood.”  Cook writes:
  
At some level, it’s hard to argue against this. Statistical procedures operate on empirical data, so it makes sense that the procedures themselves be evaluated empirically.


But I [Cook] question whether we really know that a statistical procedure works well if it isn’t well understood. Specifically, I’m skeptical of complex statistical methods whose only credentials are a handful of simulations. “We don’t have any theoretical results, buy hey, it works well in practice. Just look at the simulations.”


Every method works well on the scenarios its author publishes, almost by definition. If the method didn’t handle a scenario well, the author would publish a different scenario.
  
I agree with Cook but would give a slightly different emphasis.  I’d say that a lot of methods can work when they are done well.  See the second meta-principle liste</p><p>5 0.19578876 <a title="1527-tfidf-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-25-Why_it_doesn%E2%80%99t_make_sense_in_general_to_form_confidence_intervals_by_inverting_hypothesis_tests.html">870 andrew gelman stats-2011-08-25-Why it doesn’t make sense in general to form confidence intervals by inverting hypothesis tests</a></p>
<p>Introduction: Peter Bergman points me to  this discussion  from Cyrus of  a presentation  by Guido Imbens on design of randomized experiments.
 
Cyrus writes:
  
The standard analysis that Imbens proposes includes (1) a Fisher-type permutation test of the sharp null hypothesis–what Imbens referred to as “testing”–along with a (2) Neyman-type point estimate of the sample average treatment effect and confidence interval–what Imbens referred to as “estimation.” . . .


Imbens claimed that testing and estimation are separate enterprises with separate goals and that the two should not be confused. I [Cyrus] took it as a warning against proposals that use “inverted” tests in order to produce point estimates and confidence intervals. There is no reason that such confidence intervals will have accurate coverage except under rather dire assumptions, meaning that they are not “confidence intervals” in the way that we usually think of them.
  
I agree completely.  This is something I’ve been saying for a long</p><p>6 0.1947953 <a title="1527-tfidf-6" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-21-Random_matrices_in_the_news.html">2258 andrew gelman stats-2014-03-21-Random matrices in the news</a></p>
<p>7 0.1696008 <a title="1527-tfidf-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-09-How_to_model_distributions_that_have_outliers_in_one_direction.html">2128 andrew gelman stats-2013-12-09-How to model distributions that have outliers in one direction</a></p>
<p>8 0.16615921 <a title="1527-tfidf-8" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-06-Bootstrap_averaging%3A_Examples_where_it_works_and_where_it_doesn%E2%80%99t_work.html">2201 andrew gelman stats-2014-02-06-Bootstrap averaging: Examples where it works and where it doesn’t work</a></p>
<p>9 0.16591814 <a title="1527-tfidf-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-18-Should_we_always_be_using_the_t_and_robit_instead_of_the_normal_and_logit%3F.html">773 andrew gelman stats-2011-06-18-Should we always be using the t and robit instead of the normal and logit?</a></p>
<p>10 0.1633507 <a title="1527-tfidf-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>11 0.15518583 <a title="1527-tfidf-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<p>12 0.14987914 <a title="1527-tfidf-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-07-When_you%E2%80%99re_planning_on_fitting_a_model%2C_build_up_to_it_by_fitting_simpler_models_first.__Then%2C_once_you_have_a_model_you_like%2C_check_the_hell_out_of_it.html">1972 andrew gelman stats-2013-08-07-When you’re planning on fitting a model, build up to it by fitting simpler models first.  Then, once you have a model you like, check the hell out of it</a></p>
<p>13 0.14855807 <a title="1527-tfidf-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-20-The_pervasive_twoishness_of_statistics%3B_in_particular%2C_the_%E2%80%9Csampling_distribution%E2%80%9D_and_the_%E2%80%9Clikelihood%E2%80%9D_are_two_different_models%2C_and_that%E2%80%99s_a_good_thing.html">774 andrew gelman stats-2011-06-20-The pervasive twoishness of statistics; in particular, the “sampling distribution” and the “likelihood” are two different models, and that’s a good thing</a></p>
<p>14 0.14691548 <a title="1527-tfidf-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-10-21-The_future_%28and_past%29_of_statistical_sciences.html">2072 andrew gelman stats-2013-10-21-The future (and past) of statistical sciences</a></p>
<p>15 0.14637376 <a title="1527-tfidf-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-14-Everyone%E2%80%99s_trading_bias_for_variance_at_some_point%2C_it%E2%80%99s_just_done_at_different_places_in_the_analyses.html">1763 andrew gelman stats-2013-03-14-Everyone’s trading bias for variance at some point, it’s just done at different places in the analyses</a></p>
<p>16 0.1439178 <a title="1527-tfidf-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-21-Instead_of_%E2%80%9Cconfidence_interval%2C%E2%80%9D_let%E2%80%99s_say_%E2%80%9Cuncertainty_interval%E2%80%9D.html">480 andrew gelman stats-2010-12-21-Instead of “confidence interval,” let’s say “uncertainty interval”</a></p>
<p>17 0.14334401 <a title="1527-tfidf-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-16-Long_discussion_about_causal_inference_and_the_use_of_hierarchical_models_to_bridge_between_different_inferential_settings.html">1418 andrew gelman stats-2012-07-16-Long discussion about causal inference and the use of hierarchical models to bridge between different inferential settings</a></p>
<p>18 0.14329834 <a title="1527-tfidf-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-28-Another_argument_in_favor_of_expressing_conditional_probability_statements_using_the_population_distribution.html">56 andrew gelman stats-2010-05-28-Another argument in favor of expressing conditional probability statements using the population distribution</a></p>
<p>19 0.14182201 <a title="1527-tfidf-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-25-How_do_you_interpret_standard_errors_from_a_regression_fit_to_the_entire_population%3F.html">972 andrew gelman stats-2011-10-25-How do you interpret standard errors from a regression fit to the entire population?</a></p>
<p>20 0.14156926 <a title="1527-tfidf-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-19-The_mysterious_Gamma_%281.4%2C_0.4%29.html">669 andrew gelman stats-2011-04-19-The mysterious Gamma (1.4, 0.4)</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.264), (1, 0.182), (2, 0.065), (3, 0.009), (4, 0.018), (5, -0.007), (6, 0.021), (7, 0.033), (8, 0.076), (9, -0.053), (10, -0.019), (11, 0.011), (12, -0.074), (13, -0.019), (14, -0.099), (15, -0.017), (16, 0.01), (17, -0.025), (18, 0.04), (19, -0.057), (20, 0.085), (21, -0.045), (22, 0.012), (23, -0.034), (24, 0.094), (25, 0.018), (26, -0.079), (27, -0.014), (28, 0.052), (29, 0.1), (30, -0.033), (31, -0.043), (32, -0.017), (33, 0.038), (34, 0.046), (35, 0.041), (36, -0.057), (37, 0.068), (38, -0.052), (39, -0.008), (40, 0.095), (41, -0.052), (42, 0.02), (43, -0.041), (44, 0.037), (45, 0.003), (46, 0.026), (47, 0.021), (48, 0.027), (49, 0.035)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96141535 <a title="1527-lsi-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-10-Another_reason_why_you_can_get_good_inferences_from_a_bad_model.html">1527 andrew gelman stats-2012-10-10-Another reason why you can get good inferences from a bad model</a></p>
<p>Introduction: John Cook  considers  how people justify probability distribution assumptions:
  
Sometimes distribution assumptions are not justified.


Sometimes distributions can be derived from fundamental principles [or] . . . on theoretical grounds. For example, large samples and the central limit theorem together may justify assuming that something is normally distributed.


Often the choice of distribution is somewhat arbitrary, chosen by intuition or for convenience, and then empirically shown to work well enough.


Sometimes a distribution can be a bad fit and still work well, depending on what you’re asking of it.
  
Cook continues:
  
The last point is particularly interesting. It’s not hard to imagine that a poor fit would produce poor results. It’s surprising when a poor fit produces good results.
  
And then he gives an example of an effective but inaccurate model used to model survival times in a clinical trial.  Cook explains:
  
The [poorly-fitting] method works well because of the q</p><p>2 0.80809009 <a title="1527-lsi-2" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-19-Transformations_for_non-normal_data.html">2176 andrew gelman stats-2014-01-19-Transformations for non-normal data</a></p>
<p>Introduction: Steve Peterson writes:
  
I recently submitted a proposal on applying a Bayesian analysis to gender comparisons on motivational constructs. I had an idea on how to improve the model I used and was hoping you could give me some feedback.


The data come from a survey based on 5-point Likert scales. Different constructs are measured for each student as scores derived from averaging a student’s responses on particular subsets of survey questions. (I suppose it is not uncontroversial to treat these scores as interval measures and would be interested to hear if you have any objections.) I am comparing genders on each construct. Researchers typically use t-tests to do so.


To use a Bayesian approach I applied the programs written in R and JAGS by John Kruschke for estimating the difference of means:


http://www.indiana.edu/~kruschke/BEST/


An issue in that analysis is that the distributions of student scores are not normal. There was skewness in some of the distributions and not always in</p><p>3 0.8003791 <a title="1527-lsi-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-03-Boot.html">1881 andrew gelman stats-2013-06-03-Boot</a></p>
<p>Introduction: Joshua Hartshorne writes: 
  
  
I ran several large-N experiments (separate participants) and looked at performance against age. What we want to do is compare age-of-peak-performance across the different tasks (again, different participants).


We bootstrapped age-of-peak-performance. On each iteration, we sampled (with replacement) the X scores at each age, where X=num of participants at that age, and recorded the age at which performance peaked on that task. We then recorded the age at which performance was at peak and repeated. Once we had distributions of age-of-peak-performance, we used the means and SDs to calculate t-statistics to compare the results across different tasks. For graphical presentation, we used medians, interquartile ranges, and 95% confidence intervals (based on the distributions: the range within which 75% and 95% of the bootstrapped peaks appeared). 


While a number of people we consulted with thought this made a lot of sense, one reviewer of the paper insist</p><p>4 0.79818171 <a title="1527-lsi-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-07-Chi-square_FAIL_when_many_cells_have_small_expected_values.html">996 andrew gelman stats-2011-11-07-Chi-square FAIL when many cells have small expected values</a></p>
<p>Introduction: William Perkins, Mark Tygert, and Rachel Ward  write :
  
If a discrete probability distribution in a model being tested for goodness-of-fit is not close to uniform, then forming the Pearson χ2 statistic can involve division by nearly zero. This often leads to serious trouble in practice — even in the absence of round-off errors . . .
  
The problem is not merely that the chi-squared  statistic  doesn’t have the advertised chi-squared  distribution —a reference distribution can always be computed via simulation, either using the posterior predictive distribution or by conditioning on a point estimate of the cell expectations and then making a degrees-of-freedom sort of adjustment.
 
Rather, the problem is that, when there are lots of cells with near-zero expectation, the chi-squared test is mostly noise.
 
And this is not merely a theoretical problem.  It comes up in real examples.
 
Here’s one, taken from the classic 1992 genetics paper of Guo and Thomspson:
 
   
 
And here are the e</p><p>5 0.77860135 <a title="1527-lsi-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-03-Question_about_predictive_checks.html">1363 andrew gelman stats-2012-06-03-Question about predictive checks</a></p>
<p>Introduction: Klaas Metselaar writes: 
  
  
I [Metselaar] am currently involved in a discussion about the use of the notion “predictive” as used in “posterior predictive check”. I would argue that the notion “predictive” should be reserved for posterior checks using information not used in the determination of the posterior. 
I quote from the discussion: 
“However, the predictive uncertainty in a Bayesian calculation requires sampling from all the random variables, and this includes both the model parameters and the residual error”.


My [Metselaar's] comment:


This may be exactly the point I am worried about: shouldn’t the predictive uncertainty be defined as sampling from the posterior parameter distribution +  residual error + sampling from the prediction error distribution? 
Residual error reduces to measurement error in the case of a  model which is perfect for the sample of experiments. Measurement error could be reduced to almost zero by ideal and perfect measurement instruments. 
I would h</p><p>6 0.77469337 <a title="1527-lsi-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-09-How_to_model_distributions_that_have_outliers_in_one_direction.html">2128 andrew gelman stats-2013-12-09-How to model distributions that have outliers in one direction</a></p>
<p>7 0.77456981 <a title="1527-lsi-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-28-Another_argument_in_favor_of_expressing_conditional_probability_statements_using_the_population_distribution.html">56 andrew gelman stats-2010-05-28-Another argument in favor of expressing conditional probability statements using the population distribution</a></p>
<p>8 0.7720713 <a title="1527-lsi-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-16-%E2%80%9CReal_data_can_be_a_pain%E2%80%9D.html">1460 andrew gelman stats-2012-08-16-“Real data can be a pain”</a></p>
<p>9 0.76803041 <a title="1527-lsi-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-24-Analyzing_photon_counts.html">1509 andrew gelman stats-2012-09-24-Analyzing photon counts</a></p>
<p>10 0.75843441 <a title="1527-lsi-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-20-The_pervasive_twoishness_of_statistics%3B_in_particular%2C_the_%E2%80%9Csampling_distribution%E2%80%9D_and_the_%E2%80%9Clikelihood%E2%80%9D_are_two_different_models%2C_and_that%E2%80%99s_a_good_thing.html">774 andrew gelman stats-2011-06-20-The pervasive twoishness of statistics; in particular, the “sampling distribution” and the “likelihood” are two different models, and that’s a good thing</a></p>
<p>11 0.75227165 <a title="1527-lsi-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-25-Why_it_doesn%E2%80%99t_make_sense_in_general_to_form_confidence_intervals_by_inverting_hypothesis_tests.html">870 andrew gelman stats-2011-08-25-Why it doesn’t make sense in general to form confidence intervals by inverting hypothesis tests</a></p>
<p>12 0.75032747 <a title="1527-lsi-12" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-06-Bootstrap_averaging%3A_Examples_where_it_works_and_where_it_doesn%E2%80%99t_work.html">2201 andrew gelman stats-2014-02-06-Bootstrap averaging: Examples where it works and where it doesn’t work</a></p>
<p>13 0.74275517 <a title="1527-lsi-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-27-Average_predictive_comparisons_when_changing_a_pair_of_variables.html">1346 andrew gelman stats-2012-05-27-Average predictive comparisons when changing a pair of variables</a></p>
<p>14 0.73887575 <a title="1527-lsi-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-24-Why_it_doesn%E2%80%99t_make_sense_in_general_to_form_confidence_intervals_by_inverting_hypothesis_tests.html">1913 andrew gelman stats-2013-06-24-Why it doesn’t make sense in general to form confidence intervals by inverting hypothesis tests</a></p>
<p>15 0.72951138 <a title="1527-lsi-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-09-18-Understanding_posterior_p-values.html">2029 andrew gelman stats-2013-09-18-Understanding posterior p-values</a></p>
<p>16 0.71471435 <a title="1527-lsi-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-18-Should_we_always_be_using_the_t_and_robit_instead_of_the_normal_and_logit%3F.html">773 andrew gelman stats-2011-06-18-Should we always be using the t and robit instead of the normal and logit?</a></p>
<p>17 0.7043395 <a title="1527-lsi-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-09-The_first_version_of_my_%E2%80%9Cinference_from_iterative_simulation_using_parallel_sequences%E2%80%9D_paper%21.html">1309 andrew gelman stats-2012-05-09-The first version of my “inference from iterative simulation using parallel sequences” paper!</a></p>
<p>18 0.70245218 <a title="1527-lsi-18" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-29-Bayesian_Uncertainty_Quantification_for_Differential_Equations%21.html">2311 andrew gelman stats-2014-04-29-Bayesian Uncertainty Quantification for Differential Equations!</a></p>
<p>19 0.70216954 <a title="1527-lsi-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-30-David_Hogg_on_statistics.html">1401 andrew gelman stats-2012-06-30-David Hogg on statistics</a></p>
<p>20 0.69956213 <a title="1527-lsi-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-30-Stan_is_fast.html">1476 andrew gelman stats-2012-08-30-Stan is fast</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.012), (5, 0.021), (6, 0.032), (9, 0.011), (15, 0.039), (16, 0.067), (21, 0.062), (24, 0.127), (29, 0.046), (65, 0.024), (76, 0.01), (84, 0.026), (93, 0.016), (99, 0.388)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98951733 <a title="1527-lda-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-10-Another_reason_why_you_can_get_good_inferences_from_a_bad_model.html">1527 andrew gelman stats-2012-10-10-Another reason why you can get good inferences from a bad model</a></p>
<p>Introduction: John Cook  considers  how people justify probability distribution assumptions:
  
Sometimes distribution assumptions are not justified.


Sometimes distributions can be derived from fundamental principles [or] . . . on theoretical grounds. For example, large samples and the central limit theorem together may justify assuming that something is normally distributed.


Often the choice of distribution is somewhat arbitrary, chosen by intuition or for convenience, and then empirically shown to work well enough.


Sometimes a distribution can be a bad fit and still work well, depending on what you’re asking of it.
  
Cook continues:
  
The last point is particularly interesting. It’s not hard to imagine that a poor fit would produce poor results. It’s surprising when a poor fit produces good results.
  
And then he gives an example of an effective but inaccurate model used to model survival times in a clinical trial.  Cook explains:
  
The [poorly-fitting] method works well because of the q</p><p>2 0.98227847 <a title="1527-lda-2" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-05-27-A_whole_fleet_of_gremlins%3A__Looking_more_carefully_at_Richard_Tol%E2%80%99s_twice-corrected_paper%2C_%E2%80%9CThe_Economic_Effects_of_Climate_Change%E2%80%9D.html">2350 andrew gelman stats-2014-05-27-A whole fleet of gremlins:  Looking more carefully at Richard Tol’s twice-corrected paper, “The Economic Effects of Climate Change”</a></p>
<p>Introduction: We had a  discussion  the other day of a paper, “The Economic Effects of Climate Change,” by economist Richard Tol.
 
The paper came to my attention after I saw a  notice  from Adam Marcus that it was recently revised because of data errors.  But after looking at the paper more carefully, I see a bunch of other problems that, to me, make the whole analysis close to useless as it stands.
 
I think this is worth discussing because the paper has been somewhat influential (so far cited 328 times, according to Google Scholar) and has even been cited in the  popular press  as evidence that “Climate change has done more good than harm so far and is likely to continue doing so for most of this century . . . There are many likely effects of climate change: positive and negative, economic and ecological, humanitarian and financial. And if you aggregate them all, the overall effect is positive today — and likely to stay positive until around 2080. That was the conclusion of Professor Richard Tol</p><p>3 0.98122901 <a title="1527-lda-3" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-13-Judea_Pearl_overview_on_causal_inference%2C_and_more_general_thoughts_on_the_reexpression_of_existing_methods_by_considering_their_implicit_assumptions.html">2170 andrew gelman stats-2014-01-13-Judea Pearl overview on causal inference, and more general thoughts on the reexpression of existing methods by considering their implicit assumptions</a></p>
<p>Introduction: This  material should be familiar to many of you but could be helpful to newcomers.  Pearl writes:
  
ALL causal conclusions in nonexperimental settings must be based on untested, judgmental assumptions that investigators are prepared to defend on scientific grounds. . . .


To understand what the world should be like for a given procedure to work is of no lesser scientific value than seeking evidence for how the world works . . .


Assumptions are self-destructive in their honesty. The more explicit the assumption, the more criticism it invites . . . causal diagrams invite the harshest criticism because they make assumptions more explicit and more transparent than other representation schemes.
  
As regular readers know (for example, search this blog for “Pearl”), I have not got much out of the causal-diagrams approach myself, but in general I think that when there are multiple, mathematically equivalent methods of getting the same answer, we tend to go with the framework we are used</p><p>4 0.98095143 <a title="1527-lda-4" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-07-Selection_bias_in_the_reporting_of_shaky_research.html">2236 andrew gelman stats-2014-03-07-Selection bias in the reporting of shaky research</a></p>
<p>Introduction: I’ll reorder this week’s  posts  a bit in order to continue on a topic that came up yesterday.
 
A couple days ago a reporter wrote to me asking what I thought of  this paper  on Money, Status, and the Ovulatory Cycle.  I responded:
  
Given the quality of the  earlier paper  by these researchers, I’m not inclined to believe anything these people write.  But, to be specific, I can point out some things:


- The authors define low fertility as days 8-14.  Oddly enough, these authors in their earlier paper used days 7-14.  But according to womenshealth.gov, the most fertile days are between days 10 and 17.  The choice of these days affects their analysis, and it is not a good sign that they use different days in different papers.  (see more on this point in sections 2.3 and 3.1 of this paper:  http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf)


- They perform a lot of different analyses, and many others could be performed.  For example, “Study 1 indicates that ovul</p><p>5 0.97994232 <a title="1527-lda-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-18-IRB_nightmares.html">1539 andrew gelman stats-2012-10-18-IRB nightmares</a></p>
<p>Introduction: Andrew Perrin  nails it :
  
Twice a year, like clockwork, the ethics cops at the IRB [institutional review board, the group on campus that has to approve research involving human subjects] take a break from deciding whether or not radioactive isotopes can be administered to prison populations to cure restless-leg syndrome to dream up some fancy new way in which participating in an automated telephone poll might cause harm.
  
Perrin adds:
  
The list of exemptions to IRB review is too short and, more importantly, contains no guiding principle as to what makes exempt. . . . [and] Even exemptions require approval by the IRB.
  
He also voices a thought I’ve had many times, which is that there are all sorts of things you or I or anyone else can do on the street (for example, go up to people and ask them personal questions, drop objects and see if people pick them up, stage fights with our friends to see the reactions of bystanders, etc etc etc) but for which we have to go through an IRB</p><p>6 0.97976726 <a title="1527-lda-6" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-20-Do_differences_between_biology_and_statistics_explain_some_of_our_diverging_attitudes_regarding_criticism_and_replication_of_scientific_claims%3F.html">2218 andrew gelman stats-2014-02-20-Do differences between biology and statistics explain some of our diverging attitudes regarding criticism and replication of scientific claims?</a></p>
<p>7 0.97951126 <a title="1527-lda-7" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-21-Everything_I_need_to_know_about_Bayesian_statistics%2C_I_learned_in_eight_schools..html">2180 andrew gelman stats-2014-01-21-Everything I need to know about Bayesian statistics, I learned in eight schools.</a></p>
<p>8 0.97938126 <a title="1527-lda-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-29-The_blogroll.html">1832 andrew gelman stats-2013-04-29-The blogroll</a></p>
<p>9 0.97902447 <a title="1527-lda-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-11-06-%E2%80%9CMarginally_significant%E2%80%9D.html">2091 andrew gelman stats-2013-11-06-“Marginally significant”</a></p>
<p>10 0.97900039 <a title="1527-lda-10" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-10-Update_on_Levitt_paper_on_child_car_seats.html">1491 andrew gelman stats-2012-09-10-Update on Levitt paper on child car seats</a></p>
<p>11 0.97880912 <a title="1527-lda-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-27-What_is_%E2%80%9Cexplanation%E2%80%9D%3F.html">1742 andrew gelman stats-2013-02-27-What is “explanation”?</a></p>
<p>12 0.97815895 <a title="1527-lda-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-24-Blogs_vs._real_journalism.html">868 andrew gelman stats-2011-08-24-Blogs vs. real journalism</a></p>
<p>13 0.9781009 <a title="1527-lda-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-07-Evaluating_predictions_of_political_events.html">563 andrew gelman stats-2011-02-07-Evaluating predictions of political events</a></p>
<p>14 0.97793853 <a title="1527-lda-14" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-20-Domain_specificity%3A__Does_being_really_really_smart_or_really_really_rich_qualify_you_to_make_economic_policy%3F.html">45 andrew gelman stats-2010-05-20-Domain specificity:  Does being really really smart or really really rich qualify you to make economic policy?</a></p>
<p>15 0.97776443 <a title="1527-lda-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-02-Experimental_reasoning_in_social_science.html">785 andrew gelman stats-2011-07-02-Experimental reasoning in social science</a></p>
<p>16 0.97771382 <a title="1527-lda-16" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-12-More_on_publishing_in_journals.html">2245 andrew gelman stats-2014-03-12-More on publishing in journals</a></p>
<p>17 0.97765231 <a title="1527-lda-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-24-Question_14_of_my_final_exam_for_Design_and_Analysis_of_Sample_Surveys.html">1341 andrew gelman stats-2012-05-24-Question 14 of my final exam for Design and Analysis of Sample Surveys</a></p>
<p>18 0.97754872 <a title="1527-lda-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-21-Two_reviews_of_Nate_Silver%E2%80%99s_new_book%2C_from_Kaiser_Fung_and_Cathy_O%E2%80%99Neil.html">1634 andrew gelman stats-2012-12-21-Two reviews of Nate Silver’s new book, from Kaiser Fung and Cathy O’Neil</a></p>
<p>19 0.97753304 <a title="1527-lda-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-03-Did_Steven_Levitt_really_believe_in_2008_that_Obama_%E2%80%9Cwould_be_the_greatest_president_in_history%E2%80%9D%3F.html">1650 andrew gelman stats-2013-01-03-Did Steven Levitt really believe in 2008 that Obama “would be the greatest president in history”?</a></p>
<p>20 0.97746789 <a title="1527-lda-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-06-11-Bayes_in_the_research_conversation.html">2368 andrew gelman stats-2014-06-11-Bayes in the research conversation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
