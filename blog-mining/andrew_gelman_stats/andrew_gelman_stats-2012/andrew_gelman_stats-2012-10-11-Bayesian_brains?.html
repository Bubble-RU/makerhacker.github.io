<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1529 andrew gelman stats-2012-10-11-Bayesian brains?</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2012" href="../home/andrew_gelman_stats-2012_home.html">andrew_gelman_stats-2012</a> <a title="andrew_gelman_stats-2012-1529" href="#">andrew_gelman_stats-2012-1529</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1529 andrew gelman stats-2012-10-11-Bayesian brains?</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2012-1529-html" href="http://andrewgelman.com/2012/10/11/i-notice-a-slightly-garbled-version-of-bayesian-inference-which-provokes-some-thoughts-on-the-applicability-of-bayesian-models-of-human-reasoning/">html</a></p><p>Introduction: Psychology researcher Alison Gopnik  discusses  the idea that some of the systematic problems with human reasoning can be explained by systematic flaws in the statistical models we implicitly use.
 
I really like this idea and I’ll return to it in a bit.  But first I need to discuss a minor (but, I think, ultimately crucial) disagreement I have with how Gopnik describes Bayesian inference.  She writes:
  
The Bayesian idea is simple, but it turns out to be very powerful. It’s so powerful, in fact, that computer scientists are using it to design intelligent learning machines, and more and more psychologists think that it might explain human intelligence. Bayesian inference is a way to use statistical data to evaluate hypotheses and make predictions. These might be scientific hypotheses and predictions or everyday ones.
  
So far, so good.  Next comes the problem (as I see it).  Gopnik writes:
  
Here’s a simple bit of Bayesian election thinking. In early September, the polls suddenly im</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Psychology researcher Alison Gopnik  discusses  the idea that some of the systematic problems with human reasoning can be explained by systematic flaws in the statistical models we implicitly use. [sent-1, score-0.563]
</p><p>2 It’s so powerful, in fact, that computer scientists are using it to design intelligent learning machines, and more and more psychologists think that it might explain human intelligence. [sent-5, score-0.327]
</p><p>3 Bayesian inference is a way to use statistical data to evaluate hypotheses and make predictions. [sent-6, score-0.325]
</p><p>4 These might be scientific hypotheses and predictions or everyday ones. [sent-7, score-0.339]
</p><p>5 Combining your prior beliefs about the hypotheses and the likelihood of the data can help you . [sent-19, score-0.368]
</p><p>6 In this case, the inspiring convention idea is both likely to begin with and likely to have led to the change in the polls, so it wins out over the other two. [sent-22, score-0.551]
</p><p>7 As noted, this point is minor–I have no problem with Gopnik’s summary that one of the hypotheses “wins out over the other two. [sent-28, score-0.263]
</p><p>8 (This is probably a good place for me to plug my  article  with Kari Lock from a couple years ago on Bayesian combination of state polls and election forecasts, where we use continuous weighting. [sent-30, score-0.412]
</p><p>9 )    Blame the discrete models, not the priors    One way this seemingly minor point can matter is when we follow Gopnik’s suggestion that Bayesian inference “might explain human intelligence. [sent-31, score-0.662]
</p><p>10 But discrete thinking does not describe how much of the biological social world works. [sent-33, score-0.338]
</p><p>11 If we, as humans, take these continuous phenomena and try to model them discretely, we will trip up, in predictable ways–even if we use (discrete) Bayesian methods. [sent-35, score-0.276]
</p><p>12 To put it another way:  what if Josh Tenenbaum and his colleagues (not mentioned in Gopnik’s article but you can search for them here on the blog) are right that our brains use some sort of approximate discrete Bayesian reasoning to make decisions and perform inferences about the world? [sent-36, score-0.523]
</p><p>13 ”  She’s referring to this experiment done in her lab:  “We gave 4-year-olds and adults evidence about a toy that worked in an unusual way. [sent-39, score-0.332]
</p><p>14 The 4-year-olds were actually more likely to figure out the toy than the adults were. [sent-41, score-0.417]
</p><p>15 ”   In that example, Gopnik might well be correct:  it seems reasonable to suspect that a kid will have a better prior than an adult on how a toy works. [sent-42, score-0.351]
</p><p>16 More generally, though, I think we should avoid the temptation to think that, when a Bayesian inference goes wrong, it has to be a problem with the prior. [sent-43, score-0.325]
</p><p>17 In many cases, the model matters (for example, in our discussion above about natural-seeming but flawed discrete models). [sent-45, score-0.258]
</p><p>18 If, as I think is the case, our brains like discrete models (perhaps they can be more quickly coded and computed) but the world is continuous and varying, this suggests interesting systematic ways that our brains might be misunderstanding the world in everyday reasoning. [sent-47, score-1.27]
</p><p>19 (Conversely, if discrete models really do have major computational advantages, maybe statisticians like myself should be giving them a second look. [sent-48, score-0.358]
</p><p>20 This post had been titled, “I notice a (slightly) garbled version of Bayesian inference, which provokes some thoughts on the applicability of Bayesian models of human reasoning. [sent-51, score-0.367]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('gopnik', 0.563), ('discrete', 0.258), ('toy', 0.202), ('hypotheses', 0.19), ('bayesian', 0.19), ('brains', 0.151), ('continuous', 0.134), ('adults', 0.13), ('polls', 0.111), ('minor', 0.11), ('varying', 0.11), ('garbled', 0.109), ('systematic', 0.109), ('human', 0.107), ('models', 0.1), ('convention', 0.098), ('slightly', 0.096), ('predictable', 0.091), ('prior', 0.09), ('everyday', 0.09), ('likelihood', 0.088), ('wins', 0.087), ('likely', 0.085), ('inference', 0.084), ('world', 0.08), ('idea', 0.075), ('problem', 0.073), ('discretely', 0.066), ('led', 0.066), ('reasoning', 0.063), ('alison', 0.063), ('rationally', 0.06), ('election', 0.06), ('might', 0.059), ('kari', 0.058), ('tenenbaum', 0.058), ('think', 0.058), ('plug', 0.056), ('inspiring', 0.055), ('manipulated', 0.053), ('lock', 0.053), ('explain', 0.053), ('correct', 0.052), ('temptation', 0.052), ('applicability', 0.051), ('pollsters', 0.051), ('use', 0.051), ('intelligent', 0.05), ('matter', 0.05), ('overly', 0.05)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="1529-tfidf-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-11-Bayesian_brains%3F.html">1529 andrew gelman stats-2012-10-11-Bayesian brains?</a></p>
<p>Introduction: Psychology researcher Alison Gopnik  discusses  the idea that some of the systematic problems with human reasoning can be explained by systematic flaws in the statistical models we implicitly use.
 
I really like this idea and I’ll return to it in a bit.  But first I need to discuss a minor (but, I think, ultimately crucial) disagreement I have with how Gopnik describes Bayesian inference.  She writes:
  
The Bayesian idea is simple, but it turns out to be very powerful. It’s so powerful, in fact, that computer scientists are using it to design intelligent learning machines, and more and more psychologists think that it might explain human intelligence. Bayesian inference is a way to use statistical data to evaluate hypotheses and make predictions. These might be scientific hypotheses and predictions or everyday ones.
  
So far, so good.  Next comes the problem (as I see it).  Gopnik writes:
  
Here’s a simple bit of Bayesian election thinking. In early September, the polls suddenly im</p><p>2 0.35101178 <a title="1529-tfidf-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-17-%E2%80%9CWhy_Preschool_Shouldn%E2%80%99t_Be_Like_School%E2%80%9D%3F.html">617 andrew gelman stats-2011-03-17-“Why Preschool Shouldn’t Be Like School”?</a></p>
<p>Introduction: Under the heading, “Why Preschool Shouldn’t Be Like School,” cognitive psychologist Alison Gopnik  describes  research showing that four-year-olds learn better if they’re encouraged to discover and show to others, rather than if they’re taught what to do.  This makes sense, but it’s not clear to me why this wouldn’t apply to older kids and adults.  It’s a commonplace in teaching at all levels that students learn by doing and by demonstrating what they can do.  Even when a student is doing nothing but improvising from a template, we generally believe the student will learn better by explaining what’s going on, by having a mental model of the process to go along with the proverbial 10,000 hours or practice.  The challenge is in the implementation, how to get students interested, motivated, and focused enough to put the effort into learning.
 
So why the headline above?  Why does Gopnik’s research support the idea that preschool should be different from school?  I’m not trying to disagree</p><p>3 0.24077415 <a title="1529-tfidf-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-29-A.I._is_Whatever_We_Can%E2%80%99t_Yet_Automate.html">634 andrew gelman stats-2011-03-29-A.I. is Whatever We Can’t Yet Automate</a></p>
<p>Introduction: A common aphorism among artificial intelligence practitioners is that A.I. is whatever machines can’t currently do.  
 
Adam Gopnik, writing for the  New Yorker , has a review called  Get Smart  in the most recent issue (4 April 2011).  Ostensibly, the piece is a review of new books, one by Joshua Foer,  Moonwalking with Einstein: The Art and Science of Remembering Everything , and one by Stephen Baker  Final Jeopardy: Man vs. Machine and the Quest to Know Everything  (which would explain Baker’s spate of  Jeopardy!-related blog posts ).  But like many such pieces in highbrow magazines, the book reviews are just a cover for staking out a philosophical position.  Gopnik does a typically  New Yorker  job in explaining the title of this blog post.
  

 
Gopnik describes his mother as “a logician, linguist, and early Fortran speaker” and goes on to add that she worked on an early machine translation project in Canada.  I’m guessing she’s the Myrna Gopnik behind  this 1968 COLING paper  (LE</p><p>4 0.19805454 <a title="1529-tfidf-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-25-Ways_of_knowing.html">1469 andrew gelman stats-2012-08-25-Ways of knowing</a></p>
<p>Introduction: In  this discussion  from last month, computer science student and Judea Pearl collaborator Elias Barenboim expressed an attitude that hierarchical Bayesian methods might be fine in practice but that they lack theory, that Bayesians can’t succeed in toy problems.  I posted a P.S. there which might not have been noticed so I will put it here:
 
I now realize that there is some disagreement about what constitutes a “guarantee.”  In one of his comments, Barenboim writes, “the assurance we have that the result must hold as long as the assumptions in the model are correct should be regarded as a guarantee.”  In that sense, yes, we have guarantees!  It is fundamental to Bayesian inference that the result must hold if the assumptions in the model are correct.  We have lots of that in Bayesian Data Analysis (particularly in the first four chapters but implicitly elsewhere as well), and this is also covered in the classic books by Lindley, Jaynes, and others.  This sort of guarantee is indeed p</p><p>5 0.19799316 <a title="1529-tfidf-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-28-The_holes_in_my_philosophy_of_Bayesian_data_analysis.html">781 andrew gelman stats-2011-06-28-The holes in my philosophy of Bayesian data analysis</a></p>
<p>Introduction: I’ve been writing a lot about my philosophy of Bayesian statistics and how it fits into Popper’s ideas about falsification and Kuhn’s ideas about scientific revolutions.
 
 Here’s  my long, somewhat technical paper with Cosma Shalizi. 
 Here’s  our shorter overview for the volume on the philosophy of social science. 
 Here’s  my latest try (for an online symposium), focusing on the key issues.
 
I’m pretty happy with my approach–the familiar idea that Bayesian data analysis iterates the three steps of model building, inference, and model checking–but it does have some unresolved (maybe unresolvable) problems.  Here are a couple mentioned in the third of the above links.
 
Consider a simple model with independent data y_1, y_2, .., y_10 ~ N(θ,σ^2), with a prior distribution θ ~ N(0,10^2) and σ known and taking on some value of approximately 10. Inference about μ is straightforward, as is model checking, whether based on graphs or numerical summaries such as the sample variance and skewn</p><p>6 0.17580295 <a title="1529-tfidf-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-25-Continuous_variables_in_Bayesian_networks.html">1228 andrew gelman stats-2012-03-25-Continuous variables in Bayesian networks</a></p>
<p>7 0.16645314 <a title="1529-tfidf-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-30-Stan_Project%3A__Continuous_Relaxations_for_Discrete_MRFs.html">2003 andrew gelman stats-2013-08-30-Stan Project:  Continuous Relaxations for Discrete MRFs</a></p>
<p>8 0.16599371 <a title="1529-tfidf-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-28-More_on_Bayesian_deduction-induction.html">114 andrew gelman stats-2010-06-28-More on Bayesian deduction-induction</a></p>
<p>9 0.16308229 <a title="1529-tfidf-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-20-Kind_of_Bayesian.html">811 andrew gelman stats-2011-07-20-Kind of Bayesian</a></p>
<p>10 0.15973055 <a title="1529-tfidf-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-28-Economists_argue_about_Bayes.html">1695 andrew gelman stats-2013-01-28-Economists argue about Bayes</a></p>
<p>11 0.14745997 <a title="1529-tfidf-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-31-It_not_necessary_that_Bayesian_methods_conform_to_the_likelihood_principle.html">1554 andrew gelman stats-2012-10-31-It not necessary that Bayesian methods conform to the likelihood principle</a></p>
<p>12 0.1424417 <a title="1529-tfidf-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-25-Incoherence_of_Bayesian_data_analysis.html">1510 andrew gelman stats-2012-09-25-Incoherence of Bayesian data analysis</a></p>
<p>13 0.14002772 <a title="1529-tfidf-13" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-24-Empirical_implications_of_Empirical_Implications_of_Theoretical_Models.html">2263 andrew gelman stats-2014-03-24-Empirical implications of Empirical Implications of Theoretical Models</a></p>
<p>14 0.13405317 <a title="1529-tfidf-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<p>15 0.13364737 <a title="1529-tfidf-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-27-Overfitting.html">1431 andrew gelman stats-2012-07-27-Overfitting</a></p>
<p>16 0.13335288 <a title="1529-tfidf-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-09-Coming_to_agreement_on_philosophy_of_statistics.html">1205 andrew gelman stats-2012-03-09-Coming to agreement on philosophy of statistics</a></p>
<p>17 0.13334803 <a title="1529-tfidf-17" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-19-The_%E2%80%9Ceither-or%E2%80%9D_fallacy_of_believing_in_discrete_models%3A__an_example_of_folk_statistics.html">217 andrew gelman stats-2010-08-19-The “either-or” fallacy of believing in discrete models:  an example of folk statistics</a></p>
<p>18 0.13289793 <a title="1529-tfidf-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-30-Useful_models%2C_model_checking%2C_and_external_validation%3A__a_mini-discussion.html">244 andrew gelman stats-2010-08-30-Useful models, model checking, and external validation:  a mini-discussion</a></p>
<p>19 0.13255657 <a title="1529-tfidf-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-27-%E2%80%9CTwo_Dogmas_of_Strong_Objective_Bayesianism%E2%80%9D.html">1779 andrew gelman stats-2013-03-27-“Two Dogmas of Strong Objective Bayesianism”</a></p>
<p>20 0.13183151 <a title="1529-tfidf-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-11-12-Thinking_like_a_statistician_%28continuously%29_rather_than_like_a_civilian_%28discretely%29.html">1575 andrew gelman stats-2012-11-12-Thinking like a statistician (continuously) rather than like a civilian (discretely)</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.231), (1, 0.129), (2, -0.021), (3, 0.063), (4, -0.125), (5, -0.01), (6, -0.024), (7, 0.047), (8, 0.036), (9, -0.011), (10, -0.009), (11, 0.016), (12, -0.022), (13, -0.034), (14, 0.018), (15, 0.036), (16, 0.038), (17, -0.005), (18, -0.03), (19, 0.043), (20, -0.064), (21, 0.023), (22, -0.069), (23, -0.001), (24, -0.017), (25, -0.007), (26, 0.026), (27, -0.021), (28, -0.019), (29, 0.004), (30, -0.006), (31, -0.003), (32, -0.003), (33, -0.034), (34, 0.024), (35, -0.01), (36, -0.007), (37, -0.039), (38, 0.014), (39, -0.02), (40, -0.036), (41, 0.031), (42, 0.015), (43, 0.013), (44, -0.044), (45, 0.008), (46, 0.039), (47, 0.017), (48, 0.035), (49, 0.032)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97651911 <a title="1529-lsi-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-11-Bayesian_brains%3F.html">1529 andrew gelman stats-2012-10-11-Bayesian brains?</a></p>
<p>Introduction: Psychology researcher Alison Gopnik  discusses  the idea that some of the systematic problems with human reasoning can be explained by systematic flaws in the statistical models we implicitly use.
 
I really like this idea and I’ll return to it in a bit.  But first I need to discuss a minor (but, I think, ultimately crucial) disagreement I have with how Gopnik describes Bayesian inference.  She writes:
  
The Bayesian idea is simple, but it turns out to be very powerful. It’s so powerful, in fact, that computer scientists are using it to design intelligent learning machines, and more and more psychologists think that it might explain human intelligence. Bayesian inference is a way to use statistical data to evaluate hypotheses and make predictions. These might be scientific hypotheses and predictions or everyday ones.
  
So far, so good.  Next comes the problem (as I see it).  Gopnik writes:
  
Here’s a simple bit of Bayesian election thinking. In early September, the polls suddenly im</p><p>2 0.85884774 <a title="1529-lsi-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-24-Untangling_the_Jeffreys-Lindley_paradox.html">1182 andrew gelman stats-2012-02-24-Untangling the Jeffreys-Lindley paradox</a></p>
<p>Introduction: Ryan Ickert writes:
  
I was wondering if you’d seen  this post , by a particle physicist with some degree of influence.  Dr. Dorigo works at CERN and Fermilab.


The penultimate paragraph is:

 
From the above expression, the Frequentist researcher concludes that the tracker is indeed biased, and rejects the null hypothesis H0, since there is a less-than-2% probability (P’<α) that a result as the one observed could arise by chance! A Frequentist thus draws, strongly, the opposite conclusion than a Bayesian from the same set of data. How to solve the riddle?
 

He goes on to not solve the riddle.  Perhaps you can?


Surely with the large sample size they have (n=10^6), the precision on the frequentist p-value is pretty good, is it not?
  
My reply:
 
The first comment on the site (by Anonymous [who, just to be clear, is not me; I have no idea who wrote that comment], 22 Feb 2012, 21:27pm) pretty much nails it:  In setting up the Bayesian model, Dorigo assumed a silly distribution on th</p><p>3 0.85168415 <a title="1529-lsi-3" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-24-Non-Bayesian_analysis_of_Bayesian_agents%3F.html">1280 andrew gelman stats-2012-04-24-Non-Bayesian analysis of Bayesian agents?</a></p>
<p>Introduction: Econometrician and statistician Dale Poirier writes:
  
24 years ago (1988, Journal of Economics Perspectives) I [Poirier] noted cognitive dissonance among some economists who treat the agents in their theoretical framework as Bayesians, but then analyze the data (even in the same paper!) as a frequentist. Recently, I have found similar cases in cognitive science. I suspect other disciplines exhibit such behavior. Do you know of any examples in political science?
  
My reply:
 
I don’t know of any such examples in political science.  Game theoretic models are popular in poli sci, but I haven’t seen much in the way of models of Bayesian decision making.
 
Here are two references (not in political science) that might be helpful.
 
1.  I have  argued  that the utility model (popular in economics and political science as a way of providing “microfoundations” for analyses of aggregate behavior) is actually more of a bit of folk-psychology that should not be taken seriously.  To me, it is si</p><p>4 0.84994584 <a title="1529-lsi-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-25-Ways_of_knowing.html">1469 andrew gelman stats-2012-08-25-Ways of knowing</a></p>
<p>Introduction: In  this discussion  from last month, computer science student and Judea Pearl collaborator Elias Barenboim expressed an attitude that hierarchical Bayesian methods might be fine in practice but that they lack theory, that Bayesians can’t succeed in toy problems.  I posted a P.S. there which might not have been noticed so I will put it here:
 
I now realize that there is some disagreement about what constitutes a “guarantee.”  In one of his comments, Barenboim writes, “the assurance we have that the result must hold as long as the assumptions in the model are correct should be regarded as a guarantee.”  In that sense, yes, we have guarantees!  It is fundamental to Bayesian inference that the result must hold if the assumptions in the model are correct.  We have lots of that in Bayesian Data Analysis (particularly in the first four chapters but implicitly elsewhere as well), and this is also covered in the classic books by Lindley, Jaynes, and others.  This sort of guarantee is indeed p</p><p>5 0.84071898 <a title="1529-lsi-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-07-Philosophy_of_Bayesian_statistics%3A_my_reactions_to_Hendry.html">1157 andrew gelman stats-2012-02-07-Philosophy of Bayesian statistics: my reactions to Hendry</a></p>
<p>Introduction: Continuing with my discussion  here  and  here  of the articles in the special issue of the journal Rationality, Markets and Morals on the philosophy of Bayesian statistics:
 
   
 
David Hendry, “Empirical Economic Model Discovery and Theory Evaluation”:
 
Hendry presents a wide-ranging overview of scientific learning, with an interesting comparison of physical with social sciences.  (For some reason, he discusses many physical sciences but restricts his social-science examples to economics and psychology.)
 
The only part of Hendry’s long and interesting article that I will discuss, however, is the part where he decides to take a gratuitous swing at Bayes.  I don’t know why he did this, but maybe it’s part of some fraternity initiation thing, like TP-ing the dean’s house on Halloween.
 
Here’s the story.  Hendry writes:
  
‘Prior distributions’ widely used in Bayesian analyses, whether subjective or ‘objective’, cannot be formed in such a setting either, absent a falsely assumed crys</p><p>6 0.83983791 <a title="1529-lsi-6" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-28-More_on_Bayesian_deduction-induction.html">114 andrew gelman stats-2010-06-28-More on Bayesian deduction-induction</a></p>
<p>7 0.83934033 <a title="1529-lsi-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-27-%E2%80%9CTwo_Dogmas_of_Strong_Objective_Bayesianism%E2%80%9D.html">1779 andrew gelman stats-2013-03-27-“Two Dogmas of Strong Objective Bayesianism”</a></p>
<p>8 0.82729363 <a title="1529-lsi-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-30-Useful_models%2C_model_checking%2C_and_external_validation%3A__a_mini-discussion.html">244 andrew gelman stats-2010-08-30-Useful models, model checking, and external validation:  a mini-discussion</a></p>
<p>9 0.82203132 <a title="1529-lsi-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-22-Philosophy_of_Bayes_and_non-Bayes%3A__A_dialogue_with_Deborah_Mayo.html">291 andrew gelman stats-2010-09-22-Philosophy of Bayes and non-Bayes:  A dialogue with Deborah Mayo</a></p>
<p>10 0.82086319 <a title="1529-lsi-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-20-Kind_of_Bayesian.html">811 andrew gelman stats-2011-07-20-Kind of Bayesian</a></p>
<p>11 0.8116287 <a title="1529-lsi-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-09-Coming_to_agreement_on_philosophy_of_statistics.html">1205 andrew gelman stats-2012-03-09-Coming to agreement on philosophy of statistics</a></p>
<p>12 0.80972511 <a title="1529-lsi-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-29-Ya_don%E2%80%99t_know_Bayes%2C_Jack.html">117 andrew gelman stats-2010-06-29-Ya don’t know Bayes, Jack</a></p>
<p>13 0.80812782 <a title="1529-lsi-13" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-18-Those_wacky_anti-Bayesians_used_to_be_intimidating%2C_but_now_they%E2%80%99re_just_pathetic.html">2254 andrew gelman stats-2014-03-18-Those wacky anti-Bayesians used to be intimidating, but now they’re just pathetic</a></p>
<p>14 0.80547553 <a title="1529-lsi-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-08-The_virtues_of_incoherence%3F.html">792 andrew gelman stats-2011-07-08-The virtues of incoherence?</a></p>
<p>15 0.7967658 <a title="1529-lsi-15" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-06-11-Bayes_in_the_research_conversation.html">2368 andrew gelman stats-2014-06-11-Bayes in the research conversation</a></p>
<p>16 0.792458 <a title="1529-lsi-16" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-22-Spell-checking_example_demonstrates_key_aspects_of_Bayesian_data_analysis.html">2182 andrew gelman stats-2014-01-22-Spell-checking example demonstrates key aspects of Bayesian data analysis</a></p>
<p>17 0.79105783 <a title="1529-lsi-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-11-Why_waste_time_philosophizing%3F.html">1719 andrew gelman stats-2013-02-11-Why waste time philosophizing?</a></p>
<p>18 0.79031909 <a title="1529-lsi-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-31-What_is_a_Bayesian%3F.html">1438 andrew gelman stats-2012-07-31-What is a Bayesian?</a></p>
<p>19 0.78855401 <a title="1529-lsi-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-11-Gelman_on_Hennig_on_Gelman_on_Bayes.html">1208 andrew gelman stats-2012-03-11-Gelman on Hennig on Gelman on Bayes</a></p>
<p>20 0.78701419 <a title="1529-lsi-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-25-Incoherence_of_Bayesian_data_analysis.html">1510 andrew gelman stats-2012-09-25-Incoherence of Bayesian data analysis</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.011), (15, 0.037), (16, 0.102), (21, 0.024), (24, 0.145), (44, 0.054), (55, 0.02), (63, 0.015), (65, 0.032), (86, 0.053), (87, 0.021), (95, 0.017), (96, 0.016), (99, 0.322)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98257136 <a title="1529-lda-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-11-Bayesian_brains%3F.html">1529 andrew gelman stats-2012-10-11-Bayesian brains?</a></p>
<p>Introduction: Psychology researcher Alison Gopnik  discusses  the idea that some of the systematic problems with human reasoning can be explained by systematic flaws in the statistical models we implicitly use.
 
I really like this idea and I’ll return to it in a bit.  But first I need to discuss a minor (but, I think, ultimately crucial) disagreement I have with how Gopnik describes Bayesian inference.  She writes:
  
The Bayesian idea is simple, but it turns out to be very powerful. It’s so powerful, in fact, that computer scientists are using it to design intelligent learning machines, and more and more psychologists think that it might explain human intelligence. Bayesian inference is a way to use statistical data to evaluate hypotheses and make predictions. These might be scientific hypotheses and predictions or everyday ones.
  
So far, so good.  Next comes the problem (as I see it).  Gopnik writes:
  
Here’s a simple bit of Bayesian election thinking. In early September, the polls suddenly im</p><p>2 0.98094159 <a title="1529-lda-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-13-What_are_the_important_issues_in_ethics_and_statistics%3F__I%E2%80%99m_looking_for_your_input%21.html">1117 andrew gelman stats-2012-01-13-What are the important issues in ethics and statistics?  I’m looking for your input!</a></p>
<p>Introduction: I’ve recently started a regular column on ethics, appearing every three months in  Chance magazine .  My first column, “Open Data and Open Methods,” is  here , and my second column, “Statisticians:  When we teach, we don’t practice what we preach” (coauthored with Eric Loken) will be appearing in the next issue.
 
Statistical ethics is a wide-open topic, and I’d be very interested in everyone’s thoughts, questions, and stories.  I’d like to get beyond generic questions such as, Is it right to do a randomized trial when you think the treatment is probably better than the control?, and I’d also like to avoid the really easy questions such as, Is it ethical to copy Wikipedia entries and then sell the resulting publication for  $2800  a year?  [Note to people who are sick of hearing about this particular story:  I'll consider stopping my blogging on it, the moment that the people involved consider apologizing for their behavior.]
 
Please insert your thoughts, questions, stories, links, et</p><p>3 0.97699392 <a title="1529-lda-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-06-Early_stopping_and_penalized_likelihood.html">788 andrew gelman stats-2011-07-06-Early stopping and penalized likelihood</a></p>
<p>Introduction: Maximum likelihood gives the beat fit to the training data but in general overfits, yielding overly-noisy parameter estimates that don’t perform so well when predicting new data.  A popular solution to this overfitting problem takes advantage of the iterative nature of most maximum likelihood algorithms by stopping early.  In general, an iterative optimization algorithm goes from a starting point to the maximum of some objective function.  If the starting point has some good properties, then early stopping can work well, keeping some of the virtues of the starting point while respecting the data.  
 
This trick can be performed the other way, too, starting with the data and then processing it to move it toward a model.  That’s how the iterative proportional fitting algorithm of Deming and Stephan (1940) works to fit multivariate categorical data to known margins.
 
In any case, the trick is to stop at the right point–not so soon that you’re ignoring the data but not so late that you en</p><p>4 0.97344911 <a title="1529-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-22-Blogging_is_%E2%80%9Cdestroying_the_business_model_for_quality%E2%80%9D%3F.html">865 andrew gelman stats-2011-08-22-Blogging is “destroying the business model for quality”?</a></p>
<p>Introduction: Journalist Jonathan Rauch  writes  that the internet is Sturgeon squared:
  
This is the blogosphere. I’m not getting paid to be here. I’m here to get incredibly famous (in my case, even more incredibly famous) so that I can get paid somewhere else. . . .


The average quality of newspapers and (published) novels is far, far better than the average quality of blog posts (and–ugh!–comments). This is because people pay for newspapers and novels. What distinguishes newspapers and novels is how much does not get published in them, because people won’t pay for it. Payment is a filter, and a pretty good one. Imperfect, of course. But pointing out the defects of the old model is merely changing the subject if the new model is worse. . . .


Yes, the new model is bringing a lot of new content into being. But most of it is bad. And it’s displacing a lot of better content, by destroying the business model for quality. Even in the information economy, there’s no free lunch. . . .


Yes, there’s g</p><p>5 0.97342312 <a title="1529-lda-5" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-07-My_recent_debugging_experience.html">2161 andrew gelman stats-2014-01-07-My recent debugging experience</a></p>
<p>Introduction: OK, so this sort of thing happens sometimes.  I was working on a new idea (still working on it; if it ultimately works out—or if it doesn’t—I’ll let you know) and as part of it I was fitting little models in Stan, in a loop.  I thought it would make sense to start with linear regression with normal priors and known data variance, because then the exact solution is Gaussian and I can also work with the problem analytically.  So I programmed up the algorithm and, no surprise, it didn’t work.  I went through my R code, put in print statements here and there, and cleared out bug after bug until at least it stopped crashing.  But the algorithm still wasn’t doing what it was supposed to do.
 
So I decided to do something simpler, and just check that the Stan linear regression gave the same answer as the analytic posterior distribution:  I ran Stan for tons of iterations, then computed the sample mean and variance of the simulations.  It was an example with two coefficients—I’d originally cho</p><p>6 0.97296089 <a title="1529-lda-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-17-Replication_backlash.html">2137 andrew gelman stats-2013-12-17-Replication backlash</a></p>
<p>7 0.97233415 <a title="1529-lda-7" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-19-%E2%80%9CThe_British_amateur_who_debunked_the_mathematics_of_happiness%E2%80%9D.html">2177 andrew gelman stats-2014-01-19-“The British amateur who debunked the mathematics of happiness”</a></p>
<p>8 0.97185659 <a title="1529-lda-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-14-Everyone%E2%80%99s_trading_bias_for_variance_at_some_point%2C_it%E2%80%99s_just_done_at_different_places_in_the_analyses.html">1763 andrew gelman stats-2013-03-14-Everyone’s trading bias for variance at some point, it’s just done at different places in the analyses</a></p>
<p>9 0.97175866 <a title="1529-lda-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-07-Contest_for_developing_an_R_package_recommendation_system.html">324 andrew gelman stats-2010-10-07-Contest for developing an R package recommendation system</a></p>
<p>10 0.97174752 <a title="1529-lda-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-22-Struggles_over_the_criticism_of_the_%E2%80%9Ccannabis_users_and_IQ_change%E2%80%9D_paper.html">1910 andrew gelman stats-2013-06-22-Struggles over the criticism of the “cannabis users and IQ change” paper</a></p>
<p>11 0.97120148 <a title="1529-lda-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-31-How_to_fix_the_tabloids%3F__Toward_replicable_social_science_research.html">1878 andrew gelman stats-2013-05-31-How to fix the tabloids?  Toward replicable social science research</a></p>
<p>12 0.97102422 <a title="1529-lda-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-17-%E2%80%9CWhy_Preschool_Shouldn%E2%80%99t_Be_Like_School%E2%80%9D%3F.html">617 andrew gelman stats-2011-03-17-“Why Preschool Shouldn’t Be Like School”?</a></p>
<p>13 0.97037494 <a title="1529-lda-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-30-Retracted_articles_and_unethical_behavior_in_economics_journals%3F.html">1435 andrew gelman stats-2012-07-30-Retracted articles and unethical behavior in economics journals?</a></p>
<p>14 0.97026545 <a title="1529-lda-14" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-27-%E2%80%9CWhat_Can_we_Learn_from_the_Many_Labs_Replication_Project%3F%E2%80%9D.html">2227 andrew gelman stats-2014-02-27-“What Can we Learn from the Many Labs Replication Project?”</a></p>
<p>15 0.97006989 <a title="1529-lda-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-16-Another_day%2C_another_plagiarist.html">1266 andrew gelman stats-2012-04-16-Another day, another plagiarist</a></p>
<p>16 0.96995246 <a title="1529-lda-16" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-03-24-Empirical_implications_of_Empirical_Implications_of_Theoretical_Models.html">2263 andrew gelman stats-2014-03-24-Empirical implications of Empirical Implications of Theoretical Models</a></p>
<p>17 0.9693464 <a title="1529-lda-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-20-Likelihood_thresholds_and_decisions.html">1422 andrew gelman stats-2012-07-20-Likelihood thresholds and decisions</a></p>
<p>18 0.96922523 <a title="1529-lda-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-26-Philosophy_and_the_practice_of_Bayesian_statistics.html">110 andrew gelman stats-2010-06-26-Philosophy and the practice of Bayesian statistics</a></p>
<p>19 0.96913671 <a title="1529-lda-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-12-Misunderstanding_the_p-value.html">1760 andrew gelman stats-2013-03-12-Misunderstanding the p-value</a></p>
<p>20 0.96878296 <a title="1529-lda-20" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-24-%E2%80%9CEdlin%E2%80%99s_rule%E2%80%9D_for_routinely_scaling_down_published_estimates.html">2223 andrew gelman stats-2014-02-24-“Edlin’s rule” for routinely scaling down published estimates</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
