<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1395 andrew gelman stats-2012-06-27-Cross-validation (What is it good for?)</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2012" href="../home/andrew_gelman_stats-2012_home.html">andrew_gelman_stats-2012</a> <a title="andrew_gelman_stats-2012-1395" href="#">andrew_gelman_stats-2012-1395</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1395 andrew gelman stats-2012-06-27-Cross-validation (What is it good for?)</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2012-1395-html" href="http://andrewgelman.com/2012/06/27/cross-validation-what-is-it-good-for/">html</a></p><p>Introduction: I think cross-validation is a good way to estimate a model’s forecasting error but I don’t think it’s always such a great tool for comparing models.  I mean, sure, if the differences are dramatic, ok.  But you can easily have a few candidate models, and one model makes a lot more sense than the others (even from a purely predictive sense, I’m not talking about causality here).  The difference between the model doesn’t show up in a xval measure of total error but in the patterns of the predictions.
 
For a simple example, imagine using a linear model with positive slope to model a function that is constrained to be increasing.  If the constraint isn’t in the model, the predicted/imputed series will sometimes be nonmonotonic.  The effect on the prediction error can be so tiny as to be undetectable (or it might even increase avg prediction error to include the constraint); nonetheless, the predictions will be clearly nonsensical.
 
That’s an extreme example but I think the general point h</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 I think cross-validation is a good way to estimate a model’s forecasting error but I don’t think it’s always such a great tool for comparing models. [sent-1, score-0.529]
</p><p>2 But you can easily have a few candidate models, and one model makes a lot more sense than the others (even from a purely predictive sense, I’m not talking about causality here). [sent-3, score-0.225]
</p><p>3 The difference between the model doesn’t show up in a xval measure of total error but in the patterns of the predictions. [sent-4, score-0.451]
</p><p>4 For a simple example, imagine using a linear model with positive slope to model a function that is constrained to be increasing. [sent-5, score-0.35]
</p><p>5 If the constraint isn’t in the model, the predicted/imputed series will sometimes be nonmonotonic. [sent-6, score-0.14]
</p><p>6 The effect on the prediction error can be so tiny as to be undetectable (or it might even increase avg prediction error to include the constraint); nonetheless, the predictions will be clearly nonsensical. [sent-7, score-0.589]
</p><p>7 Think of xval as a way of estimating predictive accuracy. [sent-9, score-0.345]
</p><p>8 If you want to compare models, I think you’ll want to look more carefully at the output. [sent-10, score-0.124]
</p><p>9 The above was my response to a question that Jay Ulfelder sent off to a few quantitative political scientists:    I’m at a methodological fork in the road in a project I’m doing for the U. [sent-11, score-0.181]
</p><p>10 I know you’re all very busy, but the stakes in this project are pretty high, so if there’s any way you can take a few minutes to reply, I’d really appreciate it. [sent-14, score-0.192]
</p><p>11 As noted in the blog post I linked to at the start of this email, the aim of the project is to develop a statistical model (or models) that could be used each year to assess the risk of an onset of mass killing in countries worldwide. [sent-15, score-0.517]
</p><p>12 Based on some preliminary analysis, I’ve decided that logistic regression will work fine, so I’ve got a modeling approach in hand. [sent-16, score-0.137]
</p><p>13 My goal now is to estimate and compare the forecasting power of some simple models, individually and as ensembles. [sent-17, score-0.622]
</p><p>14 Where I’m stuck now is choosing between assessing and comparing forecasting power across model specifications via cross-validation and dealing with non-trivial missing-data problems via multiple imputation. [sent-18, score-0.992]
</p><p>15 I’ve written a script that executes 10-fold CV as a way to compare the models’ forecasting power, but that script starts with a single version of the data set. [sent-19, score-1.0]
</p><p>16 I don’t see how I can simultaneously handle the missing-data problem other than just rerunning that CV script N times, where N is the number of imputations. [sent-20, score-0.223]
</p><p>17 Based on that preliminary research, I’m expecting that the models I specify will squeeze most of the forecasting juice out of the (quite limited) data we’ve got, and that averaging forecasts across a few different models will mitigate against the “all eggs in one basket” problem. [sent-23, score-1.243]
</p><p>18 And Gary King offered this useful practical suggestion:    Since you’re only doing forecasting and not estimating causal effects, you could perhaps treat the ‘missingness’ as one type of observation. [sent-24, score-0.399]
</p><p>19 it could be that missingness in some of the variables predicts mass killings. [sent-26, score-0.254]
</p><p>20 ,  this paper  shows that when military conflict starts, the first casualty is often the collection of vital registration records. [sent-29, score-0.234]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('forecasting', 0.317), ('cv', 0.262), ('script', 0.223), ('onsets', 0.198), ('models', 0.181), ('xval', 0.181), ('missingness', 0.153), ('model', 0.143), ('constraint', 0.14), ('preliminary', 0.137), ('error', 0.127), ('compare', 0.124), ('project', 0.116), ('starts', 0.113), ('power', 0.112), ('averages', 0.111), ('mass', 0.101), ('across', 0.096), ('casualty', 0.09), ('memorial', 0.09), ('unwieldy', 0.09), ('basket', 0.09), ('onset', 0.09), ('sets', 0.089), ('extreme', 0.088), ('prediction', 0.087), ('comparing', 0.085), ('mitigate', 0.085), ('ensembles', 0.085), ('noisiness', 0.085), ('squeeze', 0.085), ('undetectable', 0.085), ('estimating', 0.082), ('predictive', 0.082), ('juice', 0.082), ('ulfelder', 0.082), ('holocaust', 0.082), ('via', 0.08), ('eggs', 0.079), ('specifications', 0.079), ('stakes', 0.076), ('avg', 0.076), ('vital', 0.074), ('differences', 0.072), ('museum', 0.071), ('registration', 0.07), ('individually', 0.069), ('killing', 0.067), ('road', 0.065), ('slope', 0.064)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="1395-tfidf-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-27-Cross-validation_%28What_is_it_good_for%3F%29.html">1395 andrew gelman stats-2012-06-27-Cross-validation (What is it good for?)</a></p>
<p>Introduction: I think cross-validation is a good way to estimate a model’s forecasting error but I don’t think it’s always such a great tool for comparing models.  I mean, sure, if the differences are dramatic, ok.  But you can easily have a few candidate models, and one model makes a lot more sense than the others (even from a purely predictive sense, I’m not talking about causality here).  The difference between the model doesn’t show up in a xval measure of total error but in the patterns of the predictions.
 
For a simple example, imagine using a linear model with positive slope to model a function that is constrained to be increasing.  If the constraint isn’t in the model, the predicted/imputed series will sometimes be nonmonotonic.  The effect on the prediction error can be so tiny as to be undetectable (or it might even increase avg prediction error to include the constraint); nonetheless, the predictions will be clearly nonsensical.
 
That’s an extreme example but I think the general point h</p><p>2 0.22141974 <a title="1395-tfidf-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-02-%E2%80%9CBased_on_my_experiences%2C_I_think_you_could_make_general_progress_by_constructing_a_solution_to_your_specific_problem.%E2%80%9D.html">1441 andrew gelman stats-2012-08-02-“Based on my experiences, I think you could make general progress by constructing a solution to your specific problem.”</a></p>
<p>Introduction: David Radwin writes:
  
I am seeking a statistic measuring an estimate’s reliability or stability as an alternative to the coefficient of variation (CV), also known as the relative standard error. The CV is the standard error of an estimate (proportion, mean, regression coefficient, etc.) divided by the estimate itself, usually expressed as a percentage. For example, if a survey finds 15% unemployment with a 6% standard error, the CV is .06/.15 = .4 = 40%.


Some US government agencies flag or suppress as unreliable any estimate with a CV over a certain threshold such as 30% or 50%. But this standard can be arbitrary (for example, 85% employment would have a much lower CV of .06/.85 = 7%), and the CV has other drawbacks I won’t elaborate here. I don’t need an evaluation of the wisdom of using the CV or anything else for measuring an estimate’s stability, but one of my projects calls for such a measure and I would like to find a better alternative.


Can you or your blog readers suggest</p><p>3 0.18103066 <a title="1395-tfidf-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-30-Useful_models%2C_model_checking%2C_and_external_validation%3A__a_mini-discussion.html">244 andrew gelman stats-2010-08-30-Useful models, model checking, and external validation:  a mini-discussion</a></p>
<p>Introduction: I sent a copy of my paper (coauthored with Cosma Shalizi) on  Philosophy and the practice of Bayesian statistics in the social sciences  to  Richard Berk , who wrote:
  
I read your paper this morning. I think we are pretty much on the same page about all models being wrong. I like very much the way you handle this in the paper. Yes, Newton’s work is wrong, but surely useful. I also like your twist on Bayesian methods. Makes good sense to me. Perhaps most important, your paper raises some difficult issues I have been trying to think more carefully about.


1. If the goal of a model is to be useful, surely we need to explore that “useful” means. At the very least, usefulness will depend on use. So a model that is useful for forecasting may or may not be useful for causal inference.


2. Usefulness will be a matter of degree. So that for each use we will need one or more metrics to represent how useful the model is. In what looks at first to be simple example, if the use is forecasting,</p><p>4 0.13991871 <a title="1395-tfidf-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Difficulties_with_Bayesian_model_averaging.html">754 andrew gelman stats-2011-06-09-Difficulties with Bayesian model averaging</a></p>
<p>Introduction: In response to  this article  by Cosma Shalizi and myself on the philosophy of Bayesian statistics, David Hogg writes:
  
I [Hogg] agree–even in physics and astronomy–that the models are not “True” in the God-like sense of being absolute reality (that is, I am not a realist); and I  have argued  (a philosophically very naive 
paper, but hey, I was new to all this) that for pretty fundamental reasons we could never arrive at the True (with a capital “T”) model of the Universe.  The goal of inference is to find the “best” model, where “best” might have something to do with prediction, or explanation, or message length, or (horror!) our utility.  Needless to say, most of my physics friends *are* realists, even in the face of “effective theories” as Newtonian mechanics is an effective theory of GR and GR is an effective theory of “quantum gravity” (this plays to your point, because if you think any theory is possibly an effective theory, how could you ever find Truth?).  I also liked the i</p><p>5 0.13932924 <a title="1395-tfidf-5" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-07-When_you%E2%80%99re_planning_on_fitting_a_model%2C_build_up_to_it_by_fitting_simpler_models_first.__Then%2C_once_you_have_a_model_you_like%2C_check_the_hell_out_of_it.html">1972 andrew gelman stats-2013-08-07-When you’re planning on fitting a model, build up to it by fitting simpler models first.  Then, once you have a model you like, check the hell out of it</a></p>
<p>Introduction: In response to  my remarks  on his online book, Think Bayes, Allen Downey wrote: 
   
I [Downey] have a question about one of your comments: 
   My [Gelman's] main criticism with both books is that they talk a lot about inference but not so much about model building or model checking (recall the three steps of Bayesian data analysis). I think it’s ok for an introductory book to focus on inference, which of course is central to the data-analytic process—but I’d like them to at least mention that Bayesian ideas arise in model building and model checking as well. 

This sounds like something I agree with, and one of the things I tried to do in the book is to put modeling decisions front and center.  But the word “modeling” is used in lots of ways, so I want to see if we are talking about the same thing.


For example, in many chapters, I start with a simple model of the scenario, do some analysis, then check whether the model is good enough, and iterate.  Here’s the discussion of modeling</p><p>6 0.13315231 <a title="1395-tfidf-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-15-More_on_AIC%2C_WAIC%2C_etc.html">1983 andrew gelman stats-2013-08-15-More on AIC, WAIC, etc</a></p>
<p>7 0.13207585 <a title="1395-tfidf-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-26-Occam.html">1392 andrew gelman stats-2012-06-26-Occam</a></p>
<p>8 0.13021746 <a title="1395-tfidf-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-11-Adding_an_error_model_to_a_deterministic_model.html">1162 andrew gelman stats-2012-02-11-Adding an error model to a deterministic model</a></p>
<p>9 0.1278404 <a title="1395-tfidf-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-18-Should_we_always_be_using_the_t_and_robit_instead_of_the_normal_and_logit%3F.html">773 andrew gelman stats-2011-06-18-Should we always be using the t and robit instead of the normal and logit?</a></p>
<p>10 0.12464315 <a title="1395-tfidf-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-22-Handling_multiple_versions_of_an_outcome_variable.html">726 andrew gelman stats-2011-05-22-Handling multiple versions of an outcome variable</a></p>
<p>11 0.12396871 <a title="1395-tfidf-11" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-14-The_robust_beauty_of_improper_linear_models_in_decision_making.html">1981 andrew gelman stats-2013-08-14-The robust beauty of improper linear models in decision making</a></p>
<p>12 0.12097748 <a title="1395-tfidf-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-10-Cross-validation_and_Bayesian_estimation_of_tuning_parameters.html">2129 andrew gelman stats-2013-12-10-Cross-validation and Bayesian estimation of tuning parameters</a></p>
<p>13 0.11984801 <a title="1395-tfidf-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-03-Question_about_predictive_checks.html">1363 andrew gelman stats-2012-06-03-Question about predictive checks</a></p>
<p>14 0.11789308 <a title="1395-tfidf-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-25-Postdoc_Position_%232%3A__Hierarchical_Modeling_and_Statistical_Graphics.html">538 andrew gelman stats-2011-01-25-Postdoc Position #2:  Hierarchical Modeling and Statistical Graphics</a></p>
<p>15 0.11672378 <a title="1395-tfidf-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-05-Xiao-Li_Meng_and_Xianchao_Xie_rethink_asymptotics.html">1406 andrew gelman stats-2012-07-05-Xiao-Li Meng and Xianchao Xie rethink asymptotics</a></p>
<p>16 0.1121964 <a title="1395-tfidf-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-15-How_I_think_about_mixture_models.html">1459 andrew gelman stats-2012-08-15-How I think about mixture models</a></p>
<p>17 0.10946695 <a title="1395-tfidf-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-27-Bayesian_model_averaging_or_fitting_a_larger_model.html">1999 andrew gelman stats-2013-08-27-Bayesian model averaging or fitting a larger model</a></p>
<p>18 0.10835455 <a title="1395-tfidf-18" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-03-Some_thoughts_on_election_forecasting.html">391 andrew gelman stats-2010-11-03-Some thoughts on election forecasting</a></p>
<p>19 0.10785772 <a title="1395-tfidf-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-04-Model_checking_and_model_understanding_in_machine_learning.html">1482 andrew gelman stats-2012-09-04-Model checking and model understanding in machine learning</a></p>
<p>20 0.10749361 <a title="1395-tfidf-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-02-Blending_results_from_two_relatively_independent_multi-level_models.html">250 andrew gelman stats-2010-09-02-Blending results from two relatively independent multi-level models</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.238), (1, 0.116), (2, 0.063), (3, 0.001), (4, 0.065), (5, 0.024), (6, -0.034), (7, -0.049), (8, 0.075), (9, 0.069), (10, 0.039), (11, 0.042), (12, -0.017), (13, -0.035), (14, -0.09), (15, 0.012), (16, 0.018), (17, -0.023), (18, -0.007), (19, -0.003), (20, 0.002), (21, -0.015), (22, 0.011), (23, -0.014), (24, -0.002), (25, -0.005), (26, -0.001), (27, -0.009), (28, -0.018), (29, 0.007), (30, -0.04), (31, 0.008), (32, -0.028), (33, -0.024), (34, 0.033), (35, -0.024), (36, 0.024), (37, -0.047), (38, -0.001), (39, -0.021), (40, -0.016), (41, 0.015), (42, -0.038), (43, 0.0), (44, -0.034), (45, -0.022), (46, -0.014), (47, -0.013), (48, -0.014), (49, -0.031)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96664435 <a title="1395-lsi-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-27-Cross-validation_%28What_is_it_good_for%3F%29.html">1395 andrew gelman stats-2012-06-27-Cross-validation (What is it good for?)</a></p>
<p>Introduction: I think cross-validation is a good way to estimate a model’s forecasting error but I don’t think it’s always such a great tool for comparing models.  I mean, sure, if the differences are dramatic, ok.  But you can easily have a few candidate models, and one model makes a lot more sense than the others (even from a purely predictive sense, I’m not talking about causality here).  The difference between the model doesn’t show up in a xval measure of total error but in the patterns of the predictions.
 
For a simple example, imagine using a linear model with positive slope to model a function that is constrained to be increasing.  If the constraint isn’t in the model, the predicted/imputed series will sometimes be nonmonotonic.  The effect on the prediction error can be so tiny as to be undetectable (or it might even increase avg prediction error to include the constraint); nonetheless, the predictions will be clearly nonsensical.
 
That’s an extreme example but I think the general point h</p><p>2 0.86763638 <a title="1395-lsi-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-02-Blending_results_from_two_relatively_independent_multi-level_models.html">250 andrew gelman stats-2010-09-02-Blending results from two relatively independent multi-level models</a></p>
<p>Introduction: David Shor writes:
  
 
I [Shor] am working on a Bayesian Forecasting model for the Mid-term elections that has two components:


1) A poll aggregation system with pooled and hierarchical house and design effects across every race with polls (Average Standard error for house seat level vote-share ~.055)


2) A Bafumi-style regression that applies national-swing to individual seats. (Average Standard error for house seat level vote-share ~.06)


Since these two estimates are essentially independent, estimates can probably be made more accurate by pooling them together. But If a house effect changes in one draw, that changes estimates in every race. Changes in regression coefficients and National swing have a similar effect.  In the face of high and possibly differing seat-to-seat correlations from each method, I’m not sure what the correct way to “blend” these models would be, either for individual or top-line seat estimates.


In the mean-time, I’m just creating variance-weighted avera</p><p>3 0.84843427 <a title="1395-lsi-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-08-I_Am_Too_Absolutely_Heteroskedastic_for_This_Probit_Model.html">1047 andrew gelman stats-2011-12-08-I Am Too Absolutely Heteroskedastic for This Probit Model</a></p>
<p>Introduction: Soren Lorensen wrote:
  
I’m working on a project that uses a binary choice model on panel data. Since I have panel data and am using MLE, I’m concerned about heteroskedasticity making my estimates inconsistent and biased. 


Are you familiar with any statistical packages with pre-built tests for heteroskedasticity in binary choice ML models? If not, is there value in cutting my data into groups over which I guess the error variance might vary and eyeballing residual plots? Have you other suggestions about how I might resolve this concern?
  
I replied that I wouldn’t worry so much about heteroskedasticity.  Breaking up the data into pieces might make sense, but for the purpose of estimating how the coefficients might vary—that is, nonlinearity and interactions.
 
Soren shot back:
  
I’m somewhat puzzled however: homoskedasticity is an identifying assumption in estimating a probit model: if we don’t have it all sorts of bad things can happen to our parameter estimates. Do you suggest n</p><p>4 0.84468347 <a title="1395-lsi-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-24-Multilevel_modeling_and_instrumental_variables.html">1468 andrew gelman stats-2012-08-24-Multilevel modeling and instrumental variables</a></p>
<p>Introduction: Terence Teo writes:
  
I was wondering if multilevel models can be used as an alternative to 2SLS or IV models to deal with (i) endogeneity and (ii) selection problems.


More concretely, I am trying to assess the impact of investment treaties on foreign investment. Aside from the fact that foreign investment is correlated over time, it may be the case that countries that already receive sufficient amounts of foreign investment need not sign treaties, and countries that sign treaties are those that need foreign investment in the first place. Countries thus “select” into treatment; treaty signing is non-random. As such, I argue that to properly estimate the impact of treaties on investment, we must model the determinants of treaty signing.


I [Teo] am currently modeling this as two separate models: (1) regress predictors on likelihood of treaty signing, (2) regress treaty (with interactions, etc) on investment (I’ve thought of using propensity score matching for this part of the model)</p><p>5 0.83638436 <a title="1395-lsi-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-17-Graphical_tools_for_understanding_multilevel_models.html">772 andrew gelman stats-2011-06-17-Graphical tools for understanding multilevel models</a></p>
<p>Introduction: There are a few things I want to do:
 
1.  Understand a fitted model using tools such as  average predictive comparisons ,  R-squared, and partial pooling factors .  In defining these concepts, Iain and I came up with some clever tricks, including (but not limited to):
 
- Separating the inputs and averaging over all possible values of the input not being altered (for average predictive comparisons);
 
- Defining partial pooling  without  referring to a raw-data or maximum-likelihood or no-pooling estimate (these don’t necessarily exist when you’re fitting logistic regression with sparse data);
 
- Defining an R-squared for each level of a multilevel model.
 
The methods get pretty complicated, though, and they have some loose ends–in particular, for average predictive comparisons with continuous input variables.
 
So now we want to implement these in R and put them into arm along with bglmer etc.
 
2.  Setting up coefplot so it works more generally (that is, so the graphics look nice</p><p>6 0.81775683 <a title="1395-lsi-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-05-Xiao-Li_Meng_and_Xianchao_Xie_rethink_asymptotics.html">1406 andrew gelman stats-2012-07-05-Xiao-Li Meng and Xianchao Xie rethink asymptotics</a></p>
<p>7 0.81528354 <a title="1395-lsi-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-26-Occam.html">1392 andrew gelman stats-2012-06-26-Occam</a></p>
<p>8 0.81298643 <a title="1395-lsi-8" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-10-Combining_data_from_many_sources.html">948 andrew gelman stats-2011-10-10-Combining data from many sources</a></p>
<p>9 0.804138 <a title="1395-lsi-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-11-Convergence_Monitoring_for_Non-Identifiable_and_Non-Parametric_Models.html">1374 andrew gelman stats-2012-06-11-Convergence Monitoring for Non-Identifiable and Non-Parametric Models</a></p>
<p>10 0.80125123 <a title="1395-lsi-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-16-Mandelbrot_and_Akaike%3A__from_taxonomy_to_smooth_runways_%28pioneering_work_in_fractals_and_self-similarity%29.html">346 andrew gelman stats-2010-10-16-Mandelbrot and Akaike:  from taxonomy to smooth runways (pioneering work in fractals and self-similarity)</a></p>
<p>11 0.8006447 <a title="1395-lsi-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-26-Including_interactions_or_not.html">823 andrew gelman stats-2011-07-26-Including interactions or not</a></p>
<p>12 0.80064309 <a title="1395-lsi-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-14-The_robust_beauty_of_improper_linear_models_in_decision_making.html">1981 andrew gelman stats-2013-08-14-The robust beauty of improper linear models in decision making</a></p>
<p>13 0.80000734 <a title="1395-lsi-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-03-Model_Makers%E2%80%99_Hippocratic_Oath.html">552 andrew gelman stats-2011-02-03-Model Makers’ Hippocratic Oath</a></p>
<p>14 0.79920626 <a title="1395-lsi-14" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-26-An_AI_can_build_and_try_out_statistical_models_using_an_open-ended_generative_grammar.html">1739 andrew gelman stats-2013-02-26-An AI can build and try out statistical models using an open-ended generative grammar</a></p>
<p>15 0.79683995 <a title="1395-lsi-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-17-Clutering_and_variance_components.html">417 andrew gelman stats-2010-11-17-Clutering and variance components</a></p>
<p>16 0.79495519 <a title="1395-lsi-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-18-Should_we_always_be_using_the_t_and_robit_instead_of_the_normal_and_logit%3F.html">773 andrew gelman stats-2011-06-18-Should we always be using the t and robit instead of the normal and logit?</a></p>
<p>17 0.79460055 <a title="1395-lsi-17" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-28-Using_predator-prey_models_on_the_Canadian_lynx_series.html">1141 andrew gelman stats-2012-01-28-Using predator-prey models on the Canadian lynx series</a></p>
<p>18 0.79437953 <a title="1395-lsi-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-24-F-f-f-fake_data.html">1735 andrew gelman stats-2013-02-24-F-f-f-fake data</a></p>
<p>19 0.79328907 <a title="1395-lsi-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-15-More_on_AIC%2C_WAIC%2C_etc.html">1983 andrew gelman stats-2013-08-15-More on AIC, WAIC, etc</a></p>
<p>20 0.78832847 <a title="1395-lsi-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-17-Hierarchical-multilevel_modeling_with_%E2%80%9Cbig_data%E2%80%9D.html">1267 andrew gelman stats-2012-04-17-Hierarchical-multilevel modeling with “big data”</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.011), (5, 0.011), (12, 0.013), (13, 0.011), (15, 0.157), (16, 0.046), (21, 0.034), (24, 0.122), (53, 0.016), (86, 0.043), (90, 0.033), (95, 0.023), (99, 0.324)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.98723209 <a title="1395-lda-1" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-08-Gratuitous_use_of_%E2%80%9CBayesian_Statistics%2C%E2%80%9D_a_branding_issue%3F.html">133 andrew gelman stats-2010-07-08-Gratuitous use of “Bayesian Statistics,” a branding issue?</a></p>
<p>Introduction: I’m on an island in Maine for a few weeks (big shout out for North Haven!)  This morning I picked up a copy of “Working Waterfront,” a newspaper that focuses on issues of coastal fishing communities.  I came across  an article  about modeling “fish” populations — actually lobsters, I guess they’re considered “fish” for regulatory purposes.  When I read it, I thought “wow, this article is really well-written, not dumbed down like articles in most newspapers.” I think it’s great that a small coastal newspaper carries reporting like this. (The online version has a few things that I don’t recall in the print version, too, so it’s even better).  But in addition to being struck by finding such a good article in a small newspaper, I was struck by this:
  
According to [University of Maine scientist Yong] Chen, there are four main areas where his model improved on the prior version. “We included the inshore trawl data from Maine and other state surveys, in addition to federal survey data; we h</p><p>2 0.98398852 <a title="1395-lda-2" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-21-Interpreting_interactions_in_discrete-data_regression.html">1908 andrew gelman stats-2013-06-21-Interpreting interactions in discrete-data regression</a></p>
<p>Introduction: Mike Johns writes:
  
Are you familiar with the work of Ai and Norton on interactions in logit/probit models? I’d be curious to hear your thoughts.


Ai, C.R. and Norton E.C. 2003. Interaction terms in logit and probit models. Economics Letters 80(1): 123-129.


A peer ref just cited this paper in reaction to a logistic model we tested and claimed that the “only” way to test an interaction in logit/probit regression is to use the cross derivative method of Ai & Norton. I’ve never heard of this issue or method. It leaves me wondering what the interaction term actually tests (something Ai & Norton don’t discuss) and why such an important discovery is not more widely known. Is this an issue that is of particular relevance to econometric analysis because they approach interactions from the difference-in-difference perspective?


Full disclosure, I’m coming from a social science/epi background. Thus, i’m not interested in the d-in-d estimator; I want to know if any variables modify the rela</p><p>3 0.98055649 <a title="1395-lda-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-30-%E2%80%9CTragedy_of_the_science-communication_commons%E2%80%9D.html">1833 andrew gelman stats-2013-04-30-“Tragedy of the science-communication commons”</a></p>
<p>Introduction: I’ve earlier written that  science is science communication —that is, the act of communicating scientific ideas and findings to ourselves and others is itself a central part of science.  My point was to push against a conventional separation between the act of science and the act of communication, the idea that science is done by scientists and communication is done by communicators.  It’s a rare bit of science that does not include communication as part of it.  As a scientist and science communicator myself, I’m particularly sensitive to devaluing of communication.  (For example, Bayesian Data Analysis is full of original research that was done in order to communicate; or, to put it another way, we often think we understand a scientific idea, but once we try to communicate it, we recognize gaps in our understanding that motivate further research.)
 
I once saw the following on one of those inspirational-sayings-for-every-day desk calendars: “To have ideas is to gather flowers. To thin</p><p>4 0.97993797 <a title="1395-lda-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-19-Statistical_discrimination_again.html">1541 andrew gelman stats-2012-10-19-Statistical discrimination again</a></p>
<p>Introduction: Mark Johnstone writes:
  
I’ve recently been investigating a new European Court of Justice ruling on insurance calculations (on behalf of MoneySuperMarket) and I found something related to statistics that caught my attention. . . . The ruling (which comes into effect in December 2012) states that insurers in Europe can no longer provide different premiums based on gender.  Despite the fact that women are statistically safer drivers, unless it’s biologically proven there is a causal relationship between being female and being a safer driver, this is now seen as an act of discrimination (more on this from the Wall Street Journal).


However, where do you stop with this?  What about age?  What about other factors?  And what does this mean for the application of statistics in general?  Is it inherently unjust in this context?


One proposal has been to fit ‘black boxes’ into cars so more individual data can be collected, as opposed to relying heavily on aggregates.


For fans of data and s</p><p>5 0.979141 <a title="1395-lda-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-08-More_on_those_dudes_who_will_pay_your_professor_%248000_to_assign_a_book_to_your_class%2C_and_related_stories_about_small-time_sleazoids.html">329 andrew gelman stats-2010-10-08-More on those dudes who will pay your professor $8000 to assign a book to your class, and related stories about small-time sleazoids</a></p>
<p>Introduction: After noticing  these remarks  on expensive textbooks and  this comment  on the company that bribes professors to use their books, Preston McAfee pointed me to  this update  (complete with a picture of some guy who keeps threatening to sue him but never gets around to it).
 
The story McAfee tells is sad but also hilarious. Especially the part about “smuck.”  It all looks like one more symptom of the imploding market for books.  Prices for intro stat and econ books go up and up (even mediocre textbooks routinely cost $150), and the publishers put more and more effort into promotion.
 
McAfee adds:
  
I [McAfee] hope a publisher sues me about posting the articles I wrote.  Even a takedown notice would be fun.  I would be pretty happy to start posting about that, especially when some of them are charging $30 per article.


Ted Bergstrom and I used state Freedom of Information acts to extract the journal price deals at state university libraries.  We have about 35 of them so far.  Like te</p><p>6 0.97825789 <a title="1395-lda-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-09-My_talks_in_DC_and_Baltimore_this_week.html">1794 andrew gelman stats-2013-04-09-My talks in DC and Baltimore this week</a></p>
<p>7 0.97663963 <a title="1395-lda-7" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-06-W%E2%80%99man_%3C_W%E2%80%99pedia%2C_again.html">945 andrew gelman stats-2011-10-06-W’man < W’pedia, again</a></p>
<p>8 0.97461706 <a title="1395-lda-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-27-%E2%80%9CTwo_Dogmas_of_Strong_Objective_Bayesianism%E2%80%9D.html">1779 andrew gelman stats-2013-03-27-“Two Dogmas of Strong Objective Bayesianism”</a></p>
<p>9 0.97392792 <a title="1395-lda-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-14-Battle_of_the_Americans%3A__Writer_at_the_American_Enterprise_Institute_disparages_the_American_Political_Science_Association.html">274 andrew gelman stats-2010-09-14-Battle of the Americans:  Writer at the American Enterprise Institute disparages the American Political Science Association</a></p>
<p>10 0.97309214 <a title="1395-lda-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-13-How_should_journals_handle_replication_studies%3F.html">762 andrew gelman stats-2011-06-13-How should journals handle replication studies?</a></p>
<p>11 0.97069603 <a title="1395-lda-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-26-The_reverse-journal-submission_system.html">1393 andrew gelman stats-2012-06-26-The reverse-journal-submission system</a></p>
<p>12 0.97052133 <a title="1395-lda-12" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-25-A_new_Bem_theory.html">1998 andrew gelman stats-2013-08-25-A new Bem theory</a></p>
<p>13 0.96828103 <a title="1395-lda-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-15-New_prize_on_causality_in_statstistics_education.html">1624 andrew gelman stats-2012-12-15-New prize on causality in statstistics education</a></p>
<p>same-blog 14 0.96575105 <a title="1395-lda-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-27-Cross-validation_%28What_is_it_good_for%3F%29.html">1395 andrew gelman stats-2012-06-27-Cross-validation (What is it good for?)</a></p>
<p>15 0.96414036 <a title="1395-lda-15" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-01-I_owe_it_all_to_the_haters.html">834 andrew gelman stats-2011-08-01-I owe it all to the haters</a></p>
<p>16 0.96397126 <a title="1395-lda-16" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-29-%E2%80%9CQuestioning_The_Lancet%2C_PLOS%2C_And_Other_Surveys_On_Iraqi_Deaths%2C_An_Interview_With_Univ._of_London_Professor_Michael_Spagat%E2%80%9D.html">2191 andrew gelman stats-2014-01-29-“Questioning The Lancet, PLOS, And Other Surveys On Iraqi Deaths, An Interview With Univ. of London Professor Michael Spagat”</a></p>
<p>17 0.96262801 <a title="1395-lda-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-15-With_a_bit_of_precognition%2C_you%E2%80%99d_have_known_I_was_going_to_post_again_on_this_topic%2C_and_with_a_lot_of_precognition%2C_you%E2%80%99d_have_known_I_was_going_to_post_today.html">576 andrew gelman stats-2011-02-15-With a bit of precognition, you’d have known I was going to post again on this topic, and with a lot of precognition, you’d have known I was going to post today</a></p>
<p>18 0.96151322 <a title="1395-lda-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-20-What_happened_that_the_journal_Psychological_Science_published_a_paper_with_no_identifiable_strengths%3F.html">1865 andrew gelman stats-2013-05-20-What happened that the journal Psychological Science published a paper with no identifiable strengths?</a></p>
<p>19 0.95824552 <a title="1395-lda-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-12-The_importance_of_style_in_academic_writing.html">902 andrew gelman stats-2011-09-12-The importance of style in academic writing</a></p>
<p>20 0.95725596 <a title="1395-lda-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-24-Statistical_ethics_violation.html">1081 andrew gelman stats-2011-12-24-Statistical ethics violation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
