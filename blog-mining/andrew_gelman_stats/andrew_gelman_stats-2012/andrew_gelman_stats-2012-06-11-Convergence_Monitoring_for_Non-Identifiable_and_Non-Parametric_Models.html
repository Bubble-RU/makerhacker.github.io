<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1374 andrew gelman stats-2012-06-11-Convergence Monitoring for Non-Identifiable and Non-Parametric Models</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2012" href="../home/andrew_gelman_stats-2012_home.html">andrew_gelman_stats-2012</a> <a title="andrew_gelman_stats-2012-1374" href="#">andrew_gelman_stats-2012-1374</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1374 andrew gelman stats-2012-06-11-Convergence Monitoring for Non-Identifiable and Non-Parametric Models</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2012-1374-html" href="http://andrewgelman.com/2012/06/11/convergence-monitoring-for-non-identifiable-and-non-parametric-models/">html</a></p><p>Introduction: Becky Passonneau and colleagues at the Center for Computational Learning Systems (CCLS) at Columbia have been working on a project for ConEd (New York’s major electric utility) to  rank structures based on vulnerability to secondary events  (e.g., transformer explosions, cable meltdowns, electrical fires).  They’ve been using the R implementation  BayesTree  of Chipman, George and McCulloch’s  Bayesian Additive Regression Trees  (BART).
 
BART is a Bayesian non-parametric method that is non-identifiable in two ways.  Firstly, it is an additive tree model with a fixed number of trees, the indexes of which aren’t identified (you get the same predictions in a model swapping the order of the trees).  This is the same kind of non-identifiability you get with any mixture model (additive or interpolated) with an exchangeable prior on the mixture components.  Secondly, the trees themselves have varying structure over samples in terms of number of nodes and their topology (depth, branching, etc</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Becky Passonneau and colleagues at the Center for Computational Learning Systems (CCLS) at Columbia have been working on a project for ConEd (New York’s major electric utility) to  rank structures based on vulnerability to secondary events  (e. [sent-1, score-0.169]
</p><p>2 Firstly, it is an additive tree model with a fixed number of trees, the indexes of which aren’t identified (you get the same predictions in a model swapping the order of the trees). [sent-6, score-0.844]
</p><p>3 This is the same kind of non-identifiability you get with any mixture model (additive or interpolated) with an exchangeable prior on the mixture components. [sent-7, score-0.371]
</p><p>4 Secondly, the trees themselves have varying structure over samples in terms of number of nodes and their topology (depth, branching, etc. [sent-8, score-0.6]
</p><p>5 The only identified parameter in a BART model is the scale parameter of the centered, normal noise distribution. [sent-11, score-0.338]
</p><p>6 For models with unidentified parameters, how can we (a) monitor convergence, and (b) compute MCMC error in the predictions? [sent-12, score-0.206]
</p><p>7 The goal is to support decision making, so predictive accuracy is the most important factor for the models. [sent-13, score-0.378]
</p><p>8 They’re also interested in which of the many features they’re plugging in turn out to be important. [sent-14, score-0.18]
</p><p>9 The discussion of convergence in the Chipman et al. [sent-15, score-0.329]
</p><p>10 10, they say they iterate until they reach “satisfactory convergence”, but don’t say how they measure convergence. [sent-18, score-0.315]
</p><p>11 26, they say “to gauge MCMC convergence, we performed four independent repetitions of 250,000 MCMC iterations and obtained essentially the same results each time”, but they don’t say what a result is in this context. [sent-20, score-0.305]
</p><p>12 I’m assuming this is predictive accuracy of some sort, such as (root) mean square error or log loss. [sent-21, score-0.228]
</p><p>13 I suggested that they monitor the linear predictors,  , or the residuals  . [sent-22, score-0.372]
</p><p>14 Andrew suggested monitoring a “whole suite of predictive outcomes. [sent-23, score-0.301]
</p><p>15 Because the models are intended to be used predictively as classifiers to support decision making, we could also measure various kinds of loss (equivalently utility) of the predictions on held-out data. [sent-25, score-0.651]
</p><p>16 I don’t know how stable the use of features is, so it may be hard to monitor the kinds of things Chipman et al. [sent-26, score-0.491]
</p><p>17 call out in their paper (like the number of nodes in the trees that are based on a given feature). [sent-27, score-0.6]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('trees', 0.349), ('chipman', 0.308), ('bart', 0.253), ('convergence', 0.25), ('monitor', 0.206), ('additive', 0.201), ('nodes', 0.169), ('mcmc', 0.168), ('predictions', 0.136), ('predictive', 0.128), ('utility', 0.113), ('kinds', 0.11), ('mixture', 0.108), ('identified', 0.102), ('accuracy', 0.1), ('features', 0.096), ('suggested', 0.094), ('satisfactory', 0.094), ('swapping', 0.094), ('vulnerability', 0.094), ('iterate', 0.094), ('predictively', 0.094), ('mcculloch', 0.094), ('explosions', 0.088), ('fires', 0.088), ('interpolated', 0.088), ('repetitions', 0.084), ('classifiers', 0.084), ('firstly', 0.084), ('plugging', 0.084), ('number', 0.082), ('exchangeable', 0.081), ('indexes', 0.081), ('cable', 0.081), ('parameter', 0.081), ('decision', 0.08), ('et', 0.079), ('suite', 0.079), ('secondly', 0.079), ('measure', 0.077), ('gauge', 0.077), ('electric', 0.075), ('model', 0.074), ('equivalently', 0.074), ('electrical', 0.074), ('bayesian', 0.073), ('depth', 0.072), ('residuals', 0.072), ('say', 0.072), ('support', 0.07)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000002 <a title="1374-tfidf-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-11-Convergence_Monitoring_for_Non-Identifiable_and_Non-Parametric_Models.html">1374 andrew gelman stats-2012-06-11-Convergence Monitoring for Non-Identifiable and Non-Parametric Models</a></p>
<p>Introduction: Becky Passonneau and colleagues at the Center for Computational Learning Systems (CCLS) at Columbia have been working on a project for ConEd (New York’s major electric utility) to  rank structures based on vulnerability to secondary events  (e.g., transformer explosions, cable meltdowns, electrical fires).  They’ve been using the R implementation  BayesTree  of Chipman, George and McCulloch’s  Bayesian Additive Regression Trees  (BART).
 
BART is a Bayesian non-parametric method that is non-identifiable in two ways.  Firstly, it is an additive tree model with a fixed number of trees, the indexes of which aren’t identified (you get the same predictions in a model swapping the order of the trees).  This is the same kind of non-identifiability you get with any mixture model (additive or interpolated) with an exchangeable prior on the mixture components.  Secondly, the trees themselves have varying structure over samples in terms of number of nodes and their topology (depth, branching, etc</p><p>2 0.15699178 <a title="1374-tfidf-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-04-23-Modeling_heterogenous_treatment_effects.html">2 andrew gelman stats-2010-04-23-Modeling heterogenous treatment effects</a></p>
<p>Introduction: Don Green and Holger Kern write  on one of my  favorite topics , treatment interactions (see also  here ):
  
We [Green and Kern] present a methodology that largely automates the search for systematic treatment effect heterogeneity in large-scale experiments. We introduce a nonparametric estimator developed in statistical learning, Bayesian Additive Regression Trees (BART), to model treatment effects that vary as a function of covariates. BART has several advantages over commonly employed parametric modeling strategies, in particular its ability to automatically detect and model relevant treatment-covariate interactions in a flexible manner.


To increase the reliability and credibility of the resulting conditional treatment effect estimates, we suggest the use of a split sample design. The data are randomly divided into two equally-sized parts, with the first part used to explore treatment effect heterogeneity and the second part used to confirm the results. This approach permits a re</p><p>3 0.13925971 <a title="1374-tfidf-3" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-07-Some_ideas_on_communicating_risks_to_the_general_public.html">455 andrew gelman stats-2010-12-07-Some ideas on communicating risks to the general public</a></p>
<p>Introduction: Aleks points me to  this research summary  from Dan Goldstein.  Good stuff.  I’ve heard of a lot of this–I actually use some of it in my intro statistics course, when we show the students how they can express probability trees using frequencies–but it’s good to see it all in one place.</p><p>4 0.12529424 <a title="1374-tfidf-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-15-How_I_think_about_mixture_models.html">1459 andrew gelman stats-2012-08-15-How I think about mixture models</a></p>
<p>Introduction: Larry Wasserman  refers  to finite mixture models as “beasts” and  writes  jokes that they “should be avoided at all costs.”
 
I’ve thought a lot about mixture models, ever since using them in an  analysis  of voting patterns that was published in 1990.  First off, I’d like to say that our model was useful so I’d prefer not to pay the cost of avoiding it.  For a quick description of our mixture model and its context, see pp. 379-380 of my  article  in the Jim Berger volume).  Actually, our case was particularly difficult because we were not even fitting a mixture model to data, we were fitting it to latent data and using the model to perform partial pooling.  My difficulties in trying to fit this model inspired our discussion of mixture models in Bayesian Data Analysis (page 109 in the second edition, in the section on “Counterexamples to the theorems”).
 
I agree with Larry that if you’re fitting a mixture model, it’s good to be aware of the problems that arise if you try to estimate</p><p>5 0.11715089 <a title="1374-tfidf-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-04-That_half-Cauchy_prior.html">184 andrew gelman stats-2010-08-04-That half-Cauchy prior</a></p>
<p>Introduction: Xiaoyu Qian writes:
  
 
I have a question when I apply the half-Cauchy prior (Gelman, 2006) for the variance parameter in a hierarchical model. The model I used is a three level IRT model equivalent to a Rasch model. The variance parameter I try to estimate is at the third level. The group size ranges from 15 to 44. The data is TIMSS 2007 data.


I used the syntax provided by the paper and found that the convergence of the standard deviation term is good (sigma.theta), however, the convergence for the parameter “xi” is not very good. Does it mean the whole model has not converged? Do you have any suggestion for this situation.


I also used the uniform prior and correlate the result with the half-Cauchy result for the standard deviation term. The results correlated .99.
 

 
My reply:  It’s not a problem if xi does not converge well.  It’s |xi|*sigma that is relevant.  And, if the number of groups is large, the prior probably won’t matter so much, which would explain your 99% correlat</p><p>6 0.11569495 <a title="1374-tfidf-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-28-Understanding_simulations_in_terms_of_predictive_inference%3F.html">1287 andrew gelman stats-2012-04-28-Understanding simulations in terms of predictive inference?</a></p>
<p>7 0.11353631 <a title="1374-tfidf-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-03-Question_about_predictive_checks.html">1363 andrew gelman stats-2012-06-03-Question about predictive checks</a></p>
<p>8 0.10845392 <a title="1374-tfidf-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-16-%E2%80%9CReal_data_can_be_a_pain%E2%80%9D.html">1460 andrew gelman stats-2012-08-16-“Real data can be a pain”</a></p>
<p>9 0.10591707 <a title="1374-tfidf-9" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-24-F-f-f-fake_data.html">1735 andrew gelman stats-2013-02-24-F-f-f-fake data</a></p>
<p>10 0.098490648 <a title="1374-tfidf-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-29-Data_mining_and_allergies.html">685 andrew gelman stats-2011-04-29-Data mining and allergies</a></p>
<p>11 0.098445088 <a title="1374-tfidf-11" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-29-Ken_Rice_presents_a_unifying_approach_to_statistical_inference_and_hypothesis_testing.html">2312 andrew gelman stats-2014-04-29-Ken Rice presents a unifying approach to statistical inference and hypothesis testing</a></p>
<p>12 0.097455695 <a title="1374-tfidf-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-25-Incoherence_of_Bayesian_data_analysis.html">1510 andrew gelman stats-2012-09-25-Incoherence of Bayesian data analysis</a></p>
<p>13 0.095388971 <a title="1374-tfidf-13" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-10-Cross-validation_and_Bayesian_estimation_of_tuning_parameters.html">2129 andrew gelman stats-2013-12-10-Cross-validation and Bayesian estimation of tuning parameters</a></p>
<p>14 0.092482239 <a title="1374-tfidf-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-09-The_pretty_picture_is_just_the_beginning_of_the_data_exploration.__But_the_pretty_picture_is_a_great_way_to_get_started.__Another_example_of_how_a_puzzle_can_make_a_graph_appealing.html">1614 andrew gelman stats-2012-12-09-The pretty picture is just the beginning of the data exploration.  But the pretty picture is a great way to get started.  Another example of how a puzzle can make a graph appealing</a></p>
<p>15 0.089106619 <a title="1374-tfidf-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-06-Some_economists_are_skeptical_about_microfoundations.html">1200 andrew gelman stats-2012-03-06-Some economists are skeptical about microfoundations</a></p>
<p>16 0.088613056 <a title="1374-tfidf-16" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-04-Bayesian_Learning_via_Stochastic_Gradient_Langevin_Dynamics.html">1443 andrew gelman stats-2012-08-04-Bayesian Learning via Stochastic Gradient Langevin Dynamics</a></p>
<p>17 0.087185442 <a title="1374-tfidf-17" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-02-12-How_to_think_about_%E2%80%9Cidentifiability%E2%80%9D_in_Bayesian_inference%3F.html">2208 andrew gelman stats-2014-02-12-How to think about “identifiability” in Bayesian inference?</a></p>
<p>18 0.084559627 <a title="1374-tfidf-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-07-When_you%E2%80%99re_planning_on_fitting_a_model%2C_build_up_to_it_by_fitting_simpler_models_first.__Then%2C_once_you_have_a_model_you_like%2C_check_the_hell_out_of_it.html">1972 andrew gelman stats-2013-08-07-When you’re planning on fitting a model, build up to it by fitting simpler models first.  Then, once you have a model you like, check the hell out of it</a></p>
<p>19 0.084309429 <a title="1374-tfidf-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-02-Interaction-based_feature_selection_and_classification_for_high-dimensional_biological_data.html">1703 andrew gelman stats-2013-02-02-Interaction-based feature selection and classification for high-dimensional biological data</a></p>
<p>20 0.083752327 <a title="1374-tfidf-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-24-New_ideas_on_DIC_from_Martyn_Plummer_and_Sumio_Watanabe.html">778 andrew gelman stats-2011-06-24-New ideas on DIC from Martyn Plummer and Sumio Watanabe</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.151), (1, 0.121), (2, 0.018), (3, 0.022), (4, 0.012), (5, 0.022), (6, 0.011), (7, -0.036), (8, -0.013), (9, 0.021), (10, 0.016), (11, -0.003), (12, -0.026), (13, -0.002), (14, -0.036), (15, 0.008), (16, 0.038), (17, 0.002), (18, -0.024), (19, -0.011), (20, 0.008), (21, 0.014), (22, 0.007), (23, -0.018), (24, 0.009), (25, -0.009), (26, -0.04), (27, 0.026), (28, 0.038), (29, -0.005), (30, -0.023), (31, 0.015), (32, 0.015), (33, -0.03), (34, 0.013), (35, -0.055), (36, -0.004), (37, -0.025), (38, -0.002), (39, -0.028), (40, -0.026), (41, 0.023), (42, -0.023), (43, 0.033), (44, -0.012), (45, -0.005), (46, 0.026), (47, 0.009), (48, 0.054), (49, -0.019)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96085411 <a title="1374-lsi-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-11-Convergence_Monitoring_for_Non-Identifiable_and_Non-Parametric_Models.html">1374 andrew gelman stats-2012-06-11-Convergence Monitoring for Non-Identifiable and Non-Parametric Models</a></p>
<p>Introduction: Becky Passonneau and colleagues at the Center for Computational Learning Systems (CCLS) at Columbia have been working on a project for ConEd (New York’s major electric utility) to  rank structures based on vulnerability to secondary events  (e.g., transformer explosions, cable meltdowns, electrical fires).  They’ve been using the R implementation  BayesTree  of Chipman, George and McCulloch’s  Bayesian Additive Regression Trees  (BART).
 
BART is a Bayesian non-parametric method that is non-identifiable in two ways.  Firstly, it is an additive tree model with a fixed number of trees, the indexes of which aren’t identified (you get the same predictions in a model swapping the order of the trees).  This is the same kind of non-identifiability you get with any mixture model (additive or interpolated) with an exchangeable prior on the mixture components.  Secondly, the trees themselves have varying structure over samples in terms of number of nodes and their topology (depth, branching, etc</p><p>2 0.82345861 <a title="1374-lsi-2" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-24-New_ideas_on_DIC_from_Martyn_Plummer_and_Sumio_Watanabe.html">778 andrew gelman stats-2011-06-24-New ideas on DIC from Martyn Plummer and Sumio Watanabe</a></p>
<p>Introduction: Martyn Plummer  replied  to my recent  blog  on DIC with information that was important enough that I thought it deserved its own blog entry.  Martyn wrote:
  
DIC has been around for 10 years now and despite being immensely popular with applied statisticians it has generated very little theoretical interest. In fact, the silence has been deafening. I [Martyn] hope my paper added some clarity.


As you say, DIC is (an approximation to) a theoretical out-of-sample predictive error. When I finished the paper I was a little embarrassed to see that I had almost perfectly reconstructed the justification of AIC as approximate cross-validation measure by Stone (1977), with a Bayesian spin of course.


But even this insight leaves a lot of choices open. You need to choose the right loss function and also which level of the model you want to replicate from. David Spiegelhalter and colleagues called this the “focus”. In practice the focus is limited to the lowest level of the model. You generall</p><p>3 0.77838928 <a title="1374-lsi-3" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-29-Bayesian_Uncertainty_Quantification_for_Differential_Equations%21.html">2311 andrew gelman stats-2014-04-29-Bayesian Uncertainty Quantification for Differential Equations!</a></p>
<p>Introduction: Mark Girolami points us to  this paper and software  (with Oksana Chkrebtii, David Campbell, and Ben Calderhead).  They write:
  
We develop a general methodology for the probabilistic integration of differential equations via model based updating of a joint prior measure on the space of functions and their temporal and spatial derivatives. This results in a posterior measure over functions reflecting how well they satisfy the system of differential equations and corresponding initial and boundary values. We show how this posterior measure can be naturally incorporated within the Kennedy and O’Hagan framework for uncertainty quantification and provides a fully Bayesian approach to model calibration. . . . A broad variety of examples are provided to illustrate the potential of this framework for characterising discretization uncertainty, including initial value, delay, and boundary value differential equations, as well as partial differential equations. We also demonstrate our methodolo</p><p>4 0.76921588 <a title="1374-lsi-4" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-15-More_on_AIC%2C_WAIC%2C_etc.html">1983 andrew gelman stats-2013-08-15-More on AIC, WAIC, etc</a></p>
<p>Introduction: Following up on our  discussion  from the other day, Angelika van der Linde sends along  this paper  from 2012 (link to journal  here ).
 
And Aki pulls out this great quote from Geisser and Eddy (1979):
  
This discussion makes clear that in the nested case this method, as Akaike’s, is not consistent; i.e., even if $M_k$ is true, it will be rejected with probability $\alpha$ as $N\to\infty$. This point is also made by Schwarz (1978).  However, from the point of view of prediction, this is of no great consequence. For large numbers of observations, a prediction based on the falsely assumed $M_k$, will not differ appreciably from one based on the true $M_k$.  For example, if we assert that two normal populations have different means when in fact they have the same mean, then the use of the group mean as opposed to the grand mean for predicting a future observation results in predictors which are asymptotically equivalent and whose predictive variances are $\sigma^2[1 + (1/2n)]$ and $\si</p><p>5 0.76190203 <a title="1374-lsi-5" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-22-Deviance%2C_DIC%2C_AIC%2C_cross-validation%2C_etc.html">776 andrew gelman stats-2011-06-22-Deviance, DIC, AIC, cross-validation, etc</a></p>
<p>Introduction: The deviance information criterion (or DIC) is an idea of Brad Carlin and others for comparing the fits of models estimated using Bayesian simulation (for more information, see  this article  by Angelika van der Linde).
 
I don’t really ever know what to make of DIC.  On one hand, it seems sensible, it handles uncertainty in inferences within each model, and it does not depend on aspects of the models that don’t affect inferences within each model (unlike Bayes factors; see discussion  here ).  On the other hand, I don’t really have any idea what I would do with DIC in any real example.  In our book we included an example of DIC–people use it and we don’t have any great alternatives–but I had to be pretty careful that the example made sense.  Unlike the usual setting where we use a method and that gives us insight into a problem, here we used our insight into the problem to make sure that in this particular case the method gave a reasonable answer.
 
One of my practical problems with D</p><p>6 0.76130795 <a title="1374-lsi-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-09-The_first_version_of_my_%E2%80%9Cinference_from_iterative_simulation_using_parallel_sequences%E2%80%9D_paper%21.html">1309 andrew gelman stats-2012-05-09-The first version of my “inference from iterative simulation using parallel sequences” paper!</a></p>
<p>7 0.75655597 <a title="1374-lsi-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-26-An_AI_can_build_and_try_out_statistical_models_using_an_open-ended_generative_grammar.html">1739 andrew gelman stats-2013-02-26-An AI can build and try out statistical models using an open-ended generative grammar</a></p>
<p>8 0.7526812 <a title="1374-lsi-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-28-Understanding_simulations_in_terms_of_predictive_inference%3F.html">1287 andrew gelman stats-2012-04-28-Understanding simulations in terms of predictive inference?</a></p>
<p>9 0.75252825 <a title="1374-lsi-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-29-Hamiltonian_Monte_Carlo_stories.html">931 andrew gelman stats-2011-09-29-Hamiltonian Monte Carlo stories</a></p>
<p>10 0.74841172 <a title="1374-lsi-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-20-Adding_more_information_can_make_the_variance_go_up_%28depending_on_your_model%29.html">810 andrew gelman stats-2011-07-20-Adding more information can make the variance go up (depending on your model)</a></p>
<p>11 0.73677617 <a title="1374-lsi-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-08-I_Am_Too_Absolutely_Heteroskedastic_for_This_Probit_Model.html">1047 andrew gelman stats-2011-12-08-I Am Too Absolutely Heteroskedastic for This Probit Model</a></p>
<p>12 0.73424661 <a title="1374-lsi-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-03-Question_about_predictive_checks.html">1363 andrew gelman stats-2012-06-03-Question about predictive checks</a></p>
<p>13 0.73420203 <a title="1374-lsi-13" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-10-19-An_interweaving-transformation_strategy_for_boosting_MCMC_efficiency.html">964 andrew gelman stats-2011-10-19-An interweaving-transformation strategy for boosting MCMC efficiency</a></p>
<p>14 0.7323823 <a title="1374-lsi-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-05-Monitor_the_efficiency_of_your_Markov_chain_sampler_using_expected_squared_jumped_distance%21.html">650 andrew gelman stats-2011-04-05-Monitor the efficiency of your Markov chain sampler using expected squared jumped distance!</a></p>
<p>15 0.72808754 <a title="1374-lsi-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-14-The_robust_beauty_of_improper_linear_models_in_decision_making.html">1981 andrew gelman stats-2013-08-14-The robust beauty of improper linear models in decision making</a></p>
<p>16 0.72750413 <a title="1374-lsi-16" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-31-Somewhat_Bayesian_multilevel_modeling.html">246 andrew gelman stats-2010-08-31-Somewhat Bayesian multilevel modeling</a></p>
<p>17 0.72686785 <a title="1374-lsi-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-10-Cross-validation_and_Bayesian_estimation_of_tuning_parameters.html">2129 andrew gelman stats-2013-12-10-Cross-validation and Bayesian estimation of tuning parameters</a></p>
<p>18 0.72448224 <a title="1374-lsi-18" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-15-How_I_think_about_mixture_models.html">1459 andrew gelman stats-2012-08-15-How I think about mixture models</a></p>
<p>19 0.72327346 <a title="1374-lsi-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-04-David_MacKay_and_Occam%E2%80%99s_Razor.html">1041 andrew gelman stats-2011-12-04-David MacKay and Occam’s Razor</a></p>
<p>20 0.71734917 <a title="1374-lsi-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-26-Occam.html">1392 andrew gelman stats-2012-06-26-Occam</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.029), (9, 0.023), (15, 0.029), (16, 0.093), (21, 0.058), (24, 0.131), (57, 0.022), (84, 0.025), (85, 0.157), (86, 0.036), (88, 0.023), (94, 0.012), (95, 0.013), (99, 0.223)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.93188488 <a title="1374-lda-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-06-11-Convergence_Monitoring_for_Non-Identifiable_and_Non-Parametric_Models.html">1374 andrew gelman stats-2012-06-11-Convergence Monitoring for Non-Identifiable and Non-Parametric Models</a></p>
<p>Introduction: Becky Passonneau and colleagues at the Center for Computational Learning Systems (CCLS) at Columbia have been working on a project for ConEd (New York’s major electric utility) to  rank structures based on vulnerability to secondary events  (e.g., transformer explosions, cable meltdowns, electrical fires).  They’ve been using the R implementation  BayesTree  of Chipman, George and McCulloch’s  Bayesian Additive Regression Trees  (BART).
 
BART is a Bayesian non-parametric method that is non-identifiable in two ways.  Firstly, it is an additive tree model with a fixed number of trees, the indexes of which aren’t identified (you get the same predictions in a model swapping the order of the trees).  This is the same kind of non-identifiability you get with any mixture model (additive or interpolated) with an exchangeable prior on the mixture components.  Secondly, the trees themselves have varying structure over samples in terms of number of nodes and their topology (depth, branching, etc</p><p>2 0.92551827 <a title="1374-lda-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-10-15-The_strange_reappearance_of_Matthew_Klam.html">1534 andrew gelman stats-2012-10-15-The strange reappearance of Matthew Klam</a></p>
<p>Introduction: A few years ago I  asked  what happened to Matthew Klam, a talented writer who has a bizarrely professional-looking webpage but didn’t seem to be writing anymore.
 
Good news!  He published  a new story  in the New Yorker!  Confusingly, he wrote it under the name “Justin Taylor,” but I’m not fooled (any more than I was fooled when that posthumous Updike story was published under the name “ Antonya Nelson “).  I’m glad to see that Klam is back in action and look forward to seeing some stories under his own name as well.</p><p>3 0.92471039 <a title="1374-lda-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-06-Calling_Jenny_Davidson_._._..html">1790 andrew gelman stats-2013-04-06-Calling Jenny Davidson . . .</a></p>
<p>Introduction: Now that you have some free time again, you’ll have to check out  these books  and tell us if they’re worth reading.
 
Claire Kirch  reports :
  
Lizzie Skurnick Books launches in September with the release of Debutante Hill by Lois Duncan. The novel, which was originally published by Dodd, Mead, in 1958, has been out of print for about three decades.


The other books on the initial list, all reissues, are A Long Day in November by Ernest J. Gaines (originally published in 1971), Happy Endings Are All Alike by Sandra Scoppettone (1979), I’ll Love You When You’re More Like Me by M.E. Kerr (1977), Secret Lives by Berthe Amoss (1979), To All My Fans, With Love, From Sylvie by Ellen Conford (1982), and Me and Fat Glenda by Lila Perl (1972). . . .


Noting that many of the books of that era beloved by teen boys are still in print – such as Isaac Asimov’s novels and The Chocolate War by Robert Cormier – Skurnick pointed out that, in contrast, many of the books that were embraced by teen gir</p><p>4 0.91586614 <a title="1374-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-28-Funniest_comment_ever.html">734 andrew gelman stats-2011-05-28-Funniest comment ever</a></p>
<p>Introduction: Here  (scroll down to the bottom; for some reason the link doesn’t go directly to the comment itself).  I’ve never actually seen a Kaypro but I remember the ads.
 
(Background  here .)</p><p>5 0.91367012 <a title="1374-lda-5" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-21-Ticket_to_Baaaath.html">2300 andrew gelman stats-2014-04-21-Ticket to Baaaath</a></p>
<p>Introduction: Ooooooh, I never ever thought I’d have a legitimate excuse to tell this story, and now I do!  The story took place many years ago, but first I have to tell you what made me think of it:
 
Rasmus Bååth  posted  the following comment last month:
  
On airplane tickets a Swedish “å” is written as “aa” resulting in Rasmus Baaaath. Once I bought a ticket online and five minutes later a guy from Lufthansa calls me and asks if I misspelled my name…
  
OK, now here’s my story (which is not nearly as good).  A long time ago (but when I was already an adult), I was in England for some reason, and I thought I’d take a day trip from London to Bath.  So here I am on line, trying to think of what to say at the ticket counter.  I remember that in England, they call Bath, Bahth.  So, should I ask for “a ticket to Bahth”?  I’m not sure, I’m afraid that it will sound silly, like I’m trying to fake an English accent.  So, when I get to the front of the line, I say, hesitantly, “I’d like a ticket to Bath?</p><p>6 0.9077369 <a title="1374-lda-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-15-n_%3D_2.html">912 andrew gelman stats-2011-09-15-n = 2</a></p>
<p>7 0.90382272 <a title="1374-lda-7" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-29-Stupid_legal_crap.html">58 andrew gelman stats-2010-05-29-Stupid legal crap</a></p>
<p>8 0.90021044 <a title="1374-lda-8" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-09-The_pretty_picture_is_just_the_beginning_of_the_data_exploration.__But_the_pretty_picture_is_a_great_way_to_get_started.__Another_example_of_how_a_puzzle_can_make_a_graph_appealing.html">1614 andrew gelman stats-2012-12-09-The pretty picture is just the beginning of the data exploration.  But the pretty picture is a great way to get started.  Another example of how a puzzle can make a graph appealing</a></p>
<p>9 0.89247644 <a title="1374-lda-9" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-28-Matching_for_preprocessing_data_for_causal_inference.html">375 andrew gelman stats-2010-10-28-Matching for preprocessing data for causal inference</a></p>
<p>10 0.89081103 <a title="1374-lda-10" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-11-17-Clutering_and_variance_components.html">417 andrew gelman stats-2010-11-17-Clutering and variance components</a></p>
<p>11 0.88911331 <a title="1374-lda-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-08-07-Non-rant.html">843 andrew gelman stats-2011-08-07-Non-rant</a></p>
<p>12 0.88572073 <a title="1374-lda-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-27-%E2%80%9CApple_confronts_the_law_of_large_numbers%E2%80%9D_._._._huh%3F.html">1187 andrew gelman stats-2012-02-27-“Apple confronts the law of large numbers” . . . huh?</a></p>
<p>13 0.88025665 <a title="1374-lda-13" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-04-The_Notorious_N.H.S.T._presents%3A__Mo_P-values_Mo_Problems.html">2281 andrew gelman stats-2014-04-04-The Notorious N.H.S.T. presents:  Mo P-values Mo Problems</a></p>
<p>14 0.87243873 <a title="1374-lda-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-22-%E2%80%9CAre_Wisconsin_Public_Employees_Underpaid%3F%E2%80%9D.html">584 andrew gelman stats-2011-02-22-“Are Wisconsin Public Employees Underpaid?”</a></p>
<p>15 0.87171155 <a title="1374-lda-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-13-Stolen_jokes.html">1318 andrew gelman stats-2012-05-13-Stolen jokes</a></p>
<p>16 0.87105286 <a title="1374-lda-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-26-Teaching_evaluations%2C_instructor_effectiveness%2C_the_Journal_of_Political_Economy%2C_and_the_Holy_Roman_Empire.html">540 andrew gelman stats-2011-01-26-Teaching evaluations, instructor effectiveness, the Journal of Political Economy, and the Holy Roman Empire</a></p>
<p>17 0.8681975 <a title="1374-lda-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-03-13-Secret_weapon_with_rare_events.html">610 andrew gelman stats-2011-03-13-Secret weapon with rare events</a></p>
<p>18 0.86492974 <a title="1374-lda-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-23-The_scalarization_of_America.html">533 andrew gelman stats-2011-01-23-The scalarization of America</a></p>
<p>19 0.8617413 <a title="1374-lda-19" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-07-27-Why_don%E2%80%99t_more_medical_discoveries_become_cures%3F.html">167 andrew gelman stats-2010-07-27-Why don’t more medical discoveries become cures?</a></p>
<p>20 0.8591342 <a title="1374-lda-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-02-19-Factual_%E2%80%93_a_new_place_to_find_data.html">1175 andrew gelman stats-2012-02-19-Factual – a new place to find data</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
