<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1121 andrew gelman stats-2012-01-15-R-squared for multilevel models</title>
</head>

<body>
<p><a title="andrew_gelman_stats" href="../andrew_gelman_stats_home.html">andrew_gelman_stats</a> <a title="andrew_gelman_stats-2012" href="../home/andrew_gelman_stats-2012_home.html">andrew_gelman_stats-2012</a> <a title="andrew_gelman_stats-2012-1121" href="#">andrew_gelman_stats-2012-1121</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1121 andrew gelman stats-2012-01-15-R-squared for multilevel models</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="andrew_gelman_stats-2012-1121-html" href="http://andrewgelman.com/2012/01/15/r-squared-for-multilevel-models/">html</a></p><p>Introduction: Fred Schiff writes: 
  
  
I’m writing to you to ask about the “R-squared” approximation procedure you suggest in your 2004 book with Dr. Hill.  [See also  this paper  with Pardoe---ed.]


I’m a media sociologist at the University of Houston.  I’ve been using HLM3 for about two years.  


Briefly about my data.  It’s a content analysis of news stories with a continuous scale dependent variable, story prominence.  I have 6090 news stories, 114 newspapers, and 59 newspaper group owners.  All the Level-1, Level-2 and dependent variables have been standardized. Since the means were zero anyway, we left the variables uncentered.  All the Level-3 ownership groups and characteristics are dichotomous scales that were left uncentered.  


PROBLEM:  The single most important result I am looking for is to compare the strength of nine competing Level-1 variables in their ability to predict and explain the outcome variable, story prominence.  We are trying to use the residuals to calculate a “R-squ</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Fred Schiff writes:        I’m writing to you to ask about the “R-squared” approximation procedure you suggest in your 2004 book with Dr. [sent-1, score-0.057]
</p><p>2 It’s a content analysis of news stories with a continuous scale dependent variable, story prominence. [sent-7, score-0.336]
</p><p>3 I have 6090 news stories, 114 newspapers, and 59 newspaper group owners. [sent-8, score-0.32]
</p><p>4 All the Level-1, Level-2 and dependent variables have been standardized. [sent-9, score-0.375]
</p><p>5 Since the means were zero anyway, we left the variables uncentered. [sent-10, score-0.341]
</p><p>6 All the Level-3 ownership groups and characteristics are dichotomous scales that were left uncentered. [sent-11, score-1.03]
</p><p>7 PROBLEM:  The single most important result I am looking for is to compare the strength of nine competing Level-1 variables in their ability to predict and explain the outcome variable, story prominence. [sent-12, score-0.44]
</p><p>8 We are trying to use the residuals to calculate a “R-squared” measure for each level as you and Hill proposed. [sent-13, score-0.323]
</p><p>9 We haven’t been able to generate OLS regression equations for each newspaper and ownership group in HLM because the manual suggests “optional settings” that are not available in our software (HLM 6. [sent-14, score-1.331]
</p><p>10 QUESTION-1 – How could we generate the estimated Bayesian residuals for level-1? [sent-16, score-0.24]
</p><p>11 QUESTION-2 – Is it legitimate to run a model where Level-1 and Level-2  variables are standardized and Level-3 variables are dichotomous dummy variables? [sent-17, score-1.089]
</p><p>12 QUESTION-3 – Is it legitimate to run models to estimate parameters for each ownership group and at the same time include the corresponding dummy variables as part of the data structure? [sent-18, score-1.447]
</p><p>13 QUESTION-4 – In equations that include Level-3 variables, is it valid to describe the results as applying selectively to the stories (L1) in newspapers (L2) owned by one ownership group (L3, coded 1) as opposed to stories in newspapers of other ownership groups (L3, coded 0)? [sent-19, score-2.745]
</p><p>14 I don’t know the HLM software so I don’t know how to use it to compute the Bayesian residuals. [sent-21, score-0.198]
</p><p>15 But you might be happy to hear that we are currently working on implementing these ideas using the lmer/glmer software in R. [sent-22, score-0.192]
</p><p>16 Once it’s been programmed in one package, it shouldn’t be hard for people to translate it into another. [sent-23, score-0.136]
</p><p>17 When in doubt, interpret coefficients by considering predictions with inputs set to various reasonable fixed values. [sent-26, score-0.125]
</p><p>18 If you have all the data loaded in, you should be able to use ownership group as a level and also include predictors at that level. [sent-29, score-1.109]
</p><p>19 I think this is reasonable but I’m not following all the details. [sent-31, score-0.062]
</p><p>20 That’s one trick we use in our book on occasion. [sent-33, score-0.123]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ownership', 0.59), ('variables', 0.262), ('hlm', 0.253), ('newspapers', 0.185), ('group', 0.162), ('dichotomous', 0.162), ('dummy', 0.162), ('stories', 0.159), ('residuals', 0.138), ('coded', 0.134), ('software', 0.129), ('equations', 0.122), ('dependent', 0.113), ('legitimate', 0.108), ('generate', 0.102), ('include', 0.095), ('newspaper', 0.094), ('optional', 0.084), ('doubt', 0.081), ('left', 0.079), ('ols', 0.078), ('groups', 0.078), ('owned', 0.076), ('selectively', 0.076), ('variable', 0.073), ('loaded', 0.072), ('programmed', 0.071), ('manual', 0.069), ('fred', 0.069), ('use', 0.069), ('run', 0.068), ('scales', 0.066), ('standardized', 0.065), ('translate', 0.065), ('nine', 0.064), ('news', 0.064), ('able', 0.063), ('implementing', 0.063), ('inputs', 0.063), ('reasonable', 0.062), ('occasion', 0.059), ('strength', 0.059), ('level', 0.058), ('calculate', 0.058), ('approximation', 0.057), ('sociologist', 0.057), ('competing', 0.055), ('characteristics', 0.055), ('hill', 0.055), ('trick', 0.054)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000002 <a title="1121-tfidf-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-15-R-squared_for_multilevel_models.html">1121 andrew gelman stats-2012-01-15-R-squared for multilevel models</a></p>
<p>Introduction: Fred Schiff writes: 
  
  
I’m writing to you to ask about the “R-squared” approximation procedure you suggest in your 2004 book with Dr. Hill.  [See also  this paper  with Pardoe---ed.]


I’m a media sociologist at the University of Houston.  I’ve been using HLM3 for about two years.  


Briefly about my data.  It’s a content analysis of news stories with a continuous scale dependent variable, story prominence.  I have 6090 news stories, 114 newspapers, and 59 newspaper group owners.  All the Level-1, Level-2 and dependent variables have been standardized. Since the means were zero anyway, we left the variables uncentered.  All the Level-3 ownership groups and characteristics are dichotomous scales that were left uncentered.  


PROBLEM:  The single most important result I am looking for is to compare the strength of nine competing Level-1 variables in their ability to predict and explain the outcome variable, story prominence.  We are trying to use the residuals to calculate a “R-squ</p><p>2 0.15778491 <a title="1121-tfidf-2" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-06-17_groups%2C_6_group-level_predictors%3A__What_to_do%3F.html">1248 andrew gelman stats-2012-04-06-17 groups, 6 group-level predictors:  What to do?</a></p>
<p>Introduction: Yi-Chun Ou writes: 
  
  
I am using a multilevel model with three levels. I read that you wrote a book about multilevel models, and wonder if you can solve the following question.  


The data structure is like this: 


Level one: customer (8444 customers) 
Level two: companys (90 companies) 
Level three: industry (17 industries) 


I use 6 level-three variables (i.e. industry characteristics) to explain the variance of the level-one effect across industries. The question here is whether there is an over-fitting problem since there are only 17 industries. I understand that this must be a problem for non-multilevel models, but is it also a problem for multilevel models?
  
My reply:  Yes, this could be a problem.  I’d suggest combining some of your variables into a common score, or using only some of the variables, or using strong priors to control the inferences.  This is an interesting and important area of statistics research, to do this sort of thing systematically.  There’s lots o</p><p>3 0.12701614 <a title="1121-tfidf-3" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-15-Exploratory_multilevel_analysis_when_group-level_variables_are_of_importance.html">1900 andrew gelman stats-2013-06-15-Exploratory multilevel analysis when group-level variables are of importance</a></p>
<p>Introduction: Steve Miller writes: 
  
  
Much of what I do is cross-national analyses of survey data (largely World Values Survey). . . . My big question pertains to (what I would call) exploratory analysis of multilevel data, especially when the group-level predictors are of theoretical importance. A lot of what I do involves analyzing cross-national survey items of citizen attitudes, typically of political leadership. These survey items are usually yes/no responses, or four-part responses indicating a level of agreement (strongly agree, agree, disagree, strongly disagree) that can be condensed into a binary variable. I believe these can be explained by reference to country-level factors. Much of the group-level variables of interest are count variables with a modal value of 0, which can be quite messy.


How would you recommend exploring the variation in the dependent variable as it could be explained by the group-level count variable of interest, before fitting the multilevel model itself? When</p><p>4 0.12357817 <a title="1121-tfidf-4" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-03-He_doesn%E2%80%99t_trust_the_fit_._._._r%3D.999.html">315 andrew gelman stats-2010-10-03-He doesn’t trust the fit . . . r=.999</a></p>
<p>Introduction: I received the following question from an education researcher:
  
 
I was wondering if I could ask you a question about an HLM model I’m working on.  The basic design is that we have 5 years of 8th grade student achievement data (standardized test scores, this is the dependent variable), 4th grade test scores, demographics (e.g., gender and ethnicity) and status wrt special ed or ELL, etc..  In addition, we have some school- or second-level information such as school averages of the student information, type of school (grade configuration), enrollment and so.  In total there are thousands of students and many schools over the 5 years of information.


The model we’re using is quite parsimonious, using only 7 student-level effects and 4 school-level effects.   What’s puzzling us is that the correlation between predicted and actual is unrealistically high…r=0.999.   We’re using the HPMIXED procedure in SAS but that shouldn’t matter.  By dropping variables, obviously we can get the corre</p><p>5 0.11973918 <a title="1121-tfidf-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-25-Continuous_variables_in_Bayesian_networks.html">1228 andrew gelman stats-2012-03-25-Continuous variables in Bayesian networks</a></p>
<p>Introduction: Antti Rasinen writes:
  
I’m a former undergrad machine learning student and a current software engineer with a Bayesian hobby. Today my two worlds collided. I ask for some enlightenment.


On your blog you’ve repeatedly advocated continuous distributions with Bayesian models. Today I read  this article  by Ricky Ho, who writes:

 
The strength of Bayesian network is it is highly scalable and can learn incrementally because all we do is to count the observed variables and update the probability distribution table. Similar to Neural Network, Bayesian network expects all data to be binary, categorical variable will need to be transformed into multiple binary variable as described above. Numeric variable is generally not a good fit for Bayesian network.
 

The last sentence seems to be at odds with what you’ve said. Sadly, I don’t have enough expertise to say which view of the world is correct. During my undergrad years our team wrote an implementation of the Junction Tree algorithm. We r</p><p>6 0.11429317 <a title="1121-tfidf-6" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-14-The_robust_beauty_of_improper_linear_models_in_decision_making.html">1981 andrew gelman stats-2013-08-14-The robust beauty of improper linear models in decision making</a></p>
<p>7 0.10789667 <a title="1121-tfidf-7" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-12-24-Estimating_and_summarizing_inference_for_hierarchical_variance_parameters_when_the_number_of_groups_is_small.html">2145 andrew gelman stats-2013-12-24-Estimating and summarizing inference for hierarchical variance parameters when the number of groups is small</a></p>
<p>8 0.10141089 <a title="1121-tfidf-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-04-Question_about_standard_range_for_social_science_correlations.html">257 andrew gelman stats-2010-09-04-Question about standard range for social science correlations</a></p>
<p>9 0.10045918 <a title="1121-tfidf-9" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-09-03-A_psychology_researcher_asks%3A__Is_Anova_dead%3F.html">888 andrew gelman stats-2011-09-03-A psychology researcher asks:  Is Anova dead?</a></p>
<p>10 0.098590277 <a title="1121-tfidf-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-04-What_are_the_key_assumptions_of_linear_regression%3F.html">1967 andrew gelman stats-2013-08-04-What are the key assumptions of linear regression?</a></p>
<p>11 0.098494314 <a title="1121-tfidf-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-07-Prior_distributions_for_regression_coefficients.html">1486 andrew gelman stats-2012-09-07-Prior distributions for regression coefficients</a></p>
<p>12 0.095261469 <a title="1121-tfidf-12" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-12-05-What_do_practitioners_need_to_know_about_regression%3F.html">451 andrew gelman stats-2010-12-05-What do practitioners need to know about regression?</a></p>
<p>13 0.095096178 <a title="1121-tfidf-13" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-18-Check_your_missing-data_imputations_using_cross-validation.html">1218 andrew gelman stats-2012-03-18-Check your missing-data imputations using cross-validation</a></p>
<p>14 0.086184546 <a title="1121-tfidf-14" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-05-More_philosophy_of_Bayes.html">1247 andrew gelman stats-2012-04-05-More philosophy of Bayes</a></p>
<p>15 0.084697433 <a title="1121-tfidf-15" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-18-Standardizing_regression_inputs.html">1462 andrew gelman stats-2012-08-18-Standardizing regression inputs</a></p>
<p>16 0.083975725 <a title="1121-tfidf-16" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-19-Index_or_indicator_variables.html">2296 andrew gelman stats-2014-04-19-Index or indicator variables</a></p>
<p>17 0.083867133 <a title="1121-tfidf-17" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Allowing_interaction_terms_to_vary.html">753 andrew gelman stats-2011-06-09-Allowing interaction terms to vary</a></p>
<p>18 0.083706662 <a title="1121-tfidf-18" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-02-12-Get_the_Data.html">569 andrew gelman stats-2011-02-12-Get the Data</a></p>
<p>19 0.080965668 <a title="1121-tfidf-19" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-12-30-Fixed_effects%2C_followed_by_Bayes_shrinkage%3F.html">1644 andrew gelman stats-2012-12-30-Fixed effects, followed by Bayes shrinkage?</a></p>
<p>20 0.080448315 <a title="1121-tfidf-20" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-21-Building_a_regression_model_._._._with_only_27_data_points.html">1506 andrew gelman stats-2012-09-21-Building a regression model . . . with only 27 data points</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/andrew_gelman_stats_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.148), (1, 0.073), (2, 0.03), (3, -0.007), (4, 0.069), (5, 0.034), (6, 0.013), (7, -0.047), (8, 0.054), (9, 0.081), (10, 0.016), (11, -0.01), (12, 0.019), (13, -0.017), (14, 0.037), (15, 0.029), (16, -0.017), (17, 0.01), (18, 0.019), (19, 0.004), (20, -0.019), (21, 0.043), (22, 0.01), (23, -0.025), (24, 0.002), (25, -0.007), (26, 0.031), (27, -0.012), (28, -0.004), (29, -0.009), (30, 0.041), (31, 0.037), (32, 0.038), (33, 0.06), (34, 0.019), (35, -0.002), (36, -0.003), (37, 0.037), (38, 0.006), (39, 0.002), (40, -0.006), (41, -0.013), (42, 0.041), (43, 0.019), (44, -0.024), (45, 0.003), (46, 0.026), (47, 0.06), (48, 0.018), (49, 0.021)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97075486 <a title="1121-lsi-1" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-15-R-squared_for_multilevel_models.html">1121 andrew gelman stats-2012-01-15-R-squared for multilevel models</a></p>
<p>Introduction: Fred Schiff writes: 
  
  
I’m writing to you to ask about the “R-squared” approximation procedure you suggest in your 2004 book with Dr. Hill.  [See also  this paper  with Pardoe---ed.]


I’m a media sociologist at the University of Houston.  I’ve been using HLM3 for about two years.  


Briefly about my data.  It’s a content analysis of news stories with a continuous scale dependent variable, story prominence.  I have 6090 news stories, 114 newspapers, and 59 newspaper group owners.  All the Level-1, Level-2 and dependent variables have been standardized. Since the means were zero anyway, we left the variables uncentered.  All the Level-3 ownership groups and characteristics are dichotomous scales that were left uncentered.  


PROBLEM:  The single most important result I am looking for is to compare the strength of nine competing Level-1 variables in their ability to predict and explain the outcome variable, story prominence.  We are trying to use the residuals to calculate a “R-squ</p><p>2 0.85130459 <a title="1121-lsi-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-05-01-Imputing_count_data.html">14 andrew gelman stats-2010-05-01-Imputing count data</a></p>
<p>Introduction: Guy asks:
  
I am analyzing an original survey of farmers in Uganda. I am hoping to use a battery of welfare proxy variables to create a single welfare index using PCA. I have quick question which I hope you can find time to address:


How do you recommend treating count data? (for example # of rooms, # of chickens, # of cows, # of radios)? In my dataset these variables are highly skewed with many responses at zero (which makes taking the natural log problematic). In the case of # of cows or chickens several obs have values in the hundreds.
  
My response:  Hereâ&euro;&trade;s what we do in our mi package in R.  We split a variable into two parts:  an indicator for whether it is positive, and the positive part.  That is, y = u*v.  Then u is binary and can be modeled using logisitc regression, and v can be modeled on the log scale.  At the end you can round to the nearest integer  if you want to avoid fractional values.</p><p>3 0.81454945 <a title="1121-lsi-3" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-04-19-Index_or_indicator_variables.html">2296 andrew gelman stats-2014-04-19-Index or indicator variables</a></p>
<p>Introduction: Someone who doesn’t want his name shared (for the perhaps reasonable reason that he’ll “one day not be confused, and would rather my confusion not live on online forever”) writes:
  
I’m exploring HLMs and stan, using your book with Jennifer Hill as my field guide to this new territory. I think I have a generally clear grasp on the material, but wanted to be sure I haven’t gone astray. 


The problem in working on involves a multi-nation survey of students, and I’m especially interested in understanding the effects of country, religion, and sex, and the interactions among those factors (using IRT to estimate individual-level ability, then estimating individual, school, and country effects).


Following the basic approach laid out in chapter 13 for such interactions between levels, I think I need to create a matrix of indicator variables for religion and sex. Elsewhere in the book, you recommend against indicator variables in favor of a single index variable. 


Am I right in thinking t</p><p>4 0.79945415 <a title="1121-lsi-4" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-08-18-Standardizing_regression_inputs.html">1462 andrew gelman stats-2012-08-18-Standardizing regression inputs</a></p>
<p>Introduction: Andy Flies, Ph.D. candidate in zoology, writes:
  
After reading your paper about scaling regression inputs by two standard deviations I found your  blog post  stating that you wished you had scaled by 1 sd and coded the binary inputs as -1 and 1.  Here is my question:


If you code the binary input as -1 and 1, do you then standardize it?  This makes sense to me because the mean of the standardized input is then zero and the sd is 1, which is what the mean and sd are for all of the other standardized inputs.  I know that if you code the binary input as 0 and 1 it should not be standardized.


Also, I am not interested in the actual units (i.e. mg/ml) of my response variable and I would like to compare a couple of different response variables that are on different scales.  Would it make sense to standardize the response variable also?
  
My reply:  No, I donâ&euro;&trade;t standardize the binary input.  The point of standardizing inputs is to make the coefs directly interpretable, but with binary i</p><p>5 0.79688883 <a title="1121-lsi-5" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-04-Question_about_standard_range_for_social_science_correlations.html">257 andrew gelman stats-2010-09-04-Question about standard range for social science correlations</a></p>
<p>Introduction: Andrew Eppig writes:
  
I’m a physicist by training who is transitioning to the social sciences. I recently came across a  reference  in the Economist to a paper on IQ and parasites which I read as I have more than a passing interest in IQ research (having read much that you and others (e.g., Shalizi, Wicherts) have written). In this paper I note that the authors find a very high correlation between national IQ and parasite prevalence. The strength of the correlation (-0.76 to -0.82) surprised me, as I’m used to much weaker correlations in the social sciences. To me, it’s a bit too high, suggesting that there are other factors at play or that one of the variables is merely a proxy for a large number of other variables. But I have no basis for this other than a gut feeling and a memory of a plot on  Language Log  about the distribution of correlation coefficients in social psychology.


So my question is this: Is a correlation in the range of (-0.82,-0.76) more likely to be a correlatio</p><p>6 0.79178554 <a title="1121-lsi-6" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-03-18-Check_your_missing-data_imputations_using_cross-validation.html">1218 andrew gelman stats-2012-03-18-Check your missing-data imputations using cross-validation</a></p>
<p>7 0.77919626 <a title="1121-lsi-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-19-Cross-validation_to_check_missing-data_imputation.html">1330 andrew gelman stats-2012-05-19-Cross-validation to check missing-data imputation</a></p>
<p>8 0.77784097 <a title="1121-lsi-8" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-15-Exploratory_multilevel_analysis_when_group-level_variables_are_of_importance.html">1900 andrew gelman stats-2013-06-15-Exploratory multilevel analysis when group-level variables are of importance</a></p>
<p>9 0.77494365 <a title="1121-lsi-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-04-06-17_groups%2C_6_group-level_predictors%3A__What_to_do%3F.html">1248 andrew gelman stats-2012-04-06-17 groups, 6 group-level predictors:  What to do?</a></p>
<p>10 0.77092785 <a title="1121-lsi-10" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-06-21-Interpreting_interactions_in_discrete-data_regression.html">1908 andrew gelman stats-2013-06-21-Interpreting interactions in discrete-data regression</a></p>
<p>11 0.76434845 <a title="1121-lsi-11" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-31-Using_factor_analysis_or_principal_components_analysis_or_measurement-error_models_for_biological_measurements_in_archaeology%3F.html">1094 andrew gelman stats-2011-12-31-Using factor analysis or principal components analysis or measurement-error models for biological measurements in archaeology?</a></p>
<p>12 0.76108807 <a title="1121-lsi-12" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-01-Modeling_y_%3D_a_%2B_b_%2B_c.html">1294 andrew gelman stats-2012-05-01-Modeling y = a + b + c</a></p>
<p>13 0.75806403 <a title="1121-lsi-13" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-09-12-GLM_%E2%80%93_exposure.html">271 andrew gelman stats-2010-09-12-GLM – exposure</a></p>
<p>14 0.75377595 <a title="1121-lsi-14" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-09-Allowing_interaction_terms_to_vary.html">753 andrew gelman stats-2011-06-09-Allowing interaction terms to vary</a></p>
<p>15 0.75158453 <a title="1121-lsi-15" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-02-02-Interaction-based_feature_selection_and_classification_for_high-dimensional_biological_data.html">1703 andrew gelman stats-2013-02-02-Interaction-based feature selection and classification for high-dimensional biological data</a></p>
<p>16 0.75079018 <a title="1121-lsi-16" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-04-20-Displaying_inferences_from_complex_models.html">1815 andrew gelman stats-2013-04-20-Displaying inferences from complex models</a></p>
<p>17 0.74518031 <a title="1121-lsi-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-14-The_robust_beauty_of_improper_linear_models_in_decision_making.html">1981 andrew gelman stats-2013-08-14-The robust beauty of improper linear models in decision making</a></p>
<p>18 0.74188441 <a title="1121-lsi-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-08-04-What_are_the_key_assumptions_of_linear_regression%3F.html">1967 andrew gelman stats-2013-08-04-What are the key assumptions of linear regression?</a></p>
<p>19 0.73687351 <a title="1121-lsi-19" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-06-15-Still_more_Mr._P_in_public_health.html">770 andrew gelman stats-2011-06-15-Still more Mr. P in public health</a></p>
<p>20 0.73518711 <a title="1121-lsi-20" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-05-10-Multiple_imputation_and_multilevel_analysis.html">704 andrew gelman stats-2011-05-10-Multiple imputation and multilevel analysis</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/andrew_gelman_stats_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(6, 0.026), (15, 0.025), (16, 0.096), (21, 0.037), (24, 0.148), (45, 0.208), (63, 0.013), (66, 0.021), (84, 0.012), (85, 0.022), (99, 0.24)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.94150645 <a title="1121-lda-1" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-07-08-Censoring_on_one_end%2C_%E2%80%9Coutliers%E2%80%9D_on_the_other%2C_what_can_we_do_with_the_middle%3F.html">791 andrew gelman stats-2011-07-08-Censoring on one end, “outliers” on the other, what can we do with the middle?</a></p>
<p>Introduction: This post was written by Phil.
 
A medical company is testing a cancer drug. They get a 16 genetically identical (or nearly identical) rats that all have the same kind of tumor, give 8 of them the drug and leave 8 untreated…or maybe they give them a placebo, I don’t know; is there a placebo effect in rats?.  Anyway, after a while the rats are killed and examined. If the tumors in the treated rats are smaller than the tumors in the untreated rats, then all of the rats have their blood tested for dozens of different proteins that are known to be associated with tumor growth or suppression.  If there is a “significant” difference in one of the protein levels, then the working assumption is that the drug increases or decreases levels of that protein and that may be the mechanism by which the drug affects cancer. All of the above is done on many different cancer types and possibly several different types of rats.  It’s just the initial screening: if things look promising, many more tests an</p><p>2 0.93624687 <a title="1121-lda-2" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-02-The_winner%E2%80%99s_curse.html">310 andrew gelman stats-2010-10-02-The winner’s curse</a></p>
<p>Introduction: If an estimate is statistically significant, it’s probably an overestimate of the magnitude of your effect.
 
P.S.  I think youall know what I mean here.  But could someone rephrase it in a more pithy manner?  I’d like to include it in our statistical lexicon.</p><p>3 0.92639506 <a title="1121-lda-3" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-09-I_was_at_a_meeting_a_couple_months_ago_._._..html">999 andrew gelman stats-2011-11-09-I was at a meeting a couple months ago . . .</a></p>
<p>Introduction: . . . and I decided to amuse myself by writing down all the management-speak words I heard:
 
“grappling” 
“early prototypes” 
“technology platform” 
“building block” 
“machine learning” 
“your team” 
“workspace” 
“tagging” 
“data exhaust” 
“monitoring a particular population” 
“collective intelligence” 
“communities of practice” 
“hackathon” 
“human resources . . . technologies”
 
Any one or two or three of these phrases might be fine, but put them all together and what you have is a festival of jargon.  A hackathon, indeed.</p><p>4 0.92399585 <a title="1121-lda-4" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-27-Richard_Stallman_and_John_McCarthy.html">1031 andrew gelman stats-2011-11-27-Richard Stallman and John McCarthy</a></p>
<p>Introduction: After blogging on quirky software pioneer  Richard Stallman , I thought it appropriate to write something about recently deceased quirky software pioneer John McCarthy, who, with the exception of being bearded, seems like he was the personal and political opposite of Stallman.
 
 Here’s  a page I found of  Stallman  McCarthy quotes (compiled by Neil Craig).  It’s a mixture of the reasonable and the unreasonable (ok, I suppose the same could be said of this blog!).
 
I wonder if he and Stallman ever met and, if so, whether they had an extended conversation.  It would be like matter and anti-matter! 
   
P.S.  I flipped through McCarthy’s pages and found one of my pet peeves.  See item 3  here , which sounds so plausible but is in fact  not true  (at least, not according to the National Election Study).  As McCarthy’s Stanford colleague Mo Fiorina can tell you, otherwise well-informed people believe all sorts of things about polarization that aren’t so.  Labeling groups of Americans as “</p><p>5 0.92325711 <a title="1121-lda-5" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-05-17-More_on_the_difficulty_of_%E2%80%9Cpreaching_what_you_practice%E2%80%9D.html">1325 andrew gelman stats-2012-05-17-More on the difficulty of “preaching what you practice”</a></p>
<p>Introduction: A couple months ago, in discussing Charles Murray’s argument that America’s social leaders should “preach what they practice” (Murray argues that they—we!—tend to lead good lives of hard work and moderation but are all too tolerant of antisocial and unproductive behavior among the lower classes), I  wrote : 
  
  
Murray does not consider the case of Joe Paterno, but in many ways the Penn State football coach fits his story well. Paterno was said to live an exemplary personal and professional life, combining traditional morality with football success—but, by his actions, he showed little concern about the morality of his players and coaches. At a professional level, Paterno rose higher and higher, and in his personal life he was a responsible adult. But he had an increasing disconnect with the real world, to the extent that horrible crimes were occurring nearby (in the physical and social senses) but he was completely insulated from the consequences for many years. Paterno’s story is s</p><p>6 0.92135936 <a title="1121-lda-6" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-01-28-NYT_shills_for_personal_DNA_tests.html">543 andrew gelman stats-2011-01-28-NYT shills for personal DNA tests</a></p>
<p>same-blog 7 0.91739571 <a title="1121-lda-7" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-01-15-R-squared_for_multilevel_models.html">1121 andrew gelman stats-2012-01-15-R-squared for multilevel models</a></p>
<p>8 0.9071281 <a title="1121-lda-8" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-06-04-A_Wikipedia_whitewash.html">69 andrew gelman stats-2010-06-04-A Wikipedia whitewash</a></p>
<p>9 0.90121818 <a title="1121-lda-9" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-07-06-Statistical_inference_and_the_secret_ballot.html">1407 andrew gelman stats-2012-07-06-Statistical inference and the secret ballot</a></p>
<p>10 0.89576066 <a title="1121-lda-10" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-11-16-Blog_bribes%21.html">1012 andrew gelman stats-2011-11-16-Blog bribes!</a></p>
<p>11 0.89417052 <a title="1121-lda-11" href="../andrew_gelman_stats-2012/andrew_gelman_stats-2012-09-20-Could_someone_please_lock_this_guy_and_Niall_Ferguson_in_a_room_together%3F.html">1504 andrew gelman stats-2012-09-20-Could someone please lock this guy and Niall Ferguson in a room together?</a></p>
<p>12 0.89050746 <a title="1121-lda-12" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-12-28-Path_sampling_for_models_of_varying_dimension.html">1089 andrew gelman stats-2011-12-28-Path sampling for models of varying dimension</a></p>
<p>13 0.88874316 <a title="1121-lda-13" href="../andrew_gelman_stats-2014/andrew_gelman_stats-2014-01-28-History_is_too_important_to_be_left_to_the_history_professors.html">2189 andrew gelman stats-2014-01-28-History is too important to be left to the history professors</a></p>
<p>14 0.88491744 <a title="1121-lda-14" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-13-Indiemapper_makes_thematic_mapping_easy.html">206 andrew gelman stats-2010-08-13-Indiemapper makes thematic mapping easy</a></p>
<p>15 0.88273031 <a title="1121-lda-15" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-10-22-A_redrawing_of_the_Red-Blue_map_in_November_2010%3F.html">362 andrew gelman stats-2010-10-22-A redrawing of the Red-Blue map in November 2010?</a></p>
<p>16 0.88016522 <a title="1121-lda-16" href="../andrew_gelman_stats-2011/andrew_gelman_stats-2011-04-20-Upper-income_people_still_don%E2%80%99t_realize_they%E2%80%99re_upper-income.html">673 andrew gelman stats-2011-04-20-Upper-income people still don’t realize they’re upper-income</a></p>
<p>17 0.87904143 <a title="1121-lda-17" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-01-07-Free_advice_from_an_academic_writing_coach%21.html">1658 andrew gelman stats-2013-01-07-Free advice from an academic writing coach!</a></p>
<p>18 0.87576932 <a title="1121-lda-18" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-03-17-The_disappearing_or_non-disappearing_middle_class.html">1767 andrew gelman stats-2013-03-17-The disappearing or non-disappearing middle class</a></p>
<p>19 0.86695862 <a title="1121-lda-19" href="../andrew_gelman_stats-2013/andrew_gelman_stats-2013-05-13-A_Structural_Comparison_of_Conspicuous_Consumption_in_China_and_the_United_States.html">1854 andrew gelman stats-2013-05-13-A Structural Comparison of Conspicuous Consumption in China and the United States</a></p>
<p>20 0.86637866 <a title="1121-lda-20" href="../andrew_gelman_stats-2010/andrew_gelman_stats-2010-08-08-Turning_pages_into_data.html">192 andrew gelman stats-2010-08-08-Turning pages into data</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
