<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>91 brendan oconnor ai-2008-01-27-Graphics! Atari Breakout and religious text NLP</title>
</head>

<body>
<p><a title="brendan_oconnor_ai" href="../brendan_oconnor_ai_home.html">brendan_oconnor_ai</a> <a title="brendan_oconnor_ai-2008" href="../home/brendan_oconnor_ai-2008_home.html">brendan_oconnor_ai-2008</a> <a title="brendan_oconnor_ai-2008-91" href="#">brendan_oconnor_ai-2008-91</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>91 brendan oconnor ai-2008-01-27-Graphics! Atari Breakout and religious text NLP</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="brendan_oconnor_ai-2008-91-html" href="http://brenocon.com/blog/2008/01/graphics-atari-breakout-and-religious-text-nlp/">html</a></p><p>Introduction: From a graphics/mod programming workshop,  modifications of “Breakout”  in awesome video form:
 
          
 
It uses  Processing , a framework designed for animation and graphicky things.  It was also used for the  Similar Diversity  visualization that maps out named entities and their common verbs across religious texts.  It’s pretty cool:     … so a name gets bigger with its frequency, I get that; but what governs the size of an arc connecting two names?  They say “common activities” — I’m guessing a distance measure on the terms in the contexts in which they appear?  I’d be most interested in a graph of their explicit interactions; something like a count of co-occurrences in a proximity window (word radius? verse? chapter? book?) might do the trick…
 
This came out a while ago, I think; here are some  related works  posted on infosthetics.com.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 From a graphics/mod programming workshop,  modifications of “Breakout”  in awesome video form:                It uses  Processing , a framework designed for animation and graphicky things. [sent-1, score-0.928]
</p><p>2 It was also used for the  Similar Diversity  visualization that maps out named entities and their common verbs across religious texts. [sent-2, score-1.189]
</p><p>3 It’s pretty cool:     … so a name gets bigger with its frequency, I get that; but what governs the size of an arc connecting two names? [sent-3, score-0.798]
</p><p>4 They say “common activities” — I’m guessing a distance measure on the terms in the contexts in which they appear? [sent-4, score-0.707]
</p><p>5 I’d be most interested in a graph of their explicit interactions; something like a count of co-occurrences in a proximity window (word radius? [sent-5, score-0.709]
</p><p>6 ) might do the trick…   This came out a while ago, I think; here are some  related works  posted on infosthetics. [sent-9, score-0.456]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('common', 0.224), ('activities', 0.177), ('animation', 0.177), ('explicit', 0.177), ('arc', 0.177), ('distance', 0.177), ('diversity', 0.177), ('frequency', 0.177), ('proximity', 0.177), ('trick', 0.177), ('came', 0.161), ('connecting', 0.161), ('entities', 0.161), ('guessing', 0.161), ('verbs', 0.161), ('religious', 0.15), ('bigger', 0.15), ('count', 0.15), ('framework', 0.15), ('maps', 0.15), ('workshop', 0.142), ('interactions', 0.142), ('named', 0.142), ('chapter', 0.142), ('contexts', 0.142), ('names', 0.129), ('awesome', 0.124), ('uses', 0.124), ('designed', 0.119), ('video', 0.119), ('measure', 0.119), ('appear', 0.115), ('programming', 0.115), ('processing', 0.112), ('visualization', 0.108), ('form', 0.108), ('terms', 0.108), ('gets', 0.105), ('graph', 0.105), ('size', 0.105), ('related', 0.103), ('ago', 0.103), ('interested', 0.1), ('name', 0.1), ('posted', 0.097), ('cool', 0.095), ('works', 0.095), ('across', 0.093), ('word', 0.087), ('book', 0.082)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="91-tfidf-1" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-01-27-Graphics%21_Atari_Breakout_and_religious_text_NLP.html">91 brendan oconnor ai-2008-01-27-Graphics! Atari Breakout and religious text NLP</a></p>
<p>Introduction: From a graphics/mod programming workshop,  modifications of “Breakout”  in awesome video form:
 
          
 
It uses  Processing , a framework designed for animation and graphicky things.  It was also used for the  Similar Diversity  visualization that maps out named entities and their common verbs across religious texts.  It’s pretty cool:     … so a name gets bigger with its frequency, I get that; but what governs the size of an arc connecting two names?  They say “common activities” — I’m guessing a distance measure on the terms in the contexts in which they appear?  I’d be most interested in a graph of their explicit interactions; something like a count of co-occurrences in a proximity window (word radius? verse? chapter? book?) might do the trick…
 
This came out a while ago, I think; here are some  related works  posted on infosthetics.com.</p><p>2 0.097898223 <a title="91-tfidf-2" href="../brendan_oconnor_ai-2006/brendan_oconnor_ai-2006-08-30-A_big%2C_fun_list_of_links_I%E2%80%99m_reading.html">44 brendan oconnor ai-2006-08-30-A big, fun list of links I’m reading</a></p>
<p>Introduction: Since blogging is hard, but reading is easy, lately I’ve taken to bookmarking interesting articles I’m reading, with the plan of blogging about them later.  This follow-through has happened a few times, but not that often.  In an amazing moment of thesis procrastination, today I sat down and figured out how to turn my  del.icio.us bookmarks  into a nice blogpost, with the plan that every week a post will appear with links I’ve recently read, or maybe I’ll use the script to generate a draft for myself that I’ll revise, or something.
 
But for this first such link post, I put in a whole bunch of them beyond just the last week — why have just a few when you could have *all* of them?  Future link posts will be shorter, I promise.
 
  Ariel Rubinstein: Freak-Freakonomics    July 2006   posted 8/19 under  economics   sarcastic, critical review of levitt & dubner’s Freakonomics 
  New Yorker review of Philip Tetlock’s book on political expert judgment   posted 8/19 under  judgment ,  psycholo</p><p>3 0.095591769 <a title="91-tfidf-3" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-26-Seeing_how_%E2%80%9Cart%E2%80%9D_and_%E2%80%9Cpharmaceuticals%E2%80%9D_are_linguistically_similar_in_web_text.html">156 brendan oconnor ai-2009-09-26-Seeing how “art” and “pharmaceuticals” are linguistically similar in web text</a></p>
<p>Introduction: Earlier this week I asked the question,
 
 How are “art” and “pharmaceuticals” similar? 
 
People sent me lots of submissions!  Some are great, some are a bit of a stretch.
  
 Overpriced by an order of magnitude.
  The letters of “art” are found embedded, in order, in “pharmaceuticals”.
  Search keywords that cost the most to advertise on?
  “Wyeth”: I think this means  this , and  this .
  “Romeo and Juliet” famously includes both “art” (wherefore art thou) and pharmaceuticals (poison!)
  Some art has been created out of pharmaceuticals.
  Some art has been created under the influence of pharmaceuticals.
   
I was asking because I was playing around with a dataset of 100,000 noun phrases’ appearances on the web, from the  Reading the Web  project at CMU.  That is, for a noun like “art”, this data has a large list of phrases in which the word “art” is used, across some 200 million web pages.  For two noun concepts, we can see what they have in common and what’s different by looking at</p><p>4 0.088507406 <a title="91-tfidf-4" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-05-27-Where_tweets_get_sent_from.html">142 brendan oconnor ai-2009-05-27-Where tweets get sent from</a></p>
<p>Introduction: Playing around with  stream.twitter.com/spritzer ,  ggplot2  and  maps / mapdata :
 
   
 
I think I like the top better, without the map lines, like those  night satellite photos : pointwise ghosts of high-end human economic development.
 
This data is a fairly extreme sample of convenience: I’m only looking at tweets posted by certain types of iPhone clients, because they conveniently report exact gps-derived latitude/longitude numbers.  ( search.twitter.com  has geographic proximity operators — which are very cool! — but they seem to usually use zip codes or other user information that’s not available in the per-tweet API data.)  So there’s only 30,000 messages out of 1.2 million  spritzer  tweets over ~3 days (itself only a small single-digit percentage sample of twitter).</p><p>5 0.062969036 <a title="91-tfidf-5" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-01-07-Perplexity_as_branching_factor%3B_as_Shannon_diversity_index.html">190 brendan oconnor ai-2013-01-07-Perplexity as branching factor; as Shannon diversity index</a></p>
<p>Introduction: A language model’s  perplexity  is exponentiated negative average log-likelihood,
 
$$\exp( -\frac{1}{N} \log(p(x)))$$
 
Where the inner term usually decomposes into a sum over individual items; for example, as \(\sum_i \log p(x_i | x_1..x_{i-1})\) or \(\sum_i \log p(x_i)\) depending on independence assumptions, where for language modeling word tokens are usually taken as the individual units.  (In which case it is the geometric mean of per-token negative log-likelihoods.)   It’s equivalent to exponentiated cross-entropy between the model and the empirical data distribution, since \(-1/N \sum_i^N \log p(x_i) = -\sum_k^K \hat{p}_k \log p_k = H(\hat{p};p)\) where \(N\) is the number of items and \(K\) is the number of discrete classes (e.g. word types for language modeling) and \(\hat{p}_k\) is the proportion of data having class \(k\).
 
A nice interpretation of any exponentiated entropy measure is as branching factor: entropy measures uncertainty in bits or nats, but in exponentiated f</p><p>6 0.062031463 <a title="91-tfidf-6" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-10-16-Is_religion_the_opiate_of_the_elite%3F.html">120 brendan oconnor ai-2008-10-16-Is religion the opiate of the elite?</a></p>
<p>7 0.061680101 <a title="91-tfidf-7" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-08-12-Beautiful_Data_book_chapter.html">151 brendan oconnor ai-2009-08-12-Beautiful Data book chapter</a></p>
<p>8 0.060389198 <a title="91-tfidf-8" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-07-15-Beta_conjugate_explorer.html">146 brendan oconnor ai-2009-07-15-Beta conjugate explorer</a></p>
<p>9 0.057021804 <a title="91-tfidf-9" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-04-08-Gapminder.org_%E2%80%94_terrific_world_data_visualizations.html">57 brendan oconnor ai-2007-04-08-Gapminder.org — terrific world data visualizations</a></p>
<p>10 0.051723357 <a title="91-tfidf-10" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-02-02-Histograms_%E2%80%94_matplotlib_vs._R.html">179 brendan oconnor ai-2012-02-02-Histograms — matplotlib vs. R</a></p>
<p>11 0.050278131 <a title="91-tfidf-11" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-03-Statistics_vs._Machine_Learning%2C_fight%21.html">129 brendan oconnor ai-2008-12-03-Statistics vs. Machine Learning, fight!</a></p>
<p>12 0.049266133 <a title="91-tfidf-12" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-02-23-Comparison_of_data_analysis_packages%3A_R%2C_Matlab%2C_SciPy%2C_Excel%2C_SAS%2C_SPSS%2C_Stata.html">135 brendan oconnor ai-2009-02-23-Comparison of data analysis packages: R, Matlab, SciPy, Excel, SAS, SPSS, Stata</a></p>
<p>13 0.047671512 <a title="91-tfidf-13" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-11-17-Correlations_%E2%80%93_cotton_picking_vs._2008_Presidential_votes.html">124 brendan oconnor ai-2008-11-17-Correlations – cotton picking vs. 2008 Presidential votes</a></p>
<p>14 0.046936776 <a title="91-tfidf-14" href="../brendan_oconnor_ai-2006/brendan_oconnor_ai-2006-06-28-Social_network-ized_economic_markets.html">40 brendan oconnor ai-2006-06-28-Social network-ized economic markets</a></p>
<p>15 0.046713151 <a title="91-tfidf-15" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-05-21-iPhone_autocorrection_error_analysis.html">170 brendan oconnor ai-2011-05-21-iPhone autocorrection error analysis</a></p>
<p>16 0.044017024 <a title="91-tfidf-16" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-08-04-Blogger_to_WordPress_migration_helper.html">149 brendan oconnor ai-2009-08-04-Blogger to WordPress migration helper</a></p>
<p>17 0.043045484 <a title="91-tfidf-17" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-08-01-Bayesian_analysis_of_intelligent_design_%28revised%21%29.html">23 brendan oconnor ai-2005-08-01-Bayesian analysis of intelligent design (revised!)</a></p>
<p>18 0.041858159 <a title="91-tfidf-18" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-08-20-Some_analysis_of_tweet_shares_and_%E2%80%9Cpredicting%E2%80%9D_election_outcomes.html">198 brendan oconnor ai-2013-08-20-Some analysis of tweet shares and “predicting” election outcomes</a></p>
<p>19 0.040667176 <a title="91-tfidf-19" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-05-18-Announcing_TweetMotif_for_summarizing_twitter_topics.html">140 brendan oconnor ai-2009-05-18-Announcing TweetMotif for summarizing twitter topics</a></p>
<p>20 0.040315162 <a title="91-tfidf-20" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-11-12-Disease_tracking_with_web_queries_and_social_messaging_%28Google%2C_Twitter%2C_Facebook%E2%80%A6%29.html">123 brendan oconnor ai-2008-11-12-Disease tracking with web queries and social messaging (Google, Twitter, Facebook…)</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/brendan_oconnor_ai_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, -0.146), (1, -0.052), (2, 0.028), (3, 0.093), (4, -0.014), (5, 0.068), (6, -0.014), (7, -0.061), (8, -0.022), (9, -0.101), (10, -0.11), (11, -0.12), (12, 0.059), (13, -0.001), (14, -0.071), (15, 0.072), (16, 0.052), (17, -0.018), (18, -0.126), (19, 0.099), (20, 0.126), (21, 0.02), (22, -0.049), (23, -0.009), (24, -0.046), (25, 0.035), (26, 0.004), (27, 0.054), (28, 0.023), (29, 0.094), (30, -0.021), (31, 0.11), (32, 0.03), (33, -0.093), (34, 0.02), (35, 0.079), (36, -0.213), (37, 0.083), (38, 0.075), (39, -0.016), (40, -0.078), (41, -0.02), (42, -0.085), (43, -0.053), (44, -0.028), (45, 0.012), (46, -0.002), (47, -0.008), (48, -0.044), (49, 0.025)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99258912 <a title="91-lsi-1" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-01-27-Graphics%21_Atari_Breakout_and_religious_text_NLP.html">91 brendan oconnor ai-2008-01-27-Graphics! Atari Breakout and religious text NLP</a></p>
<p>Introduction: From a graphics/mod programming workshop,  modifications of “Breakout”  in awesome video form:
 
          
 
It uses  Processing , a framework designed for animation and graphicky things.  It was also used for the  Similar Diversity  visualization that maps out named entities and their common verbs across religious texts.  It’s pretty cool:     … so a name gets bigger with its frequency, I get that; but what governs the size of an arc connecting two names?  They say “common activities” — I’m guessing a distance measure on the terms in the contexts in which they appear?  I’d be most interested in a graph of their explicit interactions; something like a count of co-occurrences in a proximity window (word radius? verse? chapter? book?) might do the trick…
 
This came out a while ago, I think; here are some  related works  posted on infosthetics.com.</p><p>2 0.61349702 <a title="91-lsi-2" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-05-27-Where_tweets_get_sent_from.html">142 brendan oconnor ai-2009-05-27-Where tweets get sent from</a></p>
<p>Introduction: Playing around with  stream.twitter.com/spritzer ,  ggplot2  and  maps / mapdata :
 
   
 
I think I like the top better, without the map lines, like those  night satellite photos : pointwise ghosts of high-end human economic development.
 
This data is a fairly extreme sample of convenience: I’m only looking at tweets posted by certain types of iPhone clients, because they conveniently report exact gps-derived latitude/longitude numbers.  ( search.twitter.com  has geographic proximity operators — which are very cool! — but they seem to usually use zip codes or other user information that’s not available in the per-tweet API data.)  So there’s only 30,000 messages out of 1.2 million  spritzer  tweets over ~3 days (itself only a small single-digit percentage sample of twitter).</p><p>3 0.47022071 <a title="91-lsi-3" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-07-03-Supreme_Court_justices%E2%80%99_agreement_levels.html">13 brendan oconnor ai-2005-07-03-Supreme Court justices’ agreement levels</a></p>
<p>Introduction: Cool visualization of agreement levels among Supreme Court justices .  I like how they’re ordered so that the smallest amount of agreement ends up in the lower-left.  Hopefully it’s not deceptive for certain cases: I imagine that summarizing their tendencies to vote certain ways into a one dimensional spectrum would lose important information of other dimensions of agreement or coalitions.</p><p>4 0.45226616 <a title="91-lsi-4" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-26-Seeing_how_%E2%80%9Cart%E2%80%9D_and_%E2%80%9Cpharmaceuticals%E2%80%9D_are_linguistically_similar_in_web_text.html">156 brendan oconnor ai-2009-09-26-Seeing how “art” and “pharmaceuticals” are linguistically similar in web text</a></p>
<p>Introduction: Earlier this week I asked the question,
 
 How are “art” and “pharmaceuticals” similar? 
 
People sent me lots of submissions!  Some are great, some are a bit of a stretch.
  
 Overpriced by an order of magnitude.
  The letters of “art” are found embedded, in order, in “pharmaceuticals”.
  Search keywords that cost the most to advertise on?
  “Wyeth”: I think this means  this , and  this .
  “Romeo and Juliet” famously includes both “art” (wherefore art thou) and pharmaceuticals (poison!)
  Some art has been created out of pharmaceuticals.
  Some art has been created under the influence of pharmaceuticals.
   
I was asking because I was playing around with a dataset of 100,000 noun phrases’ appearances on the web, from the  Reading the Web  project at CMU.  That is, for a noun like “art”, this data has a large list of phrases in which the word “art” is used, across some 200 million web pages.  For two noun concepts, we can see what they have in common and what’s different by looking at</p><p>5 0.4457925 <a title="91-lsi-5" href="../brendan_oconnor_ai-2006/brendan_oconnor_ai-2006-08-30-A_big%2C_fun_list_of_links_I%E2%80%99m_reading.html">44 brendan oconnor ai-2006-08-30-A big, fun list of links I’m reading</a></p>
<p>Introduction: Since blogging is hard, but reading is easy, lately I’ve taken to bookmarking interesting articles I’m reading, with the plan of blogging about them later.  This follow-through has happened a few times, but not that often.  In an amazing moment of thesis procrastination, today I sat down and figured out how to turn my  del.icio.us bookmarks  into a nice blogpost, with the plan that every week a post will appear with links I’ve recently read, or maybe I’ll use the script to generate a draft for myself that I’ll revise, or something.
 
But for this first such link post, I put in a whole bunch of them beyond just the last week — why have just a few when you could have *all* of them?  Future link posts will be shorter, I promise.
 
  Ariel Rubinstein: Freak-Freakonomics    July 2006   posted 8/19 under  economics   sarcastic, critical review of levitt & dubner’s Freakonomics 
  New Yorker review of Philip Tetlock’s book on political expert judgment   posted 8/19 under  judgment ,  psycholo</p><p>6 0.39299554 <a title="91-lsi-6" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-04-08-Gapminder.org_%E2%80%94_terrific_world_data_visualizations.html">57 brendan oconnor ai-2007-04-08-Gapminder.org — terrific world data visualizations</a></p>
<p>7 0.36274976 <a title="91-lsi-7" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-11-17-Correlations_%E2%80%93_cotton_picking_vs._2008_Presidential_votes.html">124 brendan oconnor ai-2008-11-17-Correlations – cotton picking vs. 2008 Presidential votes</a></p>
<p>8 0.35810253 <a title="91-lsi-8" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-05-18-Announcing_TweetMotif_for_summarizing_twitter_topics.html">140 brendan oconnor ai-2009-05-18-Announcing TweetMotif for summarizing twitter topics</a></p>
<p>9 0.35792914 <a title="91-lsi-9" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-10-16-Is_religion_the_opiate_of_the_elite%3F.html">120 brendan oconnor ai-2008-10-16-Is religion the opiate of the elite?</a></p>
<p>10 0.33458081 <a title="91-lsi-10" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-10-08-MyDebates.org%2C_online_polling%2C_and_potentially_the_coolest_question_corpus_ever.html">116 brendan oconnor ai-2008-10-08-MyDebates.org, online polling, and potentially the coolest question corpus ever</a></p>
<p>11 0.32706851 <a title="91-lsi-11" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-08-12-Beautiful_Data_book_chapter.html">151 brendan oconnor ai-2009-08-12-Beautiful Data book chapter</a></p>
<p>12 0.31470436 <a title="91-lsi-12" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-04-21-What_inputs_do_Monte_Carlo_algorithms_need%3F.html">195 brendan oconnor ai-2013-04-21-What inputs do Monte Carlo algorithms need?</a></p>
<p>13 0.31022394 <a title="91-lsi-13" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-22-Performance_comparison%3A_key-value_stores_for_language_model_counts.html">139 brendan oconnor ai-2009-04-22-Performance comparison: key-value stores for language model counts</a></p>
<p>14 0.3073791 <a title="91-lsi-14" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-04-The_%2460%2C000_cat%3A_deep_belief_networks_make_less_sense_for_language_than_vision.html">184 brendan oconnor ai-2012-07-04-The $60,000 cat: deep belief networks make less sense for language than vision</a></p>
<p>15 0.29159451 <a title="91-lsi-15" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-07-15-Beta_conjugate_explorer.html">146 brendan oconnor ai-2009-07-15-Beta conjugate explorer</a></p>
<p>16 0.28750592 <a title="91-lsi-16" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-08-16-A_better_Obama_vs_McCain_poll_aggregation.html">111 brendan oconnor ai-2008-08-16-A better Obama vs McCain poll aggregation</a></p>
<p>17 0.27935451 <a title="91-lsi-17" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-08-21-Berkeley_SDA_and_the_General_Social_Survey.html">186 brendan oconnor ai-2012-08-21-Berkeley SDA and the General Social Survey</a></p>
<p>18 0.26836282 <a title="91-lsi-18" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-06-05-Clinton-Obama_support_visualization.html">105 brendan oconnor ai-2008-06-05-Clinton-Obama support visualization</a></p>
<p>19 0.26774201 <a title="91-lsi-19" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-08-20-Some_analysis_of_tweet_shares_and_%E2%80%9Cpredicting%E2%80%9D_election_outcomes.html">198 brendan oconnor ai-2013-08-20-Some analysis of tweet shares and “predicting” election outcomes</a></p>
<p>20 0.26762262 <a title="91-lsi-20" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-05-21-iPhone_autocorrection_error_analysis.html">170 brendan oconnor ai-2011-05-21-iPhone autocorrection error analysis</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/brendan_oconnor_ai_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(24, 0.779), (74, 0.098)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98716879 <a title="91-lda-1" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-01-27-Graphics%21_Atari_Breakout_and_religious_text_NLP.html">91 brendan oconnor ai-2008-01-27-Graphics! Atari Breakout and religious text NLP</a></p>
<p>Introduction: From a graphics/mod programming workshop,  modifications of “Breakout”  in awesome video form:
 
          
 
It uses  Processing , a framework designed for animation and graphicky things.  It was also used for the  Similar Diversity  visualization that maps out named entities and their common verbs across religious texts.  It’s pretty cool:     … so a name gets bigger with its frequency, I get that; but what governs the size of an arc connecting two names?  They say “common activities” — I’m guessing a distance measure on the terms in the contexts in which they appear?  I’d be most interested in a graph of their explicit interactions; something like a count of co-occurrences in a proximity window (word radius? verse? chapter? book?) might do the trick…
 
This came out a while ago, I think; here are some  related works  posted on infosthetics.com.</p><p>2 0.97629279 <a title="91-lda-2" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-07-31-war_death_statistics.html">22 brendan oconnor ai-2005-07-31-war death statistics</a></p>
<p>Introduction: What a project — an impressively painstaking compilation of  20th century civilian and military casualties .  Summary: lots of people were killed.  Interesting are  the comments on morality and how prejudgement leads to differing casualty estimations  — estimates vary wildly for controversial regimes, like Castro’s Cuba (I’ve heard lots about), but are suspiciously round and agreed-upon for incidents that scholars seem to care less about, like the Congo Crisis of the 1960′s (I’ve heard not so much about.)</p><p>3 0.83800209 <a title="91-lda-3" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-22-Performance_comparison%3A_key-value_stores_for_language_model_counts.html">139 brendan oconnor ai-2009-04-22-Performance comparison: key-value stores for language model counts</a></p>
<p>Introduction: I’m doing  word and bigram counts  on a corpus of tweets.  I want to store and rapidly retrieve them later for  language model  purposes.  So there’s a big table of counts that get incremented many times.  The easiest way to get something running is to use an open-source key/value store; but which?  There’s recently been some development in this area so I thought it would be good to revisit and evaluate some options.
 
Here are timings for a single counting process: iterate over 45,000 short text messages, tokenize them, then increment counters for their unigrams and bigrams.  (The speed of the data store is only one component of performance.)  There are about 17 increments per tweet: 400k unique terms and 750k total count.  This is substantially smaller than what I need, but it’s small enough to easily test.  I used several very different architectures and packages, explained below.
  
 
 architecture
  name
  speed
   
 in-memory, within-process
  python dictionary
   2700 tweets/sec</p><p>4 0.79597485 <a title="91-lda-4" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-02-23-Comparison_of_data_analysis_packages%3A_R%2C_Matlab%2C_SciPy%2C_Excel%2C_SAS%2C_SPSS%2C_Stata.html">135 brendan oconnor ai-2009-02-23-Comparison of data analysis packages: R, Matlab, SciPy, Excel, SAS, SPSS, Stata</a></p>
<p>Introduction: Lukas  and I were trying to write a succinct comparison of the most popular packages that are typically used for data analysis.  I think most people choose one based on what people around them use or what they learn in school, so I’ve found it hard to find comparative information.  I’m posting the table here in hopes of useful comments.
  
 
 
  Name  
  Advantages  
  Disadvantages  
  Open source?  
  Typical   users  
 
 
 R 
 Library support; visualization 
 Steep learning curve 
 Yes 
 Finance; Statistics 
 
 
 Matlab 
 Elegant matrix support; visualization 
 Expensive; incomplete statistics support 
 No 
 Engineering 
 
 
 SciPy/NumPy/Matplotlib 
 Python (general-purpose programming language) 
 Immature 
 Yes 
 Engineering 
 
 
 Excel 
 Easy; visual; flexible 
 Large datasets 
 No 
 Business 
 
 
 SAS 
 Large datasets 
 Expensive; outdated programming language 
 No 
 Business; Government 
 
 
 Stata 
 Easy statistical analysis 
  
 No 
 Science 
 
 
 SPSS 
 Like Stata but more ex</p><p>5 0.27602103 <a title="91-lda-5" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-01-Binary_classification_evaluation_in_R_via_ROCR.html">136 brendan oconnor ai-2009-04-01-Binary classification evaluation in R via ROCR</a></p>
<p>Introduction: A binary classifier makes decisions with confidence levels.  Usually it’s imperfect: if you put a decision threshold anywhere, items will fall on the wrong side — errors.  I made  this a diagram  a while ago for Turker voting; same principle applies for any binary classifier.
 
     
 
So there are a zillion ways to evaluate a binary classifier.  Accuracy?  Accuracy on different item types (sens, spec)?  Accuracy on different classifier decisions (prec, npv)?  And worse, over the years every field has given these metrics different names.  Signal detection, bioinformatics, medicine, statistics, machine learning, and more I’m sure.  But in R, there’s the excellent  ROCR package  to compute and visualize all the different metrics.
 
I wanted to have a small, easy-to-use function that calls ROCR and reports the basic information I’m interested in.  For  preds , a vector of predictions (as confidence scores), and  labels , the true labels for the instances, it works like this:
 
>  binary_e</p><p>6 0.26738685 <a title="91-lda-6" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-26-Seeing_how_%E2%80%9Cart%E2%80%9D_and_%E2%80%9Cpharmaceuticals%E2%80%9D_are_linguistically_similar_in_web_text.html">156 brendan oconnor ai-2009-09-26-Seeing how “art” and “pharmaceuticals” are linguistically similar in web text</a></p>
<p>7 0.2493238 <a title="91-lda-7" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-11-21-academic_blogging.html">29 brendan oconnor ai-2005-11-21-academic blogging</a></p>
<p>8 0.24174455 <a title="91-lda-8" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-08-16-A_better_Obama_vs_McCain_poll_aggregation.html">111 brendan oconnor ai-2008-08-16-A better Obama vs McCain poll aggregation</a></p>
<p>9 0.23887429 <a title="91-lda-9" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-05-18-Announcing_TweetMotif_for_summarizing_twitter_topics.html">140 brendan oconnor ai-2009-05-18-Announcing TweetMotif for summarizing twitter topics</a></p>
<p>10 0.23648368 <a title="91-lda-10" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-11-24-Graphs_for_SANCL-2012_web_parsing_results.html">189 brendan oconnor ai-2012-11-24-Graphs for SANCL-2012 web parsing results</a></p>
<p>11 0.23410687 <a title="91-lda-11" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-17-1_billion_web_page_dataset_from_CMU.html">138 brendan oconnor ai-2009-04-17-1 billion web page dataset from CMU</a></p>
<p>12 0.22864687 <a title="91-lda-12" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-12-20-Data-driven_charity.html">86 brendan oconnor ai-2007-12-20-Data-driven charity</a></p>
<p>13 0.22722769 <a title="91-lda-13" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-01-20-Moral_psychology_on_Amazon_Mechanical_Turk.html">90 brendan oconnor ai-2008-01-20-Moral psychology on Amazon Mechanical Turk</a></p>
<p>14 0.22403619 <a title="91-lda-14" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-03-Statistics_vs._Machine_Learning%2C_fight%21.html">129 brendan oconnor ai-2008-12-03-Statistics vs. Machine Learning, fight!</a></p>
<p>15 0.22257596 <a title="91-lda-15" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-02-02-Histograms_%E2%80%94_matplotlib_vs._R.html">179 brendan oconnor ai-2012-02-02-Histograms — matplotlib vs. R</a></p>
<p>16 0.21876653 <a title="91-lda-16" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-09-19-End-to-end_NLP_packages.html">174 brendan oconnor ai-2011-09-19-End-to-end NLP packages</a></p>
<p>17 0.21374811 <a title="91-lda-17" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-12-31-List_of_probabilistic_model_mini-language_toolkits.html">157 brendan oconnor ai-2009-12-31-List of probabilistic model mini-language toolkits</a></p>
<p>18 0.20883143 <a title="91-lda-18" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-10-Don%E2%80%99t_MAWK_AWK_%E2%80%93_the_fastest_and_most_elegant_big_data_munging_language%21.html">154 brendan oconnor ai-2009-09-10-Don’t MAWK AWK – the fastest and most elegant big data munging language!</a></p>
<p>19 0.20434415 <a title="91-lda-19" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-03-27-Seth_Roberts_and_academic_blogging.html">55 brendan oconnor ai-2007-03-27-Seth Roberts and academic blogging</a></p>
<p>20 0.20408291 <a title="91-lda-20" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-01-23-SF_conference_for_data_mining_mercenaries.html">133 brendan oconnor ai-2009-01-23-SF conference for data mining mercenaries</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
