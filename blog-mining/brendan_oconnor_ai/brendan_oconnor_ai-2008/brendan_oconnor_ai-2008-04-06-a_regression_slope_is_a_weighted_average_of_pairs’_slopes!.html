<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>100 brendan oconnor ai-2008-04-06-a regression slope is a weighted average of pairs’ slopes!</title>
</head>

<body>
<p><a title="brendan_oconnor_ai" href="../brendan_oconnor_ai_home.html">brendan_oconnor_ai</a> <a title="brendan_oconnor_ai-2008" href="../home/brendan_oconnor_ai-2008_home.html">brendan_oconnor_ai-2008</a> <a title="brendan_oconnor_ai-2008-100" href="#">brendan_oconnor_ai-2008-100</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>100 brendan oconnor ai-2008-04-06-a regression slope is a weighted average of pairs’ slopes!</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="brendan_oconnor_ai-2008-100-html" href="http://brenocon.com/blog/2008/04/a-regression-slope-is-a-weighted-average-of-pairs-slopes/">html</a></p><p>Introduction: Wow, this is pretty cool:
 
 
 
From an  Andrew Gelman article  on summaring a linear regression as a simple difference between upper and lower categories.  I get the impression there are lots of weird misunderstood corners of linear models… (e.g. that “least squares regression” is a maximum likelihood estimator for a linear model with normal noise… I know so many people who didn’t learn that from their stats whatever course, and therefore find it mystifying why squared error should be used…  see this  other post from Gelman .)</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Wow, this is pretty cool:       From an  Andrew Gelman article  on summaring a linear regression as a simple difference between upper and lower categories. [sent-1, score-1.438]
</p><p>2 I get the impression there are lots of weird misunderstood corners of linear models… (e. [sent-2, score-1.05]
</p><p>3 that “least squares regression” is a maximum likelihood estimator for a linear model with normal noise… I know so many people who didn’t learn that from their stats whatever course, and therefore find it mystifying why squared error should be used…  see this  other post from Gelman . [sent-4, score-2.392]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('linear', 0.375), ('gelman', 0.329), ('regression', 0.264), ('upper', 0.216), ('wow', 0.216), ('estimator', 0.216), ('maximum', 0.216), ('misunderstood', 0.216), ('likelihood', 0.183), ('impression', 0.183), ('stats', 0.183), ('noise', 0.173), ('lower', 0.164), ('normal', 0.151), ('andrew', 0.151), ('difference', 0.145), ('weird', 0.145), ('whatever', 0.145), ('error', 0.141), ('therefore', 0.125), ('cool', 0.116), ('simple', 0.113), ('learn', 0.111), ('didn', 0.108), ('models', 0.106), ('model', 0.096), ('post', 0.091), ('find', 0.089), ('course', 0.085), ('least', 0.085), ('article', 0.083), ('know', 0.081), ('used', 0.078), ('pretty', 0.078), ('lots', 0.078), ('many', 0.074), ('people', 0.06), ('see', 0.055), ('get', 0.053)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="100-tfidf-1" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-04-06-a_regression_slope_is_a_weighted_average_of_pairs%E2%80%99_slopes%21.html">100 brendan oconnor ai-2008-04-06-a regression slope is a weighted average of pairs’ slopes!</a></p>
<p>Introduction: Wow, this is pretty cool:
 
 
 
From an  Andrew Gelman article  on summaring a linear regression as a simple difference between upper and lower categories.  I get the impression there are lots of weird misunderstood corners of linear models… (e.g. that “least squares regression” is a maximum likelihood estimator for a linear model with normal noise… I know so many people who didn’t learn that from their stats whatever course, and therefore find it mystifying why squared error should be used…  see this  other post from Gelman .)</p><p>2 0.2160413 <a title="100-tfidf-2" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-07-01-Bias_correction_sneak_peek%21.html">108 brendan oconnor ai-2008-07-01-Bias correction sneak peek!</a></p>
<p>Introduction: (Update 10/2008: actually this model doesn’t work in all cases.Â   In the final paper  we use an (even) simpler model.)
 
I really don’t have time to write up an explanation for what this is so I’ll just post the graph instead.  Each box is a scatterplot of an AMT worker’s responses versus a gold standard.  Drawn are attempts to fit linear models to each worker.  The idea is to correct for the biases of each worker.  With a linear model y ~ ax+b, the correction is correction(y) = (y-b)/a.  Arrows show such corrections.  Hilariously bad “corrections” happen.  *But*, there is also weighting: to get the “correct” answer (maximum likelihood) from several workers, you weight by a^2/stddev^2.  Despite the sometimes odd corrections, the cross-validated results from this model correlate better with the gold than the raw averaging of workers.  (Raw averaging is the maximum likelihood solution for a fixed noise model: a=1, b=0, and each worker’s variance is equal).
 
Much better explanation is c</p><p>3 0.15345179 <a title="100-tfidf-3" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-01-11-Please_report_your_SVM%E2%80%99s_kernel%21.html">164 brendan oconnor ai-2011-01-11-Please report your SVM’s kernel!</a></p>
<p>Introduction: I’m tired of reading papers that use an  SVM  but don’t say which kernel they used.  (There’s tons of such papers in NLP and, I think, other areas that do applied machine learning.)  I suspect a lot of these papers are actually using a linear kernel.
 
An un-kernelized, linear SVM is nearly the same as  logistic regression  — every feature independently increases or decreases the classifier’s output prediction.  But a quadratic kernelized SVM is much more like boosted depth-2 decision trees.  It can do automatic combinations of pairs of features — a potentially very different thing, since you can start throwing in features that don’t do anything on their own but might have useful interactions with others.  (And of course, more complicated kernels do progressively more complicated and non-linear things.)
 
I have heard people say they download an SVM package, try a bunch of different kernels, and find the linear kernel is the best. In such cases they could have just used a logistic regr</p><p>4 0.11961221 <a title="100-tfidf-4" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-03-02-Poor_man%E2%80%99s_linear_algebra_textbook.html">166 brendan oconnor ai-2011-03-02-Poor man’s linear algebra textbook</a></p>
<p>Introduction: I keep learning new bits of linear algebra all the time, but I’m always hurting for a useful reference.  I probably should get a good book (which?), but in the meantime I’m collecting several nice online sources that ML researchers seem to often recommend: The Matrix Cookbook, plus a few more tutorial/introductory pieces, aimed at an intermediate-ish level.
 
Main reference:
  
  The Matrix Cookbook  – 71 pages of identities and such.  This seems to be really popular. 
  
Tutorials/introductions:
  
  CS229 linear algebra review  – from Stanford’s ML course.  It seems to introduce all the essentials, and it’s vaguely familiar for me.  (26 pages) 
 Minka’s  Old and New Matrix Algebra Useful for Statistics  – has a great part on how to do derivatives.  (19 pages) 
 MacKay’s  The Humble Gaussian  – OK, not really pure linear algebra anymore, but quite enlightening.  (12 pages) 
  
After studying for this last stats/ML midterm, I’ve now printed them out and stuck them in a binder.  A poor</p><p>5 0.11790369 <a title="100-tfidf-5" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-03-Statistics_vs._Machine_Learning%2C_fight%21.html">129 brendan oconnor ai-2008-12-03-Statistics vs. Machine Learning, fight!</a></p>
<p>Introduction: 10/1/09 update  — well, it’s been nearly a year, and I should say not everything in this rant is totally true, and I certainly believe much less of it now.  Current take:  Statistics , not machine learning, is the real deal, but unfortunately suffers from bad marketing.  On the other hand, to the extent that bad marketing includes misguided undergraduate curriculums, there’s plenty of room to improve for everyone.
   
So it’s pretty clear by now that statistics and machine learning aren’t very different fields. I was recently pointed to   a very amusing comparison   by the excellent statistician — and machine learning expert —   Robert Tibshiriani  .  Reproduced here:  
 
  Glossary 
   
 Machine learning
  Statistics 
   
 network, graphs
  model 
   
 weights
  parameters 
   
 learning
  fitting 
   
 generalization
  test set performance 
   
 supervised learning
  regression/classiﬁcation 
   
 unsupervised learning
  density estimation, clustering 
   
 large grant = $1,000,000</p><p>6 0.087547988 <a title="100-tfidf-6" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-03-27-Seth_Roberts_and_academic_blogging.html">55 brendan oconnor ai-2007-03-27-Seth Roberts and academic blogging</a></p>
<p>7 0.085355848 <a title="100-tfidf-7" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-11-11-Memorizing_small_tables.html">177 brendan oconnor ai-2011-11-11-Memorizing small tables</a></p>
<p>8 0.080754511 <a title="100-tfidf-8" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-17-p-values%2C_CDF%E2%80%99s%2C_NLP_etc..html">185 brendan oconnor ai-2012-07-17-p-values, CDF’s, NLP etc.</a></p>
<p>9 0.079729378 <a title="100-tfidf-9" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-05-20-Log-normal_and_logistic-normal_terminology.html">169 brendan oconnor ai-2011-05-20-Log-normal and logistic-normal terminology</a></p>
<p>10 0.078151464 <a title="100-tfidf-10" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-08-12-Beautiful_Data_book_chapter.html">151 brendan oconnor ai-2009-08-12-Beautiful Data book chapter</a></p>
<p>11 0.077098399 <a title="100-tfidf-11" href="../brendan_oconnor_ai-2010/brendan_oconnor_ai-2010-08-09-An_ML-AI_approach_to_P_%21%3D_NP.html">161 brendan oconnor ai-2010-08-09-An ML-AI approach to P != NP</a></p>
<p>12 0.067109667 <a title="100-tfidf-12" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-11-24-Graphs_for_SANCL-2012_web_parsing_results.html">189 brendan oconnor ai-2012-11-24-Graphs for SANCL-2012 web parsing results</a></p>
<p>13 0.062830344 <a title="100-tfidf-13" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-10-16-Is_religion_the_opiate_of_the_elite%3F.html">120 brendan oconnor ai-2008-10-16-Is religion the opiate of the elite?</a></p>
<p>14 0.061302111 <a title="100-tfidf-14" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-03-29-Allende%E2%80%99s_cybernetic_economy_project.html">98 brendan oconnor ai-2008-03-29-Allende’s cybernetic economy project</a></p>
<p>15 0.057917975 <a title="100-tfidf-15" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-03-09-I_don%E2%80%99t_get_this_web_parsing_shared_task.html">181 brendan oconnor ai-2012-03-09-I don’t get this web parsing shared task</a></p>
<p>16 0.05452263 <a title="100-tfidf-16" href="../brendan_oconnor_ai-2014/brendan_oconnor_ai-2014-04-26-Replot%3A_departure_delays_vs_flight_time_speed-up.html">204 brendan oconnor ai-2014-04-26-Replot: departure delays vs flight time speed-up</a></p>
<p>17 0.053603232 <a title="100-tfidf-17" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-04-08-More_fun_with_Gapminder_-_Trendalyzer.html">58 brendan oconnor ai-2007-04-08-More fun with Gapminder - Trendalyzer</a></p>
<p>18 0.04801555 <a title="100-tfidf-18" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-02-23-Comparison_of_data_analysis_packages%3A_R%2C_Matlab%2C_SciPy%2C_Excel%2C_SAS%2C_SPSS%2C_Stata.html">135 brendan oconnor ai-2009-02-23-Comparison of data analysis packages: R, Matlab, SciPy, Excel, SAS, SPSS, Stata</a></p>
<p>19 0.04677825 <a title="100-tfidf-19" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-08-20-Some_analysis_of_tweet_shares_and_%E2%80%9Cpredicting%E2%80%9D_election_outcomes.html">198 brendan oconnor ai-2013-08-20-Some analysis of tweet shares and “predicting” election outcomes</a></p>
<p>20 0.046609845 <a title="100-tfidf-20" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-07-09-the_psychology_of_design_as_explanation.html">19 brendan oconnor ai-2005-07-09-the psychology of design as explanation</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/brendan_oconnor_ai_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, -0.166), (1, -0.084), (2, 0.121), (3, -0.104), (4, 0.077), (5, 0.118), (6, -0.083), (7, -0.013), (8, 0.024), (9, 0.07), (10, 0.042), (11, -0.072), (12, 0.063), (13, -0.093), (14, -0.309), (15, -0.021), (16, -0.098), (17, 0.203), (18, 0.07), (19, -0.017), (20, 0.029), (21, 0.208), (22, -0.034), (23, 0.264), (24, 0.166), (25, 0.12), (26, -0.065), (27, -0.062), (28, -0.144), (29, 0.046), (30, -0.075), (31, -0.086), (32, 0.005), (33, -0.016), (34, 0.02), (35, -0.01), (36, -0.011), (37, -0.054), (38, -0.075), (39, 0.074), (40, 0.061), (41, -0.037), (42, 0.023), (43, -0.045), (44, -0.058), (45, 0.053), (46, -0.03), (47, 0.011), (48, 0.01), (49, -0.031)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9966265 <a title="100-lsi-1" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-04-06-a_regression_slope_is_a_weighted_average_of_pairs%E2%80%99_slopes%21.html">100 brendan oconnor ai-2008-04-06-a regression slope is a weighted average of pairs’ slopes!</a></p>
<p>Introduction: Wow, this is pretty cool:
 
 
 
From an  Andrew Gelman article  on summaring a linear regression as a simple difference between upper and lower categories.  I get the impression there are lots of weird misunderstood corners of linear models… (e.g. that “least squares regression” is a maximum likelihood estimator for a linear model with normal noise… I know so many people who didn’t learn that from their stats whatever course, and therefore find it mystifying why squared error should be used…  see this  other post from Gelman .)</p><p>2 0.74171156 <a title="100-lsi-2" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-07-01-Bias_correction_sneak_peek%21.html">108 brendan oconnor ai-2008-07-01-Bias correction sneak peek!</a></p>
<p>Introduction: (Update 10/2008: actually this model doesn’t work in all cases.Â   In the final paper  we use an (even) simpler model.)
 
I really don’t have time to write up an explanation for what this is so I’ll just post the graph instead.  Each box is a scatterplot of an AMT worker’s responses versus a gold standard.  Drawn are attempts to fit linear models to each worker.  The idea is to correct for the biases of each worker.  With a linear model y ~ ax+b, the correction is correction(y) = (y-b)/a.  Arrows show such corrections.  Hilariously bad “corrections” happen.  *But*, there is also weighting: to get the “correct” answer (maximum likelihood) from several workers, you weight by a^2/stddev^2.  Despite the sometimes odd corrections, the cross-validated results from this model correlate better with the gold than the raw averaging of workers.  (Raw averaging is the maximum likelihood solution for a fixed noise model: a=1, b=0, and each worker’s variance is equal).
 
Much better explanation is c</p><p>3 0.70209074 <a title="100-lsi-3" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-01-11-Please_report_your_SVM%E2%80%99s_kernel%21.html">164 brendan oconnor ai-2011-01-11-Please report your SVM’s kernel!</a></p>
<p>Introduction: I’m tired of reading papers that use an  SVM  but don’t say which kernel they used.  (There’s tons of such papers in NLP and, I think, other areas that do applied machine learning.)  I suspect a lot of these papers are actually using a linear kernel.
 
An un-kernelized, linear SVM is nearly the same as  logistic regression  — every feature independently increases or decreases the classifier’s output prediction.  But a quadratic kernelized SVM is much more like boosted depth-2 decision trees.  It can do automatic combinations of pairs of features — a potentially very different thing, since you can start throwing in features that don’t do anything on their own but might have useful interactions with others.  (And of course, more complicated kernels do progressively more complicated and non-linear things.)
 
I have heard people say they download an SVM package, try a bunch of different kernels, and find the linear kernel is the best. In such cases they could have just used a logistic regr</p><p>4 0.64492148 <a title="100-lsi-4" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-03-02-Poor_man%E2%80%99s_linear_algebra_textbook.html">166 brendan oconnor ai-2011-03-02-Poor man’s linear algebra textbook</a></p>
<p>Introduction: I keep learning new bits of linear algebra all the time, but I’m always hurting for a useful reference.  I probably should get a good book (which?), but in the meantime I’m collecting several nice online sources that ML researchers seem to often recommend: The Matrix Cookbook, plus a few more tutorial/introductory pieces, aimed at an intermediate-ish level.
 
Main reference:
  
  The Matrix Cookbook  – 71 pages of identities and such.  This seems to be really popular. 
  
Tutorials/introductions:
  
  CS229 linear algebra review  – from Stanford’s ML course.  It seems to introduce all the essentials, and it’s vaguely familiar for me.  (26 pages) 
 Minka’s  Old and New Matrix Algebra Useful for Statistics  – has a great part on how to do derivatives.  (19 pages) 
 MacKay’s  The Humble Gaussian  – OK, not really pure linear algebra anymore, but quite enlightening.  (12 pages) 
  
After studying for this last stats/ML midterm, I’ve now printed them out and stuck them in a binder.  A poor</p><p>5 0.44813234 <a title="100-lsi-5" href="../brendan_oconnor_ai-2014/brendan_oconnor_ai-2014-04-26-Replot%3A_departure_delays_vs_flight_time_speed-up.html">204 brendan oconnor ai-2014-04-26-Replot: departure delays vs flight time speed-up</a></p>
<p>Introduction: Here’s a re-plotting of a graph in  this 538 post .  It’s looking at whether pilots speed up the flight when there’s a delay, and find that it looks like that’s the case.  This is averaged data for flights on several major transcontinental routes.
 
I’ve replotted the main graph as follows.  The x-axis is departure delay.  The y-axis is the total trip time — number of minutes since the scheduled departure time.  For an on-time departure, the average flight is 5 hours, 44 minutes.  The blue line shows what the total trip time would be if the delayed flight took that long.  Gray lines are uncertainty (I think the CI due to averaging).
 
   
 
What’s going on is, the pilots seem to be targeting a total trip time of 370-380 minutes or so.  If the departure is only slightly delayed by 10 minutes, the flight time is still the same, but delays in the 30-50 minutes range see a faster flight time which makes up for some of the delay.
 
The original post plotted the y-axis as the delta against t</p><p>6 0.42042962 <a title="100-lsi-6" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-11-11-Memorizing_small_tables.html">177 brendan oconnor ai-2011-11-11-Memorizing small tables</a></p>
<p>7 0.34418422 <a title="100-lsi-7" href="../brendan_oconnor_ai-2010/brendan_oconnor_ai-2010-08-09-An_ML-AI_approach_to_P_%21%3D_NP.html">161 brendan oconnor ai-2010-08-09-An ML-AI approach to P != NP</a></p>
<p>8 0.32841489 <a title="100-lsi-8" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-04-08-More_fun_with_Gapminder_-_Trendalyzer.html">58 brendan oconnor ai-2007-04-08-More fun with Gapminder - Trendalyzer</a></p>
<p>9 0.31722882 <a title="100-lsi-9" href="../brendan_oconnor_ai-2014/brendan_oconnor_ai-2014-02-18-Scatterplot_of_KN-PYP_language_model_results.html">202 brendan oconnor ai-2014-02-18-Scatterplot of KN-PYP language model results</a></p>
<p>10 0.31452945 <a title="100-lsi-10" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-03-Statistics_vs._Machine_Learning%2C_fight%21.html">129 brendan oconnor ai-2008-12-03-Statistics vs. Machine Learning, fight!</a></p>
<p>11 0.29886654 <a title="100-lsi-11" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-03-27-Seth_Roberts_and_academic_blogging.html">55 brendan oconnor ai-2007-03-27-Seth Roberts and academic blogging</a></p>
<p>12 0.29661268 <a title="100-lsi-12" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-05-20-Log-normal_and_logistic-normal_terminology.html">169 brendan oconnor ai-2011-05-20-Log-normal and logistic-normal terminology</a></p>
<p>13 0.29513565 <a title="100-lsi-13" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-08-12-Beautiful_Data_book_chapter.html">151 brendan oconnor ai-2009-08-12-Beautiful Data book chapter</a></p>
<p>14 0.29375404 <a title="100-lsi-14" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-10-31-tanh_is_a_rescaled_logistic_sigmoid_function.html">201 brendan oconnor ai-2013-10-31-tanh is a rescaled logistic sigmoid function</a></p>
<p>15 0.28099993 <a title="100-lsi-15" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-10-16-Is_religion_the_opiate_of_the_elite%3F.html">120 brendan oconnor ai-2008-10-16-Is religion the opiate of the elite?</a></p>
<p>16 0.23920569 <a title="100-lsi-16" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-17-p-values%2C_CDF%E2%80%99s%2C_NLP_etc..html">185 brendan oconnor ai-2012-07-17-p-values, CDF’s, NLP etc.</a></p>
<p>17 0.23424448 <a title="100-lsi-17" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-07-01-Modelling_environmentalism_thinking.html">11 brendan oconnor ai-2005-07-01-Modelling environmentalism thinking</a></p>
<p>18 0.2292708 <a title="100-lsi-18" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-12-31-List_of_probabilistic_model_mini-language_toolkits.html">157 brendan oconnor ai-2009-12-31-List of probabilistic model mini-language toolkits</a></p>
<p>19 0.22611204 <a title="100-lsi-19" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-08-20-Some_analysis_of_tweet_shares_and_%E2%80%9Cpredicting%E2%80%9D_election_outcomes.html">198 brendan oconnor ai-2013-08-20-Some analysis of tweet shares and “predicting” election outcomes</a></p>
<p>20 0.22422765 <a title="100-lsi-20" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-11-24-Graphs_for_SANCL-2012_web_parsing_results.html">189 brendan oconnor ai-2012-11-24-Graphs for SANCL-2012 web parsing results</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/brendan_oconnor_ai_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(44, 0.131), (55, 0.643), (74, 0.078)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.99808544 <a title="100-lda-1" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-07-23-R_questions_on_StackOverflow.html">148 brendan oconnor ai-2009-07-23-R questions on StackOverflow</a></p>
<p>Introduction: R is notoriously hard to learn, but there was just an effort  [1]   [2]  to populate the programming question-and-answer website  StackOverflow with content for the R language .
 
Amusingly, one of the most useful intro questions is:  How to search for “R” materials? 
 
 Mike Driscoll  (who organized an in-person conference event to get this bootstrapped) pointed out that in many ways StackOverflow is a nicer forum for help than a mailing list.  (i.e. the impressive but hard-to-approach  R-help .)  It’s more organized, easier to browse, and repetition and wrong answers can get downvoted.  (And  more thoughts from John Cook .)</p><p>2 0.99734885 <a title="100-lda-2" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-04-08-Rough_binomial_confidence_intervals.html">167 brendan oconnor ai-2011-04-08-Rough binomial confidence intervals</a></p>
<p>Introduction: I made this table a while ago and find it handy: for example, looking at a table of percentages and trying to figure out what’s meaningful or not. Why run a test if you can estimate it in your head?
 
 
 
References:Â  Wikipedia ,Â  binom.test</p><p>same-blog 3 0.98671269 <a title="100-lda-3" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-04-06-a_regression_slope_is_a_weighted_average_of_pairs%E2%80%99_slopes%21.html">100 brendan oconnor ai-2008-04-06-a regression slope is a weighted average of pairs’ slopes!</a></p>
<p>Introduction: Wow, this is pretty cool:
 
 
 
From an  Andrew Gelman article  on summaring a linear regression as a simple difference between upper and lower categories.  I get the impression there are lots of weird misunderstood corners of linear models… (e.g. that “least squares regression” is a maximum likelihood estimator for a linear model with normal noise… I know so many people who didn’t learn that from their stats whatever course, and therefore find it mystifying why squared error should be used…  see this  other post from Gelman .)</p><p>4 0.9761759 <a title="100-lda-4" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-08-25-Fukuyama%3A_Authoritarianism_is_still_against_history.html">112 brendan oconnor ai-2008-08-25-Fukuyama: Authoritarianism is still against history</a></p>
<p>Introduction: The latest on the world ideologies front –
 
In the light of Russia’s Georgia adventures, there’s been lots of talk whether this represents a new rise of authoritarian Russia, which is presumably another nail in the coffin for U.S.-led liberal democratic hegemony in the world.  Our “end of history” friend Francis Fukuyama just wrote an op-ed arguing that  Russia and China are still not big threats to liberal democracy .  There are some good points: Russia is behaving as an aggressive imperial power, but does not embrace a grand, exportable ideology with universal appeal.  Similarly with China.  They both still feel the need to pay lip service to democratic rituals and norms.  Even Nicholas Kristof’s hilarious column  chronicling his experience with China’s dubious protest registration system  concludes that even a pale mockery of democracy is progress.
 
I still like Azar Gat’s article which I wrote about last year, that  Russia and China represent authoritarian capitalism, which will</p><p>5 0.84827852 <a title="100-lda-5" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-10-05-Be_careful_with_dictionary-based_text_analysis.html">176 brendan oconnor ai-2011-10-05-Be careful with dictionary-based text analysis</a></p>
<p>Introduction: OK, everyone loves to run dictionary methods for sentiment and other text analysis — counting words from a predefined lexicon in a big corpus, in order to explore or test hypotheses about the corpus.  In particular, this is often done for sentiment analysis: count positive and negative words (according to a sentiment polarity lexicon, which was derived from human raters or previous researchers’ intuitions), and then proclaim the output yields sentiment levels of the documents.  More and more papers come out every day that do this.   I’ve done this myself.   It’s interesting and fun, but it’s easy to get a bunch of meaningless numbers if you don’t carefully validate what’s going on.  There are certainly good studies in this area that do further validation and analysis, but it’s hard to trust a study that just presents a graph with a few overly strong speculative claims as to its meaning.  This happens more than it ought to.
 
I was happy to see a similarly critical view in a nice workin</p><p>6 0.53667283 <a title="100-lda-6" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-11-13-Authoritarian_great_power_capitalism.html">81 brendan oconnor ai-2007-11-13-Authoritarian great power capitalism</a></p>
<p>7 0.326603 <a title="100-lda-7" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-12-20-Data-driven_charity.html">86 brendan oconnor ai-2007-12-20-Data-driven charity</a></p>
<p>8 0.31205195 <a title="100-lda-8" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-03-Statistics_vs._Machine_Learning%2C_fight%21.html">129 brendan oconnor ai-2008-12-03-Statistics vs. Machine Learning, fight!</a></p>
<p>9 0.29917082 <a title="100-lda-9" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-03-29-Allende%E2%80%99s_cybernetic_economy_project.html">98 brendan oconnor ai-2008-03-29-Allende’s cybernetic economy project</a></p>
<p>10 0.29471564 <a title="100-lda-10" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-03-27-Seth_Roberts_and_academic_blogging.html">55 brendan oconnor ai-2007-03-27-Seth Roberts and academic blogging</a></p>
<p>11 0.2871052 <a title="100-lda-11" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-17-p-values%2C_CDF%E2%80%99s%2C_NLP_etc..html">185 brendan oconnor ai-2012-07-17-p-values, CDF’s, NLP etc.</a></p>
<p>12 0.27073351 <a title="100-lda-12" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-08-20-Some_analysis_of_tweet_shares_and_%E2%80%9Cpredicting%E2%80%9D_election_outcomes.html">198 brendan oconnor ai-2013-08-20-Some analysis of tweet shares and “predicting” election outcomes</a></p>
<p>13 0.26326776 <a title="100-lda-13" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-04-The_%2460%2C000_cat%3A_deep_belief_networks_make_less_sense_for_language_than_vision.html">184 brendan oconnor ai-2012-07-04-The $60,000 cat: deep belief networks make less sense for language than vision</a></p>
<p>14 0.26326066 <a title="100-lda-14" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-10-Don%E2%80%99t_MAWK_AWK_%E2%80%93_the_fastest_and_most_elegant_big_data_munging_language%21.html">154 brendan oconnor ai-2009-09-10-Don’t MAWK AWK – the fastest and most elegant big data munging language!</a></p>
<p>15 0.26116008 <a title="100-lda-15" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-08-08-Haghighi_and_Klein_%282009%29%3A_Simple_Coreference_Resolution_with_Rich_Syntactic_and_Semantic_Features.html">150 brendan oconnor ai-2009-08-08-Haghighi and Klein (2009): Simple Coreference Resolution with Rich Syntactic and Semantic Features</a></p>
<p>16 0.25730148 <a title="100-lda-16" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-17-1_billion_web_page_dataset_from_CMU.html">138 brendan oconnor ai-2009-04-17-1 billion web page dataset from CMU</a></p>
<p>17 0.25315219 <a title="100-lda-17" href="../brendan_oconnor_ai-2010/brendan_oconnor_ai-2010-03-31-How_Facebook_privacy_failed_me.html">158 brendan oconnor ai-2010-03-31-How Facebook privacy failed me</a></p>
<p>18 0.25124437 <a title="100-lda-18" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-27-Facebook_sentiment_mining_predicts_presidential_polls.html">131 brendan oconnor ai-2008-12-27-Facebook sentiment mining predicts presidential polls</a></p>
<p>19 0.25058812 <a title="100-lda-19" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-08-12-Beautiful_Data_book_chapter.html">151 brendan oconnor ai-2009-08-12-Beautiful Data book chapter</a></p>
<p>20 0.24995023 <a title="100-lda-20" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-09-13-Response_on_our_movie_personas_paper.html">200 brendan oconnor ai-2013-09-13-Response on our movie personas paper</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
