<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>157 brendan oconnor ai-2009-12-31-List of probabilistic model mini-language toolkits</title>
</head>

<body>
<p><a title="brendan_oconnor_ai" href="../brendan_oconnor_ai_home.html">brendan_oconnor_ai</a> <a title="brendan_oconnor_ai-2009" href="../home/brendan_oconnor_ai-2009_home.html">brendan_oconnor_ai-2009</a> <a title="brendan_oconnor_ai-2009-157" href="#">brendan_oconnor_ai-2009-157</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>157 brendan oconnor ai-2009-12-31-List of probabilistic model mini-language toolkits</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="brendan_oconnor_ai-2009-157-html" href="http://brenocon.com/blog/2009/12/list-of-probabilistic-model-mini-language-toolkits/">html</a></p><p>Introduction: There are an increasing number of systems that attempt to allow the user to specify a probabilistic model in a high-level language — for example, declare a (Bayesian) generative model as a hierarchy of various distributions — then automatically run training and inference algorithms on a data set.  Now, you could always learn a good math library, and implement every model from scratch, but the motivation for this approach is you’ll avoid doing lots of repetitive and error-prone programming.  I’m not yet convinced that any of them completely achieve this goal, but it would be great if they succeeded and we could use high-level frameworks for everything.
 
Everyone seems to know about only a few of them, so here’s a meager attempt to list together a bunch that can be freely downloaded.  There is one package that is far more mature and been around much longer than the rest, so let’s start with:
  
 

 BUGS  – Bayesian Inference under Gibbs Sampling.  Specify a generative model, then it doe</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 There are an increasing number of systems that attempt to allow the user to specify a probabilistic model in a high-level language — for example, declare a (Bayesian) generative model as a hierarchy of various distributions — then automatically run training and inference algorithms on a data set. [sent-1, score-1.045]
</p><p>2 Now, you could always learn a good math library, and implement every model from scratch, but the motivation for this approach is you’ll avoid doing lots of repetitive and error-prone programming. [sent-2, score-0.149]
</p><p>3 Specify a generative model, then it does inference with a Gibbs sampler, thus being able to handle a wide variety of different sorts of models. [sent-6, score-0.39]
</p><p>4 The original implementation, WinBUGS, is written in Delphi, a variant of Pascal (! [sent-13, score-0.227]
</p><p>5 Any new attempts to make something new should be compared against BUGS. [sent-17, score-0.192]
</p><p>6 Their languages all fall broadly into the category of probabilistic graphical models, but there are plenty of differences and specializations and assumptions that are a project in itself to understand. [sent-19, score-0.345]
</p><p>7 Factorie  focuses on factor graphs and discriminative undirected models. [sent-21, score-0.209]
</p><p>8 It compiles the Gibbs sampler to C, so it’s much faster. [sent-49, score-0.256]
</p><p>9 Alchemy  – an implementation of the Markov Logic Network formalism, an undirected graphical model over log-linear-weighted  first-order logic . [sent-55, score-0.432]
</p><p>10 So, unlike BUGS and the above systems, there are no customized probability distributions for anything; everything is a Boltzmann (log-linear) distribution. [sent-56, score-0.196]
</p><p>11 But in fairness, all these systems are slower than customized implementations. [sent-63, score-0.265]
</p><p>12 Dyna  is specialized for  dynamic programming . [sent-64, score-0.308]
</p><p>13 But I expect that means it can train and infer with models that the above would be hopeless to handle, since dynamic programming gives you big-O efficiency gains over more general algorithms. [sent-71, score-0.245]
</p><p>14 (But on the other hand, even dynamic programming can be too generic and slow compared to direct, customized implementations. [sent-72, score-0.511]
</p><p>15 )        BLOG  – first-order logic with probability, though a fairly different formalism than MLNs. [sent-74, score-0.252]
</p><p>16 An interesting axis of variation of all these is whether the model specification language is Turing-complete or not, and to what extent training and inference can be combined with external code. [sent-81, score-0.511]
</p><p>17 The modeling languages of the first three are embedded in general procedural programming languages (Scala, C#, and Scheme respectively). [sent-84, score-0.403]
</p><p>18 BUGS’ and HBC’s languages are essentially the same as standard probabilistic model notation, though BUGS is imperative. [sent-87, score-0.489]
</p><p>19 Compiles to Turing-complete:  HBC compiles to C, and Dyna compiles to C++, which are then intended to be hacked up and/or embedded in larger programs. [sent-89, score-0.58]
</p><p>20 Another interesting variation is to what extent the systems handle probabilistic relations. [sent-91, score-0.553]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bugs', 0.283), ('compiles', 0.256), ('dyna', 0.256), ('written', 0.227), ('alchemy', 0.213), ('hbc', 0.213), ('factorie', 0.171), ('probabilistic', 0.158), ('model', 0.149), ('dynamic', 0.148), ('logic', 0.141), ('systems', 0.137), ('focuses', 0.135), ('bayesian', 0.13), ('customized', 0.128), ('inference', 0.121), ('languages', 0.119), ('formalism', 0.111), ('generative', 0.111), ('turing', 0.102), ('programming', 0.097), ('variation', 0.095), ('gibbs', 0.095), ('handle', 0.095), ('openbugs', 0.085), ('prolog', 0.085), ('technically', 0.085), ('winbugs', 0.085), ('complete', 0.078), ('training', 0.078), ('undirected', 0.074), ('unknown', 0.074), ('mature', 0.074), ('specify', 0.074), ('slow', 0.074), ('seems', 0.071), ('anything', 0.07), ('list', 0.068), ('extent', 0.068), ('graphical', 0.068), ('embedded', 0.068), ('church', 0.068), ('distributions', 0.068), ('group', 0.064), ('compared', 0.064), ('new', 0.064), ('variety', 0.063), ('specialized', 0.063), ('pain', 0.063), ('essentially', 0.063)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="157-tfidf-1" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-12-31-List_of_probabilistic_model_mini-language_toolkits.html">157 brendan oconnor ai-2009-12-31-List of probabilistic model mini-language toolkits</a></p>
<p>Introduction: There are an increasing number of systems that attempt to allow the user to specify a probabilistic model in a high-level language — for example, declare a (Bayesian) generative model as a hierarchy of various distributions — then automatically run training and inference algorithms on a data set.  Now, you could always learn a good math library, and implement every model from scratch, but the motivation for this approach is you’ll avoid doing lots of repetitive and error-prone programming.  I’m not yet convinced that any of them completely achieve this goal, but it would be great if they succeeded and we could use high-level frameworks for everything.
 
Everyone seems to know about only a few of them, so here’s a meager attempt to list together a bunch that can be freely downloaded.  There is one package that is far more mature and been around much longer than the rest, so let’s start with:
  
 

 BUGS  – Bayesian Inference under Gibbs Sampling.  Specify a generative model, then it doe</p><p>2 0.13689505 <a title="157-tfidf-2" href="../brendan_oconnor_ai-2010/brendan_oconnor_ai-2010-08-09-An_ML-AI_approach_to_P_%21%3D_NP.html">161 brendan oconnor ai-2010-08-09-An ML-AI approach to P != NP</a></p>
<p>Introduction: Like everyone, I’ve been just starting to look at the new, tentative,  proof that P != NP  from Vinay Deolalikar.  After reading the intro, what’s most striking is that probabilistic graphical models and mathematical logic are at the core of the proof.  This feels like a machine learning and artificial intelligence-centric approach to me — very different from what you usually see in mainstream CS theory.  (Maybe I should feel good that in my undergrad I basically stopped studying normal math and spent all my time with this weird stuff instead!)
 
He devotes several chapters to an introduction to graphical models — Ising models, conditional independence, MRF’s, Hammersley-Clifford, and all that other stuff you see in  Koller and Friedman  or something — and then logic and model theory!  I’m impressed.</p><p>3 0.12823939 <a title="157-tfidf-3" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-03-21-Statistics_is_big-N_logic%3F.html">54 brendan oconnor ai-2007-03-21-Statistics is big-N logic?</a></p>
<p>Introduction: I think I believe one of these things, but I’m not quite sure. 
 Statistics is just like logic, except with uncertainty. 
 
This would be true if statistics is Bayesian statistics and you buy the Bayesian inductive logic story — add induction to propositional logic, via a conditional credibility operator, and the Cox axioms imply standard probability theory as a consequence.  (That is, probability theory is logic with uncertainty.  And then a good Bayesian thinks probability theory and statistics are the same.)  Links:  Jaynes’ explanation ;  SEP article ; also  Fitelson’s article .  (Though there are negative results; all I can think of right now is a  Halpern  article on Cox; and also interesting is  Halpern and Koller .)
 
Secondly, here is another statement.
  

Statistics is just like logic, except with a big N.

  
This is a more data-driven view — the world is full of things and they need to be described.  Logical rules can help you describe things, but you also have to deal wit</p><p>4 0.12729345 <a title="157-tfidf-4" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-07-01-Modelling_environmentalism_thinking.html">11 brendan oconnor ai-2005-07-01-Modelling environmentalism thinking</a></p>
<p>Introduction: It’s a human political belief model — based on Cyc!  I’m not sure logic represents how people think all that well, but seeing the formalization of ideology is fascinating.  And besides, the methodology of cognitive modelling is awesome.   The link:  
 Modeling How People Think About Sustainability 
 
David C. James, M. P. Aff
 
LBJ School of Public Affairs The University of Texas at Austin May 2005
 
First Reader: Lodis Rhodes  Second Reader: Chandler Stolp
 
How effectively can a computer model represent the belief systems of different people? How would one go about representing a belief system using formal logic? How would that ideology react to different scenarios related to sustainable development? The author constructs the Cyc Agent-Scenario (CAS) model as a way to investigate these questions. The CAS model is built on top of ResearchCyc, a knowledge base (KB) and logical inference engine. The model consists of two agents (Libertarian and Green) and two scenarios. The model simula</p><p>5 0.12048623 <a title="157-tfidf-5" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-10-Don%E2%80%99t_MAWK_AWK_%E2%80%93_the_fastest_and_most_elegant_big_data_munging_language%21.html">154 brendan oconnor ai-2009-09-10-Don’t MAWK AWK – the fastest and most elegant big data munging language!</a></p>
<p>Introduction: update 2012-10-25 : I’ve been informed there is a new maintainer for Mawk, who has probably fixed the bugs I’ve been seeing.
  

From: Gert Hulselmans   


[The bugs you have found are] indeed true with mawk v1.3.3 which comes standard with Debian/Ubuntu.  This version is almost not developed the last 10 years.


I now already use mawk v1.3.4 maintained by another developer (Thomas E. Dickey) 
for more than a year on huge datafiles (sometimes several GB).


The problems/wrong results I had with mawk v1.3.3 sometimes are gone. In his version, normally all open/known bugs are fixed.


This version can be downloaded from:  http://invisible-island.net/mawk/ 

     update 2010-04-30  : I have since found large datasets where mawk is buggy and gives the wrong result.  nawk seems safe.   When one of these newfangled   “Big Data”   sets comes your way, the very first thing you have to do is data munging: shuffling around file formats, renaming fields and the like.  Once you’re dealing with hun</p><p>6 0.098059848 <a title="157-tfidf-6" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-02-19-Move_to_brenocon.com.html">165 brendan oconnor ai-2011-02-19-Move to brenocon.com</a></p>
<p>7 0.095703647 <a title="157-tfidf-7" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-03-Statistics_vs._Machine_Learning%2C_fight%21.html">129 brendan oconnor ai-2008-12-03-Statistics vs. Machine Learning, fight!</a></p>
<p>8 0.082715139 <a title="157-tfidf-8" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-05-20-Log-normal_and_logistic-normal_terminology.html">169 brendan oconnor ai-2011-05-20-Log-normal and logistic-normal terminology</a></p>
<p>9 0.081563398 <a title="157-tfidf-9" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-09-19-End-to-end_NLP_packages.html">174 brendan oconnor ai-2011-09-19-End-to-end NLP packages</a></p>
<p>10 0.078899533 <a title="157-tfidf-10" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-11-24-Graphs_for_SANCL-2012_web_parsing_results.html">189 brendan oconnor ai-2012-11-24-Graphs for SANCL-2012 web parsing results</a></p>
<p>11 0.070410103 <a title="157-tfidf-11" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-02-23-Comparison_of_data_analysis_packages%3A_R%2C_Matlab%2C_SciPy%2C_Excel%2C_SAS%2C_SPSS%2C_Stata.html">135 brendan oconnor ai-2009-02-23-Comparison of data analysis packages: R, Matlab, SciPy, Excel, SAS, SPSS, Stata</a></p>
<p>12 0.070228279 <a title="157-tfidf-12" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-01-07-Love_it_and_hate_it%2C_R_has_come_of_age.html">132 brendan oconnor ai-2009-01-07-Love it and hate it, R has come of age</a></p>
<p>13 0.069196396 <a title="157-tfidf-13" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-07-22-FFT%3A_Friedman_%2B_Fortran_%2B_Tricks.html">147 brendan oconnor ai-2009-07-22-FFT: Friedman + Fortran + Tricks</a></p>
<p>14 0.067544065 <a title="157-tfidf-14" href="../brendan_oconnor_ai-2010/brendan_oconnor_ai-2010-04-14-quick_note%3A_cer_et_al_2010.html">159 brendan oconnor ai-2010-04-14-quick note: cer et al 2010</a></p>
<p>15 0.067494698 <a title="157-tfidf-15" href="../brendan_oconnor_ai-2010/brendan_oconnor_ai-2010-03-31-How_Facebook_privacy_failed_me.html">158 brendan oconnor ai-2010-03-31-How Facebook privacy failed me</a></p>
<p>16 0.067427903 <a title="157-tfidf-16" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-09-13-Response_on_our_movie_personas_paper.html">200 brendan oconnor ai-2013-09-13-Response on our movie personas paper</a></p>
<p>17 0.067330427 <a title="157-tfidf-17" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-05-21-iPhone_autocorrection_error_analysis.html">170 brendan oconnor ai-2011-05-21-iPhone autocorrection error analysis</a></p>
<p>18 0.066523902 <a title="157-tfidf-18" href="../brendan_oconnor_ai-2004/brendan_oconnor_ai-2004-11-24-addiction_%26_2_problems_of_economics.html">2 brendan oconnor ai-2004-11-24-addiction & 2 problems of economics</a></p>
<p>19 0.066150106 <a title="157-tfidf-19" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-09-18-%22Machine%22_translation-vision_%28Stanford_AI_courses_online%29.html">113 brendan oconnor ai-2008-09-18-"Machine" translation-vision (Stanford AI courses online)</a></p>
<p>20 0.065955952 <a title="157-tfidf-20" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-08-20-Some_analysis_of_tweet_shares_and_%E2%80%9Cpredicting%E2%80%9D_election_outcomes.html">198 brendan oconnor ai-2013-08-20-Some analysis of tweet shares and “predicting” election outcomes</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/brendan_oconnor_ai_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, -0.257), (1, -0.055), (2, 0.06), (3, -0.086), (4, 0.104), (5, 0.16), (6, -0.043), (7, 0.104), (8, -0.021), (9, 0.123), (10, -0.067), (11, 0.128), (12, 0.077), (13, 0.006), (14, -0.039), (15, -0.091), (16, -0.007), (17, 0.025), (18, 0.007), (19, 0.019), (20, 0.146), (21, -0.088), (22, -0.074), (23, 0.011), (24, -0.083), (25, -0.0), (26, 0.026), (27, 0.012), (28, 0.036), (29, 0.002), (30, 0.078), (31, 0.015), (32, -0.032), (33, -0.071), (34, -0.028), (35, -0.135), (36, -0.01), (37, -0.011), (38, 0.007), (39, -0.081), (40, -0.047), (41, 0.047), (42, -0.062), (43, 0.042), (44, 0.009), (45, 0.038), (46, 0.029), (47, -0.128), (48, 0.079), (49, -0.096)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98258841 <a title="157-lsi-1" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-12-31-List_of_probabilistic_model_mini-language_toolkits.html">157 brendan oconnor ai-2009-12-31-List of probabilistic model mini-language toolkits</a></p>
<p>Introduction: There are an increasing number of systems that attempt to allow the user to specify a probabilistic model in a high-level language — for example, declare a (Bayesian) generative model as a hierarchy of various distributions — then automatically run training and inference algorithms on a data set.  Now, you could always learn a good math library, and implement every model from scratch, but the motivation for this approach is you’ll avoid doing lots of repetitive and error-prone programming.  I’m not yet convinced that any of them completely achieve this goal, but it would be great if they succeeded and we could use high-level frameworks for everything.
 
Everyone seems to know about only a few of them, so here’s a meager attempt to list together a bunch that can be freely downloaded.  There is one package that is far more mature and been around much longer than the rest, so let’s start with:
  
 

 BUGS  – Bayesian Inference under Gibbs Sampling.  Specify a generative model, then it doe</p><p>2 0.74875039 <a title="157-lsi-2" href="../brendan_oconnor_ai-2010/brendan_oconnor_ai-2010-08-09-An_ML-AI_approach_to_P_%21%3D_NP.html">161 brendan oconnor ai-2010-08-09-An ML-AI approach to P != NP</a></p>
<p>Introduction: Like everyone, I’ve been just starting to look at the new, tentative,  proof that P != NP  from Vinay Deolalikar.  After reading the intro, what’s most striking is that probabilistic graphical models and mathematical logic are at the core of the proof.  This feels like a machine learning and artificial intelligence-centric approach to me — very different from what you usually see in mainstream CS theory.  (Maybe I should feel good that in my undergrad I basically stopped studying normal math and spent all my time with this weird stuff instead!)
 
He devotes several chapters to an introduction to graphical models — Ising models, conditional independence, MRF’s, Hammersley-Clifford, and all that other stuff you see in  Koller and Friedman  or something — and then logic and model theory!  I’m impressed.</p><p>3 0.61567414 <a title="157-lsi-3" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-03-21-Statistics_is_big-N_logic%3F.html">54 brendan oconnor ai-2007-03-21-Statistics is big-N logic?</a></p>
<p>Introduction: I think I believe one of these things, but I’m not quite sure. 
 Statistics is just like logic, except with uncertainty. 
 
This would be true if statistics is Bayesian statistics and you buy the Bayesian inductive logic story — add induction to propositional logic, via a conditional credibility operator, and the Cox axioms imply standard probability theory as a consequence.  (That is, probability theory is logic with uncertainty.  And then a good Bayesian thinks probability theory and statistics are the same.)  Links:  Jaynes’ explanation ;  SEP article ; also  Fitelson’s article .  (Though there are negative results; all I can think of right now is a  Halpern  article on Cox; and also interesting is  Halpern and Koller .)
 
Secondly, here is another statement.
  

Statistics is just like logic, except with a big N.

  
This is a more data-driven view — the world is full of things and they need to be described.  Logical rules can help you describe things, but you also have to deal wit</p><p>4 0.61215329 <a title="157-lsi-4" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-07-01-Modelling_environmentalism_thinking.html">11 brendan oconnor ai-2005-07-01-Modelling environmentalism thinking</a></p>
<p>Introduction: It’s a human political belief model — based on Cyc!  I’m not sure logic represents how people think all that well, but seeing the formalization of ideology is fascinating.  And besides, the methodology of cognitive modelling is awesome.   The link:  
 Modeling How People Think About Sustainability 
 
David C. James, M. P. Aff
 
LBJ School of Public Affairs The University of Texas at Austin May 2005
 
First Reader: Lodis Rhodes  Second Reader: Chandler Stolp
 
How effectively can a computer model represent the belief systems of different people? How would one go about representing a belief system using formal logic? How would that ideology react to different scenarios related to sustainable development? The author constructs the Cyc Agent-Scenario (CAS) model as a way to investigate these questions. The CAS model is built on top of ResearchCyc, a knowledge base (KB) and logical inference engine. The model consists of two agents (Libertarian and Green) and two scenarios. The model simula</p><p>5 0.58148813 <a title="157-lsi-5" href="../brendan_oconnor_ai-2014/brendan_oconnor_ai-2014-02-18-Scatterplot_of_KN-PYP_language_model_results.html">202 brendan oconnor ai-2014-02-18-Scatterplot of KN-PYP language model results</a></p>
<p>Introduction: I should make a blog where all I do is scatterplot results tables from papers.  I do this once in a while to make them eaiser to understand…
 
I think the following are results are from Yee Whye Teh’s paper on hierarchical Pitman-Yor language models, and in particular comparing them to Kneser-Ney and hierarchical Dirichlets.  They’re specifically from  these slides by Yee Whye Teh (page 25) , which shows model perplexities.  Every dot is for one experimental condition, which has four different results from each of the models.  So a pair of models can be compared in one scatterplot.
 
   
 
where
  
   ikn = interpolated kneser-ney
    mkn = modified kneser-ney
    hdlm = hierarchical dirichlet
    hpylm = hierarchical pitman-yor
   
My reading: the KN’s and HPYLM are incredibly similar (as Teh argues should be the case on theoretical grounds).  MKN and HPYLM edge out IKN.  HDLM is markedly worse (this is perplexity, so lower is better).  While HDLM is a lot worse, it does best, relativ</p><p>6 0.55473632 <a title="157-lsi-6" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-22-Performance_comparison%3A_key-value_stores_for_language_model_counts.html">139 brendan oconnor ai-2009-04-22-Performance comparison: key-value stores for language model counts</a></p>
<p>7 0.5338943 <a title="157-lsi-7" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-10-Don%E2%80%99t_MAWK_AWK_%E2%80%93_the_fastest_and_most_elegant_big_data_munging_language%21.html">154 brendan oconnor ai-2009-09-10-Don’t MAWK AWK – the fastest and most elegant big data munging language!</a></p>
<p>8 0.49788648 <a title="157-lsi-8" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-02-19-Move_to_brenocon.com.html">165 brendan oconnor ai-2011-02-19-Move to brenocon.com</a></p>
<p>9 0.43400332 <a title="157-lsi-9" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-07-22-FFT%3A_Friedman_%2B_Fortran_%2B_Tricks.html">147 brendan oconnor ai-2009-07-22-FFT: Friedman + Fortran + Tricks</a></p>
<p>10 0.41658524 <a title="157-lsi-10" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-02-23-Comparison_of_data_analysis_packages%3A_R%2C_Matlab%2C_SciPy%2C_Excel%2C_SAS%2C_SPSS%2C_Stata.html">135 brendan oconnor ai-2009-02-23-Comparison of data analysis packages: R, Matlab, SciPy, Excel, SAS, SPSS, Stata</a></p>
<p>11 0.41291425 <a title="157-lsi-11" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-11-24-Graphs_for_SANCL-2012_web_parsing_results.html">189 brendan oconnor ai-2012-11-24-Graphs for SANCL-2012 web parsing results</a></p>
<p>12 0.37756455 <a title="157-lsi-12" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-04-The_%2460%2C000_cat%3A_deep_belief_networks_make_less_sense_for_language_than_vision.html">184 brendan oconnor ai-2012-07-04-The $60,000 cat: deep belief networks make less sense for language than vision</a></p>
<p>13 0.37671384 <a title="157-lsi-13" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-08-20-Some_analysis_of_tweet_shares_and_%E2%80%9Cpredicting%E2%80%9D_election_outcomes.html">198 brendan oconnor ai-2013-08-20-Some analysis of tweet shares and “predicting” election outcomes</a></p>
<p>14 0.37036139 <a title="157-lsi-14" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-01-20-Moral_psychology_on_Amazon_Mechanical_Turk.html">90 brendan oconnor ai-2008-01-20-Moral psychology on Amazon Mechanical Turk</a></p>
<p>15 0.35209793 <a title="157-lsi-15" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-17-p-values%2C_CDF%E2%80%99s%2C_NLP_etc..html">185 brendan oconnor ai-2012-07-17-p-values, CDF’s, NLP etc.</a></p>
<p>16 0.34425086 <a title="157-lsi-16" href="../brendan_oconnor_ai-2010/brendan_oconnor_ai-2010-04-14-quick_note%3A_cer_et_al_2010.html">159 brendan oconnor ai-2010-04-14-quick note: cer et al 2010</a></p>
<p>17 0.34003255 <a title="157-lsi-17" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-05-20-Log-normal_and_logistic-normal_terminology.html">169 brendan oconnor ai-2011-05-20-Log-normal and logistic-normal terminology</a></p>
<p>18 0.33913144 <a title="157-lsi-18" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-05-21-iPhone_autocorrection_error_analysis.html">170 brendan oconnor ai-2011-05-21-iPhone autocorrection error analysis</a></p>
<p>19 0.33576304 <a title="157-lsi-19" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-09-19-End-to-end_NLP_packages.html">174 brendan oconnor ai-2011-09-19-End-to-end NLP packages</a></p>
<p>20 0.32292473 <a title="157-lsi-20" href="../brendan_oconnor_ai-2010/brendan_oconnor_ai-2010-03-31-How_Facebook_privacy_failed_me.html">158 brendan oconnor ai-2010-03-31-How Facebook privacy failed me</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/brendan_oconnor_ai_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(16, 0.025), (22, 0.031), (24, 0.048), (28, 0.021), (35, 0.404), (43, 0.028), (44, 0.086), (48, 0.04), (55, 0.014), (59, 0.038), (70, 0.027), (74, 0.083), (80, 0.064)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.91582501 <a title="157-lda-1" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-12-31-List_of_probabilistic_model_mini-language_toolkits.html">157 brendan oconnor ai-2009-12-31-List of probabilistic model mini-language toolkits</a></p>
<p>Introduction: There are an increasing number of systems that attempt to allow the user to specify a probabilistic model in a high-level language — for example, declare a (Bayesian) generative model as a hierarchy of various distributions — then automatically run training and inference algorithms on a data set.  Now, you could always learn a good math library, and implement every model from scratch, but the motivation for this approach is you’ll avoid doing lots of repetitive and error-prone programming.  I’m not yet convinced that any of them completely achieve this goal, but it would be great if they succeeded and we could use high-level frameworks for everything.
 
Everyone seems to know about only a few of them, so here’s a meager attempt to list together a bunch that can be freely downloaded.  There is one package that is far more mature and been around much longer than the rest, so let’s start with:
  
 

 BUGS  – Bayesian Inference under Gibbs Sampling.  Specify a generative model, then it doe</p><p>2 0.35777426 <a title="157-lda-2" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-10-Don%E2%80%99t_MAWK_AWK_%E2%80%93_the_fastest_and_most_elegant_big_data_munging_language%21.html">154 brendan oconnor ai-2009-09-10-Don’t MAWK AWK – the fastest and most elegant big data munging language!</a></p>
<p>Introduction: update 2012-10-25 : I’ve been informed there is a new maintainer for Mawk, who has probably fixed the bugs I’ve been seeing.
  

From: Gert Hulselmans   


[The bugs you have found are] indeed true with mawk v1.3.3 which comes standard with Debian/Ubuntu.  This version is almost not developed the last 10 years.


I now already use mawk v1.3.4 maintained by another developer (Thomas E. Dickey) 
for more than a year on huge datafiles (sometimes several GB).


The problems/wrong results I had with mawk v1.3.3 sometimes are gone. In his version, normally all open/known bugs are fixed.


This version can be downloaded from:  http://invisible-island.net/mawk/ 

     update 2010-04-30  : I have since found large datasets where mawk is buggy and gives the wrong result.  nawk seems safe.   When one of these newfangled   “Big Data”   sets comes your way, the very first thing you have to do is data munging: shuffling around file formats, renaming fields and the like.  Once you’re dealing with hun</p><p>3 0.33494246 <a title="157-lda-3" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-03-Statistics_vs._Machine_Learning%2C_fight%21.html">129 brendan oconnor ai-2008-12-03-Statistics vs. Machine Learning, fight!</a></p>
<p>Introduction: 10/1/09 update  — well, it’s been nearly a year, and I should say not everything in this rant is totally true, and I certainly believe much less of it now.  Current take:  Statistics , not machine learning, is the real deal, but unfortunately suffers from bad marketing.  On the other hand, to the extent that bad marketing includes misguided undergraduate curriculums, there’s plenty of room to improve for everyone.
   
So it’s pretty clear by now that statistics and machine learning aren’t very different fields. I was recently pointed to   a very amusing comparison   by the excellent statistician — and machine learning expert —   Robert Tibshiriani  .  Reproduced here:  
 
  Glossary 
   
 Machine learning
  Statistics 
   
 network, graphs
  model 
   
 weights
  parameters 
   
 learning
  fitting 
   
 generalization
  test set performance 
   
 supervised learning
  regression/classiﬁcation 
   
 unsupervised learning
  density estimation, clustering 
   
 large grant = $1,000,000</p><p>4 0.30236059 <a title="157-lda-4" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-08-08-Haghighi_and_Klein_%282009%29%3A_Simple_Coreference_Resolution_with_Rich_Syntactic_and_Semantic_Features.html">150 brendan oconnor ai-2009-08-08-Haghighi and Klein (2009): Simple Coreference Resolution with Rich Syntactic and Semantic Features</a></p>
<p>Introduction: I haven’t done a paper review on this blog for a while, so here we go.
 
 Coreference  resolution is an interesting NLP problem.  ( Examples. )  It involves honest-to-goodness syntactic, semantic, and discourse phenomena, but still seems like a real cognitive task that humans have to solve when reading text [1].  I haven’t read the whole literature, but I’ve always been puzzled by the crop of papers on it I’ve seen in the last year or two.  There’s a big focus on fancy graph/probabilistic/constrained optimization algorithms, but often these papers gloss over the linguistic features — the core information they actually make their decisions with [2].  I never understood why the latter isn’t the most important issue.  Therefore, it was a joy to read
  
 Aria Haghighi and Dan Klein, EMNLP-2009.   “Simple Coreference Resolution with Rich Syntactic and Semantic Features.”  
  
They describe a simple, essentially non-statistical system that outperforms previous unsupervised systems, and compa</p><p>5 0.30015728 <a title="157-lda-5" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-02-23-Comparison_of_data_analysis_packages%3A_R%2C_Matlab%2C_SciPy%2C_Excel%2C_SAS%2C_SPSS%2C_Stata.html">135 brendan oconnor ai-2009-02-23-Comparison of data analysis packages: R, Matlab, SciPy, Excel, SAS, SPSS, Stata</a></p>
<p>Introduction: Lukas  and I were trying to write a succinct comparison of the most popular packages that are typically used for data analysis.  I think most people choose one based on what people around them use or what they learn in school, so I’ve found it hard to find comparative information.  I’m posting the table here in hopes of useful comments.
  
 
 
  Name  
  Advantages  
  Disadvantages  
  Open source?  
  Typical   users  
 
 
 R 
 Library support; visualization 
 Steep learning curve 
 Yes 
 Finance; Statistics 
 
 
 Matlab 
 Elegant matrix support; visualization 
 Expensive; incomplete statistics support 
 No 
 Engineering 
 
 
 SciPy/NumPy/Matplotlib 
 Python (general-purpose programming language) 
 Immature 
 Yes 
 Engineering 
 
 
 Excel 
 Easy; visual; flexible 
 Large datasets 
 No 
 Business 
 
 
 SAS 
 Large datasets 
 Expensive; outdated programming language 
 No 
 Business; Government 
 
 
 Stata 
 Easy statistical analysis 
  
 No 
 Science 
 
 
 SPSS 
 Like Stata but more ex</p><p>6 0.296756 <a title="157-lda-6" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-03-15-Feminists%2C_anarchists%2C_computational_complexity%2C_bounded_rationality%2C_nethack%2C_and_other_things_to_do.html">53 brendan oconnor ai-2007-03-15-Feminists, anarchists, computational complexity, bounded rationality, nethack, and other things to do</a></p>
<p>7 0.29193723 <a title="157-lda-7" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-04-The_%2460%2C000_cat%3A_deep_belief_networks_make_less_sense_for_language_than_vision.html">184 brendan oconnor ai-2012-07-04-The $60,000 cat: deep belief networks make less sense for language than vision</a></p>
<p>8 0.29178995 <a title="157-lda-8" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-26-Seeing_how_%E2%80%9Cart%E2%80%9D_and_%E2%80%9Cpharmaceuticals%E2%80%9D_are_linguistically_similar_in_web_text.html">156 brendan oconnor ai-2009-09-26-Seeing how “art” and “pharmaceuticals” are linguistically similar in web text</a></p>
<p>9 0.28635898 <a title="157-lda-9" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-10-02-Powerset%E2%80%99s_natural_language_search_system.html">188 brendan oconnor ai-2012-10-02-Powerset’s natural language search system</a></p>
<p>10 0.28255787 <a title="157-lda-10" href="../brendan_oconnor_ai-2004/brendan_oconnor_ai-2004-11-24-addiction_%26_2_problems_of_economics.html">2 brendan oconnor ai-2004-11-24-addiction & 2 problems of economics</a></p>
<p>11 0.28040281 <a title="157-lda-11" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-22-Performance_comparison%3A_key-value_stores_for_language_model_counts.html">139 brendan oconnor ai-2009-04-22-Performance comparison: key-value stores for language model counts</a></p>
<p>12 0.278795 <a title="157-lda-12" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-03-21-Statistics_is_big-N_logic%3F.html">54 brendan oconnor ai-2007-03-21-Statistics is big-N logic?</a></p>
<p>13 0.2762922 <a title="157-lda-13" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-09-19-End-to-end_NLP_packages.html">174 brendan oconnor ai-2011-09-19-End-to-end NLP packages</a></p>
<p>14 0.27601215 <a title="157-lda-14" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-17-p-values%2C_CDF%E2%80%99s%2C_NLP_etc..html">185 brendan oconnor ai-2012-07-17-p-values, CDF’s, NLP etc.</a></p>
<p>15 0.27214164 <a title="157-lda-15" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-04-16-Rise_and_fall_of_Dirichlet_process_clusters.html">194 brendan oconnor ai-2013-04-16-Rise and fall of Dirichlet process clusters</a></p>
<p>16 0.27153265 <a title="157-lda-16" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-05-18-Announcing_TweetMotif_for_summarizing_twitter_topics.html">140 brendan oconnor ai-2009-05-18-Announcing TweetMotif for summarizing twitter topics</a></p>
<p>17 0.27038431 <a title="157-lda-17" href="../brendan_oconnor_ai-2010/brendan_oconnor_ai-2010-03-31-How_Facebook_privacy_failed_me.html">158 brendan oconnor ai-2010-03-31-How Facebook privacy failed me</a></p>
<p>18 0.2684111 <a title="157-lda-18" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-08-20-Some_analysis_of_tweet_shares_and_%E2%80%9Cpredicting%E2%80%9D_election_outcomes.html">198 brendan oconnor ai-2013-08-20-Some analysis of tweet shares and “predicting” election outcomes</a></p>
<p>19 0.26335946 <a title="157-lda-19" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-01-Binary_classification_evaluation_in_R_via_ROCR.html">136 brendan oconnor ai-2009-04-01-Binary classification evaluation in R via ROCR</a></p>
<p>20 0.26140535 <a title="157-lda-20" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-01-07-Love_it_and_hate_it%2C_R_has_come_of_age.html">132 brendan oconnor ai-2009-01-07-Love it and hate it, R has come of age</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
