<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>156 brendan oconnor ai-2009-09-26-Seeing how “art” and “pharmaceuticals” are linguistically similar in web text</title>
</head>

<body>
<p><a title="brendan_oconnor_ai" href="../brendan_oconnor_ai_home.html">brendan_oconnor_ai</a> <a title="brendan_oconnor_ai-2009" href="../home/brendan_oconnor_ai-2009_home.html">brendan_oconnor_ai-2009</a> <a title="brendan_oconnor_ai-2009-156" href="#">brendan_oconnor_ai-2009-156</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>156 brendan oconnor ai-2009-09-26-Seeing how “art” and “pharmaceuticals” are linguistically similar in web text</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="brendan_oconnor_ai-2009-156-html" href="http://brenocon.com/blog/2009/09/seeing-how-art-and-pharmaceuticals-are-linguistically-similar-in-web-text/">html</a></p><p>Introduction: Earlier this week I asked the question,
 
 How are “art” and “pharmaceuticals” similar? 
 
People sent me lots of submissions!  Some are great, some are a bit of a stretch.
  
 Overpriced by an order of magnitude.
  The letters of “art” are found embedded, in order, in “pharmaceuticals”.
  Search keywords that cost the most to advertise on?
  “Wyeth”: I think this means  this , and  this .
  “Romeo and Juliet” famously includes both “art” (wherefore art thou) and pharmaceuticals (poison!)
  Some art has been created out of pharmaceuticals.
  Some art has been created under the influence of pharmaceuticals.
   
I was asking because I was playing around with a dataset of 100,000 noun phrases’ appearances on the web, from the  Reading the Web  project at CMU.  That is, for a noun like “art”, this data has a large list of phrases in which the word “art” is used, across some 200 million web pages.  For two noun concepts, we can see what they have in common and what’s different by looking at</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 “Romeo and Juliet” famously includes both “art” (wherefore art thou) and pharmaceuticals (poison! [sent-8, score-1.048]
</p><p>2 )   Some art has been created out of pharmaceuticals. [sent-9, score-0.642]
</p><p>3 Some art has been created under the influence of pharmaceuticals. [sent-10, score-0.642]
</p><p>4 I was asking because I was playing around with a dataset of 100,000 noun phrases’ appearances on the web, from the  Reading the Web  project at CMU. [sent-11, score-0.249]
</p><p>5 That is, for a noun like “art”, this data has a large list of phrases in which the word “art” is used, across some 200 million web pages. [sent-12, score-0.501]
</p><p>6 For two noun concepts, we can see what they have in common and what’s different by looking at examples of how people use them when writing. [sent-13, score-0.291]
</p><p>7 What they have in common is that they’re both products: you can buy, sell, produce, and store them. [sent-15, score-0.196]
</p><p>8 )  This really didn’t occur to me at first; silly me, I thought art was a thing of beauty removed from such mundane considerations. [sent-17, score-0.717]
</p><p>9 A number of the submitted answers, though, center around the theme of them both being expensive — so we have positive agreement between  corpus statistics  and human judgments! [sent-18, score-0.08]
</p><p>10 Examining massive numbers of contexts like this follows what the infinitely wise Dinosaur Comics calls  “a statistically-based descriptivist approach to semantics. [sent-19, score-0.354]
</p><p>11 Firth  put it, “You shall know a word by the company it keeps. [sent-22, score-0.093]
</p><p>12 ”  Many subtleties of the two concepts can be seen just in their context lists. [sent-23, score-0.16]
</p><p>13 For example, in the left column, we see that only art “is a commodity”. [sent-24, score-0.576]
</p><p>14 As for the data: it comes from 200 million web pages (500 million sentences), and is filtered to contexts that appear more than five hundred times in the data. [sent-29, score-0.563]
</p><p>15 It was collected as part of a research project that seeks to extract a database of knowledge from this information —  “reading the web” . [sent-30, score-0.159]
</p><p>16 Will  pointed out that in Alice in Wonderland, the Mad Hatter asks,  “Why is a raven like a writing desk? [sent-34, score-0.163]
</p><p>17 ”   I tried that query on this data, but unfortunately, it didn’t contain many instances of “raven”. [sent-35, score-0.048]
</p><p>18 However, it  does  include a proper name “Raven” — which turns out to be an  anime character . [sent-36, score-0.155]
</p><p>19 Not the first time I’ve seen the Internet’s massive amount of anime knowledge get in the way of a very serious semantic extraction system! [sent-37, score-0.391]
</p><p>20 Many thanks to  Adam , Joanna,  Will ,  Vikas , and Michael for the submitted answers. [sent-38, score-0.08]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('art', 0.576), ('pharmaceuticals', 0.425), ('contexts', 0.227), ('commodity', 0.217), ('raven', 0.163), ('common', 0.149), ('noun', 0.142), ('web', 0.13), ('anime', 0.108), ('million', 0.103), ('knowledge', 0.099), ('total', 0.092), ('phrases', 0.08), ('submitted', 0.08), ('answers', 0.08), ('massive', 0.08), ('concepts', 0.069), ('column', 0.069), ('created', 0.066), ('project', 0.06), ('amount', 0.06), ('order', 0.06), ('however', 0.053), ('many', 0.048), ('didn', 0.047), ('availability', 0.047), ('proclaiming', 0.047), ('shall', 0.047), ('letters', 0.047), ('cocktail', 0.047), ('infinitely', 0.047), ('proper', 0.047), ('silly', 0.047), ('career', 0.047), ('dinosaur', 0.047), ('intersection', 0.047), ('generations', 0.047), ('famously', 0.047), ('romeo', 0.047), ('removed', 0.047), ('store', 0.047), ('appearances', 0.047), ('desk', 0.047), ('occur', 0.047), ('prices', 0.047), ('production', 0.047), ('subtleties', 0.047), ('word', 0.046), ('seen', 0.044), ('asked', 0.043)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="156-tfidf-1" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-26-Seeing_how_%E2%80%9Cart%E2%80%9D_and_%E2%80%9Cpharmaceuticals%E2%80%9D_are_linguistically_similar_in_web_text.html">156 brendan oconnor ai-2009-09-26-Seeing how “art” and “pharmaceuticals” are linguistically similar in web text</a></p>
<p>Introduction: Earlier this week I asked the question,
 
 How are “art” and “pharmaceuticals” similar? 
 
People sent me lots of submissions!  Some are great, some are a bit of a stretch.
  
 Overpriced by an order of magnitude.
  The letters of “art” are found embedded, in order, in “pharmaceuticals”.
  Search keywords that cost the most to advertise on?
  “Wyeth”: I think this means  this , and  this .
  “Romeo and Juliet” famously includes both “art” (wherefore art thou) and pharmaceuticals (poison!)
  Some art has been created out of pharmaceuticals.
  Some art has been created under the influence of pharmaceuticals.
   
I was asking because I was playing around with a dataset of 100,000 noun phrases’ appearances on the web, from the  Reading the Web  project at CMU.  That is, for a noun like “art”, this data has a large list of phrases in which the word “art” is used, across some 200 million web pages.  For two noun concepts, we can see what they have in common and what’s different by looking at</p><p>2 0.3087337 <a title="156-tfidf-2" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-20-Quiz%3A_%E2%80%9Cart%E2%80%9D_and_%E2%80%9Cpharmaceuticals%E2%80%9D.html">155 brendan oconnor ai-2009-09-20-Quiz: “art” and “pharmaceuticals”</a></p>
<p>Introduction: A lexical semantics question:
 
 How are “art” and “pharmaceuticals” similar? 
 
I have a data-driven answer, but am curious how easy it is to guess it, and in what sense it’s valid.  I’ll post my answer and supporting evidence on Tuesday.</p><p>3 0.095591769 <a title="156-tfidf-3" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-01-27-Graphics%21_Atari_Breakout_and_religious_text_NLP.html">91 brendan oconnor ai-2008-01-27-Graphics! Atari Breakout and religious text NLP</a></p>
<p>Introduction: From a graphics/mod programming workshop,  modifications of “Breakout”  in awesome video form:
 
          
 
It uses  Processing , a framework designed for animation and graphicky things.  It was also used for the  Similar Diversity  visualization that maps out named entities and their common verbs across religious texts.  It’s pretty cool:     … so a name gets bigger with its frequency, I get that; but what governs the size of an arc connecting two names?  They say “common activities” — I’m guessing a distance measure on the terms in the contexts in which they appear?  I’d be most interested in a graph of their explicit interactions; something like a count of co-occurrences in a proximity window (word radius? verse? chapter? book?) might do the trick…
 
This came out a while ago, I think; here are some  related works  posted on infosthetics.com.</p><p>4 0.075914592 <a title="156-tfidf-4" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-11-05-Obama_street_celebrations_in_San_Francisco.html">122 brendan oconnor ai-2008-11-05-Obama street celebrations in San Francisco</a></p>
<p>Introduction: In San Francisco, it’s no secret who everyone wanted to win in this election.  Shortly after Obama’s victory speech last night, people started celebrating in the streets near my house in the Mission.  At Valencia and 19th, a big party formed and ran for several hours into the night.
     
     
  People and kids were cheering, high-fiving, playing music, and having a good time:
 
            
 
Shades of  Burning Man : There were happy combinations of alcohol, police, art cars, fireworks, and the  Extra Action Marching Band .
 
            
 
(That clip was the tensest situation I saw; after that, the police just moved everyone out of the intersection and watched carefully.)
 
            
 
            
 
I yelled “Yes we can!” and was answered “Yes we DID!”  Strangers hugged me.  I went home at 1 a.m. and the party was still going strong.  Not the worst way to celebrate making history.
 
This election was also big locally, including a tight three-way race for the local district chair</p><p>5 0.07554964 <a title="156-tfidf-5" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-03-Statistics_vs._Machine_Learning%2C_fight%21.html">129 brendan oconnor ai-2008-12-03-Statistics vs. Machine Learning, fight!</a></p>
<p>Introduction: 10/1/09 update  — well, it’s been nearly a year, and I should say not everything in this rant is totally true, and I certainly believe much less of it now.  Current take:  Statistics , not machine learning, is the real deal, but unfortunately suffers from bad marketing.  On the other hand, to the extent that bad marketing includes misguided undergraduate curriculums, there’s plenty of room to improve for everyone.
   
So it’s pretty clear by now that statistics and machine learning aren’t very different fields. I was recently pointed to   a very amusing comparison   by the excellent statistician — and machine learning expert —   Robert Tibshiriani  .  Reproduced here:  
 
  Glossary 
   
 Machine learning
  Statistics 
   
 network, graphs
  model 
   
 weights
  parameters 
   
 learning
  fitting 
   
 generalization
  test set performance 
   
 supervised learning
  regression/classiﬁcation 
   
 unsupervised learning
  density estimation, clustering 
   
 large grant = $1,000,000</p><p>6 0.071117893 <a title="156-tfidf-6" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-17-1_billion_web_page_dataset_from_CMU.html">138 brendan oconnor ai-2009-04-17-1 billion web page dataset from CMU</a></p>
<p>7 0.06867402 <a title="156-tfidf-7" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-08-08-Haghighi_and_Klein_%282009%29%3A_Simple_Coreference_Resolution_with_Rich_Syntactic_and_Semantic_Features.html">150 brendan oconnor ai-2009-08-08-Haghighi and Klein (2009): Simple Coreference Resolution with Rich Syntactic and Semantic Features</a></p>
<p>8 0.06191938 <a title="156-tfidf-8" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-10-08-MyDebates.org%2C_online_polling%2C_and_potentially_the_coolest_question_corpus_ever.html">116 brendan oconnor ai-2008-10-08-MyDebates.org, online polling, and potentially the coolest question corpus ever</a></p>
<p>9 0.061438777 <a title="156-tfidf-9" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-10-02-Powerset%E2%80%99s_natural_language_search_system.html">188 brendan oconnor ai-2012-10-02-Powerset’s natural language search system</a></p>
<p>10 0.058154561 <a title="156-tfidf-10" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-03-15-Feminists%2C_anarchists%2C_computational_complexity%2C_bounded_rationality%2C_nethack%2C_and_other_things_to_do.html">53 brendan oconnor ai-2007-03-15-Feminists, anarchists, computational complexity, bounded rationality, nethack, and other things to do</a></p>
<p>11 0.057737168 <a title="156-tfidf-11" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-04-The_%2460%2C000_cat%3A_deep_belief_networks_make_less_sense_for_language_than_vision.html">184 brendan oconnor ai-2012-07-04-The $60,000 cat: deep belief networks make less sense for language than vision</a></p>
<p>12 0.057695806 <a title="156-tfidf-12" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-22-Performance_comparison%3A_key-value_stores_for_language_model_counts.html">139 brendan oconnor ai-2009-04-22-Performance comparison: key-value stores for language model counts</a></p>
<p>13 0.055010859 <a title="156-tfidf-13" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-10-05-Be_careful_with_dictionary-based_text_analysis.html">176 brendan oconnor ai-2011-10-05-Be careful with dictionary-based text analysis</a></p>
<p>14 0.053426906 <a title="156-tfidf-14" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-03-09-I_don%E2%80%99t_get_this_web_parsing_shared_task.html">181 brendan oconnor ai-2012-03-09-I don’t get this web parsing shared task</a></p>
<p>15 0.052170418 <a title="156-tfidf-15" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-01-20-Moral_psychology_on_Amazon_Mechanical_Turk.html">90 brendan oconnor ai-2008-01-20-Moral psychology on Amazon Mechanical Turk</a></p>
<p>16 0.05153358 <a title="156-tfidf-16" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-05-19-conplot_%E2%80%93_a_console_plotter.html">103 brendan oconnor ai-2008-05-19-conplot – a console plotter</a></p>
<p>17 0.051393367 <a title="156-tfidf-17" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-10-13-Verificationism_dinosaur_comics.html">79 brendan oconnor ai-2007-10-13-Verificationism dinosaur comics</a></p>
<p>18 0.049559202 <a title="156-tfidf-18" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-27-Facebook_sentiment_mining_predicts_presidential_polls.html">131 brendan oconnor ai-2008-12-27-Facebook sentiment mining predicts presidential polls</a></p>
<p>19 0.044464558 <a title="156-tfidf-19" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-11-24-Graphs_for_SANCL-2012_web_parsing_results.html">189 brendan oconnor ai-2012-11-24-Graphs for SANCL-2012 web parsing results</a></p>
<p>20 0.04395771 <a title="156-tfidf-20" href="../brendan_oconnor_ai-2004/brendan_oconnor_ai-2004-11-20-gintis%3A_theoretical_unity_in_the_social_sciences.html">1 brendan oconnor ai-2004-11-20-gintis: theoretical unity in the social sciences</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/brendan_oconnor_ai_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, -0.186), (1, -0.055), (2, 0.016), (3, -0.015), (4, 0.017), (5, -0.118), (6, 0.061), (7, -0.025), (8, 0.021), (9, -0.096), (10, -0.081), (11, -0.063), (12, 0.099), (13, -0.003), (14, -0.125), (15, -0.016), (16, -0.117), (17, -0.119), (18, -0.249), (19, 0.26), (20, 0.053), (21, -0.038), (22, -0.11), (23, -0.224), (24, 0.198), (25, 0.116), (26, -0.063), (27, 0.078), (28, -0.147), (29, 0.114), (30, 0.023), (31, 0.128), (32, 0.049), (33, 0.029), (34, 0.084), (35, -0.032), (36, 0.069), (37, 0.086), (38, 0.134), (39, -0.165), (40, -0.003), (41, -0.015), (42, -0.049), (43, -0.014), (44, -0.034), (45, -0.047), (46, 0.038), (47, 0.022), (48, 0.038), (49, 0.037)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98742431 <a title="156-lsi-1" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-26-Seeing_how_%E2%80%9Cart%E2%80%9D_and_%E2%80%9Cpharmaceuticals%E2%80%9D_are_linguistically_similar_in_web_text.html">156 brendan oconnor ai-2009-09-26-Seeing how “art” and “pharmaceuticals” are linguistically similar in web text</a></p>
<p>Introduction: Earlier this week I asked the question,
 
 How are “art” and “pharmaceuticals” similar? 
 
People sent me lots of submissions!  Some are great, some are a bit of a stretch.
  
 Overpriced by an order of magnitude.
  The letters of “art” are found embedded, in order, in “pharmaceuticals”.
  Search keywords that cost the most to advertise on?
  “Wyeth”: I think this means  this , and  this .
  “Romeo and Juliet” famously includes both “art” (wherefore art thou) and pharmaceuticals (poison!)
  Some art has been created out of pharmaceuticals.
  Some art has been created under the influence of pharmaceuticals.
   
I was asking because I was playing around with a dataset of 100,000 noun phrases’ appearances on the web, from the  Reading the Web  project at CMU.  That is, for a noun like “art”, this data has a large list of phrases in which the word “art” is used, across some 200 million web pages.  For two noun concepts, we can see what they have in common and what’s different by looking at</p><p>2 0.86649394 <a title="156-lsi-2" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-20-Quiz%3A_%E2%80%9Cart%E2%80%9D_and_%E2%80%9Cpharmaceuticals%E2%80%9D.html">155 brendan oconnor ai-2009-09-20-Quiz: “art” and “pharmaceuticals”</a></p>
<p>Introduction: A lexical semantics question:
 
 How are “art” and “pharmaceuticals” similar? 
 
I have a data-driven answer, but am curious how easy it is to guess it, and in what sense it’s valid.  I’ll post my answer and supporting evidence on Tuesday.</p><p>3 0.45390999 <a title="156-lsi-3" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-05-19-conplot_%E2%80%93_a_console_plotter.html">103 brendan oconnor ai-2008-05-19-conplot – a console plotter</a></p>
<p>Introduction: This has to be the most quick-and-dirty data visualizer out there: I wrote an ascii art plotter script that takes a column of numbers on stdin and throws out a plot on your console.  I’ve been using it for several months to quickly look at numbers on the commandline, especially from logs and such.  (Back in school I would use gnuplot for this; R is good too.  But sometimes you want to move really fast, esp if you have a few hideous perl -pe one-liners on your hands and mucking around with temp files will interrupt your flow.)
 
Link:  github.com/brendano/conplot 
 
“Demo”:
  $ cat time.log | conplot
14601
                                                                         oooooooo
                                                                    oooooo
                                                            ooooooooo
                                                 oooooooooooo
11269                                     oooooooo
                                       o</p><p>4 0.43541175 <a title="156-lsi-4" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-01-27-Graphics%21_Atari_Breakout_and_religious_text_NLP.html">91 brendan oconnor ai-2008-01-27-Graphics! Atari Breakout and religious text NLP</a></p>
<p>Introduction: From a graphics/mod programming workshop,  modifications of “Breakout”  in awesome video form:
 
          
 
It uses  Processing , a framework designed for animation and graphicky things.  It was also used for the  Similar Diversity  visualization that maps out named entities and their common verbs across religious texts.  It’s pretty cool:     … so a name gets bigger with its frequency, I get that; but what governs the size of an arc connecting two names?  They say “common activities” — I’m guessing a distance measure on the terms in the contexts in which they appear?  I’d be most interested in a graph of their explicit interactions; something like a count of co-occurrences in a proximity window (word radius? verse? chapter? book?) might do the trick…
 
This came out a while ago, I think; here are some  related works  posted on infosthetics.com.</p><p>5 0.37773716 <a title="156-lsi-5" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-04-The_%2460%2C000_cat%3A_deep_belief_networks_make_less_sense_for_language_than_vision.html">184 brendan oconnor ai-2012-07-04-The $60,000 cat: deep belief networks make less sense for language than vision</a></p>
<p>Introduction: There was an  interesting ICML paper  this year about very large-scale training of deep belief networks (a.k.a. neural networks) for unsupervised concept extraction from images.  They ( Quoc V. Le  and colleagues at Google/Stanford) have a cute example of learning very high-level features that are evoked by images of cats (from YouTube still-image training data); one is shown below.
 
For those of us who work on machine learning and text, the question always comes up, why not DBN’s for language?  Many shallow latent-space text models have been quite successful (LSI, LDA, HMM, LPCFG…); there is hope that some sort of “deeper” concepts could be learned.  I think this is one of the most interesting areas for unsupervised language modeling right now.
 
But note it’s a bad idea to directly analogize results from image analysis to language analysis.  The problems have radically different levels of conceptual abstraction baked-in.  Consider the problem of detecting the concept of a cat; i.e.</p><p>6 0.36753801 <a title="156-lsi-6" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-06-26-Good_linguistic_semantics_textbook%3F.html">172 brendan oconnor ai-2011-06-26-Good linguistic semantics textbook?</a></p>
<p>7 0.33526698 <a title="156-lsi-7" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-22-Performance_comparison%3A_key-value_stores_for_language_model_counts.html">139 brendan oconnor ai-2009-04-22-Performance comparison: key-value stores for language model counts</a></p>
<p>8 0.29310301 <a title="156-lsi-8" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-01-20-Moral_psychology_on_Amazon_Mechanical_Turk.html">90 brendan oconnor ai-2008-01-20-Moral psychology on Amazon Mechanical Turk</a></p>
<p>9 0.28943229 <a title="156-lsi-9" href="../brendan_oconnor_ai-2014/brendan_oconnor_ai-2014-02-18-Scatterplot_of_KN-PYP_language_model_results.html">202 brendan oconnor ai-2014-02-18-Scatterplot of KN-PYP language model results</a></p>
<p>10 0.28200638 <a title="156-lsi-10" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-10-13-Verificationism_dinosaur_comics.html">79 brendan oconnor ai-2007-10-13-Verificationism dinosaur comics</a></p>
<p>11 0.26415983 <a title="156-lsi-11" href="../brendan_oconnor_ai-2006/brendan_oconnor_ai-2006-04-28-Easterly_vs._Sachs_on_global_poverty.html">35 brendan oconnor ai-2006-04-28-Easterly vs. Sachs on global poverty</a></p>
<p>12 0.25878286 <a title="156-lsi-12" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-08-21-Berkeley_SDA_and_the_General_Social_Survey.html">186 brendan oconnor ai-2012-08-21-Berkeley SDA and the General Social Survey</a></p>
<p>13 0.25325677 <a title="156-lsi-13" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-10-02-Powerset%E2%80%99s_natural_language_search_system.html">188 brendan oconnor ai-2012-10-02-Powerset’s natural language search system</a></p>
<p>14 0.24689579 <a title="156-lsi-14" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-11-05-Obama_street_celebrations_in_San_Francisco.html">122 brendan oconnor ai-2008-11-05-Obama street celebrations in San Francisco</a></p>
<p>15 0.2321398 <a title="156-lsi-15" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-10-08-MyDebates.org%2C_online_polling%2C_and_potentially_the_coolest_question_corpus_ever.html">116 brendan oconnor ai-2008-10-08-MyDebates.org, online polling, and potentially the coolest question corpus ever</a></p>
<p>16 0.23012358 <a title="156-lsi-16" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-17-1_billion_web_page_dataset_from_CMU.html">138 brendan oconnor ai-2009-04-17-1 billion web page dataset from CMU</a></p>
<p>17 0.22961263 <a title="156-lsi-17" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-27-Facebook_sentiment_mining_predicts_presidential_polls.html">131 brendan oconnor ai-2008-12-27-Facebook sentiment mining predicts presidential polls</a></p>
<p>18 0.18850818 <a title="156-lsi-18" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-11-21-Netflix_Prize.html">125 brendan oconnor ai-2008-11-21-Netflix Prize</a></p>
<p>19 0.18393011 <a title="156-lsi-19" href="../brendan_oconnor_ai-2010/brendan_oconnor_ai-2010-03-31-How_Facebook_privacy_failed_me.html">158 brendan oconnor ai-2010-03-31-How Facebook privacy failed me</a></p>
<p>20 0.18027279 <a title="156-lsi-20" href="../brendan_oconnor_ai-2014/brendan_oconnor_ai-2014-04-26-Replot%3A_departure_delays_vs_flight_time_speed-up.html">204 brendan oconnor ai-2014-04-26-Replot: departure delays vs flight time speed-up</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/brendan_oconnor_ai_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(22, 0.416), (24, 0.07), (43, 0.041), (44, 0.117), (48, 0.028), (55, 0.017), (57, 0.019), (70, 0.077), (74, 0.09), (80, 0.036)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.95641869 <a title="156-lda-1" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-09-26-EEG_for_the_Wii_and_in_your_basement.html">78 brendan oconnor ai-2007-09-26-EEG for the Wii and in your basement</a></p>
<p>Introduction: There’s a company,  Emotiv , that’s building an  EEG  interface for the game systems.  Any company with a  science-fiction-y vision statement  sounds like a good time to me:
  
Communication between man and machine has always been limited to conscious interaction, with non-conscious communication — expression, intuition, perception — reserved solely for the human realm. At Emotiv, we believe that future communication between man and machine will not only be limited to the conscious communication that exists today, but non-conscious communication will play a significant part.


Our mission is to create the ultimate interface for the next-generation of man-machine interaction, by evolving the interaction between human beings and electronic devices beyond the limits of conscious interface. Emotiv is creating technologies that allow machines to take both conscious and non-conscious inputs directly from your mind.
  
They even have a cyborg-looking woman on the page.
 
Their claim is to det</p><p>same-blog 2 0.91291434 <a title="156-lda-2" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-26-Seeing_how_%E2%80%9Cart%E2%80%9D_and_%E2%80%9Cpharmaceuticals%E2%80%9D_are_linguistically_similar_in_web_text.html">156 brendan oconnor ai-2009-09-26-Seeing how “art” and “pharmaceuticals” are linguistically similar in web text</a></p>
<p>Introduction: Earlier this week I asked the question,
 
 How are “art” and “pharmaceuticals” similar? 
 
People sent me lots of submissions!  Some are great, some are a bit of a stretch.
  
 Overpriced by an order of magnitude.
  The letters of “art” are found embedded, in order, in “pharmaceuticals”.
  Search keywords that cost the most to advertise on?
  “Wyeth”: I think this means  this , and  this .
  “Romeo and Juliet” famously includes both “art” (wherefore art thou) and pharmaceuticals (poison!)
  Some art has been created out of pharmaceuticals.
  Some art has been created under the influence of pharmaceuticals.
   
I was asking because I was playing around with a dataset of 100,000 noun phrases’ appearances on the web, from the  Reading the Web  project at CMU.  That is, for a noun like “art”, this data has a large list of phrases in which the word “art” is used, across some 200 million web pages.  For two noun concepts, we can see what they have in common and what’s different by looking at</p><p>3 0.91124636 <a title="156-lda-3" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-01-07-Love_it_and_hate_it%2C_R_has_come_of_age.html">132 brendan oconnor ai-2009-01-07-Love it and hate it, R has come of age</a></p>
<p>Introduction: Seeing a long, lavish article about R in the NEW YORK TIMES (!) really freaks me out. 
  replicate(100,  c(
  "OMG OMG,  R  is now famous?!",
  "People used to make fun of me for learning R since Splus is SO OLD!",
  "I still hear stories that SAS can do crazy tricks that make me jealous.
  But not enough to attempt learning it."
)[ floor(runif(1, min=1,max=4)) ] )  
This blog has been a long-time supporter of this both brilliant and insanely quirky statistical programming environment.  Here are some graphs I’ve made in the last year or two that have R code attached:
  
  Wisdom of small crowds  
  Simpson’s paradox via mosaic plots  
  Dolores Labs color wheel!  ( code ) 
  Political bias SVD evaluation  
  Presidential poll aggregation  
 OK, we didn’t post the code, but check out our  N-body trolley graph ! 
  
Learning R is hard because there’s a zillion packages, and the official documentation is reference-oriented.  I’ve never looked at any of the books much.  I think you ca</p><p>4 0.42887321 <a title="156-lda-4" href="../brendan_oconnor_ai-2004/brendan_oconnor_ai-2004-11-24-addiction_%26_2_problems_of_economics.html">2 brendan oconnor ai-2004-11-24-addiction & 2 problems of economics</a></p>
<p>Introduction: This is my idea based off of  Bernheim and Rangel’s  model of addict decision-making .  It’s a really neat model; it manages to relax rationality to allow someone to do something they don’t want to do because they’re addicted to it.  [Rationality assumes a nice well-ordered set of preferences; this model hypothesizes as distinction between emotional "liking" and cognitive, forward "wanting" that can conflict.]  The model is mathematically tractable, it can be used for public welfare analysis, and to top it off — it’s got neuroscientific grounding!
 
It appears to me there are two big criticisms of the economics discipline’s assumptions.  One of course is rationality.  The second has to do with the perfect structure of the market and environment that shapes both preferences and the ability to exercise them.  One critique is about social structure: consumers are not atomistic individual units, but rather exchange information and ideas along networks of patterned social relations.  (Socia</p><p>5 0.42230996 <a title="156-lda-5" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-04-The_%2460%2C000_cat%3A_deep_belief_networks_make_less_sense_for_language_than_vision.html">184 brendan oconnor ai-2012-07-04-The $60,000 cat: deep belief networks make less sense for language than vision</a></p>
<p>Introduction: There was an  interesting ICML paper  this year about very large-scale training of deep belief networks (a.k.a. neural networks) for unsupervised concept extraction from images.  They ( Quoc V. Le  and colleagues at Google/Stanford) have a cute example of learning very high-level features that are evoked by images of cats (from YouTube still-image training data); one is shown below.
 
For those of us who work on machine learning and text, the question always comes up, why not DBN’s for language?  Many shallow latent-space text models have been quite successful (LSI, LDA, HMM, LPCFG…); there is hope that some sort of “deeper” concepts could be learned.  I think this is one of the most interesting areas for unsupervised language modeling right now.
 
But note it’s a bad idea to directly analogize results from image analysis to language analysis.  The problems have radically different levels of conceptual abstraction baked-in.  Consider the problem of detecting the concept of a cat; i.e.</p><p>6 0.41954219 <a title="156-lda-6" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-03-Statistics_vs._Machine_Learning%2C_fight%21.html">129 brendan oconnor ai-2008-12-03-Statistics vs. Machine Learning, fight!</a></p>
<p>7 0.38297966 <a title="156-lda-7" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-22-Performance_comparison%3A_key-value_stores_for_language_model_counts.html">139 brendan oconnor ai-2009-04-22-Performance comparison: key-value stores for language model counts</a></p>
<p>8 0.37843084 <a title="156-lda-8" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-10-Don%E2%80%99t_MAWK_AWK_%E2%80%93_the_fastest_and_most_elegant_big_data_munging_language%21.html">154 brendan oconnor ai-2009-09-10-Don’t MAWK AWK – the fastest and most elegant big data munging language!</a></p>
<p>9 0.37662125 <a title="156-lda-9" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-02-23-Comparison_of_data_analysis_packages%3A_R%2C_Matlab%2C_SciPy%2C_Excel%2C_SAS%2C_SPSS%2C_Stata.html">135 brendan oconnor ai-2009-02-23-Comparison of data analysis packages: R, Matlab, SciPy, Excel, SAS, SPSS, Stata</a></p>
<p>10 0.37295514 <a title="156-lda-10" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-03-15-Feminists%2C_anarchists%2C_computational_complexity%2C_bounded_rationality%2C_nethack%2C_and_other_things_to_do.html">53 brendan oconnor ai-2007-03-15-Feminists, anarchists, computational complexity, bounded rationality, nethack, and other things to do</a></p>
<p>11 0.37226611 <a title="156-lda-11" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-10-05-Be_careful_with_dictionary-based_text_analysis.html">176 brendan oconnor ai-2011-10-05-Be careful with dictionary-based text analysis</a></p>
<p>12 0.37224045 <a title="156-lda-12" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-10-02-Powerset%E2%80%99s_natural_language_search_system.html">188 brendan oconnor ai-2012-10-02-Powerset’s natural language search system</a></p>
<p>13 0.36938155 <a title="156-lda-13" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-05-18-Announcing_TweetMotif_for_summarizing_twitter_topics.html">140 brendan oconnor ai-2009-05-18-Announcing TweetMotif for summarizing twitter topics</a></p>
<p>14 0.35250017 <a title="156-lda-14" href="../brendan_oconnor_ai-2006/brendan_oconnor_ai-2006-08-30-A_big%2C_fun_list_of_links_I%E2%80%99m_reading.html">44 brendan oconnor ai-2006-08-30-A big, fun list of links I’m reading</a></p>
<p>15 0.34496564 <a title="156-lda-15" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-12-31-List_of_probabilistic_model_mini-language_toolkits.html">157 brendan oconnor ai-2009-12-31-List of probabilistic model mini-language toolkits</a></p>
<p>16 0.3316893 <a title="156-lda-16" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-08-21-ConnectU.com_SQL_injection_vulnerability%3A_a_story_of_pathetic_hubris_%28and_fun_with_the_password_%E2%80%98password%E2%80%99%29.html">76 brendan oconnor ai-2007-08-21-ConnectU.com SQL injection vulnerability: a story of pathetic hubris (and fun with the password ‘password’)</a></p>
<p>17 0.33124822 <a title="156-lda-17" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-12-20-Data-driven_charity.html">86 brendan oconnor ai-2007-12-20-Data-driven charity</a></p>
<p>18 0.32735476 <a title="156-lda-18" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-08-20-Some_analysis_of_tweet_shares_and_%E2%80%9Cpredicting%E2%80%9D_election_outcomes.html">198 brendan oconnor ai-2013-08-20-Some analysis of tweet shares and “predicting” election outcomes</a></p>
<p>19 0.32576069 <a title="156-lda-19" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-08-08-Haghighi_and_Klein_%282009%29%3A_Simple_Coreference_Resolution_with_Rich_Syntactic_and_Semantic_Features.html">150 brendan oconnor ai-2009-08-08-Haghighi and Klein (2009): Simple Coreference Resolution with Rich Syntactic and Semantic Features</a></p>
<p>20 0.32161707 <a title="156-lda-20" href="../brendan_oconnor_ai-2014/brendan_oconnor_ai-2014-04-26-Replot%3A_departure_delays_vs_flight_time_speed-up.html">204 brendan oconnor ai-2014-04-26-Replot: departure delays vs flight time speed-up</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
