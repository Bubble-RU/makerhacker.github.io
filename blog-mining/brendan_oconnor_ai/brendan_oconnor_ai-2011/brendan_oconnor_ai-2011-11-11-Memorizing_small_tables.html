<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>177 brendan oconnor ai-2011-11-11-Memorizing small tables</title>
</head>

<body>
<p><a title="brendan_oconnor_ai" href="../brendan_oconnor_ai_home.html">brendan_oconnor_ai</a> <a title="brendan_oconnor_ai-2011" href="../home/brendan_oconnor_ai-2011_home.html">brendan_oconnor_ai-2011</a> <a title="brendan_oconnor_ai-2011-177" href="#">brendan_oconnor_ai-2011-177</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>177 brendan oconnor ai-2011-11-11-Memorizing small tables</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="brendan_oconnor_ai-2011-177-html" href="http://brenocon.com/blog/2011/11/memorizing-small-tables/">html</a></p><p>Introduction: Lately, I’ve been trying  to memorize very small tables, especially for better intuitions and rule-of-thumb calculations.  At the moment I have these above my desk:
 
   
 
The first one is a few entries in a natural logarithm table.  There are all these stories about how in the slide rule era, people would develop better intuitions about the scale of logarithms because they physically engaged with them all the time.  I spend lots of time looking at log-likelihoods, log-odds-ratios, and logistic regression coefficients, so I think it would be nice to have quick intuitions about what they are.  (Though the  Gelman and Hill  textbook has an interesting argument against odds scale interpretations of logistic regression coefficients.)
 
The second one are some zsh filename manipulation  shortcuts .  OK, this is more narrow than the others, but pretty useful for me at least.
 
The third one are rough unit equivalencies for data rates over time.  I find this very important for quickly determ</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Lately, I’ve been trying  to memorize very small tables, especially for better intuitions and rule-of-thumb calculations. [sent-1, score-0.45]
</p><p>2 At the moment I have these above my desk:         The first one is a few entries in a natural logarithm table. [sent-2, score-0.296]
</p><p>3 There are all these stories about how in the slide rule era, people would develop better intuitions about the scale of logarithms because they physically engaged with them all the time. [sent-3, score-0.959]
</p><p>4 I spend lots of time looking at log-likelihoods, log-odds-ratios, and logistic regression coefficients, so I think it would be nice to have quick intuitions about what they are. [sent-4, score-0.641]
</p><p>5 (Though the  Gelman and Hill  textbook has an interesting argument against odds scale interpretations of logistic regression coefficients. [sent-5, score-0.755]
</p><p>6 )   The second one are some zsh filename manipulation  shortcuts . [sent-6, score-0.539]
</p><p>7 OK, this is more narrow than the others, but pretty useful for me at least. [sent-7, score-0.205]
</p><p>8 The third one are rough unit equivalencies for data rates over time. [sent-8, score-0.378]
</p><p>9 I find this very important for quickly determining whether a long-running job is going to take a dozen minutes, or a few hours, or a few days. [sent-9, score-0.193]
</p><p>10 In particular, many data transfer commands (scp, wget, s3cmd) immediately tell you a rate per second, which you then can scale up. [sent-10, score-0.428]
</p><p>11 (And if you’re using a CPU-bound pipeline command, you can always use the amazing  pv  command to get a rate-per-second estimate. [sent-11, score-0.345]
</p><p>12 )  This table is inspired by the  “Numbers Everyone Should Know”  list. [sent-12, score-0.091]
</p><p>13 The fourth one is the  Clopper-Pearson  binomial confidence interval. [sent-13, score-0.623]
</p><p>14 Actually, the more useful ones to memorize are  Wald binomial intervals , which are easy because they’re close to \(\pm 1/\sqrt{n}\). [sent-14, score-0.656]
</p><p>15 This sticky is actually the relevant R calls (type  binom. [sent-16, score-0.107]
</p><p>16 test  and press enter); I was using small-n binomial hypothesis testing a lot recently so wanted to get more used to it. [sent-17, score-0.506]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('binomial', 0.322), ('command', 0.247), ('intuitions', 0.235), ('scale', 0.235), ('memorize', 0.215), ('logistic', 0.183), ('regression', 0.132), ('useful', 0.119), ('second', 0.115), ('pm', 0.107), ('hill', 0.107), ('manipulation', 0.107), ('enter', 0.107), ('engaged', 0.107), ('fourth', 0.107), ('interpretations', 0.107), ('relevant', 0.107), ('develop', 0.107), ('dozen', 0.107), ('era', 0.107), ('filename', 0.107), ('shortcuts', 0.107), ('desk', 0.107), ('entries', 0.107), ('transfer', 0.107), ('one', 0.103), ('odds', 0.098), ('rough', 0.098), ('lately', 0.098), ('press', 0.098), ('pipeline', 0.098), ('logarithms', 0.098), ('sqrt', 0.098), ('party', 0.091), ('rates', 0.091), ('spend', 0.091), ('inspired', 0.091), ('slide', 0.091), ('coefficients', 0.091), ('confidence', 0.091), ('narrow', 0.086), ('rule', 0.086), ('hours', 0.086), ('testing', 0.086), ('moment', 0.086), ('unit', 0.086), ('tables', 0.086), ('quickly', 0.086), ('minutes', 0.086), ('rate', 0.086)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="177-tfidf-1" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-11-11-Memorizing_small_tables.html">177 brendan oconnor ai-2011-11-11-Memorizing small tables</a></p>
<p>Introduction: Lately, I’ve been trying  to memorize very small tables, especially for better intuitions and rule-of-thumb calculations.  At the moment I have these above my desk:
 
   
 
The first one is a few entries in a natural logarithm table.  There are all these stories about how in the slide rule era, people would develop better intuitions about the scale of logarithms because they physically engaged with them all the time.  I spend lots of time looking at log-likelihoods, log-odds-ratios, and logistic regression coefficients, so I think it would be nice to have quick intuitions about what they are.  (Though the  Gelman and Hill  textbook has an interesting argument against odds scale interpretations of logistic regression coefficients.)
 
The second one are some zsh filename manipulation  shortcuts .  OK, this is more narrow than the others, but pretty useful for me at least.
 
The third one are rough unit equivalencies for data rates over time.  I find this very important for quickly determ</p><p>2 0.11140388 <a title="177-tfidf-2" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-01-11-Please_report_your_SVM%E2%80%99s_kernel%21.html">164 brendan oconnor ai-2011-01-11-Please report your SVM’s kernel!</a></p>
<p>Introduction: I’m tired of reading papers that use an  SVM  but don’t say which kernel they used.  (There’s tons of such papers in NLP and, I think, other areas that do applied machine learning.)  I suspect a lot of these papers are actually using a linear kernel.
 
An un-kernelized, linear SVM is nearly the same as  logistic regression  — every feature independently increases or decreases the classifier’s output prediction.  But a quadratic kernelized SVM is much more like boosted depth-2 decision trees.  It can do automatic combinations of pairs of features — a potentially very different thing, since you can start throwing in features that don’t do anything on their own but might have useful interactions with others.  (And of course, more complicated kernels do progressively more complicated and non-linear things.)
 
I have heard people say they download an SVM package, try a bunch of different kernels, and find the linear kernel is the best. In such cases they could have just used a logistic regr</p><p>3 0.086871877 <a title="177-tfidf-3" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-17-p-values%2C_CDF%E2%80%99s%2C_NLP_etc..html">185 brendan oconnor ai-2012-07-17-p-values, CDF’s, NLP etc.</a></p>
<p>Introduction: Update Aug 10: THIS IS NOT A SUMMARY OF THE WHOLE PAPER!  it’s whining about one particular method of analysis before talking about other things further down
   
A quick note on  Berg-Kirkpatrick et al EMNLP-2012, “An Empirical Investigation of Statistical Signiﬁcance in NLP” .  They make lots of graphs of p-values against observed magnitudes and talk about “curves”, e.g.
 
 We see the same curve-shaped trend we saw for summarization and dependency parsing. Different group comparisons, same group comparisons, and system combination comparisons form distinct curves. 
 
For example, Figure 2.
 
   
 
I fear they made 10 graphs to rediscover a basic statistical fact: a p-value comes from a null hypothesis CDF.  That’s what these “curve-shaped trends” are in all their graphs.  They are CDFs.
 
To back up, the statistical significance testing question is whether, in their notation, the observed dataset performance difference \(\delta(x)\) is “real” or not: if you were to resample the data,</p><p>4 0.085355848 <a title="177-tfidf-4" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-04-06-a_regression_slope_is_a_weighted_average_of_pairs%E2%80%99_slopes%21.html">100 brendan oconnor ai-2008-04-06-a regression slope is a weighted average of pairs’ slopes!</a></p>
<p>Introduction: Wow, this is pretty cool:
 
 
 
From an  Andrew Gelman article  on summaring a linear regression as a simple difference between upper and lower categories.  I get the impression there are lots of weird misunderstood corners of linear models… (e.g. that “least squares regression” is a maximum likelihood estimator for a linear model with normal noise… I know so many people who didn’t learn that from their stats whatever course, and therefore find it mystifying why squared error should be used…  see this  other post from Gelman .)</p><p>5 0.077489525 <a title="177-tfidf-5" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-10-31-tanh_is_a_rescaled_logistic_sigmoid_function.html">201 brendan oconnor ai-2013-10-31-tanh is a rescaled logistic sigmoid function</a></p>
<p>Introduction: This confused me for a while when I first learned it, so in case it helps anyone else:
 
The   logistic sigmoid   function, a.k.a. the  inverse logit  function, is
 
\[ g(x) = \frac{ e^x }{1 + e^x} \]
 
Its outputs range from 0 to 1, and are often interpreted as probabilities (in, say, logistic regression).
 
The   tanh   function, a.k.a. hyperbolic tangent function, is a rescaling of the logistic sigmoid, such that its outputs range from -1 to 1.  (There’s horizontal stretching as well.)
 
\[ tanh(x) = 2 g(2x) - 1 \]
 
It’s easy to show the above leads to the standard definition \( tanh(x) = \frac{e^x – e^{-x}}{e^x + e^{-x}} \).  The (-1,+1) output range tends to be more convenient for neural networks, so tanh functions show up there a lot.
 
The two functions are plotted below.  Blue is the logistic function, and red is tanh.</p><p>6 0.076189615 <a title="177-tfidf-6" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-02-02-Histograms_%E2%80%94_matplotlib_vs._R.html">179 brendan oconnor ai-2012-02-02-Histograms — matplotlib vs. R</a></p>
<p>7 0.075543858 <a title="177-tfidf-7" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-11-24-Graphs_for_SANCL-2012_web_parsing_results.html">189 brendan oconnor ai-2012-11-24-Graphs for SANCL-2012 web parsing results</a></p>
<p>8 0.066059142 <a title="177-tfidf-8" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-08-20-Some_analysis_of_tweet_shares_and_%E2%80%9Cpredicting%E2%80%9D_election_outcomes.html">198 brendan oconnor ai-2013-08-20-Some analysis of tweet shares and “predicting” election outcomes</a></p>
<p>9 0.062119 <a title="177-tfidf-9" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-06-25-idea%3A_Morals_are_heuristics_for_socially_optimal_behavior.html">6 brendan oconnor ai-2005-06-25-idea: Morals are heuristics for socially optimal behavior</a></p>
<p>10 0.062114008 <a title="177-tfidf-10" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-03-Statistics_vs._Machine_Learning%2C_fight%21.html">129 brendan oconnor ai-2008-12-03-Statistics vs. Machine Learning, fight!</a></p>
<p>11 0.060551781 <a title="177-tfidf-11" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-07-15-Beta_conjugate_explorer.html">146 brendan oconnor ai-2009-07-15-Beta conjugate explorer</a></p>
<p>12 0.060516477 <a title="177-tfidf-12" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-01-07-Love_it_and_hate_it%2C_R_has_come_of_age.html">132 brendan oconnor ai-2009-01-07-Love it and hate it, R has come of age</a></p>
<p>13 0.058009282 <a title="177-tfidf-13" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-08-08-Haghighi_and_Klein_%282009%29%3A_Simple_Coreference_Resolution_with_Rich_Syntactic_and_Semantic_Features.html">150 brendan oconnor ai-2009-08-08-Haghighi and Klein (2009): Simple Coreference Resolution with Rich Syntactic and Semantic Features</a></p>
<p>14 0.056989476 <a title="177-tfidf-14" href="../brendan_oconnor_ai-2006/brendan_oconnor_ai-2006-06-03-Rock%2C_Paper%2C_Scissors.html">39 brendan oconnor ai-2006-06-03-Rock, Paper, Scissors</a></p>
<p>15 0.055749539 <a title="177-tfidf-15" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-05-20-Log-normal_and_logistic-normal_terminology.html">169 brendan oconnor ai-2011-05-20-Log-normal and logistic-normal terminology</a></p>
<p>16 0.05572943 <a title="177-tfidf-16" href="../brendan_oconnor_ai-2014/brendan_oconnor_ai-2014-04-26-Replot%3A_departure_delays_vs_flight_time_speed-up.html">204 brendan oconnor ai-2014-04-26-Replot: departure delays vs flight time speed-up</a></p>
<p>17 0.055099458 <a title="177-tfidf-17" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-10-11-It_is_accurate_to_determine_a_blog%E2%80%99s_bias_by_what_it_links_to.html">117 brendan oconnor ai-2008-10-11-It is accurate to determine a blog’s bias by what it links to</a></p>
<p>18 0.054708388 <a title="177-tfidf-18" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-12-20-Data-driven_charity.html">86 brendan oconnor ai-2007-12-20-Data-driven charity</a></p>
<p>19 0.053076483 <a title="177-tfidf-19" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-03-14-R_scan%28%29_for_quick-and-dirty_checks.html">192 brendan oconnor ai-2013-03-14-R scan() for quick-and-dirty checks</a></p>
<p>20 0.052160867 <a title="177-tfidf-20" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-11-05-Obama_street_celebrations_in_San_Francisco.html">122 brendan oconnor ai-2008-11-05-Obama street celebrations in San Francisco</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/brendan_oconnor_ai_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, -0.203), (1, -0.074), (2, 0.078), (3, -0.074), (4, -0.03), (5, 0.021), (6, -0.031), (7, -0.063), (8, 0.004), (9, -0.073), (10, 0.062), (11, -0.084), (12, -0.041), (13, -0.055), (14, -0.016), (15, 0.028), (16, -0.086), (17, 0.116), (18, -0.011), (19, -0.061), (20, -0.045), (21, 0.026), (22, -0.023), (23, 0.125), (24, -0.055), (25, 0.111), (26, -0.114), (27, -0.02), (28, -0.053), (29, -0.049), (30, -0.006), (31, 0.058), (32, 0.145), (33, 0.004), (34, 0.015), (35, -0.122), (36, 0.027), (37, -0.038), (38, 0.012), (39, 0.052), (40, 0.051), (41, 0.079), (42, 0.092), (43, -0.036), (44, 0.019), (45, -0.06), (46, -0.111), (47, 0.007), (48, -0.046), (49, 0.172)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9695242 <a title="177-lsi-1" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-11-11-Memorizing_small_tables.html">177 brendan oconnor ai-2011-11-11-Memorizing small tables</a></p>
<p>Introduction: Lately, I’ve been trying  to memorize very small tables, especially for better intuitions and rule-of-thumb calculations.  At the moment I have these above my desk:
 
   
 
The first one is a few entries in a natural logarithm table.  There are all these stories about how in the slide rule era, people would develop better intuitions about the scale of logarithms because they physically engaged with them all the time.  I spend lots of time looking at log-likelihoods, log-odds-ratios, and logistic regression coefficients, so I think it would be nice to have quick intuitions about what they are.  (Though the  Gelman and Hill  textbook has an interesting argument against odds scale interpretations of logistic regression coefficients.)
 
The second one are some zsh filename manipulation  shortcuts .  OK, this is more narrow than the others, but pretty useful for me at least.
 
The third one are rough unit equivalencies for data rates over time.  I find this very important for quickly determ</p><p>2 0.66170639 <a title="177-lsi-2" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-01-11-Please_report_your_SVM%E2%80%99s_kernel%21.html">164 brendan oconnor ai-2011-01-11-Please report your SVM’s kernel!</a></p>
<p>Introduction: I’m tired of reading papers that use an  SVM  but don’t say which kernel they used.  (There’s tons of such papers in NLP and, I think, other areas that do applied machine learning.)  I suspect a lot of these papers are actually using a linear kernel.
 
An un-kernelized, linear SVM is nearly the same as  logistic regression  — every feature independently increases or decreases the classifier’s output prediction.  But a quadratic kernelized SVM is much more like boosted depth-2 decision trees.  It can do automatic combinations of pairs of features — a potentially very different thing, since you can start throwing in features that don’t do anything on their own but might have useful interactions with others.  (And of course, more complicated kernels do progressively more complicated and non-linear things.)
 
I have heard people say they download an SVM package, try a bunch of different kernels, and find the linear kernel is the best. In such cases they could have just used a logistic regr</p><p>3 0.59842575 <a title="177-lsi-3" href="../brendan_oconnor_ai-2014/brendan_oconnor_ai-2014-04-26-Replot%3A_departure_delays_vs_flight_time_speed-up.html">204 brendan oconnor ai-2014-04-26-Replot: departure delays vs flight time speed-up</a></p>
<p>Introduction: Here’s a re-plotting of a graph in  this 538 post .  It’s looking at whether pilots speed up the flight when there’s a delay, and find that it looks like that’s the case.  This is averaged data for flights on several major transcontinental routes.
 
I’ve replotted the main graph as follows.  The x-axis is departure delay.  The y-axis is the total trip time — number of minutes since the scheduled departure time.  For an on-time departure, the average flight is 5 hours, 44 minutes.  The blue line shows what the total trip time would be if the delayed flight took that long.  Gray lines are uncertainty (I think the CI due to averaging).
 
   
 
What’s going on is, the pilots seem to be targeting a total trip time of 370-380 minutes or so.  If the departure is only slightly delayed by 10 minutes, the flight time is still the same, but delays in the 30-50 minutes range see a faster flight time which makes up for some of the delay.
 
The original post plotted the y-axis as the delta against t</p><p>4 0.57131225 <a title="177-lsi-4" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-10-31-tanh_is_a_rescaled_logistic_sigmoid_function.html">201 brendan oconnor ai-2013-10-31-tanh is a rescaled logistic sigmoid function</a></p>
<p>Introduction: This confused me for a while when I first learned it, so in case it helps anyone else:
 
The   logistic sigmoid   function, a.k.a. the  inverse logit  function, is
 
\[ g(x) = \frac{ e^x }{1 + e^x} \]
 
Its outputs range from 0 to 1, and are often interpreted as probabilities (in, say, logistic regression).
 
The   tanh   function, a.k.a. hyperbolic tangent function, is a rescaling of the logistic sigmoid, such that its outputs range from -1 to 1.  (There’s horizontal stretching as well.)
 
\[ tanh(x) = 2 g(2x) - 1 \]
 
It’s easy to show the above leads to the standard definition \( tanh(x) = \frac{e^x – e^{-x}}{e^x + e^{-x}} \).  The (-1,+1) output range tends to be more convenient for neural networks, so tanh functions show up there a lot.
 
The two functions are plotted below.  Blue is the logistic function, and red is tanh.</p><p>5 0.54653132 <a title="177-lsi-5" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-02-02-Histograms_%E2%80%94_matplotlib_vs._R.html">179 brendan oconnor ai-2012-02-02-Histograms — matplotlib vs. R</a></p>
<p>Introduction: When possible, I like to use R for its really, really good statistical visualization capabilities.  I’m doing a modeling project in Python right now (R is too slow, bad at large data, bad at structured data, etc.), and in comparison to base R, the matplotlib library is just painful.  I wrote a toy  Metropolis  sampler for a  triangle distribution  and all I want to see is whether it looks like it’s working.  For the same dataset, here are histograms with default settings.  (Python:  pylab.hist(d) , R:  hist(d) )
 
   
 
I want to know whether my Metropolis sampler is working; those two plots give a very different idea.  Of course, you could say this is an unfair comparison, since matplotlib is only using 10 bins, while R is using 18 here — and it’s always important to vary the bin size a few times when looking at histograms.  But R’s defaults really are better: it actually uses an adaptive bin size, and the heuristic worked, choosing a reasonable number for the data.  The  hist()  manu</p><p>6 0.46302882 <a title="177-lsi-6" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-17-p-values%2C_CDF%E2%80%99s%2C_NLP_etc..html">185 brendan oconnor ai-2012-07-17-p-values, CDF’s, NLP etc.</a></p>
<p>7 0.45809507 <a title="177-lsi-7" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-04-06-a_regression_slope_is_a_weighted_average_of_pairs%E2%80%99_slopes%21.html">100 brendan oconnor ai-2008-04-06-a regression slope is a weighted average of pairs’ slopes!</a></p>
<p>8 0.43014243 <a title="177-lsi-8" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-05-20-Log-normal_and_logistic-normal_terminology.html">169 brendan oconnor ai-2011-05-20-Log-normal and logistic-normal terminology</a></p>
<p>9 0.42941171 <a title="177-lsi-9" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-04-13-Are_women_discriminated_against_in_graduate_admissions%3F_Simpson%E2%80%99s_paradox_via_R_in_three_easy_steps%21.html">101 brendan oconnor ai-2008-04-13-Are women discriminated against in graduate admissions? Simpson’s paradox via R in three easy steps!</a></p>
<p>10 0.40163189 <a title="177-lsi-10" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-09-15-Dollar_auction.html">77 brendan oconnor ai-2007-09-15-Dollar auction</a></p>
<p>11 0.39620426 <a title="177-lsi-11" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-04-08-Rough_binomial_confidence_intervals.html">167 brendan oconnor ai-2011-04-08-Rough binomial confidence intervals</a></p>
<p>12 0.38790092 <a title="177-lsi-12" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-08-16-A_better_Obama_vs_McCain_poll_aggregation.html">111 brendan oconnor ai-2008-08-16-A better Obama vs McCain poll aggregation</a></p>
<p>13 0.37980694 <a title="177-lsi-13" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-03-14-R_scan%28%29_for_quick-and-dirty_checks.html">192 brendan oconnor ai-2013-03-14-R scan() for quick-and-dirty checks</a></p>
<p>14 0.35915592 <a title="177-lsi-14" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-03-02-Poor_man%E2%80%99s_linear_algebra_textbook.html">166 brendan oconnor ai-2011-03-02-Poor man’s linear algebra textbook</a></p>
<p>15 0.35764939 <a title="177-lsi-15" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-11-24-Graphs_for_SANCL-2012_web_parsing_results.html">189 brendan oconnor ai-2012-11-24-Graphs for SANCL-2012 web parsing results</a></p>
<p>16 0.329521 <a title="177-lsi-16" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-06-25-idea%3A_Morals_are_heuristics_for_socially_optimal_behavior.html">6 brendan oconnor ai-2005-06-25-idea: Morals are heuristics for socially optimal behavior</a></p>
<p>17 0.31440616 <a title="177-lsi-17" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-08-20-Some_analysis_of_tweet_shares_and_%E2%80%9Cpredicting%E2%80%9D_election_outcomes.html">198 brendan oconnor ai-2013-08-20-Some analysis of tweet shares and “predicting” election outcomes</a></p>
<p>18 0.31161007 <a title="177-lsi-18" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-12-20-Data-driven_charity.html">86 brendan oconnor ai-2007-12-20-Data-driven charity</a></p>
<p>19 0.30960703 <a title="177-lsi-19" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-08-21-Berkeley_SDA_and_the_General_Social_Survey.html">186 brendan oconnor ai-2012-08-21-Berkeley SDA and the General Social Survey</a></p>
<p>20 0.30672777 <a title="177-lsi-20" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-01-05-Indicators_of_a_crackpot_paper.html">88 brendan oconnor ai-2008-01-05-Indicators of a crackpot paper</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/brendan_oconnor_ai_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(14, 0.636), (16, 0.028), (44, 0.063), (55, 0.047), (74, 0.126)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94005299 <a title="177-lda-1" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-11-11-Memorizing_small_tables.html">177 brendan oconnor ai-2011-11-11-Memorizing small tables</a></p>
<p>Introduction: Lately, I’ve been trying  to memorize very small tables, especially for better intuitions and rule-of-thumb calculations.  At the moment I have these above my desk:
 
   
 
The first one is a few entries in a natural logarithm table.  There are all these stories about how in the slide rule era, people would develop better intuitions about the scale of logarithms because they physically engaged with them all the time.  I spend lots of time looking at log-likelihoods, log-odds-ratios, and logistic regression coefficients, so I think it would be nice to have quick intuitions about what they are.  (Though the  Gelman and Hill  textbook has an interesting argument against odds scale interpretations of logistic regression coefficients.)
 
The second one are some zsh filename manipulation  shortcuts .  OK, this is more narrow than the others, but pretty useful for me at least.
 
The third one are rough unit equivalencies for data rates over time.  I find this very important for quickly determ</p><p>2 0.91700071 <a title="177-lda-2" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-06-11-Richard_Rorty_has_died.html">64 brendan oconnor ai-2007-06-11-Richard Rorty has died</a></p>
<p>Introduction: Richard Rorty, philosopher, dies at 75 .
 
I’ve read enough of the analytic philosophers castigating Rorty — and taken bits of classes from a few of them — that I feel I just have to love the man.
 
I remember managing to see him speak twice.  Once was on philosophy of mind at the good ol’ Sym Sys Forum.   (Video!)   (“He is wrong, but wrong in such an interesting way!” I remember one comment.)
 
Most fascinating was when he gamely participated in a discussion at this very odd Christian thought conference some groups on campus put together.  (The Veritas Forum,  here’s a link .)  He was standing there, arguing with the Christian conservatives about the nature and legitimacy of authority, but humorously ceding ground where appropriate… “Look, it’s not that all children will be active critical thinkers and discover everything for themselves.  Getting a kid a secular liberal education isn’t that much different than any other education — you have to beat it in to them.”  (That is a paraphr</p><p>3 0.23775312 <a title="177-lda-3" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-12-26-What_is_experimental_philosophy%3F.html">87 brendan oconnor ai-2007-12-26-What is experimental philosophy?</a></p>
<p>Introduction: Experimental philosophy:  
 Suppose the chairman of a company has to decide whether to adopt a new program. It would increase profits and help the environment too. “I don’t care at all about helping the environment,” the chairman says. “I just want to make as much profit as I can. Let’s start the new program.” Would you say that the chairman intended to help the environment? 
 
O.K., same circumstance. Except this time the program would harm the environment. The chairman, who still couldn’t care less about the environment, authorizes the program in order to get those profits. As expected, the bottom line goes up, the environment goes down. Would you say the chairman harmed the environment intentionally?
 
in one survey, only 23 percent of people said that the chairman in the first situation had intentionally helped the environment. When they had to think about the second situation, though, fully 82 percent thought that the chairman had intentionally harmed the environment. There’s plen</p><p>4 0.21202442 <a title="177-lda-4" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-11-12-Disease_tracking_with_web_queries_and_social_messaging_%28Google%2C_Twitter%2C_Facebook%E2%80%A6%29.html">123 brendan oconnor ai-2008-11-12-Disease tracking with web queries and social messaging (Google, Twitter, Facebook…)</a></p>
<p>Introduction: This is a good idea: in a search engine’s query logs, look for outbreaks of queries like [[flu symptoms]] in a given region.  I’ve heard (from  Roddy ) that this trick also works well on Facebook statuses (e.g. “Feeling crappy this morning, think I just got the flu”).
  
  Google Uses Web Searches to Track Flu’s Spread – NYTimes.com  
  Google Flu Trends – google.org  
  
For an example with a publicly available data feed, these queries works decently well on Twitter search:
 
 [[ flu -shot -google ]]  (high recall)
 
 [[ "muscle aches" flu -shot ]]  (high precision)
     
 
The “muscle aches” query is too sparse and the general query is too noisy, but you could imagine some more tricks to clean it up, then train a classifier, etc.  With a bit more work it looks like geolocation information can be had out of the  Twitter search API .</p><p>5 0.21105719 <a title="177-lda-5" href="../brendan_oconnor_ai-2014/brendan_oconnor_ai-2014-02-19-What_the_ACL-2014_review_scores_mean.html">203 brendan oconnor ai-2014-02-19-What the ACL-2014 review scores mean</a></p>
<p>Introduction: I’ve had several people ask me what the numbers in  ACL  reviews mean — and I can’t find anywhere online where they’re described.  (Can anyone point this out if it is somewhere?)
 
So here’s the review form, below.  They all go from 1 to 5, with 5 the best.  I think the review emails to authors only include a subset of the below — for example, “Overall Recommendation” is not included?
 
The CFP said that they have different types of review forms for different types of papers.  I think this one is for a standard full paper.  I guess what people  really  want to know is what scores tend to correspond to acceptances.  I really have no idea and I get the impression this can change year to year.  I have no involvement with the ACL conference besides being one of many, many reviewers.
 
  
  
APPROPRIATENESS (1-5)
Does the paper fit in ACL 2014? (Please answer this question in light of the desire to broaden the scope of the research areas represented at ACL.) 

5: Certainly. 
4: Probabl</p><p>6 0.20607115 <a title="177-lda-6" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-09-02-cognitive_modelling_is_rational_choice%2B%2B.html">26 brendan oconnor ai-2005-09-02-cognitive modelling is rational choice++</a></p>
<p>7 0.20586139 <a title="177-lda-7" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-06-10-Freak-Freakonomics_%28Ariel_Rubinstein_is_the_shit%21%29.html">63 brendan oconnor ai-2007-06-10-Freak-Freakonomics (Ariel Rubinstein is the shit!)</a></p>
<p>8 0.20407073 <a title="177-lda-8" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-06-05-Clinton-Obama_support_visualization.html">105 brendan oconnor ai-2008-06-05-Clinton-Obama support visualization</a></p>
<p>9 0.20300837 <a title="177-lda-9" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-10-Don%E2%80%99t_MAWK_AWK_%E2%80%93_the_fastest_and_most_elegant_big_data_munging_language%21.html">154 brendan oconnor ai-2009-09-10-Don’t MAWK AWK – the fastest and most elegant big data munging language!</a></p>
<p>10 0.202583 <a title="177-lda-10" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-09-15-Dollar_auction.html">77 brendan oconnor ai-2007-09-15-Dollar auction</a></p>
<p>11 0.20016845 <a title="177-lda-11" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-07-09-the_psychology_of_design_as_explanation.html">19 brendan oconnor ai-2005-07-09-the psychology of design as explanation</a></p>
<p>12 0.19644873 <a title="177-lda-12" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-17-1_billion_web_page_dataset_from_CMU.html">138 brendan oconnor ai-2009-04-17-1 billion web page dataset from CMU</a></p>
<p>13 0.19475034 <a title="177-lda-13" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-12-20-Data-driven_charity.html">86 brendan oconnor ai-2007-12-20-Data-driven charity</a></p>
<p>14 0.19417822 <a title="177-lda-14" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-03-Statistics_vs._Machine_Learning%2C_fight%21.html">129 brendan oconnor ai-2008-12-03-Statistics vs. Machine Learning, fight!</a></p>
<p>15 0.19356294 <a title="177-lda-15" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-08-Another_R_flashmob_today.html">152 brendan oconnor ai-2009-09-08-Another R flashmob today</a></p>
<p>16 0.18611601 <a title="177-lda-16" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-10-05-Be_careful_with_dictionary-based_text_analysis.html">176 brendan oconnor ai-2011-10-05-Be careful with dictionary-based text analysis</a></p>
<p>17 0.18507183 <a title="177-lda-17" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-10-11-It_is_accurate_to_determine_a_blog%E2%80%99s_bias_by_what_it_links_to.html">117 brendan oconnor ai-2008-10-11-It is accurate to determine a blog’s bias by what it links to</a></p>
<p>18 0.18486914 <a title="177-lda-18" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-06-17-%22Time_will_tell%2C_epistemology_won%E2%80%99t%22.html">65 brendan oconnor ai-2007-06-17-"Time will tell, epistemology won’t"</a></p>
<p>19 0.18328834 <a title="177-lda-19" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-10-02-Powerset%E2%80%99s_natural_language_search_system.html">188 brendan oconnor ai-2012-10-02-Powerset’s natural language search system</a></p>
<p>20 0.1817985 <a title="177-lda-20" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-08-20-Some_analysis_of_tweet_shares_and_%E2%80%9Cpredicting%E2%80%9D_election_outcomes.html">198 brendan oconnor ai-2013-08-20-Some analysis of tweet shares and “predicting” election outcomes</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
