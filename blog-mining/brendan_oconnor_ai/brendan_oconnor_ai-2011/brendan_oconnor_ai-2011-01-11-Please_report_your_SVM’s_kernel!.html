<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>164 brendan oconnor ai-2011-01-11-Please report your SVM’s kernel!</title>
</head>

<body>
<p><a title="brendan_oconnor_ai" href="../brendan_oconnor_ai_home.html">brendan_oconnor_ai</a> <a title="brendan_oconnor_ai-2011" href="../home/brendan_oconnor_ai-2011_home.html">brendan_oconnor_ai-2011</a> <a title="brendan_oconnor_ai-2011-164" href="#">brendan_oconnor_ai-2011-164</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>164 brendan oconnor ai-2011-01-11-Please report your SVM’s kernel!</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="brendan_oconnor_ai-2011-164-html" href="http://brenocon.com/blog/2011/01/please-report-your-svms-kernel/">html</a></p><p>Introduction: I’m tired of reading papers that use an  SVM  but don’t say which kernel they used.  (There’s tons of such papers in NLP and, I think, other areas that do applied machine learning.)  I suspect a lot of these papers are actually using a linear kernel.
 
An un-kernelized, linear SVM is nearly the same as  logistic regression  — every feature independently increases or decreases the classifier’s output prediction.  But a quadratic kernelized SVM is much more like boosted depth-2 decision trees.  It can do automatic combinations of pairs of features — a potentially very different thing, since you can start throwing in features that don’t do anything on their own but might have useful interactions with others.  (And of course, more complicated kernels do progressively more complicated and non-linear things.)
 
I have heard people say they download an SVM package, try a bunch of different kernels, and find the linear kernel is the best. In such cases they could have just used a logistic regr</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 I’m tired of reading papers that use an  SVM  but don’t say which kernel they used. [sent-1, score-0.392]
</p><p>2 (There’s tons of such papers in NLP and, I think, other areas that do applied machine learning. [sent-2, score-0.274]
</p><p>3 )  I suspect a lot of these papers are actually using a linear kernel. [sent-3, score-0.505]
</p><p>4 An un-kernelized, linear SVM is nearly the same as  logistic regression  — every feature independently increases or decreases the classifier’s output prediction. [sent-4, score-1.03]
</p><p>5 But a quadratic kernelized SVM is much more like boosted depth-2 decision trees. [sent-5, score-0.133]
</p><p>6 It can do automatic combinations of pairs of features — a potentially very different thing, since you can start throwing in features that don’t do anything on their own but might have useful interactions with others. [sent-6, score-0.648]
</p><p>7 (And of course, more complicated kernels do progressively more complicated and non-linear things. [sent-7, score-0.603]
</p><p>8 )   I have heard people say they download an SVM package, try a bunch of different kernels, and find the linear kernel is the best. [sent-8, score-0.566]
</p><p>9 In such cases they could have just used a logistic regression. [sent-9, score-0.357]
</p><p>10 You can implement SGD for it in a few lines of code! [sent-11, score-0.065]
</p><p>11 )   A linear SVM sometimes has a tiny bit better accuracy than logistic regression, because hinge loss is a tiny bit more like error rate than is log-loss. [sent-12, score-1.247]
</p><p>12 But I really doubt this would matter in any real-world application, where much bigger issues are happening (like data cleanliness, feature engineering, etc. [sent-13, score-0.35]
</p><p>13 )   If a linear classifier is doing better than non-linear ones, that’s saying something pretty important about your problem. [sent-14, score-0.52]
</p><p>14 Saying that you’re using an SVM is missing the point. [sent-15, score-0.149]
</p><p>15 Otherwise it’s just a needlessly complicated variant of logistic regression. [sent-17, score-0.571]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('svm', 0.595), ('logistic', 0.289), ('linear', 0.246), ('complicated', 0.204), ('kernel', 0.195), ('kernels', 0.195), ('regression', 0.156), ('classifier', 0.144), ('papers', 0.14), ('tiny', 0.136), ('saying', 0.13), ('feature', 0.115), ('features', 0.111), ('doubt', 0.085), ('missing', 0.085), ('otherwise', 0.085), ('increases', 0.078), ('loss', 0.078), ('decreases', 0.078), ('hinge', 0.078), ('happening', 0.078), ('variant', 0.078), ('quadratic', 0.078), ('simpler', 0.078), ('throwing', 0.078), ('sgd', 0.078), ('potentially', 0.078), ('bit', 0.077), ('automatic', 0.072), ('bigger', 0.072), ('train', 0.072), ('tons', 0.072), ('application', 0.068), ('cases', 0.068), ('interactions', 0.068), ('nearly', 0.068), ('download', 0.068), ('rate', 0.068), ('combinations', 0.065), ('implement', 0.065), ('pairs', 0.065), ('engineering', 0.065), ('using', 0.064), ('accuracy', 0.062), ('areas', 0.062), ('package', 0.057), ('faster', 0.057), ('say', 0.057), ('decision', 0.055), ('suspect', 0.055)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="164-tfidf-1" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-01-11-Please_report_your_SVM%E2%80%99s_kernel%21.html">164 brendan oconnor ai-2011-01-11-Please report your SVM’s kernel!</a></p>
<p>Introduction: I’m tired of reading papers that use an  SVM  but don’t say which kernel they used.  (There’s tons of such papers in NLP and, I think, other areas that do applied machine learning.)  I suspect a lot of these papers are actually using a linear kernel.
 
An un-kernelized, linear SVM is nearly the same as  logistic regression  — every feature independently increases or decreases the classifier’s output prediction.  But a quadratic kernelized SVM is much more like boosted depth-2 decision trees.  It can do automatic combinations of pairs of features — a potentially very different thing, since you can start throwing in features that don’t do anything on their own but might have useful interactions with others.  (And of course, more complicated kernels do progressively more complicated and non-linear things.)
 
I have heard people say they download an SVM package, try a bunch of different kernels, and find the linear kernel is the best. In such cases they could have just used a logistic regr</p><p>2 0.15345179 <a title="164-tfidf-2" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-04-06-a_regression_slope_is_a_weighted_average_of_pairs%E2%80%99_slopes%21.html">100 brendan oconnor ai-2008-04-06-a regression slope is a weighted average of pairs’ slopes!</a></p>
<p>Introduction: Wow, this is pretty cool:
 
 
 
From an  Andrew Gelman article  on summaring a linear regression as a simple difference between upper and lower categories.  I get the impression there are lots of weird misunderstood corners of linear models… (e.g. that “least squares regression” is a maximum likelihood estimator for a linear model with normal noise… I know so many people who didn’t learn that from their stats whatever course, and therefore find it mystifying why squared error should be used…  see this  other post from Gelman .)</p><p>3 0.1195465 <a title="164-tfidf-3" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-10-31-tanh_is_a_rescaled_logistic_sigmoid_function.html">201 brendan oconnor ai-2013-10-31-tanh is a rescaled logistic sigmoid function</a></p>
<p>Introduction: This confused me for a while when I first learned it, so in case it helps anyone else:
 
The   logistic sigmoid   function, a.k.a. the  inverse logit  function, is
 
\[ g(x) = \frac{ e^x }{1 + e^x} \]
 
Its outputs range from 0 to 1, and are often interpreted as probabilities (in, say, logistic regression).
 
The   tanh   function, a.k.a. hyperbolic tangent function, is a rescaling of the logistic sigmoid, such that its outputs range from -1 to 1.  (There’s horizontal stretching as well.)
 
\[ tanh(x) = 2 g(2x) - 1 \]
 
It’s easy to show the above leads to the standard definition \( tanh(x) = \frac{e^x – e^{-x}}{e^x + e^{-x}} \).  The (-1,+1) output range tends to be more convenient for neural networks, so tanh functions show up there a lot.
 
The two functions are plotted below.  Blue is the logistic function, and red is tanh.</p><p>4 0.11140388 <a title="164-tfidf-4" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-11-11-Memorizing_small_tables.html">177 brendan oconnor ai-2011-11-11-Memorizing small tables</a></p>
<p>Introduction: Lately, I’ve been trying  to memorize very small tables, especially for better intuitions and rule-of-thumb calculations.  At the moment I have these above my desk:
 
   
 
The first one is a few entries in a natural logarithm table.  There are all these stories about how in the slide rule era, people would develop better intuitions about the scale of logarithms because they physically engaged with them all the time.  I spend lots of time looking at log-likelihoods, log-odds-ratios, and logistic regression coefficients, so I think it would be nice to have quick intuitions about what they are.  (Though the  Gelman and Hill  textbook has an interesting argument against odds scale interpretations of logistic regression coefficients.)
 
The second one are some zsh filename manipulation  shortcuts .  OK, this is more narrow than the others, but pretty useful for me at least.
 
The third one are rough unit equivalencies for data rates over time.  I find this very important for quickly determ</p><p>5 0.093979821 <a title="164-tfidf-5" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-08-08-Haghighi_and_Klein_%282009%29%3A_Simple_Coreference_Resolution_with_Rich_Syntactic_and_Semantic_Features.html">150 brendan oconnor ai-2009-08-08-Haghighi and Klein (2009): Simple Coreference Resolution with Rich Syntactic and Semantic Features</a></p>
<p>Introduction: I haven’t done a paper review on this blog for a while, so here we go.
 
 Coreference  resolution is an interesting NLP problem.  ( Examples. )  It involves honest-to-goodness syntactic, semantic, and discourse phenomena, but still seems like a real cognitive task that humans have to solve when reading text [1].  I haven’t read the whole literature, but I’ve always been puzzled by the crop of papers on it I’ve seen in the last year or two.  There’s a big focus on fancy graph/probabilistic/constrained optimization algorithms, but often these papers gloss over the linguistic features — the core information they actually make their decisions with [2].  I never understood why the latter isn’t the most important issue.  Therefore, it was a joy to read
  
 Aria Haghighi and Dan Klein, EMNLP-2009.   “Simple Coreference Resolution with Rich Syntactic and Semantic Features.”  
  
They describe a simple, essentially non-statistical system that outperforms previous unsupervised systems, and compa</p><p>6 0.091800719 <a title="164-tfidf-6" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-08-Patches_to_Rainbow%2C_the_old_text_classifier_that_won%E2%80%99t_go_away.html">153 brendan oconnor ai-2009-09-08-Patches to Rainbow, the old text classifier that won’t go away</a></p>
<p>7 0.082735755 <a title="164-tfidf-7" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-03-Statistics_vs._Machine_Learning%2C_fight%21.html">129 brendan oconnor ai-2008-12-03-Statistics vs. Machine Learning, fight!</a></p>
<p>8 0.079222172 <a title="164-tfidf-8" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-03-02-Poor_man%E2%80%99s_linear_algebra_textbook.html">166 brendan oconnor ai-2011-03-02-Poor man’s linear algebra textbook</a></p>
<p>9 0.075104617 <a title="164-tfidf-9" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-11-24-Graphs_for_SANCL-2012_web_parsing_results.html">189 brendan oconnor ai-2012-11-24-Graphs for SANCL-2012 web parsing results</a></p>
<p>10 0.062911823 <a title="164-tfidf-10" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-01-Binary_classification_evaluation_in_R_via_ROCR.html">136 brendan oconnor ai-2009-04-01-Binary classification evaluation in R via ROCR</a></p>
<p>11 0.062332805 <a title="164-tfidf-11" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-05-20-Log-normal_and_logistic-normal_terminology.html">169 brendan oconnor ai-2011-05-20-Log-normal and logistic-normal terminology</a></p>
<p>12 0.059504762 <a title="164-tfidf-12" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-07-01-Bias_correction_sneak_peek%21.html">108 brendan oconnor ai-2008-07-01-Bias correction sneak peek!</a></p>
<p>13 0.054225493 <a title="164-tfidf-13" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-10-Don%E2%80%99t_MAWK_AWK_%E2%80%93_the_fastest_and_most_elegant_big_data_munging_language%21.html">154 brendan oconnor ai-2009-09-10-Don’t MAWK AWK – the fastest and most elegant big data munging language!</a></p>
<p>14 0.05240012 <a title="164-tfidf-14" href="../brendan_oconnor_ai-2014/brendan_oconnor_ai-2014-02-19-What_the_ACL-2014_review_scores_mean.html">203 brendan oconnor ai-2014-02-19-What the ACL-2014 review scores mean</a></p>
<p>15 0.049080469 <a title="164-tfidf-15" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-04-08-More_fun_with_Gapminder_-_Trendalyzer.html">58 brendan oconnor ai-2007-04-08-More fun with Gapminder - Trendalyzer</a></p>
<p>16 0.047755063 <a title="164-tfidf-16" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-11-21-Netflix_Prize.html">125 brendan oconnor ai-2008-11-21-Netflix Prize</a></p>
<p>17 0.046600781 <a title="164-tfidf-17" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-27-Facebook_sentiment_mining_predicts_presidential_polls.html">131 brendan oconnor ai-2008-12-27-Facebook sentiment mining predicts presidential polls</a></p>
<p>18 0.045723263 <a title="164-tfidf-18" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-08-20-Some_analysis_of_tweet_shares_and_%E2%80%9Cpredicting%E2%80%9D_election_outcomes.html">198 brendan oconnor ai-2013-08-20-Some analysis of tweet shares and “predicting” election outcomes</a></p>
<p>19 0.045619406 <a title="164-tfidf-19" href="../brendan_oconnor_ai-2010/brendan_oconnor_ai-2010-04-14-quick_note%3A_cer_et_al_2010.html">159 brendan oconnor ai-2010-04-14-quick note: cer et al 2010</a></p>
<p>20 0.044918202 <a title="164-tfidf-20" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-17-p-values%2C_CDF%E2%80%99s%2C_NLP_etc..html">185 brendan oconnor ai-2012-07-17-p-values, CDF’s, NLP etc.</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/brendan_oconnor_ai_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, -0.167), (1, -0.122), (2, 0.091), (3, -0.097), (4, 0.022), (5, 0.019), (6, -0.047), (7, -0.066), (8, 0.032), (9, -0.063), (10, 0.09), (11, -0.065), (12, 0.025), (13, -0.063), (14, -0.099), (15, 0.016), (16, -0.05), (17, 0.1), (18, 0.094), (19, -0.136), (20, 0.017), (21, 0.173), (22, -0.009), (23, 0.15), (24, 0.036), (25, 0.105), (26, -0.097), (27, -0.098), (28, -0.019), (29, 0.072), (30, 0.033), (31, -0.02), (32, 0.15), (33, -0.114), (34, 0.095), (35, -0.023), (36, 0.031), (37, 0.011), (38, -0.122), (39, 0.013), (40, 0.072), (41, 0.072), (42, 0.047), (43, -0.069), (44, 0.028), (45, 0.036), (46, 0.035), (47, 0.137), (48, -0.015), (49, 0.03)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98469883 <a title="164-lsi-1" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-01-11-Please_report_your_SVM%E2%80%99s_kernel%21.html">164 brendan oconnor ai-2011-01-11-Please report your SVM’s kernel!</a></p>
<p>Introduction: I’m tired of reading papers that use an  SVM  but don’t say which kernel they used.  (There’s tons of such papers in NLP and, I think, other areas that do applied machine learning.)  I suspect a lot of these papers are actually using a linear kernel.
 
An un-kernelized, linear SVM is nearly the same as  logistic regression  — every feature independently increases or decreases the classifier’s output prediction.  But a quadratic kernelized SVM is much more like boosted depth-2 decision trees.  It can do automatic combinations of pairs of features — a potentially very different thing, since you can start throwing in features that don’t do anything on their own but might have useful interactions with others.  (And of course, more complicated kernels do progressively more complicated and non-linear things.)
 
I have heard people say they download an SVM package, try a bunch of different kernels, and find the linear kernel is the best. In such cases they could have just used a logistic regr</p><p>2 0.709764 <a title="164-lsi-2" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-10-31-tanh_is_a_rescaled_logistic_sigmoid_function.html">201 brendan oconnor ai-2013-10-31-tanh is a rescaled logistic sigmoid function</a></p>
<p>Introduction: This confused me for a while when I first learned it, so in case it helps anyone else:
 
The   logistic sigmoid   function, a.k.a. the  inverse logit  function, is
 
\[ g(x) = \frac{ e^x }{1 + e^x} \]
 
Its outputs range from 0 to 1, and are often interpreted as probabilities (in, say, logistic regression).
 
The   tanh   function, a.k.a. hyperbolic tangent function, is a rescaling of the logistic sigmoid, such that its outputs range from -1 to 1.  (There’s horizontal stretching as well.)
 
\[ tanh(x) = 2 g(2x) - 1 \]
 
It’s easy to show the above leads to the standard definition \( tanh(x) = \frac{e^x – e^{-x}}{e^x + e^{-x}} \).  The (-1,+1) output range tends to be more convenient for neural networks, so tanh functions show up there a lot.
 
The two functions are plotted below.  Blue is the logistic function, and red is tanh.</p><p>3 0.68774486 <a title="164-lsi-3" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-04-06-a_regression_slope_is_a_weighted_average_of_pairs%E2%80%99_slopes%21.html">100 brendan oconnor ai-2008-04-06-a regression slope is a weighted average of pairs’ slopes!</a></p>
<p>Introduction: Wow, this is pretty cool:
 
 
 
From an  Andrew Gelman article  on summaring a linear regression as a simple difference between upper and lower categories.  I get the impression there are lots of weird misunderstood corners of linear models… (e.g. that “least squares regression” is a maximum likelihood estimator for a linear model with normal noise… I know so many people who didn’t learn that from their stats whatever course, and therefore find it mystifying why squared error should be used…  see this  other post from Gelman .)</p><p>4 0.6110782 <a title="164-lsi-4" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-11-11-Memorizing_small_tables.html">177 brendan oconnor ai-2011-11-11-Memorizing small tables</a></p>
<p>Introduction: Lately, I’ve been trying  to memorize very small tables, especially for better intuitions and rule-of-thumb calculations.  At the moment I have these above my desk:
 
   
 
The first one is a few entries in a natural logarithm table.  There are all these stories about how in the slide rule era, people would develop better intuitions about the scale of logarithms because they physically engaged with them all the time.  I spend lots of time looking at log-likelihoods, log-odds-ratios, and logistic regression coefficients, so I think it would be nice to have quick intuitions about what they are.  (Though the  Gelman and Hill  textbook has an interesting argument against odds scale interpretations of logistic regression coefficients.)
 
The second one are some zsh filename manipulation  shortcuts .  OK, this is more narrow than the others, but pretty useful for me at least.
 
The third one are rough unit equivalencies for data rates over time.  I find this very important for quickly determ</p><p>5 0.60263622 <a title="164-lsi-5" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-03-02-Poor_man%E2%80%99s_linear_algebra_textbook.html">166 brendan oconnor ai-2011-03-02-Poor man’s linear algebra textbook</a></p>
<p>Introduction: I keep learning new bits of linear algebra all the time, but I’m always hurting for a useful reference.  I probably should get a good book (which?), but in the meantime I’m collecting several nice online sources that ML researchers seem to often recommend: The Matrix Cookbook, plus a few more tutorial/introductory pieces, aimed at an intermediate-ish level.
 
Main reference:
  
  The Matrix Cookbook  – 71 pages of identities and such.  This seems to be really popular. 
  
Tutorials/introductions:
  
  CS229 linear algebra review  – from Stanford’s ML course.  It seems to introduce all the essentials, and it’s vaguely familiar for me.  (26 pages) 
 Minka’s  Old and New Matrix Algebra Useful for Statistics  – has a great part on how to do derivatives.  (19 pages) 
 MacKay’s  The Humble Gaussian  – OK, not really pure linear algebra anymore, but quite enlightening.  (12 pages) 
  
After studying for this last stats/ML midterm, I’ve now printed them out and stuck them in a binder.  A poor</p><p>6 0.49135983 <a title="164-lsi-6" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-05-20-Log-normal_and_logistic-normal_terminology.html">169 brendan oconnor ai-2011-05-20-Log-normal and logistic-normal terminology</a></p>
<p>7 0.39818898 <a title="164-lsi-7" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-11-24-Graphs_for_SANCL-2012_web_parsing_results.html">189 brendan oconnor ai-2012-11-24-Graphs for SANCL-2012 web parsing results</a></p>
<p>8 0.3944715 <a title="164-lsi-8" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-08-Patches_to_Rainbow%2C_the_old_text_classifier_that_won%E2%80%99t_go_away.html">153 brendan oconnor ai-2009-09-08-Patches to Rainbow, the old text classifier that won’t go away</a></p>
<p>9 0.35596982 <a title="164-lsi-9" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-17-p-values%2C_CDF%E2%80%99s%2C_NLP_etc..html">185 brendan oconnor ai-2012-07-17-p-values, CDF’s, NLP etc.</a></p>
<p>10 0.31830567 <a title="164-lsi-10" href="../brendan_oconnor_ai-2014/brendan_oconnor_ai-2014-04-26-Replot%3A_departure_delays_vs_flight_time_speed-up.html">204 brendan oconnor ai-2014-04-26-Replot: departure delays vs flight time speed-up</a></p>
<p>11 0.30767533 <a title="164-lsi-11" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-09-19-End-to-end_NLP_packages.html">174 brendan oconnor ai-2011-09-19-End-to-end NLP packages</a></p>
<p>12 0.29454949 <a title="164-lsi-12" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-08-08-Haghighi_and_Klein_%282009%29%3A_Simple_Coreference_Resolution_with_Rich_Syntactic_and_Semantic_Features.html">150 brendan oconnor ai-2009-08-08-Haghighi and Klein (2009): Simple Coreference Resolution with Rich Syntactic and Semantic Features</a></p>
<p>13 0.29304126 <a title="164-lsi-13" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-02-02-Histograms_%E2%80%94_matplotlib_vs._R.html">179 brendan oconnor ai-2012-02-02-Histograms — matplotlib vs. R</a></p>
<p>14 0.27492315 <a title="164-lsi-14" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-07-01-Bias_correction_sneak_peek%21.html">108 brendan oconnor ai-2008-07-01-Bias correction sneak peek!</a></p>
<p>15 0.27063379 <a title="164-lsi-15" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-04-08-More_fun_with_Gapminder_-_Trendalyzer.html">58 brendan oconnor ai-2007-04-08-More fun with Gapminder - Trendalyzer</a></p>
<p>16 0.26496893 <a title="164-lsi-16" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-12-20-Data-driven_charity.html">86 brendan oconnor ai-2007-12-20-Data-driven charity</a></p>
<p>17 0.25680038 <a title="164-lsi-17" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-07-04-City_crisis_simulation_%28e.g._terrorist_attack%29.html">14 brendan oconnor ai-2005-07-04-City crisis simulation (e.g. terrorist attack)</a></p>
<p>18 0.25484866 <a title="164-lsi-18" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-03-Statistics_vs._Machine_Learning%2C_fight%21.html">129 brendan oconnor ai-2008-12-03-Statistics vs. Machine Learning, fight!</a></p>
<p>19 0.24756803 <a title="164-lsi-19" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-01-Binary_classification_evaluation_in_R_via_ROCR.html">136 brendan oconnor ai-2009-04-01-Binary classification evaluation in R via ROCR</a></p>
<p>20 0.24109301 <a title="164-lsi-20" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-02-15-Pascal%E2%80%99s_Wager.html">50 brendan oconnor ai-2007-02-15-Pascal’s Wager</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/brendan_oconnor_ai_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(24, 0.052), (44, 0.091), (57, 0.033), (59, 0.586), (74, 0.11), (80, 0.01)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96047455 <a title="164-lda-1" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-01-11-Please_report_your_SVM%E2%80%99s_kernel%21.html">164 brendan oconnor ai-2011-01-11-Please report your SVM’s kernel!</a></p>
<p>Introduction: I’m tired of reading papers that use an  SVM  but don’t say which kernel they used.  (There’s tons of such papers in NLP and, I think, other areas that do applied machine learning.)  I suspect a lot of these papers are actually using a linear kernel.
 
An un-kernelized, linear SVM is nearly the same as  logistic regression  — every feature independently increases or decreases the classifier’s output prediction.  But a quadratic kernelized SVM is much more like boosted depth-2 decision trees.  It can do automatic combinations of pairs of features — a potentially very different thing, since you can start throwing in features that don’t do anything on their own but might have useful interactions with others.  (And of course, more complicated kernels do progressively more complicated and non-linear things.)
 
I have heard people say they download an SVM package, try a bunch of different kernels, and find the linear kernel is the best. In such cases they could have just used a logistic regr</p><p>2 0.95619899 <a title="164-lda-2" href="../brendan_oconnor_ai-2010/brendan_oconnor_ai-2010-04-14-quick_note%3A_cer_et_al_2010.html">159 brendan oconnor ai-2010-04-14-quick note: cer et al 2010</a></p>
<p>Introduction: Quick note, reading this  paper  from  their tweet .
 
 update  this reaction might be totally wrong; in particular, the conll dependencies for at least some languages were done completely by hand.
   
Malt and MSTParser were designed for the Yamada and Matsumodo dependencies formalism (the one used for the CoNLL dependency parsing shared task, from the   penn2malt   tool).  Their feature sets and probably many other design decisions were created to support that.  If you compare their outputs side-by-side, you will see that the Stanford Dependencies are a substantially different formalism; for example, compound verbs are handled very differently (the paper talks about copula example). 
I think the following conclusion is premature:
  
Notwithstanding the very large amount of research that has gone into dependency 
parsing algorithms in the last ďŹ ve years, our central conclusion is that the quality of the Charniak, Charniak-Johnson reranking, and Berkeley parsers is so high that in th</p><p>3 0.92779297 <a title="164-lda-3" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-11-20-science_writing_bad%21.html">28 brendan oconnor ai-2005-11-20-science writing bad!</a></p>
<p>Introduction: An two-step explanation for distrust of science: (1) journalists write up poor science or take out the evidence and information from a scientific study, then (2) people read that and criticize science for being unfounded, arbitrary, etc.   Link .  Some fun quotes:   
 Statistics are what causes the most fear for reporters, and so they are usually just edited out, with interesting consequences. Because science isn’t about something being true or not true: that’s a humanities graduate parody. It’s about the error bar, statistical significance, it’s about how reliable and valid the experiment was, it’s about coming to a verdict, about a hypothesis, on the back of lots of bits of evidence.  
 
 and   
 
  So how do the media work around their inability to deliver scientific evidence? They use authority figures, the very antithesis of what science is about, as if they were priests, or politicians, or parent figures. “Scientists today said … scientists revealed … scientists warned.” And if t</p><p>4 0.35348046 <a title="164-lda-4" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-08-08-Haghighi_and_Klein_%282009%29%3A_Simple_Coreference_Resolution_with_Rich_Syntactic_and_Semantic_Features.html">150 brendan oconnor ai-2009-08-08-Haghighi and Klein (2009): Simple Coreference Resolution with Rich Syntactic and Semantic Features</a></p>
<p>Introduction: I haven’t done a paper review on this blog for a while, so here we go.
 
 Coreference  resolution is an interesting NLP problem.  ( Examples. )  It involves honest-to-goodness syntactic, semantic, and discourse phenomena, but still seems like a real cognitive task that humans have to solve when reading text [1].  I haven’t read the whole literature, but I’ve always been puzzled by the crop of papers on it I’ve seen in the last year or two.  There’s a big focus on fancy graph/probabilistic/constrained optimization algorithms, but often these papers gloss over the linguistic features — the core information they actually make their decisions with [2].  I never understood why the latter isn’t the most important issue.  Therefore, it was a joy to read
  
 Aria Haghighi and Dan Klein, EMNLP-2009.   “Simple Coreference Resolution with Rich Syntactic and Semantic Features.”  
  
They describe a simple, essentially non-statistical system that outperforms previous unsupervised systems, and compa</p><p>5 0.28912815 <a title="164-lda-5" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-02-23-Comparison_of_data_analysis_packages%3A_R%2C_Matlab%2C_SciPy%2C_Excel%2C_SAS%2C_SPSS%2C_Stata.html">135 brendan oconnor ai-2009-02-23-Comparison of data analysis packages: R, Matlab, SciPy, Excel, SAS, SPSS, Stata</a></p>
<p>Introduction: Lukas  and I were trying to write a succinct comparison of the most popular packages that are typically used for data analysis.  I think most people choose one based on what people around them use or what they learn in school, so I’ve found it hard to find comparative information.  I’m posting the table here in hopes of useful comments.
  
 
 
  Name  
  Advantages  
  Disadvantages  
  Open source?  
  Typical   users  
 
 
 R 
 Library support; visualization 
 Steep learning curve 
 Yes 
 Finance; Statistics 
 
 
 Matlab 
 Elegant matrix support; visualization 
 Expensive; incomplete statistics support 
 No 
 Engineering 
 
 
 SciPy/NumPy/Matplotlib 
 Python (general-purpose programming language) 
 Immature 
 Yes 
 Engineering 
 
 
 Excel 
 Easy; visual; flexible 
 Large datasets 
 No 
 Business 
 
 
 SAS 
 Large datasets 
 Expensive; outdated programming language 
 No 
 Business; Government 
 
 
 Stata 
 Easy statistical analysis 
  
 No 
 Science 
 
 
 SPSS 
 Like Stata but more ex</p><p>6 0.28687638 <a title="164-lda-6" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-10-Don%E2%80%99t_MAWK_AWK_%E2%80%93_the_fastest_and_most_elegant_big_data_munging_language%21.html">154 brendan oconnor ai-2009-09-10-Don’t MAWK AWK – the fastest and most elegant big data munging language!</a></p>
<p>7 0.25085339 <a title="164-lda-7" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-17-p-values%2C_CDF%E2%80%99s%2C_NLP_etc..html">185 brendan oconnor ai-2012-07-17-p-values, CDF’s, NLP etc.</a></p>
<p>8 0.24811426 <a title="164-lda-8" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-11-21-Netflix_Prize.html">125 brendan oconnor ai-2008-11-21-Netflix Prize</a></p>
<p>9 0.24785016 <a title="164-lda-9" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-08-Patches_to_Rainbow%2C_the_old_text_classifier_that_won%E2%80%99t_go_away.html">153 brendan oconnor ai-2009-09-08-Patches to Rainbow, the old text classifier that won’t go away</a></p>
<p>10 0.24528733 <a title="164-lda-10" href="../brendan_oconnor_ai-2006/brendan_oconnor_ai-2006-03-26-new_kind_of_science%2C_for_real.html">32 brendan oconnor ai-2006-03-26-new kind of science, for real</a></p>
<p>11 0.24319555 <a title="164-lda-11" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-18-Information_cost_and_genocide.html">130 brendan oconnor ai-2008-12-18-Information cost and genocide</a></p>
<p>12 0.24195513 <a title="164-lda-12" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-04-The_%2460%2C000_cat%3A_deep_belief_networks_make_less_sense_for_language_than_vision.html">184 brendan oconnor ai-2012-07-04-The $60,000 cat: deep belief networks make less sense for language than vision</a></p>
<p>13 0.24113183 <a title="164-lda-13" href="../brendan_oconnor_ai-2004/brendan_oconnor_ai-2004-11-24-addiction_%26_2_problems_of_economics.html">2 brendan oconnor ai-2004-11-24-addiction & 2 problems of economics</a></p>
<p>14 0.24020778 <a title="164-lda-14" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-12-31-List_of_probabilistic_model_mini-language_toolkits.html">157 brendan oconnor ai-2009-12-31-List of probabilistic model mini-language toolkits</a></p>
<p>15 0.23740965 <a title="164-lda-15" href="../brendan_oconnor_ai-2014/brendan_oconnor_ai-2014-02-19-What_the_ACL-2014_review_scores_mean.html">203 brendan oconnor ai-2014-02-19-What the ACL-2014 review scores mean</a></p>
<p>16 0.233358 <a title="164-lda-16" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-01-20-Moral_psychology_on_Amazon_Mechanical_Turk.html">90 brendan oconnor ai-2008-01-20-Moral psychology on Amazon Mechanical Turk</a></p>
<p>17 0.23153073 <a title="164-lda-17" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-09-19-End-to-end_NLP_packages.html">174 brendan oconnor ai-2011-09-19-End-to-end NLP packages</a></p>
<p>18 0.2298277 <a title="164-lda-18" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-03-27-Seth_Roberts_and_academic_blogging.html">55 brendan oconnor ai-2007-03-27-Seth Roberts and academic blogging</a></p>
<p>19 0.21861485 <a title="164-lda-19" href="../brendan_oconnor_ai-2010/brendan_oconnor_ai-2010-03-31-How_Facebook_privacy_failed_me.html">158 brendan oconnor ai-2010-03-31-How Facebook privacy failed me</a></p>
<p>20 0.2177988 <a title="164-lda-20" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-03-Statistics_vs._Machine_Learning%2C_fight%21.html">129 brendan oconnor ai-2008-12-03-Statistics vs. Machine Learning, fight!</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
