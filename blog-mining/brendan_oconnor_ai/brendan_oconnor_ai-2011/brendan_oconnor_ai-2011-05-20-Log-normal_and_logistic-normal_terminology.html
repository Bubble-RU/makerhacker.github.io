<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>169 brendan oconnor ai-2011-05-20-Log-normal and logistic-normal terminology</title>
</head>

<body>
<p><a title="brendan_oconnor_ai" href="../brendan_oconnor_ai_home.html">brendan_oconnor_ai</a> <a title="brendan_oconnor_ai-2011" href="../home/brendan_oconnor_ai-2011_home.html">brendan_oconnor_ai-2011</a> <a title="brendan_oconnor_ai-2011-169" href="#">brendan_oconnor_ai-2011-169</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>169 brendan oconnor ai-2011-05-20-Log-normal and logistic-normal terminology</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="brendan_oconnor_ai-2011-169-html" href="http://brenocon.com/blog/2011/05/log-normal-and-logistic-normal-terminology/">html</a></p><p>Introduction: I was cleaning my office and found a back-of-envelope diagram  Shay  drew me once, so I’m writing it up to not forget.  The definitions of the  logistic-normal  and  log-normal  distributions are a little confusing with regard to their relationship to the normal distribution.  If you draw samples from one, the arrows below show the transformation to make it such you have samples from another.
 
   
 
For example, if  x ~ Normal , then transforming as  y=exp(x)  implies  y ~ LogNormal .  The adjective terminology is inverted: the logistic function goes from normal to logistic-normal, but the log function goes from log-normal to normal (other way!).  The log of the log-normal is normal, but it’s the   logit   of the logistic normal that’s normal.
 
Here are densities of these different distributions via transformations from a standard normal.
 
   In R:   x=rnorm(1e6); hist(x); hist(exp(x)/(1+exp(x)); hist(exp(x)) 
 
Just to make things more confusing, note the logistic-normal distributi</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 I was cleaning my office and found a back-of-envelope diagram  Shay  drew me once, so I’m writing it up to not forget. [sent-1, score-0.189]
</p><p>2 The definitions of the  logistic-normal  and  log-normal  distributions are a little confusing with regard to their relationship to the normal distribution. [sent-2, score-0.886]
</p><p>3 If you draw samples from one, the arrows below show the transformation to make it such you have samples from another. [sent-3, score-0.401]
</p><p>4 For example, if  x ~ Normal , then transforming as  y=exp(x)  implies  y ~ LogNormal . [sent-4, score-0.059]
</p><p>5 The adjective terminology is inverted: the logistic function goes from normal to logistic-normal, but the log function goes from log-normal to normal (other way! [sent-5, score-1.625]
</p><p>6 The log of the log-normal is normal, but it’s the   logit   of the logistic normal that’s normal. [sent-7, score-0.824]
</p><p>7 Here are densities of these different distributions via transformations from a standard normal. [sent-8, score-0.519]
</p><p>8 In R:   x=rnorm(1e6); hist(x); hist(exp(x)/(1+exp(x)); hist(exp(x))    Just to make things more confusing, note the logistic-normal distribution is completely different than the  logistic distribution . [sent-9, score-0.765]
</p><p>9 Neat fact: it arises from lots of multiplicative effects (by the  CLT , since additive effects imply the normal). [sent-12, score-0.46]
</p><p>10 ( slides ,  blogpost ) finds log-normals and stretched exponentials fit pretty well to many types of data that are often claimed to be power-law. [sent-14, score-0.205]
</p><p>11 Hm, on page 2 they talk about the log-normal, so they’re responsible for the very slight naming weirdness. [sent-16, score-0.141]
</p><p>12 The logistic-normal is a useful Bayesian prior for multinomial distributions, since in the  d -dimensional multivariate case it defines a probability distribution over the simplex (i. [sent-17, score-0.547]
</p><p>13 multinomials), similar to the  Dirichlet , but you can capture covariance effects and chain them together and other fun things, though inference can be trickier (typically via variational approximations). [sent-20, score-0.462]
</p><p>14 A biased sample of text modeling examples include  Blei and Lafferty , another  B&L; ,  Cohen and Smith ,  Eisenstein et al . [sent-21, score-0.1]
</p><p>15 OK, so maybe these distributions aren’t really related beyond involving transformations of the normal. [sent-22, score-0.5]
</p><p>16 For example, a 3-d normal (distribution over 3-space) corresponds to a logistic-normal distribution over a simplex in 3-space, having only 2 dimensions. [sent-24, score-0.852]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('normal', 0.467), ('exp', 0.297), ('distributions', 0.271), ('hist', 0.222), ('distribution', 0.215), ('logistic', 0.189), ('multivariate', 0.17), ('simplex', 0.17), ('transformations', 0.17), ('effects', 0.162), ('confusing', 0.148), ('samples', 0.135), ('diagram', 0.126), ('dimension', 0.126), ('goes', 0.104), ('et', 0.1), ('function', 0.1), ('case', 0.094), ('log', 0.094), ('via', 0.078), ('page', 0.078), ('cohen', 0.074), ('chain', 0.074), ('capture', 0.074), ('wrinkle', 0.074), ('obscure', 0.074), ('typically', 0.074), ('hm', 0.074), ('approximations', 0.074), ('covariance', 0.074), ('logit', 0.074), ('shay', 0.074), ('stretched', 0.074), ('note', 0.073), ('things', 0.073), ('imply', 0.068), ('writes', 0.068), ('blogpost', 0.068), ('arrows', 0.068), ('dirichlet', 0.068), ('since', 0.068), ('smith', 0.063), ('office', 0.063), ('responsible', 0.063), ('draw', 0.063), ('parameter', 0.063), ('slides', 0.063), ('involving', 0.059), ('implies', 0.059), ('additional', 0.057)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="169-tfidf-1" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-05-20-Log-normal_and_logistic-normal_terminology.html">169 brendan oconnor ai-2011-05-20-Log-normal and logistic-normal terminology</a></p>
<p>Introduction: I was cleaning my office and found a back-of-envelope diagram  Shay  drew me once, so I’m writing it up to not forget.  The definitions of the  logistic-normal  and  log-normal  distributions are a little confusing with regard to their relationship to the normal distribution.  If you draw samples from one, the arrows below show the transformation to make it such you have samples from another.
 
   
 
For example, if  x ~ Normal , then transforming as  y=exp(x)  implies  y ~ LogNormal .  The adjective terminology is inverted: the logistic function goes from normal to logistic-normal, but the log function goes from log-normal to normal (other way!).  The log of the log-normal is normal, but it’s the   logit   of the logistic normal that’s normal.
 
Here are densities of these different distributions via transformations from a standard normal.
 
   In R:   x=rnorm(1e6); hist(x); hist(exp(x)/(1+exp(x)); hist(exp(x)) 
 
Just to make things more confusing, note the logistic-normal distributi</p><p>2 0.1710608 <a title="169-tfidf-2" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-01-07-Perplexity_as_branching_factor%3B_as_Shannon_diversity_index.html">190 brendan oconnor ai-2013-01-07-Perplexity as branching factor; as Shannon diversity index</a></p>
<p>Introduction: A language model’s  perplexity  is exponentiated negative average log-likelihood,
 
$$\exp( -\frac{1}{N} \log(p(x)))$$
 
Where the inner term usually decomposes into a sum over individual items; for example, as \(\sum_i \log p(x_i | x_1..x_{i-1})\) or \(\sum_i \log p(x_i)\) depending on independence assumptions, where for language modeling word tokens are usually taken as the individual units.  (In which case it is the geometric mean of per-token negative log-likelihoods.)   It’s equivalent to exponentiated cross-entropy between the model and the empirical data distribution, since \(-1/N \sum_i^N \log p(x_i) = -\sum_k^K \hat{p}_k \log p_k = H(\hat{p};p)\) where \(N\) is the number of items and \(K\) is the number of discrete classes (e.g. word types for language modeling) and \(\hat{p}_k\) is the proportion of data having class \(k\).
 
A nice interpretation of any exponentiated entropy measure is as branching factor: entropy measures uncertainty in bits or nats, but in exponentiated f</p><p>3 0.12303387 <a title="169-tfidf-3" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-10-31-tanh_is_a_rescaled_logistic_sigmoid_function.html">201 brendan oconnor ai-2013-10-31-tanh is a rescaled logistic sigmoid function</a></p>
<p>Introduction: This confused me for a while when I first learned it, so in case it helps anyone else:
 
The   logistic sigmoid   function, a.k.a. the  inverse logit  function, is
 
\[ g(x) = \frac{ e^x }{1 + e^x} \]
 
Its outputs range from 0 to 1, and are often interpreted as probabilities (in, say, logistic regression).
 
The   tanh   function, a.k.a. hyperbolic tangent function, is a rescaling of the logistic sigmoid, such that its outputs range from -1 to 1.  (There’s horizontal stretching as well.)
 
\[ tanh(x) = 2 g(2x) - 1 \]
 
It’s easy to show the above leads to the standard definition \( tanh(x) = \frac{e^x – e^{-x}}{e^x + e^{-x}} \).  The (-1,+1) output range tends to be more convenient for neural networks, so tanh functions show up there a lot.
 
The two functions are plotted below.  Blue is the logistic function, and red is tanh.</p><p>4 0.10494035 <a title="169-tfidf-4" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-09-25-Information_theory_stuff.html">175 brendan oconnor ai-2011-09-25-Information theory stuff</a></p>
<p>Introduction: Actually this post is mainly to test the  MathJax  installation I put into WordPress via  this plugin .  But  information theory  is great, why not?
 
The probability of a symbol is \(p\).
 
It takes \(\log \frac{1}{p} = -\log p\) bits to encode one symbol — sometimes called its “surprisal”.  Surprisal is 0 for a 100% probable symbol, and ranges up to \(\infty\) for extremely low probability symbols.  This is because you use a coding scheme that encodes common symbols as very short strings, and less common symbols as longer ones.  (e.g.  Huffman  or  arithmetic  coding.)  We should say logarithms are base-2 so information is measured in bits.\(^*\)
 
If you have a stream of such symbols and a probability distribution \(\vec{p}\) for them, where a symbol \(i\) comes at probability \(p_i\), then the average message size is the expected surprisal:
 
\[ H(\vec{p}) = \sum_i p_i \log \frac{1}{p_i} \]
 
this is the Shannon  entropy  of the probability distribution \( \vec{p} \), which is a me</p><p>5 0.09232752 <a title="169-tfidf-5" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-17-p-values%2C_CDF%E2%80%99s%2C_NLP_etc..html">185 brendan oconnor ai-2012-07-17-p-values, CDF’s, NLP etc.</a></p>
<p>Introduction: Update Aug 10: THIS IS NOT A SUMMARY OF THE WHOLE PAPER!  it’s whining about one particular method of analysis before talking about other things further down
   
A quick note on  Berg-Kirkpatrick et al EMNLP-2012, “An Empirical Investigation of Statistical Signiﬁcance in NLP” .  They make lots of graphs of p-values against observed magnitudes and talk about “curves”, e.g.
 
 We see the same curve-shaped trend we saw for summarization and dependency parsing. Different group comparisons, same group comparisons, and system combination comparisons form distinct curves. 
 
For example, Figure 2.
 
   
 
I fear they made 10 graphs to rediscover a basic statistical fact: a p-value comes from a null hypothesis CDF.  That’s what these “curve-shaped trends” are in all their graphs.  They are CDFs.
 
To back up, the statistical significance testing question is whether, in their notation, the observed dataset performance difference \(\delta(x)\) is “real” or not: if you were to resample the data,</p><p>6 0.082715139 <a title="169-tfidf-6" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-12-31-List_of_probabilistic_model_mini-language_toolkits.html">157 brendan oconnor ai-2009-12-31-List of probabilistic model mini-language toolkits</a></p>
<p>7 0.082437649 <a title="169-tfidf-7" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-02-02-Histograms_%E2%80%94_matplotlib_vs._R.html">179 brendan oconnor ai-2012-02-02-Histograms — matplotlib vs. R</a></p>
<p>8 0.079729378 <a title="169-tfidf-8" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-04-06-a_regression_slope_is_a_weighted_average_of_pairs%E2%80%99_slopes%21.html">100 brendan oconnor ai-2008-04-06-a regression slope is a weighted average of pairs’ slopes!</a></p>
<p>9 0.07067056 <a title="169-tfidf-9" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-09-07-Kurzweil_interview.html">27 brendan oconnor ai-2005-09-07-Kurzweil interview</a></p>
<p>10 0.064842209 <a title="169-tfidf-10" href="../brendan_oconnor_ai-2010/brendan_oconnor_ai-2010-08-09-An_ML-AI_approach_to_P_%21%3D_NP.html">161 brendan oconnor ai-2010-08-09-An ML-AI approach to P != NP</a></p>
<p>11 0.063994318 <a title="169-tfidf-11" href="../brendan_oconnor_ai-2006/brendan_oconnor_ai-2006-06-28-Social_network-ized_economic_markets.html">40 brendan oconnor ai-2006-06-28-Social network-ized economic markets</a></p>
<p>12 0.062332805 <a title="169-tfidf-12" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-01-11-Please_report_your_SVM%E2%80%99s_kernel%21.html">164 brendan oconnor ai-2011-01-11-Please report your SVM’s kernel!</a></p>
<p>13 0.062076189 <a title="169-tfidf-13" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-07-15-Beta_conjugate_explorer.html">146 brendan oconnor ai-2009-07-15-Beta conjugate explorer</a></p>
<p>14 0.062072888 <a title="169-tfidf-14" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-05-18-Announcing_TweetMotif_for_summarizing_twitter_topics.html">140 brendan oconnor ai-2009-05-18-Announcing TweetMotif for summarizing twitter topics</a></p>
<p>15 0.057602428 <a title="169-tfidf-15" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-07-25-Cerealitivity.html">70 brendan oconnor ai-2007-07-25-Cerealitivity</a></p>
<p>16 0.057243731 <a title="169-tfidf-16" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-07-08-Game_outcome_graphs_%E2%80%94_prisoner%E2%80%99s_dilemma_with_FUN_ARROWS%21%21%21.html">68 brendan oconnor ai-2007-07-08-Game outcome graphs — prisoner’s dilemma with FUN ARROWS!!!</a></p>
<p>17 0.055749539 <a title="169-tfidf-17" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-11-11-Memorizing_small_tables.html">177 brendan oconnor ai-2011-11-11-Memorizing small tables</a></p>
<p>18 0.053371586 <a title="169-tfidf-18" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-04-21-What_inputs_do_Monte_Carlo_algorithms_need%3F.html">195 brendan oconnor ai-2013-04-21-What inputs do Monte Carlo algorithms need?</a></p>
<p>19 0.052205432 <a title="169-tfidf-19" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-11-24-Graphs_for_SANCL-2012_web_parsing_results.html">189 brendan oconnor ai-2012-11-24-Graphs for SANCL-2012 web parsing results</a></p>
<p>20 0.048946466 <a title="169-tfidf-20" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-10-11-It_is_accurate_to_determine_a_blog%E2%80%99s_bias_by_what_it_links_to.html">117 brendan oconnor ai-2008-10-11-It is accurate to determine a blog’s bias by what it links to</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/brendan_oconnor_ai_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, -0.17), (1, -0.064), (2, 0.095), (3, -0.107), (4, -0.028), (5, 0.201), (6, 0.076), (7, -0.168), (8, -0.173), (9, 0.054), (10, 0.038), (11, 0.013), (12, 0.012), (13, -0.013), (14, -0.023), (15, 0.064), (16, -0.02), (17, 0.015), (18, 0.055), (19, -0.064), (20, 0.023), (21, -0.012), (22, -0.042), (23, 0.066), (24, -0.079), (25, 0.089), (26, -0.095), (27, -0.044), (28, 0.029), (29, 0.061), (30, 0.029), (31, 0.115), (32, 0.095), (33, 0.069), (34, 0.164), (35, -0.056), (36, 0.022), (37, -0.009), (38, -0.078), (39, 0.034), (40, -0.033), (41, 0.062), (42, 0.065), (43, -0.078), (44, -0.026), (45, 0.068), (46, 0.03), (47, 0.069), (48, 0.179), (49, -0.041)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99009258 <a title="169-lsi-1" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-05-20-Log-normal_and_logistic-normal_terminology.html">169 brendan oconnor ai-2011-05-20-Log-normal and logistic-normal terminology</a></p>
<p>Introduction: I was cleaning my office and found a back-of-envelope diagram  Shay  drew me once, so I’m writing it up to not forget.  The definitions of the  logistic-normal  and  log-normal  distributions are a little confusing with regard to their relationship to the normal distribution.  If you draw samples from one, the arrows below show the transformation to make it such you have samples from another.
 
   
 
For example, if  x ~ Normal , then transforming as  y=exp(x)  implies  y ~ LogNormal .  The adjective terminology is inverted: the logistic function goes from normal to logistic-normal, but the log function goes from log-normal to normal (other way!).  The log of the log-normal is normal, but it’s the   logit   of the logistic normal that’s normal.
 
Here are densities of these different distributions via transformations from a standard normal.
 
   In R:   x=rnorm(1e6); hist(x); hist(exp(x)/(1+exp(x)); hist(exp(x)) 
 
Just to make things more confusing, note the logistic-normal distributi</p><p>2 0.65327203 <a title="169-lsi-2" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-10-31-tanh_is_a_rescaled_logistic_sigmoid_function.html">201 brendan oconnor ai-2013-10-31-tanh is a rescaled logistic sigmoid function</a></p>
<p>Introduction: This confused me for a while when I first learned it, so in case it helps anyone else:
 
The   logistic sigmoid   function, a.k.a. the  inverse logit  function, is
 
\[ g(x) = \frac{ e^x }{1 + e^x} \]
 
Its outputs range from 0 to 1, and are often interpreted as probabilities (in, say, logistic regression).
 
The   tanh   function, a.k.a. hyperbolic tangent function, is a rescaling of the logistic sigmoid, such that its outputs range from -1 to 1.  (There’s horizontal stretching as well.)
 
\[ tanh(x) = 2 g(2x) - 1 \]
 
It’s easy to show the above leads to the standard definition \( tanh(x) = \frac{e^x – e^{-x}}{e^x + e^{-x}} \).  The (-1,+1) output range tends to be more convenient for neural networks, so tanh functions show up there a lot.
 
The two functions are plotted below.  Blue is the logistic function, and red is tanh.</p><p>3 0.60326254 <a title="169-lsi-3" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-01-07-Perplexity_as_branching_factor%3B_as_Shannon_diversity_index.html">190 brendan oconnor ai-2013-01-07-Perplexity as branching factor; as Shannon diversity index</a></p>
<p>Introduction: A language model’s  perplexity  is exponentiated negative average log-likelihood,
 
$$\exp( -\frac{1}{N} \log(p(x)))$$
 
Where the inner term usually decomposes into a sum over individual items; for example, as \(\sum_i \log p(x_i | x_1..x_{i-1})\) or \(\sum_i \log p(x_i)\) depending on independence assumptions, where for language modeling word tokens are usually taken as the individual units.  (In which case it is the geometric mean of per-token negative log-likelihoods.)   It’s equivalent to exponentiated cross-entropy between the model and the empirical data distribution, since \(-1/N \sum_i^N \log p(x_i) = -\sum_k^K \hat{p}_k \log p_k = H(\hat{p};p)\) where \(N\) is the number of items and \(K\) is the number of discrete classes (e.g. word types for language modeling) and \(\hat{p}_k\) is the proportion of data having class \(k\).
 
A nice interpretation of any exponentiated entropy measure is as branching factor: entropy measures uncertainty in bits or nats, but in exponentiated f</p><p>4 0.59747791 <a title="169-lsi-4" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-09-25-Information_theory_stuff.html">175 brendan oconnor ai-2011-09-25-Information theory stuff</a></p>
<p>Introduction: Actually this post is mainly to test the  MathJax  installation I put into WordPress via  this plugin .  But  information theory  is great, why not?
 
The probability of a symbol is \(p\).
 
It takes \(\log \frac{1}{p} = -\log p\) bits to encode one symbol — sometimes called its “surprisal”.  Surprisal is 0 for a 100% probable symbol, and ranges up to \(\infty\) for extremely low probability symbols.  This is because you use a coding scheme that encodes common symbols as very short strings, and less common symbols as longer ones.  (e.g.  Huffman  or  arithmetic  coding.)  We should say logarithms are base-2 so information is measured in bits.\(^*\)
 
If you have a stream of such symbols and a probability distribution \(\vec{p}\) for them, where a symbol \(i\) comes at probability \(p_i\), then the average message size is the expected surprisal:
 
\[ H(\vec{p}) = \sum_i p_i \log \frac{1}{p_i} \]
 
this is the Shannon  entropy  of the probability distribution \( \vec{p} \), which is a me</p><p>5 0.48735929 <a title="169-lsi-5" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-01-11-Please_report_your_SVM%E2%80%99s_kernel%21.html">164 brendan oconnor ai-2011-01-11-Please report your SVM’s kernel!</a></p>
<p>Introduction: I’m tired of reading papers that use an  SVM  but don’t say which kernel they used.  (There’s tons of such papers in NLP and, I think, other areas that do applied machine learning.)  I suspect a lot of these papers are actually using a linear kernel.
 
An un-kernelized, linear SVM is nearly the same as  logistic regression  — every feature independently increases or decreases the classifier’s output prediction.  But a quadratic kernelized SVM is much more like boosted depth-2 decision trees.  It can do automatic combinations of pairs of features — a potentially very different thing, since you can start throwing in features that don’t do anything on their own but might have useful interactions with others.  (And of course, more complicated kernels do progressively more complicated and non-linear things.)
 
I have heard people say they download an SVM package, try a bunch of different kernels, and find the linear kernel is the best. In such cases they could have just used a logistic regr</p><p>6 0.46512106 <a title="169-lsi-6" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-04-21-What_inputs_do_Monte_Carlo_algorithms_need%3F.html">195 brendan oconnor ai-2013-04-21-What inputs do Monte Carlo algorithms need?</a></p>
<p>7 0.3954466 <a title="169-lsi-7" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-11-11-Memorizing_small_tables.html">177 brendan oconnor ai-2011-11-11-Memorizing small tables</a></p>
<p>8 0.36700824 <a title="169-lsi-8" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-17-p-values%2C_CDF%E2%80%99s%2C_NLP_etc..html">185 brendan oconnor ai-2012-07-17-p-values, CDF’s, NLP etc.</a></p>
<p>9 0.3509759 <a title="169-lsi-9" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-07-22-FFT%3A_Friedman_%2B_Fortran_%2B_Tricks.html">147 brendan oconnor ai-2009-07-22-FFT: Friedman + Fortran + Tricks</a></p>
<p>10 0.34670714 <a title="169-lsi-10" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-11-13-Bayes_update_view_of_pointwise_mutual_information.html">178 brendan oconnor ai-2011-11-13-Bayes update view of pointwise mutual information</a></p>
<p>11 0.32330841 <a title="169-lsi-11" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-07-15-Beta_conjugate_explorer.html">146 brendan oconnor ai-2009-07-15-Beta conjugate explorer</a></p>
<p>12 0.31190944 <a title="169-lsi-12" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-04-06-a_regression_slope_is_a_weighted_average_of_pairs%E2%80%99_slopes%21.html">100 brendan oconnor ai-2008-04-06-a regression slope is a weighted average of pairs’ slopes!</a></p>
<p>13 0.31185031 <a title="169-lsi-13" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-12-31-List_of_probabilistic_model_mini-language_toolkits.html">157 brendan oconnor ai-2009-12-31-List of probabilistic model mini-language toolkits</a></p>
<p>14 0.3050611 <a title="169-lsi-14" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-01-02-funny_comic.html">48 brendan oconnor ai-2007-01-02-funny comic</a></p>
<p>15 0.29837826 <a title="169-lsi-15" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-07-09-the_psychology_of_design_as_explanation.html">19 brendan oconnor ai-2005-07-09-the psychology of design as explanation</a></p>
<p>16 0.29671785 <a title="169-lsi-16" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-07-25-Cerealitivity.html">70 brendan oconnor ai-2007-07-25-Cerealitivity</a></p>
<p>17 0.29476574 <a title="169-lsi-17" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-03-14-R_scan%28%29_for_quick-and-dirty_checks.html">192 brendan oconnor ai-2013-03-14-R scan() for quick-and-dirty checks</a></p>
<p>18 0.28817675 <a title="169-lsi-18" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-22-Performance_comparison%3A_key-value_stores_for_language_model_counts.html">139 brendan oconnor ai-2009-04-22-Performance comparison: key-value stores for language model counts</a></p>
<p>19 0.28798983 <a title="169-lsi-19" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-11-24-Graphs_for_SANCL-2012_web_parsing_results.html">189 brendan oconnor ai-2012-11-24-Graphs for SANCL-2012 web parsing results</a></p>
<p>20 0.28354305 <a title="169-lsi-20" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-02-02-Histograms_%E2%80%94_matplotlib_vs._R.html">179 brendan oconnor ai-2012-02-02-Histograms — matplotlib vs. R</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/brendan_oconnor_ai_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(24, 0.018), (36, 0.028), (44, 0.061), (70, 0.055), (74, 0.115), (80, 0.618)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98189825 <a title="169-lda-1" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-05-20-Log-normal_and_logistic-normal_terminology.html">169 brendan oconnor ai-2011-05-20-Log-normal and logistic-normal terminology</a></p>
<p>Introduction: I was cleaning my office and found a back-of-envelope diagram  Shay  drew me once, so I’m writing it up to not forget.  The definitions of the  logistic-normal  and  log-normal  distributions are a little confusing with regard to their relationship to the normal distribution.  If you draw samples from one, the arrows below show the transformation to make it such you have samples from another.
 
   
 
For example, if  x ~ Normal , then transforming as  y=exp(x)  implies  y ~ LogNormal .  The adjective terminology is inverted: the logistic function goes from normal to logistic-normal, but the log function goes from log-normal to normal (other way!).  The log of the log-normal is normal, but it’s the   logit   of the logistic normal that’s normal.
 
Here are densities of these different distributions via transformations from a standard normal.
 
   In R:   x=rnorm(1e6); hist(x); hist(exp(x)/(1+exp(x)); hist(exp(x)) 
 
Just to make things more confusing, note the logistic-normal distributi</p><p>2 0.94982874 <a title="169-lda-2" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-06-29-Evangelicals_vs._Aquarians.html">66 brendan oconnor ai-2007-06-29-Evangelicals vs. Aquarians</a></p>
<p>Introduction: Just read an interesting analysis on the the simultaneous rise of the cultural left and right (“hippies and evangelicals”) through the 50′s and 60′s.   Brink Lindsey argues here  that they were both reactions to post-war material prosperity:
  
On the left gathered those who were most alive to the new possibilities created by the unprecedented mass affluence of the postwar years but at the same time were hostile to the social institutions — namely, the market and the middle-class work ethic — that created those possibilities. On the right rallied those who staunchly supported the institutions that created prosperity but who shrank from the social dynamism they were unleashing. One side denounced capitalism but gobbled its fruits; the other cursed the fruits while defending the system that bore them. Both causes were quixotic, and consequently neither fully realized its ambitions.
  
I love  neat sweeping theories of history ; I can’t take it overly seriously but it is so fun.  Lindsey</p><p>3 0.93212146 <a title="169-lda-3" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-04-16-Rise_and_fall_of_Dirichlet_process_clusters.html">194 brendan oconnor ai-2013-04-16-Rise and fall of Dirichlet process clusters</a></p>
<p>Introduction: Here’s Gibbs sampling for a  Dirichlet process 1-d mixture of Gaussians .  On 1000 data points that look like this.
 
   
 
I gave it fixed variance and a concentration and over MCMC iterations, and it looks like this.
 
   
 
The top is the number of points in a cluster.  The bottom are the cluster means.  Every cluster has a unique color.  During MCMC, clusters are created and destroyed.  Every cluster has a unique color; when a cluster dies, its color is never reused.  
 
I’m showing clusters every 100 iterations.  If there is a single point, that cluster was at that iteration but not before or after.  If there is a line, the cluster lived for at least 100 iterations.  Some clusters live long, some live short, but all eventually die.
 
Usually the model likes to think there are about two clusters, occupying positions at the two modes in the data distribution.  It also entertains the existence of several much more minor ones.  Usually these are shortlived clusters that die away.  But</p><p>4 0.90864122 <a title="169-lda-4" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-03-21-Statistics_is_big-N_logic%3F.html">54 brendan oconnor ai-2007-03-21-Statistics is big-N logic?</a></p>
<p>Introduction: I think I believe one of these things, but I’m not quite sure. 
 Statistics is just like logic, except with uncertainty. 
 
This would be true if statistics is Bayesian statistics and you buy the Bayesian inductive logic story — add induction to propositional logic, via a conditional credibility operator, and the Cox axioms imply standard probability theory as a consequence.  (That is, probability theory is logic with uncertainty.  And then a good Bayesian thinks probability theory and statistics are the same.)  Links:  Jaynes’ explanation ;  SEP article ; also  Fitelson’s article .  (Though there are negative results; all I can think of right now is a  Halpern  article on Cox; and also interesting is  Halpern and Koller .)
 
Secondly, here is another statement.
  

Statistics is just like logic, except with a big N.

  
This is a more data-driven view — the world is full of things and they need to be described.  Logical rules can help you describe things, but you also have to deal wit</p><p>5 0.53452599 <a title="169-lda-5" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-01-30-%E2%80%9CLogic_Bomb%E2%80%9D.html">134 brendan oconnor ai-2009-01-30-“Logic Bomb”</a></p>
<p>Introduction: Article:
 
 Fannie Mae Logic Bomb Would Have Caused Weeklong Shutdown | Threat Level from Wired.com .
 
I love the term “logic bomb”.  Can you pair it with a statistics bomb?  Data-driven bomb?  Or maybe the point is a connectionist bomb.</p><p>6 0.4339717 <a title="169-lda-6" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-09-25-Information_theory_stuff.html">175 brendan oconnor ai-2011-09-25-Information theory stuff</a></p>
<p>7 0.38642913 <a title="169-lda-7" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-01-Binary_classification_evaluation_in_R_via_ROCR.html">136 brendan oconnor ai-2009-04-01-Binary classification evaluation in R via ROCR</a></p>
<p>8 0.33936709 <a title="169-lda-8" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-07-15-Beta_conjugate_explorer.html">146 brendan oconnor ai-2009-07-15-Beta conjugate explorer</a></p>
<p>9 0.32973218 <a title="169-lda-9" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-04-21-What_inputs_do_Monte_Carlo_algorithms_need%3F.html">195 brendan oconnor ai-2013-04-21-What inputs do Monte Carlo algorithms need?</a></p>
<p>10 0.3283014 <a title="169-lda-10" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-03-Statistics_vs._Machine_Learning%2C_fight%21.html">129 brendan oconnor ai-2008-12-03-Statistics vs. Machine Learning, fight!</a></p>
<p>11 0.32697007 <a title="169-lda-11" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-17-p-values%2C_CDF%E2%80%99s%2C_NLP_etc..html">185 brendan oconnor ai-2012-07-17-p-values, CDF’s, NLP etc.</a></p>
<p>12 0.32504982 <a title="169-lda-12" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-12-31-List_of_probabilistic_model_mini-language_toolkits.html">157 brendan oconnor ai-2009-12-31-List of probabilistic model mini-language toolkits</a></p>
<p>13 0.31654137 <a title="169-lda-13" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-08-31-Probabilistic_interpretation_of_the_B3_coreference_resolution_metric.html">199 brendan oconnor ai-2013-08-31-Probabilistic interpretation of the B3 coreference resolution metric</a></p>
<p>14 0.3117916 <a title="169-lda-14" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-08-16-A_better_Obama_vs_McCain_poll_aggregation.html">111 brendan oconnor ai-2008-08-16-A better Obama vs McCain poll aggregation</a></p>
<p>15 0.30496132 <a title="169-lda-15" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-11-26-How_did_Freud_become_a_respected_humanist%3F%21.html">84 brendan oconnor ai-2007-11-26-How did Freud become a respected humanist?!</a></p>
<p>16 0.30275995 <a title="169-lda-16" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-10-11-It_is_accurate_to_determine_a_blog%E2%80%99s_bias_by_what_it_links_to.html">117 brendan oconnor ai-2008-10-11-It is accurate to determine a blog’s bias by what it links to</a></p>
<p>17 0.29948509 <a title="169-lda-17" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-01-07-Perplexity_as_branching_factor%3B_as_Shannon_diversity_index.html">190 brendan oconnor ai-2013-01-07-Perplexity as branching factor; as Shannon diversity index</a></p>
<p>18 0.29169741 <a title="169-lda-18" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-11-13-Authoritarian_great_power_capitalism.html">81 brendan oconnor ai-2007-11-13-Authoritarian great power capitalism</a></p>
<p>19 0.2851685 <a title="169-lda-19" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-03-15-Feminists%2C_anarchists%2C_computational_complexity%2C_bounded_rationality%2C_nethack%2C_and_other_things_to_do.html">53 brendan oconnor ai-2007-03-15-Feminists, anarchists, computational complexity, bounded rationality, nethack, and other things to do</a></p>
<p>20 0.27974132 <a title="169-lda-20" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-04-The_%2460%2C000_cat%3A_deep_belief_networks_make_less_sense_for_language_than_vision.html">184 brendan oconnor ai-2012-07-04-The $60,000 cat: deep belief networks make less sense for language than vision</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
