<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>176 brendan oconnor ai-2011-10-05-Be careful with dictionary-based text analysis</title>
</head>

<body>
<p><a title="brendan_oconnor_ai" href="../brendan_oconnor_ai_home.html">brendan_oconnor_ai</a> <a title="brendan_oconnor_ai-2011" href="../home/brendan_oconnor_ai-2011_home.html">brendan_oconnor_ai-2011</a> <a title="brendan_oconnor_ai-2011-176" href="#">brendan_oconnor_ai-2011-176</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>176 brendan oconnor ai-2011-10-05-Be careful with dictionary-based text analysis</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="brendan_oconnor_ai-2011-176-html" href="http://brenocon.com/blog/2011/10/be-careful-with-dictionary-based-text-analysis/">html</a></p><p>Introduction: OK, everyone loves to run dictionary methods for sentiment and other text analysis — counting words from a predefined lexicon in a big corpus, in order to explore or test hypotheses about the corpus.  In particular, this is often done for sentiment analysis: count positive and negative words (according to a sentiment polarity lexicon, which was derived from human raters or previous researchers’ intuitions), and then proclaim the output yields sentiment levels of the documents.  More and more papers come out every day that do this.   I’ve done this myself.   It’s interesting and fun, but it’s easy to get a bunch of meaningless numbers if you don’t carefully validate what’s going on.  There are certainly good studies in this area that do further validation and analysis, but it’s hard to trust a study that just presents a graph with a few overly strong speculative claims as to its meaning.  This happens more than it ought to.
 
I was happy to see a similarly critical view in a nice workin</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 OK, everyone loves to run dictionary methods for sentiment and other text analysis — counting words from a predefined lexicon in a big corpus, in order to explore or test hypotheses about the corpus. [sent-1, score-0.998]
</p><p>2 In particular, this is often done for sentiment analysis: count positive and negative words (according to a sentiment polarity lexicon, which was derived from human raters or previous researchers’ intuitions), and then proclaim the output yields sentiment levels of the documents. [sent-2, score-1.036]
</p><p>3 For dictionary methods to work well, the scores attached to words must closely align with how the words are used in a particular context. [sent-12, score-0.92]
</p><p>4 But  when dictionaries are created in one substantive area and then applied to another problems, serious errors can occur . [sent-14, score-0.593]
</p><p>5 Loughran and McDonald (2011) critique the increasingly common use of off the shelf dictionaries to measure the tone of statutorily required corporate earning reports in the accounting literature. [sent-16, score-0.912]
</p><p>6 They point out that many words that have a negative connotation in other contexts, like  tax ,  cost ,  crude  (oil) or  cancer , may have a positive connotation in earning reports. [sent-17, score-0.931]
</p><p>7 And words that are not identified as negative in off the shelf dictionaries may have quite negative connotation in earning reports ( unanticipated , for example). [sent-19, score-1.292]
</p><p>8 Scholars must either explicitly establish that word lists created in other contexts are applicable to a particular domain, or create a problem specific dictionary. [sent-21, score-0.454]
</p><p>9 Rather, standard practice in using dictionaries is to assume the measures created from a dictionary are correct and then apply them to the problem. [sent-24, score-1.189]
</p><p>10 While this is useful for applications, the granular measures insure that it is essentially impossible to derive gold standard evaluations based on human coding of documents, because of the difficulty of establishing reliable granular scales from humans (Krosnick, 1999). [sent-27, score-0.91]
</p><p>11 The consequence of domain specificity and lack of validation is that  most analyses based on dictionaries are built on shaky foundations. [sent-28, score-0.875]
</p><p>12 Yes, dictionaries are able to produce measures that are claimed to be about tone or emotion, but the actual properties of these measures – and how they relate to the concepts their attempting to measure – are essentially a mystery. [sent-29, score-1.42]
</p><p>13 Therefore, for scholars to effectively use dictionary methods in their future work, advances in the validation of dictionary methods must be made. [sent-30, score-1.545]
</p><p>14 We suggest two possible ways to improve validation of dictionary methods. [sent-31, score-0.524]
</p><p>15 If scholars use dictionaries to code documents into binary categories (positive or negative tone, for example), then validation based on human gold standards and the methods we describe in Section 4. [sent-33, score-1.515]
</p><p>16 Second, scholars could treat measures from dictionaries similar to how we validations from unsupervised methods are conducted (see Section 5. [sent-36, score-1.191]
</p><p>17 This would force scholars to establish that their measures of underlying concepts have properties associated with long standing expectations. [sent-38, score-0.749]
</p><p>18 And after an example analysis,     … we reiterate our skepticism of dictionary based measures. [sent-39, score-0.442]
</p><p>19 As is standard in the use of dictionary measures (for example, Young and Soroka (2011)) the measures are presented here without validation. [sent-40, score-0.94]
</p><p>20 This lack of validation is due in part because  it is exceedingly difficult to demonstrate that our scale of sentiment precisely measures differences in sentiment expressed  towards Russia. [sent-41, score-1.212]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('dictionaries', 0.467), ('dictionary', 0.303), ('measures', 0.296), ('scholars', 0.221), ('validation', 0.221), ('sentiment', 0.22), ('methods', 0.207), ('words', 0.128), ('connotation', 0.127), ('earning', 0.127), ('granular', 0.127), ('tone', 0.127), ('negative', 0.125), ('establish', 0.111), ('cancer', 0.085), ('crude', 0.085), ('documents', 0.085), ('loughran', 0.085), ('shelf', 0.085), ('section', 0.084), ('must', 0.083), ('created', 0.078), ('positive', 0.075), ('based', 0.074), ('validate', 0.074), ('analysis', 0.073), ('particular', 0.071), ('properties', 0.067), ('oil', 0.067), ('gold', 0.067), ('mcdonald', 0.067), ('lexicon', 0.067), ('example', 0.065), ('scales', 0.063), ('essentially', 0.063), ('lack', 0.059), ('contexts', 0.059), ('reports', 0.056), ('concepts', 0.054), ('scale', 0.054), ('domain', 0.054), ('may', 0.052), ('specific', 0.052), ('difficult', 0.05), ('measure', 0.05), ('due', 0.048), ('area', 0.048), ('human', 0.048), ('standard', 0.045), ('differences', 0.044)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999958 <a title="176-tfidf-1" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-10-05-Be_careful_with_dictionary-based_text_analysis.html">176 brendan oconnor ai-2011-10-05-Be careful with dictionary-based text analysis</a></p>
<p>Introduction: OK, everyone loves to run dictionary methods for sentiment and other text analysis — counting words from a predefined lexicon in a big corpus, in order to explore or test hypotheses about the corpus.  In particular, this is often done for sentiment analysis: count positive and negative words (according to a sentiment polarity lexicon, which was derived from human raters or previous researchers’ intuitions), and then proclaim the output yields sentiment levels of the documents.  More and more papers come out every day that do this.   I’ve done this myself.   It’s interesting and fun, but it’s easy to get a bunch of meaningless numbers if you don’t carefully validate what’s going on.  There are certainly good studies in this area that do further validation and analysis, but it’s hard to trust a study that just presents a graph with a few overly strong speculative claims as to its meaning.  This happens more than it ought to.
 
I was happy to see a similarly critical view in a nice workin</p><p>2 0.17294139 <a title="176-tfidf-2" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-27-Facebook_sentiment_mining_predicts_presidential_polls.html">131 brendan oconnor ai-2008-12-27-Facebook sentiment mining predicts presidential polls</a></p>
<p>Introduction: I’m a bit late blogging this, but here’s a messy, exciting — and statistically validated! — new online data source.
 
My friend  Roddy  at Facebook  wrote a post describing their sentiment analysis system , which can evaluate positive or negative sentiment toward a particular topic by looking at a large number of wall messages.  (I’d link to it, but I can’t find the URL anymore — here’s the  Lexicon , but that version only gets term frequencies but no sentiment.)
 
How they constructed their sentiment detector is interesting.  Starting with a list of positive and negative terms, they had a lexical acquisition step to gather many more candidate synonyms and misspellings — a necessity in this social media domain, where  WordNet  ain’t gonna come close!  After manually filtering these candidates, they assess the sentiment toward a mention of a topic by looking for instances of these positive and negative words nearby, along with “negation heuristics” and a few other features.
 
He describ</p><p>3 0.083756231 <a title="176-tfidf-3" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-06-14-Psychometrics_quote.html">144 brendan oconnor ai-2009-06-14-Psychometrics quote</a></p>
<p>Introduction: It is rather surprising that systematic studies of human abilities were not undertaken until the second half of the last century… An accurate method was available for measuring the circumference of the earth 2,000 years before the first systematic measures of human ability were developed.
  
–Jum Nunnally,  Psychometric Theory  (1967)
 
(Social science textbooks from the 60′s and 70′s are rad.)</p><p>4 0.070159256 <a title="176-tfidf-4" href="../brendan_oconnor_ai-2014/brendan_oconnor_ai-2014-02-19-What_the_ACL-2014_review_scores_mean.html">203 brendan oconnor ai-2014-02-19-What the ACL-2014 review scores mean</a></p>
<p>Introduction: I’ve had several people ask me what the numbers in  ACL  reviews mean — and I can’t find anywhere online where they’re described.  (Can anyone point this out if it is somewhere?)
 
So here’s the review form, below.  They all go from 1 to 5, with 5 the best.  I think the review emails to authors only include a subset of the below — for example, “Overall Recommendation” is not included?
 
The CFP said that they have different types of review forms for different types of papers.  I think this one is for a standard full paper.  I guess what people  really  want to know is what scores tend to correspond to acceptances.  I really have no idea and I get the impression this can change year to year.  I have no involvement with the ACL conference besides being one of many, many reviewers.
 
  
  
APPROPRIATENESS (1-5)
Does the paper fit in ACL 2014? (Please answer this question in light of the desire to broaden the scope of the research areas represented at ACL.) 

5: Certainly. 
4: Probabl</p><p>5 0.068525895 <a title="176-tfidf-5" href="../brendan_oconnor_ai-2004/brendan_oconnor_ai-2004-11-20-gintis%3A_theoretical_unity_in_the_social_sciences.html">1 brendan oconnor ai-2004-11-20-gintis: theoretical unity in the social sciences</a></p>
<p>Introduction: Herbert Gintis thinks  it’s time to unify the behavioral sciences. Sociology, economics, political science, human biology, anthropology and others all study the same thing, but each is based on different incompatible models of individual human behavior. There seems to be evidence that new developments have the potential to offer a more unifying theory. Evolutionary biology should be the basis of understanding much of human behavior. Rational choice and game theoretic frameworks are finding greater acceptance beyond economics; in the meantime, other fields need to absorb sociology’s emphasis on socialization — that people do things or understand the world in a way taught by society. The human behavioral sciences are still rife with many smaller inconsistencies; for example, according to Gintis, only anthropolgists look at the influence of culture across groups, but only sociologists look at culture within groups.
 
Gintis’ ultimate goal is to have a common baseline from which each disci</p><p>6 0.067639723 <a title="176-tfidf-6" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-04-The_%2460%2C000_cat%3A_deep_belief_networks_make_less_sense_for_language_than_vision.html">184 brendan oconnor ai-2012-07-04-The $60,000 cat: deep belief networks make less sense for language than vision</a></p>
<p>7 0.061092421 <a title="176-tfidf-7" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-08-08-Haghighi_and_Klein_%282009%29%3A_Simple_Coreference_Resolution_with_Rich_Syntactic_and_Semantic_Features.html">150 brendan oconnor ai-2009-08-08-Haghighi and Klein (2009): Simple Coreference Resolution with Rich Syntactic and Semantic Features</a></p>
<p>8 0.060049064 <a title="176-tfidf-8" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-07-31-war_death_statistics.html">22 brendan oconnor ai-2005-07-31-war death statistics</a></p>
<p>9 0.059449691 <a title="176-tfidf-9" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-01-02-funny_comic.html">48 brendan oconnor ai-2007-01-02-funny comic</a></p>
<p>10 0.059231408 <a title="176-tfidf-10" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-08-31-Probabilistic_interpretation_of_the_B3_coreference_resolution_metric.html">199 brendan oconnor ai-2013-08-31-Probabilistic interpretation of the B3 coreference resolution metric</a></p>
<p>11 0.058864221 <a title="176-tfidf-11" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-09-13-Response_on_our_movie_personas_paper.html">200 brendan oconnor ai-2013-09-13-Response on our movie personas paper</a></p>
<p>12 0.057062265 <a title="176-tfidf-12" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-01-07-Perplexity_as_branching_factor%3B_as_Shannon_diversity_index.html">190 brendan oconnor ai-2013-01-07-Perplexity as branching factor; as Shannon diversity index</a></p>
<p>13 0.055942919 <a title="176-tfidf-13" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-11-14-Pop_cog_neuro_is_so_sigh.html">82 brendan oconnor ai-2007-11-14-Pop cog neuro is so sigh</a></p>
<p>14 0.055010859 <a title="176-tfidf-14" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-26-Seeing_how_%E2%80%9Cart%E2%80%9D_and_%E2%80%9Cpharmaceuticals%E2%80%9D_are_linguistically_similar_in_web_text.html">156 brendan oconnor ai-2009-09-26-Seeing how “art” and “pharmaceuticals” are linguistically similar in web text</a></p>
<p>15 0.053791437 <a title="176-tfidf-15" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-03-Statistics_vs._Machine_Learning%2C_fight%21.html">129 brendan oconnor ai-2008-12-03-Statistics vs. Machine Learning, fight!</a></p>
<p>16 0.052937463 <a title="176-tfidf-16" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-22-Performance_comparison%3A_key-value_stores_for_language_model_counts.html">139 brendan oconnor ai-2009-04-22-Performance comparison: key-value stores for language model counts</a></p>
<p>17 0.044373095 <a title="176-tfidf-17" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-17-p-values%2C_CDF%E2%80%99s%2C_NLP_etc..html">185 brendan oconnor ai-2012-07-17-p-values, CDF’s, NLP etc.</a></p>
<p>18 0.044168618 <a title="176-tfidf-18" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-10-08-MyDebates.org%2C_online_polling%2C_and_potentially_the_coolest_question_corpus_ever.html">116 brendan oconnor ai-2008-10-08-MyDebates.org, online polling, and potentially the coolest question corpus ever</a></p>
<p>19 0.044154521 <a title="176-tfidf-19" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-01-07-Love_it_and_hate_it%2C_R_has_come_of_age.html">132 brendan oconnor ai-2009-01-07-Love it and hate it, R has come of age</a></p>
<p>20 0.044091128 <a title="176-tfidf-20" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-10-11-It_is_accurate_to_determine_a_blog%E2%80%99s_bias_by_what_it_links_to.html">117 brendan oconnor ai-2008-10-11-It is accurate to determine a blog’s bias by what it links to</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/brendan_oconnor_ai_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, -0.177), (1, -0.033), (2, 0.032), (3, -0.064), (4, 0.002), (5, -0.064), (6, -0.006), (7, -0.097), (8, -0.073), (9, -0.003), (10, 0.032), (11, -0.027), (12, 0.057), (13, 0.065), (14, 0.007), (15, -0.078), (16, 0.006), (17, -0.101), (18, -0.129), (19, -0.011), (20, -0.071), (21, -0.038), (22, 0.058), (23, -0.164), (24, -0.026), (25, -0.015), (26, 0.063), (27, -0.119), (28, 0.07), (29, 0.079), (30, -0.088), (31, -0.064), (32, -0.035), (33, -0.048), (34, -0.027), (35, 0.002), (36, -0.047), (37, 0.021), (38, -0.125), (39, 0.11), (40, -0.037), (41, -0.049), (42, 0.087), (43, -0.142), (44, -0.084), (45, -0.017), (46, 0.029), (47, 0.02), (48, 0.044), (49, 0.017)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98526525 <a title="176-lsi-1" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-10-05-Be_careful_with_dictionary-based_text_analysis.html">176 brendan oconnor ai-2011-10-05-Be careful with dictionary-based text analysis</a></p>
<p>Introduction: OK, everyone loves to run dictionary methods for sentiment and other text analysis — counting words from a predefined lexicon in a big corpus, in order to explore or test hypotheses about the corpus.  In particular, this is often done for sentiment analysis: count positive and negative words (according to a sentiment polarity lexicon, which was derived from human raters or previous researchers’ intuitions), and then proclaim the output yields sentiment levels of the documents.  More and more papers come out every day that do this.   I’ve done this myself.   It’s interesting and fun, but it’s easy to get a bunch of meaningless numbers if you don’t carefully validate what’s going on.  There are certainly good studies in this area that do further validation and analysis, but it’s hard to trust a study that just presents a graph with a few overly strong speculative claims as to its meaning.  This happens more than it ought to.
 
I was happy to see a similarly critical view in a nice workin</p><p>2 0.73923856 <a title="176-lsi-2" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-27-Facebook_sentiment_mining_predicts_presidential_polls.html">131 brendan oconnor ai-2008-12-27-Facebook sentiment mining predicts presidential polls</a></p>
<p>Introduction: I’m a bit late blogging this, but here’s a messy, exciting — and statistically validated! — new online data source.
 
My friend  Roddy  at Facebook  wrote a post describing their sentiment analysis system , which can evaluate positive or negative sentiment toward a particular topic by looking at a large number of wall messages.  (I’d link to it, but I can’t find the URL anymore — here’s the  Lexicon , but that version only gets term frequencies but no sentiment.)
 
How they constructed their sentiment detector is interesting.  Starting with a list of positive and negative terms, they had a lexical acquisition step to gather many more candidate synonyms and misspellings — a necessity in this social media domain, where  WordNet  ain’t gonna come close!  After manually filtering these candidates, they assess the sentiment toward a mention of a topic by looking for instances of these positive and negative words nearby, along with “negation heuristics” and a few other features.
 
He describ</p><p>3 0.562069 <a title="176-lsi-3" href="../brendan_oconnor_ai-2004/brendan_oconnor_ai-2004-12-02-go_science.html">3 brendan oconnor ai-2004-12-02-go science</a></p>
<p>Introduction: Is social science even worth doing when things like  this  get funded with hundreds of millions of federal dollars? 
 Many American youngsters participating in federally funded abstinence-only programs have been taught over the past three years that abortion can lead to sterility and suicide, that half the gay male teenagers in the United States have tested positive for the AIDS virus, and that touching a person’s genitals “can result in pregnancy,” a congressional staff analysis has found. 
 
…

  Among the misconceptions cited by Waxman’s investigators:
  • A 43-day-old fetus is a “thinking person.” • HIV, the virus that causes AIDS, can be spread via sweat and tears. • Condoms fail to prevent HIV transmission as often as 31 percent of the time in heterosexual intercourse.
 
…
 
When used properly and consistently, condoms fail to prevent pregnancy and sexually transmitted diseases (STDs) less than 3 percent of the time, federal researchers say, and it is not known how many gay teena</p><p>4 0.53583711 <a title="176-lsi-4" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-04-The_%2460%2C000_cat%3A_deep_belief_networks_make_less_sense_for_language_than_vision.html">184 brendan oconnor ai-2012-07-04-The $60,000 cat: deep belief networks make less sense for language than vision</a></p>
<p>Introduction: There was an  interesting ICML paper  this year about very large-scale training of deep belief networks (a.k.a. neural networks) for unsupervised concept extraction from images.  They ( Quoc V. Le  and colleagues at Google/Stanford) have a cute example of learning very high-level features that are evoked by images of cats (from YouTube still-image training data); one is shown below.
 
For those of us who work on machine learning and text, the question always comes up, why not DBN’s for language?  Many shallow latent-space text models have been quite successful (LSI, LDA, HMM, LPCFG…); there is hope that some sort of “deeper” concepts could be learned.  I think this is one of the most interesting areas for unsupervised language modeling right now.
 
But note it’s a bad idea to directly analogize results from image analysis to language analysis.  The problems have radically different levels of conceptual abstraction baked-in.  Consider the problem of detecting the concept of a cat; i.e.</p><p>5 0.51927751 <a title="176-lsi-5" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-01-02-funny_comic.html">48 brendan oconnor ai-2007-01-02-funny comic</a></p>
<p>Introduction: [doesn't fit well; please click.]     Thx  Words and Other Things .</p><p>6 0.48373899 <a title="176-lsi-6" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-08-31-Probabilistic_interpretation_of_the_B3_coreference_resolution_metric.html">199 brendan oconnor ai-2013-08-31-Probabilistic interpretation of the B3 coreference resolution metric</a></p>
<p>7 0.45635641 <a title="176-lsi-7" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-06-14-Psychometrics_quote.html">144 brendan oconnor ai-2009-06-14-Psychometrics quote</a></p>
<p>8 0.45479789 <a title="176-lsi-8" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-08-08-Haghighi_and_Klein_%282009%29%3A_Simple_Coreference_Resolution_with_Rich_Syntactic_and_Semantic_Features.html">150 brendan oconnor ai-2009-08-08-Haghighi and Klein (2009): Simple Coreference Resolution with Rich Syntactic and Semantic Features</a></p>
<p>9 0.40783036 <a title="176-lsi-9" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-08-13-It%E2%80%99s_all_in_a_name%3A_%22Kingdom_of_Norway%22_vs._%22Democratic_People%E2%80%99s_Republic_of_Korea%22.html">75 brendan oconnor ai-2007-08-13-It’s all in a name: "Kingdom of Norway" vs. "Democratic People’s Republic of Korea"</a></p>
<p>10 0.39542603 <a title="176-lsi-10" href="../brendan_oconnor_ai-2014/brendan_oconnor_ai-2014-02-19-What_the_ACL-2014_review_scores_mean.html">203 brendan oconnor ai-2014-02-19-What the ACL-2014 review scores mean</a></p>
<p>11 0.38497695 <a title="176-lsi-11" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-11-26-How_did_Freud_become_a_respected_humanist%3F%21.html">84 brendan oconnor ai-2007-11-26-How did Freud become a respected humanist?!</a></p>
<p>12 0.37774548 <a title="176-lsi-12" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-10-11-It_is_accurate_to_determine_a_blog%E2%80%99s_bias_by_what_it_links_to.html">117 brendan oconnor ai-2008-10-11-It is accurate to determine a blog’s bias by what it links to</a></p>
<p>13 0.36643997 <a title="176-lsi-13" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-08-16-A_better_Obama_vs_McCain_poll_aggregation.html">111 brendan oconnor ai-2008-08-16-A better Obama vs McCain poll aggregation</a></p>
<p>14 0.32669047 <a title="176-lsi-14" href="../brendan_oconnor_ai-2004/brendan_oconnor_ai-2004-11-20-gintis%3A_theoretical_unity_in_the_social_sciences.html">1 brendan oconnor ai-2004-11-20-gintis: theoretical unity in the social sciences</a></p>
<p>15 0.32055143 <a title="176-lsi-15" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-22-Performance_comparison%3A_key-value_stores_for_language_model_counts.html">139 brendan oconnor ai-2009-04-22-Performance comparison: key-value stores for language model counts</a></p>
<p>16 0.31727117 <a title="176-lsi-16" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-07-02-%24_echo_%7Bpolitical%2Csocial%2Ceconomic%7D%7Bcognition%2Cbehavior%2Csystems%7D.html">12 brendan oconnor ai-2005-07-02-$ echo {political,social,economic}{cognition,behavior,systems}</a></p>
<p>17 0.31409255 <a title="176-lsi-17" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-12-20-Data-driven_charity.html">86 brendan oconnor ai-2007-12-20-Data-driven charity</a></p>
<p>18 0.30533883 <a title="176-lsi-18" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-01-Binary_classification_evaluation_in_R_via_ROCR.html">136 brendan oconnor ai-2009-04-01-Binary classification evaluation in R via ROCR</a></p>
<p>19 0.30510256 <a title="176-lsi-19" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-18-Information_cost_and_genocide.html">130 brendan oconnor ai-2008-12-18-Information cost and genocide</a></p>
<p>20 0.29734546 <a title="176-lsi-20" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-11-13-Bayes_update_view_of_pointwise_mutual_information.html">178 brendan oconnor ai-2011-11-13-Bayes update view of pointwise mutual information</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/brendan_oconnor_ai_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(3, 0.015), (16, 0.011), (22, 0.044), (24, 0.029), (44, 0.142), (48, 0.032), (55, 0.445), (59, 0.011), (67, 0.029), (70, 0.022), (74, 0.116)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.98769796 <a title="176-lda-1" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-04-06-a_regression_slope_is_a_weighted_average_of_pairs%E2%80%99_slopes%21.html">100 brendan oconnor ai-2008-04-06-a regression slope is a weighted average of pairs’ slopes!</a></p>
<p>Introduction: Wow, this is pretty cool:
 
 
 
From an  Andrew Gelman article  on summaring a linear regression as a simple difference between upper and lower categories.  I get the impression there are lots of weird misunderstood corners of linear models… (e.g. that “least squares regression” is a maximum likelihood estimator for a linear model with normal noise… I know so many people who didn’t learn that from their stats whatever course, and therefore find it mystifying why squared error should be used…  see this  other post from Gelman .)</p><p>2 0.97672737 <a title="176-lda-2" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-07-23-R_questions_on_StackOverflow.html">148 brendan oconnor ai-2009-07-23-R questions on StackOverflow</a></p>
<p>Introduction: R is notoriously hard to learn, but there was just an effort  [1]   [2]  to populate the programming question-and-answer website  StackOverflow with content for the R language .
 
Amusingly, one of the most useful intro questions is:  How to search for “R” materials? 
 
 Mike Driscoll  (who organized an in-person conference event to get this bootstrapped) pointed out that in many ways StackOverflow is a nicer forum for help than a mailing list.  (i.e. the impressive but hard-to-approach  R-help .)  It’s more organized, easier to browse, and repetition and wrong answers can get downvoted.  (And  more thoughts from John Cook .)</p><p>3 0.96191573 <a title="176-lda-3" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-04-08-Rough_binomial_confidence_intervals.html">167 brendan oconnor ai-2011-04-08-Rough binomial confidence intervals</a></p>
<p>Introduction: I made this table a while ago and find it handy: for example, looking at a table of percentages and trying to figure out what’s meaningful or not. Why run a test if you can estimate it in your head?
 
 
 
References:Â  Wikipedia ,Â  binom.test</p><p>4 0.96189117 <a title="176-lda-4" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-08-25-Fukuyama%3A_Authoritarianism_is_still_against_history.html">112 brendan oconnor ai-2008-08-25-Fukuyama: Authoritarianism is still against history</a></p>
<p>Introduction: The latest on the world ideologies front –
 
In the light of Russia’s Georgia adventures, there’s been lots of talk whether this represents a new rise of authoritarian Russia, which is presumably another nail in the coffin for U.S.-led liberal democratic hegemony in the world.  Our “end of history” friend Francis Fukuyama just wrote an op-ed arguing that  Russia and China are still not big threats to liberal democracy .  There are some good points: Russia is behaving as an aggressive imperial power, but does not embrace a grand, exportable ideology with universal appeal.  Similarly with China.  They both still feel the need to pay lip service to democratic rituals and norms.  Even Nicholas Kristof’s hilarious column  chronicling his experience with China’s dubious protest registration system  concludes that even a pale mockery of democracy is progress.
 
I still like Azar Gat’s article which I wrote about last year, that  Russia and China represent authoritarian capitalism, which will</p><p>same-blog 5 0.9335131 <a title="176-lda-5" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-10-05-Be_careful_with_dictionary-based_text_analysis.html">176 brendan oconnor ai-2011-10-05-Be careful with dictionary-based text analysis</a></p>
<p>Introduction: OK, everyone loves to run dictionary methods for sentiment and other text analysis — counting words from a predefined lexicon in a big corpus, in order to explore or test hypotheses about the corpus.  In particular, this is often done for sentiment analysis: count positive and negative words (according to a sentiment polarity lexicon, which was derived from human raters or previous researchers’ intuitions), and then proclaim the output yields sentiment levels of the documents.  More and more papers come out every day that do this.   I’ve done this myself.   It’s interesting and fun, but it’s easy to get a bunch of meaningless numbers if you don’t carefully validate what’s going on.  There are certainly good studies in this area that do further validation and analysis, but it’s hard to trust a study that just presents a graph with a few overly strong speculative claims as to its meaning.  This happens more than it ought to.
 
I was happy to see a similarly critical view in a nice workin</p><p>6 0.59114099 <a title="176-lda-6" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-11-13-Authoritarian_great_power_capitalism.html">81 brendan oconnor ai-2007-11-13-Authoritarian great power capitalism</a></p>
<p>7 0.46502763 <a title="176-lda-7" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-03-Statistics_vs._Machine_Learning%2C_fight%21.html">129 brendan oconnor ai-2008-12-03-Statistics vs. Machine Learning, fight!</a></p>
<p>8 0.46068978 <a title="176-lda-8" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-12-20-Data-driven_charity.html">86 brendan oconnor ai-2007-12-20-Data-driven charity</a></p>
<p>9 0.41138363 <a title="176-lda-9" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-17-p-values%2C_CDF%E2%80%99s%2C_NLP_etc..html">185 brendan oconnor ai-2012-07-17-p-values, CDF’s, NLP etc.</a></p>
<p>10 0.40956387 <a title="176-lda-10" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-04-The_%2460%2C000_cat%3A_deep_belief_networks_make_less_sense_for_language_than_vision.html">184 brendan oconnor ai-2012-07-04-The $60,000 cat: deep belief networks make less sense for language than vision</a></p>
<p>11 0.40703025 <a title="176-lda-11" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-08-08-Haghighi_and_Klein_%282009%29%3A_Simple_Coreference_Resolution_with_Rich_Syntactic_and_Semantic_Features.html">150 brendan oconnor ai-2009-08-08-Haghighi and Klein (2009): Simple Coreference Resolution with Rich Syntactic and Semantic Features</a></p>
<p>12 0.40501687 <a title="176-lda-12" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-08-20-Some_analysis_of_tweet_shares_and_%E2%80%9Cpredicting%E2%80%9D_election_outcomes.html">198 brendan oconnor ai-2013-08-20-Some analysis of tweet shares and “predicting” election outcomes</a></p>
<p>13 0.4005962 <a title="176-lda-13" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-10-Don%E2%80%99t_MAWK_AWK_%E2%80%93_the_fastest_and_most_elegant_big_data_munging_language%21.html">154 brendan oconnor ai-2009-09-10-Don’t MAWK AWK – the fastest and most elegant big data munging language!</a></p>
<p>14 0.39875185 <a title="176-lda-14" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-03-27-Seth_Roberts_and_academic_blogging.html">55 brendan oconnor ai-2007-03-27-Seth Roberts and academic blogging</a></p>
<p>15 0.39593238 <a title="176-lda-15" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-10-02-Powerset%E2%80%99s_natural_language_search_system.html">188 brendan oconnor ai-2012-10-02-Powerset’s natural language search system</a></p>
<p>16 0.39495263 <a title="176-lda-16" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-17-1_billion_web_page_dataset_from_CMU.html">138 brendan oconnor ai-2009-04-17-1 billion web page dataset from CMU</a></p>
<p>17 0.39160874 <a title="176-lda-17" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-01-07-Love_it_and_hate_it%2C_R_has_come_of_age.html">132 brendan oconnor ai-2009-01-07-Love it and hate it, R has come of age</a></p>
<p>18 0.39113766 <a title="176-lda-18" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-26-Seeing_how_%E2%80%9Cart%E2%80%9D_and_%E2%80%9Cpharmaceuticals%E2%80%9D_are_linguistically_similar_in_web_text.html">156 brendan oconnor ai-2009-09-26-Seeing how “art” and “pharmaceuticals” are linguistically similar in web text</a></p>
<p>19 0.38327548 <a title="176-lda-19" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-09-13-Response_on_our_movie_personas_paper.html">200 brendan oconnor ai-2013-09-13-Response on our movie personas paper</a></p>
<p>20 0.38084489 <a title="176-lda-20" href="../brendan_oconnor_ai-2004/brendan_oconnor_ai-2004-11-24-addiction_%26_2_problems_of_economics.html">2 brendan oconnor ai-2004-11-24-addiction & 2 problems of economics</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
