<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>166 brendan oconnor ai-2011-03-02-Poor man’s linear algebra textbook</title>
</head>

<body>
<p><a title="brendan_oconnor_ai" href="../brendan_oconnor_ai_home.html">brendan_oconnor_ai</a> <a title="brendan_oconnor_ai-2011" href="../home/brendan_oconnor_ai-2011_home.html">brendan_oconnor_ai-2011</a> <a title="brendan_oconnor_ai-2011-166" href="#">brendan_oconnor_ai-2011-166</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>166 brendan oconnor ai-2011-03-02-Poor man’s linear algebra textbook</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="brendan_oconnor_ai-2011-166-html" href="http://brenocon.com/blog/2011/03/poor-mans-linear-algebra-textbook/">html</a></p><p>Introduction: I keep learning new bits of linear algebra all the time, but I’m always hurting for a useful reference.  I probably should get a good book (which?), but in the meantime I’m collecting several nice online sources that ML researchers seem to often recommend: The Matrix Cookbook, plus a few more tutorial/introductory pieces, aimed at an intermediate-ish level.
 
Main reference:
  
  The Matrix Cookbook  – 71 pages of identities and such.  This seems to be really popular. 
  
Tutorials/introductions:
  
  CS229 linear algebra review  – from Stanford’s ML course.  It seems to introduce all the essentials, and it’s vaguely familiar for me.  (26 pages) 
 Minka’s  Old and New Matrix Algebra Useful for Statistics  – has a great part on how to do derivatives.  (19 pages) 
 MacKay’s  The Humble Gaussian  – OK, not really pure linear algebra anymore, but quite enlightening.  (12 pages) 
  
After studying for this last stats/ML midterm, I’ve now printed them out and stuck them in a binder.  A poor</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 I keep learning new bits of linear algebra all the time, but I’m always hurting for a useful reference. [sent-1, score-1.335]
</p><p>2 ), but in the meantime I’m collecting several nice online sources that ML researchers seem to often recommend: The Matrix Cookbook, plus a few more tutorial/introductory pieces, aimed at an intermediate-ish level. [sent-3, score-0.442]
</p><p>3 Main reference:      The Matrix Cookbook  – 71 pages of identities and such. [sent-4, score-0.42]
</p><p>4 Tutorials/introductions:      CS229 linear algebra review  – from Stanford’s ML course. [sent-6, score-0.979]
</p><p>5 It seems to introduce all the essentials, and it’s vaguely familiar for me. [sent-7, score-0.234]
</p><p>6 (26 pages)   Minka’s  Old and New Matrix Algebra Useful for Statistics  – has a great part on how to do derivatives. [sent-8, score-0.052]
</p><p>7 (19 pages)   MacKay’s  The Humble Gaussian  – OK, not really pure linear algebra anymore, but quite enlightening. [sent-9, score-1.065]
</p><p>8 (12 pages)     After studying for this last stats/ML midterm, I’ve now printed them out and stuck them in a binder. [sent-10, score-0.199]
</p><p>9 I’d love to learn of more or different stuff out there. [sent-12, score-0.158]
</p><p>10 (There are always the appendixes of linear algebra reviews in  Hastie et al. [sent-13, score-1.198]
</p><p>11 ESL  and  Boyd+Vandenberghe CvxOpt , but I’ve always found them a little too small for usefulness+understanding. [sent-14, score-0.127]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('algebra', 0.622), ('pages', 0.316), ('linear', 0.3), ('cookbook', 0.238), ('matrix', 0.203), ('ml', 0.189), ('always', 0.127), ('stuck', 0.104), ('usefulness', 0.104), ('anymore', 0.104), ('identities', 0.104), ('boyd', 0.095), ('collecting', 0.095), ('vaguely', 0.095), ('studying', 0.095), ('gaussian', 0.095), ('meantime', 0.088), ('pieces', 0.088), ('hastie', 0.088), ('mackay', 0.083), ('pure', 0.083), ('reference', 0.079), ('bits', 0.079), ('reviews', 0.079), ('sources', 0.079), ('useful', 0.077), ('recommend', 0.076), ('familiar', 0.073), ('main', 0.073), ('keep', 0.07), ('et', 0.07), ('ok', 0.07), ('plus', 0.07), ('seems', 0.066), ('stanford', 0.065), ('man', 0.065), ('poor', 0.062), ('really', 0.06), ('researchers', 0.06), ('new', 0.06), ('statistics', 0.058), ('review', 0.057), ('probably', 0.057), ('old', 0.057), ('stuff', 0.054), ('learn', 0.053), ('part', 0.052), ('ve', 0.051), ('love', 0.051), ('online', 0.05)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="166-tfidf-1" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-03-02-Poor_man%E2%80%99s_linear_algebra_textbook.html">166 brendan oconnor ai-2011-03-02-Poor man’s linear algebra textbook</a></p>
<p>Introduction: I keep learning new bits of linear algebra all the time, but I’m always hurting for a useful reference.  I probably should get a good book (which?), but in the meantime I’m collecting several nice online sources that ML researchers seem to often recommend: The Matrix Cookbook, plus a few more tutorial/introductory pieces, aimed at an intermediate-ish level.
 
Main reference:
  
  The Matrix Cookbook  – 71 pages of identities and such.  This seems to be really popular. 
  
Tutorials/introductions:
  
  CS229 linear algebra review  – from Stanford’s ML course.  It seems to introduce all the essentials, and it’s vaguely familiar for me.  (26 pages) 
 Minka’s  Old and New Matrix Algebra Useful for Statistics  – has a great part on how to do derivatives.  (19 pages) 
 MacKay’s  The Humble Gaussian  – OK, not really pure linear algebra anymore, but quite enlightening.  (12 pages) 
  
After studying for this last stats/ML midterm, I’ve now printed them out and stuck them in a binder.  A poor</p><p>2 0.12465976 <a title="166-tfidf-2" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-03-Statistics_vs._Machine_Learning%2C_fight%21.html">129 brendan oconnor ai-2008-12-03-Statistics vs. Machine Learning, fight!</a></p>
<p>Introduction: 10/1/09 update  — well, it’s been nearly a year, and I should say not everything in this rant is totally true, and I certainly believe much less of it now.  Current take:  Statistics , not machine learning, is the real deal, but unfortunately suffers from bad marketing.  On the other hand, to the extent that bad marketing includes misguided undergraduate curriculums, there’s plenty of room to improve for everyone.
   
So it’s pretty clear by now that statistics and machine learning aren’t very different fields. I was recently pointed to   a very amusing comparison   by the excellent statistician — and machine learning expert —   Robert Tibshiriani  .  Reproduced here:  
 
  Glossary 
   
 Machine learning
  Statistics 
   
 network, graphs
  model 
   
 weights
  parameters 
   
 learning
  fitting 
   
 generalization
  test set performance 
   
 supervised learning
  regression/classiﬁcation 
   
 unsupervised learning
  density estimation, clustering 
   
 large grant = $1,000,000</p><p>3 0.11961221 <a title="166-tfidf-3" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-04-06-a_regression_slope_is_a_weighted_average_of_pairs%E2%80%99_slopes%21.html">100 brendan oconnor ai-2008-04-06-a regression slope is a weighted average of pairs’ slopes!</a></p>
<p>Introduction: Wow, this is pretty cool:
 
 
 
From an  Andrew Gelman article  on summaring a linear regression as a simple difference between upper and lower categories.  I get the impression there are lots of weird misunderstood corners of linear models… (e.g. that “least squares regression” is a maximum likelihood estimator for a linear model with normal noise… I know so many people who didn’t learn that from their stats whatever course, and therefore find it mystifying why squared error should be used…  see this  other post from Gelman .)</p><p>4 0.10766757 <a title="166-tfidf-4" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-02-23-Wasserman_on_Stats_vs_ML%2C_and_previous_comparisons.html">191 brendan oconnor ai-2013-02-23-Wasserman on Stats vs ML, and previous comparisons</a></p>
<p>Introduction: Larry Wasserman has a new position paper (forthcoming 2013) with a great comparison the Statistics and Machine Learning research cultures,  “Rise of the Machines” .  He has a very conciliatory view in terms of intellectual content, and a very pro-ML take on the research cultures.  Central to his argument is that ML has recently adopted rigorous statistical concepts, and the fast-moving conference culture (and heavy publishing by its grad students) have helped with this and other good innovations.  (I agree with a comment from Sinead that he’s going a little easy on ML, but it’s certainly worth a read.)
 
There’s now a little history of “Statistics vs Machine Learning” position papers that this can be compared to.  A classic is Leo Breiman (2001),  “Statistical Modeling: The Two Cultures” , which isn’t exactly about stats vs. ML, but is about the focus on modeling vs algorithms, and maybe about description vs. prediction.
 
It’s been a while since I’ve looked at it, but I’ve also enjoye</p><p>5 0.079222172 <a title="166-tfidf-5" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-01-11-Please_report_your_SVM%E2%80%99s_kernel%21.html">164 brendan oconnor ai-2011-01-11-Please report your SVM’s kernel!</a></p>
<p>Introduction: I’m tired of reading papers that use an  SVM  but don’t say which kernel they used.  (There’s tons of such papers in NLP and, I think, other areas that do applied machine learning.)  I suspect a lot of these papers are actually using a linear kernel.
 
An un-kernelized, linear SVM is nearly the same as  logistic regression  — every feature independently increases or decreases the classifier’s output prediction.  But a quadratic kernelized SVM is much more like boosted depth-2 decision trees.  It can do automatic combinations of pairs of features — a potentially very different thing, since you can start throwing in features that don’t do anything on their own but might have useful interactions with others.  (And of course, more complicated kernels do progressively more complicated and non-linear things.)
 
I have heard people say they download an SVM package, try a bunch of different kernels, and find the linear kernel is the best. In such cases they could have just used a logistic regr</p><p>6 0.065091811 <a title="166-tfidf-6" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-17-1_billion_web_page_dataset_from_CMU.html">138 brendan oconnor ai-2009-04-17-1 billion web page dataset from CMU</a></p>
<p>7 0.060773745 <a title="166-tfidf-7" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-04-08-More_fun_with_Gapminder_-_Trendalyzer.html">58 brendan oconnor ai-2007-04-08-More fun with Gapminder - Trendalyzer</a></p>
<p>8 0.057478987 <a title="166-tfidf-8" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-04-02-Datawocky%3A_More_data_usually_beats_better_algorithms.html">99 brendan oconnor ai-2008-04-02-Datawocky: More data usually beats better algorithms</a></p>
<p>9 0.052790042 <a title="166-tfidf-9" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-07-01-Bias_correction_sneak_peek%21.html">108 brendan oconnor ai-2008-07-01-Bias correction sneak peek!</a></p>
<p>10 0.047851849 <a title="166-tfidf-10" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-06-17-Confusion_matrix_diagrams.html">197 brendan oconnor ai-2013-06-17-Confusion matrix diagrams</a></p>
<p>11 0.043482155 <a title="166-tfidf-11" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-10-31-neo_institutional_economic_fun%21.html">80 brendan oconnor ai-2007-10-31-neo institutional economic fun!</a></p>
<p>12 0.043015204 <a title="166-tfidf-12" href="../brendan_oconnor_ai-2006/brendan_oconnor_ai-2006-06-03-Neuroeconomics_reviews.html">38 brendan oconnor ai-2006-06-03-Neuroeconomics reviews</a></p>
<p>13 0.042927254 <a title="166-tfidf-13" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-01-05-Indicators_of_a_crackpot_paper.html">88 brendan oconnor ai-2008-01-05-Indicators of a crackpot paper</a></p>
<p>14 0.042644348 <a title="166-tfidf-14" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-02-02-Histograms_%E2%80%94_matplotlib_vs._R.html">179 brendan oconnor ai-2012-02-02-Histograms — matplotlib vs. R</a></p>
<p>15 0.042631529 <a title="166-tfidf-15" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-01-07-Love_it_and_hate_it%2C_R_has_come_of_age.html">132 brendan oconnor ai-2009-01-07-Love it and hate it, R has come of age</a></p>
<p>16 0.042143684 <a title="166-tfidf-16" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-05-29-%22Stanford_Impostor%22.html">62 brendan oconnor ai-2007-05-29-"Stanford Impostor"</a></p>
<p>17 0.041441392 <a title="166-tfidf-17" href="../brendan_oconnor_ai-2010/brendan_oconnor_ai-2010-08-09-An_ML-AI_approach_to_P_%21%3D_NP.html">161 brendan oconnor ai-2010-08-09-An ML-AI approach to P != NP</a></p>
<p>18 0.04063794 <a title="166-tfidf-18" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-03-24-Quick-R%2C_the_only_decent_R_documentation_on_the_internet.html">97 brendan oconnor ai-2008-03-24-Quick-R, the only decent R documentation on the internet</a></p>
<p>19 0.038710035 <a title="166-tfidf-19" href="../brendan_oconnor_ai-2006/brendan_oconnor_ai-2006-04-28-Easterly_vs._Sachs_on_global_poverty.html">35 brendan oconnor ai-2006-04-28-Easterly vs. Sachs on global poverty</a></p>
<p>20 0.038603235 <a title="166-tfidf-20" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-10-08-Blog_move_has_landed.html">115 brendan oconnor ai-2008-10-08-Blog move has landed</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/brendan_oconnor_ai_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, -0.154), (1, -0.044), (2, 0.019), (3, -0.09), (4, 0.043), (5, 0.107), (6, -0.019), (7, 0.008), (8, 0.065), (9, -0.018), (10, 0.016), (11, 0.013), (12, -0.039), (13, -0.062), (14, -0.113), (15, 0.032), (16, 0.047), (17, 0.055), (18, 0.097), (19, -0.033), (20, -0.07), (21, 0.214), (22, 0.035), (23, 0.09), (24, 0.181), (25, 0.044), (26, -0.011), (27, -0.009), (28, -0.097), (29, -0.031), (30, 0.036), (31, -0.055), (32, 0.019), (33, 0.011), (34, 0.043), (35, 0.035), (36, -0.031), (37, 0.022), (38, -0.09), (39, 0.02), (40, -0.033), (41, 0.055), (42, -0.036), (43, 0.072), (44, -0.021), (45, -0.027), (46, 0.067), (47, 0.051), (48, 0.058), (49, 0.095)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98784095 <a title="166-lsi-1" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-03-02-Poor_man%E2%80%99s_linear_algebra_textbook.html">166 brendan oconnor ai-2011-03-02-Poor man’s linear algebra textbook</a></p>
<p>Introduction: I keep learning new bits of linear algebra all the time, but I’m always hurting for a useful reference.  I probably should get a good book (which?), but in the meantime I’m collecting several nice online sources that ML researchers seem to often recommend: The Matrix Cookbook, plus a few more tutorial/introductory pieces, aimed at an intermediate-ish level.
 
Main reference:
  
  The Matrix Cookbook  – 71 pages of identities and such.  This seems to be really popular. 
  
Tutorials/introductions:
  
  CS229 linear algebra review  – from Stanford’s ML course.  It seems to introduce all the essentials, and it’s vaguely familiar for me.  (26 pages) 
 Minka’s  Old and New Matrix Algebra Useful for Statistics  – has a great part on how to do derivatives.  (19 pages) 
 MacKay’s  The Humble Gaussian  – OK, not really pure linear algebra anymore, but quite enlightening.  (12 pages) 
  
After studying for this last stats/ML midterm, I’ve now printed them out and stuck them in a binder.  A poor</p><p>2 0.65023792 <a title="166-lsi-2" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-04-06-a_regression_slope_is_a_weighted_average_of_pairs%E2%80%99_slopes%21.html">100 brendan oconnor ai-2008-04-06-a regression slope is a weighted average of pairs’ slopes!</a></p>
<p>Introduction: Wow, this is pretty cool:
 
 
 
From an  Andrew Gelman article  on summaring a linear regression as a simple difference between upper and lower categories.  I get the impression there are lots of weird misunderstood corners of linear models… (e.g. that “least squares regression” is a maximum likelihood estimator for a linear model with normal noise… I know so many people who didn’t learn that from their stats whatever course, and therefore find it mystifying why squared error should be used…  see this  other post from Gelman .)</p><p>3 0.60509717 <a title="166-lsi-3" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-01-11-Please_report_your_SVM%E2%80%99s_kernel%21.html">164 brendan oconnor ai-2011-01-11-Please report your SVM’s kernel!</a></p>
<p>Introduction: I’m tired of reading papers that use an  SVM  but don’t say which kernel they used.  (There’s tons of such papers in NLP and, I think, other areas that do applied machine learning.)  I suspect a lot of these papers are actually using a linear kernel.
 
An un-kernelized, linear SVM is nearly the same as  logistic regression  — every feature independently increases or decreases the classifier’s output prediction.  But a quadratic kernelized SVM is much more like boosted depth-2 decision trees.  It can do automatic combinations of pairs of features — a potentially very different thing, since you can start throwing in features that don’t do anything on their own but might have useful interactions with others.  (And of course, more complicated kernels do progressively more complicated and non-linear things.)
 
I have heard people say they download an SVM package, try a bunch of different kernels, and find the linear kernel is the best. In such cases they could have just used a logistic regr</p><p>4 0.54892123 <a title="166-lsi-4" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-03-Statistics_vs._Machine_Learning%2C_fight%21.html">129 brendan oconnor ai-2008-12-03-Statistics vs. Machine Learning, fight!</a></p>
<p>Introduction: 10/1/09 update  — well, it’s been nearly a year, and I should say not everything in this rant is totally true, and I certainly believe much less of it now.  Current take:  Statistics , not machine learning, is the real deal, but unfortunately suffers from bad marketing.  On the other hand, to the extent that bad marketing includes misguided undergraduate curriculums, there’s plenty of room to improve for everyone.
   
So it’s pretty clear by now that statistics and machine learning aren’t very different fields. I was recently pointed to   a very amusing comparison   by the excellent statistician — and machine learning expert —   Robert Tibshiriani  .  Reproduced here:  
 
  Glossary 
   
 Machine learning
  Statistics 
   
 network, graphs
  model 
   
 weights
  parameters 
   
 learning
  fitting 
   
 generalization
  test set performance 
   
 supervised learning
  regression/classiﬁcation 
   
 unsupervised learning
  density estimation, clustering 
   
 large grant = $1,000,000</p><p>5 0.50258929 <a title="166-lsi-5" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-02-23-Wasserman_on_Stats_vs_ML%2C_and_previous_comparisons.html">191 brendan oconnor ai-2013-02-23-Wasserman on Stats vs ML, and previous comparisons</a></p>
<p>Introduction: Larry Wasserman has a new position paper (forthcoming 2013) with a great comparison the Statistics and Machine Learning research cultures,  “Rise of the Machines” .  He has a very conciliatory view in terms of intellectual content, and a very pro-ML take on the research cultures.  Central to his argument is that ML has recently adopted rigorous statistical concepts, and the fast-moving conference culture (and heavy publishing by its grad students) have helped with this and other good innovations.  (I agree with a comment from Sinead that he’s going a little easy on ML, but it’s certainly worth a read.)
 
There’s now a little history of “Statistics vs Machine Learning” position papers that this can be compared to.  A classic is Leo Breiman (2001),  “Statistical Modeling: The Two Cultures” , which isn’t exactly about stats vs. ML, but is about the focus on modeling vs algorithms, and maybe about description vs. prediction.
 
It’s been a while since I’ve looked at it, but I’ve also enjoye</p><p>6 0.37449566 <a title="166-lsi-6" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-05-29-%22Stanford_Impostor%22.html">62 brendan oconnor ai-2007-05-29-"Stanford Impostor"</a></p>
<p>7 0.34254032 <a title="166-lsi-7" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-07-01-Bias_correction_sneak_peek%21.html">108 brendan oconnor ai-2008-07-01-Bias correction sneak peek!</a></p>
<p>8 0.33635959 <a title="166-lsi-8" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-11-11-Memorizing_small_tables.html">177 brendan oconnor ai-2011-11-11-Memorizing small tables</a></p>
<p>9 0.33406371 <a title="166-lsi-9" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-04-08-More_fun_with_Gapminder_-_Trendalyzer.html">58 brendan oconnor ai-2007-04-08-More fun with Gapminder - Trendalyzer</a></p>
<p>10 0.33347288 <a title="166-lsi-10" href="../brendan_oconnor_ai-2010/brendan_oconnor_ai-2010-08-09-An_ML-AI_approach_to_P_%21%3D_NP.html">161 brendan oconnor ai-2010-08-09-An ML-AI approach to P != NP</a></p>
<p>11 0.31645295 <a title="166-lsi-11" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-07-22-FFT%3A_Friedman_%2B_Fortran_%2B_Tricks.html">147 brendan oconnor ai-2009-07-22-FFT: Friedman + Fortran + Tricks</a></p>
<p>12 0.30875921 <a title="166-lsi-12" href="../brendan_oconnor_ai-2014/brendan_oconnor_ai-2014-04-26-Replot%3A_departure_delays_vs_flight_time_speed-up.html">204 brendan oconnor ai-2014-04-26-Replot: departure delays vs flight time speed-up</a></p>
<p>13 0.30322623 <a title="166-lsi-13" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-10-31-tanh_is_a_rescaled_logistic_sigmoid_function.html">201 brendan oconnor ai-2013-10-31-tanh is a rescaled logistic sigmoid function</a></p>
<p>14 0.29187295 <a title="166-lsi-14" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-18-Information_cost_and_genocide.html">130 brendan oconnor ai-2008-12-18-Information cost and genocide</a></p>
<p>15 0.29109317 <a title="166-lsi-15" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-07-11-guns%2C_germs%2C_%26_steel_pbs_show%3F%21.html">20 brendan oconnor ai-2005-07-11-guns, germs, & steel pbs show?!</a></p>
<p>16 0.27844742 <a title="166-lsi-16" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-12-26-What_is_experimental_philosophy%3F.html">87 brendan oconnor ai-2007-12-26-What is experimental philosophy?</a></p>
<p>17 0.27475148 <a title="166-lsi-17" href="../brendan_oconnor_ai-2006/brendan_oconnor_ai-2006-04-28-Easterly_vs._Sachs_on_global_poverty.html">35 brendan oconnor ai-2006-04-28-Easterly vs. Sachs on global poverty</a></p>
<p>18 0.27174821 <a title="166-lsi-18" href="../brendan_oconnor_ai-2006/brendan_oconnor_ai-2006-02-21-Libertarianism_and_evolution_don%E2%80%99t_mix.html">30 brendan oconnor ai-2006-02-21-Libertarianism and evolution don’t mix</a></p>
<p>19 0.26853055 <a title="166-lsi-19" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-08-01-searchin%E2%80%99_for_our_friend%2C_homo_economicus.html">24 brendan oconnor ai-2005-08-01-searchin’ for our friend, homo economicus</a></p>
<p>20 0.26597127 <a title="166-lsi-20" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-04-The_%2460%2C000_cat%3A_deep_belief_networks_make_less_sense_for_language_than_vision.html">184 brendan oconnor ai-2012-07-04-The $60,000 cat: deep belief networks make less sense for language than vision</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/brendan_oconnor_ai_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(16, 0.05), (43, 0.043), (44, 0.084), (52, 0.013), (54, 0.536), (70, 0.031), (74, 0.106)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.94990814 <a title="166-lda-1" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-08-13-It%E2%80%99s_all_in_a_name%3A_%22Kingdom_of_Norway%22_vs._%22Democratic_People%E2%80%99s_Republic_of_Korea%22.html">75 brendan oconnor ai-2007-08-13-It’s all in a name: "Kingdom of Norway" vs. "Democratic People’s Republic of Korea"</a></p>
<p>Introduction: Sometimes it seems bad countries come with long names.  North Korea is “People’s Democratic Republic of Korea”, Libya is “Great Socialist People’s Libyan Arab Jamahiriya”, and the like.  But on the other hand, there’s plenty of counter-examples — it’s the “United Kingdom of Great Britain and Northern Ireland” and “Republic of Cuba”, after all.  Do long names with good-sounding adjectives correspond with non-democratic governments?
 
Fortunately, this can be tested.  First, what words are out there?  From the  CIA Factbook’s  data on long form names, here are some of the most popular words used by today’s countries, listed with the number of occurrences across all 194 names.  I limited to tokens that appear >= 3 times.  A majority of countries are Republics, while there are some Kingdoms, and even a few Democracies.
  
(146 of) (127 Republic) (17 Kingdom) (8 the) (8 Democratic) (6 State) (6 People’s) (5 United) (4 and) (4 Islamic) (4 Arab) (3 States) (3 Socialist) (3 Principality) (3 Is</p><p>same-blog 2 0.92671096 <a title="166-lda-2" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-03-02-Poor_man%E2%80%99s_linear_algebra_textbook.html">166 brendan oconnor ai-2011-03-02-Poor man’s linear algebra textbook</a></p>
<p>Introduction: I keep learning new bits of linear algebra all the time, but I’m always hurting for a useful reference.  I probably should get a good book (which?), but in the meantime I’m collecting several nice online sources that ML researchers seem to often recommend: The Matrix Cookbook, plus a few more tutorial/introductory pieces, aimed at an intermediate-ish level.
 
Main reference:
  
  The Matrix Cookbook  – 71 pages of identities and such.  This seems to be really popular. 
  
Tutorials/introductions:
  
  CS229 linear algebra review  – from Stanford’s ML course.  It seems to introduce all the essentials, and it’s vaguely familiar for me.  (26 pages) 
 Minka’s  Old and New Matrix Algebra Useful for Statistics  – has a great part on how to do derivatives.  (19 pages) 
 MacKay’s  The Humble Gaussian  – OK, not really pure linear algebra anymore, but quite enlightening.  (12 pages) 
  
After studying for this last stats/ML midterm, I’ve now printed them out and stuck them in a binder.  A poor</p><p>3 0.25644627 <a title="166-lda-3" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-03-Statistics_vs._Machine_Learning%2C_fight%21.html">129 brendan oconnor ai-2008-12-03-Statistics vs. Machine Learning, fight!</a></p>
<p>Introduction: 10/1/09 update  — well, it’s been nearly a year, and I should say not everything in this rant is totally true, and I certainly believe much less of it now.  Current take:  Statistics , not machine learning, is the real deal, but unfortunately suffers from bad marketing.  On the other hand, to the extent that bad marketing includes misguided undergraduate curriculums, there’s plenty of room to improve for everyone.
   
So it’s pretty clear by now that statistics and machine learning aren’t very different fields. I was recently pointed to   a very amusing comparison   by the excellent statistician — and machine learning expert —   Robert Tibshiriani  .  Reproduced here:  
 
  Glossary 
   
 Machine learning
  Statistics 
   
 network, graphs
  model 
   
 weights
  parameters 
   
 learning
  fitting 
   
 generalization
  test set performance 
   
 supervised learning
  regression/classiﬁcation 
   
 unsupervised learning
  density estimation, clustering 
   
 large grant = $1,000,000</p><p>4 0.22739097 <a title="166-lda-4" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-10-02-Powerset%E2%80%99s_natural_language_search_system.html">188 brendan oconnor ai-2012-10-02-Powerset’s natural language search system</a></p>
<p>Introduction: There’s a lot to say about  Powerset , the short-lived natural language search company (2005-2008) where I worked after college.  AI overhype, flying too close to the sun, the psychology of tech journalism and venture capitalism, etc.  A year or two ago I wrote the following bit about Powerset’s technology in response to a question  on Quora .  I’m posting a revised version here.
 
 Question:  What was Powerset’s core innovation in search?  As far as I can tell, they licensed an NLP engine. They did not have a question answering system or any system for information extraction. How was Powerset’s search engine different than Google’s?
 
 My answer:  Powerset built a system vaguely like a question-answering system on top of Xerox PARC’s NLP engine.  The output is better described as query-focused summarization rather than question answering; primarily, it matched semantic fragments of the user query against indexed semantic relations, with lots of keyword/ngram-matching fallback for when</p><p>5 0.22693649 <a title="166-lda-5" href="../brendan_oconnor_ai-2004/brendan_oconnor_ai-2004-11-24-addiction_%26_2_problems_of_economics.html">2 brendan oconnor ai-2004-11-24-addiction & 2 problems of economics</a></p>
<p>Introduction: This is my idea based off of  Bernheim and Rangel’s  model of addict decision-making .  It’s a really neat model; it manages to relax rationality to allow someone to do something they don’t want to do because they’re addicted to it.  [Rationality assumes a nice well-ordered set of preferences; this model hypothesizes as distinction between emotional "liking" and cognitive, forward "wanting" that can conflict.]  The model is mathematically tractable, it can be used for public welfare analysis, and to top it off — it’s got neuroscientific grounding!
 
It appears to me there are two big criticisms of the economics discipline’s assumptions.  One of course is rationality.  The second has to do with the perfect structure of the market and environment that shapes both preferences and the ability to exercise them.  One critique is about social structure: consumers are not atomistic individual units, but rather exchange information and ideas along networks of patterned social relations.  (Socia</p><p>6 0.22480848 <a title="166-lda-6" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-11-12-Disease_tracking_with_web_queries_and_social_messaging_%28Google%2C_Twitter%2C_Facebook%E2%80%A6%29.html">123 brendan oconnor ai-2008-11-12-Disease tracking with web queries and social messaging (Google, Twitter, Facebook…)</a></p>
<p>7 0.22429664 <a title="166-lda-7" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-03-15-Feminists%2C_anarchists%2C_computational_complexity%2C_bounded_rationality%2C_nethack%2C_and_other_things_to_do.html">53 brendan oconnor ai-2007-03-15-Feminists, anarchists, computational complexity, bounded rationality, nethack, and other things to do</a></p>
<p>8 0.22222744 <a title="166-lda-8" href="../brendan_oconnor_ai-2014/brendan_oconnor_ai-2014-02-19-What_the_ACL-2014_review_scores_mean.html">203 brendan oconnor ai-2014-02-19-What the ACL-2014 review scores mean</a></p>
<p>9 0.22163501 <a title="166-lda-9" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-04-The_%2460%2C000_cat%3A_deep_belief_networks_make_less_sense_for_language_than_vision.html">184 brendan oconnor ai-2012-07-04-The $60,000 cat: deep belief networks make less sense for language than vision</a></p>
<p>10 0.21914217 <a title="166-lda-10" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-17-1_billion_web_page_dataset_from_CMU.html">138 brendan oconnor ai-2009-04-17-1 billion web page dataset from CMU</a></p>
<p>11 0.21800691 <a title="166-lda-11" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-05-18-Announcing_TweetMotif_for_summarizing_twitter_topics.html">140 brendan oconnor ai-2009-05-18-Announcing TweetMotif for summarizing twitter topics</a></p>
<p>12 0.21742797 <a title="166-lda-12" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-08-08-Haghighi_and_Klein_%282009%29%3A_Simple_Coreference_Resolution_with_Rich_Syntactic_and_Semantic_Features.html">150 brendan oconnor ai-2009-08-08-Haghighi and Klein (2009): Simple Coreference Resolution with Rich Syntactic and Semantic Features</a></p>
<p>13 0.21559547 <a title="166-lda-13" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-09-02-cognitive_modelling_is_rational_choice%2B%2B.html">26 brendan oconnor ai-2005-09-02-cognitive modelling is rational choice++</a></p>
<p>14 0.21370235 <a title="166-lda-14" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-08-20-Some_analysis_of_tweet_shares_and_%E2%80%9Cpredicting%E2%80%9D_election_outcomes.html">198 brendan oconnor ai-2013-08-20-Some analysis of tweet shares and “predicting” election outcomes</a></p>
<p>15 0.21364364 <a title="166-lda-15" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-10-31-neo_institutional_economic_fun%21.html">80 brendan oconnor ai-2007-10-31-neo institutional economic fun!</a></p>
<p>16 0.2131139 <a title="166-lda-16" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-09-13-Response_on_our_movie_personas_paper.html">200 brendan oconnor ai-2013-09-13-Response on our movie personas paper</a></p>
<p>17 0.21293524 <a title="166-lda-17" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-06-10-Freak-Freakonomics_%28Ariel_Rubinstein_is_the_shit%21%29.html">63 brendan oconnor ai-2007-06-10-Freak-Freakonomics (Ariel Rubinstein is the shit!)</a></p>
<p>18 0.21273048 <a title="166-lda-18" href="../brendan_oconnor_ai-2006/brendan_oconnor_ai-2006-08-30-A_big%2C_fun_list_of_links_I%E2%80%99m_reading.html">44 brendan oconnor ai-2006-08-30-A big, fun list of links I’m reading</a></p>
<p>19 0.2110993 <a title="166-lda-19" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-12-20-Data-driven_charity.html">86 brendan oconnor ai-2007-12-20-Data-driven charity</a></p>
<p>20 0.20917203 <a title="166-lda-20" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-02-02-Histograms_%E2%80%94_matplotlib_vs._R.html">179 brendan oconnor ai-2012-02-02-Histograms — matplotlib vs. R</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
