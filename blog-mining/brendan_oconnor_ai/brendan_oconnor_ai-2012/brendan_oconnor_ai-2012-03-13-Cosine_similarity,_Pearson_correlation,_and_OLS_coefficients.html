<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>182 brendan oconnor ai-2012-03-13-Cosine similarity, Pearson correlation, and OLS coefficients</title>
</head>

<body>
<p><a title="brendan_oconnor_ai" href="../brendan_oconnor_ai_home.html">brendan_oconnor_ai</a> <a title="brendan_oconnor_ai-2012" href="../home/brendan_oconnor_ai-2012_home.html">brendan_oconnor_ai-2012</a> <a title="brendan_oconnor_ai-2012-182" href="#">brendan_oconnor_ai-2012-182</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>182 brendan oconnor ai-2012-03-13-Cosine similarity, Pearson correlation, and OLS coefficients</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="brendan_oconnor_ai-2012-182-html" href="http://brenocon.com/blog/2012/03/cosine-similarity-pearson-correlation-and-ols-coefficients/">html</a></p><p>Introduction: Cosine similarity, Pearson correlations, and OLS coefficients can all be viewed as variants on the inner product — tweaked in different ways for centering and magnitude (i.e. location and scale, or something like that).
 
Details:
 
You have two vectors \(x\) and \(y\) and want to measure similarity between them.  A basic similarity function is the   inner product  
 
\[ Inner(x,y) = \sum_i x_i y_i = \langle x, y \rangle \]
 
If x tends to be high where y is also high, and low where y is low, the inner product will be high — the vectors are more similar.
 
The inner product is unbounded.  One way to make it bounded between -1 and 1 is to divide by the vectors’ L2 norms, giving the   cosine similarity  
 
\[ CosSim(x,y) = \frac{\sum_i x_i y_i}{ \sqrt{ \sum_i x_i^2} \sqrt{ \sum_i y_i^2 } } 
= \frac{ \langle x,y \rangle }{ ||x||\ ||y|| } 
\]
 
This is actually bounded between 0 and 1 if x and y are non-negative.  Cosine similarity has an interpretation as the cosine of the angle between t</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Cosine similarity, Pearson correlations, and OLS coefficients can all be viewed as variants on the inner product — tweaked in different ways for centering and magnitude (i. [sent-1, score-0.333]
</p><p>2 Details:   You have two vectors \(x\) and \(y\) and want to measure similarity between them. [sent-4, score-0.319]
</p><p>3 A basic similarity function is the   inner product     \[ Inner(x,y) = \sum_i x_i y_i = \langle x, y \rangle \]   If x tends to be high where y is also high, and low where y is low, the inner product will be high — the vectors are more similar. [sent-5, score-0.985]
</p><p>4 Cosine similarity has an interpretation as the cosine of the angle between the two vectors; you can illustrate this for vectors in \(\mathbb{R}^2\) (e. [sent-8, score-0.676]
</p><p>5 Cosine similarity is not invariant to shifts. [sent-11, score-0.302]
</p><p>6 If x was shifted to x+1, the cosine similarity would change. [sent-12, score-0.538]
</p><p>7 What is invariant, though, is the   Pearson correlation  . [sent-13, score-0.109]
</p><p>8 People usually talk about cosine similarity in terms of vector angles, but it can be loosely thought of as a correlation, if you think of the vectors as paired samples. [sent-15, score-0.648]
</p><p>9 Unlike the cosine, the correlation is invariant to both scale and location changes of x and y. [sent-16, score-0.239]
</p><p>10 But unlike cosine similarity, we aren’t normalizing by \(y\)’s norm — instead we only use \(x\)’s norm (and use it twice): denominator of \(||x||\ ||y||\) versus \(||x||^2\). [sent-19, score-0.458]
</p><p>11 So OLSCoefWithIntercept is invariant to shifts of x. [sent-23, score-0.135]
</p><p>12 It’s still different than cosine similarity since it’s still not normalizing at all for y. [sent-24, score-0.601]
</p><p>13 )   Finally, what if x and y are standardized: both centered and normalized to unit standard deviation? [sent-28, score-0.236]
</p><p>14 The OLS coefficient for that is the same as the Pearson correlation between the original vectors. [sent-29, score-0.151]
</p><p>15 I’m not sure what this means or if it’s a useful fact, but:   \[ OLSCoef\left(  \sqrt{n}\frac{x-\bar{x}}{||x-\bar{x}||},  \sqrt{n}\frac{y-\bar{y}}{||y-\bar{y}||} \right) = Corr(x,y) \]   Summarizing: Cosine similarity is normalized inner product. [sent-30, score-0.548]
</p><p>16 A one-variable OLS coefficient is like cosine but with one-sided normalization. [sent-32, score-0.371]
</p><p>17 I’ve been wondering for a while why cosine similarity tends to be so useful for natural language processing applications. [sent-40, score-0.538]
</p><p>18 One implication of all the inner product stuff is computational strategies to make it faster when there’s high-dimensional sparse data — the  Friedman et al. [sent-43, score-0.37]
</p><p>19 And there’s lots of work using LSH for cosine similarity; e. [sent-47, score-0.329]
</p><p>20 ) In my experience, cosine similarity is talked about more often in text processing or machine learning contexts. [sent-56, score-0.538]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bar', 0.585), ('cosine', 0.329), ('langle', 0.295), ('rangle', 0.295), ('frac', 0.218), ('similarity', 0.209), ('inner', 0.187), ('sum', 0.15), ('product', 0.146), ('centered', 0.126), ('normalized', 0.11), ('vectors', 0.11), ('correlation', 0.109), ('mathbb', 0.105), ('pearson', 0.105), ('sqrt', 0.1), ('invariant', 0.093), ('ols', 0.092), ('align', 0.088), ('cossim', 0.084), ('yes', 0.071), ('corr', 0.063), ('intercept', 0.063), ('normalizing', 0.063), ('olscoef', 0.063), ('regression', 0.056), ('input', 0.05), ('bounded', 0.044), ('begin', 0.044), ('means', 0.042), ('cov', 0.042), ('lsh', 0.042), ('olscoefwithintercept', 0.042), ('shifts', 0.042), ('coefficient', 0.042), ('et', 0.037), ('location', 0.037), ('covariance', 0.037), ('shift', 0.033), ('norm', 0.033), ('constant', 0.033), ('linear', 0.032), ('symmetric', 0.031), ('inputs', 0.031), ('end', 0.031), ('chapter', 0.029), ('equation', 0.029), ('range', 0.029), ('derive', 0.028), ('interpretation', 0.028)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999958 <a title="182-tfidf-1" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-03-13-Cosine_similarity%2C_Pearson_correlation%2C_and_OLS_coefficients.html">182 brendan oconnor ai-2012-03-13-Cosine similarity, Pearson correlation, and OLS coefficients</a></p>
<p>Introduction: Cosine similarity, Pearson correlations, and OLS coefficients can all be viewed as variants on the inner product — tweaked in different ways for centering and magnitude (i.e. location and scale, or something like that).
 
Details:
 
You have two vectors \(x\) and \(y\) and want to measure similarity between them.  A basic similarity function is the   inner product  
 
\[ Inner(x,y) = \sum_i x_i y_i = \langle x, y \rangle \]
 
If x tends to be high where y is also high, and low where y is low, the inner product will be high — the vectors are more similar.
 
The inner product is unbounded.  One way to make it bounded between -1 and 1 is to divide by the vectors’ L2 norms, giving the   cosine similarity  
 
\[ CosSim(x,y) = \frac{\sum_i x_i y_i}{ \sqrt{ \sum_i x_i^2} \sqrt{ \sum_i y_i^2 } } 
= \frac{ \langle x,y \rangle }{ ||x||\ ||y|| } 
\]
 
This is actually bounded between 0 and 1 if x and y are non-negative.  Cosine similarity has an interpretation as the cosine of the angle between t</p><p>2 0.20803343 <a title="182-tfidf-2" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-03-18-Correlation_picture.html">193 brendan oconnor ai-2013-03-18-Correlation picture</a></p>
<p>Introduction: Paul Moore  posted a comment pointing out this great discussion of the correlation coefficient:
  
 Joseph Lee Rodgers and W. Alan Nicewander.  “Thirteen Ways to Look at the Correlation Coefficient.” The American Statistician, Vol. 42, No. 1. (Feb., 1988), pp. 59-66.   Link  
  
It’s related to the the post on  cosine similarity, correlation and OLS .  Anyway, I was just struck by the following diagram.  It almost has a pop-art feel.</p><p>3 0.11003376 <a title="182-tfidf-3" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-04-11-F-scores%2C_Dice%2C_and_Jaccard_set_similarity.html">183 brendan oconnor ai-2012-04-11-F-scores, Dice, and Jaccard set similarity</a></p>
<p>Introduction: The  Dice similarity  is the same as  F1-score ; and they are monotonic in  Jaccard similarity .  I worked this out recently but couldn’t find anything about it online so here’s a writeup.
 
Let \(A\) be the set of found items, and \(B\) the set of wanted items.  \(Prec=|AB|/|A|\), \(Rec=|AB|/|B|\).  Their harmonic mean, the \(F1\)-measure, is the same as the Dice coefficient: 
\begin{align*} 
F1(A,B) 
&= \frac{2}{1/P+ 1/R} 
 = \frac{2}{|A|/|AB| + |B|/|AB|} \\ 
Dice(A,B) 
&= \frac{2|AB|}{ |A| + |B| } \\ 
&= \frac{2 |AB|}{ (|AB| + |A \setminus B|) + (|AB| + |B \setminus A|)} \\ 
&= \frac{|AB|}{|AB| + \frac{1}{2}|A \setminus B| + \frac{1}{2} |B \setminus A|} 
\end{align*}
 
It’s nice to characterize the set comparison into the three mutually exclusive partitions \(AB\), \(A \setminus B\), and \(B \setminus A\).  This illustrates Dice’s close relationship to the Jaccard metric, 
\begin{align*} 
Jacc(A,B) 
&= \frac{|AB|}{|A \cup B|} \\ 
&= \frac{|AB|}{|AB| + |A \setminus B| + |B \setminus</p><p>4 0.07410109 <a title="182-tfidf-4" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-09-25-Information_theory_stuff.html">175 brendan oconnor ai-2011-09-25-Information theory stuff</a></p>
<p>Introduction: Actually this post is mainly to test the  MathJax  installation I put into WordPress via  this plugin .  But  information theory  is great, why not?
 
The probability of a symbol is \(p\).
 
It takes \(\log \frac{1}{p} = -\log p\) bits to encode one symbol — sometimes called its “surprisal”.  Surprisal is 0 for a 100% probable symbol, and ranges up to \(\infty\) for extremely low probability symbols.  This is because you use a coding scheme that encodes common symbols as very short strings, and less common symbols as longer ones.  (e.g.  Huffman  or  arithmetic  coding.)  We should say logarithms are base-2 so information is measured in bits.\(^*\)
 
If you have a stream of such symbols and a probability distribution \(\vec{p}\) for them, where a symbol \(i\) comes at probability \(p_i\), then the average message size is the expected surprisal:
 
\[ H(\vec{p}) = \sum_i p_i \log \frac{1}{p_i} \]
 
this is the Shannon  entropy  of the probability distribution \( \vec{p} \), which is a me</p><p>5 0.067734919 <a title="182-tfidf-5" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-11-20-science_writing_bad%21.html">28 brendan oconnor ai-2005-11-20-science writing bad!</a></p>
<p>Introduction: An two-step explanation for distrust of science: (1) journalists write up poor science or take out the evidence and information from a scientific study, then (2) people read that and criticize science for being unfounded, arbitrary, etc.   Link .  Some fun quotes:   
 Statistics are what causes the most fear for reporters, and so they are usually just edited out, with interesting consequences. Because science isn’t about something being true or not true: that’s a humanities graduate parody. It’s about the error bar, statistical significance, it’s about how reliable and valid the experiment was, it’s about coming to a verdict, about a hypothesis, on the back of lots of bits of evidence.  
 
 and   
 
  So how do the media work around their inability to deliver scientific evidence? They use authority figures, the very antithesis of what science is about, as if they were priests, or politicians, or parent figures. “Scientists today said … scientists revealed … scientists warned.” And if t</p><p>6 0.06528604 <a title="182-tfidf-6" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-11-13-Bayes_update_view_of_pointwise_mutual_information.html">178 brendan oconnor ai-2011-11-13-Bayes update view of pointwise mutual information</a></p>
<p>7 0.064136922 <a title="182-tfidf-7" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-08-31-Probabilistic_interpretation_of_the_B3_coreference_resolution_metric.html">199 brendan oconnor ai-2013-08-31-Probabilistic interpretation of the B3 coreference resolution metric</a></p>
<p>8 0.063136041 <a title="182-tfidf-8" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-05-09-Simpson%E2%80%99s_paradox_is_so_totally_solved.html">60 brendan oconnor ai-2007-05-09-Simpson’s paradox is so totally solved</a></p>
<p>9 0.057201084 <a title="182-tfidf-9" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-10-31-tanh_is_a_rescaled_logistic_sigmoid_function.html">201 brendan oconnor ai-2013-10-31-tanh is a rescaled logistic sigmoid function</a></p>
<p>10 0.050479773 <a title="182-tfidf-10" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-17-p-values%2C_CDF%E2%80%99s%2C_NLP_etc..html">185 brendan oconnor ai-2012-07-17-p-values, CDF’s, NLP etc.</a></p>
<p>11 0.047926474 <a title="182-tfidf-11" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-01-07-Perplexity_as_branching_factor%3B_as_Shannon_diversity_index.html">190 brendan oconnor ai-2013-01-07-Perplexity as branching factor; as Shannon diversity index</a></p>
<p>12 0.046385996 <a title="182-tfidf-12" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-10-11-It_is_accurate_to_determine_a_blog%E2%80%99s_bias_by_what_it_links_to.html">117 brendan oconnor ai-2008-10-11-It is accurate to determine a blog’s bias by what it links to</a></p>
<p>13 0.037592094 <a title="182-tfidf-13" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-11-11-Memorizing_small_tables.html">177 brendan oconnor ai-2011-11-11-Memorizing small tables</a></p>
<p>14 0.035245143 <a title="182-tfidf-14" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-08-20-Some_analysis_of_tweet_shares_and_%E2%80%9Cpredicting%E2%80%9D_election_outcomes.html">198 brendan oconnor ai-2013-08-20-Some analysis of tweet shares and “predicting” election outcomes</a></p>
<p>15 0.034102302 <a title="182-tfidf-15" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-04-06-a_regression_slope_is_a_weighted_average_of_pairs%E2%80%99_slopes%21.html">100 brendan oconnor ai-2008-04-06-a regression slope is a weighted average of pairs’ slopes!</a></p>
<p>16 0.031071046 <a title="182-tfidf-16" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-07-22-FFT%3A_Friedman_%2B_Fortran_%2B_Tricks.html">147 brendan oconnor ai-2009-07-22-FFT: Friedman + Fortran + Tricks</a></p>
<p>17 0.02764583 <a title="182-tfidf-17" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-03-Statistics_vs._Machine_Learning%2C_fight%21.html">129 brendan oconnor ai-2008-12-03-Statistics vs. Machine Learning, fight!</a></p>
<p>18 0.026282679 <a title="182-tfidf-18" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-11-21-Netflix_Prize.html">125 brendan oconnor ai-2008-11-21-Netflix Prize</a></p>
<p>19 0.025295142 <a title="182-tfidf-19" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-05-18-Announcing_TweetMotif_for_summarizing_twitter_topics.html">140 brendan oconnor ai-2009-05-18-Announcing TweetMotif for summarizing twitter topics</a></p>
<p>20 0.024538549 <a title="182-tfidf-20" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-01-11-Please_report_your_SVM%E2%80%99s_kernel%21.html">164 brendan oconnor ai-2011-01-11-Please report your SVM’s kernel!</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/brendan_oconnor_ai_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, -0.097), (1, -0.053), (2, 0.066), (3, -0.024), (4, -0.037), (5, 0.113), (6, -0.002), (7, -0.112), (8, -0.075), (9, -0.004), (10, -0.04), (11, -0.109), (12, 0.016), (13, 0.143), (14, -0.135), (15, 0.09), (16, -0.091), (17, -0.086), (18, 0.036), (19, -0.112), (20, -0.178), (21, 0.084), (22, 0.007), (23, 0.028), (24, -0.015), (25, -0.195), (26, -0.047), (27, 0.128), (28, -0.003), (29, -0.033), (30, -0.002), (31, -0.13), (32, -0.076), (33, -0.203), (34, 0.138), (35, -0.045), (36, 0.082), (37, 0.064), (38, 0.072), (39, -0.128), (40, -0.098), (41, 0.011), (42, 0.017), (43, 0.115), (44, 0.196), (45, 0.067), (46, -0.061), (47, -0.051), (48, -0.06), (49, 0.1)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99481702 <a title="182-lsi-1" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-03-13-Cosine_similarity%2C_Pearson_correlation%2C_and_OLS_coefficients.html">182 brendan oconnor ai-2012-03-13-Cosine similarity, Pearson correlation, and OLS coefficients</a></p>
<p>Introduction: Cosine similarity, Pearson correlations, and OLS coefficients can all be viewed as variants on the inner product — tweaked in different ways for centering and magnitude (i.e. location and scale, or something like that).
 
Details:
 
You have two vectors \(x\) and \(y\) and want to measure similarity between them.  A basic similarity function is the   inner product  
 
\[ Inner(x,y) = \sum_i x_i y_i = \langle x, y \rangle \]
 
If x tends to be high where y is also high, and low where y is low, the inner product will be high — the vectors are more similar.
 
The inner product is unbounded.  One way to make it bounded between -1 and 1 is to divide by the vectors’ L2 norms, giving the   cosine similarity  
 
\[ CosSim(x,y) = \frac{\sum_i x_i y_i}{ \sqrt{ \sum_i x_i^2} \sqrt{ \sum_i y_i^2 } } 
= \frac{ \langle x,y \rangle }{ ||x||\ ||y|| } 
\]
 
This is actually bounded between 0 and 1 if x and y are non-negative.  Cosine similarity has an interpretation as the cosine of the angle between t</p><p>2 0.77512151 <a title="182-lsi-2" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-03-18-Correlation_picture.html">193 brendan oconnor ai-2013-03-18-Correlation picture</a></p>
<p>Introduction: Paul Moore  posted a comment pointing out this great discussion of the correlation coefficient:
  
 Joseph Lee Rodgers and W. Alan Nicewander.  “Thirteen Ways to Look at the Correlation Coefficient.” The American Statistician, Vol. 42, No. 1. (Feb., 1988), pp. 59-66.   Link  
  
It’s related to the the post on  cosine similarity, correlation and OLS .  Anyway, I was just struck by the following diagram.  It almost has a pop-art feel.</p><p>3 0.77183825 <a title="182-lsi-3" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-04-11-F-scores%2C_Dice%2C_and_Jaccard_set_similarity.html">183 brendan oconnor ai-2012-04-11-F-scores, Dice, and Jaccard set similarity</a></p>
<p>Introduction: The  Dice similarity  is the same as  F1-score ; and they are monotonic in  Jaccard similarity .  I worked this out recently but couldn’t find anything about it online so here’s a writeup.
 
Let \(A\) be the set of found items, and \(B\) the set of wanted items.  \(Prec=|AB|/|A|\), \(Rec=|AB|/|B|\).  Their harmonic mean, the \(F1\)-measure, is the same as the Dice coefficient: 
\begin{align*} 
F1(A,B) 
&= \frac{2}{1/P+ 1/R} 
 = \frac{2}{|A|/|AB| + |B|/|AB|} \\ 
Dice(A,B) 
&= \frac{2|AB|}{ |A| + |B| } \\ 
&= \frac{2 |AB|}{ (|AB| + |A \setminus B|) + (|AB| + |B \setminus A|)} \\ 
&= \frac{|AB|}{|AB| + \frac{1}{2}|A \setminus B| + \frac{1}{2} |B \setminus A|} 
\end{align*}
 
It’s nice to characterize the set comparison into the three mutually exclusive partitions \(AB\), \(A \setminus B\), and \(B \setminus A\).  This illustrates Dice’s close relationship to the Jaccard metric, 
\begin{align*} 
Jacc(A,B) 
&= \frac{|AB|}{|A \cup B|} \\ 
&= \frac{|AB|}{|AB| + |A \setminus B| + |B \setminus</p><p>4 0.53309155 <a title="182-lsi-4" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-11-13-Bayes_update_view_of_pointwise_mutual_information.html">178 brendan oconnor ai-2011-11-13-Bayes update view of pointwise mutual information</a></p>
<p>Introduction: This is fun.  Pointwise Mutual Information (e.g.  Church and Hanks 1990 ) between two variable outcomes \(x\) and \(y\) is
 
\[ PMI(x,y) = \log \frac{p(x,y)}{p(x)p(y)} \]
 
It’s called “pointwise” because  Mutual Information , between two (discrete) variables X and Y, is the expectation of PMI over possible outcomes of X and Y: \( MI(X,Y) = \sum_{x,y} p(x,y) PMI(x,y) \).
 
One interpretation of PMI is it’s measuring how much deviation from independence there is — since \(p(x,y)=p(x)p(y)\) if X and Y were independent, so the ratio is how non-independent they (the outcomes) are.
 
You can get another interpretation of this quantity if you switch into conditional probabilities.  Looking just at the ratio, apply the definition of conditional probability:
 
\[ \frac{p(x,y)}{p(x)p(y)} = \frac{p(x|y)}{p(x)} \]
 
Think about doing a Bayes update for your belief about \(x\).  Start with the prior \(p(x)\), then learn \(y\) and you update to the posterior belief \(p(x|y)\).  How much your belief</p><p>5 0.37057281 <a title="182-lsi-5" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-10-11-It_is_accurate_to_determine_a_blog%E2%80%99s_bias_by_what_it_links_to.html">117 brendan oconnor ai-2008-10-11-It is accurate to determine a blog’s bias by what it links to</a></p>
<p>Introduction: Here’s a great project from Andy Baio and Joshua Schachter : they assessed the political biases of different blogs based on which articles they tend link to.  Using these political bias scores, they made a cool little Firefox extension that colors the names of different sources on the news aggregator site  Memeorandum , like so:
 
 
 
How they computed these biases is pretty neat.  Their data source was the Memeorandum site itself, which shows a particular news story, then a list of different news sites that have written articles about the topic.  Scraping out that data, Joshua constructed the adjacency matrix of sites vs. articles they linked to and ran good ol’  SVD  on it, an algorithm that can be used to summarize the very high-dimensional article linking information in just several numbers (“components” or “dimensions”) for each news site.  Basically, the algorithm groups together sites that tend to link to the same articles.  It’s not exactly clustering though; rather, it project</p><p>6 0.32998386 <a title="182-lsi-6" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-10-31-tanh_is_a_rescaled_logistic_sigmoid_function.html">201 brendan oconnor ai-2013-10-31-tanh is a rescaled logistic sigmoid function</a></p>
<p>7 0.31497857 <a title="182-lsi-7" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-08-31-Probabilistic_interpretation_of_the_B3_coreference_resolution_metric.html">199 brendan oconnor ai-2013-08-31-Probabilistic interpretation of the B3 coreference resolution metric</a></p>
<p>8 0.30509171 <a title="182-lsi-8" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-09-25-Information_theory_stuff.html">175 brendan oconnor ai-2011-09-25-Information theory stuff</a></p>
<p>9 0.26086232 <a title="182-lsi-9" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-11-20-science_writing_bad%21.html">28 brendan oconnor ai-2005-11-20-science writing bad!</a></p>
<p>10 0.24196418 <a title="182-lsi-10" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-04-21-What_inputs_do_Monte_Carlo_algorithms_need%3F.html">195 brendan oconnor ai-2013-04-21-What inputs do Monte Carlo algorithms need?</a></p>
<p>11 0.22949371 <a title="182-lsi-11" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-05-09-Simpson%E2%80%99s_paradox_is_so_totally_solved.html">60 brendan oconnor ai-2007-05-09-Simpson’s paradox is so totally solved</a></p>
<p>12 0.22357595 <a title="182-lsi-12" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-06-17-Pairwise_comparisons_for_relevance_evaluation.html">106 brendan oconnor ai-2008-06-17-Pairwise comparisons for relevance evaluation</a></p>
<p>13 0.19896831 <a title="182-lsi-13" href="../brendan_oconnor_ai-2010/brendan_oconnor_ai-2010-03-31-How_Facebook_privacy_failed_me.html">158 brendan oconnor ai-2010-03-31-How Facebook privacy failed me</a></p>
<p>14 0.18749946 <a title="182-lsi-14" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-17-p-values%2C_CDF%E2%80%99s%2C_NLP_etc..html">185 brendan oconnor ai-2012-07-17-p-values, CDF’s, NLP etc.</a></p>
<p>15 0.18386549 <a title="182-lsi-15" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-06-25-idea%3A_Morals_are_heuristics_for_socially_optimal_behavior.html">6 brendan oconnor ai-2005-06-25-idea: Morals are heuristics for socially optimal behavior</a></p>
<p>16 0.17214663 <a title="182-lsi-16" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-08-20-Some_analysis_of_tweet_shares_and_%E2%80%9Cpredicting%E2%80%9D_election_outcomes.html">198 brendan oconnor ai-2013-08-20-Some analysis of tweet shares and “predicting” election outcomes</a></p>
<p>17 0.16768387 <a title="182-lsi-17" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-07-11-guns%2C_germs%2C_%26_steel_pbs_show%3F%21.html">20 brendan oconnor ai-2005-07-11-guns, germs, & steel pbs show?!</a></p>
<p>18 0.16250637 <a title="182-lsi-18" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-03-02-Poor_man%E2%80%99s_linear_algebra_textbook.html">166 brendan oconnor ai-2011-03-02-Poor man’s linear algebra textbook</a></p>
<p>19 0.15914121 <a title="182-lsi-19" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-07-03-Supreme_Court_justices%E2%80%99_agreement_levels.html">13 brendan oconnor ai-2005-07-03-Supreme Court justices’ agreement levels</a></p>
<p>20 0.15629616 <a title="182-lsi-20" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-03-18-color_name_study_i_did.html">95 brendan oconnor ai-2008-03-18-color name study i did</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/brendan_oconnor_ai_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(3, 0.036), (14, 0.01), (24, 0.027), (32, 0.016), (44, 0.069), (48, 0.016), (55, 0.013), (57, 0.019), (74, 0.052), (80, 0.041), (86, 0.544)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9739809 <a title="182-lda-1" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-03-13-Cosine_similarity%2C_Pearson_correlation%2C_and_OLS_coefficients.html">182 brendan oconnor ai-2012-03-13-Cosine similarity, Pearson correlation, and OLS coefficients</a></p>
<p>Introduction: Cosine similarity, Pearson correlations, and OLS coefficients can all be viewed as variants on the inner product — tweaked in different ways for centering and magnitude (i.e. location and scale, or something like that).
 
Details:
 
You have two vectors \(x\) and \(y\) and want to measure similarity between them.  A basic similarity function is the   inner product  
 
\[ Inner(x,y) = \sum_i x_i y_i = \langle x, y \rangle \]
 
If x tends to be high where y is also high, and low where y is low, the inner product will be high — the vectors are more similar.
 
The inner product is unbounded.  One way to make it bounded between -1 and 1 is to divide by the vectors’ L2 norms, giving the   cosine similarity  
 
\[ CosSim(x,y) = \frac{\sum_i x_i y_i}{ \sqrt{ \sum_i x_i^2} \sqrt{ \sum_i y_i^2 } } 
= \frac{ \langle x,y \rangle }{ ||x||\ ||y|| } 
\]
 
This is actually bounded between 0 and 1 if x and y are non-negative.  Cosine similarity has an interpretation as the cosine of the angle between t</p><p>2 0.91596597 <a title="182-lda-2" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-06-14-How_much_text_versus_metadata_is_in_a_tweet%3F.html">171 brendan oconnor ai-2011-06-14-How much text versus metadata is in a tweet?</a></p>
<p>Introduction: This should have been a blog post, but I got lazy and wrote a plaintext document instead.
  
  Link  
  
For twitter, context matters: 90% of a tweet is metadata and 10% is text. Â That’s measured by (an approximation of) information content; by raw data size, it’s 95/5.</p><p>3 0.80897504 <a title="182-lda-3" href="../brendan_oconnor_ai-2010/brendan_oconnor_ai-2010-04-22-Updates%3A_CMU%2C_Facebook.html">160 brendan oconnor ai-2010-04-22-Updates: CMU, Facebook</a></p>
<p>Introduction: It’s been a good year.  Last fall I started a master’s program in the Language Technologies department at  CMU SCS , taking some great classes, hanging out with a  cool lab , and writing two new papers (for  ICWSM , involving Twitter:  polls  and  tweetmotif ; also did some  coref  work, financial text regression stuff, and looked at  social lexicography .)  I also applied to CS and stats PhD programs at several universities.  Next year I’ll be starting the PhD program in the  Machine Learning Department  here at CMU.
 
I’m excited!  Just the other day I was looking at videos on my old hard drive and found a presentation by  Tom Mitchell  on “the Discipline of Machine Learning” that I downloaded back in 2007 or so.  (Can’t find it online right now, but  this is similar .)  That might be where I heard of the department first.  Maybe some day I will be smarter than the guy who wrote  this rant  (though I am much more pro-stats and anti-ML these days…).
 
Also, I was recently named a fina</p><p>4 0.26288658 <a title="182-lda-4" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-08-21-ConnectU.com_SQL_injection_vulnerability%3A_a_story_of_pathetic_hubris_%28and_fun_with_the_password_%E2%80%98password%E2%80%99%29.html">76 brendan oconnor ai-2007-08-21-ConnectU.com SQL injection vulnerability: a story of pathetic hubris (and fun with the password ‘password’)</a></p>
<p>Introduction: This is off-topic for this blog but here goes.   ConnectU , a small college social networking site, has been in the news due to their  apparently weak lawsuit against Facebook , in which they claim  Mark Zuckerberg  stole their business plan and computer code back when they all were Harvard undergraduates.  (Judges involved have noted the case’s flimsy evidence; some technology commentators — as well as everyone I know — have noted that the business idea wasn’t all that brilliant or original in the first place.)  Zuckerberg, of course, went on to found Facebook and bring it to incredible success.
 
I tried to use the ConnectU site recently, but got an error when searching for a funny name with an apostrophe,  o’connor .  It turns out this was symptomatic of a very grave security flaw in their code, an  SQL injection vulnerability .  While Facebook recently had  a minor security-related glitch , ConnectU’s flaw is far more serious.  A malicious attacker could use this to easily break in</p><p>5 0.18860489 <a title="182-lda-5" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-01-07-Perplexity_as_branching_factor%3B_as_Shannon_diversity_index.html">190 brendan oconnor ai-2013-01-07-Perplexity as branching factor; as Shannon diversity index</a></p>
<p>Introduction: A language model’s  perplexity  is exponentiated negative average log-likelihood,
 
$$\exp( -\frac{1}{N} \log(p(x)))$$
 
Where the inner term usually decomposes into a sum over individual items; for example, as \(\sum_i \log p(x_i | x_1..x_{i-1})\) or \(\sum_i \log p(x_i)\) depending on independence assumptions, where for language modeling word tokens are usually taken as the individual units.  (In which case it is the geometric mean of per-token negative log-likelihoods.)   It’s equivalent to exponentiated cross-entropy between the model and the empirical data distribution, since \(-1/N \sum_i^N \log p(x_i) = -\sum_k^K \hat{p}_k \log p_k = H(\hat{p};p)\) where \(N\) is the number of items and \(K\) is the number of discrete classes (e.g. word types for language modeling) and \(\hat{p}_k\) is the proportion of data having class \(k\).
 
A nice interpretation of any exponentiated entropy measure is as branching factor: entropy measures uncertainty in bits or nats, but in exponentiated f</p><p>6 0.16247328 <a title="182-lda-6" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-09-25-Information_theory_stuff.html">175 brendan oconnor ai-2011-09-25-Information theory stuff</a></p>
<p>7 0.15738821 <a title="182-lda-7" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-08-31-Probabilistic_interpretation_of_the_B3_coreference_resolution_metric.html">199 brendan oconnor ai-2013-08-31-Probabilistic interpretation of the B3 coreference resolution metric</a></p>
<p>8 0.1570819 <a title="182-lda-8" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-03-Statistics_vs._Machine_Learning%2C_fight%21.html">129 brendan oconnor ai-2008-12-03-Statistics vs. Machine Learning, fight!</a></p>
<p>9 0.15495022 <a title="182-lda-9" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-08-20-Some_analysis_of_tweet_shares_and_%E2%80%9Cpredicting%E2%80%9D_election_outcomes.html">198 brendan oconnor ai-2013-08-20-Some analysis of tweet shares and “predicting” election outcomes</a></p>
<p>10 0.14902574 <a title="182-lda-10" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-08-08-Haghighi_and_Klein_%282009%29%3A_Simple_Coreference_Resolution_with_Rich_Syntactic_and_Semantic_Features.html">150 brendan oconnor ai-2009-08-08-Haghighi and Klein (2009): Simple Coreference Resolution with Rich Syntactic and Semantic Features</a></p>
<p>11 0.14873251 <a title="182-lda-11" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-27-Facebook_sentiment_mining_predicts_presidential_polls.html">131 brendan oconnor ai-2008-12-27-Facebook sentiment mining predicts presidential polls</a></p>
<p>12 0.14613502 <a title="182-lda-12" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-04-The_%2460%2C000_cat%3A_deep_belief_networks_make_less_sense_for_language_than_vision.html">184 brendan oconnor ai-2012-07-04-The $60,000 cat: deep belief networks make less sense for language than vision</a></p>
<p>13 0.14344782 <a title="182-lda-13" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-10-Don%E2%80%99t_MAWK_AWK_%E2%80%93_the_fastest_and_most_elegant_big_data_munging_language%21.html">154 brendan oconnor ai-2009-09-10-Don’t MAWK AWK – the fastest and most elegant big data munging language!</a></p>
<p>14 0.14281534 <a title="182-lda-14" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-04-16-Rise_and_fall_of_Dirichlet_process_clusters.html">194 brendan oconnor ai-2013-04-16-Rise and fall of Dirichlet process clusters</a></p>
<p>15 0.14016013 <a title="182-lda-15" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-17-p-values%2C_CDF%E2%80%99s%2C_NLP_etc..html">185 brendan oconnor ai-2012-07-17-p-values, CDF’s, NLP etc.</a></p>
<p>16 0.13995267 <a title="182-lda-16" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-02-23-Comparison_of_data_analysis_packages%3A_R%2C_Matlab%2C_SciPy%2C_Excel%2C_SAS%2C_SPSS%2C_Stata.html">135 brendan oconnor ai-2009-02-23-Comparison of data analysis packages: R, Matlab, SciPy, Excel, SAS, SPSS, Stata</a></p>
<p>17 0.13991664 <a title="182-lda-17" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-01-Binary_classification_evaluation_in_R_via_ROCR.html">136 brendan oconnor ai-2009-04-01-Binary classification evaluation in R via ROCR</a></p>
<p>18 0.13886617 <a title="182-lda-18" href="../brendan_oconnor_ai-2004/brendan_oconnor_ai-2004-11-24-addiction_%26_2_problems_of_economics.html">2 brendan oconnor ai-2004-11-24-addiction & 2 problems of economics</a></p>
<p>19 0.13847667 <a title="182-lda-19" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-07-04-City_crisis_simulation_%28e.g._terrorist_attack%29.html">14 brendan oconnor ai-2005-07-04-City crisis simulation (e.g. terrorist attack)</a></p>
<p>20 0.13827047 <a title="182-lda-20" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-22-Performance_comparison%3A_key-value_stores_for_language_model_counts.html">139 brendan oconnor ai-2009-04-22-Performance comparison: key-value stores for language model counts</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
