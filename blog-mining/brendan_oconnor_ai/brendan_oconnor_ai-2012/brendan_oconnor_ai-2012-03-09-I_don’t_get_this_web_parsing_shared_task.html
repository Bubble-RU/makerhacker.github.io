<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>181 brendan oconnor ai-2012-03-09-I don’t get this web parsing shared task</title>
</head>

<body>
<p><a title="brendan_oconnor_ai" href="../brendan_oconnor_ai_home.html">brendan_oconnor_ai</a> <a title="brendan_oconnor_ai-2012" href="../home/brendan_oconnor_ai-2012_home.html">brendan_oconnor_ai-2012</a> <a title="brendan_oconnor_ai-2012-181" href="#">brendan_oconnor_ai-2012-181</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>181 brendan oconnor ai-2012-03-09-I don’t get this web parsing shared task</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="brendan_oconnor_ai-2012-181-html" href="http://brenocon.com/blog/2012/03/i-dont-get-this-web-parsing-shared-task/">html</a></p><p>Introduction: The idea for a shared task on web parsing is really cool.  But I don’t get this one:
 
 Shared Task – SANCL 2012 (First Workshop on Syntactic Analysis of Non-Canonical Language) 
 
They’re explicitly banning
  
 Manually annotating in-domain (web) sentences
  Creating new word clusters, or anything, from as much text data as possible
   
… instead restricting participants to the data sets they release.
 
Isn’t a cycle of annotation, error analysis, and new annotations (a self-training + active-learning loop, with smarter decisions through error analysis) the hands-down best way to make an NLP tool for a new domain? Are people scared of this reality?  Am I off-base?
 
I am, of course, just advocating for our  Twitter POS tagger  approach, where we annotated some data, made a supervised tagger, and iterated on features.  The biggest weakness in that paper is we didn’t have additional iterations of error analysis.  Our lack of semi-supervised learning was  not  a weakness.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The idea for a shared task on web parsing is really cool. [sent-1, score-0.782]
</p><p>2 Isn’t a cycle of annotation, error analysis, and new annotations (a self-training + active-learning loop, with smarter decisions through error analysis) the hands-down best way to make an NLP tool for a new domain? [sent-3, score-1.513]
</p><p>3 I am, of course, just advocating for our  Twitter POS tagger  approach, where we annotated some data, made a supervised tagger, and iterated on features. [sent-6, score-0.751]
</p><p>4 The biggest weakness in that paper is we didn’t have additional iterations of error analysis. [sent-7, score-0.932]
</p><p>5 Our lack of semi-supervised learning was  not  a weakness. [sent-8, score-0.118]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('error', 0.289), ('weakness', 0.269), ('tagger', 0.269), ('shared', 0.237), ('task', 0.186), ('analysis', 0.174), ('web', 0.163), ('scared', 0.148), ('participants', 0.148), ('reality', 0.148), ('cycle', 0.148), ('restricting', 0.148), ('annotated', 0.148), ('iterations', 0.148), ('creating', 0.135), ('loop', 0.135), ('annotation', 0.135), ('explicitly', 0.135), ('supervised', 0.135), ('manually', 0.135), ('sentences', 0.135), ('smarter', 0.135), ('annotations', 0.135), ('new', 0.127), ('advocating', 0.126), ('pos', 0.126), ('workshop', 0.118), ('lack', 0.118), ('clusters', 0.118), ('syntactic', 0.118), ('additional', 0.113), ('biggest', 0.113), ('data', 0.11), ('sets', 0.108), ('domain', 0.108), ('tool', 0.103), ('parsing', 0.1), ('idea', 0.096), ('twitter', 0.096), ('decisions', 0.093), ('anything', 0.093), ('nlp', 0.093), ('instead', 0.079), ('possible', 0.078), ('text', 0.078), ('didn', 0.074), ('approach', 0.073), ('made', 0.073), ('word', 0.073), ('best', 0.067)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="181-tfidf-1" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-03-09-I_don%E2%80%99t_get_this_web_parsing_shared_task.html">181 brendan oconnor ai-2012-03-09-I don’t get this web parsing shared task</a></p>
<p>Introduction: The idea for a shared task on web parsing is really cool.  But I don’t get this one:
 
 Shared Task – SANCL 2012 (First Workshop on Syntactic Analysis of Non-Canonical Language) 
 
They’re explicitly banning
  
 Manually annotating in-domain (web) sentences
  Creating new word clusters, or anything, from as much text data as possible
   
… instead restricting participants to the data sets they release.
 
Isn’t a cycle of annotation, error analysis, and new annotations (a self-training + active-learning loop, with smarter decisions through error analysis) the hands-down best way to make an NLP tool for a new domain? Are people scared of this reality?  Am I off-base?
 
I am, of course, just advocating for our  Twitter POS tagger  approach, where we annotated some data, made a supervised tagger, and iterated on features.  The biggest weakness in that paper is we didn’t have additional iterations of error analysis.  Our lack of semi-supervised learning was  not  a weakness.</p><p>2 0.18679768 <a title="181-tfidf-2" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-08-27-CMU_Twitter_Part-of-Speech_tagger_0.2.html">173 brendan oconnor ai-2011-08-27-CMU Twitter Part-of-Speech tagger 0.2</a></p>
<p>Introduction: Announcement: We recently released a new version (0.2) of our  part-of-speech tagger for English Twitter messages , along with annotations and interface.  See the link for more details.</p><p>3 0.17001936 <a title="181-tfidf-3" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-09-21-CMU_ARK_Twitter_Part-of-Speech_Tagger_%E2%80%93_v0.3_released.html">187 brendan oconnor ai-2012-09-21-CMU ARK Twitter Part-of-Speech Tagger – v0.3 released</a></p>
<p>Introduction: We’re pleased to announce a new release of the CMU ARK Twitter Part-of-Speech 
Tagger, version 0.3.
  
 The new version is much faster (40x) and more accurate (89.2 -> 92.8) than 
  before.
  We also have released new POS-annotated data, including a dataset of one 
  tweet for each of 547 days.
  We have made available large-scale word clusters from unlabeled Twitter data 
  (217k words, 56m tweets, 847m tokens).
   
Tools, data, and a new technical report describing the release are available at: 
 www.ark.cs.cmu.edu/TweetNLP .
 
 0100100  a  1111100101110   111100000011 , Brendan</p><p>4 0.13581491 <a title="181-tfidf-4" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-03-06-data_data_data.html">93 brendan oconnor ai-2008-03-06-data data data</a></p>
<p>Introduction: This is a lot of data:
 
 Inductio Ex Machina â&euro;&ldquo; A Meta-index of Data Sets</p><p>5 0.13501731 <a title="181-tfidf-5" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-08-08-Haghighi_and_Klein_%282009%29%3A_Simple_Coreference_Resolution_with_Rich_Syntactic_and_Semantic_Features.html">150 brendan oconnor ai-2009-08-08-Haghighi and Klein (2009): Simple Coreference Resolution with Rich Syntactic and Semantic Features</a></p>
<p>Introduction: I haven’t done a paper review on this blog for a while, so here we go.
 
 Coreference  resolution is an interesting NLP problem.  ( Examples. )  It involves honest-to-goodness syntactic, semantic, and discourse phenomena, but still seems like a real cognitive task that humans have to solve when reading text [1].  I haven’t read the whole literature, but I’ve always been puzzled by the crop of papers on it I’ve seen in the last year or two.  There’s a big focus on fancy graph/probabilistic/constrained optimization algorithms, but often these papers gloss over the linguistic features — the core information they actually make their decisions with [2].  I never understood why the latter isn’t the most important issue.  Therefore, it was a joy to read
  
 Aria Haghighi and Dan Klein, EMNLP-2009.   “Simple Coreference Resolution with Rich Syntactic and Semantic Features.”  
  
They describe a simple, essentially non-statistical system that outperforms previous unsupervised systems, and compa</p><p>6 0.1277729 <a title="181-tfidf-6" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-09-19-End-to-end_NLP_packages.html">174 brendan oconnor ai-2011-09-19-End-to-end NLP packages</a></p>
<p>7 0.11337572 <a title="181-tfidf-7" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-11-24-Graphs_for_SANCL-2012_web_parsing_results.html">189 brendan oconnor ai-2012-11-24-Graphs for SANCL-2012 web parsing results</a></p>
<p>8 0.088902064 <a title="181-tfidf-8" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-04-16-Rise_and_fall_of_Dirichlet_process_clusters.html">194 brendan oconnor ai-2013-04-16-Rise and fall of Dirichlet process clusters</a></p>
<p>9 0.084077492 <a title="181-tfidf-9" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-17-p-values%2C_CDF%E2%80%99s%2C_NLP_etc..html">185 brendan oconnor ai-2012-07-17-p-values, CDF’s, NLP etc.</a></p>
<p>10 0.080732465 <a title="181-tfidf-10" href="../brendan_oconnor_ai-2010/brendan_oconnor_ai-2010-04-14-quick_note%3A_cer_et_al_2010.html">159 brendan oconnor ai-2010-04-14-quick note: cer et al 2010</a></p>
<p>11 0.079723924 <a title="181-tfidf-11" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-02-02-Histograms_%E2%80%94_matplotlib_vs._R.html">179 brendan oconnor ai-2012-02-02-Histograms — matplotlib vs. R</a></p>
<p>12 0.075395107 <a title="181-tfidf-12" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-01-23-SF_conference_for_data_mining_mercenaries.html">133 brendan oconnor ai-2009-01-23-SF conference for data mining mercenaries</a></p>
<p>13 0.070960857 <a title="181-tfidf-13" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-08-20-Some_analysis_of_tweet_shares_and_%E2%80%9Cpredicting%E2%80%9D_election_outcomes.html">198 brendan oconnor ai-2013-08-20-Some analysis of tweet shares and “predicting” election outcomes</a></p>
<p>14 0.070535243 <a title="181-tfidf-14" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-10-Don%E2%80%99t_MAWK_AWK_%E2%80%93_the_fastest_and_most_elegant_big_data_munging_language%21.html">154 brendan oconnor ai-2009-09-10-Don’t MAWK AWK – the fastest and most elegant big data munging language!</a></p>
<p>15 0.070148475 <a title="181-tfidf-15" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-06-17-Pairwise_comparisons_for_relevance_evaluation.html">106 brendan oconnor ai-2008-06-17-Pairwise comparisons for relevance evaluation</a></p>
<p>16 0.069699571 <a title="181-tfidf-16" href="../brendan_oconnor_ai-2006/brendan_oconnor_ai-2006-03-26-new_kind_of_science%2C_for_real.html">32 brendan oconnor ai-2006-03-26-new kind of science, for real</a></p>
<p>17 0.06787128 <a title="181-tfidf-17" href="../brendan_oconnor_ai-2014/brendan_oconnor_ai-2014-02-19-What_the_ACL-2014_review_scores_mean.html">203 brendan oconnor ai-2014-02-19-What the ACL-2014 review scores mean</a></p>
<p>18 0.066815287 <a title="181-tfidf-18" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-17-1_billion_web_page_dataset_from_CMU.html">138 brendan oconnor ai-2009-04-17-1 billion web page dataset from CMU</a></p>
<p>19 0.059524253 <a title="181-tfidf-19" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-10-17-Twitter_graphs_of_the_debate.html">121 brendan oconnor ai-2008-10-17-Twitter graphs of the debate</a></p>
<p>20 0.057917975 <a title="181-tfidf-20" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-04-06-a_regression_slope_is_a_weighted_average_of_pairs%E2%80%99_slopes%21.html">100 brendan oconnor ai-2008-04-06-a regression slope is a weighted average of pairs’ slopes!</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/brendan_oconnor_ai_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, -0.203), (1, -0.253), (2, 0.003), (3, 0.111), (4, 0.099), (5, -0.152), (6, -0.164), (7, -0.083), (8, -0.039), (9, 0.05), (10, 0.106), (11, 0.113), (12, 0.01), (13, 0.043), (14, 0.176), (15, 0.071), (16, -0.012), (17, -0.01), (18, 0.019), (19, -0.024), (20, 0.132), (21, 0.054), (22, -0.081), (23, 0.009), (24, 0.047), (25, 0.084), (26, -0.031), (27, 0.104), (28, -0.045), (29, -0.027), (30, 0.006), (31, 0.038), (32, -0.071), (33, -0.004), (34, -0.013), (35, -0.059), (36, 0.102), (37, -0.053), (38, -0.019), (39, -0.054), (40, 0.08), (41, 0.017), (42, -0.012), (43, 0.025), (44, 0.023), (45, 0.123), (46, 0.04), (47, -0.064), (48, -0.059), (49, 0.032)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98458087 <a title="181-lsi-1" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-03-09-I_don%E2%80%99t_get_this_web_parsing_shared_task.html">181 brendan oconnor ai-2012-03-09-I don’t get this web parsing shared task</a></p>
<p>Introduction: The idea for a shared task on web parsing is really cool.  But I don’t get this one:
 
 Shared Task – SANCL 2012 (First Workshop on Syntactic Analysis of Non-Canonical Language) 
 
They’re explicitly banning
  
 Manually annotating in-domain (web) sentences
  Creating new word clusters, or anything, from as much text data as possible
   
… instead restricting participants to the data sets they release.
 
Isn’t a cycle of annotation, error analysis, and new annotations (a self-training + active-learning loop, with smarter decisions through error analysis) the hands-down best way to make an NLP tool for a new domain? Are people scared of this reality?  Am I off-base?
 
I am, of course, just advocating for our  Twitter POS tagger  approach, where we annotated some data, made a supervised tagger, and iterated on features.  The biggest weakness in that paper is we didn’t have additional iterations of error analysis.  Our lack of semi-supervised learning was  not  a weakness.</p><p>2 0.74167919 <a title="181-lsi-2" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-09-21-CMU_ARK_Twitter_Part-of-Speech_Tagger_%E2%80%93_v0.3_released.html">187 brendan oconnor ai-2012-09-21-CMU ARK Twitter Part-of-Speech Tagger – v0.3 released</a></p>
<p>Introduction: We’re pleased to announce a new release of the CMU ARK Twitter Part-of-Speech 
Tagger, version 0.3.
  
 The new version is much faster (40x) and more accurate (89.2 -> 92.8) than 
  before.
  We also have released new POS-annotated data, including a dataset of one 
  tweet for each of 547 days.
  We have made available large-scale word clusters from unlabeled Twitter data 
  (217k words, 56m tweets, 847m tokens).
   
Tools, data, and a new technical report describing the release are available at: 
 www.ark.cs.cmu.edu/TweetNLP .
 
 0100100  a  1111100101110   111100000011 , Brendan</p><p>3 0.62831414 <a title="181-lsi-3" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-08-27-CMU_Twitter_Part-of-Speech_tagger_0.2.html">173 brendan oconnor ai-2011-08-27-CMU Twitter Part-of-Speech tagger 0.2</a></p>
<p>Introduction: Announcement: We recently released a new version (0.2) of our  part-of-speech tagger for English Twitter messages , along with annotations and interface.  See the link for more details.</p><p>4 0.56312627 <a title="181-lsi-4" href="../brendan_oconnor_ai-2010/brendan_oconnor_ai-2010-04-14-quick_note%3A_cer_et_al_2010.html">159 brendan oconnor ai-2010-04-14-quick note: cer et al 2010</a></p>
<p>Introduction: Quick note, reading this  paper  from  their tweet .
 
 update  this reaction might be totally wrong; in particular, the conll dependencies for at least some languages were done completely by hand.
   
Malt and MSTParser were designed for the Yamada and Matsumodo dependencies formalism (the one used for the CoNLL dependency parsing shared task, from the   penn2malt   tool).  Their feature sets and probably many other design decisions were created to support that.  If you compare their outputs side-by-side, you will see that the Stanford Dependencies are a substantially different formalism; for example, compound verbs are handled very differently (the paper talks about copula example). 
I think the following conclusion is premature:
  
Notwithstanding the very large amount of research that has gone into dependency 
parsing algorithms in the last ďŹ ve years, our central conclusion is that the quality of the Charniak, Charniak-Johnson reranking, and Berkeley parsers is so high that in th</p><p>5 0.5486989 <a title="181-lsi-5" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-10-Don%E2%80%99t_MAWK_AWK_%E2%80%93_the_fastest_and_most_elegant_big_data_munging_language%21.html">154 brendan oconnor ai-2009-09-10-Don’t MAWK AWK – the fastest and most elegant big data munging language!</a></p>
<p>Introduction: update 2012-10-25 : I’ve been informed there is a new maintainer for Mawk, who has probably fixed the bugs I’ve been seeing.
  

From: Gert Hulselmans   


[The bugs you have found are] indeed true with mawk v1.3.3 which comes standard with Debian/Ubuntu.  This version is almost not developed the last 10 years.


I now already use mawk v1.3.4 maintained by another developer (Thomas E. Dickey) 
for more than a year on huge datafiles (sometimes several GB).


The problems/wrong results I had with mawk v1.3.3 sometimes are gone. In his version, normally all open/known bugs are fixed.


This version can be downloaded from:  http://invisible-island.net/mawk/ 

     update 2010-04-30  : I have since found large datasets where mawk is buggy and gives the wrong result.  nawk seems safe.   When one of these newfangled   “Big Data”   sets comes your way, the very first thing you have to do is data munging: shuffling around file formats, renaming fields and the like.  Once you’re dealing with hun</p><p>6 0.50223941 <a title="181-lsi-6" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-09-19-End-to-end_NLP_packages.html">174 brendan oconnor ai-2011-09-19-End-to-end NLP packages</a></p>
<p>7 0.49900538 <a title="181-lsi-7" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-03-06-data_data_data.html">93 brendan oconnor ai-2008-03-06-data data data</a></p>
<p>8 0.49103591 <a title="181-lsi-8" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-11-24-Graphs_for_SANCL-2012_web_parsing_results.html">189 brendan oconnor ai-2012-11-24-Graphs for SANCL-2012 web parsing results</a></p>
<p>9 0.48602387 <a title="181-lsi-9" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-04-16-Rise_and_fall_of_Dirichlet_process_clusters.html">194 brendan oconnor ai-2013-04-16-Rise and fall of Dirichlet process clusters</a></p>
<p>10 0.4575237 <a title="181-lsi-10" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-08-20-Some_analysis_of_tweet_shares_and_%E2%80%9Cpredicting%E2%80%9D_election_outcomes.html">198 brendan oconnor ai-2013-08-20-Some analysis of tweet shares and “predicting” election outcomes</a></p>
<p>11 0.4250904 <a title="181-lsi-11" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-22-Performance_comparison%3A_key-value_stores_for_language_model_counts.html">139 brendan oconnor ai-2009-04-22-Performance comparison: key-value stores for language model counts</a></p>
<p>12 0.41295972 <a title="181-lsi-12" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-08-08-Haghighi_and_Klein_%282009%29%3A_Simple_Coreference_Resolution_with_Rich_Syntactic_and_Semantic_Features.html">150 brendan oconnor ai-2009-08-08-Haghighi and Klein (2009): Simple Coreference Resolution with Rich Syntactic and Semantic Features</a></p>
<p>13 0.34448716 <a title="181-lsi-13" href="../brendan_oconnor_ai-2014/brendan_oconnor_ai-2014-02-19-What_the_ACL-2014_review_scores_mean.html">203 brendan oconnor ai-2014-02-19-What the ACL-2014 review scores mean</a></p>
<p>14 0.34207493 <a title="181-lsi-14" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-03-29-Allende%E2%80%99s_cybernetic_economy_project.html">98 brendan oconnor ai-2008-03-29-Allende’s cybernetic economy project</a></p>
<p>15 0.31725407 <a title="181-lsi-15" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-01-23-SF_conference_for_data_mining_mercenaries.html">133 brendan oconnor ai-2009-01-23-SF conference for data mining mercenaries</a></p>
<p>16 0.31496236 <a title="181-lsi-16" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-02-02-Histograms_%E2%80%94_matplotlib_vs._R.html">179 brendan oconnor ai-2012-02-02-Histograms — matplotlib vs. R</a></p>
<p>17 0.31318089 <a title="181-lsi-17" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-06-14-How_much_text_versus_metadata_is_in_a_tweet%3F.html">171 brendan oconnor ai-2011-06-14-How much text versus metadata is in a tweet?</a></p>
<p>18 0.31272504 <a title="181-lsi-18" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-17-1_billion_web_page_dataset_from_CMU.html">138 brendan oconnor ai-2009-04-17-1 billion web page dataset from CMU</a></p>
<p>19 0.29556742 <a title="181-lsi-19" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-17-p-values%2C_CDF%E2%80%99s%2C_NLP_etc..html">185 brendan oconnor ai-2012-07-17-p-values, CDF’s, NLP etc.</a></p>
<p>20 0.29058129 <a title="181-lsi-20" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-04-The_%2460%2C000_cat%3A_deep_belief_networks_make_less_sense_for_language_than_vision.html">184 brendan oconnor ai-2012-07-04-The $60,000 cat: deep belief networks make less sense for language than vision</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/brendan_oconnor_ai_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(44, 0.852), (74, 0.028)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.99999881 <a title="181-lda-1" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-10-08-Blog_move_has_landed.html">115 brendan oconnor ai-2008-10-08-Blog move has landed</a></p>
<p>Introduction: We’re now live at a new location:  anyall.org/blog .  Good-bye, Blogger, it was sometimes nice knowing you.
 
This blog is now on WordPress (perhaps  behind the times ), which I’ve usually had good experiences with, e.g. for the  Dolores Labs Blog .  I also made the blog’s name more boring — the old one, “Social Science++”, was just too long and difficult to remember relative to how descriptive it was, and my interests have changed a little bit in any case.
 
All the old posts have been imported, and I  set up redirects  for all posts.  The RSS feed can’t be redirected though.
 
(One small issue: comment authors’ urls and emails failed to get imported.  I can fix it if I am given the info; if you want your old comments fixed, drop me a line.)</p><p>2 0.99946499 <a title="181-lda-2" href="../brendan_oconnor_ai-2006/brendan_oconnor_ai-2006-03-18-Mark_Turner%3A_Toward_the_Founding_of_Cognitive_Social_Science.html">31 brendan oconnor ai-2006-03-18-Mark Turner: Toward the Founding of Cognitive Social Science</a></p>
<p>Introduction: Where is social science? Where should it go? How should it get there? My answer, in a nutshell, is that social science is headed for an alliance with cognitive science.
  
 Mark Turner, 2001, Chronicle of Higher Education</p><p>3 0.99946499 <a title="181-lda-3" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-10-13-Verificationism_dinosaur_comics.html">79 brendan oconnor ai-2007-10-13-Verificationism dinosaur comics</a></p>
<p>Introduction: I love  dinosaur comics .  What a quicker way to learn philosophy than reading all those books.
 
For example:
 
   
 
Now I have an opinion on verificationism!  I like the yellow dinosaur better.  Scientific empiricism is great, dithering about strict notions of meaning, not so great.  See, isnâ&euro;&trade;t epistemology easy?</p><p>same-blog 4 0.99934393 <a title="181-lda-4" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-03-09-I_don%E2%80%99t_get_this_web_parsing_shared_task.html">181 brendan oconnor ai-2012-03-09-I don’t get this web parsing shared task</a></p>
<p>Introduction: The idea for a shared task on web parsing is really cool.  But I don’t get this one:
 
 Shared Task – SANCL 2012 (First Workshop on Syntactic Analysis of Non-Canonical Language) 
 
They’re explicitly banning
  
 Manually annotating in-domain (web) sentences
  Creating new word clusters, or anything, from as much text data as possible
   
… instead restricting participants to the data sets they release.
 
Isn’t a cycle of annotation, error analysis, and new annotations (a self-training + active-learning loop, with smarter decisions through error analysis) the hands-down best way to make an NLP tool for a new domain? Are people scared of this reality?  Am I off-base?
 
I am, of course, just advocating for our  Twitter POS tagger  approach, where we annotated some data, made a supervised tagger, and iterated on features.  The biggest weakness in that paper is we didn’t have additional iterations of error analysis.  Our lack of semi-supervised learning was  not  a weakness.</p><p>5 0.9597863 <a title="181-lda-5" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-27-Facebook_sentiment_mining_predicts_presidential_polls.html">131 brendan oconnor ai-2008-12-27-Facebook sentiment mining predicts presidential polls</a></p>
<p>Introduction: I’m a bit late blogging this, but here’s a messy, exciting — and statistically validated! — new online data source.
 
My friend  Roddy  at Facebook  wrote a post describing their sentiment analysis system , which can evaluate positive or negative sentiment toward a particular topic by looking at a large number of wall messages.  (I’d link to it, but I can’t find the URL anymore — here’s the  Lexicon , but that version only gets term frequencies but no sentiment.)
 
How they constructed their sentiment detector is interesting.  Starting with a list of positive and negative terms, they had a lexical acquisition step to gather many more candidate synonyms and misspellings — a necessity in this social media domain, where  WordNet  ain’t gonna come close!  After manually filtering these candidates, they assess the sentiment toward a mention of a topic by looking for instances of these positive and negative words nearby, along with “negation heuristics” and a few other features.
 
He describ</p><p>6 0.93842816 <a title="181-lda-6" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-08-20-Some_analysis_of_tweet_shares_and_%E2%80%9Cpredicting%E2%80%9D_election_outcomes.html">198 brendan oconnor ai-2013-08-20-Some analysis of tweet shares and “predicting” election outcomes</a></p>
<p>7 0.77709204 <a title="181-lda-7" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-09-21-CMU_ARK_Twitter_Part-of-Speech_Tagger_%E2%80%93_v0.3_released.html">187 brendan oconnor ai-2012-09-21-CMU ARK Twitter Part-of-Speech Tagger – v0.3 released</a></p>
<p>8 0.71074581 <a title="181-lda-8" href="../brendan_oconnor_ai-2006/brendan_oconnor_ai-2006-03-26-new_kind_of_science%2C_for_real.html">32 brendan oconnor ai-2006-03-26-new kind of science, for real</a></p>
<p>9 0.68535227 <a title="181-lda-9" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-04-The_%2460%2C000_cat%3A_deep_belief_networks_make_less_sense_for_language_than_vision.html">184 brendan oconnor ai-2012-07-04-The $60,000 cat: deep belief networks make less sense for language than vision</a></p>
<p>10 0.68120223 <a title="181-lda-10" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-10-Don%E2%80%99t_MAWK_AWK_%E2%80%93_the_fastest_and_most_elegant_big_data_munging_language%21.html">154 brendan oconnor ai-2009-09-10-Don’t MAWK AWK – the fastest and most elegant big data munging language!</a></p>
<p>11 0.66959018 <a title="181-lda-11" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-06-18-Turker_classifiers_and_binary_classification_threshold_calibration.html">107 brendan oconnor ai-2008-06-18-Turker classifiers and binary classification threshold calibration</a></p>
<p>12 0.66064286 <a title="181-lda-12" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-08-08-Haghighi_and_Klein_%282009%29%3A_Simple_Coreference_Resolution_with_Rich_Syntactic_and_Semantic_Features.html">150 brendan oconnor ai-2009-08-08-Haghighi and Klein (2009): Simple Coreference Resolution with Rich Syntactic and Semantic Features</a></p>
<p>13 0.64940566 <a title="181-lda-13" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-03-Statistics_vs._Machine_Learning%2C_fight%21.html">129 brendan oconnor ai-2008-12-03-Statistics vs. Machine Learning, fight!</a></p>
<p>14 0.63968927 <a title="181-lda-14" href="../brendan_oconnor_ai-2004/brendan_oconnor_ai-2004-11-24-addiction_%26_2_problems_of_economics.html">2 brendan oconnor ai-2004-11-24-addiction & 2 problems of economics</a></p>
<p>15 0.62738484 <a title="181-lda-15" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-11-24-Graphs_for_SANCL-2012_web_parsing_results.html">189 brendan oconnor ai-2012-11-24-Graphs for SANCL-2012 web parsing results</a></p>
<p>16 0.61852407 <a title="181-lda-16" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-03-20-Spending_money_on_others_makes_you_happy.html">96 brendan oconnor ai-2008-03-20-Spending money on others makes you happy</a></p>
<p>17 0.61781633 <a title="181-lda-17" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-01-Binary_classification_evaluation_in_R_via_ROCR.html">136 brendan oconnor ai-2009-04-01-Binary classification evaluation in R via ROCR</a></p>
<p>18 0.61310089 <a title="181-lda-18" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-03-24-Quick-R%2C_the_only_decent_R_documentation_on_the_internet.html">97 brendan oconnor ai-2008-03-24-Quick-R, the only decent R documentation on the internet</a></p>
<p>19 0.57982445 <a title="181-lda-19" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-17-p-values%2C_CDF%E2%80%99s%2C_NLP_etc..html">185 brendan oconnor ai-2012-07-17-p-values, CDF’s, NLP etc.</a></p>
<p>20 0.55812526 <a title="181-lda-20" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-07-11-guns%2C_germs%2C_%26_steel_pbs_show%3F%21.html">20 brendan oconnor ai-2005-07-11-guns, germs, & steel pbs show?!</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
