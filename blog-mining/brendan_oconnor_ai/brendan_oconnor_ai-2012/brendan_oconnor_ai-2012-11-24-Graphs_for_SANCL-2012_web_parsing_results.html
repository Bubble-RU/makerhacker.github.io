<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>189 brendan oconnor ai-2012-11-24-Graphs for SANCL-2012 web parsing results</title>
</head>

<body>
<p><a title="brendan_oconnor_ai" href="../brendan_oconnor_ai_home.html">brendan_oconnor_ai</a> <a title="brendan_oconnor_ai-2012" href="../home/brendan_oconnor_ai-2012_home.html">brendan_oconnor_ai-2012</a> <a title="brendan_oconnor_ai-2012-189" href="#">brendan_oconnor_ai-2012-189</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>189 brendan oconnor ai-2012-11-24-Graphs for SANCL-2012 web parsing results</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="brendan_oconnor_ai-2012-189-html" href="http://brenocon.com/blog/2012/11/graphs-for-sancl-2012-web-parsing-results/">html</a></p><p>Introduction: I was just looking at some papers from the  SANCL-2012 workshop on web parsing  from June this year, which are very interesting to those of us who wish we had good parsers for non-newspaper text.  The shared task focus was on domain adaptation from a setting of lots of Wall Street Journal annotated data and very little in-domain training data.  (Previous discussion  here ; see Ryan McDonald’s detailed comment.)   Here are some graphs of the results ( last page in the Petrov & McDonald overview ).
 
I was most interested in whether parsing accuracy on the WSJ correlates to accuracy on web text.  Fortunately, it does.  They evaluated all systems on four evaluation sets: (1) Text from a question/answer site, (2) newsgroups, (3) reviews, and (4) Wall Street Journal PTB.  Here is a graph across system entries, with the x-axis being the labeled dependency parsing accuracy on WSJPTB, and the y-axis the average accuracy on the three web evaluation sets.  Note the axis scales are different: web</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 I was just looking at some papers from the  SANCL-2012 workshop on web parsing  from June this year, which are very interesting to those of us who wish we had good parsers for non-newspaper text. [sent-1, score-0.901]
</p><p>2 )   Here are some graphs of the results ( last page in the Petrov & McDonald overview ). [sent-4, score-0.162]
</p><p>3 I was most interested in whether parsing accuracy on the WSJ correlates to accuracy on web text. [sent-5, score-1.05]
</p><p>4 They evaluated all systems on four evaluation sets: (1) Text from a question/answer site, (2) newsgroups, (3) reviews, and (4) Wall Street Journal PTB. [sent-7, score-0.34]
</p><p>5 Here is a graph across system entries, with the x-axis being the labeled dependency parsing accuracy on WSJPTB, and the y-axis the average accuracy on the three web evaluation sets. [sent-8, score-1.531]
</p><p>6 Note the axis scales are different: web accuracies are  much  worse than WSJ. [sent-9, score-0.295]
</p><p>7 There are two types of systems here: direct dependency parsers, and constituent parsers whose output is converted to dependencies then evaluated. [sent-10, score-1.16]
</p><p>8 There’s an interesting argument in the overview paper that the latter type seems to perform better out-of-domain, perhaps because they learn more about latent characteristics of English grammatical structure that transfer between domains. [sent-11, score-0.21]
</p><p>9 To check this, I grouped on this factor: blue triangles are constituents-then-convert parsers, and the red circles are direct dependency parsers. [sent-12, score-0.366]
</p><p>10 )  The relationship between WSJ versus web accuracy can be checked with linear regression on the subgroups. [sent-14, score-0.722]
</p><p>11 The constituent parsers are higher up on the graph; so indeed, the constituent parsers have higher web accuracy relative to their WSJ accuracy compared to the direct dependency parsers. [sent-15, score-2.689]
</p><p>12 A difference in the slopes might also be interesting, but the difference between them is mostly driven by a single direct-dependency outlier (top left, “DCU-Paris13″); excluding that, the slopes are quite similar. [sent-16, score-0.727]
</p><p>13 Since the slopes are similar, we shouldn’t need varying-slopes hierarchical regression to analyze the differences, so just throw it all in to one regression ( webacc ~ wsjacc + ConstitIndicator ); so constituent parsers get an absolute 1. [sent-19, score-1.072]
</p><p>14 6% better out-of-domain accuracy compared to dependency parsers with the same WSJ parsing accuracy. [sent-20, score-1.196]
</p><p>15 )  It’s not clear if this is due to better grammar learning, or if it’s due to an issue with SD giving bad conversions on web text. [sent-22, score-0.372]
</p><p>16 To see the individual systems’ names, here are both sets of numbers. [sent-23, score-0.126]
</p><p>17 These include all systems; for the scatterplot above I excluded the ones that performed worse than the non-domain-adapted baselines. [sent-24, score-0.224]
</p><p>18 (Looking at those papers, they seemed to be less elaborately engineering-heavy efforts; for example, the very interesting  Pitler paper  focuses on issues in how the representation handles conjunctions. [sent-25, score-0.126]
</p><p>19 assuming correctness is independent at the token level — though probably it should have sentence-level or doc-level grouping, of course, so they’re tighter than they should be. [sent-28, score-0.179]
</p><p>20 )   Finally, this is less useful, but here’s the scatterplot matrix of all four evaluation sets (excluding worse-than-baseline systems again). [sent-29, score-0.559]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('parsers', 0.392), ('accuracy', 0.336), ('dependency', 0.245), ('wsj', 0.231), ('web', 0.222), ('constituent', 0.211), ('slopes', 0.199), ('excluding', 0.173), ('parsing', 0.156), ('systems', 0.133), ('sets', 0.126), ('direct', 0.121), ('evaluation', 0.109), ('regression', 0.106), ('street', 0.105), ('mcdonald', 0.105), ('wall', 0.098), ('four', 0.098), ('scatterplot', 0.093), ('journal', 0.088), ('shouldn', 0.088), ('overview', 0.084), ('difference', 0.078), ('higher', 0.078), ('graphs', 0.078), ('due', 0.075), ('worse', 0.073), ('graph', 0.069), ('interesting', 0.068), ('compared', 0.067), ('papers', 0.063), ('probably', 0.063), ('newsgroups', 0.058), ('token', 0.058), ('boxes', 0.058), ('converted', 0.058), ('absolute', 0.058), ('assuming', 0.058), ('grouping', 0.058), ('labeled', 0.058), ('performed', 0.058), ('approximations', 0.058), ('binomial', 0.058), ('entries', 0.058), ('transfer', 0.058), ('checked', 0.058), ('annotated', 0.058), ('representation', 0.058), ('sd', 0.058), ('ci', 0.058)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="189-tfidf-1" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-11-24-Graphs_for_SANCL-2012_web_parsing_results.html">189 brendan oconnor ai-2012-11-24-Graphs for SANCL-2012 web parsing results</a></p>
<p>Introduction: I was just looking at some papers from the  SANCL-2012 workshop on web parsing  from June this year, which are very interesting to those of us who wish we had good parsers for non-newspaper text.  The shared task focus was on domain adaptation from a setting of lots of Wall Street Journal annotated data and very little in-domain training data.  (Previous discussion  here ; see Ryan McDonald’s detailed comment.)   Here are some graphs of the results ( last page in the Petrov & McDonald overview ).
 
I was most interested in whether parsing accuracy on the WSJ correlates to accuracy on web text.  Fortunately, it does.  They evaluated all systems on four evaluation sets: (1) Text from a question/answer site, (2) newsgroups, (3) reviews, and (4) Wall Street Journal PTB.  Here is a graph across system entries, with the x-axis being the labeled dependency parsing accuracy on WSJPTB, and the y-axis the average accuracy on the three web evaluation sets.  Note the axis scales are different: web</p><p>2 0.20469272 <a title="189-tfidf-2" href="../brendan_oconnor_ai-2010/brendan_oconnor_ai-2010-04-14-quick_note%3A_cer_et_al_2010.html">159 brendan oconnor ai-2010-04-14-quick note: cer et al 2010</a></p>
<p>Introduction: Quick note, reading this  paper  from  their tweet .
 
 update  this reaction might be totally wrong; in particular, the conll dependencies for at least some languages were done completely by hand.
   
Malt and MSTParser were designed for the Yamada and Matsumodo dependencies formalism (the one used for the CoNLL dependency parsing shared task, from the   penn2malt   tool).  Their feature sets and probably many other design decisions were created to support that.  If you compare their outputs side-by-side, you will see that the Stanford Dependencies are a substantially different formalism; for example, compound verbs are handled very differently (the paper talks about copula example). 
I think the following conclusion is premature:
  
Notwithstanding the very large amount of research that has gone into dependency 
parsing algorithms in the last ďŹ ve years, our central conclusion is that the quality of the Charniak, Charniak-Johnson reranking, and Berkeley parsers is so high that in th</p><p>3 0.1333086 <a title="189-tfidf-3" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-09-19-End-to-end_NLP_packages.html">174 brendan oconnor ai-2011-09-19-End-to-end NLP packages</a></p>
<p>Introduction: What freely available end-to-end natural language processing (NLP) systems are out there, that start with raw text, and output parses and semantic structures?  Lots of NLP research focuses on single tasks at a time, and thus produces software that does a single task at a time.  But for various applications, it is nicer to have a full end-to-end system that just runs on whatever text you give it.
 
If you believe this is a worthwhile goal (see caveat at bottom), I will postulate there aren’t a ton of such end-to-end, multilevel systems.  Here are ones I can think of.  Corrections and clarifications welcome.
  
  Stanford CoreNLP .  Raw text to  rich syntactic dependencies  ( LFG -inspired).  Also POS, NER, coreference. 
  C&C; tools .  From (sentence-segmented, tokenized?) text to rich syntactic dependencies ( CCG -based) and also a semantic representation.  POS and chunks on the way.  Does anyone use this much?  It seems underappreciated relative to its richness. 
  Senna .  Sentence-se</p><p>4 0.12158877 <a title="189-tfidf-4" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-01-Binary_classification_evaluation_in_R_via_ROCR.html">136 brendan oconnor ai-2009-04-01-Binary classification evaluation in R via ROCR</a></p>
<p>Introduction: A binary classifier makes decisions with confidence levels.  Usually it’s imperfect: if you put a decision threshold anywhere, items will fall on the wrong side — errors.  I made  this a diagram  a while ago for Turker voting; same principle applies for any binary classifier.
 
     
 
So there are a zillion ways to evaluate a binary classifier.  Accuracy?  Accuracy on different item types (sens, spec)?  Accuracy on different classifier decisions (prec, npv)?  And worse, over the years every field has given these metrics different names.  Signal detection, bioinformatics, medicine, statistics, machine learning, and more I’m sure.  But in R, there’s the excellent  ROCR package  to compute and visualize all the different metrics.
 
I wanted to have a small, easy-to-use function that calls ROCR and reports the basic information I’m interested in.  For  preds , a vector of predictions (as confidence scores), and  labels , the true labels for the instances, it works like this:
 
>  binary_e</p><p>5 0.11585218 <a title="189-tfidf-5" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-17-p-values%2C_CDF%E2%80%99s%2C_NLP_etc..html">185 brendan oconnor ai-2012-07-17-p-values, CDF’s, NLP etc.</a></p>
<p>Introduction: Update Aug 10: THIS IS NOT A SUMMARY OF THE WHOLE PAPER!  it’s whining about one particular method of analysis before talking about other things further down
   
A quick note on  Berg-Kirkpatrick et al EMNLP-2012, “An Empirical Investigation of Statistical Signiﬁcance in NLP” .  They make lots of graphs of p-values against observed magnitudes and talk about “curves”, e.g.
 
 We see the same curve-shaped trend we saw for summarization and dependency parsing. Different group comparisons, same group comparisons, and system combination comparisons form distinct curves. 
 
For example, Figure 2.
 
   
 
I fear they made 10 graphs to rediscover a basic statistical fact: a p-value comes from a null hypothesis CDF.  That’s what these “curve-shaped trends” are in all their graphs.  They are CDFs.
 
To back up, the statistical significance testing question is whether, in their notation, the observed dataset performance difference \(\delta(x)\) is “real” or not: if you were to resample the data,</p><p>6 0.11337572 <a title="189-tfidf-6" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-03-09-I_don%E2%80%99t_get_this_web_parsing_shared_task.html">181 brendan oconnor ai-2012-03-09-I don’t get this web parsing shared task</a></p>
<p>7 0.10140641 <a title="189-tfidf-7" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-03-06-data_data_data.html">93 brendan oconnor ai-2008-03-06-data data data</a></p>
<p>8 0.084994964 <a title="189-tfidf-8" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-27-Facebook_sentiment_mining_predicts_presidential_polls.html">131 brendan oconnor ai-2008-12-27-Facebook sentiment mining predicts presidential polls</a></p>
<p>9 0.078899533 <a title="189-tfidf-9" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-12-31-List_of_probabilistic_model_mini-language_toolkits.html">157 brendan oconnor ai-2009-12-31-List of probabilistic model mini-language toolkits</a></p>
<p>10 0.078163043 <a title="189-tfidf-10" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-08-08-Haghighi_and_Klein_%282009%29%3A_Simple_Coreference_Resolution_with_Rich_Syntactic_and_Semantic_Features.html">150 brendan oconnor ai-2009-08-08-Haghighi and Klein (2009): Simple Coreference Resolution with Rich Syntactic and Semantic Features</a></p>
<p>11 0.077275239 <a title="189-tfidf-11" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-08-20-Some_analysis_of_tweet_shares_and_%E2%80%9Cpredicting%E2%80%9D_election_outcomes.html">198 brendan oconnor ai-2013-08-20-Some analysis of tweet shares and “predicting” election outcomes</a></p>
<p>12 0.075543858 <a title="189-tfidf-12" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-11-11-Memorizing_small_tables.html">177 brendan oconnor ai-2011-11-11-Memorizing small tables</a></p>
<p>13 0.075104617 <a title="189-tfidf-13" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-01-11-Please_report_your_SVM%E2%80%99s_kernel%21.html">164 brendan oconnor ai-2011-01-11-Please report your SVM’s kernel!</a></p>
<p>14 0.074698545 <a title="189-tfidf-14" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-17-1_billion_web_page_dataset_from_CMU.html">138 brendan oconnor ai-2009-04-17-1 billion web page dataset from CMU</a></p>
<p>15 0.072580986 <a title="189-tfidf-15" href="../brendan_oconnor_ai-2014/brendan_oconnor_ai-2014-02-18-Scatterplot_of_KN-PYP_language_model_results.html">202 brendan oconnor ai-2014-02-18-Scatterplot of KN-PYP language model results</a></p>
<p>16 0.067947902 <a title="189-tfidf-16" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-03-Statistics_vs._Machine_Learning%2C_fight%21.html">129 brendan oconnor ai-2008-12-03-Statistics vs. Machine Learning, fight!</a></p>
<p>17 0.067109667 <a title="189-tfidf-17" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-04-06-a_regression_slope_is_a_weighted_average_of_pairs%E2%80%99_slopes%21.html">100 brendan oconnor ai-2008-04-06-a regression slope is a weighted average of pairs’ slopes!</a></p>
<p>18 0.064745471 <a title="189-tfidf-18" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-08-Patches_to_Rainbow%2C_the_old_text_classifier_that_won%E2%80%99t_go_away.html">153 brendan oconnor ai-2009-09-08-Patches to Rainbow, the old text classifier that won’t go away</a></p>
<p>19 0.060531735 <a title="189-tfidf-19" href="../brendan_oconnor_ai-2014/brendan_oconnor_ai-2014-02-19-What_the_ACL-2014_review_scores_mean.html">203 brendan oconnor ai-2014-02-19-What the ACL-2014 review scores mean</a></p>
<p>20 0.059918873 <a title="189-tfidf-20" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-06-17-Pairwise_comparisons_for_relevance_evaluation.html">106 brendan oconnor ai-2008-06-17-Pairwise comparisons for relevance evaluation</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/brendan_oconnor_ai_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, -0.225), (1, -0.18), (2, 0.095), (3, -0.079), (4, 0.089), (5, -0.083), (6, -0.072), (7, -0.08), (8, 0.017), (9, -0.138), (10, 0.112), (11, 0.013), (12, 0.081), (13, 0.036), (14, -0.03), (15, 0.001), (16, -0.097), (17, 0.086), (18, 0.071), (19, -0.135), (20, 0.173), (21, 0.072), (22, -0.003), (23, 0.0), (24, -0.091), (25, 0.071), (26, 0.009), (27, 0.039), (28, 0.139), (29, 0.005), (30, 0.141), (31, 0.011), (32, -0.097), (33, 0.088), (34, 0.059), (35, -0.093), (36, 0.035), (37, 0.052), (38, 0.115), (39, -0.008), (40, 0.051), (41, 0.166), (42, 0.005), (43, -0.059), (44, -0.04), (45, 0.011), (46, 0.035), (47, -0.117), (48, -0.057), (49, -0.116)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99080956 <a title="189-lsi-1" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-11-24-Graphs_for_SANCL-2012_web_parsing_results.html">189 brendan oconnor ai-2012-11-24-Graphs for SANCL-2012 web parsing results</a></p>
<p>Introduction: I was just looking at some papers from the  SANCL-2012 workshop on web parsing  from June this year, which are very interesting to those of us who wish we had good parsers for non-newspaper text.  The shared task focus was on domain adaptation from a setting of lots of Wall Street Journal annotated data and very little in-domain training data.  (Previous discussion  here ; see Ryan McDonald’s detailed comment.)   Here are some graphs of the results ( last page in the Petrov & McDonald overview ).
 
I was most interested in whether parsing accuracy on the WSJ correlates to accuracy on web text.  Fortunately, it does.  They evaluated all systems on four evaluation sets: (1) Text from a question/answer site, (2) newsgroups, (3) reviews, and (4) Wall Street Journal PTB.  Here is a graph across system entries, with the x-axis being the labeled dependency parsing accuracy on WSJPTB, and the y-axis the average accuracy on the three web evaluation sets.  Note the axis scales are different: web</p><p>2 0.74441159 <a title="189-lsi-2" href="../brendan_oconnor_ai-2010/brendan_oconnor_ai-2010-04-14-quick_note%3A_cer_et_al_2010.html">159 brendan oconnor ai-2010-04-14-quick note: cer et al 2010</a></p>
<p>Introduction: Quick note, reading this  paper  from  their tweet .
 
 update  this reaction might be totally wrong; in particular, the conll dependencies for at least some languages were done completely by hand.
   
Malt and MSTParser were designed for the Yamada and Matsumodo dependencies formalism (the one used for the CoNLL dependency parsing shared task, from the   penn2malt   tool).  Their feature sets and probably many other design decisions were created to support that.  If you compare their outputs side-by-side, you will see that the Stanford Dependencies are a substantially different formalism; for example, compound verbs are handled very differently (the paper talks about copula example). 
I think the following conclusion is premature:
  
Notwithstanding the very large amount of research that has gone into dependency 
parsing algorithms in the last ďŹ ve years, our central conclusion is that the quality of the Charniak, Charniak-Johnson reranking, and Berkeley parsers is so high that in th</p><p>3 0.51299047 <a title="189-lsi-3" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-17-p-values%2C_CDF%E2%80%99s%2C_NLP_etc..html">185 brendan oconnor ai-2012-07-17-p-values, CDF’s, NLP etc.</a></p>
<p>Introduction: Update Aug 10: THIS IS NOT A SUMMARY OF THE WHOLE PAPER!  it’s whining about one particular method of analysis before talking about other things further down
   
A quick note on  Berg-Kirkpatrick et al EMNLP-2012, “An Empirical Investigation of Statistical Signiﬁcance in NLP” .  They make lots of graphs of p-values against observed magnitudes and talk about “curves”, e.g.
 
 We see the same curve-shaped trend we saw for summarization and dependency parsing. Different group comparisons, same group comparisons, and system combination comparisons form distinct curves. 
 
For example, Figure 2.
 
   
 
I fear they made 10 graphs to rediscover a basic statistical fact: a p-value comes from a null hypothesis CDF.  That’s what these “curve-shaped trends” are in all their graphs.  They are CDFs.
 
To back up, the statistical significance testing question is whether, in their notation, the observed dataset performance difference \(\delta(x)\) is “real” or not: if you were to resample the data,</p><p>4 0.490646 <a title="189-lsi-4" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-03-09-I_don%E2%80%99t_get_this_web_parsing_shared_task.html">181 brendan oconnor ai-2012-03-09-I don’t get this web parsing shared task</a></p>
<p>Introduction: The idea for a shared task on web parsing is really cool.  But I don’t get this one:
 
 Shared Task – SANCL 2012 (First Workshop on Syntactic Analysis of Non-Canonical Language) 
 
They’re explicitly banning
  
 Manually annotating in-domain (web) sentences
  Creating new word clusters, or anything, from as much text data as possible
   
… instead restricting participants to the data sets they release.
 
Isn’t a cycle of annotation, error analysis, and new annotations (a self-training + active-learning loop, with smarter decisions through error analysis) the hands-down best way to make an NLP tool for a new domain? Are people scared of this reality?  Am I off-base?
 
I am, of course, just advocating for our  Twitter POS tagger  approach, where we annotated some data, made a supervised tagger, and iterated on features.  The biggest weakness in that paper is we didn’t have additional iterations of error analysis.  Our lack of semi-supervised learning was  not  a weakness.</p><p>5 0.48284525 <a title="189-lsi-5" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-09-19-End-to-end_NLP_packages.html">174 brendan oconnor ai-2011-09-19-End-to-end NLP packages</a></p>
<p>Introduction: What freely available end-to-end natural language processing (NLP) systems are out there, that start with raw text, and output parses and semantic structures?  Lots of NLP research focuses on single tasks at a time, and thus produces software that does a single task at a time.  But for various applications, it is nicer to have a full end-to-end system that just runs on whatever text you give it.
 
If you believe this is a worthwhile goal (see caveat at bottom), I will postulate there aren’t a ton of such end-to-end, multilevel systems.  Here are ones I can think of.  Corrections and clarifications welcome.
  
  Stanford CoreNLP .  Raw text to  rich syntactic dependencies  ( LFG -inspired).  Also POS, NER, coreference. 
  C&C; tools .  From (sentence-segmented, tokenized?) text to rich syntactic dependencies ( CCG -based) and also a semantic representation.  POS and chunks on the way.  Does anyone use this much?  It seems underappreciated relative to its richness. 
  Senna .  Sentence-se</p><p>6 0.47178435 <a title="189-lsi-6" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-01-Binary_classification_evaluation_in_R_via_ROCR.html">136 brendan oconnor ai-2009-04-01-Binary classification evaluation in R via ROCR</a></p>
<p>7 0.44780013 <a title="189-lsi-7" href="../brendan_oconnor_ai-2014/brendan_oconnor_ai-2014-02-18-Scatterplot_of_KN-PYP_language_model_results.html">202 brendan oconnor ai-2014-02-18-Scatterplot of KN-PYP language model results</a></p>
<p>8 0.42066514 <a title="189-lsi-8" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-10-Don%E2%80%99t_MAWK_AWK_%E2%80%93_the_fastest_and_most_elegant_big_data_munging_language%21.html">154 brendan oconnor ai-2009-09-10-Don’t MAWK AWK – the fastest and most elegant big data munging language!</a></p>
<p>9 0.41051075 <a title="189-lsi-9" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-01-11-Please_report_your_SVM%E2%80%99s_kernel%21.html">164 brendan oconnor ai-2011-01-11-Please report your SVM’s kernel!</a></p>
<p>10 0.40074837 <a title="189-lsi-10" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-12-31-List_of_probabilistic_model_mini-language_toolkits.html">157 brendan oconnor ai-2009-12-31-List of probabilistic model mini-language toolkits</a></p>
<p>11 0.38323671 <a title="189-lsi-11" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-03-06-data_data_data.html">93 brendan oconnor ai-2008-03-06-data data data</a></p>
<p>12 0.36218375 <a title="189-lsi-12" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-11-11-Memorizing_small_tables.html">177 brendan oconnor ai-2011-11-11-Memorizing small tables</a></p>
<p>13 0.36136913 <a title="189-lsi-13" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-08-08-Haghighi_and_Klein_%282009%29%3A_Simple_Coreference_Resolution_with_Rich_Syntactic_and_Semantic_Features.html">150 brendan oconnor ai-2009-08-08-Haghighi and Klein (2009): Simple Coreference Resolution with Rich Syntactic and Semantic Features</a></p>
<p>14 0.34552422 <a title="189-lsi-14" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-22-Performance_comparison%3A_key-value_stores_for_language_model_counts.html">139 brendan oconnor ai-2009-04-22-Performance comparison: key-value stores for language model counts</a></p>
<p>15 0.34209654 <a title="189-lsi-15" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-06-17-Pairwise_comparisons_for_relevance_evaluation.html">106 brendan oconnor ai-2008-06-17-Pairwise comparisons for relevance evaluation</a></p>
<p>16 0.32987586 <a title="189-lsi-16" href="../brendan_oconnor_ai-2014/brendan_oconnor_ai-2014-02-19-What_the_ACL-2014_review_scores_mean.html">203 brendan oconnor ai-2014-02-19-What the ACL-2014 review scores mean</a></p>
<p>17 0.32964307 <a title="189-lsi-17" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-01-20-Moral_psychology_on_Amazon_Mechanical_Turk.html">90 brendan oconnor ai-2008-01-20-Moral psychology on Amazon Mechanical Turk</a></p>
<p>18 0.31858796 <a title="189-lsi-18" href="../brendan_oconnor_ai-2014/brendan_oconnor_ai-2014-04-26-Replot%3A_departure_delays_vs_flight_time_speed-up.html">204 brendan oconnor ai-2014-04-26-Replot: departure delays vs flight time speed-up</a></p>
<p>19 0.30847123 <a title="189-lsi-19" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-17-1_billion_web_page_dataset_from_CMU.html">138 brendan oconnor ai-2009-04-17-1 billion web page dataset from CMU</a></p>
<p>20 0.30581257 <a title="189-lsi-20" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-05-20-Log-normal_and_logistic-normal_terminology.html">169 brendan oconnor ai-2011-05-20-Log-normal and logistic-normal terminology</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/brendan_oconnor_ai_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(9, 0.371), (14, 0.012), (24, 0.051), (43, 0.02), (44, 0.175), (48, 0.054), (55, 0.025), (57, 0.028), (61, 0.011), (70, 0.049), (74, 0.066), (80, 0.043)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.94915307 <a title="189-lda-1" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-12-09-Race_and_IQ_debate_%E2%80%93_links.html">85 brendan oconnor ai-2007-12-09-Race and IQ debate – links</a></p>
<p>Introduction: William Saletan, a writer for Slate, recently wrote  a loud series of articles  on genetic racial differences in IQ in the wake of James Watson’s  controversial remarks .  It prompted lots of discussion; here is an  excellent response from Richard Nisbett , a leading authority in the field on the environmentalist side of the debate.
 
More academic articles:  Rushton and Jensen’s 2005 review  of evidence for genetic differences; and what I’ve found to be the most balanced so far, the 1995 APA report  Inteligence: Knowns and Unknowns  which concludes for all the heated claims out there, the scientific evidence tends to be pretty weak.
 
Blog world:  Funny title from Brad DeLong ; and another  Slate response to Saletan and Rushton/Jensen .
 
The politics of the race and intelligence question is a huge distraction from trying to find out the actual truth of the matter.  But I suppose the political implications are why it attracts so much attention — for good or bad.
 
The most interesting</p><p>same-blog 2 0.93177193 <a title="189-lda-2" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-11-24-Graphs_for_SANCL-2012_web_parsing_results.html">189 brendan oconnor ai-2012-11-24-Graphs for SANCL-2012 web parsing results</a></p>
<p>Introduction: I was just looking at some papers from the  SANCL-2012 workshop on web parsing  from June this year, which are very interesting to those of us who wish we had good parsers for non-newspaper text.  The shared task focus was on domain adaptation from a setting of lots of Wall Street Journal annotated data and very little in-domain training data.  (Previous discussion  here ; see Ryan McDonald’s detailed comment.)   Here are some graphs of the results ( last page in the Petrov & McDonald overview ).
 
I was most interested in whether parsing accuracy on the WSJ correlates to accuracy on web text.  Fortunately, it does.  They evaluated all systems on four evaluation sets: (1) Text from a question/answer site, (2) newsgroups, (3) reviews, and (4) Wall Street Journal PTB.  Here is a graph across system entries, with the x-axis being the labeled dependency parsing accuracy on WSJPTB, and the y-axis the average accuracy on the three web evaluation sets.  Note the axis scales are different: web</p><p>3 0.46910048 <a title="189-lda-3" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-08-20-Some_analysis_of_tweet_shares_and_%E2%80%9Cpredicting%E2%80%9D_election_outcomes.html">198 brendan oconnor ai-2013-08-20-Some analysis of tweet shares and “predicting” election outcomes</a></p>
<p>Introduction: Everyone recently seems to be talking about  this newish paper by Digrazia, McKelvey, Bollen, and Rojas  ( pdf here ) that examines the correlation of Congressional candidate name mentions on Twitter against whether the candidate won the race.  One of the coauthors also wrote a Washington Post  Op-Ed  about it.  I read the paper and I think it’s reasonable, but their op-ed overstates their results.  It claims:
  
“In the 2010 data, our Twitter data predicted the winner in 404 out of 435 competitive races”
  
But this analysis is nowhere in their paper.  Fabio Rojas has now  posted errata/rebuttals  about the op-ed and described this analysis they did here.  There are several major issues off the bat:
  
 They didn’t ever predict 404/435 races; they only analyzed 406 races they call “competitive,” getting 92.5% (in-sample) accuracy, then extrapolated to all races to get the 435 number. 
 They’re reporting about  in-sample  predictions, which is really misleading to a non-scientific audi</p><p>4 0.44575372 <a title="189-lda-4" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-27-Facebook_sentiment_mining_predicts_presidential_polls.html">131 brendan oconnor ai-2008-12-27-Facebook sentiment mining predicts presidential polls</a></p>
<p>Introduction: I’m a bit late blogging this, but here’s a messy, exciting — and statistically validated! — new online data source.
 
My friend  Roddy  at Facebook  wrote a post describing their sentiment analysis system , which can evaluate positive or negative sentiment toward a particular topic by looking at a large number of wall messages.  (I’d link to it, but I can’t find the URL anymore — here’s the  Lexicon , but that version only gets term frequencies but no sentiment.)
 
How they constructed their sentiment detector is interesting.  Starting with a list of positive and negative terms, they had a lexical acquisition step to gather many more candidate synonyms and misspellings — a necessity in this social media domain, where  WordNet  ain’t gonna come close!  After manually filtering these candidates, they assess the sentiment toward a mention of a topic by looking for instances of these positive and negative words nearby, along with “negation heuristics” and a few other features.
 
He describ</p><p>5 0.43818694 <a title="189-lda-5" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-17-p-values%2C_CDF%E2%80%99s%2C_NLP_etc..html">185 brendan oconnor ai-2012-07-17-p-values, CDF’s, NLP etc.</a></p>
<p>Introduction: Update Aug 10: THIS IS NOT A SUMMARY OF THE WHOLE PAPER!  it’s whining about one particular method of analysis before talking about other things further down
   
A quick note on  Berg-Kirkpatrick et al EMNLP-2012, “An Empirical Investigation of Statistical Signiﬁcance in NLP” .  They make lots of graphs of p-values against observed magnitudes and talk about “curves”, e.g.
 
 We see the same curve-shaped trend we saw for summarization and dependency parsing. Different group comparisons, same group comparisons, and system combination comparisons form distinct curves. 
 
For example, Figure 2.
 
   
 
I fear they made 10 graphs to rediscover a basic statistical fact: a p-value comes from a null hypothesis CDF.  That’s what these “curve-shaped trends” are in all their graphs.  They are CDFs.
 
To back up, the statistical significance testing question is whether, in their notation, the observed dataset performance difference \(\delta(x)\) is “real” or not: if you were to resample the data,</p><p>6 0.42463523 <a title="189-lda-6" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-03-Statistics_vs._Machine_Learning%2C_fight%21.html">129 brendan oconnor ai-2008-12-03-Statistics vs. Machine Learning, fight!</a></p>
<p>7 0.41666615 <a title="189-lda-7" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-03-09-I_don%E2%80%99t_get_this_web_parsing_shared_task.html">181 brendan oconnor ai-2012-03-09-I don’t get this web parsing shared task</a></p>
<p>8 0.41394132 <a title="189-lda-8" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-01-Binary_classification_evaluation_in_R_via_ROCR.html">136 brendan oconnor ai-2009-04-01-Binary classification evaluation in R via ROCR</a></p>
<p>9 0.41209131 <a title="189-lda-9" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-10-08-Blog_move_has_landed.html">115 brendan oconnor ai-2008-10-08-Blog move has landed</a></p>
<p>10 0.40709567 <a title="189-lda-10" href="../brendan_oconnor_ai-2006/brendan_oconnor_ai-2006-03-18-Mark_Turner%3A_Toward_the_Founding_of_Cognitive_Social_Science.html">31 brendan oconnor ai-2006-03-18-Mark Turner: Toward the Founding of Cognitive Social Science</a></p>
<p>11 0.40709567 <a title="189-lda-11" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-10-13-Verificationism_dinosaur_comics.html">79 brendan oconnor ai-2007-10-13-Verificationism dinosaur comics</a></p>
<p>12 0.40650314 <a title="189-lda-12" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-08-08-Haghighi_and_Klein_%282009%29%3A_Simple_Coreference_Resolution_with_Rich_Syntactic_and_Semantic_Features.html">150 brendan oconnor ai-2009-08-08-Haghighi and Klein (2009): Simple Coreference Resolution with Rich Syntactic and Semantic Features</a></p>
<p>13 0.40518409 <a title="189-lda-13" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-10-02-Powerset%E2%80%99s_natural_language_search_system.html">188 brendan oconnor ai-2012-10-02-Powerset’s natural language search system</a></p>
<p>14 0.40247351 <a title="189-lda-14" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-04-The_%2460%2C000_cat%3A_deep_belief_networks_make_less_sense_for_language_than_vision.html">184 brendan oconnor ai-2012-07-04-The $60,000 cat: deep belief networks make less sense for language than vision</a></p>
<p>15 0.39250892 <a title="189-lda-15" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-10-Don%E2%80%99t_MAWK_AWK_%E2%80%93_the_fastest_and_most_elegant_big_data_munging_language%21.html">154 brendan oconnor ai-2009-09-10-Don’t MAWK AWK – the fastest and most elegant big data munging language!</a></p>
<p>16 0.38675955 <a title="189-lda-16" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-09-19-End-to-end_NLP_packages.html">174 brendan oconnor ai-2011-09-19-End-to-end NLP packages</a></p>
<p>17 0.38000092 <a title="189-lda-17" href="../brendan_oconnor_ai-2006/brendan_oconnor_ai-2006-08-30-A_big%2C_fun_list_of_links_I%E2%80%99m_reading.html">44 brendan oconnor ai-2006-08-30-A big, fun list of links I’m reading</a></p>
<p>18 0.37320426 <a title="189-lda-18" href="../brendan_oconnor_ai-2004/brendan_oconnor_ai-2004-11-24-addiction_%26_2_problems_of_economics.html">2 brendan oconnor ai-2004-11-24-addiction & 2 problems of economics</a></p>
<p>19 0.3697491 <a title="189-lda-19" href="../brendan_oconnor_ai-2006/brendan_oconnor_ai-2006-03-26-new_kind_of_science%2C_for_real.html">32 brendan oconnor ai-2006-03-26-new kind of science, for real</a></p>
<p>20 0.36772725 <a title="189-lda-20" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-10-11-It_is_accurate_to_determine_a_blog%E2%80%99s_bias_by_what_it_links_to.html">117 brendan oconnor ai-2008-10-11-It is accurate to determine a blog’s bias by what it links to</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
