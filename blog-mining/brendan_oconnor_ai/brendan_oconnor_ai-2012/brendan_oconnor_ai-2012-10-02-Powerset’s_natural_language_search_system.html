<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>188 brendan oconnor ai-2012-10-02-Powerset’s natural language search system</title>
</head>

<body>
<p><a title="brendan_oconnor_ai" href="../brendan_oconnor_ai_home.html">brendan_oconnor_ai</a> <a title="brendan_oconnor_ai-2012" href="../home/brendan_oconnor_ai-2012_home.html">brendan_oconnor_ai-2012</a> <a title="brendan_oconnor_ai-2012-188" href="#">brendan_oconnor_ai-2012-188</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>188 brendan oconnor ai-2012-10-02-Powerset’s natural language search system</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="brendan_oconnor_ai-2012-188-html" href="http://brenocon.com/blog/2012/10/powersets-natural-language-search-system/">html</a></p><p>Introduction: There’s a lot to say about  Powerset , the short-lived natural language search company (2005-2008) where I worked after college.  AI overhype, flying too close to the sun, the psychology of tech journalism and venture capitalism, etc.  A year or two ago I wrote the following bit about Powerset’s technology in response to a question  on Quora .  I’m posting a revised version here.
 
 Question:  What was Powerset’s core innovation in search?  As far as I can tell, they licensed an NLP engine. They did not have a question answering system or any system for information extraction. How was Powerset’s search engine different than Google’s?
 
 My answer:  Powerset built a system vaguely like a question-answering system on top of Xerox PARC’s NLP engine.  The output is better described as query-focused summarization rather than question answering; primarily, it matched semantic fragments of the user query against indexed semantic relations, with lots of keyword/ngram-matching fallback for when</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 There’s a lot to say about  Powerset , the short-lived natural language search company (2005-2008) where I worked after college. [sent-1, score-0.211]
</p><p>2 A year or two ago I wrote the following bit about Powerset’s technology in response to a question  on Quora . [sent-3, score-0.189]
</p><p>3 Question:  What was Powerset’s core innovation in search? [sent-5, score-0.166]
</p><p>4 They did not have a question answering system or any system for information extraction. [sent-7, score-0.533]
</p><p>5 My answer:  Powerset built a system vaguely like a question-answering system on top of Xerox PARC’s NLP engine. [sent-9, score-0.294]
</p><p>6 The Powerset system indexed semantic relations and entities (the latter often being wordnet/freebase nodes), did a similar analysis on the user query, then formed a database query against that index of semantic relations, synonym/hypernym expansions, and other textual information (e. [sent-11, score-1.104]
</p><p>7 Then with all the rich (complicated) index information, you have neat features for ranking and snippet generation (i. [sent-14, score-0.107]
</p><p>8 query-focused summarization), but it’s so complicated it’s easy to screw up. [sent-16, score-0.067]
</p><p>9 (And don’t get me started on trying to run a segfault-prone Tcl/Prolog/C parser under an unstable 2006-era Hadoop…)    Here is a diagram I wrote in July 2007  to try to communicate internally what the entire system was doing. [sent-17, score-0.267]
</p><p>10 This diagram only depicts the indexing pipeline; the query-time system would have required another diagram. [sent-19, score-0.363]
</p><p>11 NLP folks will note some rather surprising technology choices in some places. [sent-20, score-0.101]
</p><p>12 But the company did have >$12 million in funding at this point. [sent-24, score-0.057]
</p><p>13 Certainly no serious search engine was crazy enough to do constituent parses (and unification parses, lexical lookups, coreference, etc. [sent-26, score-0.44]
</p><p>14 ) of all sentences at index time — raising indexing costs, compared to keyword indexing, by perhaps 100x — but Powerset sure did. [sent-27, score-0.252]
</p><p>15 It’s worth noting that since then, Google has added much more question-answering and structured information search, presumably using related but different techniques than Powerset used. [sent-28, score-0.067]
</p><p>16 They now have excellent groups of highly-regarded specialists in parsing, unsupervised lexical semantics, machine translation, etc. [sent-30, score-0.067]
</p><p>17 In principle, deep semantic analysis of web text could be useful for search (and shallow NLP, like morphology and chunking, perhaps more so); but as the primary thing for a search startup to focus on, it seemed a little extreme. [sent-33, score-0.538]
</p><p>18 Usually I am cynical and say Powerset had no serious innovation for search. [sent-36, score-0.255]
</p><p>19 In general, I found that design/UI engineering people respected Powerset for the frontends, scalability engineers respected Powerset for the HBase contributions, but NLP and IR experts were highly cynical about Powerset’s technology claims. [sent-39, score-0.372]
</p><p>20 If you get a chance, try asking researchers who were at ACL 2007 in Prague about Barney Pell’s keynote; I am told a number walked out while it was underway. [sent-40, score-0.049]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('powerset', 0.672), ('semantic', 0.184), ('search', 0.154), ('system', 0.147), ('indexing', 0.145), ('google', 0.137), ('relations', 0.135), ('nlp', 0.133), ('query', 0.128), ('innovation', 0.107), ('index', 0.107), ('technology', 0.101), ('amit', 0.097), ('hbase', 0.097), ('indexed', 0.097), ('parc', 0.097), ('quora', 0.097), ('respected', 0.097), ('question', 0.088), ('parses', 0.084), ('answering', 0.084), ('summarization', 0.084), ('joseph', 0.084), ('cynical', 0.077), ('diagram', 0.071), ('serious', 0.071), ('complicated', 0.067), ('lexical', 0.067), ('information', 0.067), ('engine', 0.064), ('developed', 0.064), ('core', 0.059), ('company', 0.057), ('user', 0.055), ('try', 0.049), ('web', 0.046), ('stuff', 0.044), ('recently', 0.043), ('filed', 0.042), ('turner', 0.042), ('microsoft', 0.042), ('barney', 0.042), ('july', 0.042), ('nick', 0.042), ('pell', 0.042), ('dare', 0.042), ('nodes', 0.042), ('wherever', 0.042), ('tech', 0.042), ('commentary', 0.042)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000002 <a title="188-tfidf-1" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-10-02-Powerset%E2%80%99s_natural_language_search_system.html">188 brendan oconnor ai-2012-10-02-Powerset’s natural language search system</a></p>
<p>Introduction: There’s a lot to say about  Powerset , the short-lived natural language search company (2005-2008) where I worked after college.  AI overhype, flying too close to the sun, the psychology of tech journalism and venture capitalism, etc.  A year or two ago I wrote the following bit about Powerset’s technology in response to a question  on Quora .  I’m posting a revised version here.
 
 Question:  What was Powerset’s core innovation in search?  As far as I can tell, they licensed an NLP engine. They did not have a question answering system or any system for information extraction. How was Powerset’s search engine different than Google’s?
 
 My answer:  Powerset built a system vaguely like a question-answering system on top of Xerox PARC’s NLP engine.  The output is better described as query-focused summarization rather than question answering; primarily, it matched semantic fragments of the user query against indexed semantic relations, with lots of keyword/ngram-matching fallback for when</p><p>2 0.2755062 <a title="188-tfidf-2" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-05-13-The_best_natural_language_search_commentary_on_the_internet.html">102 brendan oconnor ai-2008-05-13-The best natural language search commentary on the internet</a></p>
<p>Introduction: With Powerset’s launch, there’s an awful lot of hot air and crappy blog posts about natural language search being written.  Instead of contributing to that mess, I prefer to direct the reader to the best writing on the topic that I’ve seen:  Fernando Pereira’s posts on search .</p><p>3 0.16472897 <a title="188-tfidf-3" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-09-19-End-to-end_NLP_packages.html">174 brendan oconnor ai-2011-09-19-End-to-end NLP packages</a></p>
<p>Introduction: What freely available end-to-end natural language processing (NLP) systems are out there, that start with raw text, and output parses and semantic structures?  Lots of NLP research focuses on single tasks at a time, and thus produces software that does a single task at a time.  But for various applications, it is nicer to have a full end-to-end system that just runs on whatever text you give it.
 
If you believe this is a worthwhile goal (see caveat at bottom), I will postulate there aren’t a ton of such end-to-end, multilevel systems.  Here are ones I can think of.  Corrections and clarifications welcome.
  
  Stanford CoreNLP .  Raw text to  rich syntactic dependencies  ( LFG -inspired).  Also POS, NER, coreference. 
  C&C; tools .  From (sentence-segmented, tokenized?) text to rich syntactic dependencies ( CCG -based) and also a semantic representation.  POS and chunks on the way.  Does anyone use this much?  It seems underappreciated relative to its richness. 
  Senna .  Sentence-se</p><p>4 0.14526352 <a title="188-tfidf-4" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-08-08-Haghighi_and_Klein_%282009%29%3A_Simple_Coreference_Resolution_with_Rich_Syntactic_and_Semantic_Features.html">150 brendan oconnor ai-2009-08-08-Haghighi and Klein (2009): Simple Coreference Resolution with Rich Syntactic and Semantic Features</a></p>
<p>Introduction: I haven’t done a paper review on this blog for a while, so here we go.
 
 Coreference  resolution is an interesting NLP problem.  ( Examples. )  It involves honest-to-goodness syntactic, semantic, and discourse phenomena, but still seems like a real cognitive task that humans have to solve when reading text [1].  I haven’t read the whole literature, but I’ve always been puzzled by the crop of papers on it I’ve seen in the last year or two.  There’s a big focus on fancy graph/probabilistic/constrained optimization algorithms, but often these papers gloss over the linguistic features — the core information they actually make their decisions with [2].  I never understood why the latter isn’t the most important issue.  Therefore, it was a joy to read
  
 Aria Haghighi and Dan Klein, EMNLP-2009.   “Simple Coreference Resolution with Rich Syntactic and Semantic Features.”  
  
They describe a simple, essentially non-statistical system that outperforms previous unsupervised systems, and compa</p><p>5 0.14334071 <a title="188-tfidf-5" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-04-08-Random_search_engine_searcher.html">59 brendan oconnor ai-2007-04-08-Random search engine searcher</a></p>
<p>Introduction: It’s sweeping the internet  — I wrote a little plugin for the firefox/internet explorer search box, so when you search it randomly picks one of several search engines.  You get to see what’s out there (you mean there’s something besides Google?) in your daily searching.
 
 Search a Random Search Engine</p><p>6 0.093172088 <a title="188-tfidf-6" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-17-1_billion_web_page_dataset_from_CMU.html">138 brendan oconnor ai-2009-04-17-1 billion web page dataset from CMU</a></p>
<p>7 0.091619842 <a title="188-tfidf-7" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-11-12-Disease_tracking_with_web_queries_and_social_messaging_%28Google%2C_Twitter%2C_Facebook%E2%80%A6%29.html">123 brendan oconnor ai-2008-11-12-Disease tracking with web queries and social messaging (Google, Twitter, Facebook…)</a></p>
<p>8 0.086787678 <a title="188-tfidf-8" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-27-Facebook_sentiment_mining_predicts_presidential_polls.html">131 brendan oconnor ai-2008-12-27-Facebook sentiment mining predicts presidential polls</a></p>
<p>9 0.083462171 <a title="188-tfidf-9" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-05-18-Announcing_TweetMotif_for_summarizing_twitter_topics.html">140 brendan oconnor ai-2009-05-18-Announcing TweetMotif for summarizing twitter topics</a></p>
<p>10 0.069866925 <a title="188-tfidf-10" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-11-24-Python_bindings_to_Google%E2%80%99s_%E2%80%9CAJAX%E2%80%9D_Search_API.html">127 brendan oconnor ai-2008-11-24-Python bindings to Google’s “AJAX” Search API</a></p>
<p>11 0.067255236 <a title="188-tfidf-11" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-17-p-values%2C_CDF%E2%80%99s%2C_NLP_etc..html">185 brendan oconnor ai-2012-07-17-p-values, CDF’s, NLP etc.</a></p>
<p>12 0.064623021 <a title="188-tfidf-12" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-09-18-%22Machine%22_translation-vision_%28Stanford_AI_courses_online%29.html">113 brendan oconnor ai-2008-09-18-"Machine" translation-vision (Stanford AI courses online)</a></p>
<p>13 0.063135222 <a title="188-tfidf-13" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-20-Quiz%3A_%E2%80%9Cart%E2%80%9D_and_%E2%80%9Cpharmaceuticals%E2%80%9D.html">155 brendan oconnor ai-2009-09-20-Quiz: “art” and “pharmaceuticals”</a></p>
<p>14 0.061438777 <a title="188-tfidf-14" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-26-Seeing_how_%E2%80%9Cart%E2%80%9D_and_%E2%80%9Cpharmaceuticals%E2%80%9D_are_linguistically_similar_in_web_text.html">156 brendan oconnor ai-2009-09-26-Seeing how “art” and “pharmaceuticals” are linguistically similar in web text</a></p>
<p>15 0.060762927 <a title="188-tfidf-15" href="../brendan_oconnor_ai-2014/brendan_oconnor_ai-2014-02-19-What_the_ACL-2014_review_scores_mean.html">203 brendan oconnor ai-2014-02-19-What the ACL-2014 review scores mean</a></p>
<p>16 0.060049169 <a title="188-tfidf-16" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-09-13-Response_on_our_movie_personas_paper.html">200 brendan oconnor ai-2013-09-13-Response on our movie personas paper</a></p>
<p>17 0.059390683 <a title="188-tfidf-17" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-06-17-%22Time_will_tell%2C_epistemology_won%E2%80%99t%22.html">65 brendan oconnor ai-2007-06-17-"Time will tell, epistemology won’t"</a></p>
<p>18 0.052634496 <a title="188-tfidf-18" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-11-24-Graphs_for_SANCL-2012_web_parsing_results.html">189 brendan oconnor ai-2012-11-24-Graphs for SANCL-2012 web parsing results</a></p>
<p>19 0.052411504 <a title="188-tfidf-19" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-04-The_%2460%2C000_cat%3A_deep_belief_networks_make_less_sense_for_language_than_vision.html">184 brendan oconnor ai-2012-07-04-The $60,000 cat: deep belief networks make less sense for language than vision</a></p>
<p>20 0.051868211 <a title="188-tfidf-20" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-10-Don%E2%80%99t_MAWK_AWK_%E2%80%93_the_fastest_and_most_elegant_big_data_munging_language%21.html">154 brendan oconnor ai-2009-09-10-Don’t MAWK AWK – the fastest and most elegant big data munging language!</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/brendan_oconnor_ai_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, -0.214), (1, -0.185), (2, 0.012), (3, 0.04), (4, -0.012), (5, -0.172), (6, 0.301), (7, 0.129), (8, -0.044), (9, -0.194), (10, -0.053), (11, 0.105), (12, 0.067), (13, 0.045), (14, -0.018), (15, 0.011), (16, -0.067), (17, -0.124), (18, 0.032), (19, -0.098), (20, 0.037), (21, -0.029), (22, -0.057), (23, 0.064), (24, 0.067), (25, 0.014), (26, 0.09), (27, 0.046), (28, -0.093), (29, 0.043), (30, -0.086), (31, 0.077), (32, -0.028), (33, 0.004), (34, 0.069), (35, 0.07), (36, 0.01), (37, -0.032), (38, -0.053), (39, 0.097), (40, 0.018), (41, 0.011), (42, 0.078), (43, -0.029), (44, 0.05), (45, -0.04), (46, -0.078), (47, 0.111), (48, -0.043), (49, 0.026)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98838413 <a title="188-lsi-1" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-10-02-Powerset%E2%80%99s_natural_language_search_system.html">188 brendan oconnor ai-2012-10-02-Powerset’s natural language search system</a></p>
<p>Introduction: There’s a lot to say about  Powerset , the short-lived natural language search company (2005-2008) where I worked after college.  AI overhype, flying too close to the sun, the psychology of tech journalism and venture capitalism, etc.  A year or two ago I wrote the following bit about Powerset’s technology in response to a question  on Quora .  I’m posting a revised version here.
 
 Question:  What was Powerset’s core innovation in search?  As far as I can tell, they licensed an NLP engine. They did not have a question answering system or any system for information extraction. How was Powerset’s search engine different than Google’s?
 
 My answer:  Powerset built a system vaguely like a question-answering system on top of Xerox PARC’s NLP engine.  The output is better described as query-focused summarization rather than question answering; primarily, it matched semantic fragments of the user query against indexed semantic relations, with lots of keyword/ngram-matching fallback for when</p><p>2 0.7751922 <a title="188-lsi-2" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-05-13-The_best_natural_language_search_commentary_on_the_internet.html">102 brendan oconnor ai-2008-05-13-The best natural language search commentary on the internet</a></p>
<p>Introduction: With Powerset’s launch, there’s an awful lot of hot air and crappy blog posts about natural language search being written.  Instead of contributing to that mess, I prefer to direct the reader to the best writing on the topic that I’ve seen:  Fernando Pereira’s posts on search .</p><p>3 0.66994458 <a title="188-lsi-3" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-04-08-Random_search_engine_searcher.html">59 brendan oconnor ai-2007-04-08-Random search engine searcher</a></p>
<p>Introduction: It’s sweeping the internet  — I wrote a little plugin for the firefox/internet explorer search box, so when you search it randomly picks one of several search engines.  You get to see what’s out there (you mean there’s something besides Google?) in your daily searching.
 
 Search a Random Search Engine</p><p>4 0.56951207 <a title="188-lsi-4" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-11-12-Disease_tracking_with_web_queries_and_social_messaging_%28Google%2C_Twitter%2C_Facebook%E2%80%A6%29.html">123 brendan oconnor ai-2008-11-12-Disease tracking with web queries and social messaging (Google, Twitter, Facebook…)</a></p>
<p>Introduction: This is a good idea: in a search engine’s query logs, look for outbreaks of queries like [[flu symptoms]] in a given region.  I’ve heard (from  Roddy ) that this trick also works well on Facebook statuses (e.g. “Feeling crappy this morning, think I just got the flu”).
  
  Google Uses Web Searches to Track Flu’s Spread – NYTimes.com  
  Google Flu Trends – google.org  
  
For an example with a publicly available data feed, these queries works decently well on Twitter search:
 
 [[ flu -shot -google ]]  (high recall)
 
 [[ "muscle aches" flu -shot ]]  (high precision)
     
 
The “muscle aches” query is too sparse and the general query is too noisy, but you could imagine some more tricks to clean it up, then train a classifier, etc.  With a bit more work it looks like geolocation information can be had out of the  Twitter search API .</p><p>5 0.51544613 <a title="188-lsi-5" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-09-19-End-to-end_NLP_packages.html">174 brendan oconnor ai-2011-09-19-End-to-end NLP packages</a></p>
<p>Introduction: What freely available end-to-end natural language processing (NLP) systems are out there, that start with raw text, and output parses and semantic structures?  Lots of NLP research focuses on single tasks at a time, and thus produces software that does a single task at a time.  But for various applications, it is nicer to have a full end-to-end system that just runs on whatever text you give it.
 
If you believe this is a worthwhile goal (see caveat at bottom), I will postulate there aren’t a ton of such end-to-end, multilevel systems.  Here are ones I can think of.  Corrections and clarifications welcome.
  
  Stanford CoreNLP .  Raw text to  rich syntactic dependencies  ( LFG -inspired).  Also POS, NER, coreference. 
  C&C; tools .  From (sentence-segmented, tokenized?) text to rich syntactic dependencies ( CCG -based) and also a semantic representation.  POS and chunks on the way.  Does anyone use this much?  It seems underappreciated relative to its richness. 
  Senna .  Sentence-se</p><p>6 0.50036383 <a title="188-lsi-6" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-08-08-Haghighi_and_Klein_%282009%29%3A_Simple_Coreference_Resolution_with_Rich_Syntactic_and_Semantic_Features.html">150 brendan oconnor ai-2009-08-08-Haghighi and Klein (2009): Simple Coreference Resolution with Rich Syntactic and Semantic Features</a></p>
<p>7 0.42853251 <a title="188-lsi-7" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-17-1_billion_web_page_dataset_from_CMU.html">138 brendan oconnor ai-2009-04-17-1 billion web page dataset from CMU</a></p>
<p>8 0.41393033 <a title="188-lsi-8" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-11-24-Python_bindings_to_Google%E2%80%99s_%E2%80%9CAJAX%E2%80%9D_Search_API.html">127 brendan oconnor ai-2008-11-24-Python bindings to Google’s “AJAX” Search API</a></p>
<p>9 0.35668093 <a title="188-lsi-9" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-27-Facebook_sentiment_mining_predicts_presidential_polls.html">131 brendan oconnor ai-2008-12-27-Facebook sentiment mining predicts presidential polls</a></p>
<p>10 0.354065 <a title="188-lsi-10" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-05-18-Announcing_TweetMotif_for_summarizing_twitter_topics.html">140 brendan oconnor ai-2009-05-18-Announcing TweetMotif for summarizing twitter topics</a></p>
<p>11 0.31937784 <a title="188-lsi-11" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-17-p-values%2C_CDF%E2%80%99s%2C_NLP_etc..html">185 brendan oconnor ai-2012-07-17-p-values, CDF’s, NLP etc.</a></p>
<p>12 0.2850315 <a title="188-lsi-12" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-04-The_%2460%2C000_cat%3A_deep_belief_networks_make_less_sense_for_language_than_vision.html">184 brendan oconnor ai-2012-07-04-The $60,000 cat: deep belief networks make less sense for language than vision</a></p>
<p>13 0.27948621 <a title="188-lsi-13" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-09-18-%22Machine%22_translation-vision_%28Stanford_AI_courses_online%29.html">113 brendan oconnor ai-2008-09-18-"Machine" translation-vision (Stanford AI courses online)</a></p>
<p>14 0.25586677 <a title="188-lsi-14" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-26-Seeing_how_%E2%80%9Cart%E2%80%9D_and_%E2%80%9Cpharmaceuticals%E2%80%9D_are_linguistically_similar_in_web_text.html">156 brendan oconnor ai-2009-09-26-Seeing how “art” and “pharmaceuticals” are linguistically similar in web text</a></p>
<p>15 0.25566265 <a title="188-lsi-15" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-03-18-color_name_study_i_did.html">95 brendan oconnor ai-2008-03-18-color name study i did</a></p>
<p>16 0.23281425 <a title="188-lsi-16" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-06-17-%22Time_will_tell%2C_epistemology_won%E2%80%99t%22.html">65 brendan oconnor ai-2007-06-17-"Time will tell, epistemology won’t"</a></p>
<p>17 0.2273145 <a title="188-lsi-17" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-06-17-Pairwise_comparisons_for_relevance_evaluation.html">106 brendan oconnor ai-2008-06-17-Pairwise comparisons for relevance evaluation</a></p>
<p>18 0.21408653 <a title="188-lsi-18" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-08-Patches_to_Rainbow%2C_the_old_text_classifier_that_won%E2%80%99t_go_away.html">153 brendan oconnor ai-2009-09-08-Patches to Rainbow, the old text classifier that won’t go away</a></p>
<p>19 0.21363416 <a title="188-lsi-19" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-20-Quiz%3A_%E2%80%9Cart%E2%80%9D_and_%E2%80%9Cpharmaceuticals%E2%80%9D.html">155 brendan oconnor ai-2009-09-20-Quiz: “art” and “pharmaceuticals”</a></p>
<p>20 0.21189566 <a title="188-lsi-20" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-06-26-Good_linguistic_semantics_textbook%3F.html">172 brendan oconnor ai-2011-06-26-Good linguistic semantics textbook?</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/brendan_oconnor_ai_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(3, 0.011), (9, 0.015), (16, 0.035), (22, 0.018), (24, 0.02), (28, 0.015), (37, 0.011), (42, 0.288), (43, 0.023), (44, 0.121), (48, 0.084), (55, 0.022), (57, 0.018), (70, 0.067), (74, 0.153)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.90192205 <a title="188-lda-1" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-10-02-Powerset%E2%80%99s_natural_language_search_system.html">188 brendan oconnor ai-2012-10-02-Powerset’s natural language search system</a></p>
<p>Introduction: There’s a lot to say about  Powerset , the short-lived natural language search company (2005-2008) where I worked after college.  AI overhype, flying too close to the sun, the psychology of tech journalism and venture capitalism, etc.  A year or two ago I wrote the following bit about Powerset’s technology in response to a question  on Quora .  I’m posting a revised version here.
 
 Question:  What was Powerset’s core innovation in search?  As far as I can tell, they licensed an NLP engine. They did not have a question answering system or any system for information extraction. How was Powerset’s search engine different than Google’s?
 
 My answer:  Powerset built a system vaguely like a question-answering system on top of Xerox PARC’s NLP engine.  The output is better described as query-focused summarization rather than question answering; primarily, it matched semantic fragments of the user query against indexed semantic relations, with lots of keyword/ngram-matching fallback for when</p><p>2 0.55401284 <a title="188-lda-2" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-03-Statistics_vs._Machine_Learning%2C_fight%21.html">129 brendan oconnor ai-2008-12-03-Statistics vs. Machine Learning, fight!</a></p>
<p>Introduction: 10/1/09 update  — well, it’s been nearly a year, and I should say not everything in this rant is totally true, and I certainly believe much less of it now.  Current take:  Statistics , not machine learning, is the real deal, but unfortunately suffers from bad marketing.  On the other hand, to the extent that bad marketing includes misguided undergraduate curriculums, there’s plenty of room to improve for everyone.
   
So it’s pretty clear by now that statistics and machine learning aren’t very different fields. I was recently pointed to   a very amusing comparison   by the excellent statistician — and machine learning expert —   Robert Tibshiriani  .  Reproduced here:  
 
  Glossary 
   
 Machine learning
  Statistics 
   
 network, graphs
  model 
   
 weights
  parameters 
   
 learning
  fitting 
   
 generalization
  test set performance 
   
 supervised learning
  regression/classiﬁcation 
   
 unsupervised learning
  density estimation, clustering 
   
 large grant = $1,000,000</p><p>3 0.53189909 <a title="188-lda-3" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-09-19-End-to-end_NLP_packages.html">174 brendan oconnor ai-2011-09-19-End-to-end NLP packages</a></p>
<p>Introduction: What freely available end-to-end natural language processing (NLP) systems are out there, that start with raw text, and output parses and semantic structures?  Lots of NLP research focuses on single tasks at a time, and thus produces software that does a single task at a time.  But for various applications, it is nicer to have a full end-to-end system that just runs on whatever text you give it.
 
If you believe this is a worthwhile goal (see caveat at bottom), I will postulate there aren’t a ton of such end-to-end, multilevel systems.  Here are ones I can think of.  Corrections and clarifications welcome.
  
  Stanford CoreNLP .  Raw text to  rich syntactic dependencies  ( LFG -inspired).  Also POS, NER, coreference. 
  C&C; tools .  From (sentence-segmented, tokenized?) text to rich syntactic dependencies ( CCG -based) and also a semantic representation.  POS and chunks on the way.  Does anyone use this much?  It seems underappreciated relative to its richness. 
  Senna .  Sentence-se</p><p>4 0.51439977 <a title="188-lda-4" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-08-08-Haghighi_and_Klein_%282009%29%3A_Simple_Coreference_Resolution_with_Rich_Syntactic_and_Semantic_Features.html">150 brendan oconnor ai-2009-08-08-Haghighi and Klein (2009): Simple Coreference Resolution with Rich Syntactic and Semantic Features</a></p>
<p>Introduction: I haven’t done a paper review on this blog for a while, so here we go.
 
 Coreference  resolution is an interesting NLP problem.  ( Examples. )  It involves honest-to-goodness syntactic, semantic, and discourse phenomena, but still seems like a real cognitive task that humans have to solve when reading text [1].  I haven’t read the whole literature, but I’ve always been puzzled by the crop of papers on it I’ve seen in the last year or two.  There’s a big focus on fancy graph/probabilistic/constrained optimization algorithms, but often these papers gloss over the linguistic features — the core information they actually make their decisions with [2].  I never understood why the latter isn’t the most important issue.  Therefore, it was a joy to read
  
 Aria Haghighi and Dan Klein, EMNLP-2009.   “Simple Coreference Resolution with Rich Syntactic and Semantic Features.”  
  
They describe a simple, essentially non-statistical system that outperforms previous unsupervised systems, and compa</p><p>5 0.50704002 <a title="188-lda-5" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-03-15-Feminists%2C_anarchists%2C_computational_complexity%2C_bounded_rationality%2C_nethack%2C_and_other_things_to_do.html">53 brendan oconnor ai-2007-03-15-Feminists, anarchists, computational complexity, bounded rationality, nethack, and other things to do</a></p>
<p>Introduction: I was planning to write some  WordNet  lookup code tonight.  But instead I’ve learned of too many intersecting things.
 
First, there are a zillion things to do this weekend ( hooray flavorpill ):
 
  Picasso and American Art  exhibit continuing at  SFMOMA .  I saw it very briefly last weekend but want some more.  And Doug claims there’s an interesting photography exhibit there too.
  Reading from  We Don’t Need Another Wave: Dispatches from the Next Generation of Feminists , a fascinating looking book I’ve seen many times in the bookstores around here.  By that I mean at least Modern Times (the neat Mission bookstore) and the Anarchist Collective Bookstore (out on the Haight).  And the reading is at Modern Times,  just down the street  from my house!  Amazing.  Tomorrow at 7:30.
  Since anarchists were just mentioned, fortuitously there also appears: the  Bay Area Anarchist Bookfair  this Saturday and Sunday!  Speakers and books down by Golden Gate Park, oh my.  

 
Can’t say I’m a ra</p><p>6 0.5002144 <a title="188-lda-6" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-04-The_%2460%2C000_cat%3A_deep_belief_networks_make_less_sense_for_language_than_vision.html">184 brendan oconnor ai-2012-07-04-The $60,000 cat: deep belief networks make less sense for language than vision</a></p>
<p>7 0.49981326 <a title="188-lda-7" href="../brendan_oconnor_ai-2014/brendan_oconnor_ai-2014-02-19-What_the_ACL-2014_review_scores_mean.html">203 brendan oconnor ai-2014-02-19-What the ACL-2014 review scores mean</a></p>
<p>8 0.49650902 <a title="188-lda-8" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-09-13-Response_on_our_movie_personas_paper.html">200 brendan oconnor ai-2013-09-13-Response on our movie personas paper</a></p>
<p>9 0.49596125 <a title="188-lda-9" href="../brendan_oconnor_ai-2006/brendan_oconnor_ai-2006-08-30-A_big%2C_fun_list_of_links_I%E2%80%99m_reading.html">44 brendan oconnor ai-2006-08-30-A big, fun list of links I’m reading</a></p>
<p>10 0.49560744 <a title="188-lda-10" href="../brendan_oconnor_ai-2004/brendan_oconnor_ai-2004-11-24-addiction_%26_2_problems_of_economics.html">2 brendan oconnor ai-2004-11-24-addiction & 2 problems of economics</a></p>
<p>11 0.49104181 <a title="188-lda-11" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-11-12-Disease_tracking_with_web_queries_and_social_messaging_%28Google%2C_Twitter%2C_Facebook%E2%80%A6%29.html">123 brendan oconnor ai-2008-11-12-Disease tracking with web queries and social messaging (Google, Twitter, Facebook…)</a></p>
<p>12 0.48715657 <a title="188-lda-12" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-12-20-Data-driven_charity.html">86 brendan oconnor ai-2007-12-20-Data-driven charity</a></p>
<p>13 0.48535424 <a title="188-lda-13" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-17-1_billion_web_page_dataset_from_CMU.html">138 brendan oconnor ai-2009-04-17-1 billion web page dataset from CMU</a></p>
<p>14 0.47722328 <a title="188-lda-14" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-06-17-Pairwise_comparisons_for_relevance_evaluation.html">106 brendan oconnor ai-2008-06-17-Pairwise comparisons for relevance evaluation</a></p>
<p>15 0.47641408 <a title="188-lda-15" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-11-14-Pop_cog_neuro_is_so_sigh.html">82 brendan oconnor ai-2007-11-14-Pop cog neuro is so sigh</a></p>
<p>16 0.47625282 <a title="188-lda-16" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-08-20-Some_analysis_of_tweet_shares_and_%E2%80%9Cpredicting%E2%80%9D_election_outcomes.html">198 brendan oconnor ai-2013-08-20-Some analysis of tweet shares and “predicting” election outcomes</a></p>
<p>17 0.47395107 <a title="188-lda-17" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-05-18-Announcing_TweetMotif_for_summarizing_twitter_topics.html">140 brendan oconnor ai-2009-05-18-Announcing TweetMotif for summarizing twitter topics</a></p>
<p>18 0.46650821 <a title="188-lda-18" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-06-10-Freak-Freakonomics_%28Ariel_Rubinstein_is_the_shit%21%29.html">63 brendan oconnor ai-2007-06-10-Freak-Freakonomics (Ariel Rubinstein is the shit!)</a></p>
<p>19 0.46576071 <a title="188-lda-19" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-09-02-cognitive_modelling_is_rational_choice%2B%2B.html">26 brendan oconnor ai-2005-09-02-cognitive modelling is rational choice++</a></p>
<p>20 0.46001148 <a title="188-lda-20" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-11-21-Netflix_Prize.html">125 brendan oconnor ai-2008-11-21-Netflix Prize</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
