<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>196 brendan oconnor ai-2013-05-08-Movie summary corpus and learning character personas</title>
</head>

<body>
<p><a title="brendan_oconnor_ai" href="../brendan_oconnor_ai_home.html">brendan_oconnor_ai</a> <a title="brendan_oconnor_ai-2013" href="../home/brendan_oconnor_ai-2013_home.html">brendan_oconnor_ai-2013</a> <a title="brendan_oconnor_ai-2013-196" href="#">brendan_oconnor_ai-2013-196</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>196 brendan oconnor ai-2013-05-08-Movie summary corpus and learning character personas</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="brendan_oconnor_ai-2013-196-html" href="http://brenocon.com/blog/2013/05/movie-summary-corpus-and-learning-character-personas/">html</a></p><p>Introduction: Here is one of our exciting just-finished  ACL  papers.   David  and I designed an algorithm that learns different types of character personas — “Protagonist”, “Love Interest”, etc — that are used in movies.
 
To do this we collected a  brand new dataset : 42,306 plot summaries of movies from Wikipedia, along with metadata like box office revenue and genre.  We ran these through parsing and coreference analysis to also create a dataset of movie characters, linked with Freebase records of the actors who portray them.  Did you see that NYT article on  quantitative analysis of film scripts ?  This dataset could answer all sorts of things they assert in that article — for example, do movies with  bowling scenes  really make less money?  We have released  the data here .
 
Our focus, though, is on narrative analysis.  We investigate  character personas : familiar character types that are repeated over and over in stories, like “Hero” or “Villian”; maybe grand mythical archetypes like “Trick</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 David  and I designed an algorithm that learns different types of character personas — “Protagonist”, “Love Interest”, etc — that are used in movies. [sent-2, score-0.674]
</p><p>2 To do this we collected a  brand new dataset : 42,306 plot summaries of movies from Wikipedia, along with metadata like box office revenue and genre. [sent-3, score-1.044]
</p><p>3 We ran these through parsing and coreference analysis to also create a dataset of movie characters, linked with Freebase records of the actors who portray them. [sent-4, score-0.574]
</p><p>4 Did you see that NYT article on  quantitative analysis of film scripts ? [sent-5, score-0.297]
</p><p>5 This dataset could answer all sorts of things they assert in that article — for example, do movies with  bowling scenes  really make less money? [sent-6, score-0.527]
</p><p>6 They are defined in part by what they do and who they are — which we can glean from their actions and descriptions in plot summaries. [sent-10, score-0.38]
</p><p>7 Our model clusters movie characters, learning posteriors like this:             Each box is one automatically learned persona cluster, along with actions and attribute words that pertain to it. [sent-11, score-1.024]
</p><p>8 For example, characters like Dracula and The Joker are always “hatching” things (hatching plans, presumably). [sent-12, score-0.578]
</p><p>9 One of our models takes the metadata features, like movie genre and gender and age of an actor, and associates them with different personas. [sent-13, score-0.53]
</p><p>10 For example, we learn the types of characters in romantic comedies versus action movies. [sent-14, score-0.554]
</p><p>11 Here are a few examples of my favorite learned personas:         One of the best things I learned about during this project was the website  TVTropes  (which we use to compare our model against). [sent-15, score-0.515]
</p><p>12 We’ll be at  ACL  this summer to present the paper. [sent-16, score-0.15]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('characters', 0.411), ('personas', 0.246), ('movie', 0.21), ('character', 0.21), ('dataset', 0.207), ('acl', 0.198), ('hatching', 0.189), ('movies', 0.164), ('metadata', 0.164), ('learned', 0.143), ('types', 0.143), ('box', 0.14), ('film', 0.14), ('david', 0.12), ('actions', 0.115), ('plot', 0.115), ('along', 0.093), ('like', 0.086), ('actors', 0.082), ('killed', 0.082), ('attribute', 0.082), ('mythical', 0.082), ('connor', 0.082), ('scripts', 0.082), ('august', 0.082), ('pertain', 0.082), ('freebase', 0.082), ('bamman', 0.082), ('noah', 0.082), ('things', 0.081), ('present', 0.075), ('descriptions', 0.075), ('interest', 0.075), ('collected', 0.075), ('etc', 0.075), ('scenes', 0.075), ('defined', 0.075), ('quantitative', 0.075), ('records', 0.075), ('narrative', 0.075), ('wise', 0.075), ('summer', 0.075), ('brendan', 0.075), ('latent', 0.075), ('best', 0.075), ('model', 0.073), ('example', 0.073), ('age', 0.07), ('smith', 0.07), ('investigate', 0.07)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="196-tfidf-1" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-05-08-Movie_summary_corpus_and_learning_character_personas.html">196 brendan oconnor ai-2013-05-08-Movie summary corpus and learning character personas</a></p>
<p>Introduction: Here is one of our exciting just-finished  ACL  papers.   David  and I designed an algorithm that learns different types of character personas — “Protagonist”, “Love Interest”, etc — that are used in movies.
 
To do this we collected a  brand new dataset : 42,306 plot summaries of movies from Wikipedia, along with metadata like box office revenue and genre.  We ran these through parsing and coreference analysis to also create a dataset of movie characters, linked with Freebase records of the actors who portray them.  Did you see that NYT article on  quantitative analysis of film scripts ?  This dataset could answer all sorts of things they assert in that article — for example, do movies with  bowling scenes  really make less money?  We have released  the data here .
 
Our focus, though, is on narrative analysis.  We investigate  character personas : familiar character types that are repeated over and over in stories, like “Hero” or “Villian”; maybe grand mythical archetypes like “Trick</p><p>2 0.21709162 <a title="196-tfidf-2" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-09-13-Response_on_our_movie_personas_paper.html">200 brendan oconnor ai-2013-09-13-Response on our movie personas paper</a></p>
<p>Introduction: Update (2013-09-17):  See  David Bamman ‘s great  guest post on Language Log  on our latent personas paper, and the big picture of interdisciplinary collaboration.
   
I’ve been informed that an interesting critique of my, David Bamman’s and Noah Smith’s ACL paper on movie personas   has appeared on the Language Log, a guest post by Hannah Alpert-Abrams and Dan Garrette  .  I posted the following as a comment on LL.   
Thanks everyone for the interesting comments.  Scholarship is an ongoing conversation, and we hope our work might contribute to it.  Responding to the concerns about   our paper  , 
We did not try to make a contribution to contemporary literary theory.  Rather, we focus on developing a computational linguistic research method of analyzing characters in stories.  We hope there is a place for both the development of new research methods, as well as actual new substantive findings.  If you think about the tremendous possibilities for computer science and humanities collabor</p><p>3 0.14682123 <a title="196-tfidf-3" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-11-21-Netflix_Prize.html">125 brendan oconnor ai-2008-11-21-Netflix Prize</a></p>
<p>Introduction: Here’s  a fascinating NYT article on the Netflix Prize  for a better movie recommendation system.  Tons of great stuff there; here’s a few highlights …
 
First, a good unsupervised learning story:
  
There’s a sort of unsettling, alien quality to their computers’ results. When the teams examine the ways that singular value decomposition is slotting movies into categories, sometimes it makes sense to them — as when the computer highlights what appears to be some essence of nerdiness in a bunch of sci-fi movies. But many categorizations are now so obscure that they cannot see the reasoning behind them. Possibly the algorithms are finding connections so deep and subconscious that customers themselves wouldn’t even recognize them. At one point, Chabbert showed me a list of movies that his algorithm had discovered share some ineffable similarity; it includes a historical movie, “Joan of Arc,” a wrestling video, “W.W.E.: SummerSlam 2004,” the comedy “It Had to Be You” and a version of Charle</p><p>4 0.084113568 <a title="196-tfidf-4" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-09-21-CMU_ARK_Twitter_Part-of-Speech_Tagger_%E2%80%93_v0.3_released.html">187 brendan oconnor ai-2012-09-21-CMU ARK Twitter Part-of-Speech Tagger – v0.3 released</a></p>
<p>Introduction: We’re pleased to announce a new release of the CMU ARK Twitter Part-of-Speech 
Tagger, version 0.3.
  
 The new version is much faster (40x) and more accurate (89.2 -> 92.8) than 
  before.
  We also have released new POS-annotated data, including a dataset of one 
  tweet for each of 547 days.
  We have made available large-scale word clusters from unlabeled Twitter data 
  (217k words, 56m tweets, 847m tokens).
   
Tools, data, and a new technical report describing the release are available at: 
 www.ark.cs.cmu.edu/TweetNLP .
 
 0100100  a  1111100101110   111100000011 , Brendan</p><p>5 0.084052123 <a title="196-tfidf-5" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-04-16-Rise_and_fall_of_Dirichlet_process_clusters.html">194 brendan oconnor ai-2013-04-16-Rise and fall of Dirichlet process clusters</a></p>
<p>Introduction: Here’s Gibbs sampling for a  Dirichlet process 1-d mixture of Gaussians .  On 1000 data points that look like this.
 
   
 
I gave it fixed variance and a concentration and over MCMC iterations, and it looks like this.
 
   
 
The top is the number of points in a cluster.  The bottom are the cluster means.  Every cluster has a unique color.  During MCMC, clusters are created and destroyed.  Every cluster has a unique color; when a cluster dies, its color is never reused.  
 
I’m showing clusters every 100 iterations.  If there is a single point, that cluster was at that iteration but not before or after.  If there is a line, the cluster lived for at least 100 iterations.  Some clusters live long, some live short, but all eventually die.
 
Usually the model likes to think there are about two clusters, occupying positions at the two modes in the data distribution.  It also entertains the existence of several much more minor ones.  Usually these are shortlived clusters that die away.  But</p><p>6 0.074754469 <a title="196-tfidf-6" href="../brendan_oconnor_ai-2014/brendan_oconnor_ai-2014-02-19-What_the_ACL-2014_review_scores_mean.html">203 brendan oconnor ai-2014-02-19-What the ACL-2014 review scores mean</a></p>
<p>7 0.069312133 <a title="196-tfidf-7" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-05-21-iPhone_autocorrection_error_analysis.html">170 brendan oconnor ai-2011-05-21-iPhone autocorrection error analysis</a></p>
<p>8 0.063620456 <a title="196-tfidf-8" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-03-Statistics_vs._Machine_Learning%2C_fight%21.html">129 brendan oconnor ai-2008-12-03-Statistics vs. Machine Learning, fight!</a></p>
<p>9 0.062220573 <a title="196-tfidf-9" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-04-The_%2460%2C000_cat%3A_deep_belief_networks_make_less_sense_for_language_than_vision.html">184 brendan oconnor ai-2012-07-04-The $60,000 cat: deep belief networks make less sense for language than vision</a></p>
<p>10 0.05647672 <a title="196-tfidf-10" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-17-p-values%2C_CDF%E2%80%99s%2C_NLP_etc..html">185 brendan oconnor ai-2012-07-17-p-values, CDF’s, NLP etc.</a></p>
<p>11 0.056382403 <a title="196-tfidf-11" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-12-20-Data-driven_charity.html">86 brendan oconnor ai-2007-12-20-Data-driven charity</a></p>
<p>12 0.055792011 <a title="196-tfidf-12" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-08-08-Haghighi_and_Klein_%282009%29%3A_Simple_Coreference_Resolution_with_Rich_Syntactic_and_Semantic_Features.html">150 brendan oconnor ai-2009-08-08-Haghighi and Klein (2009): Simple Coreference Resolution with Rich Syntactic and Semantic Features</a></p>
<p>13 0.05544759 <a title="196-tfidf-13" href="../brendan_oconnor_ai-2006/brendan_oconnor_ai-2006-08-30-A_big%2C_fun_list_of_links_I%E2%80%99m_reading.html">44 brendan oconnor ai-2006-08-30-A big, fun list of links I’m reading</a></p>
<p>14 0.054578979 <a title="196-tfidf-14" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-11-21-The_Wire%3A_Mr._Nugget.html">126 brendan oconnor ai-2008-11-21-The Wire: Mr. Nugget</a></p>
<p>15 0.054469749 <a title="196-tfidf-15" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-01-31-Food_Fight.html">92 brendan oconnor ai-2008-01-31-Food Fight</a></p>
<p>16 0.053511318 <a title="196-tfidf-16" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-02-02-Histograms_%E2%80%94_matplotlib_vs._R.html">179 brendan oconnor ai-2012-02-02-Histograms — matplotlib vs. R</a></p>
<p>17 0.052571315 <a title="196-tfidf-17" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-06-14-How_much_text_versus_metadata_is_in_a_tweet%3F.html">171 brendan oconnor ai-2011-06-14-How much text versus metadata is in a tweet?</a></p>
<p>18 0.052096769 <a title="196-tfidf-18" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-02-19-Move_to_brenocon.com.html">165 brendan oconnor ai-2011-02-19-Move to brenocon.com</a></p>
<p>19 0.049658667 <a title="196-tfidf-19" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-02-23-Comparison_of_data_analysis_packages%3A_R%2C_Matlab%2C_SciPy%2C_Excel%2C_SAS%2C_SPSS%2C_Stata.html">135 brendan oconnor ai-2009-02-23-Comparison of data analysis packages: R, Matlab, SciPy, Excel, SAS, SPSS, Stata</a></p>
<p>20 0.049483888 <a title="196-tfidf-20" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-03-18-color_name_study_i_did.html">95 brendan oconnor ai-2008-03-18-color name study i did</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/brendan_oconnor_ai_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, -0.205), (1, -0.078), (2, 0.003), (3, 0.036), (4, 0.003), (5, 0.013), (6, -0.05), (7, 0.007), (8, -0.077), (9, 0.062), (10, -0.003), (11, 0.015), (12, 0.125), (13, 0.026), (14, 0.075), (15, -0.062), (16, 0.069), (17, -0.026), (18, -0.191), (19, 0.035), (20, 0.092), (21, -0.05), (22, 0.068), (23, 0.148), (24, 0.188), (25, -0.166), (26, -0.019), (27, -0.158), (28, 0.065), (29, -0.222), (30, -0.021), (31, -0.031), (32, 0.072), (33, 0.219), (34, 0.046), (35, 0.011), (36, 0.014), (37, 0.1), (38, 0.124), (39, 0.045), (40, 0.069), (41, -0.135), (42, -0.047), (43, 0.089), (44, 0.022), (45, 0.06), (46, 0.036), (47, 0.12), (48, 0.01), (49, -0.003)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98229879 <a title="196-lsi-1" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-05-08-Movie_summary_corpus_and_learning_character_personas.html">196 brendan oconnor ai-2013-05-08-Movie summary corpus and learning character personas</a></p>
<p>Introduction: Here is one of our exciting just-finished  ACL  papers.   David  and I designed an algorithm that learns different types of character personas — “Protagonist”, “Love Interest”, etc — that are used in movies.
 
To do this we collected a  brand new dataset : 42,306 plot summaries of movies from Wikipedia, along with metadata like box office revenue and genre.  We ran these through parsing and coreference analysis to also create a dataset of movie characters, linked with Freebase records of the actors who portray them.  Did you see that NYT article on  quantitative analysis of film scripts ?  This dataset could answer all sorts of things they assert in that article — for example, do movies with  bowling scenes  really make less money?  We have released  the data here .
 
Our focus, though, is on narrative analysis.  We investigate  character personas : familiar character types that are repeated over and over in stories, like “Hero” or “Villian”; maybe grand mythical archetypes like “Trick</p><p>2 0.66182065 <a title="196-lsi-2" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-09-13-Response_on_our_movie_personas_paper.html">200 brendan oconnor ai-2013-09-13-Response on our movie personas paper</a></p>
<p>Introduction: Update (2013-09-17):  See  David Bamman ‘s great  guest post on Language Log  on our latent personas paper, and the big picture of interdisciplinary collaboration.
   
I’ve been informed that an interesting critique of my, David Bamman’s and Noah Smith’s ACL paper on movie personas   has appeared on the Language Log, a guest post by Hannah Alpert-Abrams and Dan Garrette  .  I posted the following as a comment on LL.   
Thanks everyone for the interesting comments.  Scholarship is an ongoing conversation, and we hope our work might contribute to it.  Responding to the concerns about   our paper  , 
We did not try to make a contribution to contemporary literary theory.  Rather, we focus on developing a computational linguistic research method of analyzing characters in stories.  We hope there is a place for both the development of new research methods, as well as actual new substantive findings.  If you think about the tremendous possibilities for computer science and humanities collabor</p><p>3 0.49944627 <a title="196-lsi-3" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-11-21-Netflix_Prize.html">125 brendan oconnor ai-2008-11-21-Netflix Prize</a></p>
<p>Introduction: Here’s  a fascinating NYT article on the Netflix Prize  for a better movie recommendation system.  Tons of great stuff there; here’s a few highlights …
 
First, a good unsupervised learning story:
  
There’s a sort of unsettling, alien quality to their computers’ results. When the teams examine the ways that singular value decomposition is slotting movies into categories, sometimes it makes sense to them — as when the computer highlights what appears to be some essence of nerdiness in a bunch of sci-fi movies. But many categorizations are now so obscure that they cannot see the reasoning behind them. Possibly the algorithms are finding connections so deep and subconscious that customers themselves wouldn’t even recognize them. At one point, Chabbert showed me a list of movies that his algorithm had discovered share some ineffable similarity; it includes a historical movie, “Joan of Arc,” a wrestling video, “W.W.E.: SummerSlam 2004,” the comedy “It Had to Be You” and a version of Charle</p><p>4 0.47259596 <a title="196-lsi-4" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-11-26-How_did_Freud_become_a_respected_humanist%3F%21.html">84 brendan oconnor ai-2007-11-26-How did Freud become a respected humanist?!</a></p>
<p>Introduction: Freud Is Widely Taught at Universities, Except in the Psychology Department :
  
PSYCHOANALYSIS and its ideas about the unconscious mind have spread to every nook and cranny of the culture from Salinger to “South Park,” from Fellini to foreign policy. Yet if you want to learn about psychoanalysis at the nation’s top universities, one of the last places to look may be the psychology department.


A new report by the American Psychoanalytic Association has found that while psychoanalysis — or what purports to be psychoanalysis — is alive and well in literature, film, history and just about every other subject in the humanities, psychology departments and textbooks treat it as “desiccated and dead,” a historical artifact instead of “an ongoing movement and a living, evolving process.”
  
I’ve been wondering about this for a while, ever since I heard someone describe Freud as “one of the greatest humanists who ever lived.”  I’m pretty sure he didn’t think of himself that way.  If you’re a</p><p>5 0.4441241 <a title="196-lsi-5" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-04-16-Rise_and_fall_of_Dirichlet_process_clusters.html">194 brendan oconnor ai-2013-04-16-Rise and fall of Dirichlet process clusters</a></p>
<p>Introduction: Here’s Gibbs sampling for a  Dirichlet process 1-d mixture of Gaussians .  On 1000 data points that look like this.
 
   
 
I gave it fixed variance and a concentration and over MCMC iterations, and it looks like this.
 
   
 
The top is the number of points in a cluster.  The bottom are the cluster means.  Every cluster has a unique color.  During MCMC, clusters are created and destroyed.  Every cluster has a unique color; when a cluster dies, its color is never reused.  
 
I’m showing clusters every 100 iterations.  If there is a single point, that cluster was at that iteration but not before or after.  If there is a line, the cluster lived for at least 100 iterations.  Some clusters live long, some live short, but all eventually die.
 
Usually the model likes to think there are about two clusters, occupying positions at the two modes in the data distribution.  It also entertains the existence of several much more minor ones.  Usually these are shortlived clusters that die away.  But</p><p>6 0.42340642 <a title="196-lsi-6" href="../brendan_oconnor_ai-2014/brendan_oconnor_ai-2014-02-19-What_the_ACL-2014_review_scores_mean.html">203 brendan oconnor ai-2014-02-19-What the ACL-2014 review scores mean</a></p>
<p>7 0.34223002 <a title="196-lsi-7" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-11-28-Calculating_running_variance_in_Python_and_C%2B%2B.html">128 brendan oconnor ai-2008-11-28-Calculating running variance in Python and C++</a></p>
<p>8 0.33216923 <a title="196-lsi-8" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-03-18-color_name_study_i_did.html">95 brendan oconnor ai-2008-03-18-color name study i did</a></p>
<p>9 0.32205665 <a title="196-lsi-9" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-01-02-funny_comic.html">48 brendan oconnor ai-2007-01-02-funny comic</a></p>
<p>10 0.32027063 <a title="196-lsi-10" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-22-Performance_comparison%3A_key-value_stores_for_language_model_counts.html">139 brendan oconnor ai-2009-04-22-Performance comparison: key-value stores for language model counts</a></p>
<p>11 0.31580675 <a title="196-lsi-11" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-05-21-iPhone_autocorrection_error_analysis.html">170 brendan oconnor ai-2011-05-21-iPhone autocorrection error analysis</a></p>
<p>12 0.31308675 <a title="196-lsi-12" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-04-13-Are_women_discriminated_against_in_graduate_admissions%3F_Simpson%E2%80%99s_paradox_via_R_in_three_easy_steps%21.html">101 brendan oconnor ai-2008-04-13-Are women discriminated against in graduate admissions? Simpson’s paradox via R in three easy steps!</a></p>
<p>13 0.31214288 <a title="196-lsi-13" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-01-31-Food_Fight.html">92 brendan oconnor ai-2008-01-31-Food Fight</a></p>
<p>14 0.30079716 <a title="196-lsi-14" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-08-21-Berkeley_SDA_and_the_General_Social_Survey.html">186 brendan oconnor ai-2012-08-21-Berkeley SDA and the General Social Survey</a></p>
<p>15 0.2962341 <a title="196-lsi-15" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-12-20-Data-driven_charity.html">86 brendan oconnor ai-2007-12-20-Data-driven charity</a></p>
<p>16 0.29580522 <a title="196-lsi-16" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-03-14-R_scan%28%29_for_quick-and-dirty_checks.html">192 brendan oconnor ai-2013-03-14-R scan() for quick-and-dirty checks</a></p>
<p>17 0.29048705 <a title="196-lsi-17" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-04-The_%2460%2C000_cat%3A_deep_belief_networks_make_less_sense_for_language_than_vision.html">184 brendan oconnor ai-2012-07-04-The $60,000 cat: deep belief networks make less sense for language than vision</a></p>
<p>18 0.2826739 <a title="196-lsi-18" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-11-21-The_Wire%3A_Mr._Nugget.html">126 brendan oconnor ai-2008-11-21-The Wire: Mr. Nugget</a></p>
<p>19 0.27740473 <a title="196-lsi-19" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-07-01-Modelling_environmentalism_thinking.html">11 brendan oconnor ai-2005-07-01-Modelling environmentalism thinking</a></p>
<p>20 0.26375219 <a title="196-lsi-20" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-07-08-Game_outcome_graphs_%E2%80%94_prisoner%E2%80%99s_dilemma_with_FUN_ARROWS%21%21%21.html">68 brendan oconnor ai-2007-07-08-Game outcome graphs — prisoner’s dilemma with FUN ARROWS!!!</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/brendan_oconnor_ai_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(16, 0.014), (24, 0.024), (44, 0.109), (55, 0.028), (57, 0.035), (70, 0.644), (74, 0.042), (96, 0.01)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.98500156 <a title="196-lda-1" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-06-25-1st_International_Conference_on_Computational_Models_of_Argument_%28COMMA06%29.html">5 brendan oconnor ai-2005-06-25-1st International Conference on Computational Models of Argument (COMMA06)</a></p>
<p>Introduction: does this look awesome or what? Imagine if you could computationally model the argumentation and communication in economic and political behavior. To say nothing of the AI applications too!
  
PRELIMINARY ANNOUNCEMENT 1st International Conference on Computational Models of Argument (COMMA06)


Organised by the ASPIC project (www.argumentation.org) The University of Liverpool, Liverpool, UK 11th-12th September 2006 (provisional)


General Chair: Professor Michael J. Wooldridge Programme Chair: Paul E. Dunne


Over the past decade argumentation has become increasingly important in Artificial Intelligence. It has provided a fruitful way of approaching non-monotonic and defeasible reasoning, deliberation about action, and agent communication scenarios such as negotiation. In application domains such as law, medicine and e-democracy it has come to be seen as an essential part of the reasoning.


Successful workshops have been associated with major Artificial Intelligence Conferences, notabl</p><p>2 0.96831161 <a title="196-lda-2" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-10-08-MyDebates.org%2C_online_polling%2C_and_potentially_the_coolest_question_corpus_ever.html">116 brendan oconnor ai-2008-10-08-MyDebates.org, online polling, and potentially the coolest question corpus ever</a></p>
<p>Introduction: MySpace and the  Commission on the Presidential Debates  put together a neat site,  mydebates.org , which presents the candidates’ positions through various mini-polls and such.  It even has a cool data exploration tool for the poll results … for example, here are two support maps, one for respondents over 65 and one for 18-24 year olds.
 
   
 
    Anyway, the site also takes submissions of questions for tonight’s debate.  Apparently  six million  questions were submitted, and moderator Tom Brokaw will of course use only 10 or so.  This begs a question, how were they selected?  There’s no Digg-like social filtering or anything.  You could imagine automatic methods to help narrow down the pool: Topic clustering?  Quality ranking on syntax and vocabulary?
 
 Eric Fish  suggested the obvious: probably someone picked 1000 randomly and sent them to Brokaw.
 
I’d love to see a corpus of 6 million questions on U.S. political subjects, directed at only two different people.  Anyone know anyon</p><p>same-blog 3 0.966048 <a title="196-lda-3" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-05-08-Movie_summary_corpus_and_learning_character_personas.html">196 brendan oconnor ai-2013-05-08-Movie summary corpus and learning character personas</a></p>
<p>Introduction: Here is one of our exciting just-finished  ACL  papers.   David  and I designed an algorithm that learns different types of character personas — “Protagonist”, “Love Interest”, etc — that are used in movies.
 
To do this we collected a  brand new dataset : 42,306 plot summaries of movies from Wikipedia, along with metadata like box office revenue and genre.  We ran these through parsing and coreference analysis to also create a dataset of movie characters, linked with Freebase records of the actors who portray them.  Did you see that NYT article on  quantitative analysis of film scripts ?  This dataset could answer all sorts of things they assert in that article — for example, do movies with  bowling scenes  really make less money?  We have released  the data here .
 
Our focus, though, is on narrative analysis.  We investigate  character personas : familiar character types that are repeated over and over in stories, like “Hero” or “Villian”; maybe grand mythical archetypes like “Trick</p><p>4 0.83535588 <a title="196-lda-4" href="../brendan_oconnor_ai-2006/brendan_oconnor_ai-2006-08-30-A_big%2C_fun_list_of_links_I%E2%80%99m_reading.html">44 brendan oconnor ai-2006-08-30-A big, fun list of links I’m reading</a></p>
<p>Introduction: Since blogging is hard, but reading is easy, lately I’ve taken to bookmarking interesting articles I’m reading, with the plan of blogging about them later.  This follow-through has happened a few times, but not that often.  In an amazing moment of thesis procrastination, today I sat down and figured out how to turn my  del.icio.us bookmarks  into a nice blogpost, with the plan that every week a post will appear with links I’ve recently read, or maybe I’ll use the script to generate a draft for myself that I’ll revise, or something.
 
But for this first such link post, I put in a whole bunch of them beyond just the last week — why have just a few when you could have *all* of them?  Future link posts will be shorter, I promise.
 
  Ariel Rubinstein: Freak-Freakonomics    July 2006   posted 8/19 under  economics   sarcastic, critical review of levitt & dubner’s Freakonomics 
  New Yorker review of Philip Tetlock’s book on political expert judgment   posted 8/19 under  judgment ,  psycholo</p><p>5 0.40857482 <a title="196-lda-5" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-09-13-Response_on_our_movie_personas_paper.html">200 brendan oconnor ai-2013-09-13-Response on our movie personas paper</a></p>
<p>Introduction: Update (2013-09-17):  See  David Bamman ‘s great  guest post on Language Log  on our latent personas paper, and the big picture of interdisciplinary collaboration.
   
I’ve been informed that an interesting critique of my, David Bamman’s and Noah Smith’s ACL paper on movie personas   has appeared on the Language Log, a guest post by Hannah Alpert-Abrams and Dan Garrette  .  I posted the following as a comment on LL.   
Thanks everyone for the interesting comments.  Scholarship is an ongoing conversation, and we hope our work might contribute to it.  Responding to the concerns about   our paper  , 
We did not try to make a contribution to contemporary literary theory.  Rather, we focus on developing a computational linguistic research method of analyzing characters in stories.  We hope there is a place for both the development of new research methods, as well as actual new substantive findings.  If you think about the tremendous possibilities for computer science and humanities collabor</p><p>6 0.38391834 <a title="196-lda-6" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-06-25-more_argumentation_%26_AI-formal_modelling_links.html">8 brendan oconnor ai-2005-06-25-more argumentation & AI-formal modelling links</a></p>
<p>7 0.36696264 <a title="196-lda-7" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-10-11-It_is_accurate_to_determine_a_blog%E2%80%99s_bias_by_what_it_links_to.html">117 brendan oconnor ai-2008-10-11-It is accurate to determine a blog’s bias by what it links to</a></p>
<p>8 0.34991592 <a title="196-lda-8" href="../brendan_oconnor_ai-2014/brendan_oconnor_ai-2014-04-26-Replot%3A_departure_delays_vs_flight_time_speed-up.html">204 brendan oconnor ai-2014-04-26-Replot: departure delays vs flight time speed-up</a></p>
<p>9 0.34217343 <a title="196-lda-9" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-26-Seeing_how_%E2%80%9Cart%E2%80%9D_and_%E2%80%9Cpharmaceuticals%E2%80%9D_are_linguistically_similar_in_web_text.html">156 brendan oconnor ai-2009-09-26-Seeing how “art” and “pharmaceuticals” are linguistically similar in web text</a></p>
<p>10 0.33877578 <a title="196-lda-10" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-10-02-Powerset%E2%80%99s_natural_language_search_system.html">188 brendan oconnor ai-2012-10-02-Powerset’s natural language search system</a></p>
<p>11 0.33653733 <a title="196-lda-11" href="../brendan_oconnor_ai-2006/brendan_oconnor_ai-2006-03-26-new_kind_of_science%2C_for_real.html">32 brendan oconnor ai-2006-03-26-new kind of science, for real</a></p>
<p>12 0.30467591 <a title="196-lda-12" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-05-18-Announcing_TweetMotif_for_summarizing_twitter_topics.html">140 brendan oconnor ai-2009-05-18-Announcing TweetMotif for summarizing twitter topics</a></p>
<p>13 0.29724285 <a title="196-lda-13" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-11-24-Graphs_for_SANCL-2012_web_parsing_results.html">189 brendan oconnor ai-2012-11-24-Graphs for SANCL-2012 web parsing results</a></p>
<p>14 0.27298647 <a title="196-lda-14" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-01-20-Moral_psychology_on_Amazon_Mechanical_Turk.html">90 brendan oconnor ai-2008-01-20-Moral psychology on Amazon Mechanical Turk</a></p>
<p>15 0.26928601 <a title="196-lda-15" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-03-Statistics_vs._Machine_Learning%2C_fight%21.html">129 brendan oconnor ai-2008-12-03-Statistics vs. Machine Learning, fight!</a></p>
<p>16 0.26774114 <a title="196-lda-16" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-04-The_%2460%2C000_cat%3A_deep_belief_networks_make_less_sense_for_language_than_vision.html">184 brendan oconnor ai-2012-07-04-The $60,000 cat: deep belief networks make less sense for language than vision</a></p>
<p>17 0.26612037 <a title="196-lda-17" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-07-11-guns%2C_germs%2C_%26_steel_pbs_show%3F%21.html">20 brendan oconnor ai-2005-07-11-guns, germs, & steel pbs show?!</a></p>
<p>18 0.2658307 <a title="196-lda-18" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-01-23-SF_conference_for_data_mining_mercenaries.html">133 brendan oconnor ai-2009-01-23-SF conference for data mining mercenaries</a></p>
<p>19 0.25908202 <a title="196-lda-19" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-08-16-A_better_Obama_vs_McCain_poll_aggregation.html">111 brendan oconnor ai-2008-08-16-A better Obama vs McCain poll aggregation</a></p>
<p>20 0.25754964 <a title="196-lda-20" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-11-15-Actually_that_2008_elections_voter_fMRI_study_is_batshit_insane_%28and_sleazy_too%29.html">83 brendan oconnor ai-2007-11-15-Actually that 2008 elections voter fMRI study is batshit insane (and sleazy too)</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
