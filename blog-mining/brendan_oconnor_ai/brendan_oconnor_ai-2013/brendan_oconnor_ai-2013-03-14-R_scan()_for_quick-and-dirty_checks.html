<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>192 brendan oconnor ai-2013-03-14-R scan() for quick-and-dirty checks</title>
</head>

<body>
<p><a title="brendan_oconnor_ai" href="../brendan_oconnor_ai_home.html">brendan_oconnor_ai</a> <a title="brendan_oconnor_ai-2013" href="../home/brendan_oconnor_ai-2013_home.html">brendan_oconnor_ai-2013</a> <a title="brendan_oconnor_ai-2013-192" href="#">brendan_oconnor_ai-2013-192</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>192 brendan oconnor ai-2013-03-14-R scan() for quick-and-dirty checks</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="brendan_oconnor_ai-2013-192-html" href="http://brenocon.com/blog/2013/03/r-scan-for-quick-and-dirty-checks/">html</a></p><p>Introduction: One of my favorite R tricks is  scan() .  I was using it to verify whether I wrote a sampler recently, which was supposed to output numbers uniformly between 1 and 100 into a logfile; this loads the logfile, counts the different outcomes, and plots.
 
 plot(table(scan(“log”))) 
 
As the logfile was growing, I kept replotting it and found it oddly compelling.
 
 
 
This was useful: in fact, an early version had an off-by-one bug, immediately obvious  from the plot .  And of course,  chisq.test(table(scan(“log”)))  does a null-hypothesis to check uniformity.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 I was using it to verify whether I wrote a sampler recently, which was supposed to output numbers uniformly between 1 and 100 into a logfile; this loads the logfile, counts the different outcomes, and plots. [sent-2, score-1.048]
</p><p>2 plot(table(scan(“log”)))    As the logfile was growing, I kept replotting it and found it oddly compelling. [sent-3, score-0.782]
</p><p>3 This was useful: in fact, an early version had an off-by-one bug, immediately obvious  from the plot . [sent-4, score-0.712]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('logfile', 0.534), ('scan', 0.454), ('plot', 0.249), ('log', 0.225), ('table', 0.196), ('verify', 0.178), ('kept', 0.178), ('bug', 0.162), ('tricks', 0.151), ('growing', 0.143), ('sampler', 0.143), ('immediately', 0.136), ('favorite', 0.136), ('counts', 0.136), ('supposed', 0.13), ('outcomes', 0.125), ('early', 0.125), ('obvious', 0.116), ('check', 0.112), ('output', 0.109), ('numbers', 0.1), ('recently', 0.091), ('version', 0.086), ('fact', 0.078), ('found', 0.07), ('course', 0.07), ('wrote', 0.069), ('using', 0.067), ('useful', 0.066), ('whether', 0.065), ('different', 0.051), ('one', 0.034)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="192-tfidf-1" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-03-14-R_scan%28%29_for_quick-and-dirty_checks.html">192 brendan oconnor ai-2013-03-14-R scan() for quick-and-dirty checks</a></p>
<p>Introduction: One of my favorite R tricks is  scan() .  I was using it to verify whether I wrote a sampler recently, which was supposed to output numbers uniformly between 1 and 100 into a logfile; this loads the logfile, counts the different outcomes, and plots.
 
 plot(table(scan(“log”))) 
 
As the logfile was growing, I kept replotting it and found it oddly compelling.
 
 
 
This was useful: in fact, an early version had an off-by-one bug, immediately obvious  from the plot .  And of course,  chisq.test(table(scan(“log”)))  does a null-hypothesis to check uniformity.</p><p>2 0.11866131 <a title="192-tfidf-2" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-07-25-Cerealitivity.html">70 brendan oconnor ai-2007-07-25-Cerealitivity</a></p>
<p>Introduction: This is pretty funny, an old cartoon reprinted on  Language Log .</p><p>3 0.079406515 <a title="192-tfidf-3" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-01-07-Perplexity_as_branching_factor%3B_as_Shannon_diversity_index.html">190 brendan oconnor ai-2013-01-07-Perplexity as branching factor; as Shannon diversity index</a></p>
<p>Introduction: A language model’s  perplexity  is exponentiated negative average log-likelihood,
 
$$\exp( -\frac{1}{N} \log(p(x)))$$
 
Where the inner term usually decomposes into a sum over individual items; for example, as \(\sum_i \log p(x_i | x_1..x_{i-1})\) or \(\sum_i \log p(x_i)\) depending on independence assumptions, where for language modeling word tokens are usually taken as the individual units.  (In which case it is the geometric mean of per-token negative log-likelihoods.)   It’s equivalent to exponentiated cross-entropy between the model and the empirical data distribution, since \(-1/N \sum_i^N \log p(x_i) = -\sum_k^K \hat{p}_k \log p_k = H(\hat{p};p)\) where \(N\) is the number of items and \(K\) is the number of discrete classes (e.g. word types for language modeling) and \(\hat{p}_k\) is the proportion of data having class \(k\).
 
A nice interpretation of any exponentiated entropy measure is as branching factor: entropy measures uncertainty in bits or nats, but in exponentiated f</p><p>4 0.079054669 <a title="192-tfidf-4" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-09-25-Information_theory_stuff.html">175 brendan oconnor ai-2011-09-25-Information theory stuff</a></p>
<p>Introduction: Actually this post is mainly to test the  MathJax  installation I put into WordPress via  this plugin .  But  information theory  is great, why not?
 
The probability of a symbol is \(p\).
 
It takes \(\log \frac{1}{p} = -\log p\) bits to encode one symbol — sometimes called its “surprisal”.  Surprisal is 0 for a 100% probable symbol, and ranges up to \(\infty\) for extremely low probability symbols.  This is because you use a coding scheme that encodes common symbols as very short strings, and less common symbols as longer ones.  (e.g.  Huffman  or  arithmetic  coding.)  We should say logarithms are base-2 so information is measured in bits.\(^*\)
 
If you have a stream of such symbols and a probability distribution \(\vec{p}\) for them, where a symbol \(i\) comes at probability \(p_i\), then the average message size is the expected surprisal:
 
\[ H(\vec{p}) = \sum_i p_i \log \frac{1}{p_i} \]
 
this is the Shannon  entropy  of the probability distribution \( \vec{p} \), which is a me</p><p>5 0.073849268 <a title="192-tfidf-5" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-09-07-Kurzweil_interview.html">27 brendan oconnor ai-2005-09-07-Kurzweil interview</a></p>
<p>Introduction: Ray Kurzweil interviewed  on his new book,  The Singularity Is Near .  Good points on neuroscience, artificial intelligence, nanotech and the like.  But man, I thought  Age of Spiritual Machines  was a bit wacky…  Complete model of the human brain by 2030?  Please.  (Though the observation that brain scan resolutions are doubling yearly is interesting.)
 
I like the discussion about the relationship of power and intelligence of orgnizations.  Thinking about Kurzweil’s bizarre-sounding scenarios is good because in his world, humans and organizations start becoming the same thing… which leads to insights on the intelligence of normal organizations today.</p><p>6 0.072304741 <a title="192-tfidf-6" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-04-08-Rough_binomial_confidence_intervals.html">167 brendan oconnor ai-2011-04-08-Rough binomial confidence intervals</a></p>
<p>7 0.059728257 <a title="192-tfidf-7" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-02-02-Histograms_%E2%80%94_matplotlib_vs._R.html">179 brendan oconnor ai-2012-02-02-Histograms — matplotlib vs. R</a></p>
<p>8 0.055123147 <a title="192-tfidf-8" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-04-13-Are_women_discriminated_against_in_graduate_admissions%3F_Simpson%E2%80%99s_paradox_via_R_in_three_easy_steps%21.html">101 brendan oconnor ai-2008-04-13-Are women discriminated against in graduate admissions? Simpson’s paradox via R in three easy steps!</a></p>
<p>9 0.053450175 <a title="192-tfidf-9" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-07-08-Game_outcome_graphs_%E2%80%94_prisoner%E2%80%99s_dilemma_with_FUN_ARROWS%21%21%21.html">68 brendan oconnor ai-2007-07-08-Game outcome graphs — prisoner’s dilemma with FUN ARROWS!!!</a></p>
<p>10 0.053076483 <a title="192-tfidf-10" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-11-11-Memorizing_small_tables.html">177 brendan oconnor ai-2011-11-11-Memorizing small tables</a></p>
<p>11 0.052269652 <a title="192-tfidf-11" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-05-24-Zipf%E2%80%99s_law_and_world_city_populations.html">141 brendan oconnor ai-2009-05-24-Zipf’s law and world city populations</a></p>
<p>12 0.048175424 <a title="192-tfidf-12" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-11-13-Bayes_update_view_of_pointwise_mutual_information.html">178 brendan oconnor ai-2011-11-13-Bayes update view of pointwise mutual information</a></p>
<p>13 0.045844287 <a title="192-tfidf-13" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-02-19-Move_to_brenocon.com.html">165 brendan oconnor ai-2011-02-19-Move to brenocon.com</a></p>
<p>14 0.045061059 <a title="192-tfidf-14" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-11-15-Actually_that_2008_elections_voter_fMRI_study_is_batshit_insane_%28and_sleazy_too%29.html">83 brendan oconnor ai-2007-11-15-Actually that 2008 elections voter fMRI study is batshit insane (and sleazy too)</a></p>
<p>15 0.043407358 <a title="192-tfidf-15" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-04-16-Rise_and_fall_of_Dirichlet_process_clusters.html">194 brendan oconnor ai-2013-04-16-Rise and fall of Dirichlet process clusters</a></p>
<p>16 0.041749481 <a title="192-tfidf-16" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-05-08-Movie_summary_corpus_and_learning_character_personas.html">196 brendan oconnor ai-2013-05-08-Movie summary corpus and learning character personas</a></p>
<p>17 0.039818633 <a title="192-tfidf-17" href="../brendan_oconnor_ai-2010/brendan_oconnor_ai-2010-03-31-How_Facebook_privacy_failed_me.html">158 brendan oconnor ai-2010-03-31-How Facebook privacy failed me</a></p>
<p>18 0.038954176 <a title="192-tfidf-18" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-01-Binary_classification_evaluation_in_R_via_ROCR.html">136 brendan oconnor ai-2009-04-01-Binary classification evaluation in R via ROCR</a></p>
<p>19 0.037768949 <a title="192-tfidf-19" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-09-13-Response_on_our_movie_personas_paper.html">200 brendan oconnor ai-2013-09-13-Response on our movie personas paper</a></p>
<p>20 0.037468452 <a title="192-tfidf-20" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-11-21-The_Wire%3A_Mr._Nugget.html">126 brendan oconnor ai-2008-11-21-The Wire: Mr. Nugget</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/brendan_oconnor_ai_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, -0.1), (1, -0.071), (2, 0.016), (3, -0.009), (4, -0.077), (5, 0.169), (6, 0.116), (7, -0.148), (8, -0.056), (9, 0.094), (10, 0.071), (11, 0.023), (12, 0.041), (13, -0.003), (14, 0.105), (15, 0.01), (16, -0.016), (17, 0.046), (18, -0.048), (19, 0.02), (20, -0.04), (21, -0.066), (22, 0.035), (23, -0.014), (24, 0.068), (25, 0.035), (26, -0.031), (27, 0.015), (28, 0.05), (29, -0.069), (30, 0.056), (31, 0.11), (32, 0.109), (33, 0.135), (34, -0.091), (35, -0.048), (36, 0.079), (37, -0.02), (38, -0.001), (39, 0.036), (40, 0.048), (41, -0.138), (42, 0.012), (43, -0.002), (44, 0.1), (45, -0.205), (46, -0.146), (47, -0.03), (48, -0.069), (49, 0.013)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99583411 <a title="192-lsi-1" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-03-14-R_scan%28%29_for_quick-and-dirty_checks.html">192 brendan oconnor ai-2013-03-14-R scan() for quick-and-dirty checks</a></p>
<p>Introduction: One of my favorite R tricks is  scan() .  I was using it to verify whether I wrote a sampler recently, which was supposed to output numbers uniformly between 1 and 100 into a logfile; this loads the logfile, counts the different outcomes, and plots.
 
 plot(table(scan(“log”))) 
 
As the logfile was growing, I kept replotting it and found it oddly compelling.
 
 
 
This was useful: in fact, an early version had an off-by-one bug, immediately obvious  from the plot .  And of course,  chisq.test(table(scan(“log”)))  does a null-hypothesis to check uniformity.</p><p>2 0.55363309 <a title="192-lsi-2" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-04-08-Rough_binomial_confidence_intervals.html">167 brendan oconnor ai-2011-04-08-Rough binomial confidence intervals</a></p>
<p>Introduction: I made this table a while ago and find it handy: for example, looking at a table of percentages and trying to figure out what’s meaningful or not. Why run a test if you can estimate it in your head?
 
 
 
References:Â  Wikipedia ,Â  binom.test</p><p>3 0.47614357 <a title="192-lsi-3" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-01-07-Perplexity_as_branching_factor%3B_as_Shannon_diversity_index.html">190 brendan oconnor ai-2013-01-07-Perplexity as branching factor; as Shannon diversity index</a></p>
<p>Introduction: A language model’s  perplexity  is exponentiated negative average log-likelihood,
 
$$\exp( -\frac{1}{N} \log(p(x)))$$
 
Where the inner term usually decomposes into a sum over individual items; for example, as \(\sum_i \log p(x_i | x_1..x_{i-1})\) or \(\sum_i \log p(x_i)\) depending on independence assumptions, where for language modeling word tokens are usually taken as the individual units.  (In which case it is the geometric mean of per-token negative log-likelihoods.)   It’s equivalent to exponentiated cross-entropy between the model and the empirical data distribution, since \(-1/N \sum_i^N \log p(x_i) = -\sum_k^K \hat{p}_k \log p_k = H(\hat{p};p)\) where \(N\) is the number of items and \(K\) is the number of discrete classes (e.g. word types for language modeling) and \(\hat{p}_k\) is the proportion of data having class \(k\).
 
A nice interpretation of any exponentiated entropy measure is as branching factor: entropy measures uncertainty in bits or nats, but in exponentiated f</p><p>4 0.47180632 <a title="192-lsi-4" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-09-25-Information_theory_stuff.html">175 brendan oconnor ai-2011-09-25-Information theory stuff</a></p>
<p>Introduction: Actually this post is mainly to test the  MathJax  installation I put into WordPress via  this plugin .  But  information theory  is great, why not?
 
The probability of a symbol is \(p\).
 
It takes \(\log \frac{1}{p} = -\log p\) bits to encode one symbol — sometimes called its “surprisal”.  Surprisal is 0 for a 100% probable symbol, and ranges up to \(\infty\) for extremely low probability symbols.  This is because you use a coding scheme that encodes common symbols as very short strings, and less common symbols as longer ones.  (e.g.  Huffman  or  arithmetic  coding.)  We should say logarithms are base-2 so information is measured in bits.\(^*\)
 
If you have a stream of such symbols and a probability distribution \(\vec{p}\) for them, where a symbol \(i\) comes at probability \(p_i\), then the average message size is the expected surprisal:
 
\[ H(\vec{p}) = \sum_i p_i \log \frac{1}{p_i} \]
 
this is the Shannon  entropy  of the probability distribution \( \vec{p} \), which is a me</p><p>5 0.40417302 <a title="192-lsi-5" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-07-25-Cerealitivity.html">70 brendan oconnor ai-2007-07-25-Cerealitivity</a></p>
<p>Introduction: This is pretty funny, an old cartoon reprinted on  Language Log .</p><p>6 0.39717788 <a title="192-lsi-6" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-02-02-Histograms_%E2%80%94_matplotlib_vs._R.html">179 brendan oconnor ai-2012-02-02-Histograms — matplotlib vs. R</a></p>
<p>7 0.35356972 <a title="192-lsi-7" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-11-11-Memorizing_small_tables.html">177 brendan oconnor ai-2011-11-11-Memorizing small tables</a></p>
<p>8 0.34061912 <a title="192-lsi-8" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-05-24-Zipf%E2%80%99s_law_and_world_city_populations.html">141 brendan oconnor ai-2009-05-24-Zipf’s law and world city populations</a></p>
<p>9 0.31959787 <a title="192-lsi-9" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-05-19-conplot_%E2%80%93_a_console_plotter.html">103 brendan oconnor ai-2008-05-19-conplot – a console plotter</a></p>
<p>10 0.3177194 <a title="192-lsi-10" href="../brendan_oconnor_ai-2014/brendan_oconnor_ai-2014-04-26-Replot%3A_departure_delays_vs_flight_time_speed-up.html">204 brendan oconnor ai-2014-04-26-Replot: departure delays vs flight time speed-up</a></p>
<p>11 0.31744522 <a title="192-lsi-11" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-04-13-Are_women_discriminated_against_in_graduate_admissions%3F_Simpson%E2%80%99s_paradox_via_R_in_three_easy_steps%21.html">101 brendan oconnor ai-2008-04-13-Are women discriminated against in graduate admissions? Simpson’s paradox via R in three easy steps!</a></p>
<p>12 0.29840633 <a title="192-lsi-12" href="../brendan_oconnor_ai-2010/brendan_oconnor_ai-2010-03-31-How_Facebook_privacy_failed_me.html">158 brendan oconnor ai-2010-03-31-How Facebook privacy failed me</a></p>
<p>13 0.29792702 <a title="192-lsi-13" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-05-08-Movie_summary_corpus_and_learning_character_personas.html">196 brendan oconnor ai-2013-05-08-Movie summary corpus and learning character personas</a></p>
<p>14 0.28677508 <a title="192-lsi-14" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-07-08-Game_outcome_graphs_%E2%80%94_prisoner%E2%80%99s_dilemma_with_FUN_ARROWS%21%21%21.html">68 brendan oconnor ai-2007-07-08-Game outcome graphs — prisoner’s dilemma with FUN ARROWS!!!</a></p>
<p>15 0.28562587 <a title="192-lsi-15" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-05-16-Online_Deliberation_2005_conference_blog_%26_more_is_up%21.html">4 brendan oconnor ai-2005-05-16-Online Deliberation 2005 conference blog & more is up!</a></p>
<p>16 0.28382692 <a title="192-lsi-16" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-11-13-Bayes_update_view_of_pointwise_mutual_information.html">178 brendan oconnor ai-2011-11-13-Bayes update view of pointwise mutual information</a></p>
<p>17 0.28154922 <a title="192-lsi-17" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-05-20-Log-normal_and_logistic-normal_terminology.html">169 brendan oconnor ai-2011-05-20-Log-normal and logistic-normal terminology</a></p>
<p>18 0.27775094 <a title="192-lsi-18" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-11-14-Pop_cog_neuro_is_so_sigh.html">82 brendan oconnor ai-2007-11-14-Pop cog neuro is so sigh</a></p>
<p>19 0.23732448 <a title="192-lsi-19" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-06-25-looking_for_related_blogs-links.html">7 brendan oconnor ai-2005-06-25-looking for related blogs-links</a></p>
<p>20 0.23142275 <a title="192-lsi-20" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-09-07-Kurzweil_interview.html">27 brendan oconnor ai-2005-09-07-Kurzweil interview</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/brendan_oconnor_ai_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.677), (16, 0.052), (74, 0.098)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98358989 <a title="192-lda-1" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-03-14-R_scan%28%29_for_quick-and-dirty_checks.html">192 brendan oconnor ai-2013-03-14-R scan() for quick-and-dirty checks</a></p>
<p>Introduction: One of my favorite R tricks is  scan() .  I was using it to verify whether I wrote a sampler recently, which was supposed to output numbers uniformly between 1 and 100 into a logfile; this loads the logfile, counts the different outcomes, and plots.
 
 plot(table(scan(“log”))) 
 
As the logfile was growing, I kept replotting it and found it oddly compelling.
 
 
 
This was useful: in fact, an early version had an off-by-one bug, immediately obvious  from the plot .  And of course,  chisq.test(table(scan(“log”)))  does a null-hypothesis to check uniformity.</p><p>2 0.14308771 <a title="192-lda-2" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-09-02-cognitive_modelling_is_rational_choice%2B%2B.html">26 brendan oconnor ai-2005-09-02-cognitive modelling is rational choice++</a></p>
<p>Introduction: Rational choice has been a huge imperialistic success, growing in popularity and being applied to more and more fields.  Why is this?  It’s not because the rational choice model of decision-making is particularly realistic.  Rather, it’s because rational choice is a  completely specified theory of human behavior , and therefore is great at generating hypotheses.  Given any situation involving people, rational choice can be used to generate a hypothesis about what to expect.  That is, you just ask, “What would a person do to maximize their own benefit?”
 
Similar things have been said about evolutionary psychology: you can always predict behavior by asking “what would hunter-gatherers do?”  Now, certainly both rational choice and evolutionary psychology don’t always generate  correct  hypotheses, but they’re incredibly useful because they at least give you a starting point.
 
Witness the theory of bounded rationality: just like rational choice, except amended to consider computational l</p><p>3 0.14230517 <a title="192-lda-3" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-08-Another_R_flashmob_today.html">152 brendan oconnor ai-2009-09-08-Another R flashmob today</a></p>
<p>Introduction: Dan Goldstein  sends word they’re doing  another Stackoverflow R flashmob today .  It’s a neat trick.  The  R tag there  is becoming pretty useful.</p><p>4 0.1421154 <a title="192-lda-4" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-06-10-Freak-Freakonomics_%28Ariel_Rubinstein_is_the_shit%21%29.html">63 brendan oconnor ai-2007-06-10-Freak-Freakonomics (Ariel Rubinstein is the shit!)</a></p>
<p>Introduction: I don’t care how lame anyone thinks this is, but economic theorist  Ariel Rubinstein  is the shit.  He’s funny, self-deprecating, and brilliant.  I was just re-reading  his delightful, sarcastic review of Freakonomics .  (Overly dramatized visual depiction below; hey, conflict sells.)
 
    The review consists of excerpts from his own upcoming super-worldwide-bestseller, “Freak-Freakonomics”.  It is full of golden quotes such as: 
  Chapter 2: Why do economists earn more than mathematicians?  
 
…
 
The comparison between architects and prostitutes can be applied to mathematicians and economists: The former are more skilled, highly educated and intelligent.  
 
To elaborate: 
 Levitt has never encountered a girl who dreams of being a prostitute and I have never met a child who dreams of being an economist. Like prostitutes, the skill required of economists is “not necessarily ‘specialized’” (106). And, finally, here is a new explanation for the salary gap between mathematicians and eco</p><p>5 0.14185363 <a title="192-lda-5" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-07-09-the_psychology_of_design_as_explanation.html">19 brendan oconnor ai-2005-07-09-the psychology of design as explanation</a></p>
<p>Introduction: Since I posted the link to his blog, Baron just  wrote about   Cardinal Schönborn’s anti-evolution Op-Ed piece .  I agree absolutely that people should learn about the psychology of judgment and probability for these sorts of questions, where it’s really hard to understand that random processes can generate things that seem not so random.
 
I’m still thinking about how the psychology of judgment plays in to  the analysis below .  I have a feeling that people’s intuitions are usually too hospitable for explanations based on intention.  E.g.: People are poor, therefore someone is trying to make them poor.  Organizations (corportations, governments) do things, therefore someone (say, at the top) ordered them to do these things.  Natural disasters happen, therefore someone is wishing them upon us.  Etc., etc.  I’m still not sure how a bayesian dissection of whether “looks intentful” implies “is intentful” shows us whether such an “intent-seeking” bias (hey, I have to call it something) is</p><p>6 0.14149439 <a title="192-lda-6" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-09-15-Dollar_auction.html">77 brendan oconnor ai-2007-09-15-Dollar auction</a></p>
<p>7 0.14119047 <a title="192-lda-7" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-06-05-Clinton-Obama_support_visualization.html">105 brendan oconnor ai-2008-06-05-Clinton-Obama support visualization</a></p>
<p>8 0.13962018 <a title="192-lda-8" href="../brendan_oconnor_ai-2014/brendan_oconnor_ai-2014-02-19-What_the_ACL-2014_review_scores_mean.html">203 brendan oconnor ai-2014-02-19-What the ACL-2014 review scores mean</a></p>
<p>9 0.13708153 <a title="192-lda-9" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-11-12-Disease_tracking_with_web_queries_and_social_messaging_%28Google%2C_Twitter%2C_Facebook%E2%80%A6%29.html">123 brendan oconnor ai-2008-11-12-Disease tracking with web queries and social messaging (Google, Twitter, Facebook…)</a></p>
<p>10 0.11449231 <a title="192-lda-10" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-12-20-Data-driven_charity.html">86 brendan oconnor ai-2007-12-20-Data-driven charity</a></p>
<p>11 0.11352296 <a title="192-lda-11" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-05-29-%22Stanford_Impostor%22.html">62 brendan oconnor ai-2007-05-29-"Stanford Impostor"</a></p>
<p>12 0.11135663 <a title="192-lda-12" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-17-1_billion_web_page_dataset_from_CMU.html">138 brendan oconnor ai-2009-04-17-1 billion web page dataset from CMU</a></p>
<p>13 0.11065055 <a title="192-lda-13" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-03-15-Feminists%2C_anarchists%2C_computational_complexity%2C_bounded_rationality%2C_nethack%2C_and_other_things_to_do.html">53 brendan oconnor ai-2007-03-15-Feminists, anarchists, computational complexity, bounded rationality, nethack, and other things to do</a></p>
<p>14 0.10732072 <a title="192-lda-14" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-18-Information_cost_and_genocide.html">130 brendan oconnor ai-2008-12-18-Information cost and genocide</a></p>
<p>15 0.10543071 <a title="192-lda-15" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-10-31-neo_institutional_economic_fun%21.html">80 brendan oconnor ai-2007-10-31-neo institutional economic fun!</a></p>
<p>16 0.10176211 <a title="192-lda-16" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-10-02-Powerset%E2%80%99s_natural_language_search_system.html">188 brendan oconnor ai-2012-10-02-Powerset’s natural language search system</a></p>
<p>17 0.10151003 <a title="192-lda-17" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-03-Statistics_vs._Machine_Learning%2C_fight%21.html">129 brendan oconnor ai-2008-12-03-Statistics vs. Machine Learning, fight!</a></p>
<p>18 0.099259637 <a title="192-lda-18" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-11-26-How_did_Freud_become_a_respected_humanist%3F%21.html">84 brendan oconnor ai-2007-11-26-How did Freud become a respected humanist?!</a></p>
<p>19 0.098526217 <a title="192-lda-19" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-06-25-idea%3A_Morals_are_heuristics_for_socially_optimal_behavior.html">6 brendan oconnor ai-2005-06-25-idea: Morals are heuristics for socially optimal behavior</a></p>
<p>20 0.098484956 <a title="192-lda-20" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-05-24-Rock_Paper_Scissors_psychology.html">61 brendan oconnor ai-2007-05-24-Rock Paper Scissors psychology</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
