<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>201 brendan oconnor ai-2013-10-31-tanh is a rescaled logistic sigmoid function</title>
</head>

<body>
<p><a title="brendan_oconnor_ai" href="../brendan_oconnor_ai_home.html">brendan_oconnor_ai</a> <a title="brendan_oconnor_ai-2013" href="../home/brendan_oconnor_ai-2013_home.html">brendan_oconnor_ai-2013</a> <a title="brendan_oconnor_ai-2013-201" href="#">brendan_oconnor_ai-2013-201</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>201 brendan oconnor ai-2013-10-31-tanh is a rescaled logistic sigmoid function</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="brendan_oconnor_ai-2013-201-html" href="http://brenocon.com/blog/2013/10/tanh-is-a-rescaled-logistic-sigmoid-function/">html</a></p><p>Introduction: This confused me for a while when I first learned it, so in case it helps anyone else:
 
The   logistic sigmoid   function, a.k.a. the  inverse logit  function, is
 
\[ g(x) = \frac{ e^x }{1 + e^x} \]
 
Its outputs range from 0 to 1, and are often interpreted as probabilities (in, say, logistic regression).
 
The   tanh   function, a.k.a. hyperbolic tangent function, is a rescaling of the logistic sigmoid, such that its outputs range from -1 to 1.  (There’s horizontal stretching as well.)
 
\[ tanh(x) = 2 g(2x) - 1 \]
 
It’s easy to show the above leads to the standard definition \( tanh(x) = \frac{e^x – e^{-x}}{e^x + e^{-x}} \).  The (-1,+1) output range tends to be more convenient for neural networks, so tanh functions show up there a lot.
 
The two functions are plotted below.  Blue is the logistic function, and red is tanh.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 This confused me for a while when I first learned it, so in case it helps anyone else:   The   logistic sigmoid   function, a. [sent-1, score-0.984]
</p><p>2 the  inverse logit  function, is   \[ g(x) = \frac{ e^x }{1 + e^x} \]   Its outputs range from 0 to 1, and are often interpreted as probabilities (in, say, logistic regression). [sent-4, score-1.214]
</p><p>3 hyperbolic tangent function, is a rescaling of the logistic sigmoid, such that its outputs range from -1 to 1. [sent-8, score-0.763]
</p><p>4 )   \[ tanh(x) = 2 g(2x) - 1 \]   It’s easy to show the above leads to the standard definition \( tanh(x) = \frac{e^x – e^{-x}}{e^x + e^{-x}} \). [sent-10, score-0.365]
</p><p>5 The (-1,+1) output range tends to be more convenient for neural networks, so tanh functions show up there a lot. [sent-11, score-1.481]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tanh', 0.609), ('logistic', 0.36), ('function', 0.357), ('range', 0.255), ('sigmoid', 0.243), ('functions', 0.17), ('outputs', 0.148), ('frac', 0.148), ('show', 0.114), ('horizontal', 0.106), ('logit', 0.106), ('inverse', 0.106), ('helps', 0.097), ('interpreted', 0.097), ('convenient', 0.097), ('neural', 0.097), ('probabilities', 0.097), ('leads', 0.09), ('confused', 0.085), ('networks', 0.081), ('red', 0.081), ('blue', 0.081), ('plotted', 0.081), ('definition', 0.074), ('tends', 0.074), ('regression', 0.065), ('output', 0.065), ('learned', 0.061), ('anyone', 0.058), ('else', 0.057), ('often', 0.045), ('case', 0.045), ('easy', 0.044), ('standard', 0.043), ('say', 0.036), ('two', 0.036), ('first', 0.035)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="201-tfidf-1" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-10-31-tanh_is_a_rescaled_logistic_sigmoid_function.html">201 brendan oconnor ai-2013-10-31-tanh is a rescaled logistic sigmoid function</a></p>
<p>Introduction: This confused me for a while when I first learned it, so in case it helps anyone else:
 
The   logistic sigmoid   function, a.k.a. the  inverse logit  function, is
 
\[ g(x) = \frac{ e^x }{1 + e^x} \]
 
Its outputs range from 0 to 1, and are often interpreted as probabilities (in, say, logistic regression).
 
The   tanh   function, a.k.a. hyperbolic tangent function, is a rescaling of the logistic sigmoid, such that its outputs range from -1 to 1.  (There’s horizontal stretching as well.)
 
\[ tanh(x) = 2 g(2x) - 1 \]
 
It’s easy to show the above leads to the standard definition \( tanh(x) = \frac{e^x – e^{-x}}{e^x + e^{-x}} \).  The (-1,+1) output range tends to be more convenient for neural networks, so tanh functions show up there a lot.
 
The two functions are plotted below.  Blue is the logistic function, and red is tanh.</p><p>2 0.12303387 <a title="201-tfidf-2" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-05-20-Log-normal_and_logistic-normal_terminology.html">169 brendan oconnor ai-2011-05-20-Log-normal and logistic-normal terminology</a></p>
<p>Introduction: I was cleaning my office and found a back-of-envelope diagram  Shay  drew me once, so I’m writing it up to not forget.  The definitions of the  logistic-normal  and  log-normal  distributions are a little confusing with regard to their relationship to the normal distribution.  If you draw samples from one, the arrows below show the transformation to make it such you have samples from another.
 
   
 
For example, if  x ~ Normal , then transforming as  y=exp(x)  implies  y ~ LogNormal .  The adjective terminology is inverted: the logistic function goes from normal to logistic-normal, but the log function goes from log-normal to normal (other way!).  The log of the log-normal is normal, but it’s the   logit   of the logistic normal that’s normal.
 
Here are densities of these different distributions via transformations from a standard normal.
 
   In R:   x=rnorm(1e6); hist(x); hist(exp(x)/(1+exp(x)); hist(exp(x)) 
 
Just to make things more confusing, note the logistic-normal distributi</p><p>3 0.1195465 <a title="201-tfidf-3" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-01-11-Please_report_your_SVM%E2%80%99s_kernel%21.html">164 brendan oconnor ai-2011-01-11-Please report your SVM’s kernel!</a></p>
<p>Introduction: I’m tired of reading papers that use an  SVM  but don’t say which kernel they used.  (There’s tons of such papers in NLP and, I think, other areas that do applied machine learning.)  I suspect a lot of these papers are actually using a linear kernel.
 
An un-kernelized, linear SVM is nearly the same as  logistic regression  — every feature independently increases or decreases the classifier’s output prediction.  But a quadratic kernelized SVM is much more like boosted depth-2 decision trees.  It can do automatic combinations of pairs of features — a potentially very different thing, since you can start throwing in features that don’t do anything on their own but might have useful interactions with others.  (And of course, more complicated kernels do progressively more complicated and non-linear things.)
 
I have heard people say they download an SVM package, try a bunch of different kernels, and find the linear kernel is the best. In such cases they could have just used a logistic regr</p><p>4 0.077489525 <a title="201-tfidf-4" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-11-11-Memorizing_small_tables.html">177 brendan oconnor ai-2011-11-11-Memorizing small tables</a></p>
<p>Introduction: Lately, I’ve been trying  to memorize very small tables, especially for better intuitions and rule-of-thumb calculations.  At the moment I have these above my desk:
 
   
 
The first one is a few entries in a natural logarithm table.  There are all these stories about how in the slide rule era, people would develop better intuitions about the scale of logarithms because they physically engaged with them all the time.  I spend lots of time looking at log-likelihoods, log-odds-ratios, and logistic regression coefficients, so I think it would be nice to have quick intuitions about what they are.  (Though the  Gelman and Hill  textbook has an interesting argument against odds scale interpretations of logistic regression coefficients.)
 
The second one are some zsh filename manipulation  shortcuts .  OK, this is more narrow than the others, but pretty useful for me at least.
 
The third one are rough unit equivalencies for data rates over time.  I find this very important for quickly determ</p><p>5 0.057201084 <a title="201-tfidf-5" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-03-13-Cosine_similarity%2C_Pearson_correlation%2C_and_OLS_coefficients.html">182 brendan oconnor ai-2012-03-13-Cosine similarity, Pearson correlation, and OLS coefficients</a></p>
<p>Introduction: Cosine similarity, Pearson correlations, and OLS coefficients can all be viewed as variants on the inner product — tweaked in different ways for centering and magnitude (i.e. location and scale, or something like that).
 
Details:
 
You have two vectors \(x\) and \(y\) and want to measure similarity between them.  A basic similarity function is the   inner product  
 
\[ Inner(x,y) = \sum_i x_i y_i = \langle x, y \rangle \]
 
If x tends to be high where y is also high, and low where y is low, the inner product will be high — the vectors are more similar.
 
The inner product is unbounded.  One way to make it bounded between -1 and 1 is to divide by the vectors’ L2 norms, giving the   cosine similarity  
 
\[ CosSim(x,y) = \frac{\sum_i x_i y_i}{ \sqrt{ \sum_i x_i^2} \sqrt{ \sum_i y_i^2 } } 
= \frac{ \langle x,y \rangle }{ ||x||\ ||y|| } 
\]
 
This is actually bounded between 0 and 1 if x and y are non-negative.  Cosine similarity has an interpretation as the cosine of the angle between t</p><p>6 0.04439925 <a title="201-tfidf-6" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-11-13-Bayes_update_view_of_pointwise_mutual_information.html">178 brendan oconnor ai-2011-11-13-Bayes update view of pointwise mutual information</a></p>
<p>7 0.042741839 <a title="201-tfidf-7" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-02-02-Histograms_%E2%80%94_matplotlib_vs._R.html">179 brendan oconnor ai-2012-02-02-Histograms — matplotlib vs. R</a></p>
<p>8 0.041701276 <a title="201-tfidf-8" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-03-18-color_name_study_i_did.html">95 brendan oconnor ai-2008-03-18-color name study i did</a></p>
<p>9 0.039155547 <a title="201-tfidf-9" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-04-11-F-scores%2C_Dice%2C_and_Jaccard_set_similarity.html">183 brendan oconnor ai-2012-04-11-F-scores, Dice, and Jaccard set similarity</a></p>
<p>10 0.038318437 <a title="201-tfidf-10" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-01-Binary_classification_evaluation_in_R_via_ROCR.html">136 brendan oconnor ai-2009-04-01-Binary classification evaluation in R via ROCR</a></p>
<p>11 0.038042866 <a title="201-tfidf-11" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-04-21-What_inputs_do_Monte_Carlo_algorithms_need%3F.html">195 brendan oconnor ai-2013-04-21-What inputs do Monte Carlo algorithms need?</a></p>
<p>12 0.031412836 <a title="201-tfidf-12" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-09-25-Information_theory_stuff.html">175 brendan oconnor ai-2011-09-25-Information theory stuff</a></p>
<p>13 0.029201737 <a title="201-tfidf-13" href="../brendan_oconnor_ai-2014/brendan_oconnor_ai-2014-04-26-Replot%3A_departure_delays_vs_flight_time_speed-up.html">204 brendan oconnor ai-2014-04-26-Replot: departure delays vs flight time speed-up</a></p>
<p>14 0.029059516 <a title="201-tfidf-14" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-08-31-Probabilistic_interpretation_of_the_B3_coreference_resolution_metric.html">199 brendan oconnor ai-2013-08-31-Probabilistic interpretation of the B3 coreference resolution metric</a></p>
<p>15 0.0266429 <a title="201-tfidf-15" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-03-Statistics_vs._Machine_Learning%2C_fight%21.html">129 brendan oconnor ai-2008-12-03-Statistics vs. Machine Learning, fight!</a></p>
<p>16 0.026463699 <a title="201-tfidf-16" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-06-25-idea%3A_Morals_are_heuristics_for_socially_optimal_behavior.html">6 brendan oconnor ai-2005-06-25-idea: Morals are heuristics for socially optimal behavior</a></p>
<p>17 0.02574129 <a title="201-tfidf-17" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-17-p-values%2C_CDF%E2%80%99s%2C_NLP_etc..html">185 brendan oconnor ai-2012-07-17-p-values, CDF’s, NLP etc.</a></p>
<p>18 0.022914248 <a title="201-tfidf-18" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-04-The_%2460%2C000_cat%3A_deep_belief_networks_make_less_sense_for_language_than_vision.html">184 brendan oconnor ai-2012-07-04-The $60,000 cat: deep belief networks make less sense for language than vision</a></p>
<p>19 0.022185612 <a title="201-tfidf-19" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-10-Don%E2%80%99t_MAWK_AWK_%E2%80%93_the_fastest_and_most_elegant_big_data_munging_language%21.html">154 brendan oconnor ai-2009-09-10-Don’t MAWK AWK – the fastest and most elegant big data munging language!</a></p>
<p>20 0.021768663 <a title="201-tfidf-20" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-08-16-A_better_Obama_vs_McCain_poll_aggregation.html">111 brendan oconnor ai-2008-08-16-A better Obama vs McCain poll aggregation</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/brendan_oconnor_ai_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, -0.063), (1, -0.055), (2, 0.061), (3, -0.084), (4, -0.023), (5, 0.086), (6, 0.007), (7, -0.121), (8, -0.076), (9, -0.0), (10, 0.034), (11, -0.066), (12, 0.0), (13, 0.014), (14, -0.053), (15, 0.047), (16, -0.089), (17, 0.029), (18, 0.043), (19, -0.131), (20, -0.051), (21, 0.073), (22, -0.018), (23, 0.053), (24, -0.053), (25, 0.028), (26, -0.057), (27, -0.049), (28, -0.014), (29, 0.012), (30, 0.034), (31, 0.06), (32, 0.236), (33, -0.065), (34, 0.17), (35, -0.103), (36, 0.08), (37, 0.122), (38, -0.019), (39, 0.001), (40, 0.048), (41, 0.046), (42, 0.047), (43, 0.008), (44, -0.025), (45, 0.038), (46, 0.018), (47, 0.203), (48, 0.025), (49, 0.072)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99808156 <a title="201-lsi-1" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-10-31-tanh_is_a_rescaled_logistic_sigmoid_function.html">201 brendan oconnor ai-2013-10-31-tanh is a rescaled logistic sigmoid function</a></p>
<p>Introduction: This confused me for a while when I first learned it, so in case it helps anyone else:
 
The   logistic sigmoid   function, a.k.a. the  inverse logit  function, is
 
\[ g(x) = \frac{ e^x }{1 + e^x} \]
 
Its outputs range from 0 to 1, and are often interpreted as probabilities (in, say, logistic regression).
 
The   tanh   function, a.k.a. hyperbolic tangent function, is a rescaling of the logistic sigmoid, such that its outputs range from -1 to 1.  (There’s horizontal stretching as well.)
 
\[ tanh(x) = 2 g(2x) - 1 \]
 
It’s easy to show the above leads to the standard definition \( tanh(x) = \frac{e^x – e^{-x}}{e^x + e^{-x}} \).  The (-1,+1) output range tends to be more convenient for neural networks, so tanh functions show up there a lot.
 
The two functions are plotted below.  Blue is the logistic function, and red is tanh.</p><p>2 0.65911597 <a title="201-lsi-2" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-01-11-Please_report_your_SVM%E2%80%99s_kernel%21.html">164 brendan oconnor ai-2011-01-11-Please report your SVM’s kernel!</a></p>
<p>Introduction: I’m tired of reading papers that use an  SVM  but don’t say which kernel they used.  (There’s tons of such papers in NLP and, I think, other areas that do applied machine learning.)  I suspect a lot of these papers are actually using a linear kernel.
 
An un-kernelized, linear SVM is nearly the same as  logistic regression  — every feature independently increases or decreases the classifier’s output prediction.  But a quadratic kernelized SVM is much more like boosted depth-2 decision trees.  It can do automatic combinations of pairs of features — a potentially very different thing, since you can start throwing in features that don’t do anything on their own but might have useful interactions with others.  (And of course, more complicated kernels do progressively more complicated and non-linear things.)
 
I have heard people say they download an SVM package, try a bunch of different kernels, and find the linear kernel is the best. In such cases they could have just used a logistic regr</p><p>3 0.61206406 <a title="201-lsi-3" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-05-20-Log-normal_and_logistic-normal_terminology.html">169 brendan oconnor ai-2011-05-20-Log-normal and logistic-normal terminology</a></p>
<p>Introduction: I was cleaning my office and found a back-of-envelope diagram  Shay  drew me once, so I’m writing it up to not forget.  The definitions of the  logistic-normal  and  log-normal  distributions are a little confusing with regard to their relationship to the normal distribution.  If you draw samples from one, the arrows below show the transformation to make it such you have samples from another.
 
   
 
For example, if  x ~ Normal , then transforming as  y=exp(x)  implies  y ~ LogNormal .  The adjective terminology is inverted: the logistic function goes from normal to logistic-normal, but the log function goes from log-normal to normal (other way!).  The log of the log-normal is normal, but it’s the   logit   of the logistic normal that’s normal.
 
Here are densities of these different distributions via transformations from a standard normal.
 
   In R:   x=rnorm(1e6); hist(x); hist(exp(x)/(1+exp(x)); hist(exp(x)) 
 
Just to make things more confusing, note the logistic-normal distributi</p><p>4 0.50304621 <a title="201-lsi-4" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-11-11-Memorizing_small_tables.html">177 brendan oconnor ai-2011-11-11-Memorizing small tables</a></p>
<p>Introduction: Lately, I’ve been trying  to memorize very small tables, especially for better intuitions and rule-of-thumb calculations.  At the moment I have these above my desk:
 
   
 
The first one is a few entries in a natural logarithm table.  There are all these stories about how in the slide rule era, people would develop better intuitions about the scale of logarithms because they physically engaged with them all the time.  I spend lots of time looking at log-likelihoods, log-odds-ratios, and logistic regression coefficients, so I think it would be nice to have quick intuitions about what they are.  (Though the  Gelman and Hill  textbook has an interesting argument against odds scale interpretations of logistic regression coefficients.)
 
The second one are some zsh filename manipulation  shortcuts .  OK, this is more narrow than the others, but pretty useful for me at least.
 
The third one are rough unit equivalencies for data rates over time.  I find this very important for quickly determ</p><p>5 0.41180772 <a title="201-lsi-5" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-04-21-What_inputs_do_Monte_Carlo_algorithms_need%3F.html">195 brendan oconnor ai-2013-04-21-What inputs do Monte Carlo algorithms need?</a></p>
<p>Introduction: Monte Carlo sampling algorithms (either  MCMC  or not) have a goal to attain samples from a distribution.  They can be organized by what inputs or prior knowledge about the distribution they require.  This ranges from a low amount of knowledge, as in slice sampling (just give it an unnormalized density function), to a high amount, as in Gibbs sampling (you have to decompose your distribution into individual conditionals).
 
Typical inputs include \(f(x)\), an unnormalized density or probability function for the target distribution, which returns a real number for a variable value.  \(g()\) and \(g(x)\) represent sample generation procedures (that output a variable value); some generators require an input, some do not.
 
Here are the required inputs for a few algorithms.  (For an overview, see e.g.  Ch 29 of MacKay .)  There are many more out there of course.  I’m leaving off tuning parameters.
 
Black-box samplers:  Slice sampling ,  Affine-invariant ensemble  
- unnorm density \(f(x)\</p><p>6 0.39868748 <a title="201-lsi-6" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-04-11-F-scores%2C_Dice%2C_and_Jaccard_set_similarity.html">183 brendan oconnor ai-2012-04-11-F-scores, Dice, and Jaccard set similarity</a></p>
<p>7 0.34035009 <a title="201-lsi-7" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-03-13-Cosine_similarity%2C_Pearson_correlation%2C_and_OLS_coefficients.html">182 brendan oconnor ai-2012-03-13-Cosine similarity, Pearson correlation, and OLS coefficients</a></p>
<p>8 0.30461144 <a title="201-lsi-8" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-03-02-Poor_man%E2%80%99s_linear_algebra_textbook.html">166 brendan oconnor ai-2011-03-02-Poor man’s linear algebra textbook</a></p>
<p>9 0.2968896 <a title="201-lsi-9" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-11-13-Bayes_update_view_of_pointwise_mutual_information.html">178 brendan oconnor ai-2011-11-13-Bayes update view of pointwise mutual information</a></p>
<p>10 0.29102901 <a title="201-lsi-10" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-04-06-a_regression_slope_is_a_weighted_average_of_pairs%E2%80%99_slopes%21.html">100 brendan oconnor ai-2008-04-06-a regression slope is a weighted average of pairs’ slopes!</a></p>
<p>11 0.28138334 <a title="201-lsi-11" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-09-25-Information_theory_stuff.html">175 brendan oconnor ai-2011-09-25-Information theory stuff</a></p>
<p>12 0.27933562 <a title="201-lsi-12" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-03-18-color_name_study_i_did.html">95 brendan oconnor ai-2008-03-18-color name study i did</a></p>
<p>13 0.24898045 <a title="201-lsi-13" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-02-02-Histograms_%E2%80%94_matplotlib_vs._R.html">179 brendan oconnor ai-2012-02-02-Histograms — matplotlib vs. R</a></p>
<p>14 0.24001829 <a title="201-lsi-14" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-17-p-values%2C_CDF%E2%80%99s%2C_NLP_etc..html">185 brendan oconnor ai-2012-07-17-p-values, CDF’s, NLP etc.</a></p>
<p>15 0.23622124 <a title="201-lsi-15" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-01-Binary_classification_evaluation_in_R_via_ROCR.html">136 brendan oconnor ai-2009-04-01-Binary classification evaluation in R via ROCR</a></p>
<p>16 0.23336671 <a title="201-lsi-16" href="../brendan_oconnor_ai-2006/brendan_oconnor_ai-2006-06-28-Social_network-ized_economic_markets.html">40 brendan oconnor ai-2006-06-28-Social network-ized economic markets</a></p>
<p>17 0.20429976 <a title="201-lsi-17" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-06-25-idea%3A_Morals_are_heuristics_for_socially_optimal_behavior.html">6 brendan oconnor ai-2005-06-25-idea: Morals are heuristics for socially optimal behavior</a></p>
<p>18 0.19898421 <a title="201-lsi-18" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-08-04-Blogger_to_WordPress_migration_helper.html">149 brendan oconnor ai-2009-08-04-Blogger to WordPress migration helper</a></p>
<p>19 0.18362904 <a title="201-lsi-19" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-08-21-Berkeley_SDA_and_the_General_Social_Survey.html">186 brendan oconnor ai-2012-08-21-Berkeley SDA and the General Social Survey</a></p>
<p>20 0.17818755 <a title="201-lsi-20" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-07-08-Game_outcome_graphs_%E2%80%94_prisoner%E2%80%99s_dilemma_with_FUN_ARROWS%21%21%21.html">68 brendan oconnor ai-2007-07-08-Game outcome graphs — prisoner’s dilemma with FUN ARROWS!!!</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/brendan_oconnor_ai_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(26, 0.753), (44, 0.025), (74, 0.044)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98172385 <a title="201-lda-1" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-10-31-tanh_is_a_rescaled_logistic_sigmoid_function.html">201 brendan oconnor ai-2013-10-31-tanh is a rescaled logistic sigmoid function</a></p>
<p>Introduction: This confused me for a while when I first learned it, so in case it helps anyone else:
 
The   logistic sigmoid   function, a.k.a. the  inverse logit  function, is
 
\[ g(x) = \frac{ e^x }{1 + e^x} \]
 
Its outputs range from 0 to 1, and are often interpreted as probabilities (in, say, logistic regression).
 
The   tanh   function, a.k.a. hyperbolic tangent function, is a rescaling of the logistic sigmoid, such that its outputs range from -1 to 1.  (There’s horizontal stretching as well.)
 
\[ tanh(x) = 2 g(2x) - 1 \]
 
It’s easy to show the above leads to the standard definition \( tanh(x) = \frac{e^x – e^{-x}}{e^x + e^{-x}} \).  The (-1,+1) output range tends to be more convenient for neural networks, so tanh functions show up there a lot.
 
The two functions are plotted below.  Blue is the logistic function, and red is tanh.</p><p>2 0.064880855 <a title="201-lda-2" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-11-12-Disease_tracking_with_web_queries_and_social_messaging_%28Google%2C_Twitter%2C_Facebook%E2%80%A6%29.html">123 brendan oconnor ai-2008-11-12-Disease tracking with web queries and social messaging (Google, Twitter, Facebook…)</a></p>
<p>Introduction: This is a good idea: in a search engine’s query logs, look for outbreaks of queries like [[flu symptoms]] in a given region.  I’ve heard (from  Roddy ) that this trick also works well on Facebook statuses (e.g. “Feeling crappy this morning, think I just got the flu”).
  
  Google Uses Web Searches to Track Flu’s Spread – NYTimes.com  
  Google Flu Trends – google.org  
  
For an example with a publicly available data feed, these queries works decently well on Twitter search:
 
 [[ flu -shot -google ]]  (high recall)
 
 [[ "muscle aches" flu -shot ]]  (high precision)
     
 
The “muscle aches” query is too sparse and the general query is too noisy, but you could imagine some more tricks to clean it up, then train a classifier, etc.  With a bit more work it looks like geolocation information can be had out of the  Twitter search API .</p><p>3 0.063158371 <a title="201-lda-3" href="../brendan_oconnor_ai-2014/brendan_oconnor_ai-2014-02-19-What_the_ACL-2014_review_scores_mean.html">203 brendan oconnor ai-2014-02-19-What the ACL-2014 review scores mean</a></p>
<p>Introduction: I’ve had several people ask me what the numbers in  ACL  reviews mean — and I can’t find anywhere online where they’re described.  (Can anyone point this out if it is somewhere?)
 
So here’s the review form, below.  They all go from 1 to 5, with 5 the best.  I think the review emails to authors only include a subset of the below — for example, “Overall Recommendation” is not included?
 
The CFP said that they have different types of review forms for different types of papers.  I think this one is for a standard full paper.  I guess what people  really  want to know is what scores tend to correspond to acceptances.  I really have no idea and I get the impression this can change year to year.  I have no involvement with the ACL conference besides being one of many, many reviewers.
 
  
  
APPROPRIATENESS (1-5)
Does the paper fit in ACL 2014? (Please answer this question in light of the desire to broaden the scope of the research areas represented at ACL.) 

5: Certainly. 
4: Probabl</p><p>4 0.062325791 <a title="201-lda-4" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-06-10-Freak-Freakonomics_%28Ariel_Rubinstein_is_the_shit%21%29.html">63 brendan oconnor ai-2007-06-10-Freak-Freakonomics (Ariel Rubinstein is the shit!)</a></p>
<p>Introduction: I don’t care how lame anyone thinks this is, but economic theorist  Ariel Rubinstein  is the shit.  He’s funny, self-deprecating, and brilliant.  I was just re-reading  his delightful, sarcastic review of Freakonomics .  (Overly dramatized visual depiction below; hey, conflict sells.)
 
    The review consists of excerpts from his own upcoming super-worldwide-bestseller, “Freak-Freakonomics”.  It is full of golden quotes such as: 
  Chapter 2: Why do economists earn more than mathematicians?  
 
…
 
The comparison between architects and prostitutes can be applied to mathematicians and economists: The former are more skilled, highly educated and intelligent.  
 
To elaborate: 
 Levitt has never encountered a girl who dreams of being a prostitute and I have never met a child who dreams of being an economist. Like prostitutes, the skill required of economists is “not necessarily ‘specialized’” (106). And, finally, here is a new explanation for the salary gap between mathematicians and eco</p><p>5 0.062225286 <a title="201-lda-5" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-09-02-cognitive_modelling_is_rational_choice%2B%2B.html">26 brendan oconnor ai-2005-09-02-cognitive modelling is rational choice++</a></p>
<p>Introduction: Rational choice has been a huge imperialistic success, growing in popularity and being applied to more and more fields.  Why is this?  It’s not because the rational choice model of decision-making is particularly realistic.  Rather, it’s because rational choice is a  completely specified theory of human behavior , and therefore is great at generating hypotheses.  Given any situation involving people, rational choice can be used to generate a hypothesis about what to expect.  That is, you just ask, “What would a person do to maximize their own benefit?”
 
Similar things have been said about evolutionary psychology: you can always predict behavior by asking “what would hunter-gatherers do?”  Now, certainly both rational choice and evolutionary psychology don’t always generate  correct  hypotheses, but they’re incredibly useful because they at least give you a starting point.
 
Witness the theory of bounded rationality: just like rational choice, except amended to consider computational l</p><p>6 0.061973974 <a title="201-lda-6" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-06-05-Clinton-Obama_support_visualization.html">105 brendan oconnor ai-2008-06-05-Clinton-Obama support visualization</a></p>
<p>7 0.061454009 <a title="201-lda-7" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-09-15-Dollar_auction.html">77 brendan oconnor ai-2007-09-15-Dollar auction</a></p>
<p>8 0.060616948 <a title="201-lda-8" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-07-09-the_psychology_of_design_as_explanation.html">19 brendan oconnor ai-2005-07-09-the psychology of design as explanation</a></p>
<p>9 0.058354188 <a title="201-lda-9" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-08-Another_R_flashmob_today.html">152 brendan oconnor ai-2009-09-08-Another R flashmob today</a></p>
<p>10 0.058315024 <a title="201-lda-10" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-17-1_billion_web_page_dataset_from_CMU.html">138 brendan oconnor ai-2009-04-17-1 billion web page dataset from CMU</a></p>
<p>11 0.055383153 <a title="201-lda-11" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-03-Statistics_vs._Machine_Learning%2C_fight%21.html">129 brendan oconnor ai-2008-12-03-Statistics vs. Machine Learning, fight!</a></p>
<p>12 0.054933697 <a title="201-lda-12" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-12-20-Data-driven_charity.html">86 brendan oconnor ai-2007-12-20-Data-driven charity</a></p>
<p>13 0.054315742 <a title="201-lda-13" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-03-15-Feminists%2C_anarchists%2C_computational_complexity%2C_bounded_rationality%2C_nethack%2C_and_other_things_to_do.html">53 brendan oconnor ai-2007-03-15-Feminists, anarchists, computational complexity, bounded rationality, nethack, and other things to do</a></p>
<p>14 0.053873405 <a title="201-lda-14" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-10-02-Powerset%E2%80%99s_natural_language_search_system.html">188 brendan oconnor ai-2012-10-02-Powerset’s natural language search system</a></p>
<p>15 0.053635284 <a title="201-lda-15" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-02-02-Histograms_%E2%80%94_matplotlib_vs._R.html">179 brendan oconnor ai-2012-02-02-Histograms — matplotlib vs. R</a></p>
<p>16 0.053431518 <a title="201-lda-16" href="../brendan_oconnor_ai-2004/brendan_oconnor_ai-2004-11-24-addiction_%26_2_problems_of_economics.html">2 brendan oconnor ai-2004-11-24-addiction & 2 problems of economics</a></p>
<p>17 0.052980348 <a title="201-lda-17" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-08-08-Haghighi_and_Klein_%282009%29%3A_Simple_Coreference_Resolution_with_Rich_Syntactic_and_Semantic_Features.html">150 brendan oconnor ai-2009-08-08-Haghighi and Klein (2009): Simple Coreference Resolution with Rich Syntactic and Semantic Features</a></p>
<p>18 0.052366126 <a title="201-lda-18" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-06-25-idea%3A_Morals_are_heuristics_for_socially_optimal_behavior.html">6 brendan oconnor ai-2005-06-25-idea: Morals are heuristics for socially optimal behavior</a></p>
<p>19 0.052110419 <a title="201-lda-19" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-04-The_%2460%2C000_cat%3A_deep_belief_networks_make_less_sense_for_language_than_vision.html">184 brendan oconnor ai-2012-07-04-The $60,000 cat: deep belief networks make less sense for language than vision</a></p>
<p>20 0.051230647 <a title="201-lda-20" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-08-20-Some_analysis_of_tweet_shares_and_%E2%80%9Cpredicting%E2%80%9D_election_outcomes.html">198 brendan oconnor ai-2013-08-20-Some analysis of tweet shares and “predicting” election outcomes</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
