<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>194 brendan oconnor ai-2013-04-16-Rise and fall of Dirichlet process clusters</title>
</head>

<body>
<p><a title="brendan_oconnor_ai" href="../brendan_oconnor_ai_home.html">brendan_oconnor_ai</a> <a title="brendan_oconnor_ai-2013" href="../home/brendan_oconnor_ai-2013_home.html">brendan_oconnor_ai-2013</a> <a title="brendan_oconnor_ai-2013-194" href="#">brendan_oconnor_ai-2013-194</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>194 brendan oconnor ai-2013-04-16-Rise and fall of Dirichlet process clusters</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="brendan_oconnor_ai-2013-194-html" href="http://brenocon.com/blog/2013/04/dirichlet-process-mixture-picture/">html</a></p><p>Introduction: Here’s Gibbs sampling for a  Dirichlet process 1-d mixture of Gaussians .  On 1000 data points that look like this.
 
   
 
I gave it fixed variance and a concentration and over MCMC iterations, and it looks like this.
 
   
 
The top is the number of points in a cluster.  The bottom are the cluster means.  Every cluster has a unique color.  During MCMC, clusters are created and destroyed.  Every cluster has a unique color; when a cluster dies, its color is never reused.  
 
I’m showing clusters every 100 iterations.  If there is a single point, that cluster was at that iteration but not before or after.  If there is a line, the cluster lived for at least 100 iterations.  Some clusters live long, some live short, but all eventually die.
 
Usually the model likes to think there are about two clusters, occupying positions at the two modes in the data distribution.  It also entertains the existence of several much more minor ones.  Usually these are shortlived clusters that die away.  But</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Here’s Gibbs sampling for a  Dirichlet process 1-d mixture of Gaussians . [sent-1, score-0.14]
</p><p>2 The top is the number of points in a cluster. [sent-4, score-0.122]
</p><p>3 Every cluster has a unique color; when a cluster dies, its color is never reused. [sent-8, score-1.111]
</p><p>4 If there is a single point, that cluster was at that iteration but not before or after. [sent-10, score-0.697]
</p><p>5 If there is a line, the cluster lived for at least 100 iterations. [sent-11, score-0.567]
</p><p>6 Some clusters live long, some live short, but all eventually die. [sent-12, score-0.772]
</p><p>7 Usually the model likes to think there are about two clusters, occupying positions at the two modes in the data distribution. [sent-13, score-0.243]
</p><p>8 Usually these are shortlived clusters that die away. [sent-15, score-0.488]
</p><p>9 But sometimes, they rise up and kick out one of the dominant clusters, and take over its space. [sent-16, score-0.089]
</p><p>10 This is evocative at least to me: for example, around iteration 2500 is a crisis of the two-mode regime, the fall of green and the rise of blue. [sent-17, score-0.522]
</p><p>11 (Maybe there are analogies to  ideal points and coalitions  or something, but call that future work…)   In fact the real story is a little more chaotic. [sent-18, score-0.239]
</p><p>12 Around iteration 2500 you can see blue suddenly appear in green’s territory, where it’s bouncing around trying to get data points to convert to its cause. [sent-20, score-0.538]
</p><p>13 The clusters struggle and blue eventually wins out. [sent-21, score-0.74]
</p><p>14 Ultimately, the dynamism is fake; looking at the broad sweep of history, it’s all part of a globally unchanging, static steady state of MCMC. [sent-25, score-0.183]
</p><p>15 The name of the cluster at mean -2 might change from time to time, but really, it occupies a position in the system analogous to the old regime. [sent-26, score-0.677]
</p><p>16 Actually not just “analogous” but mathematically the same, as implied by CRP exchangeability; the cluster IDs are just an auxiliary variable for the DP. [sent-27, score-0.598]
</p><p>17 And the point of MCMC is to kill the dynamism by averaging over it for useful inference. [sent-28, score-0.281]
</p><p>18 This nicely illustrates you can’t directly use the actual clusters for averaging for an MCMC mixture model, since new clusters might slide into the place of old ones. [sent-29, score-1.397]
</p><p>19 (You might average over smaller spans, maybe; or perhaps look at statistics that are invariant to changing clusters, like the probability two datapoints belong to the same cluster. [sent-30, score-0.121]
</p><p>20 Or only use a single sample, which is at least guaranteed to be consistent? [sent-31, score-0.147]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('clusters', 0.488), ('cluster', 0.439), ('mcmc', 0.243), ('iteration', 0.183), ('iternum', 0.14), ('mixture', 0.14), ('points', 0.122), ('every', 0.122), ('dynamism', 0.122), ('color', 0.122), ('iterations', 0.122), ('unique', 0.111), ('analogous', 0.111), ('gibbs', 0.103), ('variable', 0.103), ('green', 0.098), ('eventually', 0.098), ('averaging', 0.098), ('live', 0.093), ('blue', 0.093), ('rise', 0.089), ('around', 0.08), ('single', 0.075), ('least', 0.072), ('old', 0.067), ('line', 0.065), ('two', 0.061), ('coalitions', 0.061), ('static', 0.061), ('regime', 0.061), ('ultimately', 0.061), ('dies', 0.061), ('likes', 0.061), ('challenges', 0.061), ('consistent', 0.061), ('neal', 0.061), ('kill', 0.061), ('qplot', 0.061), ('ids', 0.061), ('wins', 0.061), ('territory', 0.061), ('logfile', 0.061), ('data', 0.06), ('something', 0.06), ('might', 0.06), ('usually', 0.056), ('mathematically', 0.056), ('lived', 0.056), ('nicely', 0.056), ('ideal', 0.056)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="194-tfidf-1" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-04-16-Rise_and_fall_of_Dirichlet_process_clusters.html">194 brendan oconnor ai-2013-04-16-Rise and fall of Dirichlet process clusters</a></p>
<p>Introduction: Here’s Gibbs sampling for a  Dirichlet process 1-d mixture of Gaussians .  On 1000 data points that look like this.
 
   
 
I gave it fixed variance and a concentration and over MCMC iterations, and it looks like this.
 
   
 
The top is the number of points in a cluster.  The bottom are the cluster means.  Every cluster has a unique color.  During MCMC, clusters are created and destroyed.  Every cluster has a unique color; when a cluster dies, its color is never reused.  
 
I’m showing clusters every 100 iterations.  If there is a single point, that cluster was at that iteration but not before or after.  If there is a line, the cluster lived for at least 100 iterations.  Some clusters live long, some live short, but all eventually die.
 
Usually the model likes to think there are about two clusters, occupying positions at the two modes in the data distribution.  It also entertains the existence of several much more minor ones.  Usually these are shortlived clusters that die away.  But</p><p>2 0.10833772 <a title="194-tfidf-2" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-09-21-CMU_ARK_Twitter_Part-of-Speech_Tagger_%E2%80%93_v0.3_released.html">187 brendan oconnor ai-2012-09-21-CMU ARK Twitter Part-of-Speech Tagger – v0.3 released</a></p>
<p>Introduction: We’re pleased to announce a new release of the CMU ARK Twitter Part-of-Speech 
Tagger, version 0.3.
  
 The new version is much faster (40x) and more accurate (89.2 -> 92.8) than 
  before.
  We also have released new POS-annotated data, including a dataset of one 
  tweet for each of 547 days.
  We have made available large-scale word clusters from unlabeled Twitter data 
  (217k words, 56m tweets, 847m tokens).
   
Tools, data, and a new technical report describing the release are available at: 
 www.ark.cs.cmu.edu/TweetNLP .
 
 0100100  a  1111100101110   111100000011 , Brendan</p><p>3 0.088902064 <a title="194-tfidf-3" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-03-09-I_don%E2%80%99t_get_this_web_parsing_shared_task.html">181 brendan oconnor ai-2012-03-09-I don’t get this web parsing shared task</a></p>
<p>Introduction: The idea for a shared task on web parsing is really cool.  But I don’t get this one:
 
 Shared Task – SANCL 2012 (First Workshop on Syntactic Analysis of Non-Canonical Language) 
 
They’re explicitly banning
  
 Manually annotating in-domain (web) sentences
  Creating new word clusters, or anything, from as much text data as possible
   
… instead restricting participants to the data sets they release.
 
Isn’t a cycle of annotation, error analysis, and new annotations (a self-training + active-learning loop, with smarter decisions through error analysis) the hands-down best way to make an NLP tool for a new domain? Are people scared of this reality?  Am I off-base?
 
I am, of course, just advocating for our  Twitter POS tagger  approach, where we annotated some data, made a supervised tagger, and iterated on features.  The biggest weakness in that paper is we didn’t have additional iterations of error analysis.  Our lack of semi-supervised learning was  not  a weakness.</p><p>4 0.088724308 <a title="194-tfidf-4" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-08-31-Probabilistic_interpretation_of_the_B3_coreference_resolution_metric.html">199 brendan oconnor ai-2013-08-31-Probabilistic interpretation of the B3 coreference resolution metric</a></p>
<p>Introduction: Here is an intuitive justification for the  B3  evaluation metric often used in coreference resolution, based on whether mention pairs are coreferent. If a mention from the document is chosen at random,
  
 B3-Recall is the (expected) proportion of its actual coreferents that the system thinks are coreferent with it. 
 B3-Precision is the (expected) proportion of its system-hypothesized coreferents that are actually coreferent with it. 
  
Does this look correct to people? Details below:  
 
In B3′s basic form, it’s a clustering evaluation metric, to evaluate a gold-standard clustering of mentions against a system-produced clustering of mentions.
 
Let \(G\) mean a gold-standard entity and \(S\) mean a system-predicted entity, where an entity is a set of mentions. \(i\)Â refers to a mention; there are \(n\) mentions in the document. \(G_i\) means the gold entity that contains mention \(i\); and \(S_i\) means the system entity that has \(i\).
 
The B3 precision and recall for a document</p><p>5 0.084052123 <a title="194-tfidf-5" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-05-08-Movie_summary_corpus_and_learning_character_personas.html">196 brendan oconnor ai-2013-05-08-Movie summary corpus and learning character personas</a></p>
<p>Introduction: Here is one of our exciting just-finished  ACL  papers.   David  and I designed an algorithm that learns different types of character personas — “Protagonist”, “Love Interest”, etc — that are used in movies.
 
To do this we collected a  brand new dataset : 42,306 plot summaries of movies from Wikipedia, along with metadata like box office revenue and genre.  We ran these through parsing and coreference analysis to also create a dataset of movie characters, linked with Freebase records of the actors who portray them.  Did you see that NYT article on  quantitative analysis of film scripts ?  This dataset could answer all sorts of things they assert in that article — for example, do movies with  bowling scenes  really make less money?  We have released  the data here .
 
Our focus, though, is on narrative analysis.  We investigate  character personas : familiar character types that are repeated over and over in stories, like “Hero” or “Villian”; maybe grand mythical archetypes like “Trick</p><p>6 0.080992684 <a title="194-tfidf-6" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-04-21-What_inputs_do_Monte_Carlo_algorithms_need%3F.html">195 brendan oconnor ai-2013-04-21-What inputs do Monte Carlo algorithms need?</a></p>
<p>7 0.071481094 <a title="194-tfidf-7" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-08-08-Haghighi_and_Klein_%282009%29%3A_Simple_Coreference_Resolution_with_Rich_Syntactic_and_Semantic_Features.html">150 brendan oconnor ai-2009-08-08-Haghighi and Klein (2009): Simple Coreference Resolution with Rich Syntactic and Semantic Features</a></p>
<p>8 0.069659673 <a title="194-tfidf-8" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-10-08-Blog_move_has_landed.html">115 brendan oconnor ai-2008-10-08-Blog move has landed</a></p>
<p>9 0.065107465 <a title="194-tfidf-9" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-03-Statistics_vs._Machine_Learning%2C_fight%21.html">129 brendan oconnor ai-2008-12-03-Statistics vs. Machine Learning, fight!</a></p>
<p>10 0.062037148 <a title="194-tfidf-10" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-02-23-Comparison_of_data_analysis_packages%3A_R%2C_Matlab%2C_SciPy%2C_Excel%2C_SAS%2C_SPSS%2C_Stata.html">135 brendan oconnor ai-2009-02-23-Comparison of data analysis packages: R, Matlab, SciPy, Excel, SAS, SPSS, Stata</a></p>
<p>11 0.059024621 <a title="194-tfidf-11" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-08-20-Some_analysis_of_tweet_shares_and_%E2%80%9Cpredicting%E2%80%9D_election_outcomes.html">198 brendan oconnor ai-2013-08-20-Some analysis of tweet shares and “predicting” election outcomes</a></p>
<p>12 0.057856634 <a title="194-tfidf-12" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-08-01-Bayesian_analysis_of_intelligent_design_%28revised%21%29.html">23 brendan oconnor ai-2005-08-01-Bayesian analysis of intelligent design (revised!)</a></p>
<p>13 0.056649502 <a title="194-tfidf-13" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-22-Performance_comparison%3A_key-value_stores_for_language_model_counts.html">139 brendan oconnor ai-2009-04-22-Performance comparison: key-value stores for language model counts</a></p>
<p>14 0.054767165 <a title="194-tfidf-14" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-07-01-Bias_correction_sneak_peek%21.html">108 brendan oconnor ai-2008-07-01-Bias correction sneak peek!</a></p>
<p>15 0.054360732 <a title="194-tfidf-15" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-12-31-List_of_probabilistic_model_mini-language_toolkits.html">157 brendan oconnor ai-2009-12-31-List of probabilistic model mini-language toolkits</a></p>
<p>16 0.054060161 <a title="194-tfidf-16" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-07-15-Beta_conjugate_explorer.html">146 brendan oconnor ai-2009-07-15-Beta conjugate explorer</a></p>
<p>17 0.051372409 <a title="194-tfidf-17" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-10-Don%E2%80%99t_MAWK_AWK_%E2%80%93_the_fastest_and_most_elegant_big_data_munging_language%21.html">154 brendan oconnor ai-2009-09-10-Don’t MAWK AWK – the fastest and most elegant big data munging language!</a></p>
<p>18 0.050093498 <a title="194-tfidf-18" href="../brendan_oconnor_ai-2005/brendan_oconnor_ai-2005-07-09-a_bayesian_analysis_of_intelligent_design.html">17 brendan oconnor ai-2005-07-09-a bayesian analysis of intelligent design</a></p>
<p>19 0.050051257 <a title="194-tfidf-19" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-02-23-Wasserman_on_Stats_vs_ML%2C_and_previous_comparisons.html">191 brendan oconnor ai-2013-02-23-Wasserman on Stats vs ML, and previous comparisons</a></p>
<p>20 0.049972773 <a title="194-tfidf-20" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-02-02-Histograms_%E2%80%94_matplotlib_vs._R.html">179 brendan oconnor ai-2012-02-02-Histograms — matplotlib vs. R</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/brendan_oconnor_ai_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, -0.187), (1, -0.079), (2, 0.075), (3, 0.03), (4, -0.047), (5, 0.015), (6, -0.096), (7, -0.022), (8, -0.054), (9, 0.091), (10, 0.01), (11, 0.02), (12, -0.041), (13, -0.001), (14, 0.056), (15, 0.023), (16, 0.013), (17, 0.013), (18, -0.048), (19, -0.024), (20, 0.107), (21, 0.021), (22, 0.012), (23, -0.061), (24, 0.069), (25, -0.062), (26, 0.039), (27, -0.024), (28, -0.035), (29, -0.078), (30, -0.047), (31, -0.044), (32, 0.055), (33, 0.085), (34, -0.114), (35, -0.012), (36, -0.047), (37, -0.062), (38, 0.01), (39, -0.101), (40, 0.102), (41, -0.124), (42, -0.101), (43, 0.049), (44, -0.132), (45, 0.187), (46, -0.006), (47, -0.043), (48, -0.14), (49, 0.089)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98105615 <a title="194-lsi-1" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-04-16-Rise_and_fall_of_Dirichlet_process_clusters.html">194 brendan oconnor ai-2013-04-16-Rise and fall of Dirichlet process clusters</a></p>
<p>Introduction: Here’s Gibbs sampling for a  Dirichlet process 1-d mixture of Gaussians .  On 1000 data points that look like this.
 
   
 
I gave it fixed variance and a concentration and over MCMC iterations, and it looks like this.
 
   
 
The top is the number of points in a cluster.  The bottom are the cluster means.  Every cluster has a unique color.  During MCMC, clusters are created and destroyed.  Every cluster has a unique color; when a cluster dies, its color is never reused.  
 
I’m showing clusters every 100 iterations.  If there is a single point, that cluster was at that iteration but not before or after.  If there is a line, the cluster lived for at least 100 iterations.  Some clusters live long, some live short, but all eventually die.
 
Usually the model likes to think there are about two clusters, occupying positions at the two modes in the data distribution.  It also entertains the existence of several much more minor ones.  Usually these are shortlived clusters that die away.  But</p><p>2 0.50798005 <a title="194-lsi-2" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-04-21-What_inputs_do_Monte_Carlo_algorithms_need%3F.html">195 brendan oconnor ai-2013-04-21-What inputs do Monte Carlo algorithms need?</a></p>
<p>Introduction: Monte Carlo sampling algorithms (either  MCMC  or not) have a goal to attain samples from a distribution.  They can be organized by what inputs or prior knowledge about the distribution they require.  This ranges from a low amount of knowledge, as in slice sampling (just give it an unnormalized density function), to a high amount, as in Gibbs sampling (you have to decompose your distribution into individual conditionals).
 
Typical inputs include \(f(x)\), an unnormalized density or probability function for the target distribution, which returns a real number for a variable value.  \(g()\) and \(g(x)\) represent sample generation procedures (that output a variable value); some generators require an input, some do not.
 
Here are the required inputs for a few algorithms.  (For an overview, see e.g.  Ch 29 of MacKay .)  There are many more out there of course.  I’m leaving off tuning parameters.
 
Black-box samplers:  Slice sampling ,  Affine-invariant ensemble  
- unnorm density \(f(x)\</p><p>3 0.48684263 <a title="194-lsi-3" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-05-08-Movie_summary_corpus_and_learning_character_personas.html">196 brendan oconnor ai-2013-05-08-Movie summary corpus and learning character personas</a></p>
<p>Introduction: Here is one of our exciting just-finished  ACL  papers.   David  and I designed an algorithm that learns different types of character personas — “Protagonist”, “Love Interest”, etc — that are used in movies.
 
To do this we collected a  brand new dataset : 42,306 plot summaries of movies from Wikipedia, along with metadata like box office revenue and genre.  We ran these through parsing and coreference analysis to also create a dataset of movie characters, linked with Freebase records of the actors who portray them.  Did you see that NYT article on  quantitative analysis of film scripts ?  This dataset could answer all sorts of things they assert in that article — for example, do movies with  bowling scenes  really make less money?  We have released  the data here .
 
Our focus, though, is on narrative analysis.  We investigate  character personas : familiar character types that are repeated over and over in stories, like “Hero” or “Villian”; maybe grand mythical archetypes like “Trick</p><p>4 0.48103362 <a title="194-lsi-4" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-03-09-I_don%E2%80%99t_get_this_web_parsing_shared_task.html">181 brendan oconnor ai-2012-03-09-I don’t get this web parsing shared task</a></p>
<p>Introduction: The idea for a shared task on web parsing is really cool.  But I don’t get this one:
 
 Shared Task – SANCL 2012 (First Workshop on Syntactic Analysis of Non-Canonical Language) 
 
They’re explicitly banning
  
 Manually annotating in-domain (web) sentences
  Creating new word clusters, or anything, from as much text data as possible
   
… instead restricting participants to the data sets they release.
 
Isn’t a cycle of annotation, error analysis, and new annotations (a self-training + active-learning loop, with smarter decisions through error analysis) the hands-down best way to make an NLP tool for a new domain? Are people scared of this reality?  Am I off-base?
 
I am, of course, just advocating for our  Twitter POS tagger  approach, where we annotated some data, made a supervised tagger, and iterated on features.  The biggest weakness in that paper is we didn’t have additional iterations of error analysis.  Our lack of semi-supervised learning was  not  a weakness.</p><p>5 0.46665153 <a title="194-lsi-5" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-08-31-Probabilistic_interpretation_of_the_B3_coreference_resolution_metric.html">199 brendan oconnor ai-2013-08-31-Probabilistic interpretation of the B3 coreference resolution metric</a></p>
<p>Introduction: Here is an intuitive justification for the  B3  evaluation metric often used in coreference resolution, based on whether mention pairs are coreferent. If a mention from the document is chosen at random,
  
 B3-Recall is the (expected) proportion of its actual coreferents that the system thinks are coreferent with it. 
 B3-Precision is the (expected) proportion of its system-hypothesized coreferents that are actually coreferent with it. 
  
Does this look correct to people? Details below:  
 
In B3′s basic form, it’s a clustering evaluation metric, to evaluate a gold-standard clustering of mentions against a system-produced clustering of mentions.
 
Let \(G\) mean a gold-standard entity and \(S\) mean a system-predicted entity, where an entity is a set of mentions. \(i\)Â refers to a mention; there are \(n\) mentions in the document. \(G_i\) means the gold entity that contains mention \(i\); and \(S_i\) means the system entity that has \(i\).
 
The B3 precision and recall for a document</p><p>6 0.43098199 <a title="194-lsi-6" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-09-10-Don%E2%80%99t_MAWK_AWK_%E2%80%93_the_fastest_and_most_elegant_big_data_munging_language%21.html">154 brendan oconnor ai-2009-09-10-Don’t MAWK AWK – the fastest and most elegant big data munging language!</a></p>
<p>7 0.42934591 <a title="194-lsi-7" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-22-Performance_comparison%3A_key-value_stores_for_language_model_counts.html">139 brendan oconnor ai-2009-04-22-Performance comparison: key-value stores for language model counts</a></p>
<p>8 0.42120215 <a title="194-lsi-8" href="../brendan_oconnor_ai-2014/brendan_oconnor_ai-2014-04-26-Replot%3A_departure_delays_vs_flight_time_speed-up.html">204 brendan oconnor ai-2014-04-26-Replot: departure delays vs flight time speed-up</a></p>
<p>9 0.41897213 <a title="194-lsi-9" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-09-21-CMU_ARK_Twitter_Part-of-Speech_Tagger_%E2%80%93_v0.3_released.html">187 brendan oconnor ai-2012-09-21-CMU ARK Twitter Part-of-Speech Tagger – v0.3 released</a></p>
<p>10 0.40744165 <a title="194-lsi-10" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-08-20-Some_analysis_of_tweet_shares_and_%E2%80%9Cpredicting%E2%80%9D_election_outcomes.html">198 brendan oconnor ai-2013-08-20-Some analysis of tweet shares and “predicting” election outcomes</a></p>
<p>11 0.39807791 <a title="194-lsi-11" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-02-02-Histograms_%E2%80%94_matplotlib_vs._R.html">179 brendan oconnor ai-2012-02-02-Histograms — matplotlib vs. R</a></p>
<p>12 0.39255115 <a title="194-lsi-12" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-06-29-Evangelicals_vs._Aquarians.html">66 brendan oconnor ai-2007-06-29-Evangelicals vs. Aquarians</a></p>
<p>13 0.38642806 <a title="194-lsi-13" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-05-29-%22Stanford_Impostor%22.html">62 brendan oconnor ai-2007-05-29-"Stanford Impostor"</a></p>
<p>14 0.3814967 <a title="194-lsi-14" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-06-17-Pairwise_comparisons_for_relevance_evaluation.html">106 brendan oconnor ai-2008-06-17-Pairwise comparisons for relevance evaluation</a></p>
<p>15 0.3429392 <a title="194-lsi-15" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-11-28-Calculating_running_variance_in_Python_and_C%2B%2B.html">128 brendan oconnor ai-2008-11-28-Calculating running variance in Python and C++</a></p>
<p>16 0.33681685 <a title="194-lsi-16" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-08-16-A_better_Obama_vs_McCain_poll_aggregation.html">111 brendan oconnor ai-2008-08-16-A better Obama vs McCain poll aggregation</a></p>
<p>17 0.31611729 <a title="194-lsi-17" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-07-01-Bias_correction_sneak_peek%21.html">108 brendan oconnor ai-2008-07-01-Bias correction sneak peek!</a></p>
<p>18 0.30348355 <a title="194-lsi-18" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-03-Statistics_vs._Machine_Learning%2C_fight%21.html">129 brendan oconnor ai-2008-12-03-Statistics vs. Machine Learning, fight!</a></p>
<p>19 0.29337877 <a title="194-lsi-19" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-11-21-Netflix_Prize.html">125 brendan oconnor ai-2008-11-21-Netflix Prize</a></p>
<p>20 0.29188526 <a title="194-lsi-20" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-08-08-Haghighi_and_Klein_%282009%29%3A_Simple_Coreference_Resolution_with_Rich_Syntactic_and_Semantic_Features.html">150 brendan oconnor ai-2009-08-08-Haghighi and Klein (2009): Simple Coreference Resolution with Rich Syntactic and Semantic Features</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/brendan_oconnor_ai_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(16, 0.021), (44, 0.128), (48, 0.031), (70, 0.026), (74, 0.086), (80, 0.605)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.97448528 <a title="194-lda-1" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-06-29-Evangelicals_vs._Aquarians.html">66 brendan oconnor ai-2007-06-29-Evangelicals vs. Aquarians</a></p>
<p>Introduction: Just read an interesting analysis on the the simultaneous rise of the cultural left and right (“hippies and evangelicals”) through the 50′s and 60′s.   Brink Lindsey argues here  that they were both reactions to post-war material prosperity:
  
On the left gathered those who were most alive to the new possibilities created by the unprecedented mass affluence of the postwar years but at the same time were hostile to the social institutions — namely, the market and the middle-class work ethic — that created those possibilities. On the right rallied those who staunchly supported the institutions that created prosperity but who shrank from the social dynamism they were unleashing. One side denounced capitalism but gobbled its fruits; the other cursed the fruits while defending the system that bore them. Both causes were quixotic, and consequently neither fully realized its ambitions.
  
I love  neat sweeping theories of history ; I can’t take it overly seriously but it is so fun.  Lindsey</p><p>2 0.97425616 <a title="194-lda-2" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-05-20-Log-normal_and_logistic-normal_terminology.html">169 brendan oconnor ai-2011-05-20-Log-normal and logistic-normal terminology</a></p>
<p>Introduction: I was cleaning my office and found a back-of-envelope diagram  Shay  drew me once, so I’m writing it up to not forget.  The definitions of the  logistic-normal  and  log-normal  distributions are a little confusing with regard to their relationship to the normal distribution.  If you draw samples from one, the arrows below show the transformation to make it such you have samples from another.
 
   
 
For example, if  x ~ Normal , then transforming as  y=exp(x)  implies  y ~ LogNormal .  The adjective terminology is inverted: the logistic function goes from normal to logistic-normal, but the log function goes from log-normal to normal (other way!).  The log of the log-normal is normal, but it’s the   logit   of the logistic normal that’s normal.
 
Here are densities of these different distributions via transformations from a standard normal.
 
   In R:   x=rnorm(1e6); hist(x); hist(exp(x)/(1+exp(x)); hist(exp(x)) 
 
Just to make things more confusing, note the logistic-normal distributi</p><p>same-blog 3 0.96173823 <a title="194-lda-3" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-04-16-Rise_and_fall_of_Dirichlet_process_clusters.html">194 brendan oconnor ai-2013-04-16-Rise and fall of Dirichlet process clusters</a></p>
<p>Introduction: Here’s Gibbs sampling for a  Dirichlet process 1-d mixture of Gaussians .  On 1000 data points that look like this.
 
   
 
I gave it fixed variance and a concentration and over MCMC iterations, and it looks like this.
 
   
 
The top is the number of points in a cluster.  The bottom are the cluster means.  Every cluster has a unique color.  During MCMC, clusters are created and destroyed.  Every cluster has a unique color; when a cluster dies, its color is never reused.  
 
I’m showing clusters every 100 iterations.  If there is a single point, that cluster was at that iteration but not before or after.  If there is a line, the cluster lived for at least 100 iterations.  Some clusters live long, some live short, but all eventually die.
 
Usually the model likes to think there are about two clusters, occupying positions at the two modes in the data distribution.  It also entertains the existence of several much more minor ones.  Usually these are shortlived clusters that die away.  But</p><p>4 0.90603858 <a title="194-lda-4" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-03-21-Statistics_is_big-N_logic%3F.html">54 brendan oconnor ai-2007-03-21-Statistics is big-N logic?</a></p>
<p>Introduction: I think I believe one of these things, but I’m not quite sure. 
 Statistics is just like logic, except with uncertainty. 
 
This would be true if statistics is Bayesian statistics and you buy the Bayesian inductive logic story — add induction to propositional logic, via a conditional credibility operator, and the Cox axioms imply standard probability theory as a consequence.  (That is, probability theory is logic with uncertainty.  And then a good Bayesian thinks probability theory and statistics are the same.)  Links:  Jaynes’ explanation ;  SEP article ; also  Fitelson’s article .  (Though there are negative results; all I can think of right now is a  Halpern  article on Cox; and also interesting is  Halpern and Koller .)
 
Secondly, here is another statement.
  

Statistics is just like logic, except with a big N.

  
This is a more data-driven view — the world is full of things and they need to be described.  Logical rules can help you describe things, but you also have to deal wit</p><p>5 0.55902898 <a title="194-lda-5" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-01-30-%E2%80%9CLogic_Bomb%E2%80%9D.html">134 brendan oconnor ai-2009-01-30-“Logic Bomb”</a></p>
<p>Introduction: Article:
 
 Fannie Mae Logic Bomb Would Have Caused Weeklong Shutdown | Threat Level from Wired.com .
 
I love the term “logic bomb”.  Can you pair it with a statistics bomb?  Data-driven bomb?  Or maybe the point is a connectionist bomb.</p><p>6 0.45203274 <a title="194-lda-6" href="../brendan_oconnor_ai-2011/brendan_oconnor_ai-2011-09-25-Information_theory_stuff.html">175 brendan oconnor ai-2011-09-25-Information theory stuff</a></p>
<p>7 0.43430066 <a title="194-lda-7" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-04-01-Binary_classification_evaluation_in_R_via_ROCR.html">136 brendan oconnor ai-2009-04-01-Binary classification evaluation in R via ROCR</a></p>
<p>8 0.37088686 <a title="194-lda-8" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-12-03-Statistics_vs._Machine_Learning%2C_fight%21.html">129 brendan oconnor ai-2008-12-03-Statistics vs. Machine Learning, fight!</a></p>
<p>9 0.3701725 <a title="194-lda-9" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-07-15-Beta_conjugate_explorer.html">146 brendan oconnor ai-2009-07-15-Beta conjugate explorer</a></p>
<p>10 0.35862175 <a title="194-lda-10" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-17-p-values%2C_CDF%E2%80%99s%2C_NLP_etc..html">185 brendan oconnor ai-2012-07-17-p-values, CDF’s, NLP etc.</a></p>
<p>11 0.35503438 <a title="194-lda-11" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-12-31-List_of_probabilistic_model_mini-language_toolkits.html">157 brendan oconnor ai-2009-12-31-List of probabilistic model mini-language toolkits</a></p>
<p>12 0.34389281 <a title="194-lda-12" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-04-21-What_inputs_do_Monte_Carlo_algorithms_need%3F.html">195 brendan oconnor ai-2013-04-21-What inputs do Monte Carlo algorithms need?</a></p>
<p>13 0.33728796 <a title="194-lda-13" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-08-16-A_better_Obama_vs_McCain_poll_aggregation.html">111 brendan oconnor ai-2008-08-16-A better Obama vs McCain poll aggregation</a></p>
<p>14 0.33713645 <a title="194-lda-14" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-08-31-Probabilistic_interpretation_of_the_B3_coreference_resolution_metric.html">199 brendan oconnor ai-2013-08-31-Probabilistic interpretation of the B3 coreference resolution metric</a></p>
<p>15 0.33162645 <a title="194-lda-15" href="../brendan_oconnor_ai-2013/brendan_oconnor_ai-2013-01-07-Perplexity_as_branching_factor%3B_as_Shannon_diversity_index.html">190 brendan oconnor ai-2013-01-07-Perplexity as branching factor; as Shannon diversity index</a></p>
<p>16 0.3275556 <a title="194-lda-16" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-07-04-The_%2460%2C000_cat%3A_deep_belief_networks_make_less_sense_for_language_than_vision.html">184 brendan oconnor ai-2012-07-04-The $60,000 cat: deep belief networks make less sense for language than vision</a></p>
<p>17 0.3208369 <a title="194-lda-17" href="../brendan_oconnor_ai-2008/brendan_oconnor_ai-2008-10-11-It_is_accurate_to_determine_a_blog%E2%80%99s_bias_by_what_it_links_to.html">117 brendan oconnor ai-2008-10-11-It is accurate to determine a blog’s bias by what it links to</a></p>
<p>18 0.32005161 <a title="194-lda-18" href="../brendan_oconnor_ai-2009/brendan_oconnor_ai-2009-08-08-Haghighi_and_Klein_%282009%29%3A_Simple_Coreference_Resolution_with_Rich_Syntactic_and_Semantic_Features.html">150 brendan oconnor ai-2009-08-08-Haghighi and Klein (2009): Simple Coreference Resolution with Rich Syntactic and Semantic Features</a></p>
<p>19 0.31491396 <a title="194-lda-19" href="../brendan_oconnor_ai-2012/brendan_oconnor_ai-2012-11-24-Graphs_for_SANCL-2012_web_parsing_results.html">189 brendan oconnor ai-2012-11-24-Graphs for SANCL-2012 web parsing results</a></p>
<p>20 0.30981797 <a title="194-lda-20" href="../brendan_oconnor_ai-2007/brendan_oconnor_ai-2007-11-26-How_did_Freud_become_a_respected_humanist%3F%21.html">84 brendan oconnor ai-2007-11-26-How did Freud become a respected humanist?!</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
