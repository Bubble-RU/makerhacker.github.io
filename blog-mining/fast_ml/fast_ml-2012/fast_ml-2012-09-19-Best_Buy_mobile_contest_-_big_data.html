<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>5 fast ml-2012-09-19-Best Buy mobile contest - big data</title>
</head>

<body>
<p><a title="fast_ml" href="../fast_ml_home.html">fast_ml</a> <a title="fast_ml-2012" href="../home/fast_ml-2012_home.html">fast_ml-2012</a> <a title="fast_ml-2012-5" href="#">fast_ml-2012-5</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>5 fast ml-2012-09-19-Best Buy mobile contest - big data</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="fast_ml-2012-5-html" href="http://fastml.com//best-buy-mobile-contest-big-data/">html</a></p><p>Introduction: Last time we talked about the   small data  branch of Best Buy contest . Now it’s time to tackle  the big boy . It is positioned as “cloud computing sized problem”, because there is 7GB of unpacked data, vs. younger brother’s 20MB. This is reflected in “cloud computing” and “cluster” and “Oracle” talk in  the forum , and also in small number of participating teams: so far, only six contestants managed to beat the benchmark.
 
But don’t be scared. Most of data mass is in XML product information. Training and test sets together are 378MB. Good news.
   
The really interesting thing is that we can  take the script we’ve used for small data and apply it to this contest, obtaining 0.355 in a few minutes  (benchmark is 0.304). Not impressed? With simple extension you can up the score to 0.55. Read below for details.
 
This is  the very same script , with one difference. In this challenge, benchmark recommendations differ from line to line, so we can’t just hard-code five item IDs like before</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Last time we talked about the   small data  branch of Best Buy contest . [sent-1, score-0.438]
</p><p>2 It is positioned as “cloud computing sized problem”, because there is 7GB of unpacked data, vs. [sent-3, score-0.261]
</p><p>3 This is reflected in “cloud computing” and “cluster” and “Oracle” talk in  the forum , and also in small number of participating teams: so far, only six contestants managed to beat the benchmark. [sent-5, score-0.734]
</p><p>4 The really interesting thing is that we can  take the script we’ve used for small data and apply it to this contest, obtaining 0. [sent-10, score-0.389]
</p><p>5 In this challenge, benchmark recommendations differ from line to line, so we can’t just hard-code five item IDs like before. [sent-18, score-0.66]
</p><p>6 Instead, we will read the benchmark file in parallel with test file, so that when we need benchmark items, we have them handy:         1   2        for line in reader:     popular_skus = bench_reader. [sent-19, score-0.87]
</p><p>7 txt    Main difference between the two contests, except data size, is that here we’re dealing with many product categories, not just Xbox games. [sent-26, score-0.451]
</p><p>8 The benchmark recommends most popular products  in a given category , not globally. [sent-27, score-0.501]
</p><p>9 If we build our  query -> product  mapping taking categories into account, the score will go up dramatically, as promised. [sent-28, score-0.862]
</p><p>10 This is left as an exercise for the reader , as some of those academic types say. [sent-29, score-0.638]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('categories', 0.312), ('product', 0.265), ('benchmark', 0.256), ('cloud', 0.229), ('reader', 0.207), ('small', 0.177), ('contest', 0.131), ('computing', 0.131), ('read', 0.131), ('contests', 0.13), ('recommends', 0.13), ('xbox', 0.13), ('academic', 0.13), ('account', 0.13), ('contestants', 0.13), ('fun', 0.13), ('impressed', 0.13), ('obtaining', 0.13), ('sized', 0.13), ('talked', 0.13), ('teams', 0.13), ('xml', 0.13), ('line', 0.123), ('query', 0.115), ('category', 0.115), ('forum', 0.115), ('minutes', 0.115), ('types', 0.115), ('item', 0.104), ('items', 0.104), ('buy', 0.104), ('cluster', 0.104), ('except', 0.104), ('exercise', 0.104), ('extension', 0.104), ('managed', 0.104), ('parallel', 0.104), ('six', 0.104), ('talk', 0.104), ('recommendations', 0.095), ('ids', 0.095), ('size', 0.095), ('together', 0.095), ('build', 0.088), ('really', 0.082), ('usage', 0.082), ('dealing', 0.082), ('differ', 0.082), ('left', 0.082), ('taking', 0.082)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000002 <a title="5-tfidf-1" href="../fast_ml-2012/fast_ml-2012-09-19-Best_Buy_mobile_contest_-_big_data.html">5 fast ml-2012-09-19-Best Buy mobile contest - big data</a></p>
<p>Introduction: Last time we talked about the   small data  branch of Best Buy contest . Now it’s time to tackle  the big boy . It is positioned as “cloud computing sized problem”, because there is 7GB of unpacked data, vs. younger brother’s 20MB. This is reflected in “cloud computing” and “cluster” and “Oracle” talk in  the forum , and also in small number of participating teams: so far, only six contestants managed to beat the benchmark.
 
But don’t be scared. Most of data mass is in XML product information. Training and test sets together are 378MB. Good news.
   
The really interesting thing is that we can  take the script we’ve used for small data and apply it to this contest, obtaining 0.355 in a few minutes  (benchmark is 0.304). Not impressed? With simple extension you can up the score to 0.55. Read below for details.
 
This is  the very same script , with one difference. In this challenge, benchmark recommendations differ from line to line, so we can’t just hard-code five item IDs like before</p><p>2 0.24673815 <a title="5-tfidf-2" href="../fast_ml-2012/fast_ml-2012-09-17-Best_Buy_mobile_contest.html">4 fast ml-2012-09-17-Best Buy mobile contest</a></p>
<p>Introduction: There’s a contest on Kaggle called  ACM Hackaton . Actually, there are two, one based on small data and one on big data. Here we will be talking about small data contest - specifically, about beating the benchmark - but the ideas are equally applicable to both.
 
The deal is, we have a training set from Best Buy with search queries and items which users clicked after the query, plus some other data. Items are in this case Xbox games like “Batman” or “Rocksmith” or “Call of Duty”. We are asked to predict an item user clicked given the query. Metric is MAP@5 (see  an explanation of MAP ).
   
The problem isn’t typical for Kaggle, because it doesn’t really require using machine learning in traditional sense. To beat the benchmark, it’s enough to write a short script. Concretely ;), we’re gonna build a mapping from queries to items, using the training set. It will be just a Python dictionary looking like this:
 
 'forzasteeringwheel': {'2078113': 1}, 
 'finalfantasy13': {'9461183': 3, '351</p><p>3 0.1658964 <a title="5-tfidf-3" href="../fast_ml-2012/fast_ml-2012-09-25-Best_Buy_mobile_contest_-_full_disclosure.html">6 fast ml-2012-09-25-Best Buy mobile contest - full disclosure</a></p>
<p>Introduction: Here’s the final version of the script, with two ideas improving on our previous take: searching in product names and spelling correction.
   
As far as product names go, we will use product data available in XML format to extract SKU and name for each product:
   8564564 Ace Combat 6: Fires of Liberation Platinum Hits
2755149 Ace Combat: Assault Horizon
1208344 Adrenalin Misfits
   
Further, we will process the names in the same way we processed queries:
   8564564 acecombat6firesofliberationplatinumhits
2755149 acecombatassaulthorizon
1208344 adrenalinmisfits
   
When we need to fill in some predictions, instead of taking them from the benchmark, we will search in names. If that is not enough, then we go to the benchmark.
 
But wait! There’s more. We will spell-correct test queries when looking in our  query -> sku  mapping. For example, we may have a SKU for  L.A. Noire  ( lanoire ), but not for  L.A. Noir  ( lanoir ). It is easy to see that this is basically the same query, o</p><p>4 0.14601281 <a title="5-tfidf-4" href="../fast_ml-2013/fast_ml-2013-07-05-Processing_large_files%2C_line_by_line.html">32 fast ml-2013-07-05-Processing large files, line by line</a></p>
<p>Introduction: Perhaps the most common format of data for machine learning is text files. Often data is too large to fit in memory; this is sometimes referred to as big data. But do you need to load the whole data into memory? Maybe you could at least pre-process it line by line. We show how to do this with Python. Prepare to read and possibly write some code.
   
The most common format for text files is probably CSV. For sparse data,  libsvm  format is popular. Both can be processed using csv module in Python.
   import csv

i_f = open( input_file, 'r' )
reader = csv.reader( i_f )
   
For  libsvm  you just set the delimiter to space:
   reader = csv.reader( i_f, delimiter = ' ' )
   
Then you go over the file contents. Each line is a list of strings:
   for line in reader:

    # do something with the line, for example:
    label = float( line[0] )
    # ....

    writer.writerow( line )
   
If you need to do a second pass, you just rewind the input file:
   i_f.seek( 0 )
for line in re</p><p>5 0.087565042 <a title="5-tfidf-5" href="../fast_ml-2013/fast_ml-2013-10-09-Big_data_made_easy.html">41 fast ml-2013-10-09-Big data made easy</a></p>
<p>Introduction: An overview of key points about big data. This post was inspired by a very good article about big data by Chris Stucchio (linked below). The article is about hype and technology. We hate the hype.
    Big data is hype  
Everybody talks about big data; nobody knows exactly what it is. That’s pretty much the definition of hype.  Google Trends  suggest that the term took off at the beginning of 2011 (and the searches are coming mainly from Asia, curiously).
 
 
 
Now, to put things in context:
 
 
 
Big data is right there (or maybe not quite yet?) with other slogans like  web 2.0 ,  cloud computing  and  social media .
In effect,  big data  is a generic term for:
  
 data science 
 machine learning 
 data mining 
 predictive analytics 
  
and so on. Don’t believe us? What about James Goodnight, the CEO of  SAS :
  
The term big data is being used today because computer analysts and journalists got tired of writing about cloud computing. Before cloud computing it was data warehousing or</p><p>6 0.077085175 <a title="5-tfidf-6" href="../fast_ml-2012/fast_ml-2012-08-27-Kaggle_job_recommendation_challenge.html">2 fast ml-2012-08-27-Kaggle job recommendation challenge</a></p>
<p>7 0.066231996 <a title="5-tfidf-7" href="../fast_ml-2013/fast_ml-2013-02-07-The_secret_of_the_big_guys.html">19 fast ml-2013-02-07-The secret of the big guys</a></p>
<p>8 0.06304495 <a title="5-tfidf-8" href="../fast_ml-2013/fast_ml-2013-07-09-Introducing_phraug.html">33 fast ml-2013-07-09-Introducing phraug</a></p>
<p>9 0.062868327 <a title="5-tfidf-9" href="../fast_ml-2013/fast_ml-2013-02-18-Predicting_advertised_salaries.html">20 fast ml-2013-02-18-Predicting advertised salaries</a></p>
<p>10 0.062484141 <a title="5-tfidf-10" href="../fast_ml-2012/fast_ml-2012-08-09-What_you_wanted_to_know_about_Mean_Average_Precision.html">1 fast ml-2012-08-09-What you wanted to know about Mean Average Precision</a></p>
<p>11 0.061559644 <a title="5-tfidf-11" href="../fast_ml-2014/fast_ml-2014-04-01-Exclusive_Geoff_Hinton_interview.html">57 fast ml-2014-04-01-Exclusive Geoff Hinton interview</a></p>
<p>12 0.061138812 <a title="5-tfidf-12" href="../fast_ml-2013/fast_ml-2013-04-10-Gender_discrimination.html">25 fast ml-2013-04-10-Gender discrimination</a></p>
<p>13 0.05772128 <a title="5-tfidf-13" href="../fast_ml-2012/fast_ml-2012-10-15-Merck_challenge.html">8 fast ml-2012-10-15-Merck challenge</a></p>
<p>14 0.052094795 <a title="5-tfidf-14" href="../fast_ml-2013/fast_ml-2013-08-12-Accelerometer_Biometric_Competition.html">35 fast ml-2013-08-12-Accelerometer Biometric Competition</a></p>
<p>15 0.051549453 <a title="5-tfidf-15" href="../fast_ml-2013/fast_ml-2013-06-19-Go_non-linear_with_Vowpal_Wabbit.html">31 fast ml-2013-06-19-Go non-linear with Vowpal Wabbit</a></p>
<p>16 0.051087283 <a title="5-tfidf-16" href="../fast_ml-2013/fast_ml-2013-09-09-Predicting_solar_energy_from_weather_forecasts_plus_a_NetCDF4_tutorial.html">38 fast ml-2013-09-09-Predicting solar energy from weather forecasts plus a NetCDF4 tutorial</a></p>
<p>17 0.044421598 <a title="5-tfidf-17" href="../fast_ml-2012/fast_ml-2012-10-05-Predicting_closed_questions_on_Stack_Overflow.html">7 fast ml-2012-10-05-Predicting closed questions on Stack Overflow</a></p>
<p>18 0.042426258 <a title="5-tfidf-18" href="../fast_ml-2012/fast_ml-2012-12-21-Tuning_hyperparams_automatically_with_Spearmint.html">12 fast ml-2012-12-21-Tuning hyperparams automatically with Spearmint</a></p>
<p>19 0.04221978 <a title="5-tfidf-19" href="../fast_ml-2013/fast_ml-2013-11-02-Maxing_out_the_digits.html">43 fast ml-2013-11-02-Maxing out the digits</a></p>
<p>20 0.04065479 <a title="5-tfidf-20" href="../fast_ml-2014/fast_ml-2014-05-26-Yann_LeCun%27s_answers_from_the_Reddit_AMA.html">62 fast ml-2014-05-26-Yann LeCun's answers from the Reddit AMA</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/fast_ml_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.2), (1, -0.149), (2, 0.091), (3, -0.475), (4, 0.032), (5, -0.075), (6, 0.023), (7, 0.004), (8, 0.0), (9, -0.314), (10, -0.123), (11, 0.057), (12, -0.079), (13, -0.065), (14, 0.065), (15, 0.081), (16, -0.007), (17, -0.133), (18, -0.014), (19, 0.067), (20, 0.095), (21, -0.015), (22, -0.109), (23, -0.052), (24, -0.013), (25, 0.006), (26, -0.233), (27, -0.089), (28, -0.023), (29, -0.067), (30, -0.129), (31, -0.063), (32, -0.042), (33, -0.063), (34, 0.054), (35, -0.037), (36, -0.118), (37, 0.006), (38, -0.08), (39, 0.142), (40, -0.022), (41, 0.184), (42, 0.007), (43, 0.022), (44, -0.069), (45, 0.046), (46, -0.035), (47, 0.268), (48, 0.038), (49, 0.402)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98610312 <a title="5-lsi-1" href="../fast_ml-2012/fast_ml-2012-09-19-Best_Buy_mobile_contest_-_big_data.html">5 fast ml-2012-09-19-Best Buy mobile contest - big data</a></p>
<p>Introduction: Last time we talked about the   small data  branch of Best Buy contest . Now it’s time to tackle  the big boy . It is positioned as “cloud computing sized problem”, because there is 7GB of unpacked data, vs. younger brother’s 20MB. This is reflected in “cloud computing” and “cluster” and “Oracle” talk in  the forum , and also in small number of participating teams: so far, only six contestants managed to beat the benchmark.
 
But don’t be scared. Most of data mass is in XML product information. Training and test sets together are 378MB. Good news.
   
The really interesting thing is that we can  take the script we’ve used for small data and apply it to this contest, obtaining 0.355 in a few minutes  (benchmark is 0.304). Not impressed? With simple extension you can up the score to 0.55. Read below for details.
 
This is  the very same script , with one difference. In this challenge, benchmark recommendations differ from line to line, so we can’t just hard-code five item IDs like before</p><p>2 0.46469393 <a title="5-lsi-2" href="../fast_ml-2012/fast_ml-2012-09-17-Best_Buy_mobile_contest.html">4 fast ml-2012-09-17-Best Buy mobile contest</a></p>
<p>Introduction: There’s a contest on Kaggle called  ACM Hackaton . Actually, there are two, one based on small data and one on big data. Here we will be talking about small data contest - specifically, about beating the benchmark - but the ideas are equally applicable to both.
 
The deal is, we have a training set from Best Buy with search queries and items which users clicked after the query, plus some other data. Items are in this case Xbox games like “Batman” or “Rocksmith” or “Call of Duty”. We are asked to predict an item user clicked given the query. Metric is MAP@5 (see  an explanation of MAP ).
   
The problem isn’t typical for Kaggle, because it doesn’t really require using machine learning in traditional sense. To beat the benchmark, it’s enough to write a short script. Concretely ;), we’re gonna build a mapping from queries to items, using the training set. It will be just a Python dictionary looking like this:
 
 'forzasteeringwheel': {'2078113': 1}, 
 'finalfantasy13': {'9461183': 3, '351</p><p>3 0.35166231 <a title="5-lsi-3" href="../fast_ml-2013/fast_ml-2013-07-05-Processing_large_files%2C_line_by_line.html">32 fast ml-2013-07-05-Processing large files, line by line</a></p>
<p>Introduction: Perhaps the most common format of data for machine learning is text files. Often data is too large to fit in memory; this is sometimes referred to as big data. But do you need to load the whole data into memory? Maybe you could at least pre-process it line by line. We show how to do this with Python. Prepare to read and possibly write some code.
   
The most common format for text files is probably CSV. For sparse data,  libsvm  format is popular. Both can be processed using csv module in Python.
   import csv

i_f = open( input_file, 'r' )
reader = csv.reader( i_f )
   
For  libsvm  you just set the delimiter to space:
   reader = csv.reader( i_f, delimiter = ' ' )
   
Then you go over the file contents. Each line is a list of strings:
   for line in reader:

    # do something with the line, for example:
    label = float( line[0] )
    # ....

    writer.writerow( line )
   
If you need to do a second pass, you just rewind the input file:
   i_f.seek( 0 )
for line in re</p><p>4 0.26047567 <a title="5-lsi-4" href="../fast_ml-2012/fast_ml-2012-09-25-Best_Buy_mobile_contest_-_full_disclosure.html">6 fast ml-2012-09-25-Best Buy mobile contest - full disclosure</a></p>
<p>Introduction: Here’s the final version of the script, with two ideas improving on our previous take: searching in product names and spelling correction.
   
As far as product names go, we will use product data available in XML format to extract SKU and name for each product:
   8564564 Ace Combat 6: Fires of Liberation Platinum Hits
2755149 Ace Combat: Assault Horizon
1208344 Adrenalin Misfits
   
Further, we will process the names in the same way we processed queries:
   8564564 acecombat6firesofliberationplatinumhits
2755149 acecombatassaulthorizon
1208344 adrenalinmisfits
   
When we need to fill in some predictions, instead of taking them from the benchmark, we will search in names. If that is not enough, then we go to the benchmark.
 
But wait! There’s more. We will spell-correct test queries when looking in our  query -> sku  mapping. For example, we may have a SKU for  L.A. Noire  ( lanoire ), but not for  L.A. Noir  ( lanoir ). It is easy to see that this is basically the same query, o</p><p>5 0.17377505 <a title="5-lsi-5" href="../fast_ml-2013/fast_ml-2013-10-09-Big_data_made_easy.html">41 fast ml-2013-10-09-Big data made easy</a></p>
<p>Introduction: An overview of key points about big data. This post was inspired by a very good article about big data by Chris Stucchio (linked below). The article is about hype and technology. We hate the hype.
    Big data is hype  
Everybody talks about big data; nobody knows exactly what it is. That’s pretty much the definition of hype.  Google Trends  suggest that the term took off at the beginning of 2011 (and the searches are coming mainly from Asia, curiously).
 
 
 
Now, to put things in context:
 
 
 
Big data is right there (or maybe not quite yet?) with other slogans like  web 2.0 ,  cloud computing  and  social media .
In effect,  big data  is a generic term for:
  
 data science 
 machine learning 
 data mining 
 predictive analytics 
  
and so on. Don’t believe us? What about James Goodnight, the CEO of  SAS :
  
The term big data is being used today because computer analysts and journalists got tired of writing about cloud computing. Before cloud computing it was data warehousing or</p><p>6 0.14814278 <a title="5-lsi-6" href="../fast_ml-2013/fast_ml-2013-03-25-Dimensionality_reduction_for_sparse_binary_data_-_an_overview.html">24 fast ml-2013-03-25-Dimensionality reduction for sparse binary data - an overview</a></p>
<p>7 0.1438553 <a title="5-lsi-7" href="../fast_ml-2013/fast_ml-2013-02-07-The_secret_of_the_big_guys.html">19 fast ml-2013-02-07-The secret of the big guys</a></p>
<p>8 0.13789253 <a title="5-lsi-8" href="../fast_ml-2013/fast_ml-2013-04-10-Gender_discrimination.html">25 fast ml-2013-04-10-Gender discrimination</a></p>
<p>9 0.13697758 <a title="5-lsi-9" href="../fast_ml-2013/fast_ml-2013-08-12-Accelerometer_Biometric_Competition.html">35 fast ml-2013-08-12-Accelerometer Biometric Competition</a></p>
<p>10 0.13006817 <a title="5-lsi-10" href="../fast_ml-2012/fast_ml-2012-08-27-Kaggle_job_recommendation_challenge.html">2 fast ml-2012-08-27-Kaggle job recommendation challenge</a></p>
<p>11 0.12388714 <a title="5-lsi-11" href="../fast_ml-2013/fast_ml-2013-09-09-Predicting_solar_energy_from_weather_forecasts_plus_a_NetCDF4_tutorial.html">38 fast ml-2013-09-09-Predicting solar energy from weather forecasts plus a NetCDF4 tutorial</a></p>
<p>12 0.12342736 <a title="5-lsi-12" href="../fast_ml-2012/fast_ml-2012-10-15-Merck_challenge.html">8 fast ml-2012-10-15-Merck challenge</a></p>
<p>13 0.11223497 <a title="5-lsi-13" href="../fast_ml-2013/fast_ml-2013-03-18-Large_scale_L1_feature_selection_with_Vowpal_Wabbit.html">23 fast ml-2013-03-18-Large scale L1 feature selection with Vowpal Wabbit</a></p>
<p>14 0.10847709 <a title="5-lsi-14" href="../fast_ml-2013/fast_ml-2013-07-14-Running_things_on_a_GPU.html">34 fast ml-2013-07-14-Running things on a GPU</a></p>
<p>15 0.10624247 <a title="5-lsi-15" href="../fast_ml-2013/fast_ml-2013-04-17-Regression_as_classification.html">26 fast ml-2013-04-17-Regression as classification</a></p>
<p>16 0.10032046 <a title="5-lsi-16" href="../fast_ml-2014/fast_ml-2014-05-08-Impute_missing_values_with_Amelia.html">61 fast ml-2014-05-08-Impute missing values with Amelia</a></p>
<p>17 0.09777011 <a title="5-lsi-17" href="../fast_ml-2013/fast_ml-2013-11-02-Maxing_out_the_digits.html">43 fast ml-2013-11-02-Maxing out the digits</a></p>
<p>18 0.09673395 <a title="5-lsi-18" href="../fast_ml-2013/fast_ml-2013-06-19-Go_non-linear_with_Vowpal_Wabbit.html">31 fast ml-2013-06-19-Go non-linear with Vowpal Wabbit</a></p>
<p>19 0.096598968 <a title="5-lsi-19" href="../fast_ml-2014/fast_ml-2014-03-06-PyBrain_-_a_simple_neural_networks_library_in_Python.html">54 fast ml-2014-03-06-PyBrain - a simple neural networks library in Python</a></p>
<p>20 0.096200071 <a title="5-lsi-20" href="../fast_ml-2014/fast_ml-2014-05-26-Yann_LeCun%27s_answers_from_the_Reddit_AMA.html">62 fast ml-2014-05-26-Yann LeCun's answers from the Reddit AMA</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/fast_ml_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(69, 0.069), (99, 0.822)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99308354 <a title="5-lda-1" href="../fast_ml-2012/fast_ml-2012-09-19-Best_Buy_mobile_contest_-_big_data.html">5 fast ml-2012-09-19-Best Buy mobile contest - big data</a></p>
<p>Introduction: Last time we talked about the   small data  branch of Best Buy contest . Now it’s time to tackle  the big boy . It is positioned as “cloud computing sized problem”, because there is 7GB of unpacked data, vs. younger brother’s 20MB. This is reflected in “cloud computing” and “cluster” and “Oracle” talk in  the forum , and also in small number of participating teams: so far, only six contestants managed to beat the benchmark.
 
But don’t be scared. Most of data mass is in XML product information. Training and test sets together are 378MB. Good news.
   
The really interesting thing is that we can  take the script we’ve used for small data and apply it to this contest, obtaining 0.355 in a few minutes  (benchmark is 0.304). Not impressed? With simple extension you can up the score to 0.55. Read below for details.
 
This is  the very same script , with one difference. In this challenge, benchmark recommendations differ from line to line, so we can’t just hard-code five item IDs like before</p><p>2 0.96301633 <a title="5-lda-2" href="../fast_ml-2012/fast_ml-2012-09-25-Best_Buy_mobile_contest_-_full_disclosure.html">6 fast ml-2012-09-25-Best Buy mobile contest - full disclosure</a></p>
<p>Introduction: Here’s the final version of the script, with two ideas improving on our previous take: searching in product names and spelling correction.
   
As far as product names go, we will use product data available in XML format to extract SKU and name for each product:
   8564564 Ace Combat 6: Fires of Liberation Platinum Hits
2755149 Ace Combat: Assault Horizon
1208344 Adrenalin Misfits
   
Further, we will process the names in the same way we processed queries:
   8564564 acecombat6firesofliberationplatinumhits
2755149 acecombatassaulthorizon
1208344 adrenalinmisfits
   
When we need to fill in some predictions, instead of taking them from the benchmark, we will search in names. If that is not enough, then we go to the benchmark.
 
But wait! There’s more. We will spell-correct test queries when looking in our  query -> sku  mapping. For example, we may have a SKU for  L.A. Noire  ( lanoire ), but not for  L.A. Noir  ( lanoir ). It is easy to see that this is basically the same query, o</p><p>3 0.74999994 <a title="5-lda-3" href="../fast_ml-2012/fast_ml-2012-09-17-Best_Buy_mobile_contest.html">4 fast ml-2012-09-17-Best Buy mobile contest</a></p>
<p>Introduction: There’s a contest on Kaggle called  ACM Hackaton . Actually, there are two, one based on small data and one on big data. Here we will be talking about small data contest - specifically, about beating the benchmark - but the ideas are equally applicable to both.
 
The deal is, we have a training set from Best Buy with search queries and items which users clicked after the query, plus some other data. Items are in this case Xbox games like “Batman” or “Rocksmith” or “Call of Duty”. We are asked to predict an item user clicked given the query. Metric is MAP@5 (see  an explanation of MAP ).
   
The problem isn’t typical for Kaggle, because it doesn’t really require using machine learning in traditional sense. To beat the benchmark, it’s enough to write a short script. Concretely ;), we’re gonna build a mapping from queries to items, using the training set. It will be just a Python dictionary looking like this:
 
 'forzasteeringwheel': {'2078113': 1}, 
 'finalfantasy13': {'9461183': 3, '351</p><p>4 0.6003477 <a title="5-lda-4" href="../fast_ml-2013/fast_ml-2013-01-12-Intro_to_random_forests.html">16 fast ml-2013-01-12-Intro to random forests</a></p>
<p>Introduction: Let’s step back from forays into cutting edge topics and look at a random forest, one of the most popular machine learning techniques today. Why is it so attractive?
   
First of all, decision tree ensembles have been found by  Caruana et al.  as the best overall approach for a variety of problems. Random forests, specifically, perform well both in low dimensional and high dimensional tasks.
 
There are basically two kinds of tree ensembles: bagged trees and boosted trees. Bagging means that when building each subsequent tree, we don’t look at the earlier trees, while in boosting we consider the earlier trees and strive to compensate for their weaknesses (which may lead to overfitting).
 
Random forest is an example of the bagging approach, less prone to overfit. Gradient boosted trees (notably  GBM package  in R) represent the other one. Both are very successful in many applications. Trees are also relatively fast to train, compared to some more involved methods.
 
Besides effectivnes</p><p>5 0.48386353 <a title="5-lda-5" href="../fast_ml-2013/fast_ml-2013-10-09-Big_data_made_easy.html">41 fast ml-2013-10-09-Big data made easy</a></p>
<p>Introduction: An overview of key points about big data. This post was inspired by a very good article about big data by Chris Stucchio (linked below). The article is about hype and technology. We hate the hype.
    Big data is hype  
Everybody talks about big data; nobody knows exactly what it is. That’s pretty much the definition of hype.  Google Trends  suggest that the term took off at the beginning of 2011 (and the searches are coming mainly from Asia, curiously).
 
 
 
Now, to put things in context:
 
 
 
Big data is right there (or maybe not quite yet?) with other slogans like  web 2.0 ,  cloud computing  and  social media .
In effect,  big data  is a generic term for:
  
 data science 
 machine learning 
 data mining 
 predictive analytics 
  
and so on. Don’t believe us? What about James Goodnight, the CEO of  SAS :
  
The term big data is being used today because computer analysts and journalists got tired of writing about cloud computing. Before cloud computing it was data warehousing or</p><p>6 0.3580094 <a title="5-lda-6" href="../fast_ml-2012/fast_ml-2012-10-25-So_you_want_to_work_for_Facebook.html">9 fast ml-2012-10-25-So you want to work for Facebook</a></p>
<p>7 0.35307398 <a title="5-lda-7" href="../fast_ml-2012/fast_ml-2012-08-27-Kaggle_job_recommendation_challenge.html">2 fast ml-2012-08-27-Kaggle job recommendation challenge</a></p>
<p>8 0.34736514 <a title="5-lda-8" href="../fast_ml-2013/fast_ml-2013-04-10-Gender_discrimination.html">25 fast ml-2013-04-10-Gender discrimination</a></p>
<p>9 0.33679882 <a title="5-lda-9" href="../fast_ml-2014/fast_ml-2014-05-08-Impute_missing_values_with_Amelia.html">61 fast ml-2014-05-08-Impute missing values with Amelia</a></p>
<p>10 0.30314511 <a title="5-lda-10" href="../fast_ml-2013/fast_ml-2013-03-25-Dimensionality_reduction_for_sparse_binary_data_-_an_overview.html">24 fast ml-2013-03-25-Dimensionality reduction for sparse binary data - an overview</a></p>
<p>11 0.29840973 <a title="5-lda-11" href="../fast_ml-2013/fast_ml-2013-08-12-Accelerometer_Biometric_Competition.html">35 fast ml-2013-08-12-Accelerometer Biometric Competition</a></p>
<p>12 0.29576695 <a title="5-lda-12" href="../fast_ml-2013/fast_ml-2013-05-12-And_deliver_us_from_Weka.html">28 fast ml-2013-05-12-And deliver us from Weka</a></p>
<p>13 0.28969091 <a title="5-lda-13" href="../fast_ml-2014/fast_ml-2014-04-21-Predicting_happiness_from_demographics_and_poll_answers.html">59 fast ml-2014-04-21-Predicting happiness from demographics and poll answers</a></p>
<p>14 0.2812798 <a title="5-lda-14" href="../fast_ml-2013/fast_ml-2013-12-28-Regularizing_neural_networks_with_dropout_and_with_DropConnect.html">48 fast ml-2013-12-28-Regularizing neural networks with dropout and with DropConnect</a></p>
<p>15 0.26450416 <a title="5-lda-15" href="../fast_ml-2013/fast_ml-2013-09-03-Our_followers_and_who_else_they_follow.html">37 fast ml-2013-09-03-Our followers and who else they follow</a></p>
<p>16 0.26058537 <a title="5-lda-16" href="../fast_ml-2013/fast_ml-2013-09-09-Predicting_solar_energy_from_weather_forecasts_plus_a_NetCDF4_tutorial.html">38 fast ml-2013-09-09-Predicting solar energy from weather forecasts plus a NetCDF4 tutorial</a></p>
<p>17 0.25797498 <a title="5-lda-17" href="../fast_ml-2013/fast_ml-2013-02-07-The_secret_of_the_big_guys.html">19 fast ml-2013-02-07-The secret of the big guys</a></p>
<p>18 0.25120968 <a title="5-lda-18" href="../fast_ml-2013/fast_ml-2013-11-02-Maxing_out_the_digits.html">43 fast ml-2013-11-02-Maxing out the digits</a></p>
<p>19 0.25084874 <a title="5-lda-19" href="../fast_ml-2013/fast_ml-2013-07-14-Running_things_on_a_GPU.html">34 fast ml-2013-07-14-Running things on a GPU</a></p>
<p>20 0.24775872 <a title="5-lda-20" href="../fast_ml-2013/fast_ml-2013-02-18-Predicting_advertised_salaries.html">20 fast ml-2013-02-18-Predicting advertised salaries</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
