<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>11 fast ml-2012-12-07-Predicting wine quality</title>
</head>

<body>
<p><a title="fast_ml" href="../fast_ml_home.html">fast_ml</a> <a title="fast_ml-2012" href="../home/fast_ml-2012_home.html">fast_ml-2012</a> <a title="fast_ml-2012-11" href="#">fast_ml-2012-11</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>11 fast ml-2012-12-07-Predicting wine quality</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="fast_ml-2012-11-html" href="http://fastml.com//predicting-wine-quality/">html</a></p><p>Introduction: This post is as much about wine as it is about machine learning, so if you enjoy wine, like we do, you may find it especially interesting. Here’s some  R and Matlab code , and if you want to get right to the point,  skip to the charts .
 
There’s a book by Philipp Janert called  Data Analysis with Open Source Tools , which, by the way, we would recommend. From this book we found out about  the wine quality datasets . There are two, one for red wine and one for white wine, and they are interesting because they contain quality ratings (1 - 10) for a few thousands of wines, along with their physical and chemical properties. We could probably use these properties to predict a rating for a wine.   We’ll be looking at white and red wine separately for the reasons you will see shortly.
  Principal component analysis for white wine  
Janert performs a principal component analysis (PCA) and shows a resulting plot for white wine. What’s interesting about this plot is that judging by the first tw</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 This post is as much about wine as it is about machine learning, so if you enjoy wine, like we do, you may find it especially interesting. [sent-1, score-0.579]
</p><p>2 From this book we found out about  the wine quality datasets . [sent-4, score-0.689]
</p><p>3 There are two, one for red wine and one for white wine, and they are interesting because they contain quality ratings (1 - 10) for a few thousands of wines, along with their physical and chemical properties. [sent-5, score-1.027]
</p><p>4 We’ll be looking at white and red wine separately for the reasons you will see shortly. [sent-7, score-0.832]
</p><p>5 Principal component analysis for white wine   Janert performs a principal component analysis (PCA) and shows a resulting plot for white wine. [sent-8, score-1.232]
</p><p>6 What’s interesting about this plot is that judging by the first two principal components, a quality is very much correlated with alcohol content and fixed acidity (or pH - it’s basically the same thing). [sent-9, score-1.462]
</p><p>7 It is true especially for alcohol content, which is listed on every bottle. [sent-11, score-0.507]
</p><p>8 Now let’s do some more analysis involving quality, alcohol content and fixed acidity. [sent-15, score-0.991]
</p><p>9 Let’s introduce some terminology:      good  means rating seven or higher    bad  means rating four or lower    mediocre  means rating five    OK  means rating six. [sent-18, score-1.201]
</p><p>10 White wine   Good and bad   Red means good. [sent-20, score-0.67]
</p><p>11 Horizontal axis is alcohol content, vertical axis is fixed acidity. [sent-22, score-0.921]
</p><p>12 If alcohol content is at least 11%, or better yet, 12%, you are very likely to have a good wine. [sent-23, score-0.738]
</p><p>13 Basically, you can get away with lower alcohol content if the wine has relatively less fixed acidity. [sent-27, score-1.537]
</p><p>14 If you see a wine on the right side of that line, it is very likely to be good. [sent-30, score-0.616]
</p><p>15 Red wine   Alcohol OR acidity, please       Similarly to white wines, if alcohol content is 12% or more, probably you’re all set. [sent-35, score-1.38]
</p><p>16 Also, you want a wine which is either high in alcohol or highly acidic. [sent-36, score-1.038]
</p><p>17 In other words, there are quite a lot good wines with less than 12% alcohol; what they have in common is high fixed acidity and some mediocre ones between them. [sent-37, score-1.004]
</p><p>18 Compare with a white wine case - it’s the other way around. [sent-39, score-0.687]
</p><p>19 To sum up, judging a wine on just two properties is rather simplistic. [sent-42, score-0.644]
</p><p>20 All other things being equal, we much prefer a two year old wine to a one year old wine. [sent-44, score-0.733]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('wine', 0.517), ('alcohol', 0.475), ('wines', 0.256), ('fixed', 0.243), ('acidity', 0.219), ('content', 0.218), ('rating', 0.182), ('white', 0.17), ('mediocre', 0.146), ('red', 0.145), ('quality', 0.111), ('bad', 0.107), ('charts', 0.091), ('principal', 0.089), ('axis', 0.077), ('assumptions', 0.073), ('janert', 0.073), ('ph', 0.073), ('picture', 0.073), ('terminology', 0.073), ('components', 0.073), ('properties', 0.073), ('book', 0.061), ('analysis', 0.055), ('ratings', 0.054), ('similarly', 0.054), ('judging', 0.054), ('old', 0.054), ('side', 0.054), ('year', 0.054), ('plot', 0.053), ('story', 0.049), ('vertical', 0.049), ('less', 0.048), ('means', 0.046), ('lot', 0.046), ('high', 0.046), ('likely', 0.045), ('horizontal', 0.045), ('pca', 0.045), ('tell', 0.045), ('shows', 0.041), ('component', 0.041), ('lower', 0.036), ('especially', 0.032), ('bear', 0.03), ('contain', 0.03), ('draw', 0.03), ('enjoy', 0.03), ('generalized', 0.03)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="11-tfidf-1" href="../fast_ml-2012/fast_ml-2012-12-07-Predicting_wine_quality.html">11 fast ml-2012-12-07-Predicting wine quality</a></p>
<p>Introduction: This post is as much about wine as it is about machine learning, so if you enjoy wine, like we do, you may find it especially interesting. Here’s some  R and Matlab code , and if you want to get right to the point,  skip to the charts .
 
There’s a book by Philipp Janert called  Data Analysis with Open Source Tools , which, by the way, we would recommend. From this book we found out about  the wine quality datasets . There are two, one for red wine and one for white wine, and they are interesting because they contain quality ratings (1 - 10) for a few thousands of wines, along with their physical and chemical properties. We could probably use these properties to predict a rating for a wine.   We’ll be looking at white and red wine separately for the reasons you will see shortly.
  Principal component analysis for white wine  
Janert performs a principal component analysis (PCA) and shows a resulting plot for white wine. What’s interesting about this plot is that judging by the first tw</p><p>2 0.12474725 <a title="11-tfidf-2" href="../fast_ml-2012/fast_ml-2012-12-27-Spearmint_with_a_random_forest.html">13 fast ml-2012-12-27-Spearmint with a random forest</a></p>
<p>Introduction: Now that we have  Spearmint basics  nailed, we’ll try tuning a random forest, and specifically two hyperparams: a number of trees ( ntrees ) and a number of candidate features at each split ( mtry ). Here’s  some code .
 
We’re going to use a red  wine quality  dataset. It has about 1600 examples and our goal will be to predict a rating for a wine given all the other properties.   This is a regression* task, as ratings are in (0,10) range.
 
We will split the data 80/10/10 into train, validation and test set, and use the first two to establish optimal hyperparams and then predict on the test set. As an error measure we will use RMSE.
 
At first, we will try  ntrees  between 10 and 200 and  mtry  between 3 and 11 (there’s eleven features total, so that’s the upper bound). Here are the results of two Spearmint runs with 71 and 95 tries respectively. Colors denote a validation error value:
  
  green : RMSE < 0.57 
  blue : RMSE < 0.58 
  black : RMSE >= 0.58 
  
Turns out that some diffe</p><p>3 0.061164211 <a title="11-tfidf-3" href="../fast_ml-2013/fast_ml-2013-12-15-A-B_testing_with_bayesian_bandits_in_Google_Analytics.html">47 fast ml-2013-12-15-A-B testing with bayesian bandits in Google Analytics</a></p>
<p>Introduction: A/B testing is a way to optimize a web page. Half of visitors see one version, the other half another, so you can tell which version is more conducive to your goal - for example selling something. Since June 2013 A/B testing can be conveniently done with Google Analytics. Here’s how.
   
 This article is not quite about machine learning. If you’re not interested in testing, scroll down to the  bayesian bandits section . 
  Google Content Experiments  
We remember Google Website Optimizer from a few years ago. It wasn’t exactly user friendly or slick, but it felt solid and did the job. Unfortunately, at one point in time Google pulled the plug, leaving  Genetify  as a sole free (and open source) tool for multivariate testing.  Multivariate  means testing a few elements on a page simultanously.
 
At that time they launched Content Experiments in Google Analytics, but it was a giant step backward. Content experiments were very primitive and only allowed rudimentary A/B split testing. It i</p><p>4 0.055270601 <a title="11-tfidf-4" href="../fast_ml-2013/fast_ml-2013-03-25-Dimensionality_reduction_for_sparse_binary_data_-_an_overview.html">24 fast ml-2013-03-25-Dimensionality reduction for sparse binary data - an overview</a></p>
<p>Introduction: Last time we explored dimensionality reduction in practice using Gensim’s LSI and LDA. Now, having spent some time researching the subject matter, we will give an overview of other options.
   
 UPDATE : We now consider the topic quite irrelevant, because sparse high-dimensional data is precisely where linear models shine. See  Amazon aspires to automate access control ,  Predicting advertised salaries  and  Predicting closed questions on Stack Overflow .
 
And the few most popular methods are:
  
 LSI/LSA - a multinomial PCA 
 LDA - Latent Dirichlet Allocation 
 matrix factorization, in particular non-negative variants: NMF 
 ICA, or Independent Components Analysis 
 mixtures of Bernoullis 
 stacked RBMs 
 correlated topic models, an extension of LDA 
  
We  tried the first two  before.
 
As regards matrix factorization, you do the same stuff as with movie recommendations (think Netflix challenge). The difference is, now all the matrix elements are known and we are only interested in</p><p>5 0.05240396 <a title="11-tfidf-5" href="../fast_ml-2013/fast_ml-2013-01-04-Madelon%3A_Spearmint%27s_revenge.html">14 fast ml-2013-01-04-Madelon: Spearmint's revenge</a></p>
<p>Introduction: Little Spearmint couldn’t sleep that night.  I was so close…  - he was thinking. It seemed that he had found a better than default value for one of the random forest hyperparams, but it turned out to be false. He made a decision as he fell asleep:  Next time, I will show them! 
   
The way to do this is to use a dataset that is known to produce lower error with high  mtry  values, namely previously mentioned  Madelon  from NIPS 2003 Feature Selection Challenge. Among 500 attributes, only 20 are informative, the rest are noise. That’s the reason why high  mtry  is good here: you have to consider a lot of features to find a meaningful one.
 
The dataset consists of a train, validation and test parts, with labels being available for train and validation. We will further split the training set into our train and validation sets, and use the original validation set as a test set to evaluate final results of parameter tuning.
 
As an error measure we use  Area Under Curve , or AUC, which was</p><p>6 0.042193424 <a title="11-tfidf-6" href="../fast_ml-2013/fast_ml-2013-12-07-13_NIPS_papers_that_caught_our_eye.html">46 fast ml-2013-12-07-13 NIPS papers that caught our eye</a></p>
<p>7 0.035950374 <a title="11-tfidf-7" href="../fast_ml-2012/fast_ml-2012-12-21-Tuning_hyperparams_automatically_with_Spearmint.html">12 fast ml-2012-12-21-Tuning hyperparams automatically with Spearmint</a></p>
<p>8 0.03201 <a title="11-tfidf-8" href="../fast_ml-2014/fast_ml-2014-02-20-Are_stocks_predictable%3F.html">53 fast ml-2014-02-20-Are stocks predictable?</a></p>
<p>9 0.028829416 <a title="11-tfidf-9" href="../fast_ml-2012/fast_ml-2012-10-05-Predicting_closed_questions_on_Stack_Overflow.html">7 fast ml-2012-10-05-Predicting closed questions on Stack Overflow</a></p>
<p>10 0.02814536 <a title="11-tfidf-10" href="../fast_ml-2013/fast_ml-2013-01-17-A_very_fast_denoising_autoencoder.html">18 fast ml-2013-01-17-A very fast denoising autoencoder</a></p>
<p>11 0.027786363 <a title="11-tfidf-11" href="../fast_ml-2014/fast_ml-2014-03-20-Good_representations%2C_distance%2C_metric_learning_and_supervised_dimensionality_reduction.html">55 fast ml-2014-03-20-Good representations, distance, metric learning and supervised dimensionality reduction</a></p>
<p>12 0.026893759 <a title="11-tfidf-12" href="../fast_ml-2013/fast_ml-2013-01-07-Machine_learning_courses_online.html">15 fast ml-2013-01-07-Machine learning courses online</a></p>
<p>13 0.026446443 <a title="11-tfidf-13" href="../fast_ml-2013/fast_ml-2013-02-27-Dimensionality_reduction_for_sparse_binary_data.html">21 fast ml-2013-02-27-Dimensionality reduction for sparse binary data</a></p>
<p>14 0.025100123 <a title="11-tfidf-14" href="../fast_ml-2012/fast_ml-2012-08-09-What_you_wanted_to_know_about_Mean_Average_Precision.html">1 fast ml-2012-08-09-What you wanted to know about Mean Average Precision</a></p>
<p>15 0.024747688 <a title="11-tfidf-15" href="../fast_ml-2014/fast_ml-2014-03-31-If_you_use_R%2C_you_may_want_RStudio.html">56 fast ml-2014-03-31-If you use R, you may want RStudio</a></p>
<p>16 0.024238449 <a title="11-tfidf-16" href="../fast_ml-2014/fast_ml-2014-05-26-Yann_LeCun%27s_answers_from_the_Reddit_AMA.html">62 fast ml-2014-05-26-Yann LeCun's answers from the Reddit AMA</a></p>
<p>17 0.023575524 <a title="11-tfidf-17" href="../fast_ml-2013/fast_ml-2013-07-09-Introducing_phraug.html">33 fast ml-2013-07-09-Introducing phraug</a></p>
<p>18 0.021265745 <a title="11-tfidf-18" href="../fast_ml-2013/fast_ml-2013-02-07-The_secret_of_the_big_guys.html">19 fast ml-2013-02-07-The secret of the big guys</a></p>
<p>19 0.020538833 <a title="11-tfidf-19" href="../fast_ml-2014/fast_ml-2014-01-25-Why_IPy%3A_reasons_for_using_IPython_interactively.html">51 fast ml-2014-01-25-Why IPy: reasons for using IPython interactively</a></p>
<p>20 0.02004973 <a title="11-tfidf-20" href="../fast_ml-2013/fast_ml-2013-03-18-Large_scale_L1_feature_selection_with_Vowpal_Wabbit.html">23 fast ml-2013-03-18-Large scale L1 feature selection with Vowpal Wabbit</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/fast_ml_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.107), (1, 0.086), (2, -0.091), (3, -0.053), (4, 0.029), (5, 0.095), (6, -0.052), (7, -0.073), (8, 0.104), (9, 0.014), (10, 0.209), (11, 0.22), (12, 0.014), (13, 0.033), (14, -0.197), (15, -0.118), (16, -0.35), (17, -0.084), (18, -0.329), (19, -0.177), (20, 0.094), (21, -0.026), (22, 0.056), (23, 0.044), (24, -0.282), (25, -0.32), (26, 0.046), (27, 0.048), (28, 0.363), (29, -0.034), (30, -0.14), (31, 0.153), (32, -0.014), (33, -0.105), (34, -0.041), (35, 0.121), (36, -0.023), (37, -0.103), (38, -0.157), (39, 0.156), (40, -0.14), (41, 0.066), (42, 0.036), (43, -0.093), (44, 0.069), (45, -0.033), (46, 0.086), (47, 0.058), (48, -0.011), (49, -0.045)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99309075 <a title="11-lsi-1" href="../fast_ml-2012/fast_ml-2012-12-07-Predicting_wine_quality.html">11 fast ml-2012-12-07-Predicting wine quality</a></p>
<p>Introduction: This post is as much about wine as it is about machine learning, so if you enjoy wine, like we do, you may find it especially interesting. Here’s some  R and Matlab code , and if you want to get right to the point,  skip to the charts .
 
There’s a book by Philipp Janert called  Data Analysis with Open Source Tools , which, by the way, we would recommend. From this book we found out about  the wine quality datasets . There are two, one for red wine and one for white wine, and they are interesting because they contain quality ratings (1 - 10) for a few thousands of wines, along with their physical and chemical properties. We could probably use these properties to predict a rating for a wine.   We’ll be looking at white and red wine separately for the reasons you will see shortly.
  Principal component analysis for white wine  
Janert performs a principal component analysis (PCA) and shows a resulting plot for white wine. What’s interesting about this plot is that judging by the first tw</p><p>2 0.1545604 <a title="11-lsi-2" href="../fast_ml-2012/fast_ml-2012-12-27-Spearmint_with_a_random_forest.html">13 fast ml-2012-12-27-Spearmint with a random forest</a></p>
<p>Introduction: Now that we have  Spearmint basics  nailed, we’ll try tuning a random forest, and specifically two hyperparams: a number of trees ( ntrees ) and a number of candidate features at each split ( mtry ). Here’s  some code .
 
We’re going to use a red  wine quality  dataset. It has about 1600 examples and our goal will be to predict a rating for a wine given all the other properties.   This is a regression* task, as ratings are in (0,10) range.
 
We will split the data 80/10/10 into train, validation and test set, and use the first two to establish optimal hyperparams and then predict on the test set. As an error measure we will use RMSE.
 
At first, we will try  ntrees  between 10 and 200 and  mtry  between 3 and 11 (there’s eleven features total, so that’s the upper bound). Here are the results of two Spearmint runs with 71 and 95 tries respectively. Colors denote a validation error value:
  
  green : RMSE < 0.57 
  blue : RMSE < 0.58 
  black : RMSE >= 0.58 
  
Turns out that some diffe</p><p>3 0.10795964 <a title="11-lsi-3" href="../fast_ml-2013/fast_ml-2013-03-25-Dimensionality_reduction_for_sparse_binary_data_-_an_overview.html">24 fast ml-2013-03-25-Dimensionality reduction for sparse binary data - an overview</a></p>
<p>Introduction: Last time we explored dimensionality reduction in practice using Gensim’s LSI and LDA. Now, having spent some time researching the subject matter, we will give an overview of other options.
   
 UPDATE : We now consider the topic quite irrelevant, because sparse high-dimensional data is precisely where linear models shine. See  Amazon aspires to automate access control ,  Predicting advertised salaries  and  Predicting closed questions on Stack Overflow .
 
And the few most popular methods are:
  
 LSI/LSA - a multinomial PCA 
 LDA - Latent Dirichlet Allocation 
 matrix factorization, in particular non-negative variants: NMF 
 ICA, or Independent Components Analysis 
 mixtures of Bernoullis 
 stacked RBMs 
 correlated topic models, an extension of LDA 
  
We  tried the first two  before.
 
As regards matrix factorization, you do the same stuff as with movie recommendations (think Netflix challenge). The difference is, now all the matrix elements are known and we are only interested in</p><p>4 0.082970098 <a title="11-lsi-4" href="../fast_ml-2013/fast_ml-2013-12-15-A-B_testing_with_bayesian_bandits_in_Google_Analytics.html">47 fast ml-2013-12-15-A-B testing with bayesian bandits in Google Analytics</a></p>
<p>Introduction: A/B testing is a way to optimize a web page. Half of visitors see one version, the other half another, so you can tell which version is more conducive to your goal - for example selling something. Since June 2013 A/B testing can be conveniently done with Google Analytics. Here’s how.
   
 This article is not quite about machine learning. If you’re not interested in testing, scroll down to the  bayesian bandits section . 
  Google Content Experiments  
We remember Google Website Optimizer from a few years ago. It wasn’t exactly user friendly or slick, but it felt solid and did the job. Unfortunately, at one point in time Google pulled the plug, leaving  Genetify  as a sole free (and open source) tool for multivariate testing.  Multivariate  means testing a few elements on a page simultanously.
 
At that time they launched Content Experiments in Google Analytics, but it was a giant step backward. Content experiments were very primitive and only allowed rudimentary A/B split testing. It i</p><p>5 0.061861821 <a title="11-lsi-5" href="../fast_ml-2014/fast_ml-2014-02-20-Are_stocks_predictable%3F.html">53 fast ml-2014-02-20-Are stocks predictable?</a></p>
<p>Introduction: We’d like to be able to predict stock market. That seems like a nice way of making money. We’ll address the fundamental issue: can stocks be predicted in the short term, that is a few days ahead?
   
There’s a technique that seeks to answer the question of predictability. It’s called  Forecastable Component Analysis .  Based on a new forecastability measure, ForeCA  finds an optimal transformation to separate a multivariate time series into a forecastable and an orthogonal white noise space.  The author, Georg M. Goerg*, implemented it in R package  ForeCA .
 
It might be useful in two ways:
  
 It can tell you how forecastable time series is. 
 Given a multivariate time series, let’s say a portfolio of stocks, it can find forecastable components. 
  
The idea in the second point is similiar to PCA - ForeCA is a linear dimensionality reduction technique. The main difference is that the method explicitly addresses forecastability. It does so by considering an interplay between time and</p><p>6 0.054828554 <a title="11-lsi-6" href="../fast_ml-2013/fast_ml-2013-02-07-The_secret_of_the_big_guys.html">19 fast ml-2013-02-07-The secret of the big guys</a></p>
<p>7 0.053764749 <a title="11-lsi-7" href="../fast_ml-2013/fast_ml-2013-01-17-A_very_fast_denoising_autoencoder.html">18 fast ml-2013-01-17-A very fast denoising autoencoder</a></p>
<p>8 0.052293353 <a title="11-lsi-8" href="../fast_ml-2012/fast_ml-2012-10-05-Predicting_closed_questions_on_Stack_Overflow.html">7 fast ml-2012-10-05-Predicting closed questions on Stack Overflow</a></p>
<p>9 0.051600259 <a title="11-lsi-9" href="../fast_ml-2013/fast_ml-2013-12-07-13_NIPS_papers_that_caught_our_eye.html">46 fast ml-2013-12-07-13 NIPS papers that caught our eye</a></p>
<p>10 0.046390615 <a title="11-lsi-10" href="../fast_ml-2014/fast_ml-2014-03-31-If_you_use_R%2C_you_may_want_RStudio.html">56 fast ml-2014-03-31-If you use R, you may want RStudio</a></p>
<p>11 0.046112366 <a title="11-lsi-11" href="../fast_ml-2014/fast_ml-2014-04-12-Deep_learning_these_days.html">58 fast ml-2014-04-12-Deep learning these days</a></p>
<p>12 0.046008468 <a title="11-lsi-12" href="../fast_ml-2013/fast_ml-2013-10-06-Pylearn2_in_practice.html">40 fast ml-2013-10-06-Pylearn2 in practice</a></p>
<p>13 0.045689207 <a title="11-lsi-13" href="../fast_ml-2012/fast_ml-2012-08-09-What_you_wanted_to_know_about_Mean_Average_Precision.html">1 fast ml-2012-08-09-What you wanted to know about Mean Average Precision</a></p>
<p>14 0.044524629 <a title="11-lsi-14" href="../fast_ml-2013/fast_ml-2013-01-04-Madelon%3A_Spearmint%27s_revenge.html">14 fast ml-2013-01-04-Madelon: Spearmint's revenge</a></p>
<p>15 0.04418467 <a title="11-lsi-15" href="../fast_ml-2013/fast_ml-2013-07-09-Introducing_phraug.html">33 fast ml-2013-07-09-Introducing phraug</a></p>
<p>16 0.043918628 <a title="11-lsi-16" href="../fast_ml-2013/fast_ml-2013-03-18-Large_scale_L1_feature_selection_with_Vowpal_Wabbit.html">23 fast ml-2013-03-18-Large scale L1 feature selection with Vowpal Wabbit</a></p>
<p>17 0.043676071 <a title="11-lsi-17" href="../fast_ml-2013/fast_ml-2013-01-07-Machine_learning_courses_online.html">15 fast ml-2013-01-07-Machine learning courses online</a></p>
<p>18 0.039783981 <a title="11-lsi-18" href="../fast_ml-2014/fast_ml-2014-04-21-Predicting_happiness_from_demographics_and_poll_answers.html">59 fast ml-2014-04-21-Predicting happiness from demographics and poll answers</a></p>
<p>19 0.03961752 <a title="11-lsi-19" href="../fast_ml-2013/fast_ml-2013-05-01-Deep_learning_made_easy.html">27 fast ml-2013-05-01-Deep learning made easy</a></p>
<p>20 0.037141036 <a title="11-lsi-20" href="../fast_ml-2014/fast_ml-2014-03-20-Good_representations%2C_distance%2C_metric_learning_and_supervised_dimensionality_reduction.html">55 fast ml-2014-03-20-Good representations, distance, metric learning and supervised dimensionality reduction</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/fast_ml_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(6, 0.015), (22, 0.022), (26, 0.021), (31, 0.01), (35, 0.027), (55, 0.018), (69, 0.152), (71, 0.022), (73, 0.012), (81, 0.013), (84, 0.535), (99, 0.018)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.96282059 <a title="11-lda-1" href="../fast_ml-2013/fast_ml-2013-09-03-Our_followers_and_who_else_they_follow.html">37 fast ml-2013-09-03-Our followers and who else they follow</a></p>
<p>Introduction: Recently we hit 400 followers mark on Twitter. To celebrate we decided to do some data mining on  you , specifically to discover who our followers are and who else they follow. For your viewing pleasure we packaged the results nicely with Bootstrap. Here’s some data science in action.
   
 
 
  Our followers  
This table show our 20 most popular followers as measeared by their follower count. The occasional question marks stand for non-ASCII characters. Each link opens a new window.

 
 
 
 
   
 Followers 
 Screen name 
 Name 
 Description 
 
 
 
 
   
 8685 
  pankaj  
 Pankaj Gupta 
 I lead the Personalization and Recommender Systems group at Twitter. Founded two startups in the past.       
 
 
   
 5070 
  ogrisel  
 Olivier Grisel 
 Datageek, contributor to scikit-learn, works with Python / Java / Clojure / Pig, interested in Machine Learning, NLProc, {Big|Linked|Open} Data and braaains!       
 
 
   
 4582 
  thuske  
 thuske 
 & 
 
 
   
 4442 
  ram  
 Ram Ravichandran 
 So</p><p>same-blog 2 0.92917252 <a title="11-lda-2" href="../fast_ml-2012/fast_ml-2012-12-07-Predicting_wine_quality.html">11 fast ml-2012-12-07-Predicting wine quality</a></p>
<p>Introduction: This post is as much about wine as it is about machine learning, so if you enjoy wine, like we do, you may find it especially interesting. Here’s some  R and Matlab code , and if you want to get right to the point,  skip to the charts .
 
There’s a book by Philipp Janert called  Data Analysis with Open Source Tools , which, by the way, we would recommend. From this book we found out about  the wine quality datasets . There are two, one for red wine and one for white wine, and they are interesting because they contain quality ratings (1 - 10) for a few thousands of wines, along with their physical and chemical properties. We could probably use these properties to predict a rating for a wine.   We’ll be looking at white and red wine separately for the reasons you will see shortly.
  Principal component analysis for white wine  
Janert performs a principal component analysis (PCA) and shows a resulting plot for white wine. What’s interesting about this plot is that judging by the first tw</p><p>3 0.32780355 <a title="11-lda-3" href="../fast_ml-2012/fast_ml-2012-10-25-So_you_want_to_work_for_Facebook.html">9 fast ml-2012-10-25-So you want to work for Facebook</a></p>
<p>Introduction: Good news, everyone! There’s a new contest on Kaggle -  Facebook is looking for talent . They won’t pay, but just might interview.
 
This post is in a way a bonus for active readers because most visitors of fastml.com originally come from Kaggle forums. For this competition the forums are disabled to encourage  own work . To honor this, we won’t publish any code. But  own work  doesn’t mean  original work , and we wouldn’t want to reinvent the wheel, would we?
   
The contest differs substantially from a Kaggle stereotype, if there is such a thing, in three major ways:
  
 there’s no money prizes, as mentioned above 
 it’s not a real world problem, but rather an assignment to screen job candidates (this has important consequences, described below) 
 it’s not a typical machine learning project, but rather a broader AI exercise 
  
You are given a graph of the internet, actually a snapshot of the graph for each of 15 time steps. You are also given a bunch of paths in this graph, which  a</p><p>4 0.30873862 <a title="11-lda-4" href="../fast_ml-2012/fast_ml-2012-12-27-Spearmint_with_a_random_forest.html">13 fast ml-2012-12-27-Spearmint with a random forest</a></p>
<p>Introduction: Now that we have  Spearmint basics  nailed, we’ll try tuning a random forest, and specifically two hyperparams: a number of trees ( ntrees ) and a number of candidate features at each split ( mtry ). Here’s  some code .
 
We’re going to use a red  wine quality  dataset. It has about 1600 examples and our goal will be to predict a rating for a wine given all the other properties.   This is a regression* task, as ratings are in (0,10) range.
 
We will split the data 80/10/10 into train, validation and test set, and use the first two to establish optimal hyperparams and then predict on the test set. As an error measure we will use RMSE.
 
At first, we will try  ntrees  between 10 and 200 and  mtry  between 3 and 11 (there’s eleven features total, so that’s the upper bound). Here are the results of two Spearmint runs with 71 and 95 tries respectively. Colors denote a validation error value:
  
  green : RMSE < 0.57 
  blue : RMSE < 0.58 
  black : RMSE >= 0.58 
  
Turns out that some diffe</p><p>5 0.2940031 <a title="11-lda-5" href="../fast_ml-2013/fast_ml-2013-01-07-Machine_learning_courses_online.html">15 fast ml-2013-01-07-Machine learning courses online</a></p>
<p>Introduction: How do you learn machine learning? A good way to begin is to take an online course. These courses started appearing towards the end of 2011, first from Stanford University, now from  Coursera ,  Udacity ,  edX  and other institutions. There are very many of them, including a few about machine learning.   Here’s a list:
  
 
 Introduction to Artificial Intelligence  by Sebastian Thrun and Peter Norvig. That was the first online class, and it contains two units on machine learning (units five and six). Both instructors work at Google. Sebastian Thrun is best known for building a self-driving car and Peter Norvig is a leading authority on AI, so they know what they are talking about. After the success of the class Sebastian Thrun quit Stanford to found Udacity, his online learning startup.
 
 
 Machine Learning  by Andrew Ng. Again, one of the first classes, by Stanford professor who started Coursera, the best known online learning provider today. Andrew Ng is a world class authority on m</p><p>6 0.28068483 <a title="11-lda-6" href="../fast_ml-2013/fast_ml-2013-12-28-Regularizing_neural_networks_with_dropout_and_with_DropConnect.html">48 fast ml-2013-12-28-Regularizing neural networks with dropout and with DropConnect</a></p>
<p>7 0.27893752 <a title="11-lda-7" href="../fast_ml-2013/fast_ml-2013-05-01-Deep_learning_made_easy.html">27 fast ml-2013-05-01-Deep learning made easy</a></p>
<p>8 0.27768084 <a title="11-lda-8" href="../fast_ml-2012/fast_ml-2012-12-21-Tuning_hyperparams_automatically_with_Spearmint.html">12 fast ml-2012-12-21-Tuning hyperparams automatically with Spearmint</a></p>
<p>9 0.27576771 <a title="11-lda-9" href="../fast_ml-2013/fast_ml-2013-01-04-Madelon%3A_Spearmint%27s_revenge.html">14 fast ml-2013-01-04-Madelon: Spearmint's revenge</a></p>
<p>10 0.27530351 <a title="11-lda-10" href="../fast_ml-2012/fast_ml-2012-08-09-What_you_wanted_to_know_about_Mean_Average_Precision.html">1 fast ml-2012-08-09-What you wanted to know about Mean Average Precision</a></p>
<p>11 0.27274731 <a title="11-lda-11" href="../fast_ml-2014/fast_ml-2014-01-10-Classifying_images_with_a_pre-trained_deep_network.html">49 fast ml-2014-01-10-Classifying images with a pre-trained deep network</a></p>
<p>12 0.27178162 <a title="11-lda-12" href="../fast_ml-2013/fast_ml-2013-08-12-Accelerometer_Biometric_Competition.html">35 fast ml-2013-08-12-Accelerometer Biometric Competition</a></p>
<p>13 0.26848176 <a title="11-lda-13" href="../fast_ml-2013/fast_ml-2013-11-27-Object_recognition_in_images_with_cuda-convnet.html">45 fast ml-2013-11-27-Object recognition in images with cuda-convnet</a></p>
<p>14 0.26278013 <a title="11-lda-14" href="../fast_ml-2014/fast_ml-2014-05-26-Yann_LeCun%27s_answers_from_the_Reddit_AMA.html">62 fast ml-2014-05-26-Yann LeCun's answers from the Reddit AMA</a></p>
<p>15 0.25699088 <a title="11-lda-15" href="../fast_ml-2014/fast_ml-2014-03-20-Good_representations%2C_distance%2C_metric_learning_and_supervised_dimensionality_reduction.html">55 fast ml-2014-03-20-Good representations, distance, metric learning and supervised dimensionality reduction</a></p>
<p>16 0.25422734 <a title="11-lda-16" href="../fast_ml-2013/fast_ml-2013-01-14-Feature_selection_in_practice.html">17 fast ml-2013-01-14-Feature selection in practice</a></p>
<p>17 0.25097874 <a title="11-lda-17" href="../fast_ml-2013/fast_ml-2013-11-02-Maxing_out_the_digits.html">43 fast ml-2013-11-02-Maxing out the digits</a></p>
<p>18 0.24932112 <a title="11-lda-18" href="../fast_ml-2013/fast_ml-2013-08-23-A_bag_of_words_and_a_nice_little_network.html">36 fast ml-2013-08-23-A bag of words and a nice little network</a></p>
<p>19 0.24882795 <a title="11-lda-19" href="../fast_ml-2013/fast_ml-2013-01-17-A_very_fast_denoising_autoencoder.html">18 fast ml-2013-01-17-A very fast denoising autoencoder</a></p>
<p>20 0.24800862 <a title="11-lda-20" href="../fast_ml-2014/fast_ml-2014-02-20-Are_stocks_predictable%3F.html">53 fast ml-2014-02-20-Are stocks predictable?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
