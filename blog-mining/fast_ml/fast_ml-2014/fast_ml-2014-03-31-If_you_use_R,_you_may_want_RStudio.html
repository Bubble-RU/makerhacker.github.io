<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>56 fast ml-2014-03-31-If you use R, you may want RStudio</title>
</head>

<body>
<p><a title="fast_ml" href="../fast_ml_home.html">fast_ml</a> <a title="fast_ml-2014" href="../home/fast_ml-2014_home.html">fast_ml-2014</a> <a title="fast_ml-2014-56" href="#">fast_ml-2014-56</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>56 fast ml-2014-03-31-If you use R, you may want RStudio</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="fast_ml-2014-56-html" href="http://fastml.com//if-you-use-r-you-may-want-rstudio/">html</a></p><p>Introduction: RStudio  is an IDE for R. It gives the language a bit of a slickness factor it so badly needs. The nice thing about the software, beside good looks, is that it integrates console, help pages, plots and editor (if you want it) in one place.
   
For example, instead of switching for help to a web browser, you have it right next to the console. The same thing with plots: in place of the usual overlapping windows, all plots go to one pane where you can navigate back and forth between them with arrows. You can save a plot as an image or as a PDF. While saving as image you are presented with a preview and you get to choose the image format and size. That’s the kind of detail that shows how RStudio makes working with R better.
 
You can have up to four panes on the screen:
  
 console 
 source / data 
 plots / help / files 
 history and workspace 
  
 
 
Arrange them in any way you like.
 
The source pane has a  run  button, so you can execute the current line of code in the console, or selec</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 It gives the language a bit of a slickness factor it so badly needs. [sent-2, score-0.202]
</p><p>2 The nice thing about the software, beside good looks, is that it integrates console, help pages, plots and editor (if you want it) in one place. [sent-3, score-0.715]
</p><p>3 For example, instead of switching for help to a web browser, you have it right next to the console. [sent-4, score-0.328]
</p><p>4 The same thing with plots: in place of the usual overlapping windows, all plots go to one pane where you can navigate back and forth between them with arrows. [sent-5, score-0.844]
</p><p>5 While saving as image you are presented with a preview and you get to choose the image format and size. [sent-7, score-0.438]
</p><p>6 That’s the kind of detail that shows how RStudio makes working with R better. [sent-8, score-0.124]
</p><p>7 You can have up to four panes on the screen:     console   source / data   plots / help / files   history and workspace         Arrange them in any way you like. [sent-9, score-1.507]
</p><p>8 The source pane has a  run  button, so you can execute the current line of code in the console, or select a few lines and execute the whole region. [sent-10, score-1.098]
</p><p>9 There’s a R script for each unit and Trevor walks you through it in a video. [sent-12, score-0.06]
</p><p>10 Instead of typing each command (which has its advantages) or copy-and-pasting, you just click once. [sent-13, score-0.072]
</p><p>11 The workspace shows data, variable values and functions. [sent-14, score-0.272]
</p><p>12 The history pane is somewhat underpowered - for example one can’t select a few lines (or even just one) and copy them to clipboard. [sent-16, score-0.867]
</p><p>13 One can send them to the console and to the source pane, though. [sent-17, score-0.607]
</p><p>14 RStudio also has a few nice color schemes, particularly with dark backgrounds. [sent-18, score-0.371]
</p><p>15 One shortcoming is that the dark ones don’t mix that well with white background for history, help and plots. [sent-19, score-0.557]
</p><p>16 The software is available for Linux, Mac and Windows. [sent-20, score-0.059]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('console', 0.394), ('pane', 0.394), ('plots', 0.394), ('rstudio', 0.295), ('history', 0.217), ('workspace', 0.197), ('help', 0.174), ('execute', 0.164), ('dark', 0.164), ('source', 0.131), ('image', 0.106), ('select', 0.104), ('lines', 0.092), ('windows', 0.087), ('nice', 0.082), ('switching', 0.082), ('hastie', 0.082), ('trevor', 0.082), ('advantages', 0.082), ('presented', 0.082), ('button', 0.082), ('background', 0.082), ('mac', 0.082), ('send', 0.082), ('view', 0.082), ('shows', 0.075), ('click', 0.072), ('mix', 0.072), ('preview', 0.072), ('pages', 0.072), ('badly', 0.072), ('saving', 0.072), ('browser', 0.072), ('web', 0.072), ('linux', 0.065), ('screen', 0.065), ('gives', 0.065), ('factor', 0.065), ('white', 0.065), ('statistical', 0.065), ('color', 0.065), ('editor', 0.065), ('copy', 0.06), ('particularly', 0.06), ('unit', 0.06), ('software', 0.059), ('place', 0.056), ('save', 0.056), ('current', 0.049), ('kind', 0.049)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="56-tfidf-1" href="../fast_ml-2014/fast_ml-2014-03-31-If_you_use_R%2C_you_may_want_RStudio.html">56 fast ml-2014-03-31-If you use R, you may want RStudio</a></p>
<p>Introduction: RStudio  is an IDE for R. It gives the language a bit of a slickness factor it so badly needs. The nice thing about the software, beside good looks, is that it integrates console, help pages, plots and editor (if you want it) in one place.
   
For example, instead of switching for help to a web browser, you have it right next to the console. The same thing with plots: in place of the usual overlapping windows, all plots go to one pane where you can navigate back and forth between them with arrows. You can save a plot as an image or as a PDF. While saving as image you are presented with a preview and you get to choose the image format and size. That’s the kind of detail that shows how RStudio makes working with R better.
 
You can have up to four panes on the screen:
  
 console 
 source / data 
 plots / help / files 
 history and workspace 
  
 
 
Arrange them in any way you like.
 
The source pane has a  run  button, so you can execute the current line of code in the console, or selec</p><p>2 0.082322001 <a title="56-tfidf-2" href="../fast_ml-2012/fast_ml-2012-09-01-Running_Unix_apps_on_Windows.html">3 fast ml-2012-09-01-Running Unix apps on Windows</a></p>
<p>Introduction: When it comes to machine learning, most software seems to be in either Python, Matlab or R. Plus native apps, that is, compiled C/C++. These are fastest. Most of them is written for Unix environments, for example Linux or MacOS. So how do you run them on your computer if you have Windows installed?
   
Back in the day, you re-partitioned your hard drive and installed Linux alongside Windows. The added thrill was, if something went wrong, your computer wouldn’t boot.
 
Now it’s easier. You just run Linux inside Windows, using what’s called a virtual machine. You need virtualization software and a machine image to do so.
 
Most popular software seems to be VMware. There is also VirtualBox - it is able to run VMware images. We have experience with WMware mostly, so this is what we’ll refer to.  VMware player  is free to download and use.
 
There are also  many images  available, of various flavours of Linux and other operating systems. In fact, you can run Windows inside Linux if you wish</p><p>3 0.072947845 <a title="56-tfidf-3" href="../fast_ml-2014/fast_ml-2014-01-25-Why_IPy%3A_reasons_for_using_IPython_interactively.html">51 fast ml-2014-01-25-Why IPy: reasons for using IPython interactively</a></p>
<p>Introduction: IPython  is known for the notebooks. But the first thing they list on their homepage is a “powerful interactive shell”. And that’s true - if you use Python interactively, you’ll dig IPython.
   
Here are a few features that make a difference for us:
  
 
command line history and autocomplete 
  These are basic shell features you come to expect and rely upon after a first contact with a Unix shell. The standard Python interpreter feels somewhat impoverished without them.
 
 
 %paste  
  Normally when you paste indented code, it won’t work. You can fix it with this magic command.
 
 
other  %magic  commands 
  Actually, you can skip the  % :  paste ,  run some_script.py ,  pwd ,  cd some_dir 
 
 
shell commands with  !  
  For example  !ls -las . Alas, aliases from your system shell don’t work.
 
 
looks good 
  Red and green add a nice touch, especially in a window with dark background. 
 
  
 
 If you have a light background, use  %colors LightBG  
 
Share your favourite features</p><p>4 0.052914634 <a title="56-tfidf-4" href="../fast_ml-2013/fast_ml-2013-11-27-Object_recognition_in_images_with_cuda-convnet.html">45 fast ml-2013-11-27-Object recognition in images with cuda-convnet</a></p>
<p>Introduction: Object recognition in images is where deep learning, and specifically convolutional neural networks, are often applied and benchmarked these days. To get a piece of the action, we’ll be using Alex Krizhevsky’s  cuda-convnet , a shining diamond of machine learning software, in a Kaggle competition.
   
Continuing to run things on a GPU, we turn to applying  convolutional neural networks  for object recognition. This kind of network was developed by Yann LeCun and it’s powerful, but a bit complicated:
 
  
 Image credit:  EBLearn tutorial  
 
A typical convolutional network has two parts. The first is responsible for feature extraction and consists of one or more pairs of convolution and subsampling/max-pooling layers, as you can see above. The second part is just a classic fully-connected multilayer perceptron taking extracted features as input. For a detailed explanation of all this see unit 9 in Hugo LaRochelle’s  neural networks course .
 
Daniel Nouri has an interesting story about</p><p>5 0.049751848 <a title="56-tfidf-5" href="../fast_ml-2013/fast_ml-2013-07-09-Introducing_phraug.html">33 fast ml-2013-07-09-Introducing phraug</a></p>
<p>Introduction: Recently we proposed to pre-process large files line by line. Now it’s time to introduce  phraug *, a set of Python scripts based on this idea. The scripts mostly deal with format conversion (CSV, libsvm, VW) and with few other tasks common in machine learning.
   
With  phraug  you currently can convert from one format to another:
  
 csv to libsvm 
 csv to Vowpal Wabbit 
 libsvm to csv 
 libsvm to Vowpal Wabbit 
 tsv to csv 
  
And perform some other file operations:
  
 count lines in a file 
 sample lines from a file 
 split a file into two randomly 
 split a file into a number of similiarly sized chunks 
 save a continuous subset of lines from a file (for example, first 100) 
 delete specified columns from a csv file 
 normalize (shift and scale) columns in a csv file 
  
Basically, there’s always at least one input file and usually one or more output files. An input file always stays unchanged.
 
If you’re familiar with Unix, you may notice that some of these tasks are easily ach</p><p>6 0.045181118 <a title="56-tfidf-6" href="../fast_ml-2012/fast_ml-2012-10-05-Predicting_closed_questions_on_Stack_Overflow.html">7 fast ml-2012-10-05-Predicting closed questions on Stack Overflow</a></p>
<p>7 0.04390309 <a title="56-tfidf-7" href="../fast_ml-2013/fast_ml-2013-01-14-Feature_selection_in_practice.html">17 fast ml-2013-01-14-Feature selection in practice</a></p>
<p>8 0.041029673 <a title="56-tfidf-8" href="../fast_ml-2014/fast_ml-2014-04-30-Converting_categorical_data_into_numbers_with_Pandas_and_Scikit-learn.html">60 fast ml-2014-04-30-Converting categorical data into numbers with Pandas and Scikit-learn</a></p>
<p>9 0.037388168 <a title="56-tfidf-9" href="../fast_ml-2013/fast_ml-2013-12-15-A-B_testing_with_bayesian_bandits_in_Google_Analytics.html">47 fast ml-2013-12-15-A-B testing with bayesian bandits in Google Analytics</a></p>
<p>10 0.036580253 <a title="56-tfidf-10" href="../fast_ml-2013/fast_ml-2013-01-07-Machine_learning_courses_online.html">15 fast ml-2013-01-07-Machine learning courses online</a></p>
<p>11 0.034224302 <a title="56-tfidf-11" href="../fast_ml-2012/fast_ml-2012-12-21-Tuning_hyperparams_automatically_with_Spearmint.html">12 fast ml-2012-12-21-Tuning hyperparams automatically with Spearmint</a></p>
<p>12 0.033995584 <a title="56-tfidf-12" href="../fast_ml-2013/fast_ml-2013-02-27-Dimensionality_reduction_for_sparse_binary_data.html">21 fast ml-2013-02-27-Dimensionality reduction for sparse binary data</a></p>
<p>13 0.032985132 <a title="56-tfidf-13" href="../fast_ml-2013/fast_ml-2013-02-18-Predicting_advertised_salaries.html">20 fast ml-2013-02-18-Predicting advertised salaries</a></p>
<p>14 0.031933486 <a title="56-tfidf-14" href="../fast_ml-2013/fast_ml-2013-03-18-Large_scale_L1_feature_selection_with_Vowpal_Wabbit.html">23 fast ml-2013-03-18-Large scale L1 feature selection with Vowpal Wabbit</a></p>
<p>15 0.031901091 <a title="56-tfidf-15" href="../fast_ml-2013/fast_ml-2013-11-18-CUDA_on_a_Linux_laptop.html">44 fast ml-2013-11-18-CUDA on a Linux laptop</a></p>
<p>16 0.030259442 <a title="56-tfidf-16" href="../fast_ml-2013/fast_ml-2013-09-09-Predicting_solar_energy_from_weather_forecasts_plus_a_NetCDF4_tutorial.html">38 fast ml-2013-09-09-Predicting solar energy from weather forecasts plus a NetCDF4 tutorial</a></p>
<p>17 0.02964719 <a title="56-tfidf-17" href="../fast_ml-2013/fast_ml-2013-10-09-Big_data_made_easy.html">41 fast ml-2013-10-09-Big data made easy</a></p>
<p>18 0.028547259 <a title="56-tfidf-18" href="../fast_ml-2013/fast_ml-2013-04-10-Gender_discrimination.html">25 fast ml-2013-04-10-Gender discrimination</a></p>
<p>19 0.028281126 <a title="56-tfidf-19" href="../fast_ml-2014/fast_ml-2014-05-26-Yann_LeCun%27s_answers_from_the_Reddit_AMA.html">62 fast ml-2014-05-26-Yann LeCun's answers from the Reddit AMA</a></p>
<p>20 0.028034175 <a title="56-tfidf-20" href="../fast_ml-2013/fast_ml-2013-05-01-Deep_learning_made_easy.html">27 fast ml-2013-05-01-Deep learning made easy</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/fast_ml_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.112), (1, -0.044), (2, 0.044), (3, -0.008), (4, -0.114), (5, 0.066), (6, -0.128), (7, 0.104), (8, 0.208), (9, 0.003), (10, 0.252), (11, -0.042), (12, 0.164), (13, 0.294), (14, -0.217), (15, 0.089), (16, 0.119), (17, -0.229), (18, 0.087), (19, -0.002), (20, -0.015), (21, -0.021), (22, 0.078), (23, 0.034), (24, 0.043), (25, 0.071), (26, -0.133), (27, 0.063), (28, 0.319), (29, -0.156), (30, 0.335), (31, -0.232), (32, -0.014), (33, 0.083), (34, 0.329), (35, -0.118), (36, -0.079), (37, 0.206), (38, -0.077), (39, 0.145), (40, 0.122), (41, -0.02), (42, -0.012), (43, 0.122), (44, 0.131), (45, -0.042), (46, -0.011), (47, -0.014), (48, 0.002), (49, 0.02)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98879981 <a title="56-lsi-1" href="../fast_ml-2014/fast_ml-2014-03-31-If_you_use_R%2C_you_may_want_RStudio.html">56 fast ml-2014-03-31-If you use R, you may want RStudio</a></p>
<p>Introduction: RStudio  is an IDE for R. It gives the language a bit of a slickness factor it so badly needs. The nice thing about the software, beside good looks, is that it integrates console, help pages, plots and editor (if you want it) in one place.
   
For example, instead of switching for help to a web browser, you have it right next to the console. The same thing with plots: in place of the usual overlapping windows, all plots go to one pane where you can navigate back and forth between them with arrows. You can save a plot as an image or as a PDF. While saving as image you are presented with a preview and you get to choose the image format and size. That’s the kind of detail that shows how RStudio makes working with R better.
 
You can have up to four panes on the screen:
  
 console 
 source / data 
 plots / help / files 
 history and workspace 
  
 
 
Arrange them in any way you like.
 
The source pane has a  run  button, so you can execute the current line of code in the console, or selec</p><p>2 0.1257717 <a title="56-lsi-2" href="../fast_ml-2012/fast_ml-2012-09-01-Running_Unix_apps_on_Windows.html">3 fast ml-2012-09-01-Running Unix apps on Windows</a></p>
<p>Introduction: When it comes to machine learning, most software seems to be in either Python, Matlab or R. Plus native apps, that is, compiled C/C++. These are fastest. Most of them is written for Unix environments, for example Linux or MacOS. So how do you run them on your computer if you have Windows installed?
   
Back in the day, you re-partitioned your hard drive and installed Linux alongside Windows. The added thrill was, if something went wrong, your computer wouldn’t boot.
 
Now it’s easier. You just run Linux inside Windows, using what’s called a virtual machine. You need virtualization software and a machine image to do so.
 
Most popular software seems to be VMware. There is also VirtualBox - it is able to run VMware images. We have experience with WMware mostly, so this is what we’ll refer to.  VMware player  is free to download and use.
 
There are also  many images  available, of various flavours of Linux and other operating systems. In fact, you can run Windows inside Linux if you wish</p><p>3 0.089845784 <a title="56-lsi-3" href="../fast_ml-2013/fast_ml-2013-02-27-Dimensionality_reduction_for_sparse_binary_data.html">21 fast ml-2013-02-27-Dimensionality reduction for sparse binary data</a></p>
<p>Introduction: Much of data in machine learning is sparse, that is mostly zeros, and often binary. The phenomenon may result from converting categorical variables to one-hot vectors, and from converting text to bag-of-words representation. If each feature is binary - either zero or one - then it holds exactly one bit of information. Surely we could somehow compress such data to fewer real numbers.
   
To do this, we turn to topic models, an area of research with roots in natural language processing. In NLP, a training set is called a corpus, and each document is like a row in the set. A document might be three pages of text, or just a few words, as in a tweet.
 
The idea of topic modelling is that you can group words in your corpus into relatively few topics and represent each document as a mixture of these topics. It’s attractive because you can interpret the model by looking at words that form the topics. Sometimes they seem meaningful, sometimes not. A meaningful topic might be, for example: “cric</p><p>4 0.089018978 <a title="56-lsi-4" href="../fast_ml-2014/fast_ml-2014-01-25-Why_IPy%3A_reasons_for_using_IPython_interactively.html">51 fast ml-2014-01-25-Why IPy: reasons for using IPython interactively</a></p>
<p>Introduction: IPython  is known for the notebooks. But the first thing they list on their homepage is a “powerful interactive shell”. And that’s true - if you use Python interactively, you’ll dig IPython.
   
Here are a few features that make a difference for us:
  
 
command line history and autocomplete 
  These are basic shell features you come to expect and rely upon after a first contact with a Unix shell. The standard Python interpreter feels somewhat impoverished without them.
 
 
 %paste  
  Normally when you paste indented code, it won’t work. You can fix it with this magic command.
 
 
other  %magic  commands 
  Actually, you can skip the  % :  paste ,  run some_script.py ,  pwd ,  cd some_dir 
 
 
shell commands with  !  
  For example  !ls -las . Alas, aliases from your system shell don’t work.
 
 
looks good 
  Red and green add a nice touch, especially in a window with dark background. 
 
  
 
 If you have a light background, use  %colors LightBG  
 
Share your favourite features</p><p>5 0.088289857 <a title="56-lsi-5" href="../fast_ml-2013/fast_ml-2013-11-27-Object_recognition_in_images_with_cuda-convnet.html">45 fast ml-2013-11-27-Object recognition in images with cuda-convnet</a></p>
<p>Introduction: Object recognition in images is where deep learning, and specifically convolutional neural networks, are often applied and benchmarked these days. To get a piece of the action, we’ll be using Alex Krizhevsky’s  cuda-convnet , a shining diamond of machine learning software, in a Kaggle competition.
   
Continuing to run things on a GPU, we turn to applying  convolutional neural networks  for object recognition. This kind of network was developed by Yann LeCun and it’s powerful, but a bit complicated:
 
  
 Image credit:  EBLearn tutorial  
 
A typical convolutional network has two parts. The first is responsible for feature extraction and consists of one or more pairs of convolution and subsampling/max-pooling layers, as you can see above. The second part is just a classic fully-connected multilayer perceptron taking extracted features as input. For a detailed explanation of all this see unit 9 in Hugo LaRochelle’s  neural networks course .
 
Daniel Nouri has an interesting story about</p><p>6 0.086830363 <a title="56-lsi-6" href="../fast_ml-2012/fast_ml-2012-10-05-Predicting_closed_questions_on_Stack_Overflow.html">7 fast ml-2012-10-05-Predicting closed questions on Stack Overflow</a></p>
<p>7 0.078390047 <a title="56-lsi-7" href="../fast_ml-2013/fast_ml-2013-05-01-Deep_learning_made_easy.html">27 fast ml-2013-05-01-Deep learning made easy</a></p>
<p>8 0.077164859 <a title="56-lsi-8" href="../fast_ml-2012/fast_ml-2012-12-21-Tuning_hyperparams_automatically_with_Spearmint.html">12 fast ml-2012-12-21-Tuning hyperparams automatically with Spearmint</a></p>
<p>9 0.072616145 <a title="56-lsi-9" href="../fast_ml-2014/fast_ml-2014-04-30-Converting_categorical_data_into_numbers_with_Pandas_and_Scikit-learn.html">60 fast ml-2014-04-30-Converting categorical data into numbers with Pandas and Scikit-learn</a></p>
<p>10 0.067310251 <a title="56-lsi-10" href="../fast_ml-2013/fast_ml-2013-01-14-Feature_selection_in_practice.html">17 fast ml-2013-01-14-Feature selection in practice</a></p>
<p>11 0.067151234 <a title="56-lsi-11" href="../fast_ml-2013/fast_ml-2013-07-09-Introducing_phraug.html">33 fast ml-2013-07-09-Introducing phraug</a></p>
<p>12 0.067142427 <a title="56-lsi-12" href="../fast_ml-2013/fast_ml-2013-01-07-Machine_learning_courses_online.html">15 fast ml-2013-01-07-Machine learning courses online</a></p>
<p>13 0.062215682 <a title="56-lsi-13" href="../fast_ml-2013/fast_ml-2013-09-09-Predicting_solar_energy_from_weather_forecasts_plus_a_NetCDF4_tutorial.html">38 fast ml-2013-09-09-Predicting solar energy from weather forecasts plus a NetCDF4 tutorial</a></p>
<p>14 0.059290744 <a title="56-lsi-14" href="../fast_ml-2013/fast_ml-2013-04-10-Gender_discrimination.html">25 fast ml-2013-04-10-Gender discrimination</a></p>
<p>15 0.058722299 <a title="56-lsi-15" href="../fast_ml-2013/fast_ml-2013-03-18-Large_scale_L1_feature_selection_with_Vowpal_Wabbit.html">23 fast ml-2013-03-18-Large scale L1 feature selection with Vowpal Wabbit</a></p>
<p>16 0.058029357 <a title="56-lsi-16" href="../fast_ml-2013/fast_ml-2013-02-18-Predicting_advertised_salaries.html">20 fast ml-2013-02-18-Predicting advertised salaries</a></p>
<p>17 0.057500184 <a title="56-lsi-17" href="../fast_ml-2014/fast_ml-2014-02-02-Yesterday_a_kaggler%2C_today_a_Kaggle_master%3A_a_wrap-up_of_the_cats_and_dogs_competition.html">52 fast ml-2014-02-02-Yesterday a kaggler, today a Kaggle master: a wrap-up of the cats and dogs competition</a></p>
<p>18 0.056356169 <a title="56-lsi-18" href="../fast_ml-2014/fast_ml-2014-05-26-Yann_LeCun%27s_answers_from_the_Reddit_AMA.html">62 fast ml-2014-05-26-Yann LeCun's answers from the Reddit AMA</a></p>
<p>19 0.054689832 <a title="56-lsi-19" href="../fast_ml-2014/fast_ml-2014-01-10-Classifying_images_with_a_pre-trained_deep_network.html">49 fast ml-2014-01-10-Classifying images with a pre-trained deep network</a></p>
<p>20 0.053663597 <a title="56-lsi-20" href="../fast_ml-2012/fast_ml-2012-08-27-Kaggle_job_recommendation_challenge.html">2 fast ml-2012-08-27-Kaggle job recommendation challenge</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/fast_ml_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(6, 0.025), (26, 0.035), (55, 0.057), (69, 0.064), (71, 0.016), (91, 0.649), (99, 0.02)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.91031152 <a title="56-lda-1" href="../fast_ml-2014/fast_ml-2014-03-31-If_you_use_R%2C_you_may_want_RStudio.html">56 fast ml-2014-03-31-If you use R, you may want RStudio</a></p>
<p>Introduction: RStudio  is an IDE for R. It gives the language a bit of a slickness factor it so badly needs. The nice thing about the software, beside good looks, is that it integrates console, help pages, plots and editor (if you want it) in one place.
   
For example, instead of switching for help to a web browser, you have it right next to the console. The same thing with plots: in place of the usual overlapping windows, all plots go to one pane where you can navigate back and forth between them with arrows. You can save a plot as an image or as a PDF. While saving as image you are presented with a preview and you get to choose the image format and size. That’s the kind of detail that shows how RStudio makes working with R better.
 
You can have up to four panes on the screen:
  
 console 
 source / data 
 plots / help / files 
 history and workspace 
  
 
 
Arrange them in any way you like.
 
The source pane has a  run  button, so you can execute the current line of code in the console, or selec</p><p>2 0.12366946 <a title="56-lda-2" href="../fast_ml-2013/fast_ml-2013-07-05-Processing_large_files%2C_line_by_line.html">32 fast ml-2013-07-05-Processing large files, line by line</a></p>
<p>Introduction: Perhaps the most common format of data for machine learning is text files. Often data is too large to fit in memory; this is sometimes referred to as big data. But do you need to load the whole data into memory? Maybe you could at least pre-process it line by line. We show how to do this with Python. Prepare to read and possibly write some code.
   
The most common format for text files is probably CSV. For sparse data,  libsvm  format is popular. Both can be processed using csv module in Python.
   import csv

i_f = open( input_file, 'r' )
reader = csv.reader( i_f )
   
For  libsvm  you just set the delimiter to space:
   reader = csv.reader( i_f, delimiter = ' ' )
   
Then you go over the file contents. Each line is a list of strings:
   for line in reader:

    # do something with the line, for example:
    label = float( line[0] )
    # ....

    writer.writerow( line )
   
If you need to do a second pass, you just rewind the input file:
   i_f.seek( 0 )
for line in re</p><p>3 0.12217782 <a title="56-lda-3" href="../fast_ml-2012/fast_ml-2012-10-05-Predicting_closed_questions_on_Stack_Overflow.html">7 fast ml-2012-10-05-Predicting closed questions on Stack Overflow</a></p>
<p>Introduction: This time we enter the  Stack Overflow challenge , which is about predicting a status of a given question on SO. There are five possible statuses, so it’s a multi-class classification problem.
 
We would prefer a tool able to perform multiclass classification by itself. It can be done by hand by constructing five datasets, each with binary labels (one class against all others), and then combining predictions, but it might be a bit tricky to get right - we tried. Fortunately, nice people at Yahoo, excuse us, Microsoft, recently relased a new version of  Vowpal Wabbit , and this new version supports multiclass classification.
   
In case you’re wondering, Vowpal Wabbit is a fast linear learner. We like the “fast” part and “linear” is OK for dealing with lots of words, as in this contest. In any case, with more than three million data points it wouldn’t be that easy to train a kernel SVM, a neural net or what have you.
 
VW, being a well-polished tool, has a few very convenient features.</p><p>4 0.1175337 <a title="56-lda-4" href="../fast_ml-2013/fast_ml-2013-02-18-Predicting_advertised_salaries.html">20 fast ml-2013-02-18-Predicting advertised salaries</a></p>
<p>Introduction: We’re back to Kaggle competitions. This time we will attempt to predict advertised salaries from job ads and of course beat the benchmark. The benchmark is, as usual, a random forest result. For starters, we’ll use a linear model without much preprocessing. Will it be enough?
   
  
 Congratulations! You have spotted the ceiling cat. 
 
A linear model better than a random forest - how so? Well, to train a random forest on data this big, the benchmark code extracts only 100 most common words as features, and we will use all. This approach is similiar to the one we applied in  Merck challenge . More data beats a cleverer algorithm, especially when a cleverer algorithm is unable to handle all of data (on your machine, anyway).
 
The competition is about predicting salaries from job adverts. Of course the figures usually appear in the text, so they were removed. An error metric is mean absolute error (MAE) - how refreshing to see so intuitive one.
 
The data for  Job salary prediction  con</p><p>5 0.10982095 <a title="56-lda-5" href="../fast_ml-2013/fast_ml-2013-04-10-Gender_discrimination.html">25 fast ml-2013-04-10-Gender discrimination</a></p>
<p>Introduction: There’s a contest at Kaggle held by Qatar University. They want to be able to discriminate men from women based on handwriting. For a thousand bucks, well, why not?
   
  
 Congratulations! You have spotted the ceiling cat. 
 
 
 
As  Sashi noticed on the forums , it’s not difficult to improve on the benchmarks a little bit. In particular, he mentioned feature selection, normalizing the data and using a regularized linear model. Here’s our version of the story.
 
Let’s start with normalizing. There’s a nice function for that in R,  scale() . The dataset is small, 1128 examples, so we can go ahead and use R.
 
Turns out that in its raw form, the data won’t scale. That’s because apparently there are some columns with zeros only. That makes it difficult to divide.
 
Fortunately, we know just the right tool for the task. We learned about it on Kaggle forums too. It’s a function in   caret  package  called  nearZeroVar() . It will give you indexes of all the columns which have near zero var</p><p>6 0.10808226 <a title="56-lda-6" href="../fast_ml-2013/fast_ml-2013-07-09-Introducing_phraug.html">33 fast ml-2013-07-09-Introducing phraug</a></p>
<p>7 0.10746779 <a title="56-lda-7" href="../fast_ml-2012/fast_ml-2012-12-21-Tuning_hyperparams_automatically_with_Spearmint.html">12 fast ml-2012-12-21-Tuning hyperparams automatically with Spearmint</a></p>
<p>8 0.10655983 <a title="56-lda-8" href="../fast_ml-2013/fast_ml-2013-01-14-Feature_selection_in_practice.html">17 fast ml-2013-01-14-Feature selection in practice</a></p>
<p>9 0.10623839 <a title="56-lda-9" href="../fast_ml-2013/fast_ml-2013-08-12-Accelerometer_Biometric_Competition.html">35 fast ml-2013-08-12-Accelerometer Biometric Competition</a></p>
<p>10 0.10617024 <a title="56-lda-10" href="../fast_ml-2013/fast_ml-2013-10-06-Pylearn2_in_practice.html">40 fast ml-2013-10-06-Pylearn2 in practice</a></p>
<p>11 0.10495853 <a title="56-lda-11" href="../fast_ml-2012/fast_ml-2012-12-27-Spearmint_with_a_random_forest.html">13 fast ml-2012-12-27-Spearmint with a random forest</a></p>
<p>12 0.10465599 <a title="56-lda-12" href="../fast_ml-2013/fast_ml-2013-03-18-Large_scale_L1_feature_selection_with_Vowpal_Wabbit.html">23 fast ml-2013-03-18-Large scale L1 feature selection with Vowpal Wabbit</a></p>
<p>13 0.1041629 <a title="56-lda-13" href="../fast_ml-2013/fast_ml-2013-05-01-Deep_learning_made_easy.html">27 fast ml-2013-05-01-Deep learning made easy</a></p>
<p>14 0.10307463 <a title="56-lda-14" href="../fast_ml-2013/fast_ml-2013-02-27-Dimensionality_reduction_for_sparse_binary_data.html">21 fast ml-2013-02-27-Dimensionality reduction for sparse binary data</a></p>
<p>15 0.10235558 <a title="56-lda-15" href="../fast_ml-2013/fast_ml-2013-11-02-Maxing_out_the_digits.html">43 fast ml-2013-11-02-Maxing out the digits</a></p>
<p>16 0.10155788 <a title="56-lda-16" href="../fast_ml-2012/fast_ml-2012-08-09-What_you_wanted_to_know_about_Mean_Average_Precision.html">1 fast ml-2012-08-09-What you wanted to know about Mean Average Precision</a></p>
<p>17 0.10115141 <a title="56-lda-17" href="../fast_ml-2012/fast_ml-2012-10-25-So_you_want_to_work_for_Facebook.html">9 fast ml-2012-10-25-So you want to work for Facebook</a></p>
<p>18 0.10094336 <a title="56-lda-18" href="../fast_ml-2013/fast_ml-2013-01-04-Madelon%3A_Spearmint%27s_revenge.html">14 fast ml-2013-01-04-Madelon: Spearmint's revenge</a></p>
<p>19 0.10082833 <a title="56-lda-19" href="../fast_ml-2013/fast_ml-2013-12-28-Regularizing_neural_networks_with_dropout_and_with_DropConnect.html">48 fast ml-2013-12-28-Regularizing neural networks with dropout and with DropConnect</a></p>
<p>20 0.10069567 <a title="56-lda-20" href="../fast_ml-2013/fast_ml-2013-01-12-Intro_to_random_forests.html">16 fast ml-2013-01-12-Intro to random forests</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
