<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>53 fast ml-2014-02-20-Are stocks predictable?</title>
</head>

<body>
<p><a title="fast_ml" href="../fast_ml_home.html">fast_ml</a> <a title="fast_ml-2014" href="../home/fast_ml-2014_home.html">fast_ml-2014</a> <a title="fast_ml-2014-53" href="#">fast_ml-2014-53</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>53 fast ml-2014-02-20-Are stocks predictable?</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="fast_ml-2014-53-html" href="http://fastml.com//are-stocks-predictable/">html</a></p><p>Introduction: We’d like to be able to predict stock market. That seems like a nice way of making money. We’ll address the fundamental issue: can stocks be predicted in the short term, that is a few days ahead?
   
There’s a technique that seeks to answer the question of predictability. It’s called  Forecastable Component Analysis .  Based on a new forecastability measure, ForeCA  finds an optimal transformation to separate a multivariate time series into a forecastable and an orthogonal white noise space.  The author, Georg M. Goerg*, implemented it in R package  ForeCA .
 
It might be useful in two ways:
  
 It can tell you how forecastable time series is. 
 Given a multivariate time series, let’s say a portfolio of stocks, it can find forecastable components. 
  
The idea in the second point is similiar to PCA - ForeCA is a linear dimensionality reduction technique. The main difference is that the method explicitly addresses forecastability. It does so by considering an interplay between time and</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We’ll address the fundamental issue: can stocks be predicted in the short term, that is a few days ahead? [sent-3, score-0.189]
</p><p>2 Based on a new forecastability measure, ForeCA  finds an optimal transformation to separate a multivariate time series into a forecastable and an orthogonal white noise space. [sent-6, score-0.914]
</p><p>3 It might be useful in two ways:     It can tell you how forecastable time series is. [sent-9, score-0.604]
</p><p>4 Given a multivariate time series, let’s say a portfolio of stocks, it can find forecastable components. [sent-10, score-0.547]
</p><p>5 It does so by considering an interplay between time and frequency:    Forecasting is inherently tied to the time domain. [sent-13, score-0.142]
</p><p>6 Yet, since [equations in the paper] provide a one-to-one mapping between the time and frequency domain, we can use frequency domain properties to measure forecastability. [sent-14, score-0.492]
</p><p>7 Stationary time series only   It only makes sense to apply ForeCA to at least weekly  stationary  time series. [sent-18, score-0.379]
</p><p>8 Raw stock data doesn’t fit the bill, but daily returns do. [sent-19, score-0.526]
</p><p>9 A daily return shows how price changed relative to the one day before. [sent-20, score-0.262]
</p><p>10 For example, you can learn about returns in  section 1. [sent-23, score-0.246]
</p><p>11 Daily stock data: raw (above) and returns (below). [sent-25, score-0.398]
</p><p>12 2  for an intuition about the connection between time and frequency spectrum. [sent-28, score-0.216]
</p><p>13 fEcofin, which contains the equity funds data, is no longer available at CRAN, so install it from R-Forge:    install. [sent-31, score-0.356]
</p><p>14 org/")     If you’d like to assess forecastability of a particular univariate series, use the  Omega  function:    XX <- ts(diff(log(EuStockMarkets))[-c(1:1000),]) Omega(XX)       DAX      SMI      CAC     FTSE  6. [sent-34, score-0.098]
</p><p>15 0 means not forecastable (white noise); 100 means perfectly forecastable (a sinusoid). [sent-39, score-0.788]
</p><p>16 In the equity funds example, the most forecastable component scores roughly 2. [sent-45, score-0.746]
</p><p>17 The European indexes above look more forecastable with Omega around 6%. [sent-47, score-0.394]
</p><p>18 The author says he  probably used different spectrum. [sent-49, score-0.106]
</p><p>19 method estimation - different spectrum estimators give different Omega estimates ). [sent-50, score-0.129]
</p><p>20 When dealing with finance it’s not obvious what’s the benchmark or the limit and there’s not much openness in discussing know-how. [sent-56, score-0.098]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('foreca', 0.492), ('forecastable', 0.394), ('omega', 0.295), ('daily', 0.197), ('returns', 0.181), ('equity', 0.148), ('fecofin', 0.148), ('funds', 0.148), ('mod', 0.148), ('stock', 0.148), ('stocks', 0.148), ('frequency', 0.145), ('series', 0.139), ('finance', 0.098), ('forecastability', 0.098), ('georg', 0.098), ('goerg', 0.098), ('ret', 0.098), ('stationary', 0.098), ('ts', 0.098), ('xx', 0.098), ('log', 0.084), ('multivariate', 0.082), ('lynx', 0.082), ('domain', 0.082), ('time', 0.071), ('raw', 0.069), ('white', 0.065), ('noise', 0.065), ('section', 0.065), ('return', 0.065), ('author', 0.062), ('install', 0.06), ('component', 0.056), ('page', 0.056), ('plot', 0.053), ('library', 0.052), ('paper', 0.051), ('measure', 0.049), ('different', 0.044), ('address', 0.041), ('bill', 0.041), ('explains', 0.041), ('humans', 0.041), ('recognizing', 0.041), ('spectrum', 0.041), ('repos', 0.041), ('carlo', 0.041), ('monte', 0.041), ('forecasting', 0.041)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9999997 <a title="53-tfidf-1" href="../fast_ml-2014/fast_ml-2014-02-20-Are_stocks_predictable%3F.html">53 fast ml-2014-02-20-Are stocks predictable?</a></p>
<p>Introduction: We’d like to be able to predict stock market. That seems like a nice way of making money. We’ll address the fundamental issue: can stocks be predicted in the short term, that is a few days ahead?
   
There’s a technique that seeks to answer the question of predictability. It’s called  Forecastable Component Analysis .  Based on a new forecastability measure, ForeCA  finds an optimal transformation to separate a multivariate time series into a forecastable and an orthogonal white noise space.  The author, Georg M. Goerg*, implemented it in R package  ForeCA .
 
It might be useful in two ways:
  
 It can tell you how forecastable time series is. 
 Given a multivariate time series, let’s say a portfolio of stocks, it can find forecastable components. 
  
The idea in the second point is similiar to PCA - ForeCA is a linear dimensionality reduction technique. The main difference is that the method explicitly addresses forecastability. It does so by considering an interplay between time and</p><p>2 0.06451416 <a title="53-tfidf-2" href="../fast_ml-2013/fast_ml-2013-01-17-A_very_fast_denoising_autoencoder.html">18 fast ml-2013-01-17-A very fast denoising autoencoder</a></p>
<p>Introduction: Once upon a time we were browsing machine learning papers and software. We were interested in autoencoders and found a rather unusual one. It was called  marginalized Stacked Denoising Autoencoder  and the author claimed that it  preserves the strong feature learning capacity of Stacked Denoising Autoencoders, but is orders of magnitudes faster.  We like all things fast, so we were hooked.
    About autoencoders  
 Wikipedia says  that  an autoencoder is an artificial neural network and its aim is to learn a compressed representation for a set of data. This means it is being used for dimensionality reduction . In other words, an autoencoder is a neural network meant to replicate the input. It would be trivial with a big enough number of units in a hidden layer: the network would just find an identity mapping. Hence dimensionality reduction: a hidden layer size is typically smaller than input layer.
 
mSDA is a curious specimen: it is not a neural network and it doesn’t reduce dimension</p><p>3 0.058921184 <a title="53-tfidf-3" href="../fast_ml-2014/fast_ml-2014-01-20-How_to_get_predictions_from_Pylearn2.html">50 fast ml-2014-01-20-How to get predictions from Pylearn2</a></p>
<p>Introduction: A while ago we’ve shown how to  get predictions  from a Pylearn2 model. It is a little tricky, partly because of splitting data into batches. If you’re able to fit your data in memory, you can strip the batch handling code and it becomes easier to see what’s going on. We exercise the concept to distinguish cats from dogs again, with superior results.
    Step by step  
You have a pickled model from Pylearn2. Let’s load it:
   from pylearn2.utils import serial

model_path = 'model.pkl'
model = serial.load( model_path )
   
Next, some Theano weirdness.  Theano  is a compiler for symbolic expressions and with these expressions we deal when predicting. We need to define expressions for X and Y:
   X = model.get_input_space().make_theano_batch()
Y = model.fprop( X )
   
Mind you, these are not variables, but rather descriptions of how to get variables. Y is easy to understand: just feed the data to the model and forward-propagate. X is more of an idiom, the incantations above make sur</p><p>4 0.053050522 <a title="53-tfidf-4" href="../fast_ml-2013/fast_ml-2013-05-01-Deep_learning_made_easy.html">27 fast ml-2013-05-01-Deep learning made easy</a></p>
<p>Introduction: As usual, there’s an interesting competition at Kaggle: The Black Box. It’s connected to ICML 2013 Workshop on Challenges in Representation Learning, held by the deep learning guys from Montreal.
 
There are a couple benchmarks for this competition and the best one is unusually hard to beat 1  - only less than a fourth of those taking part managed to do so. We’re among them. Here’s how.
   
The key ingredient in our success is a recently developed secret Stanford technology for deep unsupervised learning:  sparse filtering  by  Jiquan Ngiam  et al. Actually, it’s not secret. It’s  available at Github , and has one or two very appealling properties. Let us explain.
 
The main idea of deep unsupervised learning, as we understand it, is feature extraction. One of the most common applications is in  multimedia. The reason for that is that multimedia tasks, for example object recognition, are easy for humans, but difficult for computers 2 .
 
Geoff Hinton from Toronto talks about  two ends</p><p>5 0.052314825 <a title="53-tfidf-5" href="../fast_ml-2013/fast_ml-2013-12-15-A-B_testing_with_bayesian_bandits_in_Google_Analytics.html">47 fast ml-2013-12-15-A-B testing with bayesian bandits in Google Analytics</a></p>
<p>Introduction: A/B testing is a way to optimize a web page. Half of visitors see one version, the other half another, so you can tell which version is more conducive to your goal - for example selling something. Since June 2013 A/B testing can be conveniently done with Google Analytics. Here’s how.
   
 This article is not quite about machine learning. If you’re not interested in testing, scroll down to the  bayesian bandits section . 
  Google Content Experiments  
We remember Google Website Optimizer from a few years ago. It wasn’t exactly user friendly or slick, but it felt solid and did the job. Unfortunately, at one point in time Google pulled the plug, leaving  Genetify  as a sole free (and open source) tool for multivariate testing.  Multivariate  means testing a few elements on a page simultanously.
 
At that time they launched Content Experiments in Google Analytics, but it was a giant step backward. Content experiments were very primitive and only allowed rudimentary A/B split testing. It i</p><p>6 0.049545523 <a title="53-tfidf-6" href="../fast_ml-2014/fast_ml-2014-03-20-Good_representations%2C_distance%2C_metric_learning_and_supervised_dimensionality_reduction.html">55 fast ml-2014-03-20-Good representations, distance, metric learning and supervised dimensionality reduction</a></p>
<p>7 0.043709844 <a title="53-tfidf-7" href="../fast_ml-2013/fast_ml-2013-03-25-Dimensionality_reduction_for_sparse_binary_data_-_an_overview.html">24 fast ml-2013-03-25-Dimensionality reduction for sparse binary data - an overview</a></p>
<p>8 0.043418102 <a title="53-tfidf-8" href="../fast_ml-2014/fast_ml-2014-05-26-Yann_LeCun%27s_answers_from_the_Reddit_AMA.html">62 fast ml-2014-05-26-Yann LeCun's answers from the Reddit AMA</a></p>
<p>9 0.041268587 <a title="53-tfidf-9" href="../fast_ml-2012/fast_ml-2012-10-05-Predicting_closed_questions_on_Stack_Overflow.html">7 fast ml-2012-10-05-Predicting closed questions on Stack Overflow</a></p>
<p>10 0.040443283 <a title="53-tfidf-10" href="../fast_ml-2013/fast_ml-2013-12-07-13_NIPS_papers_that_caught_our_eye.html">46 fast ml-2013-12-07-13 NIPS papers that caught our eye</a></p>
<p>11 0.039894093 <a title="53-tfidf-11" href="../fast_ml-2013/fast_ml-2013-02-27-Dimensionality_reduction_for_sparse_binary_data.html">21 fast ml-2013-02-27-Dimensionality reduction for sparse binary data</a></p>
<p>12 0.0389666 <a title="53-tfidf-12" href="../fast_ml-2013/fast_ml-2013-02-18-Predicting_advertised_salaries.html">20 fast ml-2013-02-18-Predicting advertised salaries</a></p>
<p>13 0.03776326 <a title="53-tfidf-13" href="../fast_ml-2012/fast_ml-2012-12-21-Tuning_hyperparams_automatically_with_Spearmint.html">12 fast ml-2012-12-21-Tuning hyperparams automatically with Spearmint</a></p>
<p>14 0.035871588 <a title="53-tfidf-14" href="../fast_ml-2013/fast_ml-2013-04-17-Regression_as_classification.html">26 fast ml-2013-04-17-Regression as classification</a></p>
<p>15 0.034924306 <a title="53-tfidf-15" href="../fast_ml-2013/fast_ml-2013-02-07-The_secret_of_the_big_guys.html">19 fast ml-2013-02-07-The secret of the big guys</a></p>
<p>16 0.034651622 <a title="53-tfidf-16" href="../fast_ml-2014/fast_ml-2014-04-12-Deep_learning_these_days.html">58 fast ml-2014-04-12-Deep learning these days</a></p>
<p>17 0.034333773 <a title="53-tfidf-17" href="../fast_ml-2013/fast_ml-2013-04-10-Gender_discrimination.html">25 fast ml-2013-04-10-Gender discrimination</a></p>
<p>18 0.033406425 <a title="53-tfidf-18" href="../fast_ml-2013/fast_ml-2013-11-27-Object_recognition_in_images_with_cuda-convnet.html">45 fast ml-2013-11-27-Object recognition in images with cuda-convnet</a></p>
<p>19 0.032567672 <a title="53-tfidf-19" href="../fast_ml-2013/fast_ml-2013-08-12-Accelerometer_Biometric_Competition.html">35 fast ml-2013-08-12-Accelerometer Biometric Competition</a></p>
<p>20 0.032457974 <a title="53-tfidf-20" href="../fast_ml-2013/fast_ml-2013-07-05-Processing_large_files%2C_line_by_line.html">32 fast ml-2013-07-05-Processing large files, line by line</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/fast_ml_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.143), (1, 0.038), (2, 0.067), (3, -0.006), (4, -0.016), (5, 0.027), (6, -0.046), (7, -0.018), (8, -0.027), (9, 0.056), (10, 0.128), (11, 0.131), (12, -0.076), (13, -0.202), (14, -0.182), (15, -0.317), (16, -0.087), (17, -0.301), (18, 0.086), (19, 0.003), (20, 0.366), (21, -0.043), (22, 0.111), (23, -0.078), (24, 0.131), (25, 0.276), (26, 0.186), (27, 0.329), (28, -0.204), (29, -0.027), (30, -0.03), (31, -0.2), (32, 0.004), (33, 0.02), (34, -0.044), (35, 0.326), (36, -0.172), (37, -0.035), (38, -0.028), (39, 0.04), (40, 0.096), (41, -0.053), (42, 0.05), (43, 0.036), (44, -0.107), (45, -0.002), (46, -0.003), (47, -0.011), (48, 0.047), (49, 0.014)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9868086 <a title="53-lsi-1" href="../fast_ml-2014/fast_ml-2014-02-20-Are_stocks_predictable%3F.html">53 fast ml-2014-02-20-Are stocks predictable?</a></p>
<p>Introduction: We’d like to be able to predict stock market. That seems like a nice way of making money. We’ll address the fundamental issue: can stocks be predicted in the short term, that is a few days ahead?
   
There’s a technique that seeks to answer the question of predictability. It’s called  Forecastable Component Analysis .  Based on a new forecastability measure, ForeCA  finds an optimal transformation to separate a multivariate time series into a forecastable and an orthogonal white noise space.  The author, Georg M. Goerg*, implemented it in R package  ForeCA .
 
It might be useful in two ways:
  
 It can tell you how forecastable time series is. 
 Given a multivariate time series, let’s say a portfolio of stocks, it can find forecastable components. 
  
The idea in the second point is similiar to PCA - ForeCA is a linear dimensionality reduction technique. The main difference is that the method explicitly addresses forecastability. It does so by considering an interplay between time and</p><p>2 0.089012869 <a title="53-lsi-2" href="../fast_ml-2013/fast_ml-2013-05-01-Deep_learning_made_easy.html">27 fast ml-2013-05-01-Deep learning made easy</a></p>
<p>Introduction: As usual, there’s an interesting competition at Kaggle: The Black Box. It’s connected to ICML 2013 Workshop on Challenges in Representation Learning, held by the deep learning guys from Montreal.
 
There are a couple benchmarks for this competition and the best one is unusually hard to beat 1  - only less than a fourth of those taking part managed to do so. We’re among them. Here’s how.
   
The key ingredient in our success is a recently developed secret Stanford technology for deep unsupervised learning:  sparse filtering  by  Jiquan Ngiam  et al. Actually, it’s not secret. It’s  available at Github , and has one or two very appealling properties. Let us explain.
 
The main idea of deep unsupervised learning, as we understand it, is feature extraction. One of the most common applications is in  multimedia. The reason for that is that multimedia tasks, for example object recognition, are easy for humans, but difficult for computers 2 .
 
Geoff Hinton from Toronto talks about  two ends</p><p>3 0.083101928 <a title="53-lsi-3" href="../fast_ml-2013/fast_ml-2013-01-17-A_very_fast_denoising_autoencoder.html">18 fast ml-2013-01-17-A very fast denoising autoencoder</a></p>
<p>Introduction: Once upon a time we were browsing machine learning papers and software. We were interested in autoencoders and found a rather unusual one. It was called  marginalized Stacked Denoising Autoencoder  and the author claimed that it  preserves the strong feature learning capacity of Stacked Denoising Autoencoders, but is orders of magnitudes faster.  We like all things fast, so we were hooked.
    About autoencoders  
 Wikipedia says  that  an autoencoder is an artificial neural network and its aim is to learn a compressed representation for a set of data. This means it is being used for dimensionality reduction . In other words, an autoencoder is a neural network meant to replicate the input. It would be trivial with a big enough number of units in a hidden layer: the network would just find an identity mapping. Hence dimensionality reduction: a hidden layer size is typically smaller than input layer.
 
mSDA is a curious specimen: it is not a neural network and it doesn’t reduce dimension</p><p>4 0.077508464 <a title="53-lsi-4" href="../fast_ml-2012/fast_ml-2012-10-05-Predicting_closed_questions_on_Stack_Overflow.html">7 fast ml-2012-10-05-Predicting closed questions on Stack Overflow</a></p>
<p>Introduction: This time we enter the  Stack Overflow challenge , which is about predicting a status of a given question on SO. There are five possible statuses, so it’s a multi-class classification problem.
 
We would prefer a tool able to perform multiclass classification by itself. It can be done by hand by constructing five datasets, each with binary labels (one class against all others), and then combining predictions, but it might be a bit tricky to get right - we tried. Fortunately, nice people at Yahoo, excuse us, Microsoft, recently relased a new version of  Vowpal Wabbit , and this new version supports multiclass classification.
   
In case you’re wondering, Vowpal Wabbit is a fast linear learner. We like the “fast” part and “linear” is OK for dealing with lots of words, as in this contest. In any case, with more than three million data points it wouldn’t be that easy to train a kernel SVM, a neural net or what have you.
 
VW, being a well-polished tool, has a few very convenient features.</p><p>5 0.076806195 <a title="53-lsi-5" href="../fast_ml-2013/fast_ml-2013-03-25-Dimensionality_reduction_for_sparse_binary_data_-_an_overview.html">24 fast ml-2013-03-25-Dimensionality reduction for sparse binary data - an overview</a></p>
<p>Introduction: Last time we explored dimensionality reduction in practice using Gensim’s LSI and LDA. Now, having spent some time researching the subject matter, we will give an overview of other options.
   
 UPDATE : We now consider the topic quite irrelevant, because sparse high-dimensional data is precisely where linear models shine. See  Amazon aspires to automate access control ,  Predicting advertised salaries  and  Predicting closed questions on Stack Overflow .
 
And the few most popular methods are:
  
 LSI/LSA - a multinomial PCA 
 LDA - Latent Dirichlet Allocation 
 matrix factorization, in particular non-negative variants: NMF 
 ICA, or Independent Components Analysis 
 mixtures of Bernoullis 
 stacked RBMs 
 correlated topic models, an extension of LDA 
  
We  tried the first two  before.
 
As regards matrix factorization, you do the same stuff as with movie recommendations (think Netflix challenge). The difference is, now all the matrix elements are known and we are only interested in</p><p>6 0.076516017 <a title="53-lsi-6" href="../fast_ml-2014/fast_ml-2014-03-20-Good_representations%2C_distance%2C_metric_learning_and_supervised_dimensionality_reduction.html">55 fast ml-2014-03-20-Good representations, distance, metric learning and supervised dimensionality reduction</a></p>
<p>7 0.072948501 <a title="53-lsi-7" href="../fast_ml-2013/fast_ml-2013-02-27-Dimensionality_reduction_for_sparse_binary_data.html">21 fast ml-2013-02-27-Dimensionality reduction for sparse binary data</a></p>
<p>8 0.072787099 <a title="53-lsi-8" href="../fast_ml-2013/fast_ml-2013-12-15-A-B_testing_with_bayesian_bandits_in_Google_Analytics.html">47 fast ml-2013-12-15-A-B testing with bayesian bandits in Google Analytics</a></p>
<p>9 0.071312398 <a title="53-lsi-9" href="../fast_ml-2013/fast_ml-2013-12-07-13_NIPS_papers_that_caught_our_eye.html">46 fast ml-2013-12-07-13 NIPS papers that caught our eye</a></p>
<p>10 0.069566213 <a title="53-lsi-10" href="../fast_ml-2014/fast_ml-2014-05-26-Yann_LeCun%27s_answers_from_the_Reddit_AMA.html">62 fast ml-2014-05-26-Yann LeCun's answers from the Reddit AMA</a></p>
<p>11 0.067861505 <a title="53-lsi-11" href="../fast_ml-2014/fast_ml-2014-01-20-How_to_get_predictions_from_Pylearn2.html">50 fast ml-2014-01-20-How to get predictions from Pylearn2</a></p>
<p>12 0.066242158 <a title="53-lsi-12" href="../fast_ml-2013/fast_ml-2013-09-09-Predicting_solar_energy_from_weather_forecasts_plus_a_NetCDF4_tutorial.html">38 fast ml-2013-09-09-Predicting solar energy from weather forecasts plus a NetCDF4 tutorial</a></p>
<p>13 0.066234626 <a title="53-lsi-13" href="../fast_ml-2013/fast_ml-2013-02-07-The_secret_of_the_big_guys.html">19 fast ml-2013-02-07-The secret of the big guys</a></p>
<p>14 0.065126292 <a title="53-lsi-14" href="../fast_ml-2012/fast_ml-2012-12-21-Tuning_hyperparams_automatically_with_Spearmint.html">12 fast ml-2012-12-21-Tuning hyperparams automatically with Spearmint</a></p>
<p>15 0.06468001 <a title="53-lsi-15" href="../fast_ml-2014/fast_ml-2014-01-10-Classifying_images_with_a_pre-trained_deep_network.html">49 fast ml-2014-01-10-Classifying images with a pre-trained deep network</a></p>
<p>16 0.063641347 <a title="53-lsi-16" href="../fast_ml-2013/fast_ml-2013-11-18-CUDA_on_a_Linux_laptop.html">44 fast ml-2013-11-18-CUDA on a Linux laptop</a></p>
<p>17 0.062934458 <a title="53-lsi-17" href="../fast_ml-2014/fast_ml-2014-04-12-Deep_learning_these_days.html">58 fast ml-2014-04-12-Deep learning these days</a></p>
<p>18 0.06038605 <a title="53-lsi-18" href="../fast_ml-2013/fast_ml-2013-04-17-Regression_as_classification.html">26 fast ml-2013-04-17-Regression as classification</a></p>
<p>19 0.059556119 <a title="53-lsi-19" href="../fast_ml-2012/fast_ml-2012-12-07-Predicting_wine_quality.html">11 fast ml-2012-12-07-Predicting wine quality</a></p>
<p>20 0.058603089 <a title="53-lsi-20" href="../fast_ml-2013/fast_ml-2013-07-14-Running_things_on_a_GPU.html">34 fast ml-2013-07-14-Running things on a GPU</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/fast_ml_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(6, 0.018), (11, 0.516), (26, 0.045), (31, 0.032), (35, 0.031), (37, 0.012), (55, 0.015), (58, 0.016), (69, 0.117), (71, 0.023), (78, 0.021), (84, 0.032), (99, 0.023)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.91650856 <a title="53-lda-1" href="../fast_ml-2014/fast_ml-2014-02-20-Are_stocks_predictable%3F.html">53 fast ml-2014-02-20-Are stocks predictable?</a></p>
<p>Introduction: We’d like to be able to predict stock market. That seems like a nice way of making money. We’ll address the fundamental issue: can stocks be predicted in the short term, that is a few days ahead?
   
There’s a technique that seeks to answer the question of predictability. It’s called  Forecastable Component Analysis .  Based on a new forecastability measure, ForeCA  finds an optimal transformation to separate a multivariate time series into a forecastable and an orthogonal white noise space.  The author, Georg M. Goerg*, implemented it in R package  ForeCA .
 
It might be useful in two ways:
  
 It can tell you how forecastable time series is. 
 Given a multivariate time series, let’s say a portfolio of stocks, it can find forecastable components. 
  
The idea in the second point is similiar to PCA - ForeCA is a linear dimensionality reduction technique. The main difference is that the method explicitly addresses forecastability. It does so by considering an interplay between time and</p><p>2 0.23800689 <a title="53-lda-2" href="../fast_ml-2013/fast_ml-2013-05-01-Deep_learning_made_easy.html">27 fast ml-2013-05-01-Deep learning made easy</a></p>
<p>Introduction: As usual, there’s an interesting competition at Kaggle: The Black Box. It’s connected to ICML 2013 Workshop on Challenges in Representation Learning, held by the deep learning guys from Montreal.
 
There are a couple benchmarks for this competition and the best one is unusually hard to beat 1  - only less than a fourth of those taking part managed to do so. We’re among them. Here’s how.
   
The key ingredient in our success is a recently developed secret Stanford technology for deep unsupervised learning:  sparse filtering  by  Jiquan Ngiam  et al. Actually, it’s not secret. It’s  available at Github , and has one or two very appealling properties. Let us explain.
 
The main idea of deep unsupervised learning, as we understand it, is feature extraction. One of the most common applications is in  multimedia. The reason for that is that multimedia tasks, for example object recognition, are easy for humans, but difficult for computers 2 .
 
Geoff Hinton from Toronto talks about  two ends</p><p>3 0.23676945 <a title="53-lda-3" href="../fast_ml-2012/fast_ml-2012-12-27-Spearmint_with_a_random_forest.html">13 fast ml-2012-12-27-Spearmint with a random forest</a></p>
<p>Introduction: Now that we have  Spearmint basics  nailed, we’ll try tuning a random forest, and specifically two hyperparams: a number of trees ( ntrees ) and a number of candidate features at each split ( mtry ). Here’s  some code .
 
We’re going to use a red  wine quality  dataset. It has about 1600 examples and our goal will be to predict a rating for a wine given all the other properties.   This is a regression* task, as ratings are in (0,10) range.
 
We will split the data 80/10/10 into train, validation and test set, and use the first two to establish optimal hyperparams and then predict on the test set. As an error measure we will use RMSE.
 
At first, we will try  ntrees  between 10 and 200 and  mtry  between 3 and 11 (there’s eleven features total, so that’s the upper bound). Here are the results of two Spearmint runs with 71 and 95 tries respectively. Colors denote a validation error value:
  
  green : RMSE < 0.57 
  blue : RMSE < 0.58 
  black : RMSE >= 0.58 
  
Turns out that some diffe</p><p>4 0.23195238 <a title="53-lda-4" href="../fast_ml-2013/fast_ml-2013-02-07-The_secret_of_the_big_guys.html">19 fast ml-2013-02-07-The secret of the big guys</a></p>
<p>Introduction: Are you interested in linear models, or K-means clustering? Probably not much. These are very basic techniques with fancier alternatives. But here’s the bomb: when you combine those two methods for supervised learning, you can get better results than from a random forest. And maybe even faster.
   
We have already written about  Vowpal Wabbit , a fast linear learner from Yahoo/Microsoft.  Google’s response (or at least, a Google’s guy response) seems to be  Sofia-ML . The software consists of two parts: a linear learner and K-means clustering. We found Sofia a while ago and wondered about K-means: who needs K-means?
 
Here’s a clue:
  
This package can be used for learning cluster centers (…) and for mapping a given data set onto a new feature space based on the learned cluster centers.
  
Our eyes only opened when we read a certain paper, namely  An Analysis of Single-Layer Networks in Unsupervised Feature Learning  ( PDF ). The paper, by  Coates , Lee  and Ng, is about object recogni</p><p>5 0.22883323 <a title="53-lda-5" href="../fast_ml-2013/fast_ml-2013-12-28-Regularizing_neural_networks_with_dropout_and_with_DropConnect.html">48 fast ml-2013-12-28-Regularizing neural networks with dropout and with DropConnect</a></p>
<p>Introduction: We continue with CIFAR-10-based competition at Kaggle to get to know DropConnect. It’s supposed to be an improvement over dropout. And dropout is certainly one of the bigger steps forward in neural network development. Is DropConnect really better than dropout?
   
 TL;DR  DropConnect seems to offer results similiar to dropout. State of the art scores reported in the paper come from model ensembling.
  Dropout  
 Dropout , by Hinton et al., is perhaps a biggest invention in the field of neural networks in recent years. It adresses the main problem in machine learning, that is overfitting. It does so by “dropping out” some unit activations in a given layer, that is setting them to zero.  Thus it prevents co-adaptation of units and can also be seen as a method of ensembling many networks sharing the same weights. For each training example a different set of units to drop is randomly chosen.
 
The idea has a  biological inspiration . When a child is conceived, it receives half its genes f</p><p>6 0.22873184 <a title="53-lda-6" href="../fast_ml-2012/fast_ml-2012-10-25-So_you_want_to_work_for_Facebook.html">9 fast ml-2012-10-25-So you want to work for Facebook</a></p>
<p>7 0.22707918 <a title="53-lda-7" href="../fast_ml-2013/fast_ml-2013-01-17-A_very_fast_denoising_autoencoder.html">18 fast ml-2013-01-17-A very fast denoising autoencoder</a></p>
<p>8 0.22668849 <a title="53-lda-8" href="../fast_ml-2012/fast_ml-2012-12-21-Tuning_hyperparams_automatically_with_Spearmint.html">12 fast ml-2012-12-21-Tuning hyperparams automatically with Spearmint</a></p>
<p>9 0.22639932 <a title="53-lda-9" href="../fast_ml-2013/fast_ml-2013-01-04-Madelon%3A_Spearmint%27s_revenge.html">14 fast ml-2013-01-04-Madelon: Spearmint's revenge</a></p>
<p>10 0.22503479 <a title="53-lda-10" href="../fast_ml-2012/fast_ml-2012-08-09-What_you_wanted_to_know_about_Mean_Average_Precision.html">1 fast ml-2012-08-09-What you wanted to know about Mean Average Precision</a></p>
<p>11 0.22422011 <a title="53-lda-11" href="../fast_ml-2013/fast_ml-2013-01-14-Feature_selection_in_practice.html">17 fast ml-2013-01-14-Feature selection in practice</a></p>
<p>12 0.2227494 <a title="53-lda-12" href="../fast_ml-2013/fast_ml-2013-11-02-Maxing_out_the_digits.html">43 fast ml-2013-11-02-Maxing out the digits</a></p>
<p>13 0.2215203 <a title="53-lda-13" href="../fast_ml-2013/fast_ml-2013-10-06-Pylearn2_in_practice.html">40 fast ml-2013-10-06-Pylearn2 in practice</a></p>
<p>14 0.21875213 <a title="53-lda-14" href="../fast_ml-2012/fast_ml-2012-10-05-Predicting_closed_questions_on_Stack_Overflow.html">7 fast ml-2012-10-05-Predicting closed questions on Stack Overflow</a></p>
<p>15 0.21688375 <a title="53-lda-15" href="../fast_ml-2013/fast_ml-2013-03-18-Large_scale_L1_feature_selection_with_Vowpal_Wabbit.html">23 fast ml-2013-03-18-Large scale L1 feature selection with Vowpal Wabbit</a></p>
<p>16 0.21524212 <a title="53-lda-16" href="../fast_ml-2013/fast_ml-2013-02-18-Predicting_advertised_salaries.html">20 fast ml-2013-02-18-Predicting advertised salaries</a></p>
<p>17 0.21498165 <a title="53-lda-17" href="../fast_ml-2013/fast_ml-2013-04-10-Gender_discrimination.html">25 fast ml-2013-04-10-Gender discrimination</a></p>
<p>18 0.21233919 <a title="53-lda-18" href="../fast_ml-2014/fast_ml-2014-03-20-Good_representations%2C_distance%2C_metric_learning_and_supervised_dimensionality_reduction.html">55 fast ml-2014-03-20-Good representations, distance, metric learning and supervised dimensionality reduction</a></p>
<p>19 0.20861699 <a title="53-lda-19" href="../fast_ml-2013/fast_ml-2013-11-27-Object_recognition_in_images_with_cuda-convnet.html">45 fast ml-2013-11-27-Object recognition in images with cuda-convnet</a></p>
<p>20 0.20687962 <a title="53-lda-20" href="../fast_ml-2013/fast_ml-2013-08-12-Accelerometer_Biometric_Competition.html">35 fast ml-2013-08-12-Accelerometer Biometric Competition</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
