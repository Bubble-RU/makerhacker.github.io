<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>51 fast ml-2014-01-25-Why IPy: reasons for using IPython interactively</title>
</head>

<body>
<p><a title="fast_ml" href="../fast_ml_home.html">fast_ml</a> <a title="fast_ml-2014" href="../home/fast_ml-2014_home.html">fast_ml-2014</a> <a title="fast_ml-2014-51" href="#">fast_ml-2014-51</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>51 fast ml-2014-01-25-Why IPy: reasons for using IPython interactively</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="fast_ml-2014-51-html" href="http://fastml.com//why-ipy-reasons-for-using-ipython-interactively/">html</a></p><p>Introduction: IPython  is known for the notebooks. But the first thing they list on their homepage is a “powerful interactive shell”. And that’s true - if you use Python interactively, you’ll dig IPython.
   
Here are a few features that make a difference for us:
  
 
command line history and autocomplete 
  These are basic shell features you come to expect and rely upon after a first contact with a Unix shell. The standard Python interpreter feels somewhat impoverished without them.
 
 
 %paste  
  Normally when you paste indented code, it won’t work. You can fix it with this magic command.
 
 
other  %magic  commands 
  Actually, you can skip the  % :  paste ,  run some_script.py ,  pwd ,  cd some_dir 
 
 
shell commands with  !  
  For example  !ls -las . Alas, aliases from your system shell don’t work.
 
 
looks good 
  Red and green add a nice touch, especially in a window with dark background. 
 
  
 
 If you have a light background, use  %colors LightBG  
 
Share your favourite features</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 But the first thing they list on their homepage is a “powerful interactive shell”. [sent-2, score-0.362]
</p><p>2 And that’s true - if you use Python interactively, you’ll dig IPython. [sent-3, score-0.198]
</p><p>3 Here are a few features that make a difference for us:      command line history and autocomplete    These are basic shell features you come to expect and rely upon after a first contact with a Unix shell. [sent-4, score-1.395]
</p><p>4 The standard Python interpreter feels somewhat impoverished without them. [sent-5, score-0.292]
</p><p>5 %paste     Normally when you paste indented code, it won’t work. [sent-6, score-0.335]
</p><p>6 other  %magic  commands    Actually, you can skip the  % :  paste ,  run some_script. [sent-8, score-0.63]
</p><p>7 py ,  pwd ,  cd some_dir      shell commands with  ! [sent-9, score-0.809]
</p><p>8 looks good    Red and green add a nice touch, especially in a window with dark background. [sent-13, score-0.602]
</p><p>9 If you have a light background, use  %colors LightBG     Share your favourite features in the comments. [sent-14, score-0.182]
</p><p>10 For more, see  Using IPython for interactive work . [sent-15, score-0.253]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('shell', 0.506), ('paste', 0.335), ('ipython', 0.304), ('magic', 0.304), ('interactive', 0.253), ('commands', 0.202), ('dig', 0.127), ('ls', 0.127), ('normally', 0.127), ('background', 0.127), ('dark', 0.127), ('touch', 0.127), ('window', 0.112), ('upon', 0.112), ('favourite', 0.112), ('share', 0.112), ('history', 0.112), ('cd', 0.101), ('colors', 0.101), ('green', 0.101), ('feels', 0.101), ('system', 0.101), ('powerful', 0.093), ('skip', 0.093), ('rely', 0.093), ('red', 0.086), ('standard', 0.08), ('python', 0.076), ('basic', 0.075), ('unix', 0.075), ('true', 0.071), ('expect', 0.071), ('somewhat', 0.071), ('features', 0.07), ('list', 0.067), ('looks', 0.067), ('especially', 0.067), ('nice', 0.064), ('command', 0.064), ('actually', 0.064), ('add', 0.064), ('known', 0.061), ('won', 0.055), ('difference', 0.052), ('come', 0.05), ('first', 0.042), ('line', 0.04), ('without', 0.04), ('make', 0.038), ('us', 0.037)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="51-tfidf-1" href="../fast_ml-2014/fast_ml-2014-01-25-Why_IPy%3A_reasons_for_using_IPython_interactively.html">51 fast ml-2014-01-25-Why IPy: reasons for using IPython interactively</a></p>
<p>Introduction: IPython  is known for the notebooks. But the first thing they list on their homepage is a “powerful interactive shell”. And that’s true - if you use Python interactively, you’ll dig IPython.
   
Here are a few features that make a difference for us:
  
 
command line history and autocomplete 
  These are basic shell features you come to expect and rely upon after a first contact with a Unix shell. The standard Python interpreter feels somewhat impoverished without them.
 
 
 %paste  
  Normally when you paste indented code, it won’t work. You can fix it with this magic command.
 
 
other  %magic  commands 
  Actually, you can skip the  % :  paste ,  run some_script.py ,  pwd ,  cd some_dir 
 
 
shell commands with  !  
  For example  !ls -las . Alas, aliases from your system shell don’t work.
 
 
looks good 
  Red and green add a nice touch, especially in a window with dark background. 
 
  
 
 If you have a light background, use  %colors LightBG  
 
Share your favourite features</p><p>2 0.072947845 <a title="51-tfidf-2" href="../fast_ml-2014/fast_ml-2014-03-31-If_you_use_R%2C_you_may_want_RStudio.html">56 fast ml-2014-03-31-If you use R, you may want RStudio</a></p>
<p>Introduction: RStudio  is an IDE for R. It gives the language a bit of a slickness factor it so badly needs. The nice thing about the software, beside good looks, is that it integrates console, help pages, plots and editor (if you want it) in one place.
   
For example, instead of switching for help to a web browser, you have it right next to the console. The same thing with plots: in place of the usual overlapping windows, all plots go to one pane where you can navigate back and forth between them with arrows. You can save a plot as an image or as a PDF. While saving as image you are presented with a preview and you get to choose the image format and size. That’s the kind of detail that shows how RStudio makes working with R better.
 
You can have up to four panes on the screen:
  
 console 
 source / data 
 plots / help / files 
 history and workspace 
  
 
 
Arrange them in any way you like.
 
The source pane has a  run  button, so you can execute the current line of code in the console, or selec</p><p>3 0.054361328 <a title="51-tfidf-3" href="../fast_ml-2012/fast_ml-2012-09-01-Running_Unix_apps_on_Windows.html">3 fast ml-2012-09-01-Running Unix apps on Windows</a></p>
<p>Introduction: When it comes to machine learning, most software seems to be in either Python, Matlab or R. Plus native apps, that is, compiled C/C++. These are fastest. Most of them is written for Unix environments, for example Linux or MacOS. So how do you run them on your computer if you have Windows installed?
   
Back in the day, you re-partitioned your hard drive and installed Linux alongside Windows. The added thrill was, if something went wrong, your computer wouldn’t boot.
 
Now it’s easier. You just run Linux inside Windows, using what’s called a virtual machine. You need virtualization software and a machine image to do so.
 
Most popular software seems to be VMware. There is also VirtualBox - it is able to run VMware images. We have experience with WMware mostly, so this is what we’ll refer to.  VMware player  is free to download and use.
 
There are also  many images  available, of various flavours of Linux and other operating systems. In fact, you can run Windows inside Linux if you wish</p><p>4 0.048278525 <a title="51-tfidf-4" href="../fast_ml-2014/fast_ml-2014-04-30-Converting_categorical_data_into_numbers_with_Pandas_and_Scikit-learn.html">60 fast ml-2014-04-30-Converting categorical data into numbers with Pandas and Scikit-learn</a></p>
<p>Introduction: Many machine learning tools will only accept numbers as input. This may be a problem if you want to use such tool but your data includes categorical features. To represent them as numbers typically one converts each categorical feature using “one-hot encoding”, that is from a value like “BMW” or “Mercedes” to a vector of zeros and one  1 .
 
This  functionality  is available in some software libraries. We load data using Pandas, then convert categorical columns with  DictVectorizer  from scikit-learn.
   
 Pandas  is a popular Python library inspired by data frames in R. It allows easier manipulation of tabular numeric and non-numeric data. Downsides: not very intuitive, somewhat steep learning curve. For any questions you may have, Google + StackOverflow combo works well as a source of answers.
 
 UPDATE:  Turns out that Pandas has  get_dummies()  function which does what we’re after. More on this in a while.
 
We’ll use Pandas to load the data, do some cleaning and send it to Scikit-</p><p>5 0.044442572 <a title="51-tfidf-5" href="../fast_ml-2013/fast_ml-2013-11-18-CUDA_on_a_Linux_laptop.html">44 fast ml-2013-11-18-CUDA on a Linux laptop</a></p>
<p>Introduction: After testing  CUDA on a desktop , we now switch to a Linux laptop with 64-bit Xubuntu. Getting CUDA to work is harder here. Will the effort be worth the results?
   
If you have a laptop with a Nvidia card, the thing probably uses it for 3D graphics and Intel’s built-in unit for everything else. This technology is known as Optimus and it happens to make things anything but easy for running CUDA on Linux.
 
The problem is with GPU drivers, specifically between Linux being open-source and Nvidia drivers being not. This strained relation at one time prompted Linus Torvalds to  give Nvidia a finger  with great passion.
  Installing GPU drivers  
Here’s a solution for the driver problem. You need a package called  bumblebee . It makes a Nvidia card accessible to your apps. To install drivers and  bumblebee , try something along these lines:
 
 sudo apt-get install nvidia-current-updates  
 sudo apt-get install bumblebee 
 
Note that you don’t need drivers that come with a specific CUDA rel</p><p>6 0.043741629 <a title="51-tfidf-6" href="../fast_ml-2012/fast_ml-2012-08-27-Kaggle_job_recommendation_challenge.html">2 fast ml-2012-08-27-Kaggle job recommendation challenge</a></p>
<p>7 0.043253936 <a title="51-tfidf-7" href="../fast_ml-2013/fast_ml-2013-01-14-Feature_selection_in_practice.html">17 fast ml-2013-01-14-Feature selection in practice</a></p>
<p>8 0.040348269 <a title="51-tfidf-8" href="../fast_ml-2013/fast_ml-2013-01-04-Madelon%3A_Spearmint%27s_revenge.html">14 fast ml-2013-01-04-Madelon: Spearmint's revenge</a></p>
<p>9 0.033307984 <a title="51-tfidf-9" href="../fast_ml-2013/fast_ml-2013-01-17-A_very_fast_denoising_autoencoder.html">18 fast ml-2013-01-17-A very fast denoising autoencoder</a></p>
<p>10 0.031810407 <a title="51-tfidf-10" href="../fast_ml-2013/fast_ml-2013-06-19-Go_non-linear_with_Vowpal_Wabbit.html">31 fast ml-2013-06-19-Go non-linear with Vowpal Wabbit</a></p>
<p>11 0.03153703 <a title="51-tfidf-11" href="../fast_ml-2013/fast_ml-2013-07-09-Introducing_phraug.html">33 fast ml-2013-07-09-Introducing phraug</a></p>
<p>12 0.03013782 <a title="51-tfidf-12" href="../fast_ml-2013/fast_ml-2013-03-18-Large_scale_L1_feature_selection_with_Vowpal_Wabbit.html">23 fast ml-2013-03-18-Large scale L1 feature selection with Vowpal Wabbit</a></p>
<p>13 0.02985394 <a title="51-tfidf-13" href="../fast_ml-2014/fast_ml-2014-02-02-Yesterday_a_kaggler%2C_today_a_Kaggle_master%3A_a_wrap-up_of_the_cats_and_dogs_competition.html">52 fast ml-2014-02-02-Yesterday a kaggler, today a Kaggle master: a wrap-up of the cats and dogs competition</a></p>
<p>14 0.029344551 <a title="51-tfidf-14" href="../fast_ml-2013/fast_ml-2013-04-17-Regression_as_classification.html">26 fast ml-2013-04-17-Regression as classification</a></p>
<p>15 0.02873997 <a title="51-tfidf-15" href="../fast_ml-2013/fast_ml-2013-06-01-Amazon_aspires_to_automate_access_control.html">30 fast ml-2013-06-01-Amazon aspires to automate access control</a></p>
<p>16 0.028113099 <a title="51-tfidf-16" href="../fast_ml-2013/fast_ml-2013-11-27-Object_recognition_in_images_with_cuda-convnet.html">45 fast ml-2013-11-27-Object recognition in images with cuda-convnet</a></p>
<p>17 0.027860887 <a title="51-tfidf-17" href="../fast_ml-2012/fast_ml-2012-12-27-Spearmint_with_a_random_forest.html">13 fast ml-2012-12-27-Spearmint with a random forest</a></p>
<p>18 0.026300834 <a title="51-tfidf-18" href="../fast_ml-2013/fast_ml-2013-07-05-Processing_large_files%2C_line_by_line.html">32 fast ml-2013-07-05-Processing large files, line by line</a></p>
<p>19 0.023931803 <a title="51-tfidf-19" href="../fast_ml-2013/fast_ml-2013-12-15-A-B_testing_with_bayesian_bandits_in_Google_Analytics.html">47 fast ml-2013-12-15-A-B testing with bayesian bandits in Google Analytics</a></p>
<p>20 0.023812527 <a title="51-tfidf-20" href="../fast_ml-2013/fast_ml-2013-02-18-Predicting_advertised_salaries.html">20 fast ml-2013-02-18-Predicting advertised salaries</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/fast_ml_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.1), (1, -0.022), (2, -0.022), (3, 0.005), (4, -0.111), (5, 0.029), (6, -0.035), (7, 0.043), (8, 0.162), (9, -0.007), (10, 0.32), (11, -0.085), (12, 0.09), (13, 0.285), (14, -0.209), (15, 0.171), (16, 0.145), (17, -0.046), (18, -0.0), (19, 0.187), (20, 0.172), (21, 0.224), (22, -0.246), (23, -0.064), (24, -0.159), (25, 0.006), (26, -0.192), (27, 0.04), (28, -0.168), (29, 0.335), (30, -0.071), (31, 0.014), (32, -0.222), (33, -0.279), (34, -0.035), (35, 0.116), (36, -0.031), (37, -0.169), (38, 0.055), (39, -0.257), (40, -0.075), (41, -0.068), (42, 0.004), (43, -0.011), (44, 0.015), (45, -0.014), (46, 0.013), (47, -0.039), (48, 0.056), (49, 0.013)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99206573 <a title="51-lsi-1" href="../fast_ml-2014/fast_ml-2014-01-25-Why_IPy%3A_reasons_for_using_IPython_interactively.html">51 fast ml-2014-01-25-Why IPy: reasons for using IPython interactively</a></p>
<p>Introduction: IPython  is known for the notebooks. But the first thing they list on their homepage is a “powerful interactive shell”. And that’s true - if you use Python interactively, you’ll dig IPython.
   
Here are a few features that make a difference for us:
  
 
command line history and autocomplete 
  These are basic shell features you come to expect and rely upon after a first contact with a Unix shell. The standard Python interpreter feels somewhat impoverished without them.
 
 
 %paste  
  Normally when you paste indented code, it won’t work. You can fix it with this magic command.
 
 
other  %magic  commands 
  Actually, you can skip the  % :  paste ,  run some_script.py ,  pwd ,  cd some_dir 
 
 
shell commands with  !  
  For example  !ls -las . Alas, aliases from your system shell don’t work.
 
 
looks good 
  Red and green add a nice touch, especially in a window with dark background. 
 
  
 
 If you have a light background, use  %colors LightBG  
 
Share your favourite features</p><p>2 0.09380582 <a title="51-lsi-2" href="../fast_ml-2014/fast_ml-2014-03-31-If_you_use_R%2C_you_may_want_RStudio.html">56 fast ml-2014-03-31-If you use R, you may want RStudio</a></p>
<p>Introduction: RStudio  is an IDE for R. It gives the language a bit of a slickness factor it so badly needs. The nice thing about the software, beside good looks, is that it integrates console, help pages, plots and editor (if you want it) in one place.
   
For example, instead of switching for help to a web browser, you have it right next to the console. The same thing with plots: in place of the usual overlapping windows, all plots go to one pane where you can navigate back and forth between them with arrows. You can save a plot as an image or as a PDF. While saving as image you are presented with a preview and you get to choose the image format and size. That’s the kind of detail that shows how RStudio makes working with R better.
 
You can have up to four panes on the screen:
  
 console 
 source / data 
 plots / help / files 
 history and workspace 
  
 
 
Arrange them in any way you like.
 
The source pane has a  run  button, so you can execute the current line of code in the console, or selec</p><p>3 0.083607711 <a title="51-lsi-3" href="../fast_ml-2013/fast_ml-2013-06-19-Go_non-linear_with_Vowpal_Wabbit.html">31 fast ml-2013-06-19-Go non-linear with Vowpal Wabbit</a></p>
<p>Introduction: Vowpal Wabbit now supports a few modes of non-linear supervised learning. They are:
  
 a neural network with a single hidden layer 
 automatic creation of polynomial, specifically quadratic and cubic, features 
 N-grams 
  
We describe how to use them, providing examples from the Kaggle Amazon competition and for the  kin8nm  dataset.
    Neural network  
The original motivation for creating  neural network code in VW   was to win some Kaggle competitions using only vee-dub , and that goal becomes much more feasible once you have a strong non-linear learner.
 
The network seems to be a classic multi-layer perceptron with one sigmoidal hidden layer. More interestingly, it has dropout. Unfortunately, in a few tries we haven’t had much luck with the dropout.
 
Here’s an example of how to create a network with 10 hidden units:
 
 vw -d data.vw --nn 10 
  Quadratic and cubic features  
The idea of quadratic features is to create all possible combinations between original features, so that</p><p>4 0.083602384 <a title="51-lsi-4" href="../fast_ml-2013/fast_ml-2013-03-18-Large_scale_L1_feature_selection_with_Vowpal_Wabbit.html">23 fast ml-2013-03-18-Large scale L1 feature selection with Vowpal Wabbit</a></p>
<p>Introduction: The job salary prediction contest at Kaggle  offers a highly-dimensional dataset: when you convert categorical values to binary features and text columns to a bag of words, you get roughly 240k features, a number very similiar to the number of examples.
 
We present a way to select a few thousand relevant features using L1 (Lasso) regularization. A linear model seems to work just as well with those selected features as with the full set. This means we get roughly 40 times less features for a much more manageable, smaller data set.
   
  
  What you wanted to know about Lasso and Ridge  
L1 and L2 are both ways of regularization sometimes called  weight decay . Basically, we include parameter weights in a cost function. In effect, the model will try to minimize those weights by going “down the slope”. Example weights: in a linear model or in a neural network.
 
L1 is known as Lasso and L2 is known as Ridge. These names may be confusing, because a chart of Lasso looks like a ridge and a</p><p>5 0.081528775 <a title="51-lsi-5" href="../fast_ml-2013/fast_ml-2013-11-18-CUDA_on_a_Linux_laptop.html">44 fast ml-2013-11-18-CUDA on a Linux laptop</a></p>
<p>Introduction: After testing  CUDA on a desktop , we now switch to a Linux laptop with 64-bit Xubuntu. Getting CUDA to work is harder here. Will the effort be worth the results?
   
If you have a laptop with a Nvidia card, the thing probably uses it for 3D graphics and Intel’s built-in unit for everything else. This technology is known as Optimus and it happens to make things anything but easy for running CUDA on Linux.
 
The problem is with GPU drivers, specifically between Linux being open-source and Nvidia drivers being not. This strained relation at one time prompted Linus Torvalds to  give Nvidia a finger  with great passion.
  Installing GPU drivers  
Here’s a solution for the driver problem. You need a package called  bumblebee . It makes a Nvidia card accessible to your apps. To install drivers and  bumblebee , try something along these lines:
 
 sudo apt-get install nvidia-current-updates  
 sudo apt-get install bumblebee 
 
Note that you don’t need drivers that come with a specific CUDA rel</p><p>6 0.075848132 <a title="51-lsi-6" href="../fast_ml-2012/fast_ml-2012-09-01-Running_Unix_apps_on_Windows.html">3 fast ml-2012-09-01-Running Unix apps on Windows</a></p>
<p>7 0.074410282 <a title="51-lsi-7" href="../fast_ml-2014/fast_ml-2014-02-02-Yesterday_a_kaggler%2C_today_a_Kaggle_master%3A_a_wrap-up_of_the_cats_and_dogs_competition.html">52 fast ml-2014-02-02-Yesterday a kaggler, today a Kaggle master: a wrap-up of the cats and dogs competition</a></p>
<p>8 0.072075538 <a title="51-lsi-8" href="../fast_ml-2013/fast_ml-2013-01-14-Feature_selection_in_practice.html">17 fast ml-2013-01-14-Feature selection in practice</a></p>
<p>9 0.067964986 <a title="51-lsi-9" href="../fast_ml-2014/fast_ml-2014-04-30-Converting_categorical_data_into_numbers_with_Pandas_and_Scikit-learn.html">60 fast ml-2014-04-30-Converting categorical data into numbers with Pandas and Scikit-learn</a></p>
<p>10 0.06069012 <a title="51-lsi-10" href="../fast_ml-2013/fast_ml-2013-01-17-A_very_fast_denoising_autoencoder.html">18 fast ml-2013-01-17-A very fast denoising autoencoder</a></p>
<p>11 0.05906307 <a title="51-lsi-11" href="../fast_ml-2013/fast_ml-2013-01-04-Madelon%3A_Spearmint%27s_revenge.html">14 fast ml-2013-01-04-Madelon: Spearmint's revenge</a></p>
<p>12 0.058173455 <a title="51-lsi-12" href="../fast_ml-2012/fast_ml-2012-12-27-Spearmint_with_a_random_forest.html">13 fast ml-2012-12-27-Spearmint with a random forest</a></p>
<p>13 0.057974957 <a title="51-lsi-13" href="../fast_ml-2013/fast_ml-2013-04-17-Regression_as_classification.html">26 fast ml-2013-04-17-Regression as classification</a></p>
<p>14 0.056981999 <a title="51-lsi-14" href="../fast_ml-2012/fast_ml-2012-08-27-Kaggle_job_recommendation_challenge.html">2 fast ml-2012-08-27-Kaggle job recommendation challenge</a></p>
<p>15 0.056661975 <a title="51-lsi-15" href="../fast_ml-2013/fast_ml-2013-06-01-Amazon_aspires_to_automate_access_control.html">30 fast ml-2013-06-01-Amazon aspires to automate access control</a></p>
<p>16 0.056553956 <a title="51-lsi-16" href="../fast_ml-2013/fast_ml-2013-10-06-Pylearn2_in_practice.html">40 fast ml-2013-10-06-Pylearn2 in practice</a></p>
<p>17 0.055968937 <a title="51-lsi-17" href="../fast_ml-2013/fast_ml-2013-07-05-Processing_large_files%2C_line_by_line.html">32 fast ml-2013-07-05-Processing large files, line by line</a></p>
<p>18 0.053554121 <a title="51-lsi-18" href="../fast_ml-2014/fast_ml-2014-03-20-Good_representations%2C_distance%2C_metric_learning_and_supervised_dimensionality_reduction.html">55 fast ml-2014-03-20-Good representations, distance, metric learning and supervised dimensionality reduction</a></p>
<p>19 0.052223284 <a title="51-lsi-19" href="../fast_ml-2013/fast_ml-2013-07-09-Introducing_phraug.html">33 fast ml-2013-07-09-Introducing phraug</a></p>
<p>20 0.05157467 <a title="51-lsi-20" href="../fast_ml-2013/fast_ml-2013-02-07-The_secret_of_the_big_guys.html">19 fast ml-2013-02-07-The secret of the big guys</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/fast_ml_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(31, 0.051), (33, 0.64), (55, 0.05), (69, 0.101), (99, 0.018)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.92045546 <a title="51-lda-1" href="../fast_ml-2014/fast_ml-2014-01-25-Why_IPy%3A_reasons_for_using_IPython_interactively.html">51 fast ml-2014-01-25-Why IPy: reasons for using IPython interactively</a></p>
<p>Introduction: IPython  is known for the notebooks. But the first thing they list on their homepage is a “powerful interactive shell”. And that’s true - if you use Python interactively, you’ll dig IPython.
   
Here are a few features that make a difference for us:
  
 
command line history and autocomplete 
  These are basic shell features you come to expect and rely upon after a first contact with a Unix shell. The standard Python interpreter feels somewhat impoverished without them.
 
 
 %paste  
  Normally when you paste indented code, it won’t work. You can fix it with this magic command.
 
 
other  %magic  commands 
  Actually, you can skip the  % :  paste ,  run some_script.py ,  pwd ,  cd some_dir 
 
 
shell commands with  !  
  For example  !ls -las . Alas, aliases from your system shell don’t work.
 
 
looks good 
  Red and green add a nice touch, especially in a window with dark background. 
 
  
 
 If you have a light background, use  %colors LightBG  
 
Share your favourite features</p><p>2 0.16556717 <a title="51-lda-2" href="../fast_ml-2013/fast_ml-2013-05-01-Deep_learning_made_easy.html">27 fast ml-2013-05-01-Deep learning made easy</a></p>
<p>Introduction: As usual, there’s an interesting competition at Kaggle: The Black Box. It’s connected to ICML 2013 Workshop on Challenges in Representation Learning, held by the deep learning guys from Montreal.
 
There are a couple benchmarks for this competition and the best one is unusually hard to beat 1  - only less than a fourth of those taking part managed to do so. We’re among them. Here’s how.
   
The key ingredient in our success is a recently developed secret Stanford technology for deep unsupervised learning:  sparse filtering  by  Jiquan Ngiam  et al. Actually, it’s not secret. It’s  available at Github , and has one or two very appealling properties. Let us explain.
 
The main idea of deep unsupervised learning, as we understand it, is feature extraction. One of the most common applications is in  multimedia. The reason for that is that multimedia tasks, for example object recognition, are easy for humans, but difficult for computers 2 .
 
Geoff Hinton from Toronto talks about  two ends</p><p>3 0.15730035 <a title="51-lda-3" href="../fast_ml-2013/fast_ml-2013-01-04-Madelon%3A_Spearmint%27s_revenge.html">14 fast ml-2013-01-04-Madelon: Spearmint's revenge</a></p>
<p>Introduction: Little Spearmint couldn’t sleep that night.  I was so close…  - he was thinking. It seemed that he had found a better than default value for one of the random forest hyperparams, but it turned out to be false. He made a decision as he fell asleep:  Next time, I will show them! 
   
The way to do this is to use a dataset that is known to produce lower error with high  mtry  values, namely previously mentioned  Madelon  from NIPS 2003 Feature Selection Challenge. Among 500 attributes, only 20 are informative, the rest are noise. That’s the reason why high  mtry  is good here: you have to consider a lot of features to find a meaningful one.
 
The dataset consists of a train, validation and test parts, with labels being available for train and validation. We will further split the training set into our train and validation sets, and use the original validation set as a test set to evaluate final results of parameter tuning.
 
As an error measure we use  Area Under Curve , or AUC, which was</p><p>4 0.15558609 <a title="51-lda-4" href="../fast_ml-2014/fast_ml-2014-03-06-PyBrain_-_a_simple_neural_networks_library_in_Python.html">54 fast ml-2014-03-06-PyBrain - a simple neural networks library in Python</a></p>
<p>Introduction: We have already written  a few articles about Pylearn2 . Today we’ll look at PyBrain. It is another Python neural networks library, and this is where similiarites end.
   
They’re like day and night: Pylearn2 - Byzantinely complicated,  PyBrain  - simple. We attempted to train a regression model and succeeded at first take (more on this below). Try this with Pylearn2.
  
While there are a few machine learning libraries out there, PyBrain aims to be a very easy-to-use modular library that can be used by entry-level students but still offers the flexibility and algorithms for state-of-the-art research.
  
The library features classic perceptron as well as recurrent neural networks and other things, some of which, for example  Evolino , would be hard to find elsewhere.
 
On the downside, PyBrain feels unfinished, abandoned. It is no longer actively developed and the  documentation  is skimpy. There’s no modern gimmicks like dropout and rectified linear units - just good ol’ sigmoid and ta</p><p>5 0.15552555 <a title="51-lda-5" href="../fast_ml-2012/fast_ml-2012-08-09-What_you_wanted_to_know_about_Mean_Average_Precision.html">1 fast ml-2012-08-09-What you wanted to know about Mean Average Precision</a></p>
<p>Introduction: Let’s say that there are some users and some items, like movies, songs or jobs. Each user might be interested in some items. The client asks us to recommend a few items (the number is x) for each user. They will evaluate the results using mean average precision, or MAP, metric. Specifically MAP@x - this means they ask us to recommend x items for each user. So what is this MAP?
   
First, we will get M out of the way. MAP is just an average of APs, or average precision, for all users. In other words, we take the mean for Average Precision, hence Mean Average Precision. If we have 1000 users, we sum APs for each user and divide the sum by 1000. This is MAP.
 
So now, what is AP, or average precision? It may be that we don’t really need to know. But we probably need to know this:
  
 we can recommend at most x items for each user 
 it pays to submit all x recommendations, because we are not penalized for bad guesses 
 order matters, so it’s better to submit more certain recommendations fi</p><p>6 0.1550281 <a title="51-lda-6" href="../fast_ml-2012/fast_ml-2012-12-27-Spearmint_with_a_random_forest.html">13 fast ml-2012-12-27-Spearmint with a random forest</a></p>
<p>7 0.15319425 <a title="51-lda-7" href="../fast_ml-2013/fast_ml-2013-12-28-Regularizing_neural_networks_with_dropout_and_with_DropConnect.html">48 fast ml-2013-12-28-Regularizing neural networks with dropout and with DropConnect</a></p>
<p>8 0.15101495 <a title="51-lda-8" href="../fast_ml-2012/fast_ml-2012-12-21-Tuning_hyperparams_automatically_with_Spearmint.html">12 fast ml-2012-12-21-Tuning hyperparams automatically with Spearmint</a></p>
<p>9 0.14834772 <a title="51-lda-9" href="../fast_ml-2013/fast_ml-2013-07-05-Processing_large_files%2C_line_by_line.html">32 fast ml-2013-07-05-Processing large files, line by line</a></p>
<p>10 0.14798471 <a title="51-lda-10" href="../fast_ml-2013/fast_ml-2013-02-18-Predicting_advertised_salaries.html">20 fast ml-2013-02-18-Predicting advertised salaries</a></p>
<p>11 0.14663373 <a title="51-lda-11" href="../fast_ml-2013/fast_ml-2013-11-02-Maxing_out_the_digits.html">43 fast ml-2013-11-02-Maxing out the digits</a></p>
<p>12 0.14427747 <a title="51-lda-12" href="../fast_ml-2013/fast_ml-2013-01-17-A_very_fast_denoising_autoencoder.html">18 fast ml-2013-01-17-A very fast denoising autoencoder</a></p>
<p>13 0.14396451 <a title="51-lda-13" href="../fast_ml-2013/fast_ml-2013-01-14-Feature_selection_in_practice.html">17 fast ml-2013-01-14-Feature selection in practice</a></p>
<p>14 0.14335753 <a title="51-lda-14" href="../fast_ml-2013/fast_ml-2013-10-06-Pylearn2_in_practice.html">40 fast ml-2013-10-06-Pylearn2 in practice</a></p>
<p>15 0.14333551 <a title="51-lda-15" href="../fast_ml-2013/fast_ml-2013-02-07-The_secret_of_the_big_guys.html">19 fast ml-2013-02-07-The secret of the big guys</a></p>
<p>16 0.13804826 <a title="51-lda-16" href="../fast_ml-2013/fast_ml-2013-08-12-Accelerometer_Biometric_Competition.html">35 fast ml-2013-08-12-Accelerometer Biometric Competition</a></p>
<p>17 0.13803239 <a title="51-lda-17" href="../fast_ml-2013/fast_ml-2013-04-10-Gender_discrimination.html">25 fast ml-2013-04-10-Gender discrimination</a></p>
<p>18 0.13718739 <a title="51-lda-18" href="../fast_ml-2013/fast_ml-2013-03-18-Large_scale_L1_feature_selection_with_Vowpal_Wabbit.html">23 fast ml-2013-03-18-Large scale L1 feature selection with Vowpal Wabbit</a></p>
<p>19 0.13628341 <a title="51-lda-19" href="../fast_ml-2013/fast_ml-2013-05-25-More_on_sparse_filtering_and_the_Black_Box_competition.html">29 fast ml-2013-05-25-More on sparse filtering and the Black Box competition</a></p>
<p>20 0.13387161 <a title="51-lda-20" href="../fast_ml-2012/fast_ml-2012-10-25-So_you_want_to_work_for_Facebook.html">9 fast ml-2012-10-25-So you want to work for Facebook</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
