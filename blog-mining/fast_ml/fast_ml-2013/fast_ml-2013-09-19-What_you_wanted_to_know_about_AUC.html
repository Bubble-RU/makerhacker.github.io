<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>39 fast ml-2013-09-19-What you wanted to know about AUC</title>
</head>

<body>
<p><a title="fast_ml" href="../fast_ml_home.html">fast_ml</a> <a title="fast_ml-2013" href="../home/fast_ml-2013_home.html">fast_ml-2013</a> <a title="fast_ml-2013-39" href="#">fast_ml-2013-39</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>39 fast ml-2013-09-19-What you wanted to know about AUC</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="fast_ml-2013-39-html" href="http://fastml.com//what-you-wanted-to-know-about-auc/">html</a></p><p>Introduction: AUC, or Area Under Curve, is a metric for binary classification. It’s probably the second most popular one, after accuracy. Unfortunately, it’s nowhere near as intuitive. That is, until you have read this article.
   
Accuracy deals with ones and zeros, meaning you either got the class label right or you didn’t. But many classifiers are able to quantify their uncertainty about the answer by outputting a probability value. To compute accuracy from probabilities you need a threshold to decide when zero turns into one. The most natural threshold is of course 0.5.
 
Let’s suppose you have a quirky classifier. It is able to get all the answers right, but it outputs 0.7 for negative examples and 0.9 for positive examples. Clearly, a threshold of 0.5 won’t get you far here. But 0.8 would be just perfect.
 
That’s the whole point of using AUC - it considers all possible thresholds. Various thresholds result in different true positive/false positive rates. As you decrease the threshold, you get</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Accuracy deals with ones and zeros, meaning you either got the class label right or you didn’t. [sent-5, score-0.053]
</p><p>2 To compute accuracy from probabilities you need a threshold to decide when zero turns into one. [sent-7, score-0.543]
</p><p>3 It is able to get all the answers right, but it outputs 0. [sent-11, score-0.12]
</p><p>4 Various thresholds result in different true positive/false positive rates. [sent-19, score-0.244]
</p><p>5 As you decrease the threshold, you get more true positives, but also more false positives. [sent-20, score-0.264]
</p><p>6 The relation between them can be plotted:       Image credit: Wikipedia    From a random classifier you can expect as many true positives as false positives. [sent-21, score-0.657]
</p><p>7 Computing AUC   You compute AUC from a vector of predictions and a vector of true labels. [sent-27, score-0.392]
</p><p>8 auc    In Matlab (but not Octave), you have  perfcurve  which can even return the best threshold, or  optimal operating point    In R, one of the packages that provide ROC AUC is  caTools      Finer points   Let’s get more precise with naming. [sent-31, score-0.852]
</p><p>9 ROC stands for Receiver Operating Characteristic, a term from signal theory. [sent-33, score-0.053]
</p><p>10 Sometimes you may encounter references to ROC or ROC curve - think AUC then. [sent-34, score-0.168]
</p><p>11 But wait - Gael Varoquaux points out that    AUC is not always area under the curve of a ROC curve. [sent-35, score-0.441]
</p><p>12 In the situation where you have imbalanced classes, it is often more useful to report AUC for a precision-recall curve. [sent-36, score-0.173]
</p><p>13 ROC AUC is insensitive to imbalanced classes, however. [sent-37, score-0.127]
</p><p>14 9 );  % mostly negatives p = zeros(1000,1);               % always predicting zero [X,Y,T,AUC] = perfcurve(y,p,1)   % 0. [sent-39, score-0.198]
</p><p>15 In case your class labels are mostly negative or mostly positive, a classifier that always outputs 0 or 1, respectively, will achieve high accuracy. [sent-41, score-0.651]
</p><p>16 The same goes to random predictions:    y = real( rand(1000,1) > 0. [sent-44, score-0.056]
</p><p>17 1 );  % mostly positives p = rand(1000,1);                % random predictions [X,Y,T,AUC] = perfcurve(y,p,1)   % 0. [sent-45, score-0.17]
</p><p>18 9 );  % mostly negatives p = rand(1000,1);                % random predictions [X,Y,T,AUC] = perfcurve(y,p,1)   % 0. [sent-47, score-0.17]
</p><p>19 4883 plot(X,Y)        And zero zero seven, there’s one more thing you need to know about AUC: it doesn’t care about absolute values, it only cares about ranking. [sent-48, score-0.166]
</p><p>20 This means that your quirky classifier might as well output 700 for negatives and 900 for positives and it would be perfectly OK for AUC, even when you’re supposed to provide probabilities. [sent-49, score-0.603]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('roc', 0.443), ('auc', 0.403), ('rand', 0.317), ('perfcurve', 0.253), ('threshold', 0.232), ('positives', 0.211), ('curve', 0.168), ('area', 0.139), ('negatives', 0.139), ('imbalanced', 0.127), ('quirky', 0.127), ('positive', 0.126), ('classifier', 0.126), ('true', 0.118), ('mostly', 0.114), ('operating', 0.093), ('false', 0.093), ('negative', 0.093), ('always', 0.084), ('zero', 0.083), ('matlab', 0.08), ('predictions', 0.073), ('real', 0.072), ('compute', 0.067), ('probabilities', 0.067), ('vector', 0.067), ('outputs', 0.067), ('zeros', 0.063), ('classes', 0.056), ('random', 0.056), ('class', 0.053), ('hamner', 0.053), ('perfect', 0.053), ('octave', 0.053), ('stands', 0.053), ('gael', 0.053), ('varoquaux', 0.053), ('answers', 0.053), ('decrease', 0.053), ('precise', 0.053), ('relation', 0.053), ('points', 0.05), ('accuracy', 0.048), ('ben', 0.046), ('respectively', 0.046), ('versions', 0.046), ('decide', 0.046), ('classifiers', 0.046), ('favourite', 0.046), ('report', 0.046)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000002 <a title="39-tfidf-1" href="../fast_ml-2013/fast_ml-2013-09-19-What_you_wanted_to_know_about_AUC.html">39 fast ml-2013-09-19-What you wanted to know about AUC</a></p>
<p>Introduction: AUC, or Area Under Curve, is a metric for binary classification. It’s probably the second most popular one, after accuracy. Unfortunately, it’s nowhere near as intuitive. That is, until you have read this article.
   
Accuracy deals with ones and zeros, meaning you either got the class label right or you didn’t. But many classifiers are able to quantify their uncertainty about the answer by outputting a probability value. To compute accuracy from probabilities you need a threshold to decide when zero turns into one. The most natural threshold is of course 0.5.
 
Let’s suppose you have a quirky classifier. It is able to get all the answers right, but it outputs 0.7 for negative examples and 0.9 for positive examples. Clearly, a threshold of 0.5 won’t get you far here. But 0.8 would be just perfect.
 
That’s the whole point of using AUC - it considers all possible thresholds. Various thresholds result in different true positive/false positive rates. As you decrease the threshold, you get</p><p>2 0.13120295 <a title="39-tfidf-2" href="../fast_ml-2013/fast_ml-2013-01-14-Feature_selection_in_practice.html">17 fast ml-2013-01-14-Feature selection in practice</a></p>
<p>Introduction: Lately we’ve been working with the Madelon dataset. It was originally prepared for a feature selection challenge, so while we’re at it, let’s select some features. Madelon has 500 attributes, 20 of which are real, the rest being noise. Hence the ideal scenario would be to select just those 20 features.
   
Fortunately we know just the right software for this task. It’s called  mRMR , for  minimum Redundancy Maximum Relevance , and is available in C and Matlab versions for various platforms. mRMR expects a CSV file with labels in the first column and feature names in the first row. So the game plan is:
  
 combine training and validation sets into a format expected by mRMR 
 run selection 
 filter the original datasets, discarding all features but the selected ones 
 evaluate the results on the validation set 
 if all goes well, prepare and submit files for  the competition  
  
We’ll use  R scripts  for all the steps but feature selection. Now a few words about mRMR. It will show you p</p><p>3 0.10197529 <a title="39-tfidf-3" href="../fast_ml-2013/fast_ml-2013-01-04-Madelon%3A_Spearmint%27s_revenge.html">14 fast ml-2013-01-04-Madelon: Spearmint's revenge</a></p>
<p>Introduction: Little Spearmint couldn’t sleep that night.  I was so close…  - he was thinking. It seemed that he had found a better than default value for one of the random forest hyperparams, but it turned out to be false. He made a decision as he fell asleep:  Next time, I will show them! 
   
The way to do this is to use a dataset that is known to produce lower error with high  mtry  values, namely previously mentioned  Madelon  from NIPS 2003 Feature Selection Challenge. Among 500 attributes, only 20 are informative, the rest are noise. That’s the reason why high  mtry  is good here: you have to consider a lot of features to find a meaningful one.
 
The dataset consists of a train, validation and test parts, with labels being available for train and validation. We will further split the training set into our train and validation sets, and use the original validation set as a test set to evaluate final results of parameter tuning.
 
As an error measure we use  Area Under Curve , or AUC, which was</p><p>4 0.084561668 <a title="39-tfidf-4" href="../fast_ml-2013/fast_ml-2013-08-12-Accelerometer_Biometric_Competition.html">35 fast ml-2013-08-12-Accelerometer Biometric Competition</a></p>
<p>Introduction: Can you recognize users of mobile devices from accelerometer data? It’s a rather non-standard problem (we’re dealing with time series here) and an interesting one. So we wrote some code and ran EC2 and then wrote more and ran EC2 again. After much computation we had our predictions, submitted them and achieved AUC = 0.83. Yay! But there’s a twist to this story.
   
An interesting thing happened when we mistyped a command. Instead of computing AUC from a validation test set and raw predictions, we computed it using a training set:
 
 auc.py train_v.vw r.txt 
 
And got 0.86.
 
Now, the training file has a very different number of lines from the test file, but our script only cares about how many lines are in a predictions file, so as long as the other file has at least that many, it’s OK.
 
Turns out you can use this set or that set and get pretty much the same good result with each. That got us thinking. We computed a mean of predictions for each device. Here’s the plot:
 
 
 
And here’</p><p>5 0.081505194 <a title="39-tfidf-5" href="../fast_ml-2013/fast_ml-2013-06-01-Amazon_aspires_to_automate_access_control.html">30 fast ml-2013-06-01-Amazon aspires to automate access control</a></p>
<p>Introduction: This is about  Amazon access control challenge  at Kaggle. Either we’re getting smarter, or the competition is easy. Or maybe both. You can beat the benchmark quite easily and with AUC of 0.875 you’d be comfortably in the top twenty percent at the moment. We scored fourth in our first attempt - the model was quick to develop and back then there were fewer competitors.
   
 
 
Traditionally we use  Vowpal Wabbit . Just simple binary classification with the logistic loss function and 10 passes over the data.
 
It seems to work pretty well even though the classes are very unbalanced: there’s only a handful of negatives when compared to positives. Apparently Amazon employees usually get the access they request, even though sometimes they are refused.
 
Let’s look at the data. First a label and then a bunch of IDs.
   1,39353,85475,117961,118300,123472,117905,117906,290919,117908
1,17183,1540,117961,118343,123125,118536,118536,308574,118539
1,36724,14457,118219,118220,117884,117879,267952</p><p>6 0.07156451 <a title="39-tfidf-6" href="../fast_ml-2012/fast_ml-2012-12-21-Tuning_hyperparams_automatically_with_Spearmint.html">12 fast ml-2012-12-21-Tuning hyperparams automatically with Spearmint</a></p>
<p>7 0.06466151 <a title="39-tfidf-7" href="../fast_ml-2014/fast_ml-2014-04-30-Converting_categorical_data_into_numbers_with_Pandas_and_Scikit-learn.html">60 fast ml-2014-04-30-Converting categorical data into numbers with Pandas and Scikit-learn</a></p>
<p>8 0.06176896 <a title="39-tfidf-8" href="../fast_ml-2013/fast_ml-2013-02-27-Dimensionality_reduction_for_sparse_binary_data.html">21 fast ml-2013-02-27-Dimensionality reduction for sparse binary data</a></p>
<p>9 0.061322924 <a title="39-tfidf-9" href="../fast_ml-2013/fast_ml-2013-04-10-Gender_discrimination.html">25 fast ml-2013-04-10-Gender discrimination</a></p>
<p>10 0.05198383 <a title="39-tfidf-10" href="../fast_ml-2014/fast_ml-2014-04-21-Predicting_happiness_from_demographics_and_poll_answers.html">59 fast ml-2014-04-21-Predicting happiness from demographics and poll answers</a></p>
<p>11 0.047745276 <a title="39-tfidf-11" href="../fast_ml-2013/fast_ml-2013-02-07-The_secret_of_the_big_guys.html">19 fast ml-2013-02-07-The secret of the big guys</a></p>
<p>12 0.046155926 <a title="39-tfidf-12" href="../fast_ml-2012/fast_ml-2012-10-05-Predicting_closed_questions_on_Stack_Overflow.html">7 fast ml-2012-10-05-Predicting closed questions on Stack Overflow</a></p>
<p>13 0.045985762 <a title="39-tfidf-13" href="../fast_ml-2013/fast_ml-2013-08-23-A_bag_of_words_and_a_nice_little_network.html">36 fast ml-2013-08-23-A bag of words and a nice little network</a></p>
<p>14 0.045956124 <a title="39-tfidf-14" href="../fast_ml-2013/fast_ml-2013-10-06-Pylearn2_in_practice.html">40 fast ml-2013-10-06-Pylearn2 in practice</a></p>
<p>15 0.045535237 <a title="39-tfidf-15" href="../fast_ml-2014/fast_ml-2014-02-02-Yesterday_a_kaggler%2C_today_a_Kaggle_master%3A_a_wrap-up_of_the_cats_and_dogs_competition.html">52 fast ml-2014-02-02-Yesterday a kaggler, today a Kaggle master: a wrap-up of the cats and dogs competition</a></p>
<p>16 0.045360383 <a title="39-tfidf-16" href="../fast_ml-2014/fast_ml-2014-03-20-Good_representations%2C_distance%2C_metric_learning_and_supervised_dimensionality_reduction.html">55 fast ml-2014-03-20-Good representations, distance, metric learning and supervised dimensionality reduction</a></p>
<p>17 0.044696063 <a title="39-tfidf-17" href="../fast_ml-2013/fast_ml-2013-10-28-How_much_data_is_enough%3F.html">42 fast ml-2013-10-28-How much data is enough?</a></p>
<p>18 0.043995775 <a title="39-tfidf-18" href="../fast_ml-2013/fast_ml-2013-04-17-Regression_as_classification.html">26 fast ml-2013-04-17-Regression as classification</a></p>
<p>19 0.042849842 <a title="39-tfidf-19" href="../fast_ml-2012/fast_ml-2012-11-17-The_Facebook_challenge_HOWTO.html">10 fast ml-2012-11-17-The Facebook challenge HOWTO</a></p>
<p>20 0.041991819 <a title="39-tfidf-20" href="../fast_ml-2013/fast_ml-2013-11-02-Maxing_out_the_digits.html">43 fast ml-2013-11-02-Maxing out the digits</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/fast_ml_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.187), (1, -0.023), (2, -0.088), (3, -0.014), (4, -0.043), (5, 0.04), (6, -0.141), (7, 0.018), (8, 0.017), (9, 0.274), (10, 0.198), (11, -0.26), (12, -0.32), (13, -0.026), (14, 0.325), (15, 0.053), (16, -0.017), (17, -0.165), (18, -0.118), (19, 0.201), (20, 0.01), (21, -0.032), (22, 0.087), (23, -0.022), (24, -0.076), (25, 0.036), (26, 0.093), (27, -0.141), (28, -0.015), (29, 0.069), (30, -0.151), (31, -0.059), (32, 0.236), (33, -0.144), (34, 0.082), (35, -0.054), (36, -0.164), (37, 0.027), (38, 0.158), (39, 0.18), (40, -0.053), (41, 0.159), (42, 0.159), (43, -0.028), (44, 0.232), (45, -0.006), (46, -0.088), (47, -0.134), (48, 0.243), (49, -0.051)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99123549 <a title="39-lsi-1" href="../fast_ml-2013/fast_ml-2013-09-19-What_you_wanted_to_know_about_AUC.html">39 fast ml-2013-09-19-What you wanted to know about AUC</a></p>
<p>Introduction: AUC, or Area Under Curve, is a metric for binary classification. It’s probably the second most popular one, after accuracy. Unfortunately, it’s nowhere near as intuitive. That is, until you have read this article.
   
Accuracy deals with ones and zeros, meaning you either got the class label right or you didn’t. But many classifiers are able to quantify their uncertainty about the answer by outputting a probability value. To compute accuracy from probabilities you need a threshold to decide when zero turns into one. The most natural threshold is of course 0.5.
 
Let’s suppose you have a quirky classifier. It is able to get all the answers right, but it outputs 0.7 for negative examples and 0.9 for positive examples. Clearly, a threshold of 0.5 won’t get you far here. But 0.8 would be just perfect.
 
That’s the whole point of using AUC - it considers all possible thresholds. Various thresholds result in different true positive/false positive rates. As you decrease the threshold, you get</p><p>2 0.17331423 <a title="39-lsi-2" href="../fast_ml-2013/fast_ml-2013-06-01-Amazon_aspires_to_automate_access_control.html">30 fast ml-2013-06-01-Amazon aspires to automate access control</a></p>
<p>Introduction: This is about  Amazon access control challenge  at Kaggle. Either we’re getting smarter, or the competition is easy. Or maybe both. You can beat the benchmark quite easily and with AUC of 0.875 you’d be comfortably in the top twenty percent at the moment. We scored fourth in our first attempt - the model was quick to develop and back then there were fewer competitors.
   
 
 
Traditionally we use  Vowpal Wabbit . Just simple binary classification with the logistic loss function and 10 passes over the data.
 
It seems to work pretty well even though the classes are very unbalanced: there’s only a handful of negatives when compared to positives. Apparently Amazon employees usually get the access they request, even though sometimes they are refused.
 
Let’s look at the data. First a label and then a bunch of IDs.
   1,39353,85475,117961,118300,123472,117905,117906,290919,117908
1,17183,1540,117961,118343,123125,118536,118536,308574,118539
1,36724,14457,118219,118220,117884,117879,267952</p><p>3 0.14748996 <a title="39-lsi-3" href="../fast_ml-2013/fast_ml-2013-01-14-Feature_selection_in_practice.html">17 fast ml-2013-01-14-Feature selection in practice</a></p>
<p>Introduction: Lately we’ve been working with the Madelon dataset. It was originally prepared for a feature selection challenge, so while we’re at it, let’s select some features. Madelon has 500 attributes, 20 of which are real, the rest being noise. Hence the ideal scenario would be to select just those 20 features.
   
Fortunately we know just the right software for this task. It’s called  mRMR , for  minimum Redundancy Maximum Relevance , and is available in C and Matlab versions for various platforms. mRMR expects a CSV file with labels in the first column and feature names in the first row. So the game plan is:
  
 combine training and validation sets into a format expected by mRMR 
 run selection 
 filter the original datasets, discarding all features but the selected ones 
 evaluate the results on the validation set 
 if all goes well, prepare and submit files for  the competition  
  
We’ll use  R scripts  for all the steps but feature selection. Now a few words about mRMR. It will show you p</p><p>4 0.14557488 <a title="39-lsi-4" href="../fast_ml-2013/fast_ml-2013-01-04-Madelon%3A_Spearmint%27s_revenge.html">14 fast ml-2013-01-04-Madelon: Spearmint's revenge</a></p>
<p>Introduction: Little Spearmint couldn’t sleep that night.  I was so close…  - he was thinking. It seemed that he had found a better than default value for one of the random forest hyperparams, but it turned out to be false. He made a decision as he fell asleep:  Next time, I will show them! 
   
The way to do this is to use a dataset that is known to produce lower error with high  mtry  values, namely previously mentioned  Madelon  from NIPS 2003 Feature Selection Challenge. Among 500 attributes, only 20 are informative, the rest are noise. That’s the reason why high  mtry  is good here: you have to consider a lot of features to find a meaningful one.
 
The dataset consists of a train, validation and test parts, with labels being available for train and validation. We will further split the training set into our train and validation sets, and use the original validation set as a test set to evaluate final results of parameter tuning.
 
As an error measure we use  Area Under Curve , or AUC, which was</p><p>5 0.13785237 <a title="39-lsi-5" href="../fast_ml-2013/fast_ml-2013-08-12-Accelerometer_Biometric_Competition.html">35 fast ml-2013-08-12-Accelerometer Biometric Competition</a></p>
<p>Introduction: Can you recognize users of mobile devices from accelerometer data? It’s a rather non-standard problem (we’re dealing with time series here) and an interesting one. So we wrote some code and ran EC2 and then wrote more and ran EC2 again. After much computation we had our predictions, submitted them and achieved AUC = 0.83. Yay! But there’s a twist to this story.
   
An interesting thing happened when we mistyped a command. Instead of computing AUC from a validation test set and raw predictions, we computed it using a training set:
 
 auc.py train_v.vw r.txt 
 
And got 0.86.
 
Now, the training file has a very different number of lines from the test file, but our script only cares about how many lines are in a predictions file, so as long as the other file has at least that many, it’s OK.
 
Turns out you can use this set or that set and get pretty much the same good result with each. That got us thinking. We computed a mean of predictions for each device. Here’s the plot:
 
 
 
And here’</p><p>6 0.13463776 <a title="39-lsi-6" href="../fast_ml-2013/fast_ml-2013-02-27-Dimensionality_reduction_for_sparse_binary_data.html">21 fast ml-2013-02-27-Dimensionality reduction for sparse binary data</a></p>
<p>7 0.12532665 <a title="39-lsi-7" href="../fast_ml-2012/fast_ml-2012-12-21-Tuning_hyperparams_automatically_with_Spearmint.html">12 fast ml-2012-12-21-Tuning hyperparams automatically with Spearmint</a></p>
<p>8 0.10338134 <a title="39-lsi-8" href="../fast_ml-2014/fast_ml-2014-04-21-Predicting_happiness_from_demographics_and_poll_answers.html">59 fast ml-2014-04-21-Predicting happiness from demographics and poll answers</a></p>
<p>9 0.098546386 <a title="39-lsi-9" href="../fast_ml-2013/fast_ml-2013-08-23-A_bag_of_words_and_a_nice_little_network.html">36 fast ml-2013-08-23-A bag of words and a nice little network</a></p>
<p>10 0.093072981 <a title="39-lsi-10" href="../fast_ml-2013/fast_ml-2013-04-17-Regression_as_classification.html">26 fast ml-2013-04-17-Regression as classification</a></p>
<p>11 0.089953713 <a title="39-lsi-11" href="../fast_ml-2013/fast_ml-2013-04-10-Gender_discrimination.html">25 fast ml-2013-04-10-Gender discrimination</a></p>
<p>12 0.08891207 <a title="39-lsi-12" href="../fast_ml-2014/fast_ml-2014-04-30-Converting_categorical_data_into_numbers_with_Pandas_and_Scikit-learn.html">60 fast ml-2014-04-30-Converting categorical data into numbers with Pandas and Scikit-learn</a></p>
<p>13 0.084289029 <a title="39-lsi-13" href="../fast_ml-2014/fast_ml-2014-05-08-Impute_missing_values_with_Amelia.html">61 fast ml-2014-05-08-Impute missing values with Amelia</a></p>
<p>14 0.082069062 <a title="39-lsi-14" href="../fast_ml-2014/fast_ml-2014-03-20-Good_representations%2C_distance%2C_metric_learning_and_supervised_dimensionality_reduction.html">55 fast ml-2014-03-20-Good representations, distance, metric learning and supervised dimensionality reduction</a></p>
<p>15 0.077929474 <a title="39-lsi-15" href="../fast_ml-2013/fast_ml-2013-02-07-The_secret_of_the_big_guys.html">19 fast ml-2013-02-07-The secret of the big guys</a></p>
<p>16 0.075230166 <a title="39-lsi-16" href="../fast_ml-2012/fast_ml-2012-10-25-So_you_want_to_work_for_Facebook.html">9 fast ml-2012-10-25-So you want to work for Facebook</a></p>
<p>17 0.074876457 <a title="39-lsi-17" href="../fast_ml-2013/fast_ml-2013-07-05-Processing_large_files%2C_line_by_line.html">32 fast ml-2013-07-05-Processing large files, line by line</a></p>
<p>18 0.074138798 <a title="39-lsi-18" href="../fast_ml-2014/fast_ml-2014-01-10-Classifying_images_with_a_pre-trained_deep_network.html">49 fast ml-2014-01-10-Classifying images with a pre-trained deep network</a></p>
<p>19 0.073812261 <a title="39-lsi-19" href="../fast_ml-2014/fast_ml-2014-02-02-Yesterday_a_kaggler%2C_today_a_Kaggle_master%3A_a_wrap-up_of_the_cats_and_dogs_competition.html">52 fast ml-2014-02-02-Yesterday a kaggler, today a Kaggle master: a wrap-up of the cats and dogs competition</a></p>
<p>20 0.071301401 <a title="39-lsi-20" href="../fast_ml-2013/fast_ml-2013-10-28-How_much_data_is_enough%3F.html">42 fast ml-2013-10-28-How much data is enough?</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/fast_ml_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(9, 0.502), (26, 0.072), (35, 0.057), (58, 0.031), (69, 0.132), (71, 0.028), (79, 0.032), (99, 0.024)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.88196558 <a title="39-lda-1" href="../fast_ml-2013/fast_ml-2013-09-19-What_you_wanted_to_know_about_AUC.html">39 fast ml-2013-09-19-What you wanted to know about AUC</a></p>
<p>Introduction: AUC, or Area Under Curve, is a metric for binary classification. It’s probably the second most popular one, after accuracy. Unfortunately, it’s nowhere near as intuitive. That is, until you have read this article.
   
Accuracy deals with ones and zeros, meaning you either got the class label right or you didn’t. But many classifiers are able to quantify their uncertainty about the answer by outputting a probability value. To compute accuracy from probabilities you need a threshold to decide when zero turns into one. The most natural threshold is of course 0.5.
 
Let’s suppose you have a quirky classifier. It is able to get all the answers right, but it outputs 0.7 for negative examples and 0.9 for positive examples. Clearly, a threshold of 0.5 won’t get you far here. But 0.8 would be just perfect.
 
That’s the whole point of using AUC - it considers all possible thresholds. Various thresholds result in different true positive/false positive rates. As you decrease the threshold, you get</p><p>2 0.27542335 <a title="39-lda-2" href="../fast_ml-2012/fast_ml-2012-12-21-Tuning_hyperparams_automatically_with_Spearmint.html">12 fast ml-2012-12-21-Tuning hyperparams automatically with Spearmint</a></p>
<p>Introduction: The promise  
What’s attractive in machine learning? That a machine is learning, instead of a human. But an operator still has a lot of work to do. First, he has to learn how to teach a machine, in general. Then, when it comes to a concrete task, there are two main areas where a human needs to do the work (and remember, laziness is a virtue, at least for a programmer, so we’d like to minimize amount of work done by a human):
  
 data preparation 
 model tuning 
  
This story is about model tuning.
   

 
Typically, to achieve satisfactory results, first we need to convert raw data into format accepted by the model we would like to use, and then tune a few hyperparameters of the model.
 
For example, some hyperparams to tune for a random forest may be a number of trees to grow and a number of candidate features at each split ( mtry  in R randomForest). For a neural network, there are quite a lot of hyperparams: number of layers, number of neurons in each layer (specifically, in each hid</p><p>3 0.26796654 <a title="39-lda-3" href="../fast_ml-2012/fast_ml-2012-12-27-Spearmint_with_a_random_forest.html">13 fast ml-2012-12-27-Spearmint with a random forest</a></p>
<p>Introduction: Now that we have  Spearmint basics  nailed, we’ll try tuning a random forest, and specifically two hyperparams: a number of trees ( ntrees ) and a number of candidate features at each split ( mtry ). Here’s  some code .
 
We’re going to use a red  wine quality  dataset. It has about 1600 examples and our goal will be to predict a rating for a wine given all the other properties.   This is a regression* task, as ratings are in (0,10) range.
 
We will split the data 80/10/10 into train, validation and test set, and use the first two to establish optimal hyperparams and then predict on the test set. As an error measure we will use RMSE.
 
At first, we will try  ntrees  between 10 and 200 and  mtry  between 3 and 11 (there’s eleven features total, so that’s the upper bound). Here are the results of two Spearmint runs with 71 and 95 tries respectively. Colors denote a validation error value:
  
  green : RMSE < 0.57 
  blue : RMSE < 0.58 
  black : RMSE >= 0.58 
  
Turns out that some diffe</p><p>4 0.26203516 <a title="39-lda-4" href="../fast_ml-2013/fast_ml-2013-05-01-Deep_learning_made_easy.html">27 fast ml-2013-05-01-Deep learning made easy</a></p>
<p>Introduction: As usual, there’s an interesting competition at Kaggle: The Black Box. It’s connected to ICML 2013 Workshop on Challenges in Representation Learning, held by the deep learning guys from Montreal.
 
There are a couple benchmarks for this competition and the best one is unusually hard to beat 1  - only less than a fourth of those taking part managed to do so. We’re among them. Here’s how.
   
The key ingredient in our success is a recently developed secret Stanford technology for deep unsupervised learning:  sparse filtering  by  Jiquan Ngiam  et al. Actually, it’s not secret. It’s  available at Github , and has one or two very appealling properties. Let us explain.
 
The main idea of deep unsupervised learning, as we understand it, is feature extraction. One of the most common applications is in  multimedia. The reason for that is that multimedia tasks, for example object recognition, are easy for humans, but difficult for computers 2 .
 
Geoff Hinton from Toronto talks about  two ends</p><p>5 0.26174173 <a title="39-lda-5" href="../fast_ml-2013/fast_ml-2013-01-14-Feature_selection_in_practice.html">17 fast ml-2013-01-14-Feature selection in practice</a></p>
<p>Introduction: Lately we’ve been working with the Madelon dataset. It was originally prepared for a feature selection challenge, so while we’re at it, let’s select some features. Madelon has 500 attributes, 20 of which are real, the rest being noise. Hence the ideal scenario would be to select just those 20 features.
   
Fortunately we know just the right software for this task. It’s called  mRMR , for  minimum Redundancy Maximum Relevance , and is available in C and Matlab versions for various platforms. mRMR expects a CSV file with labels in the first column and feature names in the first row. So the game plan is:
  
 combine training and validation sets into a format expected by mRMR 
 run selection 
 filter the original datasets, discarding all features but the selected ones 
 evaluate the results on the validation set 
 if all goes well, prepare and submit files for  the competition  
  
We’ll use  R scripts  for all the steps but feature selection. Now a few words about mRMR. It will show you p</p><p>6 0.26072547 <a title="39-lda-6" href="../fast_ml-2012/fast_ml-2012-10-25-So_you_want_to_work_for_Facebook.html">9 fast ml-2012-10-25-So you want to work for Facebook</a></p>
<p>7 0.25969476 <a title="39-lda-7" href="../fast_ml-2013/fast_ml-2013-01-17-A_very_fast_denoising_autoencoder.html">18 fast ml-2013-01-17-A very fast denoising autoencoder</a></p>
<p>8 0.25794336 <a title="39-lda-8" href="../fast_ml-2013/fast_ml-2013-01-04-Madelon%3A_Spearmint%27s_revenge.html">14 fast ml-2013-01-04-Madelon: Spearmint's revenge</a></p>
<p>9 0.25598252 <a title="39-lda-9" href="../fast_ml-2012/fast_ml-2012-08-09-What_you_wanted_to_know_about_Mean_Average_Precision.html">1 fast ml-2012-08-09-What you wanted to know about Mean Average Precision</a></p>
<p>10 0.25145164 <a title="39-lda-10" href="../fast_ml-2013/fast_ml-2013-11-02-Maxing_out_the_digits.html">43 fast ml-2013-11-02-Maxing out the digits</a></p>
<p>11 0.24958587 <a title="39-lda-11" href="../fast_ml-2014/fast_ml-2014-02-02-Yesterday_a_kaggler%2C_today_a_Kaggle_master%3A_a_wrap-up_of_the_cats_and_dogs_competition.html">52 fast ml-2014-02-02-Yesterday a kaggler, today a Kaggle master: a wrap-up of the cats and dogs competition</a></p>
<p>12 0.24826083 <a title="39-lda-12" href="../fast_ml-2013/fast_ml-2013-12-28-Regularizing_neural_networks_with_dropout_and_with_DropConnect.html">48 fast ml-2013-12-28-Regularizing neural networks with dropout and with DropConnect</a></p>
<p>13 0.24395141 <a title="39-lda-13" href="../fast_ml-2013/fast_ml-2013-03-18-Large_scale_L1_feature_selection_with_Vowpal_Wabbit.html">23 fast ml-2013-03-18-Large scale L1 feature selection with Vowpal Wabbit</a></p>
<p>14 0.24208701 <a title="39-lda-14" href="../fast_ml-2013/fast_ml-2013-10-06-Pylearn2_in_practice.html">40 fast ml-2013-10-06-Pylearn2 in practice</a></p>
<p>15 0.24105015 <a title="39-lda-15" href="../fast_ml-2013/fast_ml-2013-02-18-Predicting_advertised_salaries.html">20 fast ml-2013-02-18-Predicting advertised salaries</a></p>
<p>16 0.23859069 <a title="39-lda-16" href="../fast_ml-2014/fast_ml-2014-03-20-Good_representations%2C_distance%2C_metric_learning_and_supervised_dimensionality_reduction.html">55 fast ml-2014-03-20-Good representations, distance, metric learning and supervised dimensionality reduction</a></p>
<p>17 0.23613816 <a title="39-lda-17" href="../fast_ml-2013/fast_ml-2013-11-27-Object_recognition_in_images_with_cuda-convnet.html">45 fast ml-2013-11-27-Object recognition in images with cuda-convnet</a></p>
<p>18 0.23606087 <a title="39-lda-18" href="../fast_ml-2014/fast_ml-2014-04-30-Converting_categorical_data_into_numbers_with_Pandas_and_Scikit-learn.html">60 fast ml-2014-04-30-Converting categorical data into numbers with Pandas and Scikit-learn</a></p>
<p>19 0.23447482 <a title="39-lda-19" href="../fast_ml-2014/fast_ml-2014-01-10-Classifying_images_with_a_pre-trained_deep_network.html">49 fast ml-2014-01-10-Classifying images with a pre-trained deep network</a></p>
<p>20 0.23323508 <a title="39-lda-20" href="../fast_ml-2013/fast_ml-2013-08-12-Accelerometer_Biometric_Competition.html">35 fast ml-2013-08-12-Accelerometer Biometric Competition</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
