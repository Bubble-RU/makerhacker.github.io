<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>47 fast ml-2013-12-15-A-B testing with bayesian bandits in Google Analytics</title>
</head>

<body>
<p><a title="fast_ml" href="../fast_ml_home.html">fast_ml</a> <a title="fast_ml-2013" href="../home/fast_ml-2013_home.html">fast_ml-2013</a> <a title="fast_ml-2013-47" href="#">fast_ml-2013-47</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>47 fast ml-2013-12-15-A-B testing with bayesian bandits in Google Analytics</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="fast_ml-2013-47-html" href="http://fastml.com//ab-testing-with-bayesian-bandits-in-google-analytics/">html</a></p><p>Introduction: A/B testing is a way to optimize a web page. Half of visitors see one version, the other half another, so you can tell which version is more conducive to your goal - for example selling something. Since June 2013 A/B testing can be conveniently done with Google Analytics. Here’s how.
   
 This article is not quite about machine learning. If you’re not interested in testing, scroll down to the  bayesian bandits section . 
  Google Content Experiments  
We remember Google Website Optimizer from a few years ago. It wasn’t exactly user friendly or slick, but it felt solid and did the job. Unfortunately, at one point in time Google pulled the plug, leaving  Genetify  as a sole free (and open source) tool for multivariate testing.  Multivariate  means testing a few elements on a page simultanously.
 
At that time they launched Content Experiments in Google Analytics, but it was a giant step backward. Content experiments were very primitive and only allowed rudimentary A/B split testing. It i</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Half of visitors see one version, the other half another, so you can tell which version is more conducive to your goal - for example selling something. [sent-2, score-0.367]
</p><p>2 If you’re not interested in testing, scroll down to the  bayesian bandits section . [sent-6, score-0.325]
</p><p>3 Multivariate  means testing a few elements on a page simultanously. [sent-10, score-0.42]
</p><p>4 Essentially, now you can have a full-fledged multivariate testing tool akin to Website Optimizer. [sent-14, score-0.315]
</p><p>5 The catch is that you need to implement content variations yourself in JavaScript. [sent-15, score-0.359]
</p><p>6 This is not as clear-cut goal, but still you can measure average time on page, page views, returning visits and so on. [sent-24, score-0.47]
</p><p>7 The setup   First you  setup the experiment  using the normal interface in GA, only skipping the part where you put variation URLs - just enter some subpages there. [sent-25, score-0.998]
</p><p>8 Now you put some Javascript in your page source:      load the content experiments script from Google, providing it the experiment ID from the GA visual interface:get a variation number it selects:   var variation = cxApi. [sent-28, score-1.726]
</p><p>9 chooseVariation();    put your normal Analytics snippet after this     Now the only thing left is actually showing the selected variation. [sent-29, score-0.391]
</p><p>10 Showing the selected variation   Suppose that you want to vary contents of a  DIV . [sent-30, score-0.595]
</p><p>11 Then if GA decides it wants to show the variation to a given user, you switch the divs: the original becomes hidden and the variation shows. [sent-36, score-1.074]
</p><p>12 Here’s an example JavaScript snippet to put at the end of HTML BODY:// original is 0     if ( variation == 1 ) {         document. [sent-37, score-0.648]
</p><p>13 Bayesian bandits   When a user first visits the page, Analytics decides which variation to show. [sent-44, score-1.051]
</p><p>14 The selected variation is stored in a cookie so when the user opens a next page or comes back tomorrow he is shown the same version. [sent-45, score-0.909]
</p><p>15 The difference is that a multi-arm bandit has many arms and you need to decide which one to pull. [sent-49, score-0.306]
</p><p>16 Here’s a link to the page with an interactive visualization. [sent-52, score-0.291]
</p><p>17 In case of web testing they are rather unlikely to change during the experiment, so that makes bandits a good solution for selecting variations to show. [sent-55, score-0.716]
</p><p>18 Note that Google uses bandits to decide which ads to show - the underlying problem is very similiar. [sent-58, score-0.418]
</p><p>19 You can either use one of the built-in metrics (including page views, bounce rate and time on page) or a custom goal defined in Analytics. [sent-62, score-0.517]
</p><p>20 This means thay you can optimize some basic metrics with a click of a button, or, with some effort, pretty much whatever you want. [sent-63, score-0.288]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('variation', 0.428), ('page', 0.291), ('bandits', 0.257), ('content', 0.205), ('google', 0.185), ('visitors', 0.171), ('decides', 0.154), ('ga', 0.154), ('variations', 0.154), ('experiment', 0.136), ('experiments', 0.136), ('testing', 0.129), ('bandit', 0.128), ('interface', 0.128), ('multivariate', 0.128), ('click', 0.113), ('web', 0.113), ('user', 0.109), ('analytics', 0.109), ('arms', 0.103), ('html', 0.103), ('june', 0.103), ('visits', 0.103), ('put', 0.102), ('optimize', 0.094), ('goal', 0.087), ('want', 0.086), ('ads', 0.086), ('javascript', 0.086), ('views', 0.086), ('showing', 0.086), ('metrics', 0.081), ('selected', 0.081), ('measure', 0.076), ('website', 0.075), ('decide', 0.075), ('setup', 0.068), ('normal', 0.068), ('bayesian', 0.068), ('original', 0.064), ('change', 0.063), ('half', 0.058), ('tool', 0.058), ('finding', 0.058), ('success', 0.058), ('custom', 0.058), ('probabilities', 0.054), ('snippet', 0.054), ('version', 0.051), ('exactly', 0.048)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999982 <a title="47-tfidf-1" href="../fast_ml-2013/fast_ml-2013-12-15-A-B_testing_with_bayesian_bandits_in_Google_Analytics.html">47 fast ml-2013-12-15-A-B testing with bayesian bandits in Google Analytics</a></p>
<p>Introduction: A/B testing is a way to optimize a web page. Half of visitors see one version, the other half another, so you can tell which version is more conducive to your goal - for example selling something. Since June 2013 A/B testing can be conveniently done with Google Analytics. Here’s how.
   
 This article is not quite about machine learning. If you’re not interested in testing, scroll down to the  bayesian bandits section . 
  Google Content Experiments  
We remember Google Website Optimizer from a few years ago. It wasn’t exactly user friendly or slick, but it felt solid and did the job. Unfortunately, at one point in time Google pulled the plug, leaving  Genetify  as a sole free (and open source) tool for multivariate testing.  Multivariate  means testing a few elements on a page simultanously.
 
At that time they launched Content Experiments in Google Analytics, but it was a giant step backward. Content experiments were very primitive and only allowed rudimentary A/B split testing. It i</p><p>2 0.093386173 <a title="47-tfidf-2" href="../fast_ml-2013/fast_ml-2013-10-09-Big_data_made_easy.html">41 fast ml-2013-10-09-Big data made easy</a></p>
<p>Introduction: An overview of key points about big data. This post was inspired by a very good article about big data by Chris Stucchio (linked below). The article is about hype and technology. We hate the hype.
    Big data is hype  
Everybody talks about big data; nobody knows exactly what it is. That’s pretty much the definition of hype.  Google Trends  suggest that the term took off at the beginning of 2011 (and the searches are coming mainly from Asia, curiously).
 
 
 
Now, to put things in context:
 
 
 
Big data is right there (or maybe not quite yet?) with other slogans like  web 2.0 ,  cloud computing  and  social media .
In effect,  big data  is a generic term for:
  
 data science 
 machine learning 
 data mining 
 predictive analytics 
  
and so on. Don’t believe us? What about James Goodnight, the CEO of  SAS :
  
The term big data is being used today because computer analysts and journalists got tired of writing about cloud computing. Before cloud computing it was data warehousing or</p><p>3 0.070475429 <a title="47-tfidf-3" href="../fast_ml-2013/fast_ml-2013-01-17-A_very_fast_denoising_autoencoder.html">18 fast ml-2013-01-17-A very fast denoising autoencoder</a></p>
<p>Introduction: Once upon a time we were browsing machine learning papers and software. We were interested in autoencoders and found a rather unusual one. It was called  marginalized Stacked Denoising Autoencoder  and the author claimed that it  preserves the strong feature learning capacity of Stacked Denoising Autoencoders, but is orders of magnitudes faster.  We like all things fast, so we were hooked.
    About autoencoders  
 Wikipedia says  that  an autoencoder is an artificial neural network and its aim is to learn a compressed representation for a set of data. This means it is being used for dimensionality reduction . In other words, an autoencoder is a neural network meant to replicate the input. It would be trivial with a big enough number of units in a hidden layer: the network would just find an identity mapping. Hence dimensionality reduction: a hidden layer size is typically smaller than input layer.
 
mSDA is a curious specimen: it is not a neural network and it doesn’t reduce dimension</p><p>4 0.064668246 <a title="47-tfidf-4" href="../fast_ml-2012/fast_ml-2012-10-05-Predicting_closed_questions_on_Stack_Overflow.html">7 fast ml-2012-10-05-Predicting closed questions on Stack Overflow</a></p>
<p>Introduction: This time we enter the  Stack Overflow challenge , which is about predicting a status of a given question on SO. There are five possible statuses, so it’s a multi-class classification problem.
 
We would prefer a tool able to perform multiclass classification by itself. It can be done by hand by constructing five datasets, each with binary labels (one class against all others), and then combining predictions, but it might be a bit tricky to get right - we tried. Fortunately, nice people at Yahoo, excuse us, Microsoft, recently relased a new version of  Vowpal Wabbit , and this new version supports multiclass classification.
   
In case you’re wondering, Vowpal Wabbit is a fast linear learner. We like the “fast” part and “linear” is OK for dealing with lots of words, as in this contest. In any case, with more than three million data points it wouldn’t be that easy to train a kernel SVM, a neural net or what have you.
 
VW, being a well-polished tool, has a few very convenient features.</p><p>5 0.064413436 <a title="47-tfidf-5" href="../fast_ml-2012/fast_ml-2012-12-21-Tuning_hyperparams_automatically_with_Spearmint.html">12 fast ml-2012-12-21-Tuning hyperparams automatically with Spearmint</a></p>
<p>Introduction: The promise  
What’s attractive in machine learning? That a machine is learning, instead of a human. But an operator still has a lot of work to do. First, he has to learn how to teach a machine, in general. Then, when it comes to a concrete task, there are two main areas where a human needs to do the work (and remember, laziness is a virtue, at least for a programmer, so we’d like to minimize amount of work done by a human):
  
 data preparation 
 model tuning 
  
This story is about model tuning.
   

 
Typically, to achieve satisfactory results, first we need to convert raw data into format accepted by the model we would like to use, and then tune a few hyperparameters of the model.
 
For example, some hyperparams to tune for a random forest may be a number of trees to grow and a number of candidate features at each split ( mtry  in R randomForest). For a neural network, there are quite a lot of hyperparams: number of layers, number of neurons in each layer (specifically, in each hid</p><p>6 0.063815914 <a title="47-tfidf-6" href="../fast_ml-2014/fast_ml-2014-01-20-How_to_get_predictions_from_Pylearn2.html">50 fast ml-2014-01-20-How to get predictions from Pylearn2</a></p>
<p>7 0.061164211 <a title="47-tfidf-7" href="../fast_ml-2012/fast_ml-2012-12-07-Predicting_wine_quality.html">11 fast ml-2012-12-07-Predicting wine quality</a></p>
<p>8 0.057437584 <a title="47-tfidf-8" href="../fast_ml-2013/fast_ml-2013-12-07-13_NIPS_papers_that_caught_our_eye.html">46 fast ml-2013-12-07-13 NIPS papers that caught our eye</a></p>
<p>9 0.05699062 <a title="47-tfidf-9" href="../fast_ml-2013/fast_ml-2013-07-05-Processing_large_files%2C_line_by_line.html">32 fast ml-2013-07-05-Processing large files, line by line</a></p>
<p>10 0.054580327 <a title="47-tfidf-10" href="../fast_ml-2013/fast_ml-2013-05-01-Deep_learning_made_easy.html">27 fast ml-2013-05-01-Deep learning made easy</a></p>
<p>11 0.053667393 <a title="47-tfidf-11" href="../fast_ml-2013/fast_ml-2013-10-06-Pylearn2_in_practice.html">40 fast ml-2013-10-06-Pylearn2 in practice</a></p>
<p>12 0.052314825 <a title="47-tfidf-12" href="../fast_ml-2014/fast_ml-2014-02-20-Are_stocks_predictable%3F.html">53 fast ml-2014-02-20-Are stocks predictable?</a></p>
<p>13 0.050540872 <a title="47-tfidf-13" href="../fast_ml-2013/fast_ml-2013-09-03-Our_followers_and_who_else_they_follow.html">37 fast ml-2013-09-03-Our followers and who else they follow</a></p>
<p>14 0.050494477 <a title="47-tfidf-14" href="../fast_ml-2014/fast_ml-2014-03-06-PyBrain_-_a_simple_neural_networks_library_in_Python.html">54 fast ml-2014-03-06-PyBrain - a simple neural networks library in Python</a></p>
<p>15 0.050402816 <a title="47-tfidf-15" href="../fast_ml-2014/fast_ml-2014-04-30-Converting_categorical_data_into_numbers_with_Pandas_and_Scikit-learn.html">60 fast ml-2014-04-30-Converting categorical data into numbers with Pandas and Scikit-learn</a></p>
<p>16 0.048809767 <a title="47-tfidf-16" href="../fast_ml-2013/fast_ml-2013-01-04-Madelon%3A_Spearmint%27s_revenge.html">14 fast ml-2013-01-04-Madelon: Spearmint's revenge</a></p>
<p>17 0.04871624 <a title="47-tfidf-17" href="../fast_ml-2014/fast_ml-2014-05-26-Yann_LeCun%27s_answers_from_the_Reddit_AMA.html">62 fast ml-2014-05-26-Yann LeCun's answers from the Reddit AMA</a></p>
<p>18 0.048444193 <a title="47-tfidf-18" href="../fast_ml-2013/fast_ml-2013-02-07-The_secret_of_the_big_guys.html">19 fast ml-2013-02-07-The secret of the big guys</a></p>
<p>19 0.047307357 <a title="47-tfidf-19" href="../fast_ml-2012/fast_ml-2012-08-09-What_you_wanted_to_know_about_Mean_Average_Precision.html">1 fast ml-2012-08-09-What you wanted to know about Mean Average Precision</a></p>
<p>20 0.046725038 <a title="47-tfidf-20" href="../fast_ml-2013/fast_ml-2013-03-18-Large_scale_L1_feature_selection_with_Vowpal_Wabbit.html">23 fast ml-2013-03-18-Large scale L1 feature selection with Vowpal Wabbit</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/fast_ml_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.19), (1, 0.024), (2, 0.046), (3, -0.09), (4, 0.027), (5, 0.067), (6, -0.042), (7, 0.084), (8, 0.122), (9, 0.144), (10, 0.21), (11, 0.183), (12, -0.075), (13, -0.192), (14, -0.353), (15, -0.191), (16, -0.111), (17, -0.052), (18, 0.026), (19, 0.166), (20, -0.243), (21, -0.115), (22, -0.143), (23, 0.122), (24, -0.033), (25, -0.073), (26, -0.095), (27, -0.076), (28, -0.13), (29, -0.024), (30, -0.038), (31, 0.171), (32, 0.165), (33, 0.201), (34, 0.233), (35, -0.129), (36, -0.005), (37, -0.094), (38, 0.334), (39, -0.131), (40, 0.083), (41, 0.087), (42, -0.232), (43, -0.112), (44, -0.136), (45, -0.006), (46, -0.101), (47, -0.057), (48, -0.0), (49, 0.043)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99215192 <a title="47-lsi-1" href="../fast_ml-2013/fast_ml-2013-12-15-A-B_testing_with_bayesian_bandits_in_Google_Analytics.html">47 fast ml-2013-12-15-A-B testing with bayesian bandits in Google Analytics</a></p>
<p>Introduction: A/B testing is a way to optimize a web page. Half of visitors see one version, the other half another, so you can tell which version is more conducive to your goal - for example selling something. Since June 2013 A/B testing can be conveniently done with Google Analytics. Here’s how.
   
 This article is not quite about machine learning. If you’re not interested in testing, scroll down to the  bayesian bandits section . 
  Google Content Experiments  
We remember Google Website Optimizer from a few years ago. It wasn’t exactly user friendly or slick, but it felt solid and did the job. Unfortunately, at one point in time Google pulled the plug, leaving  Genetify  as a sole free (and open source) tool for multivariate testing.  Multivariate  means testing a few elements on a page simultanously.
 
At that time they launched Content Experiments in Google Analytics, but it was a giant step backward. Content experiments were very primitive and only allowed rudimentary A/B split testing. It i</p><p>2 0.12114407 <a title="47-lsi-2" href="../fast_ml-2012/fast_ml-2012-10-05-Predicting_closed_questions_on_Stack_Overflow.html">7 fast ml-2012-10-05-Predicting closed questions on Stack Overflow</a></p>
<p>Introduction: This time we enter the  Stack Overflow challenge , which is about predicting a status of a given question on SO. There are five possible statuses, so it’s a multi-class classification problem.
 
We would prefer a tool able to perform multiclass classification by itself. It can be done by hand by constructing five datasets, each with binary labels (one class against all others), and then combining predictions, but it might be a bit tricky to get right - we tried. Fortunately, nice people at Yahoo, excuse us, Microsoft, recently relased a new version of  Vowpal Wabbit , and this new version supports multiclass classification.
   
In case you’re wondering, Vowpal Wabbit is a fast linear learner. We like the “fast” part and “linear” is OK for dealing with lots of words, as in this contest. In any case, with more than three million data points it wouldn’t be that easy to train a kernel SVM, a neural net or what have you.
 
VW, being a well-polished tool, has a few very convenient features.</p><p>3 0.11619791 <a title="47-lsi-3" href="../fast_ml-2012/fast_ml-2012-12-21-Tuning_hyperparams_automatically_with_Spearmint.html">12 fast ml-2012-12-21-Tuning hyperparams automatically with Spearmint</a></p>
<p>Introduction: The promise  
What’s attractive in machine learning? That a machine is learning, instead of a human. But an operator still has a lot of work to do. First, he has to learn how to teach a machine, in general. Then, when it comes to a concrete task, there are two main areas where a human needs to do the work (and remember, laziness is a virtue, at least for a programmer, so we’d like to minimize amount of work done by a human):
  
 data preparation 
 model tuning 
  
This story is about model tuning.
   

 
Typically, to achieve satisfactory results, first we need to convert raw data into format accepted by the model we would like to use, and then tune a few hyperparameters of the model.
 
For example, some hyperparams to tune for a random forest may be a number of trees to grow and a number of candidate features at each split ( mtry  in R randomForest). For a neural network, there are quite a lot of hyperparams: number of layers, number of neurons in each layer (specifically, in each hid</p><p>4 0.11462098 <a title="47-lsi-4" href="../fast_ml-2013/fast_ml-2013-10-09-Big_data_made_easy.html">41 fast ml-2013-10-09-Big data made easy</a></p>
<p>Introduction: An overview of key points about big data. This post was inspired by a very good article about big data by Chris Stucchio (linked below). The article is about hype and technology. We hate the hype.
    Big data is hype  
Everybody talks about big data; nobody knows exactly what it is. That’s pretty much the definition of hype.  Google Trends  suggest that the term took off at the beginning of 2011 (and the searches are coming mainly from Asia, curiously).
 
 
 
Now, to put things in context:
 
 
 
Big data is right there (or maybe not quite yet?) with other slogans like  web 2.0 ,  cloud computing  and  social media .
In effect,  big data  is a generic term for:
  
 data science 
 machine learning 
 data mining 
 predictive analytics 
  
and so on. Don’t believe us? What about James Goodnight, the CEO of  SAS :
  
The term big data is being used today because computer analysts and journalists got tired of writing about cloud computing. Before cloud computing it was data warehousing or</p><p>5 0.11138428 <a title="47-lsi-5" href="../fast_ml-2013/fast_ml-2013-07-05-Processing_large_files%2C_line_by_line.html">32 fast ml-2013-07-05-Processing large files, line by line</a></p>
<p>Introduction: Perhaps the most common format of data for machine learning is text files. Often data is too large to fit in memory; this is sometimes referred to as big data. But do you need to load the whole data into memory? Maybe you could at least pre-process it line by line. We show how to do this with Python. Prepare to read and possibly write some code.
   
The most common format for text files is probably CSV. For sparse data,  libsvm  format is popular. Both can be processed using csv module in Python.
   import csv

i_f = open( input_file, 'r' )
reader = csv.reader( i_f )
   
For  libsvm  you just set the delimiter to space:
   reader = csv.reader( i_f, delimiter = ' ' )
   
Then you go over the file contents. Each line is a list of strings:
   for line in reader:

    # do something with the line, for example:
    label = float( line[0] )
    # ....

    writer.writerow( line )
   
If you need to do a second pass, you just rewind the input file:
   i_f.seek( 0 )
for line in re</p><p>6 0.10766127 <a title="47-lsi-6" href="../fast_ml-2013/fast_ml-2013-05-01-Deep_learning_made_easy.html">27 fast ml-2013-05-01-Deep learning made easy</a></p>
<p>7 0.10290334 <a title="47-lsi-7" href="../fast_ml-2012/fast_ml-2012-12-07-Predicting_wine_quality.html">11 fast ml-2012-12-07-Predicting wine quality</a></p>
<p>8 0.096808463 <a title="47-lsi-8" href="../fast_ml-2013/fast_ml-2013-01-17-A_very_fast_denoising_autoencoder.html">18 fast ml-2013-01-17-A very fast denoising autoencoder</a></p>
<p>9 0.093795985 <a title="47-lsi-9" href="../fast_ml-2013/fast_ml-2013-03-25-Dimensionality_reduction_for_sparse_binary_data_-_an_overview.html">24 fast ml-2013-03-25-Dimensionality reduction for sparse binary data - an overview</a></p>
<p>10 0.093678534 <a title="47-lsi-10" href="../fast_ml-2014/fast_ml-2014-02-20-Are_stocks_predictable%3F.html">53 fast ml-2014-02-20-Are stocks predictable?</a></p>
<p>11 0.090401426 <a title="47-lsi-11" href="../fast_ml-2014/fast_ml-2014-03-06-PyBrain_-_a_simple_neural_networks_library_in_Python.html">54 fast ml-2014-03-06-PyBrain - a simple neural networks library in Python</a></p>
<p>12 0.088291503 <a title="47-lsi-12" href="../fast_ml-2013/fast_ml-2013-12-07-13_NIPS_papers_that_caught_our_eye.html">46 fast ml-2013-12-07-13 NIPS papers that caught our eye</a></p>
<p>13 0.084655046 <a title="47-lsi-13" href="../fast_ml-2012/fast_ml-2012-08-09-What_you_wanted_to_know_about_Mean_Average_Precision.html">1 fast ml-2012-08-09-What you wanted to know about Mean Average Precision</a></p>
<p>14 0.081863299 <a title="47-lsi-14" href="../fast_ml-2014/fast_ml-2014-01-20-How_to_get_predictions_from_Pylearn2.html">50 fast ml-2014-01-20-How to get predictions from Pylearn2</a></p>
<p>15 0.081638053 <a title="47-lsi-15" href="../fast_ml-2012/fast_ml-2012-10-25-So_you_want_to_work_for_Facebook.html">9 fast ml-2012-10-25-So you want to work for Facebook</a></p>
<p>16 0.080125391 <a title="47-lsi-16" href="../fast_ml-2013/fast_ml-2013-01-04-Madelon%3A_Spearmint%27s_revenge.html">14 fast ml-2013-01-04-Madelon: Spearmint's revenge</a></p>
<p>17 0.078071833 <a title="47-lsi-17" href="../fast_ml-2013/fast_ml-2013-03-18-Large_scale_L1_feature_selection_with_Vowpal_Wabbit.html">23 fast ml-2013-03-18-Large scale L1 feature selection with Vowpal Wabbit</a></p>
<p>18 0.077801093 <a title="47-lsi-18" href="../fast_ml-2013/fast_ml-2013-01-07-Machine_learning_courses_online.html">15 fast ml-2013-01-07-Machine learning courses online</a></p>
<p>19 0.077655748 <a title="47-lsi-19" href="../fast_ml-2014/fast_ml-2014-04-30-Converting_categorical_data_into_numbers_with_Pandas_and_Scikit-learn.html">60 fast ml-2014-04-30-Converting categorical data into numbers with Pandas and Scikit-learn</a></p>
<p>20 0.072311178 <a title="47-lsi-20" href="../fast_ml-2013/fast_ml-2013-04-10-Gender_discrimination.html">25 fast ml-2013-04-10-Gender discrimination</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/fast_ml_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(4, 0.5), (6, 0.042), (26, 0.023), (31, 0.033), (35, 0.011), (48, 0.026), (51, 0.017), (55, 0.013), (69, 0.13), (71, 0.026), (78, 0.014), (79, 0.027), (84, 0.013), (99, 0.038)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.920156 <a title="47-lda-1" href="../fast_ml-2013/fast_ml-2013-12-15-A-B_testing_with_bayesian_bandits_in_Google_Analytics.html">47 fast ml-2013-12-15-A-B testing with bayesian bandits in Google Analytics</a></p>
<p>Introduction: A/B testing is a way to optimize a web page. Half of visitors see one version, the other half another, so you can tell which version is more conducive to your goal - for example selling something. Since June 2013 A/B testing can be conveniently done with Google Analytics. Here’s how.
   
 This article is not quite about machine learning. If you’re not interested in testing, scroll down to the  bayesian bandits section . 
  Google Content Experiments  
We remember Google Website Optimizer from a few years ago. It wasn’t exactly user friendly or slick, but it felt solid and did the job. Unfortunately, at one point in time Google pulled the plug, leaving  Genetify  as a sole free (and open source) tool for multivariate testing.  Multivariate  means testing a few elements on a page simultanously.
 
At that time they launched Content Experiments in Google Analytics, but it was a giant step backward. Content experiments were very primitive and only allowed rudimentary A/B split testing. It i</p><p>2 0.2764242 <a title="47-lda-2" href="../fast_ml-2012/fast_ml-2012-10-25-So_you_want_to_work_for_Facebook.html">9 fast ml-2012-10-25-So you want to work for Facebook</a></p>
<p>Introduction: Good news, everyone! There’s a new contest on Kaggle -  Facebook is looking for talent . They won’t pay, but just might interview.
 
This post is in a way a bonus for active readers because most visitors of fastml.com originally come from Kaggle forums. For this competition the forums are disabled to encourage  own work . To honor this, we won’t publish any code. But  own work  doesn’t mean  original work , and we wouldn’t want to reinvent the wheel, would we?
   
The contest differs substantially from a Kaggle stereotype, if there is such a thing, in three major ways:
  
 there’s no money prizes, as mentioned above 
 it’s not a real world problem, but rather an assignment to screen job candidates (this has important consequences, described below) 
 it’s not a typical machine learning project, but rather a broader AI exercise 
  
You are given a graph of the internet, actually a snapshot of the graph for each of 15 time steps. You are also given a bunch of paths in this graph, which  a</p><p>3 0.27216229 <a title="47-lda-3" href="../fast_ml-2012/fast_ml-2012-12-21-Tuning_hyperparams_automatically_with_Spearmint.html">12 fast ml-2012-12-21-Tuning hyperparams automatically with Spearmint</a></p>
<p>Introduction: The promise  
What’s attractive in machine learning? That a machine is learning, instead of a human. But an operator still has a lot of work to do. First, he has to learn how to teach a machine, in general. Then, when it comes to a concrete task, there are two main areas where a human needs to do the work (and remember, laziness is a virtue, at least for a programmer, so we’d like to minimize amount of work done by a human):
  
 data preparation 
 model tuning 
  
This story is about model tuning.
   

 
Typically, to achieve satisfactory results, first we need to convert raw data into format accepted by the model we would like to use, and then tune a few hyperparameters of the model.
 
For example, some hyperparams to tune for a random forest may be a number of trees to grow and a number of candidate features at each split ( mtry  in R randomForest). For a neural network, there are quite a lot of hyperparams: number of layers, number of neurons in each layer (specifically, in each hid</p><p>4 0.26882562 <a title="47-lda-4" href="../fast_ml-2013/fast_ml-2013-05-01-Deep_learning_made_easy.html">27 fast ml-2013-05-01-Deep learning made easy</a></p>
<p>Introduction: As usual, there’s an interesting competition at Kaggle: The Black Box. It’s connected to ICML 2013 Workshop on Challenges in Representation Learning, held by the deep learning guys from Montreal.
 
There are a couple benchmarks for this competition and the best one is unusually hard to beat 1  - only less than a fourth of those taking part managed to do so. We’re among them. Here’s how.
   
The key ingredient in our success is a recently developed secret Stanford technology for deep unsupervised learning:  sparse filtering  by  Jiquan Ngiam  et al. Actually, it’s not secret. It’s  available at Github , and has one or two very appealling properties. Let us explain.
 
The main idea of deep unsupervised learning, as we understand it, is feature extraction. One of the most common applications is in  multimedia. The reason for that is that multimedia tasks, for example object recognition, are easy for humans, but difficult for computers 2 .
 
Geoff Hinton from Toronto talks about  two ends</p><p>5 0.26317641 <a title="47-lda-5" href="../fast_ml-2012/fast_ml-2012-12-27-Spearmint_with_a_random_forest.html">13 fast ml-2012-12-27-Spearmint with a random forest</a></p>
<p>Introduction: Now that we have  Spearmint basics  nailed, we’ll try tuning a random forest, and specifically two hyperparams: a number of trees ( ntrees ) and a number of candidate features at each split ( mtry ). Here’s  some code .
 
We’re going to use a red  wine quality  dataset. It has about 1600 examples and our goal will be to predict a rating for a wine given all the other properties.   This is a regression* task, as ratings are in (0,10) range.
 
We will split the data 80/10/10 into train, validation and test set, and use the first two to establish optimal hyperparams and then predict on the test set. As an error measure we will use RMSE.
 
At first, we will try  ntrees  between 10 and 200 and  mtry  between 3 and 11 (there’s eleven features total, so that’s the upper bound). Here are the results of two Spearmint runs with 71 and 95 tries respectively. Colors denote a validation error value:
  
  green : RMSE < 0.57 
  blue : RMSE < 0.58 
  black : RMSE >= 0.58 
  
Turns out that some diffe</p><p>6 0.25999743 <a title="47-lda-6" href="../fast_ml-2012/fast_ml-2012-10-05-Predicting_closed_questions_on_Stack_Overflow.html">7 fast ml-2012-10-05-Predicting closed questions on Stack Overflow</a></p>
<p>7 0.25767022 <a title="47-lda-7" href="../fast_ml-2012/fast_ml-2012-08-09-What_you_wanted_to_know_about_Mean_Average_Precision.html">1 fast ml-2012-08-09-What you wanted to know about Mean Average Precision</a></p>
<p>8 0.25515851 <a title="47-lda-8" href="../fast_ml-2013/fast_ml-2013-01-04-Madelon%3A_Spearmint%27s_revenge.html">14 fast ml-2013-01-04-Madelon: Spearmint's revenge</a></p>
<p>9 0.25156808 <a title="47-lda-9" href="../fast_ml-2013/fast_ml-2013-11-02-Maxing_out_the_digits.html">43 fast ml-2013-11-02-Maxing out the digits</a></p>
<p>10 0.2499382 <a title="47-lda-10" href="../fast_ml-2013/fast_ml-2013-01-17-A_very_fast_denoising_autoencoder.html">18 fast ml-2013-01-17-A very fast denoising autoencoder</a></p>
<p>11 0.24897932 <a title="47-lda-11" href="../fast_ml-2013/fast_ml-2013-01-14-Feature_selection_in_practice.html">17 fast ml-2013-01-14-Feature selection in practice</a></p>
<p>12 0.24803907 <a title="47-lda-12" href="../fast_ml-2013/fast_ml-2013-12-28-Regularizing_neural_networks_with_dropout_and_with_DropConnect.html">48 fast ml-2013-12-28-Regularizing neural networks with dropout and with DropConnect</a></p>
<p>13 0.2453362 <a title="47-lda-13" href="../fast_ml-2013/fast_ml-2013-02-07-The_secret_of_the_big_guys.html">19 fast ml-2013-02-07-The secret of the big guys</a></p>
<p>14 0.2405536 <a title="47-lda-14" href="../fast_ml-2013/fast_ml-2013-02-18-Predicting_advertised_salaries.html">20 fast ml-2013-02-18-Predicting advertised salaries</a></p>
<p>15 0.23885602 <a title="47-lda-15" href="../fast_ml-2013/fast_ml-2013-10-06-Pylearn2_in_practice.html">40 fast ml-2013-10-06-Pylearn2 in practice</a></p>
<p>16 0.23283298 <a title="47-lda-16" href="../fast_ml-2013/fast_ml-2013-01-12-Intro_to_random_forests.html">16 fast ml-2013-01-12-Intro to random forests</a></p>
<p>17 0.23089628 <a title="47-lda-17" href="../fast_ml-2013/fast_ml-2013-08-12-Accelerometer_Biometric_Competition.html">35 fast ml-2013-08-12-Accelerometer Biometric Competition</a></p>
<p>18 0.23058546 <a title="47-lda-18" href="../fast_ml-2013/fast_ml-2013-03-18-Large_scale_L1_feature_selection_with_Vowpal_Wabbit.html">23 fast ml-2013-03-18-Large scale L1 feature selection with Vowpal Wabbit</a></p>
<p>19 0.22843575 <a title="47-lda-19" href="../fast_ml-2014/fast_ml-2014-03-20-Good_representations%2C_distance%2C_metric_learning_and_supervised_dimensionality_reduction.html">55 fast ml-2014-03-20-Good representations, distance, metric learning and supervised dimensionality reduction</a></p>
<p>20 0.22650939 <a title="47-lda-20" href="../fast_ml-2013/fast_ml-2013-04-10-Gender_discrimination.html">25 fast ml-2013-04-10-Gender discrimination</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
