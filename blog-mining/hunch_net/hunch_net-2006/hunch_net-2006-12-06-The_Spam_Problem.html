<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>223 hunch net-2006-12-06-The Spam Problem</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-223" href="#">hunch_net-2006-223</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>223 hunch net-2006-12-06-The Spam Problem</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-223-html" href="http://hunch.net/?p=243">html</a></p><p>Introduction: TheNew York Timeshas an article on thegrowth of spam. Interesting facts
include: 9/10 of all email is spam, spam source identification is nearly
useless due to botnet spam senders, and image based spam (emails which consist
of an image only) are on the growth.Estimates of the cost of spam are almost
certainly far to low, because they do not account for the cost in time lost by
people.The image based spam which is currently penetrating many filters should
be catchable with a more sophisticated application of machine learning
technology. For the spam I see, the rendered images come in only a few
formats, which would be easy to recognize via a support vector machine (with
RBF kernel), neural network, or even nearest-neighbor architecture. The
mechanics of setting this up to run efficiently is the only real challenge.
This is the next step in the spam war.The response to this system is to make
the image based spam even more random. We should (essentially) expect to
seeCaptchaspam, and our</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('spam', 0.772), ('payment', 0.25), ('image', 0.205), ('email', 0.181), ('receiver', 0.148), ('recognize', 0.111), ('feature', 0.097), ('based', 0.075), ('include', 0.075), ('classifies', 0.074), ('extend', 0.074), ('cents', 0.074), ('consist', 0.074), ('rbf', 0.074), ('verifiable', 0.074), ('system', 0.074), ('cost', 0.071), ('charge', 0.069), ('identification', 0.069), ('emails', 0.069)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="223-tfidf-1" href="../hunch_net-2006/hunch_net-2006-12-06-The_Spam_Problem.html">223 hunch net-2006-12-06-The Spam Problem</a></p>
<p>Introduction: TheNew York Timeshas an article on thegrowth of spam. Interesting facts
include: 9/10 of all email is spam, spam source identification is nearly
useless due to botnet spam senders, and image based spam (emails which consist
of an image only) are on the growth.Estimates of the cost of spam are almost
certainly far to low, because they do not account for the cost in time lost by
people.The image based spam which is currently penetrating many filters should
be catchable with a more sophisticated application of machine learning
technology. For the spam I see, the rendered images come in only a few
formats, which would be easy to recognize via a support vector machine (with
RBF kernel), neural network, or even nearest-neighbor architecture. The
mechanics of setting this up to run efficiently is the only real challenge.
This is the next step in the spam war.The response to this system is to make
the image based spam even more random. We should (essentially) expect to
seeCaptchaspam, and our</p><p>2 0.3080276 <a title="223-tfidf-2" href="../hunch_net-2009/hunch_net-2009-08-16-Centmail_comments.html">367 hunch net-2009-08-16-Centmail comments</a></p>
<p>Introduction: Centmailis a scheme which makes charity donations have a secondary value, as a
stamp for email. When discussed onnewscientist,slashdot, and others, some of
the comments make the academic review process appear thoughtful. Some
prominent fallacies are:Costing money fallacy. Some commenters appear to
believe the system charges money per email. Instead, the basic idea is that
users get an extra benefit from donations to a charity and participation is
strictly voluntary. The solution to this fallacy is simply readingthe
details.Single solution fallacy. Some commenters seem to think this is
proposed as a complete solution to spam, and since not everyone will opt to
participate, it won't work. But a complete solution is not at all necessary or
even possible given theflag-day problem. Deployed machine learning systems for
fighting spam are great at taking advantage of a partial solution. The
solution to this fallacy is learning about machine learning. In the current
state of affairs, informed</p><p>3 0.18148378 <a title="223-tfidf-3" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>Introduction: How do you create an optimal environment for research? Here are some essential
ingredients that I see.Stability. University-based research is relatively good
at this. On any particular day, researchers face choices in what they will
work on. A very common tradeoff is between:easy smalldifficult bigFor
researchers without stability, the 'easy small' option wins. This is often
"ok"--a series of incremental improvements on the state of the art can add up
to something very beneficial. However, it misses one of the big potentials of
research: finding entirely new and better ways of doing things.Stability comes
in many forms. The prototypical example is tenure at a university--a tenured
professor is almost imposssible to fire which means that the professor has the
freedom to consider far horizon activities. An iron-clad guarantee of a
paycheck is not necessary--industrial research labs have succeeded well with
research positions of indefinite duration. Atnt research was a great example
of th</p><p>4 0.12450218 <a title="223-tfidf-4" href="../hunch_net-2005/hunch_net-2005-02-20-At_One_Month.html">25 hunch net-2005-02-20-At One Month</a></p>
<p>Introduction: This is near the one month point, so it seems appropriate to consider meta-
issues for the moment.The number of posts is a bit over 20.The number of
people speaking up in discussions is about 10.The number of people viewing the
site is somewhat more than 100.I am (naturally) dissatisfied with many
things.Many of thepotential useshaven't been realized. This is partly a matter
of opportunity (no conferences in the last month), partly a matter of will (no
open problems because it's hard to give them up), and partly a matter of
tradition. In academia, there is a strong tradition of trying to get
everything perfectly right before presentation. This is somewhat contradictory
to the nature of making many posts, and it's definitely contradictory to the
idea of doing "public research". If that sort of idea is to pay off, it must
be significantly more succesful than previous methods. In an effort to
continue experimenting, I'm going to use the next week as "open problems
week".Spam is a problem.</p><p>5 0.10682716 <a title="223-tfidf-5" href="../hunch_net-2005/hunch_net-2005-02-15-ESPgame_and_image_labeling.html">20 hunch net-2005-02-15-ESPgame and image labeling</a></p>
<p>Introduction: Luis von Ahnhas been running theespgamefor awhile now. The espgame provides a
picture to two randomly paired people across the web, and asks them to agree
on a label. It hasn't managed to label the web yet, but it has produced alarge
datasetof (image, label) pairs. I organized the dataset so you couldexplore
the implied bipartite graph(requires much bandwidth).Relative to other image
datasets, this one is quite large--67000 images, 358,000 labels (average of
5/image with variation from 1 to 19), and 22,000 unique labels (one every 3
images). The dataset is also very 'natural', consisting of images spidered
from the internet. The multiple label characteristic is intriguing because
'learning to learn' and metalearning techniques may be applicable. The
'natural' quality means that this dataset varies greatly in difficulty from
easy (predicting "red") to hard (predicting "funny") and potentially more
rewarding to tackle.The open problem here is, of course, to make an internet
image labelin</p><p>6 0.098524526 <a title="223-tfidf-6" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>7 0.087593876 <a title="223-tfidf-7" href="../hunch_net-2006/hunch_net-2006-02-27-The_Peekaboom_Dataset.html">159 hunch net-2006-02-27-The Peekaboom Dataset</a></p>
<p>8 0.068864264 <a title="223-tfidf-8" href="../hunch_net-2010/hunch_net-2010-06-20-2010_ICML_discussion_site.html">401 hunch net-2010-06-20-2010 ICML discussion site</a></p>
<p>9 0.065980479 <a title="223-tfidf-9" href="../hunch_net-2006/hunch_net-2006-07-12-Who_is_having_visa_problems_reaching_US_conferences%3F.html">195 hunch net-2006-07-12-Who is having visa problems reaching US conferences?</a></p>
<p>10 0.063309677 <a title="223-tfidf-10" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>11 0.057121929 <a title="223-tfidf-11" href="../hunch_net-2007/hunch_net-2007-08-25-The_Privacy_Problem.html">260 hunch net-2007-08-25-The Privacy Problem</a></p>
<p>12 0.05579339 <a title="223-tfidf-12" href="../hunch_net-2006/hunch_net-2006-11-20-Context_and_the_calculation_misperception.html">218 hunch net-2006-11-20-Context and the calculation misperception</a></p>
<p>13 0.05557904 <a title="223-tfidf-13" href="../hunch_net-2007/hunch_net-2007-10-14-NIPS_workshp%3A_Learning_Problem_Design.html">265 hunch net-2007-10-14-NIPS workshp: Learning Problem Design</a></p>
<p>14 0.052180048 <a title="223-tfidf-14" href="../hunch_net-2006/hunch_net-2006-01-30-Should_the_Input_Representation_be_a_Vector%3F.html">152 hunch net-2006-01-30-Should the Input Representation be a Vector?</a></p>
<p>15 0.049618866 <a title="223-tfidf-15" href="../hunch_net-2006/hunch_net-2006-09-12-Incentive_Compatible_Reviewing.html">207 hunch net-2006-09-12-Incentive Compatible Reviewing</a></p>
<p>16 0.048746508 <a title="223-tfidf-16" href="../hunch_net-2012/hunch_net-2012-03-06-COLT-ICML_Open_Questions_and_ICML_Instructions.html">458 hunch net-2012-03-06-COLT-ICML Open Questions and ICML Instructions</a></p>
<p>17 0.047679286 <a title="223-tfidf-17" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>18 0.046460796 <a title="223-tfidf-18" href="../hunch_net-2005/hunch_net-2005-08-01-Peekaboom.html">99 hunch net-2005-08-01-Peekaboom</a></p>
<p>19 0.045176677 <a title="223-tfidf-19" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>20 0.044197213 <a title="223-tfidf-20" href="../hunch_net-2005/hunch_net-2005-08-11-Why_Manifold-Based_Dimension_Reduction_Techniques%3F.html">102 hunch net-2005-08-11-Why Manifold-Based Dimension Reduction Techniques?</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.117), (1, 0.005), (2, 0.062), (3, -0.027), (4, 0.015), (5, 0.031), (6, -0.011), (7, 0.01), (8, 0.078), (9, 0.063), (10, 0.022), (11, 0.064), (12, 0.01), (13, 0.044), (14, -0.001), (15, -0.092), (16, 0.005), (17, 0.031), (18, -0.083), (19, 0.088), (20, 0.076), (21, 0.075), (22, -0.044), (23, -0.102), (24, -0.101), (25, 0.047), (26, 0.075), (27, -0.139), (28, -0.002), (29, -0.013), (30, 0.004), (31, 0.059), (32, 0.145), (33, 0.035), (34, -0.064), (35, 0.121), (36, -0.022), (37, 0.028), (38, -0.028), (39, -0.095), (40, -0.046), (41, 0.072), (42, -0.043), (43, 0.066), (44, -0.02), (45, 0.024), (46, 0.02), (47, 0.075), (48, 0.111), (49, 0.001)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96058148 <a title="223-lsi-1" href="../hunch_net-2006/hunch_net-2006-12-06-The_Spam_Problem.html">223 hunch net-2006-12-06-The Spam Problem</a></p>
<p>Introduction: TheNew York Timeshas an article on thegrowth of spam. Interesting facts
include: 9/10 of all email is spam, spam source identification is nearly
useless due to botnet spam senders, and image based spam (emails which consist
of an image only) are on the growth.Estimates of the cost of spam are almost
certainly far to low, because they do not account for the cost in time lost by
people.The image based spam which is currently penetrating many filters should
be catchable with a more sophisticated application of machine learning
technology. For the spam I see, the rendered images come in only a few
formats, which would be easy to recognize via a support vector machine (with
RBF kernel), neural network, or even nearest-neighbor architecture. The
mechanics of setting this up to run efficiently is the only real challenge.
This is the next step in the spam war.The response to this system is to make
the image based spam even more random. We should (essentially) expect to
seeCaptchaspam, and our</p><p>2 0.8308084 <a title="223-lsi-2" href="../hunch_net-2009/hunch_net-2009-08-16-Centmail_comments.html">367 hunch net-2009-08-16-Centmail comments</a></p>
<p>Introduction: Centmailis a scheme which makes charity donations have a secondary value, as a
stamp for email. When discussed onnewscientist,slashdot, and others, some of
the comments make the academic review process appear thoughtful. Some
prominent fallacies are:Costing money fallacy. Some commenters appear to
believe the system charges money per email. Instead, the basic idea is that
users get an extra benefit from donations to a charity and participation is
strictly voluntary. The solution to this fallacy is simply readingthe
details.Single solution fallacy. Some commenters seem to think this is
proposed as a complete solution to spam, and since not everyone will opt to
participate, it won't work. But a complete solution is not at all necessary or
even possible given theflag-day problem. Deployed machine learning systems for
fighting spam are great at taking advantage of a partial solution. The
solution to this fallacy is learning about machine learning. In the current
state of affairs, informed</p><p>3 0.60541725 <a title="223-lsi-3" href="../hunch_net-2010/hunch_net-2010-06-20-2010_ICML_discussion_site.html">401 hunch net-2010-06-20-2010 ICML discussion site</a></p>
<p>Introduction: A substantial difficulty with the 2009 and 2008ICML discussion systemwas a
communication vacuum, where authors were not informed of comments, and
commenters were not informed of responses to their comments without explicit
monitoring.Mark Reidhas setup anew discussion system for 2010with the goal of
addressing this.Mark didn't want to make it to intrusive, so you must opt-in.
As an author,find your paperand "Subscribe by email" to the comments. As a
commenter, you have the option of providing an email for follow-up
notification.</p><p>4 0.55787665 <a title="223-lsi-4" href="../hunch_net-2005/hunch_net-2005-02-15-ESPgame_and_image_labeling.html">20 hunch net-2005-02-15-ESPgame and image labeling</a></p>
<p>Introduction: Luis von Ahnhas been running theespgamefor awhile now. The espgame provides a
picture to two randomly paired people across the web, and asks them to agree
on a label. It hasn't managed to label the web yet, but it has produced alarge
datasetof (image, label) pairs. I organized the dataset so you couldexplore
the implied bipartite graph(requires much bandwidth).Relative to other image
datasets, this one is quite large--67000 images, 358,000 labels (average of
5/image with variation from 1 to 19), and 22,000 unique labels (one every 3
images). The dataset is also very 'natural', consisting of images spidered
from the internet. The multiple label characteristic is intriguing because
'learning to learn' and metalearning techniques may be applicable. The
'natural' quality means that this dataset varies greatly in difficulty from
easy (predicting "red") to hard (predicting "funny") and potentially more
rewarding to tackle.The open problem here is, of course, to make an internet
image labelin</p><p>5 0.51868594 <a title="223-lsi-5" href="../hunch_net-2006/hunch_net-2006-02-27-The_Peekaboom_Dataset.html">159 hunch net-2006-02-27-The Peekaboom Dataset</a></p>
<p>Introduction: Luis von Ahn'sPeekaboom projecthas yieldeddata(830MB).Peekaboom is the second
attempt (afterEspgame) to produce a dataset which is useful for learning to
solve vision problems based on voluntary game play. As a second attempt, it is
meant to address all of the shortcomings of the first attempt. In
particular:The locations of specific objects are provided by the data.The data
collection is far more complete and extensive.The data consists of:The source
images. (1 file per image, just short of 60K images.)The in-game events. (1
file per image, in a lispy syntax.)A description of the event language.There
is a great deal of very specific and relevant data here so the hope that this
will help solve vision problems seems quite reasonable.</p><p>6 0.49489215 <a title="223-lsi-6" href="../hunch_net-2011/hunch_net-2011-10-03-Monday_announcements.html">446 hunch net-2011-10-03-Monday announcements</a></p>
<p>7 0.4918116 <a title="223-lsi-7" href="../hunch_net-2005/hunch_net-2005-02-20-At_One_Month.html">25 hunch net-2005-02-20-At One Month</a></p>
<p>8 0.48968565 <a title="223-lsi-8" href="../hunch_net-2005/hunch_net-2005-08-01-Peekaboom.html">99 hunch net-2005-08-01-Peekaboom</a></p>
<p>9 0.40413558 <a title="223-lsi-9" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>10 0.37699753 <a title="223-lsi-10" href="../hunch_net-2005/hunch_net-2005-08-11-Why_Manifold-Based_Dimension_Reduction_Techniques%3F.html">102 hunch net-2005-08-11-Why Manifold-Based Dimension Reduction Techniques?</a></p>
<p>11 0.35795933 <a title="223-lsi-11" href="../hunch_net-2005/hunch_net-2005-09-20-Workshop_Proposal%3A_Atomic_Learning.html">114 hunch net-2005-09-20-Workshop Proposal: Atomic Learning</a></p>
<p>12 0.3470186 <a title="223-lsi-12" href="../hunch_net-2005/hunch_net-2005-10-13-Site_tweak.html">122 hunch net-2005-10-13-Site tweak</a></p>
<p>13 0.34581786 <a title="223-lsi-13" href="../hunch_net-2007/hunch_net-2007-04-28-The_Coming_Patent_Apocalypse.html">241 hunch net-2007-04-28-The Coming Patent Apocalypse</a></p>
<p>14 0.3432546 <a title="223-lsi-14" href="../hunch_net-2006/hunch_net-2006-07-12-Who_is_having_visa_problems_reaching_US_conferences%3F.html">195 hunch net-2006-07-12-Who is having visa problems reaching US conferences?</a></p>
<p>15 0.34255973 <a title="223-lsi-15" href="../hunch_net-2007/hunch_net-2007-11-05-CMU_wins_DARPA_Urban_Challenge.html">271 hunch net-2007-11-05-CMU wins DARPA Urban Challenge</a></p>
<p>16 0.33006427 <a title="223-lsi-16" href="../hunch_net-2009/hunch_net-2009-04-21-Interesting_Presentations_at_Snowbird.html">349 hunch net-2009-04-21-Interesting Presentations at Snowbird</a></p>
<p>17 0.3277508 <a title="223-lsi-17" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>18 0.32629102 <a title="223-lsi-18" href="../hunch_net-2005/hunch_net-2005-09-05-Site_Update.html">107 hunch net-2005-09-05-Site Update</a></p>
<p>19 0.32505956 <a title="223-lsi-19" href="../hunch_net-2009/hunch_net-2009-02-16-KDNuggets.html">342 hunch net-2009-02-16-KDNuggets</a></p>
<p>20 0.32478556 <a title="223-lsi-20" href="../hunch_net-2009/hunch_net-2009-05-17-Server_Update.html">354 hunch net-2009-05-17-Server Update</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(6, 0.444), (42, 0.152), (45, 0.078), (68, 0.041), (74, 0.084), (82, 0.016), (88, 0.016), (95, 0.041)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.88339323 <a title="223-lda-1" href="../hunch_net-2006/hunch_net-2006-12-06-The_Spam_Problem.html">223 hunch net-2006-12-06-The Spam Problem</a></p>
<p>Introduction: TheNew York Timeshas an article on thegrowth of spam. Interesting facts
include: 9/10 of all email is spam, spam source identification is nearly
useless due to botnet spam senders, and image based spam (emails which consist
of an image only) are on the growth.Estimates of the cost of spam are almost
certainly far to low, because they do not account for the cost in time lost by
people.The image based spam which is currently penetrating many filters should
be catchable with a more sophisticated application of machine learning
technology. For the spam I see, the rendered images come in only a few
formats, which would be easy to recognize via a support vector machine (with
RBF kernel), neural network, or even nearest-neighbor architecture. The
mechanics of setting this up to run efficiently is the only real challenge.
This is the next step in the spam war.The response to this system is to make
the image based spam even more random. We should (essentially) expect to
seeCaptchaspam, and our</p><p>2 0.87506837 <a title="223-lda-2" href="../hunch_net-2008/hunch_net-2008-10-01-NIPS_2008_workshop_on_%26%238216%3BLearning_over_Empirical_Hypothesis_Spaces%26%238217%3B.html">319 hunch net-2008-10-01-NIPS 2008 workshop on &#8216;Learning over Empirical Hypothesis Spaces&#8217;</a></p>
<p>Introduction: This workshop asks for insights how far we may/can push the theoretical
boundary of using data in the design of learning machines. Can we express our
classification rule in terms of the sample, or do we have to stick to a core
assumption of classical statistical learning theory, namely that the
hypothesis space is to be defined independent from the sample? This workshop
is particularly interested in - but not restricted to - the 'luckiness
framework' and the recently introduced notion of 'compatibility functions' in
a semi-supervised learning context (more information can be found
athttp://www.kuleuven.be/wehys).</p><p>3 0.87230235 <a title="223-lda-3" href="../hunch_net-2005/hunch_net-2005-06-17-Reopening_RL-%3EClassification.html">82 hunch net-2005-06-17-Reopening RL->Classification</a></p>
<p>Introduction: In research, it's often the case that solving a problem helps you realize that
it wasn't the right problem to solve. This is the case for the "reduce RL to
classification" problem with the solution hinted athereand turned into a
paperhere.The essential difficulty is that the method of stating and analyzing
reductions ends up being nonalgorithmic (unlike previous reductions) unless
you work with learning from teleoperated robots asGreg Grudicdoes. The
difficulty here is due to the reduction being dependent on the optimal policy
(which a human teleoperator might simulate, but which is otherwise
unavailable).So, thisproblemis "open" again with the caveat that this time we
want a more algorithmic solution.Whether or not this is feasible at all is
still unclear and evidence in either direction would greatly interest me. A
positive answer might have many practical implications in the long run.</p><p>4 0.86407942 <a title="223-lda-4" href="../hunch_net-2009/hunch_net-2009-02-16-KDNuggets.html">342 hunch net-2009-02-16-KDNuggets</a></p>
<p>Introduction: Eric Zaetsch points outKDNuggetswhich is a well-developed mailing list/news
site with aKDDflavor. This might particularly interest people looking for
industrial jobs in machine learning, as the mailing list has many such.</p><p>5 0.84671229 <a title="223-lda-5" href="../hunch_net-2005/hunch_net-2005-10-07-On-line_learning_of_regular_decision_rules.html">118 hunch net-2005-10-07-On-line learning of regular decision rules</a></p>
<p>Introduction: Many decision problems can be represented in the formFORn=1,2,â&euro;Ś:-- Reality
chooses a datumxn.-- Decision Maker chooses his decisiondn.-- Reality chooses
an observationyn.-- Decision Maker suffers lossL(yn,dn).END FOR.The
observationyncan be, for example, tomorrow's stock price and the decisiondnthe
number of shares Decision Maker chooses to buy. The datumxnideally contains
all information that might be relevant in making this decision. We do not want
to assume anything about the way Reality generates the observations and
data.Suppose there is a good and not too complex decision ruleDmapping each
datumxto a decisionD(x). Can we perform as well, or almost as well, asD,
without knowing it? This is essentially a special case of the problem ofon-
line learning.This is a simple result of this kind. Suppose the dataxnare
taken from [0,1] andL(y,d)=|y-d|. A norm ||h|| of a functionhon [0,1] is
defined by||h||2= (Integral01h(t)dt)2+ Integral01(h'(t))2dt.Decision Maker has
a strategy that guaran</p><p>6 0.84045362 <a title="223-lda-6" href="../hunch_net-2005/hunch_net-2005-06-22-Languages__of_Learning.html">84 hunch net-2005-06-22-Languages  of Learning</a></p>
<p>7 0.78733045 <a title="223-lda-7" href="../hunch_net-2008/hunch_net-2008-02-10-Complexity_Illness.html">288 hunch net-2008-02-10-Complexity Illness</a></p>
<p>8 0.73920733 <a title="223-lda-8" href="../hunch_net-2005/hunch_net-2005-08-08-Apprenticeship_Reinforcement_Learning_for_Control.html">101 hunch net-2005-08-08-Apprenticeship Reinforcement Learning for Control</a></p>
<p>9 0.57200301 <a title="223-lda-9" href="../hunch_net-2005/hunch_net-2005-02-15-ESPgame_and_image_labeling.html">20 hunch net-2005-02-15-ESPgame and image labeling</a></p>
<p>10 0.50055504 <a title="223-lda-10" href="../hunch_net-2006/hunch_net-2006-09-28-Programming_Languages_for_Machine_Learning_Implementations.html">210 hunch net-2006-09-28-Programming Languages for Machine Learning Implementations</a></p>
<p>11 0.45283541 <a title="223-lda-11" href="../hunch_net-2005/hunch_net-2005-02-25-Problem%3A_Online_Learning.html">28 hunch net-2005-02-25-Problem: Online Learning</a></p>
<p>12 0.45143491 <a title="223-lda-12" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>13 0.45095015 <a title="223-lda-13" href="../hunch_net-2010/hunch_net-2010-05-02-What%26%238217%3Bs_the_difference_between_gambling_and_rewarding_good_prediction%3F.html">397 hunch net-2010-05-02-What&#8217;s the difference between gambling and rewarding good prediction?</a></p>
<p>14 0.44957352 <a title="223-lda-14" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<p>15 0.44913656 <a title="223-lda-15" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>16 0.447391 <a title="223-lda-16" href="../hunch_net-2006/hunch_net-2006-06-30-ICML_papers.html">188 hunch net-2006-06-30-ICML papers</a></p>
<p>17 0.44224253 <a title="223-lda-17" href="../hunch_net-2009/hunch_net-2009-07-11-Interesting_papers_at_KDD.html">364 hunch net-2009-07-11-Interesting papers at KDD</a></p>
<p>18 0.43560782 <a title="223-lda-18" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>19 0.43553504 <a title="223-lda-19" href="../hunch_net-2005/hunch_net-2005-02-17-Learning_Research_Programs.html">21 hunch net-2005-02-17-Learning Research Programs</a></p>
<p>20 0.4332594 <a title="223-lda-20" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
