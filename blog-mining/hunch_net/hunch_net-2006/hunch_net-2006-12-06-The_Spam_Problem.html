<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>223 hunch net-2006-12-06-The Spam Problem</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-223" href="#">hunch_net-2006-223</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>223 hunch net-2006-12-06-The Spam Problem</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-223-html" href="http://hunch.net/?p=243">html</a></p><p>Introduction: The  New York Times  has an article on the  growth of spam .  Interesting facts include: 9/10 of all email is spam, spam source identification is nearly useless due to botnet spam senders, and image based spam (emails which consist of an image only) are on the growth.
 
Estimates of the cost of spam are almost certainly far to low, because they do not account for the cost in time lost by people.
 
The image based spam which is currently penetrating many filters should be catchable with a more sophisticated application of machine learning technology.  For the spam I see, the rendered images come in only a few formats, which would be easy to recognize via a support vector machine (with RBF kernel), neural network, or even nearest-neighbor architecture.  The mechanics of setting this up to run efficiently is the only real challenge.  This is the next step in the spam war.
 
The response to this system is to make the image based spam even more random.  We should (essentially) expect to see</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The  New York Times  has an article on the  growth of spam . [sent-1, score-0.888]
</p><p>2 Interesting facts include: 9/10 of all email is spam, spam source identification is nearly useless due to botnet spam senders, and image based spam (emails which consist of an image only) are on the growth. [sent-2, score-3.141]
</p><p>3 Estimates of the cost of spam are almost certainly far to low, because they do not account for the cost in time lost by people. [sent-3, score-1.0]
</p><p>4 The image based spam which is currently penetrating many filters should be catchable with a more sophisticated application of machine learning technology. [sent-4, score-1.15]
</p><p>5 For the spam I see, the rendered images come in only a few formats, which would be easy to recognize via a support vector machine (with RBF kernel), neural network, or even nearest-neighbor architecture. [sent-5, score-0.943]
</p><p>6 The mechanics of setting this up to run efficiently is the only real challenge. [sent-6, score-0.105]
</p><p>7 The response to this system is to make the image based spam even more random. [sent-8, score-1.108]
</p><p>8 We should (essentially) expect to see  Captcha  spam, and our inability to recognize captcha spam should persist as long as the vision problem is not solved. [sent-9, score-1.11]
</p><p>9 This hopefully degrades the value of spam to the spammers, but it may not make the value of spam nonzero. [sent-10, score-1.672]
</p><p>10 One simple economic solution is to transfer from first time sender to receiver a small amount (10 cents? [sent-12, score-0.334]
</p><p>11 If the receiver classifies the email as spam then the charge repeats on the next receipt, and otherwise it goes away. [sent-14, score-1.388]
</p><p>12 There are several difficulties with this approach: How do you change a huge system in heavy use which no one controls? [sent-15, score-0.129]
</p><p>13 For example, we could extend the mail protocol to include a payment system (using the â&euro;&oelig;X-â&euro;? [sent-18, score-0.441]
</p><p>14 lines) and use the existence of a payment as a feature in existing spam-or-not prediction systems. [sent-19, score-0.382]
</p><p>15 Over time, this feature may become the most useful feature encouraging every legitimate email user to offer a small payment with the first email to a recipient. [sent-20, score-0.962]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('spam', 0.786), ('payment', 0.236), ('image', 0.185), ('email', 0.166), ('captcha', 0.158), ('receiver', 0.14), ('recognize', 0.105), ('feature', 0.09), ('classifies', 0.07), ('extend', 0.07), ('cents', 0.07), ('consist', 0.07), ('rbf', 0.07), ('verifiable', 0.07), ('based', 0.069), ('system', 0.068), ('include', 0.067), ('charge', 0.065), ('emails', 0.065), ('cost', 0.064), ('mechanics', 0.061), ('inability', 0.061), ('estimates', 0.061), ('heavy', 0.061), ('next', 0.059), ('offer', 0.058), ('identification', 0.058), ('encouraging', 0.058), ('repeats', 0.058), ('controls', 0.056), ('existence', 0.056), ('sophisticated', 0.056), ('growth', 0.056), ('lists', 0.054), ('mailing', 0.054), ('formats', 0.054), ('filters', 0.054), ('images', 0.052), ('economic', 0.052), ('transfer', 0.052), ('value', 0.05), ('useless', 0.05), ('user', 0.05), ('lines', 0.05), ('small', 0.048), ('article', 0.046), ('goes', 0.044), ('efficiently', 0.044), ('lost', 0.044), ('time', 0.042)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000004 <a title="223-tfidf-1" href="../hunch_net-2006/hunch_net-2006-12-06-The_Spam_Problem.html">223 hunch net-2006-12-06-The Spam Problem</a></p>
<p>Introduction: The  New York Times  has an article on the  growth of spam .  Interesting facts include: 9/10 of all email is spam, spam source identification is nearly useless due to botnet spam senders, and image based spam (emails which consist of an image only) are on the growth.
 
Estimates of the cost of spam are almost certainly far to low, because they do not account for the cost in time lost by people.
 
The image based spam which is currently penetrating many filters should be catchable with a more sophisticated application of machine learning technology.  For the spam I see, the rendered images come in only a few formats, which would be easy to recognize via a support vector machine (with RBF kernel), neural network, or even nearest-neighbor architecture.  The mechanics of setting this up to run efficiently is the only real challenge.  This is the next step in the spam war.
 
The response to this system is to make the image based spam even more random.  We should (essentially) expect to see</p><p>2 0.27777895 <a title="223-tfidf-2" href="../hunch_net-2009/hunch_net-2009-08-16-Centmail_comments.html">367 hunch net-2009-08-16-Centmail comments</a></p>
<p>Introduction: Centmail  is a scheme which makes charity donations have a secondary value, as a stamp for email.  When discussed on  newscientist ,  slashdot , and others, some of the comments make the academic review process appear thoughtful   .  Some prominent fallacies are:
  
 Costing money fallacy.  Some commenters appear to believe the system charges money per email.  Instead, the basic idea is that users get an extra benefit from donations to a charity and participation is strictly voluntary.  The solution to this fallacy is simply reading  the details . 
 Single solution fallacy.  Some commenters seem to think this is proposed as a complete solution to spam, and since not everyone will opt to participate, it won’t work.  But a complete solution is not at all necessary or even possible given the  flag-day problem .  Deployed machine learning systems for fighting spam are great at taking advantage of a partial solution.  The solution to this fallacy is learning about machine learning.  In the</p><p>3 0.18899602 <a title="223-tfidf-3" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>Introduction: How do you create an optimal environment for research?  Here are some essential ingredients that I see.  
  
  Stability .  University-based research is relatively good at this.  On any particular day, researchers face choices in what they will work on.  A very common tradeoff is between:
 
 easy small 
 difficult big 
 

For researchers without stability, the ‘easy small’ option wins.  This is often “ok”—a series of incremental improvements on the state of the art can add up to something very beneficial.  However, it misses one of the big potentials of research: finding entirely new and better ways of doing things.


Stability comes in many forms.  The prototypical example is tenure at a university—a tenured professor is almost imposssible to fire which means that the professor has the freedom to consider far horizon activities.  An iron-clad guarantee of a paycheck is not necessary—industrial research labs have succeeded well with research positions of indefinite duration.  Atnt rese</p><p>4 0.16365603 <a title="223-tfidf-4" href="../hunch_net-2005/hunch_net-2005-02-20-At_One_Month.html">25 hunch net-2005-02-20-At One Month</a></p>
<p>Introduction: This is near the one month point, so it seems appropriate to consider meta-issues for the moment.
 
The number of posts is a bit over 20. 
The number of people speaking up in discussions is about 10. 
The number of people viewing the site is somewhat more than 100.
 
I am (naturally) dissatisfied with many things.
  
 Many of the  potential uses  haven’t been realized.  This is partly a matter of opportunity (no conferences in the last month), partly a matter of will (no open problems because it’s hard to give them up), and partly a matter of tradition.  In academia, there is a strong tradition of trying to get everything perfectly right before presentation.  This is somewhat contradictory to the nature of making many posts, and it’s definitely contradictory to the idea of doing “public research”.  If that sort of idea is to pay off, it must be significantly more succesful than previous methods. In an effort to continue experimenting, I’m going to use the next week as “open problems we</p><p>5 0.1266275 <a title="223-tfidf-5" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>Introduction: At the  last ICML ,  Tom Dietterich  asked me to look into systems for commenting on papers.  I’ve been slow getting to this, but it’s relevant now.
 
The essential observation is that we now have many tools for online collaboration, but they are not yet much used in academic research.  If we can find the right way to use them, then perhaps great things might happen, with extra kudos to the first conference that manages to really create an online community.  Various conferences have been poking at this.  For example,  UAI has setup a wiki , COLT has  started using   Joomla , with some dynamic content, and AAAI has been setting up a “ student blog “.  Similarly,  Dinoj Surendran  setup a twiki for the  Chicago Machine Learning Summer School , which was quite useful for coordinating events and other things.
 
I believe the most important thing is a willingness to experiment.  A good place to start seems to be enhancing existing conference websites.  For example, the  ICML 2007 papers pag</p><p>6 0.10315313 <a title="223-tfidf-6" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>7 0.091107845 <a title="223-tfidf-7" href="../hunch_net-2005/hunch_net-2005-02-15-ESPgame_and_image_labeling.html">20 hunch net-2005-02-15-ESPgame and image labeling</a></p>
<p>8 0.086426474 <a title="223-tfidf-8" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>9 0.069833413 <a title="223-tfidf-9" href="../hunch_net-2006/hunch_net-2006-02-27-The_Peekaboom_Dataset.html">159 hunch net-2006-02-27-The Peekaboom Dataset</a></p>
<p>10 0.063494496 <a title="223-tfidf-10" href="../hunch_net-2010/hunch_net-2010-06-20-2010_ICML_discussion_site.html">401 hunch net-2010-06-20-2010 ICML discussion site</a></p>
<p>11 0.057952624 <a title="223-tfidf-11" href="../hunch_net-2006/hunch_net-2006-07-12-Who_is_having_visa_problems_reaching_US_conferences%3F.html">195 hunch net-2006-07-12-Who is having visa problems reaching US conferences?</a></p>
<p>12 0.054639608 <a title="223-tfidf-12" href="../hunch_net-2007/hunch_net-2007-08-25-The_Privacy_Problem.html">260 hunch net-2007-08-25-The Privacy Problem</a></p>
<p>13 0.054174762 <a title="223-tfidf-13" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>14 0.050317552 <a title="223-tfidf-14" href="../hunch_net-2007/hunch_net-2007-10-14-NIPS_workshp%3A_Learning_Problem_Design.html">265 hunch net-2007-10-14-NIPS workshp: Learning Problem Design</a></p>
<p>15 0.049756292 <a title="223-tfidf-15" href="../hunch_net-2009/hunch_net-2009-04-21-Interesting_Presentations_at_Snowbird.html">349 hunch net-2009-04-21-Interesting Presentations at Snowbird</a></p>
<p>16 0.04917042 <a title="223-tfidf-16" href="../hunch_net-2006/hunch_net-2006-11-20-Context_and_the_calculation_misperception.html">218 hunch net-2006-11-20-Context and the calculation misperception</a></p>
<p>17 0.04741523 <a title="223-tfidf-17" href="../hunch_net-2006/hunch_net-2006-01-30-Should_the_Input_Representation_be_a_Vector%3F.html">152 hunch net-2006-01-30-Should the Input Representation be a Vector?</a></p>
<p>18 0.044979908 <a title="223-tfidf-18" href="../hunch_net-2006/hunch_net-2006-09-12-Incentive_Compatible_Reviewing.html">207 hunch net-2006-09-12-Incentive Compatible Reviewing</a></p>
<p>19 0.043223165 <a title="223-tfidf-19" href="../hunch_net-2006/hunch_net-2006-07-09-The_Stock_Prediction_Machine_Learning_Problem.html">193 hunch net-2006-07-09-The Stock Prediction Machine Learning Problem</a></p>
<p>20 0.042016711 <a title="223-tfidf-20" href="../hunch_net-2005/hunch_net-2005-08-11-Why_Manifold-Based_Dimension_Reduction_Techniques%3F.html">102 hunch net-2005-08-11-Why Manifold-Based Dimension Reduction Techniques?</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.114), (1, 0.001), (2, -0.05), (3, 0.052), (4, -0.021), (5, 0.011), (6, -0.018), (7, -0.001), (8, 0.004), (9, -0.018), (10, -0.095), (11, -0.031), (12, -0.055), (13, 0.011), (14, 0.009), (15, 0.01), (16, -0.115), (17, -0.067), (18, -0.024), (19, 0.149), (20, -0.062), (21, -0.015), (22, -0.014), (23, -0.041), (24, 0.031), (25, 0.049), (26, 0.032), (27, 0.042), (28, 0.029), (29, -0.086), (30, 0.017), (31, 0.005), (32, -0.075), (33, 0.029), (34, 0.031), (35, 0.083), (36, 0.044), (37, -0.07), (38, -0.013), (39, -0.056), (40, -0.007), (41, -0.015), (42, -0.02), (43, -0.022), (44, -0.066), (45, -0.162), (46, -0.096), (47, 0.024), (48, -0.104), (49, -0.04)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96573192 <a title="223-lsi-1" href="../hunch_net-2006/hunch_net-2006-12-06-The_Spam_Problem.html">223 hunch net-2006-12-06-The Spam Problem</a></p>
<p>Introduction: The  New York Times  has an article on the  growth of spam .  Interesting facts include: 9/10 of all email is spam, spam source identification is nearly useless due to botnet spam senders, and image based spam (emails which consist of an image only) are on the growth.
 
Estimates of the cost of spam are almost certainly far to low, because they do not account for the cost in time lost by people.
 
The image based spam which is currently penetrating many filters should be catchable with a more sophisticated application of machine learning technology.  For the spam I see, the rendered images come in only a few formats, which would be easy to recognize via a support vector machine (with RBF kernel), neural network, or even nearest-neighbor architecture.  The mechanics of setting this up to run efficiently is the only real challenge.  This is the next step in the spam war.
 
The response to this system is to make the image based spam even more random.  We should (essentially) expect to see</p><p>2 0.8200649 <a title="223-lsi-2" href="../hunch_net-2009/hunch_net-2009-08-16-Centmail_comments.html">367 hunch net-2009-08-16-Centmail comments</a></p>
<p>Introduction: Centmail  is a scheme which makes charity donations have a secondary value, as a stamp for email.  When discussed on  newscientist ,  slashdot , and others, some of the comments make the academic review process appear thoughtful   .  Some prominent fallacies are:
  
 Costing money fallacy.  Some commenters appear to believe the system charges money per email.  Instead, the basic idea is that users get an extra benefit from donations to a charity and participation is strictly voluntary.  The solution to this fallacy is simply reading  the details . 
 Single solution fallacy.  Some commenters seem to think this is proposed as a complete solution to spam, and since not everyone will opt to participate, it won’t work.  But a complete solution is not at all necessary or even possible given the  flag-day problem .  Deployed machine learning systems for fighting spam are great at taking advantage of a partial solution.  The solution to this fallacy is learning about machine learning.  In the</p><p>3 0.58818495 <a title="223-lsi-3" href="../hunch_net-2005/hunch_net-2005-02-20-At_One_Month.html">25 hunch net-2005-02-20-At One Month</a></p>
<p>Introduction: This is near the one month point, so it seems appropriate to consider meta-issues for the moment.
 
The number of posts is a bit over 20. 
The number of people speaking up in discussions is about 10. 
The number of people viewing the site is somewhat more than 100.
 
I am (naturally) dissatisfied with many things.
  
 Many of the  potential uses  haven’t been realized.  This is partly a matter of opportunity (no conferences in the last month), partly a matter of will (no open problems because it’s hard to give them up), and partly a matter of tradition.  In academia, there is a strong tradition of trying to get everything perfectly right before presentation.  This is somewhat contradictory to the nature of making many posts, and it’s definitely contradictory to the idea of doing “public research”.  If that sort of idea is to pay off, it must be significantly more succesful than previous methods. In an effort to continue experimenting, I’m going to use the next week as “open problems we</p><p>4 0.48444048 <a title="223-lsi-4" href="../hunch_net-2005/hunch_net-2005-02-15-ESPgame_and_image_labeling.html">20 hunch net-2005-02-15-ESPgame and image labeling</a></p>
<p>Introduction: Luis von Ahn  has been running the  espgame  for awhile now.  The espgame provides a picture to two randomly paired people across the web, and asks them to agree on a label.  It hasn’t managed to label the web yet, but it has produced a  large dataset  of (image, label) pairs.  I organized the dataset so you could  explore the implied bipartite graph  (requires much bandwidth).
 
Relative to other image datasets, this one is quite large—67000 images, 358,000 labels (average of 5/image with variation from 1 to 19), and 22,000 unique labels (one every 3 images).  The dataset is also very ‘natural’, consisting of images spidered from the internet.  The multiple label characteristic is intriguing because ‘learning to learn’ and metalearning techniques may be applicable.  The ‘natural’ quality means that this dataset varies greatly in difficulty from easy (predicting “red”) to hard (predicting “funny”) and potentially more rewarding to tackle.
 
The open problem here is, of course, to make</p><p>5 0.4831962 <a title="223-lsi-5" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>Introduction: At the  last ICML ,  Tom Dietterich  asked me to look into systems for commenting on papers.  I’ve been slow getting to this, but it’s relevant now.
 
The essential observation is that we now have many tools for online collaboration, but they are not yet much used in academic research.  If we can find the right way to use them, then perhaps great things might happen, with extra kudos to the first conference that manages to really create an online community.  Various conferences have been poking at this.  For example,  UAI has setup a wiki , COLT has  started using   Joomla , with some dynamic content, and AAAI has been setting up a “ student blog “.  Similarly,  Dinoj Surendran  setup a twiki for the  Chicago Machine Learning Summer School , which was quite useful for coordinating events and other things.
 
I believe the most important thing is a willingness to experiment.  A good place to start seems to be enhancing existing conference websites.  For example, the  ICML 2007 papers pag</p><p>6 0.48100859 <a title="223-lsi-6" href="../hunch_net-2006/hunch_net-2006-07-12-Who_is_having_visa_problems_reaching_US_conferences%3F.html">195 hunch net-2006-07-12-Who is having visa problems reaching US conferences?</a></p>
<p>7 0.46269169 <a title="223-lsi-7" href="../hunch_net-2006/hunch_net-2006-02-27-The_Peekaboom_Dataset.html">159 hunch net-2006-02-27-The Peekaboom Dataset</a></p>
<p>8 0.42262223 <a title="223-lsi-8" href="../hunch_net-2009/hunch_net-2009-05-17-Server_Update.html">354 hunch net-2009-05-17-Server Update</a></p>
<p>9 0.41554987 <a title="223-lsi-9" href="../hunch_net-2010/hunch_net-2010-05-20-Google_Predict.html">399 hunch net-2010-05-20-Google Predict</a></p>
<p>10 0.40230802 <a title="223-lsi-10" href="../hunch_net-2005/hunch_net-2005-12-22-Yes_%2C_I_am_applying.html">142 hunch net-2005-12-22-Yes , I am applying</a></p>
<p>11 0.39487159 <a title="223-lsi-11" href="../hunch_net-2005/hunch_net-2005-08-01-Peekaboom.html">99 hunch net-2005-08-01-Peekaboom</a></p>
<p>12 0.39094701 <a title="223-lsi-12" href="../hunch_net-2005/hunch_net-2005-05-11-Visa_Casualties.html">69 hunch net-2005-05-11-Visa Casualties</a></p>
<p>13 0.38584027 <a title="223-lsi-13" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>14 0.38497406 <a title="223-lsi-14" href="../hunch_net-2006/hunch_net-2006-01-25-1_year.html">151 hunch net-2006-01-25-1 year</a></p>
<p>15 0.38284734 <a title="223-lsi-15" href="../hunch_net-2006/hunch_net-2006-01-30-Should_the_Input_Representation_be_a_Vector%3F.html">152 hunch net-2006-01-30-Should the Input Representation be a Vector?</a></p>
<p>16 0.3819803 <a title="223-lsi-16" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>17 0.37917811 <a title="223-lsi-17" href="../hunch_net-2010/hunch_net-2010-06-20-2010_ICML_discussion_site.html">401 hunch net-2010-06-20-2010 ICML discussion site</a></p>
<p>18 0.37511468 <a title="223-lsi-18" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>19 0.37255776 <a title="223-lsi-19" href="../hunch_net-2005/hunch_net-2005-12-09-Machine_Learning_Thoughts.html">137 hunch net-2005-12-09-Machine Learning Thoughts</a></p>
<p>20 0.3644127 <a title="223-lsi-20" href="../hunch_net-2009/hunch_net-2009-06-01-Multitask_Poisoning.html">358 hunch net-2009-06-01-Multitask Poisoning</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.023), (3, 0.046), (10, 0.027), (27, 0.181), (31, 0.327), (38, 0.018), (48, 0.013), (53, 0.047), (55, 0.1), (94, 0.064), (95, 0.018)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.97441757 <a title="223-lda-1" href="../hunch_net-2005/hunch_net-2005-10-08-We_have_a_winner.html">119 hunch net-2005-10-08-We have a winner</a></p>
<p>Introduction: The DARPA grandchallenge is a big contest for autonomous robot vehicle driving.  It was run once in 2004 for the first time and all teams did badly.  This year was notably different with the  Stanford  and  CMU  teams succesfully completing the course.  A number of details are  here  and  wikipedia has continuing coverage .
 
A formal winner hasnâ&euro;&trade;t been declared yet although Stanford completed the course quickest.  
 
The Stanford and CMU teams deserve a large round of applause as they have strongly demonstrated the feasibility of autonomous vehicles.
 
The good news for machine learning is that the Stanford team (at least) is using some machine learning techniques.</p><p>same-blog 2 0.85314739 <a title="223-lda-2" href="../hunch_net-2006/hunch_net-2006-12-06-The_Spam_Problem.html">223 hunch net-2006-12-06-The Spam Problem</a></p>
<p>Introduction: The  New York Times  has an article on the  growth of spam .  Interesting facts include: 9/10 of all email is spam, spam source identification is nearly useless due to botnet spam senders, and image based spam (emails which consist of an image only) are on the growth.
 
Estimates of the cost of spam are almost certainly far to low, because they do not account for the cost in time lost by people.
 
The image based spam which is currently penetrating many filters should be catchable with a more sophisticated application of machine learning technology.  For the spam I see, the rendered images come in only a few formats, which would be easy to recognize via a support vector machine (with RBF kernel), neural network, or even nearest-neighbor architecture.  The mechanics of setting this up to run efficiently is the only real challenge.  This is the next step in the spam war.
 
The response to this system is to make the image based spam even more random.  We should (essentially) expect to see</p><p>3 0.56161702 <a title="223-lda-3" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>Introduction: When thinking about how best to review papers, it seems helpful to have some conception of what good reviewing is.  As far as I can tell, this is almost always only discussed in the specific context of a paper (i.e. your rejected paper), or at most an area (i.e. what a “good paper” looks like for that area) rather than general principles.  Neither individual papers or areas are sufficiently general for a large conference—every paper differs in the details, and what if you want to build a new area and/or cross areas?
 
An unavoidable reason for reviewing is that the community of research is too large.  In particular, it is not possible for a researcher to read every paper which someone thinks might be of interest.  This reason for reviewing exists independent of constraints on rooms or scheduling formats of individual conferences.  Indeed, history suggests that physical constraints are relatively meaningless over the long term — growing conferences simply use more rooms and/or change fo</p><p>4 0.55751836 <a title="223-lda-4" href="../hunch_net-2008/hunch_net-2008-10-14-Who_is_Responsible_for_a_Bad_Review%3F.html">320 hunch net-2008-10-14-Who is Responsible for a Bad Review?</a></p>
<p>Introduction: Although I’m greatly interested in machine learning, I think it must be admitted that there is a large amount of low quality logic being used in reviews.  The problem is bad enough that sometimes I wonder if the  Byzantine generals  limit has been exceeded.  For example, I’ve seen recent reviews where the given reasons for rejecting are:
  
 [ NIPS ] Theorem A is uninteresting because Theorem B is uninteresting. 
 [ UAI ] When you learn by memorization, the problem addressed is trivial. 
 [NIPS] The proof is in the appendix.  
 [NIPS] This has been done before.  (… but not giving any relevant citations)  
  
Just for the record I want to point out what’s wrong with these reviews.  A future world in which such reasons never come up again would be great, but I’m sure these errors will be committed many times more in the future.
  
 This is nonsense.  A theorem should be evaluated based on it’s merits, rather than the merits of another theorem. 
 Learning by memorization requires an expon</p><p>5 0.55747372 <a title="223-lda-5" href="../hunch_net-2005/hunch_net-2005-03-13-Avoiding_Bad_Reviewing.html">40 hunch net-2005-03-13-Avoiding Bad Reviewing</a></p>
<p>Introduction: If we accept that bad reviewing often occurs and want to fix it, the question is “how”?
 
Reviewing is done by paper writers just like yourself, so a good proxy for this question is asking “How can I be a better reviewer?”  Here are a few things I’ve learned by trial (and error), as a paper writer, and as a reviewer.
  
 The secret ingredient is careful thought.  There is no good substitution for a deep and careful understanding. 
 Avoid reviewing papers that you feel competitive about.  You almost certainly will be asked to review papers that feel competitive if you work on subjects of common interest.  But, the feeling of competition can easily lead to bad judgement. 
 If you feel biased for some other reason, then you should avoid reviewing.  For example… 
 Feeling angry or threatened by a paper is a form of bias.  See above. 
 Double blind yourself (avoid looking at the name even in a single-blind situation).  The significant effect of a name you recognize is making you pay close a</p><p>6 0.5572775 <a title="223-lda-6" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>7 0.55482411 <a title="223-lda-7" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>8 0.548823 <a title="223-lda-8" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>9 0.54721922 <a title="223-lda-9" href="../hunch_net-2007/hunch_net-2007-01-02-Retrospective.html">225 hunch net-2007-01-02-Retrospective</a></p>
<p>10 0.54653376 <a title="223-lda-10" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>11 0.54586565 <a title="223-lda-11" href="../hunch_net-2012/hunch_net-2012-04-09-ICML_author_feedback_is_open.html">461 hunch net-2012-04-09-ICML author feedback is open</a></p>
<p>12 0.54579395 <a title="223-lda-12" href="../hunch_net-2005/hunch_net-2005-07-21-Six_Months.html">96 hunch net-2005-07-21-Six Months</a></p>
<p>13 0.54550385 <a title="223-lda-13" href="../hunch_net-2012/hunch_net-2012-05-02-ICML%3A_Behind_the_Scenes.html">463 hunch net-2012-05-02-ICML: Behind the Scenes</a></p>
<p>14 0.54486245 <a title="223-lda-14" href="../hunch_net-2006/hunch_net-2006-07-11-New_Models.html">194 hunch net-2006-07-11-New Models</a></p>
<p>15 0.54456973 <a title="223-lda-15" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>16 0.54442966 <a title="223-lda-16" href="../hunch_net-2006/hunch_net-2006-09-12-Incentive_Compatible_Reviewing.html">207 hunch net-2006-09-12-Incentive Compatible Reviewing</a></p>
<p>17 0.54434681 <a title="223-lda-17" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>18 0.54432023 <a title="223-lda-18" href="../hunch_net-2005/hunch_net-2005-01-26-Watchword%3A_Probability.html">5 hunch net-2005-01-26-Watchword: Probability</a></p>
<p>19 0.54349947 <a title="223-lda-19" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>20 0.54347771 <a title="223-lda-20" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
