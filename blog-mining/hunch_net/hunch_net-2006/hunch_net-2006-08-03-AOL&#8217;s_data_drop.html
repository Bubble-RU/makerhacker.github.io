<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>200 hunch net-2006-08-03-AOL&#8217;s data drop</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-200" href="#">hunch_net-2006-200</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>200 hunch net-2006-08-03-AOL&#8217;s data drop</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-200-html" href="http://hunch.net/?p=220">html</a></p><p>Introduction: AOL has  released  several large search engine related datasets.  This looks like a pretty impressive data release, and it is a big opportunity for people everywhere to worry about search engine related learning problems, if they want.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 AOL has  released  several large search engine related datasets. [sent-1, score-1.492]
</p><p>2 This looks like a pretty impressive data release, and it is a big opportunity for people everywhere to worry about search engine related learning problems, if they want. [sent-2, score-2.763]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('engine', 0.527), ('search', 0.352), ('aol', 0.305), ('everywhere', 0.274), ('worry', 0.254), ('related', 0.25), ('release', 0.227), ('released', 0.222), ('impressive', 0.222), ('looks', 0.198), ('opportunity', 0.195), ('pretty', 0.15), ('big', 0.121), ('want', 0.096), ('data', 0.084), ('large', 0.078), ('problems', 0.072), ('like', 0.065), ('several', 0.063), ('people', 0.051), ('learning', 0.02)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="200-tfidf-1" href="../hunch_net-2006/hunch_net-2006-08-03-AOL%26%238217%3Bs_data_drop.html">200 hunch net-2006-08-03-AOL&#8217;s data drop</a></p>
<p>Introduction: AOL has  released  several large search engine related datasets.  This looks like a pretty impressive data release, and it is a big opportunity for people everywhere to worry about search engine related learning problems, if they want.</p><p>2 0.1919615 <a title="200-tfidf-2" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<p>Introduction: I want to comment on the “Bing copies Google” discussion  here ,  here , and  here , because there are data-related issues which the general public may not understand, and some of the framing seems substantially misleading to me.
 
As a not-distant-outsider, let me mention the sources of bias I may have.  I work at  Yahoo! , which has started using  Bing .  This might predispose me towards Bing, but on the other hand I’m still at Yahoo!, and have been using  Linux  exclusively as an OS for many years, including even a couple minor kernel patches.  And,  on the gripping hand , I’ve spent quite a bit of time thinking about the basic  principles of incorporating user feedback in machine learning .  Also note, this post is not  related to official Yahoo! policy, it’s just my personal view.
 
 The issue  Google engineers inserted synthetic responses to synthetic queries on google.com, then executed the synthetic searches on google.com using Internet Explorer with the Bing toolbar and later</p><p>3 0.16348863 <a title="200-tfidf-3" href="../hunch_net-2006/hunch_net-2006-02-11-Yahoo%26%238217%3Bs_Learning_Problems..html">156 hunch net-2006-02-11-Yahoo&#8217;s Learning Problems.</a></p>
<p>Introduction: I just visited  Yahoo Research  which has several fundamental learning problems near to (or beyond) the set of problems we know how to solve well.  Here are 3 of them.
  
  Ranking   This is the canonical problem of all search engines.  It is made extra difficult for several reasons.
 
 There is relatively little “good” supervised learning data and a great deal of data with some signal (such as click through rates). 
 The learning must occur in a partially adversarial environment. Many people very actively attempt to place themselves at the top of 
rankings. 
 It is not even quite clear whether the problem should be posed as ‘ranking’ or as ‘regression’ which is then used to produce a 
ranking. 
 
 
  Collaborative filtering  Yahoo has a large number of recommendation systems for music, movies, etc…  In these sorts of systems, users specify how they liked a set of things, and then the system can (hopefully) find some more examples of things they might like 
by reasoning across multiple</p><p>4 0.15537947 <a title="200-tfidf-4" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>Introduction: “Search” is the other branch of AI research which has been succesful.   Concrete examples include  Deep Blue  which beat the world chess champion and  Chinook  the champion checkers program.  A set of core search techniques exist including A * , alpha-beta pruning, and others that can be applied to any of many different search problems.
 
Given this, it may be surprising to learn that there has been relatively little succesful work on combining prediction and search.  Given also that  humans typically solve search problems using a number of predictive heuristics to narrow in on a solution, we might be surprised again.  However, the big successful search-based systems have typically not used “smart” search algorithms.  Insteady they have optimized for very fast search.  This is not for lack of trying… many people have tried to synthesize search and prediction to various degrees of success.   For example,  Knightcap  achieves good-but-not-stellar chess playing performance, and  TD-gammon</p><p>5 0.14283222 <a title="200-tfidf-5" href="../hunch_net-2010/hunch_net-2010-03-12-Netflix_Challenge_2_Canceled.html">390 hunch net-2010-03-12-Netflix Challenge 2 Canceled</a></p>
<p>Introduction: The  second Netflix prize is canceled  due to  privacy problems .  I continue to believe my original assessment of this paper, that the privacy break was somewhat overstated.  I still haven’t seen any serious privacy failures on the scale of the  AOL search log release .
 
I expect privacy concerns to continue to be a big issue when dealing with data releases by companies or governments.  The theory of maintaining privacy while using data is improving, but it is not yet in a state where the limits of what’s possible are clear let alone how to achieve these limits in a manner friendly to a prediction competition.</p><p>6 0.12191123 <a title="200-tfidf-6" href="../hunch_net-2008/hunch_net-2008-01-18-Datasets.html">284 hunch net-2008-01-18-Datasets</a></p>
<p>7 0.10297895 <a title="200-tfidf-7" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>8 0.097962365 <a title="200-tfidf-8" href="../hunch_net-2007/hunch_net-2007-08-25-The_Privacy_Problem.html">260 hunch net-2007-08-25-The Privacy Problem</a></p>
<p>9 0.096672006 <a title="200-tfidf-9" href="../hunch_net-2010/hunch_net-2010-12-02-Traffic_Prediction_Problem.html">418 hunch net-2010-12-02-Traffic Prediction Problem</a></p>
<p>10 0.083711043 <a title="200-tfidf-10" href="../hunch_net-2005/hunch_net-2005-07-11-AAAI_blog.html">92 hunch net-2005-07-11-AAAI blog</a></p>
<p>11 0.076240852 <a title="200-tfidf-11" href="../hunch_net-2006/hunch_net-2006-05-08-Big_machine_learning.html">178 hunch net-2006-05-08-Big machine learning</a></p>
<p>12 0.075248294 <a title="200-tfidf-12" href="../hunch_net-2007/hunch_net-2007-09-30-NIPS_workshops_are_out..html">264 hunch net-2007-09-30-NIPS workshops are out.</a></p>
<p>13 0.059065904 <a title="200-tfidf-13" href="../hunch_net-2007/hunch_net-2007-10-24-Contextual_Bandits.html">269 hunch net-2007-10-24-Contextual Bandits</a></p>
<p>14 0.057657845 <a title="200-tfidf-14" href="../hunch_net-2011/hunch_net-2011-02-17-What_does_Watson_mean%3F.html">424 hunch net-2011-02-17-What does Watson mean?</a></p>
<p>15 0.056404576 <a title="200-tfidf-15" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>16 0.055055402 <a title="200-tfidf-16" href="../hunch_net-2005/hunch_net-2005-07-13-%26%238220%3BSister_Conference%26%238221%3B_presentations.html">93 hunch net-2005-07-13-&#8220;Sister Conference&#8221; presentations</a></p>
<p>17 0.053094659 <a title="200-tfidf-17" href="../hunch_net-2006/hunch_net-2006-10-22-Exemplar_programming.html">215 hunch net-2006-10-22-Exemplar programming</a></p>
<p>18 0.052955128 <a title="200-tfidf-18" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>19 0.052243423 <a title="200-tfidf-19" href="../hunch_net-2005/hunch_net-2005-04-21-Dynamic_Programming_Generalizations_and_Their_Use.html">58 hunch net-2005-04-21-Dynamic Programming Generalizations and Their Use</a></p>
<p>20 0.05054082 <a title="200-tfidf-20" href="../hunch_net-2011/hunch_net-2011-06-22-Ultra_LDA.html">436 hunch net-2011-06-22-Ultra LDA</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.084), (1, 0.003), (2, -0.078), (3, 0.024), (4, -0.009), (5, -0.007), (6, -0.031), (7, 0.002), (8, -0.001), (9, -0.022), (10, -0.074), (11, 0.14), (12, -0.031), (13, 0.085), (14, -0.138), (15, 0.038), (16, -0.04), (17, 0.004), (18, 0.072), (19, -0.096), (20, 0.133), (21, -0.031), (22, 0.027), (23, 0.038), (24, -0.011), (25, 0.147), (26, 0.095), (27, 0.03), (28, -0.014), (29, 0.051), (30, -0.026), (31, 0.04), (32, -0.093), (33, -0.021), (34, 0.013), (35, -0.038), (36, 0.037), (37, 0.013), (38, 0.067), (39, 0.061), (40, 0.0), (41, 0.066), (42, -0.07), (43, -0.085), (44, 0.13), (45, -0.001), (46, -0.053), (47, 0.004), (48, -0.033), (49, -0.038)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97233593 <a title="200-lsi-1" href="../hunch_net-2006/hunch_net-2006-08-03-AOL%26%238217%3Bs_data_drop.html">200 hunch net-2006-08-03-AOL&#8217;s data drop</a></p>
<p>Introduction: AOL has  released  several large search engine related datasets.  This looks like a pretty impressive data release, and it is a big opportunity for people everywhere to worry about search engine related learning problems, if they want.</p><p>2 0.76938975 <a title="200-lsi-2" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<p>Introduction: I want to comment on the “Bing copies Google” discussion  here ,  here , and  here , because there are data-related issues which the general public may not understand, and some of the framing seems substantially misleading to me.
 
As a not-distant-outsider, let me mention the sources of bias I may have.  I work at  Yahoo! , which has started using  Bing .  This might predispose me towards Bing, but on the other hand I’m still at Yahoo!, and have been using  Linux  exclusively as an OS for many years, including even a couple minor kernel patches.  And,  on the gripping hand , I’ve spent quite a bit of time thinking about the basic  principles of incorporating user feedback in machine learning .  Also note, this post is not  related to official Yahoo! policy, it’s just my personal view.
 
 The issue  Google engineers inserted synthetic responses to synthetic queries on google.com, then executed the synthetic searches on google.com using Internet Explorer with the Bing toolbar and later</p><p>3 0.66801697 <a title="200-lsi-3" href="../hunch_net-2010/hunch_net-2010-03-12-Netflix_Challenge_2_Canceled.html">390 hunch net-2010-03-12-Netflix Challenge 2 Canceled</a></p>
<p>Introduction: The  second Netflix prize is canceled  due to  privacy problems .  I continue to believe my original assessment of this paper, that the privacy break was somewhat overstated.  I still haven’t seen any serious privacy failures on the scale of the  AOL search log release .
 
I expect privacy concerns to continue to be a big issue when dealing with data releases by companies or governments.  The theory of maintaining privacy while using data is improving, but it is not yet in a state where the limits of what’s possible are clear let alone how to achieve these limits in a manner friendly to a prediction competition.</p><p>4 0.64776152 <a title="200-lsi-4" href="../hunch_net-2006/hunch_net-2006-02-11-Yahoo%26%238217%3Bs_Learning_Problems..html">156 hunch net-2006-02-11-Yahoo&#8217;s Learning Problems.</a></p>
<p>Introduction: I just visited  Yahoo Research  which has several fundamental learning problems near to (or beyond) the set of problems we know how to solve well.  Here are 3 of them.
  
  Ranking   This is the canonical problem of all search engines.  It is made extra difficult for several reasons.
 
 There is relatively little “good” supervised learning data and a great deal of data with some signal (such as click through rates). 
 The learning must occur in a partially adversarial environment. Many people very actively attempt to place themselves at the top of 
rankings. 
 It is not even quite clear whether the problem should be posed as ‘ranking’ or as ‘regression’ which is then used to produce a 
ranking. 
 
 
  Collaborative filtering  Yahoo has a large number of recommendation systems for music, movies, etc…  In these sorts of systems, users specify how they liked a set of things, and then the system can (hopefully) find some more examples of things they might like 
by reasoning across multiple</p><p>5 0.56311893 <a title="200-lsi-5" href="../hunch_net-2006/hunch_net-2006-05-08-Big_machine_learning.html">178 hunch net-2006-05-08-Big machine learning</a></p>
<p>Introduction: According to the  New York Times ,  Yahoo is releasing Project Panama shortly .  Project Panama is about better predicting which advertisements are relevant to a search, implying a higher click through rate, implying larger income for  Yahoo .  There are two things that seem interesting here:
  
 A significant portion of that improved accuracy is almost certainly machine learning at work. 
 The quantitative effect is huge—the estimate in the article is $600*10 6 . 
  
 Google  already has such improvements and  Microsoft Search  is surely working on them, which suggest this is (perhaps) a $10 9  per year machine learning problem. 
 
The exact methodology under use is unlikely to be publicly discussed in the near future because of the competitive enivironment.  Hopefully we’ll have some public “war stories” at some point in the future when this information becomes less sensitive.  For now, it’s reassuring to simply note that machine learning is having a big impact.</p><p>6 0.53752536 <a title="200-lsi-6" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>7 0.53668767 <a title="200-lsi-7" href="../hunch_net-2007/hunch_net-2007-08-25-The_Privacy_Problem.html">260 hunch net-2007-08-25-The Privacy Problem</a></p>
<p>8 0.49634403 <a title="200-lsi-8" href="../hunch_net-2009/hunch_net-2009-07-11-Interesting_papers_at_KDD.html">364 hunch net-2009-07-11-Interesting papers at KDD</a></p>
<p>9 0.48953658 <a title="200-lsi-9" href="../hunch_net-2008/hunch_net-2008-08-04-Electoralmarkets.com.html">312 hunch net-2008-08-04-Electoralmarkets.com</a></p>
<p>10 0.46938974 <a title="200-lsi-10" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>11 0.43290374 <a title="200-lsi-11" href="../hunch_net-2010/hunch_net-2010-12-26-NIPS_2010.html">420 hunch net-2010-12-26-NIPS 2010</a></p>
<p>12 0.41716412 <a title="200-lsi-12" href="../hunch_net-2010/hunch_net-2010-12-02-Traffic_Prediction_Problem.html">418 hunch net-2010-12-02-Traffic Prediction Problem</a></p>
<p>13 0.38775173 <a title="200-lsi-13" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>14 0.37634468 <a title="200-lsi-14" href="../hunch_net-2005/hunch_net-2005-02-02-Kolmogorov_Complexity_and_Googling.html">10 hunch net-2005-02-02-Kolmogorov Complexity and Googling</a></p>
<p>15 0.34521252 <a title="200-lsi-15" href="../hunch_net-2005/hunch_net-2005-10-20-Machine_Learning_in_the_News.html">125 hunch net-2005-10-20-Machine Learning in the News</a></p>
<p>16 0.3346861 <a title="200-lsi-16" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>17 0.31432009 <a title="200-lsi-17" href="../hunch_net-2011/hunch_net-2011-10-03-Monday_announcements.html">446 hunch net-2011-10-03-Monday announcements</a></p>
<p>18 0.29555047 <a title="200-lsi-18" href="../hunch_net-2005/hunch_net-2005-04-21-Dynamic_Programming_Generalizations_and_Their_Use.html">58 hunch net-2005-04-21-Dynamic Programming Generalizations and Their Use</a></p>
<p>19 0.28699574 <a title="200-lsi-19" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>20 0.28490007 <a title="200-lsi-20" href="../hunch_net-2007/hunch_net-2007-09-30-NIPS_workshops_are_out..html">264 hunch net-2007-09-30-NIPS workshops are out.</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(27, 0.166), (84, 0.448), (94, 0.185)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.90686375 <a title="200-lda-1" href="../hunch_net-2012/hunch_net-2012-06-15-Normal_Deviate_and_the_UCSC_Machine_Learning_Summer_School.html">467 hunch net-2012-06-15-Normal Deviate and the UCSC Machine Learning Summer School</a></p>
<p>Introduction: Larry Wasserman  has started the  Normal Deviate  blog which I added to the blogroll on the right.
 
 Manfred Warmuth  points out the  UCSC machine learning summer school  running July 9-20 which may be of particular interest to those in silicon valley.</p><p>same-blog 2 0.89402938 <a title="200-lda-2" href="../hunch_net-2006/hunch_net-2006-08-03-AOL%26%238217%3Bs_data_drop.html">200 hunch net-2006-08-03-AOL&#8217;s data drop</a></p>
<p>Introduction: AOL has  released  several large search engine related datasets.  This looks like a pretty impressive data release, and it is a big opportunity for people everywhere to worry about search engine related learning problems, if they want.</p><p>3 0.867544 <a title="200-lda-3" href="../hunch_net-2009/hunch_net-2009-12-09-Inherent_Uncertainty.html">383 hunch net-2009-12-09-Inherent Uncertainty</a></p>
<p>Introduction: I’d like to point out  Inherent Uncertainty , which I’ve added to the ML blog post scanner on the right.   My understanding from  Jake  is that the intention is to have a multiauthor blog which is more specialized towards learning theory/game theory than this one.  Nevertheless, several of the posts seem to be of wider interest.</p><p>4 0.7892676 <a title="200-lda-4" href="../hunch_net-2005/hunch_net-2005-10-12-The_unrealized_potential_of_the_research_lab.html">121 hunch net-2005-10-12-The unrealized potential of the research lab</a></p>
<p>Introduction: I attended the  IBM research 60th anniversary .  IBM research is, by any reasonable account, the industrial research lab which has managed to bring the most value to it’s parent company over the long term.  This can be seen by simply counting the survivors: IBM research is the only older research lab which has not gone through a period of massive firing.  (Note that there are also  new research labs .)
 
Despite this impressive record, IBM research has failed, by far, to achieve it’s potential.  Examples which came up in this meeting include:
  
 It took about a decade to produce DRAM after it was invented in the lab.  (In fact, Intel produced it first.) 
 Relational databases and SQL were invented and then languished.  It was only under external competition that IBM released it’s own relational database.  Why didn’t IBM grow an  Oracle division ? 
 An early lead in IP networking hardware did not result in IBM growing a  Cisco division .  Why not? 
  
And remember … IBM research is a s</p><p>5 0.75031483 <a title="200-lda-5" href="../hunch_net-2010/hunch_net-2010-09-21-Regretting_the_dead.html">411 hunch net-2010-09-21-Regretting the dead</a></p>
<p>Introduction: Nikos  pointed out this  new york times  article about  poor clinical design killing people .  For those of us who study learning from exploration information this is a reminder that low regret algorithms are particularly important, as regret in clinical trials is measured by patient deaths.
 
Two obvious improvements on the experimental design are:
  
 With reasonable record keeping of existing outcomes for the standard treatments, there is no need to explicitly assign people to a control group with the standard treatment, as that approach is effectively explored with great certainty.  Asserting otherwise would imply that the nature of effective treatments for cancer has changed between now and a year ago, which denies the value of any clinical trial. 
 An optimal experimental design will smoothly phase between exploration and exploitation as evidence for a new treatment shows that it can be effective.  This is old tech, for example in the  EXP3.P algorithm (page 12 aka 59)  although</p><p>6 0.69171494 <a title="200-lda-6" href="../hunch_net-2006/hunch_net-2006-11-22-Explicit_Randomization_in_Learning_algorithms.html">219 hunch net-2006-11-22-Explicit Randomization in Learning algorithms</a></p>
<p>7 0.62434363 <a title="200-lda-7" href="../hunch_net-2005/hunch_net-2005-12-22-Yes_%2C_I_am_applying.html">142 hunch net-2005-12-22-Yes , I am applying</a></p>
<p>8 0.55235791 <a title="200-lda-8" href="../hunch_net-2009/hunch_net-2009-01-21-Nearly_all_natural_problems_require_nonlinearity.html">337 hunch net-2009-01-21-Nearly all natural problems require nonlinearity</a></p>
<p>9 0.48069525 <a title="200-lda-9" href="../hunch_net-2005/hunch_net-2005-06-13-Wikis_for_Summer_Schools_and_Workshops.html">81 hunch net-2005-06-13-Wikis for Summer Schools and Workshops</a></p>
<p>10 0.47331706 <a title="200-lda-10" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>11 0.47313514 <a title="200-lda-11" href="../hunch_net-2009/hunch_net-2009-03-18-Parallel_ML_primitives.html">346 hunch net-2009-03-18-Parallel ML primitives</a></p>
<p>12 0.47220087 <a title="200-lda-12" href="../hunch_net-2005/hunch_net-2005-03-04-The_Big_O_and_Constants_in_Learning.html">35 hunch net-2005-03-04-The Big O and Constants in Learning</a></p>
<p>13 0.47071254 <a title="200-lda-13" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>14 0.46208224 <a title="200-lda-14" href="../hunch_net-2005/hunch_net-2005-09-26-Prediction_Bounds_as_the_Mathematics_of_Science.html">115 hunch net-2005-09-26-Prediction Bounds as the Mathematics of Science</a></p>
<p>15 0.45661744 <a title="200-lda-15" href="../hunch_net-2005/hunch_net-2005-03-17-Going_all_the_Way%2C_Sometimes.html">42 hunch net-2005-03-17-Going all the Way, Sometimes</a></p>
<p>16 0.44871068 <a title="200-lda-16" href="../hunch_net-2008/hunch_net-2008-08-24-Mass_Customized_Medicine_in_the_Future%3F.html">314 hunch net-2008-08-24-Mass Customized Medicine in the Future?</a></p>
<p>17 0.44083217 <a title="200-lda-17" href="../hunch_net-2007/hunch_net-2007-01-26-Parallel_Machine_Learning_Problems.html">229 hunch net-2007-01-26-Parallel Machine Learning Problems</a></p>
<p>18 0.44003165 <a title="200-lda-18" href="../hunch_net-2006/hunch_net-2006-12-04-Structural_Problems_in_NIPS_Decision_Making.html">221 hunch net-2006-12-04-Structural Problems in NIPS Decision Making</a></p>
<p>19 0.4351711 <a title="200-lda-19" href="../hunch_net-2005/hunch_net-2005-12-07-Is_the_Google_way_the_way_for_machine_learning%3F.html">136 hunch net-2005-12-07-Is the Google way the way for machine learning?</a></p>
<p>20 0.42934439 <a title="200-lda-20" href="../hunch_net-2007/hunch_net-2007-12-21-Vowpal_Wabbit_Code_Release.html">281 hunch net-2007-12-21-Vowpal Wabbit Code Release</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
