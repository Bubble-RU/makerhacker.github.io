<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>198 hunch net-2006-07-25-Upcoming conference</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-198" href="#">hunch_net-2006-198</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>198 hunch net-2006-07-25-Upcoming conference</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-198-html" href="http://hunch.net/?p=216">html</a></p><p>Introduction: The Workshop for Women in Machine Learning will be held in San Diego on
October 4, 2006.For details see the workshop
website:http://www.seas.upenn.edu/~wiml/</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('women', 0.414), ('workshop', 0.386), ('http', 0.384), ('diego', 0.362), ('san', 0.332), ('october', 0.301), ('held', 0.286), ('website', 0.253), ('details', 0.187), ('see', 0.118), ('machine', 0.064), ('learning', 0.027)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="198-tfidf-1" href="../hunch_net-2006/hunch_net-2006-07-25-Upcoming_conference.html">198 hunch net-2006-07-25-Upcoming conference</a></p>
<p>Introduction: The Workshop for Women in Machine Learning will be held in San Diego on
October 4, 2006.For details see the workshop
website:http://www.seas.upenn.edu/~wiml/</p><p>2 0.24890101 <a title="198-tfidf-2" href="../hunch_net-2007/hunch_net-2007-02-22-Create_Your_Own_ICML_Workshop.html">234 hunch net-2007-02-22-Create Your Own ICML Workshop</a></p>
<p>Introduction: As usualICML 2007will be hosting aworkshop programto be held this year on June
24th. The success of the program depends on having researchers like you
propose interesting workshop topics and then organize the workshops. I'd like
to encourage all of you to consider sending a workshop proposal. The proposal
deadline has been extended to March 5. See the workshop web-site for
details.Organizing a workshop is a unique way to gather an international group
of researchers together to focus for an entire day on a topic of your
choosing. I've always found that the cost of organizing a workshop is not so
large, and very low compared to the benefits. The topic and format of a
workshop are limited only by your imagination (and the attractiveness to
potential participants) and need not follow the usual model of a mini-
conference on a particular ML sub-area. Hope to see some interesting proposals
rolling in.</p><p>3 0.21229446 <a title="198-tfidf-3" href="../hunch_net-2007/hunch_net-2007-04-30-COLT_2007.html">242 hunch net-2007-04-30-COLT 2007</a></p>
<p>Introduction: Registration for COLT 2007 is now open.The conference will take place on 13-15
June, 2007, in San Diego, California, as part of the 2007 Federated Computing
Research Conference (FCRC), which includes STOC, Complexity, and EC.The
website for COLT: http://www.learningtheory.org/colt2007/index.htmlThe early
registration deadline is May 11, and the cutoff date for discounted hotel
rates is May 9.Before registering, take note that the fees are substantially
lower for members of ACM and/or SIGACT than for nonmembers. If you've been
contemplating joining either of these two societies (annual dues: $99 for ACM,
$18 for SIGACT), now would be a good time!</p><p>4 0.17468548 <a title="198-tfidf-4" href="../hunch_net-2011/hunch_net-2011-01-16-2011_Summer_Conference_Deadline_Season.html">422 hunch net-2011-01-16-2011 Summer Conference Deadline Season</a></p>
<p>Introduction: Machine learning always welcomes the new year with paper deadlines for summer
conferences. This year, we have:ConferencePaper DeadlineWhen/WhereDouble
blind?Author Feedback?NotesICMLFebruary 1June 28-July 2, Bellevue, Washington,
USAYYWeak colocation withACLCOLTFebruary 11July 9-July 11, Budapest,
HungaryNNcolocated withFOCMKDDFebruary 11/18August 21-24, San Diego,
California, USANNUAIMarch 18July 14-17, Barcelona, SpainYNThe larger
conferences are on the west coast in the United States, while the smaller ones
are in Europe.</p><p>5 0.14862955 <a title="198-tfidf-5" href="../hunch_net-2005/hunch_net-2005-03-24-The_Role_of_Workshops.html">46 hunch net-2005-03-24-The Role of Workshops</a></p>
<p>Introduction: A good workshop is often far more interesting than the papers at a conference.
This happens because a workshop has a much tighter focus than a conference.
Since you choose the workshops fitting your interest, the increased relevance
can greatly enhance the level of your interest and attention. Roughly
speaking, a workshop program consists of elements related to a subject of your
interest. The main conference program consists of elements related to
someone's interest (which is rarely your own). Workshops are more about doing
research while conferences are more about presenting research.Several
conferences have associated workshop programs, some with deadlines due
shortly.ICML workshopsDue April 1IJCAI workshopsDeadlines VaryKDD workshopsNot
yet finalizedAnyone going to these conferences should examine the workshops
and see if any are of interest. (If none are, then maybe you should organize
one next year.)</p><p>6 0.13260885 <a title="198-tfidf-6" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>7 0.13096312 <a title="198-tfidf-7" href="../hunch_net-2013/hunch_net-2013-08-31-Extreme_Classification_workshop_at_NIPS.html">488 hunch net-2013-08-31-Extreme Classification workshop at NIPS</a></p>
<p>8 0.12769297 <a title="198-tfidf-8" href="../hunch_net-2008/hunch_net-2008-09-04-Fall_ML_Conferences.html">316 hunch net-2008-09-04-Fall ML Conferences</a></p>
<p>9 0.10287723 <a title="198-tfidf-9" href="../hunch_net-2008/hunch_net-2008-01-23-Why_Workshop%3F.html">285 hunch net-2008-01-23-Why Workshop?</a></p>
<p>10 0.10112605 <a title="198-tfidf-10" href="../hunch_net-2005/hunch_net-2005-09-19-NIPS_Workshops.html">113 hunch net-2005-09-19-NIPS Workshops</a></p>
<p>11 0.096133105 <a title="198-tfidf-11" href="../hunch_net-2008/hunch_net-2008-10-01-NIPS_2008_workshop_on_%26%238216%3BLearning_over_Empirical_Hypothesis_Spaces%26%238217%3B.html">319 hunch net-2008-10-01-NIPS 2008 workshop on &#8216;Learning over Empirical Hypothesis Spaces&#8217;</a></p>
<p>12 0.093981192 <a title="198-tfidf-12" href="../hunch_net-2013/hunch_net-2013-06-29-The_Benefits_of_Double-Blind_Review.html">485 hunch net-2013-06-29-The Benefits of Double-Blind Review</a></p>
<p>13 0.092173971 <a title="198-tfidf-13" href="../hunch_net-2012/hunch_net-2012-08-27-NYAS_ML_2012_and_ICML_2013.html">472 hunch net-2012-08-27-NYAS ML 2012 and ICML 2013</a></p>
<p>14 0.089335337 <a title="198-tfidf-14" href="../hunch_net-2005/hunch_net-2005-10-19-Workshop%3A_Atomic_Learning.html">124 hunch net-2005-10-19-Workshop: Atomic Learning</a></p>
<p>15 0.087593019 <a title="198-tfidf-15" href="../hunch_net-2005/hunch_net-2005-06-10-Workshops_are_not_Conferences.html">80 hunch net-2005-06-10-Workshops are not Conferences</a></p>
<p>16 0.085265838 <a title="198-tfidf-16" href="../hunch_net-2007/hunch_net-2007-12-12-Workshop_Summary%26%238212%3BPrinciples_of_Learning_Problem_Design.html">277 hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</a></p>
<p>17 0.072937064 <a title="198-tfidf-17" href="../hunch_net-2007/hunch_net-2007-10-14-NIPS_workshp%3A_Learning_Problem_Design.html">265 hunch net-2007-10-14-NIPS workshp: Learning Problem Design</a></p>
<p>18 0.072803631 <a title="198-tfidf-18" href="../hunch_net-2007/hunch_net-2007-01-04-2007_Summer_Machine_Learning_Conferences.html">226 hunch net-2007-01-04-2007 Summer Machine Learning Conferences</a></p>
<p>19 0.0710245 <a title="198-tfidf-19" href="../hunch_net-2007/hunch_net-2007-12-19-Cool_and_interesting_things_seen_at_NIPS.html">279 hunch net-2007-12-19-Cool and interesting things seen at NIPS</a></p>
<p>20 0.069568127 <a title="198-tfidf-20" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.055), (1, 0.076), (2, 0.075), (3, 0.208), (4, -0.032), (5, -0.067), (6, -0.054), (7, -0.001), (8, -0.056), (9, 0.126), (10, -0.022), (11, 0.074), (12, -0.023), (13, 0.004), (14, -0.031), (15, -0.085), (16, -0.008), (17, 0.096), (18, -0.04), (19, 0.002), (20, -0.139), (21, -0.083), (22, 0.011), (23, 0.193), (24, -0.178), (25, -0.003), (26, 0.045), (27, -0.108), (28, 0.024), (29, 0.018), (30, -0.078), (31, 0.015), (32, 0.021), (33, -0.088), (34, -0.015), (35, -0.06), (36, -0.061), (37, 0.026), (38, 0.024), (39, -0.006), (40, -0.07), (41, -0.119), (42, 0.044), (43, -0.04), (44, -0.052), (45, -0.059), (46, -0.016), (47, -0.005), (48, -0.0), (49, -0.02)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96780324 <a title="198-lsi-1" href="../hunch_net-2006/hunch_net-2006-07-25-Upcoming_conference.html">198 hunch net-2006-07-25-Upcoming conference</a></p>
<p>Introduction: The Workshop for Women in Machine Learning will be held in San Diego on
October 4, 2006.For details see the workshop
website:http://www.seas.upenn.edu/~wiml/</p><p>2 0.8319664 <a title="198-lsi-2" href="../hunch_net-2007/hunch_net-2007-02-22-Create_Your_Own_ICML_Workshop.html">234 hunch net-2007-02-22-Create Your Own ICML Workshop</a></p>
<p>Introduction: As usualICML 2007will be hosting aworkshop programto be held this year on June
24th. The success of the program depends on having researchers like you
propose interesting workshop topics and then organize the workshops. I'd like
to encourage all of you to consider sending a workshop proposal. The proposal
deadline has been extended to March 5. See the workshop web-site for
details.Organizing a workshop is a unique way to gather an international group
of researchers together to focus for an entire day on a topic of your
choosing. I've always found that the cost of organizing a workshop is not so
large, and very low compared to the benefits. The topic and format of a
workshop are limited only by your imagination (and the attractiveness to
potential participants) and need not follow the usual model of a mini-
conference on a particular ML sub-area. Hope to see some interesting proposals
rolling in.</p><p>3 0.60770118 <a title="198-lsi-3" href="../hunch_net-2008/hunch_net-2008-10-01-NIPS_2008_workshop_on_%26%238216%3BLearning_over_Empirical_Hypothesis_Spaces%26%238217%3B.html">319 hunch net-2008-10-01-NIPS 2008 workshop on &#8216;Learning over Empirical Hypothesis Spaces&#8217;</a></p>
<p>Introduction: This workshop asks for insights how far we may/can push the theoretical
boundary of using data in the design of learning machines. Can we express our
classification rule in terms of the sample, or do we have to stick to a core
assumption of classical statistical learning theory, namely that the
hypothesis space is to be defined independent from the sample? This workshop
is particularly interested in - but not restricted to - the 'luckiness
framework' and the recently introduced notion of 'compatibility functions' in
a semi-supervised learning context (more information can be found
athttp://www.kuleuven.be/wehys).</p><p>4 0.59342468 <a title="198-lsi-4" href="../hunch_net-2005/hunch_net-2005-03-24-The_Role_of_Workshops.html">46 hunch net-2005-03-24-The Role of Workshops</a></p>
<p>Introduction: A good workshop is often far more interesting than the papers at a conference.
This happens because a workshop has a much tighter focus than a conference.
Since you choose the workshops fitting your interest, the increased relevance
can greatly enhance the level of your interest and attention. Roughly
speaking, a workshop program consists of elements related to a subject of your
interest. The main conference program consists of elements related to
someone's interest (which is rarely your own). Workshops are more about doing
research while conferences are more about presenting research.Several
conferences have associated workshop programs, some with deadlines due
shortly.ICML workshopsDue April 1IJCAI workshopsDeadlines VaryKDD workshopsNot
yet finalizedAnyone going to these conferences should examine the workshops
and see if any are of interest. (If none are, then maybe you should organize
one next year.)</p><p>5 0.53098887 <a title="198-lsi-5" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>Introduction: Founding a successful new conference is extraordinarily difficult. As a
conference founder, you must manage to attract a significant number of good
papers--enough to entice the participants into participating next year and to
(generally) to grow the conference. For someone choosing to participate in a
new conference, there is a very significant decision to make: do you send a
paper to some new conference with no guarantee that the conference will work
out? Or do you send it to another (possibly less related) conference that you
are sure will work?The conference founding problem is a joint agreement
problem with a very significant barrier. Workshops are a way around this
problem, and workshops attached to conferences are a particularly effective
means for this. A workshop at a conference is sure to have people available to
speak and attend and is sure to have a large audience available. Presenting
work at a workshop is not generally exclusive: it can also be presented at a
conference. F</p><p>6 0.52467191 <a title="198-lsi-6" href="../hunch_net-2005/hunch_net-2005-10-19-Workshop%3A_Atomic_Learning.html">124 hunch net-2005-10-19-Workshop: Atomic Learning</a></p>
<p>7 0.50808436 <a title="198-lsi-7" href="../hunch_net-2008/hunch_net-2008-01-23-Why_Workshop%3F.html">285 hunch net-2008-01-23-Why Workshop?</a></p>
<p>8 0.50623465 <a title="198-lsi-8" href="../hunch_net-2008/hunch_net-2008-10-19-NIPS_2008_workshop_on_Kernel_Learning.html">321 hunch net-2008-10-19-NIPS 2008 workshop on Kernel Learning</a></p>
<p>9 0.50381613 <a title="198-lsi-9" href="../hunch_net-2005/hunch_net-2005-06-10-Workshops_are_not_Conferences.html">80 hunch net-2005-06-10-Workshops are not Conferences</a></p>
<p>10 0.50127208 <a title="198-lsi-10" href="../hunch_net-2005/hunch_net-2005-09-20-Workshop_Proposal%3A_Atomic_Learning.html">114 hunch net-2005-09-20-Workshop Proposal: Atomic Learning</a></p>
<p>11 0.49269384 <a title="198-lsi-11" href="../hunch_net-2012/hunch_net-2012-07-17-MUCMD_and_BayLearn.html">470 hunch net-2012-07-17-MUCMD and BayLearn</a></p>
<p>12 0.47056356 <a title="198-lsi-12" href="../hunch_net-2007/hunch_net-2007-10-14-NIPS_workshp%3A_Learning_Problem_Design.html">265 hunch net-2007-10-14-NIPS workshp: Learning Problem Design</a></p>
<p>13 0.42948627 <a title="198-lsi-13" href="../hunch_net-2010/hunch_net-2010-08-20-The_Workshop_on_Cores%2C_Clusters%2C_and_Clouds.html">404 hunch net-2010-08-20-The Workshop on Cores, Clusters, and Clouds</a></p>
<p>14 0.42315412 <a title="198-lsi-14" href="../hunch_net-2005/hunch_net-2005-09-19-NIPS_Workshops.html">113 hunch net-2005-09-19-NIPS Workshops</a></p>
<p>15 0.41588983 <a title="198-lsi-15" href="../hunch_net-2009/hunch_net-2009-11-09-NYAS_ML_Symposium_this_year..html">377 hunch net-2009-11-09-NYAS ML Symposium this year.</a></p>
<p>16 0.35859516 <a title="198-lsi-16" href="../hunch_net-2011/hunch_net-2011-01-16-2011_Summer_Conference_Deadline_Season.html">422 hunch net-2011-01-16-2011 Summer Conference Deadline Season</a></p>
<p>17 0.33876669 <a title="198-lsi-17" href="../hunch_net-2007/hunch_net-2007-12-12-Workshop_Summary%26%238212%3BPrinciples_of_Learning_Problem_Design.html">277 hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</a></p>
<p>18 0.31292936 <a title="198-lsi-18" href="../hunch_net-2012/hunch_net-2012-02-20-Berkeley_Streaming_Data_Workshop.html">455 hunch net-2012-02-20-Berkeley Streaming Data Workshop</a></p>
<p>19 0.31028277 <a title="198-lsi-19" href="../hunch_net-2007/hunch_net-2007-01-04-2007_Summer_Machine_Learning_Conferences.html">226 hunch net-2007-01-04-2007 Summer Machine Learning Conferences</a></p>
<p>20 0.28069934 <a title="198-lsi-20" href="../hunch_net-2008/hunch_net-2008-09-04-Fall_ML_Conferences.html">316 hunch net-2008-09-04-Fall ML Conferences</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(42, 0.053), (60, 0.709)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96675247 <a title="198-lda-1" href="../hunch_net-2006/hunch_net-2006-07-25-Upcoming_conference.html">198 hunch net-2006-07-25-Upcoming conference</a></p>
<p>Introduction: The Workshop for Women in Machine Learning will be held in San Diego on
October 4, 2006.For details see the workshop
website:http://www.seas.upenn.edu/~wiml/</p><p>2 0.69087726 <a title="198-lda-2" href="../hunch_net-2012/hunch_net-2012-07-09-Videolectures.html">469 hunch net-2012-07-09-Videolectures</a></p>
<p>Introduction: Yaserpoints out some nicelyvideotaped machine learning lecturesatCaltech.
Yaser taught me machine learning, and I always found the lectures clear and
interesting, so I expect many people can benefit from watching. Relative
toAndrew Ng'sML classthere are somewhat different areas of emphasis but the
topic is the same, so picking and choosing the union may be helpful.</p><p>3 0.57337254 <a title="198-lda-3" href="../hunch_net-2005/hunch_net-2005-05-29-Bad_ideas.html">76 hunch net-2005-05-29-Bad ideas</a></p>
<p>Introduction: I found these two essays on bad ideas interesting. Neither of these is written
from the viewpoint of research, but they are both highly relevant.Why smart
people have bad ideasby Paul GrahamWhy smart people defend bad ideasby Scott
Berkun (which appeared onslashdot)In my experience, bad ideas are
commonandover confidence in ideas is common. This overconfidence can take
either the form of excessive condemnation or excessive praise. Some of this is
necessary to the process of research. For example, some overconfidence in the
value of your own research is expected and probably necessary to motivate your
own investigation. Since research is a rather risky business, much of it does
not pan out. Learning to accept when something does not pan out is a critical
skill which is sometimes never acquired.Excessive condemnation can be a real
ill when it's encountered. This has two effects:When the penalty for being
wrong is too large, it means people have a great investment in defending
"their" ide</p><p>4 0.31661445 <a title="198-lda-4" href="../hunch_net-2005/hunch_net-2005-10-26-Fallback_Analysis_is_a_Secret_to_Useful_Algorithms.html">126 hunch net-2005-10-26-Fallback Analysis is a Secret to Useful Algorithms</a></p>
<p>Introduction: The ideal of theoretical algorithm analysis is to construct an algorithm with
accompanying optimality theorems proving that it is a useful algorithm. This
ideal often fails, particularly for learning algorithms and theory. The
general form of a theorem is:IfpreconditionsThenpostconditionsWhen we design
learning algorithms it is very common to come up with precondition assumptions
such as "the data is IID", "the learning problem is drawn from a known
distribution over learning problems", or "there is a perfect classifier". All
of these example preconditions can be false for real-world problems in ways
that are not easily detectable. This means that algorithms derived and
justified by these very common forms of analysis may be prone to catastrophic
failure in routine (mis)application.Wecanhope for better. Several different
kinds of learning algorithm analysis have been developed some of which have
fewer preconditions. Simply demanding that these forms of analysis be used may
be too stron</p><p>5 0.18197405 <a title="198-lda-5" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>Introduction: Claireasked me to be on the SODA program committee this year, which was quite
a bit of work.I had a relatively light load--merely 49 theory papers. Many of
these papers were not on subjects that I was expert about, so (as is common
for theory conferences) I found various reviewers that I trusted to help
review the papers. I ended up reviewing about 1/3 personally. There were a
couple instances where I ended up overruling a subreviewer whose logic seemed
off, but otherwise I generally let their reviews stand.There are some
differences in standards for paper reviews between the machine learning and
theory communities. In machine learning it is expected that a review be
detailed, while in the theory community this is often not the case. Every
paper given to me ended up with a review varying between somewhat and very
detailed.I'm sure not every author was happy with the outcome. While we did
our best to make good decisions, they were difficult decisions to make. For
example, if there is a</p><p>6 0.17831352 <a title="198-lda-6" href="../hunch_net-2007/hunch_net-2007-04-30-COLT_2007.html">242 hunch net-2007-04-30-COLT 2007</a></p>
<p>7 0.074971028 <a title="198-lda-7" href="../hunch_net-2008/hunch_net-2008-04-27-Watchword%3A_Supervised_Learning.html">299 hunch net-2008-04-27-Watchword: Supervised Learning</a></p>
<p>8 0.074916899 <a title="198-lda-8" href="../hunch_net-2006/hunch_net-2006-01-23-On_Coding_via_Mutual_Information_%26%23038%3B_Bayes_Nets.html">150 hunch net-2006-01-23-On Coding via Mutual Information &#038; Bayes Nets</a></p>
<p>9 0.074912101 <a title="198-lda-9" href="../hunch_net-2012/hunch_net-2012-10-26-ML_Symposium_and_Strata-Hadoop_World.html">475 hunch net-2012-10-26-ML Symposium and Strata-Hadoop World</a></p>
<p>10 0.074891016 <a title="198-lda-10" href="../hunch_net-2005/hunch_net-2005-05-06-Don%26%238217%3Bt_mix_the_solution_into_the_problem.html">67 hunch net-2005-05-06-Don&#8217;t mix the solution into the problem</a></p>
<p>11 0.07483796 <a title="198-lda-11" href="../hunch_net-2005/hunch_net-2005-12-09-Some_NIPS_papers.html">138 hunch net-2005-12-09-Some NIPS papers</a></p>
<p>12 0.074830428 <a title="198-lda-12" href="../hunch_net-2006/hunch_net-2006-06-15-IJCAI_is_out_of_season.html">184 hunch net-2006-06-15-IJCAI is out of season</a></p>
<p>13 0.074787326 <a title="198-lda-13" href="../hunch_net-2005/hunch_net-2005-03-29-Academic_Mechanism_Design.html">48 hunch net-2005-03-29-Academic Mechanism Design</a></p>
<p>14 0.074655615 <a title="198-lda-14" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>15 0.074639849 <a title="198-lda-15" href="../hunch_net-2005/hunch_net-2005-07-07-The_Limits_of_Learning_Theory.html">90 hunch net-2005-07-07-The Limits of Learning Theory</a></p>
<p>16 0.074032694 <a title="198-lda-16" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>17 0.073936909 <a title="198-lda-17" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<p>18 0.073281728 <a title="198-lda-18" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>19 0.072816022 <a title="198-lda-19" href="../hunch_net-2005/hunch_net-2005-02-23-Problem%3A_Reinforcement_Learning_with_Classification.html">27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</a></p>
<p>20 0.072518431 <a title="198-lda-20" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
