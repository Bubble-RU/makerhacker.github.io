<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>198 hunch net-2006-07-25-Upcoming conference</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-198" href="#">hunch_net-2006-198</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>198 hunch net-2006-07-25-Upcoming conference</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-198-html" href="http://hunch.net/?p=216">html</a></p><p>Introduction: The Workshop for Women in Machine Learning will be held in San Diego on October 4, 2006.
 
For details see the workshop website:
 
http://www.seas.upenn.edu/~wiml/</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The Workshop for Women in Machine Learning will be held in San Diego on October 4, 2006. [sent-1, score-0.307]
</p><p>2 For details see the workshop website:   http://www. [sent-2, score-0.659]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('women', 0.445), ('diego', 0.371), ('workshop', 0.346), ('san', 0.344), ('http', 0.333), ('held', 0.307), ('october', 0.307), ('website', 0.263), ('details', 0.195), ('see', 0.118), ('machine', 0.065), ('learning', 0.026)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="198-tfidf-1" href="../hunch_net-2006/hunch_net-2006-07-25-Upcoming_conference.html">198 hunch net-2006-07-25-Upcoming conference</a></p>
<p>Introduction: The Workshop for Women in Machine Learning will be held in San Diego on October 4, 2006.
 
For details see the workshop website:
 
http://www.seas.upenn.edu/~wiml/</p><p>2 0.21530257 <a title="198-tfidf-2" href="../hunch_net-2007/hunch_net-2007-02-22-Create_Your_Own_ICML_Workshop.html">234 hunch net-2007-02-22-Create Your Own ICML Workshop</a></p>
<p>Introduction: As usual  ICML 2007  will be hosting a  workshop program  to be held this year on June 24th. The success of the program depends on having researchers like you propose interesting workshop topics and then organize the workshops. I’d like to encourage all of you to consider sending a workshop proposal. The proposal deadline has been extended to March 5. See the workshop web-site for details. 
 
Organizing a workshop is a unique way to gather an international group of researchers together to focus for an entire day on a topic of your choosing. I’ve always found that the cost of organizing a workshop is not so large, and very low compared to the benefits. The topic and format of a workshop are limited only by your imagination (and the attractiveness to potential participants) and need not follow the usual model of a mini-conference on a particular ML sub-area.  Hope to see some interesting proposals rolling in.</p><p>3 0.19363725 <a title="198-tfidf-3" href="../hunch_net-2007/hunch_net-2007-04-30-COLT_2007.html">242 hunch net-2007-04-30-COLT 2007</a></p>
<p>Introduction: Registration for COLT 2007 is now open.
 
The conference will take place on 13-15 June, 2007, in San Diego, California, as part of the 2007 Federated Computing Research Conference (FCRC), which includes STOC, Complexity, and EC.
 
The website for COLT: http://www.learningtheory.org/colt2007/index.html
 
The early registration deadline is May 11, and the cutoff date for discounted hotel rates is May 9.
 
Before registering, take note that the fees are substantially lower for members of ACM and/or SIGACT than for nonmembers. If youâ&euro;&trade;ve been contemplating joining either of these two societies (annual dues: $99 for ACM, $18 for SIGACT), now would be a good time!</p><p>4 0.17307028 <a title="198-tfidf-4" href="../hunch_net-2013/hunch_net-2013-08-31-Extreme_Classification_workshop_at_NIPS.html">488 hunch net-2013-08-31-Extreme Classification workshop at NIPS</a></p>
<p>Introduction: Manik  and I are organizing the  extreme classification  workshop at NIPS this year.  We have a number of good speakers lined up, but I would further encourage anyone working in the area to submit an abstract by October 9.  I believe this is an idea whose time has now come.
 
The NIPS website doesnâ&euro;&trade;t have other workshops listed yet, but I expect several others to be of significant interest.</p><p>5 0.13133158 <a title="198-tfidf-5" href="../hunch_net-2011/hunch_net-2011-09-03-Fall_Machine_Learning_Events.html">443 hunch net-2011-09-03-Fall Machine Learning Events</a></p>
<p>Introduction: Many Machine Learning related events are coming up this fall.
  
  September 9 ,  abstracts for the New York Machine Learning Symposium  are due.  Send a 2 page pdf, if interested, and note that we:
 
 widened submissions to be from anybody rather than students. 
 set aside a larger fraction of time for contributed submissions.  
 
 
  September 15 , there is a  machine learning meetup , where I’ll be discussing terascale learning at AOL. 
  September 16 , there is a  CS&Econ; day  at New York Academy of Sciences.  This is not ML focused, but it’s easy to imagine interest. 
  September 23 and later   NIPS workshop  submissions start coming due.  As usual, there are too many good ones, so I won’t be able to attend all those that interest me.  I do hope some workshop makers consider ICML this coming summer, as we are increasing to a 2 day format for you.  Here are a few that interest me:
 
  Big Learning  is about dealing with lots of data.  Abstracts are due  September 30 . 
 The  Bayes</p><p>6 0.13078384 <a title="198-tfidf-6" href="../hunch_net-2008/hunch_net-2008-10-01-NIPS_2008_workshop_on_%26%238216%3BLearning_over_Empirical_Hypothesis_Spaces%26%238217%3B.html">319 hunch net-2008-10-01-NIPS 2008 workshop on &#8216;Learning over Empirical Hypothesis Spaces&#8217;</a></p>
<p>7 0.11716714 <a title="198-tfidf-7" href="../hunch_net-2008/hunch_net-2008-09-04-Fall_ML_Conferences.html">316 hunch net-2008-09-04-Fall ML Conferences</a></p>
<p>8 0.11606872 <a title="198-tfidf-8" href="../hunch_net-2011/hunch_net-2011-01-16-2011_Summer_Conference_Deadline_Season.html">422 hunch net-2011-01-16-2011 Summer Conference Deadline Season</a></p>
<p>9 0.11402659 <a title="198-tfidf-9" href="../hunch_net-2007/hunch_net-2007-10-15-NIPS_workshops_extended_to_3_days.html">266 hunch net-2007-10-15-NIPS workshops extended to 3 days</a></p>
<p>10 0.10741237 <a title="198-tfidf-10" href="../hunch_net-2007/hunch_net-2007-01-04-2007_Summer_Machine_Learning_Conferences.html">226 hunch net-2007-01-04-2007 Summer Machine Learning Conferences</a></p>
<p>11 0.10724886 <a title="198-tfidf-11" href="../hunch_net-2007/hunch_net-2007-11-16-MLSS_2008.html">273 hunch net-2007-11-16-MLSS 2008</a></p>
<p>12 0.10317709 <a title="198-tfidf-12" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>13 0.10312376 <a title="198-tfidf-13" href="../hunch_net-2008/hunch_net-2008-01-23-Why_Workshop%3F.html">285 hunch net-2008-01-23-Why Workshop?</a></p>
<p>14 0.097642183 <a title="198-tfidf-14" href="../hunch_net-2005/hunch_net-2005-10-19-Workshop%3A_Atomic_Learning.html">124 hunch net-2005-10-19-Workshop: Atomic Learning</a></p>
<p>15 0.097624235 <a title="198-tfidf-15" href="../hunch_net-2005/hunch_net-2005-03-24-The_Role_of_Workshops.html">46 hunch net-2005-03-24-The Role of Workshops</a></p>
<p>16 0.09495154 <a title="198-tfidf-16" href="../hunch_net-2013/hunch_net-2013-06-29-The_Benefits_of_Double-Blind_Review.html">485 hunch net-2013-06-29-The Benefits of Double-Blind Review</a></p>
<p>17 0.09363652 <a title="198-tfidf-17" href="../hunch_net-2012/hunch_net-2012-08-27-NYAS_ML_2012_and_ICML_2013.html">472 hunch net-2012-08-27-NYAS ML 2012 and ICML 2013</a></p>
<p>18 0.092946373 <a title="198-tfidf-18" href="../hunch_net-2011/hunch_net-2011-04-23-ICML_workshops_due.html">433 hunch net-2011-04-23-ICML workshops due</a></p>
<p>19 0.088106275 <a title="198-tfidf-19" href="../hunch_net-2008/hunch_net-2008-10-19-NIPS_2008_workshop_on_Kernel_Learning.html">321 hunch net-2008-10-19-NIPS 2008 workshop on Kernel Learning</a></p>
<p>20 0.087268606 <a title="198-tfidf-20" href="../hunch_net-2005/hunch_net-2005-06-10-Workshops_are_not_Conferences.html">80 hunch net-2005-06-10-Workshops are not Conferences</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.06), (1, -0.087), (2, -0.112), (3, -0.147), (4, 0.032), (5, 0.098), (6, 0.046), (7, 0.013), (8, -0.015), (9, -0.057), (10, -0.01), (11, -0.088), (12, 0.103), (13, 0.051), (14, -0.017), (15, -0.034), (16, -0.051), (17, 0.087), (18, -0.143), (19, -0.018), (20, -0.208), (21, -0.098), (22, -0.062), (23, 0.066), (24, 0.051), (25, 0.059), (26, 0.056), (27, -0.072), (28, -0.011), (29, -0.009), (30, -0.023), (31, 0.023), (32, 0.035), (33, -0.11), (34, -0.086), (35, 0.009), (36, 0.038), (37, -0.108), (38, 0.132), (39, 0.041), (40, -0.096), (41, 0.08), (42, -0.052), (43, -0.05), (44, -0.005), (45, -0.07), (46, 0.013), (47, -0.051), (48, -0.032), (49, -0.075)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97625571 <a title="198-lsi-1" href="../hunch_net-2006/hunch_net-2006-07-25-Upcoming_conference.html">198 hunch net-2006-07-25-Upcoming conference</a></p>
<p>Introduction: The Workshop for Women in Machine Learning will be held in San Diego on October 4, 2006.
 
For details see the workshop website:
 
http://www.seas.upenn.edu/~wiml/</p><p>2 0.81589913 <a title="198-lsi-2" href="../hunch_net-2007/hunch_net-2007-02-22-Create_Your_Own_ICML_Workshop.html">234 hunch net-2007-02-22-Create Your Own ICML Workshop</a></p>
<p>Introduction: As usual  ICML 2007  will be hosting a  workshop program  to be held this year on June 24th. The success of the program depends on having researchers like you propose interesting workshop topics and then organize the workshops. I’d like to encourage all of you to consider sending a workshop proposal. The proposal deadline has been extended to March 5. See the workshop web-site for details. 
 
Organizing a workshop is a unique way to gather an international group of researchers together to focus for an entire day on a topic of your choosing. I’ve always found that the cost of organizing a workshop is not so large, and very low compared to the benefits. The topic and format of a workshop are limited only by your imagination (and the attractiveness to potential participants) and need not follow the usual model of a mini-conference on a particular ML sub-area.  Hope to see some interesting proposals rolling in.</p><p>3 0.69689626 <a title="198-lsi-3" href="../hunch_net-2005/hunch_net-2005-10-19-Workshop%3A_Atomic_Learning.html">124 hunch net-2005-10-19-Workshop: Atomic Learning</a></p>
<p>Introduction: We are planning to have a workshop on atomic learning Jan 7 & 8 at TTI-Chicago.  
 
 Details are here .
 
The earlier request for interest is  here .
 
The primary deadline is abstracts due Nov. 20 to jl@tti-c.org.</p><p>4 0.63540518 <a title="198-lsi-4" href="../hunch_net-2008/hunch_net-2008-10-19-NIPS_2008_workshop_on_Kernel_Learning.html">321 hunch net-2008-10-19-NIPS 2008 workshop on Kernel Learning</a></p>
<p>Introduction: Weâ&euro;&trade;d like to invite hunch.net readers to participate in the NIPS 2008 workshop on kernel learning.  While the main focus is on automatically learning kernels from data, we are also also looking at the broader questions of feature selection, multi-task learning and multi-view learning. There are no restrictions on the learning problem being addressed (regression, classification, etc), and both theoretical and applied work will be considered. The deadline for submissions is  October 24 .
 
More detail can be found  here .
 
Corinna Cortes, Arthur Gretton, Gert Lanckriet, Mehryar Mohri, Afshin Rostamizadeh</p><p>5 0.59719115 <a title="198-lsi-5" href="../hunch_net-2008/hunch_net-2008-10-01-NIPS_2008_workshop_on_%26%238216%3BLearning_over_Empirical_Hypothesis_Spaces%26%238217%3B.html">319 hunch net-2008-10-01-NIPS 2008 workshop on &#8216;Learning over Empirical Hypothesis Spaces&#8217;</a></p>
<p>Introduction: This workshop asks for insights how far we may/can push the theoretical boundary of using data in the design of learning machines. Can we express our classification rule in terms of the sample, or do we have to stick to a core assumption of classical statistical learning theory, namely that the hypothesis space is to be defined independent from the sample? This workshop is particularly interested in – but not restricted to – the ‘luckiness framework’ and the recently introduced  notion of ‘compatibility functions’ in a semi-supervised learning context (more information can be found at  http://www.kuleuven.be/wehys ).</p><p>6 0.52256298 <a title="198-lsi-6" href="../hunch_net-2011/hunch_net-2011-04-23-ICML_workshops_due.html">433 hunch net-2011-04-23-ICML workshops due</a></p>
<p>7 0.51508003 <a title="198-lsi-7" href="../hunch_net-2013/hunch_net-2013-08-31-Extreme_Classification_workshop_at_NIPS.html">488 hunch net-2013-08-31-Extreme Classification workshop at NIPS</a></p>
<p>8 0.50089735 <a title="198-lsi-8" href="../hunch_net-2005/hunch_net-2005-06-10-Workshops_are_not_Conferences.html">80 hunch net-2005-06-10-Workshops are not Conferences</a></p>
<p>9 0.48304158 <a title="198-lsi-9" href="../hunch_net-2011/hunch_net-2011-09-03-Fall_Machine_Learning_Events.html">443 hunch net-2011-09-03-Fall Machine Learning Events</a></p>
<p>10 0.47774377 <a title="198-lsi-10" href="../hunch_net-2012/hunch_net-2012-02-20-Berkeley_Streaming_Data_Workshop.html">455 hunch net-2012-02-20-Berkeley Streaming Data Workshop</a></p>
<p>11 0.47040594 <a title="198-lsi-11" href="../hunch_net-2010/hunch_net-2010-08-20-The_Workshop_on_Cores%2C_Clusters%2C_and_Clouds.html">404 hunch net-2010-08-20-The Workshop on Cores, Clusters, and Clouds</a></p>
<p>12 0.4618727 <a title="198-lsi-12" href="../hunch_net-2007/hunch_net-2007-10-14-NIPS_workshp%3A_Learning_Problem_Design.html">265 hunch net-2007-10-14-NIPS workshp: Learning Problem Design</a></p>
<p>13 0.46182162 <a title="198-lsi-13" href="../hunch_net-2012/hunch_net-2012-12-29-Simons_Institute_Big_Data_Program.html">476 hunch net-2012-12-29-Simons Institute Big Data Program</a></p>
<p>14 0.42242861 <a title="198-lsi-14" href="../hunch_net-2012/hunch_net-2012-03-13-The_Submodularity_workshop_and_Lucca_Professorship.html">459 hunch net-2012-03-13-The Submodularity workshop and Lucca Professorship</a></p>
<p>15 0.41455719 <a title="198-lsi-15" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>16 0.40383404 <a title="198-lsi-16" href="../hunch_net-2010/hunch_net-2010-11-18-ICML_2011_%26%238211%3B_Call_for_Tutorials.html">417 hunch net-2010-11-18-ICML 2011 &#8211; Call for Tutorials</a></p>
<p>17 0.39401153 <a title="198-lsi-17" href="../hunch_net-2009/hunch_net-2009-12-07-Vowpal_Wabbit_version_4.0%2C_and_a_NIPS_heresy.html">381 hunch net-2009-12-07-Vowpal Wabbit version 4.0, and a NIPS heresy</a></p>
<p>18 0.38513222 <a title="198-lsi-18" href="../hunch_net-2005/hunch_net-2005-09-19-NIPS_Workshops.html">113 hunch net-2005-09-19-NIPS Workshops</a></p>
<p>19 0.38320768 <a title="198-lsi-19" href="../hunch_net-2005/hunch_net-2005-09-20-Workshop_Proposal%3A_Atomic_Learning.html">114 hunch net-2005-09-20-Workshop Proposal: Atomic Learning</a></p>
<p>20 0.36683768 <a title="198-lsi-20" href="../hunch_net-2005/hunch_net-2005-03-24-The_Role_of_Workshops.html">46 hunch net-2005-03-24-The Role of Workshops</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(55, 0.025), (60, 0.737)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.90654969 <a title="198-lda-1" href="../hunch_net-2006/hunch_net-2006-07-25-Upcoming_conference.html">198 hunch net-2006-07-25-Upcoming conference</a></p>
<p>Introduction: The Workshop for Women in Machine Learning will be held in San Diego on October 4, 2006.
 
For details see the workshop website:
 
http://www.seas.upenn.edu/~wiml/</p><p>2 0.51457208 <a title="198-lda-2" href="../hunch_net-2006/hunch_net-2006-02-27-The_Peekaboom_Dataset.html">159 hunch net-2006-02-27-The Peekaboom Dataset</a></p>
<p>Introduction: Luis von Ahn â&euro;&tilde;s  Peekaboom project  has yielded  data  (830MB).
 
Peekaboom is the second attempt (after  Espgame ) to produce a dataset which is useful for learning to solve vision problems based on voluntary game play.  As a second attempt, it is meant to address all of the shortcomings of the first attempt.  In particular:
  
 The locations of specific objects are provided by the data. 
 The data collection is far more complete and extensive. 
  
The data consists of:
  
 The source images. (1 file per image, just short of 60K images.) 
 The in-game events. (1 file per image, in a lispy syntax.) 
 A description of the event language. 
  
There is a great deal of very specific and relevant data here so the hope that this will help solve vision problems seems quite reasonable.</p><p>3 0.26345 <a title="198-lda-3" href="../hunch_net-2011/hunch_net-2011-02-17-What_does_Watson_mean%3F.html">424 hunch net-2011-02-17-What does Watson mean?</a></p>
<p>Introduction: Watson  convincingly beat the best champion  Jeopardy!  players.  The apparent significance of this varies hugely, depending on your background knowledge about the related machine learning, NLP, and search technology.  For a random person, this might seem evidence of serious machine intelligence, while for people working on the system itself, it probably seems like a reasonably good assemblage of existing technologies with several twists to make the entire system work.
 
Above all, I think we should congratulate the people who managed to put together and execute this project—many years of effort by a diverse set of highly skilled people were needed to make this happen.  In academia, it’s pretty difficult for one professor to assemble that quantity of talent, and in industry it’s rarely the case that such a capable group has both a worthwhile project and the support needed to pursue something like this for several years before success.
 
 Alina  invited me to the Jeopardy watching party</p><p>4 0.25877899 <a title="198-lda-4" href="../hunch_net-2006/hunch_net-2006-03-02-Why_do_people_count_for_learning%3F.html">160 hunch net-2006-03-02-Why do people count for learning?</a></p>
<p>Introduction: This post is about a confusion of mine with respect to many commonly used machine learning algorithms.   
 
A simple example where this comes up is Bayes net prediction.  A Bayes net where a directed acyclic graph over a set of nodes where each node is associated with a variable and the edges indicate dependence.  The joint probability distribution over the variables is given by a set of conditional probabilities.  For example, a very simple Bayes net might express: 
 P(A,B,C) = P(A | B,C)P(B)P(C) 
 
What I don’t understand is the mechanism commonly used to estimate  P(A | B, C) .  If we let  N(A,B,C)  be the number of instances of  A,B,C  then people sometimes form an estimate according to: 
  P’(A | B,C) = N(A,B,C) / N /[N(B)/N * N(C)/N] = N(A,B,C) N /[N(B)  N(C)]   
… in other words, people just estimate  P’(A | B,C)  according to observed relative frequencies.  This is a reasonable technique when you have a large number of samples compared to the size space  A x B x C , but it (nat</p><p>5 0.14471874 <a title="198-lda-5" href="../hunch_net-2007/hunch_net-2007-04-30-COLT_2007.html">242 hunch net-2007-04-30-COLT 2007</a></p>
<p>Introduction: Registration for COLT 2007 is now open.
 
The conference will take place on 13-15 June, 2007, in San Diego, California, as part of the 2007 Federated Computing Research Conference (FCRC), which includes STOC, Complexity, and EC.
 
The website for COLT: http://www.learningtheory.org/colt2007/index.html
 
The early registration deadline is May 11, and the cutoff date for discounted hotel rates is May 9.
 
Before registering, take note that the fees are substantially lower for members of ACM and/or SIGACT than for nonmembers. If youâ&euro;&trade;ve been contemplating joining either of these two societies (annual dues: $99 for ACM, $18 for SIGACT), now would be a good time!</p><p>6 0.13804677 <a title="198-lda-6" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<p>7 0.11063883 <a title="198-lda-7" href="../hunch_net-2009/hunch_net-2009-11-23-ICML_2009_Workshops_%28and_Tutorials%29.html">379 hunch net-2009-11-23-ICML 2009 Workshops (and Tutorials)</a></p>
<p>8 0.087167531 <a title="198-lda-8" href="../hunch_net-2007/hunch_net-2007-01-04-2007_Summer_Machine_Learning_Conferences.html">226 hunch net-2007-01-04-2007 Summer Machine Learning Conferences</a></p>
<p>9 0.08366596 <a title="198-lda-9" href="../hunch_net-2011/hunch_net-2011-01-16-2011_Summer_Conference_Deadline_Season.html">422 hunch net-2011-01-16-2011 Summer Conference Deadline Season</a></p>
<p>10 0.061690375 <a title="198-lda-10" href="../hunch_net-2011/hunch_net-2011-07-11-Interesting_Neural_Network_Papers_at_ICML_2011.html">438 hunch net-2011-07-11-Interesting Neural Network Papers at ICML 2011</a></p>
<p>11 0.060960501 <a title="198-lda-11" href="../hunch_net-2010/hunch_net-2010-10-28-NY_ML_Symposium_2010.html">415 hunch net-2010-10-28-NY ML Symposium 2010</a></p>
<p>12 0.056312077 <a title="198-lda-12" href="../hunch_net-2013/hunch_net-2013-06-29-The_Benefits_of_Double-Blind_Review.html">485 hunch net-2013-06-29-The Benefits of Double-Blind Review</a></p>
<p>13 0.053119302 <a title="198-lda-13" href="../hunch_net-2007/hunch_net-2007-02-22-Create_Your_Own_ICML_Workshop.html">234 hunch net-2007-02-22-Create Your Own ICML Workshop</a></p>
<p>14 0.033507638 <a title="198-lda-14" href="../hunch_net-2008/hunch_net-2008-11-11-COLT_CFP.html">326 hunch net-2008-11-11-COLT CFP</a></p>
<p>15 0.033507638 <a title="198-lda-15" href="../hunch_net-2012/hunch_net-2012-05-12-ICML_accepted_papers_and_early_registration.html">465 hunch net-2012-05-12-ICML accepted papers and early registration</a></p>
<p>16 0.033015214 <a title="198-lda-16" href="../hunch_net-2012/hunch_net-2012-08-27-NYAS_ML_2012_and_ICML_2013.html">472 hunch net-2012-08-27-NYAS ML 2012 and ICML 2013</a></p>
<p>17 0.032151628 <a title="198-lda-17" href="../hunch_net-2007/hunch_net-2007-11-05-CMU_wins_DARPA_Urban_Challenge.html">271 hunch net-2007-11-05-CMU wins DARPA Urban Challenge</a></p>
<p>18 0.032131691 <a title="198-lda-18" href="../hunch_net-2011/hunch_net-2011-10-03-Monday_announcements.html">446 hunch net-2011-10-03-Monday announcements</a></p>
<p>19 0.031660032 <a title="198-lda-19" href="../hunch_net-2008/hunch_net-2008-05-25-Inappropriate_Mathematics_for_Machine_Learning.html">302 hunch net-2008-05-25-Inappropriate Mathematics for Machine Learning</a></p>
<p>20 0.031586774 <a title="198-lda-20" href="../hunch_net-2005/hunch_net-2005-02-15-ESPgame_and_image_labeling.html">20 hunch net-2005-02-15-ESPgame and image labeling</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
