<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>192 hunch net-2006-07-08-Some recent papers</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-192" href="#">hunch_net-2006-192</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>192 hunch net-2006-07-08-Some recent papers</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-192-html" href="http://hunch.net/?p=208">html</a></p><p>Introduction: It was a fine time for learning in Pittsburgh. John and Sam mentioned some of my favorites. Here’s a few more worth checking out:
 
Online Multitask Learning 
Ofer Dekel, Phil Long, Yoram Singer 
This is on my reading list. Definitely an area I’m interested in.
 
Maximum Entropy Distribution Estimation with Generalized Regularization 
Miroslav DudÃƒÂ­k, Robert E. Schapire
 
Learning near-optimal policies with Bellman-residual minimization based fitted policy iteration and a single sample path 
AndrÃƒÂ¡s Antos, Csaba SzepesvÃƒÂ¡ri, RÃƒÂ©mi Munos 
Again, on the list to read. I saw Csaba and Remi talk about this and related work at an ICML Workshop on Kernel Reinforcement Learning. The big question in my head is how this compares/contrasts with existing work in  reductions to reinforcement learning.  Are there advantages/disadvantages?
 
 Higher Order Learning On Graphs>  by Sameer Agarwal, Kristin Branson, and Serge Belongie, looks to be interesteding. They seem to poo-poo “tensorization</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Here’s a few more worth checking out:   Online Multitask Learning  Ofer Dekel, Phil Long, Yoram Singer  This is on my reading list. [sent-3, score-0.101]
</p><p>2 Schapire   Learning near-optimal policies with Bellman-residual minimization based fitted policy iteration and a single sample path  AndrÃƒÂ¡s Antos, Csaba SzepesvÃƒÂ¡ri, RÃƒÂ©mi Munos  Again, on the list to read. [sent-6, score-0.42]
</p><p>3 I saw Csaba and Remi talk about this and related work at an ICML Workshop on Kernel Reinforcement Learning. [sent-7, score-0.116]
</p><p>4 The big question in my head is how this compares/contrasts with existing work in  reductions to reinforcement learning. [sent-8, score-0.378]
</p><p>5 They seem to poo-poo “tensorization” of existing graph algorithms. [sent-11, score-0.115]
</p><p>6 Cover Trees for Nearest Neighbor  (Alina Beygelzimer, Sham Kakade, John Langford) finally seems to have gotten published. [sent-12, score-0.326]
</p><p>7 It’s an embarrassment to the community that it took this long– and a reminder of how diligent one has to be in ensuring good work gets published. [sent-13, score-0.219]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('csaba', 0.257), ('finally', 0.187), ('john', 0.146), ('raina', 0.139), ('dud', 0.139), ('miroslav', 0.139), ('belongie', 0.139), ('branson', 0.139), ('daphne', 0.139), ('dekel', 0.139), ('gotten', 0.139), ('serge', 0.139), ('szepesv', 0.139), ('reinforcement', 0.135), ('singer', 0.128), ('head', 0.128), ('ofer', 0.128), ('yoram', 0.128), ('phil', 0.121), ('ri', 0.121), ('rajat', 0.121), ('beygelzimer', 0.121), ('reminder', 0.121), ('langford', 0.121), ('agarwal', 0.121), ('sam', 0.116), ('regular', 0.116), ('em', 0.116), ('saw', 0.116), ('existing', 0.115), ('iteration', 0.111), ('priors', 0.107), ('path', 0.107), ('graphs', 0.104), ('constructing', 0.104), ('multitask', 0.104), ('minimization', 0.104), ('kakade', 0.101), ('ng', 0.101), ('generalized', 0.101), ('checking', 0.101), ('long', 0.098), ('entropy', 0.098), ('regularization', 0.098), ('took', 0.098), ('policies', 0.098), ('estimation', 0.096), ('sham', 0.094), ('definitely', 0.094), ('schapire', 0.094)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="192-tfidf-1" href="../hunch_net-2006/hunch_net-2006-07-08-Some_recent_papers.html">192 hunch net-2006-07-08-Some recent papers</a></p>
<p>Introduction: It was a fine time for learning in Pittsburgh. John and Sam mentioned some of my favorites. Here’s a few more worth checking out:
 
Online Multitask Learning 
Ofer Dekel, Phil Long, Yoram Singer 
This is on my reading list. Definitely an area I’m interested in.
 
Maximum Entropy Distribution Estimation with Generalized Regularization 
Miroslav DudÃƒÂ­k, Robert E. Schapire
 
Learning near-optimal policies with Bellman-residual minimization based fitted policy iteration and a single sample path 
AndrÃƒÂ¡s Antos, Csaba SzepesvÃƒÂ¡ri, RÃƒÂ©mi Munos 
Again, on the list to read. I saw Csaba and Remi talk about this and related work at an ICML Workshop on Kernel Reinforcement Learning. The big question in my head is how this compares/contrasts with existing work in  reductions to reinforcement learning.  Are there advantages/disadvantages?
 
 Higher Order Learning On Graphs>  by Sameer Agarwal, Kristin Branson, and Serge Belongie, looks to be interesteding. They seem to poo-poo “tensorization</p><p>2 0.13429154 <a title="192-tfidf-2" href="../hunch_net-2006/hunch_net-2006-06-16-Regularization_%3D_Robustness.html">185 hunch net-2006-06-16-Regularization = Robustness</a></p>
<p>Introduction: The Gibbs-Jaynes theorem is a classical result that tells us that the highest entropy distribution (most uncertain, least committed, etc.) subject to expectation constraints on a set of features is an exponential family distribution with the features as sufficient statistics. In math,
 
argmax_p H(p) 
s.t. E_p[f_i] = c_i
 
is given by e^{\sum \lambda_i f_i}/Z. (Z here is the necessary normalization constraint, and the lambdas are free parameters we set to meet the expectation constraints).
 
A great deal of statistical mechanics flows from this result, and it has proven very fruitful in learning as well. (Motivating work in models in text learning and Conditional Random Fields, for instance. ) The result has been demonstrated a number of ways. One of the most elegant is the Ã¢â‚¬Å“geometricÃ¢â‚¬Â version  here .
 
In the case when the expectation constraints come from data, this tells us that the maximum entropy distribution is exactly the maximum likelihood distribution in the expone</p><p>3 0.12989897 <a title="192-tfidf-3" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>Introduction: The papers which interested me most at  ICML  and  COLT  2010 were:
  
  Thomas Walsh ,  Kaushik Subramanian ,  Michael Littman  and  Carlos Diuk   Generalizing Apprenticeship Learning across Hypothesis Classes .  This paper formalizes and provides algorithms with guarantees for mixed-mode apprenticeship and traditional reinforcement learning algorithms, allowing RL algorithms that perform better than for either setting alone. 
  István Szita  and  Csaba Szepesvári   Model-based reinforcement learning with nearly tight exploration complexity bounds .  This paper and  another represent the frontier of best-known algorithm for Reinforcement Learning in a Markov Decision Process. 
  James Martens   Deep learning via Hessian-free optimization .  About a new not-quite-online second order gradient algorithm for learning deep functional structures.  Potentially this is very powerful because while people have often talked about end-to-end learning, it has rarely worked in practice. 
  Chrisoph</p><p>4 0.091470882 <a title="192-tfidf-4" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>Introduction: This post is about a general technique for problem solving which I’ve never seen taught (in full generality), but which I’ve found very useful.
 
Many problems in computer science turn out to be discretely difficult.  The best known version of such problems are NP-hard problems, but I mean ‘discretely difficult’ in a much more general way, which I only know how to capture by examples.
  
  ERM  In empirical risk minimization, you choose a minimum error rate classifier from a set of classifiers.  This is NP hard for common sets, but it can be much harder, depending on the set. 
  Experts  In the online learning with experts setting, you try to predict well so as to compete with a set of (adversarial) experts.  Here the alternating quantifiers of you and an adversary playing out a game can yield a dynamic programming problem that grows exponentially. 
  Policy Iteration  The problem with policy iteration is that you learn a new policy with respect to an old policy, which implies that sim</p><p>5 0.090209223 <a title="192-tfidf-5" href="../hunch_net-2005/hunch_net-2005-02-23-Problem%3A_Reinforcement_Learning_with_Classification.html">27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</a></p>
<p>Introduction: At an intuitive level, the question here is “Can reinforcement learning be solved with classification?”  
 
 Problem  Construct a reinforcement learning algorithm with near-optimal expected sum of rewards in the  direct experience model  given access to a classifier learning algorithm which has a small error rate or regret on all posed classification problems.  The definition of “posed” here is slightly murky.  I consider a problem “posed” if there is an algorithm for constructing labeled classification examples.
 
 Past Work 
  
 There exists a  reduction of reinforcement learning to classification given a generative model.   A generative model is an inherently stronger assumption than the direct experience model. 
 Other  work on learning reductions  may be important. 
 Several algorithms for solving reinforcement learning in the direct experience model exist.  Most, such as  E 3  ,  Factored-E 3  , and  metric-E 3   and  Rmax  require that the observation be the state.  Recent work</p><p>6 0.086497784 <a title="192-tfidf-6" href="../hunch_net-2010/hunch_net-2010-08-20-The_Workshop_on_Cores%2C_Clusters%2C_and_Clouds.html">404 hunch net-2010-08-20-The Workshop on Cores, Clusters, and Clouds</a></p>
<p>7 0.083321214 <a title="192-tfidf-7" href="../hunch_net-2006/hunch_net-2006-06-24-Online_convex_optimization_at_COLT.html">186 hunch net-2006-06-24-Online convex optimization at COLT</a></p>
<p>8 0.081756912 <a title="192-tfidf-8" href="../hunch_net-2010/hunch_net-2010-01-13-Sam_Roweis_died.html">386 hunch net-2010-01-13-Sam Roweis died</a></p>
<p>9 0.080711633 <a title="192-tfidf-9" href="../hunch_net-2005/hunch_net-2005-02-01-NIPS%3A_Online_Bayes.html">8 hunch net-2005-02-01-NIPS: Online Bayes</a></p>
<p>10 0.078137167 <a title="192-tfidf-10" href="../hunch_net-2006/hunch_net-2006-03-17-Multitask_learning_is_Black-Boxable.html">164 hunch net-2006-03-17-Multitask learning is Black-Boxable</a></p>
<p>11 0.076760747 <a title="192-tfidf-11" href="../hunch_net-2005/hunch_net-2005-08-08-Apprenticeship_Reinforcement_Learning_for_Control.html">101 hunch net-2005-08-08-Apprenticeship Reinforcement Learning for Control</a></p>
<p>12 0.076651506 <a title="192-tfidf-12" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>13 0.074961051 <a title="192-tfidf-13" href="../hunch_net-2005/hunch_net-2005-05-29-Maximum_Margin_Mismatch%3F.html">77 hunch net-2005-05-29-Maximum Margin Mismatch?</a></p>
<p>14 0.073893204 <a title="192-tfidf-14" href="../hunch_net-2010/hunch_net-2010-12-26-NIPS_2010.html">420 hunch net-2010-12-26-NIPS 2010</a></p>
<p>15 0.072290726 <a title="192-tfidf-15" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>16 0.07051082 <a title="192-tfidf-16" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>17 0.068706848 <a title="192-tfidf-17" href="../hunch_net-2006/hunch_net-2006-06-30-ICML_papers.html">188 hunch net-2006-06-30-ICML papers</a></p>
<p>18 0.066294037 <a title="192-tfidf-18" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<p>19 0.065670274 <a title="192-tfidf-19" href="../hunch_net-2011/hunch_net-2011-07-11-Interesting_Neural_Network_Papers_at_ICML_2011.html">438 hunch net-2011-07-11-Interesting Neural Network Papers at ICML 2011</a></p>
<p>20 0.062859096 <a title="192-tfidf-20" href="../hunch_net-2007/hunch_net-2007-12-19-Cool_and_interesting_things_seen_at_NIPS.html">279 hunch net-2007-12-19-Cool and interesting things seen at NIPS</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.145), (1, 0.006), (2, -0.013), (3, -0.035), (4, 0.091), (5, 0.016), (6, -0.002), (7, -0.04), (8, -0.001), (9, -0.03), (10, 0.044), (11, -0.024), (12, -0.068), (13, 0.033), (14, 0.046), (15, 0.004), (16, -0.019), (17, 0.038), (18, 0.006), (19, 0.007), (20, -0.054), (21, -0.092), (22, -0.032), (23, -0.002), (24, 0.058), (25, 0.071), (26, -0.039), (27, 0.06), (28, 0.033), (29, 0.107), (30, 0.041), (31, -0.074), (32, -0.041), (33, -0.045), (34, -0.028), (35, -0.017), (36, -0.003), (37, 0.116), (38, 0.008), (39, 0.003), (40, -0.025), (41, -0.055), (42, 0.074), (43, 0.03), (44, 0.031), (45, -0.016), (46, 0.027), (47, -0.011), (48, -0.054), (49, -0.028)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94517028 <a title="192-lsi-1" href="../hunch_net-2006/hunch_net-2006-07-08-Some_recent_papers.html">192 hunch net-2006-07-08-Some recent papers</a></p>
<p>Introduction: It was a fine time for learning in Pittsburgh. John and Sam mentioned some of my favorites. Here’s a few more worth checking out:
 
Online Multitask Learning 
Ofer Dekel, Phil Long, Yoram Singer 
This is on my reading list. Definitely an area I’m interested in.
 
Maximum Entropy Distribution Estimation with Generalized Regularization 
Miroslav DudÃƒÂ­k, Robert E. Schapire
 
Learning near-optimal policies with Bellman-residual minimization based fitted policy iteration and a single sample path 
AndrÃƒÂ¡s Antos, Csaba SzepesvÃƒÂ¡ri, RÃƒÂ©mi Munos 
Again, on the list to read. I saw Csaba and Remi talk about this and related work at an ICML Workshop on Kernel Reinforcement Learning. The big question in my head is how this compares/contrasts with existing work in  reductions to reinforcement learning.  Are there advantages/disadvantages?
 
 Higher Order Learning On Graphs>  by Sameer Agarwal, Kristin Branson, and Serge Belongie, looks to be interesteding. They seem to poo-poo “tensorization</p><p>2 0.6352402 <a title="192-lsi-2" href="../hunch_net-2005/hunch_net-2005-08-08-Apprenticeship_Reinforcement_Learning_for_Control.html">101 hunch net-2005-08-08-Apprenticeship Reinforcement Learning for Control</a></p>
<p>Introduction: Pieter Abbeel  presented a paper with  Andrew Ng  at  ICML  on  Exploration and Apprenticeship Learning in Reinforcement Learning .  The basic idea of this algorithm is:
  
 Collect data from a human controlling a machine. 
 Build a transition model based upon the experience. 
 Build a policy which optimizes the transition model. 
 Evaluate the policy.  If it works well, halt, otherwise add the experience into the pool and go to (2). 
  
The paper proves that this technique will converge to some policy with expected performance near human expected performance assuming the  world fits certain assumptions (MDP or linear dynamics).  
 
This general idea of apprenticeship learning (i.e. incorporating data from an expert) seems very compelling because (a) humans often learn this way and (b) much harder problems can be solved.   For (a), the notion of teaching is about transferring knowledge from an expert to novices, often via demonstration. To see (b), note that we can create intricate rei</p><p>3 0.58157116 <a title="192-lsi-3" href="../hunch_net-2005/hunch_net-2005-05-29-Maximum_Margin_Mismatch%3F.html">77 hunch net-2005-05-29-Maximum Margin Mismatch?</a></p>
<p>Introduction: John makes a fascinating point about structured classification (and slightly scooped my post!). Maximum Margin Markov Networks (M3N) are an interesting example of the second class of structured classifiers (where the classification of one label depends on the others), and one of my favorite papers. I’m not alone: the paper won the best student paper award at NIPS in 2003. 
 
There are some things I find odd about the paper. For instance, it says of probabilistic models 
  
“cannot handle high dimensional feature spaces and lack strong theoretical guarrantees.” 
  
I’m aware of no such limitations. Also: 
  

“Unfortunately, even probabilistic graphical models that are trained discriminatively do not achieve the same level of performance as SVMs, especially when kernel features are used.”

  
This is quite interesting and contradicts my own experience as well as that of a number of people  I   greatly  
 respect . I wonder what the root cause is: perhaps there is something different abo</p><p>4 0.54633111 <a title="192-lsi-4" href="../hunch_net-2006/hunch_net-2006-06-16-Regularization_%3D_Robustness.html">185 hunch net-2006-06-16-Regularization = Robustness</a></p>
<p>Introduction: The Gibbs-Jaynes theorem is a classical result that tells us that the highest entropy distribution (most uncertain, least committed, etc.) subject to expectation constraints on a set of features is an exponential family distribution with the features as sufficient statistics. In math,
 
argmax_p H(p) 
s.t. E_p[f_i] = c_i
 
is given by e^{\sum \lambda_i f_i}/Z. (Z here is the necessary normalization constraint, and the lambdas are free parameters we set to meet the expectation constraints).
 
A great deal of statistical mechanics flows from this result, and it has proven very fruitful in learning as well. (Motivating work in models in text learning and Conditional Random Fields, for instance. ) The result has been demonstrated a number of ways. One of the most elegant is the Ã¢â‚¬Å“geometricÃ¢â‚¬Â version  here .
 
In the case when the expectation constraints come from data, this tells us that the maximum entropy distribution is exactly the maximum likelihood distribution in the expone</p><p>5 0.50551492 <a title="192-lsi-5" href="../hunch_net-2006/hunch_net-2006-07-05-more_icml_papers.html">189 hunch net-2006-07-05-more icml papers</a></p>
<p>Introduction: Here are a few other papers I enjoyed from ICML06.
 
Topic Models:
  
   
Dynamic Topic Models  
David Blei, John Lafferty 
A nice model for how topics in LDA type models can evolve over time, 
using a linear dynamical system on the natural parameters and a very 
clever structured variational approximation (in which the mean field 
parameters are pseudo-observations of a virtual LDS). Like all Blei 
papers, he makes it look easy, but it is extremely impressive. 
   
Pachinko Allocation  
Wei Li, Andrew McCallum 
A very elegant (but computationally challenging) model which induces 
correlation amongst topics using a multi-level DAG whose interior nodes 
are “super-topics” and “sub-topics” and whose leaves are the 
vocabulary words. Makes the slumbering monster of structure learning stir. 
  
Sequence Analysis (I missed these talks since I was chairing another session)
  
   
Online Decoding of Markov Models with Latency Constraints  
Mukund Narasimhan, Paul Viola, Michael Shilman 
An “a</p><p>6 0.49551049 <a title="192-lsi-6" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>7 0.48281154 <a title="192-lsi-7" href="../hunch_net-2006/hunch_net-2006-06-30-ICML_papers.html">188 hunch net-2006-06-30-ICML papers</a></p>
<p>8 0.46256679 <a title="192-lsi-8" href="../hunch_net-2005/hunch_net-2005-12-28-Yet_more_nips_thoughts.html">144 hunch net-2005-12-28-Yet more nips thoughts</a></p>
<p>9 0.46239778 <a title="192-lsi-9" href="../hunch_net-2009/hunch_net-2009-06-24-Interesting_papers_at_UAICMOLT_2009.html">361 hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</a></p>
<p>10 0.45402041 <a title="192-lsi-10" href="../hunch_net-2005/hunch_net-2005-12-11-More_NIPS_Papers.html">139 hunch net-2005-12-11-More NIPS Papers</a></p>
<p>11 0.45005882 <a title="192-lsi-11" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>12 0.44737846 <a title="192-lsi-12" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>13 0.44504222 <a title="192-lsi-13" href="../hunch_net-2011/hunch_net-2011-08-01-Interesting_papers_at_COLT_2011.html">439 hunch net-2011-08-01-Interesting papers at COLT 2011</a></p>
<p>14 0.44042155 <a title="192-lsi-14" href="../hunch_net-2009/hunch_net-2009-12-27-Interesting_things_at_NIPS_2009.html">385 hunch net-2009-12-27-Interesting things at NIPS 2009</a></p>
<p>15 0.42759708 <a title="192-lsi-15" href="../hunch_net-2005/hunch_net-2005-02-23-Problem%3A_Reinforcement_Learning_with_Classification.html">27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</a></p>
<p>16 0.41686475 <a title="192-lsi-16" href="../hunch_net-2005/hunch_net-2005-04-21-Dynamic_Programming_Generalizations_and_Their_Use.html">58 hunch net-2005-04-21-Dynamic Programming Generalizations and Their Use</a></p>
<p>17 0.41598186 <a title="192-lsi-17" href="../hunch_net-2007/hunch_net-2007-12-20-Cool_and_Interesting_things_at_NIPS%2C_take_three.html">280 hunch net-2007-12-20-Cool and Interesting things at NIPS, take three</a></p>
<p>18 0.41501969 <a title="192-lsi-18" href="../hunch_net-2007/hunch_net-2007-12-19-Cool_and_interesting_things_seen_at_NIPS.html">279 hunch net-2007-12-19-Cool and interesting things seen at NIPS</a></p>
<p>19 0.41125736 <a title="192-lsi-19" href="../hunch_net-2007/hunch_net-2007-12-12-Workshop_Summary%26%238212%3BPrinciples_of_Learning_Problem_Design.html">277 hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</a></p>
<p>20 0.40867007 <a title="192-lsi-20" href="../hunch_net-2005/hunch_net-2005-09-20-Workshop_Proposal%3A_Atomic_Learning.html">114 hunch net-2005-09-20-Workshop Proposal: Atomic Learning</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(9, 0.062), (27, 0.162), (38, 0.022), (53, 0.073), (55, 0.083), (67, 0.507)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.88102734 <a title="192-lda-1" href="../hunch_net-2006/hunch_net-2006-07-08-Some_recent_papers.html">192 hunch net-2006-07-08-Some recent papers</a></p>
<p>Introduction: It was a fine time for learning in Pittsburgh. John and Sam mentioned some of my favorites. Here’s a few more worth checking out:
 
Online Multitask Learning 
Ofer Dekel, Phil Long, Yoram Singer 
This is on my reading list. Definitely an area I’m interested in.
 
Maximum Entropy Distribution Estimation with Generalized Regularization 
Miroslav DudÃƒÂ­k, Robert E. Schapire
 
Learning near-optimal policies with Bellman-residual minimization based fitted policy iteration and a single sample path 
AndrÃƒÂ¡s Antos, Csaba SzepesvÃƒÂ¡ri, RÃƒÂ©mi Munos 
Again, on the list to read. I saw Csaba and Remi talk about this and related work at an ICML Workshop on Kernel Reinforcement Learning. The big question in my head is how this compares/contrasts with existing work in  reductions to reinforcement learning.  Are there advantages/disadvantages?
 
 Higher Order Learning On Graphs>  by Sameer Agarwal, Kristin Branson, and Serge Belongie, looks to be interesteding. They seem to poo-poo “tensorization</p><p>2 0.72943938 <a title="192-lda-2" href="../hunch_net-2008/hunch_net-2008-04-21-The_Science_2.0_article.html">296 hunch net-2008-04-21-The Science 2.0 article</a></p>
<p>Introduction: I found the article about  science using modern tools interesting , especially the part about ‘blogophobia’, which in my experience is often a substantial issue: many potential guest posters aren’t quite ready, because of the fear of a permanent public mistake, because it is particularly hard to write about the unknown (the essence of research), and because the system for public credit doesn’t yet really handle blog posts.
 
So far, science has been relatively resistant to discussing research on blogs.  Some things need to change to get there.  Public tolerance of the occasional mistake is essential, as is a willingness to cite (and credit) blogs as freely as papers.  
 
I’ve often run into another reason for holding back myself: I don’t want to overtalk my own research.  Nevertheless, I’m slowly changing to the opinion that I’m holding back too much: the real power of a blog in research is that it can be used to confer with many people, and that just makes research work better.</p><p>3 0.70688289 <a title="192-lda-3" href="../hunch_net-2006/hunch_net-2006-05-21-NIPS_paper_evaluation_criteria.html">180 hunch net-2006-05-21-NIPS paper evaluation criteria</a></p>
<p>Introduction: John Platt , who is PC-chair for  NIPS 2006  has organized a  NIPS paper evaluation criteria  document with input from the  program committee  and others.  
 
The document contains specific advice about what is appropriate for the various subareas within NIPS.  It may be very helpful, because the standards of evaluation for papers varies significantly.  
 
This is a bit of an experiment: the hope is that by carefully thinking about and stating what is important, authors can better understand whether and where their work fits.
 
Update: The  general submission page  and  Author instruction including how to submit an appendix .</p><p>4 0.54427552 <a title="192-lda-4" href="../hunch_net-2012/hunch_net-2012-05-02-ICML%3A_Behind_the_Scenes.html">463 hunch net-2012-05-02-ICML: Behind the Scenes</a></p>
<p>Introduction: This is a rather long post, detailing the ICML 2012 review process. The goal is to make the process more transparent, help authors understand how we came to a decision, and discuss the strengths and weaknesses of this process for future conference organizers.
 
 Microsoft’s Conference Management Toolkit (CMT)  
We chose to use  CMT  over other conference management software mainly because of its rich toolkit. The interface is sub-optimal (to say the least!) but it has extensive capabilities (to handle bids, author response, resubmissions, etc.), good import/export mechanisms (to process the data elsewhere), excellent technical support (to answer late night emails, add new functionalities).  Overall, it was the right choice, although we hope a designer will look at that interface sometime soon!
 
 Toronto Matching System (TMS)  
  TMS  is now being used by many major conferences in our field (including NIPS and UAI). It is an automated system (developed by  Laurent Charlin  and  Rich Ze</p><p>5 0.46089193 <a title="192-lda-5" href="../hunch_net-2007/hunch_net-2007-07-01-Watchword%3A_Online_Learning.html">252 hunch net-2007-07-01-Watchword: Online Learning</a></p>
<p>Introduction: It turns out that many different people use the term “Online Learning”, and often they don’t have the same definition in mind.  Here’s a list of the possibilities I know of.  
  
  Online Information Setting   Online learning refers to a  problem  in which unlabeled data comes, a prediction is made, and then feedback is acquired. 
  Online Adversarial Setting  Online learning refers to  algorithms  in the Online Information Setting which satisfy guarantees of the form: “For all possible sequences of observations, the algorithim has regret at most  log ( number of strategies)  with respect to the best strategy in a set.”  This is sometimes called online learning with experts. 
  Online Optimization Constraint  Online learning refers to optimizing a predictor via a learning algorithm tunes parameters on a per-example basis.  This may or may not be applied in the Online Information Setting, and the strategy may or may not satisfy Adversarial setting theory. 
  Online Computational Constra</p><p>6 0.41037065 <a title="192-lda-6" href="../hunch_net-2005/hunch_net-2005-03-15-The_State_of_Tight_Bounds.html">41 hunch net-2005-03-15-The State of Tight Bounds</a></p>
<p>7 0.40123907 <a title="192-lda-7" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>8 0.37266579 <a title="192-lda-8" href="../hunch_net-2005/hunch_net-2005-05-29-Maximum_Margin_Mismatch%3F.html">77 hunch net-2005-05-29-Maximum Margin Mismatch?</a></p>
<p>9 0.35659432 <a title="192-lda-9" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>10 0.35621729 <a title="192-lda-10" href="../hunch_net-2013/hunch_net-2013-01-07-NYU_Large_Scale_Machine_Learning_Class.html">478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</a></p>
<p>11 0.35277271 <a title="192-lda-11" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<p>12 0.34925506 <a title="192-lda-12" href="../hunch_net-2006/hunch_net-2006-01-25-1_year.html">151 hunch net-2006-01-25-1 year</a></p>
<p>13 0.34544057 <a title="192-lda-13" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>14 0.3437283 <a title="192-lda-14" href="../hunch_net-2009/hunch_net-2009-06-24-Interesting_papers_at_UAICMOLT_2009.html">361 hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</a></p>
<p>15 0.34371412 <a title="192-lda-15" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>16 0.341968 <a title="192-lda-16" href="../hunch_net-2007/hunch_net-2007-01-02-Retrospective.html">225 hunch net-2007-01-02-Retrospective</a></p>
<p>17 0.34053344 <a title="192-lda-17" href="../hunch_net-2006/hunch_net-2006-04-27-Conferences%2C_Workshops%2C_and_Tutorials.html">174 hunch net-2006-04-27-Conferences, Workshops, and Tutorials</a></p>
<p>18 0.33930168 <a title="192-lda-18" href="../hunch_net-2006/hunch_net-2006-02-18-Multiplication_of_Learned_Probabilities_is_Dangerous.html">157 hunch net-2006-02-18-Multiplication of Learned Probabilities is Dangerous</a></p>
<p>19 0.33866367 <a title="192-lda-19" href="../hunch_net-2008/hunch_net-2008-12-07-A_NIPS_paper.html">330 hunch net-2008-12-07-A NIPS paper</a></p>
<p>20 0.33741403 <a title="192-lda-20" href="../hunch_net-2006/hunch_net-2006-01-18-Is_Multitask_Learning_Black-Boxable%3F.html">149 hunch net-2006-01-18-Is Multitask Learning Black-Boxable?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
