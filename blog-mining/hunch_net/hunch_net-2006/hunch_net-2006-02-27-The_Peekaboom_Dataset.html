<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>159 hunch net-2006-02-27-The Peekaboom Dataset</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-159" href="#">hunch_net-2006-159</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>159 hunch net-2006-02-27-The Peekaboom Dataset</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-159-html" href="http://hunch.net/?p=170">html</a></p><p>Introduction: Luis von Ahn'sPeekaboom projecthas yieldeddata(830MB).Peekaboom is the second
attempt (afterEspgame) to produce a dataset which is useful for learning to
solve vision problems based on voluntary game play. As a second attempt, it is
meant to address all of the shortcomings of the first attempt. In
particular:The locations of specific objects are provided by the data.The data
collection is far more complete and extensive.The data consists of:The source
images. (1 file per image, just short of 60K images.)The in-game events. (1
file per image, in a lispy syntax.)A description of the event language.There
is a great deal of very specific and relevant data here so the hope that this
will help solve vision problems seems quite reasonable.</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('file', 0.341), ('image', 0.283), ('vision', 0.25), ('attempt', 0.222), ('specific', 0.222), ('projecthas', 0.205), ('shortcomings', 0.205), ('voluntary', 0.205), ('per', 0.197), ('luis', 0.19), ('locations', 0.179), ('von', 0.171), ('consists', 0.171), ('second', 0.169), ('data', 0.164), ('collection', 0.153), ('objects', 0.149), ('provided', 0.145), ('solve', 0.143), ('meant', 0.132)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="159-tfidf-1" href="../hunch_net-2006/hunch_net-2006-02-27-The_Peekaboom_Dataset.html">159 hunch net-2006-02-27-The Peekaboom Dataset</a></p>
<p>Introduction: Luis von Ahn'sPeekaboom projecthas yieldeddata(830MB).Peekaboom is the second
attempt (afterEspgame) to produce a dataset which is useful for learning to
solve vision problems based on voluntary game play. As a second attempt, it is
meant to address all of the shortcomings of the first attempt. In
particular:The locations of specific objects are provided by the data.The data
collection is far more complete and extensive.The data consists of:The source
images. (1 file per image, just short of 60K images.)The in-game events. (1
file per image, in a lispy syntax.)A description of the event language.There
is a great deal of very specific and relevant data here so the hope that this
will help solve vision problems seems quite reasonable.</p><p>2 0.18997753 <a title="159-tfidf-2" href="../hunch_net-2005/hunch_net-2005-02-15-ESPgame_and_image_labeling.html">20 hunch net-2005-02-15-ESPgame and image labeling</a></p>
<p>Introduction: Luis von Ahnhas been running theespgamefor awhile now. The espgame provides a
picture to two randomly paired people across the web, and asks them to agree
on a label. It hasn't managed to label the web yet, but it has produced alarge
datasetof (image, label) pairs. I organized the dataset so you couldexplore
the implied bipartite graph(requires much bandwidth).Relative to other image
datasets, this one is quite large--67000 images, 358,000 labels (average of
5/image with variation from 1 to 19), and 22,000 unique labels (one every 3
images). The dataset is also very 'natural', consisting of images spidered
from the internet. The multiple label characteristic is intriguing because
'learning to learn' and metalearning techniques may be applicable. The
'natural' quality means that this dataset varies greatly in difficulty from
easy (predicting "red") to hard (predicting "funny") and potentially more
rewarding to tackle.The open problem here is, of course, to make an internet
image labelin</p><p>3 0.14789689 <a title="159-tfidf-3" href="../hunch_net-2005/hunch_net-2005-08-01-Peekaboom.html">99 hunch net-2005-08-01-Peekaboom</a></p>
<p>Introduction: Luishas releasedPeekabooma successor toESPgame(game site). The purpose of the
game is similar--using the actions of people playing a game to gather data
helpful in solving AI.Peekaboom gathers more detailed, and perhaps more
useful, data about vision. For ESPgame, the byproduct of the game was mutually
agreed upon labels for common images. For Peekaboom, the location of the
subimage generating the label is revealed by the game as well. Given knowledge
about what portion of the image is related to a label it may be more feasible
learn to recognize the appropriate parts.There isn't a dataset yet available
for this game as there is for ESPgame, but hopefully a significant number of
people will play and we'll have one to work wtih soon.</p><p>4 0.089469895 <a title="159-tfidf-4" href="../hunch_net-2008/hunch_net-2008-01-28-Sufficient_Computation.html">287 hunch net-2008-01-28-Sufficient Computation</a></p>
<p>Introduction: Do we have computer hardware sufficient for AI? This question is difficult to
answer, but here's a try:One way to achieve AI is by simulating a human brain.
A human brain has about 1015synapses which operate at about 102per second
implying about 1017bit ops per second.A modern computer runs at
109cycles/second and operates on 102bits per cycle implying 1011bits processed
per second.The gap here is only 6 orders of magnitude, which can be plausibly
surpassed via cluster machines. For example, theBlueGene/Loperates 105nodes
(one order of magnitude short). It's peak recorded performance is about
0.5*1015FLOPS which translates to about 1016bit ops per second, which is
nearly 1017.There are many criticisms (both positive and negative) for this
argument.Simulation of a human brain might require substantially more detail.
Perhaps an additional 102is required per neuron.We may not need to simulate a
human brain to achieve AI. There are certainly many examples where we have
been able to design</p><p>5 0.088862248 <a title="159-tfidf-5" href="../hunch_net-2006/hunch_net-2006-09-28-Programming_Languages_for_Machine_Learning_Implementations.html">210 hunch net-2006-09-28-Programming Languages for Machine Learning Implementations</a></p>
<p>Introduction: Machine learning algorithms have a much better chance of being widely adopted
if they are implemented in some easy-to-use code. There are several important
concerns associated with machine learning which stress programming languages
on the ease-of-use vs. speed frontier.SpeedThe rate at which data sources are
growing seems to be outstripping the rate at which computational power is
growing, so it is important that we be able to eak out every bit of
computational power. Garbage collected languages (java,ocaml,perlandpython)
often have several issues here.Garbage collection often implies that floating
point numbers are "boxed": every float is represented by a pointer to a float.
Boxing can cause an order of magnitude slowdown because an extra nonlocalized
memory reference is made, and accesses to main memory can are many CPU cycles
long.Garbage collection often implies that considerably more memory is used
than is necessary. This has a variable effect. In some circumstances it
results in</p><p>6 0.087593876 <a title="159-tfidf-6" href="../hunch_net-2006/hunch_net-2006-12-06-The_Spam_Problem.html">223 hunch net-2006-12-06-The Spam Problem</a></p>
<p>7 0.083314762 <a title="159-tfidf-7" href="../hunch_net-2006/hunch_net-2006-04-02-Mad_%28Neuro%29science.html">168 hunch net-2006-04-02-Mad (Neuro)science</a></p>
<p>8 0.077865571 <a title="159-tfidf-8" href="../hunch_net-2007/hunch_net-2007-08-25-The_Privacy_Problem.html">260 hunch net-2007-08-25-The Privacy Problem</a></p>
<p>9 0.072979495 <a title="159-tfidf-9" href="../hunch_net-2010/hunch_net-2010-04-14-MLcomp%3A_a_website_for_objectively_comparing_ML_algorithms.html">393 hunch net-2010-04-14-MLcomp: a website for objectively comparing ML algorithms</a></p>
<p>10 0.072681203 <a title="159-tfidf-10" href="../hunch_net-2006/hunch_net-2006-03-02-Why_do_people_count_for_learning%3F.html">160 hunch net-2006-03-02-Why do people count for learning?</a></p>
<p>11 0.069834255 <a title="159-tfidf-11" href="../hunch_net-2005/hunch_net-2005-05-29-Maximum_Margin_Mismatch%3F.html">77 hunch net-2005-05-29-Maximum Margin Mismatch?</a></p>
<p>12 0.067119502 <a title="159-tfidf-12" href="../hunch_net-2005/hunch_net-2005-05-10-Learning_Reductions_are_Reductionist.html">68 hunch net-2005-05-10-Learning Reductions are Reductionist</a></p>
<p>13 0.067071728 <a title="159-tfidf-13" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>14 0.06661129 <a title="159-tfidf-14" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>15 0.066134371 <a title="159-tfidf-15" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>16 0.065778539 <a title="159-tfidf-16" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>17 0.065659896 <a title="159-tfidf-17" href="../hunch_net-2005/hunch_net-2005-12-07-Is_the_Google_way_the_way_for_machine_learning%3F.html">136 hunch net-2005-12-07-Is the Google way the way for machine learning?</a></p>
<p>18 0.065625243 <a title="159-tfidf-18" href="../hunch_net-2009/hunch_net-2009-04-21-Interesting_Presentations_at_Snowbird.html">349 hunch net-2009-04-21-Interesting Presentations at Snowbird</a></p>
<p>19 0.065526232 <a title="159-tfidf-19" href="../hunch_net-2005/hunch_net-2005-03-21-Research_Styles_in_Machine_Learning.html">44 hunch net-2005-03-21-Research Styles in Machine Learning</a></p>
<p>20 0.062545493 <a title="159-tfidf-20" href="../hunch_net-2005/hunch_net-2005-03-24-The_Role_of_Workshops.html">46 hunch net-2005-03-24-The Role of Workshops</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.122), (1, -0.024), (2, 0.061), (3, -0.018), (4, -0.045), (5, 0.052), (6, -0.075), (7, -0.003), (8, 0.067), (9, 0.054), (10, -0.03), (11, 0.039), (12, 0.038), (13, 0.025), (14, 0.055), (15, -0.088), (16, 0.071), (17, 0.023), (18, 0.019), (19, 0.012), (20, 0.03), (21, 0.055), (22, 0.036), (23, -0.067), (24, -0.023), (25, 0.042), (26, -0.025), (27, -0.058), (28, -0.055), (29, -0.029), (30, 0.047), (31, -0.001), (32, 0.092), (33, -0.008), (34, -0.191), (35, -0.006), (36, 0.104), (37, 0.018), (38, 0.035), (39, -0.077), (40, -0.033), (41, 0.043), (42, -0.096), (43, 0.055), (44, -0.004), (45, -0.125), (46, -0.022), (47, 0.16), (48, 0.035), (49, 0.012)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97004086 <a title="159-lsi-1" href="../hunch_net-2006/hunch_net-2006-02-27-The_Peekaboom_Dataset.html">159 hunch net-2006-02-27-The Peekaboom Dataset</a></p>
<p>Introduction: Luis von Ahn'sPeekaboom projecthas yieldeddata(830MB).Peekaboom is the second
attempt (afterEspgame) to produce a dataset which is useful for learning to
solve vision problems based on voluntary game play. As a second attempt, it is
meant to address all of the shortcomings of the first attempt. In
particular:The locations of specific objects are provided by the data.The data
collection is far more complete and extensive.The data consists of:The source
images. (1 file per image, just short of 60K images.)The in-game events. (1
file per image, in a lispy syntax.)A description of the event language.There
is a great deal of very specific and relevant data here so the hope that this
will help solve vision problems seems quite reasonable.</p><p>2 0.8350721 <a title="159-lsi-2" href="../hunch_net-2005/hunch_net-2005-08-01-Peekaboom.html">99 hunch net-2005-08-01-Peekaboom</a></p>
<p>Introduction: Luishas releasedPeekabooma successor toESPgame(game site). The purpose of the
game is similar--using the actions of people playing a game to gather data
helpful in solving AI.Peekaboom gathers more detailed, and perhaps more
useful, data about vision. For ESPgame, the byproduct of the game was mutually
agreed upon labels for common images. For Peekaboom, the location of the
subimage generating the label is revealed by the game as well. Given knowledge
about what portion of the image is related to a label it may be more feasible
learn to recognize the appropriate parts.There isn't a dataset yet available
for this game as there is for ESPgame, but hopefully a significant number of
people will play and we'll have one to work wtih soon.</p><p>3 0.80326378 <a title="159-lsi-3" href="../hunch_net-2005/hunch_net-2005-02-15-ESPgame_and_image_labeling.html">20 hunch net-2005-02-15-ESPgame and image labeling</a></p>
<p>Introduction: Luis von Ahnhas been running theespgamefor awhile now. The espgame provides a
picture to two randomly paired people across the web, and asks them to agree
on a label. It hasn't managed to label the web yet, but it has produced alarge
datasetof (image, label) pairs. I organized the dataset so you couldexplore
the implied bipartite graph(requires much bandwidth).Relative to other image
datasets, this one is quite large--67000 images, 358,000 labels (average of
5/image with variation from 1 to 19), and 22,000 unique labels (one every 3
images). The dataset is also very 'natural', consisting of images spidered
from the internet. The multiple label characteristic is intriguing because
'learning to learn' and metalearning techniques may be applicable. The
'natural' quality means that this dataset varies greatly in difficulty from
easy (predicting "red") to hard (predicting "funny") and potentially more
rewarding to tackle.The open problem here is, of course, to make an internet
image labelin</p><p>4 0.57270098 <a title="159-lsi-4" href="../hunch_net-2006/hunch_net-2006-12-06-The_Spam_Problem.html">223 hunch net-2006-12-06-The Spam Problem</a></p>
<p>Introduction: TheNew York Timeshas an article on thegrowth of spam. Interesting facts
include: 9/10 of all email is spam, spam source identification is nearly
useless due to botnet spam senders, and image based spam (emails which consist
of an image only) are on the growth.Estimates of the cost of spam are almost
certainly far to low, because they do not account for the cost in time lost by
people.The image based spam which is currently penetrating many filters should
be catchable with a more sophisticated application of machine learning
technology. For the spam I see, the rendered images come in only a few
formats, which would be easy to recognize via a support vector machine (with
RBF kernel), neural network, or even nearest-neighbor architecture. The
mechanics of setting this up to run efficiently is the only real challenge.
This is the next step in the spam war.The response to this system is to make
the image based spam even more random. We should (essentially) expect to
seeCaptchaspam, and our</p><p>5 0.52080417 <a title="159-lsi-5" href="../hunch_net-2006/hunch_net-2006-02-07-Pittsburgh_Mind_Reading_Competition.html">155 hunch net-2006-02-07-Pittsburgh Mind Reading Competition</a></p>
<p>Introduction: Francisco Pereirapoints out a funPrediction Competition. Francisco says:DARPA
is sponsoring a competition to analyze data from an unusual functional
Magnetic Resonance Imaging experiment. Subjects watch videos inside the
scanner while fMRI data are acquired. Unbeknownst to these subjects, the
videos have been seen by a panel of other subjects that labeled each instant
with labels in categories such as representation (are there tools, body parts,
motion, sound), location, presence of actors, emotional content, etc.The
challenge is to predict all of these different labels on an instant-by-instant
basis from the fMRI data. A few reasons why this is particularly
interesting:This is beyond the current state of the art, but not inconceivably
hard.This is a new type of experiment design current analysis methods cannot
deal with.This is an opportunity to work with a heavily examined and
preprocessed neuroimaging dataset.DARPA is offering prizes!</p><p>6 0.46496868 <a title="159-lsi-6" href="../hunch_net-2009/hunch_net-2009-04-21-Interesting_Presentations_at_Snowbird.html">349 hunch net-2009-04-21-Interesting Presentations at Snowbird</a></p>
<p>7 0.45923772 <a title="159-lsi-7" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>8 0.45569003 <a title="159-lsi-8" href="../hunch_net-2005/hunch_net-2005-08-11-Why_Manifold-Based_Dimension_Reduction_Techniques%3F.html">102 hunch net-2005-08-11-Why Manifold-Based Dimension Reduction Techniques?</a></p>
<p>9 0.43919289 <a title="159-lsi-9" href="../hunch_net-2005/hunch_net-2005-05-29-Maximum_Margin_Mismatch%3F.html">77 hunch net-2005-05-29-Maximum Margin Mismatch?</a></p>
<p>10 0.41061169 <a title="159-lsi-10" href="../hunch_net-2007/hunch_net-2007-12-12-Workshop_Summary%26%238212%3BPrinciples_of_Learning_Problem_Design.html">277 hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</a></p>
<p>11 0.40540916 <a title="159-lsi-11" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>12 0.39977613 <a title="159-lsi-12" href="../hunch_net-2006/hunch_net-2006-05-21-NIPS_paper_evaluation_criteria.html">180 hunch net-2006-05-21-NIPS paper evaluation criteria</a></p>
<p>13 0.39894706 <a title="159-lsi-13" href="../hunch_net-2005/hunch_net-2005-12-28-Yet_more_nips_thoughts.html">144 hunch net-2005-12-28-Yet more nips thoughts</a></p>
<p>14 0.38757738 <a title="159-lsi-14" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>15 0.38433963 <a title="159-lsi-15" href="../hunch_net-2005/hunch_net-2005-04-27-DARPA_project%3A_LAGR.html">63 hunch net-2005-04-27-DARPA project: LAGR</a></p>
<p>16 0.38392031 <a title="159-lsi-16" href="../hunch_net-2005/hunch_net-2005-12-07-Is_the_Google_way_the_way_for_machine_learning%3F.html">136 hunch net-2005-12-07-Is the Google way the way for machine learning?</a></p>
<p>17 0.3779752 <a title="159-lsi-17" href="../hunch_net-2010/hunch_net-2010-04-14-MLcomp%3A_a_website_for_objectively_comparing_ML_algorithms.html">393 hunch net-2010-04-14-MLcomp: a website for objectively comparing ML algorithms</a></p>
<p>18 0.37782794 <a title="159-lsi-18" href="../hunch_net-2005/hunch_net-2005-04-25-Embeddings%3A_what_are_they_good_for%3F.html">61 hunch net-2005-04-25-Embeddings: what are they good for?</a></p>
<p>19 0.37153181 <a title="159-lsi-19" href="../hunch_net-2011/hunch_net-2011-10-03-Monday_announcements.html">446 hunch net-2011-10-03-Monday announcements</a></p>
<p>20 0.36825448 <a title="159-lsi-20" href="../hunch_net-2006/hunch_net-2006-02-11-Yahoo%26%238217%3Bs_Learning_Problems..html">156 hunch net-2006-02-11-Yahoo&#8217;s Learning Problems.</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(18, 0.435), (35, 0.106), (42, 0.248), (74, 0.072)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.90573442 <a title="159-lda-1" href="../hunch_net-2006/hunch_net-2006-02-27-The_Peekaboom_Dataset.html">159 hunch net-2006-02-27-The Peekaboom Dataset</a></p>
<p>Introduction: Luis von Ahn'sPeekaboom projecthas yieldeddata(830MB).Peekaboom is the second
attempt (afterEspgame) to produce a dataset which is useful for learning to
solve vision problems based on voluntary game play. As a second attempt, it is
meant to address all of the shortcomings of the first attempt. In
particular:The locations of specific objects are provided by the data.The data
collection is far more complete and extensive.The data consists of:The source
images. (1 file per image, just short of 60K images.)The in-game events. (1
file per image, in a lispy syntax.)A description of the event language.There
is a great deal of very specific and relevant data here so the hope that this
will help solve vision problems seems quite reasonable.</p><p>2 0.77738845 <a title="159-lda-2" href="../hunch_net-2014/hunch_net-2014-02-16-Metacademy%3A_a_package_manager_for_knowledge.html">493 hunch net-2014-02-16-Metacademy: a package manager for knowledge</a></p>
<p>Introduction: In recent years, there’s been an explosion of free educational resources that
make high-level knowledge and skills accessible to an ever-wider group of
people. In your own field, you probably have a good idea of where to look for
the answer to any particular question. But outside your areas of expertise,
sifting through textbooks, Wikipedia articles, research papers, and online
lectures can be bewildering (unless you’re fortunate enough to have a
knowledgeable colleague to consult). What are the key concepts in the field,
how do they relate to each other, which ones should you learn, and where
should you learn them?Courses are a major vehicle for packaging educational
materials for a broad audience. The trouble is that they’re typically meant to
be consumed linearly, regardless of your specific background or goals. Also,
unless thousands of other people have had the same background and learning
goals, there may not even be a course that fits your needs. Recently, we
(Roger GrosseandCol</p><p>3 0.76408017 <a title="159-lda-3" href="../hunch_net-2009/hunch_net-2009-10-10-ALT_2009.html">374 hunch net-2009-10-10-ALT 2009</a></p>
<p>Introduction: I attendedALT("Algorithmic Learning Theory") for the first time this year. My
impression is ALT = 0.5 COLT, by attendance and also by some more intangible
"what do I get from it?" measure. There are many differences which can't quite
be described this way though. The program for ALT seems to be substantially
more diverse than COLT, which is both a weakness and a strength.One paper that
might interest people generally is:Alexey ChernovandVladimir Vovk,Prediction
with Expert Evaluators' Advice. The basic observation here is that in the
online learning with experts setting you can simultaneously compete with
several compatible loss functions simultaneously. Restated, debating between
competing with log loss and squared loss is a waste of breath, because it's
almost free to compete with them both simultaneously. This might interest
anyone who has run into "which loss function?" debates that come up
periodically.</p><p>4 0.6788919 <a title="159-lda-4" href="../hunch_net-2006/hunch_net-2006-02-11-Yahoo%26%238217%3Bs_Learning_Problems..html">156 hunch net-2006-02-11-Yahoo&#8217;s Learning Problems.</a></p>
<p>Introduction: I just visitedYahoo Researchwhich has several fundamental learning problems
near to (or beyond) the set of problems we know how to solve well. Here are 3
of them.RankingThis is the canonical problem of all search engines. It is made
extra difficult for several reasons.There is relatively little "good"
supervised learning data and a great deal of data with some signal (such as
click through rates).The learning must occur in a partially adversarial
environment. Many people very actively attempt to place themselves at the top
ofrankings.It is not even quite clear whether the problem should be posed as
'ranking' or as 'regression' which is then used to produce
aranking.Collaborative filteringYahoo has a large number of recommendation
systems for music, movies, etcâ&euro;Ś In these sorts of systems, users specify how
they liked a set of things, and then the system can (hopefully) find some more
examples of things they might likeby reasoning across multiple such
sets.Exploration with Generalization</p><p>5 0.52272928 <a title="159-lda-5" href="../hunch_net-2007/hunch_net-2007-01-15-The_Machine_Learning_Department.html">228 hunch net-2007-01-15-The Machine Learning Department</a></p>
<p>Introduction: Carnegie MellonSchool of Computer Sciencehas the first academicMachine
Learning department. This department already existed as theCenter for
Automated Learning and Discovery, but recently changed it's name.The reason
for changing the name is obvious: very few people think of themselves as
"Automated Learner and Discoverers", but there are number of people who think
of themselves as "Machine Learners". Machine learning is both more succinct
and recognizable--good properties for a name.A more interesting question is
"Should there be a Machine Learning Department?".Tom Mitchellhas a
relevantwhitepaperclaiming that machine learning is answering a different
question than other fields or departments. The fundamental debate here is "Is
machine learning different from statistics?"At a cultural level, there is no
real debate: they are different. Machine learning is characterized by several
very active large peer reviewed conferences, operating in a computer science
mode. Statistics tends to fun</p><p>6 0.52202821 <a title="159-lda-6" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>7 0.51292604 <a title="159-lda-7" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">23 hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<p>8 0.51186132 <a title="159-lda-8" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>9 0.51184702 <a title="159-lda-9" href="../hunch_net-2005/hunch_net-2005-04-10-Is_the_Goal_Understanding_or_Prediction%3F.html">55 hunch net-2005-04-10-Is the Goal Understanding or Prediction?</a></p>
<p>10 0.50995344 <a title="159-lda-10" href="../hunch_net-2006/hunch_net-2006-09-28-Programming_Languages_for_Machine_Learning_Implementations.html">210 hunch net-2006-09-28-Programming Languages for Machine Learning Implementations</a></p>
<p>11 0.50881201 <a title="159-lda-11" href="../hunch_net-2007/hunch_net-2007-10-17-Online_as_the_new_adjective.html">267 hunch net-2007-10-17-Online as the new adjective</a></p>
<p>12 0.50825071 <a title="159-lda-12" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>13 0.50815105 <a title="159-lda-13" href="../hunch_net-2005/hunch_net-2005-05-16-Regret_minimizing_vs_error_limiting_reductions.html">72 hunch net-2005-05-16-Regret minimizing vs error limiting reductions</a></p>
<p>14 0.50806767 <a title="159-lda-14" href="../hunch_net-2005/hunch_net-2005-05-21-What_is_the_right_form_of_modularity_in_structured_prediction%3F.html">74 hunch net-2005-05-21-What is the right form of modularity in structured prediction?</a></p>
<p>15 0.50758946 <a title="159-lda-15" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>16 0.50758475 <a title="159-lda-16" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>17 0.5070585 <a title="159-lda-17" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>18 0.50673634 <a title="159-lda-18" href="../hunch_net-2007/hunch_net-2007-01-10-A_Deep_Belief_Net_Learning_Problem.html">227 hunch net-2007-01-10-A Deep Belief Net Learning Problem</a></p>
<p>19 0.50666636 <a title="159-lda-19" href="../hunch_net-2007/hunch_net-2007-01-02-Retrospective.html">225 hunch net-2007-01-02-Retrospective</a></p>
<p>20 0.50570041 <a title="159-lda-20" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
