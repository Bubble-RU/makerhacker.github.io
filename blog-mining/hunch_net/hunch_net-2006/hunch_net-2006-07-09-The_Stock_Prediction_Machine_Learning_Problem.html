<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>193 hunch net-2006-07-09-The Stock Prediction Machine Learning Problem</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-193" href="#">hunch_net-2006-193</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>193 hunch net-2006-07-09-The Stock Prediction Machine Learning Problem</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-193-html" href="http://hunch.net/?p=210">html</a></p><p>Introduction: …is discussed in  this nytimes article .  I generally expect such approaches to become more common since computers are getting faster, machine learning is getting better, and data is becoming more plentiful.   This is another example where machine learning technology may have a huge economic impact.  Some side notes:
  
 We-in-research know almost nothing about how these things are done (because it is typically a corporate secret). 
 … but the limited discussion in the article seem naive from a machine learning viewpoint.
 
 The learning process used apparently often fails to take into account transaction costs. 
 What little of the approaches is discussed appears modeling based.  It seems plausible that more direct prediction methods can yield an edge. 
 
 
 One difficulty with stock picking as a research topic is that it is inherently a zero sum game (for every winner, there is a loser).  Much of the rest of research is positive sum (basically, everyone wins).</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 I generally expect such approaches to become more common since computers are getting faster, machine learning is getting better, and data is becoming more plentiful. [sent-2, score-1.13]
</p><p>2 This is another example where machine learning technology may have a huge economic impact. [sent-3, score-0.455]
</p><p>3 Some side notes:     We-in-research know almost nothing about how these things are done (because it is typically a corporate secret). [sent-4, score-0.474]
</p><p>4 … but the limited discussion in the article seem naive from a machine learning viewpoint. [sent-5, score-0.695]
</p><p>5 The learning process used apparently often fails to take into account transaction costs. [sent-6, score-0.587]
</p><p>6 What little of the approaches is discussed appears modeling based. [sent-7, score-0.588]
</p><p>7 It seems plausible that more direct prediction methods can yield an edge. [sent-8, score-0.422]
</p><p>8 One difficulty with stock picking as a research topic is that it is inherently a zero sum game (for every winner, there is a loser). [sent-9, score-1.259]
</p><p>9 Much of the rest of research is positive sum (basically, everyone wins). [sent-10, score-0.656]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('article', 0.268), ('sum', 0.244), ('nytimes', 0.203), ('transaction', 0.203), ('discussed', 0.197), ('getting', 0.195), ('secret', 0.188), ('wins', 0.177), ('corporate', 0.169), ('approaches', 0.159), ('naive', 0.152), ('economic', 0.152), ('stock', 0.147), ('apparently', 0.147), ('picking', 0.143), ('modeling', 0.14), ('winner', 0.137), ('basically', 0.134), ('notes', 0.134), ('zero', 0.131), ('computers', 0.129), ('fails', 0.129), ('direct', 0.129), ('game', 0.126), ('becoming', 0.126), ('nothing', 0.124), ('positive', 0.116), ('technology', 0.113), ('rest', 0.111), ('inherently', 0.11), ('yield', 0.11), ('faster', 0.11), ('account', 0.108), ('side', 0.108), ('limited', 0.106), ('plausible', 0.104), ('topic', 0.103), ('huge', 0.102), ('everyone', 0.096), ('difficulty', 0.092), ('appears', 0.092), ('research', 0.089), ('machine', 0.088), ('become', 0.084), ('discussion', 0.081), ('methods', 0.079), ('expect', 0.078), ('generally', 0.076), ('every', 0.074), ('almost', 0.073)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="193-tfidf-1" href="../hunch_net-2006/hunch_net-2006-07-09-The_Stock_Prediction_Machine_Learning_Problem.html">193 hunch net-2006-07-09-The Stock Prediction Machine Learning Problem</a></p>
<p>Introduction: …is discussed in  this nytimes article .  I generally expect such approaches to become more common since computers are getting faster, machine learning is getting better, and data is becoming more plentiful.   This is another example where machine learning technology may have a huge economic impact.  Some side notes:
  
 We-in-research know almost nothing about how these things are done (because it is typically a corporate secret). 
 … but the limited discussion in the article seem naive from a machine learning viewpoint.
 
 The learning process used apparently often fails to take into account transaction costs. 
 What little of the approaches is discussed appears modeling based.  It seems plausible that more direct prediction methods can yield an edge. 
 
 
 One difficulty with stock picking as a research topic is that it is inherently a zero sum game (for every winner, there is a loser).  Much of the rest of research is positive sum (basically, everyone wins).</p><p>2 0.13623413 <a title="193-tfidf-2" href="../hunch_net-2005/hunch_net-2005-09-14-The_Predictionist_Viewpoint.html">112 hunch net-2005-09-14-The Predictionist Viewpoint</a></p>
<p>Introduction: Virtually every discipline of significant human endeavor has a way explaining itself as fundamental and important.  In all the cases I know of, they are both right (they are vital) and wrong (they are not solely vital).
  
 Politics.  This is the one that everyone is familiar with at the moment.  “What could be more important than the process of making decisions?” 
 Science and Technology.  This is the one that we-the-academics are familiar with.  “The loss of modern science and technology would be catastrophic.” 
 Military.  “Without the military, a nation will be invaded and destroyed.” 
 (insert your favorite here) 
  
Within science and technology, the same thing happens again.
  
 Mathematics. “What could be more important than a precise language for establishing truths?” 
 Physics.  “Nothing is more fundamental than the laws which govern the universe.  Understanding them is the key to understanding everything else.” 
 Biology.  “Without life, we wouldn’t be here, so clearly the s</p><p>3 0.12096928 <a title="193-tfidf-3" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>Introduction: The internet has significantly effected the way we do research but it’s capabilities have not yet been fully realized.
 
First, let’s acknowledge some known effects.
  
  Self-publishing  By default, all researchers in machine learning (and more generally computer science and physics) place their papers online for anyone to download.  The exact mechanism differs—physicists tend to use a central repository ( Arxiv ) while computer scientists tend to place the papers on their webpage.  Arxiv has been slowly growing in subject breadth so it now sometimes used by computer scientists. 
  Collaboration  Email has enabled working remotely with coauthors.  This has allowed collaborationis which would not otherwise have been possible and generally speeds research. 
  
Now, let’s look at attempts to go further.
  
  Blogs  (like this one) allow public discussion about topics which are not easily categorized as “a new idea in machine learning” (like this topic). 
  Organization  of some subfield</p><p>4 0.11479516 <a title="193-tfidf-4" href="../hunch_net-2005/hunch_net-2005-07-21-Six_Months.html">96 hunch net-2005-07-21-Six Months</a></p>
<p>Introduction: This is the  6 month point  in the “run a research blog” experiment, so it seems like a good point to take stock and assess.  
 
One fundamental question is: “Is it worth it?”  The idea of running a research blog will never become widely popular and useful unless it actually aids research.  On the negative side, composing ideas for a post and maintaining a blog takes a significant amount of time.  On the positive side, the process might yield better research because there is an opportunity for better, faster feedback implying better, faster thinking.
 
My answer at the moment is a provisional “yes”.  Running the blog has been incidentally helpful in several ways:
  
 It is sometimes educational.  example  
 More often, the process of composing thoughts well enough to post simply aids thinking.  This has resulted in a couple solutions to problems of  interest  (and perhaps more over time).  If you really want to solve a problem, letting the world know is helpful.  This isn’t necessarily</p><p>5 0.10775023 <a title="193-tfidf-5" href="../hunch_net-2005/hunch_net-2005-09-04-Science_in_the_Government.html">106 hunch net-2005-09-04-Science in the Government</a></p>
<p>Introduction: I found the article on “ Political Science ” at the  New York Times  interesting.  Essentially the article is about allegations that the US government has been systematically distorting scientific views.   With a  petition  by some  7000+ scientists  alleging such behavior this is clearly a significant concern.
 
One thing not mentioned explicitly in this discussion is that there are fundamental cultural differences between academic research and the rest of the world.  In academic research, careful, clear thought is valued.  This value is achieved by both formal and informal mechanisms.  One example of a formal mechanism is peer review.
 
In contrast, in the land of politics, the basic value is agreement.  It is only with some amount of agreement that a new law can be passed or other actions can be taken.  Since Science (with a capitol ‘S’) has accomplished many things, it can be a significant tool in persuading people.  This makes it compelling for a politician to use science as a mec</p><p>6 0.087727301 <a title="193-tfidf-6" href="../hunch_net-2009/hunch_net-2009-08-03-Carbon_in_Computer_Science_Research.html">366 hunch net-2009-08-03-Carbon in Computer Science Research</a></p>
<p>7 0.086064264 <a title="193-tfidf-7" href="../hunch_net-2006/hunch_net-2006-02-18-Multiplication_of_Learned_Probabilities_is_Dangerous.html">157 hunch net-2006-02-18-Multiplication of Learned Probabilities is Dangerous</a></p>
<p>8 0.083586112 <a title="193-tfidf-8" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>9 0.083240539 <a title="193-tfidf-9" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>10 0.080063544 <a title="193-tfidf-10" href="../hunch_net-2009/hunch_net-2009-01-08-Predictive_Analytics_World.html">335 hunch net-2009-01-08-Predictive Analytics World</a></p>
<p>11 0.079982616 <a title="193-tfidf-11" href="../hunch_net-2006/hunch_net-2006-05-08-Big_machine_learning.html">178 hunch net-2006-05-08-Big machine learning</a></p>
<p>12 0.079432033 <a title="193-tfidf-12" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>13 0.07844907 <a title="193-tfidf-13" href="../hunch_net-2005/hunch_net-2005-08-01-Peekaboom.html">99 hunch net-2005-08-01-Peekaboom</a></p>
<p>14 0.077681221 <a title="193-tfidf-14" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>15 0.077635065 <a title="193-tfidf-15" href="../hunch_net-2008/hunch_net-2008-11-26-Efficient_Reinforcement_Learning_in_MDPs.html">328 hunch net-2008-11-26-Efficient Reinforcement Learning in MDPs</a></p>
<p>16 0.077094533 <a title="193-tfidf-16" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>17 0.077090003 <a title="193-tfidf-17" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>18 0.076557696 <a title="193-tfidf-18" href="../hunch_net-2005/hunch_net-2005-02-23-Problem%3A_Reinforcement_Learning_with_Classification.html">27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</a></p>
<p>19 0.075203113 <a title="193-tfidf-19" href="../hunch_net-2006/hunch_net-2006-12-05-Recruitment_Conferences.html">222 hunch net-2006-12-05-Recruitment Conferences</a></p>
<p>20 0.075160645 <a title="193-tfidf-20" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.18), (1, 0.01), (2, -0.067), (3, 0.075), (4, -0.061), (5, -0.007), (6, -0.017), (7, 0.035), (8, 0.003), (9, -0.001), (10, -0.038), (11, 0.0), (12, -0.0), (13, -0.002), (14, -0.043), (15, -0.008), (16, -0.055), (17, -0.018), (18, 0.026), (19, -0.034), (20, 0.007), (21, -0.113), (22, -0.065), (23, 0.05), (24, -0.043), (25, -0.016), (26, 0.081), (27, 0.09), (28, 0.001), (29, -0.011), (30, 0.04), (31, 0.021), (32, -0.024), (33, 0.014), (34, -0.005), (35, 0.043), (36, -0.028), (37, 0.075), (38, -0.03), (39, -0.006), (40, 0.06), (41, -0.03), (42, -0.104), (43, -0.02), (44, -0.063), (45, -0.017), (46, -0.044), (47, 0.03), (48, -0.034), (49, 0.061)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.93504792 <a title="193-lsi-1" href="../hunch_net-2006/hunch_net-2006-07-09-The_Stock_Prediction_Machine_Learning_Problem.html">193 hunch net-2006-07-09-The Stock Prediction Machine Learning Problem</a></p>
<p>Introduction: …is discussed in  this nytimes article .  I generally expect such approaches to become more common since computers are getting faster, machine learning is getting better, and data is becoming more plentiful.   This is another example where machine learning technology may have a huge economic impact.  Some side notes:
  
 We-in-research know almost nothing about how these things are done (because it is typically a corporate secret). 
 … but the limited discussion in the article seem naive from a machine learning viewpoint.
 
 The learning process used apparently often fails to take into account transaction costs. 
 What little of the approaches is discussed appears modeling based.  It seems plausible that more direct prediction methods can yield an edge. 
 
 
 One difficulty with stock picking as a research topic is that it is inherently a zero sum game (for every winner, there is a loser).  Much of the rest of research is positive sum (basically, everyone wins).</p><p>2 0.72029871 <a title="193-lsi-2" href="../hunch_net-2005/hunch_net-2005-09-04-Science_in_the_Government.html">106 hunch net-2005-09-04-Science in the Government</a></p>
<p>Introduction: I found the article on “ Political Science ” at the  New York Times  interesting.  Essentially the article is about allegations that the US government has been systematically distorting scientific views.   With a  petition  by some  7000+ scientists  alleging such behavior this is clearly a significant concern.
 
One thing not mentioned explicitly in this discussion is that there are fundamental cultural differences between academic research and the rest of the world.  In academic research, careful, clear thought is valued.  This value is achieved by both formal and informal mechanisms.  One example of a formal mechanism is peer review.
 
In contrast, in the land of politics, the basic value is agreement.  It is only with some amount of agreement that a new law can be passed or other actions can be taken.  Since Science (with a capitol ‘S’) has accomplished many things, it can be a significant tool in persuading people.  This makes it compelling for a politician to use science as a mec</p><p>3 0.65365434 <a title="193-lsi-3" href="../hunch_net-2005/hunch_net-2005-09-14-The_Predictionist_Viewpoint.html">112 hunch net-2005-09-14-The Predictionist Viewpoint</a></p>
<p>Introduction: Virtually every discipline of significant human endeavor has a way explaining itself as fundamental and important.  In all the cases I know of, they are both right (they are vital) and wrong (they are not solely vital).
  
 Politics.  This is the one that everyone is familiar with at the moment.  “What could be more important than the process of making decisions?” 
 Science and Technology.  This is the one that we-the-academics are familiar with.  “The loss of modern science and technology would be catastrophic.” 
 Military.  “Without the military, a nation will be invaded and destroyed.” 
 (insert your favorite here) 
  
Within science and technology, the same thing happens again.
  
 Mathematics. “What could be more important than a precise language for establishing truths?” 
 Physics.  “Nothing is more fundamental than the laws which govern the universe.  Understanding them is the key to understanding everything else.” 
 Biology.  “Without life, we wouldn’t be here, so clearly the s</p><p>4 0.61607939 <a title="193-lsi-4" href="../hunch_net-2009/hunch_net-2009-08-03-Carbon_in_Computer_Science_Research.html">366 hunch net-2009-08-03-Carbon in Computer Science Research</a></p>
<p>Introduction: Al Gore ‘s  film  and gradually more assertive and thorough science has managed to mostly shift the debate on climate change from “Is it happening?” to “What should be done?”  In that context, it’s worthwhile to think a bit about what can be done within computer science research.
 
There are two things we can think about:
  
  Doing Research  At a cartoon level, computer science research consists of some combination of commuting to&from; work, writing programs, running them on computers, writing papers, and presenting them at conferences.  A typical computer has a power usage on the order of 100 Watts, which works out to 2.4 kiloWatt-hours/day.  Looking up  David MacKay ‘s  reference on power usage per person , it becomes clear that this is a relatively minor part of the lifestyle, although it could become substantial if many more computers are required.  Much larger costs are associated with commuting (which is in common with many people) and attending conferences.  Since local commuti</p><p>5 0.60665661 <a title="193-lsi-5" href="../hunch_net-2005/hunch_net-2005-10-20-Machine_Learning_in_the_News.html">125 hunch net-2005-10-20-Machine Learning in the News</a></p>
<p>Introduction: The New York Times had a  short interview  about machine learning in datamining being used pervasively by the IRS and large corporations to predict who to audit and who to target for various marketing campaigns.  This is a big application area of machine learning.  It can be harmful (learning + databases = another way to invade privacy) or beneficial (as google demonstrates, better targeting of marketing campaigns is far less annoying).  This is yet more evidence that we can not rely upon “I’m just another fish in the school” logic for our expectations about treatment by government and large corporations.</p><p>6 0.60034579 <a title="193-lsi-6" href="../hunch_net-2007/hunch_net-2007-04-28-The_Coming_Patent_Apocalypse.html">241 hunch net-2007-04-28-The Coming Patent Apocalypse</a></p>
<p>7 0.57808089 <a title="193-lsi-7" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>8 0.55238658 <a title="193-lsi-8" href="../hunch_net-2007/hunch_net-2007-08-25-The_Privacy_Problem.html">260 hunch net-2007-08-25-The Privacy Problem</a></p>
<p>9 0.54625458 <a title="193-lsi-9" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>10 0.54272634 <a title="193-lsi-10" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>11 0.54005814 <a title="193-lsi-11" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>12 0.53916162 <a title="193-lsi-12" href="../hunch_net-2006/hunch_net-2006-09-18-What_is_missing_for_online_collaborative_research%3F.html">208 hunch net-2006-09-18-What is missing for online collaborative research?</a></p>
<p>13 0.53815359 <a title="193-lsi-13" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>14 0.53444153 <a title="193-lsi-14" href="../hunch_net-2010/hunch_net-2010-05-02-What%26%238217%3Bs_the_difference_between_gambling_and_rewarding_good_prediction%3F.html">397 hunch net-2010-05-02-What&#8217;s the difference between gambling and rewarding good prediction?</a></p>
<p>15 0.53351557 <a title="193-lsi-15" href="../hunch_net-2008/hunch_net-2008-01-06-Research_Political_Issues.html">282 hunch net-2008-01-06-Research Political Issues</a></p>
<p>16 0.53066319 <a title="193-lsi-16" href="../hunch_net-2005/hunch_net-2005-10-12-The_unrealized_potential_of_the_research_lab.html">121 hunch net-2005-10-12-The unrealized potential of the research lab</a></p>
<p>17 0.52629542 <a title="193-lsi-17" href="../hunch_net-2007/hunch_net-2007-06-23-Machine_Learning_Jobs_are_Growing_on_Trees.html">250 hunch net-2007-06-23-Machine Learning Jobs are Growing on Trees</a></p>
<p>18 0.52498233 <a title="193-lsi-18" href="../hunch_net-2008/hunch_net-2008-08-24-Mass_Customized_Medicine_in_the_Future%3F.html">314 hunch net-2008-08-24-Mass Customized Medicine in the Future?</a></p>
<p>19 0.51582408 <a title="193-lsi-19" href="../hunch_net-2013/hunch_net-2013-11-21-Ben_Taskar_is_gone.html">491 hunch net-2013-11-21-Ben Taskar is gone</a></p>
<p>20 0.51531172 <a title="193-lsi-20" href="../hunch_net-2009/hunch_net-2009-02-22-Effective_Research_Funding.html">344 hunch net-2009-02-22-Effective Research Funding</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.353), (27, 0.135), (38, 0.061), (53, 0.073), (55, 0.093), (94, 0.162), (95, 0.016)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.9260326 <a title="193-lda-1" href="../hunch_net-2005/hunch_net-2005-06-29-Not_EM_for_clustering_at_COLT.html">87 hunch net-2005-06-29-Not EM for clustering at COLT</a></p>
<p>Introduction: One standard approach for clustering data with a set of gaussians is using EM.  Roughly speaking, you pick a set of  k  random guassians and then use alternating expectation maximization to (hopefully) find a set of guassians that “explain” the data well.  This process is difficult to work with because EM can become “stuck” in local optima.   There are various hacks like “rerun with  t  different random starting points”.
 
One cool observation is that this can often be solved via other algorithm which do  not  suffer from local optima.  This is an early  paper  which shows this.  Ravi Kannan presented a  new paper  showing this is possible in a much more adaptive setting.  
 
A very rough summary of these papers is that by projecting into a lower dimensional space, it is computationally tractable to pick out the gross  structure of the data.  It is unclear how well these algorithms work in practice, but they might be effective, especially if used as a subroutine of the form:
  
 Projec</p><p>same-blog 2 0.88469881 <a title="193-lda-2" href="../hunch_net-2006/hunch_net-2006-07-09-The_Stock_Prediction_Machine_Learning_Problem.html">193 hunch net-2006-07-09-The Stock Prediction Machine Learning Problem</a></p>
<p>Introduction: …is discussed in  this nytimes article .  I generally expect such approaches to become more common since computers are getting faster, machine learning is getting better, and data is becoming more plentiful.   This is another example where machine learning technology may have a huge economic impact.  Some side notes:
  
 We-in-research know almost nothing about how these things are done (because it is typically a corporate secret). 
 … but the limited discussion in the article seem naive from a machine learning viewpoint.
 
 The learning process used apparently often fails to take into account transaction costs. 
 What little of the approaches is discussed appears modeling based.  It seems plausible that more direct prediction methods can yield an edge. 
 
 
 One difficulty with stock picking as a research topic is that it is inherently a zero sum game (for every winner, there is a loser).  Much of the rest of research is positive sum (basically, everyone wins).</p><p>3 0.88076204 <a title="193-lda-3" href="../hunch_net-2012/hunch_net-2012-09-29-Vowpal_Wabbit%2C_version_7.0.html">473 hunch net-2012-09-29-Vowpal Wabbit, version 7.0</a></p>
<p>Introduction: A new version of  VW  is  out .  The primary changes are:
  
  Learning Reductions : I’ve wanted to get  learning reductions  working and we’ve finally done it.  Not everything is implemented yet, but VW now supports direct:
 
 Multiclass Classification  –oaa  or  –ect . 
 Cost Sensitive Multiclass Classification  –csoaa  or  –wap . 
 Contextual Bandit Classification  –cb . 
 Sequential Structured Prediction   –searn  or  –dagger  
 

In addition, it is now easy to build your own custom learning reductions for various plausible uses: feature diddling, custom structured prediction problems, or alternate learning reductions.  This effort is far from done, but it is now in a generally useful state.  Note that all learning reductions inherit the ability to do cluster parallel learning.

 
  Library interface :  VW now has a basic library interface.  The library provides most of the functionality of VW, with the limitation that it is monolithic and nonreentrant.  These will be improved over</p><p>4 0.85437727 <a title="193-lda-4" href="../hunch_net-2006/hunch_net-2006-10-22-Exemplar_programming.html">215 hunch net-2006-10-22-Exemplar programming</a></p>
<p>Introduction: There are many different abstractions for problem definition and solution.  Here are a few examples:
  
 Functional programming: a set of functions are defined.  The composed execution of these functions yields the solution. 
 Linear programming: a set of constraints and a linear objective function are defined.  An LP solver finds the constrained optimum. 
 Quadratic programming: Like linear programming, but the language is a little more flexible (and the solution slower). 
 Convex programming: like quadratic programming, but the language is more flexible (and the solutions even slower). 
 Dynamic programming: a recursive definition of the problem is defined and then solved efficiently via caching tricks. 
 SAT programming: A problem is specified as a satisfiability involving a conjunction of a disjunction of boolean variables.  A general engine attempts to find a good satisfying assignment.  For example  Kautz’s   blackbox  planner. 
  
These abstractions have different tradeoffs betw</p><p>5 0.83487773 <a title="193-lda-5" href="../hunch_net-2005/hunch_net-2005-04-26-To_calibrate_or_not%3F.html">62 hunch net-2005-04-26-To calibrate or not?</a></p>
<p>Introduction: A calibrated predictor is one which predicts the probability of a binary event with the property: For all predictions  p , the proportion of the time that  1  is observed is  p .
 
Since there are infinitely many  p , this definition must be “softened” to make sense for any finite number of samples.  The standard method for “softening” is to consider all predictions in a small neighborhood about each possible  p .
 
A great deal of effort has been devoted to strategies for achieving calibrated (such as  here ) prediction.  With statements like: (under minimal conditions) you can always make calibrated predictions.  
 
Given the strength of these statements, we might conclude we are done, but that would be a “confusion of ends”.  A confusion of ends arises in the following way:
  
 We want good probabilistic predictions. 
 Good probabilistic predictions are calibrated. 
 Therefore, we want calibrated predictions. 
  
The “Therefore” step misses the fact that calibration is a necessary b</p><p>6 0.72440863 <a title="193-lda-6" href="../hunch_net-2005/hunch_net-2005-05-21-What_is_the_right_form_of_modularity_in_structured_prediction%3F.html">74 hunch net-2005-05-21-What is the right form of modularity in structured prediction?</a></p>
<p>7 0.60395199 <a title="193-lda-7" href="../hunch_net-2005/hunch_net-2005-11-28-A_question_of_quantification.html">133 hunch net-2005-11-28-A question of quantification</a></p>
<p>8 0.55850863 <a title="193-lda-8" href="../hunch_net-2006/hunch_net-2006-12-04-Structural_Problems_in_NIPS_Decision_Making.html">221 hunch net-2006-12-04-Structural Problems in NIPS Decision Making</a></p>
<p>9 0.54901624 <a title="193-lda-9" href="../hunch_net-2013/hunch_net-2013-12-01-NIPS_tutorials_and_Vowpal_Wabbit_7.4.html">492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</a></p>
<p>10 0.54555458 <a title="193-lda-10" href="../hunch_net-2007/hunch_net-2007-09-16-Optimizing_Machine_Learning_Programs.html">262 hunch net-2007-09-16-Optimizing Machine Learning Programs</a></p>
<p>11 0.54504228 <a title="193-lda-11" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>12 0.54438019 <a title="193-lda-12" href="../hunch_net-2009/hunch_net-2009-06-03-Functionally_defined_Nonlinear_Dynamic_Models.html">359 hunch net-2009-06-03-Functionally defined Nonlinear Dynamic Models</a></p>
<p>13 0.54335219 <a title="193-lda-13" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>14 0.53988642 <a title="193-lda-14" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>15 0.53903198 <a title="193-lda-15" href="../hunch_net-2005/hunch_net-2005-01-26-Watchword%3A_Probability.html">5 hunch net-2005-01-26-Watchword: Probability</a></p>
<p>16 0.53837502 <a title="193-lda-16" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>17 0.53656864 <a title="193-lda-17" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>18 0.53577858 <a title="193-lda-18" href="../hunch_net-2007/hunch_net-2007-09-18-It%26%238217%3Bs_MDL_Jim%2C_but_not_as_we_know_it%26%238230%3B%28on_Bayes%2C_MDL_and_consistency%29.html">263 hunch net-2007-09-18-It&#8217;s MDL Jim, but not as we know it&#8230;(on Bayes, MDL and consistency)</a></p>
<p>19 0.5310843 <a title="193-lda-19" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>20 0.53053975 <a title="193-lda-20" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
