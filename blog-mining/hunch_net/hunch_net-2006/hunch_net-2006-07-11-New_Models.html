<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>194 hunch net-2006-07-11-New Models</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-194" href="#">hunch_net-2006-194</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>194 hunch net-2006-07-11-New Models</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-194-html" href="http://hunch.net/?p=202">html</a></p><p>Introduction: How should we, as researchers in machine learning, organize ourselves?
 
The most immediate measurable objective of computer science research is publishing a paper.  The most difficult aspect of publishing a paper is having reviewers accept and recommend it for publication.  The simplest mechanism for doing this is to show theoretical progress on some standard, well-known easily understood problem.
 
In doing this, we often fall into a local minima of the research process.  The basic problem in machine learning is that it is very unclear that the mathematical model is the right one for the (or some) real problem.  A good mathematical model in machine learning should have one fundamental trait: it should aid the design of effective learning algorithms.  To date, our ability to solve interesting learning problems (speech recognition, machine translation, object recognition, etc…) remains limited (although improving), so the “rightness” of our models is in doubt.
 
If our mathematical mod</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The most immediate measurable objective of computer science research is publishing a paper. [sent-2, score-0.385]
</p><p>2 The most difficult aspect of publishing a paper is having reviewers accept and recommend it for publication. [sent-3, score-0.207]
</p><p>3 In doing this, we often fall into a local minima of the research process. [sent-5, score-0.262]
</p><p>4 The basic problem in machine learning is that it is very unclear that the mathematical model is the right one for the (or some) real problem. [sent-6, score-0.876]
</p><p>5 A good mathematical model in machine learning should have one fundamental trait: it should aid the design of effective learning algorithms. [sent-7, score-0.876]
</p><p>6 To date, our ability to solve interesting learning problems (speech recognition, machine translation, object recognition, etc…) remains limited (although improving), so the “rightness” of our models is in doubt. [sent-8, score-0.38]
</p><p>7 If our mathematical models are bad, the simple mechanism of research above can not yield the end goal. [sent-9, score-1.005]
</p><p>8 (This should be agreed on even by people who disagree about what the end goal of machine learning is! [sent-10, score-0.361]
</p><p>9 )  Instead, research which proposes and investigates new mathematical models for machine learning might yield the end goal. [sent-11, score-1.329]
</p><p>10 Coming up with a new mathematical model is just plain not easy. [sent-13, score-1.017]
</p><p>11 Some sources of inspiration include:    Watching carefully: what happens succesfully in practice, can often be abstracted into a mathematical model. [sent-14, score-0.742]
</p><p>12 Swapping fields: In other fields (for example  crypto ), other methods of analysis have been developed. [sent-15, score-0.27]
</p><p>13 Model repair:  Existing mathematical models often have easily comprehendable failure modes. [sent-17, score-0.84]
</p><p>14 By thinking about how to avoid such failure modes, we can sometimes produce a new mathematical model. [sent-18, score-0.71]
</p><p>15 Often, when trying to converge on a new model, we think of it in terms of the difference with respect to an older model, leading to a tangled explanation. [sent-21, score-0.21]
</p><p>16 For a reviewer with limited time, it is very tempting to assume that any particular paper is operating in some familiar model and fail out. [sent-23, score-0.501]
</p><p>17 You can’t be too blunt about saying “this isn’t the model you are thinking about”. [sent-25, score-0.651]
</p><p>18 When people don’t have a reference frame to understand the new model, they are unlikely to follow up, as is necessary for success in academia. [sent-27, score-0.427]
</p><p>19 The good news here is that a succesful new model can be a  big  win. [sent-28, score-0.537]
</p><p>20 I wish it was an easier win: the barriers to success are formidably high, and it seems we should do everything possible to lower the barriers to success for the sake of improving research. [sent-29, score-0.82]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('model', 0.401), ('mathematical', 0.391), ('models', 0.196), ('barriers', 0.168), ('success', 0.139), ('new', 0.136), ('end', 0.127), ('recognition', 0.122), ('publishing', 0.118), ('fields', 0.118), ('improving', 0.11), ('research', 0.106), ('yield', 0.104), ('limited', 0.1), ('failure', 0.099), ('rightness', 0.096), ('abstracted', 0.096), ('blunt', 0.096), ('inspiration', 0.096), ('investigates', 0.096), ('sake', 0.096), ('super', 0.096), ('reviewers', 0.089), ('repair', 0.089), ('measurable', 0.089), ('plain', 0.089), ('proposes', 0.089), ('succeeding', 0.089), ('watching', 0.089), ('difficulty', 0.088), ('thinking', 0.084), ('swapping', 0.084), ('machine', 0.084), ('mechanism', 0.081), ('succesfully', 0.08), ('frame', 0.08), ('agreed', 0.08), ('often', 0.079), ('crypto', 0.077), ('minima', 0.077), ('modes', 0.077), ('methods', 0.075), ('easily', 0.075), ('translation', 0.074), ('converge', 0.074), ('continues', 0.074), ('unlikely', 0.072), ('objective', 0.072), ('disagree', 0.07), ('saying', 0.07)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="194-tfidf-1" href="../hunch_net-2006/hunch_net-2006-07-11-New_Models.html">194 hunch net-2006-07-11-New Models</a></p>
<p>Introduction: How should we, as researchers in machine learning, organize ourselves?
 
The most immediate measurable objective of computer science research is publishing a paper.  The most difficult aspect of publishing a paper is having reviewers accept and recommend it for publication.  The simplest mechanism for doing this is to show theoretical progress on some standard, well-known easily understood problem.
 
In doing this, we often fall into a local minima of the research process.  The basic problem in machine learning is that it is very unclear that the mathematical model is the right one for the (or some) real problem.  A good mathematical model in machine learning should have one fundamental trait: it should aid the design of effective learning algorithms.  To date, our ability to solve interesting learning problems (speech recognition, machine translation, object recognition, etc…) remains limited (although improving), so the “rightness” of our models is in doubt.
 
If our mathematical mod</p><p>2 0.26173189 <a title="194-tfidf-2" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>Introduction: I wanted to expand on this  post  and some of the previous  problems/research directions  about where learning theory might make large strides.  
  
  Why theory?   The essential reason for theory is “intuition extension”.  A very good applied learning person can master some particular application domain yielding the best computer algorithms for solving that problem.  A very good theory can take the intuitions discovered by this and other applied learning people and extend them to new domains in a relatively automatic fashion.  To do this, we take these basic intuitions and try to find a mathematical model that:
 
 Explains the basic intuitions. 
 Makes new testable predictions about how to learn. 
 Succeeds in so learning. 
 

This is “intuition extension”: taking what we have learned somewhere else and applying it in new domains.  It is fundamentally useful to everyone because it increases the level of automation in solving problems.

 
  Where next for learning theory?  I like the a</p><p>3 0.19835004 <a title="194-tfidf-3" href="../hunch_net-2005/hunch_net-2005-12-04-Watchword%3A_model.html">135 hunch net-2005-12-04-Watchword: model</a></p>
<p>Introduction: In everyday use a model is a system which explains the behavior of some system, hopefully at the level where some alteration of the model predicts some alteration of the real-world system.   In machine learning “model” has several variant definitions.
  
  Everyday .  The common definition is sometimes used. 
  Parameterized . Sometimes model is a short-hand for “parameterized model”.  Here, it refers to a model with unspecified free parameters.  In the Bayesian learning approach, you typically have a prior over (everyday) models. 
  Predictive .  Even further from everyday use is the predictive model.  Examples of this are “my model is a decision tree” or “my model is a support vector machine”.  Here, there is no real sense in which an SVM explains the underlying process.  For example, an SVM tells us nothing in particular about how alterations to the real-world system would create a change. 
  
Which definition is being used at any particular time is important information.  For examp</p><p>4 0.18078065 <a title="194-tfidf-4" href="../hunch_net-2008/hunch_net-2008-05-23-Three_levels_of_addressing_the_Netflix_Prize.html">301 hunch net-2008-05-23-Three levels of addressing the Netflix Prize</a></p>
<p>Introduction: In October 2006, the online movie renter, Netflix, announced the  Netflix Prize  contest. They published a comprehensive dataset including more than 100 million movie ratings, which were performed by about 480,000 real customers on 17,770 movies. Ã‚Â   Competitors in the challenge are required to estimate a few million ratings. Ã‚Â   To win the “grand prize,” they need to deliver a 10% improvement in the prediction error compared with the results of Cinematch, Netflix’s proprietary recommender system. Best current results deliver 9.12%  improvement , which is quite close to the 10% goal, yet painfully distant. 
   Ã‚Â   The Netflix Prize breathed new life and excitement into recommender systems research. The competition allowed the wide research community to access a large scale, real life dataset. Beyond this, the competition changed the rules of the game. Claiming that your nice idea could outperform some mediocre algorithms on some toy dataset is no longer acceptable. Now researcher</p><p>5 0.16840719 <a title="194-tfidf-5" href="../hunch_net-2005/hunch_net-2005-07-23-Interesting_papers_at_ACL.html">97 hunch net-2005-07-23-Interesting papers at ACL</a></p>
<p>Introduction: A  recent discussion  indicated that one goal of this blog might be to allow people to post comments about recent papers that they liked.  I think this could potentially be very useful, especially for those with diverse interests but only finite time to read through conference proceedings.   ACL 2005  recently completed, and here are four papers from that conference that I thought were either good or perhaps of interest to a machine learning audience.
 
David Chiang,   A Hierarchical Phrase-Based Model for Statistical Machine Translation  . (Best paper award.) This paper takes the standard phrase-based MT model that is popular in our field (basically, translate a sentence by individually translating phrases and reordering them according to a complicated statistical model) and extends it to take into account hierarchy in phrases, so that you can learn things like “X ‘s Y” -> “Y de X” in chinese, where X and Y are arbitrary phrases. This takes a step toward linguistic syntax for MT, whic</p><p>6 0.15896864 <a title="194-tfidf-6" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>7 0.15409045 <a title="194-tfidf-7" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>8 0.1225839 <a title="194-tfidf-8" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>9 0.12218384 <a title="194-tfidf-9" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>10 0.11521645 <a title="194-tfidf-10" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>11 0.11240302 <a title="194-tfidf-11" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>12 0.10961941 <a title="194-tfidf-12" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>13 0.10869491 <a title="194-tfidf-13" href="../hunch_net-2005/hunch_net-2005-07-04-The_Health_of_COLT.html">89 hunch net-2005-07-04-The Health of COLT</a></p>
<p>14 0.10739357 <a title="194-tfidf-14" href="../hunch_net-2006/hunch_net-2006-03-09-Use_of_Notation.html">162 hunch net-2006-03-09-Use of Notation</a></p>
<p>15 0.10358946 <a title="194-tfidf-15" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>16 0.10328845 <a title="194-tfidf-16" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>17 0.1018286 <a title="194-tfidf-17" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>18 0.10168904 <a title="194-tfidf-18" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>19 0.10060982 <a title="194-tfidf-19" href="../hunch_net-2005/hunch_net-2005-01-24-The_Humanloop_Spectrum_of_Machine_Learning.html">3 hunch net-2005-01-24-The Humanloop Spectrum of Machine Learning</a></p>
<p>20 0.098267354 <a title="194-tfidf-20" href="../hunch_net-2005/hunch_net-2005-03-21-Research_Styles_in_Machine_Learning.html">44 hunch net-2005-03-21-Research Styles in Machine Learning</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.245), (1, -0.012), (2, -0.016), (3, 0.114), (4, -0.002), (5, -0.043), (6, 0.036), (7, 0.069), (8, 0.1), (9, -0.083), (10, 0.061), (11, -0.069), (12, -0.1), (13, 0.007), (14, 0.052), (15, 0.034), (16, 0.121), (17, 0.069), (18, 0.135), (19, -0.182), (20, -0.077), (21, -0.087), (22, -0.096), (23, -0.058), (24, 0.011), (25, 0.007), (26, -0.074), (27, -0.063), (28, -0.094), (29, -0.104), (30, 0.022), (31, 0.117), (32, -0.028), (33, 0.06), (34, 0.002), (35, 0.044), (36, 0.068), (37, -0.14), (38, -0.079), (39, -0.151), (40, -0.064), (41, -0.001), (42, -0.138), (43, 0.012), (44, 0.039), (45, 0.004), (46, 0.035), (47, -0.047), (48, 0.063), (49, -0.05)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97928941 <a title="194-lsi-1" href="../hunch_net-2006/hunch_net-2006-07-11-New_Models.html">194 hunch net-2006-07-11-New Models</a></p>
<p>Introduction: How should we, as researchers in machine learning, organize ourselves?
 
The most immediate measurable objective of computer science research is publishing a paper.  The most difficult aspect of publishing a paper is having reviewers accept and recommend it for publication.  The simplest mechanism for doing this is to show theoretical progress on some standard, well-known easily understood problem.
 
In doing this, we often fall into a local minima of the research process.  The basic problem in machine learning is that it is very unclear that the mathematical model is the right one for the (or some) real problem.  A good mathematical model in machine learning should have one fundamental trait: it should aid the design of effective learning algorithms.  To date, our ability to solve interesting learning problems (speech recognition, machine translation, object recognition, etc…) remains limited (although improving), so the “rightness” of our models is in doubt.
 
If our mathematical mod</p><p>2 0.85481632 <a title="194-lsi-2" href="../hunch_net-2005/hunch_net-2005-12-04-Watchword%3A_model.html">135 hunch net-2005-12-04-Watchword: model</a></p>
<p>Introduction: In everyday use a model is a system which explains the behavior of some system, hopefully at the level where some alteration of the model predicts some alteration of the real-world system.   In machine learning “model” has several variant definitions.
  
  Everyday .  The common definition is sometimes used. 
  Parameterized . Sometimes model is a short-hand for “parameterized model”.  Here, it refers to a model with unspecified free parameters.  In the Bayesian learning approach, you typically have a prior over (everyday) models. 
  Predictive .  Even further from everyday use is the predictive model.  Examples of this are “my model is a decision tree” or “my model is a support vector machine”.  Here, there is no real sense in which an SVM explains the underlying process.  For example, an SVM tells us nothing in particular about how alterations to the real-world system would create a change. 
  
Which definition is being used at any particular time is important information.  For examp</p><p>3 0.75377822 <a title="194-lsi-3" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>Introduction: I wanted to expand on this  post  and some of the previous  problems/research directions  about where learning theory might make large strides.  
  
  Why theory?   The essential reason for theory is “intuition extension”.  A very good applied learning person can master some particular application domain yielding the best computer algorithms for solving that problem.  A very good theory can take the intuitions discovered by this and other applied learning people and extend them to new domains in a relatively automatic fashion.  To do this, we take these basic intuitions and try to find a mathematical model that:
 
 Explains the basic intuitions. 
 Makes new testable predictions about how to learn. 
 Succeeds in so learning. 
 

This is “intuition extension”: taking what we have learned somewhere else and applying it in new domains.  It is fundamentally useful to everyone because it increases the level of automation in solving problems.

 
  Where next for learning theory?  I like the a</p><p>4 0.75051093 <a title="194-lsi-4" href="../hunch_net-2008/hunch_net-2008-05-23-Three_levels_of_addressing_the_Netflix_Prize.html">301 hunch net-2008-05-23-Three levels of addressing the Netflix Prize</a></p>
<p>Introduction: In October 2006, the online movie renter, Netflix, announced the  Netflix Prize  contest. They published a comprehensive dataset including more than 100 million movie ratings, which were performed by about 480,000 real customers on 17,770 movies. Ã‚Â   Competitors in the challenge are required to estimate a few million ratings. Ã‚Â   To win the “grand prize,” they need to deliver a 10% improvement in the prediction error compared with the results of Cinematch, Netflix’s proprietary recommender system. Best current results deliver 9.12%  improvement , which is quite close to the 10% goal, yet painfully distant. 
   Ã‚Â   The Netflix Prize breathed new life and excitement into recommender systems research. The competition allowed the wide research community to access a large scale, real life dataset. Beyond this, the competition changed the rules of the game. Claiming that your nice idea could outperform some mediocre algorithms on some toy dataset is no longer acceptable. Now researcher</p><p>5 0.74304163 <a title="194-lsi-5" href="../hunch_net-2005/hunch_net-2005-07-23-Interesting_papers_at_ACL.html">97 hunch net-2005-07-23-Interesting papers at ACL</a></p>
<p>Introduction: A  recent discussion  indicated that one goal of this blog might be to allow people to post comments about recent papers that they liked.  I think this could potentially be very useful, especially for those with diverse interests but only finite time to read through conference proceedings.   ACL 2005  recently completed, and here are four papers from that conference that I thought were either good or perhaps of interest to a machine learning audience.
 
David Chiang,   A Hierarchical Phrase-Based Model for Statistical Machine Translation  . (Best paper award.) This paper takes the standard phrase-based MT model that is popular in our field (basically, translate a sentence by individually translating phrases and reordering them according to a complicated statistical model) and extends it to take into account hierarchy in phrases, so that you can learn things like “X ‘s Y” -> “Y de X” in chinese, where X and Y are arbitrary phrases. This takes a step toward linguistic syntax for MT, whic</p><p>6 0.65989816 <a title="194-lsi-6" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>7 0.6506362 <a title="194-lsi-7" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>8 0.56502116 <a title="194-lsi-8" href="../hunch_net-2011/hunch_net-2011-08-06-Interesting_thing_at_UAI_2011.html">440 hunch net-2011-08-06-Interesting thing at UAI 2011</a></p>
<p>9 0.51944435 <a title="194-lsi-9" href="../hunch_net-2006/hunch_net-2006-04-14-JMLR_is_a_success.html">172 hunch net-2006-04-14-JMLR is a success</a></p>
<p>10 0.50313157 <a title="194-lsi-10" href="../hunch_net-2006/hunch_net-2006-03-09-Use_of_Notation.html">162 hunch net-2006-03-09-Use of Notation</a></p>
<p>11 0.49214178 <a title="194-lsi-11" href="../hunch_net-2006/hunch_net-2006-07-05-more_icml_papers.html">189 hunch net-2006-07-05-more icml papers</a></p>
<p>12 0.48351416 <a title="194-lsi-12" href="../hunch_net-2006/hunch_net-2006-04-09-Progress_in_Machine_Translation.html">171 hunch net-2006-04-09-Progress in Machine Translation</a></p>
<p>13 0.48313379 <a title="194-lsi-13" href="../hunch_net-2007/hunch_net-2007-12-20-Cool_and_Interesting_things_at_NIPS%2C_take_three.html">280 hunch net-2007-12-20-Cool and Interesting things at NIPS, take three</a></p>
<p>14 0.47871608 <a title="194-lsi-14" href="../hunch_net-2006/hunch_net-2006-09-18-What_is_missing_for_online_collaborative_research%3F.html">208 hunch net-2006-09-18-What is missing for online collaborative research?</a></p>
<p>15 0.46688876 <a title="194-lsi-15" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>16 0.4636254 <a title="194-lsi-16" href="../hunch_net-2005/hunch_net-2005-03-17-Going_all_the_Way%2C_Sometimes.html">42 hunch net-2005-03-17-Going all the Way, Sometimes</a></p>
<p>17 0.4618223 <a title="194-lsi-17" href="../hunch_net-2005/hunch_net-2005-03-21-Research_Styles_in_Machine_Learning.html">44 hunch net-2005-03-21-Research Styles in Machine Learning</a></p>
<p>18 0.45939258 <a title="194-lsi-18" href="../hunch_net-2005/hunch_net-2005-07-07-The_Limits_of_Learning_Theory.html">90 hunch net-2005-07-07-The Limits of Learning Theory</a></p>
<p>19 0.45886645 <a title="194-lsi-19" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>20 0.45748755 <a title="194-lsi-20" href="../hunch_net-2005/hunch_net-2005-12-22-Yes_%2C_I_am_applying.html">142 hunch net-2005-12-22-Yes , I am applying</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(3, 0.055), (21, 0.042), (27, 0.235), (37, 0.192), (38, 0.055), (53, 0.094), (55, 0.101), (64, 0.017), (94, 0.066), (95, 0.059)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.92801577 <a title="194-lda-1" href="../hunch_net-2009/hunch_net-2009-08-26-Another_10-year_paper_in_Machine_Learning.html">368 hunch net-2009-08-26-Another 10-year paper in Machine Learning</a></p>
<p>Introduction: When I was thinking about the best “10 year paper” for  ICML , I also took a look at a few other conferences.  Here is one from 10 years ago that interested me:
 
 David McAllester   PAC-Bayesian Model Averaging ,  COLT  1999.    2001 Journal Draft . 
 
Prior to this paper, the only mechanism known for controlling or estimating the necessary sample complexity for learning over continuously parameterized predictors was VC theory and variants, all of which suffered from a basic problem: they were incredibly pessimistic in practice.  This meant that only very gross guidance could be provided for learning algorithm design.  The PAC-Bayes bound provided an alternative approach to sample complexity bounds which was radically tighter, quantitatively.  It also imported and explained many of the motivations for Bayesian learning in a way that learning theory and perhaps optimization people might appreciate.  Since this paper came out, there have been a number of moderately successful attempts t</p><p>2 0.92576337 <a title="194-lda-2" href="../hunch_net-2005/hunch_net-2005-12-09-Some_NIPS_papers.html">138 hunch net-2005-12-09-Some NIPS papers</a></p>
<p>Introduction: Here is a set of papers that I found interesting (and why).
  
  A PAC-Bayes approach to the Set Covering Machine  improves the set covering machine.  The set covering machine approach is a new way to do classification characterized by a very close connection between theory and algorithm.  At this point, the approach seems to be competing well with SVMs in about all dimensions: similar computational speed, similar accuracy, stronger learning theory guarantees, more general information source (a kernel has strictly more structure than a metric), and more sparsity.  Developing a classification algorithm is not very easy, but the results so far are encouraging. 
  Off-Road Obstacle Avoidance through End-to-End Learning  and  Learning Depth from Single Monocular Images  both effectively showed that depth information can be predicted from camera images (using notably different techniques).  This ability is strongly enabling because cameras are cheap, tiny, light, and potentially provider lo</p><p>3 0.906663 <a title="194-lda-3" href="../hunch_net-2005/hunch_net-2005-01-19-Why_I_decided_to_run_a_weblog..html">1 hunch net-2005-01-19-Why I decided to run a weblog.</a></p>
<p>Introduction: I have decided to run a weblog on machine learning and learning theory research.  Here are some reasons:
 
1) Weblogs enable new functionality:
  
 Public comment on papers.  No mechanism for this exists at conferences and most journals.  I have encountered it once for a  science  paper.   Some communities have mailing lists supporting this, but not machine learning or learning theory.  I have often read papers and found myself wishing there was some method to consider other’s questions and read the replies. 
 Conference shortlists.  One of the most common conversations at a conference is “what did you find interesting?”  There is no explicit mechanism for sharing this information at conferences, and it’s easy to imagine that it would be handy to do so. 
 Evaluation and comment on research directions.  Papers are almost exclusively about new research, rather than evaluation (and consideration) of research directions.  This last role is satisfied by funding agencies to some extent, but</p><p>4 0.89935815 <a title="194-lda-4" href="../hunch_net-2011/hunch_net-2011-04-18-A_paper_not_at_Snowbird.html">431 hunch net-2011-04-18-A paper not at Snowbird</a></p>
<p>Introduction: Unfortunately, a scheduling failure meant I missed all of  AIStat  and most of the  learning workshop , otherwise known as Snowbird, when it’s at  Snowbird .  
 
At snowbird, the talk on  Sum-Product  networks by  Hoifung Poon  stood out to me ( Pedro Domingos  is a coauthor.).  The basic point was that by appropriately constructing networks based on sums and products, the normalization problem in probabilistic models is eliminated, yielding a highly tractable yet flexible representation+learning algorithm.  As an algorithm, this is noticeably cleaner than deep belief networks with a claim to being an order of magnitude faster and working better on an image completion task.
 
Snowbird doesn’t have real papers—just the abstract above.  I look forward to seeing the paper.  (added: Rodrigo points out the deep learning workshop  draft .)</p><p>same-blog 5 0.89133751 <a title="194-lda-5" href="../hunch_net-2006/hunch_net-2006-07-11-New_Models.html">194 hunch net-2006-07-11-New Models</a></p>
<p>Introduction: How should we, as researchers in machine learning, organize ourselves?
 
The most immediate measurable objective of computer science research is publishing a paper.  The most difficult aspect of publishing a paper is having reviewers accept and recommend it for publication.  The simplest mechanism for doing this is to show theoretical progress on some standard, well-known easily understood problem.
 
In doing this, we often fall into a local minima of the research process.  The basic problem in machine learning is that it is very unclear that the mathematical model is the right one for the (or some) real problem.  A good mathematical model in machine learning should have one fundamental trait: it should aid the design of effective learning algorithms.  To date, our ability to solve interesting learning problems (speech recognition, machine translation, object recognition, etc…) remains limited (although improving), so the “rightness” of our models is in doubt.
 
If our mathematical mod</p><p>6 0.85968381 <a title="194-lda-6" href="../hunch_net-2005/hunch_net-2005-02-17-Learning_Research_Programs.html">21 hunch net-2005-02-17-Learning Research Programs</a></p>
<p>7 0.81455451 <a title="194-lda-7" href="../hunch_net-2005/hunch_net-2005-04-27-DARPA_project%3A_LAGR.html">63 hunch net-2005-04-27-DARPA project: LAGR</a></p>
<p>8 0.805049 <a title="194-lda-8" href="../hunch_net-2013/hunch_net-2013-01-07-NYU_Large_Scale_Machine_Learning_Class.html">478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</a></p>
<p>9 0.8046878 <a title="194-lda-9" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>10 0.80446959 <a title="194-lda-10" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>11 0.80422181 <a title="194-lda-11" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>12 0.80315399 <a title="194-lda-12" href="../hunch_net-2007/hunch_net-2007-01-10-A_Deep_Belief_Net_Learning_Problem.html">227 hunch net-2007-01-10-A Deep Belief Net Learning Problem</a></p>
<p>13 0.80293834 <a title="194-lda-13" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>14 0.8019833 <a title="194-lda-14" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>15 0.80179423 <a title="194-lda-15" href="../hunch_net-2005/hunch_net-2005-02-03-Learning_Theory%2C_by_assumption.html">12 hunch net-2005-02-03-Learning Theory, by assumption</a></p>
<p>16 0.80123484 <a title="194-lda-16" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>17 0.80119979 <a title="194-lda-17" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>18 0.80015737 <a title="194-lda-18" href="../hunch_net-2009/hunch_net-2009-01-21-Nearly_all_natural_problems_require_nonlinearity.html">337 hunch net-2009-01-21-Nearly all natural problems require nonlinearity</a></p>
<p>19 0.79920495 <a title="194-lda-19" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>20 0.79849452 <a title="194-lda-20" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
