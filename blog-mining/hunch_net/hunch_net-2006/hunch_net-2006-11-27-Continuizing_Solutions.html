<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>220 hunch net-2006-11-27-Continuizing Solutions</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-220" href="#">hunch_net-2006-220</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>220 hunch net-2006-11-27-Continuizing Solutions</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-220-html" href="http://hunch.net/?p=240">html</a></p><p>Introduction: This post is about a general technique for problem solving which I've never
seen taught (in full generality), but which I've found very useful.Many
problems in computer science turn out to be discretely difficult. The best
known version of such problems are NP-hard problems, but I mean 'discretely
difficult' in a much more general way, which I only know how to capture by
examples.ERMIn empirical risk minimization, you choose a minimum error rate
classifier from a set of classifiers. This is NP hard for common sets, but it
can be much harder, depending on the set.ExpertsIn the online learning with
experts setting, you try to predict well so as to compete with a set of
(adversarial) experts. Here the alternating quantifiers of you and an
adversary playing out a game can yield a dynamic programming problem that
grows exponentially.Policy IterationThe problem with policy iteration is that
you learn a new policy with respect to an old policy, which implies that
simply adopting the new polic</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 This post is about a general technique for problem solving which I've never seen taught (in full generality), but which I've found very useful. [sent-1, score-0.349]
</p><p>2 Many problems in computer science turn out to be discretely difficult. [sent-2, score-0.092]
</p><p>3 The best known version of such problems are NP-hard problems, but I mean 'discretely difficult' in a much more general way, which I only know how to capture by examples. [sent-3, score-0.273]
</p><p>4 ERMIn empirical risk minimization, you choose a minimum error rate classifier from a set of classifiers. [sent-4, score-0.218]
</p><p>5 This is NP hard for common sets, but it can be much harder, depending on the set. [sent-5, score-0.069]
</p><p>6 ExpertsIn the online learning with experts setting, you try to predict well so as to compete with a set of (adversarial) experts. [sent-6, score-0.33]
</p><p>7 Here the alternating quantifiers of you and an adversary playing out a game can yield a dynamic programming problem that grows exponentially. [sent-7, score-0.76]
</p><p>8 Policy IterationThe problem with policy iteration is that you learn a new policy with respect to an old policy, which implies that simply adopting the new policy can go very wrong. [sent-8, score-2.311]
</p><p>9 For each of these problems, there are "continuized" solutions which can yield smaller computation, more elegant mathematics, or both. [sent-9, score-0.367]
</p><p>10 ERMBy shifting from choosing a single classifier to choosing a stochastic classifier we can prove a new style of bound which is significantly tighter, easier to state, and easier to understand than traditional bounds in the traditional setting. [sent-10, score-1.344]
</p><p>11 ExpertsBy giving the adversary slightly more power--the ability to split experts and have them fractionally predict one way vs. [sent-12, score-0.555]
</p><p>12 another, the optimal policy becomes much easier to compute (quadratic in the horizon, or maybe less). [sent-13, score-0.644]
</p><p>13 Policy IterationFor policy iteration, by stochastically mixing the old and the new policy, we can find a new policy better than the old policy. [sent-15, score-1.612]
</p><p>14 The first and second examples both involve a setting shift, which may not be valid--in general your setting should reflect your real problem rather than the thing which is easy to solve. [sent-18, score-0.583]
</p><p>15 However, even with the setting shift, the solutions appear so compellingly more elegant that it is hard to not hope to use them in a solution to the original setting. [sent-19, score-0.581]
</p><p>16 I have not seen a good formulation of the general approach of continuizing. [sent-20, score-0.294]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('policy', 0.496), ('iteration', 0.283), ('experts', 0.181), ('old', 0.165), ('setting', 0.156), ('easier', 0.148), ('elegant', 0.148), ('classifier', 0.144), ('adversary', 0.137), ('traditional', 0.134), ('shift', 0.129), ('yield', 0.113), ('choosing', 0.112), ('general', 0.112), ('solutions', 0.106), ('quantifiers', 0.102), ('continuous', 0.102), ('adopting', 0.102), ('compellingly', 0.102), ('new', 0.098), ('bound', 0.096), ('alternating', 0.094), ('mixing', 0.094), ('formulation', 0.094), ('danger', 0.094), ('quadratic', 0.094), ('problems', 0.092), ('split', 0.089), ('np', 0.089), ('grows', 0.089), ('seen', 0.088), ('horizon', 0.085), ('conservative', 0.082), ('reflect', 0.082), ('generality', 0.079), ('predict', 0.079), ('problem', 0.077), ('minimization', 0.076), ('tighter', 0.074), ('dynamic', 0.074), ('risk', 0.074), ('shifting', 0.074), ('playing', 0.074), ('taught', 0.072), ('compete', 0.07), ('mathematics', 0.069), ('slightly', 0.069), ('capture', 0.069), ('hard', 0.069), ('sets', 0.067)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000002 <a title="220-tfidf-1" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>Introduction: This post is about a general technique for problem solving which I've never
seen taught (in full generality), but which I've found very useful.Many
problems in computer science turn out to be discretely difficult. The best
known version of such problems are NP-hard problems, but I mean 'discretely
difficult' in a much more general way, which I only know how to capture by
examples.ERMIn empirical risk minimization, you choose a minimum error rate
classifier from a set of classifiers. This is NP hard for common sets, but it
can be much harder, depending on the set.ExpertsIn the online learning with
experts setting, you try to predict well so as to compete with a set of
(adversarial) experts. Here the alternating quantifiers of you and an
adversary playing out a game can yield a dynamic programming problem that
grows exponentially.Policy IterationThe problem with policy iteration is that
you learn a new policy with respect to an old policy, which implies that
simply adopting the new polic</p><p>2 0.2192906 <a title="220-tfidf-2" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>Introduction: One thing which is clear on a little reflection is that there exists a single
master learning problem capable of encoding essentially all learning problems.
This problem is of course a very general sort of reinforcement learning where
the world interacts with an agent as:The world announces an observationx.The
agent makes a choicea.The world announces a rewardr.The goal here is to
maximize the sum of the rewards over the time of the agent. No particular
structure relatingxtoaoratoris implied by this setting so we do not know
effective general algorithms for the agent. It's very easy to prove lower
bounds showing that an agent cannot hope to succeed here--just consider the
case where actions are unrelated to rewards. Nevertheless, there is a real
sense in which essentially all forms of life are agents operating in this
setting, somehow succeeding. The gap between these observations drives
research--How can we find tractable specializations of the master problem
general enough to provide</p><p>3 0.14778863 <a title="220-tfidf-3" href="../hunch_net-2007/hunch_net-2007-10-24-Contextual_Bandits.html">269 hunch net-2007-10-24-Contextual Bandits</a></p>
<p>Introduction: One of the fundamental underpinnings of the internet is advertising based
content. This has become much more effective due to targeted advertising where
ads are specifically matched to interests. Everyone is familiar with this,
because everyone uses search engines and all search engines try to make money
this way.The problem of matching ads to interests is a natural machine
learning problem in some ways since there is much information in who clicks on
what. A fundamental problem with this information is that it is not supervised
--in particular a click-or-not on one ad doesn't generally tell you if a
different ad would have been clicked on. This implies we have a fundamental
exploration problem.A standard mathematical setting for this situation is
"k-Armed Bandits", often with various relevant embellishments. Thek-Armed
Bandit setting works on a round-by-round basis. On each round:A policy chooses
armafrom1ofkarms (i.e. 1 of k ads).The world reveals the rewardraof the chosen
arm (i.e.</p><p>4 0.14531246 <a title="220-tfidf-4" href="../hunch_net-2005/hunch_net-2005-08-08-Apprenticeship_Reinforcement_Learning_for_Control.html">101 hunch net-2005-08-08-Apprenticeship Reinforcement Learning for Control</a></p>
<p>Introduction: Pieter Abbeelpresented a paper withAndrew NgatICMLonExploration and
Apprenticeship Learning in Reinforcement Learning. The basic idea of this
algorithm is:Collect data from a human controlling a machine.Build a
transition model based upon the experience.Build a policy which optimizes the
transition model.Evaluate the policy. If it works well, halt, otherwise add
the experience into the pool and go to (2).The paper proves that this
technique will converge to some policy with expected performance near human
expected performance assuming the world fits certain assumptions (MDP or
linear dynamics).This general idea of apprenticeship learning (i.e.
incorporating data from an expert) seems very compelling because (a) humans
often learn this way and (b) much harder problems can be solved. For (a), the
notion of teaching is about transferring knowledge from an expert to novices,
often via demonstration. To see (b), note that we can create intricate
reinforcement learning problems where a parti</p><p>5 0.14226091 <a title="220-tfidf-5" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>Introduction: What?Reductions are machines which turn solvers for one problem into solvers
for another problem.Why?Reductions are useful for several reasons.Laziness.
Reducing a problem to classification make at least 10 learning algorithms
available to solve a problem. Inventing 10 learning algorithms is quite a bit
of work. Similarly, programming a reduction is often trivial, while
programming a learning algorithm is a great deal of work.Crystallization. The
problems we often want to solve in learning are worst-case-impossible, but
average case feasible. By reducing all problems onto one or a few primitives,
we can fine tune these primitives to perform well on real-world problems with
greater precision due to the greater number of problems to validate
on.Theoretical Organization. By studying what reductions are easy vs. hard vs.
impossible, we can learn which problems are roughly equivalent in difficulty
and which are much harder.What we know now.Typesafe reductions. In the
beginning, there was th</p><p>6 0.13293478 <a title="220-tfidf-6" href="../hunch_net-2007/hunch_net-2007-06-14-Interesting_Papers_at_COLT_2007.html">247 hunch net-2007-06-14-Interesting Papers at COLT 2007</a></p>
<p>7 0.12544718 <a title="220-tfidf-7" href="../hunch_net-2008/hunch_net-2008-01-06-Research_Political_Issues.html">282 hunch net-2008-01-06-Research Political Issues</a></p>
<p>8 0.122196 <a title="220-tfidf-8" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>9 0.12056436 <a title="220-tfidf-9" href="../hunch_net-2005/hunch_net-2005-08-04-Why_Reinforcement_Learning_is_Important.html">100 hunch net-2005-08-04-Why Reinforcement Learning is Important</a></p>
<p>10 0.11579745 <a title="220-tfidf-10" href="../hunch_net-2005/hunch_net-2005-09-08-Online_Learning_as_the_Mathematics_of_Accountability.html">109 hunch net-2005-09-08-Online Learning as the Mathematics of Accountability</a></p>
<p>11 0.1151631 <a title="220-tfidf-11" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<p>12 0.11132721 <a title="220-tfidf-12" href="../hunch_net-2006/hunch_net-2006-10-22-Exemplar_programming.html">215 hunch net-2006-10-22-Exemplar programming</a></p>
<p>13 0.1005564 <a title="220-tfidf-13" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>14 0.099094391 <a title="220-tfidf-14" href="../hunch_net-2005/hunch_net-2005-04-21-Dynamic_Programming_Generalizations_and_Their_Use.html">58 hunch net-2005-04-21-Dynamic Programming Generalizations and Their Use</a></p>
<p>15 0.098149769 <a title="220-tfidf-15" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>16 0.097678915 <a title="220-tfidf-16" href="../hunch_net-2008/hunch_net-2008-09-12-How_do_we_get_weak_action_dependence_for_learning_with_partial_observations%3F.html">317 hunch net-2008-09-12-How do we get weak action dependence for learning with partial observations?</a></p>
<p>17 0.097360633 <a title="220-tfidf-17" href="../hunch_net-2006/hunch_net-2006-07-08-Some_recent_papers.html">192 hunch net-2006-07-08-Some recent papers</a></p>
<p>18 0.097034171 <a title="220-tfidf-18" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>19 0.095823437 <a title="220-tfidf-19" href="../hunch_net-2005/hunch_net-2005-04-16-Which_Assumptions_are_Reasonable%3F.html">57 hunch net-2005-04-16-Which Assumptions are Reasonable?</a></p>
<p>20 0.095311962 <a title="220-tfidf-20" href="../hunch_net-2007/hunch_net-2007-08-12-Exponentiated_Gradient.html">258 hunch net-2007-08-12-Exponentiated Gradient</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.225), (1, -0.11), (2, -0.002), (3, -0.044), (4, -0.032), (5, -0.142), (6, 0.092), (7, 0.014), (8, -0.035), (9, -0.067), (10, -0.042), (11, -0.043), (12, 0.026), (13, -0.031), (14, 0.043), (15, 0.052), (16, -0.049), (17, 0.036), (18, -0.089), (19, 0.119), (20, -0.021), (21, 0.035), (22, -0.044), (23, -0.051), (24, -0.113), (25, -0.055), (26, -0.078), (27, -0.002), (28, 0.009), (29, 0.054), (30, 0.036), (31, -0.012), (32, 0.145), (33, -0.007), (34, 0.02), (35, -0.018), (36, 0.017), (37, 0.026), (38, 0.079), (39, -0.033), (40, -0.038), (41, -0.061), (42, 0.028), (43, 0.013), (44, -0.02), (45, 0.025), (46, -0.098), (47, 0.06), (48, -0.056), (49, -0.077)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9795509 <a title="220-lsi-1" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>Introduction: This post is about a general technique for problem solving which I've never
seen taught (in full generality), but which I've found very useful.Many
problems in computer science turn out to be discretely difficult. The best
known version of such problems are NP-hard problems, but I mean 'discretely
difficult' in a much more general way, which I only know how to capture by
examples.ERMIn empirical risk minimization, you choose a minimum error rate
classifier from a set of classifiers. This is NP hard for common sets, but it
can be much harder, depending on the set.ExpertsIn the online learning with
experts setting, you try to predict well so as to compete with a set of
(adversarial) experts. Here the alternating quantifiers of you and an
adversary playing out a game can yield a dynamic programming problem that
grows exponentially.Policy IterationThe problem with policy iteration is that
you learn a new policy with respect to an old policy, which implies that
simply adopting the new polic</p><p>2 0.74047917 <a title="220-lsi-2" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>Introduction: One thing which is clear on a little reflection is that there exists a single
master learning problem capable of encoding essentially all learning problems.
This problem is of course a very general sort of reinforcement learning where
the world interacts with an agent as:The world announces an observationx.The
agent makes a choicea.The world announces a rewardr.The goal here is to
maximize the sum of the rewards over the time of the agent. No particular
structure relatingxtoaoratoris implied by this setting so we do not know
effective general algorithms for the agent. It's very easy to prove lower
bounds showing that an agent cannot hope to succeed here--just consider the
case where actions are unrelated to rewards. Nevertheless, there is a real
sense in which essentially all forms of life are agents operating in this
setting, somehow succeeding. The gap between these observations drives
research--How can we find tractable specializations of the master problem
general enough to provide</p><p>3 0.68464971 <a title="220-lsi-3" href="../hunch_net-2009/hunch_net-2009-10-03-Static_vs._Dynamic_multiclass_prediction.html">373 hunch net-2009-10-03-Static vs. Dynamic multiclass prediction</a></p>
<p>Introduction: I have had interesting discussions about distinction between static vs.
dynamic classes withKishoreandHal.The distinction arises in multiclass
prediction settings. A static set of classes is given by a set of
labels{1,â&euro;Ś,k}and the goal is generally to choose the most likely label given
features. The static approach is the one that we typically analyze and think
about in machine learning.The dynamic setting is one that is often used in
practice. The basic idea is that the number of classes is not fixed, varying
on a per example basis. These different classes are generally defined by a
choice of features.The distinction between these two settings as far as theory
goes, appears to be very substantial. For example, in the static setting,
inlearning reductions land, we have techniques now for robustO(log(k))time
prediction in many multiclass setting variants. In the dynamic setting, the
best techniques known areO(k), and furthermore this exponential gap may be
essential, at least without fur</p><p>4 0.67113334 <a title="220-lsi-4" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<p>Introduction: There weretwopapersat ICML presenting learning algorithms for acontextual
bandit-style setting, where the loss for all labels is not known, but the loss
for one label is known. (The first might require aexploration
scavengingviewpoint to understand if the experimental assignment was
nonrandom.) I strongly approve of these papers and further work in this
setting and its variants, because I expect it to become more important than
supervised learning. As a quick review, we are thinking about situations where
repeatedly:The world reveals feature values (aka context information).A policy
chooses an action.The world provides a reward.Sometimes this is done in an
online fashion where the policy can change based on immediate feedback and
sometimes it's done in a batch setting where many samples are collected before
the policy can change. If you haven't spent time thinking about the setting,
you might want to because there are many natural applications.I'm going to
pick on the Banditron paper (</p><p>5 0.65257752 <a title="220-lsi-5" href="../hunch_net-2007/hunch_net-2007-10-24-Contextual_Bandits.html">269 hunch net-2007-10-24-Contextual Bandits</a></p>
<p>Introduction: One of the fundamental underpinnings of the internet is advertising based
content. This has become much more effective due to targeted advertising where
ads are specifically matched to interests. Everyone is familiar with this,
because everyone uses search engines and all search engines try to make money
this way.The problem of matching ads to interests is a natural machine
learning problem in some ways since there is much information in who clicks on
what. A fundamental problem with this information is that it is not supervised
--in particular a click-or-not on one ad doesn't generally tell you if a
different ad would have been clicked on. This implies we have a fundamental
exploration problem.A standard mathematical setting for this situation is
"k-Armed Bandits", often with various relevant embellishments. Thek-Armed
Bandit setting works on a round-by-round basis. On each round:A policy chooses
armafrom1ofkarms (i.e. 1 of k ads).The world reveals the rewardraof the chosen
arm (i.e.</p><p>6 0.59970397 <a title="220-lsi-6" href="../hunch_net-2005/hunch_net-2005-08-04-Why_Reinforcement_Learning_is_Important.html">100 hunch net-2005-08-04-Why Reinforcement Learning is Important</a></p>
<p>7 0.58974385 <a title="220-lsi-7" href="../hunch_net-2008/hunch_net-2008-09-12-How_do_we_get_weak_action_dependence_for_learning_with_partial_observations%3F.html">317 hunch net-2008-09-12-How do we get weak action dependence for learning with partial observations?</a></p>
<p>8 0.58315682 <a title="220-lsi-8" href="../hunch_net-2005/hunch_net-2005-08-08-Apprenticeship_Reinforcement_Learning_for_Control.html">101 hunch net-2005-08-08-Apprenticeship Reinforcement Learning for Control</a></p>
<p>9 0.56984204 <a title="220-lsi-9" href="../hunch_net-2006/hunch_net-2006-09-09-How_to_solve_an_NP_hard_problem_in_quadratic_time.html">206 hunch net-2006-09-09-How to solve an NP hard problem in quadratic time</a></p>
<p>10 0.5668515 <a title="220-lsi-10" href="../hunch_net-2006/hunch_net-2006-05-01-A_conversation_between_Theo_and_Pat.html">176 hunch net-2006-05-01-A conversation between Theo and Pat</a></p>
<p>11 0.56637228 <a title="220-lsi-11" href="../hunch_net-2005/hunch_net-2005-05-06-Don%26%238217%3Bt_mix_the_solution_into_the_problem.html">67 hunch net-2005-05-06-Don&#8217;t mix the solution into the problem</a></p>
<p>12 0.55789 <a title="220-lsi-12" href="../hunch_net-2005/hunch_net-2005-04-21-Dynamic_Programming_Generalizations_and_Their_Use.html">58 hunch net-2005-04-21-Dynamic Programming Generalizations and Their Use</a></p>
<p>13 0.55713814 <a title="220-lsi-13" href="../hunch_net-2009/hunch_net-2009-01-07-Interesting_Papers_at_SODA_2009.html">334 hunch net-2009-01-07-Interesting Papers at SODA 2009</a></p>
<p>14 0.55112517 <a title="220-lsi-14" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>15 0.54853457 <a title="220-lsi-15" href="../hunch_net-2008/hunch_net-2008-01-06-Research_Political_Issues.html">282 hunch net-2008-01-06-Research Political Issues</a></p>
<p>16 0.52489245 <a title="220-lsi-16" href="../hunch_net-2005/hunch_net-2005-09-08-Online_Learning_as_the_Mathematics_of_Accountability.html">109 hunch net-2005-09-08-Online Learning as the Mathematics of Accountability</a></p>
<p>17 0.52286613 <a title="220-lsi-17" href="../hunch_net-2005/hunch_net-2005-02-25-Problem%3A_Online_Learning.html">28 hunch net-2005-02-25-Problem: Online Learning</a></p>
<p>18 0.51913804 <a title="220-lsi-18" href="../hunch_net-2005/hunch_net-2005-07-13-Text_Entailment_at_AAAI.html">94 hunch net-2005-07-13-Text Entailment at AAAI</a></p>
<p>19 0.51477814 <a title="220-lsi-19" href="../hunch_net-2006/hunch_net-2006-04-05-What_is_state%3F.html">169 hunch net-2006-04-05-What is state?</a></p>
<p>20 0.51221079 <a title="220-lsi-20" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(35, 0.035), (38, 0.014), (42, 0.272), (45, 0.076), (55, 0.034), (68, 0.038), (69, 0.014), (74, 0.103), (91, 0.28), (95, 0.038)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.94748884 <a title="220-lda-1" href="../hunch_net-2007/hunch_net-2007-05-09-The_Missing_Bound.html">244 hunch net-2007-05-09-The Missing Bound</a></p>
<p>Introduction: Sham Kakadepoints out that we are missing a bound.Suppose we
havemsamplesxdrawn IID from some distributionD. Through the magic of
exponential moment method we know that:If the range ofxis bounded by an
interval of sizeI, aChernoff/Hoeffding style boundgives us a bound on the
deviations likeO(I/m0.5)(at least in crude form). A proof is on page 9here.If
the range ofxis bounded, and the variance (or a bound on the variance) is
known, thenBennett's boundcan give tighter results (*). This can be a huge
improvment when the true variance small.What's missing here is a bound that
depends on the observed variance rather than a bound on the variance. This
means that many people attempt to use Bennett's bound (incorrectly) by
plugging the observed variance in as the true variance, invalidating the bound
application. Most of the time, they get away with it, but this is a dangerous
move when doing machine learning. In machine learning, we are typically trying
to find a predictor with 0 expected los</p><p>same-blog 2 0.93050718 <a title="220-lda-2" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>Introduction: This post is about a general technique for problem solving which I've never
seen taught (in full generality), but which I've found very useful.Many
problems in computer science turn out to be discretely difficult. The best
known version of such problems are NP-hard problems, but I mean 'discretely
difficult' in a much more general way, which I only know how to capture by
examples.ERMIn empirical risk minimization, you choose a minimum error rate
classifier from a set of classifiers. This is NP hard for common sets, but it
can be much harder, depending on the set.ExpertsIn the online learning with
experts setting, you try to predict well so as to compete with a set of
(adversarial) experts. Here the alternating quantifiers of you and an
adversary playing out a game can yield a dynamic programming problem that
grows exponentially.Policy IterationThe problem with policy iteration is that
you learn a new policy with respect to an old policy, which implies that
simply adopting the new polic</p><p>3 0.92578447 <a title="220-lda-3" href="../hunch_net-2010/hunch_net-2010-08-21-Rob_Schapire_at_NYC_ML_Meetup.html">405 hunch net-2010-08-21-Rob Schapire at NYC ML Meetup</a></p>
<p>Introduction: I've been wanting to attend theNYC ML Meetupfor some time and hope to make
itnext week on the 25th.Rob Schapireis talking about "Playing Repeated Games",
which in my experience is far more relevant to machine learning than the title
might indicate.</p><p>4 0.92482519 <a title="220-lda-4" href="../hunch_net-2007/hunch_net-2007-11-05-CMU_wins_DARPA_Urban_Challenge.html">271 hunch net-2007-11-05-CMU wins DARPA Urban Challenge</a></p>
<p>Introduction: Theresults have been posted, withCMU first,Stanford second, andVirginia Tech
Third.Considering that this was an open event (at least for people in the US),
this was a very strong showing for research at universities (instead of
defense contractors, for example). Some details should become public at
theNIPS workshops.Slashdothas apostwith many comments.</p><p>5 0.90519077 <a title="220-lda-5" href="../hunch_net-2009/hunch_net-2009-01-07-Interesting_Papers_at_SODA_2009.html">334 hunch net-2009-01-07-Interesting Papers at SODA 2009</a></p>
<p>Introduction: Several talks seem potentially interesting to ML folks at this year's SODA
.Maria-Florina Balcan,Avrim Blum, andAnupam Gupta,Approximate Clustering
without the Approximation. This paper gives reasonable algorithms with
provable approximation guarantees for k-median and other notions of
clustering. It's conceptually interesting, because it's the second example
I've seen where NP hardness is subverted by changing the problem definition
subtle but reasonable way. Essentially, they show that if any near-
approximation to an optimal solution is good, then it's computationally easy
to find a near-optimal solution. This subtle shift bears serious thought. A
similar one occurred inour ranking paperwith respect to minimum feedback
arcset. With two known examples, it suggests that many more NP-complete
problems might be finessed into irrelevance in this style.Yury
LifshitsandShengyu Zhang,Combinatorial Algorithms for Nearest Neighbors, Near-
Duplicates, and Small-World Design. The basic idea of</p><p>6 0.85527885 <a title="220-lda-6" href="../hunch_net-2006/hunch_net-2006-01-18-Is_Multitask_Learning_Black-Boxable%3F.html">149 hunch net-2006-01-18-Is Multitask Learning Black-Boxable?</a></p>
<p>7 0.84976649 <a title="220-lda-7" href="../hunch_net-2007/hunch_net-2007-12-21-Vowpal_Wabbit_Code_Release.html">281 hunch net-2007-12-21-Vowpal Wabbit Code Release</a></p>
<p>8 0.79661769 <a title="220-lda-8" href="../hunch_net-2005/hunch_net-2005-02-23-Problem%3A_Reinforcement_Learning_with_Classification.html">27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</a></p>
<p>9 0.7750178 <a title="220-lda-9" href="../hunch_net-2011/hunch_net-2011-10-24-2011_ML_symposium_and_the_bears.html">448 hunch net-2011-10-24-2011 ML symposium and the bears</a></p>
<p>10 0.74055177 <a title="220-lda-10" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>11 0.73777699 <a title="220-lda-11" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>12 0.72531366 <a title="220-lda-12" href="../hunch_net-2008/hunch_net-2008-04-30-Concerns_about_the_Large_Scale_Learning_Challenge.html">300 hunch net-2008-04-30-Concerns about the Large Scale Learning Challenge</a></p>
<p>13 0.72469383 <a title="220-lda-13" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>14 0.72186089 <a title="220-lda-14" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>15 0.72141469 <a title="220-lda-15" href="../hunch_net-2009/hunch_net-2009-12-27-Interesting_things_at_NIPS_2009.html">385 hunch net-2009-12-27-Interesting things at NIPS 2009</a></p>
<p>16 0.72132802 <a title="220-lda-16" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>17 0.72047347 <a title="220-lda-17" href="../hunch_net-2005/hunch_net-2005-03-17-Going_all_the_Way%2C_Sometimes.html">42 hunch net-2005-03-17-Going all the Way, Sometimes</a></p>
<p>18 0.71919483 <a title="220-lda-18" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>19 0.71916163 <a title="220-lda-19" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>20 0.71880847 <a title="220-lda-20" href="../hunch_net-2009/hunch_net-2009-06-01-Multitask_Poisoning.html">358 hunch net-2009-06-01-Multitask Poisoning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
