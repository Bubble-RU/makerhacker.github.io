<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>220 hunch net-2006-11-27-Continuizing Solutions</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-220" href="#">hunch_net-2006-220</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>220 hunch net-2006-11-27-Continuizing Solutions</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-220-html" href="http://hunch.net/?p=240">html</a></p><p>Introduction: This post is about a general technique for problem solving which I’ve never seen taught (in full generality), but which I’ve found very useful.
 
Many problems in computer science turn out to be discretely difficult.  The best known version of such problems are NP-hard problems, but I mean ‘discretely difficult’ in a much more general way, which I only know how to capture by examples.
  
  ERM  In empirical risk minimization, you choose a minimum error rate classifier from a set of classifiers.  This is NP hard for common sets, but it can be much harder, depending on the set. 
  Experts  In the online learning with experts setting, you try to predict well so as to compete with a set of (adversarial) experts.  Here the alternating quantifiers of you and an adversary playing out a game can yield a dynamic programming problem that grows exponentially. 
  Policy Iteration  The problem with policy iteration is that you learn a new policy with respect to an old policy, which implies that sim</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 This post is about a general technique for problem solving which I’ve never seen taught (in full generality), but which I’ve found very useful. [sent-1, score-0.301]
</p><p>2 Many problems in computer science turn out to be discretely difficult. [sent-2, score-0.279]
</p><p>3 The best known version of such problems are NP-hard problems, but I mean ‘discretely difficult’ in a much more general way, which I only know how to capture by examples. [sent-3, score-0.174]
</p><p>4 ERM  In empirical risk minimization, you choose a minimum error rate classifier from a set of classifiers. [sent-4, score-0.188]
</p><p>5 This is NP hard for common sets, but it can be much harder, depending on the set. [sent-5, score-0.059]
</p><p>6 Experts  In the online learning with experts setting, you try to predict well so as to compete with a set of (adversarial) experts. [sent-6, score-0.384]
</p><p>7 Here the alternating quantifiers of you and an adversary playing out a game can yield a dynamic programming problem that grows exponentially. [sent-7, score-0.654]
</p><p>8 Policy Iteration  The problem with policy iteration is that you learn a new policy with respect to an old policy, which implies that simply adopting the new policy can go very wrong. [sent-8, score-2.343]
</p><p>9 For each of these problems, there are “continuized” solutions which can yield smaller computation, more elegant mathematics, or both. [sent-9, score-0.319]
</p><p>10 ERM  By shifting from choosing a single classifier to choosing a stochastic classifier we can prove a new style of bound which is significantly tighter, easier to state, and easier to understand than traditional bounds in the traditional setting. [sent-10, score-1.155]
</p><p>11 Experts  By giving the adversary slightly more power—the ability to split experts and have them fractionally predict one way vs. [sent-12, score-0.518]
</p><p>12 another, the optimal policy becomes much easier to compute (quadratic in the horizon, or maybe less). [sent-13, score-0.641]
</p><p>13 Policy Iteration  For policy iteration, by stochastically mixing the old and the new policy, we can find a new policy better than the old policy. [sent-15, score-1.54]
</p><p>14 The first and second examples both involve a setting shift, which may not be valid—in general your setting should reflect your real problem rather than the thing which is easy to solve. [sent-18, score-0.496]
</p><p>15 However, even with the setting shift, the solutions appear so compellingly more elegant that it is hard to not hope to use them in a solution to the original setting. [sent-19, score-0.502]
</p><p>16 I have not seen a good formulation of the general approach of continuizing. [sent-20, score-0.256]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('policy', 0.511), ('iteration', 0.357), ('experts', 0.256), ('discretely', 0.201), ('erm', 0.143), ('old', 0.141), ('setting', 0.132), ('easier', 0.13), ('elegant', 0.13), ('classifier', 0.123), ('adversary', 0.118), ('traditional', 0.115), ('shift', 0.113), ('choosing', 0.098), ('yield', 0.097), ('general', 0.096), ('solutions', 0.092), ('quantifiers', 0.089), ('continuous', 0.089), ('adopting', 0.089), ('compellingly', 0.089), ('alternating', 0.083), ('formulation', 0.083), ('danger', 0.083), ('bound', 0.081), ('new', 0.079), ('problems', 0.078), ('mixing', 0.078), ('split', 0.078), ('quadratic', 0.078), ('seen', 0.077), ('np', 0.074), ('grows', 0.074), ('horizon', 0.071), ('reflect', 0.071), ('valid', 0.071), ('generality', 0.067), ('conservative', 0.067), ('minimization', 0.067), ('predict', 0.066), ('tighter', 0.065), ('risk', 0.065), ('playing', 0.065), ('problem', 0.065), ('dynamic', 0.063), ('shifting', 0.063), ('taught', 0.063), ('compete', 0.062), ('mathematics', 0.059), ('hard', 0.059)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000002 <a title="220-tfidf-1" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>Introduction: This post is about a general technique for problem solving which I’ve never seen taught (in full generality), but which I’ve found very useful.
 
Many problems in computer science turn out to be discretely difficult.  The best known version of such problems are NP-hard problems, but I mean ‘discretely difficult’ in a much more general way, which I only know how to capture by examples.
  
  ERM  In empirical risk minimization, you choose a minimum error rate classifier from a set of classifiers.  This is NP hard for common sets, but it can be much harder, depending on the set. 
  Experts  In the online learning with experts setting, you try to predict well so as to compete with a set of (adversarial) experts.  Here the alternating quantifiers of you and an adversary playing out a game can yield a dynamic programming problem that grows exponentially. 
  Policy Iteration  The problem with policy iteration is that you learn a new policy with respect to an old policy, which implies that sim</p><p>2 0.20302184 <a title="220-tfidf-2" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>Introduction: One thing which is clear on a little reflection is that there exists a single master learning problem capable of encoding essentially all learning problems.  This problem is of course a very general sort of reinforcement learning where the world interacts with an agent as:
  
 The world announces an observation  x . 
 The agent makes a choice  a . 
 The world announces a reward  r . 
  
The goal here is to maximize the sum of the rewards over the time of the agent.  No particular structure relating  x  to  a  or  a  to  r  is implied by this setting so we do not know effective general algorithms for the agent.  It’s very easy to prove lower bounds showing that an agent cannot hope to succeed here—just consider the case where actions are unrelated to rewards.  Nevertheless, there is a real sense in which essentially all forms of life are agents operating in this setting, somehow succeeding.  The gap between these observations drives research—How can we find tractable specializations of</p><p>3 0.15269674 <a title="220-tfidf-3" href="../hunch_net-2007/hunch_net-2007-10-24-Contextual_Bandits.html">269 hunch net-2007-10-24-Contextual Bandits</a></p>
<p>Introduction: One of the fundamental underpinnings of the internet is advertising based content.  This has become much more effective due to targeted advertising where ads are specifically matched to interests.  Everyone is familiar with this, because everyone uses search engines and all search engines try to make money this way.
 
The problem of matching ads to interests is a natural machine learning problem in some ways since there is much information in who clicks on what.  A fundamental problem with this information is that it is not supervised—in particular a click-or-not on one ad doesn’t generally tell you if a different ad would have been clicked on.  This implies we have a fundamental exploration problem.
 
A standard mathematical setting for this situation is “ k -Armed Bandits”, often with various relevant embellishments.  The  k -Armed Bandit setting works on a round-by-round basis.  On each round:
  
 A policy chooses arm  a  from  1  of  k  arms (i.e. 1 of k ads). 
 The world reveals t</p><p>4 0.1405932 <a title="220-tfidf-4" href="../hunch_net-2005/hunch_net-2005-09-08-Online_Learning_as_the_Mathematics_of_Accountability.html">109 hunch net-2005-09-08-Online Learning as the Mathematics of Accountability</a></p>
<p>Introduction: Accountability is a social problem.  When someone screws up, do you fire them?  Or do you accept the error and let them continue?  This is a very difficult problem and we all know of stories where the wrong decision was made.
 
 Online learning  (as meant here), is a subfield of learning theory which analyzes the online learning model.  
 
In the online learning model, there are a set of hypotheses or “experts”.  On any instantance  x , each expert makes a prediction  y .  A master algorithm  A  uses these predictions to form it’s own prediction  y A   and then  learns the correct prediction  y *  .  This process repeats.
 
The goal of online learning is to find a master algorithm  A  which uses the advice of the experts to make good predictions.  In particular, we typically want to guarantee that the master algorithm performs almost as well as the best expert.  If  L(e)  is the loss of expert  e  and  L(A)  is the loss of the master algorithm, it is often possible to prove:   L(A) les</p><p>5 0.1360846 <a title="220-tfidf-5" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>Introduction: What?  Reductions are machines which turn solvers for one problem into solvers for another problem. 
 Why?  Reductions are useful for several reasons.
  
  Laziness .  Reducing a problem to classification make at least 10 learning algorithms available to solve a problem.  Inventing 10 learning algorithms is quite a bit of work.  Similarly, programming a reduction is often trivial, while programming a learning algorithm is a great deal of work. 
  Crystallization .  The problems we often want to solve in learning are worst-case-impossible, but average case feasible.  By reducing all problems onto one or a few primitives, we can fine tune these primitives to perform well on real-world problems with greater precision due to the greater number of problems to validate on. 
  Theoretical Organization .  By studying what reductions are easy vs. hard vs. impossible, we can learn which problems are roughly equivalent in difficulty and which are much harder. 
  
 What we know now .
 
 Typesafe r</p><p>6 0.13296539 <a title="220-tfidf-6" href="../hunch_net-2005/hunch_net-2005-08-04-Why_Reinforcement_Learning_is_Important.html">100 hunch net-2005-08-04-Why Reinforcement Learning is Important</a></p>
<p>7 0.13228545 <a title="220-tfidf-7" href="../hunch_net-2006/hunch_net-2006-11-22-Explicit_Randomization_in_Learning_algorithms.html">219 hunch net-2006-11-22-Explicit Randomization in Learning algorithms</a></p>
<p>8 0.13062106 <a title="220-tfidf-8" href="../hunch_net-2005/hunch_net-2005-08-08-Apprenticeship_Reinforcement_Learning_for_Control.html">101 hunch net-2005-08-08-Apprenticeship Reinforcement Learning for Control</a></p>
<p>9 0.11454307 <a title="220-tfidf-9" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>10 0.11168518 <a title="220-tfidf-10" href="../hunch_net-2008/hunch_net-2008-01-06-Research_Political_Issues.html">282 hunch net-2008-01-06-Research Political Issues</a></p>
<p>11 0.1019682 <a title="220-tfidf-11" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<p>12 0.10108186 <a title="220-tfidf-12" href="../hunch_net-2007/hunch_net-2007-06-14-Interesting_Papers_at_COLT_2007.html">247 hunch net-2007-06-14-Interesting Papers at COLT 2007</a></p>
<p>13 0.094187997 <a title="220-tfidf-13" href="../hunch_net-2005/hunch_net-2005-04-16-Which_Assumptions_are_Reasonable%3F.html">57 hunch net-2005-04-16-Which Assumptions are Reasonable?</a></p>
<p>14 0.093599558 <a title="220-tfidf-14" href="../hunch_net-2006/hunch_net-2006-10-22-Exemplar_programming.html">215 hunch net-2006-10-22-Exemplar programming</a></p>
<p>15 0.093538247 <a title="220-tfidf-15" href="../hunch_net-2005/hunch_net-2005-04-01-Basic_computer_science_research_takes_a_hit.html">50 hunch net-2005-04-01-Basic computer science research takes a hit</a></p>
<p>16 0.09287221 <a title="220-tfidf-16" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>17 0.092073351 <a title="220-tfidf-17" href="../hunch_net-2005/hunch_net-2005-09-05-Site_Update.html">107 hunch net-2005-09-05-Site Update</a></p>
<p>18 0.091470882 <a title="220-tfidf-18" href="../hunch_net-2006/hunch_net-2006-07-08-Some_recent_papers.html">192 hunch net-2006-07-08-Some recent papers</a></p>
<p>19 0.087848216 <a title="220-tfidf-19" href="../hunch_net-2005/hunch_net-2005-02-01-NIPS%3A_Online_Bayes.html">8 hunch net-2005-02-01-NIPS: Online Bayes</a></p>
<p>20 0.08568982 <a title="220-tfidf-20" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.201), (1, 0.1), (2, 0.005), (3, 0.014), (4, 0.024), (5, -0.072), (6, 0.082), (7, -0.009), (8, -0.053), (9, 0.043), (10, 0.107), (11, 0.056), (12, 0.038), (13, 0.032), (14, -0.002), (15, -0.002), (16, 0.028), (17, -0.109), (18, 0.039), (19, 0.119), (20, -0.064), (21, -0.029), (22, -0.077), (23, 0.009), (24, -0.006), (25, 0.106), (26, -0.054), (27, 0.123), (28, 0.048), (29, 0.04), (30, -0.067), (31, -0.003), (32, 0.081), (33, 0.047), (34, 0.036), (35, 0.05), (36, 0.072), (37, 0.083), (38, 0.042), (39, -0.034), (40, -0.006), (41, -0.006), (42, 0.041), (43, 0.052), (44, -0.022), (45, -0.032), (46, 0.083), (47, -0.072), (48, 0.011), (49, -0.009)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97469306 <a title="220-lsi-1" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>Introduction: This post is about a general technique for problem solving which I’ve never seen taught (in full generality), but which I’ve found very useful.
 
Many problems in computer science turn out to be discretely difficult.  The best known version of such problems are NP-hard problems, but I mean ‘discretely difficult’ in a much more general way, which I only know how to capture by examples.
  
  ERM  In empirical risk minimization, you choose a minimum error rate classifier from a set of classifiers.  This is NP hard for common sets, but it can be much harder, depending on the set. 
  Experts  In the online learning with experts setting, you try to predict well so as to compete with a set of (adversarial) experts.  Here the alternating quantifiers of you and an adversary playing out a game can yield a dynamic programming problem that grows exponentially. 
  Policy Iteration  The problem with policy iteration is that you learn a new policy with respect to an old policy, which implies that sim</p><p>2 0.74505579 <a title="220-lsi-2" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>Introduction: One thing which is clear on a little reflection is that there exists a single master learning problem capable of encoding essentially all learning problems.  This problem is of course a very general sort of reinforcement learning where the world interacts with an agent as:
  
 The world announces an observation  x . 
 The agent makes a choice  a . 
 The world announces a reward  r . 
  
The goal here is to maximize the sum of the rewards over the time of the agent.  No particular structure relating  x  to  a  or  a  to  r  is implied by this setting so we do not know effective general algorithms for the agent.  It’s very easy to prove lower bounds showing that an agent cannot hope to succeed here—just consider the case where actions are unrelated to rewards.  Nevertheless, there is a real sense in which essentially all forms of life are agents operating in this setting, somehow succeeding.  The gap between these observations drives research—How can we find tractable specializations of</p><p>3 0.7165038 <a title="220-lsi-3" href="../hunch_net-2007/hunch_net-2007-10-24-Contextual_Bandits.html">269 hunch net-2007-10-24-Contextual Bandits</a></p>
<p>Introduction: One of the fundamental underpinnings of the internet is advertising based content.  This has become much more effective due to targeted advertising where ads are specifically matched to interests.  Everyone is familiar with this, because everyone uses search engines and all search engines try to make money this way.
 
The problem of matching ads to interests is a natural machine learning problem in some ways since there is much information in who clicks on what.  A fundamental problem with this information is that it is not supervised—in particular a click-or-not on one ad doesn’t generally tell you if a different ad would have been clicked on.  This implies we have a fundamental exploration problem.
 
A standard mathematical setting for this situation is “ k -Armed Bandits”, often with various relevant embellishments.  The  k -Armed Bandit setting works on a round-by-round basis.  On each round:
  
 A policy chooses arm  a  from  1  of  k  arms (i.e. 1 of k ads). 
 The world reveals t</p><p>4 0.69090003 <a title="220-lsi-4" href="../hunch_net-2008/hunch_net-2008-09-12-How_do_we_get_weak_action_dependence_for_learning_with_partial_observations%3F.html">317 hunch net-2008-09-12-How do we get weak action dependence for learning with partial observations?</a></p>
<p>Introduction: This post is about contextual bandit problems where, repeatedly:
  
 The world chooses features  x  and rewards for each action  r 1 ,…,r k   then announces the features  x  (but not the rewards). 
 A policy chooses an action  a . 
 The world announces the reward  r a   
  
The goal in these situations is to learn a policy which maximizes  r a   in expectation efficiently.  I’m thinking about all situations which fit the above setting, whether they are drawn IID or adversarially from round to round and whether they involve past logged data or rapidly learning via interaction.
 
One common drawback of all algorithms for solving this setting, is that they have a poor dependence on the number of actions.  For example if  k  is the number of actions,  EXP4 (page 66)  has a dependence on  k 0.5  ,  epoch-greedy  (and the simpler epsilon greedy) have a dependence on  k 1/3  , and the  offset tree  has a dependence on  k-1 .  These results aren’t directly comparable because different things a</p><p>5 0.67899531 <a title="220-lsi-5" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<p>Introduction: There were  two   papers  at ICML presenting learning algorithms for a  contextual bandit -style setting, where the loss for all labels is not known, but the loss for one label is known.  (The first might require a  exploration scavenging  viewpoint to understand if the experimental assignment was nonrandom.)  I strongly approve of these papers and further work in this setting and its variants, because I expect it to become more important than supervised learning.  As a quick review, we are thinking about situations where repeatedly:
  
 The world reveals feature values (aka context information). 
 A policy chooses an action. 
 The world provides a reward. 
  
Sometimes this is done in an online fashion where the policy can change based on immediate feedback and sometimes it’s done in a batch setting where many samples are collected before the policy can change.  If you haven’t spent time thinking about the setting, you might want to because there are many natural applications.
 
I’m g</p><p>6 0.62866789 <a title="220-lsi-6" href="../hunch_net-2009/hunch_net-2009-10-03-Static_vs._Dynamic_multiclass_prediction.html">373 hunch net-2009-10-03-Static vs. Dynamic multiclass prediction</a></p>
<p>7 0.60205555 <a title="220-lsi-7" href="../hunch_net-2005/hunch_net-2005-08-08-Apprenticeship_Reinforcement_Learning_for_Control.html">101 hunch net-2005-08-08-Apprenticeship Reinforcement Learning for Control</a></p>
<p>8 0.55541402 <a title="220-lsi-8" href="../hunch_net-2005/hunch_net-2005-08-04-Why_Reinforcement_Learning_is_Important.html">100 hunch net-2005-08-04-Why Reinforcement Learning is Important</a></p>
<p>9 0.53157032 <a title="220-lsi-9" href="../hunch_net-2006/hunch_net-2006-03-12-Online_learning_or_online_preservation_of_learning%3F.html">163 hunch net-2006-03-12-Online learning or online preservation of learning?</a></p>
<p>10 0.5265789 <a title="220-lsi-10" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>11 0.52403843 <a title="220-lsi-11" href="../hunch_net-2005/hunch_net-2005-04-21-Dynamic_Programming_Generalizations_and_Their_Use.html">58 hunch net-2005-04-21-Dynamic Programming Generalizations and Their Use</a></p>
<p>12 0.5196104 <a title="220-lsi-12" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>13 0.50396621 <a title="220-lsi-13" href="../hunch_net-2005/hunch_net-2005-09-08-Online_Learning_as_the_Mathematics_of_Accountability.html">109 hunch net-2005-09-08-Online Learning as the Mathematics of Accountability</a></p>
<p>14 0.48508129 <a title="220-lsi-14" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>15 0.48347697 <a title="220-lsi-15" href="../hunch_net-2006/hunch_net-2006-01-13-Benchmarks_for_RL.html">148 hunch net-2006-01-13-Benchmarks for RL</a></p>
<p>16 0.47827372 <a title="220-lsi-16" href="../hunch_net-2010/hunch_net-2010-03-15-The_Efficient_Robust_Conditional_Probability_Estimation_Problem.html">391 hunch net-2010-03-15-The Efficient Robust Conditional Probability Estimation Problem</a></p>
<p>17 0.47164208 <a title="220-lsi-17" href="../hunch_net-2009/hunch_net-2009-01-07-Interesting_Papers_at_SODA_2009.html">334 hunch net-2009-01-07-Interesting Papers at SODA 2009</a></p>
<p>18 0.46884894 <a title="220-lsi-18" href="../hunch_net-2005/hunch_net-2005-06-06-Exact_Online_Learning_for_Classification.html">78 hunch net-2005-06-06-Exact Online Learning for Classification</a></p>
<p>19 0.46702385 <a title="220-lsi-19" href="../hunch_net-2006/hunch_net-2006-04-05-What_is_state%3F.html">169 hunch net-2006-04-05-What is state?</a></p>
<p>20 0.46246567 <a title="220-lsi-20" href="../hunch_net-2005/hunch_net-2005-02-25-Problem%3A_Online_Learning.html">28 hunch net-2005-02-25-Problem: Online Learning</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.045), (3, 0.025), (27, 0.301), (53, 0.048), (55, 0.111), (75, 0.168), (77, 0.045), (94, 0.073), (95, 0.077)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.93808615 <a title="220-lda-1" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>Introduction: This post is about a general technique for problem solving which I’ve never seen taught (in full generality), but which I’ve found very useful.
 
Many problems in computer science turn out to be discretely difficult.  The best known version of such problems are NP-hard problems, but I mean ‘discretely difficult’ in a much more general way, which I only know how to capture by examples.
  
  ERM  In empirical risk minimization, you choose a minimum error rate classifier from a set of classifiers.  This is NP hard for common sets, but it can be much harder, depending on the set. 
  Experts  In the online learning with experts setting, you try to predict well so as to compete with a set of (adversarial) experts.  Here the alternating quantifiers of you and an adversary playing out a game can yield a dynamic programming problem that grows exponentially. 
  Policy Iteration  The problem with policy iteration is that you learn a new policy with respect to an old policy, which implies that sim</p><p>2 0.8708868 <a title="220-lda-2" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>Introduction: A little over 4 years ago,  Sanjoy   made a post  saying roughly “we should  study active learning theoretically, because not much is understood”.   
 
At the time, we did not understand basic things such as whether or not it was possible to PAC-learn with an active algorithm without making strong assumptions about the noise rate.  In other words, the fundamental question was “can we do it?”
 
The nature of the question has fundamentally changed in my mind.   The answer is to the previous question is “yes”, both information theoretically and computationally, most places where supervised learning could be applied.  
 
In many situation, the question has now changed to: “is it worth it?”  Is the programming and computational overhead low enough to make the label cost savings of active learning worthwhile?  Currently, there are situations where this question could go either way.  Much of the challenge for the future is in figuring out how to make active learning easier or more worthwhile.</p><p>3 0.86703724 <a title="220-lda-3" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>Introduction: If you search for “online learning” with any  major   search   engine , it’s interesting to note that zero of the results are for online machine learning.  This may not be a mistake if you are committed to a global ordering.  In other words, the number of people specifically interested in the least interesting top-10 online human learning result might exceed the number of people interested in online machine learning, even given the presence of the other 9 results.  The essential observation here is that the process of human learning is a big business (around 5% of GDP) effecting virtually everyone.  
 
The internet is changing this dramatically, by altering the economics of teaching. Consider two possibilities:
  
 The classroom-style teaching environment continues as is, with many teachers for the same subject. 
 All the teachers for one subject get together, along with perhaps a factor of 2 more people who are experts in online delivery.  They spend a factor of 4 more time designing</p><p>4 0.86103344 <a title="220-lda-4" href="../hunch_net-2007/hunch_net-2007-01-02-Retrospective.html">225 hunch net-2007-01-02-Retrospective</a></p>
<p>Introduction: It’s been almost two years since this blog began.  In that time, I’ve learned enough to shift my expectations in several ways.
  
 Initially, the idea was for a general purpose ML blog where different people could contribute posts.  What has actually happened is most posts come from me, with a few guest posts that I greatly value.  There are a few reasons I see for this.
 
  Overload .  A couple years ago, I had not fully appreciated just how busy life gets for a researcher.  Making a post is not simply a matter of getting to it, but rather of prioritizing between {writing a grant, finishing an overdue review, writing a paper, teaching a class, writing a program, etc…}.  This is a substantial transition away from what life as a graduate student is like.  At some point the question is not “when will I get to it?” but rather “will I get to it?” and the answer starts to become “no” most of the time. 
  Feedback failure .  This blog currently receives about 3K unique visitors per day from</p><p>5 0.86083049 <a title="220-lda-5" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>Introduction: Few would mistake the process of academic paper review for a fair process, but sometimes the unfairness seems particularly striking.  This is most easily seen by comparison:
  
 
 Paper 
  Banditron  
  Offset Tree  
 Notes 
 
 
 Problem Scope 
 Multiclass problems where only the loss of one choice can be probed. 
 Strictly greater: Cost sensitive multiclass problems where only the loss of one choice can be probed. 
 Often generalizations don’t matter.  That’s not the case here, since every plausible application I’ve thought of involves loss functions substantially different from 0/1. 
 
 
 What’s new 
 Analysis and Experiments 
 Algorithm, Analysis, and Experiments 
  As far as I know, the essence of the more general problem was first stated and analyzed with the  EXP4 algorithm (page 16)  (1998).  It’s also the time horizon 1 simplification of the Reinforcement Learning setting for the  random trajectory method (page 15)  (2002).  The Banditron algorithm itself is functionally identi</p><p>6 0.85931015 <a title="220-lda-6" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>7 0.85903382 <a title="220-lda-7" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>8 0.85894614 <a title="220-lda-8" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>9 0.85792542 <a title="220-lda-9" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>10 0.85766351 <a title="220-lda-10" href="../hunch_net-2006/hunch_net-2006-07-11-New_Models.html">194 hunch net-2006-07-11-New Models</a></p>
<p>11 0.85634577 <a title="220-lda-11" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>12 0.85580754 <a title="220-lda-12" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>13 0.85572398 <a title="220-lda-13" href="../hunch_net-2007/hunch_net-2007-02-02-Thoughts_regarding_%26%238220%3BIs_machine_learning_different_from_statistics%3F%26%238221%3B.html">230 hunch net-2007-02-02-Thoughts regarding &#8220;Is machine learning different from statistics?&#8221;</a></p>
<p>14 0.85428065 <a title="220-lda-14" href="../hunch_net-2007/hunch_net-2007-08-12-Exponentiated_Gradient.html">258 hunch net-2007-08-12-Exponentiated Gradient</a></p>
<p>15 0.85374582 <a title="220-lda-15" href="../hunch_net-2008/hunch_net-2008-03-23-Interactive_Machine_Learning.html">293 hunch net-2008-03-23-Interactive Machine Learning</a></p>
<p>16 0.85176563 <a title="220-lda-16" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>17 0.85064632 <a title="220-lda-17" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>18 0.84977436 <a title="220-lda-18" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>19 0.84842396 <a title="220-lda-19" href="../hunch_net-2009/hunch_net-2009-09-21-Netflix_finishes_%28and_starts%29.html">371 hunch net-2009-09-21-Netflix finishes (and starts)</a></p>
<p>20 0.84812236 <a title="220-lda-20" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
