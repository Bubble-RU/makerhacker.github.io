<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>148 hunch net-2006-01-13-Benchmarks for RL</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-148" href="#">hunch_net-2006-148</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>148 hunch net-2006-01-13-Benchmarks for RL</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-148-html" href="http://hunch.net/?p=159">html</a></p><p>Introduction: A couple years ago, Drew Bagnell and I started the  RLBench project  to setup a suite of reinforcement learning benchmark problems.  We haven’t been able to touch it (due to lack of time) for a year so the project is on hold.  Luckily, there are several other projects such as  CLSquare  and  RL-Glue  with a similar goal, and we strongly endorse their continued development.
 
I would like to explain why, especially in the context of criticism of other learning benchmarks.   For example, sometimes the  UCI Machine Learning Repository  is criticized.  There are two criticisms I know of:
  
 Learning algorithms have overfit to the problems in the repository.  It is easy to imagine a mechanism for this happening unintentionally.  Strong evidence of this would be provided by learning algorithms which perform great on the UCI machine learning repository but very badly (relative to other learning algorithms) on non-UCI learning problems.  I have seen little evidence of this but it remains a po</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 A couple years ago, Drew Bagnell and I started the  RLBench project  to setup a suite of reinforcement learning benchmark problems. [sent-1, score-0.997]
</p><p>2 There are two criticisms I know of:     Learning algorithms have overfit to the problems in the repository. [sent-6, score-0.417]
</p><p>3 Strong evidence of this would be provided by learning algorithms which perform great on the UCI machine learning repository but very badly (relative to other learning algorithms) on non-UCI learning problems. [sent-8, score-0.825]
</p><p>4 A well run prediction competition provides some of the best evidence available about which learning algorithms work well. [sent-11, score-0.302]
</p><p>5 There are still some mechanisms for being misled (too many entries, not enough testing data, unlucky choice of learning problem that your algorithm just has the wrong bias for), but they are more controlled than anywhere else. [sent-12, score-0.525]
</p><p>6 Typically, there is little reason to believe that simulated data reflects the structure of real-world problems. [sent-18, score-0.477]
</p><p>7 The repository enforced a certain interface which many natural learning problems did not fit into. [sent-20, score-0.898]
</p><p>8 Some interface must be enforced in order for a benchmark/repository to be useful. [sent-21, score-0.287]
</p><p>9 This implies that problems fitting the interface are preferred both in submission of new problems and in choice of algorithms to solve. [sent-22, score-0.658]
</p><p>10 This is a reasonable complaint, but it’s basically a complaint that the UCI machine learning repository was too succesful. [sent-23, score-0.592]
</p><p>11 It is important to consider the benefits of a benchmark suite as well. [sent-26, score-0.49]
</p><p>12 The standard in reinforcement learning for experimental studies in reinforcement learning algorithms has been showing that some new algorithm does something reasonable on one or two reinforcement learning problems. [sent-27, score-1.301]
</p><p>13 Naturally, what this implies in practice is that the algorithms were hand tuned to work on these one-or-two problems, and there was relatively little emphasis on building a general purpose reinforcement learning algorithm. [sent-28, score-0.733]
</p><p>14 This is strongly at odds with the end goal of the reinforcement learning community! [sent-29, score-0.48]
</p><p>15 The way to avoid this is by setting up a benchmark suite (the more problems, the better). [sent-30, score-0.49]
</p><p>16 With a large set of problems that people can easily test on, the natural emphasis in empirical algorithm development shifts towards general purpose algorithms. [sent-31, score-0.716]
</p><p>17 When people reimplement simulators for reinforcement learning problems, it turns out these reimplementations typically differ in the details, and these details can radically alter the difficulty of the problem. [sent-33, score-0.474]
</p><p>18 Again, a large set of problems people can easily test on solves this problem because the precise definition of the problem is shared. [sent-35, score-0.366]
</p><p>19 One sometimes reasonable view of the world is that if you define a problem publicly, it is as good as solved since there are plenty of smart people out there eager to prove themselves. [sent-36, score-0.325]
</p><p>20 According to this view of the world, setting up a benchmark is a vital task. [sent-37, score-0.361]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('repository', 0.28), ('benchmark', 0.28), ('reinforcement', 0.257), ('simulated', 0.255), ('suite', 0.21), ('uci', 0.182), ('misled', 0.17), ('problems', 0.165), ('benchmarks', 0.151), ('interface', 0.147), ('competitions', 0.14), ('enforced', 0.14), ('complaint', 0.132), ('algorithms', 0.12), ('emphasis', 0.107), ('evidence', 0.101), ('reasonable', 0.099), ('purpose', 0.089), ('natural', 0.085), ('project', 0.085), ('setup', 0.084), ('learning', 0.081), ('view', 0.081), ('little', 0.079), ('strongly', 0.079), ('data', 0.077), ('rlbench', 0.076), ('touch', 0.076), ('continued', 0.076), ('eager', 0.076), ('hill', 0.076), ('unlucky', 0.076), ('empirical', 0.073), ('sanity', 0.07), ('webserver', 0.07), ('car', 0.07), ('kept', 0.07), ('reimplement', 0.07), ('problem', 0.069), ('algorithm', 0.068), ('details', 0.066), ('shifts', 0.066), ('announce', 0.066), ('criticisms', 0.066), ('luckily', 0.066), ('overfit', 0.066), ('reflects', 0.066), ('test', 0.063), ('goal', 0.063), ('choice', 0.061)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999934 <a title="148-tfidf-1" href="../hunch_net-2006/hunch_net-2006-01-13-Benchmarks_for_RL.html">148 hunch net-2006-01-13-Benchmarks for RL</a></p>
<p>Introduction: A couple years ago, Drew Bagnell and I started the  RLBench project  to setup a suite of reinforcement learning benchmark problems.  We haven’t been able to touch it (due to lack of time) for a year so the project is on hold.  Luckily, there are several other projects such as  CLSquare  and  RL-Glue  with a similar goal, and we strongly endorse their continued development.
 
I would like to explain why, especially in the context of criticism of other learning benchmarks.   For example, sometimes the  UCI Machine Learning Repository  is criticized.  There are two criticisms I know of:
  
 Learning algorithms have overfit to the problems in the repository.  It is easy to imagine a mechanism for this happening unintentionally.  Strong evidence of this would be provided by learning algorithms which perform great on the UCI machine learning repository but very badly (relative to other learning algorithms) on non-UCI learning problems.  I have seen little evidence of this but it remains a po</p><p>2 0.16976093 <a title="148-tfidf-2" href="../hunch_net-2005/hunch_net-2005-02-23-Problem%3A_Reinforcement_Learning_with_Classification.html">27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</a></p>
<p>Introduction: At an intuitive level, the question here is “Can reinforcement learning be solved with classification?”  
 
 Problem  Construct a reinforcement learning algorithm with near-optimal expected sum of rewards in the  direct experience model  given access to a classifier learning algorithm which has a small error rate or regret on all posed classification problems.  The definition of “posed” here is slightly murky.  I consider a problem “posed” if there is an algorithm for constructing labeled classification examples.
 
 Past Work 
  
 There exists a  reduction of reinforcement learning to classification given a generative model.   A generative model is an inherently stronger assumption than the direct experience model. 
 Other  work on learning reductions  may be important. 
 Several algorithms for solving reinforcement learning in the direct experience model exist.  Most, such as  E 3  ,  Factored-E 3  , and  metric-E 3   and  Rmax  require that the observation be the state.  Recent work</p><p>3 0.13297714 <a title="148-tfidf-3" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>Introduction: Exploration is one of the big unsolved problems in machine learning. This isn’t for lack of trying—there are many models of exploration which have been analyzed in many different ways by many different groups of people. At some point, it is worthwhile to sit back and see what has been done across these many models.
  
   Reinforcement Learning  (1) . Reinforcement learning has traditionally focused on Markov Decision Processes where the next state  s’    is given by a conditional distribution  P(s’|s,a)  given the current state  s  and action  a .  The typical result here is that certain specific algorithms controlling an agent can behave within  e  of optimal for horizon  T  except for  poly(1/e,T,S,A)  “wasted” experiences (with high probability).  This started with  E 3   by  Satinder Singh  and  Michael Kearns .  Sham Kakade’s thesis  has significant discussion. Extensions have typically been of the form “under extra assumptions, we can prove more”, for example  Factored-E 3   and</p><p>4 0.13082638 <a title="148-tfidf-4" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>Introduction: I’ve had serious conversations with several people who believe that the theory in machine learning is “only useful for getting papers published”.  That’s a compelling statement, as I’ve seen many papers where the algorithm clearly came first, and the theoretical justification for it came second, purely as a perceived means to improve the chance of publication. 
 
Naturally, I disagree and believe that learning theory has much more substantial applications.  
 
Even in core learning algorithm design, I’ve found learning theory to be useful, although it’s application is more subtle than many realize.  The most straightforward applications can fail, because (as expectation suggests) worst case bounds tend to be loose in practice (*).  In my experience, considering learning theory when designing an algorithm has two important effects in practice:
  
 It can help make your algorithm behave right at a crude level of analysis, leaving finer details to tuning or common sense.  The best example</p><p>5 0.12282889 <a title="148-tfidf-5" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>Introduction: There are several different flavors of Machine Learning classes.  Many classes are of the ‘zoo’ sort: many different learning algorithms are presented.  Others avoid the zoo by not covering the full scope of machine learning.  
 
This is my view of what makes a good machine learning class, along with why.  I’d like to specifically invite comment on whether things are missing, misemphasized, or misplaced.
  
 
 Phase 
 Subject 
 Why? 
 
 
 Introduction 
 What is a machine learning problem? 
 A good understanding of the characteristics of machine learning problems seems essential.  Characteristics include: a data source, some hope the data is predictive, and a need for generalization.  This is probably best taught in a case study manner: lay out the specifics of some problem and then ask “Is this a machine learning problem?” 
 
 
 Introduction 
 Machine Learning Problem Identification 
 Identification and recognition of the type of learning problems is (obviously) a very important step i</p><p>6 0.12130676 <a title="148-tfidf-6" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>7 0.11782699 <a title="148-tfidf-7" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>8 0.11706179 <a title="148-tfidf-8" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>9 0.11321969 <a title="148-tfidf-9" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>10 0.11269147 <a title="148-tfidf-10" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>11 0.11265998 <a title="148-tfidf-11" href="../hunch_net-2007/hunch_net-2007-10-19-Second_Annual_Reinforcement_Learning_Competition.html">268 hunch net-2007-10-19-Second Annual Reinforcement Learning Competition</a></p>
<p>12 0.11014488 <a title="148-tfidf-12" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>13 0.10977628 <a title="148-tfidf-13" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>14 0.10799025 <a title="148-tfidf-14" href="../hunch_net-2006/hunch_net-2006-03-02-Why_do_people_count_for_learning%3F.html">160 hunch net-2006-03-02-Why do people count for learning?</a></p>
<p>15 0.10669062 <a title="148-tfidf-15" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>16 0.10563168 <a title="148-tfidf-16" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>17 0.10424498 <a title="148-tfidf-17" href="../hunch_net-2005/hunch_net-2005-01-24-Holy_grails_of_machine_learning%3F.html">2 hunch net-2005-01-24-Holy grails of machine learning?</a></p>
<p>18 0.096863501 <a title="148-tfidf-18" href="../hunch_net-2005/hunch_net-2005-08-04-Why_Reinforcement_Learning_is_Important.html">100 hunch net-2005-08-04-Why Reinforcement Learning is Important</a></p>
<p>19 0.096578084 <a title="148-tfidf-19" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>20 0.092985086 <a title="148-tfidf-20" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.254), (1, 0.073), (2, -0.046), (3, 0.034), (4, 0.028), (5, -0.048), (6, 0.014), (7, 0.039), (8, 0.014), (9, 0.003), (10, -0.041), (11, 0.026), (12, 0.018), (13, 0.075), (14, -0.017), (15, 0.035), (16, 0.055), (17, -0.115), (18, -0.048), (19, 0.034), (20, 0.005), (21, -0.024), (22, -0.039), (23, -0.064), (24, -0.067), (25, -0.001), (26, -0.024), (27, -0.037), (28, -0.028), (29, -0.004), (30, -0.04), (31, -0.015), (32, -0.006), (33, -0.018), (34, 0.009), (35, -0.024), (36, 0.105), (37, 0.027), (38, 0.114), (39, -0.038), (40, 0.019), (41, 0.029), (42, 0.003), (43, 0.063), (44, 0.021), (45, 0.096), (46, 0.028), (47, 0.022), (48, 0.016), (49, 0.001)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94003916 <a title="148-lsi-1" href="../hunch_net-2006/hunch_net-2006-01-13-Benchmarks_for_RL.html">148 hunch net-2006-01-13-Benchmarks for RL</a></p>
<p>Introduction: A couple years ago, Drew Bagnell and I started the  RLBench project  to setup a suite of reinforcement learning benchmark problems.  We haven’t been able to touch it (due to lack of time) for a year so the project is on hold.  Luckily, there are several other projects such as  CLSquare  and  RL-Glue  with a similar goal, and we strongly endorse their continued development.
 
I would like to explain why, especially in the context of criticism of other learning benchmarks.   For example, sometimes the  UCI Machine Learning Repository  is criticized.  There are two criticisms I know of:
  
 Learning algorithms have overfit to the problems in the repository.  It is easy to imagine a mechanism for this happening unintentionally.  Strong evidence of this would be provided by learning algorithms which perform great on the UCI machine learning repository but very badly (relative to other learning algorithms) on non-UCI learning problems.  I have seen little evidence of this but it remains a po</p><p>2 0.75121826 <a title="148-lsi-2" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>Introduction: The  International Planning Competition  (IPC) is a biennial event organized in the context of the  International Conference on Automated Planning and Scheduling  (ICAPS). This year, for the first time, there will a learning track of the competition. For more information you can go to the competition  web-site . 
 
The competitions are typically organized around a number of planning domains that can vary from year to year, where a planning domain is simply a class of problems that share a common action schema—e.g. Blocksworld is a well-known planning domain that contains a problem instance each possible initial tower configuration and goal configuration. Some other domains have included Logistics, Airport, Freecell, PipesWorld, and many  others . For each domain the competition includes a number of problems (say 40-50) and the planners are run on each problem with a time limit for each problem (around 30 minutes). The problems are hard enough that many problems are not solved within th</p><p>3 0.70881706 <a title="148-lsi-3" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>Introduction: I’ve had serious conversations with several people who believe that the theory in machine learning is “only useful for getting papers published”.  That’s a compelling statement, as I’ve seen many papers where the algorithm clearly came first, and the theoretical justification for it came second, purely as a perceived means to improve the chance of publication. 
 
Naturally, I disagree and believe that learning theory has much more substantial applications.  
 
Even in core learning algorithm design, I’ve found learning theory to be useful, although it’s application is more subtle than many realize.  The most straightforward applications can fail, because (as expectation suggests) worst case bounds tend to be loose in practice (*).  In my experience, considering learning theory when designing an algorithm has two important effects in practice:
  
 It can help make your algorithm behave right at a crude level of analysis, leaving finer details to tuning or common sense.  The best example</p><p>4 0.69816279 <a title="148-lsi-4" href="../hunch_net-2005/hunch_net-2005-10-26-Fallback_Analysis_is_a_Secret_to_Useful_Algorithms.html">126 hunch net-2005-10-26-Fallback Analysis is a Secret to Useful Algorithms</a></p>
<p>Introduction: The ideal of theoretical algorithm analysis is to construct an algorithm with accompanying optimality theorems proving that it is a useful algorithm.  This ideal often fails, particularly for learning algorithms and theory.  The general form of a theorem is:  If  preconditions  Then  postconditions     When we design learning algorithms it is very common to come up with precondition assumptions such as “the data is IID”, “the learning problem is drawn from a known distribution over learning problems”, or “there is a perfect classifier”.  All of these example preconditions can be false for real-world problems in ways that are not easily detectable.  This means that algorithms derived and justified by these very common forms of analysis may be prone to catastrophic failure in routine (mis)application.
 
We  can  hope for better.  Several different kinds of learning algorithm analysis have been developed some of which have fewer preconditions.  Simply demanding that these forms of analysi</p><p>5 0.69557065 <a title="148-lsi-5" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>Introduction: This post is partly meant as an advertisement for the  reductions tutorial   Alina ,  Bianca , and I are planning to do at  ICML .  Please come, if you are interested.
 
Many research programs can be thought of as finding and building new useful abstractions.  The running example I’ll use is  learning reductions  where I have experience.  The basic abstraction here is that we can build a learning algorithm capable of solving classification problems up to a small expected regret.   This is used repeatedly to solve more complex problems.
 
In working on a new abstraction, I think you typically run into many substantial problems of understanding, which make publishing particularly difficult.
  
 It is difficult to seriously discuss the reason behind or mechanism for abstraction in a conference paper with small page limits.  People rarely see such discussions and hence have little basis on which to think about new abstractions.    Another difficulty is that when building an abstraction, yo</p><p>6 0.69140202 <a title="148-lsi-6" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>7 0.6830157 <a title="148-lsi-7" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>8 0.68183345 <a title="148-lsi-8" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>9 0.68165594 <a title="148-lsi-9" href="../hunch_net-2005/hunch_net-2005-02-26-Problem%3A_Reductions_and_Relative_Ranking_Metrics.html">31 hunch net-2005-02-26-Problem: Reductions and Relative Ranking Metrics</a></p>
<p>10 0.67974442 <a title="148-lsi-10" href="../hunch_net-2005/hunch_net-2005-02-23-Problem%3A_Reinforcement_Learning_with_Classification.html">27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</a></p>
<p>11 0.67762512 <a title="148-lsi-11" href="../hunch_net-2005/hunch_net-2005-08-22-Do_you_believe_in_induction%3F.html">104 hunch net-2005-08-22-Do you believe in induction?</a></p>
<p>12 0.6766749 <a title="148-lsi-12" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>13 0.66715729 <a title="148-lsi-13" href="../hunch_net-2005/hunch_net-2005-02-12-ROC_vs._Accuracy_vs._AROC.html">18 hunch net-2005-02-12-ROC vs. Accuracy vs. AROC</a></p>
<p>14 0.66623676 <a title="148-lsi-14" href="../hunch_net-2005/hunch_net-2005-08-08-Apprenticeship_Reinforcement_Learning_for_Control.html">101 hunch net-2005-08-08-Apprenticeship Reinforcement Learning for Control</a></p>
<p>15 0.65703398 <a title="148-lsi-15" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>16 0.65581769 <a title="148-lsi-16" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>17 0.64995104 <a title="148-lsi-17" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>18 0.64329839 <a title="148-lsi-18" href="../hunch_net-2005/hunch_net-2005-02-25-Problem%3A_Online_Learning.html">28 hunch net-2005-02-25-Problem: Online Learning</a></p>
<p>19 0.64018488 <a title="148-lsi-19" href="../hunch_net-2005/hunch_net-2005-08-04-Why_Reinforcement_Learning_is_Important.html">100 hunch net-2005-08-04-Why Reinforcement Learning is Important</a></p>
<p>20 0.63660014 <a title="148-lsi-20" href="../hunch_net-2009/hunch_net-2009-06-03-Functionally_defined_Nonlinear_Dynamic_Models.html">359 hunch net-2009-06-03-Functionally defined Nonlinear Dynamic Models</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(3, 0.015), (10, 0.03), (25, 0.315), (27, 0.198), (30, 0.019), (38, 0.024), (49, 0.028), (53, 0.056), (55, 0.124), (68, 0.015), (77, 0.013), (94, 0.089)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.88016254 <a title="148-lda-1" href="../hunch_net-2006/hunch_net-2006-05-08-Big_machine_learning.html">178 hunch net-2006-05-08-Big machine learning</a></p>
<p>Introduction: According to the  New York Times ,  Yahoo is releasing Project Panama shortly .  Project Panama is about better predicting which advertisements are relevant to a search, implying a higher click through rate, implying larger income for  Yahoo .  There are two things that seem interesting here:
  
 A significant portion of that improved accuracy is almost certainly machine learning at work. 
 The quantitative effect is huge—the estimate in the article is $600*10 6 . 
  
 Google  already has such improvements and  Microsoft Search  is surely working on them, which suggest this is (perhaps) a $10 9  per year machine learning problem. 
 
The exact methodology under use is unlikely to be publicly discussed in the near future because of the competitive enivironment.  Hopefully we’ll have some public “war stories” at some point in the future when this information becomes less sensitive.  For now, it’s reassuring to simply note that machine learning is having a big impact.</p><p>same-blog 2 0.82894993 <a title="148-lda-2" href="../hunch_net-2006/hunch_net-2006-01-13-Benchmarks_for_RL.html">148 hunch net-2006-01-13-Benchmarks for RL</a></p>
<p>Introduction: A couple years ago, Drew Bagnell and I started the  RLBench project  to setup a suite of reinforcement learning benchmark problems.  We haven’t been able to touch it (due to lack of time) for a year so the project is on hold.  Luckily, there are several other projects such as  CLSquare  and  RL-Glue  with a similar goal, and we strongly endorse their continued development.
 
I would like to explain why, especially in the context of criticism of other learning benchmarks.   For example, sometimes the  UCI Machine Learning Repository  is criticized.  There are two criticisms I know of:
  
 Learning algorithms have overfit to the problems in the repository.  It is easy to imagine a mechanism for this happening unintentionally.  Strong evidence of this would be provided by learning algorithms which perform great on the UCI machine learning repository but very badly (relative to other learning algorithms) on non-UCI learning problems.  I have seen little evidence of this but it remains a po</p><p>3 0.80314118 <a title="148-lda-3" href="../hunch_net-2006/hunch_net-2006-03-12-Online_learning_or_online_preservation_of_learning%3F.html">163 hunch net-2006-03-12-Online learning or online preservation of learning?</a></p>
<p>Introduction: In the online learning with experts setting, you observe a set of predictions, make a decision, and then observe the truth.  This process repeats indefinitely.  In this setting, it is possible to prove theorems of the sort: 
 master algorithm error count < = k* best predictor error count + c*log(number of predictors) 
 
Is this a statement about learning or about preservation of learning?  We did some experiments to analyze the new  Binning algorithm  which works in this setting.  For several UCI datasets, we reprocessed them so that features could be used as predictors and then applied several master algorithms.  The first graph confirms that Binning is indeed a better algorithm according to the tightness of the upper bound.
 
    
Here, “Best” is the performance of the best expert. “V. Bound” is the bound for  Vovk ‘s algorithm (the previous best).  “Bound” is the bound for the Binning algorithm.  “Binning” is the performance of the Binning algorithm.  The Binning algorithm clearly h</p><p>4 0.76369196 <a title="148-lda-4" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>Introduction: The internet has significantly effected the way we do research but it’s capabilities have not yet been fully realized.
 
First, let’s acknowledge some known effects.
  
  Self-publishing  By default, all researchers in machine learning (and more generally computer science and physics) place their papers online for anyone to download.  The exact mechanism differs—physicists tend to use a central repository ( Arxiv ) while computer scientists tend to place the papers on their webpage.  Arxiv has been slowly growing in subject breadth so it now sometimes used by computer scientists. 
  Collaboration  Email has enabled working remotely with coauthors.  This has allowed collaborationis which would not otherwise have been possible and generally speeds research. 
  
Now, let’s look at attempts to go further.
  
  Blogs  (like this one) allow public discussion about topics which are not easily categorized as “a new idea in machine learning” (like this topic). 
  Organization  of some subfield</p><p>5 0.61804891 <a title="148-lda-5" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>Introduction: Unfortunately, I ended up sick for much of this ICML.  I did manage to catch one interesting paper:
 
 Richard Socher ,  Cliff Lin ,  Andrew Y. Ng , and  Christopher D. Manning   Parsing Natural Scenes and Natural Language with Recursive Neural Networks .
 
I invited Richard to share his list of interesting papers, so hopefully we’ll hear from him soon.  In the meantime,  Paul  and  Hal  have posted some lists.
 
 the future 
 
 Joelle  and I are program chairs for ICML 2012 in  Edinburgh , which I previously enjoyed visiting in  2005 .  This is a huge responsibility, that we hope to accomplish well.  A part of this (perhaps the most fun part), is imagining how we can make ICML better.  A key and critical constraint is choosing things that can be accomplished.  So far we have:
  
  Colocation . The first thing we looked into was potential colocations.  We quickly discovered that many other conferences precomitted their location.  For the future, getting a colocation with  ACL  or  SIGI</p><p>6 0.61319578 <a title="148-lda-6" href="../hunch_net-2005/hunch_net-2005-03-13-Avoiding_Bad_Reviewing.html">40 hunch net-2005-03-13-Avoiding Bad Reviewing</a></p>
<p>7 0.61126435 <a title="148-lda-7" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>8 0.61019874 <a title="148-lda-8" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>9 0.60976434 <a title="148-lda-9" href="../hunch_net-2008/hunch_net-2008-10-14-Who_is_Responsible_for_a_Bad_Review%3F.html">320 hunch net-2008-10-14-Who is Responsible for a Bad Review?</a></p>
<p>10 0.60913467 <a title="148-lda-10" href="../hunch_net-2005/hunch_net-2005-07-21-Six_Months.html">96 hunch net-2005-07-21-Six Months</a></p>
<p>11 0.60823429 <a title="148-lda-11" href="../hunch_net-2005/hunch_net-2005-03-21-Research_Styles_in_Machine_Learning.html">44 hunch net-2005-03-21-Research Styles in Machine Learning</a></p>
<p>12 0.60771322 <a title="148-lda-12" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>13 0.60707235 <a title="148-lda-13" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>14 0.60695642 <a title="148-lda-14" href="../hunch_net-2006/hunch_net-2006-09-12-Incentive_Compatible_Reviewing.html">207 hunch net-2006-09-12-Incentive Compatible Reviewing</a></p>
<p>15 0.60675555 <a title="148-lda-15" href="../hunch_net-2009/hunch_net-2009-11-23-ICML_2009_Workshops_%28and_Tutorials%29.html">379 hunch net-2009-11-23-ICML 2009 Workshops (and Tutorials)</a></p>
<p>16 0.60612303 <a title="148-lda-16" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>17 0.60580623 <a title="148-lda-17" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>18 0.60538375 <a title="148-lda-18" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>19 0.60508204 <a title="148-lda-19" href="../hunch_net-2007/hunch_net-2007-01-02-Retrospective.html">225 hunch net-2007-01-02-Retrospective</a></p>
<p>20 0.60392636 <a title="148-lda-20" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
