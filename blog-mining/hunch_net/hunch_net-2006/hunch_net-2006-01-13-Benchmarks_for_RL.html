<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>148 hunch net-2006-01-13-Benchmarks for RL</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-148" href="#">hunch_net-2006-148</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>148 hunch net-2006-01-13-Benchmarks for RL</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-148-html" href="http://hunch.net/?p=159">html</a></p><p>Introduction: A couple years ago, Drew Bagnell and I started theRLBench projectto setup a
suite of reinforcement learning benchmark problems. We haven't been able to
touch it (due to lack of time) for a year so the project is on hold. Luckily,
there are several other projects such asCLSquareandRL-Gluewith a similar goal,
and we strongly endorse their continued development.I would like to explain
why, especially in the context of criticism of other learning benchmarks. For
example, sometimes theUCI Machine Learning Repositoryis criticized. There are
two criticisms I know of:Learning algorithms have overfit to the problems in
the repository. It is easy to imagine a mechanism for this happening
unintentionally. Strong evidence of this would be provided by learning
algorithms which perform great on the UCI machine learning repository but very
badly (relative to other learning algorithms) on non-UCI learning problems. I
have seen little evidence of this but it remains a point of concern. There is
a natur</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 A couple years ago, Drew Bagnell and I started theRLBench projectto setup a suite of reinforcement learning benchmark problems. [sent-1, score-1.097]
</p><p>2 There are two criticisms I know of:Learning algorithms have overfit to the problems in the repository. [sent-6, score-0.452]
</p><p>3 Strong evidence of this would be provided by learning algorithms which perform great on the UCI machine learning repository but very badly (relative to other learning algorithms) on non-UCI learning problems. [sent-8, score-0.856]
</p><p>4 I have seen little evidence of this but it remains a point of concern. [sent-9, score-0.192]
</p><p>5 A well run prediction competition provides some of the best evidence available about which learning algorithms work well. [sent-11, score-0.333]
</p><p>6 There are still some mechanisms for being misled (too many entries, not enough testing data, unlucky choice of learning problem that your algorithm just has the wrong bias for), but they are more controlled than anywhere else. [sent-12, score-0.572]
</p><p>7 The repository enforced a certain interface which many natural learning problems did not fit into. [sent-20, score-0.916]
</p><p>8 Some interface must be enforced in order for a benchmark/repository to be useful. [sent-21, score-0.308]
</p><p>9 This implies that problems fitting the interface are preferred both in submission of new problems and in choice of algorithms to solve. [sent-22, score-0.718]
</p><p>10 This is a reasonable complaint, but it's basically a complaint that the UCI machine learning repository was too succesful. [sent-23, score-0.582]
</p><p>11 It is important to consider the benefits of a benchmark suite as well. [sent-26, score-0.538]
</p><p>12 The standard in reinforcement learning for experimental studies in reinforcement learning algorithms has been showing that some new algorithm does something reasonable on one or two reinforcement learning problems. [sent-27, score-1.471]
</p><p>13 Naturally, what this implies in practice is that the algorithms were hand tuned to work on these one-or-two problems, and there was relatively little emphasis on building a general purpose reinforcement learning algorithm. [sent-28, score-0.81]
</p><p>14 This is strongly at odds with the end goal of the reinforcement learning community! [sent-29, score-0.538]
</p><p>15 The way to avoid this is by setting up a benchmark suite (the more problems, the better). [sent-30, score-0.538]
</p><p>16 With a large set of problems that people can easily test on, the natural emphasis in empirical algorithm development shifts towards general purpose algorithms. [sent-31, score-0.774]
</p><p>17 When people reimplement simulators for reinforcement learning problems, it turns out these reimplementations typically differ in the details, and these details can radically alter the difficulty of the problem. [sent-33, score-0.532]
</p><p>18 Again, a large set of problems people can easily test on solves this problem because the precise definition of the problem is shared. [sent-35, score-0.403]
</p><p>19 One sometimes reasonable view of the world is that if you define a problem publicly, it is as good as solved since there are plenty of smart people out there eager to prove themselves. [sent-36, score-0.35]
</p><p>20 According to this view of the world, setting up a benchmark is a vital task. [sent-37, score-0.384]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('benchmark', 0.297), ('reinforcement', 0.292), ('suite', 0.241), ('repository', 0.241), ('problems', 0.181), ('misled', 0.181), ('interface', 0.159), ('enforced', 0.149), ('complaint', 0.14), ('uci', 0.134), ('algorithms', 0.131), ('emphasis', 0.114), ('evidence', 0.108), ('reasonable', 0.107), ('purpose', 0.095), ('learning', 0.094), ('setup', 0.093), ('natural', 0.092), ('view', 0.087), ('strongly', 0.085), ('little', 0.084), ('theuci', 0.08), ('touch', 0.08), ('benchmarks', 0.08), ('car', 0.08), ('continued', 0.08), ('drew', 0.08), ('eager', 0.08), ('hill', 0.08), ('projectto', 0.08), ('unlucky', 0.08), ('empirical', 0.077), ('problem', 0.076), ('algorithm', 0.075), ('sanity', 0.074), ('competitions', 0.074), ('webserver', 0.074), ('kept', 0.074), ('reimplement', 0.074), ('details', 0.072), ('shifts', 0.07), ('announce', 0.07), ('criticisms', 0.07), ('luckily', 0.07), ('overfit', 0.07), ('reflects', 0.07), ('test', 0.07), ('bagnell', 0.067), ('goal', 0.067), ('choice', 0.066)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="148-tfidf-1" href="../hunch_net-2006/hunch_net-2006-01-13-Benchmarks_for_RL.html">148 hunch net-2006-01-13-Benchmarks for RL</a></p>
<p>Introduction: A couple years ago, Drew Bagnell and I started theRLBench projectto setup a
suite of reinforcement learning benchmark problems. We haven't been able to
touch it (due to lack of time) for a year so the project is on hold. Luckily,
there are several other projects such asCLSquareandRL-Gluewith a similar goal,
and we strongly endorse their continued development.I would like to explain
why, especially in the context of criticism of other learning benchmarks. For
example, sometimes theUCI Machine Learning Repositoryis criticized. There are
two criticisms I know of:Learning algorithms have overfit to the problems in
the repository. It is easy to imagine a mechanism for this happening
unintentionally. Strong evidence of this would be provided by learning
algorithms which perform great on the UCI machine learning repository but very
badly (relative to other learning algorithms) on non-UCI learning problems. I
have seen little evidence of this but it remains a point of concern. There is
a natur</p><p>2 0.20342104 <a title="148-tfidf-2" href="../hunch_net-2005/hunch_net-2005-02-23-Problem%3A_Reinforcement_Learning_with_Classification.html">27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</a></p>
<p>Introduction: At an intuitive level, the question here is "Can reinforcement learning be
solved with classification?"ProblemConstruct a reinforcement learning
algorithm with near-optimal expected sum of rewards in thedirect experience
modelgiven access to a classifier learning algorithm which has a small error
rate or regret on all posed classification problems. The definition of "posed"
here is slightly murky. I consider a problem "posed" if there is an algorithm
for constructing labeled classification examples.Past WorkThere exists
areduction of reinforcement learning to classification given a generative
model.A generative model is an inherently stronger assumption than the direct
experience model.Otherwork on learning reductionsmay be important.Several
algorithms for solving reinforcement learning in the direct experience model
exist. Most, such asE3,Factored-E3, andmetric-E3andRmaxrequire that the
observation be the state. Recent workextends this approach to POMDPs.This
problem is related topred</p><p>3 0.15201436 <a title="148-tfidf-3" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>Introduction: There are several different flavors of Machine Learning classes. Many classes
are of the 'zoo' sort: many different learning algorithms are presented.
Others avoid the zoo by not covering the full scope of machine learning.This
is my view of what makes a good machine learning class, along with why. I'd
like to specifically invite comment on whether things are missing,
misemphasized, or misplaced.PhaseSubjectWhy?IntroductionWhat is a machine
learning problem?A good understanding of the characteristics of machine
learning problems seems essential. Characteristics include: a data source,
some hope the data is predictive, and a need for generalization. This is
probably best taught in a case study manner: lay out the specifics of some
problem and then ask "Is this a machine learning problem?"IntroductionMachine
Learning Problem IdentificationIdentification and recognition of the type of
learning problems is (obviously) a very important step in solving such
problems. People need to be famili</p><p>4 0.14516103 <a title="148-tfidf-4" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>Introduction: I've had serious conversations with several people who believe that the theory
in machine learning is "only useful for getting papers published". That's a
compelling statement, as I've seen many papers where the algorithm clearly
came first, and the theoretical justification for it came second, purely as a
perceived means to improve the chance of publication.Naturally, I disagree and
believe that learning theory has much more substantial applications.Even in
core learning algorithm design, I've found learning theory to be useful,
although it's application is more subtle than many realize. The most
straightforward applications can fail, because (as expectation suggests) worst
case bounds tend to be loose in practice (*). In my experience, considering
learning theory when designing an algorithm has two important effects in
practice:It can help make your algorithm behave right at a crude level of
analysis, leaving finer details to tuning or common sense. The best example I
have of this is</p><p>5 0.13331266 <a title="148-tfidf-5" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>Introduction: One thing which is clear on a little reflection is that there exists a single
master learning problem capable of encoding essentially all learning problems.
This problem is of course a very general sort of reinforcement learning where
the world interacts with an agent as:The world announces an observationx.The
agent makes a choicea.The world announces a rewardr.The goal here is to
maximize the sum of the rewards over the time of the agent. No particular
structure relatingxtoaoratoris implied by this setting so we do not know
effective general algorithms for the agent. It's very easy to prove lower
bounds showing that an agent cannot hope to succeed here--just consider the
case where actions are unrelated to rewards. Nevertheless, there is a real
sense in which essentially all forms of life are agents operating in this
setting, somehow succeeding. The gap between these observations drives
research--How can we find tractable specializations of the master problem
general enough to provide</p><p>6 0.13023825 <a title="148-tfidf-6" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>7 0.12738559 <a title="148-tfidf-7" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>8 0.12722947 <a title="148-tfidf-8" href="../hunch_net-2007/hunch_net-2007-10-19-Second_Annual_Reinforcement_Learning_Competition.html">268 hunch net-2007-10-19-Second Annual Reinforcement Learning Competition</a></p>
<p>9 0.12501974 <a title="148-tfidf-9" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>10 0.12450973 <a title="148-tfidf-10" href="../hunch_net-2005/hunch_net-2005-01-24-Holy_grails_of_machine_learning%3F.html">2 hunch net-2005-01-24-Holy grails of machine learning?</a></p>
<p>11 0.12068072 <a title="148-tfidf-11" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>12 0.1200982 <a title="148-tfidf-12" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>13 0.11824869 <a title="148-tfidf-13" href="../hunch_net-2005/hunch_net-2005-08-04-Why_Reinforcement_Learning_is_Important.html">100 hunch net-2005-08-04-Why Reinforcement Learning is Important</a></p>
<p>14 0.11817694 <a title="148-tfidf-14" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>15 0.11764548 <a title="148-tfidf-15" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>16 0.11714928 <a title="148-tfidf-16" href="../hunch_net-2006/hunch_net-2006-03-02-Why_do_people_count_for_learning%3F.html">160 hunch net-2006-03-02-Why do people count for learning?</a></p>
<p>17 0.11673582 <a title="148-tfidf-17" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>18 0.11027437 <a title="148-tfidf-18" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>19 0.10839086 <a title="148-tfidf-19" href="../hunch_net-2005/hunch_net-2005-02-25-Problem%3A_Online_Learning.html">28 hunch net-2005-02-25-Problem: Online Learning</a></p>
<p>20 0.10738589 <a title="148-tfidf-20" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.276), (1, -0.078), (2, 0.047), (3, -0.042), (4, -0.051), (5, -0.019), (6, -0.038), (7, 0.055), (8, -0.008), (9, 0.003), (10, -0.047), (11, -0.097), (12, 0.085), (13, 0.017), (14, 0.058), (15, 0.041), (16, -0.057), (17, -0.047), (18, -0.0), (19, 0.092), (20, 0.057), (21, -0.026), (22, 0.02), (23, -0.017), (24, -0.056), (25, 0.021), (26, -0.003), (27, 0.049), (28, -0.017), (29, 0.077), (30, -0.102), (31, -0.031), (32, -0.062), (33, -0.008), (34, -0.074), (35, 0.003), (36, 0.014), (37, 0.022), (38, 0.045), (39, -0.124), (40, -0.015), (41, -0.066), (42, -0.013), (43, -0.038), (44, -0.013), (45, -0.026), (46, -0.031), (47, -0.066), (48, 0.047), (49, 0.005)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.93939537 <a title="148-lsi-1" href="../hunch_net-2006/hunch_net-2006-01-13-Benchmarks_for_RL.html">148 hunch net-2006-01-13-Benchmarks for RL</a></p>
<p>Introduction: A couple years ago, Drew Bagnell and I started theRLBench projectto setup a
suite of reinforcement learning benchmark problems. We haven't been able to
touch it (due to lack of time) for a year so the project is on hold. Luckily,
there are several other projects such asCLSquareandRL-Gluewith a similar goal,
and we strongly endorse their continued development.I would like to explain
why, especially in the context of criticism of other learning benchmarks. For
example, sometimes theUCI Machine Learning Repositoryis criticized. There are
two criticisms I know of:Learning algorithms have overfit to the problems in
the repository. It is easy to imagine a mechanism for this happening
unintentionally. Strong evidence of this would be provided by learning
algorithms which perform great on the UCI machine learning repository but very
badly (relative to other learning algorithms) on non-UCI learning problems. I
have seen little evidence of this but it remains a point of concern. There is
a natur</p><p>2 0.79474384 <a title="148-lsi-2" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>Introduction: TheInternational Planning Competition(IPC) is a biennial event organized in
the context of theInternational Conference on Automated Planning and
Scheduling(ICAPS). This year, for the first time, there will a learning track
of the competition. For more information you can go to the competitionweb-
site.The competitions are typically organized around a number of planning
domains that can vary from year to year, where a planning domain is simply a
class of problems that share a common action schema--e.g. Blocksworld is a
well-known planning domain that contains a problem instance each possible
initial tower configuration and goal configuration. Some other domains have
included Logistics, Airport, Freecell, PipesWorld, and manyothers. For each
domain the competition includes a number of problems (say 40-50) and the
planners are run on each problem with a time limit for each problem (around 30
minutes). The problems are hard enough that many problems are not solved
within the time limit.Giv</p><p>3 0.76491362 <a title="148-lsi-3" href="../hunch_net-2005/hunch_net-2005-02-23-Problem%3A_Reinforcement_Learning_with_Classification.html">27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</a></p>
<p>Introduction: At an intuitive level, the question here is "Can reinforcement learning be
solved with classification?"ProblemConstruct a reinforcement learning
algorithm with near-optimal expected sum of rewards in thedirect experience
modelgiven access to a classifier learning algorithm which has a small error
rate or regret on all posed classification problems. The definition of "posed"
here is slightly murky. I consider a problem "posed" if there is an algorithm
for constructing labeled classification examples.Past WorkThere exists
areduction of reinforcement learning to classification given a generative
model.A generative model is an inherently stronger assumption than the direct
experience model.Otherwork on learning reductionsmay be important.Several
algorithms for solving reinforcement learning in the direct experience model
exist. Most, such asE3,Factored-E3, andmetric-E3andRmaxrequire that the
observation be the state. Recent workextends this approach to POMDPs.This
problem is related topred</p><p>4 0.73600101 <a title="148-lsi-4" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>Introduction: I've had serious conversations with several people who believe that the theory
in machine learning is "only useful for getting papers published". That's a
compelling statement, as I've seen many papers where the algorithm clearly
came first, and the theoretical justification for it came second, purely as a
perceived means to improve the chance of publication.Naturally, I disagree and
believe that learning theory has much more substantial applications.Even in
core learning algorithm design, I've found learning theory to be useful,
although it's application is more subtle than many realize. The most
straightforward applications can fail, because (as expectation suggests) worst
case bounds tend to be loose in practice (*). In my experience, considering
learning theory when designing an algorithm has two important effects in
practice:It can help make your algorithm behave right at a crude level of
analysis, leaving finer details to tuning or common sense. The best example I
have of this is</p><p>5 0.73177803 <a title="148-lsi-5" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>Introduction: This post is partly meant as an advertisement for thereductions
tutorialAlina,Bianca, and I are planning to do atICML. Please come, if you are
interested.Many research programs can be thought of as finding and building
new useful abstractions. The running example I'll use islearning
reductionswhere I have experience. The basic abstraction here is that we can
build a learning algorithm capable of solving classification problems up to a
small expected regret. This is used repeatedly to solve more complex
problems.In working on a new abstraction, I think you typically run into many
substantial problems of understanding, which make publishing particularly
difficult.It is difficult to seriously discuss the reason behind or mechanism
for abstraction in a conference paper with small page limits. People rarely
see such discussions and hence have little basis on which to think about new
abstractions. Another difficulty is that when building an abstraction, you
often don't know the right way to</p><p>6 0.71789813 <a title="148-lsi-6" href="../hunch_net-2005/hunch_net-2005-01-24-Holy_grails_of_machine_learning%3F.html">2 hunch net-2005-01-24-Holy grails of machine learning?</a></p>
<p>7 0.7134853 <a title="148-lsi-7" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>8 0.71066707 <a title="148-lsi-8" href="../hunch_net-2005/hunch_net-2005-08-08-Apprenticeship_Reinforcement_Learning_for_Control.html">101 hunch net-2005-08-08-Apprenticeship Reinforcement Learning for Control</a></p>
<p>9 0.70228422 <a title="148-lsi-9" href="../hunch_net-2005/hunch_net-2005-02-25-Problem%3A_Online_Learning.html">28 hunch net-2005-02-25-Problem: Online Learning</a></p>
<p>10 0.69641274 <a title="148-lsi-10" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>11 0.69614798 <a title="148-lsi-11" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>12 0.67787033 <a title="148-lsi-12" href="../hunch_net-2005/hunch_net-2005-02-12-ROC_vs._Accuracy_vs._AROC.html">18 hunch net-2005-02-12-ROC vs. Accuracy vs. AROC</a></p>
<p>13 0.67777896 <a title="148-lsi-13" href="../hunch_net-2005/hunch_net-2005-03-17-Going_all_the_Way%2C_Sometimes.html">42 hunch net-2005-03-17-Going all the Way, Sometimes</a></p>
<p>14 0.67670465 <a title="148-lsi-14" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>15 0.66878164 <a title="148-lsi-15" href="../hunch_net-2005/hunch_net-2005-08-22-Do_you_believe_in_induction%3F.html">104 hunch net-2005-08-22-Do you believe in induction?</a></p>
<p>16 0.66840518 <a title="148-lsi-16" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>17 0.66377598 <a title="148-lsi-17" href="../hunch_net-2005/hunch_net-2005-10-26-Fallback_Analysis_is_a_Secret_to_Useful_Algorithms.html">126 hunch net-2005-10-26-Fallback Analysis is a Secret to Useful Algorithms</a></p>
<p>18 0.65920174 <a title="148-lsi-18" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>19 0.65874952 <a title="148-lsi-19" href="../hunch_net-2005/hunch_net-2005-08-04-Why_Reinforcement_Learning_is_Important.html">100 hunch net-2005-08-04-Why Reinforcement Learning is Important</a></p>
<p>20 0.65264761 <a title="148-lsi-20" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.023), (14, 0.012), (20, 0.1), (35, 0.065), (39, 0.011), (42, 0.289), (50, 0.025), (61, 0.123), (68, 0.04), (69, 0.056), (74, 0.098), (95, 0.062)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.93567961 <a title="148-lda-1" href="../hunch_net-2006/hunch_net-2006-01-13-Benchmarks_for_RL.html">148 hunch net-2006-01-13-Benchmarks for RL</a></p>
<p>Introduction: A couple years ago, Drew Bagnell and I started theRLBench projectto setup a
suite of reinforcement learning benchmark problems. We haven't been able to
touch it (due to lack of time) for a year so the project is on hold. Luckily,
there are several other projects such asCLSquareandRL-Gluewith a similar goal,
and we strongly endorse their continued development.I would like to explain
why, especially in the context of criticism of other learning benchmarks. For
example, sometimes theUCI Machine Learning Repositoryis criticized. There are
two criticisms I know of:Learning algorithms have overfit to the problems in
the repository. It is easy to imagine a mechanism for this happening
unintentionally. Strong evidence of this would be provided by learning
algorithms which perform great on the UCI machine learning repository but very
badly (relative to other learning algorithms) on non-UCI learning problems. I
have seen little evidence of this but it remains a point of concern. There is
a natur</p><p>2 0.92068255 <a title="148-lda-2" href="../hunch_net-2005/hunch_net-2005-11-02-Progress_in_Active_Learning.html">127 hunch net-2005-11-02-Progress in Active Learning</a></p>
<p>Introduction: Several bits of progress have been made sinceSanjoypointed out the
significantlack of theoretical understanding of active learning. This is an
update on the progress I know of. As a refresher, active learning as meant
here is:There is a source of unlabeled data.There is an oracle from which
labels can be requested for unlabeled data produced by the source.The goal is
to perform well with minimal use of the oracle.Here is what I've
learned:Sanjoy has developed sufficient and semi-necessary conditions for
active learning given the assumptions of IID data and "realizability" (that
one of the classifiers is a correct classifier).Nina,Alina, and I developed an
algorithm for active learning relying on only the assumption of IID data. A
draft ishere.Nicolo,Claudio, andLucashowed that it is possible to do active
learning in an entirely adversarial setting for linear threshold
classifiershere. This was published a year or two ago and I recently learned
about it.All of these results are relative</p><p>3 0.92024243 <a title="148-lda-3" href="../hunch_net-2012/hunch_net-2012-03-06-COLT-ICML_Open_Questions_and_ICML_Instructions.html">458 hunch net-2012-03-06-COLT-ICML Open Questions and ICML Instructions</a></p>
<p>Introduction: Sashais theopen problemschair for bothCOLTandICML. Open problems will be
presented in a joint session in the evening of the COLT/ICML overlap day. COLT
has a history of open sessions, but this is new for ICML. If you have a
difficult theoretically definable problem in machine learning, consider
submitting it for review,due March 16. You'll benefit three ways:The effort of
writing down a precise formulation of what you want often helps you understand
the nature of the problem.Your problem will be officially published and
citable.You might have it solved by some very intelligent bored people.The
general idea could easily be applied to any problem which can be crisply
stated with an easily verifiable solution, and we may consider expanding this
in later years, but for this year all problems need to be of a theoretical
variety.Joelleand I (andMahdi, andLaurent) finished an initial assignment
ofProgram CommitteeandArea Chairsto papers. We'll be updatinginstructions for
the PCand ACsas we fi</p><p>4 0.91207463 <a title="148-lda-4" href="../hunch_net-2005/hunch_net-2005-06-29-Not_EM_for_clustering_at_COLT.html">87 hunch net-2005-06-29-Not EM for clustering at COLT</a></p>
<p>Introduction: One standard approach for clustering data with a set of gaussians is using EM.
Roughly speaking, you pick a set ofkrandom guassians and then use alternating
expectation maximization to (hopefully) find a set of guassians that "explain"
the data well. This process is difficult to work with because EM can become
"stuck" in local optima. There are various hacks like "rerun withtdifferent
random starting points".One cool observation is that this can often be solved
via other algorithm which donotsuffer from local optima. This is an
earlypaperwhich shows this. Ravi Kannan presented anew papershowing this is
possible in a much more adaptive setting.A very rough summary of these papers
is that by projecting into a lower dimensional space, it is computationally
tractable to pick out the gross structure of the data. It is unclear how well
these algorithms work in practice, but they might be effective, especially if
used as a subroutine of the form:Project to low dimensional space.Pick out
gross</p><p>5 0.90032554 <a title="148-lda-5" href="../hunch_net-2005/hunch_net-2005-04-06-Structured_Regret_Minimization.html">53 hunch net-2005-04-06-Structured Regret Minimization</a></p>
<p>Introduction: Geoff Gordon made an interesting presentation at thesnowbird learning
workshopdiscussing the use of no-regret algorithms for the use of several
robot-related learning problems. There seems to be a drafthere. This seems
interesting in two ways:Drawback RemovalOne of the significant problems with
these online algorithms is that they can't cope with structure very easily.
This drawback is addressed for certain structures.ExperimentsOne criticism of
such algorithms is that they are too "worst case". Several experiments suggest
that protecting yourself against this worst case does not necessarily incur a
great loss.</p><p>6 0.87634039 <a title="148-lda-6" href="../hunch_net-2010/hunch_net-2010-03-15-The_Efficient_Robust_Conditional_Probability_Estimation_Problem.html">391 hunch net-2010-03-15-The Efficient Robust Conditional Probability Estimation Problem</a></p>
<p>7 0.86879241 <a title="148-lda-7" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>8 0.86864477 <a title="148-lda-8" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>9 0.86857253 <a title="148-lda-9" href="../hunch_net-2013/hunch_net-2013-12-01-NIPS_tutorials_and_Vowpal_Wabbit_7.4.html">492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</a></p>
<p>10 0.8679086 <a title="148-lda-10" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>11 0.86764795 <a title="148-lda-11" href="../hunch_net-2009/hunch_net-2009-09-21-Netflix_finishes_%28and_starts%29.html">371 hunch net-2009-09-21-Netflix finishes (and starts)</a></p>
<p>12 0.86723328 <a title="148-lda-12" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>13 0.86708856 <a title="148-lda-13" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>14 0.86577207 <a title="148-lda-14" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>15 0.86419648 <a title="148-lda-15" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>16 0.86408198 <a title="148-lda-16" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>17 0.86337572 <a title="148-lda-17" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>18 0.86232364 <a title="148-lda-18" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">23 hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<p>19 0.86223453 <a title="148-lda-19" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>20 0.86199117 <a title="148-lda-20" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
