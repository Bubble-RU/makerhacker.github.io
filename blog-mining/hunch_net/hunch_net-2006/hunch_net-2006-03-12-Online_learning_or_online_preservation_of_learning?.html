<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>163 hunch net-2006-03-12-Online learning or online preservation of learning?</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-163" href="#">hunch_net-2006-163</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>163 hunch net-2006-03-12-Online learning or online preservation of learning?</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-163-html" href="http://hunch.net/?p=174">html</a></p><p>Introduction: In the online learning with experts setting, you observe a set of predictions,
make a decision, and then observe the truth. This process repeats
indefinitely. In this setting, it is possible to prove theorems of the
sort:master algorithm error count < = k* best predictor error count +
c*log(number of predictors)Is this a statement about learning or about
preservation of learning? We did some experiments to analyze the newBinning
algorithmwhich works in this setting. For several UCI datasets, we reprocessed
them so that features could be used as predictors and then applied several
master algorithms. The first graph confirms that Binning is indeed a better
algorithm according to the tightness of the upper bound.Here, "Best" is the
performance of the best expert. "V. Bound" is the bound forVovk's algorithm
(the previous best). "Bound" is the bound for the Binning algorithm. "Binning"
is the performance of the Binning algorithm. The Binning algorithm clearly has
a tighter bound, and the pe</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In the online learning with experts setting, you observe a set of predictions, make a decision, and then observe the truth. [sent-1, score-0.401]
</p><p>2 In this setting, it is possible to prove theorems of the sort:master algorithm error count < = k* best predictor error count + c*log(number of predictors)Is this a statement about learning or about preservation of learning? [sent-3, score-0.818]
</p><p>3 For several UCI datasets, we reprocessed them so that features could be used as predictors and then applied several master algorithms. [sent-5, score-0.196]
</p><p>4 The first graph confirms that Binning is indeed a better algorithm according to the tightness of the upper bound. [sent-6, score-0.508]
</p><p>5 Bound" is the bound forVovk's algorithm (the previous best). [sent-9, score-0.463]
</p><p>6 The Binning algorithm clearly has a tighter bound, and the performance bound is clearly a sharp constraint on the algorithm performance. [sent-12, score-0.935]
</p><p>7 "Bin" is the performance of Binning (identical to the previous graph). [sent-14, score-0.261]
</p><p>8 BW isBinomial weighting, which is (roughly) the deterministic version of Binning. [sent-15, score-0.168]
</p><p>9 Both BW and WM are deterministic algorithms which implies their performance bounds are perhaps a factor of 2 worse than Binning or Vovk's algorithm. [sent-17, score-0.43]
</p><p>10 In contrast, the actual performance (rather than performance bound) of the deterministic algorithms is sometimes even better than the best expert (negative regret? [sent-18, score-0.734]
</p><p>11 A consistent negative correlation between "online bound tightness" and "learning performance" is observed. [sent-21, score-0.402]
</p><p>12 "One reply is that we are testing in the wrong setting. [sent-23, score-0.113]
</p><p>13 This isn't a convincing answer to me because many (or perhaps most) situations are not that adversarial. [sent-25, score-0.091]
</p><p>14 This is not convincing because many other learning algorithm do as well or better with the given features/experts. [sent-27, score-0.273]
</p><p>15 Another possibility is "you can start out running Binning, and when it pulls ahead of it's bound run any learning algorithm. [sent-28, score-0.331]
</p><p>16 If the learning algorithm does badly, you can switch back to Binning and preserve the guarantee. [sent-29, score-0.291]
</p><p>17 My best current understanding is that "online learning with experts" is really "online preservation of learning": the goal of the algorithm is to preserve whatever predictive ability the individual predictors have. [sent-31, score-0.733]
</p><p>18 This understanding fits the form of the theory statement well. [sent-32, score-0.119]
</p><p>19 For example, charity events sometimes work according to the following form:All participants exchange dollars for bogobucks. [sent-34, score-0.346]
</p><p>20 An online preservation algorithm has the property that if you acquire enough bogobucks in comparison to the number of participants, you can guarantee winning the prize. [sent-37, score-0.563]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('binning', 0.641), ('bound', 0.272), ('preservation', 0.217), ('performance', 0.204), ('deterministic', 0.168), ('bw', 0.144), ('wm', 0.144), ('algorithm', 0.134), ('experts', 0.114), ('tightness', 0.112), ('participants', 0.112), ('best', 0.11), ('predictors', 0.109), ('uci', 0.107), ('preserve', 0.107), ('online', 0.105), ('count', 0.093), ('observe', 0.091), ('convincing', 0.091), ('graph', 0.087), ('master', 0.087), ('negative', 0.074), ('clearly', 0.07), ('statement', 0.069), ('confirms', 0.064), ('examining', 0.064), ('scenarios', 0.064), ('according', 0.063), ('gamble', 0.059), ('ahead', 0.059), ('bin', 0.059), ('charity', 0.059), ('dollars', 0.059), ('safety', 0.059), ('bounds', 0.058), ('wrong', 0.057), ('previous', 0.057), ('correlation', 0.056), ('reply', 0.056), ('whatever', 0.056), ('winning', 0.056), ('exchange', 0.053), ('repeats', 0.053), ('acquire', 0.051), ('sharp', 0.051), ('error', 0.051), ('switch', 0.05), ('fits', 0.05), ('setting', 0.049), ('better', 0.048)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="163-tfidf-1" href="../hunch_net-2006/hunch_net-2006-03-12-Online_learning_or_online_preservation_of_learning%3F.html">163 hunch net-2006-03-12-Online learning or online preservation of learning?</a></p>
<p>Introduction: In the online learning with experts setting, you observe a set of predictions,
make a decision, and then observe the truth. This process repeats
indefinitely. In this setting, it is possible to prove theorems of the
sort:master algorithm error count < = k* best predictor error count +
c*log(number of predictors)Is this a statement about learning or about
preservation of learning? We did some experiments to analyze the newBinning
algorithmwhich works in this setting. For several UCI datasets, we reprocessed
them so that features could be used as predictors and then applied several
master algorithms. The first graph confirms that Binning is indeed a better
algorithm according to the tightness of the upper bound.Here, "Best" is the
performance of the best expert. "V. Bound" is the bound forVovk's algorithm
(the previous best). "Bound" is the bound for the Binning algorithm. "Binning"
is the performance of the Binning algorithm. The Binning algorithm clearly has
a tighter bound, and the pe</p><p>2 0.22129825 <a title="163-tfidf-2" href="../hunch_net-2006/hunch_net-2006-11-20-Context_and_the_calculation_misperception.html">218 hunch net-2006-11-20-Context and the calculation misperception</a></p>
<p>Introduction: This post is really for peoplenotin machine learning (or related fields). It
is about a common misperception which affects people who have not thought
about the process of trying to predict somethinng. Hopefully, by precisely
stating it, we can remove it.Suppose we have a set of events, each described
by a vector of features.01011101011101000111110011000101110Suppose we want to
predict the value of the first feature given the others. One approach is to
bin the data byonefeature. For the above example, we might partition the data
according to feature 2, then observe that when feature 2 is 0 the label
(feature 1) is mostly 1. On the other hand, when feature 2 is 1, the label
(feature 1) is mostly 0. Using this simple rule we get an observed error rate
of 3/7.There are two issues here. The first is that this is really a training
error rate, and (hence) may be an overoptimistic prediction. This is not a
very serious issue as long as there are a reasonable number of representative
examples.</p><p>3 0.15091822 <a title="163-tfidf-3" href="../hunch_net-2005/hunch_net-2005-09-08-Online_Learning_as_the_Mathematics_of_Accountability.html">109 hunch net-2005-09-08-Online Learning as the Mathematics of Accountability</a></p>
<p>Introduction: Accountability is a social problem. When someone screws up, do you fire them?
Or do you accept the error and let them continue? This is a very difficult
problem and we all know of stories where the wrong decision was made.Online
learning(as meant here), is a subfield of learning theory which analyzes the
online learning model.In the online learning model, there are a set of
hypotheses or "experts". On any instantancex, each expert makes a predictiony.
A master algorithmAuses these predictions to form it's own predictionyAand
then learns the correct predictiony*. This process repeats.The goal of online
learning is to find a master algorithmAwhich uses the advice of the experts to
make good predictions. In particular, we typically want to guarantee that the
master algorithm performs almost as well as the best expert. IfL(e)is the loss
of experteandL(A)is the loss of the master algorithm, it is often possible to
prove:L(A) less than mineL(e) + log(number of experts)over all sequences.In
p</p><p>4 0.13656732 <a title="163-tfidf-4" href="../hunch_net-2007/hunch_net-2007-05-09-The_Missing_Bound.html">244 hunch net-2007-05-09-The Missing Bound</a></p>
<p>Introduction: Sham Kakadepoints out that we are missing a bound.Suppose we
havemsamplesxdrawn IID from some distributionD. Through the magic of
exponential moment method we know that:If the range ofxis bounded by an
interval of sizeI, aChernoff/Hoeffding style boundgives us a bound on the
deviations likeO(I/m0.5)(at least in crude form). A proof is on page 9here.If
the range ofxis bounded, and the variance (or a bound on the variance) is
known, thenBennett's boundcan give tighter results (*). This can be a huge
improvment when the true variance small.What's missing here is a bound that
depends on the observed variance rather than a bound on the variance. This
means that many people attempt to use Bennett's bound (incorrectly) by
plugging the observed variance in as the true variance, invalidating the bound
application. Most of the time, they get away with it, but this is a dangerous
move when doing machine learning. In machine learning, we are typically trying
to find a predictor with 0 expected los</p><p>5 0.13619979 <a title="163-tfidf-5" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>Introduction: "Overfitting" is traditionally defined as training some flexible
representation so that it memorizes the data but fails to predict well in the
future. For this post, I will define overfitting more generally as over-
representing the performance of systems. There are two styles of general
overfitting: overrepresenting performance on particular datasets and
(implicitly) overrepresenting performance of a method on future datasets.We
should all be aware of these methods, avoid them where possible, and take them
into account otherwise. I have used "reproblem" and "old datasets", and may
have participated in "overfitting by review"--some of these are very difficult
to avoid.NameMethodExplanationRemedyTraditional overfittingTrain a complex
predictor on too-few examples.Hold out pristine examples for testing.Use a
simpler predictor.Get more training examples.Integrate over many
predictors.Reject papers which do this.Parameter tweak overfittingUse a
learning algorithm with many parameters. Choo</p><p>6 0.12778732 <a title="163-tfidf-6" href="../hunch_net-2006/hunch_net-2006-04-06-Bounds_greater_than_1.html">170 hunch net-2006-04-06-Bounds greater than 1</a></p>
<p>7 0.12488861 <a title="163-tfidf-7" href="../hunch_net-2005/hunch_net-2005-03-15-The_State_of_Tight_Bounds.html">41 hunch net-2005-03-15-The State of Tight Bounds</a></p>
<p>8 0.12266549 <a title="163-tfidf-8" href="../hunch_net-2005/hunch_net-2005-09-26-Prediction_Bounds_as_the_Mathematics_of_Science.html">115 hunch net-2005-09-26-Prediction Bounds as the Mathematics of Science</a></p>
<p>9 0.11337063 <a title="163-tfidf-9" href="../hunch_net-2005/hunch_net-2005-06-28-A_COLT_paper.html">85 hunch net-2005-06-28-A COLT paper</a></p>
<p>10 0.11074094 <a title="163-tfidf-10" href="../hunch_net-2006/hunch_net-2006-11-22-Explicit_Randomization_in_Learning_algorithms.html">219 hunch net-2006-11-22-Explicit Randomization in Learning algorithms</a></p>
<p>11 0.10975177 <a title="163-tfidf-11" href="../hunch_net-2005/hunch_net-2005-02-01-NIPS%3A_Online_Bayes.html">8 hunch net-2005-02-01-NIPS: Online Bayes</a></p>
<p>12 0.10966364 <a title="163-tfidf-12" href="../hunch_net-2005/hunch_net-2005-11-16-The_Everything_Ensemble_Edge.html">131 hunch net-2005-11-16-The Everything Ensemble Edge</a></p>
<p>13 0.10923685 <a title="163-tfidf-13" href="../hunch_net-2009/hunch_net-2009-08-26-Another_10-year_paper_in_Machine_Learning.html">368 hunch net-2009-08-26-Another 10-year paper in Machine Learning</a></p>
<p>14 0.10911998 <a title="163-tfidf-14" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>15 0.10758308 <a title="163-tfidf-15" href="../hunch_net-2005/hunch_net-2005-06-18-Lower_Bounds_for_Learning_Reductions.html">83 hunch net-2005-06-18-Lower Bounds for Learning Reductions</a></p>
<p>16 0.10526921 <a title="163-tfidf-16" href="../hunch_net-2007/hunch_net-2007-08-12-Exponentiated_Gradient.html">258 hunch net-2007-08-12-Exponentiated Gradient</a></p>
<p>17 0.099415116 <a title="163-tfidf-17" href="../hunch_net-2005/hunch_net-2005-05-16-Regret_minimizing_vs_error_limiting_reductions.html">72 hunch net-2005-05-16-Regret minimizing vs error limiting reductions</a></p>
<p>18 0.098502219 <a title="163-tfidf-18" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<p>19 0.095928133 <a title="163-tfidf-19" href="../hunch_net-2007/hunch_net-2007-06-14-Interesting_Papers_at_COLT_2007.html">247 hunch net-2007-06-14-Interesting Papers at COLT 2007</a></p>
<p>20 0.09330377 <a title="163-tfidf-20" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.179), (1, -0.13), (2, -0.063), (3, 0.02), (4, -0.063), (5, -0.074), (6, 0.115), (7, 0.009), (8, 0.004), (9, -0.064), (10, -0.08), (11, -0.073), (12, -0.154), (13, -0.045), (14, -0.07), (15, 0.044), (16, -0.076), (17, 0.035), (18, 0.08), (19, 0.031), (20, 0.012), (21, 0.004), (22, 0.003), (23, -0.011), (24, -0.084), (25, -0.12), (26, -0.013), (27, -0.08), (28, 0.023), (29, -0.014), (30, 0.001), (31, 0.032), (32, 0.051), (33, -0.079), (34, 0.016), (35, -0.065), (36, -0.077), (37, -0.047), (38, -0.061), (39, -0.05), (40, -0.058), (41, -0.013), (42, -0.006), (43, -0.041), (44, -0.019), (45, -0.066), (46, 0.001), (47, 0.056), (48, 0.057), (49, 0.115)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94899052 <a title="163-lsi-1" href="../hunch_net-2006/hunch_net-2006-03-12-Online_learning_or_online_preservation_of_learning%3F.html">163 hunch net-2006-03-12-Online learning or online preservation of learning?</a></p>
<p>Introduction: In the online learning with experts setting, you observe a set of predictions,
make a decision, and then observe the truth. This process repeats
indefinitely. In this setting, it is possible to prove theorems of the
sort:master algorithm error count < = k* best predictor error count +
c*log(number of predictors)Is this a statement about learning or about
preservation of learning? We did some experiments to analyze the newBinning
algorithmwhich works in this setting. For several UCI datasets, we reprocessed
them so that features could be used as predictors and then applied several
master algorithms. The first graph confirms that Binning is indeed a better
algorithm according to the tightness of the upper bound.Here, "Best" is the
performance of the best expert. "V. Bound" is the bound forVovk's algorithm
(the previous best). "Bound" is the bound for the Binning algorithm. "Binning"
is the performance of the Binning algorithm. The Binning algorithm clearly has
a tighter bound, and the pe</p><p>2 0.73183686 <a title="163-lsi-2" href="../hunch_net-2005/hunch_net-2005-06-28-A_COLT_paper.html">85 hunch net-2005-06-28-A COLT paper</a></p>
<p>Introduction: I found Tong Zhang's paper onData Dependent Concentration Bounds for
Sequential Prediction Algorithmsinteresting. Roughly speaking, it states a
tight bound on the future error rate for online learning algorithms assuming
that samples are drawn independently. This bound is easily computed and will
make the progressive validation approaches usedheresignificantly more
practical.</p><p>3 0.71523368 <a title="163-lsi-3" href="../hunch_net-2006/hunch_net-2006-04-06-Bounds_greater_than_1.html">170 hunch net-2006-04-06-Bounds greater than 1</a></p>
<p>Introduction: Nati SrebroandShai Ben-Davidhave apaperatCOLTwhich, in the appendix, proves
something very striking: several previous error bounds arealwaysgreater than
1.BackgroundOne branch of learning theory focuses on theorems whichAssume
samples are drawn IID from an unknown distributionD.Fix a set of
classifiersFind a high probability bound on the maximum true error rate (with
respect toD) as a function of the empirical error rate on the training
set.Many of these bounds become extremely complex and hairy.CurrentEveryone
working on this subject wants "tighter bounds", however there are different
definitions of "tighter". Some groups focus on "functional tightness" (getting
the right functional dependency between the size of the training set and a
parameterization of the hypothesis space) whileothersfocus on "practical
tightness" (finding bounds which work well on practical problems). (I am
definitely in the second camp.)One of the dangers of striving for "functional
tightness" is that the bound</p><p>4 0.64758718 <a title="163-lsi-4" href="../hunch_net-2005/hunch_net-2005-06-06-Exact_Online_Learning_for_Classification.html">78 hunch net-2005-06-06-Exact Online Learning for Classification</a></p>
<p>Introduction: Jacob Abernethy and I have found a computationally tractable method for
computing an optimal (or near optimal depending on setting) master algorithm
combining expert predictions addressingthis open problem. A draft ishere.The
effect of this improvement seems to be about a factor of2decrease in the
regret (= error rate minus best possible error rate) for the low error rate
situation. (At large error rates, there may be no significant
difference.)There are some unfinished details still to consider:When we remove
all of the approximation slack from online learning, is the result a
satisfying learning algorithm, in practice? I consider online learning is one
of the more compelling methods of analyzing and deriving algorithms, but that
expectation must be either met or not by this algorithmSome extra details: The
algorithm is optimal given a small amount of side information (kin the draft).
What is the best way to remove this side information? The removal is necessary
for a practical algori</p><p>5 0.59353834 <a title="163-lsi-5" href="../hunch_net-2005/hunch_net-2005-03-15-The_State_of_Tight_Bounds.html">41 hunch net-2005-03-15-The State of Tight Bounds</a></p>
<p>Introduction: What?Bounds are mathematical formulas relating observations to future error
rates assuming that data is drawn independently. In classical statistics, they
are calld confidence intervals.Why?Good Judgement. In many applications of
learning, it is desirable to know how well the learned predictor works in the
future. This helps you decide if the problem is solved or not.Learning
Essence. The form of some of these bounds helps you understand what the
essence of learning is.Algorithm Design. Some of these bounds suggest,
motivate, or even directly imply learning algorithms.What We Know NowThere are
several families of bounds, based on how information is used.Testing Bounds.
These are methods which use labeled data not used in training to estimate the
future error rate. Examples include thetest set bound,progressive
validationalsohereandhere,train and test bounds, and cross-validation (but see
thebig open problem). These techniques are the best available for goal (1)
above, but provide littl</p><p>6 0.58291 <a title="163-lsi-6" href="../hunch_net-2005/hunch_net-2005-09-26-Prediction_Bounds_as_the_Mathematics_of_Science.html">115 hunch net-2005-09-26-Prediction Bounds as the Mathematics of Science</a></p>
<p>7 0.58119285 <a title="163-lsi-7" href="../hunch_net-2007/hunch_net-2007-05-09-The_Missing_Bound.html">244 hunch net-2007-05-09-The Missing Bound</a></p>
<p>8 0.57251042 <a title="163-lsi-8" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<p>9 0.56076574 <a title="163-lsi-9" href="../hunch_net-2005/hunch_net-2005-09-08-Online_Learning_as_the_Mathematics_of_Accountability.html">109 hunch net-2005-09-08-Online Learning as the Mathematics of Accountability</a></p>
<p>10 0.55134761 <a title="163-lsi-10" href="../hunch_net-2005/hunch_net-2005-11-28-A_question_of_quantification.html">133 hunch net-2005-11-28-A question of quantification</a></p>
<p>11 0.54459608 <a title="163-lsi-11" href="../hunch_net-2005/hunch_net-2005-02-28-Regularization.html">33 hunch net-2005-02-28-Regularization</a></p>
<p>12 0.53751957 <a title="163-lsi-12" href="../hunch_net-2005/hunch_net-2005-03-04-The_Big_O_and_Constants_in_Learning.html">35 hunch net-2005-03-04-The Big O and Constants in Learning</a></p>
<p>13 0.51595104 <a title="163-lsi-13" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>14 0.51496667 <a title="163-lsi-14" href="../hunch_net-2009/hunch_net-2009-08-26-Another_10-year_paper_in_Machine_Learning.html">368 hunch net-2009-08-26-Another 10-year paper in Machine Learning</a></p>
<p>15 0.51024979 <a title="163-lsi-15" href="../hunch_net-2005/hunch_net-2005-11-16-The_Everything_Ensemble_Edge.html">131 hunch net-2005-11-16-The Everything Ensemble Edge</a></p>
<p>16 0.50705224 <a title="163-lsi-16" href="../hunch_net-2007/hunch_net-2007-06-14-Interesting_Papers_at_COLT_2007.html">247 hunch net-2007-06-14-Interesting Papers at COLT 2007</a></p>
<p>17 0.50691617 <a title="163-lsi-17" href="../hunch_net-2005/hunch_net-2005-03-18-Binomial_Weighting.html">43 hunch net-2005-03-18-Binomial Weighting</a></p>
<p>18 0.50120527 <a title="163-lsi-18" href="../hunch_net-2005/hunch_net-2005-02-25-Problem%3A_Online_Learning.html">28 hunch net-2005-02-25-Problem: Online Learning</a></p>
<p>19 0.49264553 <a title="163-lsi-19" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>20 0.48342147 <a title="163-lsi-20" href="../hunch_net-2005/hunch_net-2005-06-18-Lower_Bounds_for_Learning_Reductions.html">83 hunch net-2005-06-18-Lower Bounds for Learning Reductions</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(4, 0.25), (7, 0.034), (35, 0.069), (42, 0.242), (45, 0.042), (68, 0.073), (69, 0.02), (74, 0.068), (76, 0.041), (88, 0.025), (93, 0.036)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.92838937 <a title="163-lda-1" href="../hunch_net-2008/hunch_net-2008-04-26-Eliminating_the_Birthday_Paradox_for_Universal_Features.html">298 hunch net-2008-04-26-Eliminating the Birthday Paradox for Universal Features</a></p>
<p>Introduction: I want to expand onthis postwhich describes one of the core tricks for
makingVowpal Wabbitfast and easy to use when learning from text.The central
trick is converting a word (or any other parseable quantity) into a number via
a hash function.Kishoretells me this is a relatively old trick in NLP land,
but it has some added advantages when doing online learning, because you can
learn directly from the existing data without preprocessing the data to create
features (destroying the online property) or using an expensive hashtable
lookup (slowing things down).A central concern for this approach is
collisions, which create a loss of information. If you usemfeatures in an
index space of sizenthe birthday paradox suggests a collision ifm > n0.5,
essentially because there arem2pairs. This is pretty bad, because it says that
with a vocabulary of105features, you might need to have1010entries in your
table.It turns out that redundancy is great for dealing with
collisions.Alexand I worked out a cou</p><p>same-blog 2 0.88590103 <a title="163-lda-2" href="../hunch_net-2006/hunch_net-2006-03-12-Online_learning_or_online_preservation_of_learning%3F.html">163 hunch net-2006-03-12-Online learning or online preservation of learning?</a></p>
<p>Introduction: In the online learning with experts setting, you observe a set of predictions,
make a decision, and then observe the truth. This process repeats
indefinitely. In this setting, it is possible to prove theorems of the
sort:master algorithm error count < = k* best predictor error count +
c*log(number of predictors)Is this a statement about learning or about
preservation of learning? We did some experiments to analyze the newBinning
algorithmwhich works in this setting. For several UCI datasets, we reprocessed
them so that features could be used as predictors and then applied several
master algorithms. The first graph confirms that Binning is indeed a better
algorithm according to the tightness of the upper bound.Here, "Best" is the
performance of the best expert. "V. Bound" is the bound forVovk's algorithm
(the previous best). "Bound" is the bound for the Binning algorithm. "Binning"
is the performance of the Binning algorithm. The Binning algorithm clearly has
a tighter bound, and the pe</p><p>3 0.85842437 <a title="163-lda-3" href="../hunch_net-2011/hunch_net-2011-08-06-Interesting_thing_at_UAI_2011.html">440 hunch net-2011-08-06-Interesting thing at UAI 2011</a></p>
<p>Introduction: I had a chance to attendUAIthis year, where several papers interested me,
including:Hoifung PoonandPedro DomingosSum-Product Networks: A New Deep
Architecture. We'vealready discussed this one, but in a nutshell, they
identify a large class of efficiently normalizable distributions and do
learning with it.Yao-Liang YuandDale Schuurmans,Rank/norm regularization with
closed-form solutions: Application to subspace clustering. This paper is about
matrices, and in particular they prove that certain matrices are the solution
of matrix optimizations. I'm not matrix inclined enough to fully appreciate
this one, but I believe many others may be, and anytime closed form solutions
come into play, you get 2 order of magnitude speedups, as they show
experimentally.Laurent Charlin,Richard ZemelandCraig Boutilier,A Framework for
Optimizing Paper Matching. This is about what works in matching papers to
reviewers, as has been tested at several previous NIPS. We are looking into
using this system for ICM</p><p>4 0.8300488 <a title="163-lda-4" href="../hunch_net-2007/hunch_net-2007-02-16-The_Forgetting.html">233 hunch net-2007-02-16-The Forgetting</a></p>
<p>Introduction: How many papers do you remember from 2006? 2005? 2002? 1997? 1987? 1967? One
way to judge this would be to look at the citations of the papers you write--
how many came from which year? For myself, the answers on recent papers
are:year200620052002199719871967count4105100This spectrum is fairly typical of
papers in general. There are many reasons that citations are focused on recent
papers.The number of papers being published continues to grow. This is not a
very significant effect, because the rate of publication has not grown nearly
as fast.Dead men don't reject your papers for not citing them. This reason
seems lame, because it's a distortion from the ideal of science. Nevertheless,
it must be stated because the effect can be significant.In 1997, I started as
a PhD student. Naturally, papers after 1997 are better remembered because they
were absorbed in real time. A large fraction of people writing papers and
attending conferences haven't been doing it for 10 years.Old papers aren't</p><p>5 0.72799271 <a title="163-lda-5" href="../hunch_net-2006/hunch_net-2006-03-02-Why_do_people_count_for_learning%3F.html">160 hunch net-2006-03-02-Why do people count for learning?</a></p>
<p>Introduction: This post is about a confusion of mine with respect to many commonly used
machine learning algorithms.A simple example where this comes up is Bayes net
prediction. A Bayes net where a directed acyclic graph over a set of nodes
where each node is associated with a variable and the edges indicate
dependence. The joint probability distribution over the variables is given by
a set of conditional probabilities. For example, a very simple Bayes net might
express:P(A,B,C) = P(A | B,C)P(B)P(C)What I don't understand is the mechanism
commonly used to estimateP(A | B, C). If we letN(A,B,C)be the number of
instances ofA,B,Cthen people sometimes form an estimate according to:P'(A |
B,C) = N(A,B,C) / N /[N(B)/N * N(C)/N] = N(A,B,C) N /[N(B) N(C)]â&euro;Ś in other
words, people just estimateP'(A | B,C)according to observed relative
frequencies. This is a reasonable technique when you have a large number of
samples compared to the size spaceA x B x C, but it (naturally) falls apart
when this is not the case</p><p>6 0.71503347 <a title="163-lda-6" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>7 0.71426278 <a title="163-lda-7" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>8 0.71386701 <a title="163-lda-8" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<p>9 0.7127707 <a title="163-lda-9" href="../hunch_net-2007/hunch_net-2007-08-12-Exponentiated_Gradient.html">258 hunch net-2007-08-12-Exponentiated Gradient</a></p>
<p>10 0.71103621 <a title="163-lda-10" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>11 0.71064687 <a title="163-lda-11" href="../hunch_net-2010/hunch_net-2010-03-15-The_Efficient_Robust_Conditional_Probability_Estimation_Problem.html">391 hunch net-2010-03-15-The Efficient Robust Conditional Probability Estimation Problem</a></p>
<p>12 0.71056443 <a title="163-lda-12" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>13 0.7101897 <a title="163-lda-13" href="../hunch_net-2007/hunch_net-2007-08-19-Choice_of_Metrics.html">259 hunch net-2007-08-19-Choice of Metrics</a></p>
<p>14 0.70959127 <a title="163-lda-14" href="../hunch_net-2007/hunch_net-2007-01-10-A_Deep_Belief_Net_Learning_Problem.html">227 hunch net-2007-01-10-A Deep Belief Net Learning Problem</a></p>
<p>15 0.70780295 <a title="163-lda-15" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>16 0.70769936 <a title="163-lda-16" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>17 0.70731318 <a title="163-lda-17" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<p>18 0.70698982 <a title="163-lda-18" href="../hunch_net-2011/hunch_net-2011-10-24-2011_ML_symposium_and_the_bears.html">448 hunch net-2011-10-24-2011 ML symposium and the bears</a></p>
<p>19 0.70679915 <a title="163-lda-19" href="../hunch_net-2006/hunch_net-2006-07-13-Regression_vs._Classification_as_a_Primitive.html">196 hunch net-2006-07-13-Regression vs. Classification as a Primitive</a></p>
<p>20 0.70524579 <a title="163-lda-20" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
