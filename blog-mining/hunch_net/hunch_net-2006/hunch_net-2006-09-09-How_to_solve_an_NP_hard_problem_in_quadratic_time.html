<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>206 hunch net-2006-09-09-How to solve an NP hard problem in quadratic time</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-206" href="#">hunch_net-2006-206</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>206 hunch net-2006-09-09-How to solve an NP hard problem in quadratic time</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-206-html" href="http://hunch.net/?p=226">html</a></p><p>Introduction: This title is a lie, but it is a special lie which has a bit of
truth.Ifnplayers each play each other, you have a tournament. How do you order
the players from weakest to strongest?The standard first attempt is "find the
ordering which agrees with the tournament on as many player pairs as
possible". This is called the "minimum feedback arcset" problem in the CS
theory literature and it is a well known NP-hard problem. A basic guarantee
holds for the solution to this problem: if there is some "true" intrinsic
ordering, and the outcome of the tournament disagreesktimes (due to noise for
instance), then the output ordering will disagree with the original ordering
on at most2kedges (and no solution can be better).One standard approach to
tractably solving an NP-hard problem is to find another algorithm with an
approximation guarantee. For example,Don Coppersmith,Lisa FleischerandAtri
Rudraproved thatordering players according to the number of wins is a
5-approximation to the NP-hard proble</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The standard first attempt is "find the ordering which agrees with the tournament on as many player pairs as possible". [sent-4, score-1.017]
</p><p>2 A basic guarantee holds for the solution to this problem: if there is some "true" intrinsic ordering, and the outcome of the tournament disagreesktimes (due to noise for instance), then the output ordering will disagree with the original ordering on at most2kedges (and no solution can be better). [sent-6, score-1.783]
</p><p>3 One standard approach to tractably solving an NP-hard problem is to find another algorithm with an approximation guarantee. [sent-7, score-0.309]
</p><p>4 For example,Don Coppersmith,Lisa FleischerandAtri Rudraproved thatordering players according to the number of wins is a 5-approximation to the NP-hard problem. [sent-8, score-0.418]
</p><p>5 An even better approach is to realize that the NP hard problem may not be the real problem. [sent-9, score-0.184]
</p><p>6 The real problem may be finding a good approximation to the "true" intrinsic ordering given noisy tournament information. [sent-10, score-1.039]
</p><p>7 In a learning setting, the simplest form of ranking problem is "bipartite ranking" where every element has a value of either0or1and we want to order0s before1s. [sent-11, score-0.228]
</p><p>8 A common way to measure the performance of bipartite ranking is according to "area under the ROC curve" (AUC) = 1 - the fraction of out-of-order pairs. [sent-12, score-0.444]
</p><p>9 Nina,Alina,Gregand I proved that if we learn a comparison function which errs onkdissimilar pairs, then ordering according to the number of wins yields an order within 4k edge reversals of the original ordering. [sent-13, score-0.995]
</p><p>10 As a reduction statement(*), this shows that an error rate ofefor a learned pairwise binary classifier produces an ordering with an expected AUC of1 - 4e. [sent-14, score-1.262]
</p><p>11 The same inequality even holds for a (stronger) regret transform. [sent-15, score-0.29]
</p><p>12 Ifr = e - eminis the regret of the binary pairwise classifier, then the AUC regret is bounded by4r. [sent-16, score-0.69]
</p><p>13 (Hereeminis the error rate of the best possible classifier which predicts knowing the most likely outcome. [sent-17, score-0.205]
</p><p>14 ) The regret result extends to more general measures of ordering than simply AUC. [sent-18, score-0.757]
</p><p>15 We were unable to find any examples where ordering according to the degree produced more than a2rAUC regret. [sent-19, score-0.811]
</p><p>16 At the end of the day, we have an algorithm with satisfies precisely the same guarantee as the NP hard solution. [sent-21, score-0.229]
</p><p>17 The learning lesson is that a good pairwise comparator implies the ability to rank well according to AUC. [sent-23, score-0.716]
</p><p>18 The general research lesson is that an NP hard problem for an approximate solution is not an intrinsic obstacle. [sent-24, score-0.566]
</p><p>19 (*) To prove the reduction, you must make sure that your form pairwise examples in the right way. [sent-26, score-0.409]
</p><p>20 Your source of pairwise ordering examples must be uniform over the dissimilar pairs containing one example with label1and one example with label0. [sent-27, score-1.187]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ordering', 0.502), ('pairwise', 0.345), ('np', 0.241), ('auc', 0.207), ('tournament', 0.192), ('according', 0.169), ('intrinsic', 0.166), ('pairs', 0.147), ('bipartite', 0.138), ('lesson', 0.138), ('regret', 0.137), ('ranking', 0.137), ('lie', 0.128), ('players', 0.128), ('wins', 0.121), ('holds', 0.098), ('classifier', 0.097), ('hard', 0.093), ('problem', 0.091), ('approximation', 0.088), ('original', 0.084), ('guarantee', 0.083), ('solution', 0.078), ('find', 0.076), ('binary', 0.071), ('reduction', 0.07), ('greg', 0.069), ('ofefor', 0.069), ('dissimilar', 0.069), ('strongest', 0.064), ('errs', 0.064), ('player', 0.064), ('rank', 0.064), ('examples', 0.064), ('extends', 0.06), ('containing', 0.06), ('lessons', 0.06), ('true', 0.058), ('measures', 0.058), ('agrees', 0.058), ('curve', 0.055), ('roc', 0.055), ('inequality', 0.055), ('satisfy', 0.055), ('error', 0.055), ('order', 0.055), ('standard', 0.054), ('satisfies', 0.053), ('literature', 0.053), ('rate', 0.053)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="206-tfidf-1" href="../hunch_net-2006/hunch_net-2006-09-09-How_to_solve_an_NP_hard_problem_in_quadratic_time.html">206 hunch net-2006-09-09-How to solve an NP hard problem in quadratic time</a></p>
<p>Introduction: This title is a lie, but it is a special lie which has a bit of
truth.Ifnplayers each play each other, you have a tournament. How do you order
the players from weakest to strongest?The standard first attempt is "find the
ordering which agrees with the tournament on as many player pairs as
possible". This is called the "minimum feedback arcset" problem in the CS
theory literature and it is a well known NP-hard problem. A basic guarantee
holds for the solution to this problem: if there is some "true" intrinsic
ordering, and the outcome of the tournament disagreesktimes (due to noise for
instance), then the output ordering will disagree with the original ordering
on at most2kedges (and no solution can be better).One standard approach to
tractably solving an NP-hard problem is to find another algorithm with an
approximation guarantee. For example,Don Coppersmith,Lisa FleischerandAtri
Rudraproved thatordering players according to the number of wins is a
5-approximation to the NP-hard proble</p><p>2 0.1686985 <a title="206-tfidf-2" href="../hunch_net-2005/hunch_net-2005-05-16-Regret_minimizing_vs_error_limiting_reductions.html">72 hunch net-2005-05-16-Regret minimizing vs error limiting reductions</a></p>
<p>Introduction: This post is about a reductions-related problem that I find mysterious. There
are two kinds of reductions analysis currently under consideration.Error
limiting reductions. Here, the goal is to bound the error rate of the created
classifier in terms of the error rate of the binary classifiers that you
reduce to. A very simple example of this is thaterror correcting output
codeswhere it is possible to prove that for certain codes, the multiclass
error rate is at most 4 * the binary classifier error rate.Regret minimizing
reductions. Here, the goal is to bound theregretof the created classifier in
terms of theregretof the binary classifiers reduced to. The regret is the
error rate minus the minimum error rate. When the learning problem is noisy
the minimum error rate may not be0. An analagous result for reget is that for
aprobabilistic error correcting output code, multiclass regret is at most 4 *
(binary regret)0.5.The use of "regret" is more desirable than the use of error
rates, becaus</p><p>3 0.1616499 <a title="206-tfidf-3" href="../hunch_net-2007/hunch_net-2007-08-19-Choice_of_Metrics.html">259 hunch net-2007-08-19-Choice of Metrics</a></p>
<p>Introduction: How do we judge success in Machine Learning? AsAaronnotes, the best way is to
use the loss imposed on you by the world. This turns out to be infeasible
sometimes for various reasons. The ones I've seen are:The learned prediction
is used in some complicated process that does not give the feedback necessary
to understand the prediction's impact on the loss.The prediction is used by
some other system which expects some semantics to the predicted value. This is
similar to the previous example, except that the issue is design modularity
rather than engineering modularity.The correct loss function is simply unknown
(and perhaps unknowable, except by experimentation).In these situations, it's
unclear what metric for evaluation should be chosen. This post has some design
advice for this murkier case. I'm using the word "metric" here to distinguish
the fact that we are considering methods forevaluatingpredictive systems
rather than a loss imposed by the real world or a loss which is optimized b</p><p>4 0.16038974 <a title="206-tfidf-4" href="../hunch_net-2007/hunch_net-2007-05-08-Conditional_Tournaments_for_Multiclass_to_Binary.html">243 hunch net-2007-05-08-Conditional Tournaments for Multiclass to Binary</a></p>
<p>Introduction: Thisproblemhas been cracked (but not quite completely solved) byAlina,Pradeep,
andI. The problem is essentially finding a better way to reduce multiclass
classification to binary classification. The solution is to use a carefully
crafted tournament, the simplest version of which is asingle elimination
tournamentwhere the "players" are the different classes. An example of the
structure is here:For the single elimination tournament, we can prove that:For
all multiclass problemsD, for all learned binary classifiersc, the regret of
an induced multiclass classifier is bounded by the regret of the binary
classifier timeslog2k. Restated:regmulticlass(D,Filter_tree_test(c)) <=
regbinary(Filter_tree_train(D),c)Here:Filter_tree_train(D)is the induced
binary classification problemFilter_tree_test(c)is the induced multiclass
classifier.regmulticlassis the multiclass regret (= difference between error
rate and minimum possible error rate)regbinaryis the binary regretThis result
has a slight depende</p><p>5 0.12424687 <a title="206-tfidf-5" href="../hunch_net-2006/hunch_net-2006-05-23-What_is_the_best_regret_transform_reduction_from_multiclass_to_binary%3F.html">181 hunch net-2006-05-23-What is the best regret transform reduction from multiclass to binary?</a></p>
<p>Introduction: This post is about an open problem in learning reductions.BackgroundA
reduction might transform a a multiclass prediction problem where there
arekpossible labels into a binary learning problem where there are only 2
possible labels. On this induced binary problem we might learn a binary
classifier with some error ratee. After subtracting the minimum possible
(Bayes) error rateb, we get a regretr = e - b. ThePECOC(Probabilistic Error
Correcting Output Code) reduction has the property that binary regretrimplies
multiclass regret at most4r0.5.The problemThis is not the "rightest" answer.
Consider thek=2case, where we reduce binary to binary. There exists a
reduction (the identity) with the property that regretrimplies regretr. This
is substantially superior to the transform given by the PECOC reduction, which
suggests that a better reduction may exist for generalk. For example, we can
not rule out the possibility that a reductionRexists with regret transform
guaranteeing binary regretrimp</p><p>6 0.1231796 <a title="206-tfidf-6" href="../hunch_net-2010/hunch_net-2010-03-15-The_Efficient_Robust_Conditional_Probability_Estimation_Problem.html">391 hunch net-2010-03-15-The Efficient Robust Conditional Probability Estimation Problem</a></p>
<p>7 0.10913707 <a title="206-tfidf-7" href="../hunch_net-2009/hunch_net-2009-01-07-Interesting_Papers_at_SODA_2009.html">334 hunch net-2009-01-07-Interesting Papers at SODA 2009</a></p>
<p>8 0.10430115 <a title="206-tfidf-8" href="../hunch_net-2005/hunch_net-2005-02-12-ROC_vs._Accuracy_vs._AROC.html">18 hunch net-2005-02-12-ROC vs. Accuracy vs. AROC</a></p>
<p>9 0.096699573 <a title="206-tfidf-9" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>10 0.095214702 <a title="206-tfidf-10" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>11 0.094137892 <a title="206-tfidf-11" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>12 0.094087094 <a title="206-tfidf-12" href="../hunch_net-2006/hunch_net-2006-03-23-The_Approximation_Argument.html">165 hunch net-2006-03-23-The Approximation Argument</a></p>
<p>13 0.087705992 <a title="206-tfidf-13" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>14 0.084092088 <a title="206-tfidf-14" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>15 0.083718091 <a title="206-tfidf-15" href="../hunch_net-2005/hunch_net-2005-05-06-Don%26%238217%3Bt_mix_the_solution_into_the_problem.html">67 hunch net-2005-05-06-Don&#8217;t mix the solution into the problem</a></p>
<p>16 0.083339199 <a title="206-tfidf-16" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<p>17 0.08296635 <a title="206-tfidf-17" href="../hunch_net-2005/hunch_net-2005-08-11-Why_Manifold-Based_Dimension_Reduction_Techniques%3F.html">102 hunch net-2005-08-11-Why Manifold-Based Dimension Reduction Techniques?</a></p>
<p>18 0.080827266 <a title="206-tfidf-18" href="../hunch_net-2007/hunch_net-2007-03-15-Alternative_Machine_Learning_Reductions_Definitions.html">236 hunch net-2007-03-15-Alternative Machine Learning Reductions Definitions</a></p>
<p>19 0.079465263 <a title="206-tfidf-19" href="../hunch_net-2010/hunch_net-2010-02-26-Yahoo%21_ML_events.html">389 hunch net-2010-02-26-Yahoo! ML events</a></p>
<p>20 0.077933304 <a title="206-tfidf-20" href="../hunch_net-2007/hunch_net-2007-01-10-A_Deep_Belief_Net_Learning_Problem.html">227 hunch net-2007-01-10-A Deep Belief Net Learning Problem</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.174), (1, -0.113), (2, -0.082), (3, -0.001), (4, 0.025), (5, -0.161), (6, 0.072), (7, 0.049), (8, 0.108), (9, 0.012), (10, 0.013), (11, 0.03), (12, -0.02), (13, 0.004), (14, 0.003), (15, 0.042), (16, -0.031), (17, -0.013), (18, -0.063), (19, -0.004), (20, -0.035), (21, 0.071), (22, 0.024), (23, -0.046), (24, 0.114), (25, 0.002), (26, 0.041), (27, -0.016), (28, 0.088), (29, 0.028), (30, 0.015), (31, -0.034), (32, -0.001), (33, 0.043), (34, 0.028), (35, -0.001), (36, 0.034), (37, 0.097), (38, 0.066), (39, 0.01), (40, -0.075), (41, 0.001), (42, 0.069), (43, -0.059), (44, -0.002), (45, 0.009), (46, -0.023), (47, 0.035), (48, -0.013), (49, -0.007)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96063536 <a title="206-lsi-1" href="../hunch_net-2006/hunch_net-2006-09-09-How_to_solve_an_NP_hard_problem_in_quadratic_time.html">206 hunch net-2006-09-09-How to solve an NP hard problem in quadratic time</a></p>
<p>Introduction: This title is a lie, but it is a special lie which has a bit of
truth.Ifnplayers each play each other, you have a tournament. How do you order
the players from weakest to strongest?The standard first attempt is "find the
ordering which agrees with the tournament on as many player pairs as
possible". This is called the "minimum feedback arcset" problem in the CS
theory literature and it is a well known NP-hard problem. A basic guarantee
holds for the solution to this problem: if there is some "true" intrinsic
ordering, and the outcome of the tournament disagreesktimes (due to noise for
instance), then the output ordering will disagree with the original ordering
on at most2kedges (and no solution can be better).One standard approach to
tractably solving an NP-hard problem is to find another algorithm with an
approximation guarantee. For example,Don Coppersmith,Lisa FleischerandAtri
Rudraproved thatordering players according to the number of wins is a
5-approximation to the NP-hard proble</p><p>2 0.76362693 <a title="206-lsi-2" href="../hunch_net-2007/hunch_net-2007-05-08-Conditional_Tournaments_for_Multiclass_to_Binary.html">243 hunch net-2007-05-08-Conditional Tournaments for Multiclass to Binary</a></p>
<p>Introduction: Thisproblemhas been cracked (but not quite completely solved) byAlina,Pradeep,
andI. The problem is essentially finding a better way to reduce multiclass
classification to binary classification. The solution is to use a carefully
crafted tournament, the simplest version of which is asingle elimination
tournamentwhere the "players" are the different classes. An example of the
structure is here:For the single elimination tournament, we can prove that:For
all multiclass problemsD, for all learned binary classifiersc, the regret of
an induced multiclass classifier is bounded by the regret of the binary
classifier timeslog2k. Restated:regmulticlass(D,Filter_tree_test(c)) <=
regbinary(Filter_tree_train(D),c)Here:Filter_tree_train(D)is the induced
binary classification problemFilter_tree_test(c)is the induced multiclass
classifier.regmulticlassis the multiclass regret (= difference between error
rate and minimum possible error rate)regbinaryis the binary regretThis result
has a slight depende</p><p>3 0.71137136 <a title="206-lsi-3" href="../hunch_net-2006/hunch_net-2006-05-23-What_is_the_best_regret_transform_reduction_from_multiclass_to_binary%3F.html">181 hunch net-2006-05-23-What is the best regret transform reduction from multiclass to binary?</a></p>
<p>Introduction: This post is about an open problem in learning reductions.BackgroundA
reduction might transform a a multiclass prediction problem where there
arekpossible labels into a binary learning problem where there are only 2
possible labels. On this induced binary problem we might learn a binary
classifier with some error ratee. After subtracting the minimum possible
(Bayes) error rateb, we get a regretr = e - b. ThePECOC(Probabilistic Error
Correcting Output Code) reduction has the property that binary regretrimplies
multiclass regret at most4r0.5.The problemThis is not the "rightest" answer.
Consider thek=2case, where we reduce binary to binary. There exists a
reduction (the identity) with the property that regretrimplies regretr. This
is substantially superior to the transform given by the PECOC reduction, which
suggests that a better reduction may exist for generalk. For example, we can
not rule out the possibility that a reductionRexists with regret transform
guaranteeing binary regretrimp</p><p>4 0.69503582 <a title="206-lsi-4" href="../hunch_net-2005/hunch_net-2005-05-16-Regret_minimizing_vs_error_limiting_reductions.html">72 hunch net-2005-05-16-Regret minimizing vs error limiting reductions</a></p>
<p>Introduction: This post is about a reductions-related problem that I find mysterious. There
are two kinds of reductions analysis currently under consideration.Error
limiting reductions. Here, the goal is to bound the error rate of the created
classifier in terms of the error rate of the binary classifiers that you
reduce to. A very simple example of this is thaterror correcting output
codeswhere it is possible to prove that for certain codes, the multiclass
error rate is at most 4 * the binary classifier error rate.Regret minimizing
reductions. Here, the goal is to bound theregretof the created classifier in
terms of theregretof the binary classifiers reduced to. The regret is the
error rate minus the minimum error rate. When the learning problem is noisy
the minimum error rate may not be0. An analagous result for reget is that for
aprobabilistic error correcting output code, multiclass regret is at most 4 *
(binary regret)0.5.The use of "regret" is more desirable than the use of error
rates, becaus</p><p>5 0.63909006 <a title="206-lsi-5" href="../hunch_net-2010/hunch_net-2010-03-15-The_Efficient_Robust_Conditional_Probability_Estimation_Problem.html">391 hunch net-2010-03-15-The Efficient Robust Conditional Probability Estimation Problem</a></p>
<p>Introduction: I'm offering a reward of $1000 for a solution to this problem. This joins
thecross validation problemwhich I'm offering a$500 rewardfor. I believe both
of these problems are hard but plausibly solvable, and plausibly with a
solution of substantial practical value. While it's unlikely these rewards are
worth your time on an hourly wage basis, the recognition for solving them
definitely should beThe ProblemThe problem is finding a general, robust, and
efficient mechanism for estimating a conditional probabilityP(y|x)where
robustness and efficiency are measured using techniques from learning
reductions.In particular, suppose we have access to a binary regression
oracleBwhich has two interfaces--one for specifying training information and
one for testing. Training information is specified asB(x',y')wherex'is a
feature vector andy'is a scalar in[0,1]with no value returned. Testing is done
according toB(x')with a value in[0,1]returned.A learning reduction consists of
two algorithmsRandR-1whi</p><p>6 0.62144727 <a title="206-lsi-6" href="../hunch_net-2006/hunch_net-2006-07-13-Regression_vs._Classification_as_a_Primitive.html">196 hunch net-2006-07-13-Regression vs. Classification as a Primitive</a></p>
<p>7 0.57394099 <a title="206-lsi-7" href="../hunch_net-2005/hunch_net-2005-08-11-Why_Manifold-Based_Dimension_Reduction_Techniques%3F.html">102 hunch net-2005-08-11-Why Manifold-Based Dimension Reduction Techniques?</a></p>
<p>8 0.56815839 <a title="206-lsi-8" href="../hunch_net-2005/hunch_net-2005-03-18-Binomial_Weighting.html">43 hunch net-2005-03-18-Binomial Weighting</a></p>
<p>9 0.56442279 <a title="206-lsi-9" href="../hunch_net-2005/hunch_net-2005-05-06-Don%26%238217%3Bt_mix_the_solution_into_the_problem.html">67 hunch net-2005-05-06-Don&#8217;t mix the solution into the problem</a></p>
<p>10 0.56221759 <a title="206-lsi-10" href="../hunch_net-2007/hunch_net-2007-06-14-Interesting_Papers_at_COLT_2007.html">247 hunch net-2007-06-14-Interesting Papers at COLT 2007</a></p>
<p>11 0.55379176 <a title="206-lsi-11" href="../hunch_net-2005/hunch_net-2005-06-06-Exact_Online_Learning_for_Classification.html">78 hunch net-2005-06-06-Exact Online Learning for Classification</a></p>
<p>12 0.55218685 <a title="206-lsi-12" href="../hunch_net-2006/hunch_net-2006-05-01-A_conversation_between_Theo_and_Pat.html">176 hunch net-2006-05-01-A conversation between Theo and Pat</a></p>
<p>13 0.53001672 <a title="206-lsi-13" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>14 0.52182853 <a title="206-lsi-14" href="../hunch_net-2007/hunch_net-2007-03-15-Alternative_Machine_Learning_Reductions_Definitions.html">236 hunch net-2007-03-15-Alternative Machine Learning Reductions Definitions</a></p>
<p>15 0.52149022 <a title="206-lsi-15" href="../hunch_net-2006/hunch_net-2006-02-18-Multiplication_of_Learned_Probabilities_is_Dangerous.html">157 hunch net-2006-02-18-Multiplication of Learned Probabilities is Dangerous</a></p>
<p>16 0.52074331 <a title="206-lsi-16" href="../hunch_net-2009/hunch_net-2009-01-07-Interesting_Papers_at_SODA_2009.html">334 hunch net-2009-01-07-Interesting Papers at SODA 2009</a></p>
<p>17 0.50586975 <a title="206-lsi-17" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>18 0.50373167 <a title="206-lsi-18" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<p>19 0.50300187 <a title="206-lsi-19" href="../hunch_net-2005/hunch_net-2005-02-21-Problem%3A_Cross_Validation.html">26 hunch net-2005-02-21-Problem: Cross Validation</a></p>
<p>20 0.49499854 <a title="206-lsi-20" href="../hunch_net-2005/hunch_net-2005-02-12-ROC_vs._Accuracy_vs._AROC.html">18 hunch net-2005-02-12-ROC vs. Accuracy vs. AROC</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(6, 0.038), (35, 0.036), (42, 0.239), (68, 0.061), (74, 0.047), (76, 0.043), (82, 0.024), (87, 0.023), (88, 0.012), (91, 0.011), (97, 0.364)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.86182821 <a title="206-lda-1" href="../hunch_net-2006/hunch_net-2006-09-09-How_to_solve_an_NP_hard_problem_in_quadratic_time.html">206 hunch net-2006-09-09-How to solve an NP hard problem in quadratic time</a></p>
<p>Introduction: This title is a lie, but it is a special lie which has a bit of
truth.Ifnplayers each play each other, you have a tournament. How do you order
the players from weakest to strongest?The standard first attempt is "find the
ordering which agrees with the tournament on as many player pairs as
possible". This is called the "minimum feedback arcset" problem in the CS
theory literature and it is a well known NP-hard problem. A basic guarantee
holds for the solution to this problem: if there is some "true" intrinsic
ordering, and the outcome of the tournament disagreesktimes (due to noise for
instance), then the output ordering will disagree with the original ordering
on at most2kedges (and no solution can be better).One standard approach to
tractably solving an NP-hard problem is to find another algorithm with an
approximation guarantee. For example,Don Coppersmith,Lisa FleischerandAtri
Rudraproved thatordering players according to the number of wins is a
5-approximation to the NP-hard proble</p><p>2 0.82618177 <a title="206-lda-2" href="../hunch_net-2007/hunch_net-2007-04-28-The_Coming_Patent_Apocalypse.html">241 hunch net-2007-04-28-The Coming Patent Apocalypse</a></p>
<p>Introduction: Many people in computer science believe that patents are problematic. The
truth is even worse--the patent system in the US is fundamentally broken in
ways that will require much more significant reform thanis being considered
now.The myth of the patent is the following: Patents are a mechanism for
inventors to be compensated according to the value of their inventions while
making the invention available to all. This myth sounds pretty desirable, but
the reality is a strange distortion slowly leading towards collapse.There are
many problems associated with patents, but I would like to focus on just two
of them:Patent TrollsThe way that patents have generally worked over the last
several decades is that they were a tool of large companies. Large companies
would amass a large number of patents and then cross-license each other's
patents--in effect saying "we agree to owe each other nothing". Smaller
companies would sometimes lose in this game, essentially because they didn't
have enough p</p><p>3 0.82001239 <a title="206-lda-3" href="../hunch_net-2009/hunch_net-2009-01-19-Netflix_prize_within_epsilon.html">336 hunch net-2009-01-19-Netflix prize within epsilon</a></p>
<p>Introduction: The competitors for theNetflix Prizeare tantalizingly close winning the
million dollar prize. This year,BellKorandCommendo Researchsent a combined
solution that won theprogress prize. Reading thewriteups2is instructive.
Several aspects of solutions are taken for granted including stochastic
gradient descent, ensemble prediction, and targeting residuals (a form of
boosting). Relatively to last year, it appears that many approaches have added
parameterizations, especially for the purpose of modeling through time.The big
question is: will they make the big prize? At this point, the level of
complexity in entering the competition is prohibitive, so perhaps only the
existing competitors will continue to try. (This equation might change
drastically if the teams open source their existing solutions, including
parameter settings.) One fear is that the progress is asymptoting on the wrong
side of the 10% threshold. In the first year, the teams progressed through
84.3% of the 10% gap, and in the</p><p>4 0.81864178 <a title="206-lda-4" href="../hunch_net-2012/hunch_net-2012-02-24-ICML%2B50%25.html">456 hunch net-2012-02-24-ICML+50%</a></p>
<p>Introduction: TheICMLpaper deadline has passed.Joelleand I were surprised to see the number
of submissions jump from last year by about 50% to around 900 submissions. A
tiny portion of these are immediate rejects(*), so this is a much larger set
of papers than expected. The number of workshop submissions also doubled
compared to last year, so ICML may grow significantly this year, if we can
manage to handle the load well. The prospect of making 900 good decisions is
fundamentally daunting, and success will rely heavily on theprogram
committeeandarea chairsat this point.For those who want to rubberneck a bit
more, here's a breakdown of submissions by primary topic of submitted
papers:66 Reinforcement Learning 52 Supervised Learning 51 Clustering 46
Kernel Methods 40 Optimization Algorithms 39 Feature Selection and
Dimensionality Reduction 33 Learning Theory 33 Graphical Models 33
Applications 29 Probabilistic Models 29 NN & Deep Learning 26 Transfer and
Multi-Task Learning 25 Online Learning 25 Activ</p><p>5 0.74614573 <a title="206-lda-5" href="../hunch_net-2008/hunch_net-2008-03-15-COLT_Open_Problems.html">292 hunch net-2008-03-15-COLT Open Problems</a></p>
<p>Introduction: COLT has acall for open problemsdue March 21. I encourage anyone with a
specifiable open problem to write it down and send it in. Just the effort of
specifying an open problem precisely and concisely has been very helpful for
my own solutions, and there is a substantial chance others will solve it. To
increase the chance someone will take it up, you can even put a bounty on the
solution. (Perhaps I should raise the$500 bountyon theK-fold cross-validation
problemas it hasn't yet been solved).</p><p>6 0.71806723 <a title="206-lda-6" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<p>7 0.59593254 <a title="206-lda-7" href="../hunch_net-2007/hunch_net-2007-08-19-Choice_of_Metrics.html">259 hunch net-2007-08-19-Choice of Metrics</a></p>
<p>8 0.57994634 <a title="206-lda-8" href="../hunch_net-2005/hunch_net-2005-02-12-ROC_vs._Accuracy_vs._AROC.html">18 hunch net-2005-02-12-ROC vs. Accuracy vs. AROC</a></p>
<p>9 0.56468815 <a title="206-lda-9" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>10 0.56320781 <a title="206-lda-10" href="../hunch_net-2007/hunch_net-2007-01-10-A_Deep_Belief_Net_Learning_Problem.html">227 hunch net-2007-01-10-A Deep Belief Net Learning Problem</a></p>
<p>11 0.55950695 <a title="206-lda-11" href="../hunch_net-2010/hunch_net-2010-03-15-The_Efficient_Robust_Conditional_Probability_Estimation_Problem.html">391 hunch net-2010-03-15-The Efficient Robust Conditional Probability Estimation Problem</a></p>
<p>12 0.55900598 <a title="206-lda-12" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>13 0.55896258 <a title="206-lda-13" href="../hunch_net-2005/hunch_net-2005-05-16-Regret_minimizing_vs_error_limiting_reductions.html">72 hunch net-2005-05-16-Regret minimizing vs error limiting reductions</a></p>
<p>14 0.55881172 <a title="206-lda-14" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<p>15 0.55880433 <a title="206-lda-15" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>16 0.55703211 <a title="206-lda-16" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>17 0.55654287 <a title="206-lda-17" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>18 0.55648232 <a title="206-lda-18" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>19 0.554389 <a title="206-lda-19" href="../hunch_net-2006/hunch_net-2006-03-02-Why_do_people_count_for_learning%3F.html">160 hunch net-2006-03-02-Why do people count for learning?</a></p>
<p>20 0.55436331 <a title="206-lda-20" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
