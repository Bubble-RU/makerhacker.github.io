<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>161 hunch net-2006-03-05-&#8220;Structural&#8221; Learning</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-161" href="#">hunch_net-2006-161</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>161 hunch net-2006-03-05-&#8220;Structural&#8221; Learning</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-161-html" href="http://hunch.net/?p=172">html</a></p><p>Introduction: Fernando Pereirapointed outAndo andZhang'spaperon "structural" learning.
Structural learning is multitask learning on subproblems created from
unlabeled data.The basic idea is to take a look at the unlabeled data and
create many supervised problems. On text data, which they test on, these
subproblems might be of the form "Given surrounding words predict the middle
word". The hope here is that successfully predicting on these subproblems is
relevant to the prediction of your core problem.In the long run, the precise
mechanism used (essentially, linear predictors with parameters tied by a
common matrix) and the precise problems formed may not be critical. What seems
critical is that the hope is realized: the technique provides a significant
edge in practice.Some basic questions about this approach are:Are there
effective automated mechanisms for creating the subproblems?Is it necessary to
use a shared representation?</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Structural learning is multitask learning on subproblems created from unlabeled data. [sent-2, score-0.967]
</p><p>2 The basic idea is to take a look at the unlabeled data and create many supervised problems. [sent-3, score-0.767]
</p><p>3 On text data, which they test on, these subproblems might be of the form "Given surrounding words predict the middle word". [sent-4, score-1.209]
</p><p>4 The hope here is that successfully predicting on these subproblems is relevant to the prediction of your core problem. [sent-5, score-1.059]
</p><p>5 In the long run, the precise mechanism used (essentially, linear predictors with parameters tied by a common matrix) and the precise problems formed may not be critical. [sent-6, score-1.159]
</p><p>6 What seems critical is that the hope is realized: the technique provides a significant edge in practice. [sent-7, score-0.568]
</p><p>7 Some basic questions about this approach are:Are there effective automated mechanisms for creating the subproblems? [sent-8, score-0.552]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('subproblems', 0.517), ('structural', 0.311), ('unlabeled', 0.218), ('precise', 0.214), ('fernando', 0.178), ('middle', 0.165), ('formed', 0.156), ('tied', 0.156), ('surrounding', 0.156), ('successfully', 0.156), ('realized', 0.142), ('shared', 0.142), ('multitask', 0.133), ('matrix', 0.129), ('hope', 0.128), ('text', 0.123), ('edge', 0.115), ('automated', 0.107), ('word', 0.107), ('mechanisms', 0.104), ('words', 0.102), ('parameters', 0.102), ('predictors', 0.1), ('created', 0.099), ('data', 0.095), ('technique', 0.095), ('basic', 0.095), ('predicting', 0.094), ('representation', 0.093), ('critical', 0.093), ('supervised', 0.09), ('creating', 0.086), ('core', 0.085), ('necessary', 0.083), ('effective', 0.081), ('look', 0.08), ('questions', 0.079), ('relevant', 0.079), ('provides', 0.078), ('test', 0.077), ('linear', 0.077), ('run', 0.076), ('mechanism', 0.075), ('create', 0.07), ('essentially', 0.069), ('predict', 0.069), ('long', 0.065), ('idea', 0.06), ('significant', 0.059), ('take', 0.059)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="161-tfidf-1" href="../hunch_net-2006/hunch_net-2006-03-05-%26%238220%3BStructural%26%238221%3B_Learning.html">161 hunch net-2006-03-05-&#8220;Structural&#8221; Learning</a></p>
<p>Introduction: Fernando Pereirapointed outAndo andZhang'spaperon "structural" learning.
Structural learning is multitask learning on subproblems created from
unlabeled data.The basic idea is to take a look at the unlabeled data and
create many supervised problems. On text data, which they test on, these
subproblems might be of the form "Given surrounding words predict the middle
word". The hope here is that successfully predicting on these subproblems is
relevant to the prediction of your core problem.In the long run, the precise
mechanism used (essentially, linear predictors with parameters tied by a
common matrix) and the precise problems formed may not be critical. What seems
critical is that the hope is realized: the technique provides a significant
edge in practice.Some basic questions about this approach are:Are there
effective automated mechanisms for creating the subproblems?Is it necessary to
use a shared representation?</p><p>2 0.18225597 <a title="161-tfidf-2" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>Introduction: One of the common trends in machine learning has been an emphasis on the use
of unlabeled data. The argument goes something like "there aren't many labeled
web pages out there, but there are ahugenumber of web pages, so we must find a
way to take advantage of them." There are several standard approaches for
doing this:Unsupervised Learning. You use only unlabeled data. In a typical
application, you cluster the data and hope that the clusters somehow
correspond to what you care about.Semisupervised Learning. You use both
unlabeled and labeled data to build a predictor. The unlabeled data influences
the learned predictor in some way.Active Learning. You have unlabeled data and
access to a labeling oracle. You interactively choose which examples to label
so as to optimize prediction accuracy.It seems there is a fourth approach
worth serious investigation--automated labeling. The approach goes as
follows:Identify some subset of observed values to predict from the
others.Build a predictor.U</p><p>3 0.1635142 <a title="161-tfidf-3" href="../hunch_net-2006/hunch_net-2006-01-18-Is_Multitask_Learning_Black-Boxable%3F.html">149 hunch net-2006-01-18-Is Multitask Learning Black-Boxable?</a></p>
<p>Introduction: Multitask learning is the learning to predict multiple outputs given the same
input. Mathematically, we might think of this as trying to learn a functionf:X
-> {0,1}n. Structured learning is similar at this level of abstraction. Many
people have worked on solving multitask learning (for exampleRich Caruana)
using methods which share an internal representation. On other words, the the
computation and learning of theith prediction is shared with the computation
and learning of thejth prediction. Another way to ask this question is: can we
avoid sharing the internal representation?For example, itmightbe feasible to
solve multitask learning by some process feeding theith predictionf(x)iinto
thejth predictorf(x,f(x)i)j,If the answer is "no", then it implies we can not
take binary classification as a basic primitive in the process of solving
prediction problems. If the answer is "yes", then we can reuse binary
classification algorithms to solve multitask learning problems.Finding a
satisfyin</p><p>4 0.1456579 <a title="161-tfidf-4" href="../hunch_net-2005/hunch_net-2005-05-10-Learning_Reductions_are_Reductionist.html">68 hunch net-2005-05-10-Learning Reductions are Reductionist</a></p>
<p>Introduction: This is about a fundamental motivation for the investigation of reductions in
learning. It applies to many pieces of work other than my own.The reductionist
approach to problem solving is characterized by taking a problem, decomposing
it into as-small-as-possible subproblems, discovering how to solve the
subproblems, and then discovering how to use the solutions to the subproblems
to solve larger problems. The reductionist approach to solving problems has
often payed offverywell. Computer science related examples of the reductionist
approach include:Reducing computation to the transistor. All of our CPUs are
built from transistors.Reducing rendering of images to rendering a triangle
(or other simple polygons). Computers can now render near-realistic scenes in
real time. The big breakthrough came from learning how to render many
triangles quickly.This approach to problem solving extends well beyond
computer science. Many fields of science focus on theories making predictions
about very</p><p>5 0.12579221 <a title="161-tfidf-5" href="../hunch_net-2006/hunch_net-2006-03-17-Multitask_learning_is_Black-Boxable.html">164 hunch net-2006-03-17-Multitask learning is Black-Boxable</a></p>
<p>Introduction: Multitask learning is the problem of jointly predicting multiple labels
simultaneously with one system. A basic question iswhether or not multitask
learning can be decomposed into one (or more) single prediction problems. It
seems the answer to this is "yes", in a fairly straightforward manner.The
basic idea is that a controlled input feature is equivalent to an extra
output. Suppose we have some process generating examples:(x,y1,y2) in
Swherey1andy2are labels for two different tasks. Then, we could reprocess the
data to the formSb(S) = {((x,i),yi): (x,y1,y2) in S, i in {1,2}}and then learn
a classifierc:X x {1,2} -> Y. Note that(x,i)is the (composite) input. At
testing time, given an inputx, we can querycfor the predicted values of y1and
y2using(x,1)and(x,2).A strong form of equivalence can be stated between these
tasks. In particular, suppose we have a multitask learning algorithmMLwhich
learns a multitask predictorm:X -> Y x Y. Then the following theorem can be
proved:For allMLfor a</p><p>6 0.10824949 <a title="161-tfidf-6" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>7 0.1080034 <a title="161-tfidf-7" href="../hunch_net-2005/hunch_net-2005-04-14-Families_of_Learning_Theory_Statements.html">56 hunch net-2005-04-14-Families of Learning Theory Statements</a></p>
<p>8 0.1026981 <a title="161-tfidf-8" href="../hunch_net-2007/hunch_net-2007-12-12-Workshop_Summary%26%238212%3BPrinciples_of_Learning_Problem_Design.html">277 hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</a></p>
<p>9 0.098887652 <a title="161-tfidf-9" href="../hunch_net-2008/hunch_net-2008-04-27-Watchword%3A_Supervised_Learning.html">299 hunch net-2008-04-27-Watchword: Supervised Learning</a></p>
<p>10 0.09637814 <a title="161-tfidf-10" href="../hunch_net-2009/hunch_net-2009-01-21-Nearly_all_natural_problems_require_nonlinearity.html">337 hunch net-2009-01-21-Nearly all natural problems require nonlinearity</a></p>
<p>11 0.09122137 <a title="161-tfidf-11" href="../hunch_net-2012/hunch_net-2012-05-12-ICML_accepted_papers_and_early_registration.html">465 hunch net-2012-05-12-ICML accepted papers and early registration</a></p>
<p>12 0.08853589 <a title="161-tfidf-12" href="../hunch_net-2007/hunch_net-2007-01-26-Parallel_Machine_Learning_Problems.html">229 hunch net-2007-01-26-Parallel Machine Learning Problems</a></p>
<p>13 0.087141708 <a title="161-tfidf-13" href="../hunch_net-2005/hunch_net-2005-09-14-The_Predictionist_Viewpoint.html">112 hunch net-2005-09-14-The Predictionist Viewpoint</a></p>
<p>14 0.081538722 <a title="161-tfidf-14" href="../hunch_net-2008/hunch_net-2008-12-07-A_NIPS_paper.html">330 hunch net-2008-12-07-A NIPS paper</a></p>
<p>15 0.075486146 <a title="161-tfidf-15" href="../hunch_net-2011/hunch_net-2011-08-06-Interesting_thing_at_UAI_2011.html">440 hunch net-2011-08-06-Interesting thing at UAI 2011</a></p>
<p>16 0.075314164 <a title="161-tfidf-16" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>17 0.07453794 <a title="161-tfidf-17" href="../hunch_net-2007/hunch_net-2007-10-14-NIPS_workshp%3A_Learning_Problem_Design.html">265 hunch net-2007-10-14-NIPS workshp: Learning Problem Design</a></p>
<p>18 0.073942974 <a title="161-tfidf-18" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>19 0.072454005 <a title="161-tfidf-19" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>20 0.069821954 <a title="161-tfidf-20" href="../hunch_net-2005/hunch_net-2005-06-18-Lower_Bounds_for_Learning_Reductions.html">83 hunch net-2005-06-18-Lower Bounds for Learning Reductions</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.151), (1, -0.084), (2, 0.034), (3, -0.018), (4, -0.062), (5, 0.017), (6, -0.056), (7, 0.025), (8, 0.017), (9, 0.042), (10, -0.053), (11, 0.052), (12, 0.079), (13, 0.038), (14, 0.054), (15, -0.071), (16, 0.005), (17, 0.01), (18, 0.068), (19, -0.022), (20, -0.019), (21, 0.028), (22, 0.137), (23, -0.031), (24, 0.046), (25, -0.049), (26, -0.034), (27, -0.099), (28, -0.068), (29, -0.007), (30, 0.002), (31, 0.083), (32, -0.054), (33, -0.069), (34, 0.047), (35, -0.093), (36, 0.063), (37, -0.034), (38, -0.082), (39, 0.073), (40, -0.031), (41, -0.019), (42, 0.05), (43, 0.056), (44, 0.092), (45, 0.074), (46, 0.052), (47, 0.04), (48, -0.11), (49, -0.022)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96879017 <a title="161-lsi-1" href="../hunch_net-2006/hunch_net-2006-03-05-%26%238220%3BStructural%26%238221%3B_Learning.html">161 hunch net-2006-03-05-&#8220;Structural&#8221; Learning</a></p>
<p>Introduction: Fernando Pereirapointed outAndo andZhang'spaperon "structural" learning.
Structural learning is multitask learning on subproblems created from
unlabeled data.The basic idea is to take a look at the unlabeled data and
create many supervised problems. On text data, which they test on, these
subproblems might be of the form "Given surrounding words predict the middle
word". The hope here is that successfully predicting on these subproblems is
relevant to the prediction of your core problem.In the long run, the precise
mechanism used (essentially, linear predictors with parameters tied by a
common matrix) and the precise problems formed may not be critical. What seems
critical is that the hope is realized: the technique provides a significant
edge in practice.Some basic questions about this approach are:Are there
effective automated mechanisms for creating the subproblems?Is it necessary to
use a shared representation?</p><p>2 0.73996437 <a title="161-lsi-2" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>Introduction: One of the common trends in machine learning has been an emphasis on the use
of unlabeled data. The argument goes something like "there aren't many labeled
web pages out there, but there are ahugenumber of web pages, so we must find a
way to take advantage of them." There are several standard approaches for
doing this:Unsupervised Learning. You use only unlabeled data. In a typical
application, you cluster the data and hope that the clusters somehow
correspond to what you care about.Semisupervised Learning. You use both
unlabeled and labeled data to build a predictor. The unlabeled data influences
the learned predictor in some way.Active Learning. You have unlabeled data and
access to a labeling oracle. You interactively choose which examples to label
so as to optimize prediction accuracy.It seems there is a fourth approach
worth serious investigation--automated labeling. The approach goes as
follows:Identify some subset of observed values to predict from the
others.Build a predictor.U</p><p>3 0.71871924 <a title="161-lsi-3" href="../hunch_net-2006/hunch_net-2006-03-17-Multitask_learning_is_Black-Boxable.html">164 hunch net-2006-03-17-Multitask learning is Black-Boxable</a></p>
<p>Introduction: Multitask learning is the problem of jointly predicting multiple labels
simultaneously with one system. A basic question iswhether or not multitask
learning can be decomposed into one (or more) single prediction problems. It
seems the answer to this is "yes", in a fairly straightforward manner.The
basic idea is that a controlled input feature is equivalent to an extra
output. Suppose we have some process generating examples:(x,y1,y2) in
Swherey1andy2are labels for two different tasks. Then, we could reprocess the
data to the formSb(S) = {((x,i),yi): (x,y1,y2) in S, i in {1,2}}and then learn
a classifierc:X x {1,2} -> Y. Note that(x,i)is the (composite) input. At
testing time, given an inputx, we can querycfor the predicted values of y1and
y2using(x,1)and(x,2).A strong form of equivalence can be stated between these
tasks. In particular, suppose we have a multitask learning algorithmMLwhich
learns a multitask predictorm:X -> Y x Y. Then the following theorem can be
proved:For allMLfor a</p><p>4 0.70504409 <a title="161-lsi-4" href="../hunch_net-2006/hunch_net-2006-01-18-Is_Multitask_Learning_Black-Boxable%3F.html">149 hunch net-2006-01-18-Is Multitask Learning Black-Boxable?</a></p>
<p>Introduction: Multitask learning is the learning to predict multiple outputs given the same
input. Mathematically, we might think of this as trying to learn a functionf:X
-> {0,1}n. Structured learning is similar at this level of abstraction. Many
people have worked on solving multitask learning (for exampleRich Caruana)
using methods which share an internal representation. On other words, the the
computation and learning of theith prediction is shared with the computation
and learning of thejth prediction. Another way to ask this question is: can we
avoid sharing the internal representation?For example, itmightbe feasible to
solve multitask learning by some process feeding theith predictionf(x)iinto
thejth predictorf(x,f(x)i)j,If the answer is "no", then it implies we can not
take binary classification as a basic primitive in the process of solving
prediction problems. If the answer is "yes", then we can reuse binary
classification algorithms to solve multitask learning problems.Finding a
satisfyin</p><p>5 0.55223423 <a title="161-lsi-5" href="../hunch_net-2007/hunch_net-2007-12-12-Workshop_Summary%26%238212%3BPrinciples_of_Learning_Problem_Design.html">277 hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</a></p>
<p>Introduction: This is a summary of theworkshop on Learning Problem DesignwhichAlinaand I ran
atNIPSthis year.The first question many people have is "What is learning
problem design?" This workshop is about admitting that solving learning
problems does not start with labeled data, but rather somewhere before. When
humans are hired to produce labels, this is usually not a serious problem
because you can tell them precisely what semantics you want the labels to
have, and we can fix some set of features in advance. However, when other
methods are used this becomes more problematic. This focus is important for
Machine Learning because there are very large quantities of data which are not
labeled by a hired human.The title of the workshop was a bit ambitious,
because a workshop is not long enough to synthesize a diversity of approaches
into a coherent set of principles. For me, the posters at the end of the
workshop were quite helpful in getting approaches to gel.Here are some answers
to "where do the lab</p><p>6 0.53072691 <a title="161-lsi-6" href="../hunch_net-2006/hunch_net-2006-11-06-Data_Linkage_Problems.html">217 hunch net-2006-11-06-Data Linkage Problems</a></p>
<p>7 0.52994639 <a title="161-lsi-7" href="../hunch_net-2008/hunch_net-2008-04-27-Watchword%3A_Supervised_Learning.html">299 hunch net-2008-04-27-Watchword: Supervised Learning</a></p>
<p>8 0.51417691 <a title="161-lsi-8" href="../hunch_net-2005/hunch_net-2005-07-07-The_Limits_of_Learning_Theory.html">90 hunch net-2005-07-07-The Limits of Learning Theory</a></p>
<p>9 0.49429861 <a title="161-lsi-9" href="../hunch_net-2006/hunch_net-2006-04-02-Mad_%28Neuro%29science.html">168 hunch net-2006-04-02-Mad (Neuro)science</a></p>
<p>10 0.49201792 <a title="161-lsi-10" href="../hunch_net-2009/hunch_net-2009-06-03-Functionally_defined_Nonlinear_Dynamic_Models.html">359 hunch net-2009-06-03-Functionally defined Nonlinear Dynamic Models</a></p>
<p>11 0.47556439 <a title="161-lsi-11" href="../hunch_net-2005/hunch_net-2005-03-22-Active_learning.html">45 hunch net-2005-03-22-Active learning</a></p>
<p>12 0.47253406 <a title="161-lsi-12" href="../hunch_net-2005/hunch_net-2005-05-10-Learning_Reductions_are_Reductionist.html">68 hunch net-2005-05-10-Learning Reductions are Reductionist</a></p>
<p>13 0.46259588 <a title="161-lsi-13" href="../hunch_net-2005/hunch_net-2005-07-13-Text_Entailment_at_AAAI.html">94 hunch net-2005-07-13-Text Entailment at AAAI</a></p>
<p>14 0.45925435 <a title="161-lsi-14" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>15 0.45733422 <a title="161-lsi-15" href="../hunch_net-2007/hunch_net-2007-09-16-Optimizing_Machine_Learning_Programs.html">262 hunch net-2007-09-16-Optimizing Machine Learning Programs</a></p>
<p>16 0.45037529 <a title="161-lsi-16" href="../hunch_net-2005/hunch_net-2005-09-20-Workshop_Proposal%3A_Atomic_Learning.html">114 hunch net-2005-09-20-Workshop Proposal: Atomic Learning</a></p>
<p>17 0.44667915 <a title="161-lsi-17" href="../hunch_net-2006/hunch_net-2006-01-30-Should_the_Input_Representation_be_a_Vector%3F.html">152 hunch net-2006-01-30-Should the Input Representation be a Vector?</a></p>
<p>18 0.44535381 <a title="161-lsi-18" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>19 0.44141251 <a title="161-lsi-19" href="../hunch_net-2008/hunch_net-2008-08-24-Mass_Customized_Medicine_in_the_Future%3F.html">314 hunch net-2008-08-24-Mass Customized Medicine in the Future?</a></p>
<p>20 0.4404991 <a title="161-lsi-20" href="../hunch_net-2009/hunch_net-2009-04-02-Asymmophobia.html">348 hunch net-2009-04-02-Asymmophobia</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(35, 0.055), (42, 0.216), (68, 0.032), (74, 0.157), (96, 0.414)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.95354998 <a title="161-lda-1" href="../hunch_net-2013/hunch_net-2013-07-24-ICML_2012_videos_lost.html">487 hunch net-2013-07-24-ICML 2012 videos lost</a></p>
<p>Introduction: A big ouch--all the videos for ICML 2012 were lost in a shuffle. Rajnish sends
the below, but if anyone can help that would be greatly appreciated.
----------------------------------------------------Sincere apologies to ICML
community for loosing 2012 archived videosWhat happened: In order to publish
2013 videos, we decided to move 2012 videos to another server. We have a
weekly backup service from the provider but after removing the videos from the
current server, when we tried to retrieve the 2012 videos from backup service,
the backup did not work because of provider-specific requirements that we had
ignored while removing the data from previous server.What are we doing about
this: At this point, we are still looking into raw footage to find if we can
retrieve some of the videos, but following are the steps we are taking to make
sure this does not happen again in future:(1) We are going to create a channel
on Vimeo (and potentially on YouTube) and we will publish there the p-in-p-</p><p>same-blog 2 0.88429224 <a title="161-lda-2" href="../hunch_net-2006/hunch_net-2006-03-05-%26%238220%3BStructural%26%238221%3B_Learning.html">161 hunch net-2006-03-05-&#8220;Structural&#8221; Learning</a></p>
<p>Introduction: Fernando Pereirapointed outAndo andZhang'spaperon "structural" learning.
Structural learning is multitask learning on subproblems created from
unlabeled data.The basic idea is to take a look at the unlabeled data and
create many supervised problems. On text data, which they test on, these
subproblems might be of the form "Given surrounding words predict the middle
word". The hope here is that successfully predicting on these subproblems is
relevant to the prediction of your core problem.In the long run, the precise
mechanism used (essentially, linear predictors with parameters tied by a
common matrix) and the precise problems formed may not be critical. What seems
critical is that the hope is realized: the technique provides a significant
edge in practice.Some basic questions about this approach are:Are there
effective automated mechanisms for creating the subproblems?Is it necessary to
use a shared representation?</p><p>3 0.61713904 <a title="161-lda-3" href="../hunch_net-2005/hunch_net-2005-02-15-ESPgame_and_image_labeling.html">20 hunch net-2005-02-15-ESPgame and image labeling</a></p>
<p>Introduction: Luis von Ahnhas been running theespgamefor awhile now. The espgame provides a
picture to two randomly paired people across the web, and asks them to agree
on a label. It hasn't managed to label the web yet, but it has produced alarge
datasetof (image, label) pairs. I organized the dataset so you couldexplore
the implied bipartite graph(requires much bandwidth).Relative to other image
datasets, this one is quite large--67000 images, 358,000 labels (average of
5/image with variation from 1 to 19), and 22,000 unique labels (one every 3
images). The dataset is also very 'natural', consisting of images spidered
from the internet. The multiple label characteristic is intriguing because
'learning to learn' and metalearning techniques may be applicable. The
'natural' quality means that this dataset varies greatly in difficulty from
easy (predicting "red") to hard (predicting "funny") and potentially more
rewarding to tackle.The open problem here is, of course, to make an internet
image labelin</p><p>4 0.54032516 <a title="161-lda-4" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>Introduction: In the quest to understand what good reviewing is, perhaps it's worthwhile to
think about what good research is. One way to think about good research is in
terms of a producer/consumer model.In the producer/consumer model of research,
for any element of research there are producers (authors and coauthors of
papers, for example) and consumers (people who use the papers to make new
papers or code solving problems). An produced bit of research is judged as
"good" if it is used by many consumers. There are two basic questions which
immediately arise:Is this a good model of research?Are there alternatives?The
producer/consumer model has some difficulties which can be (partially)
addressed.Disconnect.A group of people doing research on some subject may
become disconnected from the rest of the world. Each person uses the research
of other people in the group so it appears good research is being done, but
the group has no impact on the rest of the world. One way to detect this is by
looking at</p><p>5 0.53808498 <a title="161-lda-5" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>Introduction: One of the common trends in machine learning has been an emphasis on the use
of unlabeled data. The argument goes something like "there aren't many labeled
web pages out there, but there are ahugenumber of web pages, so we must find a
way to take advantage of them." There are several standard approaches for
doing this:Unsupervised Learning. You use only unlabeled data. In a typical
application, you cluster the data and hope that the clusters somehow
correspond to what you care about.Semisupervised Learning. You use both
unlabeled and labeled data to build a predictor. The unlabeled data influences
the learned predictor in some way.Active Learning. You have unlabeled data and
access to a labeling oracle. You interactively choose which examples to label
so as to optimize prediction accuracy.It seems there is a fourth approach
worth serious investigation--automated labeling. The approach goes as
follows:Identify some subset of observed values to predict from the
others.Build a predictor.U</p><p>6 0.53674179 <a title="161-lda-6" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>7 0.53290409 <a title="161-lda-7" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>8 0.53283179 <a title="161-lda-8" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>9 0.53280866 <a title="161-lda-9" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>10 0.53207856 <a title="161-lda-10" href="../hunch_net-2005/hunch_net-2005-02-17-Learning_Research_Programs.html">21 hunch net-2005-02-17-Learning Research Programs</a></p>
<p>11 0.53141123 <a title="161-lda-11" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>12 0.52966273 <a title="161-lda-12" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>13 0.52822018 <a title="161-lda-13" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">23 hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<p>14 0.52781397 <a title="161-lda-14" href="../hunch_net-2006/hunch_net-2006-05-05-An_ICML_reject.html">177 hunch net-2006-05-05-An ICML reject</a></p>
<p>15 0.52697498 <a title="161-lda-15" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>16 0.5264377 <a title="161-lda-16" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>17 0.52632248 <a title="161-lda-17" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>18 0.5261426 <a title="161-lda-18" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>19 0.52589017 <a title="161-lda-19" href="../hunch_net-2009/hunch_net-2009-02-22-Effective_Research_Funding.html">344 hunch net-2009-02-22-Effective Research Funding</a></p>
<p>20 0.52360362 <a title="161-lda-20" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
