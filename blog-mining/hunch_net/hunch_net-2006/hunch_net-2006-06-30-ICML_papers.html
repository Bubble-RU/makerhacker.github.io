<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>188 hunch net-2006-06-30-ICML papers</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-188" href="#">hunch_net-2006-188</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>188 hunch net-2006-06-30-ICML papers</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-188-html" href="http://hunch.net/?p=203">html</a></p><p>Introduction: Here are some ICML papers which interested me.Arindam Banerjeehad apaperwhich
notes that PAC-Bayes bounds, a core theorem in online learning, and the
optimality of Bayesian learning statements share a core inequality in their
proof.Pieter Abbeel,Morgan QuigleyandAndrew Y. Nghave apaperdiscussing RL
techniques for learning given a bad (but not too bad) model of the world.Nina
BalcanandAvrim Blumhave apaperwhich discusses how to learn given a similarity
function rather than a kernel. A similarity function requires less structure
than a kernel, implying that a learning algorithm using a similarity function
might be applied in situations where no effective kernel is evident.Nathan
Ratliff,Drew Bagnell, andMarty Zinkevichhave apaperdescribing an algorithm
which attempts to fuse A*path planning with learning of transition costs based
on human demonstration.Papers (2), (3), and (4), all seem like an initial pass
at solving interesting problems which push the domain in which learning is
applic</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('similarity', 0.408), ('apaperwhich', 0.382), ('function', 0.201), ('kernel', 0.189), ('core', 0.163), ('fuse', 0.157), ('multitrack', 0.157), ('discusses', 0.157), ('bad', 0.157), ('optimality', 0.148), ('bagnell', 0.142), ('papers', 0.14), ('transition', 0.136), ('push', 0.136), ('path', 0.136), ('inequality', 0.136), ('notes', 0.131), ('interested', 0.119), ('costs', 0.117), ('pass', 0.117)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="188-tfidf-1" href="../hunch_net-2006/hunch_net-2006-06-30-ICML_papers.html">188 hunch net-2006-06-30-ICML papers</a></p>
<p>Introduction: Here are some ICML papers which interested me.Arindam Banerjeehad apaperwhich
notes that PAC-Bayes bounds, a core theorem in online learning, and the
optimality of Bayesian learning statements share a core inequality in their
proof.Pieter Abbeel,Morgan QuigleyandAndrew Y. Nghave apaperdiscussing RL
techniques for learning given a bad (but not too bad) model of the world.Nina
BalcanandAvrim Blumhave apaperwhich discusses how to learn given a similarity
function rather than a kernel. A similarity function requires less structure
than a kernel, implying that a learning algorithm using a similarity function
might be applied in situations where no effective kernel is evident.Nathan
Ratliff,Drew Bagnell, andMarty Zinkevichhave apaperdescribing an algorithm
which attempts to fuse A*path planning with learning of transition costs based
on human demonstration.Papers (2), (3), and (4), all seem like an initial pass
at solving interesting problems which push the domain in which learning is
applic</p><p>2 0.11799119 <a title="188-tfidf-2" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>Introduction: TheInternational Planning Competition(IPC) is a biennial event organized in
the context of theInternational Conference on Automated Planning and
Scheduling(ICAPS). This year, for the first time, there will a learning track
of the competition. For more information you can go to the competitionweb-
site.The competitions are typically organized around a number of planning
domains that can vary from year to year, where a planning domain is simply a
class of problems that share a common action schema--e.g. Blocksworld is a
well-known planning domain that contains a problem instance each possible
initial tower configuration and goal configuration. Some other domains have
included Logistics, Airport, Freecell, PipesWorld, and manyothers. For each
domain the competition includes a number of problems (say 40-50) and the
planners are run on each problem with a time limit for each problem (around 30
minutes). The problems are hard enough that many problems are not solved
within the time limit.Giv</p><p>3 0.10412032 <a title="188-tfidf-3" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>Introduction: Normally, I don't indulge in posters forICML, but this year is naturally an
exception for me. If you want one, there are a small numberleft here, if you
sign up before February.It also seems worthwhile to give some sense of the
scope and reviewing criteria for ICML for authors considering submitting
papers. At ICML, the (very large) program committee does the reviewing which
informs final decisions by area chairs on most papers. Program chairs setup
the process, deal with exceptions or disagreements, and provide advice for the
reviewing process. Providing advice is tricky (and easily misleading) because
a conference is a community, and in the end the aggregate interests of the
community determine the conference. Nevertheless, as a program chair this year
it seems worthwhile to state the overall philosophy I have and what I plan to
encourage (and occasionally discourage).At the highest level, I believe ICML
exists to further research into machine learning, which I generally think of
as</p><p>4 0.09630347 <a title="188-tfidf-4" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>Introduction: Machine learning has a new kind of "scaling to larger problems" to worry
about: scaling with the amount of contextual information. The standard
development path for a machine learning application in practice seems to be
the following:Marginal. In the beginning, there was "majority vote". At this
stage, it isn't necessary to understand that you have a prediction problem.
People just realize that one answer is right sometimes and another answer
other times. In machine learning terms, this corresponds to making a
prediction without side information.First context. A clever person realizes
that some bit of informationx1could be helpful. Ifx1is discrete, they
condition on it and make a predictorh(x1), typically by counting. If they are
clever, then they also do some smoothing. Ifx1is some real valued parameter,
it's very common to make a threshold cutoff. Often, these tasks are simply
done by hand.Second. Another clever person (or perhaps the same one) realizes
that some other bit of informa</p><p>5 0.092003115 <a title="188-tfidf-5" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>Introduction: Attempts to abstract and study machine learning are within some given
framework or mathematical model. It turns out that all of these models are
significantly flawed for the purpose of studying machine learning. I've
created a table (below) outlining the major flaws in some common models of
machine learning.The point here is not simply "woe unto us". There are several
implications which seem important.The multitude of models is a point of
continuing confusion. It is common for people to learn about machine learning
within one framework which often becomes there "home framework" through which
they attempt to filter all machine learning. (Have you met people who can only
think in terms of kernels? Only via Bayes Law? Only via PAC Learning?)
Explicitly understanding the existence of these other frameworks can help
resolve the confusion. This is particularly important when reviewing and
particularly important for students.Algorithms which conform to multiple
approaches can have substantial</p><p>6 0.085819535 <a title="188-tfidf-6" href="../hunch_net-2005/hunch_net-2005-02-01-NIPS%3A_Online_Bayes.html">8 hunch net-2005-02-01-NIPS: Online Bayes</a></p>
<p>7 0.083342925 <a title="188-tfidf-7" href="../hunch_net-2005/hunch_net-2005-08-08-Apprenticeship_Reinforcement_Learning_for_Control.html">101 hunch net-2005-08-08-Apprenticeship Reinforcement Learning for Control</a></p>
<p>8 0.081889212 <a title="188-tfidf-8" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>9 0.081683241 <a title="188-tfidf-9" href="../hunch_net-2009/hunch_net-2009-12-27-Interesting_things_at_NIPS_2009.html">385 hunch net-2009-12-27-Interesting things at NIPS 2009</a></p>
<p>10 0.081115879 <a title="188-tfidf-10" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>11 0.078273878 <a title="188-tfidf-11" href="../hunch_net-2006/hunch_net-2006-01-06-MLTV.html">146 hunch net-2006-01-06-MLTV</a></p>
<p>12 0.075433195 <a title="188-tfidf-12" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>13 0.074858814 <a title="188-tfidf-13" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>14 0.074605465 <a title="188-tfidf-14" href="../hunch_net-2009/hunch_net-2009-01-07-Interesting_Papers_at_SODA_2009.html">334 hunch net-2009-01-07-Interesting Papers at SODA 2009</a></p>
<p>15 0.072229162 <a title="188-tfidf-15" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>16 0.071148813 <a title="188-tfidf-16" href="../hunch_net-2007/hunch_net-2007-02-16-The_Forgetting.html">233 hunch net-2007-02-16-The Forgetting</a></p>
<p>17 0.07032378 <a title="188-tfidf-17" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>18 0.069949746 <a title="188-tfidf-18" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>19 0.069572173 <a title="188-tfidf-19" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>20 0.068876341 <a title="188-tfidf-20" href="../hunch_net-2005/hunch_net-2005-01-24-The_Humanloop_Spectrum_of_Machine_Learning.html">3 hunch net-2005-01-24-The Humanloop Spectrum of Machine Learning</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.178), (1, -0.007), (2, -0.023), (3, 0.036), (4, -0.068), (5, 0.014), (6, -0.068), (7, -0.037), (8, -0.021), (9, 0.011), (10, 0.006), (11, -0.024), (12, -0.012), (13, 0.011), (14, -0.02), (15, 0.064), (16, 0.047), (17, -0.061), (18, 0.008), (19, 0.002), (20, 0.001), (21, -0.011), (22, -0.062), (23, -0.056), (24, -0.038), (25, -0.083), (26, 0.016), (27, 0.016), (28, 0.002), (29, -0.014), (30, 0.017), (31, 0.038), (32, -0.021), (33, 0.014), (34, 0.01), (35, 0.073), (36, 0.0), (37, 0.093), (38, -0.003), (39, -0.004), (40, -0.063), (41, -0.033), (42, -0.003), (43, 0.09), (44, 0.065), (45, -0.019), (46, -0.018), (47, -0.049), (48, -0.013), (49, -0.008)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.93059522 <a title="188-lsi-1" href="../hunch_net-2006/hunch_net-2006-06-30-ICML_papers.html">188 hunch net-2006-06-30-ICML papers</a></p>
<p>Introduction: Here are some ICML papers which interested me.Arindam Banerjeehad apaperwhich
notes that PAC-Bayes bounds, a core theorem in online learning, and the
optimality of Bayesian learning statements share a core inequality in their
proof.Pieter Abbeel,Morgan QuigleyandAndrew Y. Nghave apaperdiscussing RL
techniques for learning given a bad (but not too bad) model of the world.Nina
BalcanandAvrim Blumhave apaperwhich discusses how to learn given a similarity
function rather than a kernel. A similarity function requires less structure
than a kernel, implying that a learning algorithm using a similarity function
might be applied in situations where no effective kernel is evident.Nathan
Ratliff,Drew Bagnell, andMarty Zinkevichhave apaperdescribing an algorithm
which attempts to fuse A*path planning with learning of transition costs based
on human demonstration.Papers (2), (3), and (4), all seem like an initial pass
at solving interesting problems which push the domain in which learning is
applic</p><p>2 0.68742222 <a title="188-lsi-2" href="../hunch_net-2005/hunch_net-2005-03-08-Fast_Physics_for_Learning.html">37 hunch net-2005-03-08-Fast Physics for Learning</a></p>
<p>Introduction: While everyone is silently working on ICML submissions, I found this
discussion about afast physics simulatorchip interesting from a learning
viewpoint. In many cases, learning attempts to predict the outcome of physical
processes. Access to a fast simulator for these processes might be quite
helpful in predicting the outcome. Bayesian learning in particular may
directly benefit while many other algorithms (like support vector machines)
might have their speed greatly increased.The biggest drawback is that writing
software for these odd architectures is always difficult and time consuming,
but a several-orders-of-magnitude speedup might make that worthwhile.</p><p>3 0.61899531 <a title="188-lsi-3" href="../hunch_net-2007/hunch_net-2007-12-20-Cool_and_Interesting_things_at_NIPS%2C_take_three.html">280 hunch net-2007-12-20-Cool and Interesting things at NIPS, take three</a></p>
<p>Introduction: Following up on Hal Daume's post and John's post on cool and interesting
things seen at NIPS I'll post my own little list of neat papers here as well.
Of course it's going to be biased towards what I think is interesting. Also, I
have to say that I hadn't been able to see many papers this year at nips due
to myself being too busy, so please feel free to contribute the papers that
you liked1. P. Mudigonda, V. Kolmogorov, P. Torr. An Analysis of Convex
Relaxations for MAP Estimation. A surprising paper which shows that many of
the more sophisticated convex relaxations that had been proposed recently
turns out to be subsumed by the simplest LP relaxation. Be careful next time
you try a cool new convex relaxation!2. D. Sontag, T. Jaakkola. New Outer
Bounds on the Marginal Polytope. The title says it all. The marginal polytope
is the set of local marginal distributions over subsets of variables that are
globally consistent in the sense that there is at least one distribution over
all the va</p><p>4 0.60203755 <a title="188-lsi-4" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>Introduction: The papers which interested me most atICMLandCOLT2010 were:Thomas
Walsh,Kaushik Subramanian,Michael LittmanandCarlos DiukGeneralizing
Apprenticeship Learning across Hypothesis Classes. This paper formalizes and
provides algorithms with guarantees for mixed-mode apprenticeship and
traditional reinforcement learning algorithms, allowing RL algorithms that
perform better than for either setting alone.István SzitaandCsaba
SzepesváriModel-based reinforcement learning with nearly tight exploration
complexity bounds. This paper andanotherrepresent the frontier of best-known
algorithm for Reinforcement Learning in a Markov Decision Process.James
MartensDeep learning via Hessian-free optimization. About a new not-quite-
online second order gradient algorithm for learning deep functional
structures. Potentially this is very powerful because while people have often
talked about end-to-end learning, it has rarely worked in practice.Chrisoph
Sawade,Niels Landwehr,Steffen Bickel. andTobias SchefferA</p><p>5 0.5934909 <a title="188-lsi-5" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>Introduction: Normally, I don't indulge in posters forICML, but this year is naturally an
exception for me. If you want one, there are a small numberleft here, if you
sign up before February.It also seems worthwhile to give some sense of the
scope and reviewing criteria for ICML for authors considering submitting
papers. At ICML, the (very large) program committee does the reviewing which
informs final decisions by area chairs on most papers. Program chairs setup
the process, deal with exceptions or disagreements, and provide advice for the
reviewing process. Providing advice is tricky (and easily misleading) because
a conference is a community, and in the end the aggregate interests of the
community determine the conference. Nevertheless, as a program chair this year
it seems worthwhile to state the overall philosophy I have and what I plan to
encourage (and occasionally discourage).At the highest level, I believe ICML
exists to further research into machine learning, which I generally think of
as</p><p>6 0.59015381 <a title="188-lsi-6" href="../hunch_net-2009/hunch_net-2009-08-26-Another_10-year_paper_in_Machine_Learning.html">368 hunch net-2009-08-26-Another 10-year paper in Machine Learning</a></p>
<p>7 0.5883103 <a title="188-lsi-7" href="../hunch_net-2005/hunch_net-2005-12-14-More_NIPS_Papers_II.html">140 hunch net-2005-12-14-More NIPS Papers II</a></p>
<p>8 0.57732451 <a title="188-lsi-8" href="../hunch_net-2005/hunch_net-2005-07-23-Interesting_papers_at_ACL.html">97 hunch net-2005-07-23-Interesting papers at ACL</a></p>
<p>9 0.57556915 <a title="188-lsi-9" href="../hunch_net-2005/hunch_net-2005-08-08-Apprenticeship_Reinforcement_Learning_for_Control.html">101 hunch net-2005-08-08-Apprenticeship Reinforcement Learning for Control</a></p>
<p>10 0.56147027 <a title="188-lsi-10" href="../hunch_net-2007/hunch_net-2007-07-20-Motivation_should_be_the_Responsibility_of_the_Reviewer.html">256 hunch net-2007-07-20-Motivation should be the Responsibility of the Reviewer</a></p>
<p>11 0.55959088 <a title="188-lsi-11" href="../hunch_net-2009/hunch_net-2009-06-24-Interesting_papers_at_UAICMOLT_2009.html">361 hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</a></p>
<p>12 0.55336285 <a title="188-lsi-12" href="../hunch_net-2008/hunch_net-2008-11-10-ICML_Reviewing_Criteria.html">325 hunch net-2008-11-10-ICML Reviewing Criteria</a></p>
<p>13 0.54852605 <a title="188-lsi-13" href="../hunch_net-2009/hunch_net-2009-12-27-Interesting_things_at_NIPS_2009.html">385 hunch net-2009-12-27-Interesting things at NIPS 2009</a></p>
<p>14 0.54790401 <a title="188-lsi-14" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>15 0.54777938 <a title="188-lsi-15" href="../hunch_net-2009/hunch_net-2009-01-07-Interesting_Papers_at_SODA_2009.html">334 hunch net-2009-01-07-Interesting Papers at SODA 2009</a></p>
<p>16 0.54488504 <a title="188-lsi-16" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>17 0.54233479 <a title="188-lsi-17" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>18 0.53705889 <a title="188-lsi-18" href="../hunch_net-2011/hunch_net-2011-08-06-Interesting_thing_at_UAI_2011.html">440 hunch net-2011-08-06-Interesting thing at UAI 2011</a></p>
<p>19 0.53638345 <a title="188-lsi-19" href="../hunch_net-2005/hunch_net-2005-12-09-Some_NIPS_papers.html">138 hunch net-2005-12-09-Some NIPS papers</a></p>
<p>20 0.53456658 <a title="188-lsi-20" href="../hunch_net-2006/hunch_net-2006-07-05-more_icml_papers.html">189 hunch net-2006-07-05-more icml papers</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(39, 0.412), (42, 0.248), (45, 0.089), (74, 0.128)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.97325754 <a title="188-lda-1" href="../hunch_net-2006/hunch_net-2006-04-17-Rexa_is_live.html">173 hunch net-2006-04-17-Rexa is live</a></p>
<p>Introduction: Rexais now publicly available. Anyone can create an account and login.Rexa is
similar toCiteseerandGoogle Scholarin functionality with more emphasis on the
use of machine learning for intelligent information extraction. For example,
Rexa can automatically display a picture on an author's homepage when the
author is searched for.</p><p>2 0.9587329 <a title="188-lda-2" href="../hunch_net-2007/hunch_net-2007-07-13-The_View_From_China.html">255 hunch net-2007-07-13-The View From China</a></p>
<p>Introduction: I'm visiting Beijing for thePao-Lu Hsu Statistics Conferenceon Machine
Learning.I had several discussions about the state of Chinese research. Given
the large population and economy, you might expect substantial research--more
than has been observed at international conferences. The fundamental problem
seems to be theCultural Revolutionwhich lobotimized higher education, and the
research associated with it. There has been a process of slow recovery since
then, which has begun to be felt in the research world via increased
participation in international conferences and (now) conferences in China.The
amount of effort going into construction in Beijing is very impressive--people
are literally building a skyscraper at night outside the window of the hotel
I'm staying at (and this is not unusual). If a small fraction of this effort
is later focused onto supporting research, the effect could be very
substantial. General growth in China's research portfolio should be expected.</p><p>3 0.94303489 <a title="188-lda-3" href="../hunch_net-2005/hunch_net-2005-02-10-Conferences%2C_Dates%2C_Locations.html">17 hunch net-2005-02-10-Conferences, Dates, Locations</a></p>
<p>Introduction: ConferenceLocateDateCOLTBertinoro, ItalyJune 27-30AAAIPittsburgh, PA, USAJuly
9-13UAIEdinburgh, ScotlandJuly 26-29IJCAIEdinburgh, ScotlandJuly 30 - August
5ICMLBonn, GermanyAugust 7-11KDDChicago, IL, USAAugust 21-24The big winner
this year is Europe. This is partly a coincidence, and partly due to the
general internationalization of science over the last few years. Withcuts to
basic sciencein the US and increased hassle for visitors, conferences outside
the US become more attractive. Europe and Australia/New Zealand are the
immediate winners because they have the science, infrastructure, and english
in place. China and India are possible future winners.</p><p>4 0.93691903 <a title="188-lda-4" href="../hunch_net-2005/hunch_net-2005-05-14-NIPS.html">71 hunch net-2005-05-14-NIPS</a></p>
<p>Introduction: NIPSis the big winter conference of learning.Paper due date: June 3rd.
(Tweaked thanks toFei Sha.)Location: Vancouver (main program) Dec. 5-8 and
Whistler (workshops) Dec 9-10, BC, CanadaNIPS is larger than all of the other
learning conferences, partly because it's the only one at that time of year. I
recommend the workshops which are often quite interesting and energetic.</p><p>5 0.91301769 <a title="188-lda-5" href="../hunch_net-2005/hunch_net-2005-12-11-More_NIPS_Papers.html">139 hunch net-2005-12-11-More NIPS Papers</a></p>
<p>Introduction: Let me add to John's post with a few of my own favouritesfrom this year's
conference. First, let me say thatSanjoy's talk,Coarse Sample Complexity
Bounds for ActiveLearningwas also one of my favourites, as was theForgettron
paper.I also really enjoyed the last third ofChristos'talkon the complexity of
finding Nash equilibria.And, speaking of tagging, I thinkthe U.Mass Citeseer
replacement systemRexafrom the demo track is very cool.Finally, let me add my
recommendations for specific papers:Z. Ghahramani, K. Heller:Bayesian Sets[no
preprint](A very elegant probabilistic information retrieval style modelof
which objects are "most like" a given subset of objects.)T. Griffiths, Z.
Ghahramani:Infinite Latent Feature Models andthe Indian Buffet
Process[preprint](A Dirichlet style prior over infinite binary matrices
withbeautiful exchangeability properties.)K. Weinberger, J. Blitzer, L.
Saul:Distance Metric Learning forLarge Margin Nearest Neighbor
Classification[preprint](A nice idea about ho</p><p>6 0.84473872 <a title="188-lda-6" href="../hunch_net-2011/hunch_net-2011-04-18-A_paper_not_at_Snowbird.html">431 hunch net-2011-04-18-A paper not at Snowbird</a></p>
<p>7 0.82613051 <a title="188-lda-7" href="../hunch_net-2006/hunch_net-2006-01-25-1_year.html">151 hunch net-2006-01-25-1 year</a></p>
<p>same-blog 8 0.79707712 <a title="188-lda-8" href="../hunch_net-2006/hunch_net-2006-06-30-ICML_papers.html">188 hunch net-2006-06-30-ICML papers</a></p>
<p>9 0.68404114 <a title="188-lda-9" href="../hunch_net-2006/hunch_net-2006-09-12-Incentive_Compatible_Reviewing.html">207 hunch net-2006-09-12-Incentive Compatible Reviewing</a></p>
<p>10 0.59765261 <a title="188-lda-10" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>11 0.57175946 <a title="188-lda-11" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>12 0.57131475 <a title="188-lda-12" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>13 0.57049763 <a title="188-lda-13" href="../hunch_net-2010/hunch_net-2010-04-24-COLT_Treasurer_is_now_Phil_Long.html">394 hunch net-2010-04-24-COLT Treasurer is now Phil Long</a></p>
<p>14 0.5683645 <a title="188-lda-14" href="../hunch_net-2005/hunch_net-2005-02-20-At_One_Month.html">25 hunch net-2005-02-20-At One Month</a></p>
<p>15 0.56111646 <a title="188-lda-15" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>16 0.5571546 <a title="188-lda-16" href="../hunch_net-2005/hunch_net-2005-02-15-ESPgame_and_image_labeling.html">20 hunch net-2005-02-15-ESPgame and image labeling</a></p>
<p>17 0.55512065 <a title="188-lda-17" href="../hunch_net-2005/hunch_net-2005-12-07-Is_the_Google_way_the_way_for_machine_learning%3F.html">136 hunch net-2005-12-07-Is the Google way the way for machine learning?</a></p>
<p>18 0.5541212 <a title="188-lda-18" href="../hunch_net-2009/hunch_net-2009-12-27-Interesting_things_at_NIPS_2009.html">385 hunch net-2009-12-27-Interesting things at NIPS 2009</a></p>
<p>19 0.55247241 <a title="188-lda-19" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>20 0.55238938 <a title="188-lda-20" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
