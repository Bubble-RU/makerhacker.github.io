<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>177 hunch net-2006-05-05-An ICML reject</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-177" href="#">hunch_net-2006-177</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>177 hunch net-2006-05-05-An ICML reject</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-177-html" href="http://hunch.net/?p=188">html</a></p><p>Introduction: Hal,Daniel, and I have been working on the algorithmSearnfor structured
prediction. This was just conditionally accepted and then rejected from ICML,
and we were quite surprised. By any reasonable criteria, it seems this is an
interesting algorithm.Prediction Performance: Searn performed better than any
other algorithm on all the problems we tested against using the same feature
set. This is true even using the numbers reported by authors in their
papers.Theoretical underpinning. Searn is a reduction which comes with a
reduction guarantee: the good performance on a base classifiers implies good
performance for the overall system. No other theorem of this type has been
made for other structured prediction algorithms, as far as we know.Speed.
Searn has no problem handling much larger datasets than other algorithms we
tested against.Simplicity. Given code for a binary classifier and a problem-
specific search algorithm, only a few tens of lines are necessary to implement
Searn.Generality.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Prediction Performance: Searn performed better than any other algorithm on all the problems we tested against using the same feature set. [sent-4, score-0.194]
</p><p>2 Searn is a reduction which comes with a reduction guarantee: the good performance on a base classifiers implies good performance for the overall system. [sent-7, score-0.677]
</p><p>3 It can use (and cope with) arbitrary loss functions over the data. [sent-15, score-0.185]
</p><p>4 A very typical (although often unstated) tradeoff is expending extra computation to gain better predictive performance in practice. [sent-18, score-0.285]
</p><p>5 One endorsement of this comes from a reviewer who saidIn the end, I do think there is something there, but I think its introduction should have been like that for CRF. [sent-26, score-0.172]
</p><p>6 The SPC stated:The results, though, which essentially show that good local classifiers imply good global performance, are not that significant, and hold for other approaches that use local classifiers as building blocks. [sent-30, score-1.088]
</p><p>7 After all, perfect local classifiers provide perfect local accuracy, and therefore provide perfect global accuracy, and again provide perfect global loss of any kind. [sent-31, score-2.452]
</p><p>8 Both sentences are simply false in the setting we consider. [sent-32, score-0.198]
</p><p>9 In particular, no other algorithms appear to have a good local performance to global performance guarantee for general global loss functions. [sent-33, score-1.489]
</p><p>10 Furthermore, it isnotthe case that perfect local performance implies perfect global performance except (perhaps) in a noise free world. [sent-34, score-1.528]
</p><p>11 Most of us believe that the problems we address typically contain fundamental ambiguities and noise (that was certainly our mathematical model). [sent-35, score-0.193]
</p><p>12 It's easy to setup a (noisy) distribution over inputs+loss such that best- possible-up-to-the-noise-limit local predictors are globally suboptimal. [sent-36, score-0.241]
</p><p>13 The SPC wanted us to contrast withMichael Collins, Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms, EMNLP02. [sent-37, score-0.189]
</p><p>14 I believe any reasonable reading of these and the Searn paper will find the ACL04 paper superceeds the EMNLP02 paper in relevance. [sent-39, score-0.207]
</p><p>15 IBT is a modification of the earlier algorithm which integrates global information (in the form of problem specificconstraints) into the local training process yielding performance gains. [sent-50, score-0.908]
</p><p>16 Searn is made to cope with global loss functions rather than global constraints. [sent-52, score-0.843]
</p><p>17 For whatever reason, it is psychologically difficult to build on rejected work. [sent-57, score-0.187]
</p><p>18 If you think something is true but aren't sure, it is appropriate to say "I think â&euro;Ś" rather than simply asserting it as a fact. [sent-65, score-0.376]
</p><p>19 If there are no comments or simply comments about Searn, that's fine. [sent-68, score-0.2]
</p><p>20 This post violates a standard: avoiding talking about specific papers the poster has been working on. [sent-73, score-0.178]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('searn', 0.438), ('global', 0.3), ('local', 0.241), ('perfect', 0.223), ('performance', 0.209), ('spc', 0.195), ('classifiers', 0.153), ('collins', 0.118), ('ibt', 0.118), ('punyakanok', 0.118), ('roth', 0.118), ('false', 0.112), ('loss', 0.106), ('asserting', 0.105), ('withmichael', 0.105), ('rejected', 0.1), ('modification', 0.097), ('structured', 0.093), ('build', 0.087), ('simply', 0.086), ('perceptron', 0.081), ('cope', 0.079), ('tradeoff', 0.076), ('tested', 0.074), ('provide', 0.073), ('accuracy', 0.069), ('paper', 0.069), ('us', 0.069), ('true', 0.067), ('noise', 0.065), ('post', 0.063), ('guarantee', 0.063), ('stated', 0.062), ('wanted', 0.061), ('algorithm', 0.061), ('algorithms', 0.061), ('problems', 0.059), ('contrast', 0.059), ('think', 0.059), ('weak', 0.059), ('experiments', 0.059), ('case', 0.058), ('made', 0.058), ('papers', 0.058), ('comments', 0.057), ('specific', 0.057), ('looking', 0.055), ('reviewer', 0.054), ('reduction', 0.053), ('code', 0.053)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="177-tfidf-1" href="../hunch_net-2006/hunch_net-2006-05-05-An_ICML_reject.html">177 hunch net-2006-05-05-An ICML reject</a></p>
<p>Introduction: Hal,Daniel, and I have been working on the algorithmSearnfor structured
prediction. This was just conditionally accepted and then rejected from ICML,
and we were quite surprised. By any reasonable criteria, it seems this is an
interesting algorithm.Prediction Performance: Searn performed better than any
other algorithm on all the problems we tested against using the same feature
set. This is true even using the numbers reported by authors in their
papers.Theoretical underpinning. Searn is a reduction which comes with a
reduction guarantee: the good performance on a base classifiers implies good
performance for the overall system. No other theorem of this type has been
made for other structured prediction algorithms, as far as we know.Speed.
Searn has no problem handling much larger datasets than other algorithms we
tested against.Simplicity. Given code for a binary classifier and a problem-
specific search algorithm, only a few tens of lines are necessary to implement
Searn.Generality.</p><p>2 0.16623555 <a title="177-tfidf-2" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>Introduction: This post is partly meant as an advertisement for thereductions
tutorialAlina,Bianca, and I are planning to do atICML. Please come, if you are
interested.Many research programs can be thought of as finding and building
new useful abstractions. The running example I'll use islearning
reductionswhere I have experience. The basic abstraction here is that we can
build a learning algorithm capable of solving classification problems up to a
small expected regret. This is used repeatedly to solve more complex
problems.In working on a new abstraction, I think you typically run into many
substantial problems of understanding, which make publishing particularly
difficult.It is difficult to seriously discuss the reason behind or mechanism
for abstraction in a conference paper with small page limits. People rarely
see such discussions and hence have little basis on which to think about new
abstractions. Another difficulty is that when building an abstraction, you
often don't know the right way to</p><p>3 0.15092729 <a title="177-tfidf-3" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>Introduction: "Overfitting" is traditionally defined as training some flexible
representation so that it memorizes the data but fails to predict well in the
future. For this post, I will define overfitting more generally as over-
representing the performance of systems. There are two styles of general
overfitting: overrepresenting performance on particular datasets and
(implicitly) overrepresenting performance of a method on future datasets.We
should all be aware of these methods, avoid them where possible, and take them
into account otherwise. I have used "reproblem" and "old datasets", and may
have participated in "overfitting by review"--some of these are very difficult
to avoid.NameMethodExplanationRemedyTraditional overfittingTrain a complex
predictor on too-few examples.Hold out pristine examples for testing.Use a
simpler predictor.Get more training examples.Integrate over many
predictors.Reject papers which do this.Parameter tweak overfittingUse a
learning algorithm with many parameters. Choo</p><p>4 0.14056812 <a title="177-tfidf-4" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>Introduction: Few would mistake the process of academic paper review for a fair process, but
sometimes the unfairness seems particularly striking. This is most easily seen
by comparison:PaperBanditronOffset TreeNotesProblem ScopeMulticlass problems
where only the loss of one choice can be probed.Strictly greater: Cost
sensitive multiclass problems where only the loss of one choice can be
probed.Often generalizations don't matter. That's not the case here, since
every plausible application I've thought of involves loss functions
substantially different from 0/1.What's newAnalysis and ExperimentsAlgorithm,
Analysis, and ExperimentsAs far as I know, the essence of the more general
problem was first stated and analyzed with theEXP4 algorithm (page 16)(1998).
It's also the time horizon 1 simplification of the Reinforcement Learning
setting for therandom trajectory method (page 15)(2002). The Banditron
algorithm itself is functionally identical toOne-Step RL with Traces (page
122)(2003) inBianca's thesis</p><p>5 0.1307226 <a title="177-tfidf-5" href="../hunch_net-2005/hunch_net-2005-06-29-Not_EM_for_clustering_at_COLT.html">87 hunch net-2005-06-29-Not EM for clustering at COLT</a></p>
<p>Introduction: One standard approach for clustering data with a set of gaussians is using EM.
Roughly speaking, you pick a set ofkrandom guassians and then use alternating
expectation maximization to (hopefully) find a set of guassians that "explain"
the data well. This process is difficult to work with because EM can become
"stuck" in local optima. There are various hacks like "rerun withtdifferent
random starting points".One cool observation is that this can often be solved
via other algorithm which donotsuffer from local optima. This is an
earlypaperwhich shows this. Ravi Kannan presented anew papershowing this is
possible in a much more adaptive setting.A very rough summary of these papers
is that by projecting into a lower dimensional space, it is computationally
tractable to pick out the gross structure of the data. It is unclear how well
these algorithms work in practice, but they might be effective, especially if
used as a subroutine of the form:Project to low dimensional space.Pick out
gross</p><p>6 0.12698135 <a title="177-tfidf-6" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>7 0.12337886 <a title="177-tfidf-7" href="../hunch_net-2005/hunch_net-2005-06-08-Question%3A_%26%238220%3BWhen_is_the_right_time_to_insert_the_loss_function%3F%26%238221%3B.html">79 hunch net-2005-06-08-Question: &#8220;When is the right time to insert the loss function?&#8221;</a></p>
<p>8 0.11780314 <a title="177-tfidf-8" href="../hunch_net-2005/hunch_net-2005-03-18-Binomial_Weighting.html">43 hunch net-2005-03-18-Binomial Weighting</a></p>
<p>9 0.11680833 <a title="177-tfidf-9" href="../hunch_net-2010/hunch_net-2010-12-26-NIPS_2010.html">420 hunch net-2010-12-26-NIPS 2010</a></p>
<p>10 0.11564527 <a title="177-tfidf-10" href="../hunch_net-2005/hunch_net-2005-04-04-Grounds_for_Rejection.html">52 hunch net-2005-04-04-Grounds for Rejection</a></p>
<p>11 0.11354899 <a title="177-tfidf-11" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>12 0.11224067 <a title="177-tfidf-12" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>13 0.11158703 <a title="177-tfidf-13" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>14 0.1114581 <a title="177-tfidf-14" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>15 0.11076652 <a title="177-tfidf-15" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>16 0.10986143 <a title="177-tfidf-16" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>17 0.10958079 <a title="177-tfidf-17" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<p>18 0.10920129 <a title="177-tfidf-18" href="../hunch_net-2005/hunch_net-2005-02-01-Watchword%3A_Loss.html">9 hunch net-2005-02-01-Watchword: Loss</a></p>
<p>19 0.1049979 <a title="177-tfidf-19" href="../hunch_net-2010/hunch_net-2010-04-24-COLT_Treasurer_is_now_Phil_Long.html">394 hunch net-2010-04-24-COLT Treasurer is now Phil Long</a></p>
<p>20 0.10485256 <a title="177-tfidf-20" href="../hunch_net-2005/hunch_net-2005-11-16-The_Everything_Ensemble_Edge.html">131 hunch net-2005-11-16-The Everything Ensemble Edge</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.287), (1, -0.031), (2, -0.105), (3, -0.012), (4, 0.055), (5, 0.017), (6, 0.052), (7, -0.014), (8, 0.01), (9, 0.035), (10, -0.0), (11, -0.02), (12, -0.075), (13, 0.039), (14, -0.0), (15, 0.043), (16, 0.039), (17, -0.088), (18, -0.008), (19, 0.007), (20, 0.054), (21, -0.024), (22, 0.11), (23, -0.039), (24, 0.032), (25, 0.04), (26, 0.01), (27, 0.02), (28, 0.027), (29, 0.006), (30, -0.03), (31, 0.101), (32, -0.091), (33, 0.095), (34, 0.027), (35, 0.029), (36, 0.017), (37, -0.017), (38, 0.047), (39, -0.119), (40, -0.048), (41, -0.021), (42, -0.049), (43, 0.013), (44, -0.022), (45, -0.03), (46, 0.054), (47, 0.009), (48, -0.005), (49, -0.027)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97378498 <a title="177-lsi-1" href="../hunch_net-2006/hunch_net-2006-05-05-An_ICML_reject.html">177 hunch net-2006-05-05-An ICML reject</a></p>
<p>Introduction: Hal,Daniel, and I have been working on the algorithmSearnfor structured
prediction. This was just conditionally accepted and then rejected from ICML,
and we were quite surprised. By any reasonable criteria, it seems this is an
interesting algorithm.Prediction Performance: Searn performed better than any
other algorithm on all the problems we tested against using the same feature
set. This is true even using the numbers reported by authors in their
papers.Theoretical underpinning. Searn is a reduction which comes with a
reduction guarantee: the good performance on a base classifiers implies good
performance for the overall system. No other theorem of this type has been
made for other structured prediction algorithms, as far as we know.Speed.
Searn has no problem handling much larger datasets than other algorithms we
tested against.Simplicity. Given code for a binary classifier and a problem-
specific search algorithm, only a few tens of lines are necessary to implement
Searn.Generality.</p><p>2 0.72531539 <a title="177-lsi-2" href="../hunch_net-2005/hunch_net-2005-02-12-ROC_vs._Accuracy_vs._AROC.html">18 hunch net-2005-02-12-ROC vs. Accuracy vs. AROC</a></p>
<p>Introduction: Foster Provostand I discussed the merits of ROC curves vs. accuracy
estimation. Here is a quick summary of our discussion.The "Receiver Operating
Characteristic" (ROC) curve is an alternative to accuracy for the evaluation
of learning algorithms on natural datasets. The ROC curve is acurveand not a
single number statistic. In particular, this means that the comparison of two
algorithms on a dataset does not always produce an obvious order.Accuracy (= 1
- error rate) is a standard method used to evaluate learning algorithms. It is
a single-number summary of performance.AROC is the area under the ROC curve.
It is a single number summary of performance.The comparison of these metrics
is a subtle affair, because in machine learning, they are compared on
different natural datasets. This makes some sense if we accept the hypothesis
"Performance on past learning problems (roughly) predicts performance on
future learning problems."The ROC vs. accuracy discussion is often conflated
with "is the</p><p>3 0.64451152 <a title="177-lsi-3" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>Introduction: One of the confusing things about research is that progress is very hard to
measure. One of the consequences of being in a hard-to-measure environment is
that the wrong things are often measured.Lines of CodeThe classical example of
this phenomenon is the old lines-of-code-produced metric for programming. It
is easy to imagine systems for producing many lines of code with very little
work that accomplish very little.Paper countIn academia, a "paper count" is an
analog of "lines of code", and it suffers from the same failure modes. The
obvious failure mode here is that we end up with a large number of
uninteresting papers since people end up spending a lot of time optimizing
this metric.ComplexityAnother metric, is "complexity" (in the eye of a
reviewer) of a paper. There is a common temptation to make a method appear
more complex than it is in order for reviewers to judge it worthy of
publication. The failure mode here is unclean thinking. Simple effective
methods are often overlooked</p><p>4 0.6345799 <a title="177-lsi-4" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>Introduction: "Overfitting" is traditionally defined as training some flexible
representation so that it memorizes the data but fails to predict well in the
future. For this post, I will define overfitting more generally as over-
representing the performance of systems. There are two styles of general
overfitting: overrepresenting performance on particular datasets and
(implicitly) overrepresenting performance of a method on future datasets.We
should all be aware of these methods, avoid them where possible, and take them
into account otherwise. I have used "reproblem" and "old datasets", and may
have participated in "overfitting by review"--some of these are very difficult
to avoid.NameMethodExplanationRemedyTraditional overfittingTrain a complex
predictor on too-few examples.Hold out pristine examples for testing.Use a
simpler predictor.Get more training examples.Integrate over many
predictors.Reject papers which do this.Parameter tweak overfittingUse a
learning algorithm with many parameters. Choo</p><p>5 0.62186736 <a title="177-lsi-5" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>Introduction: Normally, I don't indulge in posters forICML, but this year is naturally an
exception for me. If you want one, there are a small numberleft here, if you
sign up before February.It also seems worthwhile to give some sense of the
scope and reviewing criteria for ICML for authors considering submitting
papers. At ICML, the (very large) program committee does the reviewing which
informs final decisions by area chairs on most papers. Program chairs setup
the process, deal with exceptions or disagreements, and provide advice for the
reviewing process. Providing advice is tricky (and easily misleading) because
a conference is a community, and in the end the aggregate interests of the
community determine the conference. Nevertheless, as a program chair this year
it seems worthwhile to state the overall philosophy I have and what I plan to
encourage (and occasionally discourage).At the highest level, I believe ICML
exists to further research into machine learning, which I generally think of
as</p><p>6 0.60115331 <a title="177-lsi-6" href="../hunch_net-2005/hunch_net-2005-04-04-Grounds_for_Rejection.html">52 hunch net-2005-04-04-Grounds for Rejection</a></p>
<p>7 0.59966671 <a title="177-lsi-7" href="../hunch_net-2005/hunch_net-2005-06-29-Not_EM_for_clustering_at_COLT.html">87 hunch net-2005-06-29-Not EM for clustering at COLT</a></p>
<p>8 0.591515 <a title="177-lsi-8" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>9 0.58233386 <a title="177-lsi-9" href="../hunch_net-2005/hunch_net-2005-11-16-The_Everything_Ensemble_Edge.html">131 hunch net-2005-11-16-The Everything Ensemble Edge</a></p>
<p>10 0.57716513 <a title="177-lsi-10" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>11 0.57244384 <a title="177-lsi-11" href="../hunch_net-2006/hunch_net-2006-12-04-Structural_Problems_in_NIPS_Decision_Making.html">221 hunch net-2006-12-04-Structural Problems in NIPS Decision Making</a></p>
<p>12 0.56687355 <a title="177-lsi-12" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>13 0.56014895 <a title="177-lsi-13" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<p>14 0.5591141 <a title="177-lsi-14" href="../hunch_net-2007/hunch_net-2007-04-13-What_to_do_with_an_unreasonable_conditional_accept.html">238 hunch net-2007-04-13-What to do with an unreasonable conditional accept</a></p>
<p>15 0.55497777 <a title="177-lsi-15" href="../hunch_net-2005/hunch_net-2005-05-29-Maximum_Margin_Mismatch%3F.html">77 hunch net-2005-05-29-Maximum Margin Mismatch?</a></p>
<p>16 0.55161035 <a title="177-lsi-16" href="../hunch_net-2005/hunch_net-2005-10-26-Fallback_Analysis_is_a_Secret_to_Useful_Algorithms.html">126 hunch net-2005-10-26-Fallback Analysis is a Secret to Useful Algorithms</a></p>
<p>17 0.54547811 <a title="177-lsi-17" href="../hunch_net-2005/hunch_net-2005-11-05-The_design_of_a_computing_cluster.html">128 hunch net-2005-11-05-The design of a computing cluster</a></p>
<p>18 0.54430389 <a title="177-lsi-18" href="../hunch_net-2007/hunch_net-2007-12-20-Cool_and_Interesting_things_at_NIPS%2C_take_three.html">280 hunch net-2007-12-20-Cool and Interesting things at NIPS, take three</a></p>
<p>19 0.53962022 <a title="177-lsi-19" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>20 0.53839028 <a title="177-lsi-20" href="../hunch_net-2008/hunch_net-2008-11-10-ICML_Reviewing_Criteria.html">325 hunch net-2008-11-10-ICML Reviewing Criteria</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(12, 0.178), (35, 0.088), (42, 0.264), (45, 0.035), (56, 0.034), (68, 0.047), (69, 0.013), (74, 0.143), (82, 0.035), (95, 0.045), (98, 0.027)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.94629622 <a title="177-lda-1" href="../hunch_net-2008/hunch_net-2008-11-09-A_Healthy__COLT.html">324 hunch net-2008-11-09-A Healthy  COLT</a></p>
<p>Introduction: Awhile ago, we discussed the health ofCOLT.COLT 2008substantially addressed my
concerns. The papers were diverse and several were interesting. Attendance was
up, which is particularly notable in Europe. In my opinion, the colocation
with UAI and ICML was the best colocation since 1998.And, perhaps best of all,
registration ended up being free for all students due to various grants from
theAcademy of Finland,Google,IBM, andYahoo.A basic question is: what went
right? There seem to be several answers.Cost-wise, COLT had sufficient grants
to alleviate the high cost of the Euro and location at a university
substantially reduces the cost compared to a hotel.Organization-wise, the
Finns were great with hordes of volunteers helping set everything up. Having
too many volunteers is a good failure mode.Organization-wise, it was clear
that all 3 program chairs were cooperating in designing the program
.Facilities-wise, proximity in time and space made the colocation much more
real than many others</p><p>same-blog 2 0.94367373 <a title="177-lda-2" href="../hunch_net-2006/hunch_net-2006-05-05-An_ICML_reject.html">177 hunch net-2006-05-05-An ICML reject</a></p>
<p>Introduction: Hal,Daniel, and I have been working on the algorithmSearnfor structured
prediction. This was just conditionally accepted and then rejected from ICML,
and we were quite surprised. By any reasonable criteria, it seems this is an
interesting algorithm.Prediction Performance: Searn performed better than any
other algorithm on all the problems we tested against using the same feature
set. This is true even using the numbers reported by authors in their
papers.Theoretical underpinning. Searn is a reduction which comes with a
reduction guarantee: the good performance on a base classifiers implies good
performance for the overall system. No other theorem of this type has been
made for other structured prediction algorithms, as far as we know.Speed.
Searn has no problem handling much larger datasets than other algorithms we
tested against.Simplicity. Given code for a binary classifier and a problem-
specific search algorithm, only a few tens of lines are necessary to implement
Searn.Generality.</p><p>3 0.91505337 <a title="177-lda-3" href="../hunch_net-2013/hunch_net-2013-12-01-NIPS_tutorials_and_Vowpal_Wabbit_7.4.html">492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</a></p>
<p>Introduction: At NIPS I'm giving atutorial on Learning to Interact. In essence this is about
dealing with causality in a contextual bandit framework. Relative toprevious
tutorials, I'll be covering several new results that changed my understanding
of the nature of the problem. Note thatJudea PearlandElias Bareinboimhave
atutorial on causality. This might appear similar, but is quite different in
practice. Pearl and Bareinboim's tutorial will be about the general concepts
while mine will be about total mastery of the simplest nontrivial case,
including code. Luckily, they have the right order. I recommend going to bothI
also just released version 7.4 ofVowpal Wabbit. When I was a frustrated
learning theorist, I did not understand why people were not using learning
reductions to solve problems. I've been slowly discovering why with VW, and
addressing the issues. One of the issues is that machine learning itself was
not automatic enough, while another is that creating a very low overhead
process for do</p><p>4 0.88645464 <a title="177-lda-4" href="../hunch_net-2007/hunch_net-2007-04-18-%2450K_Spock_Challenge.html">239 hunch net-2007-04-18-$50K Spock Challenge</a></p>
<p>Introduction: Apparently, the companySpockis setting up a$50k entity resolution challenge.
$50k is much less than the Netflix challenge, but it's effectively the same as
Netflix untilsomeone reaches 10%. It's also nice that the Spock challenge has
a short duration. The (visible) test set is of size 25k and the training set
has size 75k.</p><p>5 0.85208899 <a title="177-lda-5" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>Introduction: The 2006 Machine Learning Summer School in Taipei, Taiwan ended on August 4,
2006. It has been a very exciting two weeks for a record crowd of 245
participants (including speakers and organizers) from 18 countries. We had a
lineup of speakers that is hard to match up for other similar events (see
ourWIKIfor more information). With this lineup, it is difficult for us as
organizers to screw it up too bad. Also, since we have pretty good
infrastructure for international meetings and experienced staff at NTUST and
Academia Sinica, plus the reputation established by previous MLSS series, it
was relatively easy for us to attract registrations and simply enjoyed this
two-week long party of machine learning.In the end of MLSS we distributed a
survey form for participants to fill in. I will report what we found from this
survey, together with the registration data and word-of-mouth from
participants.The first question is designed to find out how our participants
learned about MLSS 2006 Taipei.</p><p>6 0.85017848 <a title="177-lda-6" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>7 0.84737712 <a title="177-lda-7" href="../hunch_net-2005/hunch_net-2005-09-12-Fast_Gradient_Descent.html">111 hunch net-2005-09-12-Fast Gradient Descent</a></p>
<p>8 0.84636819 <a title="177-lda-8" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>9 0.84185201 <a title="177-lda-9" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>10 0.84038144 <a title="177-lda-10" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">23 hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<p>11 0.84036303 <a title="177-lda-11" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>12 0.84022248 <a title="177-lda-12" href="../hunch_net-2009/hunch_net-2009-02-22-Effective_Research_Funding.html">344 hunch net-2009-02-22-Effective Research Funding</a></p>
<p>13 0.83990526 <a title="177-lda-13" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>14 0.83919287 <a title="177-lda-14" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>15 0.83896542 <a title="177-lda-15" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>16 0.83796686 <a title="177-lda-16" href="../hunch_net-2005/hunch_net-2005-03-17-Going_all_the_Way%2C_Sometimes.html">42 hunch net-2005-03-17-Going all the Way, Sometimes</a></p>
<p>17 0.83666795 <a title="177-lda-17" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<p>18 0.83556187 <a title="177-lda-18" href="../hunch_net-2006/hunch_net-2006-09-28-Programming_Languages_for_Machine_Learning_Implementations.html">210 hunch net-2006-09-28-Programming Languages for Machine Learning Implementations</a></p>
<p>19 0.83544731 <a title="177-lda-19" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>20 0.83539069 <a title="177-lda-20" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
