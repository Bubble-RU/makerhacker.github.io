<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>202 hunch net-2006-08-10-Precision is not accuracy</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-202" href="#">hunch_net-2006-202</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>202 hunch net-2006-08-10-Precision is not accuracy</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-202-html" href="http://hunch.net/?p=204">html</a></p><p>Introduction: In my experience, there are two different groups of people who believe the
same thing: the mathematics encountered in typical machine learning conference
papers is often of questionable value.The two groups who agree on this are
applied machine learning people who have given up on math, and mature
theoreticians who understand the limits of theory.Partly, this is just a
statement about where we are with respect to machine learning. In particular,
we have no mechanism capable of generating a prescription for how to solve all
learning problems. In the absence of such certainty, people try to come up
with formalisms that partially describe and motivate how and why they do
things. This is natural and healthy--we might hope that it will eventually
lead to just such a mechanism.But, part of this is simply an emphasis on
complexity over clarity. A very natural and simple theoretical statement is
often obscured by complexifications. Common sources of complexification
include:GeneralizationBy tr</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('complexifications', 0.374), ('math', 0.294), ('gun', 0.281), ('notation', 0.2), ('complexification', 0.187), ('relies', 0.187), ('precision', 0.187), ('type', 0.158), ('hard', 0.14), ('statement', 0.133), ('theorem', 0.121), ('accurate', 0.115), ('encountered', 0.112), ('fix', 0.105), ('often', 0.105), ('groups', 0.103), ('worst', 0.102), ('assumptions', 0.09), ('upon', 0.088), ('cumbersome', 0.083)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000002 <a title="202-tfidf-1" href="../hunch_net-2006/hunch_net-2006-08-10-Precision_is_not_accuracy.html">202 hunch net-2006-08-10-Precision is not accuracy</a></p>
<p>Introduction: In my experience, there are two different groups of people who believe the
same thing: the mathematics encountered in typical machine learning conference
papers is often of questionable value.The two groups who agree on this are
applied machine learning people who have given up on math, and mature
theoreticians who understand the limits of theory.Partly, this is just a
statement about where we are with respect to machine learning. In particular,
we have no mechanism capable of generating a prescription for how to solve all
learning problems. In the absence of such certainty, people try to come up
with formalisms that partially describe and motivate how and why they do
things. This is natural and healthy--we might hope that it will eventually
lead to just such a mechanism.But, part of this is simply an emphasis on
complexity over clarity. A very natural and simple theoretical statement is
often obscured by complexifications. Common sources of complexification
include:GeneralizationBy tr</p><p>2 0.19582519 <a title="202-tfidf-2" href="../hunch_net-2006/hunch_net-2006-03-09-Use_of_Notation.html">162 hunch net-2006-03-09-Use of Notation</a></p>
<p>Introduction: For most people, a mathematical notation is like a language: you learn it and
stick with it. For people doing mathematical research, however, this is not
enough: they must design new notations for new problems. The design of good
notation is both hard and worthwhile since a bad initial notation can retard a
line of research greatly.Before we had mathematical notation, equations were
all written out in language. Since words have multiple meanings and variable
precedences, long equations written out in language can be extraordinarily
difficult and sometimes fundamentally ambiguous. A good representative example
of this is the legalese in the tax code. Since we want greater precision and
clarity, we adopt mathematical notation.One fundamental thing to understand
about mathematical notation, is that humans as logic verifiers, are barely
capable. This is the fundamental reason why one notation can be much better
than another. This observation is easier to miss than you might expect
because,</p><p>3 0.12612262 <a title="202-tfidf-3" href="../hunch_net-2005/hunch_net-2005-05-12-Math_on_the_Web.html">70 hunch net-2005-05-12-Math on the Web</a></p>
<p>Introduction: Andrej Bauer has setup aMathematics and ComputationBlog. As a first step he
has tried to address the persistent and annoying problem of math on the web.
As a basic tool for precisely stating and transfering understanding of
technical subjects, mathematics is very necessary. Despite this necessity,
every mechanism for expressing mathematics on the web seems unnaturally
clumsy. Here are some of the methods and their drawbacks:MathMLThis was
supposed to be the answer, but it has two severe drawbacks: "Internet
Explorer" doesn't read it and the language is an example of push-XML-to-the-
limit which no one would ever consider writing in. (In contrast, html is easy
to write in.) It's also very annoying that math fonts must be installed
independent of the browser, even for mozilla based browsers.Create inline
images. This has several big drawbacks: font size is fixed for all viewers,
you can't cut & paste inside the images, and you can't hyperlink from (say)
symbol to definition.Math Worldis</p><p>4 0.12413231 <a title="202-tfidf-4" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>Introduction: There are several different flavors of Machine Learning classes. Many classes
are of the 'zoo' sort: many different learning algorithms are presented.
Others avoid the zoo by not covering the full scope of machine learning.This
is my view of what makes a good machine learning class, along with why. I'd
like to specifically invite comment on whether things are missing,
misemphasized, or misplaced.PhaseSubjectWhy?IntroductionWhat is a machine
learning problem?A good understanding of the characteristics of machine
learning problems seems essential. Characteristics include: a data source,
some hope the data is predictive, and a need for generalization. This is
probably best taught in a case study manner: lay out the specifics of some
problem and then ask "Is this a machine learning problem?"IntroductionMachine
Learning Problem IdentificationIdentification and recognition of the type of
learning problems is (obviously) a very important step in solving such
problems. People need to be famili</p><p>5 0.12189734 <a title="202-tfidf-5" href="../hunch_net-2006/hunch_net-2006-04-06-Bounds_greater_than_1.html">170 hunch net-2006-04-06-Bounds greater than 1</a></p>
<p>Introduction: Nati SrebroandShai Ben-Davidhave apaperatCOLTwhich, in the appendix, proves
something very striking: several previous error bounds arealwaysgreater than
1.BackgroundOne branch of learning theory focuses on theorems whichAssume
samples are drawn IID from an unknown distributionD.Fix a set of
classifiersFind a high probability bound on the maximum true error rate (with
respect toD) as a function of the empirical error rate on the training
set.Many of these bounds become extremely complex and hairy.CurrentEveryone
working on this subject wants "tighter bounds", however there are different
definitions of "tighter". Some groups focus on "functional tightness" (getting
the right functional dependency between the size of the training set and a
parameterization of the hypothesis space) whileothersfocus on "practical
tightness" (finding bounds which work well on practical problems). (I am
definitely in the second camp.)One of the dangers of striving for "functional
tightness" is that the bound</p><p>6 0.12179639 <a title="202-tfidf-6" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>7 0.11412644 <a title="202-tfidf-7" href="../hunch_net-2005/hunch_net-2005-08-22-Do_you_believe_in_induction%3F.html">104 hunch net-2005-08-22-Do you believe in induction?</a></p>
<p>8 0.10948943 <a title="202-tfidf-8" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>9 0.10359515 <a title="202-tfidf-9" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>10 0.10264497 <a title="202-tfidf-10" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>11 0.10063046 <a title="202-tfidf-11" href="../hunch_net-2012/hunch_net-2012-04-09-ICML_author_feedback_is_open.html">461 hunch net-2012-04-09-ICML author feedback is open</a></p>
<p>12 0.097217061 <a title="202-tfidf-12" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>13 0.096758656 <a title="202-tfidf-13" href="../hunch_net-2008/hunch_net-2008-10-14-Who_is_Responsible_for_a_Bad_Review%3F.html">320 hunch net-2008-10-14-Who is Responsible for a Bad Review?</a></p>
<p>14 0.094180755 <a title="202-tfidf-14" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>15 0.093822032 <a title="202-tfidf-15" href="../hunch_net-2006/hunch_net-2006-06-25-Presentation_of_Proofs_is_Hard..html">187 hunch net-2006-06-25-Presentation of Proofs is Hard.</a></p>
<p>16 0.093811937 <a title="202-tfidf-16" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>17 0.093532056 <a title="202-tfidf-17" href="../hunch_net-2005/hunch_net-2005-06-18-Lower_Bounds_for_Learning_Reductions.html">83 hunch net-2005-06-18-Lower Bounds for Learning Reductions</a></p>
<p>18 0.093304224 <a title="202-tfidf-18" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>19 0.092149802 <a title="202-tfidf-19" href="../hunch_net-2005/hunch_net-2005-03-09-Bad_Reviewing.html">38 hunch net-2005-03-09-Bad Reviewing</a></p>
<p>20 0.089629196 <a title="202-tfidf-20" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.232), (1, 0.022), (2, -0.042), (3, -0.094), (4, 0.014), (5, -0.053), (6, -0.053), (7, -0.004), (8, -0.011), (9, -0.001), (10, -0.01), (11, -0.011), (12, 0.022), (13, -0.036), (14, -0.081), (15, 0.017), (16, -0.024), (17, -0.069), (18, 0.009), (19, 0.004), (20, 0.063), (21, 0.002), (22, 0.002), (23, -0.048), (24, -0.005), (25, 0.025), (26, -0.056), (27, -0.022), (28, -0.001), (29, 0.002), (30, 0.055), (31, -0.009), (32, -0.049), (33, 0.008), (34, -0.048), (35, -0.072), (36, -0.045), (37, 0.032), (38, 0.11), (39, 0.082), (40, 0.033), (41, -0.046), (42, -0.059), (43, -0.062), (44, 0.032), (45, -0.024), (46, 0.014), (47, 0.016), (48, 0.058), (49, 0.048)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94281065 <a title="202-lsi-1" href="../hunch_net-2006/hunch_net-2006-08-10-Precision_is_not_accuracy.html">202 hunch net-2006-08-10-Precision is not accuracy</a></p>
<p>Introduction: In my experience, there are two different groups of people who believe the
same thing: the mathematics encountered in typical machine learning conference
papers is often of questionable value.The two groups who agree on this are
applied machine learning people who have given up on math, and mature
theoreticians who understand the limits of theory.Partly, this is just a
statement about where we are with respect to machine learning. In particular,
we have no mechanism capable of generating a prescription for how to solve all
learning problems. In the absence of such certainty, people try to come up
with formalisms that partially describe and motivate how and why they do
things. This is natural and healthy--we might hope that it will eventually
lead to just such a mechanism.But, part of this is simply an emphasis on
complexity over clarity. A very natural and simple theoretical statement is
often obscured by complexifications. Common sources of complexification
include:GeneralizationBy tr</p><p>2 0.85294503 <a title="202-lsi-2" href="../hunch_net-2006/hunch_net-2006-03-09-Use_of_Notation.html">162 hunch net-2006-03-09-Use of Notation</a></p>
<p>Introduction: For most people, a mathematical notation is like a language: you learn it and
stick with it. For people doing mathematical research, however, this is not
enough: they must design new notations for new problems. The design of good
notation is both hard and worthwhile since a bad initial notation can retard a
line of research greatly.Before we had mathematical notation, equations were
all written out in language. Since words have multiple meanings and variable
precedences, long equations written out in language can be extraordinarily
difficult and sometimes fundamentally ambiguous. A good representative example
of this is the legalese in the tax code. Since we want greater precision and
clarity, we adopt mathematical notation.One fundamental thing to understand
about mathematical notation, is that humans as logic verifiers, are barely
capable. This is the fundamental reason why one notation can be much better
than another. This observation is easier to miss than you might expect
because,</p><p>3 0.70376277 <a title="202-lsi-3" href="../hunch_net-2005/hunch_net-2005-08-22-Do_you_believe_in_induction%3F.html">104 hunch net-2005-08-22-Do you believe in induction?</a></p>
<p>Introduction: Foster Provostgave a talk at the ICMLmetalearning workshopon "metalearning"
and the "no free lunch theorem" which seems worth summarizing.As a review: the
no free lunch theorem is the most complicated way we know of to say that
abiasis required in order to learn. The simplest way to see this is in a
nonprobabilistic setting. If you are given examples of the form(x,y)and you
wish to predictyfromxthen any prediction mechanism errs half the time in
expectation over all sequences of examples. The proof of this is very simple:
on every example a predictor must make some prediction and by symmetry over
the set of sequences it will be wrong half the time and right half the time.
The basic idea of this proof has been applied to many other settings.The
simplistic interpretation of this theorem which many people jump to is
"machine learning is dead" since there can be no single learning algorithm
which can solve all learning problems. This is the wrong way to think about
it. In the real world, w</p><p>4 0.68276268 <a title="202-lsi-4" href="../hunch_net-2008/hunch_net-2008-07-04-More_Presentation_Preparation.html">307 hunch net-2008-07-04-More Presentation Preparation</a></p>
<p>Introduction: We've discussedpresentation preparation before, but I have one more thing to
add:transitioning. For a research presentation, it is substantially helpful
for the audience if transitions are clear. A common outline for a research
presentation in machine leanring is:The problem. Presentations which don't
describe the problem almost immediately lose people, because the context is
missing to understand the detail.Prior relevant work. In many cases, a paper
builds on some previous bit of work which must be understood in order to
understand what the paper does. A common failure mode seems to be spending too
much time on prior work. Discuss just the relevant aspects of prior work in
the language of your work. Sometimes this is missing when unneeded.What we
did. For theory papers in particular, it is often not possible to really cover
the details. Prioritizing what you present can be very important.How it
worked. Many papers in Machine Learning have some sort of experimental test of
the algorit</p><p>5 0.67747074 <a title="202-lsi-5" href="../hunch_net-2005/hunch_net-2005-03-17-Going_all_the_Way%2C_Sometimes.html">42 hunch net-2005-03-17-Going all the Way, Sometimes</a></p>
<p>Introduction: At many points in research, you face a choice: should I keep on improving some
old piece of technology or should I do something new? For example:Should I
refine bounds to make them tighter?Should I take some learning theory and turn
it into a learning algorithm?Should I implement the learning algorithm?Should
I test the learning algorithm widely?Should I release the algorithm as source
code?Should I go see what problems people actually need to solve?The universal
temptation of people attracted to research is doing something new. That is
sometimes the right decision, but is also often not. I'd like to discuss some
reasons why not.ExpertiseOnce expertise are developed on some subject, you are
the right person to refine them.What is the real problem?Continually improving
a piece of technology is a mechanism forcing you to confront this question. In
many cases, this confrontation is uncomfortable because you discover that your
method has fundamental flaws with respect to solving the real p</p><p>6 0.67646325 <a title="202-lsi-6" href="../hunch_net-2006/hunch_net-2006-06-25-Presentation_of_Proofs_is_Hard..html">187 hunch net-2006-06-25-Presentation of Proofs is Hard.</a></p>
<p>7 0.67440379 <a title="202-lsi-7" href="../hunch_net-2005/hunch_net-2005-05-12-Math_on_the_Web.html">70 hunch net-2005-05-12-Math on the Web</a></p>
<p>8 0.62244833 <a title="202-lsi-8" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>9 0.62083846 <a title="202-lsi-9" href="../hunch_net-2005/hunch_net-2005-04-16-Which_Assumptions_are_Reasonable%3F.html">57 hunch net-2005-04-16-Which Assumptions are Reasonable?</a></p>
<p>10 0.61421633 <a title="202-lsi-10" href="../hunch_net-2007/hunch_net-2007-06-21-Presentation_Preparation.html">249 hunch net-2007-06-21-Presentation Preparation</a></p>
<p>11 0.61329579 <a title="202-lsi-11" href="../hunch_net-2007/hunch_net-2007-07-20-Motivation_should_be_the_Responsibility_of_the_Reviewer.html">256 hunch net-2007-07-20-Motivation should be the Responsibility of the Reviewer</a></p>
<p>12 0.60771841 <a title="202-lsi-12" href="../hunch_net-2005/hunch_net-2005-04-10-Is_the_Goal_Understanding_or_Prediction%3F.html">55 hunch net-2005-04-10-Is the Goal Understanding or Prediction?</a></p>
<p>13 0.60459453 <a title="202-lsi-13" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>14 0.60274607 <a title="202-lsi-14" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>15 0.60177344 <a title="202-lsi-15" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>16 0.58876348 <a title="202-lsi-16" href="../hunch_net-2006/hunch_net-2006-01-06-MLTV.html">146 hunch net-2006-01-06-MLTV</a></p>
<p>17 0.5864284 <a title="202-lsi-17" href="../hunch_net-2006/hunch_net-2006-09-28-Programming_Languages_for_Machine_Learning_Implementations.html">210 hunch net-2006-09-28-Programming Languages for Machine Learning Implementations</a></p>
<p>18 0.57013965 <a title="202-lsi-18" href="../hunch_net-2006/hunch_net-2006-09-12-Incentive_Compatible_Reviewing.html">207 hunch net-2006-09-12-Incentive Compatible Reviewing</a></p>
<p>19 0.56847042 <a title="202-lsi-19" href="../hunch_net-2005/hunch_net-2005-03-21-Research_Styles_in_Machine_Learning.html">44 hunch net-2005-03-21-Research Styles in Machine Learning</a></p>
<p>20 0.5652281 <a title="202-lsi-20" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(35, 0.029), (38, 0.309), (42, 0.203), (45, 0.017), (68, 0.077), (74, 0.158), (76, 0.052), (82, 0.015), (88, 0.026), (91, 0.018), (95, 0.012)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.96300769 <a title="202-lda-1" href="../hunch_net-2005/hunch_net-2005-02-27-Antilearning%3A_When_proximity_goes_bad.html">32 hunch net-2005-02-27-Antilearning: When proximity goes bad</a></p>
<p>Introduction: Joel Preddmentioned"Antilearning" byAdam Kowalczyk, which is interesting from
a foundational intuitions viewpoint.There is a pervasive intuition that
"nearby things tend to have the same label". This intuition is instantiated in
SVMs, nearest neighbor classifiers, decision trees, and neural networks. It
turns out there are natural problems where this intuition is opposite of the
truth.One natural situation where this occurs is in competition. For example,
whenIntelfails to meet its earnings estimate, is this evidence thatAMDis doing
badly also? Or evidence that AMD is doing well?This violation of the proximity
intuition means that when the number of examples is few,negatinga classifier
which attempts to exploit proximity can provide predictive power (thus, the
term "antilearning").</p><p>2 0.9563002 <a title="202-lda-2" href="../hunch_net-2008/hunch_net-2008-06-30-ICML_has_a_comment_system.html">305 hunch net-2008-06-30-ICML has a comment system</a></p>
<p>Introduction: Mark Reidhas stepped up and created acomment system for ICML paperswhichGreger
Lindenhas tightly integrated.My understanding is that Mark spent quite a bit
of time on the details, and there are some cool features like working latex
math mode. This is an excellent chance for the ICML community to experiment
with making ICML year-round, so I hope it works out. Please do consider
experimenting with it.</p><p>3 0.91390854 <a title="202-lda-3" href="../hunch_net-2008/hunch_net-2008-09-04-Fall_ML_Conferences.html">316 hunch net-2008-09-04-Fall ML Conferences</a></p>
<p>Introduction: If you are in the New York area and interested in machine learning, consider
submitting a 2 page abstract to theML symposiumby tomorrow (Sept 5th)
midnight. It's a fun one day affair on October 10 in an awesome location
overlooking the world trade center site.A bit further off (but a real
conference) is theAI and Statsdeadline on November 5, to be held in Florida
April 16-19.</p><p>4 0.90976787 <a title="202-lda-4" href="../hunch_net-2008/hunch_net-2008-02-27-The_Stats_Handicap.html">290 hunch net-2008-02-27-The Stats Handicap</a></p>
<p>Introduction: Graduating students in Statistics appear to be at a substantial handicap
compared to graduating students in Machine Learning, despite being in
substantially overlapping subjects.The problem seems to be cultural.
Statistics comes from a mathematics background which emphasizes large
publications slowly published under review at journals. Machine Learning comes
from a Computer Science background which emphasizes quick publishing at
reviewed conferences. This has a number of implications:Graduating statistics
PhDs often have 0-2 publications while graduating machine learning PhDs might
have 5-15.Graduating ML students have had a chance for others to build on
their work. Stats students have had no such chance.Graduating ML students have
attended a number of conferences and presented their work, giving them a
chance to meet people. Stats students have had fewer chances of this sort.In
short, Stats students have had relatively few chances to distinguish
themselves and are heavily reliant on t</p><p>5 0.88910651 <a title="202-lda-5" href="../hunch_net-2007/hunch_net-2007-06-14-Interesting_Papers_at_COLT_2007.html">247 hunch net-2007-06-14-Interesting Papers at COLT 2007</a></p>
<p>Introduction: Here are two papers that seem particularly interesting at this year's
COLT.Gilles BlanchardandFranÃƒÂ§ois Fleuret,Occam's Hammer. When we are
interested in very tight bounds on the true error rate of a classifier, it is
tempting to use a PAC-Bayes bound which can (empirically) bequite tight. A
disadvantage of the PAC-Bayes bound is that it applies to a classifier which
is randomized over a set of base classifiers rather than a single classifier.
This paper shows that a similar bound can be proved which holds for a single
classifier drawn from the set. The ability to safely use a single classifier
is very nice. This technique applies generically to any base bound, so it has
other applications covered in the paper.Adam Tauman Kalai.Learning Nested
Halfspaces and Uphill Decision Trees. Classification PAC-learning, where you
prove that any problem amongst some set is polytime learnable with respect to
any distribution over the inputXis extraordinarily challenging as judged by
lack of progr</p><p>same-blog 6 0.88054127 <a title="202-lda-6" href="../hunch_net-2006/hunch_net-2006-08-10-Precision_is_not_accuracy.html">202 hunch net-2006-08-10-Precision is not accuracy</a></p>
<p>7 0.85901874 <a title="202-lda-7" href="../hunch_net-2011/hunch_net-2011-10-10-ML_Symposium_and_ICML_details.html">447 hunch net-2011-10-10-ML Symposium and ICML details</a></p>
<p>8 0.73913181 <a title="202-lda-8" href="../hunch_net-2007/hunch_net-2007-10-24-Contextual_Bandits.html">269 hunch net-2007-10-24-Contextual Bandits</a></p>
<p>9 0.68627751 <a title="202-lda-9" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>10 0.66516495 <a title="202-lda-10" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>11 0.66051799 <a title="202-lda-11" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>12 0.65409178 <a title="202-lda-12" href="../hunch_net-2005/hunch_net-2005-08-11-Why_Manifold-Based_Dimension_Reduction_Techniques%3F.html">102 hunch net-2005-08-11-Why Manifold-Based Dimension Reduction Techniques?</a></p>
<p>13 0.6461274 <a title="202-lda-13" href="../hunch_net-2009/hunch_net-2009-02-22-Effective_Research_Funding.html">344 hunch net-2009-02-22-Effective Research Funding</a></p>
<p>14 0.63965827 <a title="202-lda-14" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>15 0.63846505 <a title="202-lda-15" href="../hunch_net-2005/hunch_net-2005-09-10-%26%238220%3BFailure%26%238221%3B_is_an_option.html">110 hunch net-2005-09-10-&#8220;Failure&#8221; is an option</a></p>
<p>16 0.63790059 <a title="202-lda-16" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>17 0.63592803 <a title="202-lda-17" href="../hunch_net-2011/hunch_net-2011-11-26-Giving_Thanks.html">449 hunch net-2011-11-26-Giving Thanks</a></p>
<p>18 0.6333552 <a title="202-lda-18" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>19 0.63224679 <a title="202-lda-19" href="../hunch_net-2008/hunch_net-2008-07-02-Proprietary_Data_in_Academic_Research%3F.html">306 hunch net-2008-07-02-Proprietary Data in Academic Research?</a></p>
<p>20 0.63084376 <a title="202-lda-20" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
