<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>205 hunch net-2006-09-07-Objective and subjective interpretations of probability</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-205" href="#">hunch_net-2006-205</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>205 hunch net-2006-09-07-Objective and subjective interpretations of probability</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-205-html" href="http://hunch.net/?p=225">html</a></p><p>Introduction: An amusing tidbit (reproduced without permission) from Herman Chernoff's
delightful monograph, "Sequential analysis and optimal design":The use of
randomization raises a philosophical question which is articulated by the
following probably apocryphal anecdote.The metallurgist told his friend the
statistician how he planned to test the effect of heat on the strength of a
metal bar by sawing the bar into six pieces. The first two would go into the
hot oven, the next two into the medium oven, and the last two into the cool
oven. The statistician, horrified, explained how he should randomize to avoid
the effect of a possible gradient of strength in the metal bar. The method of
randomization was applied, and it turned out that the randomized experiment
called for putting the first two pieces into the hot oven, the next two into
the medium oven, and the last two into the cool oven. "Obviously, we can't do
that," said the metallurgist. "On the contrary, you have to do that," said the
statisti</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 An amusing tidbit (reproduced without permission) from Herman Chernoff's delightful monograph, "Sequential analysis and optimal design":The use of randomization raises a philosophical question which is articulated by the following probably apocryphal anecdote. [sent-1, score-0.456]
</p><p>2 The metallurgist told his friend the statistician how he planned to test the effect of heat on the strength of a metal bar by sawing the bar into six pieces. [sent-2, score-1.529]
</p><p>3 The first two would go into the hot oven, the next two into the medium oven, and the last two into the cool oven. [sent-3, score-0.651]
</p><p>4 The statistician, horrified, explained how he should randomize to avoid the effect of a possible gradient of strength in the metal bar. [sent-4, score-0.602]
</p><p>5 The method of randomization was applied, and it turned out that the randomized experiment called for putting the first two pieces into the hot oven, the next two into the medium oven, and the last two into the cool oven. [sent-5, score-1.01]
</p><p>6 "On the contrary, you have to do that," said the statistician. [sent-7, score-0.103]
</p><p>7 In a "larger" design or sample, the effect of a reasonable randomization scheme could be such that this obvious difficulty would almost certainly not happen. [sent-9, score-0.838]
</p><p>8 In this small problem, the effect may not be cancelled out, but the statistician still has a right to close his eyes to the design actually selected if he is satisfied with "playing fair". [sent-11, score-0.897]
</p><p>9 That is, if he instructs an agent to select the design and he analyzes the results, assuming there are no gradients, his conclusions will beunbiasedin the sense that a tendency to overestimate is balanced on the average by a tendency to underestimate the desired quantities. [sent-12, score-0.798]
</p><p>10 However, this tendency may be substantial as measured by the variability of the estimates which will be affected by substantial gradients. [sent-13, score-0.338]
</p><p>11 On the other hand, following the natural inclination to reject an obviously unsatisfactory design resulting from randomization puts the statistician in the position of not "playing fair". [sent-14, score-1.319]
</p><p>12 What is worse for anobjectivestatistician, he has no way of evaluating in advance how good his procedure is if he can change the rules in the middle of the experiment. [sent-15, score-0.062]
</p><p>13 When randomization leads to the original unsatisfactory design, he is aware of this information and unwilling to accept the design. [sent-17, score-0.6]
</p><p>14 In general, the religious Bayesian states that no good and only harm can come from randomized experiments. [sent-18, score-0.103]
</p><p>15 In principle, he is opposed even to random sampling in opinion polling. [sent-19, score-0.062]
</p><p>16 However, this principle puts him in untenable computational positions, and a pragmatic Bayesian will often ignore what seems useless design information if there are no obvious quirks in a randomly selected sample. [sent-20, score-0.704]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('statistician', 0.377), ('randomization', 0.322), ('oven', 0.302), ('strength', 0.223), ('design', 0.223), ('bar', 0.176), ('effect', 0.163), ('metal', 0.151), ('tendency', 0.142), ('estimates', 0.134), ('heat', 0.134), ('selected', 0.134), ('unsatisfactory', 0.134), ('puts', 0.124), ('hot', 0.112), ('medium', 0.107), ('randomized', 0.103), ('said', 0.103), ('principle', 0.097), ('playing', 0.097), ('assuming', 0.095), ('two', 0.095), ('fair', 0.082), ('original', 0.082), ('cool', 0.081), ('obviously', 0.077), ('amusing', 0.067), ('underestimate', 0.067), ('reproduced', 0.067), ('raises', 0.067), ('gradients', 0.067), ('cancel', 0.067), ('herman', 0.067), ('monograph', 0.067), ('overestimate', 0.067), ('six', 0.067), ('would', 0.066), ('gradient', 0.065), ('obvious', 0.064), ('sample', 0.064), ('unwilling', 0.062), ('permission', 0.062), ('planned', 0.062), ('conclusions', 0.062), ('middle', 0.062), ('affected', 0.062), ('chernoff', 0.062), ('inclination', 0.062), ('opposed', 0.062), ('pragmatic', 0.062)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999976 <a title="205-tfidf-1" href="../hunch_net-2006/hunch_net-2006-09-07-Objective_and_subjective_interpretations_of_probability.html">205 hunch net-2006-09-07-Objective and subjective interpretations of probability</a></p>
<p>Introduction: An amusing tidbit (reproduced without permission) from Herman Chernoff's
delightful monograph, "Sequential analysis and optimal design":The use of
randomization raises a philosophical question which is articulated by the
following probably apocryphal anecdote.The metallurgist told his friend the
statistician how he planned to test the effect of heat on the strength of a
metal bar by sawing the bar into six pieces. The first two would go into the
hot oven, the next two into the medium oven, and the last two into the cool
oven. The statistician, horrified, explained how he should randomize to avoid
the effect of a possible gradient of strength in the metal bar. The method of
randomization was applied, and it turned out that the randomized experiment
called for putting the first two pieces into the hot oven, the next two into
the medium oven, and the last two into the cool oven. "Obviously, we can't do
that," said the metallurgist. "On the contrary, you have to do that," said the
statisti</p><p>2 0.13412236 <a title="205-tfidf-2" href="../hunch_net-2006/hunch_net-2006-11-22-Explicit_Randomization_in_Learning_algorithms.html">219 hunch net-2006-11-22-Explicit Randomization in Learning algorithms</a></p>
<p>Introduction: There are a number of learning algorithms which explicitly incorporate
randomness into their execution. This includes at amongst others:Neural
Networks. Neural networks use randomization to assign initial
weights.Boltzmann Machines/Deep Belief Networks. Boltzmann machines are
something like a stochastic version of multinode logistic regression. The use
of randomness is more essential in Boltzmann machines, because the predicted
value at test time also uses randomness.Bagging. Bagging is a process where a
learning algorithm is run several different times on several different
datasets, creating a final predictor which makes a majority vote.Policy
descent. Several algorithms in reinforcement learning such asConservative
Policy Iterationuse random bits to create stochastic policies.Experts
algorithms. Randomized weighted majority use random bits as a part of the
prediction process to achieve better theoretical guarantees.A basic question
is: "Should there be explicit randomization in learn</p><p>3 0.089319237 <a title="205-tfidf-3" href="../hunch_net-2010/hunch_net-2010-09-21-Regretting_the_dead.html">411 hunch net-2010-09-21-Regretting the dead</a></p>
<p>Introduction: Nikospointed out thisnew york timesarticle aboutpoor clinical design killing
people. For those of us who study learning from exploration information this
is a reminder that low regret algorithms are particularly important, as regret
in clinical trials is measured by patient deaths.Two obvious improvements on
the experimental design are:With reasonable record keeping of existing
outcomes for the standard treatments, there is no need to explicitly assign
people to a control group with the standard treatment, as that approach is
effectively explored with great certainty. Asserting otherwise would imply
that the nature of effective treatments for cancer has changed between now and
a year ago, which denies the value of any clinical trial.An optimal
experimental design will smoothly phase between exploration and exploitation
as evidence for a new treatment shows that it can be effective. This is old
tech, for example in theEXP3.P algorithm (page 12 aka 59)although I prefer the
generalized an</p><p>4 0.082510374 <a title="205-tfidf-4" href="../hunch_net-2008/hunch_net-2008-04-30-Concerns_about_the_Large_Scale_Learning_Challenge.html">300 hunch net-2008-04-30-Concerns about the Large Scale Learning Challenge</a></p>
<p>Introduction: Thelarge scale learning challengefor ICML interests me a great deal, although
I have concerns about the way it is structured.From theinstructions page,
several issues come up:Large DefinitionMy personal definition of dataset size
is:smallA dataset is small if a human could look at the dataset and plausibly
find a good solution.mediumA dataset is mediumsize if it fits in the RAM of a
reasonably priced computer.largeA large dataset does not fit in the RAM of a
reasonably priced computer.By this definition, all of the datasets are medium
sized. This might sound like a pissing match over dataset size, but I believe
it is more than that.The fundamental reason for these definitions is that they
correspond to transitions in the sorts of approaches which are feasible. From
small to medium, the ability to use a human as the learning algorithm
degrades. From medium to large, it becomes essential to have learning
algorithms that don't require random access to examples.No Loading TimeThe
medium sc</p><p>5 0.068811998 <a title="205-tfidf-5" href="../hunch_net-2005/hunch_net-2005-03-29-Academic_Mechanism_Design.html">48 hunch net-2005-03-29-Academic Mechanism Design</a></p>
<p>Introduction: From game theory, there is a notion of "mechanism design": setting up the
structure of the world so that participants have some incentive to do sane
things (rather than obviously counterproductive things). Application of this
principle to academic research may be fruitful.What is misdesigned about
academic research?TheJMLGguides give many hints.The common nature ofbad
reviewingalso suggests the system isn't working optimally.There are many ways
to experimentally"cheat" in machine learning.Funding Prisoner's Delimma. Good
researchers often write grant proposals for funding rather than doing
research. Since the pool of grant money is finite, this means that grant
proposals are often rejected, implying that more must be written. This is
essentially a "prisoner's delimma": anyone not writing grant proposals loses,
but the entire process of doing research is slowed by distraction. If everyone
wrote 1/2 as many grant proposals, roughly the same distribution of funding
would occur, and time w</p><p>6 0.06665495 <a title="205-tfidf-6" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>7 0.062427312 <a title="205-tfidf-7" href="../hunch_net-2008/hunch_net-2008-04-26-Eliminating_the_Birthday_Paradox_for_Universal_Features.html">298 hunch net-2008-04-26-Eliminating the Birthday Paradox for Universal Features</a></p>
<p>8 0.061078522 <a title="205-tfidf-8" href="../hunch_net-2005/hunch_net-2005-04-23-Advantages_and_Disadvantages_of_Bayesian_Learning.html">60 hunch net-2005-04-23-Advantages and Disadvantages of Bayesian Learning</a></p>
<p>9 0.059398498 <a title="205-tfidf-9" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>10 0.058273785 <a title="205-tfidf-10" href="../hunch_net-2005/hunch_net-2005-03-13-Avoiding_Bad_Reviewing.html">40 hunch net-2005-03-13-Avoiding Bad Reviewing</a></p>
<p>11 0.057601415 <a title="205-tfidf-11" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>12 0.057284929 <a title="205-tfidf-12" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>13 0.057140741 <a title="205-tfidf-13" href="../hunch_net-2005/hunch_net-2005-09-12-Fast_Gradient_Descent.html">111 hunch net-2005-09-12-Fast Gradient Descent</a></p>
<p>14 0.056380507 <a title="205-tfidf-14" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>15 0.055752769 <a title="205-tfidf-15" href="../hunch_net-2005/hunch_net-2005-03-10-Breaking_Abstractions.html">39 hunch net-2005-03-10-Breaking Abstractions</a></p>
<p>16 0.055356689 <a title="205-tfidf-16" href="../hunch_net-2006/hunch_net-2006-03-27-Gradients_everywhere.html">167 hunch net-2006-03-27-Gradients everywhere</a></p>
<p>17 0.054852661 <a title="205-tfidf-17" href="../hunch_net-2005/hunch_net-2005-11-16-The_Everything_Ensemble_Edge.html">131 hunch net-2005-11-16-The Everything Ensemble Edge</a></p>
<p>18 0.054556582 <a title="205-tfidf-18" href="../hunch_net-2008/hunch_net-2008-08-24-Mass_Customized_Medicine_in_the_Future%3F.html">314 hunch net-2008-08-24-Mass Customized Medicine in the Future?</a></p>
<p>19 0.054476663 <a title="205-tfidf-19" href="../hunch_net-2007/hunch_net-2007-02-16-The_Forgetting.html">233 hunch net-2007-02-16-The Forgetting</a></p>
<p>20 0.05377271 <a title="205-tfidf-20" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.128), (1, -0.019), (2, -0.004), (3, -0.032), (4, -0.004), (5, 0.0), (6, -0.036), (7, 0.018), (8, -0.002), (9, -0.001), (10, 0.004), (11, -0.005), (12, -0.037), (13, -0.012), (14, -0.007), (15, -0.058), (16, -0.003), (17, -0.003), (18, -0.016), (19, -0.037), (20, -0.053), (21, 0.019), (22, -0.019), (23, -0.034), (24, -0.0), (25, 0.046), (26, -0.007), (27, -0.034), (28, 0.065), (29, -0.003), (30, 0.029), (31, 0.048), (32, -0.012), (33, -0.005), (34, 0.039), (35, 0.051), (36, 0.052), (37, -0.001), (38, -0.112), (39, -0.024), (40, -0.037), (41, -0.003), (42, 0.004), (43, -0.02), (44, -0.03), (45, 0.01), (46, 0.017), (47, -0.089), (48, -0.129), (49, 0.081)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97455782 <a title="205-lsi-1" href="../hunch_net-2006/hunch_net-2006-09-07-Objective_and_subjective_interpretations_of_probability.html">205 hunch net-2006-09-07-Objective and subjective interpretations of probability</a></p>
<p>Introduction: An amusing tidbit (reproduced without permission) from Herman Chernoff's
delightful monograph, "Sequential analysis and optimal design":The use of
randomization raises a philosophical question which is articulated by the
following probably apocryphal anecdote.The metallurgist told his friend the
statistician how he planned to test the effect of heat on the strength of a
metal bar by sawing the bar into six pieces. The first two would go into the
hot oven, the next two into the medium oven, and the last two into the cool
oven. The statistician, horrified, explained how he should randomize to avoid
the effect of a possible gradient of strength in the metal bar. The method of
randomization was applied, and it turned out that the randomized experiment
called for putting the first two pieces into the hot oven, the next two into
the medium oven, and the last two into the cool oven. "Obviously, we can't do
that," said the metallurgist. "On the contrary, you have to do that," said the
statisti</p><p>2 0.69608766 <a title="205-lsi-2" href="../hunch_net-2010/hunch_net-2010-09-21-Regretting_the_dead.html">411 hunch net-2010-09-21-Regretting the dead</a></p>
<p>Introduction: Nikospointed out thisnew york timesarticle aboutpoor clinical design killing
people. For those of us who study learning from exploration information this
is a reminder that low regret algorithms are particularly important, as regret
in clinical trials is measured by patient deaths.Two obvious improvements on
the experimental design are:With reasonable record keeping of existing
outcomes for the standard treatments, there is no need to explicitly assign
people to a control group with the standard treatment, as that approach is
effectively explored with great certainty. Asserting otherwise would imply
that the nature of effective treatments for cancer has changed between now and
a year ago, which denies the value of any clinical trial.An optimal
experimental design will smoothly phase between exploration and exploitation
as evidence for a new treatment shows that it can be effective. This is old
tech, for example in theEXP3.P algorithm (page 12 aka 59)although I prefer the
generalized an</p><p>3 0.52771878 <a title="205-lsi-3" href="../hunch_net-2005/hunch_net-2005-04-26-To_calibrate_or_not%3F.html">62 hunch net-2005-04-26-To calibrate or not?</a></p>
<p>Introduction: A calibrated predictor is one which predicts the probability of a binary event
with the property: For all predictionsp, the proportion of the time that1is
observed isp.Since there are infinitely manyp, this definition must be
"softened" to make sense for any finite number of samples. The standard method
for "softening" is to consider all predictions in a small neighborhood about
each possiblep.A great deal of effort has been devoted to strategies for
achieving calibrated (such ashere) prediction. With statements like: (under
minimal conditions) you can always make calibrated predictions.Given the
strength of these statements, we might conclude we are done, but that would be
a "confusion of ends". A confusion of ends arises in the following way:We want
good probabilistic predictions.Good probabilistic predictions are
calibrated.Therefore, we want calibrated predictions.The "Therefore" step
misses the fact that calibration is a necessary but not
asufficientcharacterization of good probab</p><p>4 0.51958519 <a title="205-lsi-4" href="../hunch_net-2005/hunch_net-2005-03-29-Academic_Mechanism_Design.html">48 hunch net-2005-03-29-Academic Mechanism Design</a></p>
<p>Introduction: From game theory, there is a notion of "mechanism design": setting up the
structure of the world so that participants have some incentive to do sane
things (rather than obviously counterproductive things). Application of this
principle to academic research may be fruitful.What is misdesigned about
academic research?TheJMLGguides give many hints.The common nature ofbad
reviewingalso suggests the system isn't working optimally.There are many ways
to experimentally"cheat" in machine learning.Funding Prisoner's Delimma. Good
researchers often write grant proposals for funding rather than doing
research. Since the pool of grant money is finite, this means that grant
proposals are often rejected, implying that more must be written. This is
essentially a "prisoner's delimma": anyone not writing grant proposals loses,
but the entire process of doing research is slowed by distraction. If everyone
wrote 1/2 as many grant proposals, roughly the same distribution of funding
would occur, and time w</p><p>5 0.48499885 <a title="205-lsi-5" href="../hunch_net-2005/hunch_net-2005-10-07-On-line_learning_of_regular_decision_rules.html">118 hunch net-2005-10-07-On-line learning of regular decision rules</a></p>
<p>Introduction: Many decision problems can be represented in the formFORn=1,2,â&euro;Ś:-- Reality
chooses a datumxn.-- Decision Maker chooses his decisiondn.-- Reality chooses
an observationyn.-- Decision Maker suffers lossL(yn,dn).END FOR.The
observationyncan be, for example, tomorrow's stock price and the decisiondnthe
number of shares Decision Maker chooses to buy. The datumxnideally contains
all information that might be relevant in making this decision. We do not want
to assume anything about the way Reality generates the observations and
data.Suppose there is a good and not too complex decision ruleDmapping each
datumxto a decisionD(x). Can we perform as well, or almost as well, asD,
without knowing it? This is essentially a special case of the problem ofon-
line learning.This is a simple result of this kind. Suppose the dataxnare
taken from [0,1] andL(y,d)=|y-d|. A norm ||h|| of a functionhon [0,1] is
defined by||h||2= (Integral01h(t)dt)2+ Integral01(h'(t))2dt.Decision Maker has
a strategy that guaran</p><p>6 0.46911776 <a title="205-lsi-6" href="../hunch_net-2008/hunch_net-2008-08-04-Electoralmarkets.com.html">312 hunch net-2008-08-04-Electoralmarkets.com</a></p>
<p>7 0.46886548 <a title="205-lsi-7" href="../hunch_net-2005/hunch_net-2005-11-05-The_design_of_a_computing_cluster.html">128 hunch net-2005-11-05-The design of a computing cluster</a></p>
<p>8 0.46351817 <a title="205-lsi-8" href="../hunch_net-2009/hunch_net-2009-06-03-Functionally_defined_Nonlinear_Dynamic_Models.html">359 hunch net-2009-06-03-Functionally defined Nonlinear Dynamic Models</a></p>
<p>9 0.46325523 <a title="205-lsi-9" href="../hunch_net-2006/hunch_net-2006-02-02-Introspectionism_as_a_Disease.html">153 hunch net-2006-02-02-Introspectionism as a Disease</a></p>
<p>10 0.44296879 <a title="205-lsi-10" href="../hunch_net-2005/hunch_net-2005-03-10-Breaking_Abstractions.html">39 hunch net-2005-03-10-Breaking Abstractions</a></p>
<p>11 0.43402764 <a title="205-lsi-11" href="../hunch_net-2008/hunch_net-2008-04-30-Concerns_about_the_Large_Scale_Learning_Challenge.html">300 hunch net-2008-04-30-Concerns about the Large Scale Learning Challenge</a></p>
<p>12 0.43200839 <a title="205-lsi-12" href="../hunch_net-2005/hunch_net-2005-06-29-Not_EM_for_clustering_at_COLT.html">87 hunch net-2005-06-29-Not EM for clustering at COLT</a></p>
<p>13 0.41731805 <a title="205-lsi-13" href="../hunch_net-2008/hunch_net-2008-08-24-Mass_Customized_Medicine_in_the_Future%3F.html">314 hunch net-2008-08-24-Mass Customized Medicine in the Future?</a></p>
<p>14 0.41701782 <a title="205-lsi-14" href="../hunch_net-2006/hunch_net-2006-03-27-Gradients_everywhere.html">167 hunch net-2006-03-27-Gradients everywhere</a></p>
<p>15 0.41146109 <a title="205-lsi-15" href="../hunch_net-2006/hunch_net-2006-11-06-Data_Linkage_Problems.html">217 hunch net-2006-11-06-Data Linkage Problems</a></p>
<p>16 0.40949783 <a title="205-lsi-16" href="../hunch_net-2007/hunch_net-2007-04-28-The_Coming_Patent_Apocalypse.html">241 hunch net-2007-04-28-The Coming Patent Apocalypse</a></p>
<p>17 0.40690187 <a title="205-lsi-17" href="../hunch_net-2005/hunch_net-2005-02-09-Intuitions_from_applied_learning.html">16 hunch net-2005-02-09-Intuitions from applied learning</a></p>
<p>18 0.40515283 <a title="205-lsi-18" href="../hunch_net-2008/hunch_net-2008-04-12-It_Doesn%26%238217%3Bt_Stop.html">295 hunch net-2008-04-12-It Doesn&#8217;t Stop</a></p>
<p>19 0.40310109 <a title="205-lsi-19" href="../hunch_net-2009/hunch_net-2009-12-09-Future_Publication_Models_%40_NIPS.html">382 hunch net-2009-12-09-Future Publication Models @ NIPS</a></p>
<p>20 0.40190351 <a title="205-lsi-20" href="../hunch_net-2005/hunch_net-2005-08-11-Why_Manifold-Based_Dimension_Reduction_Techniques%3F.html">102 hunch net-2005-08-11-Why Manifold-Based Dimension Reduction Techniques?</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(25, 0.012), (35, 0.031), (42, 0.2), (45, 0.015), (53, 0.419), (63, 0.02), (68, 0.023), (69, 0.033), (74, 0.082), (76, 0.012), (82, 0.041), (88, 0.027)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.95660871 <a title="205-lda-1" href="../hunch_net-2009/hunch_net-2009-12-09-Inherent_Uncertainty.html">383 hunch net-2009-12-09-Inherent Uncertainty</a></p>
<p>Introduction: I'd like to point outInherent Uncertainty, which I've added to the ML blog
post scanner on the right. My understanding fromJakeis that the intention is
to have a multiauthor blog which is more specialized towards learning
theory/game theory than this one. Nevertheless, several of the posts seem to
be of wider interest.</p><p>2 0.95390195 <a title="205-lda-2" href="../hunch_net-2010/hunch_net-2010-04-28-CI_Fellows_program_renewed.html">396 hunch net-2010-04-28-CI Fellows program renewed</a></p>
<p>Introduction: Lev Reyzinpoints out theCI Fellows program is renewed. CI Fellows are
essentiallyNSFfunded computer science postdocs for universities and industry
research labs. I've been lucky and happy to have Lev visit me for a year
underlast year's program, so I strongly recommend participating if it suits
you.As with last year, the application timeline is very short, with everything
due by May 23.</p><p>same-blog 3 0.87198114 <a title="205-lda-3" href="../hunch_net-2006/hunch_net-2006-09-07-Objective_and_subjective_interpretations_of_probability.html">205 hunch net-2006-09-07-Objective and subjective interpretations of probability</a></p>
<p>Introduction: An amusing tidbit (reproduced without permission) from Herman Chernoff's
delightful monograph, "Sequential analysis and optimal design":The use of
randomization raises a philosophical question which is articulated by the
following probably apocryphal anecdote.The metallurgist told his friend the
statistician how he planned to test the effect of heat on the strength of a
metal bar by sawing the bar into six pieces. The first two would go into the
hot oven, the next two into the medium oven, and the last two into the cool
oven. The statistician, horrified, explained how he should randomize to avoid
the effect of a possible gradient of strength in the metal bar. The method of
randomization was applied, and it turned out that the randomized experiment
called for putting the first two pieces into the hot oven, the next two into
the medium oven, and the last two into the cool oven. "Obviously, we can't do
that," said the metallurgist. "On the contrary, you have to do that," said the
statisti</p><p>4 0.83765948 <a title="205-lda-4" href="../hunch_net-2005/hunch_net-2005-09-19-NIPS_Workshops.html">113 hunch net-2005-09-19-NIPS Workshops</a></p>
<p>Introduction: Attendance at theNIPS workshopsis highly recommended for both research and
learning. Unfortunately, there does not yet appear to be a public list of
workshops. However, I found the following workshop webpages of
interest:Machine Learning in FinanceLearning to RankFoundations of Active
LearningMachine Learning Based Robotics in Unstructured EnvironmentsThere
aremanymore workshops. In fact, there are so many that it is not plausible
anyone can attend every workshop they are interested in. Maybe in future years
the organizers can spread them out over more days to reduce overlap.Many of
these workshops are accepting presentation proposals (due mid-October).</p><p>5 0.80746579 <a title="205-lda-5" href="../hunch_net-2005/hunch_net-2005-07-01-The_Role_of_Impromptu_Talks.html">88 hunch net-2005-07-01-The Role of Impromptu Talks</a></p>
<p>Introduction: COLThad an impromptu session which seemed as interesting or more interesting
than any other single technical session (despite being only an hour long).
There are several roles that an impromptu session can play
including:Announcing new work since the paper deadline. Letting this happen
now rather than later helps aid the process of research.Discussing a paper
that was rejected. Reviewers err sometimes and an impromptu session provides a
means to remedy that.Entertainment. We all like to have a bit of fun.For
design, the following seem important:Impromptu speakers should not have much
time. At COLT, it was 8 minutes, but I have seen even 5 work well.The entire
impromptu session should not last too long because the format is dense and
promotes restlessness. A half hour or hour can work well.Impromptu talks are a
mechanism to let a little bit of chaos into the schedule. They will be chaotic
in content, presentation, and usefulness. The fundamental advantage of this
chaos is that it provid</p><p>6 0.6833443 <a title="205-lda-6" href="../hunch_net-2009/hunch_net-2009-07-31-Vowpal_Wabbit_Open_Source_Project.html">365 hunch net-2009-07-31-Vowpal Wabbit Open Source Project</a></p>
<p>7 0.60016233 <a title="205-lda-7" href="../hunch_net-2005/hunch_net-2005-02-03-Learning_Theory%2C_by_assumption.html">12 hunch net-2005-02-03-Learning Theory, by assumption</a></p>
<p>8 0.47305083 <a title="205-lda-8" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>9 0.4676891 <a title="205-lda-9" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>10 0.46625391 <a title="205-lda-10" href="../hunch_net-2005/hunch_net-2005-03-10-Breaking_Abstractions.html">39 hunch net-2005-03-10-Breaking Abstractions</a></p>
<p>11 0.46441209 <a title="205-lda-11" href="../hunch_net-2011/hunch_net-2011-10-24-2011_ML_symposium_and_the_bears.html">448 hunch net-2011-10-24-2011 ML symposium and the bears</a></p>
<p>12 0.46354058 <a title="205-lda-12" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>13 0.46275583 <a title="205-lda-13" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>14 0.46093738 <a title="205-lda-14" href="../hunch_net-2011/hunch_net-2011-02-17-What_does_Watson_mean%3F.html">424 hunch net-2011-02-17-What does Watson mean?</a></p>
<p>15 0.46079654 <a title="205-lda-15" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<p>16 0.46022359 <a title="205-lda-16" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>17 0.46011189 <a title="205-lda-17" href="../hunch_net-2009/hunch_net-2009-06-01-Multitask_Poisoning.html">358 hunch net-2009-06-01-Multitask Poisoning</a></p>
<p>18 0.45943919 <a title="205-lda-18" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>19 0.4594329 <a title="205-lda-19" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>20 0.45909882 <a title="205-lda-20" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
