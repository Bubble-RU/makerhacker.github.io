<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>217 hunch net-2006-11-06-Data Linkage Problems</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-217" href="#">hunch_net-2006-217</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>217 hunch net-2006-11-06-Data Linkage Problems</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-217-html" href="http://hunch.net/?p=237">html</a></p><p>Introduction: Data linkage is a problem which seems to come up in various applied machine
learning problems. I have heard it mentioned in various data mining contexts,
but it seems relatively less studied for systemic reasons.A very simple
version of the data linkage problem is a cross hospital patient record merge.
Suppose a patient (John Doe) is admitted to a hospital (General Health),
treated, and released. Later, John Doe is admitted to a second hospital
(Health General), treated, and released. Given a large number of records of
this sort, it becomes very tempting to try and predict the outcomes of
treatments. This is reasonably straightforward as a machine learning problem
if there is a shared unique identifier for John Doe used by General Health and
Health General along with time stamps. We can merge the records and create
examples of the form "Given symptoms and treatment, did the patient come back
to a hospital within the next year?" These examples could be fed into a
learning algorithm, and</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Data linkage is a problem which seems to come up in various applied machine learning problems. [sent-1, score-0.544]
</p><p>2 A very simple version of the data linkage problem is a cross hospital patient record merge. [sent-3, score-1.245]
</p><p>3 Suppose a patient (John Doe) is admitted to a hospital (General Health), treated, and released. [sent-4, score-0.571]
</p><p>4 Later, John Doe is admitted to a second hospital (Health General), treated, and released. [sent-5, score-0.4]
</p><p>5 Given a large number of records of this sort, it becomes very tempting to try and predict the outcomes of treatments. [sent-6, score-0.514]
</p><p>6 This is reasonably straightforward as a machine learning problem if there is a shared unique identifier for John Doe used by General Health and Health General along with time stamps. [sent-7, score-0.424]
</p><p>7 We can merge the records and create examples of the form "Given symptoms and treatment, did the patient come back to a hospital within the next year? [sent-8, score-0.673]
</p><p>8 " These examples could be fed into a learning algorithm, and we could attempt to predict whether a return occurs. [sent-9, score-0.244]
</p><p>9 The problem is that General Health and Health General don't have any shared unique identifier for John Doe. [sent-10, score-0.424]
</p><p>10 Although this is just one example, data linkage problems seem to be endemic to learning applications. [sent-13, score-0.569]
</p><p>11 Sometimes minor changes to what information is recorded can strongly disambiguate. [sent-15, score-0.148]
</p><p>12 For example, there is a big difference between recording the pages visited at a website versus tracking the sequence of pages visited. [sent-16, score-0.404]
</p><p>13 The essential thing to think about when designing the information to record is: How will I track the consequences of decisions? [sent-17, score-0.228]
</p><p>14 First predict which records should be linked, based upon a smaller dataset that is hand checked. [sent-19, score-0.406]
</p><p>15 A common approach to improving performance is turning a double approximation (given x predict y, given y predict z) into a single approximation (given x predict z). [sent-29, score-1.009]
</p><p>16 A method for achieving single approximation here is tricky because we have ancillary information about the intermediate prediction. [sent-30, score-0.44]
</p><p>17 The Bayesian approach of "specify a prior, then use Bayes law to get a posterior, then predict with the posterior" is attractive here because we often have strong prior beliefs about at least the linkage portion of the problem. [sent-32, score-0.79]
</p><p>18 The data linkage problem also makes very clear the tension between privacy and machine learning. [sent-34, score-0.681]
</p><p>19 And yet, linking records can result in unexpectedly large pools of information on individuals. [sent-36, score-0.364]
</p><p>20 Furthermore explicitly sensitive information (like credit card numbers) might easily be the most useful bit of information for linkage. [sent-37, score-0.358]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('linkage', 0.486), ('hospital', 0.286), ('health', 0.269), ('records', 0.216), ('doe', 0.208), ('predict', 0.19), ('patient', 0.171), ('information', 0.148), ('link', 0.143), ('john', 0.141), ('identifier', 0.139), ('unique', 0.128), ('approximation', 0.117), ('admitted', 0.114), ('treated', 0.114), ('outcomes', 0.108), ('general', 0.102), ('shared', 0.099), ('given', 0.092), ('pages', 0.09), ('posterior', 0.087), ('data', 0.083), ('cross', 0.081), ('record', 0.08), ('improved', 0.07), ('intermediate', 0.062), ('card', 0.062), ('recording', 0.062), ('stage', 0.062), ('predictor', 0.061), ('problem', 0.058), ('systemic', 0.057), ('birthday', 0.057), ('attractive', 0.057), ('ancillary', 0.057), ('index', 0.057), ('linked', 0.057), ('treatments', 0.057), ('turning', 0.057), ('prior', 0.057), ('single', 0.056), ('representing', 0.054), ('suggestion', 0.054), ('tension', 0.054), ('versus', 0.054), ('jump', 0.054), ('fed', 0.054), ('tracking', 0.054), ('contexts', 0.054), ('visited', 0.054)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9999997 <a title="217-tfidf-1" href="../hunch_net-2006/hunch_net-2006-11-06-Data_Linkage_Problems.html">217 hunch net-2006-11-06-Data Linkage Problems</a></p>
<p>Introduction: Data linkage is a problem which seems to come up in various applied machine
learning problems. I have heard it mentioned in various data mining contexts,
but it seems relatively less studied for systemic reasons.A very simple
version of the data linkage problem is a cross hospital patient record merge.
Suppose a patient (John Doe) is admitted to a hospital (General Health),
treated, and released. Later, John Doe is admitted to a second hospital
(Health General), treated, and released. Given a large number of records of
this sort, it becomes very tempting to try and predict the outcomes of
treatments. This is reasonably straightforward as a machine learning problem
if there is a shared unique identifier for John Doe used by General Health and
Health General along with time stamps. We can merge the records and create
examples of the form "Given symptoms and treatment, did the patient come back
to a hospital within the next year?" These examples could be fed into a
learning algorithm, and</p><p>2 0.19164529 <a title="217-tfidf-2" href="../hunch_net-2011/hunch_net-2011-04-11-The_Heritage_Health_Prize.html">430 hunch net-2011-04-11-The Heritage Health Prize</a></p>
<p>Introduction: TheHeritage Health Prizeis potentially the largest prediction prize yet at
$3M, which is sure to get many people interested. Several elements of the
competition may be worth discussing.The most straightforward way for HPN to
deploy this predictor is in determining who to cover with insurance. This
might easily cover the costs of running the contest itself, but the value to
the health system of a whole is minimal, as people not covered still exist.
While HPN itself is a provider network, they have active relationships with a
number of insurance companies, and the right to resell any entrant. It's worth
keeping in mind that the research and development may nevertheless end up
being useful in the longer term, especially as entrants also keep the right to
their code.Thejudging metricis something I haven't seen previously. If a
patient has probability 0.5 of being in the hospital 0 days and probability
0.5 of being in the hospital ~53.6 days, the optimal prediction in expectation
is ~6.4 da</p><p>3 0.14100905 <a title="217-tfidf-3" href="../hunch_net-2006/hunch_net-2006-03-23-The_Approximation_Argument.html">165 hunch net-2006-03-23-The Approximation Argument</a></p>
<p>Introduction: An argument is sometimes made that the Bayesian way is the "right" way to do
machine learning. This is a serious argument which deserves a serious reply.
The approximation argument is a serious reply for which I have not yet seen a
reply2.The idea for the Bayesian approach is quite simple, elegant, and
general. Essentially, you first specify a priorP(D)over possible
processesDproducing the data, observe the data, then condition on the data
according to Bayes law to construct a posterior:P(D|x) = P(x|D)P(D)/P(x)After
this, hard decisions are made (such as "turn left" or "turn right") by
choosing the one which minimizes the expected (with respect to the posterior)
loss.This basic idea is reused thousands of times with various choices
ofP(D)and loss functions which is unsurprising given the many nice
properties:There is an extremely strong associated guarantee: If the actual
distribution generating the data is drawn fromP(D)there is no better method.
One way to think about this is that in</p><p>4 0.11681756 <a title="217-tfidf-4" href="../hunch_net-2008/hunch_net-2008-08-24-Mass_Customized_Medicine_in_the_Future%3F.html">314 hunch net-2008-08-24-Mass Customized Medicine in the Future?</a></p>
<p>Introduction: This post is about a technology which could develop in the future.Right now, a
new drug might be tested by finding patients with some diagnosis and giving or
not giving them a drug according to a secret randomization. The outcome is
observed, and if the average outcome for those treated is measurably better
than the average outcome for those not treated, the drug might become a
standard treatment.Generalizing this, a filterFsorts people into two groups:
those for treatmentAand those not for treatmentBbased upon observationsx. To
measure the outcome, you randomize between treatment and nontreatment of
groupAand measure the relative performance of the treatment.A problem often
arises: in many cases the treated group does not do better than the nontreated
group. A basic question is: does this mean the treatment is bad? With respect
to the filterFit may mean that, but with respect to another filterF', the
treatment might be very effective. For example, a drug might work great for
people wh</p><p>5 0.11118101 <a title="217-tfidf-5" href="../hunch_net-2009/hunch_net-2009-08-27-New_York_Area_Machine_Learning_Events.html">369 hunch net-2009-08-27-New York Area Machine Learning Events</a></p>
<p>Introduction: Several events are happening in the NY area.Barriers in Computational Learning
Theory Workshop, Aug 28.That's tomorrow near Princeton. I'm looking forward to
speaking at this one on "Getting around Barriers in Learning Theory", but
several other talks are of interest, particularly to the CS theory
inclined.Claudia Perlichis running theINFORMS Data Mining Contestwith a
deadline of Sept. 25. This is a contest using real health record data (they
partnered withHealthCare Intelligence) to predict transfers and mortality. In
the current US health care reform debate, the case studies of high costs we
hear strongly suggest machine learning & statistics can save many billions.The
Singularity Summit October 3&4\. This is for the AIists out there. Several of
the talks look interesting, although unfortunately I'll miss it
forALT.Predictive Analytics World, Oct 20-21. This is stretching the
definition of "New York Area" a bit, but the train to DC is reasonable. This
is a conference of case studies</p><p>6 0.10315546 <a title="217-tfidf-6" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>7 0.096676044 <a title="217-tfidf-7" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>8 0.094484568 <a title="217-tfidf-8" href="../hunch_net-2005/hunch_net-2005-02-03-Learning_Theory%2C_by_assumption.html">12 hunch net-2005-02-03-Learning Theory, by assumption</a></p>
<p>9 0.093821861 <a title="217-tfidf-9" href="../hunch_net-2006/hunch_net-2006-11-20-Context_and_the_calculation_misperception.html">218 hunch net-2006-11-20-Context and the calculation misperception</a></p>
<p>10 0.090528637 <a title="217-tfidf-10" href="../hunch_net-2005/hunch_net-2005-04-23-Advantages_and_Disadvantages_of_Bayesian_Learning.html">60 hunch net-2005-04-23-Advantages and Disadvantages of Bayesian Learning</a></p>
<p>11 0.089827277 <a title="217-tfidf-11" href="../hunch_net-2005/hunch_net-2005-03-02-Prior%2C_%26%238220%3BPrior%26%238221%3B_and_Bias.html">34 hunch net-2005-03-02-Prior, &#8220;Prior&#8221; and Bias</a></p>
<p>12 0.088090301 <a title="217-tfidf-12" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>13 0.085481927 <a title="217-tfidf-13" href="../hunch_net-2008/hunch_net-2008-08-04-Electoralmarkets.com.html">312 hunch net-2008-08-04-Electoralmarkets.com</a></p>
<p>14 0.084218524 <a title="217-tfidf-14" href="../hunch_net-2010/hunch_net-2010-09-21-Regretting_the_dead.html">411 hunch net-2010-09-21-Regretting the dead</a></p>
<p>15 0.082770109 <a title="217-tfidf-15" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>16 0.082655348 <a title="217-tfidf-16" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>17 0.08204516 <a title="217-tfidf-17" href="../hunch_net-2007/hunch_net-2007-08-25-The_Privacy_Problem.html">260 hunch net-2007-08-25-The Privacy Problem</a></p>
<p>18 0.081910476 <a title="217-tfidf-18" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>19 0.076792367 <a title="217-tfidf-19" href="../hunch_net-2006/hunch_net-2006-09-12-Incentive_Compatible_Reviewing.html">207 hunch net-2006-09-12-Incentive Compatible Reviewing</a></p>
<p>20 0.07671836 <a title="217-tfidf-20" href="../hunch_net-2005/hunch_net-2005-02-21-Problem%3A_Cross_Validation.html">26 hunch net-2005-02-21-Problem: Cross Validation</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.193), (1, -0.067), (2, 0.02), (3, -0.021), (4, -0.01), (5, 0.022), (6, -0.1), (7, -0.011), (8, 0.089), (9, 0.001), (10, -0.11), (11, 0.021), (12, -0.036), (13, -0.031), (14, -0.007), (15, -0.111), (16, 0.023), (17, 0.046), (18, -0.052), (19, -0.006), (20, -0.013), (21, 0.007), (22, 0.074), (23, -0.065), (24, -0.017), (25, 0.001), (26, 0.017), (27, 0.027), (28, 0.013), (29, -0.041), (30, -0.1), (31, -0.076), (32, 0.029), (33, -0.04), (34, 0.105), (35, -0.064), (36, 0.107), (37, 0.048), (38, 0.012), (39, 0.026), (40, -0.043), (41, 0.009), (42, 0.04), (43, 0.024), (44, 0.025), (45, -0.015), (46, -0.039), (47, -0.033), (48, -0.101), (49, -0.036)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95599508 <a title="217-lsi-1" href="../hunch_net-2006/hunch_net-2006-11-06-Data_Linkage_Problems.html">217 hunch net-2006-11-06-Data Linkage Problems</a></p>
<p>Introduction: Data linkage is a problem which seems to come up in various applied machine
learning problems. I have heard it mentioned in various data mining contexts,
but it seems relatively less studied for systemic reasons.A very simple
version of the data linkage problem is a cross hospital patient record merge.
Suppose a patient (John Doe) is admitted to a hospital (General Health),
treated, and released. Later, John Doe is admitted to a second hospital
(Health General), treated, and released. Given a large number of records of
this sort, it becomes very tempting to try and predict the outcomes of
treatments. This is reasonably straightforward as a machine learning problem
if there is a shared unique identifier for John Doe used by General Health and
Health General along with time stamps. We can merge the records and create
examples of the form "Given symptoms and treatment, did the patient come back
to a hospital within the next year?" These examples could be fed into a
learning algorithm, and</p><p>2 0.63761187 <a title="217-lsi-2" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>Introduction: One of the common trends in machine learning has been an emphasis on the use
of unlabeled data. The argument goes something like "there aren't many labeled
web pages out there, but there are ahugenumber of web pages, so we must find a
way to take advantage of them." There are several standard approaches for
doing this:Unsupervised Learning. You use only unlabeled data. In a typical
application, you cluster the data and hope that the clusters somehow
correspond to what you care about.Semisupervised Learning. You use both
unlabeled and labeled data to build a predictor. The unlabeled data influences
the learned predictor in some way.Active Learning. You have unlabeled data and
access to a labeling oracle. You interactively choose which examples to label
so as to optimize prediction accuracy.It seems there is a fourth approach
worth serious investigation--automated labeling. The approach goes as
follows:Identify some subset of observed values to predict from the
others.Build a predictor.U</p><p>3 0.6334902 <a title="217-lsi-3" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>Introduction: Machine learning has a new kind of "scaling to larger problems" to worry
about: scaling with the amount of contextual information. The standard
development path for a machine learning application in practice seems to be
the following:Marginal. In the beginning, there was "majority vote". At this
stage, it isn't necessary to understand that you have a prediction problem.
People just realize that one answer is right sometimes and another answer
other times. In machine learning terms, this corresponds to making a
prediction without side information.First context. A clever person realizes
that some bit of informationx1could be helpful. Ifx1is discrete, they
condition on it and make a predictorh(x1), typically by counting. If they are
clever, then they also do some smoothing. Ifx1is some real valued parameter,
it's very common to make a threshold cutoff. Often, these tasks are simply
done by hand.Second. Another clever person (or perhaps the same one) realizes
that some other bit of informa</p><p>4 0.60341042 <a title="217-lsi-4" href="../hunch_net-2011/hunch_net-2011-04-11-The_Heritage_Health_Prize.html">430 hunch net-2011-04-11-The Heritage Health Prize</a></p>
<p>Introduction: TheHeritage Health Prizeis potentially the largest prediction prize yet at
$3M, which is sure to get many people interested. Several elements of the
competition may be worth discussing.The most straightforward way for HPN to
deploy this predictor is in determining who to cover with insurance. This
might easily cover the costs of running the contest itself, but the value to
the health system of a whole is minimal, as people not covered still exist.
While HPN itself is a provider network, they have active relationships with a
number of insurance companies, and the right to resell any entrant. It's worth
keeping in mind that the research and development may nevertheless end up
being useful in the longer term, especially as entrants also keep the right to
their code.Thejudging metricis something I haven't seen previously. If a
patient has probability 0.5 of being in the hospital 0 days and probability
0.5 of being in the hospital ~53.6 days, the optimal prediction in expectation
is ~6.4 da</p><p>5 0.59939682 <a title="217-lsi-5" href="../hunch_net-2008/hunch_net-2008-08-24-Mass_Customized_Medicine_in_the_Future%3F.html">314 hunch net-2008-08-24-Mass Customized Medicine in the Future?</a></p>
<p>Introduction: This post is about a technology which could develop in the future.Right now, a
new drug might be tested by finding patients with some diagnosis and giving or
not giving them a drug according to a secret randomization. The outcome is
observed, and if the average outcome for those treated is measurably better
than the average outcome for those not treated, the drug might become a
standard treatment.Generalizing this, a filterFsorts people into two groups:
those for treatmentAand those not for treatmentBbased upon observationsx. To
measure the outcome, you randomize between treatment and nontreatment of
groupAand measure the relative performance of the treatment.A problem often
arises: in many cases the treated group does not do better than the nontreated
group. A basic question is: does this mean the treatment is bad? With respect
to the filterFit may mean that, but with respect to another filterF', the
treatment might be very effective. For example, a drug might work great for
people wh</p><p>6 0.58883226 <a title="217-lsi-6" href="../hunch_net-2006/hunch_net-2006-03-23-The_Approximation_Argument.html">165 hunch net-2006-03-23-The Approximation Argument</a></p>
<p>7 0.57858855 <a title="217-lsi-7" href="../hunch_net-2006/hunch_net-2006-03-02-Why_do_people_count_for_learning%3F.html">160 hunch net-2006-03-02-Why do people count for learning?</a></p>
<p>8 0.57508463 <a title="217-lsi-8" href="../hunch_net-2005/hunch_net-2005-03-02-Prior%2C_%26%238220%3BPrior%26%238221%3B_and_Bias.html">34 hunch net-2005-03-02-Prior, &#8220;Prior&#8221; and Bias</a></p>
<p>9 0.57446396 <a title="217-lsi-9" href="../hunch_net-2007/hunch_net-2007-08-25-The_Privacy_Problem.html">260 hunch net-2007-08-25-The Privacy Problem</a></p>
<p>10 0.56873858 <a title="217-lsi-10" href="../hunch_net-2005/hunch_net-2005-02-03-Learning_Theory%2C_by_assumption.html">12 hunch net-2005-02-03-Learning Theory, by assumption</a></p>
<p>11 0.56575245 <a title="217-lsi-11" href="../hunch_net-2006/hunch_net-2006-03-05-%26%238220%3BStructural%26%238221%3B_Learning.html">161 hunch net-2006-03-05-&#8220;Structural&#8221; Learning</a></p>
<p>12 0.56444383 <a title="217-lsi-12" href="../hunch_net-2009/hunch_net-2009-06-03-Functionally_defined_Nonlinear_Dynamic_Models.html">359 hunch net-2009-06-03-Functionally defined Nonlinear Dynamic Models</a></p>
<p>13 0.55360395 <a title="217-lsi-13" href="../hunch_net-2008/hunch_net-2008-04-26-Eliminating_the_Birthday_Paradox_for_Universal_Features.html">298 hunch net-2008-04-26-Eliminating the Birthday Paradox for Universal Features</a></p>
<p>14 0.55219769 <a title="217-lsi-14" href="../hunch_net-2006/hunch_net-2006-01-23-On_Coding_via_Mutual_Information_%26%23038%3B_Bayes_Nets.html">150 hunch net-2006-01-23-On Coding via Mutual Information &#038; Bayes Nets</a></p>
<p>15 0.55186689 <a title="217-lsi-15" href="../hunch_net-2008/hunch_net-2008-08-04-Electoralmarkets.com.html">312 hunch net-2008-08-04-Electoralmarkets.com</a></p>
<p>16 0.54615253 <a title="217-lsi-16" href="../hunch_net-2006/hunch_net-2006-02-18-Multiplication_of_Learned_Probabilities_is_Dangerous.html">157 hunch net-2006-02-18-Multiplication of Learned Probabilities is Dangerous</a></p>
<p>17 0.52752453 <a title="217-lsi-17" href="../hunch_net-2007/hunch_net-2007-09-18-It%26%238217%3Bs_MDL_Jim%2C_but_not_as_we_know_it%26%238230%3B%28on_Bayes%2C_MDL_and_consistency%29.html">263 hunch net-2007-09-18-It&#8217;s MDL Jim, but not as we know it&#8230;(on Bayes, MDL and consistency)</a></p>
<p>18 0.52376574 <a title="217-lsi-18" href="../hunch_net-2008/hunch_net-2008-12-07-A_NIPS_paper.html">330 hunch net-2008-12-07-A NIPS paper</a></p>
<p>19 0.51844782 <a title="217-lsi-19" href="../hunch_net-2006/hunch_net-2006-11-20-Context_and_the_calculation_misperception.html">218 hunch net-2006-11-20-Context and the calculation misperception</a></p>
<p>20 0.5092724 <a title="217-lsi-20" href="../hunch_net-2005/hunch_net-2005-08-11-Why_Manifold-Based_Dimension_Reduction_Techniques%3F.html">102 hunch net-2005-08-11-Why Manifold-Based Dimension Reduction Techniques?</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(4, 0.012), (8, 0.21), (35, 0.092), (39, 0.011), (42, 0.193), (45, 0.025), (50, 0.029), (62, 0.026), (68, 0.017), (74, 0.133), (76, 0.054), (82, 0.044), (88, 0.035), (95, 0.015)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.91104949 <a title="217-lda-1" href="../hunch_net-2011/hunch_net-2011-08-01-Interesting_papers_at_COLT_2011.html">439 hunch net-2011-08-01-Interesting papers at COLT 2011</a></p>
<p>Introduction: SinceJohndid not attendCOLTthis year, I have been volunteered to report back
on the hot stuff at this year's meeting. The conference seemed to have pretty
high quality stuff this year, and I found plenty of interesting papers on all
the three days. I'm gonna pick some of my favorites going through the program
in a chronological order.The first session on matrices seemed interesting for
two reasons. First, the papers were quite nice. But more interestingly, this
is a topic that has had a lot of presence in Statistics and Compressed sensing
literature recently. So it was good to see high-dimensional matrices finally
make their entry at COLT. The paper ofOhadandShaionCollaborative Filtering
with the Trace Norm: Learning, Bounding, and Transducingprovides non-trivial
guarantees on trace norm regularization in an agnostic setup, while Rina
andNatishow how Rademacher averages can be used to get sharper results for
matrix completion problems in their paperConcentration-Based Guarantees for
Lo</p><p>same-blog 2 0.88174063 <a title="217-lda-2" href="../hunch_net-2006/hunch_net-2006-11-06-Data_Linkage_Problems.html">217 hunch net-2006-11-06-Data Linkage Problems</a></p>
<p>Introduction: Data linkage is a problem which seems to come up in various applied machine
learning problems. I have heard it mentioned in various data mining contexts,
but it seems relatively less studied for systemic reasons.A very simple
version of the data linkage problem is a cross hospital patient record merge.
Suppose a patient (John Doe) is admitted to a hospital (General Health),
treated, and released. Later, John Doe is admitted to a second hospital
(Health General), treated, and released. Given a large number of records of
this sort, it becomes very tempting to try and predict the outcomes of
treatments. This is reasonably straightforward as a machine learning problem
if there is a shared unique identifier for John Doe used by General Health and
Health General along with time stamps. We can merge the records and create
examples of the form "Given symptoms and treatment, did the patient come back
to a hospital within the next year?" These examples could be fed into a
learning algorithm, and</p><p>3 0.84250355 <a title="217-lda-3" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>Introduction: TheInternational Planning Competition(IPC) is a biennial event organized in
the context of theInternational Conference on Automated Planning and
Scheduling(ICAPS). This year, for the first time, there will a learning track
of the competition. For more information you can go to the competitionweb-
site.The competitions are typically organized around a number of planning
domains that can vary from year to year, where a planning domain is simply a
class of problems that share a common action schema--e.g. Blocksworld is a
well-known planning domain that contains a problem instance each possible
initial tower configuration and goal configuration. Some other domains have
included Logistics, Airport, Freecell, PipesWorld, and manyothers. For each
domain the competition includes a number of problems (say 40-50) and the
planners are run on each problem with a time limit for each problem (around 30
minutes). The problems are hard enough that many problems are not solved
within the time limit.Giv</p><p>4 0.84242117 <a title="217-lda-4" href="../hunch_net-2006/hunch_net-2006-07-08-MaxEnt_contradicts_Bayes_Rule%3F.html">191 hunch net-2006-07-08-MaxEnt contradicts Bayes Rule?</a></p>
<p>Introduction: A few weeks ago I readthis. David Blei and I spent some time thinking hard
about this a few years back (thanks to Kary Myers for pointing us to it):In
short I was thinking that Ã¢â‚¬Å“bayesian belief updatingÃ¢â‚¬Â and
Ã¢â‚¬Å“maximum entropyÃ¢â‚¬Â were two othogonal principles. But it appear
that they are not, and that they can even be in conflict !Example (from Kass
1996); consider a Die (6 sides), consider prior knowledge E[X]=3.5.Maximum
entropy leads to P(X)= (1/6, 1/6, 1/6, 1/6, 1/6, 1/6).Now consider a new piece
of evidence A=Ã¢â‚¬ÂX is an odd numberÃ¢â‚¬ÂBayesian posterior P(X|A)=
P(A|X) P(X) = (1/3, 0, 1/3, 0, 1/3, 0).But MaxEnt with the constraints
E[X]=3.5 and E[Indicator function of A]=1 leads to (.22, 0, .32, 0, .47, 0) !!
(note that E[Indicator function of A]=P(A))Indeed, for MaxEnt, because there
is no more Ã¢â‚¬Ëœ6Ã¢â‚¬Â², big numbers must be more probable to ensure an
average of 3.5. For bayesian updating, P(X|A) doesnÃ¢â‚¬â„¢t have to have a
3.5 expectation. P(X) a</p><p>5 0.74535537 <a title="217-lda-5" href="../hunch_net-2009/hunch_net-2009-12-24-Top_graduates_this_season.html">384 hunch net-2009-12-24-Top graduates this season</a></p>
<p>Introduction: I would like to point out 3 graduates this season as having my confidence they
are capable of doing great things.Daniel Hsuhas diverse papers with diverse
coauthors on {active learning, mulitlabeling, temporal learning, â&euro;Ś} each
covering new algorithms and methods of analysis. He is also a capable
programmer, having helped me with some nitty-gritty details of cluster
parallelVowpal Wabbitthis summer. He has an excellent tendency to just get
things done.Nicolas Lambertdoesn't nominally work in machine learning, but
I've found his work inelicitationrelevant nevertheless. In essence, elicitable
properties are closely related to learnable properties, and the elicitation
complexity is related to a notion of learning complexity. See theSurrogate
regret bounds paperfor some related discussion. Few people successfully work
at such a general level that it crosses fields, but he's one of them.Yisong
Yueis deeply focused on interactive learning, which he has attacked at all
levels: theory, algorit</p><p>6 0.7313832 <a title="217-lda-6" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>7 0.73095191 <a title="217-lda-7" href="../hunch_net-2009/hunch_net-2009-02-22-Effective_Research_Funding.html">344 hunch net-2009-02-22-Effective Research Funding</a></p>
<p>8 0.73020291 <a title="217-lda-8" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>9 0.73009253 <a title="217-lda-9" href="../hunch_net-2012/hunch_net-2012-06-05-ICML_acceptance_statistics.html">466 hunch net-2012-06-05-ICML acceptance statistics</a></p>
<p>10 0.72837466 <a title="217-lda-10" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>11 0.72539312 <a title="217-lda-11" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>12 0.72511774 <a title="217-lda-12" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>13 0.72359157 <a title="217-lda-13" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>14 0.72308701 <a title="217-lda-14" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>15 0.72153831 <a title="217-lda-15" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">23 hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<p>16 0.72042274 <a title="217-lda-16" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>17 0.72010422 <a title="217-lda-17" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>18 0.71757066 <a title="217-lda-18" href="../hunch_net-2006/hunch_net-2006-09-12-Incentive_Compatible_Reviewing.html">207 hunch net-2006-09-12-Incentive Compatible Reviewing</a></p>
<p>19 0.71745867 <a title="217-lda-19" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>20 0.71741897 <a title="217-lda-20" href="../hunch_net-2011/hunch_net-2011-08-06-Interesting_thing_at_UAI_2011.html">440 hunch net-2011-08-06-Interesting thing at UAI 2011</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
