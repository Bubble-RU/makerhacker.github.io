<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>156 hunch net-2006-02-11-Yahoo&#8217;s Learning Problems.</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-156" href="#">hunch_net-2006-156</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>156 hunch net-2006-02-11-Yahoo&#8217;s Learning Problems.</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-156-html" href="http://hunch.net/?p=167">html</a></p><p>Introduction: I just visited  Yahoo Research  which has several fundamental learning problems near to (or beyond) the set of problems we know how to solve well.  Here are 3 of them.
  
  Ranking   This is the canonical problem of all search engines.  It is made extra difficult for several reasons.
 
 There is relatively little “good” supervised learning data and a great deal of data with some signal (such as click through rates). 
 The learning must occur in a partially adversarial environment. Many people very actively attempt to place themselves at the top of 
rankings. 
 It is not even quite clear whether the problem should be posed as ‘ranking’ or as ‘regression’ which is then used to produce a 
ranking. 
 
 
  Collaborative filtering  Yahoo has a large number of recommendation systems for music, movies, etc…  In these sorts of systems, users specify how they liked a set of things, and then the system can (hopefully) find some more examples of things they might like 
by reasoning across multiple</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 I just visited  Yahoo Research  which has several fundamental learning problems near to (or beyond) the set of problems we know how to solve well. [sent-1, score-0.37]
</p><p>2 Ranking   This is the canonical problem of all search engines. [sent-3, score-0.361]
</p><p>3 There is relatively little “good” supervised learning data and a great deal of data with some signal (such as click through rates). [sent-5, score-0.269]
</p><p>4 The learning must occur in a partially adversarial environment. [sent-6, score-0.088]
</p><p>5 Many people very actively attempt to place themselves at the top of  rankings. [sent-7, score-0.206]
</p><p>6 It is not even quite clear whether the problem should be posed as ‘ranking’ or as ‘regression’ which is then used to produce a  ranking. [sent-8, score-0.098]
</p><p>7 Exploration with Generalization  The cash cow of  search engines is displaying advertisements which are relevant to search along with search results. [sent-10, score-1.487]
</p><p>8 Better targeting these advertisements makes money (a small improvement might be worth $millions) and improves the value of the search engine for the user. [sent-11, score-1.054]
</p><p>9 It is natural to predict the set of advertisements which maximize the advertising payoff. [sent-12, score-0.823]
</p><p>10 This natural idea is stymied by both the extreme  multiplicity of advertisements under contract (think millions) and a lack of ability to measure hypotheticals like “What would have  happened if we had displayed a different set of advertisements for this (query,user) pair instead? [sent-13, score-1.693]
</p><p>11 ”  This is a combined exploration and  generalization problem. [sent-14, score-0.338]
</p><p>12 Good solutions to any of these problems would be extremely useful (and not just at Yahoo). [sent-15, score-0.201]
</p><p>13 Even further small improvements on the existing solutions may be very useful. [sent-16, score-0.181]
</p><p>14 For those interested, Yahoo (as an organization) knows these are learning problems and is very actively interested in solving them. [sent-17, score-0.452]
</p><p>15 Yahoo Research is committed to a relatively open method of solving these problems. [sent-18, score-0.247]
</p><p>16 Dennis DeCoste  is one contact point for machine learning research at Yahoo Research. [sent-19, score-0.175]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('advertisements', 0.461), ('yahoo', 0.368), ('search', 0.282), ('millions', 0.184), ('generalization', 0.142), ('ranking', 0.136), ('actively', 0.129), ('exploration', 0.123), ('set', 0.111), ('solutions', 0.109), ('stymied', 0.105), ('displayed', 0.105), ('pair', 0.105), ('posed', 0.098), ('multiplicity', 0.098), ('contract', 0.098), ('movies', 0.098), ('research', 0.093), ('problems', 0.092), ('signal', 0.092), ('engines', 0.092), ('click', 0.092), ('occur', 0.088), ('maximize', 0.088), ('cash', 0.088), ('systems', 0.085), ('relatively', 0.085), ('reasoning', 0.084), ('engine', 0.084), ('targeting', 0.084), ('advertising', 0.084), ('collaborative', 0.084), ('committed', 0.084), ('contact', 0.082), ('natural', 0.079), ('canonical', 0.079), ('knows', 0.079), ('solving', 0.078), ('top', 0.077), ('organization', 0.075), ('recommendation', 0.075), ('visited', 0.075), ('interested', 0.074), ('combined', 0.073), ('filtering', 0.073), ('liked', 0.073), ('small', 0.072), ('users', 0.071), ('improves', 0.071), ('happened', 0.07)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="156-tfidf-1" href="../hunch_net-2006/hunch_net-2006-02-11-Yahoo%26%238217%3Bs_Learning_Problems..html">156 hunch net-2006-02-11-Yahoo&#8217;s Learning Problems.</a></p>
<p>Introduction: I just visited  Yahoo Research  which has several fundamental learning problems near to (or beyond) the set of problems we know how to solve well.  Here are 3 of them.
  
  Ranking   This is the canonical problem of all search engines.  It is made extra difficult for several reasons.
 
 There is relatively little “good” supervised learning data and a great deal of data with some signal (such as click through rates). 
 The learning must occur in a partially adversarial environment. Many people very actively attempt to place themselves at the top of 
rankings. 
 It is not even quite clear whether the problem should be posed as ‘ranking’ or as ‘regression’ which is then used to produce a 
ranking. 
 
 
  Collaborative filtering  Yahoo has a large number of recommendation systems for music, movies, etc…  In these sorts of systems, users specify how they liked a set of things, and then the system can (hopefully) find some more examples of things they might like 
by reasoning across multiple</p><p>2 0.30142906 <a title="156-tfidf-2" href="../hunch_net-2006/hunch_net-2006-04-30-John_Langford_%26%238211%3B%3E_Yahoo_Research%2C_NY.html">175 hunch net-2006-04-30-John Langford &#8211;> Yahoo Research, NY</a></p>
<p>Introduction: I will join  Yahoo Research  (in New York) after my contract ends at  TTI-Chicago .
 
The deciding reasons are:
  
 Yahoo is running into many hard learning problems.  This is precisely the situation where basic research might hope to have the greatest impact. 
 Yahoo Research understands research including publishing, conferences, etc… 
 Yahoo Research is growing, so there is a chance I can help it grow well. 
 Yahoo understands the internet, including (but not at all limited to) experimenting with research blogs. 
  
In the end, Yahoo Research seems like the place where I might have a chance to make the greatest difference.  
 
Yahoo (as a company) has made a strong bet on Yahoo Research.  We-the-researchers all hope that bet will pay off, and this seems plausible.  I’ll certainly have fun trying.</p><p>3 0.24615458 <a title="156-tfidf-3" href="../hunch_net-2006/hunch_net-2006-05-08-Big_machine_learning.html">178 hunch net-2006-05-08-Big machine learning</a></p>
<p>Introduction: According to the  New York Times ,  Yahoo is releasing Project Panama shortly .  Project Panama is about better predicting which advertisements are relevant to a search, implying a higher click through rate, implying larger income for  Yahoo .  There are two things that seem interesting here:
  
 A significant portion of that improved accuracy is almost certainly machine learning at work. 
 The quantitative effect is huge—the estimate in the article is $600*10 6 . 
  
 Google  already has such improvements and  Microsoft Search  is surely working on them, which suggest this is (perhaps) a $10 9  per year machine learning problem. 
 
The exact methodology under use is unlikely to be publicly discussed in the near future because of the competitive enivironment.  Hopefully we’ll have some public “war stories” at some point in the future when this information becomes less sensitive.  For now, it’s reassuring to simply note that machine learning is having a big impact.</p><p>4 0.17477106 <a title="156-tfidf-4" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<p>Introduction: I want to comment on the “Bing copies Google” discussion  here ,  here , and  here , because there are data-related issues which the general public may not understand, and some of the framing seems substantially misleading to me.
 
As a not-distant-outsider, let me mention the sources of bias I may have.  I work at  Yahoo! , which has started using  Bing .  This might predispose me towards Bing, but on the other hand I’m still at Yahoo!, and have been using  Linux  exclusively as an OS for many years, including even a couple minor kernel patches.  And,  on the gripping hand , I’ve spent quite a bit of time thinking about the basic  principles of incorporating user feedback in machine learning .  Also note, this post is not  related to official Yahoo! policy, it’s just my personal view.
 
 The issue  Google engineers inserted synthetic responses to synthetic queries on google.com, then executed the synthetic searches on google.com using Internet Explorer with the Bing toolbar and later</p><p>5 0.17184018 <a title="156-tfidf-5" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>Introduction: “Search” is the other branch of AI research which has been succesful.   Concrete examples include  Deep Blue  which beat the world chess champion and  Chinook  the champion checkers program.  A set of core search techniques exist including A * , alpha-beta pruning, and others that can be applied to any of many different search problems.
 
Given this, it may be surprising to learn that there has been relatively little succesful work on combining prediction and search.  Given also that  humans typically solve search problems using a number of predictive heuristics to narrow in on a solution, we might be surprised again.  However, the big successful search-based systems have typically not used “smart” search algorithms.  Insteady they have optimized for very fast search.  This is not for lack of trying… many people have tried to synthesize search and prediction to various degrees of success.   For example,  Knightcap  achieves good-but-not-stellar chess playing performance, and  TD-gammon</p><p>6 0.16628957 <a title="156-tfidf-6" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>7 0.16348863 <a title="156-tfidf-7" href="../hunch_net-2006/hunch_net-2006-08-03-AOL%26%238217%3Bs_data_drop.html">200 hunch net-2006-08-03-AOL&#8217;s data drop</a></p>
<p>8 0.12948102 <a title="156-tfidf-8" href="../hunch_net-2010/hunch_net-2010-02-26-Yahoo%21_ML_events.html">389 hunch net-2010-02-26-Yahoo! ML events</a></p>
<p>9 0.1189247 <a title="156-tfidf-9" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>10 0.11846599 <a title="156-tfidf-10" href="../hunch_net-2007/hunch_net-2007-10-24-Contextual_Bandits.html">269 hunch net-2007-10-24-Contextual Bandits</a></p>
<p>11 0.11597265 <a title="156-tfidf-11" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>12 0.11319853 <a title="156-tfidf-12" href="../hunch_net-2011/hunch_net-2011-02-25-Yahoo%21_Machine_Learning_grant_due_March_11.html">425 hunch net-2011-02-25-Yahoo! Machine Learning grant due March 11</a></p>
<p>13 0.1120394 <a title="156-tfidf-13" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>14 0.1062197 <a title="156-tfidf-14" href="../hunch_net-2009/hunch_net-2009-07-11-Interesting_papers_at_KDD.html">364 hunch net-2009-07-11-Interesting papers at KDD</a></p>
<p>15 0.10333464 <a title="156-tfidf-15" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>16 0.10255069 <a title="156-tfidf-16" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>17 0.097792082 <a title="156-tfidf-17" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>18 0.094224446 <a title="156-tfidf-18" href="../hunch_net-2007/hunch_net-2007-08-25-The_Privacy_Problem.html">260 hunch net-2007-08-25-The Privacy Problem</a></p>
<p>19 0.09259782 <a title="156-tfidf-19" href="../hunch_net-2009/hunch_net-2009-06-03-Functionally_defined_Nonlinear_Dynamic_Models.html">359 hunch net-2009-06-03-Functionally defined Nonlinear Dynamic Models</a></p>
<p>20 0.085778125 <a title="156-tfidf-20" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.205), (1, 0.025), (2, -0.113), (3, 0.094), (4, -0.071), (5, -0.045), (6, 0.002), (7, 0.067), (8, -0.079), (9, -0.041), (10, 0.002), (11, 0.145), (12, -0.061), (13, 0.113), (14, -0.141), (15, 0.146), (16, -0.087), (17, -0.064), (18, 0.037), (19, -0.153), (20, 0.112), (21, 0.07), (22, 0.107), (23, -0.024), (24, 0.019), (25, 0.2), (26, 0.124), (27, -0.016), (28, -0.066), (29, 0.031), (30, -0.071), (31, 0.017), (32, -0.099), (33, -0.011), (34, 0.058), (35, 0.097), (36, 0.033), (37, -0.012), (38, 0.026), (39, 0.03), (40, 0.041), (41, -0.051), (42, -0.059), (43, -0.047), (44, 0.097), (45, -0.019), (46, 0.059), (47, 0.136), (48, 0.022), (49, 0.013)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95299762 <a title="156-lsi-1" href="../hunch_net-2006/hunch_net-2006-02-11-Yahoo%26%238217%3Bs_Learning_Problems..html">156 hunch net-2006-02-11-Yahoo&#8217;s Learning Problems.</a></p>
<p>Introduction: I just visited  Yahoo Research  which has several fundamental learning problems near to (or beyond) the set of problems we know how to solve well.  Here are 3 of them.
  
  Ranking   This is the canonical problem of all search engines.  It is made extra difficult for several reasons.
 
 There is relatively little “good” supervised learning data and a great deal of data with some signal (such as click through rates). 
 The learning must occur in a partially adversarial environment. Many people very actively attempt to place themselves at the top of 
rankings. 
 It is not even quite clear whether the problem should be posed as ‘ranking’ or as ‘regression’ which is then used to produce a 
ranking. 
 
 
  Collaborative filtering  Yahoo has a large number of recommendation systems for music, movies, etc…  In these sorts of systems, users specify how they liked a set of things, and then the system can (hopefully) find some more examples of things they might like 
by reasoning across multiple</p><p>2 0.81494606 <a title="156-lsi-2" href="../hunch_net-2006/hunch_net-2006-05-08-Big_machine_learning.html">178 hunch net-2006-05-08-Big machine learning</a></p>
<p>Introduction: According to the  New York Times ,  Yahoo is releasing Project Panama shortly .  Project Panama is about better predicting which advertisements are relevant to a search, implying a higher click through rate, implying larger income for  Yahoo .  There are two things that seem interesting here:
  
 A significant portion of that improved accuracy is almost certainly machine learning at work. 
 The quantitative effect is huge—the estimate in the article is $600*10 6 . 
  
 Google  already has such improvements and  Microsoft Search  is surely working on them, which suggest this is (perhaps) a $10 9  per year machine learning problem. 
 
The exact methodology under use is unlikely to be publicly discussed in the near future because of the competitive enivironment.  Hopefully we’ll have some public “war stories” at some point in the future when this information becomes less sensitive.  For now, it’s reassuring to simply note that machine learning is having a big impact.</p><p>3 0.75532019 <a title="156-lsi-3" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<p>Introduction: I want to comment on the “Bing copies Google” discussion  here ,  here , and  here , because there are data-related issues which the general public may not understand, and some of the framing seems substantially misleading to me.
 
As a not-distant-outsider, let me mention the sources of bias I may have.  I work at  Yahoo! , which has started using  Bing .  This might predispose me towards Bing, but on the other hand I’m still at Yahoo!, and have been using  Linux  exclusively as an OS for many years, including even a couple minor kernel patches.  And,  on the gripping hand , I’ve spent quite a bit of time thinking about the basic  principles of incorporating user feedback in machine learning .  Also note, this post is not  related to official Yahoo! policy, it’s just my personal view.
 
 The issue  Google engineers inserted synthetic responses to synthetic queries on google.com, then executed the synthetic searches on google.com using Internet Explorer with the Bing toolbar and later</p><p>4 0.7235328 <a title="156-lsi-4" href="../hunch_net-2006/hunch_net-2006-08-03-AOL%26%238217%3Bs_data_drop.html">200 hunch net-2006-08-03-AOL&#8217;s data drop</a></p>
<p>Introduction: AOL has  released  several large search engine related datasets.  This looks like a pretty impressive data release, and it is a big opportunity for people everywhere to worry about search engine related learning problems, if they want.</p><p>5 0.72101867 <a title="156-lsi-5" href="../hunch_net-2006/hunch_net-2006-04-30-John_Langford_%26%238211%3B%3E_Yahoo_Research%2C_NY.html">175 hunch net-2006-04-30-John Langford &#8211;> Yahoo Research, NY</a></p>
<p>Introduction: I will join  Yahoo Research  (in New York) after my contract ends at  TTI-Chicago .
 
The deciding reasons are:
  
 Yahoo is running into many hard learning problems.  This is precisely the situation where basic research might hope to have the greatest impact. 
 Yahoo Research understands research including publishing, conferences, etc… 
 Yahoo Research is growing, so there is a chance I can help it grow well. 
 Yahoo understands the internet, including (but not at all limited to) experimenting with research blogs. 
  
In the end, Yahoo Research seems like the place where I might have a chance to make the greatest difference.  
 
Yahoo (as a company) has made a strong bet on Yahoo Research.  We-the-researchers all hope that bet will pay off, and this seems plausible.  I’ll certainly have fun trying.</p><p>6 0.63760829 <a title="156-lsi-6" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>7 0.61745155 <a title="156-lsi-7" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>8 0.5110395 <a title="156-lsi-8" href="../hunch_net-2010/hunch_net-2010-02-26-Yahoo%21_ML_events.html">389 hunch net-2010-02-26-Yahoo! ML events</a></p>
<p>9 0.50336647 <a title="156-lsi-9" href="../hunch_net-2009/hunch_net-2009-01-27-Key_Scientific_Challenges.html">339 hunch net-2009-01-27-Key Scientific Challenges</a></p>
<p>10 0.44863236 <a title="156-lsi-10" href="../hunch_net-2011/hunch_net-2011-02-17-What_does_Watson_mean%3F.html">424 hunch net-2011-02-17-What does Watson mean?</a></p>
<p>11 0.41985422 <a title="156-lsi-11" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>12 0.41620618 <a title="156-lsi-12" href="../hunch_net-2005/hunch_net-2005-10-03-Not_ICML.html">117 hunch net-2005-10-03-Not ICML</a></p>
<p>13 0.41541085 <a title="156-lsi-13" href="../hunch_net-2005/hunch_net-2005-02-26-Problem%3A_Reductions_and_Relative_Ranking_Metrics.html">31 hunch net-2005-02-26-Problem: Reductions and Relative Ranking Metrics</a></p>
<p>14 0.40661141 <a title="156-lsi-14" href="../hunch_net-2011/hunch_net-2011-02-25-Yahoo%21_Machine_Learning_grant_due_March_11.html">425 hunch net-2011-02-25-Yahoo! Machine Learning grant due March 11</a></p>
<p>15 0.40631452 <a title="156-lsi-15" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>16 0.40556112 <a title="156-lsi-16" href="../hunch_net-2007/hunch_net-2007-10-24-Contextual_Bandits.html">269 hunch net-2007-10-24-Contextual Bandits</a></p>
<p>17 0.39956829 <a title="156-lsi-17" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>18 0.39919004 <a title="156-lsi-18" href="../hunch_net-2007/hunch_net-2007-08-25-The_Privacy_Problem.html">260 hunch net-2007-08-25-The Privacy Problem</a></p>
<p>19 0.39429215 <a title="156-lsi-19" href="../hunch_net-2012/hunch_net-2012-02-29-Key_Scientific_Challenges_and_the_Franklin_Symposium.html">457 hunch net-2012-02-29-Key Scientific Challenges and the Franklin Symposium</a></p>
<p>20 0.38931203 <a title="156-lsi-20" href="../hunch_net-2005/hunch_net-2005-02-12-ROC_vs._Accuracy_vs._AROC.html">18 hunch net-2005-02-12-ROC vs. Accuracy vs. AROC</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(10, 0.027), (27, 0.202), (38, 0.068), (53, 0.042), (55, 0.082), (68, 0.3), (79, 0.03), (94, 0.127), (95, 0.029)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.92572832 <a title="156-lda-1" href="../hunch_net-2005/hunch_net-2005-03-29-Academic_Mechanism_Design.html">48 hunch net-2005-03-29-Academic Mechanism Design</a></p>
<p>Introduction: From game theory, there is a notion of “mechanism design”: setting up the structure of the world so that participants have some incentive to do sane things (rather than obviously counterproductive things).  Application of this principle to academic research may be fruitful.
 
What is misdesigned about academic research?
  
 The  JMLG  guides give many hints. 
 The common nature of  bad reviewing  also suggests the system isn’t working optimally. 
 There are many ways to experimentally  “cheat” in machine learning . 
  Funding Prisoner’s Delimma.  Good researchers often write grant proposals for funding rather than doing research.  Since the pool of grant money is finite, this means that grant proposals are often rejected, implying that more must be written.  This is essentially a “prisoner’s delimma”: anyone not writing grant proposals loses, but the entire process of doing research is slowed by distraction.  If everyone wrote 1/2 as many grant proposals, roughly the same distribution</p><p>2 0.88920462 <a title="156-lda-2" href="../hunch_net-2011/hunch_net-2011-03-27-Vowpal_Wabbit%2C_v5.1.html">428 hunch net-2011-03-27-Vowpal Wabbit, v5.1</a></p>
<p>Introduction: I just created  version 5.1  of  vowpal wabbit .  This almost entirely a bugfix release, so it’s an easy upgrade from v5.0.
 
In addition:
  
 There is now a  mailing list , which I and several other developers are subscribed to. 
 The main website has shifted to the wiki on github.  This means that anyone with a github account can now edit it. 
 I’m planning to give a tutorial tomorrow on it at  eHarmony / the LA machine learning meetup  at 10am.  Drop by if you’re interested. 
  
The status of VW amongst other open source projects has changed.  When VW first came out, it was relatively unique amongst existing projects in terms of features.  At this point, many other projects have started to appreciate the value of the design choices here.  This includes:
  
  Mahout , which now has an SGD implementation. 
  Shogun , where  Soeren  is keen on  incorporating features . 
  LibLinear , where they won the KDD best paper award for  out-of-core learning . 
  
This is expected—any open sourc</p><p>3 0.88536799 <a title="156-lda-3" href="../hunch_net-2005/hunch_net-2005-09-10-%26%238220%3BFailure%26%238221%3B_is_an_option.html">110 hunch net-2005-09-10-&#8220;Failure&#8221; is an option</a></p>
<p>Introduction: This is about the hard choices that graduate students must make.
 
The cultural definition of success in academic research is to:
  
 Produce good research which many other people appreciate. 
 Produce many students who go on to do the same. 
  
There are fundamental reasons why this is success in the local culture.   Good research appreciated by others means access to jobs.  Many students succesful in the same way implies that there are a number of people who think in a similar way and appreciate your work.
 
In order to graduate, a phd student must live in an academic culture for a period of several years. It is common to adopt the culture’s definition of success during this time.  It’s also common for many phd students discover they are not suited to an academic research lifestyle.  This collision of values and abilities naturally results in depression.
 
The most fundamental advice when this happens is: change something.  Pick a new advisor.  Pick a new research topic.  Or leave th</p><p>same-blog 4 0.87611037 <a title="156-lda-4" href="../hunch_net-2006/hunch_net-2006-02-11-Yahoo%26%238217%3Bs_Learning_Problems..html">156 hunch net-2006-02-11-Yahoo&#8217;s Learning Problems.</a></p>
<p>Introduction: I just visited  Yahoo Research  which has several fundamental learning problems near to (or beyond) the set of problems we know how to solve well.  Here are 3 of them.
  
  Ranking   This is the canonical problem of all search engines.  It is made extra difficult for several reasons.
 
 There is relatively little “good” supervised learning data and a great deal of data with some signal (such as click through rates). 
 The learning must occur in a partially adversarial environment. Many people very actively attempt to place themselves at the top of 
rankings. 
 It is not even quite clear whether the problem should be posed as ‘ranking’ or as ‘regression’ which is then used to produce a 
ranking. 
 
 
  Collaborative filtering  Yahoo has a large number of recommendation systems for music, movies, etc…  In these sorts of systems, users specify how they liked a set of things, and then the system can (hopefully) find some more examples of things they might like 
by reasoning across multiple</p><p>5 0.78026098 <a title="156-lda-5" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>Introduction: One view of machine learning is that it’s about how to program computers to predict well.  This suggests a broader research program centered around the more pervasive goal of simply predicting well. 
There are many distinct strands of this broader research program which are only partially unified.  Here are the ones that I know of:
  
  Learning Theory .  Learning theory focuses on several topics related to the dynamics and process of prediction.  Convergence bounds like the  VC bound   give an intellectual foundation to many learning algorithms.  Online learning algorithms like  Weighted Majority  provide an alternate purely game theoretic foundation for learning.   Boosting algorithms  yield algorithms for purifying prediction abiliity.   Reduction algorithms  provide means for changing esoteric problems into well known ones. 
  Machine Learning .  A great deal of experience has accumulated in practical algorithm design from a mixture of paradigms, including bayesian, biological, opt</p><p>6 0.72051138 <a title="156-lda-6" href="../hunch_net-2005/hunch_net-2005-05-02-Reviewing_techniques_for_conferences.html">65 hunch net-2005-05-02-Reviewing techniques for conferences</a></p>
<p>7 0.65439647 <a title="156-lda-7" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>8 0.65084016 <a title="156-lda-8" href="../hunch_net-2007/hunch_net-2007-01-02-Retrospective.html">225 hunch net-2007-01-02-Retrospective</a></p>
<p>9 0.64922565 <a title="156-lda-9" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>10 0.64921367 <a title="156-lda-10" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>11 0.6479184 <a title="156-lda-11" href="../hunch_net-2006/hunch_net-2006-09-12-Incentive_Compatible_Reviewing.html">207 hunch net-2006-09-12-Incentive Compatible Reviewing</a></p>
<p>12 0.64761615 <a title="156-lda-12" href="../hunch_net-2010/hunch_net-2010-09-21-Regretting_the_dead.html">411 hunch net-2010-09-21-Regretting the dead</a></p>
<p>13 0.64523643 <a title="156-lda-13" href="../hunch_net-2009/hunch_net-2009-08-03-Carbon_in_Computer_Science_Research.html">366 hunch net-2009-08-03-Carbon in Computer Science Research</a></p>
<p>14 0.644952 <a title="156-lda-14" href="../hunch_net-2009/hunch_net-2009-06-01-Multitask_Poisoning.html">358 hunch net-2009-06-01-Multitask Poisoning</a></p>
<p>15 0.64150804 <a title="156-lda-15" href="../hunch_net-2006/hunch_net-2006-12-04-Structural_Problems_in_NIPS_Decision_Making.html">221 hunch net-2006-12-04-Structural Problems in NIPS Decision Making</a></p>
<p>16 0.64149064 <a title="156-lda-16" href="../hunch_net-2011/hunch_net-2011-11-26-Giving_Thanks.html">449 hunch net-2011-11-26-Giving Thanks</a></p>
<p>17 0.64113116 <a title="156-lda-17" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>18 0.63979518 <a title="156-lda-18" href="../hunch_net-2009/hunch_net-2009-12-09-Future_Publication_Models_%40_NIPS.html">382 hunch net-2009-12-09-Future Publication Models @ NIPS</a></p>
<p>19 0.6381864 <a title="156-lda-19" href="../hunch_net-2009/hunch_net-2009-06-03-Functionally_defined_Nonlinear_Dynamic_Models.html">359 hunch net-2009-06-03-Functionally defined Nonlinear Dynamic Models</a></p>
<p>20 0.63801658 <a title="156-lda-20" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
