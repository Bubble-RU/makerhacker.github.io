<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>215 hunch net-2006-10-22-Exemplar programming</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-215" href="#">hunch_net-2006-215</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>215 hunch net-2006-10-22-Exemplar programming</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-215-html" href="http://hunch.net/?p=235">html</a></p><p>Introduction: There are many different abstractions for problem definition and solution.
Here are a few examples:Functional programming: a set of functions are
defined. The composed execution of these functions yields the solution.Linear
programming: a set of constraints and a linear objective function are defined.
An LP solver finds the constrained optimum.Quadratic programming: Like linear
programming, but the language is a little more flexible (and the solution
slower).Convex programming: like quadratic programming, but the language is
more flexible (and the solutions even slower).Dynamic programming: a recursive
definition of the problem is defined and then solved efficiently via caching
tricks.SAT programming: A problem is specified as a satisfiability involving a
conjunction of a disjunction of boolean variables. A general engine attempts
to find a good satisfying assignment. For exampleKautz'sblackboxplanner.These
abstractions have different tradeoffs between ease of use, generality, and the</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 There are many different abstractions for problem definition and solution. [sent-1, score-0.328]
</p><p>2 The composed execution of these functions yields the solution. [sent-3, score-0.214]
</p><p>3 Linear programming: a set of constraints and a linear objective function are defined. [sent-4, score-0.185]
</p><p>4 Quadratic programming: Like linear programming, but the language is a little more flexible (and the solution slower). [sent-6, score-0.291]
</p><p>5 Convex programming: like quadratic programming, but the language is more flexible (and the solutions even slower). [sent-7, score-0.296]
</p><p>6 Dynamic programming: a recursive definition of the problem is defined and then solved efficiently via caching tricks. [sent-8, score-0.392]
</p><p>7 SAT programming: A problem is specified as a satisfiability involving a conjunction of a disjunction of boolean variables. [sent-9, score-0.186]
</p><p>8 These abstractions have different tradeoffs between ease of use, generality, and the effectiveness of existing solutions. [sent-12, score-0.323]
</p><p>9 Exemplar programming is creating examples of a (input,output) pairs which are used by an algorithm to predict a function from input to output. [sent-14, score-0.759]
</p><p>10 There are several subtle issues related to overfitting, independence, and representativeness of the samples which take significant effort to describe to an unfamiliar person. [sent-17, score-0.213]
</p><p>11 Making this abstraction easier to use via careful language design is an area where effort may pay off. [sent-18, score-0.419]
</p><p>12 How effectve are the exemplar programming solvers (aka learning algorithms)? [sent-19, score-1.011]
</p><p>13 Exemplar programming is a viewpoint of machine learning which (mostly) ignores statistics, prior information, and the possibility of overfitting. [sent-27, score-0.915]
</p><p>14 That's a great deal to ignore, but there are gains as well. [sent-28, score-0.172]
</p><p>15 Exemplar programming creates a split between problem solution and problem formation. [sent-29, score-0.993]
</p><p>16 This is important because the problem solver can heavily optimized (for speed, scalibility, effectiveness on common problems, etcâ&euro;Ś) making the process of apply machine learning simply a matter of specifying the exemplars. [sent-30, score-0.537]
</p><p>17 The formation/solution split helps us focus on problem formation independent of solution. [sent-31, score-0.32]
</p><p>18 The big gains in machine learning in the last decade seem to be discovering how to apply to new areas. [sent-32, score-0.485]
</p><p>19 A significant step in any application to a new area is discovering the right way to formulate the problem. [sent-33, score-0.324]
</p><p>20 Exemplar programming seems to be a useful viewpoint for machine learning in "big data" problems with many examples where significant prior information is lacking. [sent-34, score-1.059]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('programming', 0.629), ('exemplar', 0.306), ('gains', 0.172), ('solver', 0.153), ('abstractions', 0.134), ('split', 0.134), ('effectiveness', 0.122), ('problem', 0.115), ('flexible', 0.114), ('slower', 0.114), ('abstraction', 0.114), ('language', 0.111), ('discovering', 0.108), ('apply', 0.088), ('viewpoint', 0.086), ('definition', 0.079), ('solvers', 0.076), ('phrased', 0.076), ('formulate', 0.076), ('increasingly', 0.076), ('significant', 0.076), ('functions', 0.076), ('conjunction', 0.071), ('composed', 0.071), ('finds', 0.071), ('lp', 0.071), ('formation', 0.071), ('ignores', 0.071), ('quadratic', 0.071), ('recursive', 0.071), ('prior', 0.07), ('examples', 0.07), ('effective', 0.07), ('effort', 0.07), ('problems', 0.069), ('tradeoffs', 0.067), ('aka', 0.067), ('execution', 0.067), ('unfamiliar', 0.067), ('caching', 0.067), ('linear', 0.066), ('engine', 0.064), ('area', 0.064), ('constrained', 0.061), ('function', 0.06), ('via', 0.06), ('machine', 0.059), ('generality', 0.059), ('objective', 0.059), ('big', 0.058)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000002 <a title="215-tfidf-1" href="../hunch_net-2006/hunch_net-2006-10-22-Exemplar_programming.html">215 hunch net-2006-10-22-Exemplar programming</a></p>
<p>Introduction: There are many different abstractions for problem definition and solution.
Here are a few examples:Functional programming: a set of functions are
defined. The composed execution of these functions yields the solution.Linear
programming: a set of constraints and a linear objective function are defined.
An LP solver finds the constrained optimum.Quadratic programming: Like linear
programming, but the language is a little more flexible (and the solution
slower).Convex programming: like quadratic programming, but the language is
more flexible (and the solutions even slower).Dynamic programming: a recursive
definition of the problem is defined and then solved efficiently via caching
tricks.SAT programming: A problem is specified as a satisfiability involving a
conjunction of a disjunction of boolean variables. A general engine attempts
to find a good satisfying assignment. For exampleKautz'sblackboxplanner.These
abstractions have different tradeoffs between ease of use, generality, and the</p><p>2 0.15681325 <a title="215-tfidf-2" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>Introduction: This post is partly meant as an advertisement for thereductions
tutorialAlina,Bianca, and I are planning to do atICML. Please come, if you are
interested.Many research programs can be thought of as finding and building
new useful abstractions. The running example I'll use islearning
reductionswhere I have experience. The basic abstraction here is that we can
build a learning algorithm capable of solving classification problems up to a
small expected regret. This is used repeatedly to solve more complex
problems.In working on a new abstraction, I think you typically run into many
substantial problems of understanding, which make publishing particularly
difficult.It is difficult to seriously discuss the reason behind or mechanism
for abstraction in a conference paper with small page limits. People rarely
see such discussions and hence have little basis on which to think about new
abstractions. Another difficulty is that when building an abstraction, you
often don't know the right way to</p><p>3 0.14467324 <a title="215-tfidf-3" href="../hunch_net-2006/hunch_net-2006-09-28-Programming_Languages_for_Machine_Learning_Implementations.html">210 hunch net-2006-09-28-Programming Languages for Machine Learning Implementations</a></p>
<p>Introduction: Machine learning algorithms have a much better chance of being widely adopted
if they are implemented in some easy-to-use code. There are several important
concerns associated with machine learning which stress programming languages
on the ease-of-use vs. speed frontier.SpeedThe rate at which data sources are
growing seems to be outstripping the rate at which computational power is
growing, so it is important that we be able to eak out every bit of
computational power. Garbage collected languages (java,ocaml,perlandpython)
often have several issues here.Garbage collection often implies that floating
point numbers are "boxed": every float is represented by a pointer to a float.
Boxing can cause an order of magnitude slowdown because an extra nonlocalized
memory reference is made, and accesses to main memory can are many CPU cycles
long.Garbage collection often implies that considerably more memory is used
than is necessary. This has a variable effect. In some circumstances it
results in</p><p>4 0.14257224 <a title="215-tfidf-4" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>Introduction: Suppose you have a dataset with 2 terafeatures (we only count nonzero entries
in a datamatrix), and want to learn a good linear predictor in a reasonable
amount of time. How do you do it? As a learning theorist, the first thing you
do is pray that this is too much data for the number of parameters--but that's
not the case, there are around 16 billion examples, 16 million parameters, and
people really care about a high quality predictor, so subsampling is not a
good strategy.Alekhvisited us last summer, and we had a breakthrough
(seeherefor details), coming up with the first learning algorithm I've seen
that is provably faster thanany futuresingle machine learning algorithm. The
proof of this is simple: We can output a optimal-up-to-precision linear
predictor faster than the data can be streamed through the network interface
of any single machine involved in the computation.It is necessary but not
sufficient to have an effective communication infrastructure. It is necessary
but not suff</p><p>5 0.13499837 <a title="215-tfidf-5" href="../hunch_net-2005/hunch_net-2005-04-21-Dynamic_Programming_Generalizations_and_Their_Use.html">58 hunch net-2005-04-21-Dynamic Programming Generalizations and Their Use</a></p>
<p>Introduction: David Mcallestergave a talk about thispaper(withPedro Felzenszwalb). I'll try
to give a high level summary of why it's interesting.Dynamic programming is
most familiar as instantiated by Viterbi decoding in a hidden markov model. It
is a general paradigm for problem solving where subproblems are solved and
used to solve larger problems. In the Viterbi decoding example, the subproblem
is "What is the most probable path ending at each state at timestept?", and
the larger problem is the same except at timestept+1. There are a few
optimizations you can do here:Dynamic Programming -> queued Dynamic
Programming. Keep track of the "cost so far" (or "most probable path") and
(carefully) only look at extensions to paths likely to yield the shortest
path. "Carefully" here is defined byDijkstra's shortest path algorithm.queued
Dynamic programming -> A*Add a lower bound on the cost to complete a path (or
an upper bound on the probability of a completion) for the priority queue of
Dijkstra's shorte</p><p>6 0.12562284 <a title="215-tfidf-6" href="../hunch_net-2006/hunch_net-2006-01-08-Debugging_Your_Brain.html">147 hunch net-2006-01-08-Debugging Your Brain</a></p>
<p>7 0.12198815 <a title="215-tfidf-7" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>8 0.1194737 <a title="215-tfidf-8" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>9 0.11930601 <a title="215-tfidf-9" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>10 0.11651618 <a title="215-tfidf-10" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<p>11 0.1147389 <a title="215-tfidf-11" href="../hunch_net-2007/hunch_net-2007-01-26-Parallel_Machine_Learning_Problems.html">229 hunch net-2007-01-26-Parallel Machine Learning Problems</a></p>
<p>12 0.11364479 <a title="215-tfidf-12" href="../hunch_net-2007/hunch_net-2007-01-15-The_Machine_Learning_Department.html">228 hunch net-2007-01-15-The Machine Learning Department</a></p>
<p>13 0.11155553 <a title="215-tfidf-13" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>14 0.11132721 <a title="215-tfidf-14" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>15 0.10967686 <a title="215-tfidf-15" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>16 0.10299441 <a title="215-tfidf-16" href="../hunch_net-2005/hunch_net-2005-03-04-The_Big_O_and_Constants_in_Learning.html">35 hunch net-2005-03-04-The Big O and Constants in Learning</a></p>
<p>17 0.097932331 <a title="215-tfidf-17" href="../hunch_net-2009/hunch_net-2009-12-24-Top_graduates_this_season.html">384 hunch net-2009-12-24-Top graduates this season</a></p>
<p>18 0.095583946 <a title="215-tfidf-18" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>19 0.095392033 <a title="215-tfidf-19" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>20 0.087998129 <a title="215-tfidf-20" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.201), (1, -0.088), (2, 0.053), (3, -0.05), (4, -0.033), (5, -0.021), (6, -0.051), (7, 0.051), (8, 0.014), (9, 0.084), (10, -0.051), (11, -0.021), (12, 0.13), (13, -0.057), (14, -0.009), (15, 0.006), (16, -0.008), (17, -0.023), (18, 0.073), (19, 0.026), (20, -0.064), (21, -0.025), (22, -0.007), (23, -0.028), (24, -0.016), (25, 0.047), (26, -0.043), (27, -0.025), (28, -0.021), (29, -0.052), (30, 0.119), (31, -0.119), (32, -0.016), (33, -0.049), (34, 0.108), (35, 0.031), (36, -0.097), (37, -0.027), (38, 0.134), (39, -0.068), (40, -0.045), (41, -0.07), (42, -0.041), (43, 0.017), (44, -0.032), (45, 0.038), (46, -0.07), (47, 0.067), (48, -0.05), (49, -0.098)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94718343 <a title="215-lsi-1" href="../hunch_net-2006/hunch_net-2006-10-22-Exemplar_programming.html">215 hunch net-2006-10-22-Exemplar programming</a></p>
<p>Introduction: There are many different abstractions for problem definition and solution.
Here are a few examples:Functional programming: a set of functions are
defined. The composed execution of these functions yields the solution.Linear
programming: a set of constraints and a linear objective function are defined.
An LP solver finds the constrained optimum.Quadratic programming: Like linear
programming, but the language is a little more flexible (and the solution
slower).Convex programming: like quadratic programming, but the language is
more flexible (and the solutions even slower).Dynamic programming: a recursive
definition of the problem is defined and then solved efficiently via caching
tricks.SAT programming: A problem is specified as a satisfiability involving a
conjunction of a disjunction of boolean variables. A general engine attempts
to find a good satisfying assignment. For exampleKautz'sblackboxplanner.These
abstractions have different tradeoffs between ease of use, generality, and the</p><p>2 0.71139163 <a title="215-lsi-2" href="../hunch_net-2006/hunch_net-2006-09-28-Programming_Languages_for_Machine_Learning_Implementations.html">210 hunch net-2006-09-28-Programming Languages for Machine Learning Implementations</a></p>
<p>Introduction: Machine learning algorithms have a much better chance of being widely adopted
if they are implemented in some easy-to-use code. There are several important
concerns associated with machine learning which stress programming languages
on the ease-of-use vs. speed frontier.SpeedThe rate at which data sources are
growing seems to be outstripping the rate at which computational power is
growing, so it is important that we be able to eak out every bit of
computational power. Garbage collected languages (java,ocaml,perlandpython)
often have several issues here.Garbage collection often implies that floating
point numbers are "boxed": every float is represented by a pointer to a float.
Boxing can cause an order of magnitude slowdown because an extra nonlocalized
memory reference is made, and accesses to main memory can are many CPU cycles
long.Garbage collection often implies that considerably more memory is used
than is necessary. This has a variable effect. In some circumstances it
results in</p><p>3 0.64453572 <a title="215-lsi-3" href="../hunch_net-2005/hunch_net-2005-05-10-Learning_Reductions_are_Reductionist.html">68 hunch net-2005-05-10-Learning Reductions are Reductionist</a></p>
<p>Introduction: This is about a fundamental motivation for the investigation of reductions in
learning. It applies to many pieces of work other than my own.The reductionist
approach to problem solving is characterized by taking a problem, decomposing
it into as-small-as-possible subproblems, discovering how to solve the
subproblems, and then discovering how to use the solutions to the subproblems
to solve larger problems. The reductionist approach to solving problems has
often payed offverywell. Computer science related examples of the reductionist
approach include:Reducing computation to the transistor. All of our CPUs are
built from transistors.Reducing rendering of images to rendering a triangle
(or other simple polygons). Computers can now render near-realistic scenes in
real time. The big breakthrough came from learning how to render many
triangles quickly.This approach to problem solving extends well beyond
computer science. Many fields of science focus on theories making predictions
about very</p><p>4 0.64351642 <a title="215-lsi-4" href="../hunch_net-2005/hunch_net-2005-06-22-Languages__of_Learning.html">84 hunch net-2005-06-22-Languages  of Learning</a></p>
<p>Introduction: A language is a set of primitives which can be combined to succesfully create
complex objects. Languages arise in all sorts of situations: mechanical
construction, martial arts, communication, etcâ&euro;Ś Languages appear to be the key
to succesfully creating complex objects--it is difficult to come up with any
convincing example of a complex object which is not built using some language.
Since languages are so crucial to success, it is interesting to organize
various machine learning research programs by language.The most common
language in machine learning are languages for representing the solution to
machine learning. This includes:Bayes Nets and Graphical ModelsA language for
representing probability distributions. The key concept supporting modularity
is conditional independence.Michael Kearnshas been working on extending this
to game theory.Kernelized Linear ClassifiersA language for representing linear
separators, possibly in a large space. The key form of modularity here is
kerneliza</p><p>5 0.62677568 <a title="215-lsi-5" href="../hunch_net-2007/hunch_net-2007-01-26-Parallel_Machine_Learning_Problems.html">229 hunch net-2007-01-26-Parallel Machine Learning Problems</a></p>
<p>Introduction: Parallel machine learning is a subject rarely addressed at machine learning
conferences. Nevertheless, it seems likely to increase in importance
because:Data set sizes appear to be growing substantially faster than
computation. Essentially, this happens because more and more sensors of
various sorts are being hooked up to the internet.Serial speedups of
processors seem are relatively stalled. The new trend is to make processors
more powerful by making themmulticore.BothAMDandIntelare making dual core
designs standard, with plans for more parallelism in the future.IBM'sCell
processorhas (essentially) 9 cores.Modern graphics chips can have an order of
magnitude more separate execution units.The meaning of 'core' varies a bit
from processor to processor, but the overall trend seems quite clear.So, how
do we parallelize machine learning algorithms?The simplest and most common
technique is to simply run the same learning algorithm with different
parameters on different processors. Cluster m</p><p>6 0.5982939 <a title="215-lsi-6" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>7 0.58566087 <a title="215-lsi-7" href="../hunch_net-2006/hunch_net-2006-01-08-Debugging_Your_Brain.html">147 hunch net-2006-01-08-Debugging Your Brain</a></p>
<p>8 0.58167934 <a title="215-lsi-8" href="../hunch_net-2007/hunch_net-2007-01-15-The_Machine_Learning_Department.html">228 hunch net-2007-01-15-The Machine Learning Department</a></p>
<p>9 0.58143705 <a title="215-lsi-9" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>10 0.58002162 <a title="215-lsi-10" href="../hunch_net-2005/hunch_net-2005-04-21-Dynamic_Programming_Generalizations_and_Their_Use.html">58 hunch net-2005-04-21-Dynamic Programming Generalizations and Their Use</a></p>
<p>11 0.57867217 <a title="215-lsi-11" href="../hunch_net-2005/hunch_net-2005-05-06-Don%26%238217%3Bt_mix_the_solution_into_the_problem.html">67 hunch net-2005-05-06-Don&#8217;t mix the solution into the problem</a></p>
<p>12 0.56237775 <a title="215-lsi-12" href="../hunch_net-2007/hunch_net-2007-09-16-Optimizing_Machine_Learning_Programs.html">262 hunch net-2007-09-16-Optimizing Machine Learning Programs</a></p>
<p>13 0.55822301 <a title="215-lsi-13" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>14 0.55059904 <a title="215-lsi-14" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>15 0.5489372 <a title="215-lsi-15" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>16 0.54403526 <a title="215-lsi-16" href="../hunch_net-2009/hunch_net-2009-08-03-Carbon_in_Computer_Science_Research.html">366 hunch net-2009-08-03-Carbon in Computer Science Research</a></p>
<p>17 0.54297513 <a title="215-lsi-17" href="../hunch_net-2006/hunch_net-2006-03-24-NLPers.html">166 hunch net-2006-03-24-NLPers</a></p>
<p>18 0.54171562 <a title="215-lsi-18" href="../hunch_net-2005/hunch_net-2005-03-04-The_Big_O_and_Constants_in_Learning.html">35 hunch net-2005-03-04-The Big O and Constants in Learning</a></p>
<p>19 0.54167229 <a title="215-lsi-19" href="../hunch_net-2007/hunch_net-2007-02-02-Thoughts_regarding_%26%238220%3BIs_machine_learning_different_from_statistics%3F%26%238221%3B.html">230 hunch net-2007-02-02-Thoughts regarding &#8220;Is machine learning different from statistics?&#8221;</a></p>
<p>20 0.53567219 <a title="215-lsi-20" href="../hunch_net-2009/hunch_net-2009-01-21-Nearly_all_natural_problems_require_nonlinearity.html">337 hunch net-2009-01-21-Nearly all natural problems require nonlinearity</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(6, 0.014), (35, 0.014), (42, 0.243), (45, 0.055), (68, 0.032), (74, 0.073), (91, 0.012), (95, 0.456)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.99008274 <a title="215-lda-1" href="../hunch_net-2010/hunch_net-2010-11-18-ICML_2011_%26%238211%3B_Call_for_Tutorials.html">417 hunch net-2010-11-18-ICML 2011 &#8211; Call for Tutorials</a></p>
<p>Introduction: I would like to encourage people to consider giving a tutorial at next years
ICML. The ideal tutorial attracts a wide audience, provides a gentle and
easily taught introduction to the chosen research area, and also covers the
most important contributions in depth.Submissions are due January 14 Â (about
two weeks before paper
deadline).http://www.icml-2011.org/tutorials.phpRegards,Ulf</p><p>2 0.97160631 <a title="215-lda-2" href="../hunch_net-2007/hunch_net-2007-02-11-24.html">232 hunch net-2007-02-11-24</a></p>
<p>Introduction: To commemorate theTwenty Fourth Annual International Conference on Machine
Learning(ICML-07), the FOX Network has decided to launch a new spin-off series
in prime time. Through unofficial sources, I have obtained thestory arcfor the
first season, which appears frighteningly realistic.</p><p>3 0.9695248 <a title="215-lda-3" href="../hunch_net-2008/hunch_net-2008-07-04-More_Presentation_Preparation.html">307 hunch net-2008-07-04-More Presentation Preparation</a></p>
<p>Introduction: We've discussedpresentation preparation before, but I have one more thing to
add:transitioning. For a research presentation, it is substantially helpful
for the audience if transitions are clear. A common outline for a research
presentation in machine leanring is:The problem. Presentations which don't
describe the problem almost immediately lose people, because the context is
missing to understand the detail.Prior relevant work. In many cases, a paper
builds on some previous bit of work which must be understood in order to
understand what the paper does. A common failure mode seems to be spending too
much time on prior work. Discuss just the relevant aspects of prior work in
the language of your work. Sometimes this is missing when unneeded.What we
did. For theory papers in particular, it is often not possible to really cover
the details. Prioritizing what you present can be very important.How it
worked. Many papers in Machine Learning have some sort of experimental test of
the algorit</p><p>4 0.96936697 <a title="215-lda-4" href="../hunch_net-2009/hunch_net-2009-11-23-ICML_2009_Workshops_%28and_Tutorials%29.html">379 hunch net-2009-11-23-ICML 2009 Workshops (and Tutorials)</a></p>
<p>Introduction: I'm theworkshops chairforICMLthis year. As such, I would like to personally
encourage people to consider running a workshop.My general view of workshops
is that they are excellent as opportunities to discuss and develop research
directions--some of my best work has come from collaborations at workshops and
several workshops have substantially altered my thinking about various
problems. My experience running workshops is that setting them up and making
them fly often appears much harder than it actually is, and the workshops
often come off much better than expected in the end. Submissions are due
January 18, two weeks before papers.Similarly,Ben Taskaris looking for
goodtutorials, which is complementary. Workshops are about exploring a
subject, while a tutorial is about distilling it down into an easily taught
essence, a vital part of the research process. Tutorials are due February 13,
two weeks after papers.</p><p>5 0.95125806 <a title="215-lda-5" href="../hunch_net-2006/hunch_net-2006-01-08-Debugging_Your_Brain.html">147 hunch net-2006-01-08-Debugging Your Brain</a></p>
<p>Introduction: One part of doing research is debugging your understanding of reality. This is
hard work: How do you even discover where you misunderstand? If you discover a
misunderstanding, how do you go about removing it?The process of debugging
computer programs is quite analogous to debugging reality misunderstandings.
This is natural--a bug in a computer program is a misunderstanding between you
and the computer about what you said. Many of the familiar techniques from
debugging have exact parallels.DetailsWhen programming, there are often signs
that some bug exists like: "the graph my program output is shifted a little
bit" = maybe you have an indexing error. In debugging yourself, we often have
some impression that something is "not right". These impressions should be
addressed directly and immediately. (Some people have the habit of suppressing
worries in favor of excess certainty. That's not healthy for research.)Corner
CasesA "corner case" is an input to a program which is extreme in some w</p><p>6 0.91362798 <a title="215-lda-6" href="../hunch_net-2005/hunch_net-2005-05-10-Learning_Reductions_are_Reductionist.html">68 hunch net-2005-05-10-Learning Reductions are Reductionist</a></p>
<p>7 0.90966666 <a title="215-lda-7" href="../hunch_net-2010/hunch_net-2010-12-02-Traffic_Prediction_Problem.html">418 hunch net-2010-12-02-Traffic Prediction Problem</a></p>
<p>same-blog 8 0.90002108 <a title="215-lda-8" href="../hunch_net-2006/hunch_net-2006-10-22-Exemplar_programming.html">215 hunch net-2006-10-22-Exemplar programming</a></p>
<p>9 0.88518238 <a title="215-lda-9" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>10 0.83113599 <a title="215-lda-10" href="../hunch_net-2012/hunch_net-2012-04-09-ICML_author_feedback_is_open.html">461 hunch net-2012-04-09-ICML author feedback is open</a></p>
<p>11 0.7692498 <a title="215-lda-11" href="../hunch_net-2005/hunch_net-2005-03-24-The_Role_of_Workshops.html">46 hunch net-2005-03-24-The Role of Workshops</a></p>
<p>12 0.70627695 <a title="215-lda-12" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>13 0.67408574 <a title="215-lda-13" href="../hunch_net-2007/hunch_net-2007-06-21-Presentation_Preparation.html">249 hunch net-2007-06-21-Presentation Preparation</a></p>
<p>14 0.66725206 <a title="215-lda-14" href="../hunch_net-2011/hunch_net-2011-08-20-The_Large_Scale_Learning_Survey_Tutorial.html">442 hunch net-2011-08-20-The Large Scale Learning Survey Tutorial</a></p>
<p>15 0.6605804 <a title="215-lda-15" href="../hunch_net-2006/hunch_net-2006-09-28-Programming_Languages_for_Machine_Learning_Implementations.html">210 hunch net-2006-09-28-Programming Languages for Machine Learning Implementations</a></p>
<p>16 0.65622896 <a title="215-lda-16" href="../hunch_net-2009/hunch_net-2009-10-26-NIPS_workshops.html">375 hunch net-2009-10-26-NIPS workshops</a></p>
<p>17 0.63449627 <a title="215-lda-17" href="../hunch_net-2009/hunch_net-2009-12-24-Top_graduates_this_season.html">384 hunch net-2009-12-24-Top graduates this season</a></p>
<p>18 0.63279665 <a title="215-lda-18" href="../hunch_net-2013/hunch_net-2013-12-01-NIPS_tutorials_and_Vowpal_Wabbit_7.4.html">492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</a></p>
<p>19 0.63214886 <a title="215-lda-19" href="../hunch_net-2005/hunch_net-2005-03-17-Going_all_the_Way%2C_Sometimes.html">42 hunch net-2005-03-17-Going all the Way, Sometimes</a></p>
<p>20 0.61641848 <a title="215-lda-20" href="../hunch_net-2005/hunch_net-2005-09-26-Prediction_Bounds_as_the_Mathematics_of_Science.html">115 hunch net-2005-09-26-Prediction Bounds as the Mathematics of Science</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
