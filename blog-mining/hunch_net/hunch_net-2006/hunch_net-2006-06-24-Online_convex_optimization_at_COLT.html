<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>186 hunch net-2006-06-24-Online convex optimization at COLT</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-186" href="#">hunch_net-2006-186</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>186 hunch net-2006-06-24-Online convex optimization at COLT</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-186-html" href="http://hunch.net/?p=199">html</a></p><p>Introduction: AtICML 2003,Marty Zinkevichproposedthe online convex optimization setting and
showed that a particular gradient descent algorithm has regret O(T0.5) with
respect to the best predictor where T is the number of rounds. This seems to
be a nice model for online learning, and there has been some significant
follow-up work.AtCOLT 2006Elad Hazan,Adam Kalai,Satyen Kale, andAmit
Agarwalpresenteda modification which takes a Newton stepguaranteeing O(log T)
regret when the first and second derivatives are bounded.Then they applied
these algorithms to portfolio managementatICML 2006(withRobert Schapire)
yielding some very fun graphs.</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('newton', 0.307), ('regret', 0.305), ('modification', 0.284), ('schapire', 0.268), ('portfolio', 0.268), ('graphs', 0.245), ('showed', 0.229), ('yielding', 0.223), ('online', 0.201), ('convex', 0.198), ('fun', 0.194), ('nice', 0.176), ('descent', 0.171), ('takes', 0.164), ('log', 0.152), ('optimization', 0.151), ('predictor', 0.151), ('gradient', 0.149), ('applied', 0.133), ('second', 0.126)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="186-tfidf-1" href="../hunch_net-2006/hunch_net-2006-06-24-Online_convex_optimization_at_COLT.html">186 hunch net-2006-06-24-Online convex optimization at COLT</a></p>
<p>Introduction: AtICML 2003,Marty Zinkevichproposedthe online convex optimization setting and
showed that a particular gradient descent algorithm has regret O(T0.5) with
respect to the best predictor where T is the number of rounds. This seems to
be a nice model for online learning, and there has been some significant
follow-up work.AtCOLT 2006Elad Hazan,Adam Kalai,Satyen Kale, andAmit
Agarwalpresenteda modification which takes a Newton stepguaranteeing O(log T)
regret when the first and second derivatives are bounded.Then they applied
these algorithms to portfolio managementatICML 2006(withRobert Schapire)
yielding some very fun graphs.</p><p>2 0.19658241 <a title="186-tfidf-2" href="../hunch_net-2007/hunch_net-2007-08-12-Exponentiated_Gradient.html">258 hunch net-2007-08-12-Exponentiated Gradient</a></p>
<p>Introduction: TheExponentiated Gradientalgorithm byManfred WarmuthandJyrki Kivinencame out
just as I was starting graduate school, so I missed it both at a conference
and in class. It's a fine algorithm which has a remarkable theoretical
statement accompanying it.The essential statement holds in the "online
learning with an adversary" setting. Initially, there are of set ofnweights,
which might have values(1/n,â&euro;Ś,1/n), (or any other values from a probability
distribution). Everything happens in a round-by-round fashion. On each round,
the following happens:The world reveals a set of featuresx in {0,1}n. In the
online learning with an adversary literature, the features are called
"experts" and thought of as subpredictors, but this interpretation isn't
necessary--you can just use feature values as experts (or maybe the feature
value and the negation of the feature value as two experts).EG makes a
prediction according toy' = w . x(dot product).The world reveals the truthy in
[0,1].EG updates the weights</p><p>3 0.16780744 <a title="186-tfidf-3" href="../hunch_net-2005/hunch_net-2005-09-12-Fast_Gradient_Descent.html">111 hunch net-2005-09-12-Fast Gradient Descent</a></p>
<p>Introduction: Nic Schaudolphhas been developing a fast gradient descent algorithm
calledStochastic Meta-Descent(SMD).Gradient descent is currently untrendy in
the machine learning community, but there remains a large number of people
using gradient descent on neural networks or other architectures from when it
was trendy in the early 1990s. There are three problems with gradient
descent.Gradient descent does not necessarily produce easily reproduced
results. Typical algorithms start with "set the initial parameters to small
random values".The design of the representation that gradient descent is
applied to is often nontrivial. In particular, knowing exactly how to build a
large neural network so that it will perform well requires knowledge which has
not been made easily applicable.Gradient descent can be slow. Obviously,
taking infinitesimal steps in the direction of the gradient would take
forever, so some finite step size must be used. What exactly this step size
should be is unclear. Many people</p><p>4 0.1535196 <a title="186-tfidf-4" href="../hunch_net-2005/hunch_net-2005-02-01-NIPS%3A_Online_Bayes.html">8 hunch net-2005-02-01-NIPS: Online Bayes</a></p>
<p>Introduction: One nice use for this blog is to consider and discuss papers that that have
appeared at recent conferences. I really enjoyed Andrew Ng and Sham Kakade's
paperOnline Bounds for Bayesian Algorithms. From the paper:The philosophy
taken in the Bayesian methodology is often at odds withthat in the online
learning community…. the online learning settingmakes rather minimal
assumptions on the conditions under which thedata are being presented to the
learner â€”usually, Nature could provideexamples in an adversarial manner. We
study the performance ofBayesian algorithms in a more adversarial setting… We
providecompetitive bounds when the cost function is the log loss, and
wecompare our performance to the best model in our model class (as inthe
experts setting).It's a very nice analysis of some of my favorite algorithms
that all hinges around a beautiful theorem:Let Q be any distribution over
parameters theta. Then for all sequences S:L_{Bayes}(S) leq L_Q(S) +
KL(Q|P)where P is our prior and th</p><p>5 0.13935748 <a title="186-tfidf-5" href="../hunch_net-2009/hunch_net-2009-12-27-Interesting_things_at_NIPS_2009.html">385 hunch net-2009-12-27-Interesting things at NIPS 2009</a></p>
<p>Introduction: Several papers at NIPS caught my attention.Elad HazanandSatyen Kale,Online
Submodular OptimizationThey define an algorithm for online optimization of
submodular functions with regret guarantees. This places submodular
optimization roughly on par with online convex optimization as tractable
settings for online learning.Elad HazanandSatyen KaleOn Stochastic and Worst-
Case Models of Investing. At it's core, this is yet another example of
modifying worst-case online learning to deal with variance, but the
application to financial models is particularly cool and it seems plausibly
superior other common approaches for financial modeling.Mark Palatucci,Dean
Pomerlau,Tom Mitchell, andGeoff HintonZero Shot Learning with Semantic Output
CodesThe goal here is predicting a label in a multiclass supervised setting
where the label never occurs in the training data. They have some basic
analysis and also a nice application to FMRI brain reading.Shobha
Venkataraman,Avrim Blum,Dawn Song,Subhabrata Sen</p><p>6 0.13339415 <a title="186-tfidf-6" href="../hunch_net-2007/hunch_net-2007-07-01-Watchword%3A_Online_Learning.html">252 hunch net-2007-07-01-Watchword: Online Learning</a></p>
<p>7 0.11150052 <a title="186-tfidf-7" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>8 0.11107944 <a title="186-tfidf-8" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>9 0.10613194 <a title="186-tfidf-9" href="../hunch_net-2005/hunch_net-2005-05-16-Regret_minimizing_vs_error_limiting_reductions.html">72 hunch net-2005-05-16-Regret minimizing vs error limiting reductions</a></p>
<p>10 0.1056949 <a title="186-tfidf-10" href="../hunch_net-2007/hunch_net-2007-10-17-Online_as_the_new_adjective.html">267 hunch net-2007-10-17-Online as the new adjective</a></p>
<p>11 0.10451742 <a title="186-tfidf-11" href="../hunch_net-2005/hunch_net-2005-07-11-AAAI_blog.html">92 hunch net-2005-07-11-AAAI blog</a></p>
<p>12 0.10358655 <a title="186-tfidf-12" href="../hunch_net-2007/hunch_net-2007-12-21-Vowpal_Wabbit_Code_Release.html">281 hunch net-2007-12-21-Vowpal Wabbit Code Release</a></p>
<p>13 0.10098988 <a title="186-tfidf-13" href="../hunch_net-2010/hunch_net-2010-05-10-Aggregation_of_estimators%2C_sparsity_in_high_dimension_and_computational_feasibility.html">398 hunch net-2010-05-10-Aggregation of estimators, sparsity in high dimension and computational feasibility</a></p>
<p>14 0.095618159 <a title="186-tfidf-14" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">23 hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<p>15 0.094260693 <a title="186-tfidf-15" href="../hunch_net-2006/hunch_net-2006-05-23-What_is_the_best_regret_transform_reduction_from_multiclass_to_binary%3F.html">181 hunch net-2006-05-23-What is the best regret transform reduction from multiclass to binary?</a></p>
<p>16 0.09225399 <a title="186-tfidf-16" href="../hunch_net-2011/hunch_net-2011-08-01-Interesting_papers_at_COLT_2011.html">439 hunch net-2011-08-01-Interesting papers at COLT 2011</a></p>
<p>17 0.091032684 <a title="186-tfidf-17" href="../hunch_net-2005/hunch_net-2005-06-06-Exact_Online_Learning_for_Classification.html">78 hunch net-2005-06-06-Exact Online Learning for Classification</a></p>
<p>18 0.090330862 <a title="186-tfidf-18" href="../hunch_net-2007/hunch_net-2007-06-24-Interesting_Papers_at_ICML_2007.html">251 hunch net-2007-06-24-Interesting Papers at ICML 2007</a></p>
<p>19 0.088259488 <a title="186-tfidf-19" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>20 0.087301947 <a title="186-tfidf-20" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.145), (1, -0.131), (2, -0.043), (3, 0.093), (4, -0.062), (5, 0.076), (6, 0.171), (7, -0.013), (8, -0.037), (9, -0.042), (10, 0.13), (11, -0.137), (12, -0.099), (13, 0.015), (14, -0.033), (15, 0.084), (16, 0.03), (17, -0.076), (18, -0.214), (19, -0.04), (20, -0.068), (21, -0.128), (22, 0.014), (23, 0.072), (24, -0.054), (25, -0.059), (26, 0.081), (27, -0.089), (28, 0.046), (29, -0.043), (30, 0.097), (31, -0.055), (32, 0.031), (33, -0.042), (34, -0.065), (35, -0.051), (36, -0.015), (37, -0.082), (38, 0.061), (39, 0.031), (40, -0.068), (41, 0.032), (42, -0.048), (43, 0.059), (44, 0.061), (45, 0.103), (46, 0.018), (47, -0.012), (48, 0.029), (49, -0.011)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97751278 <a title="186-lsi-1" href="../hunch_net-2006/hunch_net-2006-06-24-Online_convex_optimization_at_COLT.html">186 hunch net-2006-06-24-Online convex optimization at COLT</a></p>
<p>Introduction: AtICML 2003,Marty Zinkevichproposedthe online convex optimization setting and
showed that a particular gradient descent algorithm has regret O(T0.5) with
respect to the best predictor where T is the number of rounds. This seems to
be a nice model for online learning, and there has been some significant
follow-up work.AtCOLT 2006Elad Hazan,Adam Kalai,Satyen Kale, andAmit
Agarwalpresenteda modification which takes a Newton stepguaranteeing O(log T)
regret when the first and second derivatives are bounded.Then they applied
these algorithms to portfolio managementatICML 2006(withRobert Schapire)
yielding some very fun graphs.</p><p>2 0.64350951 <a title="186-lsi-2" href="../hunch_net-2007/hunch_net-2007-08-12-Exponentiated_Gradient.html">258 hunch net-2007-08-12-Exponentiated Gradient</a></p>
<p>Introduction: TheExponentiated Gradientalgorithm byManfred WarmuthandJyrki Kivinencame out
just as I was starting graduate school, so I missed it both at a conference
and in class. It's a fine algorithm which has a remarkable theoretical
statement accompanying it.The essential statement holds in the "online
learning with an adversary" setting. Initially, there are of set ofnweights,
which might have values(1/n,â&euro;Ś,1/n), (or any other values from a probability
distribution). Everything happens in a round-by-round fashion. On each round,
the following happens:The world reveals a set of featuresx in {0,1}n. In the
online learning with an adversary literature, the features are called
"experts" and thought of as subpredictors, but this interpretation isn't
necessary--you can just use feature values as experts (or maybe the feature
value and the negation of the feature value as two experts).EG makes a
prediction according toy' = w . x(dot product).The world reveals the truthy in
[0,1].EG updates the weights</p><p>3 0.61067104 <a title="186-lsi-3" href="../hunch_net-2007/hunch_net-2007-07-01-Watchword%3A_Online_Learning.html">252 hunch net-2007-07-01-Watchword: Online Learning</a></p>
<p>Introduction: It turns out that many different people use the term "Online Learning", and
often they don't have the same definition in mind. Here's a list of the
possibilities I know of.Online Information SettingOnline learning refers to
aproblemin which unlabeled data comes, a prediction is made, and then feedback
is acquired.Online Adversarial SettingOnline learning refers toalgorithmsin
the Online Information Setting which satisfy guarantees of the form: "For all
possible sequences of observations, the algorithim has regret at mostlog (
number of strategies)with respect to the best strategy in a set." This is
sometimes called online learning with experts.Online Optimization
ConstraintOnline learning refers to optimizing a predictor via a learning
algorithm tunes parameters on a per-example basis. This may or may not be
applied in the Online Information Setting, and the strategy may or may not
satisfy Adversarial setting theory.Online Computational ConstraintOnline
learning refers to an algorithmi</p><p>4 0.60766196 <a title="186-lsi-4" href="../hunch_net-2005/hunch_net-2005-02-01-NIPS%3A_Online_Bayes.html">8 hunch net-2005-02-01-NIPS: Online Bayes</a></p>
<p>Introduction: One nice use for this blog is to consider and discuss papers that that have
appeared at recent conferences. I really enjoyed Andrew Ng and Sham Kakade's
paperOnline Bounds for Bayesian Algorithms. From the paper:The philosophy
taken in the Bayesian methodology is often at odds withthat in the online
learning community…. the online learning settingmakes rather minimal
assumptions on the conditions under which thedata are being presented to the
learner â€”usually, Nature could provideexamples in an adversarial manner. We
study the performance ofBayesian algorithms in a more adversarial setting… We
providecompetitive bounds when the cost function is the log loss, and
wecompare our performance to the best model in our model class (as inthe
experts setting).It's a very nice analysis of some of my favorite algorithms
that all hinges around a beautiful theorem:Let Q be any distribution over
parameters theta. Then for all sequences S:L_{Bayes}(S) leq L_Q(S) +
KL(Q|P)where P is our prior and th</p><p>5 0.55523813 <a title="186-lsi-5" href="../hunch_net-2007/hunch_net-2007-10-17-Online_as_the_new_adjective.html">267 hunch net-2007-10-17-Online as the new adjective</a></p>
<p>Introduction: Online learning is in vogue, which means we should expect to see in the near
future:Online boosting.Online decision trees.Online SVMs. (actually, we've
already seen)Online deep learning.Online parallel learning.etc…There are three
fundamental drivers of this trend.Increasing size of datasets makes online
algorithms attractive.Online learning can simply be more efficient than batch
learning. Here is a picture from a class on online learning:The point of this
picture is that even in 3 dimensions and even with linear constraints, finding
the minima of a set in an online fashion can be typically faster than finding
the minima in a batch fashion. To see this, note that there is a minimal
number of gradient updates (i.e. 2) required in order to reach the minima in
the typical case. Given this, it's best to do these updates as quickly as
possible, which implies doing the first update online (i.e. before seeing all
the examples) is preferred. Note that this is the simplest possible setting--
m</p><p>6 0.54673427 <a title="186-lsi-6" href="../hunch_net-2009/hunch_net-2009-12-27-Interesting_things_at_NIPS_2009.html">385 hunch net-2009-12-27-Interesting things at NIPS 2009</a></p>
<p>7 0.5337404 <a title="186-lsi-7" href="../hunch_net-2005/hunch_net-2005-06-06-Exact_Online_Learning_for_Classification.html">78 hunch net-2005-06-06-Exact Online Learning for Classification</a></p>
<p>8 0.52791107 <a title="186-lsi-8" href="../hunch_net-2005/hunch_net-2005-09-08-Online_Learning_as_the_Mathematics_of_Accountability.html">109 hunch net-2005-09-08-Online Learning as the Mathematics of Accountability</a></p>
<p>9 0.50008684 <a title="186-lsi-9" href="../hunch_net-2005/hunch_net-2005-09-12-Fast_Gradient_Descent.html">111 hunch net-2005-09-12-Fast Gradient Descent</a></p>
<p>10 0.4782978 <a title="186-lsi-10" href="../hunch_net-2010/hunch_net-2010-05-10-Aggregation_of_estimators%2C_sparsity_in_high_dimension_and_computational_feasibility.html">398 hunch net-2010-05-10-Aggregation of estimators, sparsity in high dimension and computational feasibility</a></p>
<p>11 0.47068056 <a title="186-lsi-11" href="../hunch_net-2008/hunch_net-2008-07-06-To_Dual_or_Not.html">308 hunch net-2008-07-06-To Dual or Not</a></p>
<p>12 0.46110657 <a title="186-lsi-12" href="../hunch_net-2007/hunch_net-2007-12-21-Vowpal_Wabbit_Code_Release.html">281 hunch net-2007-12-21-Vowpal Wabbit Code Release</a></p>
<p>13 0.45967206 <a title="186-lsi-13" href="../hunch_net-2006/hunch_net-2006-03-27-Gradients_everywhere.html">167 hunch net-2006-03-27-Gradients everywhere</a></p>
<p>14 0.44341582 <a title="186-lsi-14" href="../hunch_net-2011/hunch_net-2011-08-15-Vowpal_Wabbit_6.0.html">441 hunch net-2011-08-15-Vowpal Wabbit 6.0</a></p>
<p>15 0.42275712 <a title="186-lsi-15" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>16 0.41050485 <a title="186-lsi-16" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>17 0.40975517 <a title="186-lsi-17" href="../hunch_net-2005/hunch_net-2005-11-28-A_question_of_quantification.html">133 hunch net-2005-11-28-A question of quantification</a></p>
<p>18 0.40758106 <a title="186-lsi-18" href="../hunch_net-2006/hunch_net-2006-07-05-more_icml_papers.html">189 hunch net-2006-07-05-more icml papers</a></p>
<p>19 0.39978153 <a title="186-lsi-19" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>20 0.39749488 <a title="186-lsi-20" href="../hunch_net-2005/hunch_net-2005-02-25-Problem%3A_Online_Learning.html">28 hunch net-2005-02-25-Problem: Online Learning</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(35, 0.635), (42, 0.205)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.98675984 <a title="186-lda-1" href="../hunch_net-2013/hunch_net-2013-09-20-No_NY_ML_Symposium_in_2013%2C_and_some_good_news.html">489 hunch net-2013-09-20-No NY ML Symposium in 2013, and some good news</a></p>
<p>Introduction: There will be no New York ML Symposium this year. The core issue is thatNYASis
disorganized by people leaving, pushing back the date, with the current
candidate a spring symposium on March 28.Gunnarand I were outvoted here--we
were gung ho on organizing a fall symposium, but the rest of the committee
wants to wait.In some good news, most of theICML 2012 videoshave been restored
from a deep backup.</p><p>same-blog 2 0.95413744 <a title="186-lda-2" href="../hunch_net-2006/hunch_net-2006-06-24-Online_convex_optimization_at_COLT.html">186 hunch net-2006-06-24-Online convex optimization at COLT</a></p>
<p>Introduction: AtICML 2003,Marty Zinkevichproposedthe online convex optimization setting and
showed that a particular gradient descent algorithm has regret O(T0.5) with
respect to the best predictor where T is the number of rounds. This seems to
be a nice model for online learning, and there has been some significant
follow-up work.AtCOLT 2006Elad Hazan,Adam Kalai,Satyen Kale, andAmit
Agarwalpresenteda modification which takes a Newton stepguaranteeing O(log T)
regret when the first and second derivatives are bounded.Then they applied
these algorithms to portfolio managementatICML 2006(withRobert Schapire)
yielding some very fun graphs.</p><p>3 0.90963143 <a title="186-lda-3" href="../hunch_net-2010/hunch_net-2010-12-04-Vowpal_Wabbit%2C_version_5.0%2C_and_the_second_heresy.html">419 hunch net-2010-12-04-Vowpal Wabbit, version 5.0, and the second heresy</a></p>
<p>Introduction: I've releasedversion 5.0of theVowpal Wabbitonline learning software. The major
number has changed since thelast releasebecause I regard all earlier versions
as obsolete--there are several new algorithms & features including substantial
changes and upgrades to the default learning algorithm.The biggest changes are
new algorithms:Nikosand I improved the default algorithm. The basic update
rule still uses gradient descent, but the size of the update is carefully
controlled so that it's impossible to overrun the label. In addition, the
normalization has changed. Computationally, these changes are virtually free
and yield better results, sometimes much better. Less careful updates can be
reenabled with -loss_function classic, although results are still not
identical to previous due to normalization changes.Nikos also implemented the
per-feature learning rates as per thesetwopapers. Often, this works better
than the default algorithm. It isn't the default because it isn't (yet) as
adaptable</p><p>4 0.90753216 <a title="186-lda-4" href="../hunch_net-2005/hunch_net-2005-05-29-Maximum_Margin_Mismatch%3F.html">77 hunch net-2005-05-29-Maximum Margin Mismatch?</a></p>
<p>Introduction: John makes a fascinating point about structured classification (and slightly
scooped my post!). Maximum Margin Markov Networks (M3N) are an interesting
example of the second class of structured classifiers (where the
classification of one label depends on the others), and one of my favorite
papers. I'm not alone: the paper won the best student paper award at NIPS in
2003.There are some things I find odd about the paper. For instance, it says
of probabilistic models"cannot handle high dimensional feature spaces and lack
strong theoretical guarrantees."I'm aware of no such limitations.
Also:"Unfortunately, even probabilistic graphical models that are trained
discriminatively do not achieve the same level of performance as SVMs,
especially when kernel features are used."This is quite interesting and
contradicts my own experience as well as that of a number of
peopleIgreatlyrespect. I wonder what the root cause is: perhaps there is
something different about the data Ben+Carlos were working</p><p>5 0.88930261 <a title="186-lda-5" href="../hunch_net-2006/hunch_net-2006-05-16-The_value_of_the_orthodox_view_of_Boosting.html">179 hunch net-2006-05-16-The value of the orthodox view of Boosting</a></p>
<p>Introduction: The term "boosting" comes from the idea of using a meta-algorithm which takes
"weak" learners (that may be able to only barely predict slightly better than
random) and turn them into strongly capable learners (which predict very
well).Adaboostin 1995 was the first widely used (and useful) boosting
algorithm, although there were theoretical boosting algorithms floating around
since 1990 (see the bottom ofthis page).Since then, many different
interpretations of why boosting works have arisen. There is significant
discussion about these different views in theannals of statistics, including
aresponsebyYoav FreundandRobert Schapire.I believe there is a great deal of
value to be found in the original view of boosting (meta-algorithm for
creating a strong learner from a weak learner). This is not a claim that one
particular viewpoint obviates the value of all others, but rather that no
other viewpoint seems to really capture important properties.Comparing with
all other views of boosting is t</p><p>6 0.82373607 <a title="186-lda-6" href="../hunch_net-2005/hunch_net-2005-05-21-What_is_the_right_form_of_modularity_in_structured_prediction%3F.html">74 hunch net-2005-05-21-What is the right form of modularity in structured prediction?</a></p>
<p>7 0.82105374 <a title="186-lda-7" href="../hunch_net-2005/hunch_net-2005-06-13-Wikis_for_Summer_Schools_and_Workshops.html">81 hunch net-2005-06-13-Wikis for Summer Schools and Workshops</a></p>
<p>8 0.78391576 <a title="186-lda-8" href="../hunch_net-2007/hunch_net-2007-01-15-The_Machine_Learning_Department.html">228 hunch net-2007-01-15-The Machine Learning Department</a></p>
<p>9 0.73360777 <a title="186-lda-9" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>10 0.60670948 <a title="186-lda-10" href="../hunch_net-2011/hunch_net-2011-08-15-Vowpal_Wabbit_6.0.html">441 hunch net-2011-08-15-Vowpal Wabbit 6.0</a></p>
<p>11 0.58923352 <a title="186-lda-11" href="../hunch_net-2005/hunch_net-2005-04-28-Science_Fiction_and_Research.html">64 hunch net-2005-04-28-Science Fiction and Research</a></p>
<p>12 0.58403099 <a title="186-lda-12" href="../hunch_net-2008/hunch_net-2008-08-04-Electoralmarkets.com.html">312 hunch net-2008-08-04-Electoralmarkets.com</a></p>
<p>13 0.55108285 <a title="186-lda-13" href="../hunch_net-2005/hunch_net-2005-03-10-Breaking_Abstractions.html">39 hunch net-2005-03-10-Breaking Abstractions</a></p>
<p>14 0.54154491 <a title="186-lda-14" href="../hunch_net-2011/hunch_net-2011-04-18-A_paper_not_at_Snowbird.html">431 hunch net-2011-04-18-A paper not at Snowbird</a></p>
<p>15 0.54023612 <a title="186-lda-15" href="../hunch_net-2006/hunch_net-2006-03-27-Gradients_everywhere.html">167 hunch net-2006-03-27-Gradients everywhere</a></p>
<p>16 0.53490484 <a title="186-lda-16" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">23 hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<p>17 0.52880293 <a title="186-lda-17" href="../hunch_net-2013/hunch_net-2013-11-09-Graduates_and_Postdocs.html">490 hunch net-2013-11-09-Graduates and Postdocs</a></p>
<p>18 0.52255404 <a title="186-lda-18" href="../hunch_net-2012/hunch_net-2012-06-05-ICML_acceptance_statistics.html">466 hunch net-2012-06-05-ICML acceptance statistics</a></p>
<p>19 0.51684129 <a title="186-lda-19" href="../hunch_net-2008/hunch_net-2008-12-07-A_NIPS_paper.html">330 hunch net-2008-12-07-A NIPS paper</a></p>
<p>20 0.51043069 <a title="186-lda-20" href="../hunch_net-2007/hunch_net-2007-12-20-Cool_and_Interesting_things_at_NIPS%2C_take_three.html">280 hunch net-2007-12-20-Cool and Interesting things at NIPS, take three</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
