<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>219 hunch net-2006-11-22-Explicit Randomization in Learning algorithms</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-219" href="#">hunch_net-2006-219</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>219 hunch net-2006-11-22-Explicit Randomization in Learning algorithms</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-219-html" href="http://hunch.net/?p=239">html</a></p><p>Introduction: There are a number of learning algorithms which explicitly incorporate
randomness into their execution. This includes at amongst others:Neural
Networks. Neural networks use randomization to assign initial
weights.Boltzmann Machines/Deep Belief Networks. Boltzmann machines are
something like a stochastic version of multinode logistic regression. The use
of randomness is more essential in Boltzmann machines, because the predicted
value at test time also uses randomness.Bagging. Bagging is a process where a
learning algorithm is run several different times on several different
datasets, creating a final predictor which makes a majority vote.Policy
descent. Several algorithms in reinforcement learning such asConservative
Policy Iterationuse random bits to create stochastic policies.Experts
algorithms. Randomized weighted majority use random bits as a part of the
prediction process to achieve better theoretical guarantees.A basic question
is: "Should there be explicit randomization in learn</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 There are a number of learning algorithms which explicitly incorporate randomness into their execution. [sent-1, score-0.391]
</p><p>2 The use of randomness is more essential in Boltzmann machines, because the predicted value at test time also uses randomness. [sent-6, score-0.311]
</p><p>3 Bagging is a process where a learning algorithm is run several different times on several different datasets, creating a final predictor which makes a majority vote. [sent-8, score-0.339]
</p><p>4 Several algorithms in reinforcement learning such asConservative Policy Iterationuse random bits to create stochastic policies. [sent-10, score-0.907]
</p><p>5 Randomized weighted majority use random bits as a part of the prediction process to achieve better theoretical guarantees. [sent-12, score-0.892]
</p><p>6 " It seems perverse to feed extra random bits into your prediction process since they don't contain any information about the problem itself. [sent-14, score-0.758]
</p><p>7 This question is not just philosophy--we might hope that deterministic version of learning algorithms are both more accurate and faster. [sent-16, score-0.691]
</p><p>8 In the case of a neural network, if every weight started as 0, the gradient of the loss with respect to every weight would be the same, implying that after updating, all weights remain the same. [sent-19, score-0.466]
</p><p>9 Using random numbers to initialize weights breaks this symmetry. [sent-20, score-0.635]
</p><p>10 It is easy to believe that there are good deterministic methods for symmetry breaking. [sent-21, score-0.443]
</p><p>11 A basic observation is that deterministic learning algorithms tend to overfit. [sent-23, score-0.626]
</p><p>12 Bagging avoids this by randomizing the input of these learning algorithms in the hope that directions of overfit for individual predictions cancel out. [sent-24, score-0.56]
</p><p>13 Similarly, using random bits internally as in a deep belief network avoids overfitting by forcing the algorithm to learn a robust-to-noise set of internal weights, which are then robust-to-overfit. [sent-25, score-0.891]
</p><p>14 Large margin learning algorithms and maximum entropy learning algorithms can be understood as deterministic operations attempting to achieve the same goal. [sent-26, score-1.11]
</p><p>15 A significant gap remains between randomized and deterministic learning algorithms: the deterministic versions just deal with linear predictions while the randomized techniques seem to yield improvements in general. [sent-27, score-1.515]
</p><p>16 In reinforcement learning, it's hard to optimize a policy over multiple timesteps because the optimal decision at timestep 2 is dependent on the decision at timestep 1 and vice versa. [sent-29, score-0.776]
</p><p>17 PSDPcan be understood as a derandomization of CPI which trades off increased computation (learning a new predictor for each timestep individually). [sent-31, score-0.394]
</p><p>18 Some algorithms, such as randomized weighted majority are designed to work against adversaries who know your algorithm,except for random bits. [sent-34, score-0.777]
</p><p>19 The current state-of-the-art is that random bits provide performance (computational and predictive) which we don't know (or at least can't prove we know) how to achieve without randomization. [sent-36, score-0.593]
</p><p>20 Can randomization be removed or is it essential to good learning algorithms? [sent-37, score-0.391]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('deterministic', 0.38), ('randomized', 0.28), ('random', 0.262), ('timestep', 0.244), ('randomization', 0.232), ('bits', 0.23), ('algorithms', 0.185), ('boltzmann', 0.163), ('weights', 0.163), ('bagging', 0.145), ('randomness', 0.145), ('majority', 0.143), ('neural', 0.119), ('avoids', 0.105), ('achieve', 0.101), ('essential', 0.098), ('stochastic', 0.094), ('weighted', 0.092), ('weight', 0.092), ('machines', 0.089), ('network', 0.083), ('understood', 0.079), ('policy', 0.078), ('numbers', 0.078), ('belief', 0.076), ('reinforcement', 0.075), ('predictions', 0.074), ('feed', 0.072), ('internally', 0.072), ('cyclic', 0.072), ('cancel', 0.072), ('perverse', 0.072), ('initialize', 0.072), ('timesteps', 0.072), ('predictor', 0.071), ('uses', 0.068), ('avoid', 0.065), ('version', 0.065), ('process', 0.064), ('forcing', 0.063), ('symmetry', 0.063), ('updating', 0.063), ('vice', 0.063), ('overfit', 0.063), ('provably', 0.063), ('learning', 0.061), ('versions', 0.06), ('breaks', 0.06), ('contain', 0.058), ('attempting', 0.058)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="219-tfidf-1" href="../hunch_net-2006/hunch_net-2006-11-22-Explicit_Randomization_in_Learning_algorithms.html">219 hunch net-2006-11-22-Explicit Randomization in Learning algorithms</a></p>
<p>Introduction: There are a number of learning algorithms which explicitly incorporate
randomness into their execution. This includes at amongst others:Neural
Networks. Neural networks use randomization to assign initial
weights.Boltzmann Machines/Deep Belief Networks. Boltzmann machines are
something like a stochastic version of multinode logistic regression. The use
of randomness is more essential in Boltzmann machines, because the predicted
value at test time also uses randomness.Bagging. Bagging is a process where a
learning algorithm is run several different times on several different
datasets, creating a final predictor which makes a majority vote.Policy
descent. Several algorithms in reinforcement learning such asConservative
Policy Iterationuse random bits to create stochastic policies.Experts
algorithms. Randomized weighted majority use random bits as a part of the
prediction process to achieve better theoretical guarantees.A basic question
is: "Should there be explicit randomization in learn</p><p>2 0.21028528 <a title="219-tfidf-2" href="../hunch_net-2007/hunch_net-2007-01-10-A_Deep_Belief_Net_Learning_Problem.html">227 hunch net-2007-01-10-A Deep Belief Net Learning Problem</a></p>
<p>Introduction: "Deep learning" is used to describe learning architectures which have
significant depth (as a circuit).One claimis that shallow architectures (one
or two layers) can not concisely represent some functions while a circuit with
more depth can concisely represent these same functions. Proving lower bounds
on the size of a circuit is substantially harder than upper bounds (which are
constructive), but some results are known.Luca Trevisan'sclass notesdetail how
XOR is not concisely representable by "AC0â&euro;ł (= constant depth unbounded fan-in
AND, OR, NOT gates). This doesn't quite prove that depth is necessary for the
representations commonly used in learning (such as a thresholded weighted
sum), but it is strongly suggestive that this is so.Examples like this are a
bit disheartening because existing algorithms for deep learning (deep belief
nets, gradient descent on deep neural networks, and a perhaps decision trees
depending on who you ask) can't learn XOR very easily. Evidence so far
sugges</p><p>3 0.13946903 <a title="219-tfidf-3" href="../hunch_net-2005/hunch_net-2005-02-09-Intuitions_from_applied_learning.html">16 hunch net-2005-02-09-Intuitions from applied learning</a></p>
<p>Introduction: Since learning is far from an exact science, it's good to pay attention to
basic intuitions of applied learning. Here are a few I've
collected.IntegrationIn Bayesian learning, the posterior is computed by an
integral, and the optimal thing to do is to predict according to this
integral. This phenomena seems to be far more general. Bagging, Boosting,
SVMs, and Neural Networks all take advantage of this idea to some extent. The
phenomena is more general: you can average over many differentclassification
predictorsto improve performance.
Sources:Zoubin,CaruanaDifferentiationDifferent pieces of an average should
differentiate to achieve good performance by different methods. This is know
as the 'symmetry breaking' problem for neural networks, and it's why weights
are initialized randomly. Boosting explicitly attempts to achieve good
differentiation by creating new, different, learning problems. Sources:Yann
LeCun,Phil LongDeep RepresentationHaving a deep representation is necessary
for hav</p><p>4 0.13412236 <a title="219-tfidf-4" href="../hunch_net-2006/hunch_net-2006-09-07-Objective_and_subjective_interpretations_of_probability.html">205 hunch net-2006-09-07-Objective and subjective interpretations of probability</a></p>
<p>Introduction: An amusing tidbit (reproduced without permission) from Herman Chernoff's
delightful monograph, "Sequential analysis and optimal design":The use of
randomization raises a philosophical question which is articulated by the
following probably apocryphal anecdote.The metallurgist told his friend the
statistician how he planned to test the effect of heat on the strength of a
metal bar by sawing the bar into six pieces. The first two would go into the
hot oven, the next two into the medium oven, and the last two into the cool
oven. The statistician, horrified, explained how he should randomize to avoid
the effect of a possible gradient of strength in the metal bar. The method of
randomization was applied, and it turned out that the randomized experiment
called for putting the first two pieces into the hot oven, the next two into
the medium oven, and the last two into the cool oven. "Obviously, we can't do
that," said the metallurgist. "On the contrary, you have to do that," said the
statisti</p><p>5 0.1266966 <a title="219-tfidf-5" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>Introduction: Many learning algorithms used in practice are fairly simple. Viewed
representationally, many prediction algorithms either compute a linear
separator of basic features (perceptron, winnow, weighted majority, SVM) or
perhaps a linear separator of slightly more complex features (2-layer neural
networks or kernelized SVMs). Should we go beyond this, and start using "deep"
representations?What is deep learning?Intuitively, deep learning is about
learning to predict in ways which can involve complex dependencies between the
input (observed) features.Specifying this more rigorously turns out to be
rather difficult. Consider the following cases:SVM with Gaussian Kernel. This
is not considered deep learning, because an SVM with a gaussian kernel can't
succinctly represent certain decision surfaces. One ofYann LeCun's examples is
recognizing objects based on pixel values. An SVM will need a new support
vector for each significantly different background. Since the number of
distinct backgrounds i</p><p>6 0.12596656 <a title="219-tfidf-6" href="../hunch_net-2007/hunch_net-2007-06-19-How_is_Compressed_Sensing_going_to_change_Machine_Learning_%3F.html">248 hunch net-2007-06-19-How is Compressed Sensing going to change Machine Learning ?</a></p>
<p>7 0.1245422 <a title="219-tfidf-7" href="../hunch_net-2007/hunch_net-2007-08-28-Live_ML_Class.html">261 hunch net-2007-08-28-Live ML Class</a></p>
<p>8 0.12243283 <a title="219-tfidf-8" href="../hunch_net-2011/hunch_net-2011-07-11-Interesting_Neural_Network_Papers_at_ICML_2011.html">438 hunch net-2011-07-11-Interesting Neural Network Papers at ICML 2011</a></p>
<p>9 0.12201735 <a title="219-tfidf-9" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>10 0.12081188 <a title="219-tfidf-10" href="../hunch_net-2009/hunch_net-2009-04-02-Asymmophobia.html">348 hunch net-2009-04-02-Asymmophobia</a></p>
<p>11 0.11823748 <a title="219-tfidf-11" href="../hunch_net-2006/hunch_net-2006-01-30-Should_the_Input_Representation_be_a_Vector%3F.html">152 hunch net-2006-01-30-Should the Input Representation be a Vector?</a></p>
<p>12 0.11074094 <a title="219-tfidf-12" href="../hunch_net-2006/hunch_net-2006-03-12-Online_learning_or_online_preservation_of_learning%3F.html">163 hunch net-2006-03-12-Online learning or online preservation of learning?</a></p>
<p>13 0.10310614 <a title="219-tfidf-13" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>14 0.10092223 <a title="219-tfidf-14" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>15 0.099949129 <a title="219-tfidf-15" href="../hunch_net-2005/hunch_net-2005-11-16-The_Everything_Ensemble_Edge.html">131 hunch net-2005-11-16-The Everything Ensemble Edge</a></p>
<p>16 0.09567982 <a title="219-tfidf-16" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>17 0.094666772 <a title="219-tfidf-17" href="../hunch_net-2008/hunch_net-2008-04-26-Eliminating_the_Birthday_Paradox_for_Universal_Features.html">298 hunch net-2008-04-26-Eliminating the Birthday Paradox for Universal Features</a></p>
<p>18 0.092249513 <a title="219-tfidf-18" href="../hunch_net-2007/hunch_net-2007-08-12-Exponentiated_Gradient.html">258 hunch net-2007-08-12-Exponentiated Gradient</a></p>
<p>19 0.089534886 <a title="219-tfidf-19" href="../hunch_net-2006/hunch_net-2006-01-13-Benchmarks_for_RL.html">148 hunch net-2006-01-13-Benchmarks for RL</a></p>
<p>20 0.087797031 <a title="219-tfidf-20" href="../hunch_net-2010/hunch_net-2010-03-15-The_Efficient_Robust_Conditional_Probability_Estimation_Problem.html">391 hunch net-2010-03-15-The Efficient Robust Conditional Probability Estimation Problem</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.211), (1, -0.135), (2, -0.017), (3, -0.01), (4, -0.077), (5, 0.063), (6, -0.068), (7, 0.023), (8, -0.001), (9, 0.021), (10, 0.114), (11, -0.084), (12, -0.046), (13, 0.024), (14, 0.015), (15, -0.056), (16, -0.11), (17, 0.075), (18, -0.032), (19, 0.075), (20, 0.031), (21, -0.004), (22, 0.035), (23, -0.0), (24, -0.017), (25, 0.056), (26, -0.076), (27, 0.051), (28, 0.066), (29, -0.038), (30, 0.029), (31, 0.051), (32, 0.039), (33, -0.066), (34, 0.03), (35, 0.016), (36, 0.039), (37, 0.036), (38, -0.016), (39, -0.002), (40, -0.043), (41, -0.113), (42, 0.046), (43, -0.044), (44, -0.077), (45, 0.021), (46, 0.061), (47, -0.054), (48, 0.004), (49, 0.041)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.92117137 <a title="219-lsi-1" href="../hunch_net-2006/hunch_net-2006-11-22-Explicit_Randomization_in_Learning_algorithms.html">219 hunch net-2006-11-22-Explicit Randomization in Learning algorithms</a></p>
<p>Introduction: There are a number of learning algorithms which explicitly incorporate
randomness into their execution. This includes at amongst others:Neural
Networks. Neural networks use randomization to assign initial
weights.Boltzmann Machines/Deep Belief Networks. Boltzmann machines are
something like a stochastic version of multinode logistic regression. The use
of randomness is more essential in Boltzmann machines, because the predicted
value at test time also uses randomness.Bagging. Bagging is a process where a
learning algorithm is run several different times on several different
datasets, creating a final predictor which makes a majority vote.Policy
descent. Several algorithms in reinforcement learning such asConservative
Policy Iterationuse random bits to create stochastic policies.Experts
algorithms. Randomized weighted majority use random bits as a part of the
prediction process to achieve better theoretical guarantees.A basic question
is: "Should there be explicit randomization in learn</p><p>2 0.73976666 <a title="219-lsi-2" href="../hunch_net-2005/hunch_net-2005-02-09-Intuitions_from_applied_learning.html">16 hunch net-2005-02-09-Intuitions from applied learning</a></p>
<p>Introduction: Since learning is far from an exact science, it's good to pay attention to
basic intuitions of applied learning. Here are a few I've
collected.IntegrationIn Bayesian learning, the posterior is computed by an
integral, and the optimal thing to do is to predict according to this
integral. This phenomena seems to be far more general. Bagging, Boosting,
SVMs, and Neural Networks all take advantage of this idea to some extent. The
phenomena is more general: you can average over many differentclassification
predictorsto improve performance.
Sources:Zoubin,CaruanaDifferentiationDifferent pieces of an average should
differentiate to achieve good performance by different methods. This is know
as the 'symmetry breaking' problem for neural networks, and it's why weights
are initialized randomly. Boosting explicitly attempts to achieve good
differentiation by creating new, different, learning problems. Sources:Yann
LeCun,Phil LongDeep RepresentationHaving a deep representation is necessary
for hav</p><p>3 0.72348666 <a title="219-lsi-3" href="../hunch_net-2006/hunch_net-2006-01-30-Should_the_Input_Representation_be_a_Vector%3F.html">152 hunch net-2006-01-30-Should the Input Representation be a Vector?</a></p>
<p>Introduction: Let's suppose that we are trying to create a general purpose machine learning
box. The box is fed many examples of the function it is supposed to learn and
(hopefully) succeeds.To date, most such attempts to produce a box of this form
take a vector as input. The elements of the vector might be bits, real
numbers, or 'categorical' data (a discrete set of values).On the other hand,
there are a number of succesful applications of machine learning which do not
seem to use a vector representation as input. For example, in
vision,convolutional neural networkshave been used to solve several vision
problems. The input to the convolutional neural network is essentially the raw
camera image as amatrix. In learning for natural languages, several people
have had success on problems like parts-of-speech tagging using predictors
restricted to a window surrounding the word to be predicted.A vector window
and a matrix both imply a notion of locality which is being actively and
effectively used by thes</p><p>4 0.65605348 <a title="219-lsi-4" href="../hunch_net-2007/hunch_net-2007-01-10-A_Deep_Belief_Net_Learning_Problem.html">227 hunch net-2007-01-10-A Deep Belief Net Learning Problem</a></p>
<p>Introduction: "Deep learning" is used to describe learning architectures which have
significant depth (as a circuit).One claimis that shallow architectures (one
or two layers) can not concisely represent some functions while a circuit with
more depth can concisely represent these same functions. Proving lower bounds
on the size of a circuit is substantially harder than upper bounds (which are
constructive), but some results are known.Luca Trevisan'sclass notesdetail how
XOR is not concisely representable by "AC0â&euro;ł (= constant depth unbounded fan-in
AND, OR, NOT gates). This doesn't quite prove that depth is necessary for the
representations commonly used in learning (such as a thresholded weighted
sum), but it is strongly suggestive that this is so.Examples like this are a
bit disheartening because existing algorithms for deep learning (deep belief
nets, gradient descent on deep neural networks, and a perhaps decision trees
depending on who you ask) can't learn XOR very easily. Evidence so far
sugges</p><p>5 0.6521526 <a title="219-lsi-5" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>Introduction: Many learning algorithms used in practice are fairly simple. Viewed
representationally, many prediction algorithms either compute a linear
separator of basic features (perceptron, winnow, weighted majority, SVM) or
perhaps a linear separator of slightly more complex features (2-layer neural
networks or kernelized SVMs). Should we go beyond this, and start using "deep"
representations?What is deep learning?Intuitively, deep learning is about
learning to predict in ways which can involve complex dependencies between the
input (observed) features.Specifying this more rigorously turns out to be
rather difficult. Consider the following cases:SVM with Gaussian Kernel. This
is not considered deep learning, because an SVM with a gaussian kernel can't
succinctly represent certain decision surfaces. One ofYann LeCun's examples is
recognizing objects based on pixel values. An SVM will need a new support
vector for each significantly different background. Since the number of
distinct backgrounds i</p><p>6 0.60684931 <a title="219-lsi-6" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>7 0.60296506 <a title="219-lsi-7" href="../hunch_net-2005/hunch_net-2005-11-16-The_Everything_Ensemble_Edge.html">131 hunch net-2005-11-16-The Everything Ensemble Edge</a></p>
<p>8 0.59398258 <a title="219-lsi-8" href="../hunch_net-2011/hunch_net-2011-07-11-Interesting_Neural_Network_Papers_at_ICML_2011.html">438 hunch net-2011-07-11-Interesting Neural Network Papers at ICML 2011</a></p>
<p>9 0.58074588 <a title="219-lsi-9" href="../hunch_net-2008/hunch_net-2008-09-12-How_do_we_get_weak_action_dependence_for_learning_with_partial_observations%3F.html">317 hunch net-2008-09-12-How do we get weak action dependence for learning with partial observations?</a></p>
<p>10 0.57611203 <a title="219-lsi-10" href="../hunch_net-2008/hunch_net-2008-04-26-Eliminating_the_Birthday_Paradox_for_Universal_Features.html">298 hunch net-2008-04-26-Eliminating the Birthday Paradox for Universal Features</a></p>
<p>11 0.57143939 <a title="219-lsi-11" href="../hunch_net-2006/hunch_net-2006-05-16-The_value_of_the_orthodox_view_of_Boosting.html">179 hunch net-2006-05-16-The value of the orthodox view of Boosting</a></p>
<p>12 0.55478573 <a title="219-lsi-12" href="../hunch_net-2009/hunch_net-2009-04-02-Asymmophobia.html">348 hunch net-2009-04-02-Asymmophobia</a></p>
<p>13 0.55466539 <a title="219-lsi-13" href="../hunch_net-2007/hunch_net-2007-06-19-How_is_Compressed_Sensing_going_to_change_Machine_Learning_%3F.html">248 hunch net-2007-06-19-How is Compressed Sensing going to change Machine Learning ?</a></p>
<p>14 0.53680158 <a title="219-lsi-14" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>15 0.53441882 <a title="219-lsi-15" href="../hunch_net-2006/hunch_net-2006-07-17-A_Winner.html">197 hunch net-2006-07-17-A Winner</a></p>
<p>16 0.53002501 <a title="219-lsi-16" href="../hunch_net-2007/hunch_net-2007-07-06-Idempotent-capable_Predictors.html">253 hunch net-2007-07-06-Idempotent-capable Predictors</a></p>
<p>17 0.52978039 <a title="219-lsi-17" href="../hunch_net-2005/hunch_net-2005-10-26-Fallback_Analysis_is_a_Secret_to_Useful_Algorithms.html">126 hunch net-2005-10-26-Fallback Analysis is a Secret to Useful Algorithms</a></p>
<p>18 0.52891201 <a title="219-lsi-18" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<p>19 0.52705771 <a title="219-lsi-19" href="../hunch_net-2010/hunch_net-2010-08-23-Boosted_Decision_Trees_for_Deep_Learning.html">407 hunch net-2010-08-23-Boosted Decision Trees for Deep Learning</a></p>
<p>20 0.51943791 <a title="219-lsi-20" href="../hunch_net-2005/hunch_net-2005-09-12-Fast_Gradient_Descent.html">111 hunch net-2005-09-12-Fast Gradient Descent</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(6, 0.027), (35, 0.061), (42, 0.248), (45, 0.048), (50, 0.012), (68, 0.121), (74, 0.092), (81, 0.235), (88, 0.014), (95, 0.038)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.9521001 <a title="219-lda-1" href="../hunch_net-2010/hunch_net-2010-10-08-An_easy_proof_of_the_Chernoff-Hoeffding_bound.html">413 hunch net-2010-10-08-An easy proof of the Chernoff-Hoeffding bound</a></p>
<p>Introduction: Textbooks invariably seem to carry the proof that uses Markov's inequality,
moment-generating functions, and Taylor approximations. Here's an easier
way.For, letbe the KL divergence between a coin of biasand one of
bias:Theorem:Suppose you doindependent tosses of a coin of bias. The
probability of seeingheads or more, for, is at most. So is the probability of
seeingheads or less, for.Remark:By Pinsker's inequality,.ProofLet's do
thecase; the other is identical.Letbe the distribution overinduced by a coin
of bias, and likewisefor a coin of bias. Letbe the set of all sequences
oftosses which containheads or more. We'd like to show thatis unlikely
under.Pick any, with sayheads. Then:Sincefor every, we haveand we're done.</p><p>2 0.91402459 <a title="219-lda-2" href="../hunch_net-2010/hunch_net-2010-05-02-What%26%238217%3Bs_the_difference_between_gambling_and_rewarding_good_prediction%3F.html">397 hunch net-2010-05-02-What&#8217;s the difference between gambling and rewarding good prediction?</a></p>
<p>Introduction: After amajor financial crisis, there is much discussion about howfinance has
become a casinogambling with other's money, keeping the winnings, and walking
away when the money is lost.When thinking about financial reform, all the many
losers in the above scenario are apt to take the view that this activity
should be completely, or nearly completely curtailed. But, a more thoughtful
view is that sometimes there is a real sense in which there are right and
wrong decisions, and we as a society would really prefer that the people most
likely to make right decisions are making them. A crucial question then is:
"What is the difference between gambling and rewarding good
prediction?"Wediscussed this before the financial crisis. The cheat-sheet
sketch is that the online learning against an adversary problem, algorithm,
and theorems, provide a good mathematical model for thinking about this
question. What I would like to do here is map this onto various types of
financial transactions. The basic</p><p>same-blog 3 0.89920938 <a title="219-lda-3" href="../hunch_net-2006/hunch_net-2006-11-22-Explicit_Randomization_in_Learning_algorithms.html">219 hunch net-2006-11-22-Explicit Randomization in Learning algorithms</a></p>
<p>Introduction: There are a number of learning algorithms which explicitly incorporate
randomness into their execution. This includes at amongst others:Neural
Networks. Neural networks use randomization to assign initial
weights.Boltzmann Machines/Deep Belief Networks. Boltzmann machines are
something like a stochastic version of multinode logistic regression. The use
of randomness is more essential in Boltzmann machines, because the predicted
value at test time also uses randomness.Bagging. Bagging is a process where a
learning algorithm is run several different times on several different
datasets, creating a final predictor which makes a majority vote.Policy
descent. Several algorithms in reinforcement learning such asConservative
Policy Iterationuse random bits to create stochastic policies.Experts
algorithms. Randomized weighted majority use random bits as a part of the
prediction process to achieve better theoretical guarantees.A basic question
is: "Should there be explicit randomization in learn</p><p>4 0.89477783 <a title="219-lda-4" href="../hunch_net-2007/hunch_net-2007-06-21-Presentation_Preparation.html">249 hunch net-2007-06-21-Presentation Preparation</a></p>
<p>Introduction: A big part of doing research is presenting it at a conference. Since many
people start out shy of public presentations, this can be a substantial
challenge. Here are a few notes which might be helpful when thinking about
preparing a presentation on research.Motivate. Talks which don't start by
describing the problem to solve cause many people to zone out.Prioritize. It
is typical that you have more things to say than time to say them, and many
presenters fall into the failure mode of trying to say too much. This is an
easy-to-understand failure mode as it's very natural to want to include
everything. A basic fact is: you can't. Example of this are:Your slides are so
densely full of equations and words that you can't cover them.Your talk runs
over and a moderator prioritizes for you by cutting you off.You motor-mouth
through the presentation, and the information absorption rate of the audience
prioritizes in some uncontrolled fashion.The rate of flow of concepts simply
exceeds the infor</p><p>5 0.88175988 <a title="219-lda-5" href="../hunch_net-2008/hunch_net-2008-08-18-Radford_Neal_starts_a_blog.html">313 hunch net-2008-08-18-Radford Neal starts a blog</a></p>
<p>Introduction: hereon statistics, ML, CS, and other things he knows well.</p><p>6 0.8201586 <a title="219-lda-6" href="../hunch_net-2005/hunch_net-2005-02-17-Learning_Research_Programs.html">21 hunch net-2005-02-17-Learning Research Programs</a></p>
<p>7 0.76753175 <a title="219-lda-7" href="../hunch_net-2005/hunch_net-2005-09-10-%26%238220%3BFailure%26%238221%3B_is_an_option.html">110 hunch net-2005-09-10-&#8220;Failure&#8221; is an option</a></p>
<p>8 0.76610762 <a title="219-lda-8" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>9 0.76308 <a title="219-lda-9" href="../hunch_net-2005/hunch_net-2005-02-01-Watchword%3A_Loss.html">9 hunch net-2005-02-01-Watchword: Loss</a></p>
<p>10 0.76245934 <a title="219-lda-10" href="../hunch_net-2007/hunch_net-2007-08-12-Exponentiated_Gradient.html">258 hunch net-2007-08-12-Exponentiated Gradient</a></p>
<p>11 0.75939381 <a title="219-lda-11" href="../hunch_net-2005/hunch_net-2005-06-08-Question%3A_%26%238220%3BWhen_is_the_right_time_to_insert_the_loss_function%3F%26%238221%3B.html">79 hunch net-2005-06-08-Question: &#8220;When is the right time to insert the loss function?&#8221;</a></p>
<p>12 0.75928879 <a title="219-lda-12" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>13 0.7583251 <a title="219-lda-13" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>14 0.75797164 <a title="219-lda-14" href="../hunch_net-2008/hunch_net-2008-04-12-It_Doesn%26%238217%3Bt_Stop.html">295 hunch net-2008-04-12-It Doesn&#8217;t Stop</a></p>
<p>15 0.75788897 <a title="219-lda-15" href="../hunch_net-2005/hunch_net-2005-09-12-Fast_Gradient_Descent.html">111 hunch net-2005-09-12-Fast Gradient Descent</a></p>
<p>16 0.75621641 <a title="219-lda-16" href="../hunch_net-2011/hunch_net-2011-07-11-Interesting_Neural_Network_Papers_at_ICML_2011.html">438 hunch net-2011-07-11-Interesting Neural Network Papers at ICML 2011</a></p>
<p>17 0.75540417 <a title="219-lda-17" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>18 0.7545014 <a title="219-lda-18" href="../hunch_net-2007/hunch_net-2007-08-19-Choice_of_Metrics.html">259 hunch net-2007-08-19-Choice of Metrics</a></p>
<p>19 0.75149924 <a title="219-lda-19" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<p>20 0.75134814 <a title="219-lda-20" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
