<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-203" href="#">hunch_net-2006-203</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-203-html" href="http://hunch.net/?p=221">html</a></p><p>Introduction: The 2006 Machine Learning Summer School in Taipei, Taiwan ended on August 4, 2006. It has been a very exciting two weeks for a record crowd of 245 participants (including speakers and organizers) from 18 countries. We had a lineup of speakers that is hard to match up for other similar events (see our  WIKI  for more information). With this lineup, it is difficult for us as organizers to screw it up too bad.  Also, since we have pretty good infrastructure for international meetings and experienced staff at NTUST and Academia Sinica, plus the reputation established by previous MLSS series, it was relatively easy for us to attract registrations and simply enjoyed this two-week long party of machine learning.
 
In the end of MLSS we distributed a survey form for participants to fill in. I will report what we found from this survey, together with the registration data and word-of-mouth from participants.  
 
The first question is designed to find out how our participants learned about MLSS</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 It has been a very exciting two weeks for a record crowd of 245 participants (including speakers and organizers) from 18 countries. [sent-2, score-0.867]
</p><p>2 In the end of MLSS we distributed a survey form for participants to fill in. [sent-6, score-0.602]
</p><p>3 The first question is designed to find out how our participants learned about MLSS 2006 Taipei. [sent-8, score-0.645]
</p><p>4 Unfortunately, most of the participants learned about MLSS from their advisors and it is difficult for us to track how their advisors learned about MLSS. [sent-9, score-0.819]
</p><p>5 Asked about why they attended MLSS, as expected, about 2/3 replied that they wanted to use ML and 1/3 replied that they wanted to do ML research. [sent-15, score-0.731]
</p><p>6 Most of participants attended all talks, which is consistent with our record. [sent-16, score-0.535]
</p><p>7 Asked about what makes it difficult for them to understand the talks, about half replied mathematics, about a quarter replied “no examples” and less than a quarter replied English. [sent-18, score-0.953]
</p><p>8 Finally, all talk topics were mentioned as being helpful by our participants, especially those talks that are of more introductory nature, such as graphical models by Sam Roweis, SVM by Chih-Jen Lin, and Boosting by Gunnar Ratsch, while talks with many theorems and proofs are less popular. [sent-19, score-0.527]
</p><p>9 A quick fix for this problem is to provide Web pointers to previous MLSS slides and video and urge registered participants to take a look at them in advance to prepare themselves before attending MLSS. [sent-23, score-0.736]
</p><p>10 Then our participants would like organizers to design more activities to encourage interaction with speakers and among participants. [sent-28, score-0.98]
</p><p>11 We could have let our speakers “expose” to participants more often than staying in a cozy VIP lounge. [sent-30, score-0.874]
</p><p>12 We could have also provided online and physical chat board for participants to expose their contact IDs. [sent-31, score-0.646]
</p><p>13 It turned out that our speakers were so good that they covered and adapted to others related talks and made the entire program appear like a carefully designed coherent one. [sent-34, score-0.771]
</p><p>14 So most participants liked the program and only one complaint was about this part. [sent-35, score-0.548]
</p><p>15 One cluster of participants is looking for new research topics on ML or trying to enhance their understanding of some advanced topics on ML. [sent-37, score-0.757]
</p><p>16 If MLSS is designed for them, speakers can present their latest or even ongoing research results. [sent-38, score-0.514]
</p><p>17 To them, speakers need to present more examples, show them applications, and present mature results. [sent-42, score-0.485]
</p><p>18 We also designed a graduate credit program to give registered students a preview and prerequisite math background. [sent-46, score-0.534]
</p><p>19 I think we could have done a better job helping our participants understand the nature of the summer school and be prepared. [sent-48, score-0.746]
</p><p>20 Finally, on behalf of the steering committee, I would like to take this chance to thank Alex Smola, Bernhard Scholkoph and John Langford for their help to put together this excellent lineup of speakers and the positive examples they established in previous MLSS series for us to learn from. [sent-49, score-0.988]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('participants', 0.478), ('speakers', 0.331), ('mlss', 0.273), ('replied', 0.257), ('talks', 0.195), ('lineup', 0.154), ('ml', 0.144), ('organizers', 0.114), ('designed', 0.106), ('expose', 0.103), ('finally', 0.092), ('quarter', 0.091), ('prerequisite', 0.091), ('topics', 0.081), ('advisors', 0.08), ('wanted', 0.08), ('job', 0.08), ('present', 0.077), ('registered', 0.076), ('established', 0.076), ('together', 0.074), ('students', 0.07), ('program', 0.07), ('related', 0.069), ('better', 0.068), ('could', 0.065), ('slides', 0.065), ('survey', 0.065), ('chance', 0.064), ('svm', 0.063), ('series', 0.062), ('international', 0.062), ('math', 0.062), ('learned', 0.061), ('us', 0.059), ('distributed', 0.059), ('advance', 0.059), ('graduate', 0.059), ('cluster', 0.059), ('trying', 0.058), ('previous', 0.058), ('record', 0.058), ('would', 0.057), ('web', 0.057), ('attended', 0.057), ('graphical', 0.056), ('school', 0.055), ('asked', 0.054), ('mostly', 0.053), ('examples', 0.053)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="203-tfidf-1" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>Introduction: The 2006 Machine Learning Summer School in Taipei, Taiwan ended on August 4, 2006. It has been a very exciting two weeks for a record crowd of 245 participants (including speakers and organizers) from 18 countries. We had a lineup of speakers that is hard to match up for other similar events (see our  WIKI  for more information). With this lineup, it is difficult for us as organizers to screw it up too bad.  Also, since we have pretty good infrastructure for international meetings and experienced staff at NTUST and Academia Sinica, plus the reputation established by previous MLSS series, it was relatively easy for us to attract registrations and simply enjoyed this two-week long party of machine learning.
 
In the end of MLSS we distributed a survey form for participants to fill in. I will report what we found from this survey, together with the registration data and word-of-mouth from participants.  
 
The first question is designed to find out how our participants learned about MLSS</p><p>2 0.23825438 <a title="203-tfidf-2" href="../hunch_net-2005/hunch_net-2005-05-28-Running_A_Machine_Learning_Summer_School.html">75 hunch net-2005-05-28-Running A Machine Learning Summer School</a></p>
<p>Introduction: We just finished the  Chicago 2005 Machine Learning Summer School .  The school was 2 weeks long with about 130 (or 140 counting the speakers) participants.   For perspective, this is perhaps the largest graduate level machine learning class I am aware of anywhere and anytime (previous  MLSS s have been close).  Overall, it seemed to go well, although the students are the real authority on this.  For those who missed it, DVDs will be available from our Slovenian friends.  Email  Mrs Spela Sitar  of the Jozsef Stefan Institute for details. 
 
The following are some notes for future planning and those interested. 
 Good Decisions 
  
 Acquiring the larger-than-necessary “Assembly Hall” at  International House .  Our attendance came in well above our expectations, so this was a critical early decision that made a huge difference. 
 The invited speakers were key.  They made a huge difference in the quality of the content. 
 Delegating early and often was important.  One key difficulty here</p><p>3 0.18379641 <a title="203-tfidf-3" href="../hunch_net-2005/hunch_net-2005-06-10-Workshops_are_not_Conferences.html">80 hunch net-2005-06-10-Workshops are not Conferences</a></p>
<p>Introduction: … and you should use that fact.
 
A workshop differs from a conference in that it is about a focused group of people worrying about a focused topic.  It also differs in that a workshop is typically a “one-time affair” rather than a series.  (The  Snowbird learning workshop  counts as a conference in this respect.)  
 
A common failure mode of both organizers and speakers at a workshop is to treat it as a conference.  This is “ok”, but it is not really taking advantage of the situation.  Here are some things I’ve learned:
  
 For speakers: A smaller audience means it can be more interactive.  Interactive means a better chance to avoid losing your audience and a more interesting presentation (because you can adapt to your audience).  Greater focus amongst the participants means you can get to the heart of the matter more easily, and discuss tradeoffs more carefully.  Unlike conferences, relevance is more valued than newness. 
 For organizers: Not everything needs to be in a conference st</p><p>4 0.15681313 <a title="203-tfidf-4" href="../hunch_net-2008/hunch_net-2008-10-20-New_York%26%238217%3Bs_ML_Day.html">322 hunch net-2008-10-20-New York&#8217;s ML Day</a></p>
<p>Introduction: I’m not as naturally exuberant as  Muthu   2  or  David  about  CS/Econ  day, but I believe it and  ML day  were certainly successful.
 
At the CS/Econ day, I particularly enjoyed  Toumas Sandholm’s  talk which showed a commanding depth of understanding and application in automated auctions.
 
For the machine learning day, I enjoyed several talks and posters (I better, I helped pick them.).  What stood out to me was number of people attending: 158 registered, a level qualifying as “scramble to find seats”.  My rule of thumb for workshops/conferences is that the number of attendees is often something like the number of submissions.  That isn’t the case here, where there were just 4 invited speakers and 30-or-so posters.  Presumably, the difference is due to a critical mass of Machine Learning interested people in the area and the ease of their attendance.  
 
Are there other areas where a local Machine Learning day would fly?  It’s easy to imagine something working out in the San Franci</p><p>5 0.13768262 <a title="203-tfidf-5" href="../hunch_net-2005/hunch_net-2005-06-13-Wikis_for_Summer_Schools_and_Workshops.html">81 hunch net-2005-06-13-Wikis for Summer Schools and Workshops</a></p>
<p>Introduction: Chicago ’05  ended a couple of weeks ago. This was the sixth  Machine Learning Summer School , and the second one that used a  wiki . (The first was Berder ’04, thanks to Gunnar Raetsch.) Wikis are relatively easy to set up, greatly aid social interaction, and should be used a lot more at summer schools and workshops. They can even be used as the meeting’s webpage, as a permanent record of its participants’ collaborations — see for example the wiki/website for last year’s  NVO Summer School .
 
A basic wiki is a collection of editable webpages, maintained by software called a   wiki engine . The engine used at both Berder and Chicago was  TikiWiki  — it is well documented and gets you something running fast. It uses PHP and MySQL, but doesn’t require you to know either. Tikiwiki has far more features than most wikis, as it is  really a full  Content Management System . (My thanks to Sebastian Stark for pointing this out.) Here are the features we found most useful:
  



  Bulletin boa</p><p>6 0.12922458 <a title="203-tfidf-6" href="../hunch_net-2009/hunch_net-2009-11-09-NYAS_ML_Symposium_this_year..html">377 hunch net-2009-11-09-NYAS ML Symposium this year.</a></p>
<p>7 0.12460285 <a title="203-tfidf-7" href="../hunch_net-2007/hunch_net-2007-08-28-Live_ML_Class.html">261 hunch net-2007-08-28-Live ML Class</a></p>
<p>8 0.11118764 <a title="203-tfidf-8" href="../hunch_net-2007/hunch_net-2007-02-22-Create_Your_Own_ICML_Workshop.html">234 hunch net-2007-02-22-Create Your Own ICML Workshop</a></p>
<p>9 0.11012911 <a title="203-tfidf-9" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>10 0.10982668 <a title="203-tfidf-10" href="../hunch_net-2011/hunch_net-2011-10-24-2011_ML_symposium_and_the_bears.html">448 hunch net-2011-10-24-2011 ML symposium and the bears</a></p>
<p>11 0.10914357 <a title="203-tfidf-11" href="../hunch_net-2005/hunch_net-2005-11-16-MLSS_2006.html">130 hunch net-2005-11-16-MLSS 2006</a></p>
<p>12 0.10503451 <a title="203-tfidf-12" href="../hunch_net-2012/hunch_net-2012-02-29-Key_Scientific_Challenges_and_the_Franklin_Symposium.html">457 hunch net-2012-02-29-Key Scientific Challenges and the Franklin Symposium</a></p>
<p>13 0.10500566 <a title="203-tfidf-13" href="../hunch_net-2009/hunch_net-2009-08-27-New_York_Area_Machine_Learning_Events.html">369 hunch net-2009-08-27-New York Area Machine Learning Events</a></p>
<p>14 0.10290992 <a title="203-tfidf-14" href="../hunch_net-2013/hunch_net-2013-08-31-Extreme_Classification_workshop_at_NIPS.html">488 hunch net-2013-08-31-Extreme Classification workshop at NIPS</a></p>
<p>15 0.099663787 <a title="203-tfidf-15" href="../hunch_net-2009/hunch_net-2009-09-21-Netflix_finishes_%28and_starts%29.html">371 hunch net-2009-09-21-Netflix finishes (and starts)</a></p>
<p>16 0.097937033 <a title="203-tfidf-16" href="../hunch_net-2009/hunch_net-2009-05-30-Many_ways_to_Learn_this_summer.html">357 hunch net-2009-05-30-Many ways to Learn this summer</a></p>
<p>17 0.095958479 <a title="203-tfidf-17" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>18 0.090427347 <a title="203-tfidf-18" href="../hunch_net-2011/hunch_net-2011-11-26-Giving_Thanks.html">449 hunch net-2011-11-26-Giving Thanks</a></p>
<p>19 0.089892417 <a title="203-tfidf-19" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>20 0.084524162 <a title="203-tfidf-20" href="../hunch_net-2012/hunch_net-2012-10-26-ML_Symposium_and_Strata-Hadoop_World.html">475 hunch net-2012-10-26-ML Symposium and Strata-Hadoop World</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.206), (1, -0.082), (2, -0.122), (3, 0.001), (4, -0.034), (5, 0.04), (6, -0.015), (7, -0.026), (8, -0.068), (9, -0.1), (10, 0.039), (11, -0.012), (12, 0.086), (13, 0.037), (14, 0.091), (15, -0.034), (16, -0.016), (17, 0.204), (18, 0.067), (19, 0.157), (20, 0.044), (21, -0.052), (22, 0.097), (23, -0.094), (24, 0.052), (25, 0.019), (26, 0.026), (27, -0.017), (28, 0.054), (29, 0.079), (30, -0.124), (31, -0.046), (32, 0.067), (33, -0.121), (34, -0.01), (35, -0.072), (36, -0.048), (37, -0.023), (38, 0.017), (39, -0.085), (40, 0.013), (41, -0.006), (42, -0.072), (43, 0.029), (44, -0.034), (45, 0.085), (46, 0.052), (47, 0.104), (48, -0.089), (49, 0.013)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97345465 <a title="203-lsi-1" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>Introduction: The 2006 Machine Learning Summer School in Taipei, Taiwan ended on August 4, 2006. It has been a very exciting two weeks for a record crowd of 245 participants (including speakers and organizers) from 18 countries. We had a lineup of speakers that is hard to match up for other similar events (see our  WIKI  for more information). With this lineup, it is difficult for us as organizers to screw it up too bad.  Also, since we have pretty good infrastructure for international meetings and experienced staff at NTUST and Academia Sinica, plus the reputation established by previous MLSS series, it was relatively easy for us to attract registrations and simply enjoyed this two-week long party of machine learning.
 
In the end of MLSS we distributed a survey form for participants to fill in. I will report what we found from this survey, together with the registration data and word-of-mouth from participants.  
 
The first question is designed to find out how our participants learned about MLSS</p><p>2 0.77830827 <a title="203-lsi-2" href="../hunch_net-2005/hunch_net-2005-05-28-Running_A_Machine_Learning_Summer_School.html">75 hunch net-2005-05-28-Running A Machine Learning Summer School</a></p>
<p>Introduction: We just finished the  Chicago 2005 Machine Learning Summer School .  The school was 2 weeks long with about 130 (or 140 counting the speakers) participants.   For perspective, this is perhaps the largest graduate level machine learning class I am aware of anywhere and anytime (previous  MLSS s have been close).  Overall, it seemed to go well, although the students are the real authority on this.  For those who missed it, DVDs will be available from our Slovenian friends.  Email  Mrs Spela Sitar  of the Jozsef Stefan Institute for details. 
 
The following are some notes for future planning and those interested. 
 Good Decisions 
  
 Acquiring the larger-than-necessary “Assembly Hall” at  International House .  Our attendance came in well above our expectations, so this was a critical early decision that made a huge difference. 
 The invited speakers were key.  They made a huge difference in the quality of the content. 
 Delegating early and often was important.  One key difficulty here</p><p>3 0.70115989 <a title="203-lsi-3" href="../hunch_net-2008/hunch_net-2008-10-20-New_York%26%238217%3Bs_ML_Day.html">322 hunch net-2008-10-20-New York&#8217;s ML Day</a></p>
<p>Introduction: I’m not as naturally exuberant as  Muthu   2  or  David  about  CS/Econ  day, but I believe it and  ML day  were certainly successful.
 
At the CS/Econ day, I particularly enjoyed  Toumas Sandholm’s  talk which showed a commanding depth of understanding and application in automated auctions.
 
For the machine learning day, I enjoyed several talks and posters (I better, I helped pick them.).  What stood out to me was number of people attending: 158 registered, a level qualifying as “scramble to find seats”.  My rule of thumb for workshops/conferences is that the number of attendees is often something like the number of submissions.  That isn’t the case here, where there were just 4 invited speakers and 30-or-so posters.  Presumably, the difference is due to a critical mass of Machine Learning interested people in the area and the ease of their attendance.  
 
Are there other areas where a local Machine Learning day would fly?  It’s easy to imagine something working out in the San Franci</p><p>4 0.70052266 <a title="203-lsi-4" href="../hunch_net-2005/hunch_net-2005-06-13-Wikis_for_Summer_Schools_and_Workshops.html">81 hunch net-2005-06-13-Wikis for Summer Schools and Workshops</a></p>
<p>Introduction: Chicago ’05  ended a couple of weeks ago. This was the sixth  Machine Learning Summer School , and the second one that used a  wiki . (The first was Berder ’04, thanks to Gunnar Raetsch.) Wikis are relatively easy to set up, greatly aid social interaction, and should be used a lot more at summer schools and workshops. They can even be used as the meeting’s webpage, as a permanent record of its participants’ collaborations — see for example the wiki/website for last year’s  NVO Summer School .
 
A basic wiki is a collection of editable webpages, maintained by software called a   wiki engine . The engine used at both Berder and Chicago was  TikiWiki  — it is well documented and gets you something running fast. It uses PHP and MySQL, but doesn’t require you to know either. Tikiwiki has far more features than most wikis, as it is  really a full  Content Management System . (My thanks to Sebastian Stark for pointing this out.) Here are the features we found most useful:
  



  Bulletin boa</p><p>5 0.59351438 <a title="203-lsi-5" href="../hunch_net-2005/hunch_net-2005-06-10-Workshops_are_not_Conferences.html">80 hunch net-2005-06-10-Workshops are not Conferences</a></p>
<p>Introduction: … and you should use that fact.
 
A workshop differs from a conference in that it is about a focused group of people worrying about a focused topic.  It also differs in that a workshop is typically a “one-time affair” rather than a series.  (The  Snowbird learning workshop  counts as a conference in this respect.)  
 
A common failure mode of both organizers and speakers at a workshop is to treat it as a conference.  This is “ok”, but it is not really taking advantage of the situation.  Here are some things I’ve learned:
  
 For speakers: A smaller audience means it can be more interactive.  Interactive means a better chance to avoid losing your audience and a more interesting presentation (because you can adapt to your audience).  Greater focus amongst the participants means you can get to the heart of the matter more easily, and discuss tradeoffs more carefully.  Unlike conferences, relevance is more valued than newness. 
 For organizers: Not everything needs to be in a conference st</p><p>6 0.58854973 <a title="203-lsi-6" href="../hunch_net-2009/hunch_net-2009-11-09-NYAS_ML_Symposium_this_year..html">377 hunch net-2009-11-09-NYAS ML Symposium this year.</a></p>
<p>7 0.5095467 <a title="203-lsi-7" href="../hunch_net-2010/hunch_net-2010-08-21-Rob_Schapire_at_NYC_ML_Meetup.html">405 hunch net-2010-08-21-Rob Schapire at NYC ML Meetup</a></p>
<p>8 0.50331783 <a title="203-lsi-8" href="../hunch_net-2007/hunch_net-2007-06-21-Presentation_Preparation.html">249 hunch net-2007-06-21-Presentation Preparation</a></p>
<p>9 0.49625167 <a title="203-lsi-9" href="../hunch_net-2007/hunch_net-2007-08-28-Live_ML_Class.html">261 hunch net-2007-08-28-Live ML Class</a></p>
<p>10 0.48857939 <a title="203-lsi-10" href="../hunch_net-2006/hunch_net-2006-01-06-MLTV.html">146 hunch net-2006-01-06-MLTV</a></p>
<p>11 0.48753196 <a title="203-lsi-11" href="../hunch_net-2014/hunch_net-2014-02-16-Metacademy%3A_a_package_manager_for_knowledge.html">493 hunch net-2014-02-16-Metacademy: a package manager for knowledge</a></p>
<p>12 0.4858942 <a title="203-lsi-12" href="../hunch_net-2011/hunch_net-2011-11-26-Giving_Thanks.html">449 hunch net-2011-11-26-Giving Thanks</a></p>
<p>13 0.48437968 <a title="203-lsi-13" href="../hunch_net-2009/hunch_net-2009-05-30-Many_ways_to_Learn_this_summer.html">357 hunch net-2009-05-30-Many ways to Learn this summer</a></p>
<p>14 0.47209749 <a title="203-lsi-14" href="../hunch_net-2011/hunch_net-2011-10-24-2011_ML_symposium_and_the_bears.html">448 hunch net-2011-10-24-2011 ML symposium and the bears</a></p>
<p>15 0.46505141 <a title="203-lsi-15" href="../hunch_net-2007/hunch_net-2007-11-16-MLSS_2008.html">273 hunch net-2007-11-16-MLSS 2008</a></p>
<p>16 0.45743579 <a title="203-lsi-16" href="../hunch_net-2005/hunch_net-2005-01-26-Summer_Schools.html">4 hunch net-2005-01-26-Summer Schools</a></p>
<p>17 0.44608951 <a title="203-lsi-17" href="../hunch_net-2005/hunch_net-2005-05-11-Visa_Casualties.html">69 hunch net-2005-05-11-Visa Casualties</a></p>
<p>18 0.4459416 <a title="203-lsi-18" href="../hunch_net-2005/hunch_net-2005-07-01-The_Role_of_Impromptu_Talks.html">88 hunch net-2005-07-01-The Role of Impromptu Talks</a></p>
<p>19 0.44191653 <a title="203-lsi-19" href="../hunch_net-2012/hunch_net-2012-10-26-ML_Symposium_and_Strata-Hadoop_World.html">475 hunch net-2012-10-26-ML Symposium and Strata-Hadoop World</a></p>
<p>20 0.43682185 <a title="203-lsi-20" href="../hunch_net-2010/hunch_net-2010-10-28-NY_ML_Symposium_2010.html">415 hunch net-2010-10-28-NY ML Symposium 2010</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(4, 0.019), (13, 0.02), (27, 0.135), (35, 0.015), (38, 0.043), (48, 0.04), (49, 0.015), (53, 0.062), (55, 0.128), (67, 0.025), (92, 0.29), (94, 0.065), (95, 0.057)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.92305219 <a title="203-lda-1" href="../hunch_net-2009/hunch_net-2009-06-26-Netflix_nearly_done.html">362 hunch net-2009-06-26-Netflix nearly done</a></p>
<p>Introduction: A $1M qualifying result was achieved on the  public Netflix test set  by a  3-way ensemble team .  This is just in time for  Yehuda ‘s presentation at  KDD , which I’m sure will be one of the best attended ever.  
 
This isn’t quite over—there are a few days for another super-conglomerate team to come together and there is some small chance that the performance is nonrepresentative of the final test set, but I expect not.  
 
Regardless of the final outcome, the biggest lesson for ML from the Netflix contest has been the formidable performance edge of ensemble methods.</p><p>2 0.9158321 <a title="203-lda-2" href="../hunch_net-2007/hunch_net-2007-11-14-BellKor_wins_Netflix.html">272 hunch net-2007-11-14-BellKor wins Netflix</a></p>
<p>Introduction: … but only the little prize.  The  BellKor team  focused on integrating predictions from many different methods.  The base methods consist of:
  
 Nearest Neighbor Methods 
 Matrix Factorization Methods (asymmetric and symmetric) 
 Linear Regression on various feature spaces 
 Restricted Boltzman Machines 
  
The final predictor was an ensemble (as was reasonable to expect), although it’s a little bit more complicated than just a weighted average—it’s essentially a customized learning algorithm.  Base approaches (1)-(3) seem like relatively well-known approaches (although I haven’t seen the asymmetric factorization variant before).  RBMs are the new approach.
 
The  writeup  is pretty clear for more details.
 
The contestants are close to reaching the big prize, but the last 1.5% is probably at least as hard as what’s been done.  A few new structurally different methods for making predictions may need to be discovered and added into the mixture.  In other words, research may be require</p><p>3 0.90473294 <a title="203-lda-3" href="../hunch_net-2007/hunch_net-2007-04-13-What_to_do_with_an_unreasonable_conditional_accept.html">238 hunch net-2007-04-13-What to do with an unreasonable conditional accept</a></p>
<p>Introduction: Last year about this time, we received a conditional accept for the  searn paper , which asked us to reference a paper that was not reasonable to cite because there was strictly more relevant work by the same authors that we already cited.  We wrote a response explaining this, and didn’t cite it in the final draft, giving the SPC an excuse to  reject the paper , leading to unhappiness for all.
 
Later,  Sanjoy Dasgupta  suggested that an alternative was to talk to the PC chair instead, as soon as you see that a conditional accept is unreasonable.   William Cohen  and I spoke about this by email, the relevant bit of which is:
  

If an SPC asks for a revision that is inappropriate, the correct 
action is to contact the chairs as soon as the decision is made, 
clearly explaining what the problem is, so we can decide whether or 
not to over-rule the SPC.  As you say, this is extra work for us 
chairs, but that’s part of the job, and we’re willing to do that sort 
of work to improve the ov</p><p>same-blog 4 0.87827349 <a title="203-lda-4" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>Introduction: The 2006 Machine Learning Summer School in Taipei, Taiwan ended on August 4, 2006. It has been a very exciting two weeks for a record crowd of 245 participants (including speakers and organizers) from 18 countries. We had a lineup of speakers that is hard to match up for other similar events (see our  WIKI  for more information). With this lineup, it is difficult for us as organizers to screw it up too bad.  Also, since we have pretty good infrastructure for international meetings and experienced staff at NTUST and Academia Sinica, plus the reputation established by previous MLSS series, it was relatively easy for us to attract registrations and simply enjoyed this two-week long party of machine learning.
 
In the end of MLSS we distributed a survey form for participants to fill in. I will report what we found from this survey, together with the registration data and word-of-mouth from participants.  
 
The first question is designed to find out how our participants learned about MLSS</p><p>5 0.67663354 <a title="203-lda-5" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>Introduction: Unfortunately, I ended up sick for much of this ICML.  I did manage to catch one interesting paper:
 
 Richard Socher ,  Cliff Lin ,  Andrew Y. Ng , and  Christopher D. Manning   Parsing Natural Scenes and Natural Language with Recursive Neural Networks .
 
I invited Richard to share his list of interesting papers, so hopefully we’ll hear from him soon.  In the meantime,  Paul  and  Hal  have posted some lists.
 
 the future 
 
 Joelle  and I are program chairs for ICML 2012 in  Edinburgh , which I previously enjoyed visiting in  2005 .  This is a huge responsibility, that we hope to accomplish well.  A part of this (perhaps the most fun part), is imagining how we can make ICML better.  A key and critical constraint is choosing things that can be accomplished.  So far we have:
  
  Colocation . The first thing we looked into was potential colocations.  We quickly discovered that many other conferences precomitted their location.  For the future, getting a colocation with  ACL  or  SIGI</p><p>6 0.63639975 <a title="203-lda-6" href="../hunch_net-2012/hunch_net-2012-05-02-ICML%3A_Behind_the_Scenes.html">463 hunch net-2012-05-02-ICML: Behind the Scenes</a></p>
<p>7 0.62506837 <a title="203-lda-7" href="../hunch_net-2005/hunch_net-2005-05-28-Running_A_Machine_Learning_Summer_School.html">75 hunch net-2005-05-28-Running A Machine Learning Summer School</a></p>
<p>8 0.61007434 <a title="203-lda-8" href="../hunch_net-2012/hunch_net-2012-04-09-ICML_author_feedback_is_open.html">461 hunch net-2012-04-09-ICML author feedback is open</a></p>
<p>9 0.60444117 <a title="203-lda-9" href="../hunch_net-2008/hunch_net-2008-03-23-Interactive_Machine_Learning.html">293 hunch net-2008-03-23-Interactive Machine Learning</a></p>
<p>10 0.60024887 <a title="203-lda-10" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>11 0.59902668 <a title="203-lda-11" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>12 0.58053046 <a title="203-lda-12" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<p>13 0.57809377 <a title="203-lda-13" href="../hunch_net-2006/hunch_net-2006-09-12-Incentive_Compatible_Reviewing.html">207 hunch net-2006-09-12-Incentive Compatible Reviewing</a></p>
<p>14 0.57535094 <a title="203-lda-14" href="../hunch_net-2012/hunch_net-2012-06-05-ICML_acceptance_statistics.html">466 hunch net-2012-06-05-ICML acceptance statistics</a></p>
<p>15 0.57370049 <a title="203-lda-15" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>16 0.57150209 <a title="203-lda-16" href="../hunch_net-2012/hunch_net-2012-03-06-COLT-ICML_Open_Questions_and_ICML_Instructions.html">458 hunch net-2012-03-06-COLT-ICML Open Questions and ICML Instructions</a></p>
<p>17 0.56917143 <a title="203-lda-17" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>18 0.56812352 <a title="203-lda-18" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>19 0.56618118 <a title="203-lda-19" href="../hunch_net-2005/hunch_net-2005-03-13-Avoiding_Bad_Reviewing.html">40 hunch net-2005-03-13-Avoiding Bad Reviewing</a></p>
<p>20 0.56490958 <a title="203-lda-20" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
