<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>207 hunch net-2006-09-12-Incentive Compatible Reviewing</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-207" href="#">hunch_net-2006-207</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>207 hunch net-2006-09-12-Incentive Compatible Reviewing</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-207-html" href="http://hunch.net/?p=228">html</a></p><p>Introduction: Reviewing is a fairly formal process which is integral to the way academia is
run. Given this integral nature, the quality of reviewing is often
frustrating. I've seen plenty of examples of false statements, misbeliefs,
reading what isn't written, etcâ&euro;Ś, and I'm sure many other people have as
well.Recently, mechanisms like double blind review and author feedback have
been introduced to try to make the process more fair and accurate in many
machine learning (and related) conferences. My personal experience is that
these mechanisms help, especially the author feedback. Nevertheless, some
problems remain.The game theory take on reviewing is that the incentive for
truthful reviewing isn't there. Since reviewers are also authors, there are
sometimes perverse incentives created and acted upon. (Incidentially, these
incentives can be both positive and negative.)Setting up a truthful reviewing
system is tricky because their is no final reference truth available in any
acceptable (say: subyear)</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('reviewing', 0.27), ('incentive', 0.241), ('reviewer', 0.207), ('scoring', 0.201), ('mechanisms', 0.187), ('truthful', 0.181), ('try', 0.169), ('truth', 0.167), ('reviewers', 0.15), ('say', 0.147), ('rule', 0.145), ('judging', 0.141), ('tracking', 0.141), ('reviews', 0.136), ('integral', 0.134), ('incentives', 0.134), ('compatible', 0.134), ('system', 0.133), ('reading', 0.132), ('proper', 0.12)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="207-tfidf-1" href="../hunch_net-2006/hunch_net-2006-09-12-Incentive_Compatible_Reviewing.html">207 hunch net-2006-09-12-Incentive Compatible Reviewing</a></p>
<p>Introduction: Reviewing is a fairly formal process which is integral to the way academia is
run. Given this integral nature, the quality of reviewing is often
frustrating. I've seen plenty of examples of false statements, misbeliefs,
reading what isn't written, etcâ&euro;Ś, and I'm sure many other people have as
well.Recently, mechanisms like double blind review and author feedback have
been introduced to try to make the process more fair and accurate in many
machine learning (and related) conferences. My personal experience is that
these mechanisms help, especially the author feedback. Nevertheless, some
problems remain.The game theory take on reviewing is that the incentive for
truthful reviewing isn't there. Since reviewers are also authors, there are
sometimes perverse incentives created and acted upon. (Incidentially, these
incentives can be both positive and negative.)Setting up a truthful reviewing
system is tricky because their is no final reference truth available in any
acceptable (say: subyear)</p><p>2 0.29375315 <a title="207-tfidf-2" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>Introduction: When thinking about how best to review papers, it seems helpful to have some
conception of what good reviewing is. As far as I can tell, this is almost
always only discussed in the specific context of a paper (i.e. your rejected
paper), or at most an area (i.e. what a "good paper" looks like for that area)
rather than general principles. Neither individual papers or areas are
sufficiently general for a large conference--every paper differs in the
details, and what if you want to build a new area and/or cross areas?An
unavoidable reason for reviewing is that the community of research is too
large. In particular, it is not possible for a researcher to read every paper
which someone thinks might be of interest. This reason for reviewing exists
independent of constraints on rooms or scheduling formats of individual
conferences. Indeed, history suggests that physical constraints are relatively
meaningless over the long term -- growing conferences simply use more rooms
and/or change formats</p><p>3 0.25242978 <a title="207-tfidf-3" href="../hunch_net-2010/hunch_net-2010-04-26-Compassionate_Reviewing.html">395 hunch net-2010-04-26-Compassionate Reviewing</a></p>
<p>Introduction: Most long conversations between academics seem to converge on the topic of
reviewing where almost no one is happy. A basic question is: Should most
people be happy?The case against is straightforward. Anyone who watches the
flow of papers realizes that most papers amount to little in the longer term.
By it's nature research is brutal, where the second-best method is worthless,
and the second person to discover things typically gets no credit. If you
think about this for a moment, it's very different from most other human
endeavors. The second best migrant laborer, construction worker, manager,
conductor, quarterback, etcâ&euro;Ś all can manage quite well. If a reviewer has even
a vaguely predictive sense of what's important in the longer term, then most
people submitting papers will be unhappy.But this argument unravels, in my
experience. Perhaps half of reviews are thoughtless or simply wrong with a
small part being simply malicious. And yet, I'm sure that most reviewers
genuinely believe th</p><p>4 0.23760983 <a title="207-tfidf-4" href="../hunch_net-2005/hunch_net-2005-03-13-Avoiding_Bad_Reviewing.html">40 hunch net-2005-03-13-Avoiding Bad Reviewing</a></p>
<p>Introduction: If we accept that bad reviewing often occurs and want to fix it, the question
is "how"?Reviewing is done by paper writers just like yourself, so a good
proxy for this question is asking "How can I be a better reviewer?" Here are a
few things I've learned by trial (and error), as a paper writer, and as a
reviewer.The secret ingredient is careful thought. There is no good
substitution for a deep and careful understanding.Avoid reviewing papers that
you feel competitive about. You almost certainly will be asked to review
papers that feel competitive if you work on subjects of common interest. But,
the feeling of competition can easily lead to bad judgement.If you feel biased
for some other reason, then you should avoid reviewing. For exampleâ&euro;ŚFeeling
angry or threatened by a paper is a form of bias. See above.Double blind
yourself (avoid looking at the name even in a single-blind situation). The
significant effect of a name you recognize is making you pay close attention
to a paper. Since</p><p>5 0.23430018 <a title="207-tfidf-5" href="../hunch_net-2012/hunch_net-2012-04-09-ICML_author_feedback_is_open.html">461 hunch net-2012-04-09-ICML author feedback is open</a></p>
<p>Introduction: as of last night, late.When the reviewing deadline passed Wednesday night 15%
of reviews were still missing, much higher than I expected. Between late
reviews coming in, ACs working overtime through the weekend, and people
willing to help in the pinch another ~390 reviews came in, reducing the
missing mass to 0.2%. Nailing that last bit and a similar quantity of papers
with uniformly low confidence reviews is what remains to be done in terms of
basic reviews. We are trying to make all of those happen this week so authors
have some chance to respond.I was surprised by the quantity of late reviews,
and I think that's an area where ICML needs to improve in future years. Good
reviews are not done in a rush--they are done by setting aside time (like an
afternoon), and carefully reading the paper while thinking about implications.
Many reviewers do this well but a significant minority aren't good at
scheduling their personal time. In this situation there are several ways to
fail:Give early w</p><p>6 0.21836109 <a title="207-tfidf-6" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>7 0.21404298 <a title="207-tfidf-7" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>8 0.20134602 <a title="207-tfidf-8" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>9 0.20080289 <a title="207-tfidf-9" href="../hunch_net-2008/hunch_net-2008-10-14-Who_is_Responsible_for_a_Bad_Review%3F.html">320 hunch net-2008-10-14-Who is Responsible for a Bad Review?</a></p>
<p>10 0.19736688 <a title="207-tfidf-10" href="../hunch_net-2008/hunch_net-2008-09-03-Bidding_Problems.html">315 hunch net-2008-09-03-Bidding Problems</a></p>
<p>11 0.17828536 <a title="207-tfidf-11" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>12 0.17209068 <a title="207-tfidf-12" href="../hunch_net-2005/hunch_net-2005-03-09-Bad_Reviewing.html">38 hunch net-2005-03-09-Bad Reviewing</a></p>
<p>13 0.16351715 <a title="207-tfidf-13" href="../hunch_net-2012/hunch_net-2012-05-02-ICML%3A_Behind_the_Scenes.html">463 hunch net-2012-05-02-ICML: Behind the Scenes</a></p>
<p>14 0.15992147 <a title="207-tfidf-14" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>15 0.15888649 <a title="207-tfidf-15" href="../hunch_net-2012/hunch_net-2012-06-29-ICML_survey_and_comments.html">468 hunch net-2012-06-29-ICML survey and comments</a></p>
<p>16 0.15763676 <a title="207-tfidf-16" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>17 0.1551418 <a title="207-tfidf-17" href="../hunch_net-2005/hunch_net-2005-05-02-Reviewing_techniques_for_conferences.html">65 hunch net-2005-05-02-Reviewing techniques for conferences</a></p>
<p>18 0.15495601 <a title="207-tfidf-18" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>19 0.13650161 <a title="207-tfidf-19" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>20 0.13524936 <a title="207-tfidf-20" href="../hunch_net-2008/hunch_net-2008-02-10-Complexity_Illness.html">288 hunch net-2008-02-10-Complexity Illness</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.284), (1, 0.225), (2, -0.212), (3, -0.105), (4, -0.001), (5, 0.033), (6, -0.042), (7, 0.041), (8, 0.007), (9, 0.054), (10, -0.024), (11, 0.02), (12, 0.013), (13, -0.061), (14, -0.01), (15, -0.051), (16, -0.087), (17, 0.018), (18, -0.031), (19, -0.038), (20, -0.02), (21, 0.03), (22, 0.066), (23, -0.014), (24, -0.03), (25, 0.004), (26, 0.004), (27, 0.028), (28, -0.03), (29, -0.063), (30, 0.012), (31, -0.011), (32, -0.032), (33, -0.001), (34, 0.034), (35, 0.0), (36, 0.021), (37, -0.068), (38, -0.014), (39, 0.049), (40, 0.007), (41, 0.044), (42, 0.017), (43, 0.011), (44, 0.046), (45, 0.035), (46, -0.013), (47, -0.002), (48, 0.082), (49, 0.022)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97931391 <a title="207-lsi-1" href="../hunch_net-2006/hunch_net-2006-09-12-Incentive_Compatible_Reviewing.html">207 hunch net-2006-09-12-Incentive Compatible Reviewing</a></p>
<p>Introduction: Reviewing is a fairly formal process which is integral to the way academia is
run. Given this integral nature, the quality of reviewing is often
frustrating. I've seen plenty of examples of false statements, misbeliefs,
reading what isn't written, etcâ&euro;Ś, and I'm sure many other people have as
well.Recently, mechanisms like double blind review and author feedback have
been introduced to try to make the process more fair and accurate in many
machine learning (and related) conferences. My personal experience is that
these mechanisms help, especially the author feedback. Nevertheless, some
problems remain.The game theory take on reviewing is that the incentive for
truthful reviewing isn't there. Since reviewers are also authors, there are
sometimes perverse incentives created and acted upon. (Incidentially, these
incentives can be both positive and negative.)Setting up a truthful reviewing
system is tricky because their is no final reference truth available in any
acceptable (say: subyear)</p><p>2 0.88511699 <a title="207-lsi-2" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>Introduction: When thinking about how best to review papers, it seems helpful to have some
conception of what good reviewing is. As far as I can tell, this is almost
always only discussed in the specific context of a paper (i.e. your rejected
paper), or at most an area (i.e. what a "good paper" looks like for that area)
rather than general principles. Neither individual papers or areas are
sufficiently general for a large conference--every paper differs in the
details, and what if you want to build a new area and/or cross areas?An
unavoidable reason for reviewing is that the community of research is too
large. In particular, it is not possible for a researcher to read every paper
which someone thinks might be of interest. This reason for reviewing exists
independent of constraints on rooms or scheduling formats of individual
conferences. Indeed, history suggests that physical constraints are relatively
meaningless over the long term -- growing conferences simply use more rooms
and/or change formats</p><p>3 0.88340038 <a title="207-lsi-3" href="../hunch_net-2012/hunch_net-2012-04-09-ICML_author_feedback_is_open.html">461 hunch net-2012-04-09-ICML author feedback is open</a></p>
<p>Introduction: as of last night, late.When the reviewing deadline passed Wednesday night 15%
of reviews were still missing, much higher than I expected. Between late
reviews coming in, ACs working overtime through the weekend, and people
willing to help in the pinch another ~390 reviews came in, reducing the
missing mass to 0.2%. Nailing that last bit and a similar quantity of papers
with uniformly low confidence reviews is what remains to be done in terms of
basic reviews. We are trying to make all of those happen this week so authors
have some chance to respond.I was surprised by the quantity of late reviews,
and I think that's an area where ICML needs to improve in future years. Good
reviews are not done in a rush--they are done by setting aside time (like an
afternoon), and carefully reading the paper while thinking about implications.
Many reviewers do this well but a significant minority aren't good at
scheduling their personal time. In this situation there are several ways to
fail:Give early w</p><p>4 0.86486369 <a title="207-lsi-4" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>Introduction: Conferences exist as part of the process of doing research. They provide many
roles including "announcing research", "meeting people", and "point of
reference". Not all conferences are alike so a basic question is: "to what
extent do individual conferences attempt to aid research?" This question is
very difficult to answer in any satisfying way. What we can do is compare
details of the process across multiple conferences.CommentsThe average quality
of comments across conferences can vary dramatically. At one extreme, the
tradition in CS theory conferences is to provide essentially zero feedback. At
the other extreme, some conferences have a strong tradition of providing
detailed constructive feedback. Detailed feedback can give authors significant
guidance about how to improve research. This is the most subjective
entry.BlindVirtually all conferences offer single blind review where authors
do not know reviewers. Some also providedouble blindreview where reviewers do
not know authors. T</p><p>5 0.85406655 <a title="207-lsi-5" href="../hunch_net-2008/hunch_net-2008-09-03-Bidding_Problems.html">315 hunch net-2008-09-03-Bidding Problems</a></p>
<p>Introduction: One way that many conferences in machine learning assign reviewers to papers
is via bidding, which has steps something like:Invite people to reviewAccept
papersReviewers look at title and abstract and state the papers they are
interested in reviewing.Some massaging happens, but reviewers often get
approximately the papers they bid for.At the ICML business meeting,Andrew
McCallumsuggested getting rid of bidding for papers. A couple reasons were
given:PrivacyThe title and abstract of the entire set of papers is visible to
every participating reviewer. Some authors might be uncomfortable about this
for submitted papers. I'm not sympathetic to this reason: the point of
submitting a paper to review is to publish it, so the value (if any) of not
publishing a part of it a little bit earlier seems limited.CliquesA bidding
system is gameable. If you have 3 buddies and you inform each other of your
submissions, you can each bid for your friend's papers and express a
disinterest in others. There</p><p>6 0.85380936 <a title="207-lsi-6" href="../hunch_net-2005/hunch_net-2005-03-13-Avoiding_Bad_Reviewing.html">40 hunch net-2005-03-13-Avoiding Bad Reviewing</a></p>
<p>7 0.81820267 <a title="207-lsi-7" href="../hunch_net-2010/hunch_net-2010-04-26-Compassionate_Reviewing.html">395 hunch net-2010-04-26-Compassionate Reviewing</a></p>
<p>8 0.80351663 <a title="207-lsi-8" href="../hunch_net-2008/hunch_net-2008-10-14-Who_is_Responsible_for_a_Bad_Review%3F.html">320 hunch net-2008-10-14-Who is Responsible for a Bad Review?</a></p>
<p>9 0.78643835 <a title="207-lsi-9" href="../hunch_net-2012/hunch_net-2012-05-02-ICML%3A_Behind_the_Scenes.html">463 hunch net-2012-05-02-ICML: Behind the Scenes</a></p>
<p>10 0.764759 <a title="207-lsi-10" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>11 0.76176155 <a title="207-lsi-11" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>12 0.75368148 <a title="207-lsi-12" href="../hunch_net-2012/hunch_net-2012-06-29-ICML_survey_and_comments.html">468 hunch net-2012-06-29-ICML survey and comments</a></p>
<p>13 0.7474854 <a title="207-lsi-13" href="../hunch_net-2009/hunch_net-2009-07-09-The_Machine_Learning_Forum.html">363 hunch net-2009-07-09-The Machine Learning Forum</a></p>
<p>14 0.73886693 <a title="207-lsi-14" href="../hunch_net-2013/hunch_net-2013-06-29-The_Benefits_of_Double-Blind_Review.html">485 hunch net-2013-06-29-The Benefits of Double-Blind Review</a></p>
<p>15 0.72913086 <a title="207-lsi-15" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>16 0.72797632 <a title="207-lsi-16" href="../hunch_net-2005/hunch_net-2005-03-09-Bad_Reviewing.html">38 hunch net-2005-03-09-Bad Reviewing</a></p>
<p>17 0.70099974 <a title="207-lsi-17" href="../hunch_net-2006/hunch_net-2006-12-04-Structural_Problems_in_NIPS_Decision_Making.html">221 hunch net-2006-12-04-Structural Problems in NIPS Decision Making</a></p>
<p>18 0.70033306 <a title="207-lsi-18" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>19 0.68110925 <a title="207-lsi-19" href="../hunch_net-2012/hunch_net-2012-06-05-ICML_acceptance_statistics.html">466 hunch net-2012-06-05-ICML acceptance statistics</a></p>
<p>20 0.66538161 <a title="207-lsi-20" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(6, 0.033), (35, 0.035), (39, 0.177), (42, 0.239), (68, 0.047), (69, 0.064), (74, 0.207), (76, 0.013), (82, 0.077), (95, 0.026)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94396311 <a title="207-lda-1" href="../hunch_net-2006/hunch_net-2006-09-12-Incentive_Compatible_Reviewing.html">207 hunch net-2006-09-12-Incentive Compatible Reviewing</a></p>
<p>Introduction: Reviewing is a fairly formal process which is integral to the way academia is
run. Given this integral nature, the quality of reviewing is often
frustrating. I've seen plenty of examples of false statements, misbeliefs,
reading what isn't written, etcâ&euro;Ś, and I'm sure many other people have as
well.Recently, mechanisms like double blind review and author feedback have
been introduced to try to make the process more fair and accurate in many
machine learning (and related) conferences. My personal experience is that
these mechanisms help, especially the author feedback. Nevertheless, some
problems remain.The game theory take on reviewing is that the incentive for
truthful reviewing isn't there. Since reviewers are also authors, there are
sometimes perverse incentives created and acted upon. (Incidentially, these
incentives can be both positive and negative.)Setting up a truthful reviewing
system is tricky because their is no final reference truth available in any
acceptable (say: subyear)</p><p>2 0.92440885 <a title="207-lda-2" href="../hunch_net-2006/hunch_net-2006-01-25-1_year.html">151 hunch net-2006-01-25-1 year</a></p>
<p>Introduction: At the one year (+5 days) anniversary, the natural question is: "Was it
helpful for research?"Answer: Yes, and so it shall continue.Some evidence is
provided by noticing that I am about a factor of 2 more overloaded with paper
ideas than I've ever previously been. It is always hard to estimate
counterfactual worlds, but I expect that this is also a factor of 2 more than
"What if I had not started the blog?"As for "Why?", there seem to be two
primary effects.A blog is a mechanism for connecting with people who either
think like you or are interested in the same problems. This allows for
concentration of thinking which is very helpful in solving problems.The
process of stating things you don't understand publicly is very helpful in
understanding them. Sometimes you are simply forced to express them in a way
which aids understanding. Sometimes someone else says something which helps.
And sometimes you discover that someone else has already solved the
problem.There are drawbacks which shou</p><p>3 0.90519488 <a title="207-lda-3" href="../hunch_net-2005/hunch_net-2005-12-11-More_NIPS_Papers.html">139 hunch net-2005-12-11-More NIPS Papers</a></p>
<p>Introduction: Let me add to John's post with a few of my own favouritesfrom this year's
conference. First, let me say thatSanjoy's talk,Coarse Sample Complexity
Bounds for ActiveLearningwas also one of my favourites, as was theForgettron
paper.I also really enjoyed the last third ofChristos'talkon the complexity of
finding Nash equilibria.And, speaking of tagging, I thinkthe U.Mass Citeseer
replacement systemRexafrom the demo track is very cool.Finally, let me add my
recommendations for specific papers:Z. Ghahramani, K. Heller:Bayesian Sets[no
preprint](A very elegant probabilistic information retrieval style modelof
which objects are "most like" a given subset of objects.)T. Griffiths, Z.
Ghahramani:Infinite Latent Feature Models andthe Indian Buffet
Process[preprint](A Dirichlet style prior over infinite binary matrices
withbeautiful exchangeability properties.)K. Weinberger, J. Blitzer, L.
Saul:Distance Metric Learning forLarge Margin Nearest Neighbor
Classification[preprint](A nice idea about ho</p><p>4 0.90383834 <a title="207-lda-4" href="../hunch_net-2007/hunch_net-2007-07-13-The_View_From_China.html">255 hunch net-2007-07-13-The View From China</a></p>
<p>Introduction: I'm visiting Beijing for thePao-Lu Hsu Statistics Conferenceon Machine
Learning.I had several discussions about the state of Chinese research. Given
the large population and economy, you might expect substantial research--more
than has been observed at international conferences. The fundamental problem
seems to be theCultural Revolutionwhich lobotimized higher education, and the
research associated with it. There has been a process of slow recovery since
then, which has begun to be felt in the research world via increased
participation in international conferences and (now) conferences in China.The
amount of effort going into construction in Beijing is very impressive--people
are literally building a skyscraper at night outside the window of the hotel
I'm staying at (and this is not unusual). If a small fraction of this effort
is later focused onto supporting research, the effect could be very
substantial. General growth in China's research portfolio should be expected.</p><p>5 0.88657528 <a title="207-lda-5" href="../hunch_net-2006/hunch_net-2006-06-30-ICML_papers.html">188 hunch net-2006-06-30-ICML papers</a></p>
<p>Introduction: Here are some ICML papers which interested me.Arindam Banerjeehad apaperwhich
notes that PAC-Bayes bounds, a core theorem in online learning, and the
optimality of Bayesian learning statements share a core inequality in their
proof.Pieter Abbeel,Morgan QuigleyandAndrew Y. Nghave apaperdiscussing RL
techniques for learning given a bad (but not too bad) model of the world.Nina
BalcanandAvrim Blumhave apaperwhich discusses how to learn given a similarity
function rather than a kernel. A similarity function requires less structure
than a kernel, implying that a learning algorithm using a similarity function
might be applied in situations where no effective kernel is evident.Nathan
Ratliff,Drew Bagnell, andMarty Zinkevichhave apaperdescribing an algorithm
which attempts to fuse A*path planning with learning of transition costs based
on human demonstration.Papers (2), (3), and (4), all seem like an initial pass
at solving interesting problems which push the domain in which learning is
applic</p><p>6 0.86716723 <a title="207-lda-6" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>7 0.86171758 <a title="207-lda-7" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>8 0.86037076 <a title="207-lda-8" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>9 0.8598389 <a title="207-lda-9" href="../hunch_net-2006/hunch_net-2006-04-17-Rexa_is_live.html">173 hunch net-2006-04-17-Rexa is live</a></p>
<p>10 0.85914284 <a title="207-lda-10" href="../hunch_net-2010/hunch_net-2010-04-26-Compassionate_Reviewing.html">395 hunch net-2010-04-26-Compassionate Reviewing</a></p>
<p>11 0.85849577 <a title="207-lda-11" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>12 0.85571992 <a title="207-lda-12" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>13 0.85172218 <a title="207-lda-13" href="../hunch_net-2010/hunch_net-2010-04-24-COLT_Treasurer_is_now_Phil_Long.html">394 hunch net-2010-04-24-COLT Treasurer is now Phil Long</a></p>
<p>14 0.84900463 <a title="207-lda-14" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>15 0.84710425 <a title="207-lda-15" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>16 0.84621894 <a title="207-lda-16" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>17 0.84602582 <a title="207-lda-17" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>18 0.84596866 <a title="207-lda-18" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>19 0.84430116 <a title="207-lda-19" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>20 0.84162492 <a title="207-lda-20" href="../hunch_net-2006/hunch_net-2006-12-04-Structural_Problems_in_NIPS_Decision_Making.html">221 hunch net-2006-12-04-Structural Problems in NIPS Decision Making</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
