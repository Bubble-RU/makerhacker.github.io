<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>224 hunch net-2006-12-12-Interesting Papers at NIPS 2006</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-224" href="#">hunch_net-2006-224</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>224 hunch net-2006-12-12-Interesting Papers at NIPS 2006</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-224-html" href="http://hunch.net/?p=244">html</a></p><p>Introduction: Here are some papers that I found surprisingly interesting.
  
  Yoshua Bengio , Pascal Lamblin, Dan Popovici, Hugo Larochelle,  Greedy Layer-wise Training of Deep Networks . Empirically investigates some of the design choices behind deep belief networks.
 
  Long Zhu , Yuanhao Chen,  Alan Yuille  Unsupervised Learning of a Probabilistic Grammar for Object Detection and Parsing.  An unsupervised method for detecting objects using simple feature filters that works remarkably well on the (supervised)  caltech-101 dataset . 
  Shai Ben-David ,  John Blitzer ,  Koby Crammer , and  Fernando Pereira ,  Analysis of Representations for Domain Adaptation .  This is the first analysis I’ve seen of learning with respect to samples drawn differently from the evaluation distribution which depends on reasonable measurable quantities. 
  
All of these papers turn out to have a common theme—the power of unlabeled data to do generically useful things.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Here are some papers that I found surprisingly interesting. [sent-1, score-0.215]
</p><p>2 Empirically investigates some of the design choices behind deep belief networks. [sent-3, score-0.662]
</p><p>3 An unsupervised method for detecting objects using simple feature filters that works remarkably well on the (supervised)  caltech-101 dataset . [sent-5, score-0.897]
</p><p>4 This is the first analysis I’ve seen of learning with respect to samples drawn differently from the evaluation distribution which depends on reasonable measurable quantities. [sent-7, score-0.966]
</p><p>5 All of these papers turn out to have a common theme—the power of unlabeled data to do generically useful things. [sent-8, score-0.551]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('unsupervised', 0.242), ('differently', 0.175), ('blitzer', 0.175), ('investigates', 0.175), ('adaptation', 0.175), ('chen', 0.175), ('crammer', 0.175), ('detecting', 0.175), ('koby', 0.175), ('theme', 0.175), ('pereira', 0.162), ('measurable', 0.162), ('detection', 0.162), ('deep', 0.16), ('bengio', 0.153), ('yoshua', 0.153), ('alan', 0.153), ('generically', 0.153), ('fernando', 0.146), ('pascal', 0.14), ('dan', 0.14), ('analysis', 0.136), ('remarkably', 0.135), ('filters', 0.135), ('behind', 0.135), ('greedy', 0.131), ('depends', 0.131), ('shai', 0.131), ('surprisingly', 0.124), ('objects', 0.121), ('domain', 0.121), ('object', 0.111), ('representations', 0.111), ('evaluation', 0.107), ('unlabeled', 0.107), ('turn', 0.102), ('choices', 0.1), ('power', 0.098), ('empirically', 0.096), ('probabilistic', 0.092), ('belief', 0.092), ('drawn', 0.092), ('john', 0.092), ('papers', 0.091), ('dataset', 0.089), ('networks', 0.088), ('supervised', 0.088), ('samples', 0.087), ('training', 0.079), ('seen', 0.076)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="224-tfidf-1" href="../hunch_net-2006/hunch_net-2006-12-12-Interesting_Papers_at_NIPS_2006.html">224 hunch net-2006-12-12-Interesting Papers at NIPS 2006</a></p>
<p>Introduction: Here are some papers that I found surprisingly interesting.
  
  Yoshua Bengio , Pascal Lamblin, Dan Popovici, Hugo Larochelle,  Greedy Layer-wise Training of Deep Networks . Empirically investigates some of the design choices behind deep belief networks.
 
  Long Zhu , Yuanhao Chen,  Alan Yuille  Unsupervised Learning of a Probabilistic Grammar for Object Detection and Parsing.  An unsupervised method for detecting objects using simple feature filters that works remarkably well on the (supervised)  caltech-101 dataset . 
  Shai Ben-David ,  John Blitzer ,  Koby Crammer , and  Fernando Pereira ,  Analysis of Representations for Domain Adaptation .  This is the first analysis I’ve seen of learning with respect to samples drawn differently from the evaluation distribution which depends on reasonable measurable quantities. 
  
All of these papers turn out to have a common theme—the power of unlabeled data to do generically useful things.</p><p>2 0.158685 <a title="224-tfidf-2" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>Introduction: Many learning algorithms used in practice are fairly simple.  Viewed representationally, many prediction algorithms either compute a linear separator of basic features (perceptron, winnow, weighted majority, SVM) or perhaps a linear separator of slightly more complex features (2-layer neural networks or kernelized SVMs).  Should we go beyond this, and start using “deep” representations?
 
 What is deep learning?  
Intuitively, deep learning is about learning to predict in ways which can involve complex dependencies between the input (observed) features.
 
Specifying this more rigorously turns out to be rather difficult.  Consider the following cases:
  
 SVM with Gaussian Kernel.  This is not considered deep learning, because an SVM with a gaussian kernel can’t succinctly represent certain decision surfaces.  One of  Yann LeCun ‘s examples is recognizing objects based on pixel values.  An SVM will need a new support vector for each significantly different background.  Since the number</p><p>3 0.14028822 <a title="224-tfidf-3" href="../hunch_net-2011/hunch_net-2011-07-11-Interesting_Neural_Network_Papers_at_ICML_2011.html">438 hunch net-2011-07-11-Interesting Neural Network Papers at ICML 2011</a></p>
<p>Introduction: Maybe it’s too early to call, but with four separate Neural Network sessions at this year’s  ICML ,  it looks like Neural Networks are making a comeback. Here are my  highlights of these sessions. In general, my feeling is that these  papers both demystify deep learning and show its broader applicability.
 
The first observation I made is that the once disreputable “Neural” nomenclature is being used again  in lieu of  “deep learning”. Maybe it’s because Adam Coates et al. showed that single layer networks can work surprisingly well.
  
  An Analysis of Single-Layer Networks in Unsupervised Feature       Learning ,  Adam Coates ,  Honglak Lee ,  Andrew Y. Ng  (AISTATS 2011) 
  The Importance of Encoding Versus Training with Sparse Coding and Vector Quantization ,  Adam Coates ,  Andrew Y. Ng  (ICML 2011) 
  
Another surprising result out of Andrew Ng’s group comes from Andrew  Saxe et al. who show that certain convolutional pooling architectures  can obtain close to state-of-the-art pe</p><p>4 0.11035947 <a title="224-tfidf-4" href="../hunch_net-2010/hunch_net-2010-08-23-Boosted_Decision_Trees_for_Deep_Learning.html">407 hunch net-2010-08-23-Boosted Decision Trees for Deep Learning</a></p>
<p>Introduction: About 4 years ago, I speculated that  decision trees qualify as a deep learning algorithm  because they can make decisions which are substantially nonlinear in the input representation.   Ping Li  has  proved this correct, empirically  at  UAI  by showing that boosted decision trees can beat deep belief networks on versions of  Mnist  which are artificially hardened so as to make them solvable only by deep learning algorithms.  
 
This is an important point, because the ability to solve these sorts of problems is probably the best objective definition of a deep learning algorithm we have.   Iâ&euro;&trade;m not that surprised.  In my experience, if you can accept the computational drawbacks of a boosted decision tree, they can achieve pretty good performance.
 
 Geoff Hinton  once told me that the great thing about deep belief networks is that they work.  I understand that Ping had very substantial difficulty in getting this published, so I hope some reviewers step up to the standard of valuing wha</p><p>5 0.1064233 <a title="224-tfidf-5" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>Introduction: One of the common trends in machine learning has been an emphasis on the use of unlabeled data.  The argument goes something like “there aren’t many labeled web pages out there, but there are a  huge  number of web pages, so we must find a way to take advantage of them.”  There are several standard approaches for doing this:
  
  Unsupervised Learning .  You use only unlabeled data.  In a typical application, you cluster the data and hope that the clusters somehow correspond to what you care about. 
 Semisupervised Learning.  You use both unlabeled and labeled data to build a predictor.  The unlabeled data influences the learned predictor in some way. 
  Active Learning . You have unlabeled data and access to a labeling oracle.  You interactively choose which examples to label so as to optimize prediction accuracy. 
  
It seems there is a fourth approach worth serious investigation—automated labeling.  The approach goes as follows:
  
 Identify some subset of observed values to predict</p><p>6 0.096173212 <a title="224-tfidf-6" href="../hunch_net-2009/hunch_net-2009-06-24-Interesting_papers_at_UAICMOLT_2009.html">361 hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</a></p>
<p>7 0.092699975 <a title="224-tfidf-7" href="../hunch_net-2007/hunch_net-2007-01-10-A_Deep_Belief_Net_Learning_Problem.html">227 hunch net-2007-01-10-A Deep Belief Net Learning Problem</a></p>
<p>8 0.08761254 <a title="224-tfidf-8" href="../hunch_net-2006/hunch_net-2006-03-05-%26%238220%3BStructural%26%238221%3B_Learning.html">161 hunch net-2006-03-05-&#8220;Structural&#8221; Learning</a></p>
<p>9 0.084330983 <a title="224-tfidf-9" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>10 0.080325514 <a title="224-tfidf-10" href="../hunch_net-2012/hunch_net-2012-02-24-ICML%2B50%25.html">456 hunch net-2012-02-24-ICML+50%</a></p>
<p>11 0.078055263 <a title="224-tfidf-11" href="../hunch_net-2005/hunch_net-2005-03-22-Active_learning.html">45 hunch net-2005-03-22-Active learning</a></p>
<p>12 0.074160092 <a title="224-tfidf-12" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<p>13 0.071845949 <a title="224-tfidf-13" href="../hunch_net-2007/hunch_net-2007-12-20-Cool_and_Interesting_things_at_NIPS%2C_take_three.html">280 hunch net-2007-12-20-Cool and Interesting things at NIPS, take three</a></p>
<p>14 0.069688663 <a title="224-tfidf-14" href="../hunch_net-2009/hunch_net-2009-06-03-Functionally_defined_Nonlinear_Dynamic_Models.html">359 hunch net-2009-06-03-Functionally defined Nonlinear Dynamic Models</a></p>
<p>15 0.068595856 <a title="224-tfidf-15" href="../hunch_net-2008/hunch_net-2008-04-27-Watchword%3A_Supervised_Learning.html">299 hunch net-2008-04-27-Watchword: Supervised Learning</a></p>
<p>16 0.068094581 <a title="224-tfidf-16" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>17 0.066928372 <a title="224-tfidf-17" href="../hunch_net-2011/hunch_net-2011-04-18-A_paper_not_at_Snowbird.html">431 hunch net-2011-04-18-A paper not at Snowbird</a></p>
<p>18 0.064099386 <a title="224-tfidf-18" href="../hunch_net-2005/hunch_net-2005-02-04-JMLG.html">13 hunch net-2005-02-04-JMLG</a></p>
<p>19 0.063692354 <a title="224-tfidf-19" href="../hunch_net-2006/hunch_net-2006-03-23-The_Approximation_Argument.html">165 hunch net-2006-03-23-The Approximation Argument</a></p>
<p>20 0.061541468 <a title="224-tfidf-20" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.126), (1, 0.043), (2, 0.01), (3, -0.023), (4, 0.13), (5, 0.002), (6, -0.029), (7, 0.006), (8, 0.058), (9, -0.084), (10, -0.012), (11, -0.01), (12, -0.131), (13, -0.079), (14, -0.053), (15, 0.109), (16, -0.081), (17, 0.099), (18, -0.074), (19, 0.026), (20, 0.058), (21, 0.015), (22, 0.033), (23, -0.01), (24, -0.006), (25, -0.036), (26, -0.015), (27, 0.037), (28, 0.009), (29, -0.027), (30, 0.058), (31, 0.059), (32, 0.057), (33, 0.022), (34, 0.005), (35, -0.043), (36, 0.05), (37, 0.012), (38, -0.104), (39, -0.035), (40, 0.036), (41, 0.063), (42, -0.005), (43, 0.023), (44, -0.025), (45, 0.045), (46, 0.008), (47, -0.096), (48, 0.021), (49, -0.037)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97680587 <a title="224-lsi-1" href="../hunch_net-2006/hunch_net-2006-12-12-Interesting_Papers_at_NIPS_2006.html">224 hunch net-2006-12-12-Interesting Papers at NIPS 2006</a></p>
<p>Introduction: Here are some papers that I found surprisingly interesting.
  
  Yoshua Bengio , Pascal Lamblin, Dan Popovici, Hugo Larochelle,  Greedy Layer-wise Training of Deep Networks . Empirically investigates some of the design choices behind deep belief networks.
 
  Long Zhu , Yuanhao Chen,  Alan Yuille  Unsupervised Learning of a Probabilistic Grammar for Object Detection and Parsing.  An unsupervised method for detecting objects using simple feature filters that works remarkably well on the (supervised)  caltech-101 dataset . 
  Shai Ben-David ,  John Blitzer ,  Koby Crammer , and  Fernando Pereira ,  Analysis of Representations for Domain Adaptation .  This is the first analysis I’ve seen of learning with respect to samples drawn differently from the evaluation distribution which depends on reasonable measurable quantities. 
  
All of these papers turn out to have a common theme—the power of unlabeled data to do generically useful things.</p><p>2 0.70917523 <a title="224-lsi-2" href="../hunch_net-2011/hunch_net-2011-07-11-Interesting_Neural_Network_Papers_at_ICML_2011.html">438 hunch net-2011-07-11-Interesting Neural Network Papers at ICML 2011</a></p>
<p>Introduction: Maybe it’s too early to call, but with four separate Neural Network sessions at this year’s  ICML ,  it looks like Neural Networks are making a comeback. Here are my  highlights of these sessions. In general, my feeling is that these  papers both demystify deep learning and show its broader applicability.
 
The first observation I made is that the once disreputable “Neural” nomenclature is being used again  in lieu of  “deep learning”. Maybe it’s because Adam Coates et al. showed that single layer networks can work surprisingly well.
  
  An Analysis of Single-Layer Networks in Unsupervised Feature       Learning ,  Adam Coates ,  Honglak Lee ,  Andrew Y. Ng  (AISTATS 2011) 
  The Importance of Encoding Versus Training with Sparse Coding and Vector Quantization ,  Adam Coates ,  Andrew Y. Ng  (ICML 2011) 
  
Another surprising result out of Andrew Ng’s group comes from Andrew  Saxe et al. who show that certain convolutional pooling architectures  can obtain close to state-of-the-art pe</p><p>3 0.70171678 <a title="224-lsi-3" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>Introduction: Many learning algorithms used in practice are fairly simple.  Viewed representationally, many prediction algorithms either compute a linear separator of basic features (perceptron, winnow, weighted majority, SVM) or perhaps a linear separator of slightly more complex features (2-layer neural networks or kernelized SVMs).  Should we go beyond this, and start using “deep” representations?
 
 What is deep learning?  
Intuitively, deep learning is about learning to predict in ways which can involve complex dependencies between the input (observed) features.
 
Specifying this more rigorously turns out to be rather difficult.  Consider the following cases:
  
 SVM with Gaussian Kernel.  This is not considered deep learning, because an SVM with a gaussian kernel can’t succinctly represent certain decision surfaces.  One of  Yann LeCun ‘s examples is recognizing objects based on pixel values.  An SVM will need a new support vector for each significantly different background.  Since the number</p><p>4 0.63098246 <a title="224-lsi-4" href="../hunch_net-2013/hunch_net-2013-01-01-Deep_Learning_2012.html">477 hunch net-2013-01-01-Deep Learning 2012</a></p>
<p>Introduction: 2012 was a tumultuous year for me, but it was undeniably a great year for deep learning efforts.  Signs of this include:
  
 Winning a  Kaggle competition . 
 Wide adoption of  deep learning for speech recognition . 
 Significant  industry support . 
 Gains in  image   recognition . 
  
This is a rare event in research: a significant capability breakout.  Congratulations are definitely in order for those who managed to achieve it.  At this point, deep learning algorithms seem like a choice undeniably worth investigating for real applications with significant data.</p><p>5 0.62379622 <a title="224-lsi-5" href="../hunch_net-2010/hunch_net-2010-08-23-Boosted_Decision_Trees_for_Deep_Learning.html">407 hunch net-2010-08-23-Boosted Decision Trees for Deep Learning</a></p>
<p>Introduction: About 4 years ago, I speculated that  decision trees qualify as a deep learning algorithm  because they can make decisions which are substantially nonlinear in the input representation.   Ping Li  has  proved this correct, empirically  at  UAI  by showing that boosted decision trees can beat deep belief networks on versions of  Mnist  which are artificially hardened so as to make them solvable only by deep learning algorithms.  
 
This is an important point, because the ability to solve these sorts of problems is probably the best objective definition of a deep learning algorithm we have.   Iâ&euro;&trade;m not that surprised.  In my experience, if you can accept the computational drawbacks of a boosted decision tree, they can achieve pretty good performance.
 
 Geoff Hinton  once told me that the great thing about deep belief networks is that they work.  I understand that Ping had very substantial difficulty in getting this published, so I hope some reviewers step up to the standard of valuing wha</p><p>6 0.56117779 <a title="224-lsi-6" href="../hunch_net-2005/hunch_net-2005-02-09-Intuitions_from_applied_learning.html">16 hunch net-2005-02-09-Intuitions from applied learning</a></p>
<p>7 0.55377167 <a title="224-lsi-7" href="../hunch_net-2007/hunch_net-2007-01-10-A_Deep_Belief_Net_Learning_Problem.html">227 hunch net-2007-01-10-A Deep Belief Net Learning Problem</a></p>
<p>8 0.54184264 <a title="224-lsi-8" href="../hunch_net-2011/hunch_net-2011-04-18-A_paper_not_at_Snowbird.html">431 hunch net-2011-04-18-A paper not at Snowbird</a></p>
<p>9 0.54149932 <a title="224-lsi-9" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>10 0.51570487 <a title="224-lsi-10" href="../hunch_net-2006/hunch_net-2006-03-05-%26%238220%3BStructural%26%238221%3B_Learning.html">161 hunch net-2006-03-05-&#8220;Structural&#8221; Learning</a></p>
<p>11 0.50991052 <a title="224-lsi-11" href="../hunch_net-2005/hunch_net-2005-03-22-Active_learning.html">45 hunch net-2005-03-22-Active learning</a></p>
<p>12 0.48849407 <a title="224-lsi-12" href="../hunch_net-2012/hunch_net-2012-02-24-ICML%2B50%25.html">456 hunch net-2012-02-24-ICML+50%</a></p>
<p>13 0.456081 <a title="224-lsi-13" href="../hunch_net-2009/hunch_net-2009-06-24-Interesting_papers_at_UAICMOLT_2009.html">361 hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</a></p>
<p>14 0.45274413 <a title="224-lsi-14" href="../hunch_net-2008/hunch_net-2008-11-28-A_Bumper_Crop_of_Machine_Learning_Graduates.html">329 hunch net-2008-11-28-A Bumper Crop of Machine Learning Graduates</a></p>
<p>15 0.449781 <a title="224-lsi-15" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>16 0.44123295 <a title="224-lsi-16" href="../hunch_net-2005/hunch_net-2005-12-11-More_NIPS_Papers.html">139 hunch net-2005-12-11-More NIPS Papers</a></p>
<p>17 0.4404749 <a title="224-lsi-17" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<p>18 0.42792928 <a title="224-lsi-18" href="../hunch_net-2012/hunch_net-2012-06-05-ICML_acceptance_statistics.html">466 hunch net-2012-06-05-ICML acceptance statistics</a></p>
<p>19 0.4235962 <a title="224-lsi-19" href="../hunch_net-2006/hunch_net-2006-01-30-Should_the_Input_Representation_be_a_Vector%3F.html">152 hunch net-2006-01-30-Should the Input Representation be a Vector?</a></p>
<p>20 0.41963586 <a title="224-lsi-20" href="../hunch_net-2007/hunch_net-2007-12-12-Workshop_Summary%26%238212%3BPrinciples_of_Learning_Problem_Design.html">277 hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(27, 0.175), (49, 0.594), (53, 0.065), (55, 0.033), (95, 0.024)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.99230707 <a title="224-lda-1" href="../hunch_net-2009/hunch_net-2009-01-23-An_Active_Learning_Survey.html">338 hunch net-2009-01-23-An Active Learning Survey</a></p>
<p>Introduction: Burr Settles  wrote a fairly comprehensive  survey of active learning .  He intends to maintain and update the survey, so send him any suggestions you have.</p><p>same-blog 2 0.95007014 <a title="224-lda-2" href="../hunch_net-2006/hunch_net-2006-12-12-Interesting_Papers_at_NIPS_2006.html">224 hunch net-2006-12-12-Interesting Papers at NIPS 2006</a></p>
<p>Introduction: Here are some papers that I found surprisingly interesting.
  
  Yoshua Bengio , Pascal Lamblin, Dan Popovici, Hugo Larochelle,  Greedy Layer-wise Training of Deep Networks . Empirically investigates some of the design choices behind deep belief networks.
 
  Long Zhu , Yuanhao Chen,  Alan Yuille  Unsupervised Learning of a Probabilistic Grammar for Object Detection and Parsing.  An unsupervised method for detecting objects using simple feature filters that works remarkably well on the (supervised)  caltech-101 dataset . 
  Shai Ben-David ,  John Blitzer ,  Koby Crammer , and  Fernando Pereira ,  Analysis of Representations for Domain Adaptation .  This is the first analysis I’ve seen of learning with respect to samples drawn differently from the evaluation distribution which depends on reasonable measurable quantities. 
  
All of these papers turn out to have a common theme—the power of unlabeled data to do generically useful things.</p><p>3 0.91067743 <a title="224-lda-3" href="../hunch_net-2005/hunch_net-2005-10-13-Site_tweak.html">122 hunch net-2005-10-13-Site tweak</a></p>
<p>Introduction: Several people have had difficulty with comments which seem to have an allowed language significantly poorer than posts.  The set of allowed html tags has been increased and the  markdown filter  has been put in place to try to make commenting easier.  Iâ&euro;&trade;ll put some examples into the comments of this post.</p><p>4 0.66119635 <a title="224-lda-4" href="../hunch_net-2009/hunch_net-2009-07-31-Vowpal_Wabbit_Open_Source_Project.html">365 hunch net-2009-07-31-Vowpal Wabbit Open Source Project</a></p>
<p>Introduction: Today brings a new release of the  Vowpal Wabbit  fast online learning software.  This time, unlike the previous release, the project itself is going open source, developing via  github .  For example, the lastest and greatest can be downloaded via:
  
git clone git://github.com/JohnLangford/vowpal_wabbit.git
  
If you aren’t familiar with  git , it’s a distributed version control system which supports quick and easy branching, as well as reconciliation.
 
This version of the code is confirmed to compile without complaint on at least some flavors of OSX as well as Linux boxes.
 
As much of the point of this project is pushing the limits of fast and effective machine learning, let me mention a few datapoints from my experience.
  
 The program can effectively scale up to batch-style training on sparse terafeature (i.e. 10 12  sparse feature) size datasets.  The limiting factor is typically i/o. 
 I started using the the real datasets from the  large-scale learning  workshop as a conve</p><p>5 0.64124745 <a title="224-lda-5" href="../hunch_net-2005/hunch_net-2005-03-08-Fast_Physics_for_Learning.html">37 hunch net-2005-03-08-Fast Physics for Learning</a></p>
<p>Introduction: While everyone is silently working on ICML submissions, I found this discussion about a  fast physics simulator  chip interesting from a learning viewpoint.  In many cases, learning attempts to predict the outcome of physical processes.  Access to a fast simulator for these processes might be quite helpful in predicting the outcome.  Bayesian learning in particular may directly benefit while many other algorithms (like support vector machines) might have their speed greatly increased.
 
The biggest drawback is that writing software for these odd architectures is always difficult and time consuming, but a several-orders-of-magnitude speedup might make that worthwhile.</p><p>6 0.61048108 <a title="224-lda-6" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">23 hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<p>7 0.58539182 <a title="224-lda-7" href="../hunch_net-2009/hunch_net-2009-04-02-Asymmophobia.html">348 hunch net-2009-04-02-Asymmophobia</a></p>
<p>8 0.42608926 <a title="224-lda-8" href="../hunch_net-2009/hunch_net-2009-06-03-Functionally_defined_Nonlinear_Dynamic_Models.html">359 hunch net-2009-06-03-Functionally defined Nonlinear Dynamic Models</a></p>
<p>9 0.37653542 <a title="224-lda-9" href="../hunch_net-2011/hunch_net-2011-07-11-Interesting_Neural_Network_Papers_at_ICML_2011.html">438 hunch net-2011-07-11-Interesting Neural Network Papers at ICML 2011</a></p>
<p>10 0.34128365 <a title="224-lda-10" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>11 0.33946875 <a title="224-lda-11" href="../hunch_net-2007/hunch_net-2007-12-20-Cool_and_Interesting_things_at_NIPS%2C_take_three.html">280 hunch net-2007-12-20-Cool and Interesting things at NIPS, take three</a></p>
<p>12 0.33376661 <a title="224-lda-12" href="../hunch_net-2014/hunch_net-2014-02-16-Metacademy%3A_a_package_manager_for_knowledge.html">493 hunch net-2014-02-16-Metacademy: a package manager for knowledge</a></p>
<p>13 0.32846683 <a title="224-lda-13" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>14 0.32739869 <a title="224-lda-14" href="../hunch_net-2007/hunch_net-2007-01-10-A_Deep_Belief_Net_Learning_Problem.html">227 hunch net-2007-01-10-A Deep Belief Net Learning Problem</a></p>
<p>15 0.31474113 <a title="224-lda-15" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>16 0.31413978 <a title="224-lda-16" href="../hunch_net-2006/hunch_net-2006-07-11-New_Models.html">194 hunch net-2006-07-11-New Models</a></p>
<p>17 0.31232601 <a title="224-lda-17" href="../hunch_net-2008/hunch_net-2008-11-28-A_Bumper_Crop_of_Machine_Learning_Graduates.html">329 hunch net-2008-11-28-A Bumper Crop of Machine Learning Graduates</a></p>
<p>18 0.30815247 <a title="224-lda-18" href="../hunch_net-2005/hunch_net-2005-12-28-Yet_more_nips_thoughts.html">144 hunch net-2005-12-28-Yet more nips thoughts</a></p>
<p>19 0.3075639 <a title="224-lda-19" href="../hunch_net-2005/hunch_net-2005-01-26-Watchword%3A_Probability.html">5 hunch net-2005-01-26-Watchword: Probability</a></p>
<p>20 0.30237007 <a title="224-lda-20" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
