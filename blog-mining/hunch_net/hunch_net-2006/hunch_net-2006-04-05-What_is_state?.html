<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>169 hunch net-2006-04-05-What is state?</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-169" href="#">hunch_net-2006-169</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>169 hunch net-2006-04-05-What is state?</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-169-html" href="http://hunch.net/?p=180">html</a></p><p>Introduction: In reinforcement learning (and sometimes other settings), there is a notion of “state”.   Based upon the state various predictions are made such as “Which action should be taken next?” or “How much cumulative reward do I expect if I take some action from this state?”  Given the importance of state, it is important to examine the meaning.   There are actually several distinct options and it turns out the definition variation is very important in motivating different pieces of work.
  
 Newtonian State.  State is the physical pose of the world.  Under this definition, there are  very  many states, often too many for explicit representation.  This is also the definition typically used in games. 
 Abstracted State.  State is an abstracted physical state of the world.  “Is the door open or closed?” “Are you in room A or not?” The number of states is much smaller here.  A basic issue here is: “How do you compute the state from observations?” 
 Mathematical State.  State is a sufficient stati</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In reinforcement learning (and sometimes other settings), there is a notion of “state”. [sent-1, score-0.1]
</p><p>2 Based upon the state various predictions are made such as “Which action should be taken next? [sent-2, score-0.993]
</p><p>3 ” or “How much cumulative reward do I expect if I take some action from this state? [sent-3, score-0.335]
</p><p>4 ”  Given the importance of state, it is important to examine the meaning. [sent-4, score-0.185]
</p><p>5 There are actually several distinct options and it turns out the definition variation is very important in motivating different pieces of work. [sent-5, score-0.693]
</p><p>6 Under this definition, there are  very  many states, often too many for explicit representation. [sent-8, score-0.057]
</p><p>7 This is also the definition typically used in games. [sent-9, score-0.148]
</p><p>8 State is an abstracted physical state of the world. [sent-11, score-1.068]
</p><p>9 A basic issue here is: “How do you compute the state from observations? [sent-15, score-0.83]
</p><p>10 State is a sufficient statistic of observations for making necessary predictions. [sent-17, score-0.407]
</p><p>11 State is the internal belief/understanding/etc… which changes an agent’s actions in different circumstances. [sent-19, score-0.391]
</p><p>12 ” This is like the mathematical version of state, except that portions of the statistic which can not be learned are irrelevant. [sent-21, score-0.434]
</p><p>13 There are only observations (one of which might be a reward) and actions. [sent-23, score-0.165]
</p><p>14 This is more reasonable than it might sound because state is a derived quantity and the necessity of that derivation is unclear. [sent-24, score-0.994]
</p><p>15 The different questions these notions of state motivate can have large practical consequences on the algorithms used. [sent-26, score-1.037]
</p><p>16 It is not clear yet what the “right” notion of state is—we just don’t know what works well. [sent-27, score-0.824]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('state', 0.724), ('internal', 0.201), ('abstracted', 0.194), ('statistic', 0.194), ('observations', 0.165), ('physical', 0.15), ('definition', 0.148), ('reward', 0.121), ('states', 0.117), ('action', 0.117), ('notion', 0.1), ('mathematical', 0.099), ('newtonian', 0.097), ('cumulative', 0.097), ('door', 0.097), ('portions', 0.09), ('notions', 0.085), ('motivating', 0.085), ('pose', 0.081), ('variation', 0.081), ('consequences', 0.078), ('closed', 0.078), ('motivate', 0.075), ('examine', 0.075), ('options', 0.075), ('derived', 0.075), ('different', 0.075), ('necessity', 0.073), ('agent', 0.069), ('distinct', 0.067), ('quantity', 0.064), ('actions', 0.063), ('moment', 0.061), ('settings', 0.059), ('compute', 0.058), ('sound', 0.058), ('pieces', 0.057), ('explicit', 0.057), ('smaller', 0.057), ('important', 0.056), ('room', 0.056), ('importance', 0.054), ('taken', 0.052), ('changes', 0.052), ('upon', 0.051), ('except', 0.051), ('predictions', 0.049), ('turns', 0.049), ('sufficient', 0.048), ('issue', 0.048)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999982 <a title="169-tfidf-1" href="../hunch_net-2006/hunch_net-2006-04-05-What_is_state%3F.html">169 hunch net-2006-04-05-What is state?</a></p>
<p>Introduction: In reinforcement learning (and sometimes other settings), there is a notion of “state”.   Based upon the state various predictions are made such as “Which action should be taken next?” or “How much cumulative reward do I expect if I take some action from this state?”  Given the importance of state, it is important to examine the meaning.   There are actually several distinct options and it turns out the definition variation is very important in motivating different pieces of work.
  
 Newtonian State.  State is the physical pose of the world.  Under this definition, there are  very  many states, often too many for explicit representation.  This is also the definition typically used in games. 
 Abstracted State.  State is an abstracted physical state of the world.  “Is the door open or closed?” “Are you in room A or not?” The number of states is much smaller here.  A basic issue here is: “How do you compute the state from observations?” 
 Mathematical State.  State is a sufficient stati</p><p>2 0.17329448 <a title="169-tfidf-2" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>Introduction: One thing which is clear on a little reflection is that there exists a single master learning problem capable of encoding essentially all learning problems.  This problem is of course a very general sort of reinforcement learning where the world interacts with an agent as:
  
 The world announces an observation  x . 
 The agent makes a choice  a . 
 The world announces a reward  r . 
  
The goal here is to maximize the sum of the rewards over the time of the agent.  No particular structure relating  x  to  a  or  a  to  r  is implied by this setting so we do not know effective general algorithms for the agent.  It’s very easy to prove lower bounds showing that an agent cannot hope to succeed here—just consider the case where actions are unrelated to rewards.  Nevertheless, there is a real sense in which essentially all forms of life are agents operating in this setting, somehow succeeding.  The gap between these observations drives research—How can we find tractable specializations of</p><p>3 0.17006955 <a title="169-tfidf-3" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>Introduction: Exploration is one of the big unsolved problems in machine learning. This isn’t for lack of trying—there are many models of exploration which have been analyzed in many different ways by many different groups of people. At some point, it is worthwhile to sit back and see what has been done across these many models.
  
   Reinforcement Learning  (1) . Reinforcement learning has traditionally focused on Markov Decision Processes where the next state  s’    is given by a conditional distribution  P(s’|s,a)  given the current state  s  and action  a .  The typical result here is that certain specific algorithms controlling an agent can behave within  e  of optimal for horizon  T  except for  poly(1/e,T,S,A)  “wasted” experiences (with high probability).  This started with  E 3   by  Satinder Singh  and  Michael Kearns .  Sham Kakade’s thesis  has significant discussion. Extensions have typically been of the form “under extra assumptions, we can prove more”, for example  Factored-E 3   and</p><p>4 0.15216465 <a title="169-tfidf-4" href="../hunch_net-2005/hunch_net-2005-08-04-Why_Reinforcement_Learning_is_Important.html">100 hunch net-2005-08-04-Why Reinforcement Learning is Important</a></p>
<p>Introduction: One prescription for solving a problem well is:
  
 State the problem, in the simplest way possible. In particular, this statement should involve no contamination with or anticipation of the solution. 
 Think about solutions to the stated problem. 
  
Stating a problem in a succinct and crisp manner tends to invite a simple elegant solution.  When a problem can not be stated succinctly, we  wonder if the problem is even understood. (And when a problem is not understood, we wonder if a solution can be meaningful.)
 
Reinforcement learning does step (1) well.  It provides a clean simple language to state general AI problems.  In reinforcement learning there is a set of actions  A , a set of observations  O , and a reward  r .  The reinforcement learning problem, in general, is defined by a conditional measure  D( o, r | (o,r,a) * )  which produces an observation  o  and a reward  r  given a history  (o,r,a) *  .  The goal in reinforcement learning is to find a policy  pi:(o,r,a) *  -> a</p><p>5 0.14836198 <a title="169-tfidf-5" href="../hunch_net-2009/hunch_net-2009-06-03-Functionally_defined_Nonlinear_Dynamic_Models.html">359 hunch net-2009-06-03-Functionally defined Nonlinear Dynamic Models</a></p>
<p>Introduction: Suppose we have a set of observations over time  x 1 ,x 2 ,…,x t   and want to predict some future event  y t+1  .  An inevitable problem arises, because learning a predictor  h(x 1 ,…,x t )  of  y t+1   is generically intractable due to the size of the input.  To make this problem tractable, what’s necessary is a method for summarizing the relevant information in past observations for the purpose of prediction in the future.  In other words, state is required.
 
Existing approaches for deriving state have some limitations.
  
  Hidden Markov models  learned with EM suffer from local minima, use tabular learning approaches which provide dubious generalization ability, and often require substantial a.priori specification of the observations. 
  Kalman Filters  and  Particle Filters  are very parametric in the sense that substantial information must be specified up front. 
 Dynamic Bayesian Networks ( graphical models  through time) require substantial a.priori specification and often re</p><p>6 0.092246234 <a title="169-tfidf-6" href="../hunch_net-2005/hunch_net-2005-02-23-Problem%3A_Reinforcement_Learning_with_Classification.html">27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</a></p>
<p>7 0.089794397 <a title="169-tfidf-7" href="../hunch_net-2006/hunch_net-2006-10-08-Incompatibilities_between_classical_confidence_intervals_and_learning..html">213 hunch net-2006-10-08-Incompatibilities between classical confidence intervals and learning.</a></p>
<p>8 0.086635426 <a title="169-tfidf-8" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>9 0.086073235 <a title="169-tfidf-9" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>10 0.082740545 <a title="169-tfidf-10" href="../hunch_net-2005/hunch_net-2005-04-21-Dynamic_Programming_Generalizations_and_Their_Use.html">58 hunch net-2005-04-21-Dynamic Programming Generalizations and Their Use</a></p>
<p>11 0.082603626 <a title="169-tfidf-11" href="../hunch_net-2005/hunch_net-2005-01-24-Holy_grails_of_machine_learning%3F.html">2 hunch net-2005-01-24-Holy grails of machine learning?</a></p>
<p>12 0.080809243 <a title="169-tfidf-12" href="../hunch_net-2008/hunch_net-2008-09-12-How_do_we_get_weak_action_dependence_for_learning_with_partial_observations%3F.html">317 hunch net-2008-09-12-How do we get weak action dependence for learning with partial observations?</a></p>
<p>13 0.080217861 <a title="169-tfidf-13" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>14 0.080054916 <a title="169-tfidf-14" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>15 0.078579754 <a title="169-tfidf-15" href="../hunch_net-2010/hunch_net-2010-03-12-Netflix_Challenge_2_Canceled.html">390 hunch net-2010-03-12-Netflix Challenge 2 Canceled</a></p>
<p>16 0.074592993 <a title="169-tfidf-16" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>17 0.069908157 <a title="169-tfidf-17" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>18 0.068686873 <a title="169-tfidf-18" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>19 0.067473054 <a title="169-tfidf-19" href="../hunch_net-2005/hunch_net-2005-10-16-Complexity%3A_It%26%238217%3Bs_all_in_your_head.html">123 hunch net-2005-10-16-Complexity: It&#8217;s all in your head</a></p>
<p>20 0.067190528 <a title="169-tfidf-20" href="../hunch_net-2007/hunch_net-2007-07-13-The_View_From_China.html">255 hunch net-2007-07-13-The View From China</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.129), (1, 0.049), (2, -0.01), (3, 0.041), (4, 0.04), (5, -0.049), (6, 0.044), (7, 0.061), (8, 0.051), (9, 0.009), (10, 0.026), (11, -0.017), (12, 0.013), (13, 0.115), (14, -0.046), (15, 0.012), (16, 0.013), (17, -0.048), (18, 0.07), (19, 0.035), (20, -0.07), (21, 0.041), (22, -0.042), (23, 0.059), (24, -0.114), (25, 0.097), (26, -0.157), (27, -0.051), (28, -0.09), (29, 0.064), (30, 0.027), (31, -0.024), (32, -0.005), (33, -0.018), (34, -0.02), (35, -0.005), (36, 0.099), (37, 0.006), (38, 0.051), (39, -0.086), (40, 0.011), (41, 0.002), (42, 0.096), (43, 0.062), (44, -0.021), (45, 0.046), (46, -0.022), (47, 0.024), (48, -0.047), (49, -0.059)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99014288 <a title="169-lsi-1" href="../hunch_net-2006/hunch_net-2006-04-05-What_is_state%3F.html">169 hunch net-2006-04-05-What is state?</a></p>
<p>Introduction: In reinforcement learning (and sometimes other settings), there is a notion of “state”.   Based upon the state various predictions are made such as “Which action should be taken next?” or “How much cumulative reward do I expect if I take some action from this state?”  Given the importance of state, it is important to examine the meaning.   There are actually several distinct options and it turns out the definition variation is very important in motivating different pieces of work.
  
 Newtonian State.  State is the physical pose of the world.  Under this definition, there are  very  many states, often too many for explicit representation.  This is also the definition typically used in games. 
 Abstracted State.  State is an abstracted physical state of the world.  “Is the door open or closed?” “Are you in room A or not?” The number of states is much smaller here.  A basic issue here is: “How do you compute the state from observations?” 
 Mathematical State.  State is a sufficient stati</p><p>2 0.65968555 <a title="169-lsi-2" href="../hunch_net-2005/hunch_net-2005-08-04-Why_Reinforcement_Learning_is_Important.html">100 hunch net-2005-08-04-Why Reinforcement Learning is Important</a></p>
<p>Introduction: One prescription for solving a problem well is:
  
 State the problem, in the simplest way possible. In particular, this statement should involve no contamination with or anticipation of the solution. 
 Think about solutions to the stated problem. 
  
Stating a problem in a succinct and crisp manner tends to invite a simple elegant solution.  When a problem can not be stated succinctly, we  wonder if the problem is even understood. (And when a problem is not understood, we wonder if a solution can be meaningful.)
 
Reinforcement learning does step (1) well.  It provides a clean simple language to state general AI problems.  In reinforcement learning there is a set of actions  A , a set of observations  O , and a reward  r .  The reinforcement learning problem, in general, is defined by a conditional measure  D( o, r | (o,r,a) * )  which produces an observation  o  and a reward  r  given a history  (o,r,a) *  .  The goal in reinforcement learning is to find a policy  pi:(o,r,a) *  -> a</p><p>3 0.60232985 <a title="169-lsi-3" href="../hunch_net-2009/hunch_net-2009-06-03-Functionally_defined_Nonlinear_Dynamic_Models.html">359 hunch net-2009-06-03-Functionally defined Nonlinear Dynamic Models</a></p>
<p>Introduction: Suppose we have a set of observations over time  x 1 ,x 2 ,…,x t   and want to predict some future event  y t+1  .  An inevitable problem arises, because learning a predictor  h(x 1 ,…,x t )  of  y t+1   is generically intractable due to the size of the input.  To make this problem tractable, what’s necessary is a method for summarizing the relevant information in past observations for the purpose of prediction in the future.  In other words, state is required.
 
Existing approaches for deriving state have some limitations.
  
  Hidden Markov models  learned with EM suffer from local minima, use tabular learning approaches which provide dubious generalization ability, and often require substantial a.priori specification of the observations. 
  Kalman Filters  and  Particle Filters  are very parametric in the sense that substantial information must be specified up front. 
 Dynamic Bayesian Networks ( graphical models  through time) require substantial a.priori specification and often re</p><p>4 0.54734951 <a title="169-lsi-4" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>Introduction: One thing which is clear on a little reflection is that there exists a single master learning problem capable of encoding essentially all learning problems.  This problem is of course a very general sort of reinforcement learning where the world interacts with an agent as:
  
 The world announces an observation  x . 
 The agent makes a choice  a . 
 The world announces a reward  r . 
  
The goal here is to maximize the sum of the rewards over the time of the agent.  No particular structure relating  x  to  a  or  a  to  r  is implied by this setting so we do not know effective general algorithms for the agent.  It’s very easy to prove lower bounds showing that an agent cannot hope to succeed here—just consider the case where actions are unrelated to rewards.  Nevertheless, there is a real sense in which essentially all forms of life are agents operating in this setting, somehow succeeding.  The gap between these observations drives research—How can we find tractable specializations of</p><p>5 0.54461884 <a title="169-lsi-5" href="../hunch_net-2005/hunch_net-2005-02-23-Problem%3A_Reinforcement_Learning_with_Classification.html">27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</a></p>
<p>Introduction: At an intuitive level, the question here is “Can reinforcement learning be solved with classification?”  
 
 Problem  Construct a reinforcement learning algorithm with near-optimal expected sum of rewards in the  direct experience model  given access to a classifier learning algorithm which has a small error rate or regret on all posed classification problems.  The definition of “posed” here is slightly murky.  I consider a problem “posed” if there is an algorithm for constructing labeled classification examples.
 
 Past Work 
  
 There exists a  reduction of reinforcement learning to classification given a generative model.   A generative model is an inherently stronger assumption than the direct experience model. 
 Other  work on learning reductions  may be important. 
 Several algorithms for solving reinforcement learning in the direct experience model exist.  Most, such as  E 3  ,  Factored-E 3  , and  metric-E 3   and  Rmax  require that the observation be the state.  Recent work</p><p>6 0.54335493 <a title="169-lsi-6" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>7 0.51509088 <a title="169-lsi-7" href="../hunch_net-2007/hunch_net-2007-10-24-Contextual_Bandits.html">269 hunch net-2007-10-24-Contextual Bandits</a></p>
<p>8 0.5035457 <a title="169-lsi-8" href="../hunch_net-2005/hunch_net-2005-08-08-Apprenticeship_Reinforcement_Learning_for_Control.html">101 hunch net-2005-08-08-Apprenticeship Reinforcement Learning for Control</a></p>
<p>9 0.48965704 <a title="169-lsi-9" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>10 0.4854317 <a title="169-lsi-10" href="../hunch_net-2008/hunch_net-2008-09-12-How_do_we_get_weak_action_dependence_for_learning_with_partial_observations%3F.html">317 hunch net-2008-09-12-How do we get weak action dependence for learning with partial observations?</a></p>
<p>11 0.47475764 <a title="169-lsi-11" href="../hunch_net-2006/hunch_net-2006-01-13-Benchmarks_for_RL.html">148 hunch net-2006-01-13-Benchmarks for RL</a></p>
<p>12 0.47418675 <a title="169-lsi-12" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>13 0.46881977 <a title="169-lsi-13" href="../hunch_net-2006/hunch_net-2006-10-08-Incompatibilities_between_classical_confidence_intervals_and_learning..html">213 hunch net-2006-10-08-Incompatibilities between classical confidence intervals and learning.</a></p>
<p>14 0.44817194 <a title="169-lsi-14" href="../hunch_net-2006/hunch_net-2006-02-02-Introspectionism_as_a_Disease.html">153 hunch net-2006-02-02-Introspectionism as a Disease</a></p>
<p>15 0.43825448 <a title="169-lsi-15" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>16 0.4380129 <a title="169-lsi-16" href="../hunch_net-2005/hunch_net-2005-04-21-Dynamic_Programming_Generalizations_and_Their_Use.html">58 hunch net-2005-04-21-Dynamic Programming Generalizations and Their Use</a></p>
<p>17 0.42745921 <a title="169-lsi-17" href="../hunch_net-2008/hunch_net-2008-02-17-The_Meaning_of_Confidence.html">289 hunch net-2008-02-17-The Meaning of Confidence</a></p>
<p>18 0.41854984 <a title="169-lsi-18" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>19 0.4045043 <a title="169-lsi-19" href="../hunch_net-2005/hunch_net-2005-01-24-Holy_grails_of_machine_learning%3F.html">2 hunch net-2005-01-24-Holy grails of machine learning?</a></p>
<p>20 0.38240474 <a title="169-lsi-20" href="../hunch_net-2008/hunch_net-2008-06-09-The_Minimum_Sample_Complexity_of_Importance_Weighting.html">303 hunch net-2008-06-09-The Minimum Sample Complexity of Importance Weighting</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(3, 0.027), (14, 0.024), (27, 0.111), (48, 0.02), (52, 0.219), (53, 0.106), (55, 0.151), (69, 0.013), (77, 0.052), (79, 0.036), (94, 0.086), (95, 0.032)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9252398 <a title="169-lda-1" href="../hunch_net-2006/hunch_net-2006-04-05-What_is_state%3F.html">169 hunch net-2006-04-05-What is state?</a></p>
<p>Introduction: In reinforcement learning (and sometimes other settings), there is a notion of “state”.   Based upon the state various predictions are made such as “Which action should be taken next?” or “How much cumulative reward do I expect if I take some action from this state?”  Given the importance of state, it is important to examine the meaning.   There are actually several distinct options and it turns out the definition variation is very important in motivating different pieces of work.
  
 Newtonian State.  State is the physical pose of the world.  Under this definition, there are  very  many states, often too many for explicit representation.  This is also the definition typically used in games. 
 Abstracted State.  State is an abstracted physical state of the world.  “Is the door open or closed?” “Are you in room A or not?” The number of states is much smaller here.  A basic issue here is: “How do you compute the state from observations?” 
 Mathematical State.  State is a sufficient stati</p><p>2 0.74449939 <a title="169-lda-2" href="../hunch_net-2005/hunch_net-2005-01-26-Watchword%3A_Probability.html">5 hunch net-2005-01-26-Watchword: Probability</a></p>
<p>Introduction: Probability is one of the most confusingly used words in machine learning.  There are at least 3 distinct ways the word is used. 
  
  Bayesian  The Bayesian notion of probability is a ‘degree of belief’.   The degree of belief that some event (i.e. “stock goes up” or “stock goes down”) occurs can be measured by asking a sequence of questions of the form “Would you bet the stock goes up or down at  Y  to 1 odds?” A consistent better will switch from ‘for’ to ‘against’ at some single value of  Y .  The probability is then  Y/(Y+1) .  Bayesian probabilities express lack of knowledge rather than randomization.  They are useful in learning because we often lack knowledge and expressing that lack flexibly makes the learning algorithms work better.  Bayesian Learning uses ‘probability’ in this way exclusively. 
  Frequentist  The Frequentist notion of probability is a rate of occurence.  A rate of occurrence can be measured by doing an experiment many times.  If an event occurs  k  times in</p><p>3 0.67626864 <a title="169-lda-3" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>Introduction: Conferences exist as part of the process of doing research.  They provide many roles including “announcing research”, “meeting people”, and  “point of reference”.  Not all conferences are alike so a basic question is: “to what extent do individual conferences attempt to aid research?”  This question is very difficult to answer in any satisfying way.  What we can do is compare details of the process across multiple conferences.
  
  Comments   The average quality of comments across conferences can vary dramatically.  At one extreme, the tradition in CS theory conferences is to provide essentially zero feedback.  At the other extreme, some conferences have a strong tradition of providing detailed constructive feedback.  Detailed feedback can give authors significant guidance about how to improve research.  This is the most subjective entry. 
  Blind  Virtually all conferences offer single blind review where authors do not know reviewers.  Some also provide  double blind  review where rev</p><p>4 0.67442495 <a title="169-lda-4" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<p>Introduction: I want to comment on the “Bing copies Google” discussion  here ,  here , and  here , because there are data-related issues which the general public may not understand, and some of the framing seems substantially misleading to me.
 
As a not-distant-outsider, let me mention the sources of bias I may have.  I work at  Yahoo! , which has started using  Bing .  This might predispose me towards Bing, but on the other hand I’m still at Yahoo!, and have been using  Linux  exclusively as an OS for many years, including even a couple minor kernel patches.  And,  on the gripping hand , I’ve spent quite a bit of time thinking about the basic  principles of incorporating user feedback in machine learning .  Also note, this post is not  related to official Yahoo! policy, it’s just my personal view.
 
 The issue  Google engineers inserted synthetic responses to synthetic queries on google.com, then executed the synthetic searches on google.com using Internet Explorer with the Bing toolbar and later</p><p>5 0.66347897 <a title="169-lda-5" href="../hunch_net-2005/hunch_net-2005-03-13-Avoiding_Bad_Reviewing.html">40 hunch net-2005-03-13-Avoiding Bad Reviewing</a></p>
<p>Introduction: If we accept that bad reviewing often occurs and want to fix it, the question is “how”?
 
Reviewing is done by paper writers just like yourself, so a good proxy for this question is asking “How can I be a better reviewer?”  Here are a few things I’ve learned by trial (and error), as a paper writer, and as a reviewer.
  
 The secret ingredient is careful thought.  There is no good substitution for a deep and careful understanding. 
 Avoid reviewing papers that you feel competitive about.  You almost certainly will be asked to review papers that feel competitive if you work on subjects of common interest.  But, the feeling of competition can easily lead to bad judgement. 
 If you feel biased for some other reason, then you should avoid reviewing.  For example… 
 Feeling angry or threatened by a paper is a form of bias.  See above. 
 Double blind yourself (avoid looking at the name even in a single-blind situation).  The significant effect of a name you recognize is making you pay close a</p><p>6 0.66321075 <a title="169-lda-6" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>7 0.66115725 <a title="169-lda-7" href="../hunch_net-2005/hunch_net-2005-12-07-Is_the_Google_way_the_way_for_machine_learning%3F.html">136 hunch net-2005-12-07-Is the Google way the way for machine learning?</a></p>
<p>8 0.65457278 <a title="169-lda-8" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>9 0.65032095 <a title="169-lda-9" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>10 0.64887041 <a title="169-lda-10" href="../hunch_net-2006/hunch_net-2006-01-25-1_year.html">151 hunch net-2006-01-25-1 year</a></p>
<p>11 0.64555109 <a title="169-lda-11" href="../hunch_net-2010/hunch_net-2010-04-26-Compassionate_Reviewing.html">395 hunch net-2010-04-26-Compassionate Reviewing</a></p>
<p>12 0.64274853 <a title="169-lda-12" href="../hunch_net-2012/hunch_net-2012-04-09-ICML_author_feedback_is_open.html">461 hunch net-2012-04-09-ICML author feedback is open</a></p>
<p>13 0.63899618 <a title="169-lda-13" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>14 0.63801718 <a title="169-lda-14" href="../hunch_net-2005/hunch_net-2005-07-21-Six_Months.html">96 hunch net-2005-07-21-Six Months</a></p>
<p>15 0.63389146 <a title="169-lda-15" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>16 0.6329897 <a title="169-lda-16" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<p>17 0.63225961 <a title="169-lda-17" href="../hunch_net-2007/hunch_net-2007-11-02-The_Machine_Learning_Award_goes_to_%26%238230%3B.html">270 hunch net-2007-11-02-The Machine Learning Award goes to &#8230;</a></p>
<p>18 0.62829345 <a title="169-lda-18" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>19 0.62668371 <a title="169-lda-19" href="../hunch_net-2010/hunch_net-2010-09-17-New_York_Area_Machine_Learning_Events.html">410 hunch net-2010-09-17-New York Area Machine Learning Events</a></p>
<p>20 0.62646961 <a title="169-lda-20" href="../hunch_net-2010/hunch_net-2010-10-29-To_Vidoelecture_or_not.html">416 hunch net-2010-10-29-To Vidoelecture or not</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
