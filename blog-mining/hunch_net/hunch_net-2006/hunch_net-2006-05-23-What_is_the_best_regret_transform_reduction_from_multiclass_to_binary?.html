<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>181 hunch net-2006-05-23-What is the best regret transform reduction from multiclass to binary?</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-181" href="#">hunch_net-2006-181</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>181 hunch net-2006-05-23-What is the best regret transform reduction from multiclass to binary?</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-181-html" href="http://hunch.net/?p=192">html</a></p><p>Introduction: This post is about an open problem in learning reductions.BackgroundA
reduction might transform a a multiclass prediction problem where there
arekpossible labels into a binary learning problem where there are only 2
possible labels. On this induced binary problem we might learn a binary
classifier with some error ratee. After subtracting the minimum possible
(Bayes) error rateb, we get a regretr = e - b. ThePECOC(Probabilistic Error
Correcting Output Code) reduction has the property that binary regretrimplies
multiclass regret at most4r0.5.The problemThis is not the "rightest" answer.
Consider thek=2case, where we reduce binary to binary. There exists a
reduction (the identity) with the property that regretrimplies regretr. This
is substantially superior to the transform given by the PECOC reduction, which
suggests that a better reduction may exist for generalk. For example, we can
not rule out the possibility that a reductionRexists with regret transform
guaranteeing binary regretrimp</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('transform', 0.404), ('binary', 0.371), ('reduction', 0.366), ('regretrimplies', 0.27), ('pecoc', 0.27), ('multiclass', 0.258), ('regret', 0.179), ('ecoc', 0.167), ('induced', 0.144), ('error', 0.143), ('correcting', 0.127), ('concern', 0.11), ('property', 0.108), ('output', 0.097), ('regretr', 0.09), ('guaranteeing', 0.09), ('thepecoc', 0.09), ('problem', 0.085), ('classifier', 0.085), ('lemma', 0.083)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000004 <a title="181-tfidf-1" href="../hunch_net-2006/hunch_net-2006-05-23-What_is_the_best_regret_transform_reduction_from_multiclass_to_binary%3F.html">181 hunch net-2006-05-23-What is the best regret transform reduction from multiclass to binary?</a></p>
<p>Introduction: This post is about an open problem in learning reductions.BackgroundA
reduction might transform a a multiclass prediction problem where there
arekpossible labels into a binary learning problem where there are only 2
possible labels. On this induced binary problem we might learn a binary
classifier with some error ratee. After subtracting the minimum possible
(Bayes) error rateb, we get a regretr = e - b. ThePECOC(Probabilistic Error
Correcting Output Code) reduction has the property that binary regretrimplies
multiclass regret at most4r0.5.The problemThis is not the "rightest" answer.
Consider thek=2case, where we reduce binary to binary. There exists a
reduction (the identity) with the property that regretrimplies regretr. This
is substantially superior to the transform given by the PECOC reduction, which
suggests that a better reduction may exist for generalk. For example, we can
not rule out the possibility that a reductionRexists with regret transform
guaranteeing binary regretrimp</p><p>2 0.40915006 <a title="181-tfidf-2" href="../hunch_net-2005/hunch_net-2005-05-16-Regret_minimizing_vs_error_limiting_reductions.html">72 hunch net-2005-05-16-Regret minimizing vs error limiting reductions</a></p>
<p>Introduction: This post is about a reductions-related problem that I find mysterious. There
are two kinds of reductions analysis currently under consideration.Error
limiting reductions. Here, the goal is to bound the error rate of the created
classifier in terms of the error rate of the binary classifiers that you
reduce to. A very simple example of this is thaterror correcting output
codeswhere it is possible to prove that for certain codes, the multiclass
error rate is at most 4 * the binary classifier error rate.Regret minimizing
reductions. Here, the goal is to bound theregretof the created classifier in
terms of theregretof the binary classifiers reduced to. The regret is the
error rate minus the minimum error rate. When the learning problem is noisy
the minimum error rate may not be0. An analagous result for reget is that for
aprobabilistic error correcting output code, multiclass regret is at most 4 *
(binary regret)0.5.The use of "regret" is more desirable than the use of error
rates, becaus</p><p>3 0.32555208 <a title="181-tfidf-3" href="../hunch_net-2007/hunch_net-2007-05-08-Conditional_Tournaments_for_Multiclass_to_Binary.html">243 hunch net-2007-05-08-Conditional Tournaments for Multiclass to Binary</a></p>
<p>Introduction: Thisproblemhas been cracked (but not quite completely solved) byAlina,Pradeep,
andI. The problem is essentially finding a better way to reduce multiclass
classification to binary classification. The solution is to use a carefully
crafted tournament, the simplest version of which is asingle elimination
tournamentwhere the "players" are the different classes. An example of the
structure is here:For the single elimination tournament, we can prove that:For
all multiclass problemsD, for all learned binary classifiersc, the regret of
an induced multiclass classifier is bounded by the regret of the binary
classifier timeslog2k. Restated:regmulticlass(D,Filter_tree_test(c)) <=
regbinary(Filter_tree_train(D),c)Here:Filter_tree_train(D)is the induced
binary classification problemFilter_tree_test(c)is the induced multiclass
classifier.regmulticlassis the multiclass regret (= difference between error
rate and minimum possible error rate)regbinaryis the binary regretThis result
has a slight depende</p><p>4 0.27154276 <a title="181-tfidf-4" href="../hunch_net-2007/hunch_net-2007-03-15-Alternative_Machine_Learning_Reductions_Definitions.html">236 hunch net-2007-03-15-Alternative Machine Learning Reductions Definitions</a></p>
<p>Introduction: A type of prediction problem is specified by the type of samples produced by a
data source (Example:X x {0,1},X x [0,1],X x {1,2,3,4,5}, etc…) and a loss
function (0/1 loss, squared error loss, cost sensitive losses, etc…). For
simplicity, we'll assume that all losses have a minimum of zero.For this post,
we can think of a learning reduction asA mappingRfrom samples of one
typeT(like multiclass classification) to another typeT'(like binary
classification).A mappingQfrom predictors for typeT'to predictors for
typeT.The simplest sort of learning reduction is a "loss reduction". The idea
in a loss reduction is to prove a statement of the form:TheoremFor all base
predictorsb, for all distributionsDover examples of typeT:E(x,y) ~
DLT(y,Q(b,x)) <= f(E(x',y')~R(D)LT'(y',b(x')))HereLTis the loss for the
typeTproblem andLT'is the loss for the typeT'problem. Also,R(D)is the
distribution over samples induced by first drawing fromDand then mapping the
sample viaR. The functionf()is the loss transf</p><p>5 0.24406797 <a title="181-tfidf-5" href="../hunch_net-2010/hunch_net-2010-03-15-The_Efficient_Robust_Conditional_Probability_Estimation_Problem.html">391 hunch net-2010-03-15-The Efficient Robust Conditional Probability Estimation Problem</a></p>
<p>Introduction: I'm offering a reward of $1000 for a solution to this problem. This joins
thecross validation problemwhich I'm offering a$500 rewardfor. I believe both
of these problems are hard but plausibly solvable, and plausibly with a
solution of substantial practical value. While it's unlikely these rewards are
worth your time on an hourly wage basis, the recognition for solving them
definitely should beThe ProblemThe problem is finding a general, robust, and
efficient mechanism for estimating a conditional probabilityP(y|x)where
robustness and efficiency are measured using techniques from learning
reductions.In particular, suppose we have access to a binary regression
oracleBwhich has two interfaces--one for specifying training information and
one for testing. Training information is specified asB(x',y')wherex'is a
feature vector andy'is a scalar in[0,1]with no value returned. Testing is done
according toB(x')with a value in[0,1]returned.A learning reduction consists of
two algorithmsRandR-1whi</p><p>6 0.20555902 <a title="181-tfidf-6" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>7 0.18880369 <a title="181-tfidf-7" href="../hunch_net-2006/hunch_net-2006-07-13-Regression_vs._Classification_as_a_Primitive.html">196 hunch net-2006-07-13-Regression vs. Classification as a Primitive</a></p>
<p>8 0.15311921 <a title="181-tfidf-8" href="../hunch_net-2008/hunch_net-2008-11-16-Observations_on_Linearity_for_Reductions_to_Regression.html">327 hunch net-2008-11-16-Observations on Linearity for Reductions to Regression</a></p>
<p>9 0.13725851 <a title="181-tfidf-9" href="../hunch_net-2005/hunch_net-2005-08-18-SVM_Adaptability.html">103 hunch net-2005-08-18-SVM Adaptability</a></p>
<p>10 0.12424687 <a title="181-tfidf-10" href="../hunch_net-2006/hunch_net-2006-09-09-How_to_solve_an_NP_hard_problem_in_quadratic_time.html">206 hunch net-2006-09-09-How to solve an NP hard problem in quadratic time</a></p>
<p>11 0.11734523 <a title="181-tfidf-11" href="../hunch_net-2011/hunch_net-2011-08-01-Interesting_papers_at_COLT_2011.html">439 hunch net-2011-08-01-Interesting papers at COLT 2011</a></p>
<p>12 0.11137071 <a title="181-tfidf-12" href="../hunch_net-2006/hunch_net-2006-05-01-A_conversation_between_Theo_and_Pat.html">176 hunch net-2006-05-01-A conversation between Theo and Pat</a></p>
<p>13 0.10796078 <a title="181-tfidf-13" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>14 0.10342957 <a title="181-tfidf-14" href="../hunch_net-2005/hunch_net-2005-08-11-Why_Manifold-Based_Dimension_Reduction_Techniques%3F.html">102 hunch net-2005-08-11-Why Manifold-Based Dimension Reduction Techniques?</a></p>
<p>15 0.097630978 <a title="181-tfidf-15" href="../hunch_net-2013/hunch_net-2013-12-01-NIPS_tutorials_and_Vowpal_Wabbit_7.4.html">492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</a></p>
<p>16 0.096455045 <a title="181-tfidf-16" href="../hunch_net-2005/hunch_net-2005-06-18-Lower_Bounds_for_Learning_Reductions.html">83 hunch net-2005-06-18-Lower Bounds for Learning Reductions</a></p>
<p>17 0.095583677 <a title="181-tfidf-17" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<p>18 0.094260693 <a title="181-tfidf-18" href="../hunch_net-2006/hunch_net-2006-06-24-Online_convex_optimization_at_COLT.html">186 hunch net-2006-06-24-Online convex optimization at COLT</a></p>
<p>19 0.091894224 <a title="181-tfidf-19" href="../hunch_net-2005/hunch_net-2005-06-06-Exact_Online_Learning_for_Classification.html">78 hunch net-2005-06-06-Exact Online Learning for Classification</a></p>
<p>20 0.090546496 <a title="181-tfidf-20" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.155), (1, -0.164), (2, -0.116), (3, 0.05), (4, 0.043), (5, -0.327), (6, 0.226), (7, 0.089), (8, 0.226), (9, 0.163), (10, 0.161), (11, 0.079), (12, 0.063), (13, 0.057), (14, 0.018), (15, 0.011), (16, 0.05), (17, 0.021), (18, -0.108), (19, -0.063), (20, -0.055), (21, -0.03), (22, 0.082), (23, 0.028), (24, 0.084), (25, 0.041), (26, 0.084), (27, 0.051), (28, 0.015), (29, -0.025), (30, 0.047), (31, -0.036), (32, 0.006), (33, 0.085), (34, -0.018), (35, -0.05), (36, 0.052), (37, 0.068), (38, -0.004), (39, 0.047), (40, 0.033), (41, 0.125), (42, -0.055), (43, 0.041), (44, 0.044), (45, 0.03), (46, 0.033), (47, -0.002), (48, -0.017), (49, -0.005)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98713952 <a title="181-lsi-1" href="../hunch_net-2006/hunch_net-2006-05-23-What_is_the_best_regret_transform_reduction_from_multiclass_to_binary%3F.html">181 hunch net-2006-05-23-What is the best regret transform reduction from multiclass to binary?</a></p>
<p>Introduction: This post is about an open problem in learning reductions.BackgroundA
reduction might transform a a multiclass prediction problem where there
arekpossible labels into a binary learning problem where there are only 2
possible labels. On this induced binary problem we might learn a binary
classifier with some error ratee. After subtracting the minimum possible
(Bayes) error rateb, we get a regretr = e - b. ThePECOC(Probabilistic Error
Correcting Output Code) reduction has the property that binary regretrimplies
multiclass regret at most4r0.5.The problemThis is not the "rightest" answer.
Consider thek=2case, where we reduce binary to binary. There exists a
reduction (the identity) with the property that regretrimplies regretr. This
is substantially superior to the transform given by the PECOC reduction, which
suggests that a better reduction may exist for generalk. For example, we can
not rule out the possibility that a reductionRexists with regret transform
guaranteeing binary regretrimp</p><p>2 0.93933153 <a title="181-lsi-2" href="../hunch_net-2007/hunch_net-2007-05-08-Conditional_Tournaments_for_Multiclass_to_Binary.html">243 hunch net-2007-05-08-Conditional Tournaments for Multiclass to Binary</a></p>
<p>Introduction: Thisproblemhas been cracked (but not quite completely solved) byAlina,Pradeep,
andI. The problem is essentially finding a better way to reduce multiclass
classification to binary classification. The solution is to use a carefully
crafted tournament, the simplest version of which is asingle elimination
tournamentwhere the "players" are the different classes. An example of the
structure is here:For the single elimination tournament, we can prove that:For
all multiclass problemsD, for all learned binary classifiersc, the regret of
an induced multiclass classifier is bounded by the regret of the binary
classifier timeslog2k. Restated:regmulticlass(D,Filter_tree_test(c)) <=
regbinary(Filter_tree_train(D),c)Here:Filter_tree_train(D)is the induced
binary classification problemFilter_tree_test(c)is the induced multiclass
classifier.regmulticlassis the multiclass regret (= difference between error
rate and minimum possible error rate)regbinaryis the binary regretThis result
has a slight depende</p><p>3 0.85106599 <a title="181-lsi-3" href="../hunch_net-2005/hunch_net-2005-05-16-Regret_minimizing_vs_error_limiting_reductions.html">72 hunch net-2005-05-16-Regret minimizing vs error limiting reductions</a></p>
<p>Introduction: This post is about a reductions-related problem that I find mysterious. There
are two kinds of reductions analysis currently under consideration.Error
limiting reductions. Here, the goal is to bound the error rate of the created
classifier in terms of the error rate of the binary classifiers that you
reduce to. A very simple example of this is thaterror correcting output
codeswhere it is possible to prove that for certain codes, the multiclass
error rate is at most 4 * the binary classifier error rate.Regret minimizing
reductions. Here, the goal is to bound theregretof the created classifier in
terms of theregretof the binary classifiers reduced to. The regret is the
error rate minus the minimum error rate. When the learning problem is noisy
the minimum error rate may not be0. An analagous result for reget is that for
aprobabilistic error correcting output code, multiclass regret is at most 4 *
(binary regret)0.5.The use of "regret" is more desirable than the use of error
rates, becaus</p><p>4 0.69041878 <a title="181-lsi-4" href="../hunch_net-2006/hunch_net-2006-07-13-Regression_vs._Classification_as_a_Primitive.html">196 hunch net-2006-07-13-Regression vs. Classification as a Primitive</a></p>
<p>Introduction: Forlearning reductionswe have been concentrating on reducing various complex
learning problems to binary classification. This choice needs to be actively
questioned, because it was not carefully considered.Binary clasification is
learning a classifierc:X -> {0,1}so as to minimize the probability of being
wrong,Prx,y~D(c(x)y).The primary alternative candidate seems to be squared
error regression. In squared error regression, you learn a regressors:X ->
[0,1]so as to minimize squared error,Ex,y~D(s(x)-y)2.It is difficult to judge
one primitive against another. The judgement must at least partially be made
on nontheoretical grounds because (essentially) we are evaluating a choice
between two axioms/assumptions.These two primitives are significantly related.
Classification can be reduced to regression in the obvious way: you use the
regressor to predictD(y=1|x), then threshold at0.5. For this simple reduction
a squared error regret ofrimplies a classification regret of at mostr0.5.
Regress</p><p>5 0.66674376 <a title="181-lsi-5" href="../hunch_net-2007/hunch_net-2007-03-15-Alternative_Machine_Learning_Reductions_Definitions.html">236 hunch net-2007-03-15-Alternative Machine Learning Reductions Definitions</a></p>
<p>Introduction: A type of prediction problem is specified by the type of samples produced by a
data source (Example:X x {0,1},X x [0,1],X x {1,2,3,4,5}, etc…) and a loss
function (0/1 loss, squared error loss, cost sensitive losses, etc…). For
simplicity, we'll assume that all losses have a minimum of zero.For this post,
we can think of a learning reduction asA mappingRfrom samples of one
typeT(like multiclass classification) to another typeT'(like binary
classification).A mappingQfrom predictors for typeT'to predictors for
typeT.The simplest sort of learning reduction is a "loss reduction". The idea
in a loss reduction is to prove a statement of the form:TheoremFor all base
predictorsb, for all distributionsDover examples of typeT:E(x,y) ~
DLT(y,Q(b,x)) <= f(E(x',y')~R(D)LT'(y',b(x')))HereLTis the loss for the
typeTproblem andLT'is the loss for the typeT'problem. Also,R(D)is the
distribution over samples induced by first drawing fromDand then mapping the
sample viaR. The functionf()is the loss transf</p><p>6 0.66384637 <a title="181-lsi-6" href="../hunch_net-2010/hunch_net-2010-03-15-The_Efficient_Robust_Conditional_Probability_Estimation_Problem.html">391 hunch net-2010-03-15-The Efficient Robust Conditional Probability Estimation Problem</a></p>
<p>7 0.63501477 <a title="181-lsi-7" href="../hunch_net-2008/hunch_net-2008-11-16-Observations_on_Linearity_for_Reductions_to_Regression.html">327 hunch net-2008-11-16-Observations on Linearity for Reductions to Regression</a></p>
<p>8 0.63147014 <a title="181-lsi-8" href="../hunch_net-2006/hunch_net-2006-09-09-How_to_solve_an_NP_hard_problem_in_quadratic_time.html">206 hunch net-2006-09-09-How to solve an NP hard problem in quadratic time</a></p>
<p>9 0.595456 <a title="181-lsi-9" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>10 0.46973994 <a title="181-lsi-10" href="../hunch_net-2005/hunch_net-2005-06-06-Exact_Online_Learning_for_Classification.html">78 hunch net-2005-06-06-Exact Online Learning for Classification</a></p>
<p>11 0.45723659 <a title="181-lsi-11" href="../hunch_net-2006/hunch_net-2006-05-01-A_conversation_between_Theo_and_Pat.html">176 hunch net-2006-05-01-A conversation between Theo and Pat</a></p>
<p>12 0.44730303 <a title="181-lsi-12" href="../hunch_net-2011/hunch_net-2011-08-01-Interesting_papers_at_COLT_2011.html">439 hunch net-2011-08-01-Interesting papers at COLT 2011</a></p>
<p>13 0.44563287 <a title="181-lsi-13" href="../hunch_net-2005/hunch_net-2005-06-17-Reopening_RL-%3EClassification.html">82 hunch net-2005-06-17-Reopening RL->Classification</a></p>
<p>14 0.39534718 <a title="181-lsi-14" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>15 0.39481509 <a title="181-lsi-15" href="../hunch_net-2005/hunch_net-2005-08-18-SVM_Adaptability.html">103 hunch net-2005-08-18-SVM Adaptability</a></p>
<p>16 0.37044021 <a title="181-lsi-16" href="../hunch_net-2012/hunch_net-2012-09-29-Vowpal_Wabbit%2C_version_7.0.html">473 hunch net-2012-09-29-Vowpal Wabbit, version 7.0</a></p>
<p>17 0.35892621 <a title="181-lsi-17" href="../hunch_net-2005/hunch_net-2005-08-11-Why_Manifold-Based_Dimension_Reduction_Techniques%3F.html">102 hunch net-2005-08-11-Why Manifold-Based Dimension Reduction Techniques?</a></p>
<p>18 0.35149613 <a title="181-lsi-18" href="../hunch_net-2013/hunch_net-2013-12-01-NIPS_tutorials_and_Vowpal_Wabbit_7.4.html">492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</a></p>
<p>19 0.34867236 <a title="181-lsi-19" href="../hunch_net-2006/hunch_net-2006-02-18-Multiplication_of_Learned_Probabilities_is_Dangerous.html">157 hunch net-2006-02-18-Multiplication of Learned Probabilities is Dangerous</a></p>
<p>20 0.33755392 <a title="181-lsi-20" href="../hunch_net-2007/hunch_net-2007-06-14-Interesting_Papers_at_COLT_2007.html">247 hunch net-2007-06-14-Interesting Papers at COLT 2007</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(17, 0.298), (35, 0.085), (38, 0.036), (42, 0.272), (45, 0.022), (74, 0.063), (88, 0.084), (95, 0.011)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.87679726 <a title="181-lda-1" href="../hunch_net-2006/hunch_net-2006-05-23-What_is_the_best_regret_transform_reduction_from_multiclass_to_binary%3F.html">181 hunch net-2006-05-23-What is the best regret transform reduction from multiclass to binary?</a></p>
<p>Introduction: This post is about an open problem in learning reductions.BackgroundA
reduction might transform a a multiclass prediction problem where there
arekpossible labels into a binary learning problem where there are only 2
possible labels. On this induced binary problem we might learn a binary
classifier with some error ratee. After subtracting the minimum possible
(Bayes) error rateb, we get a regretr = e - b. ThePECOC(Probabilistic Error
Correcting Output Code) reduction has the property that binary regretrimplies
multiclass regret at most4r0.5.The problemThis is not the "rightest" answer.
Consider thek=2case, where we reduce binary to binary. There exists a
reduction (the identity) with the property that regretrimplies regretr. This
is substantially superior to the transform given by the PECOC reduction, which
suggests that a better reduction may exist for generalk. For example, we can
not rule out the possibility that a reductionRexists with regret transform
guaranteeing binary regretrimp</p><p>2 0.81349093 <a title="181-lda-2" href="../hunch_net-2009/hunch_net-2009-10-26-NIPS_workshops.html">375 hunch net-2009-10-26-NIPS workshops</a></p>
<p>Introduction: Many of theNIPS workshopshave a deadline about now, and the NIPSearly
registration deadline is Nov. 6. Several interest me:Adaptive Sensing, Active
Learning, and Experimental Designdue 10/27.Discrete Optimization in Machine
Learning: Submodularity, Sparsity & Polyhedra, due Nov. 6.Large-Scale Machine
Learning: Parallelism and Massive Datasets, due 10/23 (i.e. past)Analysis and
Design of Algorithms for Interactive Machine Learning, due 10/30.And I'm sure
many of the others interest others. Workshops are great as a mechanism for
research, so take a look if there is any chance you might be interested.</p><p>3 0.76446968 <a title="181-lda-3" href="../hunch_net-2011/hunch_net-2011-11-26-Giving_Thanks.html">449 hunch net-2011-11-26-Giving Thanks</a></p>
<p>Introduction: Thanksgiving is perhaps my favorite holiday, because pausing your life and
giving thanks provides a needed moment of perspective.As a researcher, I am
most thankful for my education, without which I could not function. I want to
share this, because it provides some sense of how a researcher starts.My long
term memory seems to function particularly well, which makes any education I
get is particularly useful.I am naturally obsessive, which makes me chase down
details until I fully understand things. Natural obsessiveness can go wrong,
of course, but it's a great ally when you absolutely must get things right.My
childhood was all in one hometown, which was a conscious sacrifice on the part
of my father, implying disruptions from moving around were eliminated. I'm not
sure how important this was since travel has it's own benefits, but it bears
thought.I had several great teachers in grade school, and naturally gravitated
towards teachers over classmates, as they seemed more interesting. I</p><p>4 0.71049988 <a title="181-lda-4" href="../hunch_net-2007/hunch_net-2007-08-12-Exponentiated_Gradient.html">258 hunch net-2007-08-12-Exponentiated Gradient</a></p>
<p>Introduction: TheExponentiated Gradientalgorithm byManfred WarmuthandJyrki Kivinencame out
just as I was starting graduate school, so I missed it both at a conference
and in class. It's a fine algorithm which has a remarkable theoretical
statement accompanying it.The essential statement holds in the "online
learning with an adversary" setting. Initially, there are of set ofnweights,
which might have values(1/n,â&euro;Ś,1/n), (or any other values from a probability
distribution). Everything happens in a round-by-round fashion. On each round,
the following happens:The world reveals a set of featuresx in {0,1}n. In the
online learning with an adversary literature, the features are called
"experts" and thought of as subpredictors, but this interpretation isn't
necessary--you can just use feature values as experts (or maybe the feature
value and the negation of the feature value as two experts).EG makes a
prediction according toy' = w . x(dot product).The world reveals the truthy in
[0,1].EG updates the weights</p><p>5 0.69013602 <a title="181-lda-5" href="../hunch_net-2013/hunch_net-2013-01-07-NYU_Large_Scale_Machine_Learning_Class.html">478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</a></p>
<p>Introduction: Yann LeCunand I are coteaching a class onLarge Scale Machine Learningstarting
late Januaryat NYU. This class will cover many tricks to get machine learning
working well on datasets with many features, examples, and classes, along with
several elements of deep learning and support systems enabling the
previous.This is not a beginning class--you really need to have taken a basic
machine learning class previously to follow along. Students will be able to
run and experiment with large scale learning algorithms sinceYahoo!has donated
servers which are being configured into a small scaleHadoopcluster. We are
planning to cover the frontier of research in scalable learning algorithms, so
good class projects could easily lead to papers.For me, this is a chance to
teach on many topics of past research. In general, it seems like researchers
should engage in at least occasional teaching of research, both as a proof of
teachability and to see their own research through that lens. More generally,
I</p><p>6 0.68954015 <a title="181-lda-6" href="../hunch_net-2005/hunch_net-2005-05-16-Regret_minimizing_vs_error_limiting_reductions.html">72 hunch net-2005-05-16-Regret minimizing vs error limiting reductions</a></p>
<p>7 0.68102252 <a title="181-lda-7" href="../hunch_net-2006/hunch_net-2006-11-20-Context_and_the_calculation_misperception.html">218 hunch net-2006-11-20-Context and the calculation misperception</a></p>
<p>8 0.67857784 <a title="181-lda-8" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>9 0.66941762 <a title="181-lda-9" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>10 0.66852713 <a title="181-lda-10" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>11 0.6683594 <a title="181-lda-11" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>12 0.6682781 <a title="181-lda-12" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>13 0.66685772 <a title="181-lda-13" href="../hunch_net-2006/hunch_net-2006-02-11-Yahoo%26%238217%3Bs_Learning_Problems..html">156 hunch net-2006-02-11-Yahoo&#8217;s Learning Problems.</a></p>
<p>14 0.66636497 <a title="181-lda-14" href="../hunch_net-2011/hunch_net-2011-10-24-2011_ML_symposium_and_the_bears.html">448 hunch net-2011-10-24-2011 ML symposium and the bears</a></p>
<p>15 0.66633207 <a title="181-lda-15" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>16 0.66615283 <a title="181-lda-16" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<p>17 0.6644938 <a title="181-lda-17" href="../hunch_net-2011/hunch_net-2011-02-17-What_does_Watson_mean%3F.html">424 hunch net-2011-02-17-What does Watson mean?</a></p>
<p>18 0.66355985 <a title="181-lda-18" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>19 0.66294193 <a title="181-lda-19" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>20 0.6619541 <a title="181-lda-20" href="../hunch_net-2005/hunch_net-2005-02-03-Learning_Theory%2C_by_assumption.html">12 hunch net-2005-02-03-Learning Theory, by assumption</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
