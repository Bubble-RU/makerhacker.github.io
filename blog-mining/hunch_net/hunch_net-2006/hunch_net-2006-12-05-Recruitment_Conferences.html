<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>222 hunch net-2006-12-05-Recruitment Conferences</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-222" href="#">hunch_net-2006-222</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>222 hunch net-2006-12-05-Recruitment Conferences</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-222-html" href="http://hunch.net/?p=242">html</a></p><p>Introduction: One of the subsidiary roles of conferences is recruitment.NIPSis optimally
placed in time for this because it falls right before the major recruitment
season.I personally found job hunting embarrassing, and was relatively inept
at it. I expect this is true of many people, because it is not something done
often.The basic rule is: make the plausible hirers aware of your interest.
Anycorporate sponsoris a "plausible", regardless of whether or not there is a
booth.CRAand theacm job centerare other reasonable sources.There are
substantial differences between the different possibilities. Putting some
effort into understanding the distinctions is a good idea, although you should
always remember where the other person is coming from.</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('job', 0.319), ('plausible', 0.282), ('recruitment', 0.274), ('falls', 0.254), ('roles', 0.254), ('distinctions', 0.228), ('regardless', 0.219), ('placed', 0.219), ('putting', 0.219), ('differences', 0.199), ('major', 0.199), ('remember', 0.17), ('rule', 0.165), ('personally', 0.162), ('coming', 0.157), ('aware', 0.148), ('person', 0.144), ('effort', 0.125), ('whether', 0.119), ('true', 0.116)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="222-tfidf-1" href="../hunch_net-2006/hunch_net-2006-12-05-Recruitment_Conferences.html">222 hunch net-2006-12-05-Recruitment Conferences</a></p>
<p>Introduction: One of the subsidiary roles of conferences is recruitment.NIPSis optimally
placed in time for this because it falls right before the major recruitment
season.I personally found job hunting embarrassing, and was relatively inept
at it. I expect this is true of many people, because it is not something done
often.The basic rule is: make the plausible hirers aware of your interest.
Anycorporate sponsoris a "plausible", regardless of whether or not there is a
booth.CRAand theacm job centerare other reasonable sources.There are
substantial differences between the different possibilities. Putting some
effort into understanding the distinctions is a good idea, although you should
always remember where the other person is coming from.</p><p>2 0.10562468 <a title="222-tfidf-2" href="../hunch_net-2005/hunch_net-2005-12-22-Yes_%2C_I_am_applying.html">142 hunch net-2005-12-22-Yes , I am applying</a></p>
<p>Introduction: Every year about now hundreds of applicants apply for a research/teaching job
with the timing governed by the university recruitment schedule. This time,
it's my turn--the hat's in the ring, I am a contender, etcâ&euro;Ś What I have heard
is that this year is good in both directions--both an increased supply and an
increased demand for machine learning expertise.I consider this post a bit of
an abuse as it is neither about general research nor machine learning. Please
forgive me this once.My hope is that I will learn about new places interested
in funding basic research--it's easy to imagine that I have overlooked
possibilities.I am not dogmatic about where I end up in any particular way.
Several earlier posts detail what I think of as a good research environment,
so I will avoid a repeat. A few more details seem important:Application. There
is often a tension between basic research and immediate application. This
tension is not as strong as might be expected in my case. As evidence, many of</p><p>3 0.096397862 <a title="222-tfidf-3" href="../hunch_net-2008/hunch_net-2008-05-25-Inappropriate_Mathematics_for_Machine_Learning.html">302 hunch net-2008-05-25-Inappropriate Mathematics for Machine Learning</a></p>
<p>Introduction: Reviewers and students are sometimes greatly concerned by the distinction
between:Anopen setand aclosed set.ASupremumand aMaximum.An event which happens
with probability 1 and an event that always happens.I don't appreciate this
distinction in machine learning & learning theory. All machine learning takes
place (by definition) on a machine where every parameter has finite precision.
Consequently, every set is closed, a maximal element always exists, and
probability 1 events always happen.The fundamental issue here is that
substantial parts of mathematics don't appear well-matched to computation in
the physical world, because the mathematics has concerns which are unphysical.
This mismatched mathematics makes irrelevant distinctions. We can ask "what
mathematics is appropriate to computation?"Andrejhas convinced me that a
pretty good answer to this question isconstructive mathematics.So, here's a
basic challenge: Can anyone name a situation where any of the distinctions
above (or simila</p><p>4 0.08974389 <a title="222-tfidf-4" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>Introduction: Claireasked me to be on the SODA program committee this year, which was quite
a bit of work.I had a relatively light load--merely 49 theory papers. Many of
these papers were not on subjects that I was expert about, so (as is common
for theory conferences) I found various reviewers that I trusted to help
review the papers. I ended up reviewing about 1/3 personally. There were a
couple instances where I ended up overruling a subreviewer whose logic seemed
off, but otherwise I generally let their reviews stand.There are some
differences in standards for paper reviews between the machine learning and
theory communities. In machine learning it is expected that a review be
detailed, while in the theory community this is often not the case. Every
paper given to me ended up with a review varying between somewhat and very
detailed.I'm sure not every author was happy with the outcome. While we did
our best to make good decisions, they were difficult decisions to make. For
example, if there is a</p><p>5 0.086036153 <a title="222-tfidf-5" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">23 hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<p>Introduction: This is apaperby Yann LeCun and Fu Jie Huang published atAISTAT 2005. I found
this paper very difficult to read, but it does have some point about a
computational shortcut.This paper takes for granted that the method of solving
a problem is gradient descent on parameters. Given this assumption, the
question arises: Do you want to do gradient descent on a probabilistic model
or something else?All (conditional) probabilistic models have the formp(y|x) =
f(x,y)/Z(x)whereZ(x) = sumyf(x,y)(the paper calls- log f(x,y)an "energy").
Iffis parameterized by somew, the gradient has a term forZ(x), and hence for
every value ofy. The paper claims, that such models can be optimized for
classification purposes using only the correctyand the othery' not ywhich
maximizesf(x,y). This can even be done on unnormalizable models. The paper
further claims that this can be done with an approximate maximum. These claims
are plausible based on experimental results and intuition.It wouldn't surprise
me to learn</p><p>6 0.076527223 <a title="222-tfidf-6" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>7 0.074982405 <a title="222-tfidf-7" href="../hunch_net-2009/hunch_net-2009-08-03-Carbon_in_Computer_Science_Research.html">366 hunch net-2009-08-03-Carbon in Computer Science Research</a></p>
<p>8 0.073771141 <a title="222-tfidf-8" href="../hunch_net-2007/hunch_net-2007-04-13-What_to_do_with_an_unreasonable_conditional_accept.html">238 hunch net-2007-04-13-What to do with an unreasonable conditional accept</a></p>
<p>9 0.073654719 <a title="222-tfidf-9" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>10 0.073612571 <a title="222-tfidf-10" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>11 0.067887641 <a title="222-tfidf-11" href="../hunch_net-2012/hunch_net-2012-09-29-Vowpal_Wabbit%2C_version_7.0.html">473 hunch net-2012-09-29-Vowpal Wabbit, version 7.0</a></p>
<p>12 0.066512935 <a title="222-tfidf-12" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>13 0.06382338 <a title="222-tfidf-13" href="../hunch_net-2012/hunch_net-2012-05-12-ICML_accepted_papers_and_early_registration.html">465 hunch net-2012-05-12-ICML accepted papers and early registration</a></p>
<p>14 0.061633836 <a title="222-tfidf-14" href="../hunch_net-2007/hunch_net-2007-11-02-The_Machine_Learning_Award_goes_to_%26%238230%3B.html">270 hunch net-2007-11-02-The Machine Learning Award goes to &#8230;</a></p>
<p>15 0.061300047 <a title="222-tfidf-15" href="../hunch_net-2006/hunch_net-2006-04-27-Conferences%2C_Workshops%2C_and_Tutorials.html">174 hunch net-2006-04-27-Conferences, Workshops, and Tutorials</a></p>
<p>16 0.061199471 <a title="222-tfidf-16" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>17 0.060755678 <a title="222-tfidf-17" href="../hunch_net-2006/hunch_net-2006-10-04-Health_of_Conferences_Wiki.html">212 hunch net-2006-10-04-Health of Conferences Wiki</a></p>
<p>18 0.060620904 <a title="222-tfidf-18" href="../hunch_net-2009/hunch_net-2009-01-07-Interesting_Papers_at_SODA_2009.html">334 hunch net-2009-01-07-Interesting Papers at SODA 2009</a></p>
<p>19 0.059597582 <a title="222-tfidf-19" href="../hunch_net-2013/hunch_net-2013-05-04-COLT_and_ICML_registration.html">482 hunch net-2013-05-04-COLT and ICML registration</a></p>
<p>20 0.058272906 <a title="222-tfidf-20" href="../hunch_net-2006/hunch_net-2006-07-12-Who_is_having_visa_problems_reaching_US_conferences%3F.html">195 hunch net-2006-07-12-Who is having visa problems reaching US conferences?</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.126), (1, 0.053), (2, 0.043), (3, -0.022), (4, 0.045), (5, 0.002), (6, -0.007), (7, 0.025), (8, -0.013), (9, 0.037), (10, -0.004), (11, 0.031), (12, -0.004), (13, -0.06), (14, -0.026), (15, 0.018), (16, 0.016), (17, 0.02), (18, 0.01), (19, 0.0), (20, 0.019), (21, 0.034), (22, -0.07), (23, -0.032), (24, 0.024), (25, 0.032), (26, 0.008), (27, 0.072), (28, -0.003), (29, 0.026), (30, 0.003), (31, -0.006), (32, 0.015), (33, 0.066), (34, 0.06), (35, -0.044), (36, -0.062), (37, 0.004), (38, -0.009), (39, 0.054), (40, -0.016), (41, -0.092), (42, 0.077), (43, 0.042), (44, -0.11), (45, -0.019), (46, 0.034), (47, 0.042), (48, -0.077), (49, 0.067)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9742853 <a title="222-lsi-1" href="../hunch_net-2006/hunch_net-2006-12-05-Recruitment_Conferences.html">222 hunch net-2006-12-05-Recruitment Conferences</a></p>
<p>Introduction: One of the subsidiary roles of conferences is recruitment.NIPSis optimally
placed in time for this because it falls right before the major recruitment
season.I personally found job hunting embarrassing, and was relatively inept
at it. I expect this is true of many people, because it is not something done
often.The basic rule is: make the plausible hirers aware of your interest.
Anycorporate sponsoris a "plausible", regardless of whether or not there is a
booth.CRAand theacm job centerare other reasonable sources.There are
substantial differences between the different possibilities. Putting some
effort into understanding the distinctions is a good idea, although you should
always remember where the other person is coming from.</p><p>2 0.54646075 <a title="222-lsi-2" href="../hunch_net-2009/hunch_net-2009-08-03-Carbon_in_Computer_Science_Research.html">366 hunch net-2009-08-03-Carbon in Computer Science Research</a></p>
<p>Introduction: Al Gore'sfilmand gradually more assertive and thorough science has managed to
mostly shift the debate on climate change from "Is it happening?" to "What
should be done?" In that context, it's worthwhile to think a bit about what
can be done within computer science research.There are two things we can think
about:Doing ResearchAt a cartoon level, computer science research consists of
some combination of commuting to&from; work, writing programs, running them on
computers, writing papers, and presenting them at conferences. A typical
computer has a power usage on the order of 100 Watts, which works out to 2.4
kiloWatt-hours/day. Looking upDavid MacKay'sreference on power usage per
person, it becomes clear that this is a relatively minor part of the
lifestyle, although it could become substantial if many more computers are
required. Much larger costs are associated with commuting (which is in common
with many people) and attending conferences. Since local commuting is common
across many pe</p><p>3 0.4952364 <a title="222-lsi-3" href="../hunch_net-2007/hunch_net-2007-07-13-The_View_From_China.html">255 hunch net-2007-07-13-The View From China</a></p>
<p>Introduction: I'm visiting Beijing for thePao-Lu Hsu Statistics Conferenceon Machine
Learning.I had several discussions about the state of Chinese research. Given
the large population and economy, you might expect substantial research--more
than has been observed at international conferences. The fundamental problem
seems to be theCultural Revolutionwhich lobotimized higher education, and the
research associated with it. There has been a process of slow recovery since
then, which has begun to be felt in the research world via increased
participation in international conferences and (now) conferences in China.The
amount of effort going into construction in Beijing is very impressive--people
are literally building a skyscraper at night outside the window of the hotel
I'm staying at (and this is not unusual). If a small fraction of this effort
is later focused onto supporting research, the effect could be very
substantial. General growth in China's research portfolio should be expected.</p><p>4 0.49103677 <a title="222-lsi-4" href="../hunch_net-2007/hunch_net-2007-07-28-Asking_questions.html">257 hunch net-2007-07-28-Asking questions</a></p>
<p>Introduction: There are very substantial differences in how question asking is viewed
culturally. For example, all of the following are common:If no one asks a
question, then no one is paying attention.To ask a question is disrespectful
of the speaker.Asking a question is admitting your own ignorance.The first
view seems to be the right one for research, for several reasons.Research is
quite hard--it's difficult to guess how people won't understand something in
advance while preparing a presentation. Consequently, it's very common to lose
people. No worthwhile presenter wants that.Real understanding is precious. By
asking a question, you are really declaring "I want to understand", and
everyone should respect that.Asking a question wakes you up. I don't mean from
"asleep" to "awake" but from "awake" to "really awake". It's easy to drift
through something sort-of-understanding. When you ask a question, especially
because you are on the spot, you will do much better.Some of these effects
might seem mi</p><p>5 0.49090812 <a title="222-lsi-5" href="../hunch_net-2007/hunch_net-2007-02-10-Best_Practices_for_Collaboration.html">231 hunch net-2007-02-10-Best Practices for Collaboration</a></p>
<p>Introduction: Many people, especially students, haven't had an opportunity to collaborate
with other researchers. Collaboration, especially with remote people can be
tricky. Here are some observations of what has worked for me on collaborations
involving a few people.Travel and DiscussAlmost all collaborations start with
in-person discussion. This implies that travel is often necessary. We can hope
that in the future we'll have better systems for starting collaborations
remotely (such as blogs), but we aren't quite there yet.Enable your
collaborator. A collaboration can fall apart because one collaborator disables
another. This sounds stupid (and it is), but it's far easier than you might
think.Avoid Duplication. Discovering that you and a collaborator have been
editing the same thing and now need to waste time reconciling changes is
annoying. The best way to avoid this to be explicit about who has write
permission to what. Most of the time, a write lock is held for the entire
document, just to be s</p><p>6 0.47582921 <a title="222-lsi-6" href="../hunch_net-2008/hunch_net-2008-04-12-It_Doesn%26%238217%3Bt_Stop.html">295 hunch net-2008-04-12-It Doesn&#8217;t Stop</a></p>
<p>7 0.4636696 <a title="222-lsi-7" href="../hunch_net-2011/hunch_net-2011-11-26-Giving_Thanks.html">449 hunch net-2011-11-26-Giving Thanks</a></p>
<p>8 0.45837086 <a title="222-lsi-8" href="../hunch_net-2005/hunch_net-2005-09-04-Science_in_the_Government.html">106 hunch net-2005-09-04-Science in the Government</a></p>
<p>9 0.45543948 <a title="222-lsi-9" href="../hunch_net-2005/hunch_net-2005-05-11-Visa_Casualties.html">69 hunch net-2005-05-11-Visa Casualties</a></p>
<p>10 0.45131934 <a title="222-lsi-10" href="../hunch_net-2005/hunch_net-2005-04-10-Is_the_Goal_Understanding_or_Prediction%3F.html">55 hunch net-2005-04-10-Is the Goal Understanding or Prediction?</a></p>
<p>11 0.4449164 <a title="222-lsi-11" href="../hunch_net-2007/hunch_net-2007-11-14-BellKor_wins_Netflix.html">272 hunch net-2007-11-14-BellKor wins Netflix</a></p>
<p>12 0.44313756 <a title="222-lsi-12" href="../hunch_net-2005/hunch_net-2005-07-01-The_Role_of_Impromptu_Talks.html">88 hunch net-2005-07-01-The Role of Impromptu Talks</a></p>
<p>13 0.43579921 <a title="222-lsi-13" href="../hunch_net-2006/hunch_net-2006-07-12-Who_is_having_visa_problems_reaching_US_conferences%3F.html">195 hunch net-2006-07-12-Who is having visa problems reaching US conferences?</a></p>
<p>14 0.42663005 <a title="222-lsi-14" href="../hunch_net-2006/hunch_net-2006-01-06-MLTV.html">146 hunch net-2006-01-06-MLTV</a></p>
<p>15 0.42555022 <a title="222-lsi-15" href="../hunch_net-2007/hunch_net-2007-07-20-Motivation_should_be_the_Responsibility_of_the_Reviewer.html">256 hunch net-2007-07-20-Motivation should be the Responsibility of the Reviewer</a></p>
<p>16 0.41639799 <a title="222-lsi-16" href="../hunch_net-2008/hunch_net-2008-01-06-Research_Political_Issues.html">282 hunch net-2008-01-06-Research Political Issues</a></p>
<p>17 0.41592085 <a title="222-lsi-17" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>18 0.41357332 <a title="222-lsi-18" href="../hunch_net-2006/hunch_net-2006-12-04-Structural_Problems_in_NIPS_Decision_Making.html">221 hunch net-2006-12-04-Structural Problems in NIPS Decision Making</a></p>
<p>19 0.40566799 <a title="222-lsi-19" href="../hunch_net-2009/hunch_net-2009-06-01-Multitask_Poisoning.html">358 hunch net-2009-06-01-Multitask Poisoning</a></p>
<p>20 0.40192679 <a title="222-lsi-20" href="../hunch_net-2005/hunch_net-2005-02-25-Why_Papers%3F.html">30 hunch net-2005-02-25-Why Papers?</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(42, 0.174), (57, 0.526), (74, 0.154)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.93151891 <a title="222-lda-1" href="../hunch_net-2012/hunch_net-2012-07-17-MUCMD_and_BayLearn.html">470 hunch net-2012-07-17-MUCMD and BayLearn</a></p>
<p>Introduction: The workshop on theMeaningful Use of Complex Medical Datais happening again,
August 9-12 in LA, nearUAIon Catalina Island August 15-17. I enjoyed my visit
last year, and expect this year to be interesting also.The firstBay Area
Machine Learning Symposiumis August 30 atGoogle. Abstracts are due July 30.</p><p>2 0.91953015 <a title="222-lda-2" href="../hunch_net-2013/hunch_net-2013-03-22-I%26%238217%3Bm_a_bandit.html">480 hunch net-2013-03-22-I&#8217;m a bandit</a></p>
<p>Introduction: Sebastien Bubeck has anew ML blogfocused on optimization and partial feedback
which may interest people.</p><p>same-blog 3 0.82339495 <a title="222-lda-3" href="../hunch_net-2006/hunch_net-2006-12-05-Recruitment_Conferences.html">222 hunch net-2006-12-05-Recruitment Conferences</a></p>
<p>Introduction: One of the subsidiary roles of conferences is recruitment.NIPSis optimally
placed in time for this because it falls right before the major recruitment
season.I personally found job hunting embarrassing, and was relatively inept
at it. I expect this is true of many people, because it is not something done
often.The basic rule is: make the plausible hirers aware of your interest.
Anycorporate sponsoris a "plausible", regardless of whether or not there is a
booth.CRAand theacm job centerare other reasonable sources.There are
substantial differences between the different possibilities. Putting some
effort into understanding the distinctions is a good idea, although you should
always remember where the other person is coming from.</p><p>4 0.52426815 <a title="222-lda-4" href="../hunch_net-2008/hunch_net-2008-11-10-ICML_Reviewing_Criteria.html">325 hunch net-2008-11-10-ICML Reviewing Criteria</a></p>
<p>Introduction: Michael LittmanandLeon Bottouhave decided to use a franchise program chair
approach toreviewing at ICMLthis year. I'll be one of the area chairs, so I
wanted to mention a few things if you are thinking about naming me.I take
reviewing seriously. That means papers to be reviewed are read, the
implications are considered, and decisions are only made after that. I do my
best to be fair, and there are zero subjects that I consider categorical
rejects. I don't consider severalarguments for rejection-not-on-the-merits
reasonable.I am generally interested in papers that (a) analyze new models of
machine learning, (b) provide new algorithms, and (c) show that they work
empirically on plausibly real problems. If a paper has the trifecta, I'm
particularly interested. With 2 out of 3, I might be interested. I often find
papers with only one element harder to accept, including papers with just
(a).I'm a bit tough. I rarely jump-up-and-down about a paper, because I
believe that great progress is ra</p><p>5 0.45511937 <a title="222-lda-5" href="../hunch_net-2012/hunch_net-2012-06-05-ICML_acceptance_statistics.html">466 hunch net-2012-06-05-ICML acceptance statistics</a></p>
<p>Introduction: People are naturally interested in slicing the ICML acceptance statistics in
various ways. Here's a rundown for the top categories.18/66 = 0.27in
(0.18,0.36)Reinforcement Learning10/52 = 0.19in (0.17,0.37)Supervised
Learning9/51 = 0.18not in (0.18, 0.37)Clustering12/46 = 0.26in (0.17,
0.37)Kernel Methods11/40 = 0.28in (0.15, 0.4)Optimization Algorithms8/33 =
0.24in (0.15, 0.39)Learning Theory14/33 = 0.42not in (0.15, 0.39)Graphical
Models10/32 = 0.31in (0.15, 0.41)Applications (+5 invited)8/29 = 0.28in (0.14,
0.41])Probabilistic Models13/29 = 0.45not in (0.14, 0.41)NN & Deep
Learning8/26 = 0.31in (0.12, 0.42)Transfer and Multi-Task Learning13/25 =
0.52not in (0.12, 0.44)Online Learning5/25 = 0.20in (0.12, 0.44)Active
Learning6/22 = 0.27in (0.14, 0.41)Semi-Supervised Learning7/20 = 0.35in (0.1,
0.45)Statistical Methods4/20 = 0.20in (0.1, 0.45)Sparsity and Compressed
Sensing1/19 = 0.05not in (0.11, 0.42)Ensemble Methods5/18 = 0.28in (0.11,
0.44)Structured Output Prediction4/18 = 0.22in (</p><p>6 0.40181425 <a title="222-lda-6" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>7 0.40095714 <a title="222-lda-7" href="../hunch_net-2005/hunch_net-2005-02-17-Learning_Research_Programs.html">21 hunch net-2005-02-17-Learning Research Programs</a></p>
<p>8 0.39206934 <a title="222-lda-8" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>9 0.39114243 <a title="222-lda-9" href="../hunch_net-2007/hunch_net-2007-01-26-Parallel_Machine_Learning_Problems.html">229 hunch net-2007-01-26-Parallel Machine Learning Problems</a></p>
<p>10 0.38894206 <a title="222-lda-10" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>11 0.38847858 <a title="222-lda-11" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>12 0.38638967 <a title="222-lda-12" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>13 0.38605624 <a title="222-lda-13" href="../hunch_net-2006/hunch_net-2006-09-18-What_is_missing_for_online_collaborative_research%3F.html">208 hunch net-2006-09-18-What is missing for online collaborative research?</a></p>
<p>14 0.38552296 <a title="222-lda-14" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>15 0.38417888 <a title="222-lda-15" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>16 0.38141125 <a title="222-lda-16" href="../hunch_net-2006/hunch_net-2006-12-04-Structural_Problems_in_NIPS_Decision_Making.html">221 hunch net-2006-12-04-Structural Problems in NIPS Decision Making</a></p>
<p>17 0.38136446 <a title="222-lda-17" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>18 0.37964544 <a title="222-lda-18" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>19 0.37854415 <a title="222-lda-19" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>20 0.37844157 <a title="222-lda-20" href="../hunch_net-2006/hunch_net-2006-07-11-New_Models.html">194 hunch net-2006-07-11-New Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
