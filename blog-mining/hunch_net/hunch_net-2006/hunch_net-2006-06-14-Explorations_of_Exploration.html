<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>183 hunch net-2006-06-14-Explorations of Exploration</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-183" href="#">hunch_net-2006-183</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>183 hunch net-2006-06-14-Explorations of Exploration</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-183-html" href="http://hunch.net/?p=195">html</a></p><p>Introduction: Exploration is one of the big unsolved problems in machine learning. This
isn't for lack of trying--there are many models of exploration which have been
analyzed in many different ways by many different groups of people. At some
point, it is worthwhile to sit back and see what has been done across these
many models.Reinforcement Learning(1). Reinforcement learning has
traditionally focused on Markov Decision Processes where the next states'is
given by a conditional distributionP(s'|s,a)given the current statesand
actiona. The typical result here is that certain specific algorithms
controlling an agent can behave withineof optimal for horizonTexcept
forpoly(1/e,T,S,A)"wasted" experiences (with high probability). This started
withE3bySatinder SinghandMichael Kearns.Sham Kakade's thesishas significant
discussion. Extensions have typically been of the form "under extra
assumptions, we can prove more", for exampleFactored-E3andMetric-E3. (It turns
out that the number of wasted samples can b</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 This isn't for lack of trying--there are many models of exploration which have been analyzed in many different ways by many different groups of people. [sent-2, score-0.226]
</p><p>2 The typical result here is that certain specific algorithms controlling an agent can behave withineof optimal for horizonTexcept forpoly(1/e,T,S,A)"wasted" experiences (with high probability). [sent-6, score-0.513]
</p><p>3 (It turns out that the number of wasted samples can beless than the number of bits required to describe an MDP. [sent-10, score-0.79]
</p><p>4 ) A weakness of all these results is that they rely upon (a) assumptions which are often false for real applications, (b) state spaces are too large, and (c) make a gurantee that is rather weak. [sent-11, score-0.492]
</p><p>5 Good performance is only guaranteed after suffering the possibly catastrophic consequences of exploration. [sent-12, score-0.219]
</p><p>6 Several recent papers have been attempting to analyze reinforcement learning via reduction. [sent-14, score-0.229]
</p><p>7 To date, all results are either nonconstructive or involve the use of various hints (oracle access to an optimal policy, the distribution over states of an optimal policy etcâ&euro;Ś) which short-circuit the need to explore. [sent-15, score-0.701]
</p><p>8 Much of the rest of reinforcement learning has something to do with exploration, but it's difficult to summarize succinctly. [sent-18, score-0.229]
</p><p>9 The typical result here says that you can compete well with the best constant action after some wasted actions. [sent-22, score-0.515]
</p><p>10 The exact number of wasted actions varies with the precise setting, but it is typically linear in the number of actions. [sent-23, score-0.57]
</p><p>11 Active Learning(1)The common current use of this term has to do with "selective sampling"=choosing unlabeled samples to label so as to minimize the number of labels required to learn a good predictor (typically a classifier). [sent-25, score-0.581]
</p><p>12 A typical result has the form: Given that your classifier comes from restricted classCand the labeled data distribution obeys some constraint, the number of adaptively labeled samples requiredO(log (1/e))whereeis the error rate. [sent-26, score-1.105]
</p><p>13 (It turns out that theeven noisy distributions are allowed. [sent-27, score-0.183]
</p><p>14 ) The constraints on distributions and hypothesis spaces required to achieve these speedups are often severe. [sent-28, score-0.33]
</p><p>15 The distinguishing difference with respect to selective sampling is that the a labeled can be requested for any unlabeled point (not just those drawn according to some natural distribution). [sent-30, score-0.531]
</p><p>16 Several relatively strong results hold for membership query learning, but there is a significant drawback: it seems that the ability to query for a label on an arbitrary point is not very natural. [sent-31, score-0.843]
</p><p>17 For example, imagine query whether a text document is about sports or politics when the text is generated at random. [sent-32, score-0.5]
</p><p>18 Often, the data generating distribution is assumed to come from some specific parametric family. [sent-34, score-0.217]
</p><p>19 This failure is either by design (making assumptions which are simply rarely met), by failure to prove interesting results, or both. [sent-37, score-0.462]
</p><p>20 Active learning deals with generalization, online learning can deal with adversarial situations, and reinforcement learning deals with the situation where your choices influence what you can later learn. [sent-39, score-0.637]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('wasted', 0.268), ('exploration', 0.226), ('query', 0.217), ('reinforcement', 0.159), ('membership', 0.153), ('hints', 0.153), ('typical', 0.152), ('labeled', 0.138), ('distribution', 0.134), ('selective', 0.134), ('deals', 0.134), ('assumptions', 0.124), ('spaces', 0.118), ('samples', 0.117), ('optimal', 0.112), ('number', 0.111), ('results', 0.107), ('required', 0.106), ('distributions', 0.106), ('text', 0.106), ('design', 0.095), ('result', 0.095), ('sampling', 0.095), ('unlabeled', 0.094), ('policy', 0.083), ('specific', 0.083), ('unfortunately', 0.081), ('failure', 0.081), ('prove', 0.081), ('typically', 0.08), ('label', 0.079), ('back', 0.078), ('turns', 0.077), ('obeys', 0.077), ('arms', 0.077), ('suffering', 0.077), ('current', 0.074), ('classifier', 0.072), ('state', 0.072), ('sketchy', 0.071), ('payoffs', 0.071), ('adaptively', 0.071), ('experiences', 0.071), ('politics', 0.071), ('catastrophic', 0.071), ('distributionp', 0.071), ('guaranteed', 0.071), ('weakness', 0.071), ('learning', 0.07), ('point', 0.07)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999982 <a title="183-tfidf-1" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>Introduction: Exploration is one of the big unsolved problems in machine learning. This
isn't for lack of trying--there are many models of exploration which have been
analyzed in many different ways by many different groups of people. At some
point, it is worthwhile to sit back and see what has been done across these
many models.Reinforcement Learning(1). Reinforcement learning has
traditionally focused on Markov Decision Processes where the next states'is
given by a conditional distributionP(s'|s,a)given the current statesand
actiona. The typical result here is that certain specific algorithms
controlling an agent can behave withineof optimal for horizonTexcept
forpoly(1/e,T,S,A)"wasted" experiences (with high probability). This started
withE3bySatinder SinghandMichael Kearns.Sham Kakade's thesishas significant
discussion. Extensions have typically been of the form "under extra
assumptions, we can prove more", for exampleFactored-E3andMetric-E3. (It turns
out that the number of wasted samples can b</p><p>2 0.21272756 <a title="183-tfidf-2" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>Introduction: One thing which is clear on a little reflection is that there exists a single
master learning problem capable of encoding essentially all learning problems.
This problem is of course a very general sort of reinforcement learning where
the world interacts with an agent as:The world announces an observationx.The
agent makes a choicea.The world announces a rewardr.The goal here is to
maximize the sum of the rewards over the time of the agent. No particular
structure relatingxtoaoratoris implied by this setting so we do not know
effective general algorithms for the agent. It's very easy to prove lower
bounds showing that an agent cannot hope to succeed here--just consider the
case where actions are unrelated to rewards. Nevertheless, there is a real
sense in which essentially all forms of life are agents operating in this
setting, somehow succeeding. The gap between these observations drives
research--How can we find tractable specializations of the master problem
general enough to provide</p><p>3 0.17889383 <a title="183-tfidf-3" href="../hunch_net-2005/hunch_net-2005-03-22-Active_learning.html">45 hunch net-2005-03-22-Active learning</a></p>
<p>Introduction: Often, unlabeled data is easy to come by but labels are expensive. For
instance, if you're building a speech recognizer, it's easy enough to get raw
speech samples -- just walk around with a microphone -- but labeling even one
of these samples is a tedious process in which a human must examine the speech
signal and carefully segment it into phonemes. In the field of active
learning, the goal is as usual to construct an accurate classifier, but the
labels of the data points are initially hidden and there is a charge for each
label you want revealed. The hope is that by intelligent adaptive querying,
you can get away with significantly fewer labels than you would need in a
regular supervised learning framework.Here's an example. Suppose the data lie
on the real line, and the classifiers are simple thresholding functions, H =
{hw}:hw(x) = 1 if x > w, and 0 otherwise.VC theory tells us that if the
underlying distribution P can be classified perfectly by some hypothesis in H
(called thereal</p><p>4 0.17199455 <a title="183-tfidf-4" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>Introduction: Attempts to abstract and study machine learning are within some given
framework or mathematical model. It turns out that all of these models are
significantly flawed for the purpose of studying machine learning. I've
created a table (below) outlining the major flaws in some common models of
machine learning.The point here is not simply "woe unto us". There are several
implications which seem important.The multitude of models is a point of
continuing confusion. It is common for people to learn about machine learning
within one framework which often becomes there "home framework" through which
they attempt to filter all machine learning. (Have you met people who can only
think in terms of kernels? Only via Bayes Law? Only via PAC Learning?)
Explicitly understanding the existence of these other frameworks can help
resolve the confusion. This is particularly important when reviewing and
particularly important for students.Algorithms which conform to multiple
approaches can have substantial</p><p>5 0.1607419 <a title="183-tfidf-5" href="../hunch_net-2005/hunch_net-2005-11-02-Progress_in_Active_Learning.html">127 hunch net-2005-11-02-Progress in Active Learning</a></p>
<p>Introduction: Several bits of progress have been made sinceSanjoypointed out the
significantlack of theoretical understanding of active learning. This is an
update on the progress I know of. As a refresher, active learning as meant
here is:There is a source of unlabeled data.There is an oracle from which
labels can be requested for unlabeled data produced by the source.The goal is
to perform well with minimal use of the oracle.Here is what I've
learned:Sanjoy has developed sufficient and semi-necessary conditions for
active learning given the assumptions of IID data and "realizability" (that
one of the classifiers is a correct classifier).Nina,Alina, and I developed an
algorithm for active learning relying on only the assumption of IID data. A
draft ishere.Nicolo,Claudio, andLucashowed that it is possible to do active
learning in an entirely adversarial setting for linear threshold
classifiershere. This was published a year or two ago and I recently learned
about it.All of these results are relative</p><p>6 0.14080015 <a title="183-tfidf-6" href="../hunch_net-2005/hunch_net-2005-12-07-Is_the_Google_way_the_way_for_machine_learning%3F.html">136 hunch net-2005-12-07-Is the Google way the way for machine learning?</a></p>
<p>7 0.13859747 <a title="183-tfidf-7" href="../hunch_net-2005/hunch_net-2005-02-23-Problem%3A_Reinforcement_Learning_with_Classification.html">27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</a></p>
<p>8 0.13364339 <a title="183-tfidf-8" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>9 0.12659526 <a title="183-tfidf-9" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>10 0.12501974 <a title="183-tfidf-10" href="../hunch_net-2006/hunch_net-2006-01-13-Benchmarks_for_RL.html">148 hunch net-2006-01-13-Benchmarks for RL</a></p>
<p>11 0.1239436 <a title="183-tfidf-11" href="../hunch_net-2008/hunch_net-2008-09-12-How_do_we_get_weak_action_dependence_for_learning_with_partial_observations%3F.html">317 hunch net-2008-09-12-How do we get weak action dependence for learning with partial observations?</a></p>
<p>12 0.12387073 <a title="183-tfidf-12" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>13 0.12243473 <a title="183-tfidf-13" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<p>14 0.122196 <a title="183-tfidf-14" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>15 0.12201538 <a title="183-tfidf-15" href="../hunch_net-2007/hunch_net-2007-02-02-Thoughts_regarding_%26%238220%3BIs_machine_learning_different_from_statistics%3F%26%238221%3B.html">230 hunch net-2007-02-02-Thoughts regarding &#8220;Is machine learning different from statistics?&#8221;</a></p>
<p>16 0.1213684 <a title="183-tfidf-16" href="../hunch_net-2007/hunch_net-2007-11-28-Computational_Consequences_of_Classification.html">274 hunch net-2007-11-28-Computational Consequences of Classification</a></p>
<p>17 0.12096879 <a title="183-tfidf-17" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>18 0.11959302 <a title="183-tfidf-18" href="../hunch_net-2006/hunch_net-2006-04-05-What_is_state%3F.html">169 hunch net-2006-04-05-What is state?</a></p>
<p>19 0.11383069 <a title="183-tfidf-19" href="../hunch_net-2005/hunch_net-2005-04-16-Which_Assumptions_are_Reasonable%3F.html">57 hunch net-2005-04-16-Which Assumptions are Reasonable?</a></p>
<p>20 0.11379465 <a title="183-tfidf-20" href="../hunch_net-2005/hunch_net-2005-08-08-Apprenticeship_Reinforcement_Learning_for_Control.html">101 hunch net-2005-08-08-Apprenticeship Reinforcement Learning for Control</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.293), (1, -0.146), (2, -0.017), (3, -0.016), (4, -0.102), (5, -0.065), (6, -0.024), (7, 0.002), (8, -0.095), (9, -0.056), (10, -0.05), (11, -0.006), (12, 0.029), (13, 0.042), (14, -0.001), (15, -0.046), (16, 0.012), (17, 0.027), (18, -0.052), (19, 0.043), (20, -0.027), (21, 0.023), (22, 0.056), (23, 0.004), (24, 0.028), (25, 0.009), (26, 0.029), (27, 0.111), (28, 0.015), (29, -0.005), (30, 0.03), (31, 0.044), (32, 0.126), (33, 0.002), (34, -0.093), (35, -0.013), (36, 0.058), (37, -0.112), (38, -0.003), (39, -0.007), (40, 0.046), (41, -0.096), (42, 0.014), (43, -0.073), (44, -0.012), (45, 0.016), (46, -0.044), (47, 0.004), (48, -0.02), (49, -0.003)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.93707913 <a title="183-lsi-1" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>Introduction: Exploration is one of the big unsolved problems in machine learning. This
isn't for lack of trying--there are many models of exploration which have been
analyzed in many different ways by many different groups of people. At some
point, it is worthwhile to sit back and see what has been done across these
many models.Reinforcement Learning(1). Reinforcement learning has
traditionally focused on Markov Decision Processes where the next states'is
given by a conditional distributionP(s'|s,a)given the current statesand
actiona. The typical result here is that certain specific algorithms
controlling an agent can behave withineof optimal for horizonTexcept
forpoly(1/e,T,S,A)"wasted" experiences (with high probability). This started
withE3bySatinder SinghandMichael Kearns.Sham Kakade's thesishas significant
discussion. Extensions have typically been of the form "under extra
assumptions, we can prove more", for exampleFactored-E3andMetric-E3. (It turns
out that the number of wasted samples can b</p><p>2 0.73821241 <a title="183-lsi-2" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>Introduction: One thing which is clear on a little reflection is that there exists a single
master learning problem capable of encoding essentially all learning problems.
This problem is of course a very general sort of reinforcement learning where
the world interacts with an agent as:The world announces an observationx.The
agent makes a choicea.The world announces a rewardr.The goal here is to
maximize the sum of the rewards over the time of the agent. No particular
structure relatingxtoaoratoris implied by this setting so we do not know
effective general algorithms for the agent. It's very easy to prove lower
bounds showing that an agent cannot hope to succeed here--just consider the
case where actions are unrelated to rewards. Nevertheless, there is a real
sense in which essentially all forms of life are agents operating in this
setting, somehow succeeding. The gap between these observations drives
research--How can we find tractable specializations of the master problem
general enough to provide</p><p>3 0.68149775 <a title="183-lsi-3" href="../hunch_net-2005/hunch_net-2005-03-22-Active_learning.html">45 hunch net-2005-03-22-Active learning</a></p>
<p>Introduction: Often, unlabeled data is easy to come by but labels are expensive. For
instance, if you're building a speech recognizer, it's easy enough to get raw
speech samples -- just walk around with a microphone -- but labeling even one
of these samples is a tedious process in which a human must examine the speech
signal and carefully segment it into phonemes. In the field of active
learning, the goal is as usual to construct an accurate classifier, but the
labels of the data points are initially hidden and there is a charge for each
label you want revealed. The hope is that by intelligent adaptive querying,
you can get away with significantly fewer labels than you would need in a
regular supervised learning framework.Here's an example. Suppose the data lie
on the real line, and the classifiers are simple thresholding functions, H =
{hw}:hw(x) = 1 if x > w, and 0 otherwise.VC theory tells us that if the
underlying distribution P can be classified perfectly by some hypothesis in H
(called thereal</p><p>4 0.67836279 <a title="183-lsi-4" href="../hunch_net-2005/hunch_net-2005-08-08-Apprenticeship_Reinforcement_Learning_for_Control.html">101 hunch net-2005-08-08-Apprenticeship Reinforcement Learning for Control</a></p>
<p>Introduction: Pieter Abbeelpresented a paper withAndrew NgatICMLonExploration and
Apprenticeship Learning in Reinforcement Learning. The basic idea of this
algorithm is:Collect data from a human controlling a machine.Build a
transition model based upon the experience.Build a policy which optimizes the
transition model.Evaluate the policy. If it works well, halt, otherwise add
the experience into the pool and go to (2).The paper proves that this
technique will converge to some policy with expected performance near human
expected performance assuming the world fits certain assumptions (MDP or
linear dynamics).This general idea of apprenticeship learning (i.e.
incorporating data from an expert) seems very compelling because (a) humans
often learn this way and (b) much harder problems can be solved. For (a), the
notion of teaching is about transferring knowledge from an expert to novices,
often via demonstration. To see (b), note that we can create intricate
reinforcement learning problems where a parti</p><p>5 0.65855414 <a title="183-lsi-5" href="../hunch_net-2005/hunch_net-2005-11-02-Progress_in_Active_Learning.html">127 hunch net-2005-11-02-Progress in Active Learning</a></p>
<p>Introduction: Several bits of progress have been made sinceSanjoypointed out the
significantlack of theoretical understanding of active learning. This is an
update on the progress I know of. As a refresher, active learning as meant
here is:There is a source of unlabeled data.There is an oracle from which
labels can be requested for unlabeled data produced by the source.The goal is
to perform well with minimal use of the oracle.Here is what I've
learned:Sanjoy has developed sufficient and semi-necessary conditions for
active learning given the assumptions of IID data and "realizability" (that
one of the classifiers is a correct classifier).Nina,Alina, and I developed an
algorithm for active learning relying on only the assumption of IID data. A
draft ishere.Nicolo,Claudio, andLucashowed that it is possible to do active
learning in an entirely adversarial setting for linear threshold
classifiershere. This was published a year or two ago and I recently learned
about it.All of these results are relative</p><p>6 0.63974416 <a title="183-lsi-6" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>7 0.63523811 <a title="183-lsi-7" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<p>8 0.62571794 <a title="183-lsi-8" href="../hunch_net-2005/hunch_net-2005-04-16-Which_Assumptions_are_Reasonable%3F.html">57 hunch net-2005-04-16-Which Assumptions are Reasonable?</a></p>
<p>9 0.61361247 <a title="183-lsi-9" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>10 0.60696048 <a title="183-lsi-10" href="../hunch_net-2005/hunch_net-2005-01-31-Watchword%3A_Assumption.html">7 hunch net-2005-01-31-Watchword: Assumption</a></p>
<p>11 0.5947687 <a title="183-lsi-11" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>12 0.58673525 <a title="183-lsi-12" href="../hunch_net-2006/hunch_net-2006-05-01-A_conversation_between_Theo_and_Pat.html">176 hunch net-2006-05-01-A conversation between Theo and Pat</a></p>
<p>13 0.576199 <a title="183-lsi-13" href="../hunch_net-2005/hunch_net-2005-04-25-Embeddings%3A_what_are_they_good_for%3F.html">61 hunch net-2005-04-25-Embeddings: what are they good for?</a></p>
<p>14 0.57582098 <a title="183-lsi-14" href="../hunch_net-2006/hunch_net-2006-11-22-Explicit_Randomization_in_Learning_algorithms.html">219 hunch net-2006-11-22-Explicit Randomization in Learning algorithms</a></p>
<p>15 0.57160592 <a title="183-lsi-15" href="../hunch_net-2008/hunch_net-2008-09-12-How_do_we_get_weak_action_dependence_for_learning_with_partial_observations%3F.html">317 hunch net-2008-09-12-How do we get weak action dependence for learning with partial observations?</a></p>
<p>16 0.56811261 <a title="183-lsi-16" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<p>17 0.56615549 <a title="183-lsi-17" href="../hunch_net-2005/hunch_net-2005-02-25-Problem%3A_Online_Learning.html">28 hunch net-2005-02-25-Problem: Online Learning</a></p>
<p>18 0.56066954 <a title="183-lsi-18" href="../hunch_net-2007/hunch_net-2007-07-01-Watchword%3A_Online_Learning.html">252 hunch net-2007-07-01-Watchword: Online Learning</a></p>
<p>19 0.55663925 <a title="183-lsi-19" href="../hunch_net-2007/hunch_net-2007-10-24-Contextual_Bandits.html">269 hunch net-2007-10-24-Contextual Bandits</a></p>
<p>20 0.55570775 <a title="183-lsi-20" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.019), (6, 0.028), (16, 0.035), (23, 0.133), (35, 0.07), (42, 0.263), (45, 0.026), (50, 0.036), (68, 0.073), (74, 0.126), (76, 0.044), (88, 0.023), (95, 0.041)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94757664 <a title="183-lda-1" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>Introduction: Exploration is one of the big unsolved problems in machine learning. This
isn't for lack of trying--there are many models of exploration which have been
analyzed in many different ways by many different groups of people. At some
point, it is worthwhile to sit back and see what has been done across these
many models.Reinforcement Learning(1). Reinforcement learning has
traditionally focused on Markov Decision Processes where the next states'is
given by a conditional distributionP(s'|s,a)given the current statesand
actiona. The typical result here is that certain specific algorithms
controlling an agent can behave withineof optimal for horizonTexcept
forpoly(1/e,T,S,A)"wasted" experiences (with high probability). This started
withE3bySatinder SinghandMichael Kearns.Sham Kakade's thesishas significant
discussion. Extensions have typically been of the form "under extra
assumptions, we can prove more", for exampleFactored-E3andMetric-E3. (It turns
out that the number of wasted samples can b</p><p>2 0.94154698 <a title="183-lda-2" href="../hunch_net-2007/hunch_net-2007-09-18-It%26%238217%3Bs_MDL_Jim%2C_but_not_as_we_know_it%26%238230%3B%28on_Bayes%2C_MDL_and_consistency%29.html">263 hunch net-2007-09-18-It&#8217;s MDL Jim, but not as we know it&#8230;(on Bayes, MDL and consistency)</a></p>
<p>Introduction: I have recently completeda 500+ page-book on MDL, the first comprehensive
overview of the field (yes, this is a sneak advertisement).Chapter 17compares
MDL to a menagerie of other methods and paradigms for learning and statistics.
By far the most time (20 pages) is spent on the relation between MDL and
Bayes. My two main points here are:In sharp contrast to Bayes, MDL is by
definition based on designing universal codes for the data relative to some
given (parametric or nonparametric) probabilistic model M. By some theorems
due toAndrew Barron, MDL inferencemusttherefore be statistically consistent,
and it is immune to Bayesian inconsistency results such as those by Diaconis,
Freedman and Barron (I explain what I mean by "inconsistency" further below).
Hence, MDL must be different from Bayes!In contrast to what has sometimes been
claimed, practical MDL algorithms do have a subjective component (which in
many, but not all cases, may be implemented by something similar to a Bayesian
prior</p><p>3 0.91434097 <a title="183-lda-3" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>Introduction: This post is partly meant as an advertisement for thereductions
tutorialAlina,Bianca, and I are planning to do atICML. Please come, if you are
interested.Many research programs can be thought of as finding and building
new useful abstractions. The running example I'll use islearning
reductionswhere I have experience. The basic abstraction here is that we can
build a learning algorithm capable of solving classification problems up to a
small expected regret. This is used repeatedly to solve more complex
problems.In working on a new abstraction, I think you typically run into many
substantial problems of understanding, which make publishing particularly
difficult.It is difficult to seriously discuss the reason behind or mechanism
for abstraction in a conference paper with small page limits. People rarely
see such discussions and hence have little basis on which to think about new
abstractions. Another difficulty is that when building an abstraction, you
often don't know the right way to</p><p>4 0.8946467 <a title="183-lda-4" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>Introduction: Attempts to abstract and study machine learning are within some given
framework or mathematical model. It turns out that all of these models are
significantly flawed for the purpose of studying machine learning. I've
created a table (below) outlining the major flaws in some common models of
machine learning.The point here is not simply "woe unto us". There are several
implications which seem important.The multitude of models is a point of
continuing confusion. It is common for people to learn about machine learning
within one framework which often becomes there "home framework" through which
they attempt to filter all machine learning. (Have you met people who can only
think in terms of kernels? Only via Bayes Law? Only via PAC Learning?)
Explicitly understanding the existence of these other frameworks can help
resolve the confusion. This is particularly important when reviewing and
particularly important for students.Algorithms which conform to multiple
approaches can have substantial</p><p>5 0.88475752 <a title="183-lda-5" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>Introduction: Normally, I don't indulge in posters forICML, but this year is naturally an
exception for me. If you want one, there are a small numberleft here, if you
sign up before February.It also seems worthwhile to give some sense of the
scope and reviewing criteria for ICML for authors considering submitting
papers. At ICML, the (very large) program committee does the reviewing which
informs final decisions by area chairs on most papers. Program chairs setup
the process, deal with exceptions or disagreements, and provide advice for the
reviewing process. Providing advice is tricky (and easily misleading) because
a conference is a community, and in the end the aggregate interests of the
community determine the conference. Nevertheless, as a program chair this year
it seems worthwhile to state the overall philosophy I have and what I plan to
encourage (and occasionally discourage).At the highest level, I believe ICML
exists to further research into machine learning, which I generally think of
as</p><p>6 0.88269293 <a title="183-lda-6" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>7 0.88144094 <a title="183-lda-7" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>8 0.8807534 <a title="183-lda-8" href="../hunch_net-2005/hunch_net-2005-03-17-Going_all_the_Way%2C_Sometimes.html">42 hunch net-2005-03-17-Going all the Way, Sometimes</a></p>
<p>9 0.87933689 <a title="183-lda-9" href="../hunch_net-2008/hunch_net-2008-05-23-Three_levels_of_addressing_the_Netflix_Prize.html">301 hunch net-2008-05-23-Three levels of addressing the Netflix Prize</a></p>
<p>10 0.8792243 <a title="183-lda-10" href="../hunch_net-2005/hunch_net-2005-03-09-Bad_Reviewing.html">38 hunch net-2005-03-09-Bad Reviewing</a></p>
<p>11 0.87913477 <a title="183-lda-11" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>12 0.87738466 <a title="183-lda-12" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">23 hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<p>13 0.87607849 <a title="183-lda-13" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>14 0.87567675 <a title="183-lda-14" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<p>15 0.87567359 <a title="183-lda-15" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>16 0.87388575 <a title="183-lda-16" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>17 0.87319601 <a title="183-lda-17" href="../hunch_net-2011/hunch_net-2011-02-17-What_does_Watson_mean%3F.html">424 hunch net-2011-02-17-What does Watson mean?</a></p>
<p>18 0.87311506 <a title="183-lda-18" href="../hunch_net-2007/hunch_net-2007-08-19-Choice_of_Metrics.html">259 hunch net-2007-08-19-Choice of Metrics</a></p>
<p>19 0.8720237 <a title="183-lda-19" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>20 0.87173915 <a title="183-lda-20" href="../hunch_net-2006/hunch_net-2006-05-05-An_ICML_reject.html">177 hunch net-2006-05-05-An ICML reject</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
