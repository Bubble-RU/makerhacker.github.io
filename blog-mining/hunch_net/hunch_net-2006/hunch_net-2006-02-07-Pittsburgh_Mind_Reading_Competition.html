<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>155 hunch net-2006-02-07-Pittsburgh Mind Reading Competition</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-155" href="#">hunch_net-2006-155</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>155 hunch net-2006-02-07-Pittsburgh Mind Reading Competition</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-155-html" href="http://hunch.net/?p=166">html</a></p><p>Introduction: Francisco Pereirapoints out a funPrediction Competition. Francisco says:DARPA
is sponsoring a competition to analyze data from an unusual functional
Magnetic Resonance Imaging experiment. Subjects watch videos inside the
scanner while fMRI data are acquired. Unbeknownst to these subjects, the
videos have been seen by a panel of other subjects that labeled each instant
with labels in categories such as representation (are there tools, body parts,
motion, sound), location, presence of actors, emotional content, etc.The
challenge is to predict all of these different labels on an instant-by-instant
basis from the fMRI data. A few reasons why this is particularly
interesting:This is beyond the current state of the art, but not inconceivably
hard.This is a new type of experiment design current analysis methods cannot
deal with.This is an opportunity to work with a heavily examined and
preprocessed neuroimaging dataset.DARPA is offering prizes!</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('fmri', 0.327), ('subjects', 0.324), ('francisco', 0.303), ('videos', 0.303), ('labels', 0.185), ('panel', 0.164), ('instant', 0.164), ('scanner', 0.164), ('unusual', 0.164), ('current', 0.159), ('actors', 0.151), ('offering', 0.151), ('darpa', 0.143), ('watch', 0.143), ('presence', 0.143), ('sponsoring', 0.143), ('art', 0.136), ('prizes', 0.136), ('inside', 0.126), ('categories', 0.122)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="155-tfidf-1" href="../hunch_net-2006/hunch_net-2006-02-07-Pittsburgh_Mind_Reading_Competition.html">155 hunch net-2006-02-07-Pittsburgh Mind Reading Competition</a></p>
<p>Introduction: Francisco Pereirapoints out a funPrediction Competition. Francisco says:DARPA
is sponsoring a competition to analyze data from an unusual functional
Magnetic Resonance Imaging experiment. Subjects watch videos inside the
scanner while fMRI data are acquired. Unbeknownst to these subjects, the
videos have been seen by a panel of other subjects that labeled each instant
with labels in categories such as representation (are there tools, body parts,
motion, sound), location, presence of actors, emotional content, etc.The
challenge is to predict all of these different labels on an instant-by-instant
basis from the fMRI data. A few reasons why this is particularly
interesting:This is beyond the current state of the art, but not inconceivably
hard.This is a new type of experiment design current analysis methods cannot
deal with.This is an opportunity to work with a heavily examined and
preprocessed neuroimaging dataset.DARPA is offering prizes!</p><p>2 0.23350862 <a title="155-tfidf-2" href="../hunch_net-2013/hunch_net-2013-07-24-ICML_2012_videos_lost.html">487 hunch net-2013-07-24-ICML 2012 videos lost</a></p>
<p>Introduction: A big ouch--all the videos for ICML 2012 were lost in a shuffle. Rajnish sends
the below, but if anyone can help that would be greatly appreciated.
----------------------------------------------------Sincere apologies to ICML
community for loosing 2012 archived videosWhat happened: In order to publish
2013 videos, we decided to move 2012 videos to another server. We have a
weekly backup service from the provider but after removing the videos from the
current server, when we tried to retrieve the 2012 videos from backup service,
the backup did not work because of provider-specific requirements that we had
ignored while removing the data from previous server.What are we doing about
this: At this point, we are still looking into raw footage to find if we can
retrieve some of the videos, but following are the steps we are taking to make
sure this does not happen again in future:(1) We are going to create a channel
on Vimeo (and potentially on YouTube) and we will publish there the p-in-p-</p><p>3 0.10553119 <a title="155-tfidf-3" href="../hunch_net-2008/hunch_net-2008-11-10-ICML_Reviewing_Criteria.html">325 hunch net-2008-11-10-ICML Reviewing Criteria</a></p>
<p>Introduction: Michael LittmanandLeon Bottouhave decided to use a franchise program chair
approach toreviewing at ICMLthis year. I'll be one of the area chairs, so I
wanted to mention a few things if you are thinking about naming me.I take
reviewing seriously. That means papers to be reviewed are read, the
implications are considered, and decisions are only made after that. I do my
best to be fair, and there are zero subjects that I consider categorical
rejects. I don't consider severalarguments for rejection-not-on-the-merits
reasonable.I am generally interested in papers that (a) analyze new models of
machine learning, (b) provide new algorithms, and (c) show that they work
empirically on plausibly real problems. If a paper has the trifecta, I'm
particularly interested. With 2 out of 3, I might be interested. I often find
papers with only one element harder to accept, including papers with just
(a).I'm a bit tough. I rarely jump-up-and-down about a paper, because I
believe that great progress is ra</p><p>4 0.092462562 <a title="155-tfidf-4" href="../hunch_net-2007/hunch_net-2007-12-12-Workshop_Summary%26%238212%3BPrinciples_of_Learning_Problem_Design.html">277 hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</a></p>
<p>Introduction: This is a summary of theworkshop on Learning Problem DesignwhichAlinaand I ran
atNIPSthis year.The first question many people have is "What is learning
problem design?" This workshop is about admitting that solving learning
problems does not start with labeled data, but rather somewhere before. When
humans are hired to produce labels, this is usually not a serious problem
because you can tell them precisely what semantics you want the labels to
have, and we can fix some set of features in advance. However, when other
methods are used this becomes more problematic. This focus is important for
Machine Learning because there are very large quantities of data which are not
labeled by a hired human.The title of the workshop was a bit ambitious,
because a workshop is not long enough to synthesize a diversity of approaches
into a coherent set of principles. For me, the posters at the end of the
workshop were quite helpful in getting approaches to gel.Here are some answers
to "where do the lab</p><p>5 0.083516262 <a title="155-tfidf-5" href="../hunch_net-2005/hunch_net-2005-03-22-Active_learning.html">45 hunch net-2005-03-22-Active learning</a></p>
<p>Introduction: Often, unlabeled data is easy to come by but labels are expensive. For
instance, if you're building a speech recognizer, it's easy enough to get raw
speech samples -- just walk around with a microphone -- but labeling even one
of these samples is a tedious process in which a human must examine the speech
signal and carefully segment it into phonemes. In the field of active
learning, the goal is as usual to construct an accurate classifier, but the
labels of the data points are initially hidden and there is a charge for each
label you want revealed. The hope is that by intelligent adaptive querying,
you can get away with significantly fewer labels than you would need in a
regular supervised learning framework.Here's an example. Suppose the data lie
on the real line, and the classifiers are simple thresholding functions, H =
{hw}:hw(x) = 1 if x > w, and 0 otherwise.VC theory tells us that if the
underlying distribution P can be classified perfectly by some hypothesis in H
(called thereal</p><p>6 0.071058884 <a title="155-tfidf-6" href="../hunch_net-2010/hunch_net-2010-02-26-Yahoo%21_ML_events.html">389 hunch net-2010-02-26-Yahoo! ML events</a></p>
<p>7 0.066567831 <a title="155-tfidf-7" href="../hunch_net-2006/hunch_net-2006-04-05-What_is_state%3F.html">169 hunch net-2006-04-05-What is state?</a></p>
<p>8 0.066086546 <a title="155-tfidf-8" href="../hunch_net-2007/hunch_net-2007-10-19-Second_Annual_Reinforcement_Learning_Competition.html">268 hunch net-2007-10-19-Second Annual Reinforcement Learning Competition</a></p>
<p>9 0.064617127 <a title="155-tfidf-9" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>10 0.060327247 <a title="155-tfidf-10" href="../hunch_net-2005/hunch_net-2005-04-01-Basic_computer_science_research_takes_a_hit.html">50 hunch net-2005-04-01-Basic computer science research takes a hit</a></p>
<p>11 0.060311332 <a title="155-tfidf-11" href="../hunch_net-2005/hunch_net-2005-02-25-Solution%3A_Reinforcement_Learning_with_Classification.html">29 hunch net-2005-02-25-Solution: Reinforcement Learning with Classification</a></p>
<p>12 0.059865542 <a title="155-tfidf-12" href="../hunch_net-2009/hunch_net-2009-12-09-Inherent_Uncertainty.html">383 hunch net-2009-12-09-Inherent Uncertainty</a></p>
<p>13 0.057709627 <a title="155-tfidf-13" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>14 0.05491301 <a title="155-tfidf-14" href="../hunch_net-2011/hunch_net-2011-03-20-KDD_Cup_2011.html">427 hunch net-2011-03-20-KDD Cup 2011</a></p>
<p>15 0.051377825 <a title="155-tfidf-15" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>16 0.04967396 <a title="155-tfidf-16" href="../hunch_net-2011/hunch_net-2011-10-10-ML_Symposium_and_ICML_details.html">447 hunch net-2011-10-10-ML Symposium and ICML details</a></p>
<p>17 0.049122289 <a title="155-tfidf-17" href="../hunch_net-2009/hunch_net-2009-12-27-Interesting_things_at_NIPS_2009.html">385 hunch net-2009-12-27-Interesting things at NIPS 2009</a></p>
<p>18 0.048945263 <a title="155-tfidf-18" href="../hunch_net-2013/hunch_net-2013-06-10-The_Large_Scale_Learning_class_notes.html">483 hunch net-2013-06-10-The Large Scale Learning class notes</a></p>
<p>19 0.047190003 <a title="155-tfidf-19" href="../hunch_net-2008/hunch_net-2008-10-20-New_York%26%238217%3Bs_ML_Day.html">322 hunch net-2008-10-20-New York&#8217;s ML Day</a></p>
<p>20 0.047153361 <a title="155-tfidf-20" href="../hunch_net-2006/hunch_net-2006-04-06-Bounds_greater_than_1.html">170 hunch net-2006-04-06-Bounds greater than 1</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.085), (1, -0.012), (2, 0.033), (3, 0.01), (4, -0.042), (5, 0.006), (6, -0.021), (7, -0.037), (8, 0.061), (9, -0.016), (10, -0.065), (11, 0.056), (12, -0.001), (13, 0.053), (14, 0.036), (15, -0.021), (16, -0.021), (17, -0.01), (18, 0.019), (19, -0.035), (20, 0.036), (21, -0.022), (22, 0.03), (23, -0.009), (24, 0.012), (25, -0.01), (26, -0.072), (27, 0.036), (28, -0.037), (29, 0.034), (30, 0.054), (31, 0.027), (32, 0.057), (33, 0.041), (34, -0.089), (35, -0.072), (36, 0.072), (37, -0.107), (38, 0.003), (39, -0.013), (40, 0.001), (41, -0.093), (42, -0.057), (43, 0.106), (44, 0.032), (45, -0.045), (46, -0.052), (47, -0.016), (48, 0.034), (49, -0.062)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97404808 <a title="155-lsi-1" href="../hunch_net-2006/hunch_net-2006-02-07-Pittsburgh_Mind_Reading_Competition.html">155 hunch net-2006-02-07-Pittsburgh Mind Reading Competition</a></p>
<p>Introduction: Francisco Pereirapoints out a funPrediction Competition. Francisco says:DARPA
is sponsoring a competition to analyze data from an unusual functional
Magnetic Resonance Imaging experiment. Subjects watch videos inside the
scanner while fMRI data are acquired. Unbeknownst to these subjects, the
videos have been seen by a panel of other subjects that labeled each instant
with labels in categories such as representation (are there tools, body parts,
motion, sound), location, presence of actors, emotional content, etc.The
challenge is to predict all of these different labels on an instant-by-instant
basis from the fMRI data. A few reasons why this is particularly
interesting:This is beyond the current state of the art, but not inconceivably
hard.This is a new type of experiment design current analysis methods cannot
deal with.This is an opportunity to work with a heavily examined and
preprocessed neuroimaging dataset.DARPA is offering prizes!</p><p>2 0.58404011 <a title="155-lsi-2" href="../hunch_net-2013/hunch_net-2013-07-24-ICML_2012_videos_lost.html">487 hunch net-2013-07-24-ICML 2012 videos lost</a></p>
<p>Introduction: A big ouch--all the videos for ICML 2012 were lost in a shuffle. Rajnish sends
the below, but if anyone can help that would be greatly appreciated.
----------------------------------------------------Sincere apologies to ICML
community for loosing 2012 archived videosWhat happened: In order to publish
2013 videos, we decided to move 2012 videos to another server. We have a
weekly backup service from the provider but after removing the videos from the
current server, when we tried to retrieve the 2012 videos from backup service,
the backup did not work because of provider-specific requirements that we had
ignored while removing the data from previous server.What are we doing about
this: At this point, we are still looking into raw footage to find if we can
retrieve some of the videos, but following are the steps we are taking to make
sure this does not happen again in future:(1) We are going to create a channel
on Vimeo (and potentially on YouTube) and we will publish there the p-in-p-</p><p>3 0.52990824 <a title="155-lsi-3" href="../hunch_net-2006/hunch_net-2006-02-27-The_Peekaboom_Dataset.html">159 hunch net-2006-02-27-The Peekaboom Dataset</a></p>
<p>Introduction: Luis von Ahn'sPeekaboom projecthas yieldeddata(830MB).Peekaboom is the second
attempt (afterEspgame) to produce a dataset which is useful for learning to
solve vision problems based on voluntary game play. As a second attempt, it is
meant to address all of the shortcomings of the first attempt. In
particular:The locations of specific objects are provided by the data.The data
collection is far more complete and extensive.The data consists of:The source
images. (1 file per image, just short of 60K images.)The in-game events. (1
file per image, in a lispy syntax.)A description of the event language.There
is a great deal of very specific and relevant data here so the hope that this
will help solve vision problems seems quite reasonable.</p><p>4 0.50813431 <a title="155-lsi-4" href="../hunch_net-2005/hunch_net-2005-08-01-Peekaboom.html">99 hunch net-2005-08-01-Peekaboom</a></p>
<p>Introduction: Luishas releasedPeekabooma successor toESPgame(game site). The purpose of the
game is similar--using the actions of people playing a game to gather data
helpful in solving AI.Peekaboom gathers more detailed, and perhaps more
useful, data about vision. For ESPgame, the byproduct of the game was mutually
agreed upon labels for common images. For Peekaboom, the location of the
subimage generating the label is revealed by the game as well. Given knowledge
about what portion of the image is related to a label it may be more feasible
learn to recognize the appropriate parts.There isn't a dataset yet available
for this game as there is for ESPgame, but hopefully a significant number of
people will play and we'll have one to work wtih soon.</p><p>5 0.48441455 <a title="155-lsi-5" href="../hunch_net-2005/hunch_net-2005-04-25-Embeddings%3A_what_are_they_good_for%3F.html">61 hunch net-2005-04-25-Embeddings: what are they good for?</a></p>
<p>Introduction: I've been looking at some recent embeddings work, and am struck by how
beautiful the theory and algorithms are. It also makes me wonder, what are
embeddings good for?A few things immediately come to mind:(1) For
visualization of high-dimensional data sets.In this case, one would like good
algorithms for embedding specifically into 2- and 3-dimensional Euclidean
spaces.(2) For nonparametric modeling.The usual nonparametric models
(histograms, nearest neighbor) often require resources which are exponential
in the dimension. So if the data actually lie close to some low-
dimensionalsurface, it might be a good idea to first identify this surface and
embed the data before applying the model.Incidentally, for applications like
these, it's important to have a functional mapping from high to low dimension,
which some techniques do not yield up.(3) As a prelude to classifier
learning.The hope here is presumably that learning will be easier in the low-
dimensional space, because of (i) better ge</p><p>6 0.47470009 <a title="155-lsi-6" href="../hunch_net-2010/hunch_net-2010-08-24-Alex_Smola_starts_a_blog.html">408 hunch net-2010-08-24-Alex Smola starts a blog</a></p>
<p>7 0.47281069 <a title="155-lsi-7" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>8 0.46178305 <a title="155-lsi-8" href="../hunch_net-2005/hunch_net-2005-08-08-Apprenticeship_Reinforcement_Learning_for_Control.html">101 hunch net-2005-08-08-Apprenticeship Reinforcement Learning for Control</a></p>
<p>9 0.41921085 <a title="155-lsi-9" href="../hunch_net-2006/hunch_net-2006-07-08-Some_recent_papers.html">192 hunch net-2006-07-08-Some recent papers</a></p>
<p>10 0.41018352 <a title="155-lsi-10" href="../hunch_net-2007/hunch_net-2007-10-19-Second_Annual_Reinforcement_Learning_Competition.html">268 hunch net-2007-10-19-Second Annual Reinforcement Learning Competition</a></p>
<p>11 0.40477234 <a title="155-lsi-11" href="../hunch_net-2005/hunch_net-2005-02-15-ESPgame_and_image_labeling.html">20 hunch net-2005-02-15-ESPgame and image labeling</a></p>
<p>12 0.40062031 <a title="155-lsi-12" href="../hunch_net-2005/hunch_net-2005-05-29-Maximum_Margin_Mismatch%3F.html">77 hunch net-2005-05-29-Maximum Margin Mismatch?</a></p>
<p>13 0.3877587 <a title="155-lsi-13" href="../hunch_net-2012/hunch_net-2012-02-20-Berkeley_Streaming_Data_Workshop.html">455 hunch net-2012-02-20-Berkeley Streaming Data Workshop</a></p>
<p>14 0.38067576 <a title="155-lsi-14" href="../hunch_net-2012/hunch_net-2012-08-24-Patterns_for_research_in_machine_learning.html">471 hunch net-2012-08-24-Patterns for research in machine learning</a></p>
<p>15 0.37297541 <a title="155-lsi-15" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>16 0.37062082 <a title="155-lsi-16" href="../hunch_net-2005/hunch_net-2005-12-28-Yet_more_nips_thoughts.html">144 hunch net-2005-12-28-Yet more nips thoughts</a></p>
<p>17 0.36849007 <a title="155-lsi-17" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>18 0.36326241 <a title="155-lsi-18" href="../hunch_net-2005/hunch_net-2005-01-24-The_Humanloop_Spectrum_of_Machine_Learning.html">3 hunch net-2005-01-24-The Humanloop Spectrum of Machine Learning</a></p>
<p>19 0.36018628 <a title="155-lsi-19" href="../hunch_net-2005/hunch_net-2005-09-26-Prediction_Bounds_as_the_Mathematics_of_Science.html">115 hunch net-2005-09-26-Prediction Bounds as the Mathematics of Science</a></p>
<p>20 0.35566676 <a title="155-lsi-20" href="../hunch_net-2005/hunch_net-2005-12-07-Is_the_Google_way_the_way_for_machine_learning%3F.html">136 hunch net-2005-12-07-Is the Google way the way for machine learning?</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(7, 0.521), (35, 0.061), (42, 0.2), (68, 0.013), (74, 0.045), (95, 0.032)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.96924907 <a title="155-lda-1" href="../hunch_net-2005/hunch_net-2005-02-19-Machine_learning_reading_groups.html">24 hunch net-2005-02-19-Machine learning reading groups</a></p>
<p>Introduction: Yaroslav collected an extensive list ofmachine learning reading groups.</p><p>2 0.93149251 <a title="155-lda-2" href="../hunch_net-2013/hunch_net-2013-05-04-COLT_and_ICML_registration.html">482 hunch net-2013-05-04-COLT and ICML registration</a></p>
<p>Introduction: Sebastien Bubeckpoints outCOLTregistrationwith a May 13 early registration
deadline. The local organizers have done an admirable job of containing costs
with a $300 registration fee.ICMLregistrationis also available, at about an x3
higher cost. My understanding is that this is partly due to the costs of a
larger conference being harder to contain, partly due to ICML lasting twice as
long with tutorials and workshops, and partly because the conference
organizers were a bit over-conservative in various ways.</p><p>same-blog 3 0.91765773 <a title="155-lda-3" href="../hunch_net-2006/hunch_net-2006-02-07-Pittsburgh_Mind_Reading_Competition.html">155 hunch net-2006-02-07-Pittsburgh Mind Reading Competition</a></p>
<p>Introduction: Francisco Pereirapoints out a funPrediction Competition. Francisco says:DARPA
is sponsoring a competition to analyze data from an unusual functional
Magnetic Resonance Imaging experiment. Subjects watch videos inside the
scanner while fMRI data are acquired. Unbeknownst to these subjects, the
videos have been seen by a panel of other subjects that labeled each instant
with labels in categories such as representation (are there tools, body parts,
motion, sound), location, presence of actors, emotional content, etc.The
challenge is to predict all of these different labels on an instant-by-instant
basis from the fMRI data. A few reasons why this is particularly
interesting:This is beyond the current state of the art, but not inconceivably
hard.This is a new type of experiment design current analysis methods cannot
deal with.This is an opportunity to work with a heavily examined and
preprocessed neuroimaging dataset.DARPA is offering prizes!</p><p>4 0.69838887 <a title="155-lda-4" href="../hunch_net-2009/hunch_net-2009-07-09-The_Machine_Learning_Forum.html">363 hunch net-2009-07-09-The Machine Learning Forum</a></p>
<p>Introduction: Dear Fellow Machine Learners,For the past year or so I have become
increasingly frustrated with the peer review system in our field. I constantly
get asked to review papers in which I have no interest. At the same time, as
an action editor in JMLR, I constantly have to harass people to review papers.
When I send papers to conferences and to journals I often get rejected with
reviews that, at least in my mind, make no sense. Finally, I have a very hard
time keeping up with the best new work, because I don't know where to look for
itâ&euro;ŚI decided to try an do something to improve the situation. I started a new
web site, which I decided to call "The machine learning forum" the URL
ishttp://themachinelearningforum.orgThe main idea behind this web site is to
remove anonymity from the review process. In this site, all opinions are
attributed to the actual person that expressed them. I expect that this will
improve the quality of the reviews. An obvious other effect is that there will
be fewer n</p><p>5 0.67187929 <a title="155-lda-5" href="../hunch_net-2005/hunch_net-2005-09-08-Online_Learning_as_the_Mathematics_of_Accountability.html">109 hunch net-2005-09-08-Online Learning as the Mathematics of Accountability</a></p>
<p>Introduction: Accountability is a social problem. When someone screws up, do you fire them?
Or do you accept the error and let them continue? This is a very difficult
problem and we all know of stories where the wrong decision was made.Online
learning(as meant here), is a subfield of learning theory which analyzes the
online learning model.In the online learning model, there are a set of
hypotheses or "experts". On any instantancex, each expert makes a predictiony.
A master algorithmAuses these predictions to form it's own predictionyAand
then learns the correct predictiony*. This process repeats.The goal of online
learning is to find a master algorithmAwhich uses the advice of the experts to
make good predictions. In particular, we typically want to guarantee that the
master algorithm performs almost as well as the best expert. IfL(e)is the loss
of experteandL(A)is the loss of the master algorithm, it is often possible to
prove:L(A) less than mineL(e) + log(number of experts)over all sequences.In
p</p><p>6 0.64602172 <a title="155-lda-6" href="../hunch_net-2009/hunch_net-2009-06-24-Interesting_papers_at_UAICMOLT_2009.html">361 hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</a></p>
<p>7 0.46949208 <a title="155-lda-7" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>8 0.41650245 <a title="155-lda-8" href="../hunch_net-2006/hunch_net-2006-03-12-Online_learning_or_online_preservation_of_learning%3F.html">163 hunch net-2006-03-12-Online learning or online preservation of learning?</a></p>
<p>9 0.39175358 <a title="155-lda-9" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>10 0.38222086 <a title="155-lda-10" href="../hunch_net-2010/hunch_net-2010-03-15-The_Efficient_Robust_Conditional_Probability_Estimation_Problem.html">391 hunch net-2010-03-15-The Efficient Robust Conditional Probability Estimation Problem</a></p>
<p>11 0.38178837 <a title="155-lda-11" href="../hunch_net-2007/hunch_net-2007-08-12-Exponentiated_Gradient.html">258 hunch net-2007-08-12-Exponentiated Gradient</a></p>
<p>12 0.38159317 <a title="155-lda-12" href="../hunch_net-2005/hunch_net-2005-02-01-NIPS%3A_Online_Bayes.html">8 hunch net-2005-02-01-NIPS: Online Bayes</a></p>
<p>13 0.38101816 <a title="155-lda-13" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>14 0.3790696 <a title="155-lda-14" href="../hunch_net-2008/hunch_net-2008-07-15-Interesting_papers_at_COLT_%28and_a_bit_of_UAI_%26%23038%3B_workshops%29.html">310 hunch net-2008-07-15-Interesting papers at COLT (and a bit of UAI &#038; workshops)</a></p>
<p>15 0.37766862 <a title="155-lda-15" href="../hunch_net-2011/hunch_net-2011-08-01-Interesting_papers_at_COLT_2011.html">439 hunch net-2011-08-01-Interesting papers at COLT 2011</a></p>
<p>16 0.37327769 <a title="155-lda-16" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>17 0.37295225 <a title="155-lda-17" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>18 0.37279689 <a title="155-lda-18" href="../hunch_net-2009/hunch_net-2009-10-10-ALT_2009.html">374 hunch net-2009-10-10-ALT 2009</a></p>
<p>19 0.37265879 <a title="155-lda-19" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>20 0.37226123 <a title="155-lda-20" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
