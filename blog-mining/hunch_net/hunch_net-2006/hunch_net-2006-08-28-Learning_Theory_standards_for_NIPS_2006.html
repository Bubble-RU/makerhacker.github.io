<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-204" href="#">hunch_net-2006-204</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-204-html" href="http://hunch.net/?p=223">html</a></p><p>Introduction: Bob Williamsonand I are the learning theory PC members atNIPSthis year. This
is some attempt to state the standards and tests I applied to the papers. I
think it is a good idea to talk about this for two reasons:Making community
standards a matter of public record seems healthy. It give us a chance to
debate what is and is not the right standard. It might even give us a bit more
consistency across the years.It may save us all time. There are a number of
papers submitted which just aren't there yet. Avoiding submitting is the right
decision in this case.There are several criteria for judging a paper. All of
these were active this year. Some criteria are uncontroversial while others
may be so.The paper must have a theorem establishing something new for which
it is possible to derive high confidence in the correctness of the results. A
surprising number of papers fail this test. This criteria seems essential to
the definition of "theory".Missing theorem statementMissing proofThis isn't an</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('criteria', 0.282), ('theorem', 0.229), ('sometimes', 0.181), ('right', 0.167), ('proof', 0.157), ('expressed', 0.151), ('correctly', 0.146), ('standards', 0.141), ('decisions', 0.14), ('papers', 0.129), ('paper', 0.124), ('applied', 0.123), ('test', 0.123), ('give', 0.123), ('theory', 0.115), ('confidence', 0.113), ('model', 0.112), ('fail', 0.11), ('theorems', 0.108), ('mathematical', 0.098)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="204-tfidf-1" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>Introduction: Bob Williamsonand I are the learning theory PC members atNIPSthis year. This
is some attempt to state the standards and tests I applied to the papers. I
think it is a good idea to talk about this for two reasons:Making community
standards a matter of public record seems healthy. It give us a chance to
debate what is and is not the right standard. It might even give us a bit more
consistency across the years.It may save us all time. There are a number of
papers submitted which just aren't there yet. Avoiding submitting is the right
decision in this case.There are several criteria for judging a paper. All of
these were active this year. Some criteria are uncontroversial while others
may be so.The paper must have a theorem establishing something new for which
it is possible to derive high confidence in the correctness of the results. A
surprising number of papers fail this test. This criteria seems essential to
the definition of "theory".Missing theorem statementMissing proofThis isn't an</p><p>2 0.21510346 <a title="204-tfidf-2" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>Introduction: Normally, I don't indulge in posters forICML, but this year is naturally an
exception for me. If you want one, there are a small numberleft here, if you
sign up before February.It also seems worthwhile to give some sense of the
scope and reviewing criteria for ICML for authors considering submitting
papers. At ICML, the (very large) program committee does the reviewing which
informs final decisions by area chairs on most papers. Program chairs setup
the process, deal with exceptions or disagreements, and provide advice for the
reviewing process. Providing advice is tricky (and easily misleading) because
a conference is a community, and in the end the aggregate interests of the
community determine the conference. Nevertheless, as a program chair this year
it seems worthwhile to state the overall philosophy I have and what I plan to
encourage (and occasionally discourage).At the highest level, I believe ICML
exists to further research into machine learning, which I generally think of
as</p><p>3 0.19254458 <a title="204-tfidf-3" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>Introduction: I wanted to expand on thispostand some of the previousproblems/research
directionsabout where learning theory might make large strides.Why theory?The
essential reason for theory is "intuition extension". A very good applied
learning person can master some particular application domain yielding the
best computer algorithms for solving that problem. A very good theory can take
the intuitions discovered by this and other applied learning people and extend
them to new domains in a relatively automatic fashion. To do this, we take
these basic intuitions and try to find a mathematical model that:Explains the
basic intuitions.Makes new testable predictions about how to learn.Succeeds in
so learning.This is "intuition extension": taking what we have learned
somewhere else and applying it in new domains. It is fundamentally useful to
everyone because it increases the level of automation in solving
problems.Where next for learning theory?I like the analogy with physics. Back
before we-the-humans</p><p>4 0.19081424 <a title="204-tfidf-4" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>Introduction: Claireasked me to be on the SODA program committee this year, which was quite
a bit of work.I had a relatively light load--merely 49 theory papers. Many of
these papers were not on subjects that I was expert about, so (as is common
for theory conferences) I found various reviewers that I trusted to help
review the papers. I ended up reviewing about 1/3 personally. There were a
couple instances where I ended up overruling a subreviewer whose logic seemed
off, but otherwise I generally let their reviews stand.There are some
differences in standards for paper reviews between the machine learning and
theory communities. In machine learning it is expected that a review be
detailed, while in the theory community this is often not the case. Every
paper given to me ended up with a review varying between somewhat and very
detailed.I'm sure not every author was happy with the outcome. While we did
our best to make good decisions, they were difficult decisions to make. For
example, if there is a</p><p>5 0.18912764 <a title="204-tfidf-5" href="../hunch_net-2006/hunch_net-2006-06-25-Presentation_of_Proofs_is_Hard..html">187 hunch net-2006-06-25-Presentation of Proofs is Hard.</a></p>
<p>Introduction: When presenting part of theReinforcement Learning theory tutorialatICML 2006,
I was forcibly reminded of this.There are several difficulties.When creating
the presentation, the correct level of detail is tricky. With too much detail,
the proof takes too much time and people may be lost to boredom. With too
little detail, the steps of the proof involve too-great a jump. This is very
difficult to judge.What may be an easy step in the careful thought of a quiet
room is not so easy when you are occupied by the process of presentation.What
may be easy after having gone over this (and other) proofs is not so easy to
follow in the first pass by a viewer.These problems seem only correctable by
process of repeated test-and-revise.When presenting the proof, simply speaking
with sufficient precision is substantially harder than in normal conversation
(where precision is not so critical). Practice can help here.When presenting
the proof, going at the right pace for understanding is difficult. When</p><p>6 0.17180528 <a title="204-tfidf-6" href="../hunch_net-2006/hunch_net-2006-07-11-New_Models.html">194 hunch net-2006-07-11-New Models</a></p>
<p>7 0.16342005 <a title="204-tfidf-7" href="../hunch_net-2006/hunch_net-2006-12-04-Structural_Problems_in_NIPS_Decision_Making.html">221 hunch net-2006-12-04-Structural Problems in NIPS Decision Making</a></p>
<p>8 0.16285773 <a title="204-tfidf-8" href="../hunch_net-2008/hunch_net-2008-11-10-ICML_Reviewing_Criteria.html">325 hunch net-2008-11-10-ICML Reviewing Criteria</a></p>
<p>9 0.16187462 <a title="204-tfidf-9" href="../hunch_net-2008/hunch_net-2008-10-14-Who_is_Responsible_for_a_Bad_Review%3F.html">320 hunch net-2008-10-14-Who is Responsible for a Bad Review?</a></p>
<p>10 0.14834718 <a title="204-tfidf-10" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>11 0.14725973 <a title="204-tfidf-11" href="../hunch_net-2005/hunch_net-2005-08-22-Do_you_believe_in_induction%3F.html">104 hunch net-2005-08-22-Do you believe in induction?</a></p>
<p>12 0.13229853 <a title="204-tfidf-12" href="../hunch_net-2005/hunch_net-2005-04-04-Grounds_for_Rejection.html">52 hunch net-2005-04-04-Grounds for Rejection</a></p>
<p>13 0.13143782 <a title="204-tfidf-13" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>14 0.13096248 <a title="204-tfidf-14" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>15 0.12981331 <a title="204-tfidf-15" href="../hunch_net-2005/hunch_net-2005-03-09-Bad_Reviewing.html">38 hunch net-2005-03-09-Bad Reviewing</a></p>
<p>16 0.12937948 <a title="204-tfidf-16" href="../hunch_net-2007/hunch_net-2007-02-16-The_Forgetting.html">233 hunch net-2007-02-16-The Forgetting</a></p>
<p>17 0.12537085 <a title="204-tfidf-17" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>18 0.12443026 <a title="204-tfidf-18" href="../hunch_net-2006/hunch_net-2006-10-08-Incompatibilities_between_classical_confidence_intervals_and_learning..html">213 hunch net-2006-10-08-Incompatibilities between classical confidence intervals and learning.</a></p>
<p>19 0.12034991 <a title="204-tfidf-19" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>20 0.11302744 <a title="204-tfidf-20" href="../hunch_net-2005/hunch_net-2005-03-17-Going_all_the_Way%2C_Sometimes.html">42 hunch net-2005-03-17-Going all the Way, Sometimes</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.299), (1, 0.06), (2, -0.11), (3, -0.07), (4, -0.049), (5, -0.01), (6, -0.053), (7, -0.051), (8, -0.071), (9, -0.099), (10, -0.003), (11, 0.02), (12, -0.001), (13, 0.039), (14, -0.065), (15, 0.131), (16, 0.068), (17, -0.173), (18, -0.034), (19, -0.014), (20, 0.055), (21, 0.004), (22, -0.031), (23, 0.077), (24, 0.001), (25, 0.139), (26, 0.028), (27, 0.028), (28, -0.012), (29, -0.001), (30, -0.076), (31, 0.06), (32, -0.099), (33, 0.003), (34, -0.069), (35, -0.049), (36, -0.016), (37, 0.081), (38, 0.034), (39, 0.068), (40, -0.059), (41, 0.107), (42, 0.049), (43, 0.0), (44, 0.029), (45, -0.016), (46, -0.001), (47, 0.032), (48, 0.064), (49, 0.085)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97848779 <a title="204-lsi-1" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>Introduction: Bob Williamsonand I are the learning theory PC members atNIPSthis year. This
is some attempt to state the standards and tests I applied to the papers. I
think it is a good idea to talk about this for two reasons:Making community
standards a matter of public record seems healthy. It give us a chance to
debate what is and is not the right standard. It might even give us a bit more
consistency across the years.It may save us all time. There are a number of
papers submitted which just aren't there yet. Avoiding submitting is the right
decision in this case.There are several criteria for judging a paper. All of
these were active this year. Some criteria are uncontroversial while others
may be so.The paper must have a theorem establishing something new for which
it is possible to derive high confidence in the correctness of the results. A
surprising number of papers fail this test. This criteria seems essential to
the definition of "theory".Missing theorem statementMissing proofThis isn't an</p><p>2 0.75073892 <a title="204-lsi-2" href="../hunch_net-2005/hunch_net-2005-04-04-Grounds_for_Rejection.html">52 hunch net-2005-04-04-Grounds for Rejection</a></p>
<p>Introduction: It's reviewing season right now, so I thought I would list (at a high level)
the sorts of problems which I see in papers. Hopefully, this will help us all
write better papers.The following flaws are fatal to any paper:Incorrect
theorem or lemma statementsA typo might be "ok", if it can be understood. Any
theorem or lemma which indicates an incorrect understanding of reality must be
rejected. Not doing so would severely harm the integrity of the conference. A
paper rejected for this reason must be fixed.Lack of UnderstandingIf a paper
is understood by none of the (typically 3) reviewers then it must be rejected
for the same reason. This is more controversial than it sounds because there
are some people who maximize paper complexity in the hope of impressing the
reviewer. The tactic sometimes succeeds with some reviewers (but not with
me).As a reviewer, I sometimes get lost for stupid reasons. This is why an
anonymizedcommunication channelwith the author can be very helpful.Bad
ideaRarel</p><p>3 0.678819 <a title="204-lsi-3" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>Introduction: Normally, I don't indulge in posters forICML, but this year is naturally an
exception for me. If you want one, there are a small numberleft here, if you
sign up before February.It also seems worthwhile to give some sense of the
scope and reviewing criteria for ICML for authors considering submitting
papers. At ICML, the (very large) program committee does the reviewing which
informs final decisions by area chairs on most papers. Program chairs setup
the process, deal with exceptions or disagreements, and provide advice for the
reviewing process. Providing advice is tricky (and easily misleading) because
a conference is a community, and in the end the aggregate interests of the
community determine the conference. Nevertheless, as a program chair this year
it seems worthwhile to state the overall philosophy I have and what I plan to
encourage (and occasionally discourage).At the highest level, I believe ICML
exists to further research into machine learning, which I generally think of
as</p><p>4 0.6754089 <a title="204-lsi-4" href="../hunch_net-2007/hunch_net-2007-07-20-Motivation_should_be_the_Responsibility_of_the_Reviewer.html">256 hunch net-2007-07-20-Motivation should be the Responsibility of the Reviewer</a></p>
<p>Introduction: The prevailing wisdom in machine learning seems to be that motivating a paper
is the responsibility of the author. I think this is a harmful view--instead,
it's healthier for the community to regard this as the responsibility of the
reviewer.There are lots of reasons to prefer a reviewer-responsibility
approach.Authors are the most biased possible source of information about the
motivation of the paper. Systems which rely upon very biased sources of
information are inherently unreliable.Authors are highly variable in their
ability and desire to express motivation for their work. This adds greatly to
variance on acceptance of an idea, and it can systematically discriminate or
accentuate careers. It's great if you have a career accentuated by awesome
wording choice, but wise decision making by reviewers is important for the
field.The motivation section in a paper doesn'tdoanything in some sense--it's
there to get the paper in. Reading the motivation of a paper is of little use
in helping</p><p>5 0.66835815 <a title="204-lsi-5" href="../hunch_net-2006/hunch_net-2006-06-25-Presentation_of_Proofs_is_Hard..html">187 hunch net-2006-06-25-Presentation of Proofs is Hard.</a></p>
<p>Introduction: When presenting part of theReinforcement Learning theory tutorialatICML 2006,
I was forcibly reminded of this.There are several difficulties.When creating
the presentation, the correct level of detail is tricky. With too much detail,
the proof takes too much time and people may be lost to boredom. With too
little detail, the steps of the proof involve too-great a jump. This is very
difficult to judge.What may be an easy step in the careful thought of a quiet
room is not so easy when you are occupied by the process of presentation.What
may be easy after having gone over this (and other) proofs is not so easy to
follow in the first pass by a viewer.These problems seem only correctable by
process of repeated test-and-revise.When presenting the proof, simply speaking
with sufficient precision is substantially harder than in normal conversation
(where precision is not so critical). Practice can help here.When presenting
the proof, going at the right pace for understanding is difficult. When</p><p>6 0.65987653 <a title="204-lsi-6" href="../hunch_net-2007/hunch_net-2007-02-16-The_Forgetting.html">233 hunch net-2007-02-16-The Forgetting</a></p>
<p>7 0.65849459 <a title="204-lsi-7" href="../hunch_net-2005/hunch_net-2005-07-23-Interesting_papers_at_ACL.html">97 hunch net-2005-07-23-Interesting papers at ACL</a></p>
<p>8 0.65777308 <a title="204-lsi-8" href="../hunch_net-2005/hunch_net-2005-03-17-Going_all_the_Way%2C_Sometimes.html">42 hunch net-2005-03-17-Going all the Way, Sometimes</a></p>
<p>9 0.65637589 <a title="204-lsi-9" href="../hunch_net-2006/hunch_net-2006-07-11-New_Models.html">194 hunch net-2006-07-11-New Models</a></p>
<p>10 0.65274 <a title="204-lsi-10" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>11 0.65120149 <a title="204-lsi-11" href="../hunch_net-2006/hunch_net-2006-08-10-Precision_is_not_accuracy.html">202 hunch net-2006-08-10-Precision is not accuracy</a></p>
<p>12 0.64014804 <a title="204-lsi-12" href="../hunch_net-2005/hunch_net-2005-03-09-Bad_Reviewing.html">38 hunch net-2005-03-09-Bad Reviewing</a></p>
<p>13 0.62518263 <a title="204-lsi-13" href="../hunch_net-2006/hunch_net-2006-12-04-Structural_Problems_in_NIPS_Decision_Making.html">221 hunch net-2006-12-04-Structural Problems in NIPS Decision Making</a></p>
<p>14 0.6163134 <a title="204-lsi-14" href="../hunch_net-2005/hunch_net-2005-02-25-Why_Papers%3F.html">30 hunch net-2005-02-25-Why Papers?</a></p>
<p>15 0.6045506 <a title="204-lsi-15" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>16 0.60341954 <a title="204-lsi-16" href="../hunch_net-2005/hunch_net-2005-08-22-Do_you_believe_in_induction%3F.html">104 hunch net-2005-08-22-Do you believe in induction?</a></p>
<p>17 0.59151024 <a title="204-lsi-17" href="../hunch_net-2011/hunch_net-2011-08-06-Interesting_thing_at_UAI_2011.html">440 hunch net-2011-08-06-Interesting thing at UAI 2011</a></p>
<p>18 0.58562356 <a title="204-lsi-18" href="../hunch_net-2008/hunch_net-2008-11-10-ICML_Reviewing_Criteria.html">325 hunch net-2008-11-10-ICML Reviewing Criteria</a></p>
<p>19 0.57737768 <a title="204-lsi-19" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>20 0.57580149 <a title="204-lsi-20" href="../hunch_net-2006/hunch_net-2006-03-09-Use_of_Notation.html">162 hunch net-2006-03-09-Use of Notation</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.214), (35, 0.015), (42, 0.259), (45, 0.036), (50, 0.056), (68, 0.063), (69, 0.053), (74, 0.165), (76, 0.018), (82, 0.028), (95, 0.022)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.97177798 <a title="204-lda-1" href="../hunch_net-2005/hunch_net-2005-03-05-Funding_Research.html">36 hunch net-2005-03-05-Funding Research</a></p>
<p>Introduction: The funding of research (and machine learning research) is an issue which
seems to have become more significant in the United States over the last
decade. The word "research" is applied broadly here to science, mathematics,
and engineering.There are two essential difficulties with funding
research:LongshotPaying a researcher is often a big gamble. Most research
projects don't pan out, but a few big payoffs can make it all
worthwhile.Information OnlyMuch of research is about finding the right way to
think about or do something.The Longshot difficulty means that there is high
variance in payoffs. This can be compensated for by funding many different
research projects, reducing variance.The Information-Only difficulty means
that it's hard to extract a profit directly from many types of research, so
companies have difficulty justifying basic research. (Patents are a mechanism
for doing this. They are often extraordinarily clumsy or simply not
applicable.)These two difficulties together imp</p><p>2 0.97088522 <a title="204-lda-2" href="../hunch_net-2006/hunch_net-2006-01-06-MLTV.html">146 hunch net-2006-01-06-MLTV</a></p>
<p>Introduction: As part of aPASCALproject, the Slovenians have been filming various machine
learning events and placing them on the webhere. This includes, for example,
theChicago 2005 Machine Learning Summer Schoolas well as a number of other
summer schools, workshops, and conferences.There are some significant caveats
here--for example, I can't access it from Linux. Based upon the webserver
logs, I expect that is a problem for most people--Computer scientists are
particularly nonstandard in their choice of computing platform.Nevertheless,
the core idea here is excellent and details of compatibility can be fixed
later. With modern technology toys, there is no fundamental reason why the
process of announcing new work at a conference should happen only once and
only for the people who could make it to that room in that conference. The
problems solved include:The multitrack vs. single-track debate. ("Sometimes
the single track doesn't interest me" vs. "When it's multitrack I miss good
talks""I couldn't</p><p>same-blog 3 0.93800855 <a title="204-lda-3" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>Introduction: Bob Williamsonand I are the learning theory PC members atNIPSthis year. This
is some attempt to state the standards and tests I applied to the papers. I
think it is a good idea to talk about this for two reasons:Making community
standards a matter of public record seems healthy. It give us a chance to
debate what is and is not the right standard. It might even give us a bit more
consistency across the years.It may save us all time. There are a number of
papers submitted which just aren't there yet. Avoiding submitting is the right
decision in this case.There are several criteria for judging a paper. All of
these were active this year. Some criteria are uncontroversial while others
may be so.The paper must have a theorem establishing something new for which
it is possible to derive high confidence in the correctness of the results. A
surprising number of papers fail this test. This criteria seems essential to
the definition of "theory".Missing theorem statementMissing proofThis isn't an</p><p>4 0.92831141 <a title="204-lda-4" href="../hunch_net-2009/hunch_net-2009-06-26-Netflix_nearly_done.html">362 hunch net-2009-06-26-Netflix nearly done</a></p>
<p>Introduction: A $1M qualifying result was achieved on thepublic Netflix test setby a3-way
ensemble team. This is just in time forYehuda's presentation atKDD, which I'm
sure will be one of the best attended ever.This isn't quite over--there are a
few days for another super-conglomerate team to come together and there is
some small chance that the performance is nonrepresentative of the final test
set, but I expect not.Regardless of the final outcome, the biggest lesson for
ML from the Netflix contest has been the formidable performance edge of
ensemble methods.</p><p>5 0.89355147 <a title="204-lda-5" href="../hunch_net-2007/hunch_net-2007-12-19-Cool_and_interesting_things_seen_at_NIPS.html">279 hunch net-2007-12-19-Cool and interesting things seen at NIPS</a></p>
<p>Introduction: I learned a number of things atNIPS.The financial people were there in greater
force than previously.Two Sigmasponsored NIPS whileDRW Tradinghad a
booth.Theadversarial machine learning workshophad a number of talks about
interesting applications where an adversary really is out to try and mess up
your learning algorithm. This is very different from the situation we often
think of where the world is oblivious to our learning. This may present new
and convincing applications for the learning-against-an-adversary work common
atCOLT.There were several interesing papers.Sanjoy Dasgupta,Daniel Hsu,
andClaire Monteleonihad a paper onGeneral Agnostic Active Learning. The basic
idea is that active learning can be done via reduction to a form of supervised
learning problem. This is great, because we have many supervised learning
algorithms from which the benefits of active learning may be derived.Joseph
BradleyandRobert Schapirehad aPaper on Filterboost. Filterboost is an online
boosting algorit</p><p>6 0.88013262 <a title="204-lda-6" href="../hunch_net-2005/hunch_net-2005-04-01-Basic_computer_science_research_takes_a_hit.html">50 hunch net-2005-04-01-Basic computer science research takes a hit</a></p>
<p>7 0.87350941 <a title="204-lda-7" href="../hunch_net-2006/hunch_net-2006-02-04-Research_Budget_Changes.html">154 hunch net-2006-02-04-Research Budget Changes</a></p>
<p>8 0.8733471 <a title="204-lda-8" href="../hunch_net-2013/hunch_net-2013-04-15-NEML_II.html">481 hunch net-2013-04-15-NEML II</a></p>
<p>9 0.8473016 <a title="204-lda-9" href="../hunch_net-2009/hunch_net-2009-02-22-Effective_Research_Funding.html">344 hunch net-2009-02-22-Effective Research Funding</a></p>
<p>10 0.844163 <a title="204-lda-10" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>11 0.83339483 <a title="204-lda-11" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>12 0.82860488 <a title="204-lda-12" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>13 0.82459134 <a title="204-lda-13" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>14 0.82040304 <a title="204-lda-14" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<p>15 0.81795681 <a title="204-lda-15" href="../hunch_net-2008/hunch_net-2008-01-06-Research_Political_Issues.html">282 hunch net-2008-01-06-Research Political Issues</a></p>
<p>16 0.81770414 <a title="204-lda-16" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>17 0.81715971 <a title="204-lda-17" href="../hunch_net-2011/hunch_net-2011-11-26-Giving_Thanks.html">449 hunch net-2011-11-26-Giving Thanks</a></p>
<p>18 0.81306112 <a title="204-lda-18" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>19 0.81292713 <a title="204-lda-19" href="../hunch_net-2006/hunch_net-2006-12-04-Structural_Problems_in_NIPS_Decision_Making.html">221 hunch net-2006-12-04-Structural Problems in NIPS Decision Making</a></p>
<p>20 0.81164175 <a title="204-lda-20" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
