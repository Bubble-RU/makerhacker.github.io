<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>168 hunch net-2006-04-02-Mad (Neuro)science</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-168" href="#">hunch_net-2006-168</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>168 hunch net-2006-04-02-Mad (Neuro)science</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-168-html" href="http://hunch.net/?p=179">html</a></p><p>Introduction: One of the questions facing machine learning as a field is "Can we produce a
generalized learning system that can solve a wide array of standard learning
problems?" The answer is trivial: "yes, just have children".Of course, that
wasn't really the question. The refined question is "Are there simple-to-
implement generalized learning systems that can solve a wide array of standard
learning problems?" The answer to this is less clear. The ability of animals
(and people ) to learn might be due to megabytes encoded in the DNA. If this
algorithmic complexity isnecessaryto solve machine learning, the field faces a
daunting task in replicating it on a computer.This observation suggests a
possibility: if you can show that few bits of DNA are needed for learning in
animals, then this provides evidence that machine learning (as a field) has a
hope of big success with relatively little effort.It is well known that
specific portions of the brain have specific functionality across individuals.
Ther</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('region', 0.315), ('specific', 0.252), ('brain', 0.246), ('encoded', 0.244), ('functionality', 0.233), ('auditory', 0.21), ('neuroscience', 0.21), ('generalized', 0.203), ('animals', 0.173), ('children', 0.173), ('field', 0.142), ('arise', 0.139), ('addressing', 0.135), ('array', 0.135), ('wide', 0.123), ('surely', 0.116), ('areas', 0.114), ('experiments', 0.104), ('answer', 0.103), ('solve', 0.098)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="168-tfidf-1" href="../hunch_net-2006/hunch_net-2006-04-02-Mad_%28Neuro%29science.html">168 hunch net-2006-04-02-Mad (Neuro)science</a></p>
<p>Introduction: One of the questions facing machine learning as a field is "Can we produce a
generalized learning system that can solve a wide array of standard learning
problems?" The answer is trivial: "yes, just have children".Of course, that
wasn't really the question. The refined question is "Are there simple-to-
implement generalized learning systems that can solve a wide array of standard
learning problems?" The answer to this is less clear. The ability of animals
(and people ) to learn might be due to megabytes encoded in the DNA. If this
algorithmic complexity isnecessaryto solve machine learning, the field faces a
daunting task in replicating it on a computer.This observation suggests a
possibility: if you can show that few bits of DNA are needed for learning in
animals, then this provides evidence that machine learning (as a field) has a
hope of big success with relatively little effort.It is well known that
specific portions of the brain have specific functionality across individuals.
Ther</p><p>2 0.096421406 <a title="168-tfidf-2" href="../hunch_net-2005/hunch_net-2005-07-07-The_Limits_of_Learning_Theory.html">90 hunch net-2005-07-07-The Limits of Learning Theory</a></p>
<p>Introduction: Suppose we had an infinitely powerful mathematician sitting in a room and
proving theorems about learning. Could he solve machine learning?The answer is
"no". This answer is both obvious and sometimes underappreciated.There are
several ways to conclude that somebiasis necessary in order to succesfully
learn. For example, suppose we are trying to solve classification. At
prediction time, we observe some featuresXand want to make a prediction of
either0or1. Bias is what makes us prefer one answer over the other based on
past experience. In order to learn we must:Have a bias. Always predicting0is
as likely as1is useless.Have the "right" bias. Predicting1when the answer
is0is also not helpful.The implication of "have a bias" is that we can not
design effective learning algorithms with "a uniform prior over all
possibilities". The implication of "have the 'right' bias" is that our
mathematician fails since "right" is defined with respect to the solutions to
problems encountered in the real</p><p>3 0.094529822 <a title="168-tfidf-3" href="../hunch_net-2005/hunch_net-2005-05-10-Learning_Reductions_are_Reductionist.html">68 hunch net-2005-05-10-Learning Reductions are Reductionist</a></p>
<p>Introduction: This is about a fundamental motivation for the investigation of reductions in
learning. It applies to many pieces of work other than my own.The reductionist
approach to problem solving is characterized by taking a problem, decomposing
it into as-small-as-possible subproblems, discovering how to solve the
subproblems, and then discovering how to use the solutions to the subproblems
to solve larger problems. The reductionist approach to solving problems has
often payed offverywell. Computer science related examples of the reductionist
approach include:Reducing computation to the transistor. All of our CPUs are
built from transistors.Reducing rendering of images to rendering a triangle
(or other simple polygons). Computers can now render near-realistic scenes in
real time. The big breakthrough came from learning how to render many
triangles quickly.This approach to problem solving extends well beyond
computer science. Many fields of science focus on theories making predictions
about very</p><p>4 0.09268371 <a title="168-tfidf-4" href="../hunch_net-2006/hunch_net-2006-04-17-Rexa_is_live.html">173 hunch net-2006-04-17-Rexa is live</a></p>
<p>Introduction: Rexais now publicly available. Anyone can create an account and login.Rexa is
similar toCiteseerandGoogle Scholarin functionality with more emphasis on the
use of machine learning for intelligent information extraction. For example,
Rexa can automatically display a picture on an author's homepage when the
author is searched for.</p><p>5 0.092446983 <a title="168-tfidf-5" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>Introduction: Researchers are typically confronted with big problems that they have no idea
how to solve. In trying to come up with a solution, a natural approach is to
decompose the big problem into a set of subproblems whose solution yields a
solution to the larger problem. This approach can go wrong in several
ways.Decomposition failure. The solution to the decomposition does not in fact
yield a solution to the overall problem.Artificial hardness. The subproblems
created are sufficient if solved to solve the overall problem, but they are
harder than necessary.As you can see, computational complexity forms a
relatively new (in research-history) razor by which to judge an approach
sufficient but not necessary.In my experience, the artificial hardness problem
is very common. Many researchers abdicate the responsibility of choosing a
problem to work on to other people. This process starts very naturally as a
graduate student, when an incoming student might have relatively little idea
about how to do</p><p>6 0.090194233 <a title="168-tfidf-6" href="../hunch_net-2005/hunch_net-2005-03-21-Research_Styles_in_Machine_Learning.html">44 hunch net-2005-03-21-Research Styles in Machine Learning</a></p>
<p>7 0.087768018 <a title="168-tfidf-7" href="../hunch_net-2009/hunch_net-2009-05-19-CI_Fellows.html">355 hunch net-2009-05-19-CI Fellows</a></p>
<p>8 0.087170824 <a title="168-tfidf-8" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>9 0.086433969 <a title="168-tfidf-9" href="../hunch_net-2007/hunch_net-2007-01-15-The_Machine_Learning_Department.html">228 hunch net-2007-01-15-The Machine Learning Department</a></p>
<p>10 0.083314762 <a title="168-tfidf-10" href="../hunch_net-2006/hunch_net-2006-02-27-The_Peekaboom_Dataset.html">159 hunch net-2006-02-27-The Peekaboom Dataset</a></p>
<p>11 0.08185032 <a title="168-tfidf-11" href="../hunch_net-2008/hunch_net-2008-01-28-Sufficient_Computation.html">287 hunch net-2008-01-28-Sufficient Computation</a></p>
<p>12 0.078193635 <a title="168-tfidf-12" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>13 0.07625138 <a title="168-tfidf-13" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>14 0.07359343 <a title="168-tfidf-14" href="../hunch_net-2011/hunch_net-2011-08-06-Interesting_thing_at_UAI_2011.html">440 hunch net-2011-08-06-Interesting thing at UAI 2011</a></p>
<p>15 0.071052641 <a title="168-tfidf-15" href="../hunch_net-2007/hunch_net-2007-09-16-Optimizing_Machine_Learning_Programs.html">262 hunch net-2007-09-16-Optimizing Machine Learning Programs</a></p>
<p>16 0.070305124 <a title="168-tfidf-16" href="../hunch_net-2006/hunch_net-2006-01-18-Is_Multitask_Learning_Black-Boxable%3F.html">149 hunch net-2006-01-18-Is Multitask Learning Black-Boxable?</a></p>
<p>17 0.067639127 <a title="168-tfidf-17" href="../hunch_net-2008/hunch_net-2008-07-02-Proprietary_Data_in_Academic_Research%3F.html">306 hunch net-2008-07-02-Proprietary Data in Academic Research?</a></p>
<p>18 0.066906556 <a title="168-tfidf-18" href="../hunch_net-2009/hunch_net-2009-05-08-Computability_in_Artificial_Intelligence.html">353 hunch net-2009-05-08-Computability in Artificial Intelligence</a></p>
<p>19 0.066304468 <a title="168-tfidf-19" href="../hunch_net-2011/hunch_net-2011-02-17-What_does_Watson_mean%3F.html">424 hunch net-2011-02-17-What does Watson mean?</a></p>
<p>20 0.065905869 <a title="168-tfidf-20" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.152), (1, -0.023), (2, 0.057), (3, -0.042), (4, -0.029), (5, 0.002), (6, -0.064), (7, 0.012), (8, 0.017), (9, 0.011), (10, 0.023), (11, -0.001), (12, 0.104), (13, -0.046), (14, 0.084), (15, 0.023), (16, 0.034), (17, -0.026), (18, 0.032), (19, 0.02), (20, 0.046), (21, 0.048), (22, 0.004), (23, 0.031), (24, 0.008), (25, -0.045), (26, -0.044), (27, -0.024), (28, -0.063), (29, -0.038), (30, -0.02), (31, 0.038), (32, -0.008), (33, -0.056), (34, -0.005), (35, 0.07), (36, 0.01), (37, 0.028), (38, -0.051), (39, 0.05), (40, 0.011), (41, -0.032), (42, 0.049), (43, 0.0), (44, -0.008), (45, 0.014), (46, 0.058), (47, 0.026), (48, 0.005), (49, -0.04)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.88719255 <a title="168-lsi-1" href="../hunch_net-2006/hunch_net-2006-04-02-Mad_%28Neuro%29science.html">168 hunch net-2006-04-02-Mad (Neuro)science</a></p>
<p>Introduction: One of the questions facing machine learning as a field is "Can we produce a
generalized learning system that can solve a wide array of standard learning
problems?" The answer is trivial: "yes, just have children".Of course, that
wasn't really the question. The refined question is "Are there simple-to-
implement generalized learning systems that can solve a wide array of standard
learning problems?" The answer to this is less clear. The ability of animals
(and people ) to learn might be due to megabytes encoded in the DNA. If this
algorithmic complexity isnecessaryto solve machine learning, the field faces a
daunting task in replicating it on a computer.This observation suggests a
possibility: if you can show that few bits of DNA are needed for learning in
animals, then this provides evidence that machine learning (as a field) has a
hope of big success with relatively little effort.It is well known that
specific portions of the brain have specific functionality across individuals.
Ther</p><p>2 0.69265199 <a title="168-lsi-2" href="../hunch_net-2006/hunch_net-2006-01-18-Is_Multitask_Learning_Black-Boxable%3F.html">149 hunch net-2006-01-18-Is Multitask Learning Black-Boxable?</a></p>
<p>Introduction: Multitask learning is the learning to predict multiple outputs given the same
input. Mathematically, we might think of this as trying to learn a functionf:X
-> {0,1}n. Structured learning is similar at this level of abstraction. Many
people have worked on solving multitask learning (for exampleRich Caruana)
using methods which share an internal representation. On other words, the the
computation and learning of theith prediction is shared with the computation
and learning of thejth prediction. Another way to ask this question is: can we
avoid sharing the internal representation?For example, itmightbe feasible to
solve multitask learning by some process feeding theith predictionf(x)iinto
thejth predictorf(x,f(x)i)j,If the answer is "no", then it implies we can not
take binary classification as a basic primitive in the process of solving
prediction problems. If the answer is "yes", then we can reuse binary
classification algorithms to solve multitask learning problems.Finding a
satisfyin</p><p>3 0.69198608 <a title="168-lsi-3" href="../hunch_net-2005/hunch_net-2005-07-07-The_Limits_of_Learning_Theory.html">90 hunch net-2005-07-07-The Limits of Learning Theory</a></p>
<p>Introduction: Suppose we had an infinitely powerful mathematician sitting in a room and
proving theorems about learning. Could he solve machine learning?The answer is
"no". This answer is both obvious and sometimes underappreciated.There are
several ways to conclude that somebiasis necessary in order to succesfully
learn. For example, suppose we are trying to solve classification. At
prediction time, we observe some featuresXand want to make a prediction of
either0or1. Bias is what makes us prefer one answer over the other based on
past experience. In order to learn we must:Have a bias. Always predicting0is
as likely as1is useless.Have the "right" bias. Predicting1when the answer
is0is also not helpful.The implication of "have a bias" is that we can not
design effective learning algorithms with "a uniform prior over all
possibilities". The implication of "have the 'right' bias" is that our
mathematician fails since "right" is defined with respect to the solutions to
problems encountered in the real</p><p>4 0.6183188 <a title="168-lsi-4" href="../hunch_net-2006/hunch_net-2006-01-30-Should_the_Input_Representation_be_a_Vector%3F.html">152 hunch net-2006-01-30-Should the Input Representation be a Vector?</a></p>
<p>Introduction: Let's suppose that we are trying to create a general purpose machine learning
box. The box is fed many examples of the function it is supposed to learn and
(hopefully) succeeds.To date, most such attempts to produce a box of this form
take a vector as input. The elements of the vector might be bits, real
numbers, or 'categorical' data (a discrete set of values).On the other hand,
there are a number of succesful applications of machine learning which do not
seem to use a vector representation as input. For example, in
vision,convolutional neural networkshave been used to solve several vision
problems. The input to the convolutional neural network is essentially the raw
camera image as amatrix. In learning for natural languages, several people
have had success on problems like parts-of-speech tagging using predictors
restricted to a window surrounding the word to be predicted.A vector window
and a matrix both imply a notion of locality which is being actively and
effectively used by thes</p><p>5 0.6150831 <a title="168-lsi-5" href="../hunch_net-2005/hunch_net-2005-05-10-Learning_Reductions_are_Reductionist.html">68 hunch net-2005-05-10-Learning Reductions are Reductionist</a></p>
<p>Introduction: This is about a fundamental motivation for the investigation of reductions in
learning. It applies to many pieces of work other than my own.The reductionist
approach to problem solving is characterized by taking a problem, decomposing
it into as-small-as-possible subproblems, discovering how to solve the
subproblems, and then discovering how to use the solutions to the subproblems
to solve larger problems. The reductionist approach to solving problems has
often payed offverywell. Computer science related examples of the reductionist
approach include:Reducing computation to the transistor. All of our CPUs are
built from transistors.Reducing rendering of images to rendering a triangle
(or other simple polygons). Computers can now render near-realistic scenes in
real time. The big breakthrough came from learning how to render many
triangles quickly.This approach to problem solving extends well beyond
computer science. Many fields of science focus on theories making predictions
about very</p><p>6 0.61229426 <a title="168-lsi-6" href="../hunch_net-2006/hunch_net-2006-02-02-Introspectionism_as_a_Disease.html">153 hunch net-2006-02-02-Introspectionism as a Disease</a></p>
<p>7 0.60059255 <a title="168-lsi-7" href="../hunch_net-2014/hunch_net-2014-02-16-Metacademy%3A_a_package_manager_for_knowledge.html">493 hunch net-2014-02-16-Metacademy: a package manager for knowledge</a></p>
<p>8 0.59171134 <a title="168-lsi-8" href="../hunch_net-2006/hunch_net-2006-03-17-Multitask_learning_is_Black-Boxable.html">164 hunch net-2006-03-17-Multitask learning is Black-Boxable</a></p>
<p>9 0.56494343 <a title="168-lsi-9" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<p>10 0.55660719 <a title="168-lsi-10" href="../hunch_net-2007/hunch_net-2007-09-16-Optimizing_Machine_Learning_Programs.html">262 hunch net-2007-09-16-Optimizing Machine Learning Programs</a></p>
<p>11 0.55180877 <a title="168-lsi-11" href="../hunch_net-2007/hunch_net-2007-01-15-The_Machine_Learning_Department.html">228 hunch net-2007-01-15-The Machine Learning Department</a></p>
<p>12 0.55115312 <a title="168-lsi-12" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>13 0.54439062 <a title="168-lsi-13" href="../hunch_net-2009/hunch_net-2009-01-21-Nearly_all_natural_problems_require_nonlinearity.html">337 hunch net-2009-01-21-Nearly all natural problems require nonlinearity</a></p>
<p>14 0.54416472 <a title="168-lsi-14" href="../hunch_net-2009/hunch_net-2009-05-08-Computability_in_Artificial_Intelligence.html">353 hunch net-2009-05-08-Computability in Artificial Intelligence</a></p>
<p>15 0.54357576 <a title="168-lsi-15" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>16 0.54319698 <a title="168-lsi-16" href="../hunch_net-2007/hunch_net-2007-02-02-Thoughts_regarding_%26%238220%3BIs_machine_learning_different_from_statistics%3F%26%238221%3B.html">230 hunch net-2007-02-02-Thoughts regarding &#8220;Is machine learning different from statistics?&#8221;</a></p>
<p>17 0.54269266 <a title="168-lsi-17" href="../hunch_net-2006/hunch_net-2006-03-05-%26%238220%3BStructural%26%238221%3B_Learning.html">161 hunch net-2006-03-05-&#8220;Structural&#8221; Learning</a></p>
<p>18 0.53939217 <a title="168-lsi-18" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>19 0.52782744 <a title="168-lsi-19" href="../hunch_net-2005/hunch_net-2005-09-20-Workshop_Proposal%3A_Atomic_Learning.html">114 hunch net-2005-09-20-Workshop Proposal: Atomic Learning</a></p>
<p>20 0.52722204 <a title="168-lsi-20" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(35, 0.041), (42, 0.268), (45, 0.015), (50, 0.021), (67, 0.018), (69, 0.021), (70, 0.433), (74, 0.038), (76, 0.037)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.89950234 <a title="168-lda-1" href="../hunch_net-2007/hunch_net-2007-10-14-NIPS_workshp%3A_Learning_Problem_Design.html">265 hunch net-2007-10-14-NIPS workshp: Learning Problem Design</a></p>
<p>Introduction: Alinaand I are organizing a workshop onLearning Problem DesignatNIPS.What is
learning problem design?It's about being clever in creating learning problems
from otherwise unlabeled data. Read the webpage above for examples.I want to
participate!Email us before Nov. 1 with a description of what you want to talk
about.</p><p>same-blog 2 0.79106396 <a title="168-lda-2" href="../hunch_net-2006/hunch_net-2006-04-02-Mad_%28Neuro%29science.html">168 hunch net-2006-04-02-Mad (Neuro)science</a></p>
<p>Introduction: One of the questions facing machine learning as a field is "Can we produce a
generalized learning system that can solve a wide array of standard learning
problems?" The answer is trivial: "yes, just have children".Of course, that
wasn't really the question. The refined question is "Are there simple-to-
implement generalized learning systems that can solve a wide array of standard
learning problems?" The answer to this is less clear. The ability of animals
(and people ) to learn might be due to megabytes encoded in the DNA. If this
algorithmic complexity isnecessaryto solve machine learning, the field faces a
daunting task in replicating it on a computer.This observation suggests a
possibility: if you can show that few bits of DNA are needed for learning in
animals, then this provides evidence that machine learning (as a field) has a
hope of big success with relatively little effort.It is well known that
specific portions of the brain have specific functionality across individuals.
Ther</p><p>3 0.68231881 <a title="168-lda-3" href="../hunch_net-2005/hunch_net-2005-02-01-NIPS%3A_Online_Bayes.html">8 hunch net-2005-02-01-NIPS: Online Bayes</a></p>
<p>Introduction: One nice use for this blog is to consider and discuss papers that that have
appeared at recent conferences. I really enjoyed Andrew Ng and Sham Kakade's
paperOnline Bounds for Bayesian Algorithms. From the paper:The philosophy
taken in the Bayesian methodology is often at odds withthat in the online
learning community…. the online learning settingmakes rather minimal
assumptions on the conditions under which thedata are being presented to the
learner â€”usually, Nature could provideexamples in an adversarial manner. We
study the performance ofBayesian algorithms in a more adversarial setting… We
providecompetitive bounds when the cost function is the log loss, and
wecompare our performance to the best model in our model class (as inthe
experts setting).It's a very nice analysis of some of my favorite algorithms
that all hinges around a beautiful theorem:Let Q be any distribution over
parameters theta. Then for all sequences S:L_{Bayes}(S) leq L_Q(S) +
KL(Q|P)where P is our prior and th</p><p>4 0.63837761 <a title="168-lda-4" href="../hunch_net-2006/hunch_net-2006-04-05-What_is_state%3F.html">169 hunch net-2006-04-05-What is state?</a></p>
<p>Introduction: In reinforcement learning (and sometimes other settings), there is a notion of
"state". Based upon the state various predictions are made such as "Which
action should be taken next?" or "How much cumulative reward do I expect if I
take some action from this state?" Given the importance of state, it is
important to examine the meaning. There are actually several distinct options
and it turns out the definition variation is very important in motivating
different pieces of work.Newtonian State. State is the physical pose of the
world. Under this definition, there areverymany states, often too many for
explicit representation. This is also the definition typically used in
games.Abstracted State. State is an abstracted physical state of the world.
"Is the door open or closed?" "Are you in room A or not?" The number of states
is much smaller here. A basic issue here is: "How do you compute the state
from observations?"Mathematical State. State is a sufficient statistic of
observations for ma</p><p>5 0.62709445 <a title="168-lda-5" href="../hunch_net-2006/hunch_net-2006-12-04-Structural_Problems_in_NIPS_Decision_Making.html">221 hunch net-2006-12-04-Structural Problems in NIPS Decision Making</a></p>
<p>Introduction: This is a very difficult post to write, because it is about a perenially
touchy subject. Nevertheless, it is an important one which needs to be thought
about carefully.There are a few things which should be understood:The system
is changing and responsive. We-the-authors are we-the-reviewers, we-the-PC,
and even we-the-NIPS-board. NIPS has implemented 'secondary program chairs',
'author response', and 'double blind reviewing' in the last few years to help
with the decision process, and more changes may happen in the future.Agreement
creates a perception of correctness. When any PC meets and makes a group
decision about a paper, there is a strong tendency for the reinforcement
inherent in a group decision to create the perception of correctness. For the
many people who have been on the NIPS PC it's reasonable to entertain a
healthy skepticism in the face of this reinforcing certainty.This post is
about structural problems. What problems arise because of the structure of the
process? The</p><p>6 0.52865517 <a title="168-lda-6" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>7 0.52772307 <a title="168-lda-7" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<p>8 0.52766544 <a title="168-lda-8" href="../hunch_net-2005/hunch_net-2005-03-21-Research_Styles_in_Machine_Learning.html">44 hunch net-2005-03-21-Research Styles in Machine Learning</a></p>
<p>9 0.52733302 <a title="168-lda-9" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>10 0.5270853 <a title="168-lda-10" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>11 0.52457869 <a title="168-lda-11" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>12 0.52450687 <a title="168-lda-12" href="../hunch_net-2005/hunch_net-2005-12-09-Some_NIPS_papers.html">138 hunch net-2005-12-09-Some NIPS papers</a></p>
<p>13 0.52376342 <a title="168-lda-13" href="../hunch_net-2007/hunch_net-2007-02-02-Thoughts_regarding_%26%238220%3BIs_machine_learning_different_from_statistics%3F%26%238221%3B.html">230 hunch net-2007-02-02-Thoughts regarding &#8220;Is machine learning different from statistics?&#8221;</a></p>
<p>14 0.52366209 <a title="168-lda-14" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>15 0.52335787 <a title="168-lda-15" href="../hunch_net-2005/hunch_net-2005-07-07-The_Limits_of_Learning_Theory.html">90 hunch net-2005-07-07-The Limits of Learning Theory</a></p>
<p>16 0.52323663 <a title="168-lda-16" href="../hunch_net-2005/hunch_net-2005-04-23-Advantages_and_Disadvantages_of_Bayesian_Learning.html">60 hunch net-2005-04-23-Advantages and Disadvantages of Bayesian Learning</a></p>
<p>17 0.52310723 <a title="168-lda-17" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>18 0.52279681 <a title="168-lda-18" href="../hunch_net-2005/hunch_net-2005-03-29-Academic_Mechanism_Design.html">48 hunch net-2005-03-29-Academic Mechanism Design</a></p>
<p>19 0.52264428 <a title="168-lda-19" href="../hunch_net-2006/hunch_net-2006-06-15-IJCAI_is_out_of_season.html">184 hunch net-2006-06-15-IJCAI is out of season</a></p>
<p>20 0.52256209 <a title="168-lda-20" href="../hunch_net-2005/hunch_net-2005-05-06-Don%26%238217%3Bt_mix_the_solution_into_the_problem.html">67 hunch net-2005-05-06-Don&#8217;t mix the solution into the problem</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
