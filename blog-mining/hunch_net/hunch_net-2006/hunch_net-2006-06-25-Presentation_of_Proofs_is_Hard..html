<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>187 hunch net-2006-06-25-Presentation of Proofs is Hard.</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-187" href="#">hunch_net-2006-187</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>187 hunch net-2006-06-25-Presentation of Proofs is Hard.</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-187-html" href="http://hunch.net/?p=200">html</a></p><p>Introduction: When presenting part of the  Reinforcement Learning theory tutorial  at  ICML 2006 , I was forcibly reminded of this.
 
There are several difficulties.
  
  When creating the presentation, the correct level of detail is tricky.  With too much detail, the proof takes too much time and people may be lost to boredom.  With too little detail, the steps of the proof involve too-great a jump. This is very difficult to judge.
 
 What may be an easy step in the careful thought of a quiet room is not so easy when you are occupied by the process of presentation. 
 What may be easy after having gone over this (and other) proofs is not so easy to follow in the first pass by a viewer. 
 

  These problems seem only correctable by process of repeated test-and-revise.
 
 When presenting the proof, simply speaking with sufficient precision is substantially harder than in normal conversation (where precision is not so critical).  Practice can help here. 
 When presenting the proof, going at the right p</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 When presenting part of the  Reinforcement Learning theory tutorial  at  ICML 2006 , I was forcibly reminded of this. [sent-1, score-0.417]
</p><p>2 When creating the presentation, the correct level of detail is tricky. [sent-3, score-0.495]
</p><p>3 With too much detail, the proof takes too much time and people may be lost to boredom. [sent-4, score-0.566]
</p><p>4 With too little detail, the steps of the proof involve too-great a jump. [sent-5, score-0.574]
</p><p>5 What may be an easy step in the careful thought of a quiet room is not so easy when you are occupied by the process of presentation. [sent-7, score-0.507]
</p><p>6 What may be easy after having gone over this (and other) proofs is not so easy to follow in the first pass by a viewer. [sent-8, score-0.726]
</p><p>7 These problems seem only correctable by process of repeated test-and-revise. [sent-9, score-0.305]
</p><p>8 When presenting the proof, simply speaking with sufficient precision is substantially harder than in normal conversation (where precision is not so critical). [sent-10, score-0.898]
</p><p>9 When presenting the proof, going at the right pace for understanding is difficult. [sent-12, score-0.633]
</p><p>10 When we use a blackboard/whiteboard, a natural reasonable pace is imposed by the process of writing. [sent-13, score-0.588]
</p><p>11 Unfortunately, writing doesn’t scale well to large audiences for vision reasons, losing this natural pacing mechanism. [sent-14, score-0.351]
</p><p>12 It is difficult to entertain with a proofÃ¢â‚¬â€there is nothing particularly funny about it. [sent-15, score-0.432]
</p><p>13 This particularly matters for a large audience which tends to naturally develop an expectation of being entertained. [sent-16, score-0.466]
</p><p>14 Given all these difficulties, it is very tempting to avoid presenting proofs. [sent-17, score-0.392]
</p><p>15 Avoiding the proof in any serious detail is fairly reasonable in a conference presentation—the time is too short and the people viewing are too heavily overloaded to follow the logic well. [sent-18, score-1.387]
</p><p>16 The “right” level of detail is often the theorem statement. [sent-19, score-0.495]
</p><p>17 Nevertheless, avoidance is not always possible because the proof is one of the more powerful mechanisms we have for doing research. [sent-20, score-0.677]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('proof', 0.429), ('detail', 0.4), ('presenting', 0.312), ('pace', 0.255), ('precision', 0.169), ('follow', 0.141), ('easy', 0.135), ('presentation', 0.124), ('funny', 0.113), ('avoidance', 0.113), ('audiences', 0.113), ('correctable', 0.113), ('entertain', 0.105), ('reminded', 0.105), ('process', 0.098), ('level', 0.095), ('repeated', 0.094), ('overloaded', 0.094), ('normal', 0.094), ('proofs', 0.094), ('viewing', 0.091), ('natural', 0.085), ('conversation', 0.085), ('losing', 0.085), ('matters', 0.085), ('heavily', 0.082), ('tempting', 0.08), ('develop', 0.08), ('tends', 0.08), ('gone', 0.078), ('avoiding', 0.078), ('pass', 0.076), ('imposed', 0.076), ('audience', 0.076), ('logic', 0.076), ('reasonable', 0.074), ('steps', 0.073), ('particularly', 0.073), ('difficult', 0.072), ('careful', 0.072), ('expectation', 0.072), ('involve', 0.072), ('lost', 0.07), ('nothing', 0.069), ('speaking', 0.069), ('powerful', 0.069), ('vision', 0.068), ('may', 0.067), ('right', 0.066), ('mechanisms', 0.066)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999982 <a title="187-tfidf-1" href="../hunch_net-2006/hunch_net-2006-06-25-Presentation_of_Proofs_is_Hard..html">187 hunch net-2006-06-25-Presentation of Proofs is Hard.</a></p>
<p>Introduction: When presenting part of the  Reinforcement Learning theory tutorial  at  ICML 2006 , I was forcibly reminded of this.
 
There are several difficulties.
  
  When creating the presentation, the correct level of detail is tricky.  With too much detail, the proof takes too much time and people may be lost to boredom.  With too little detail, the steps of the proof involve too-great a jump. This is very difficult to judge.
 
 What may be an easy step in the careful thought of a quiet room is not so easy when you are occupied by the process of presentation. 
 What may be easy after having gone over this (and other) proofs is not so easy to follow in the first pass by a viewer. 
 

  These problems seem only correctable by process of repeated test-and-revise.
 
 When presenting the proof, simply speaking with sufficient precision is substantially harder than in normal conversation (where precision is not so critical).  Practice can help here. 
 When presenting the proof, going at the right p</p><p>2 0.19260126 <a title="187-tfidf-2" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>Introduction: Bob Williamson  and I are the learning theory PC members at  NIPS  this year.  This is some attempt to state the standards and tests I applied to the papers.  I think it is a good idea to talk about this for two reasons:
  
 Making community standards a matter of public record seems healthy.  It give us a chance to debate what is and is not the right standard.  It might even give us a bit more consistency across the years. 
 It may save us all time.  There are a number of papers submitted which just aren’t there yet.  Avoiding submitting is the right decision in this case. 
  
There are several criteria for judging a paper.  All of these were active this year.  Some criteria are uncontroversial while others may be so.
  
 The paper must have a theorem establishing something new for which it is possible to derive high confidence in the correctness of the results.  A surprising number of papers fail this test.  This criteria seems essential to the definition of “theory”.
 
  Missing theo</p><p>3 0.13750234 <a title="187-tfidf-3" href="../hunch_net-2010/hunch_net-2010-09-28-Machined_Learnings.html">412 hunch net-2010-09-28-Machined Learnings</a></p>
<p>Introduction: Paul Mineiro  has started  Machined Learnings  where heâ&euro;&trade;s seriously attempting to do ML research in public.  I personally need to read through in greater detail, as much of it is learning reduction related, trying to deal with the sorts of complex source problems that come up in practice.</p><p>4 0.13556869 <a title="187-tfidf-4" href="../hunch_net-2005/hunch_net-2005-08-22-Do_you_believe_in_induction%3F.html">104 hunch net-2005-08-22-Do you believe in induction?</a></p>
<p>Introduction: Foster Provost  gave a talk at the ICML  metalearning workshop  on “metalearning” and the “no free lunch theorem” which seems worth summarizing.
 
As a review: the no free lunch theorem is the most complicated way we know of to say that a  bias  is required in order to learn.  The simplest way to see this is in a nonprobabilistic setting.  If you are given examples of the form  (x,y)  and you wish to predict  y  from  x  then any prediction mechanism errs half the time in expectation over all sequences of examples.  The proof of this is very simple: on every example a predictor must make some prediction and by symmetry over the set of sequences it will be wrong half the time and right half the time.  The basic idea of this proof has been applied to many other settings.
 
The simplistic interpretation of this theorem which many people jump to is “machine learning is dead” since there can be no single learning algorithm which can solve all learning problems.  This is the wrong way to thi</p><p>5 0.10819178 <a title="187-tfidf-5" href="../hunch_net-2008/hunch_net-2008-10-14-Who_is_Responsible_for_a_Bad_Review%3F.html">320 hunch net-2008-10-14-Who is Responsible for a Bad Review?</a></p>
<p>Introduction: Although I’m greatly interested in machine learning, I think it must be admitted that there is a large amount of low quality logic being used in reviews.  The problem is bad enough that sometimes I wonder if the  Byzantine generals  limit has been exceeded.  For example, I’ve seen recent reviews where the given reasons for rejecting are:
  
 [ NIPS ] Theorem A is uninteresting because Theorem B is uninteresting. 
 [ UAI ] When you learn by memorization, the problem addressed is trivial. 
 [NIPS] The proof is in the appendix.  
 [NIPS] This has been done before.  (… but not giving any relevant citations)  
  
Just for the record I want to point out what’s wrong with these reviews.  A future world in which such reasons never come up again would be great, but I’m sure these errors will be committed many times more in the future.
  
 This is nonsense.  A theorem should be evaluated based on it’s merits, rather than the merits of another theorem. 
 Learning by memorization requires an expon</p><p>6 0.090655915 <a title="187-tfidf-6" href="../hunch_net-2008/hunch_net-2008-07-04-More_Presentation_Preparation.html">307 hunch net-2008-07-04-More Presentation Preparation</a></p>
<p>7 0.089603886 <a title="187-tfidf-7" href="../hunch_net-2006/hunch_net-2006-08-10-Precision_is_not_accuracy.html">202 hunch net-2006-08-10-Precision is not accuracy</a></p>
<p>8 0.082320847 <a title="187-tfidf-8" href="../hunch_net-2007/hunch_net-2007-06-21-Presentation_Preparation.html">249 hunch net-2007-06-21-Presentation Preparation</a></p>
<p>9 0.081253305 <a title="187-tfidf-9" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>10 0.080633417 <a title="187-tfidf-10" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>11 0.079262927 <a title="187-tfidf-11" href="../hunch_net-2005/hunch_net-2005-03-09-Bad_Reviewing.html">38 hunch net-2005-03-09-Bad Reviewing</a></p>
<p>12 0.074661613 <a title="187-tfidf-12" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>13 0.074504182 <a title="187-tfidf-13" href="../hunch_net-2005/hunch_net-2005-12-22-Yes_%2C_I_am_applying.html">142 hunch net-2005-12-22-Yes , I am applying</a></p>
<p>14 0.073908471 <a title="187-tfidf-14" href="../hunch_net-2010/hunch_net-2010-04-26-Compassionate_Reviewing.html">395 hunch net-2010-04-26-Compassionate Reviewing</a></p>
<p>15 0.073490568 <a title="187-tfidf-15" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>16 0.073239677 <a title="187-tfidf-16" href="../hunch_net-2005/hunch_net-2005-02-23-Problem%3A_Reinforcement_Learning_with_Classification.html">27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</a></p>
<p>17 0.072478853 <a title="187-tfidf-17" href="../hunch_net-2005/hunch_net-2005-05-28-Running_A_Machine_Learning_Summer_School.html">75 hunch net-2005-05-28-Running A Machine Learning Summer School</a></p>
<p>18 0.072323635 <a title="187-tfidf-18" href="../hunch_net-2008/hunch_net-2008-11-16-Observations_on_Linearity_for_Reductions_to_Regression.html">327 hunch net-2008-11-16-Observations on Linearity for Reductions to Regression</a></p>
<p>19 0.071304679 <a title="187-tfidf-19" href="../hunch_net-2008/hunch_net-2008-10-19-NIPS_2008_workshop_on_Kernel_Learning.html">321 hunch net-2008-10-19-NIPS 2008 workshop on Kernel Learning</a></p>
<p>20 0.070794635 <a title="187-tfidf-20" href="../hunch_net-2005/hunch_net-2005-06-10-Workshops_are_not_Conferences.html">80 hunch net-2005-06-10-Workshops are not Conferences</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.16), (1, -0.03), (2, 0.002), (3, 0.056), (4, -0.025), (5, 0.013), (6, 0.049), (7, 0.012), (8, -0.002), (9, 0.021), (10, -0.023), (11, -0.037), (12, 0.046), (13, 0.056), (14, 0.055), (15, -0.013), (16, 0.054), (17, 0.043), (18, -0.044), (19, -0.001), (20, -0.019), (21, -0.037), (22, -0.002), (23, -0.049), (24, -0.006), (25, -0.012), (26, -0.022), (27, -0.117), (28, -0.116), (29, -0.009), (30, -0.044), (31, -0.052), (32, -0.099), (33, 0.068), (34, -0.006), (35, -0.076), (36, 0.032), (37, 0.077), (38, -0.008), (39, -0.017), (40, 0.01), (41, 0.021), (42, -0.031), (43, -0.024), (44, -0.051), (45, 0.037), (46, 0.016), (47, 0.09), (48, 0.051), (49, -0.009)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9772768 <a title="187-lsi-1" href="../hunch_net-2006/hunch_net-2006-06-25-Presentation_of_Proofs_is_Hard..html">187 hunch net-2006-06-25-Presentation of Proofs is Hard.</a></p>
<p>Introduction: When presenting part of the  Reinforcement Learning theory tutorial  at  ICML 2006 , I was forcibly reminded of this.
 
There are several difficulties.
  
  When creating the presentation, the correct level of detail is tricky.  With too much detail, the proof takes too much time and people may be lost to boredom.  With too little detail, the steps of the proof involve too-great a jump. This is very difficult to judge.
 
 What may be an easy step in the careful thought of a quiet room is not so easy when you are occupied by the process of presentation. 
 What may be easy after having gone over this (and other) proofs is not so easy to follow in the first pass by a viewer. 
 

  These problems seem only correctable by process of repeated test-and-revise.
 
 When presenting the proof, simply speaking with sufficient precision is substantially harder than in normal conversation (where precision is not so critical).  Practice can help here. 
 When presenting the proof, going at the right p</p><p>2 0.73827964 <a title="187-lsi-2" href="../hunch_net-2007/hunch_net-2007-06-21-Presentation_Preparation.html">249 hunch net-2007-06-21-Presentation Preparation</a></p>
<p>Introduction: A big part of doing research is presenting it at a conference.  Since many people start out shy of public presentations, this can be a substantial challenge.  Here are a few notes which might be helpful when thinking about preparing a presentation on research.
  
  Motivate .  Talks which don’t start by describing the problem to solve cause many people to zone out. 
  Prioritize .  It is typical that you have more things to say than time to say them, and many presenters fall into the failure mode of trying to say too much.  This is an easy-to-understand failure mode as it’s very natural to want to include everything.  A basic fact is: you can’t.  Example of this are:
 
 Your slides are so densely full of equations and words that you can’t cover them. 
 Your talk runs over and a moderator prioritizes for you by cutting you off. 
 You motor-mouth through the presentation, and the information absorption rate of the audience prioritizes in some uncontrolled fashion. 
 The rate of flow of c</p><p>3 0.72415614 <a title="187-lsi-3" href="../hunch_net-2006/hunch_net-2006-03-09-Use_of_Notation.html">162 hunch net-2006-03-09-Use of Notation</a></p>
<p>Introduction: For most people, a mathematical notation is like a language: you learn it and stick with it.  For people doing mathematical research, however, this is not enough: they must design new notations for new problems.  The design of good notation is both hard and worthwhile since a bad initial notation can retard a line of research greatly.  
 
Before we had mathematical notation, equations were all written out in language.  Since words have multiple meanings and variable precedences, long equations written out in language can be extraordinarily difficult and sometimes fundamentally ambiguous.  A good representative example of this is the legalese in the tax code.  Since we want greater precision and clarity, we adopt mathematical notation.
 
One fundamental thing to understand about mathematical notation, is that humans as logic verifiers, are barely capable.  This is the fundamental reason why one notation can be much better than another.  This observation is easier to miss than you might</p><p>4 0.7080552 <a title="187-lsi-4" href="../hunch_net-2006/hunch_net-2006-08-10-Precision_is_not_accuracy.html">202 hunch net-2006-08-10-Precision is not accuracy</a></p>
<p>Introduction: In my experience, there are two different groups of people who believe the same thing: the mathematics encountered in typical machine learning conference papers is often of questionable value. 
The two groups who agree on this are applied machine learning people who have given up on math, and mature theoreticians who understand the limits of theory. 
 
Partly, this is just a statement about where we are with respect to machine learning.  In particular, we have no mechanism capable of generating a prescription for how to solve all learning problems.  In the absence of such certainty, people try to come up with formalisms that partially describe and motivate how and why they do things.  This is natural and healthy—we might hope that it will eventually lead to just such a mechanism.
 
But, part of this is simply an emphasis on complexity over clarity.  A very natural and simple theoretical statement is often obscured by complexifications.  Common sources of complexification include:</p><p>5 0.66677922 <a title="187-lsi-5" href="../hunch_net-2008/hunch_net-2008-07-04-More_Presentation_Preparation.html">307 hunch net-2008-07-04-More Presentation Preparation</a></p>
<p>Introduction: We’ve discussed  presentation preparation before , but I have one more thing to add:  transitioning .  For a research presentation, it is substantially helpful for the audience if transitions are clear.  A common outline for a research presentation in machine leanring is:
  
  The problem .  Presentations which don’t describe the problem almost immediately lose people, because the context is missing to understand the detail. 
  Prior relevant work .  In many cases, a paper builds on some previous bit of work which must be understood in order to understand what the paper does.  A common failure mode seems to be spending too much time on prior work.  Discuss just the relevant aspects of prior work in the language of your work.  Sometimes this is missing when unneeded. 
  What we did . For theory papers in particular, it is often not possible to really cover the details.  Prioritizing what you present can be very important. 
  How it worked .  Many papers in Machine Learning have some sor</p><p>6 0.6080184 <a title="187-lsi-6" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>7 0.60399967 <a title="187-lsi-7" href="../hunch_net-2005/hunch_net-2005-07-10-Thinking_the_Unthought.html">91 hunch net-2005-07-10-Thinking the Unthought</a></p>
<p>8 0.60187006 <a title="187-lsi-8" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>9 0.5812853 <a title="187-lsi-9" href="../hunch_net-2005/hunch_net-2005-05-29-Bad_ideas.html">76 hunch net-2005-05-29-Bad ideas</a></p>
<p>10 0.57498366 <a title="187-lsi-10" href="../hunch_net-2005/hunch_net-2005-08-22-Do_you_believe_in_induction%3F.html">104 hunch net-2005-08-22-Do you believe in induction?</a></p>
<p>11 0.54031885 <a title="187-lsi-11" href="../hunch_net-2005/hunch_net-2005-03-21-Research_Styles_in_Machine_Learning.html">44 hunch net-2005-03-21-Research Styles in Machine Learning</a></p>
<p>12 0.52213174 <a title="187-lsi-12" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>13 0.51934695 <a title="187-lsi-13" href="../hunch_net-2005/hunch_net-2005-10-26-Fallback_Analysis_is_a_Secret_to_Useful_Algorithms.html">126 hunch net-2005-10-26-Fallback Analysis is a Secret to Useful Algorithms</a></p>
<p>14 0.51184428 <a title="187-lsi-14" href="../hunch_net-2005/hunch_net-2005-04-16-Which_Assumptions_are_Reasonable%3F.html">57 hunch net-2005-04-16-Which Assumptions are Reasonable?</a></p>
<p>15 0.50962913 <a title="187-lsi-15" href="../hunch_net-2007/hunch_net-2007-02-10-Best_Practices_for_Collaboration.html">231 hunch net-2007-02-10-Best Practices for Collaboration</a></p>
<p>16 0.50827366 <a title="187-lsi-16" href="../hunch_net-2007/hunch_net-2007-07-28-Asking_questions.html">257 hunch net-2007-07-28-Asking questions</a></p>
<p>17 0.48985958 <a title="187-lsi-17" href="../hunch_net-2005/hunch_net-2005-03-17-Going_all_the_Way%2C_Sometimes.html">42 hunch net-2005-03-17-Going all the Way, Sometimes</a></p>
<p>18 0.48456082 <a title="187-lsi-18" href="../hunch_net-2005/hunch_net-2005-05-17-A_Short_Guide_to_PhD_Graduate_Study.html">73 hunch net-2005-05-17-A Short Guide to PhD Graduate Study</a></p>
<p>19 0.47084263 <a title="187-lsi-19" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>20 0.46741021 <a title="187-lsi-20" href="../hunch_net-2006/hunch_net-2006-01-08-Debugging_Your_Brain.html">147 hunch net-2006-01-08-Debugging Your Brain</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(27, 0.134), (38, 0.018), (53, 0.072), (55, 0.144), (56, 0.38), (94, 0.081), (95, 0.065)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.91846651 <a title="187-lda-1" href="../hunch_net-2006/hunch_net-2006-06-25-Presentation_of_Proofs_is_Hard..html">187 hunch net-2006-06-25-Presentation of Proofs is Hard.</a></p>
<p>Introduction: When presenting part of the  Reinforcement Learning theory tutorial  at  ICML 2006 , I was forcibly reminded of this.
 
There are several difficulties.
  
  When creating the presentation, the correct level of detail is tricky.  With too much detail, the proof takes too much time and people may be lost to boredom.  With too little detail, the steps of the proof involve too-great a jump. This is very difficult to judge.
 
 What may be an easy step in the careful thought of a quiet room is not so easy when you are occupied by the process of presentation. 
 What may be easy after having gone over this (and other) proofs is not so easy to follow in the first pass by a viewer. 
 

  These problems seem only correctable by process of repeated test-and-revise.
 
 When presenting the proof, simply speaking with sufficient precision is substantially harder than in normal conversation (where precision is not so critical).  Practice can help here. 
 When presenting the proof, going at the right p</p><p>2 0.80420363 <a title="187-lda-2" href="../hunch_net-2007/hunch_net-2007-06-23-Machine_Learning_Jobs_are_Growing_on_Trees.html">250 hunch net-2007-06-23-Machine Learning Jobs are Growing on Trees</a></p>
<p>Introduction: The consensus of several discussions at ICML is that the number of jobs for people knowing machine learning well substantially exceeds supply.  This is my experience as well.  Demand comes from many places, but I’ve seen particularly strong demand from trading companies and internet startups.
 
Like all interest bursts, this one will probably pass because of economic recession or other distractions.  Nevertheless, the general outlook for machine learning in business seems to be good.  Machine learning is all about optimization when there is uncertainty and lots of data.  The quantity of data available is growing quickly as computer-run processes and sensors become more common, and the quality of the data is dropping since there is little editorial control in it’s collection.  Machine Learning is a difficult subject to master (*), so those who do should remain in demand over the long term.
 
(*) In fact, it would be reasonable to claim that no one has mastered it—there are just some peo</p><p>3 0.79008037 <a title="187-lda-3" href="../hunch_net-2008/hunch_net-2008-07-04-More_Presentation_Preparation.html">307 hunch net-2008-07-04-More Presentation Preparation</a></p>
<p>Introduction: We’ve discussed  presentation preparation before , but I have one more thing to add:  transitioning .  For a research presentation, it is substantially helpful for the audience if transitions are clear.  A common outline for a research presentation in machine leanring is:
  
  The problem .  Presentations which don’t describe the problem almost immediately lose people, because the context is missing to understand the detail. 
  Prior relevant work .  In many cases, a paper builds on some previous bit of work which must be understood in order to understand what the paper does.  A common failure mode seems to be spending too much time on prior work.  Discuss just the relevant aspects of prior work in the language of your work.  Sometimes this is missing when unneeded. 
  What we did . For theory papers in particular, it is often not possible to really cover the details.  Prioritizing what you present can be very important. 
  How it worked .  Many papers in Machine Learning have some sor</p><p>4 0.75979412 <a title="187-lda-4" href="../hunch_net-2009/hunch_net-2009-05-24-2009_ICML_discussion_site.html">356 hunch net-2009-05-24-2009 ICML discussion site</a></p>
<p>Introduction: Mark Reid  has setup a  discussion site for ICML papers  again this year and  Monica Dinculescu  has linked it in from the ICML site.  Last year’s attempt appears to have been an acceptable but not wild success as a little bit of fruitful discussion occurred.  I’m hoping this year will be a bit more of a success—please don’t be shy   
 
I’d like to also point out that  ICML ‘s early  registration  deadline has a few hours left, while  UAI ‘s and  COLT ‘s are in a week.</p><p>5 0.72086519 <a title="187-lda-5" href="../hunch_net-2012/hunch_net-2012-03-24-David_Waltz.html">460 hunch net-2012-03-24-David Waltz</a></p>
<p>Introduction: has  died .   He lived a full life.  I know him personally as a founder of the  Center for Computational Learning Systems  and the  New York Machine Learning Symposium , both of which have sheltered and promoted the advancement of machine learning.  I expect much of the New York area machine learning community will miss him, as well as many others around the world.</p><p>6 0.71998388 <a title="187-lda-6" href="../hunch_net-2006/hunch_net-2006-08-10-Precision_is_not_accuracy.html">202 hunch net-2006-08-10-Precision is not accuracy</a></p>
<p>7 0.54261583 <a title="187-lda-7" href="../hunch_net-2007/hunch_net-2007-06-21-Presentation_Preparation.html">249 hunch net-2007-06-21-Presentation Preparation</a></p>
<p>8 0.53271699 <a title="187-lda-8" href="../hunch_net-2010/hunch_net-2010-10-29-To_Vidoelecture_or_not.html">416 hunch net-2010-10-29-To Vidoelecture or not</a></p>
<p>9 0.53072 <a title="187-lda-9" href="../hunch_net-2009/hunch_net-2009-11-23-ICML_2009_Workshops_%28and_Tutorials%29.html">379 hunch net-2009-11-23-ICML 2009 Workshops (and Tutorials)</a></p>
<p>10 0.52669466 <a title="187-lda-10" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>11 0.52587575 <a title="187-lda-11" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>12 0.52227604 <a title="187-lda-12" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>13 0.51884288 <a title="187-lda-13" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>14 0.50445718 <a title="187-lda-14" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<p>15 0.50024474 <a title="187-lda-15" href="../hunch_net-2005/hunch_net-2005-03-13-Avoiding_Bad_Reviewing.html">40 hunch net-2005-03-13-Avoiding Bad Reviewing</a></p>
<p>16 0.49905956 <a title="187-lda-16" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>17 0.49854651 <a title="187-lda-17" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>18 0.49534288 <a title="187-lda-18" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<p>19 0.49072778 <a title="187-lda-19" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<p>20 0.48743856 <a title="187-lda-20" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
