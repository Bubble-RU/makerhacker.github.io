<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>176 hunch net-2006-05-01-A conversation between Theo and Pat</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-176" href="#">hunch_net-2006-176</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>176 hunch net-2006-05-01-A conversation between Theo and Pat</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-176-html" href="http://hunch.net/?p=187">html</a></p><p>Introduction: Pat (the practitioner)I need to do multiclass classification and I only have a
decision tree.Theo (the thoeretician)Use anerror correcting output code.PatOh,
that's cool. But the created binary problems seem unintuitive. I'm not sure
the decision tree can solve them.TheoOh? Is your problem a decision
list?PatNo, I don't think so.TheoHmm. Are the classes well separated by axis
aligned splits?PatErr, maybe. I'm not sure.TheoWell, if they are, under the
IID assumption I can tell you how many samples you need.PatIID? The data is
definitely not IID.TheoOh dear.PatCan we get back to the choice of ECOC? I
suspect we need to build it dynamically in response to which subsets of the
labels are empirically separable from each other.TheoOk. What do you know
about your problem?PatNot much. My friend just gave me the dataset.TheoThen,
no one can help you.Pat(What a fuzzy thinker. Theo keeps jumping to
assumptions that just aren't true.)Theo(What a fuzzy thinker. Pat's problem is
unsolvable without m</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('theo', 0.462), ('fuzzy', 0.308), ('pat', 0.308), ('lives', 0.212), ('assumptions', 0.148), ('decision', 0.143), ('listen', 0.137), ('magic', 0.137), ('problem', 0.129), ('axis', 0.127), ('ecoc', 0.127), ('aligned', 0.127), ('dynamically', 0.127), ('practitioner', 0.127), ('love', 0.12), ('subsets', 0.12), ('keeps', 0.114), ('friend', 0.106), ('heard', 0.106), ('world', 0.103)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="176-tfidf-1" href="../hunch_net-2006/hunch_net-2006-05-01-A_conversation_between_Theo_and_Pat.html">176 hunch net-2006-05-01-A conversation between Theo and Pat</a></p>
<p>Introduction: Pat (the practitioner)I need to do multiclass classification and I only have a
decision tree.Theo (the thoeretician)Use anerror correcting output code.PatOh,
that's cool. But the created binary problems seem unintuitive. I'm not sure
the decision tree can solve them.TheoOh? Is your problem a decision
list?PatNo, I don't think so.TheoHmm. Are the classes well separated by axis
aligned splits?PatErr, maybe. I'm not sure.TheoWell, if they are, under the
IID assumption I can tell you how many samples you need.PatIID? The data is
definitely not IID.TheoOh dear.PatCan we get back to the choice of ECOC? I
suspect we need to build it dynamically in response to which subsets of the
labels are empirically separable from each other.TheoOk. What do you know
about your problem?PatNot much. My friend just gave me the dataset.TheoThen,
no one can help you.Pat(What a fuzzy thinker. Theo keeps jumping to
assumptions that just aren't true.)Theo(What a fuzzy thinker. Pat's problem is
unsolvable without m</p><p>2 0.11137071 <a title="176-tfidf-2" href="../hunch_net-2006/hunch_net-2006-05-23-What_is_the_best_regret_transform_reduction_from_multiclass_to_binary%3F.html">181 hunch net-2006-05-23-What is the best regret transform reduction from multiclass to binary?</a></p>
<p>Introduction: This post is about an open problem in learning reductions.BackgroundA
reduction might transform a a multiclass prediction problem where there
arekpossible labels into a binary learning problem where there are only 2
possible labels. On this induced binary problem we might learn a binary
classifier with some error ratee. After subtracting the minimum possible
(Bayes) error rateb, we get a regretr = e - b. ThePECOC(Probabilistic Error
Correcting Output Code) reduction has the property that binary regretrimplies
multiclass regret at most4r0.5.The problemThis is not the "rightest" answer.
Consider thek=2case, where we reduce binary to binary. There exists a
reduction (the identity) with the property that regretrimplies regretr. This
is substantially superior to the transform given by the PECOC reduction, which
suggests that a better reduction may exist for generalk. For example, we can
not rule out the possibility that a reductionRexists with regret transform
guaranteeing binary regretrimp</p><p>3 0.10173221 <a title="176-tfidf-3" href="../hunch_net-2009/hunch_net-2009-10-03-Static_vs._Dynamic_multiclass_prediction.html">373 hunch net-2009-10-03-Static vs. Dynamic multiclass prediction</a></p>
<p>Introduction: I have had interesting discussions about distinction between static vs.
dynamic classes withKishoreandHal.The distinction arises in multiclass
prediction settings. A static set of classes is given by a set of
labels{1,â&euro;Ś,k}and the goal is generally to choose the most likely label given
features. The static approach is the one that we typically analyze and think
about in machine learning.The dynamic setting is one that is often used in
practice. The basic idea is that the number of classes is not fixed, varying
on a per example basis. These different classes are generally defined by a
choice of features.The distinction between these two settings as far as theory
goes, appears to be very substantial. For example, in the static setting,
inlearning reductions land, we have techniques now for robustO(log(k))time
prediction in many multiclass setting variants. In the dynamic setting, the
best techniques known areO(k), and furthermore this exponential gap may be
essential, at least without fur</p><p>4 0.09555006 <a title="176-tfidf-4" href="../hunch_net-2005/hunch_net-2005-05-16-Regret_minimizing_vs_error_limiting_reductions.html">72 hunch net-2005-05-16-Regret minimizing vs error limiting reductions</a></p>
<p>Introduction: This post is about a reductions-related problem that I find mysterious. There
are two kinds of reductions analysis currently under consideration.Error
limiting reductions. Here, the goal is to bound the error rate of the created
classifier in terms of the error rate of the binary classifiers that you
reduce to. A very simple example of this is thaterror correcting output
codeswhere it is possible to prove that for certain codes, the multiclass
error rate is at most 4 * the binary classifier error rate.Regret minimizing
reductions. Here, the goal is to bound theregretof the created classifier in
terms of theregretof the binary classifiers reduced to. The regret is the
error rate minus the minimum error rate. When the learning problem is noisy
the minimum error rate may not be0. An analagous result for reget is that for
aprobabilistic error correcting output code, multiclass regret is at most 4 *
(binary regret)0.5.The use of "regret" is more desirable than the use of error
rates, becaus</p><p>5 0.092046663 <a title="176-tfidf-5" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>Introduction: Attempts to abstract and study machine learning are within some given
framework or mathematical model. It turns out that all of these models are
significantly flawed for the purpose of studying machine learning. I've
created a table (below) outlining the major flaws in some common models of
machine learning.The point here is not simply "woe unto us". There are several
implications which seem important.The multitude of models is a point of
continuing confusion. It is common for people to learn about machine learning
within one framework which often becomes there "home framework" through which
they attempt to filter all machine learning. (Have you met people who can only
think in terms of kernels? Only via Bayes Law? Only via PAC Learning?)
Explicitly understanding the existence of these other frameworks can help
resolve the confusion. This is particularly important when reviewing and
particularly important for students.Algorithms which conform to multiple
approaches can have substantial</p><p>6 0.091312557 <a title="176-tfidf-6" href="../hunch_net-2005/hunch_net-2005-01-31-Watchword%3A_Assumption.html">7 hunch net-2005-01-31-Watchword: Assumption</a></p>
<p>7 0.082522303 <a title="176-tfidf-7" href="../hunch_net-2007/hunch_net-2007-05-08-Conditional_Tournaments_for_Multiclass_to_Binary.html">243 hunch net-2007-05-08-Conditional Tournaments for Multiclass to Binary</a></p>
<p>8 0.081656091 <a title="176-tfidf-8" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>9 0.077798553 <a title="176-tfidf-9" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>10 0.077248693 <a title="176-tfidf-10" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>11 0.076503068 <a title="176-tfidf-11" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>12 0.073401764 <a title="176-tfidf-12" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>13 0.072029144 <a title="176-tfidf-13" href="../hunch_net-2005/hunch_net-2005-11-02-Progress_in_Active_Learning.html">127 hunch net-2005-11-02-Progress in Active Learning</a></p>
<p>14 0.071958773 <a title="176-tfidf-14" href="../hunch_net-2005/hunch_net-2005-02-03-Learning_Theory%2C_by_assumption.html">12 hunch net-2005-02-03-Learning Theory, by assumption</a></p>
<p>15 0.068560377 <a title="176-tfidf-15" href="../hunch_net-2008/hunch_net-2008-11-16-Observations_on_Linearity_for_Reductions_to_Regression.html">327 hunch net-2008-11-16-Observations on Linearity for Reductions to Regression</a></p>
<p>16 0.068348423 <a title="176-tfidf-16" href="../hunch_net-2005/hunch_net-2005-04-16-Which_Assumptions_are_Reasonable%3F.html">57 hunch net-2005-04-16-Which Assumptions are Reasonable?</a></p>
<p>17 0.064014725 <a title="176-tfidf-17" href="../hunch_net-2005/hunch_net-2005-10-07-On-line_learning_of_regular_decision_rules.html">118 hunch net-2005-10-07-On-line learning of regular decision rules</a></p>
<p>18 0.063880637 <a title="176-tfidf-18" href="../hunch_net-2005/hunch_net-2005-05-06-Don%26%238217%3Bt_mix_the_solution_into_the_problem.html">67 hunch net-2005-05-06-Don&#8217;t mix the solution into the problem</a></p>
<p>19 0.063320398 <a title="176-tfidf-19" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>20 0.059969023 <a title="176-tfidf-20" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.14), (1, -0.069), (2, -0.032), (3, -0.027), (4, -0.018), (5, -0.102), (6, -0.018), (7, -0.001), (8, 0.037), (9, 0.016), (10, 0.007), (11, 0.009), (12, 0.039), (13, 0.011), (14, -0.001), (15, -0.003), (16, -0.014), (17, 0.003), (18, -0.024), (19, 0.023), (20, -0.012), (21, 0.02), (22, 0.055), (23, 0.054), (24, 0.023), (25, 0.015), (26, 0.001), (27, 0.053), (28, 0.028), (29, 0.024), (30, -0.03), (31, 0.044), (32, 0.009), (33, 0.053), (34, -0.036), (35, -0.028), (36, -0.005), (37, 0.02), (38, 0.036), (39, 0.027), (40, 0.009), (41, 0.013), (42, -0.028), (43, 0.041), (44, -0.034), (45, 0.006), (46, -0.072), (47, 0.066), (48, -0.041), (49, 0.018)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94605589 <a title="176-lsi-1" href="../hunch_net-2006/hunch_net-2006-05-01-A_conversation_between_Theo_and_Pat.html">176 hunch net-2006-05-01-A conversation between Theo and Pat</a></p>
<p>Introduction: Pat (the practitioner)I need to do multiclass classification and I only have a
decision tree.Theo (the thoeretician)Use anerror correcting output code.PatOh,
that's cool. But the created binary problems seem unintuitive. I'm not sure
the decision tree can solve them.TheoOh? Is your problem a decision
list?PatNo, I don't think so.TheoHmm. Are the classes well separated by axis
aligned splits?PatErr, maybe. I'm not sure.TheoWell, if they are, under the
IID assumption I can tell you how many samples you need.PatIID? The data is
definitely not IID.TheoOh dear.PatCan we get back to the choice of ECOC? I
suspect we need to build it dynamically in response to which subsets of the
labels are empirically separable from each other.TheoOk. What do you know
about your problem?PatNot much. My friend just gave me the dataset.TheoThen,
no one can help you.Pat(What a fuzzy thinker. Theo keeps jumping to
assumptions that just aren't true.)Theo(What a fuzzy thinker. Pat's problem is
unsolvable without m</p><p>2 0.65518975 <a title="176-lsi-2" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>Introduction: What?Reductions are machines which turn solvers for one problem into solvers
for another problem.Why?Reductions are useful for several reasons.Laziness.
Reducing a problem to classification make at least 10 learning algorithms
available to solve a problem. Inventing 10 learning algorithms is quite a bit
of work. Similarly, programming a reduction is often trivial, while
programming a learning algorithm is a great deal of work.Crystallization. The
problems we often want to solve in learning are worst-case-impossible, but
average case feasible. By reducing all problems onto one or a few primitives,
we can fine tune these primitives to perform well on real-world problems with
greater precision due to the greater number of problems to validate
on.Theoretical Organization. By studying what reductions are easy vs. hard vs.
impossible, we can learn which problems are roughly equivalent in difficulty
and which are much harder.What we know now.Typesafe reductions. In the
beginning, there was th</p><p>3 0.64300686 <a title="176-lsi-3" href="../hunch_net-2010/hunch_net-2010-03-15-The_Efficient_Robust_Conditional_Probability_Estimation_Problem.html">391 hunch net-2010-03-15-The Efficient Robust Conditional Probability Estimation Problem</a></p>
<p>Introduction: I'm offering a reward of $1000 for a solution to this problem. This joins
thecross validation problemwhich I'm offering a$500 rewardfor. I believe both
of these problems are hard but plausibly solvable, and plausibly with a
solution of substantial practical value. While it's unlikely these rewards are
worth your time on an hourly wage basis, the recognition for solving them
definitely should beThe ProblemThe problem is finding a general, robust, and
efficient mechanism for estimating a conditional probabilityP(y|x)where
robustness and efficiency are measured using techniques from learning
reductions.In particular, suppose we have access to a binary regression
oracleBwhich has two interfaces--one for specifying training information and
one for testing. Training information is specified asB(x',y')wherex'is a
feature vector andy'is a scalar in[0,1]with no value returned. Testing is done
according toB(x')with a value in[0,1]returned.A learning reduction consists of
two algorithmsRandR-1whi</p><p>4 0.63574249 <a title="176-lsi-4" href="../hunch_net-2006/hunch_net-2006-09-09-How_to_solve_an_NP_hard_problem_in_quadratic_time.html">206 hunch net-2006-09-09-How to solve an NP hard problem in quadratic time</a></p>
<p>Introduction: This title is a lie, but it is a special lie which has a bit of
truth.Ifnplayers each play each other, you have a tournament. How do you order
the players from weakest to strongest?The standard first attempt is "find the
ordering which agrees with the tournament on as many player pairs as
possible". This is called the "minimum feedback arcset" problem in the CS
theory literature and it is a well known NP-hard problem. A basic guarantee
holds for the solution to this problem: if there is some "true" intrinsic
ordering, and the outcome of the tournament disagreesktimes (due to noise for
instance), then the output ordering will disagree with the original ordering
on at most2kedges (and no solution can be better).One standard approach to
tractably solving an NP-hard problem is to find another algorithm with an
approximation guarantee. For example,Don Coppersmith,Lisa FleischerandAtri
Rudraproved thatordering players according to the number of wins is a
5-approximation to the NP-hard proble</p><p>5 0.61736548 <a title="176-lsi-5" href="../hunch_net-2005/hunch_net-2005-02-27-Antilearning%3A_When_proximity_goes_bad.html">32 hunch net-2005-02-27-Antilearning: When proximity goes bad</a></p>
<p>Introduction: Joel Preddmentioned"Antilearning" byAdam Kowalczyk, which is interesting from
a foundational intuitions viewpoint.There is a pervasive intuition that
"nearby things tend to have the same label". This intuition is instantiated in
SVMs, nearest neighbor classifiers, decision trees, and neural networks. It
turns out there are natural problems where this intuition is opposite of the
truth.One natural situation where this occurs is in competition. For example,
whenIntelfails to meet its earnings estimate, is this evidence thatAMDis doing
badly also? Or evidence that AMD is doing well?This violation of the proximity
intuition means that when the number of examples is few,negatinga classifier
which attempts to exploit proximity can provide predictive power (thus, the
term "antilearning").</p><p>6 0.58782583 <a title="176-lsi-6" href="../hunch_net-2006/hunch_net-2006-05-23-What_is_the_best_regret_transform_reduction_from_multiclass_to_binary%3F.html">181 hunch net-2006-05-23-What is the best regret transform reduction from multiclass to binary?</a></p>
<p>7 0.5644021 <a title="176-lsi-7" href="../hunch_net-2005/hunch_net-2005-02-23-Problem%3A_Reinforcement_Learning_with_Classification.html">27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</a></p>
<p>8 0.56400883 <a title="176-lsi-8" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>9 0.5559929 <a title="176-lsi-9" href="../hunch_net-2005/hunch_net-2005-01-31-Watchword%3A_Assumption.html">7 hunch net-2005-01-31-Watchword: Assumption</a></p>
<p>10 0.55443436 <a title="176-lsi-10" href="../hunch_net-2005/hunch_net-2005-05-06-Don%26%238217%3Bt_mix_the_solution_into_the_problem.html">67 hunch net-2005-05-06-Don&#8217;t mix the solution into the problem</a></p>
<p>11 0.55170465 <a title="176-lsi-11" href="../hunch_net-2006/hunch_net-2006-02-18-Multiplication_of_Learned_Probabilities_is_Dangerous.html">157 hunch net-2006-02-18-Multiplication of Learned Probabilities is Dangerous</a></p>
<p>12 0.54641539 <a title="176-lsi-12" href="../hunch_net-2005/hunch_net-2005-04-16-Which_Assumptions_are_Reasonable%3F.html">57 hunch net-2005-04-16-Which Assumptions are Reasonable?</a></p>
<p>13 0.54454643 <a title="176-lsi-13" href="../hunch_net-2005/hunch_net-2005-10-07-On-line_learning_of_regular_decision_rules.html">118 hunch net-2005-10-07-On-line learning of regular decision rules</a></p>
<p>14 0.53662437 <a title="176-lsi-14" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>15 0.53429538 <a title="176-lsi-15" href="../hunch_net-2007/hunch_net-2007-05-08-Conditional_Tournaments_for_Multiclass_to_Binary.html">243 hunch net-2007-05-08-Conditional Tournaments for Multiclass to Binary</a></p>
<p>16 0.52149487 <a title="176-lsi-16" href="../hunch_net-2005/hunch_net-2005-08-22-Do_you_believe_in_induction%3F.html">104 hunch net-2005-08-22-Do you believe in induction?</a></p>
<p>17 0.51855743 <a title="176-lsi-17" href="../hunch_net-2007/hunch_net-2007-03-15-Alternative_Machine_Learning_Reductions_Definitions.html">236 hunch net-2007-03-15-Alternative Machine Learning Reductions Definitions</a></p>
<p>18 0.5064348 <a title="176-lsi-18" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>19 0.5014236 <a title="176-lsi-19" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>20 0.49884221 <a title="176-lsi-20" href="../hunch_net-2005/hunch_net-2005-08-08-Apprenticeship_Reinforcement_Learning_for_Control.html">101 hunch net-2005-08-08-Apprenticeship Reinforcement Learning for Control</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(35, 0.037), (42, 0.216), (45, 0.038), (50, 0.072), (74, 0.096), (90, 0.418)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.83927619 <a title="176-lda-1" href="../hunch_net-2006/hunch_net-2006-05-01-A_conversation_between_Theo_and_Pat.html">176 hunch net-2006-05-01-A conversation between Theo and Pat</a></p>
<p>Introduction: Pat (the practitioner)I need to do multiclass classification and I only have a
decision tree.Theo (the thoeretician)Use anerror correcting output code.PatOh,
that's cool. But the created binary problems seem unintuitive. I'm not sure
the decision tree can solve them.TheoOh? Is your problem a decision
list?PatNo, I don't think so.TheoHmm. Are the classes well separated by axis
aligned splits?PatErr, maybe. I'm not sure.TheoWell, if they are, under the
IID assumption I can tell you how many samples you need.PatIID? The data is
definitely not IID.TheoOh dear.PatCan we get back to the choice of ECOC? I
suspect we need to build it dynamically in response to which subsets of the
labels are empirically separable from each other.TheoOk. What do you know
about your problem?PatNot much. My friend just gave me the dataset.TheoThen,
no one can help you.Pat(What a fuzzy thinker. Theo keeps jumping to
assumptions that just aren't true.)Theo(What a fuzzy thinker. Pat's problem is
unsolvable without m</p><p>2 0.79152519 <a title="176-lda-2" href="../hunch_net-2007/hunch_net-2007-04-21-Videolectures.net.html">240 hunch net-2007-04-21-Videolectures.net</a></p>
<p>Introduction: Davorhas been working to setupvideolectures.netwhich is the new site for the
many lecturesmentioned here. (Tragically, they seem to only be available in
windows media format.) I went throughmy own projectsand added a few links to
the videos. The day when every result is a set of {paper, slides, video} isn't
quite here yet, but it's within sight. (For many papers, of course, code is a
4th component.)</p><p>3 0.62440753 <a title="176-lda-3" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>Introduction: At thelast ICML,Tom Dietterichasked me to look into systems for commenting on
papers. I've been slow getting to this, but it's relevant now.The essential
observation is that we now have many tools for online collaboration, but they
are not yet much used in academic research. If we can find the right way to
use them, then perhaps great things might happen, with extra kudos to the
first conference that manages to really create an online community. Various
conferences have been poking at this. For example,UAI has setup a wiki, COLT
hasstarted usingJoomla, with some dynamic content, and AAAI has been setting
up a "student blog". Similarly,Dinoj Surendransetup a twiki for theChicago
Machine Learning Summer School, which was quite useful for coordinating events
and other things.I believe the most important thing is a willingness to
experiment. A good place to start seems to be enhancing existing conference
websites. For example, theICML 2007 papers pageis basically only useful via
grep. A mu</p><p>4 0.5031898 <a title="176-lda-4" href="../hunch_net-2008/hunch_net-2008-11-28-A_Bumper_Crop_of_Machine_Learning_Graduates.html">329 hunch net-2008-11-28-A Bumper Crop of Machine Learning Graduates</a></p>
<p>Introduction: My impression is that this is a particularly strong year for machine learning
graduates. Here's my short list of the strong graduates I know. Analpha (for
perversity's sake) by last name:Jenn Wortmann. When Jenn visited us for the
summer, she hadone,two,three,fourpapers. That is typical--she's smart,
capable, and follows up many directions of research. I believe approximately
all of her many papers are on different subjects.Ruslan Salakhutdinov.
AScience paper on bijective dimensionality reduction, mastered and improved on
deep belief nets which seems like an important flavor of nonlinear learning,
and in my experience he's very fast, capable and creative at problem
solving.Marc'Aurelio Ranzato. I haven't spoken with Marc very much, but he had
a great visit at Yahoo! this summer, and has an impressive portfolio of
applications and improvements on convolutional neural networks and other deep
learning algorithms.Lihong Li. Lihong developed theKWIK ("Knows what it
Knows") learning framewo</p><p>5 0.49678978 <a title="176-lda-5" href="../hunch_net-2007/hunch_net-2007-02-02-Thoughts_regarding_%26%238220%3BIs_machine_learning_different_from_statistics%3F%26%238221%3B.html">230 hunch net-2007-02-02-Thoughts regarding &#8220;Is machine learning different from statistics?&#8221;</a></p>
<p>Introduction: Given John's recent posts on CMU's new machine learning department and "Deep
Learning," I asked for an opportunity to give a computational learning theory
perspective on these issues.To my mind, the answer to the question "Are the
core problems from machine learning different from the core problems of
statistics?" is a clear Yes. The point of this post is to describe a core
problem in machine learning that is computational in nature and will appeal to
statistical learning folk (as an extreme example note that if P=NP- which, for
all we know, is true- then we would suddenly find almost all of our favorite
machine learning problems considerably more tractable).If the central question
of statistical learning theory were crudely summarized as "given a hypothesis
with a certain loss bound over a test set, how well will it generalize?" then
the central question of computational learning theory might be "how can we
find such a hypothesis efficently (e.g., in polynomial-time)?"With this in
min</p><p>6 0.49165675 <a title="176-lda-6" href="../hunch_net-2005/hunch_net-2005-07-23-Interesting_papers_at_ACL.html">97 hunch net-2005-07-23-Interesting papers at ACL</a></p>
<p>7 0.48902923 <a title="176-lda-7" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>8 0.48645049 <a title="176-lda-8" href="../hunch_net-2011/hunch_net-2011-02-17-What_does_Watson_mean%3F.html">424 hunch net-2011-02-17-What does Watson mean?</a></p>
<p>9 0.48543081 <a title="176-lda-9" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>10 0.48469439 <a title="176-lda-10" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>11 0.48458889 <a title="176-lda-11" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>12 0.48440695 <a title="176-lda-12" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>13 0.48439223 <a title="176-lda-13" href="../hunch_net-2005/hunch_net-2005-12-07-Is_the_Google_way_the_way_for_machine_learning%3F.html">136 hunch net-2005-12-07-Is the Google way the way for machine learning?</a></p>
<p>14 0.48287672 <a title="176-lda-14" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>15 0.48271811 <a title="176-lda-15" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>16 0.4824301 <a title="176-lda-16" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>17 0.48170108 <a title="176-lda-17" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>18 0.48135772 <a title="176-lda-18" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>19 0.48071659 <a title="176-lda-19" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>20 0.48066166 <a title="176-lda-20" href="../hunch_net-2006/hunch_net-2006-04-05-What_is_state%3F.html">169 hunch net-2006-04-05-What is state?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
