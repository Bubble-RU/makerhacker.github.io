<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>173 hunch net-2006-04-17-Rexa is live</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-173" href="#">hunch_net-2006-173</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>173 hunch net-2006-04-17-Rexa is live</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-173-html" href="http://hunch.net/?p=184">html</a></p><p>Introduction: Rexa  is now publicly available.  Anyone can create an account and login.  
 
Rexa is similar to  Citeseer  and  Google Scholar  in functionality with more emphasis on the use of machine learning for intelligent information extraction.   For example, Rexa can automatically display a picture on an authorâ&euro;&trade;s homepage when the author is searched for.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Rexa is similar to  Citeseer  and  Google Scholar  in functionality with more emphasis on the use of machine learning for intelligent information extraction. [sent-3, score-0.834]
</p><p>2 For example, Rexa can automatically display a picture on an authorâ&euro;&trade;s homepage when the author is searched for. [sent-4, score-0.801]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('rexa', 0.71), ('citeseer', 0.237), ('scholar', 0.237), ('author', 0.229), ('picture', 0.213), ('functionality', 0.205), ('display', 0.205), ('publicly', 0.186), ('emphasis', 0.181), ('intelligent', 0.172), ('automatically', 0.154), ('google', 0.144), ('account', 0.137), ('anyone', 0.105), ('create', 0.099), ('similar', 0.095), ('information', 0.075), ('use', 0.054), ('example', 0.049), ('machine', 0.037), ('learning', 0.015)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="173-tfidf-1" href="../hunch_net-2006/hunch_net-2006-04-17-Rexa_is_live.html">173 hunch net-2006-04-17-Rexa is live</a></p>
<p>Introduction: Rexa  is now publicly available.  Anyone can create an account and login.  
 
Rexa is similar to  Citeseer  and  Google Scholar  in functionality with more emphasis on the use of machine learning for intelligent information extraction.   For example, Rexa can automatically display a picture on an authorâ&euro;&trade;s homepage when the author is searched for.</p><p>2 0.096778527 <a title="173-tfidf-2" href="../hunch_net-2005/hunch_net-2005-12-11-More_NIPS_Papers.html">139 hunch net-2005-12-11-More NIPS Papers</a></p>
<p>Introduction: Let me add to John’s post with a few of my own favourites 
from this year’s conference. First, let me say that 
Sanjoy’s talk,  Coarse Sample Complexity Bounds for Active 
Learning  was also one of my favourites, as was the 
  
Forgettron paper .
 

I also really enjoyed the last third of 
 Christos’  talk 
on the complexity of finding Nash equilibria.

 

And, speaking of tagging, I think 
the U.Mass Citeseer replacement system 
 Rexa  from the demo track is very cool.

 

Finally, let me add my recommendations for specific papers:
  
  Z. Ghahramani, K. Heller:  Bayesian Sets  
[no preprint] 
(A very elegant probabilistic information retrieval style model 
of which objects are “most like” a given subset of objects.)
 
 T. Griffiths, Z. Ghahramani:  Infinite Latent Feature Models and 
the Indian Buffet Process  
[  
preprint ] 
(A Dirichlet style prior over infinite binary matrices with 
beautiful exchangeability properties.)
 
 K. Weinberger, J. Blitzer, L. Saul:  Distance Metric Lea</p><p>3 0.08750394 <a title="173-tfidf-3" href="../hunch_net-2006/hunch_net-2006-09-18-What_is_missing_for_online_collaborative_research%3F.html">208 hunch net-2006-09-18-What is missing for online collaborative research?</a></p>
<p>Introduction: The internet has recently made the research process much smoother: papers are easy to obtain, citations are easy to follow, and unpublished “tutorials” are often available. Yet, new research fields can look very complicated to outsiders or newcomers. Every paper is like a small piece of an unfinished jigsaw puzzle: to understand just one publication, a researcher without experience in the field will typically have to follow several layers of citations, and many of the papers he encounters have a great deal of repeated information. Furthermore, from one publication to the next, notation and terminology may not be consistent which can further confuse the reader.
 
But the internet is now proving to be an extremely useful medium for collaboration and knowledge aggregation. Online forums allow users to ask and answer questions and to share ideas. The recent phenomenon of Wikipedia provides a proof-of-concept for the “anyone can edit” system. Can such models be used to facilitate research a</p><p>4 0.075093925 <a title="173-tfidf-4" href="../hunch_net-2005/hunch_net-2005-05-02-Reviewing_techniques_for_conferences.html">65 hunch net-2005-05-02-Reviewing techniques for conferences</a></p>
<p>Introduction: The many reviews following the many paper deadlines are just about over.  AAAI and ICML in particular were experimenting with several reviewing techniques.  
  
 Double Blind: AAAI and ICML were both double blind this year.  It seemed (overall) beneficial, but two problems arose.
 
 For theoretical papers, with a lot to say, authors often leave out the proofs.  This is very hard to cope with under a double blind review because (1) you can not trust the authors got the proof right but (2) a blanket “reject” hits many probably-good papers.  Perhaps authors should more strongly favor proof-complete papers sent to double blind conferences. 
 On the author side, double blind reviewing is actually somewhat disruptive to research.  In particular, it discourages the author from talking about the subject, which is one of the mechanisms of research.  This is not a great drawback, but it is one not previously appreciated. 
 
 
 Author feedback: AAAI and ICML did author feedback this year. It seem</p><p>5 0.072353102 <a title="173-tfidf-5" href="../hunch_net-2007/hunch_net-2007-12-17-New_Machine_Learning_mailing_list.html">278 hunch net-2007-12-17-New Machine Learning mailing list</a></p>
<p>Introduction: IMLS  (which is the nonprofit running ICML) has setup a new mailing list for  Machine Learning News .  The list address is ML-news@googlegroups.com, and signup requires a google account (which you can create).  Only members can send messages.</p><p>6 0.063251443 <a title="173-tfidf-6" href="../hunch_net-2006/hunch_net-2006-02-04-Research_Budget_Changes.html">154 hunch net-2006-02-04-Research Budget Changes</a></p>
<p>7 0.058073178 <a title="173-tfidf-7" href="../hunch_net-2012/hunch_net-2012-06-29-ICML_survey_and_comments.html">468 hunch net-2012-06-29-ICML survey and comments</a></p>
<p>8 0.057870008 <a title="173-tfidf-8" href="../hunch_net-2008/hunch_net-2008-11-04-Rise_of_the_Machines.html">323 hunch net-2008-11-04-Rise of the Machines</a></p>
<p>9 0.052657358 <a title="173-tfidf-9" href="../hunch_net-2012/hunch_net-2012-04-09-ICML_author_feedback_is_open.html">461 hunch net-2012-04-09-ICML author feedback is open</a></p>
<p>10 0.052293368 <a title="173-tfidf-10" href="../hunch_net-2006/hunch_net-2006-04-02-Mad_%28Neuro%29science.html">168 hunch net-2006-04-02-Mad (Neuro)science</a></p>
<p>11 0.052016336 <a title="173-tfidf-11" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<p>12 0.04916282 <a title="173-tfidf-12" href="../hunch_net-2007/hunch_net-2007-10-17-Online_as_the_new_adjective.html">267 hunch net-2007-10-17-Online as the new adjective</a></p>
<p>13 0.048350945 <a title="173-tfidf-13" href="../hunch_net-2006/hunch_net-2006-05-08-Big_machine_learning.html">178 hunch net-2006-05-08-Big machine learning</a></p>
<p>14 0.048342414 <a title="173-tfidf-14" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>15 0.045982964 <a title="173-tfidf-15" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>16 0.045695312 <a title="173-tfidf-16" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>17 0.044597059 <a title="173-tfidf-17" href="../hunch_net-2005/hunch_net-2005-10-03-Not_ICML.html">117 hunch net-2005-10-03-Not ICML</a></p>
<p>18 0.042772736 <a title="173-tfidf-18" href="../hunch_net-2008/hunch_net-2008-11-11-COLT_CFP.html">326 hunch net-2008-11-11-COLT CFP</a></p>
<p>19 0.04172571 <a title="173-tfidf-19" href="../hunch_net-2008/hunch_net-2008-12-12-Summer_Conferences.html">331 hunch net-2008-12-12-Summer Conferences</a></p>
<p>20 0.039405987 <a title="173-tfidf-20" href="../hunch_net-2006/hunch_net-2006-01-13-Benchmarks_for_RL.html">148 hunch net-2006-01-13-Benchmarks for RL</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.056), (1, -0.041), (2, 0.014), (3, 0.016), (4, -0.001), (5, -0.012), (6, -0.021), (7, -0.001), (8, -0.011), (9, 0.001), (10, -0.012), (11, -0.023), (12, -0.003), (13, -0.012), (14, -0.009), (15, -0.009), (16, -0.046), (17, -0.035), (18, -0.002), (19, 0.009), (20, 0.032), (21, -0.003), (22, 0.008), (23, -0.001), (24, 0.025), (25, -0.027), (26, 0.066), (27, 0.09), (28, -0.053), (29, 0.018), (30, -0.056), (31, 0.028), (32, 0.096), (33, -0.037), (34, 0.026), (35, 0.082), (36, -0.015), (37, -0.024), (38, -0.03), (39, -0.023), (40, -0.047), (41, -0.024), (42, -0.069), (43, -0.025), (44, -0.003), (45, 0.038), (46, 0.005), (47, 0.058), (48, -0.013), (49, 0.002)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9551881 <a title="173-lsi-1" href="../hunch_net-2006/hunch_net-2006-04-17-Rexa_is_live.html">173 hunch net-2006-04-17-Rexa is live</a></p>
<p>Introduction: Rexa  is now publicly available.  Anyone can create an account and login.  
 
Rexa is similar to  Citeseer  and  Google Scholar  in functionality with more emphasis on the use of machine learning for intelligent information extraction.   For example, Rexa can automatically display a picture on an authorâ&euro;&trade;s homepage when the author is searched for.</p><p>2 0.44668141 <a title="173-lsi-2" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>Introduction: The internet has significantly effected the way we do research but it’s capabilities have not yet been fully realized.
 
First, let’s acknowledge some known effects.
  
  Self-publishing  By default, all researchers in machine learning (and more generally computer science and physics) place their papers online for anyone to download.  The exact mechanism differs—physicists tend to use a central repository ( Arxiv ) while computer scientists tend to place the papers on their webpage.  Arxiv has been slowly growing in subject breadth so it now sometimes used by computer scientists. 
  Collaboration  Email has enabled working remotely with coauthors.  This has allowed collaborationis which would not otherwise have been possible and generally speeds research. 
  
Now, let’s look at attempts to go further.
  
  Blogs  (like this one) allow public discussion about topics which are not easily categorized as “a new idea in machine learning” (like this topic). 
  Organization  of some subfield</p><p>3 0.43368557 <a title="173-lsi-3" href="../hunch_net-2006/hunch_net-2006-05-08-Big_machine_learning.html">178 hunch net-2006-05-08-Big machine learning</a></p>
<p>Introduction: According to the  New York Times ,  Yahoo is releasing Project Panama shortly .  Project Panama is about better predicting which advertisements are relevant to a search, implying a higher click through rate, implying larger income for  Yahoo .  There are two things that seem interesting here:
  
 A significant portion of that improved accuracy is almost certainly machine learning at work. 
 The quantitative effect is huge—the estimate in the article is $600*10 6 . 
  
 Google  already has such improvements and  Microsoft Search  is surely working on them, which suggest this is (perhaps) a $10 9  per year machine learning problem. 
 
The exact methodology under use is unlikely to be publicly discussed in the near future because of the competitive enivironment.  Hopefully we’ll have some public “war stories” at some point in the future when this information becomes less sensitive.  For now, it’s reassuring to simply note that machine learning is having a big impact.</p><p>4 0.42662942 <a title="173-lsi-4" href="../hunch_net-2008/hunch_net-2008-12-12-Summer_Conferences.html">331 hunch net-2008-12-12-Summer Conferences</a></p>
<p>Introduction: Here’s a handy table for the summer conferences.
  
 
 Conference 
 Deadline 
 Reviewer Targeting 
 Double Blind 
 Author Feedback 
 Location 
 Date 
 
 
  ICML  ( wrong ICML ) 
 January 26 
 Yes 
 Yes 
 Yes 
 Montreal, Canada 
 June 14-17 
 
 
  COLT  
 February 13 
 No 
 No 
 Yes 
 Montreal 
 June 19-21 
 
 
  UAI  
 March 13 
 No 
 Yes 
 No 
 Montreal 
 June 19-21 
 
 
  KDD  
 February 2/6 
 No 
 No 
 No 
 Paris, France 
 June 28-July 1 
 
  
Reviewer targeting is new this year.  The idea is that many poor decisions happen because the papers go to reviewers who are unqualified, and the hope is that allowing authors to point out who is qualified results in better decisions.  In my experience, this is a reasonable idea to test.
 
Both UAI and COLT are experimenting this year as well with double blind and author feedback, respectively.  Of the two, I believe author feedback is more important, as I’ve seen it make a difference.  However, I still consider double blind reviewing a net wi</p><p>5 0.41498724 <a title="173-lsi-5" href="../hunch_net-2008/hunch_net-2008-11-11-COLT_CFP.html">326 hunch net-2008-11-11-COLT CFP</a></p>
<p>Introduction: Adam Klivans , points out the  COLT call for papers .  The important points are: 
  
 Due Feb 13. 
 Montreal, June 18-21. 
 This year, there is author feedback.</p><p>6 0.41280812 <a title="173-lsi-6" href="../hunch_net-2007/hunch_net-2007-12-17-New_Machine_Learning_mailing_list.html">278 hunch net-2007-12-17-New Machine Learning mailing list</a></p>
<p>7 0.41217548 <a title="173-lsi-7" href="../hunch_net-2008/hunch_net-2008-11-04-Rise_of_the_Machines.html">323 hunch net-2008-11-04-Rise of the Machines</a></p>
<p>8 0.41156685 <a title="173-lsi-8" href="../hunch_net-2005/hunch_net-2005-06-13-Wikis_for_Summer_Schools_and_Workshops.html">81 hunch net-2005-06-13-Wikis for Summer Schools and Workshops</a></p>
<p>9 0.40077761 <a title="173-lsi-9" href="../hunch_net-2009/hunch_net-2009-05-17-Server_Update.html">354 hunch net-2009-05-17-Server Update</a></p>
<p>10 0.38106811 <a title="173-lsi-10" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>11 0.37847382 <a title="173-lsi-11" href="../hunch_net-2005/hunch_net-2005-05-02-Reviewing_techniques_for_conferences.html">65 hunch net-2005-05-02-Reviewing techniques for conferences</a></p>
<p>12 0.37534225 <a title="173-lsi-12" href="../hunch_net-2006/hunch_net-2006-10-04-Health_of_Conferences_Wiki.html">212 hunch net-2006-10-04-Health of Conferences Wiki</a></p>
<p>13 0.36006245 <a title="173-lsi-13" href="../hunch_net-2006/hunch_net-2006-09-18-What_is_missing_for_online_collaborative_research%3F.html">208 hunch net-2006-09-18-What is missing for online collaborative research?</a></p>
<p>14 0.35160342 <a title="173-lsi-14" href="../hunch_net-2005/hunch_net-2005-02-02-Kolmogorov_Complexity_and_Googling.html">10 hunch net-2005-02-02-Kolmogorov Complexity and Googling</a></p>
<p>15 0.35140479 <a title="173-lsi-15" href="../hunch_net-2009/hunch_net-2009-02-16-KDNuggets.html">342 hunch net-2009-02-16-KDNuggets</a></p>
<p>16 0.34830639 <a title="173-lsi-16" href="../hunch_net-2005/hunch_net-2005-02-19-Machine_learning_reading_groups.html">24 hunch net-2005-02-19-Machine learning reading groups</a></p>
<p>17 0.3425056 <a title="173-lsi-17" href="../hunch_net-2009/hunch_net-2009-07-09-The_Machine_Learning_Forum.html">363 hunch net-2009-07-09-The Machine Learning Forum</a></p>
<p>18 0.33618805 <a title="173-lsi-18" href="../hunch_net-2006/hunch_net-2006-07-09-The_Stock_Prediction_Machine_Learning_Problem.html">193 hunch net-2006-07-09-The Stock Prediction Machine Learning Problem</a></p>
<p>19 0.33407357 <a title="173-lsi-19" href="../hunch_net-2005/hunch_net-2005-10-13-Site_tweak.html">122 hunch net-2005-10-13-Site tweak</a></p>
<p>20 0.33151293 <a title="173-lsi-20" href="../hunch_net-2005/hunch_net-2005-10-03-Not_ICML.html">117 hunch net-2005-10-03-Not ICML</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(27, 0.051), (55, 0.146), (62, 0.589)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.91810131 <a title="173-lda-1" href="../hunch_net-2006/hunch_net-2006-04-17-Rexa_is_live.html">173 hunch net-2006-04-17-Rexa is live</a></p>
<p>Introduction: Rexa  is now publicly available.  Anyone can create an account and login.  
 
Rexa is similar to  Citeseer  and  Google Scholar  in functionality with more emphasis on the use of machine learning for intelligent information extraction.   For example, Rexa can automatically display a picture on an authorâ&euro;&trade;s homepage when the author is searched for.</p><p>2 0.70117819 <a title="173-lda-2" href="../hunch_net-2006/hunch_net-2006-07-12-Who_is_having_visa_problems_reaching_US_conferences%3F.html">195 hunch net-2006-07-12-Who is having visa problems reaching US conferences?</a></p>
<p>Introduction: Many of the large machine learning conferences were in the US this summer.  A common problem which students from abroad encounter is visa issues. 
Just getting a visa to visit can be pretty rough: you stand around in lines, sometimes for days.  Even worse is the timing with respect to ticket buying.  Airplane tickets typically need to be bought well in advance on nonrefundable terms to secure a reasonable rate for air travel.  When a visa is denied, as happens reasonably often, a very expensive ticket is burnt.
 
A serious effort is under way to raise this as in issue in need of fixing.  Over the long term, effectively driving research conferences to locate outside of the US seems an unwise policy.   Robert Schapire  is planning to talk to a congressman.   Sally Goldman  suggested putting together a list of problem cases, and  Phil Long  setup an email address  immigration.and.confs@gmail.com  to collect them.
 
If you (or someone you know) has had insurmountable difficulties reaching</p><p>3 0.60971045 <a title="173-lda-3" href="../hunch_net-2010/hunch_net-2010-04-24-COLT_Treasurer_is_now_Phil_Long.html">394 hunch net-2010-04-24-COLT Treasurer is now Phil Long</a></p>
<p>Introduction: For about 5 years, I’ve been the treasurer of the Association for Computational Learning, otherwise known as COLT, taking over from  John Case  before me.  A transfer of duties to  Phil Long  is now about complete.  This probably matters to almost no one, but I wanted to describe things a bit for those interested.
 
The immediate impetus for this decision was unhappiness over reviewing decisions at  COLT 2009 , one as an author and several as a member of the program committee.  I seem to have disagreements fairly often about what is important work, partly because I’m focused on learning theory with practical implications, partly because I define learning theory more broadly than is typical amongst COLT members, and partly because COLT suffers a bit from insider-clique issues.  The degree to which these issues come up varies substantially each year so last year is not predictive of this one.  And, it’s important to understand that COLT remains healthy with these issues not nearly so bad</p><p>4 0.55067849 <a title="173-lda-4" href="../hunch_net-2005/hunch_net-2005-11-05-The_design_of_a_computing_cluster.html">128 hunch net-2005-11-05-The design of a computing cluster</a></p>
<p>Introduction: This is about the design of a computing cluster from the viewpoint of applied machine learning using current technology.  We just built a small one at TTI so this is some evidence of what is feasible and thoughts about the design choices.
  
  Architecture   There are several architectural choices.
 
 AMD Athlon64 based system.  This seems to have the cheapest bang/buck.  Maximum RAM is typically 2-3GB. 
 AMD Opteron based system. Opterons provide the additional capability to buy an SMP motherboard with two chips, and the motherboards often support 16GB of RAM.  The RAM is also the more expensive error correcting type. 
 Intel PIV or Xeon based system.  The PIV and Xeon based systems are the intel analog of the above 2.  Due to architectural design reasons, these chips tend to run a bit hotter and be a bit more expensive. 
 Dual core chips.  Both Intel and AMD have chips that actually have 2 processors embedded in them. 

In the end, we decided to go with option (2).  Roughly speaking,</p><p>5 0.48127806 <a title="173-lda-5" href="../hunch_net-2007/hunch_net-2007-09-18-It%26%238217%3Bs_MDL_Jim%2C_but_not_as_we_know_it%26%238230%3B%28on_Bayes%2C_MDL_and_consistency%29.html">263 hunch net-2007-09-18-It&#8217;s MDL Jim, but not as we know it&#8230;(on Bayes, MDL and consistency)</a></p>
<p>Introduction: I have recently completed  a 500+ page-book on MDL , the first comprehensive overview of the field (yes, this is a sneak advertisement    ). 
 Chapter 17  compares MDL to a menagerie of other methods and paradigms for learning and statistics. By far the most time (20 pages) is spent on the relation between MDL and Bayes. My two main points here are:
  
  In sharp contrast to Bayes, MDL is by definition based on designing universal codes for the data relative to some given (parametric or nonparametric) probabilistic model M. By some theorems due to  Andrew Barron , MDL inference  must  therefore be statistically consistent, and it is immune to Bayesian inconsistency results such as those by Diaconis, Freedman and Barron (I explain what I mean by “inconsistency” further below).  Hence, MDL must be different from Bayes! 
 In contrast to what has sometimes been claimed, practical MDL algorithms do have a subjective component (which in many, but not all cases, may be implemented by somethin</p><p>6 0.25412792 <a title="173-lda-6" href="../hunch_net-2008/hunch_net-2008-05-25-Inappropriate_Mathematics_for_Machine_Learning.html">302 hunch net-2008-05-25-Inappropriate Mathematics for Machine Learning</a></p>
<p>7 0.25407073 <a title="173-lda-7" href="../hunch_net-2005/hunch_net-2005-02-15-ESPgame_and_image_labeling.html">20 hunch net-2005-02-15-ESPgame and image labeling</a></p>
<p>8 0.25405306 <a title="173-lda-8" href="../hunch_net-2011/hunch_net-2011-10-03-Monday_announcements.html">446 hunch net-2011-10-03-Monday announcements</a></p>
<p>9 0.25394514 <a title="173-lda-9" href="../hunch_net-2007/hunch_net-2007-11-05-CMU_wins_DARPA_Urban_Challenge.html">271 hunch net-2007-11-05-CMU wins DARPA Urban Challenge</a></p>
<p>10 0.25317377 <a title="173-lda-10" href="../hunch_net-2011/hunch_net-2011-10-24-2011_ML_symposium_and_the_bears.html">448 hunch net-2011-10-24-2011 ML symposium and the bears</a></p>
<p>11 0.25012597 <a title="173-lda-11" href="../hunch_net-2005/hunch_net-2005-07-07-The_Limits_of_Learning_Theory.html">90 hunch net-2005-07-07-The Limits of Learning Theory</a></p>
<p>12 0.24961615 <a title="173-lda-12" href="../hunch_net-2012/hunch_net-2012-08-27-NYAS_ML_2012_and_ICML_2013.html">472 hunch net-2012-08-27-NYAS ML 2012 and ICML 2013</a></p>
<p>13 0.24286455 <a title="173-lda-13" href="../hunch_net-2007/hunch_net-2007-11-02-The_Machine_Learning_Award_goes_to_%26%238230%3B.html">270 hunch net-2007-11-02-The Machine Learning Award goes to &#8230;</a></p>
<p>14 0.24009053 <a title="173-lda-14" href="../hunch_net-2008/hunch_net-2008-11-11-COLT_CFP.html">326 hunch net-2008-11-11-COLT CFP</a></p>
<p>15 0.24009053 <a title="173-lda-15" href="../hunch_net-2012/hunch_net-2012-05-12-ICML_accepted_papers_and_early_registration.html">465 hunch net-2012-05-12-ICML accepted papers and early registration</a></p>
<p>16 0.23915415 <a title="173-lda-16" href="../hunch_net-2008/hunch_net-2008-12-12-Summer_Conferences.html">331 hunch net-2008-12-12-Summer Conferences</a></p>
<p>17 0.23805961 <a title="173-lda-17" href="../hunch_net-2010/hunch_net-2010-04-26-Compassionate_Reviewing.html">395 hunch net-2010-04-26-Compassionate Reviewing</a></p>
<p>18 0.2344484 <a title="173-lda-18" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<p>19 0.22931075 <a title="173-lda-19" href="../hunch_net-2010/hunch_net-2010-01-19-Deadline_Season%2C_2010.html">387 hunch net-2010-01-19-Deadline Season, 2010</a></p>
<p>20 0.22525455 <a title="173-lda-20" href="../hunch_net-2005/hunch_net-2005-05-02-Reviewing_techniques_for_conferences.html">65 hunch net-2005-05-02-Reviewing techniques for conferences</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
