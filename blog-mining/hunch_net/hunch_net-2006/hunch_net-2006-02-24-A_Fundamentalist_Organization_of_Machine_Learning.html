<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-158" href="#">hunch_net-2006-158</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-158-html" href="http://hunch.net/?p=169">html</a></p><p>Introduction: There are several different flavors of Machine Learning classes.  Many classes are of the ‘zoo’ sort: many different learning algorithms are presented.  Others avoid the zoo by not covering the full scope of machine learning.  
 
This is my view of what makes a good machine learning class, along with why.  I’d like to specifically invite comment on whether things are missing, misemphasized, or misplaced.
  
 
 Phase 
 Subject 
 Why? 
 
 
 Introduction 
 What is a machine learning problem? 
 A good understanding of the characteristics of machine learning problems seems essential.  Characteristics include: a data source, some hope the data is predictive, and a need for generalization.  This is probably best taught in a case study manner: lay out the specifics of some problem and then ask “Is this a machine learning problem?” 
 
 
 Introduction 
 Machine Learning Problem Identification 
 Identification and recognition of the type of learning problems is (obviously) a very important step i</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Many classes are of the ‘zoo’ sort: many different learning algorithms are presented. [sent-2, score-0.216]
</p><p>2 Others avoid the zoo by not covering the full scope of machine learning. [sent-3, score-0.396]
</p><p>3 This is my view of what makes a good machine learning class, along with why. [sent-4, score-0.274]
</p><p>4 Introduction   What is a machine learning problem? [sent-7, score-0.274]
</p><p>5 A good understanding of the characteristics of machine learning problems seems essential. [sent-8, score-0.484]
</p><p>6 This is probably best taught in a case study manner: lay out the specifics of some problem and then ask “Is this a machine learning problem? [sent-10, score-0.425]
</p><p>7 ”       Introduction   Machine Learning Problem Identification   Identification and recognition of the type of learning problems is (obviously) a very important step in solving such problems. [sent-11, score-0.212]
</p><p>8 Introduction   Example algorithm 1   To really understand machine learning, a couple learning algorithms must be understood in detail. [sent-13, score-0.517]
</p><p>9 The reason why the number is “2″ and not “1″ or “3″ is that 2 is the minimum number required to make people naturally aware of the degrees of freedom available in learning algorithm design. [sent-15, score-0.366]
</p><p>10 Analysis   Bias for Learning   The need for a good bias is one of the defining characteristics of learning. [sent-16, score-0.506]
</p><p>11 This statement is generic so it will always apply to one degree or another. [sent-18, score-0.242]
</p><p>12 This is the boosting observation: that it is possible to bootstrap predictive ability to create a better overall system. [sent-20, score-0.272]
</p><p>13 Analysis   Learning can be transformed   This is the reductions observation: that the ability to solve one kind of learning problems implies the ability to solve other kinds of leanring problems. [sent-22, score-0.675]
</p><p>14 Analysis   Learning can be preserved   This is the online learning with experts observation: that we can have a master algorithm which preserves the best learning performance of subalgorithms. [sent-24, score-0.439]
</p><p>15 Analysis   Hardness of Learning   It turns out that there are several different ways in which machine learning can be hard including computational and information theoretic hardness. [sent-28, score-0.347]
</p><p>16 An understanding of how and why learning algorithms can fail seems important to understand the process. [sent-30, score-0.288]
</p><p>17 Applications   Vision   One example of how learning is applied to solve vision problems. [sent-31, score-0.329]
</p><p>18 Applications   Robotics   Ditto for robotics       Applications   Speech   Ditto for speech       Applications   Businesses   Ditto for businesses          Where is machine learning going? [sent-33, score-0.661]
</p><p>19 It should be understood that the field of machine learning is changing rapidly. [sent-35, score-0.366]
</p><p>20 The emphasis here is on fundamentals: generally applicable mathematical statements and understandings of the learning problem. [sent-36, score-0.343]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ditto', 0.378), ('introduction', 0.269), ('characteristics', 0.21), ('applications', 0.197), ('analysis', 0.196), ('zoo', 0.189), ('statement', 0.177), ('businesses', 0.155), ('identification', 0.14), ('learning', 0.14), ('bias', 0.136), ('machine', 0.134), ('robotics', 0.126), ('emphasis', 0.119), ('observation', 0.115), ('speech', 0.106), ('ability', 0.104), ('vision', 0.101), ('predictive', 0.095), ('need', 0.093), ('understood', 0.092), ('similarly', 0.09), ('solve', 0.088), ('preserves', 0.084), ('integrity', 0.084), ('understandings', 0.084), ('degrees', 0.084), ('language', 0.081), ('leanring', 0.078), ('flavors', 0.078), ('specifics', 0.078), ('algorithms', 0.076), ('algorithm', 0.075), ('transformed', 0.073), ('lay', 0.073), ('bootstrap', 0.073), ('overfit', 0.073), ('theoretic', 0.073), ('avoid', 0.073), ('important', 0.072), ('cut', 0.07), ('concept', 0.07), ('etc', 0.07), ('classification', 0.069), ('freedom', 0.067), ('phase', 0.067), ('defining', 0.067), ('priors', 0.065), ('generic', 0.065), ('insert', 0.065)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="158-tfidf-1" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>Introduction: There are several different flavors of Machine Learning classes.  Many classes are of the ‘zoo’ sort: many different learning algorithms are presented.  Others avoid the zoo by not covering the full scope of machine learning.  
 
This is my view of what makes a good machine learning class, along with why.  I’d like to specifically invite comment on whether things are missing, misemphasized, or misplaced.
  
 
 Phase 
 Subject 
 Why? 
 
 
 Introduction 
 What is a machine learning problem? 
 A good understanding of the characteristics of machine learning problems seems essential.  Characteristics include: a data source, some hope the data is predictive, and a need for generalization.  This is probably best taught in a case study manner: lay out the specifics of some problem and then ask “Is this a machine learning problem?” 
 
 
 Introduction 
 Machine Learning Problem Identification 
 Identification and recognition of the type of learning problems is (obviously) a very important step i</p><p>2 0.14866036 <a title="158-tfidf-2" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>Introduction: I’ve had serious conversations with several people who believe that the theory in machine learning is “only useful for getting papers published”.  That’s a compelling statement, as I’ve seen many papers where the algorithm clearly came first, and the theoretical justification for it came second, purely as a perceived means to improve the chance of publication. 
 
Naturally, I disagree and believe that learning theory has much more substantial applications.  
 
Even in core learning algorithm design, I’ve found learning theory to be useful, although it’s application is more subtle than many realize.  The most straightforward applications can fail, because (as expectation suggests) worst case bounds tend to be loose in practice (*).  In my experience, considering learning theory when designing an algorithm has two important effects in practice:
  
 It can help make your algorithm behave right at a crude level of analysis, leaving finer details to tuning or common sense.  The best example</p><p>3 0.14810413 <a title="158-tfidf-3" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>Introduction: Attempts to abstract and study machine learning are within some given framework or mathematical model.  It turns out that all of these models are significantly flawed for the purpose of studying machine learning.  I’ve created a table (below) outlining the major flaws in some common models of machine learning.
 
The point here is not simply “woe unto us”.  There are several implications which seem important.
  
 The multitude of models is a point of continuing confusion.  It is common for people to learn about machine learning within one framework which often becomes there “home framework” through which they attempt to filter all machine learning.  (Have you met people who can only think in terms of kernels?  Only via Bayes Law? Only via PAC Learning?)  Explicitly understanding the existence of these other frameworks can help resolve the confusion.  This is particularly important when reviewing and particularly important for students. 
 Algorithms which conform to multiple approaches c</p><p>4 0.14640154 <a title="158-tfidf-4" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>Introduction: One of the remarkable things about machine learning is how diverse it is.  The viewpoints of Bayesian learning, reinforcement learning, graphical models, supervised learning, unsupervised learning, genetic programming, etc… share little enough overlap that many people can and do make their careers within one without touching, or even necessarily understanding the others.
 
There are two fundamental reasons why this is possible.
  
 For many problems, many approaches work in the sense that they do something useful.  This is true empirically, where for many problems we can observe that many different approaches yield better performance than any constant predictor.  It’s also true in theory, where we know that for any set of predictors representable in a finite amount of RAM, minimizing training error over the set of predictors does something nontrivial when there are a sufficient number of examples. 
 There is nothing like a unifying problem defining the field.  In many other areas there</p><p>5 0.14055836 <a title="158-tfidf-5" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>Introduction: What?  Reductions are machines which turn solvers for one problem into solvers for another problem. 
 Why?  Reductions are useful for several reasons.
  
  Laziness .  Reducing a problem to classification make at least 10 learning algorithms available to solve a problem.  Inventing 10 learning algorithms is quite a bit of work.  Similarly, programming a reduction is often trivial, while programming a learning algorithm is a great deal of work. 
  Crystallization .  The problems we often want to solve in learning are worst-case-impossible, but average case feasible.  By reducing all problems onto one or a few primitives, we can fine tune these primitives to perform well on real-world problems with greater precision due to the greater number of problems to validate on. 
  Theoretical Organization .  By studying what reductions are easy vs. hard vs. impossible, we can learn which problems are roughly equivalent in difficulty and which are much harder. 
  
 What we know now .
 
 Typesafe r</p><p>6 0.12612534 <a title="158-tfidf-6" href="../hunch_net-2005/hunch_net-2005-02-03-Learning_Theory%2C_by_assumption.html">12 hunch net-2005-02-03-Learning Theory, by assumption</a></p>
<p>7 0.12296529 <a title="158-tfidf-7" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>8 0.12282889 <a title="158-tfidf-8" href="../hunch_net-2006/hunch_net-2006-01-13-Benchmarks_for_RL.html">148 hunch net-2006-01-13-Benchmarks for RL</a></p>
<p>9 0.12077426 <a title="158-tfidf-9" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>10 0.11582124 <a title="158-tfidf-10" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>11 0.11463309 <a title="158-tfidf-11" href="../hunch_net-2005/hunch_net-2005-04-14-Families_of_Learning_Theory_Statements.html">56 hunch net-2005-04-14-Families of Learning Theory Statements</a></p>
<p>12 0.11327409 <a title="158-tfidf-12" href="../hunch_net-2005/hunch_net-2005-07-07-The_Limits_of_Learning_Theory.html">90 hunch net-2005-07-07-The Limits of Learning Theory</a></p>
<p>13 0.11258768 <a title="158-tfidf-13" href="../hunch_net-2005/hunch_net-2005-09-08-Online_Learning_as_the_Mathematics_of_Accountability.html">109 hunch net-2005-09-08-Online Learning as the Mathematics of Accountability</a></p>
<p>14 0.11224008 <a title="158-tfidf-14" href="../hunch_net-2007/hunch_net-2007-03-15-Alternative_Machine_Learning_Reductions_Definitions.html">236 hunch net-2007-03-15-Alternative Machine Learning Reductions Definitions</a></p>
<p>15 0.10934982 <a title="158-tfidf-15" href="../hunch_net-2005/hunch_net-2005-02-23-Problem%3A_Reinforcement_Learning_with_Classification.html">27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</a></p>
<p>16 0.10603522 <a title="158-tfidf-16" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>17 0.10594858 <a title="158-tfidf-17" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>18 0.10535058 <a title="158-tfidf-18" href="../hunch_net-2005/hunch_net-2005-10-26-Fallback_Analysis_is_a_Secret_to_Useful_Algorithms.html">126 hunch net-2005-10-26-Fallback Analysis is a Secret to Useful Algorithms</a></p>
<p>19 0.10461318 <a title="158-tfidf-19" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>20 0.10408168 <a title="158-tfidf-20" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.267), (1, 0.122), (2, -0.048), (3, 0.014), (4, 0.037), (5, -0.072), (6, 0.048), (7, 0.042), (8, 0.05), (9, -0.028), (10, -0.027), (11, -0.096), (12, 0.069), (13, 0.025), (14, 0.016), (15, 0.066), (16, 0.094), (17, -0.045), (18, -0.047), (19, -0.062), (20, 0.079), (21, -0.036), (22, -0.012), (23, -0.141), (24, -0.025), (25, -0.059), (26, 0.046), (27, -0.001), (28, -0.042), (29, 0.058), (30, -0.013), (31, 0.034), (32, -0.015), (33, -0.012), (34, -0.035), (35, -0.016), (36, -0.013), (37, -0.017), (38, 0.07), (39, 0.011), (40, 0.007), (41, 0.061), (42, -0.027), (43, -0.017), (44, -0.08), (45, -0.006), (46, -0.006), (47, -0.012), (48, 0.038), (49, -0.023)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9542523 <a title="158-lsi-1" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>Introduction: There are several different flavors of Machine Learning classes.  Many classes are of the ‘zoo’ sort: many different learning algorithms are presented.  Others avoid the zoo by not covering the full scope of machine learning.  
 
This is my view of what makes a good machine learning class, along with why.  I’d like to specifically invite comment on whether things are missing, misemphasized, or misplaced.
  
 
 Phase 
 Subject 
 Why? 
 
 
 Introduction 
 What is a machine learning problem? 
 A good understanding of the characteristics of machine learning problems seems essential.  Characteristics include: a data source, some hope the data is predictive, and a need for generalization.  This is probably best taught in a case study manner: lay out the specifics of some problem and then ask “Is this a machine learning problem?” 
 
 
 Introduction 
 Machine Learning Problem Identification 
 Identification and recognition of the type of learning problems is (obviously) a very important step i</p><p>2 0.796462 <a title="158-lsi-2" href="../hunch_net-2006/hunch_net-2006-01-13-Benchmarks_for_RL.html">148 hunch net-2006-01-13-Benchmarks for RL</a></p>
<p>Introduction: A couple years ago, Drew Bagnell and I started the  RLBench project  to setup a suite of reinforcement learning benchmark problems.  We haven’t been able to touch it (due to lack of time) for a year so the project is on hold.  Luckily, there are several other projects such as  CLSquare  and  RL-Glue  with a similar goal, and we strongly endorse their continued development.
 
I would like to explain why, especially in the context of criticism of other learning benchmarks.   For example, sometimes the  UCI Machine Learning Repository  is criticized.  There are two criticisms I know of:
  
 Learning algorithms have overfit to the problems in the repository.  It is easy to imagine a mechanism for this happening unintentionally.  Strong evidence of this would be provided by learning algorithms which perform great on the UCI machine learning repository but very badly (relative to other learning algorithms) on non-UCI learning problems.  I have seen little evidence of this but it remains a po</p><p>3 0.78886229 <a title="158-lsi-3" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>Introduction: One of the remarkable things about machine learning is how diverse it is.  The viewpoints of Bayesian learning, reinforcement learning, graphical models, supervised learning, unsupervised learning, genetic programming, etc… share little enough overlap that many people can and do make their careers within one without touching, or even necessarily understanding the others.
 
There are two fundamental reasons why this is possible.
  
 For many problems, many approaches work in the sense that they do something useful.  This is true empirically, where for many problems we can observe that many different approaches yield better performance than any constant predictor.  It’s also true in theory, where we know that for any set of predictors representable in a finite amount of RAM, minimizing training error over the set of predictors does something nontrivial when there are a sufficient number of examples. 
 There is nothing like a unifying problem defining the field.  In many other areas there</p><p>4 0.77853179 <a title="158-lsi-4" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>Introduction: Attempts to abstract and study machine learning are within some given framework or mathematical model.  It turns out that all of these models are significantly flawed for the purpose of studying machine learning.  I’ve created a table (below) outlining the major flaws in some common models of machine learning.
 
The point here is not simply “woe unto us”.  There are several implications which seem important.
  
 The multitude of models is a point of continuing confusion.  It is common for people to learn about machine learning within one framework which often becomes there “home framework” through which they attempt to filter all machine learning.  (Have you met people who can only think in terms of kernels?  Only via Bayes Law? Only via PAC Learning?)  Explicitly understanding the existence of these other frameworks can help resolve the confusion.  This is particularly important when reviewing and particularly important for students. 
 Algorithms which conform to multiple approaches c</p><p>5 0.75228238 <a title="158-lsi-5" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>Introduction: I’ve had serious conversations with several people who believe that the theory in machine learning is “only useful for getting papers published”.  That’s a compelling statement, as I’ve seen many papers where the algorithm clearly came first, and the theoretical justification for it came second, purely as a perceived means to improve the chance of publication. 
 
Naturally, I disagree and believe that learning theory has much more substantial applications.  
 
Even in core learning algorithm design, I’ve found learning theory to be useful, although it’s application is more subtle than many realize.  The most straightforward applications can fail, because (as expectation suggests) worst case bounds tend to be loose in practice (*).  In my experience, considering learning theory when designing an algorithm has two important effects in practice:
  
 It can help make your algorithm behave right at a crude level of analysis, leaving finer details to tuning or common sense.  The best example</p><p>6 0.74488425 <a title="158-lsi-6" href="../hunch_net-2005/hunch_net-2005-02-03-Learning_Theory%2C_by_assumption.html">12 hunch net-2005-02-03-Learning Theory, by assumption</a></p>
<p>7 0.74346185 <a title="158-lsi-7" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>8 0.72812963 <a title="158-lsi-8" href="../hunch_net-2006/hunch_net-2006-04-02-Mad_%28Neuro%29science.html">168 hunch net-2006-04-02-Mad (Neuro)science</a></p>
<p>9 0.72443235 <a title="158-lsi-9" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>10 0.71729565 <a title="158-lsi-10" href="../hunch_net-2005/hunch_net-2005-10-26-Fallback_Analysis_is_a_Secret_to_Useful_Algorithms.html">126 hunch net-2005-10-26-Fallback Analysis is a Secret to Useful Algorithms</a></p>
<p>11 0.7171368 <a title="158-lsi-11" href="../hunch_net-2006/hunch_net-2006-08-10-Precision_is_not_accuracy.html">202 hunch net-2006-08-10-Precision is not accuracy</a></p>
<p>12 0.71612602 <a title="158-lsi-12" href="../hunch_net-2005/hunch_net-2005-02-25-Problem%3A_Online_Learning.html">28 hunch net-2005-02-25-Problem: Online Learning</a></p>
<p>13 0.71519202 <a title="158-lsi-13" href="../hunch_net-2005/hunch_net-2005-08-22-Do_you_believe_in_induction%3F.html">104 hunch net-2005-08-22-Do you believe in induction?</a></p>
<p>14 0.71002001 <a title="158-lsi-14" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>15 0.70099771 <a title="158-lsi-15" href="../hunch_net-2007/hunch_net-2007-07-06-Idempotent-capable_Predictors.html">253 hunch net-2007-07-06-Idempotent-capable Predictors</a></p>
<p>16 0.69447213 <a title="158-lsi-16" href="../hunch_net-2009/hunch_net-2009-01-21-Nearly_all_natural_problems_require_nonlinearity.html">337 hunch net-2009-01-21-Nearly all natural problems require nonlinearity</a></p>
<p>17 0.69373226 <a title="158-lsi-17" href="../hunch_net-2005/hunch_net-2005-11-28-A_question_of_quantification.html">133 hunch net-2005-11-28-A question of quantification</a></p>
<p>18 0.68733281 <a title="158-lsi-18" href="../hunch_net-2009/hunch_net-2009-04-02-Asymmophobia.html">348 hunch net-2009-04-02-Asymmophobia</a></p>
<p>19 0.68355834 <a title="158-lsi-19" href="../hunch_net-2006/hunch_net-2006-01-30-Should_the_Input_Representation_be_a_Vector%3F.html">152 hunch net-2006-01-30-Should the Input Representation be a Vector?</a></p>
<p>20 0.68019575 <a title="158-lsi-20" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(3, 0.028), (10, 0.037), (16, 0.04), (27, 0.234), (37, 0.024), (38, 0.023), (40, 0.15), (53, 0.115), (55, 0.077), (64, 0.018), (83, 0.019), (94, 0.09), (95, 0.036)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.91764635 <a title="158-lda-1" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>Introduction: There are several different flavors of Machine Learning classes.  Many classes are of the ‘zoo’ sort: many different learning algorithms are presented.  Others avoid the zoo by not covering the full scope of machine learning.  
 
This is my view of what makes a good machine learning class, along with why.  I’d like to specifically invite comment on whether things are missing, misemphasized, or misplaced.
  
 
 Phase 
 Subject 
 Why? 
 
 
 Introduction 
 What is a machine learning problem? 
 A good understanding of the characteristics of machine learning problems seems essential.  Characteristics include: a data source, some hope the data is predictive, and a need for generalization.  This is probably best taught in a case study manner: lay out the specifics of some problem and then ask “Is this a machine learning problem?” 
 
 
 Introduction 
 Machine Learning Problem Identification 
 Identification and recognition of the type of learning problems is (obviously) a very important step i</p><p>2 0.87122238 <a title="158-lda-2" href="../hunch_net-2007/hunch_net-2007-01-10-A_Deep_Belief_Net_Learning_Problem.html">227 hunch net-2007-01-10-A Deep Belief Net Learning Problem</a></p>
<p>Introduction: “Deep learning” is used to describe learning architectures which have significant depth (as a circuit).  
 
 One claim  is that shallow architectures (one or two layers) can not concisely represent some functions while a circuit with more depth can concisely represent these same functions.  Proving lower bounds on the size of a circuit is substantially harder than upper bounds (which are constructive), but some results are known.   Luca Trevisan ‘s  class notes  detail how XOR is not concisely representable by “AC0″ (= constant depth unbounded fan-in AND, OR, NOT gates).  This doesn’t quite prove that depth is necessary for the representations commonly used in learning (such as a thresholded weighted sum), but it is strongly suggestive that this is so.
 
Examples like this are a bit disheartening because existing algorithms for deep learning (deep belief nets, gradient descent on deep neural networks, and a perhaps decision trees depending on who you ask) can’t learn XOR very easily.</p><p>3 0.86978698 <a title="158-lda-3" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>Introduction: Muthu  invited me to the workshop on  algorithms in the field , with the goal of providing a sense of where near-term research should go.  When the time came though, I bargained for a post instead, which provides a chance for many other people to comment.
 
There are several things I didn’t fully understand when I went to Yahoo! about 5 years ago.  I’d like to repeat them as people in academia may not yet understand them intuitively.
  
 Almost all the big impact algorithms operate in pseudo-linear or better time.  Think about caching, hashing, sorting, filtering, etc… and you have a sense of what some of the most heavily used algorithms are.  This matters quite a bit to Machine Learning research, because people often work with superlinear time algorithms and languages.  Two very common examples of this are graphical models, where inference is often a superlinear operation—think about the  n 2   dependence on the number of states in a  Hidden Markov Model  and Kernelized  Support Vecto</p><p>4 0.85697794 <a title="158-lda-4" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>Introduction: One of the remarkable things about machine learning is how diverse it is.  The viewpoints of Bayesian learning, reinforcement learning, graphical models, supervised learning, unsupervised learning, genetic programming, etc… share little enough overlap that many people can and do make their careers within one without touching, or even necessarily understanding the others.
 
There are two fundamental reasons why this is possible.
  
 For many problems, many approaches work in the sense that they do something useful.  This is true empirically, where for many problems we can observe that many different approaches yield better performance than any constant predictor.  It’s also true in theory, where we know that for any set of predictors representable in a finite amount of RAM, minimizing training error over the set of predictors does something nontrivial when there are a sufficient number of examples. 
 There is nothing like a unifying problem defining the field.  In many other areas there</p><p>5 0.85436332 <a title="158-lda-5" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>Introduction: Many people in Machine Learning don’t fully understand the impact of computation, as demonstrated by a lack of  big-O  analysis of new learning algorithms.  This is important—some current active research programs are fundamentally flawed w.r.t. computation, and other research programs are directly motivated by it.  When considering a learning algorithm, I think about the following questions:
  
 How does the learning algorithm scale with the number of examples  m ?  Any algorithm using all of the data is at least  O(m) , but in many cases this is  O(m 2 )  (naive nearest neighbor for self-prediction) or unknown (k-means or many other optimization algorithms).  The unknown case is very common, and it can mean (for example) that the algorithm isn’t convergent or simply that the amount of computation isn’t controlled. 
 The above question can also be asked for test cases.  In some applications, test-time performance is of great importance. 
 How does the algorithm scale with the number of</p><p>6 0.85409981 <a title="158-lda-6" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>7 0.85082912 <a title="158-lda-7" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>8 0.85055637 <a title="158-lda-8" href="../hunch_net-2013/hunch_net-2013-01-07-NYU_Large_Scale_Machine_Learning_Class.html">478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</a></p>
<p>9 0.84951049 <a title="158-lda-9" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>10 0.84931576 <a title="158-lda-10" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>11 0.84807485 <a title="158-lda-11" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>12 0.84774595 <a title="158-lda-12" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>13 0.84682065 <a title="158-lda-13" href="../hunch_net-2006/hunch_net-2006-07-11-New_Models.html">194 hunch net-2006-07-11-New Models</a></p>
<p>14 0.8460421 <a title="158-lda-14" href="../hunch_net-2006/hunch_net-2006-09-12-Incentive_Compatible_Reviewing.html">207 hunch net-2006-09-12-Incentive Compatible Reviewing</a></p>
<p>15 0.84600353 <a title="158-lda-15" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>16 0.8451165 <a title="158-lda-16" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>17 0.84469754 <a title="158-lda-17" href="../hunch_net-2009/hunch_net-2009-06-01-Multitask_Poisoning.html">358 hunch net-2009-06-01-Multitask Poisoning</a></p>
<p>18 0.84394377 <a title="158-lda-18" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>19 0.84335577 <a title="158-lda-19" href="../hunch_net-2009/hunch_net-2009-01-21-Nearly_all_natural_problems_require_nonlinearity.html">337 hunch net-2009-01-21-Nearly all natural problems require nonlinearity</a></p>
<p>20 0.84320712 <a title="158-lda-20" href="../hunch_net-2009/hunch_net-2009-06-03-Functionally_defined_Nonlinear_Dynamic_Models.html">359 hunch net-2009-06-03-Functionally defined Nonlinear Dynamic Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
