<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>190 hunch net-2006-07-06-Branch Prediction Competition</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-190" href="#">hunch_net-2006-190</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>190 hunch net-2006-07-06-Branch Prediction Competition</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-190-html" href="http://hunch.net/?p=207">html</a></p><p>Introduction: Alan Fernpoints out thesecond branch prediction challenge(due September 29)
which is a follow up to thefirst branch prediction competition. Branch
prediction is one of the fundamental learning problems of the computer age:
without it our computers might run an order of magnitude slower. This is a
tough problem since there are sharp constraints on time and space complexity
in an online environment. For machine learning, the "idealistic track" may fit
well. Essentially, they remove these constraints to gain a weak upper bound on
what might be done.</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('branch', 0.562), ('constraints', 0.249), ('age', 0.214), ('alan', 0.214), ('thefirst', 0.214), ('thesecond', 0.214), ('prediction', 0.209), ('september', 0.198), ('tough', 0.187), ('sharp', 0.171), ('remove', 0.152), ('gain', 0.148), ('magnitude', 0.141), ('computers', 0.136), ('follow', 0.133), ('upper', 0.131), ('challenge', 0.125), ('track', 0.125), ('fit', 0.121), ('weak', 0.119)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="190-tfidf-1" href="../hunch_net-2006/hunch_net-2006-07-06-Branch_Prediction_Competition.html">190 hunch net-2006-07-06-Branch Prediction Competition</a></p>
<p>Introduction: Alan Fernpoints out thesecond branch prediction challenge(due September 29)
which is a follow up to thefirst branch prediction competition. Branch
prediction is one of the fundamental learning problems of the computer age:
without it our computers might run an order of magnitude slower. This is a
tough problem since there are sharp constraints on time and space complexity
in an online environment. For machine learning, the "idealistic track" may fit
well. Essentially, they remove these constraints to gain a weak upper bound on
what might be done.</p><p>2 0.12936534 <a title="190-tfidf-2" href="../hunch_net-2005/hunch_net-2005-11-07-Prediction_Competitions.html">129 hunch net-2005-11-07-Prediction Competitions</a></p>
<p>Introduction: There are two prediction competitions currently in the air.ThePerformance
Prediction ChallengebyIsabelle Guyon. Good entries minimize a weighted 0/1
loss + the difference between a prediction of this loss and the observed truth
on 5 datasets. Isabelle tells me all of the problems are "real world" and the
test datasets are large enough (17K minimum) that the winner should be well
determined by ability rather than luck. This is due March 1.ThePredictive
Uncertainty ChallengebyGavin Cawley. Good entries minimize log loss on real
valued output variables for one synthetic and 3 "real" datasets related to
atmospheric prediction. The use of log loss (which can be infinite and hence
is never convergent) and smaller test sets of size 1K to 7K examples makes the
winner of this contest more luck dependent. Nevertheless, the contest may be
of some interest particularly to the branch of learning (typically Bayes
learning) which prefers to optimize log loss.May the best predictor win.</p><p>3 0.10355823 <a title="190-tfidf-3" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>Introduction: "Search" is the other branch of AI research which has been succesful. Concrete
examples includeDeep Bluewhich beat the world chess champion andChinookthe
champion checkers program. A set of core search techniques exist including A*,
alpha-beta pruning, and others that can be applied to any of many different
search problems.Given this, it may be surprising to learn that there has been
relatively little succesful work on combining prediction and search. Given
also that humans typically solve search problems using a number of predictive
heuristics to narrow in on a solution, we might be surprised again. However,
the big successful search-based systems have typically not used "smart" search
algorithms. Insteady they have optimized for very fast search. This is not for
lack of tryingâ&euro;Ś many people have tried to synthesize search and prediction to
various degrees of success. For example,Knightcapachieves good-but-not-stellar
chess playing performance, andTD-gammonhas achieved near-optimal Bac</p><p>4 0.099307716 <a title="190-tfidf-4" href="../hunch_net-2006/hunch_net-2006-04-06-Bounds_greater_than_1.html">170 hunch net-2006-04-06-Bounds greater than 1</a></p>
<p>Introduction: Nati SrebroandShai Ben-Davidhave apaperatCOLTwhich, in the appendix, proves
something very striking: several previous error bounds arealwaysgreater than
1.BackgroundOne branch of learning theory focuses on theorems whichAssume
samples are drawn IID from an unknown distributionD.Fix a set of
classifiersFind a high probability bound on the maximum true error rate (with
respect toD) as a function of the empirical error rate on the training
set.Many of these bounds become extremely complex and hairy.CurrentEveryone
working on this subject wants "tighter bounds", however there are different
definitions of "tighter". Some groups focus on "functional tightness" (getting
the right functional dependency between the size of the training set and a
parameterization of the hypothesis space) whileothersfocus on "practical
tightness" (finding bounds which work well on practical problems). (I am
definitely in the second camp.)One of the dangers of striving for "functional
tightness" is that the bound</p><p>5 0.093544953 <a title="190-tfidf-5" href="../hunch_net-2005/hunch_net-2005-02-03-Learning_Theory%2C_by_assumption.html">12 hunch net-2005-02-03-Learning Theory, by assumption</a></p>
<p>Introduction: One way to organize learning theory is by assumption (in theassumption = axiom
sense), from no assumptions to many assumptions. As you travel down this list,
the statements become stronger, but the scope of applicability decreases.No
assumptionsOnline learningThere exist a meta prediction algorithm which
compete well with the best element of any set of prediction
algorithms.Universal LearningUsing a "bias" of 2- description length of turing
machinein learning is equivalent to all other computable biases up to some
constant.ReductionsThe ability to predict well on classification problems is
equivalent to the ability to predict well on many other learning
problems.Independent and Identically Distributed (IID) DataPerformance
PredictionBased upon past performance, you can predict future
performance.Uniform ConvergencePerformance prediction works even after
choosing classifiers based on the data from large sets of classifiers.IID and
partial constraints on the data sourcePAC LearningThere</p><p>6 0.085647948 <a title="190-tfidf-6" href="../hunch_net-2007/hunch_net-2007-10-17-Online_as_the_new_adjective.html">267 hunch net-2007-10-17-Online as the new adjective</a></p>
<p>7 0.068721928 <a title="190-tfidf-7" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<p>8 0.065949075 <a title="190-tfidf-8" href="../hunch_net-2005/hunch_net-2005-02-28-Regularization.html">33 hunch net-2005-02-28-Regularization</a></p>
<p>9 0.06170195 <a title="190-tfidf-9" href="../hunch_net-2005/hunch_net-2005-06-18-Lower_Bounds_for_Learning_Reductions.html">83 hunch net-2005-06-18-Lower Bounds for Learning Reductions</a></p>
<p>10 0.061182965 <a title="190-tfidf-10" href="../hunch_net-2005/hunch_net-2005-08-22-Do_you_believe_in_induction%3F.html">104 hunch net-2005-08-22-Do you believe in induction?</a></p>
<p>11 0.061084814 <a title="190-tfidf-11" href="../hunch_net-2012/hunch_net-2012-06-05-ICML_acceptance_statistics.html">466 hunch net-2012-06-05-ICML acceptance statistics</a></p>
<p>12 0.059044093 <a title="190-tfidf-12" href="../hunch_net-2010/hunch_net-2010-03-12-Netflix_Challenge_2_Canceled.html">390 hunch net-2010-03-12-Netflix Challenge 2 Canceled</a></p>
<p>13 0.057862911 <a title="190-tfidf-13" href="../hunch_net-2008/hunch_net-2008-02-10-Complexity_Illness.html">288 hunch net-2008-02-10-Complexity Illness</a></p>
<p>14 0.057222225 <a title="190-tfidf-14" href="../hunch_net-2006/hunch_net-2006-09-18-What_is_missing_for_online_collaborative_research%3F.html">208 hunch net-2006-09-18-What is missing for online collaborative research?</a></p>
<p>15 0.056900762 <a title="190-tfidf-15" href="../hunch_net-2005/hunch_net-2005-06-28-A_COLT_paper.html">85 hunch net-2005-06-28-A COLT paper</a></p>
<p>16 0.056158636 <a title="190-tfidf-16" href="../hunch_net-2006/hunch_net-2006-06-16-Regularization_%3D_Robustness.html">185 hunch net-2006-06-16-Regularization = Robustness</a></p>
<p>17 0.055915337 <a title="190-tfidf-17" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>18 0.053570691 <a title="190-tfidf-18" href="../hunch_net-2005/hunch_net-2005-06-06-Exact_Online_Learning_for_Classification.html">78 hunch net-2005-06-06-Exact Online Learning for Classification</a></p>
<p>19 0.052786663 <a title="190-tfidf-19" href="../hunch_net-2007/hunch_net-2007-01-15-The_Machine_Learning_Department.html">228 hunch net-2007-01-15-The Machine Learning Department</a></p>
<p>20 0.052448973 <a title="190-tfidf-20" href="../hunch_net-2006/hunch_net-2006-03-12-Online_learning_or_online_preservation_of_learning%3F.html">163 hunch net-2006-03-12-Online learning or online preservation of learning?</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.114), (1, -0.048), (2, 0.006), (3, 0.012), (4, 0.008), (5, -0.008), (6, 0.01), (7, 0.01), (8, 0.052), (9, -0.041), (10, -0.071), (11, -0.008), (12, -0.057), (13, -0.079), (14, 0.081), (15, 0.03), (16, -0.038), (17, -0.014), (18, 0.04), (19, -0.047), (20, -0.02), (21, 0.026), (22, -0.054), (23, -0.006), (24, -0.058), (25, -0.088), (26, 0.005), (27, -0.084), (28, -0.007), (29, -0.043), (30, -0.014), (31, 0.037), (32, 0.088), (33, 0.024), (34, 0.109), (35, -0.05), (36, 0.017), (37, -0.052), (38, 0.01), (39, 0.031), (40, 0.053), (41, -0.008), (42, 0.027), (43, 0.026), (44, 0.017), (45, 0.116), (46, 0.008), (47, -0.062), (48, 0.059), (49, -0.011)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9347232 <a title="190-lsi-1" href="../hunch_net-2006/hunch_net-2006-07-06-Branch_Prediction_Competition.html">190 hunch net-2006-07-06-Branch Prediction Competition</a></p>
<p>Introduction: Alan Fernpoints out thesecond branch prediction challenge(due September 29)
which is a follow up to thefirst branch prediction competition. Branch
prediction is one of the fundamental learning problems of the computer age:
without it our computers might run an order of magnitude slower. This is a
tough problem since there are sharp constraints on time and space complexity
in an online environment. For machine learning, the "idealistic track" may fit
well. Essentially, they remove these constraints to gain a weak upper bound on
what might be done.</p><p>2 0.55522585 <a title="190-lsi-2" href="../hunch_net-2005/hunch_net-2005-06-28-A_COLT_paper.html">85 hunch net-2005-06-28-A COLT paper</a></p>
<p>Introduction: I found Tong Zhang's paper onData Dependent Concentration Bounds for
Sequential Prediction Algorithmsinteresting. Roughly speaking, it states a
tight bound on the future error rate for online learning algorithms assuming
that samples are drawn independently. This bound is easily computed and will
make the progressive validation approaches usedheresignificantly more
practical.</p><p>3 0.50263739 <a title="190-lsi-3" href="../hunch_net-2009/hunch_net-2009-10-03-Static_vs._Dynamic_multiclass_prediction.html">373 hunch net-2009-10-03-Static vs. Dynamic multiclass prediction</a></p>
<p>Introduction: I have had interesting discussions about distinction between static vs.
dynamic classes withKishoreandHal.The distinction arises in multiclass
prediction settings. A static set of classes is given by a set of
labels{1,â&euro;Ś,k}and the goal is generally to choose the most likely label given
features. The static approach is the one that we typically analyze and think
about in machine learning.The dynamic setting is one that is often used in
practice. The basic idea is that the number of classes is not fixed, varying
on a per example basis. These different classes are generally defined by a
choice of features.The distinction between these two settings as far as theory
goes, appears to be very substantial. For example, in the static setting,
inlearning reductions land, we have techniques now for robustO(log(k))time
prediction in many multiclass setting variants. In the dynamic setting, the
best techniques known areO(k), and furthermore this exponential gap may be
essential, at least without fur</p><p>4 0.48728997 <a title="190-lsi-4" href="../hunch_net-2006/hunch_net-2006-04-06-Bounds_greater_than_1.html">170 hunch net-2006-04-06-Bounds greater than 1</a></p>
<p>Introduction: Nati SrebroandShai Ben-Davidhave apaperatCOLTwhich, in the appendix, proves
something very striking: several previous error bounds arealwaysgreater than
1.BackgroundOne branch of learning theory focuses on theorems whichAssume
samples are drawn IID from an unknown distributionD.Fix a set of
classifiersFind a high probability bound on the maximum true error rate (with
respect toD) as a function of the empirical error rate on the training
set.Many of these bounds become extremely complex and hairy.CurrentEveryone
working on this subject wants "tighter bounds", however there are different
definitions of "tighter". Some groups focus on "functional tightness" (getting
the right functional dependency between the size of the training set and a
parameterization of the hypothesis space) whileothersfocus on "practical
tightness" (finding bounds which work well on practical problems). (I am
definitely in the second camp.)One of the dangers of striving for "functional
tightness" is that the bound</p><p>5 0.46941563 <a title="190-lsi-5" href="../hunch_net-2005/hunch_net-2005-06-18-Lower_Bounds_for_Learning_Reductions.html">83 hunch net-2005-06-18-Lower Bounds for Learning Reductions</a></p>
<p>Introduction: Learning reductionstransform a solver of one type of learning problem into a
solver of another type of learning problem. When we analyze these for
robustness we can make statement of the form "ReductionRhas the property that
regretr(or loss) on subproblems of typeAimplies regret at mostf ( r )on the
original problem of typeB".A lower bound for a learning reduction would have
the form "for all reductionsR, there exists a learning problem of typeBand
learning algorithm for problems of typeAwhere regretron induced problems
impliesat leastregretf ( r )forB".The pursuit of lower bounds is often
questionable because, unlike upper bounds, they do not yield practical
algorithms. Nevertheless, they may be helpful as a tool for thinking about
what is learnable and how learnable it is. This has already come
uphereandhere.At the moment, there is no coherent theory of lower bounds for
learning reductions, and we have little understanding of how feasible they are
or which techniques may be useful in</p><p>6 0.46321273 <a title="190-lsi-6" href="../hunch_net-2007/hunch_net-2007-05-09-The_Missing_Bound.html">244 hunch net-2007-05-09-The Missing Bound</a></p>
<p>7 0.46308061 <a title="190-lsi-7" href="../hunch_net-2005/hunch_net-2005-09-26-Prediction_Bounds_as_the_Mathematics_of_Science.html">115 hunch net-2005-09-26-Prediction Bounds as the Mathematics of Science</a></p>
<p>8 0.45926565 <a title="190-lsi-8" href="../hunch_net-2005/hunch_net-2005-11-07-Prediction_Competitions.html">129 hunch net-2005-11-07-Prediction Competitions</a></p>
<p>9 0.44728673 <a title="190-lsi-9" href="../hunch_net-2005/hunch_net-2005-03-15-The_State_of_Tight_Bounds.html">41 hunch net-2005-03-15-The State of Tight Bounds</a></p>
<p>10 0.44193479 <a title="190-lsi-10" href="../hunch_net-2005/hunch_net-2005-07-13-Text_Entailment_at_AAAI.html">94 hunch net-2005-07-13-Text Entailment at AAAI</a></p>
<p>11 0.43319598 <a title="190-lsi-11" href="../hunch_net-2005/hunch_net-2005-02-28-Regularization.html">33 hunch net-2005-02-28-Regularization</a></p>
<p>12 0.42544323 <a title="190-lsi-12" href="../hunch_net-2005/hunch_net-2005-09-14-The_Predictionist_Viewpoint.html">112 hunch net-2005-09-14-The Predictionist Viewpoint</a></p>
<p>13 0.42443046 <a title="190-lsi-13" href="../hunch_net-2005/hunch_net-2005-04-14-Families_of_Learning_Theory_Statements.html">56 hunch net-2005-04-14-Families of Learning Theory Statements</a></p>
<p>14 0.42111734 <a title="190-lsi-14" href="../hunch_net-2007/hunch_net-2007-10-17-Online_as_the_new_adjective.html">267 hunch net-2007-10-17-Online as the new adjective</a></p>
<p>15 0.41690105 <a title="190-lsi-15" href="../hunch_net-2005/hunch_net-2005-02-03-Learning_Theory%2C_by_assumption.html">12 hunch net-2005-02-03-Learning Theory, by assumption</a></p>
<p>16 0.41150913 <a title="190-lsi-16" href="../hunch_net-2006/hunch_net-2006-07-09-The_Stock_Prediction_Machine_Learning_Problem.html">193 hunch net-2006-07-09-The Stock Prediction Machine Learning Problem</a></p>
<p>17 0.40875536 <a title="190-lsi-17" href="../hunch_net-2007/hunch_net-2007-02-02-Thoughts_regarding_%26%238220%3BIs_machine_learning_different_from_statistics%3F%26%238221%3B.html">230 hunch net-2007-02-02-Thoughts regarding &#8220;Is machine learning different from statistics?&#8221;</a></p>
<p>18 0.40783378 <a title="190-lsi-18" href="../hunch_net-2011/hunch_net-2011-04-11-The_Heritage_Health_Prize.html">430 hunch net-2011-04-11-The Heritage Health Prize</a></p>
<p>19 0.40203378 <a title="190-lsi-19" href="../hunch_net-2009/hunch_net-2009-11-29-AI_Safety.html">380 hunch net-2009-11-29-AI Safety</a></p>
<p>20 0.39334965 <a title="190-lsi-20" href="../hunch_net-2006/hunch_net-2006-03-12-Online_learning_or_online_preservation_of_learning%3F.html">163 hunch net-2006-03-12-Online learning or online preservation of learning?</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(35, 0.036), (42, 0.241), (74, 0.068), (88, 0.501)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.98085463 <a title="190-lda-1" href="../hunch_net-2011/hunch_net-2011-04-06-COLT_open_questions.html">429 hunch net-2011-04-06-COLT open questions</a></p>
<p>Introduction: AlinaandJakepoint out the COLTCall for Open Questionsdue May 11. In general,
this is cool, and worth doing if you can come up with a crisp question. In my
case, I particularly enjoyedcrafting an open questionwith precisely a form
such that acritic targeting my paperswould be forced to confront their fallacy
or make a case for the reward. But less esoterically, this is a way to get the
attention of some very smart people focused on a problem that really matters,
which is the real value.</p><p>2 0.97436535 <a title="190-lda-2" href="../hunch_net-2005/hunch_net-2005-04-08-Fast_SVMs.html">54 hunch net-2005-04-08-Fast SVMs</a></p>
<p>Introduction: There was apresentation at snowbirdabout parallelized support vector machines.
In many cases, people parallelize by ignoring serial operations, but that is
not what happened here--they parallelize with optimizations. Consequently,
this seems to be the fastest SVM in existence.There is a relatedpaper here.</p><p>3 0.9686954 <a title="190-lda-3" href="../hunch_net-2012/hunch_net-2012-12-29-Simons_Institute_Big_Data_Program.html">476 hunch net-2012-12-29-Simons Institute Big Data Program</a></p>
<p>Introduction: Michael Jordansends the below:The newSimons Institute for the Theory of
Computingwill begin organizing semester-long programs starting in 2013.One of
our first programs, set for Fall 2013, will be on the "Theoretical
Foundationsof Big Data Analysis". The organizers of this program are Michael
Jordan (chair),Stephen Boyd, Peter Buehlmann, Ravi Kannan, Michael Mahoney,
and Muthu
Muthukrishnan.Seehttp://simons.berkeley.edu/program_bigdata2013.htmlfor more
information onthe program.The Simons Institute has created a number of
"Research Fellowships" for youngresearchers (within at most six years of the
award of their PhD) who wish toparticipate in Institute programs, including
the Big Data program. Individualswho already hold postdoctoral positions or
who are junior faculty are welcometo apply, as are finishing PhDs.Please note
that the application deadline is January 15, 2013. Further detailsare
available athttp://simons.berkeley.edu/fellows.html.Mike Jordan</p><p>4 0.91977739 <a title="190-lda-4" href="../hunch_net-2008/hunch_net-2008-11-11-COLT_CFP.html">326 hunch net-2008-11-11-COLT CFP</a></p>
<p>Introduction: Adam Klivans, points out theCOLT call for papers. The important points are:Due
Feb 13.Montreal, June 18-21.This year, there is author feedback.</p><p>same-blog 5 0.89836109 <a title="190-lda-5" href="../hunch_net-2006/hunch_net-2006-07-06-Branch_Prediction_Competition.html">190 hunch net-2006-07-06-Branch Prediction Competition</a></p>
<p>Introduction: Alan Fernpoints out thesecond branch prediction challenge(due September 29)
which is a follow up to thefirst branch prediction competition. Branch
prediction is one of the fundamental learning problems of the computer age:
without it our computers might run an order of magnitude slower. This is a
tough problem since there are sharp constraints on time and space complexity
in an online environment. For machine learning, the "idealistic track" may fit
well. Essentially, they remove these constraints to gain a weak upper bound on
what might be done.</p><p>6 0.89310431 <a title="190-lda-6" href="../hunch_net-2011/hunch_net-2011-12-13-Vowpal_Wabbit_version_6.1_%26%23038%3B_the_NIPS_tutorial.html">451 hunch net-2011-12-13-Vowpal Wabbit version 6.1 &#038; the NIPS tutorial</a></p>
<p>7 0.88163912 <a title="190-lda-7" href="../hunch_net-2007/hunch_net-2007-05-08-Conditional_Tournaments_for_Multiclass_to_Binary.html">243 hunch net-2007-05-08-Conditional Tournaments for Multiclass to Binary</a></p>
<p>8 0.82844687 <a title="190-lda-8" href="../hunch_net-2006/hunch_net-2006-11-20-Context_and_the_calculation_misperception.html">218 hunch net-2006-11-20-Context and the calculation misperception</a></p>
<p>9 0.688447 <a title="190-lda-9" href="../hunch_net-2013/hunch_net-2013-01-07-NYU_Large_Scale_Machine_Learning_Class.html">478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</a></p>
<p>10 0.58518076 <a title="190-lda-10" href="../hunch_net-2005/hunch_net-2005-05-16-Regret_minimizing_vs_error_limiting_reductions.html">72 hunch net-2005-05-16-Regret minimizing vs error limiting reductions</a></p>
<p>11 0.58291411 <a title="190-lda-11" href="../hunch_net-2006/hunch_net-2006-05-23-What_is_the_best_regret_transform_reduction_from_multiclass_to_binary%3F.html">181 hunch net-2006-05-23-What is the best regret transform reduction from multiclass to binary?</a></p>
<p>12 0.54401022 <a title="190-lda-12" href="../hunch_net-2011/hunch_net-2011-03-27-Vowpal_Wabbit%2C_v5.1.html">428 hunch net-2011-03-27-Vowpal Wabbit, v5.1</a></p>
<p>13 0.54120475 <a title="190-lda-13" href="../hunch_net-2008/hunch_net-2008-03-15-COLT_Open_Problems.html">292 hunch net-2008-03-15-COLT Open Problems</a></p>
<p>14 0.50829715 <a title="190-lda-14" href="../hunch_net-2011/hunch_net-2011-02-17-What_does_Watson_mean%3F.html">424 hunch net-2011-02-17-What does Watson mean?</a></p>
<p>15 0.50394255 <a title="190-lda-15" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>16 0.50206572 <a title="190-lda-16" href="../hunch_net-2010/hunch_net-2010-03-15-The_Efficient_Robust_Conditional_Probability_Estimation_Problem.html">391 hunch net-2010-03-15-The Efficient Robust Conditional Probability Estimation Problem</a></p>
<p>17 0.50107825 <a title="190-lda-17" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<p>18 0.49231786 <a title="190-lda-18" href="../hunch_net-2009/hunch_net-2009-07-31-Vowpal_Wabbit_Open_Source_Project.html">365 hunch net-2009-07-31-Vowpal Wabbit Open Source Project</a></p>
<p>19 0.49002132 <a title="190-lda-19" href="../hunch_net-2012/hunch_net-2012-09-29-Vowpal_Wabbit%2C_version_7.0.html">473 hunch net-2012-09-29-Vowpal Wabbit, version 7.0</a></p>
<p>20 0.48850864 <a title="190-lda-20" href="../hunch_net-2005/hunch_net-2005-02-28-Regularization.html">33 hunch net-2005-02-28-Regularization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
