<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>190 hunch net-2006-07-06-Branch Prediction Competition</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-190" href="#">hunch_net-2006-190</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>190 hunch net-2006-07-06-Branch Prediction Competition</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-190-html" href="http://hunch.net/?p=207">html</a></p><p>Introduction: Alan Fern  points out the  second branch prediction challenge  (due September 29) which is a follow up to the  first branch prediction competition .  Branch prediction is one of the fundamental learning problems of the computer age: without it our computers might run an order of magnitude slower.  This is a tough problem since there are sharp constraints on time and space complexity in an online environment.  For machine learning, the “idealistic track” may fit well.  Essentially, they remove these constraints to gain a weak upper bound on what might be done.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Alan Fern  points out the  second branch prediction challenge  (due September 29) which is a follow up to the  first branch prediction competition . [sent-1, score-2.172]
</p><p>2 Branch prediction is one of the fundamental learning problems of the computer age: without it our computers might run an order of magnitude slower. [sent-2, score-1.139]
</p><p>3 This is a tough problem since there are sharp constraints on time and space complexity in an online environment. [sent-3, score-1.015]
</p><p>4 For machine learning, the “idealistic track” may fit well. [sent-4, score-0.2]
</p><p>5 Essentially, they remove these constraints to gain a weak upper bound on what might be done. [sent-5, score-1.005]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('branch', 0.577), ('constraints', 0.248), ('age', 0.22), ('prediction', 0.204), ('fern', 0.204), ('september', 0.192), ('alan', 0.192), ('tough', 0.192), ('sharp', 0.176), ('remove', 0.156), ('gain', 0.152), ('magnitude', 0.145), ('computers', 0.139), ('follow', 0.137), ('upper', 0.134), ('competition', 0.13), ('track', 0.126), ('fit', 0.124), ('weak', 0.119), ('challenge', 0.115), ('bound', 0.1), ('computer', 0.097), ('space', 0.097), ('might', 0.096), ('fundamental', 0.094), ('run', 0.092), ('points', 0.089), ('order', 0.087), ('without', 0.086), ('second', 0.086), ('complexity', 0.085), ('essentially', 0.083), ('done', 0.075), ('due', 0.073), ('online', 0.07), ('since', 0.063), ('first', 0.053), ('problems', 0.048), ('time', 0.044), ('may', 0.044), ('problem', 0.04), ('machine', 0.032), ('learning', 0.026), ('one', 0.025)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="190-tfidf-1" href="../hunch_net-2006/hunch_net-2006-07-06-Branch_Prediction_Competition.html">190 hunch net-2006-07-06-Branch Prediction Competition</a></p>
<p>Introduction: Alan Fern  points out the  second branch prediction challenge  (due September 29) which is a follow up to the  first branch prediction competition .  Branch prediction is one of the fundamental learning problems of the computer age: without it our computers might run an order of magnitude slower.  This is a tough problem since there are sharp constraints on time and space complexity in an online environment.  For machine learning, the “idealistic track” may fit well.  Essentially, they remove these constraints to gain a weak upper bound on what might be done.</p><p>2 0.1397101 <a title="190-tfidf-2" href="../hunch_net-2005/hunch_net-2005-11-07-Prediction_Competitions.html">129 hunch net-2005-11-07-Prediction Competitions</a></p>
<p>Introduction: There are two prediction competitions currently in the air.  
  
 The  Performance Prediction Challenge   by  Isabelle Guyon .  Good entries minimize a weighted 0/1 loss + the difference between a prediction of this loss and the observed truth on 5 datasets.  Isabelle tells me all of the problems are “real world” and the test datasets are large enough (17K minimum) that the winner should be well determined by ability rather than luck.  This is due March 1. 
 The  Predictive Uncertainty Challenge  by  Gavin Cawley .  Good entries minimize log loss on real valued output variables for one synthetic and 3 “real” datasets related to atmospheric prediction. The use of log loss (which can be infinite and hence is never convergent) and smaller test sets of size 1K to 7K examples makes the winner of this contest more luck dependent.  Nevertheless, the contest may be of some interest particularly to the branch of learning (typically Bayes learning) which prefers to optimize log loss. 
  
May the</p><p>3 0.13470274 <a title="190-tfidf-3" href="../hunch_net-2011/hunch_net-2011-09-03-Fall_Machine_Learning_Events.html">443 hunch net-2011-09-03-Fall Machine Learning Events</a></p>
<p>Introduction: Many Machine Learning related events are coming up this fall.
  
  September 9 ,  abstracts for the New York Machine Learning Symposium  are due.  Send a 2 page pdf, if interested, and note that we:
 
 widened submissions to be from anybody rather than students. 
 set aside a larger fraction of time for contributed submissions.  
 
 
  September 15 , there is a  machine learning meetup , where I’ll be discussing terascale learning at AOL. 
  September 16 , there is a  CS&Econ; day  at New York Academy of Sciences.  This is not ML focused, but it’s easy to imagine interest. 
  September 23 and later   NIPS workshop  submissions start coming due.  As usual, there are too many good ones, so I won’t be able to attend all those that interest me.  I do hope some workshop makers consider ICML this coming summer, as we are increasing to a 2 day format for you.  Here are a few that interest me:
 
  Big Learning  is about dealing with lots of data.  Abstracts are due  September 30 . 
 The  Bayes</p><p>4 0.10014088 <a title="190-tfidf-4" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>Introduction: “Search” is the other branch of AI research which has been succesful.   Concrete examples include  Deep Blue  which beat the world chess champion and  Chinook  the champion checkers program.  A set of core search techniques exist including A * , alpha-beta pruning, and others that can be applied to any of many different search problems.
 
Given this, it may be surprising to learn that there has been relatively little succesful work on combining prediction and search.  Given also that  humans typically solve search problems using a number of predictive heuristics to narrow in on a solution, we might be surprised again.  However, the big successful search-based systems have typically not used “smart” search algorithms.  Insteady they have optimized for very fast search.  This is not for lack of trying… many people have tried to synthesize search and prediction to various degrees of success.   For example,  Knightcap  achieves good-but-not-stellar chess playing performance, and  TD-gammon</p><p>5 0.098822914 <a title="190-tfidf-5" href="../hunch_net-2006/hunch_net-2006-04-06-Bounds_greater_than_1.html">170 hunch net-2006-04-06-Bounds greater than 1</a></p>
<p>Introduction: Nati Srebro  and  Shai Ben-David  have a  paper  at  COLT  which, in the appendix, proves something very striking: several previous error bounds are  always  greater than 1.
 
 Background  One branch of learning theory focuses on theorems which
  
 Assume samples are drawn IID from an unknown distribution  D . 
 Fix a set of classifiers 
 Find a high probability bound on the maximum true error rate (with respect to  D ) as a function of the empirical error rate on the training set.
 
  
Many of these bounds become extremely complex and hairy.

 
 Current  Everyone working on this subject wants “tighter bounds”, however there are different definitions of “tighter”.  Some groups focus on “functional tightness” (getting the right functional dependency between the size of the training set and a parameterization of the hypothesis space) while  others  focus on “practical tightness” (finding bounds which work well on practical problems).  (I am definitely in the second camp.)
 
One of the da</p><p>6 0.095279895 <a title="190-tfidf-6" href="../hunch_net-2005/hunch_net-2005-02-03-Learning_Theory%2C_by_assumption.html">12 hunch net-2005-02-03-Learning Theory, by assumption</a></p>
<p>7 0.09066847 <a title="190-tfidf-7" href="../hunch_net-2007/hunch_net-2007-10-17-Online_as_the_new_adjective.html">267 hunch net-2007-10-17-Online as the new adjective</a></p>
<p>8 0.087605335 <a title="190-tfidf-8" href="../hunch_net-2007/hunch_net-2007-10-19-Second_Annual_Reinforcement_Learning_Competition.html">268 hunch net-2007-10-19-Second Annual Reinforcement Learning Competition</a></p>
<p>9 0.080537446 <a title="190-tfidf-9" href="../hunch_net-2010/hunch_net-2010-12-02-Traffic_Prediction_Problem.html">418 hunch net-2010-12-02-Traffic Prediction Problem</a></p>
<p>10 0.073512979 <a title="190-tfidf-10" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<p>11 0.069085598 <a title="190-tfidf-11" href="../hunch_net-2009/hunch_net-2009-05-19-CI_Fellows.html">355 hunch net-2009-05-19-CI Fellows</a></p>
<p>12 0.068238102 <a title="190-tfidf-12" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>13 0.066935293 <a title="190-tfidf-13" href="../hunch_net-2005/hunch_net-2005-02-28-Regularization.html">33 hunch net-2005-02-28-Regularization</a></p>
<p>14 0.062728599 <a title="190-tfidf-14" href="../hunch_net-2012/hunch_net-2012-06-05-ICML_acceptance_statistics.html">466 hunch net-2012-06-05-ICML acceptance statistics</a></p>
<p>15 0.05964331 <a title="190-tfidf-15" href="../hunch_net-2008/hunch_net-2008-02-10-Complexity_Illness.html">288 hunch net-2008-02-10-Complexity Illness</a></p>
<p>16 0.059186239 <a title="190-tfidf-16" href="../hunch_net-2010/hunch_net-2010-02-26-Yahoo%21_ML_events.html">389 hunch net-2010-02-26-Yahoo! ML events</a></p>
<p>17 0.056585602 <a title="190-tfidf-17" href="../hunch_net-2005/hunch_net-2005-09-08-Online_Learning_as_the_Mathematics_of_Accountability.html">109 hunch net-2005-09-08-Online Learning as the Mathematics of Accountability</a></p>
<p>18 0.056414362 <a title="190-tfidf-18" href="../hunch_net-2006/hunch_net-2006-09-18-What_is_missing_for_online_collaborative_research%3F.html">208 hunch net-2006-09-18-What is missing for online collaborative research?</a></p>
<p>19 0.056066092 <a title="190-tfidf-19" href="../hunch_net-2005/hunch_net-2005-09-14-The_Predictionist_Viewpoint.html">112 hunch net-2005-09-14-The Predictionist Viewpoint</a></p>
<p>20 0.055861402 <a title="190-tfidf-20" href="../hunch_net-2010/hunch_net-2010-12-26-NIPS_2010.html">420 hunch net-2010-12-26-NIPS 2010</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.125), (1, 0.039), (2, -0.026), (3, -0.034), (4, -0.007), (5, -0.011), (6, -0.032), (7, 0.015), (8, -0.02), (9, -0.04), (10, 0.012), (11, 0.096), (12, 0.013), (13, -0.071), (14, -0.033), (15, -0.053), (16, 0.043), (17, -0.078), (18, 0.029), (19, 0.022), (20, -0.009), (21, -0.033), (22, -0.151), (23, 0.035), (24, 0.033), (25, 0.04), (26, 0.156), (27, -0.011), (28, -0.056), (29, 0.01), (30, 0.145), (31, 0.079), (32, 0.071), (33, -0.016), (34, -0.086), (35, -0.09), (36, 0.003), (37, 0.074), (38, -0.019), (39, 0.042), (40, -0.024), (41, -0.019), (42, 0.006), (43, -0.041), (44, 0.035), (45, -0.039), (46, -0.006), (47, 0.065), (48, 0.06), (49, 0.033)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95854795 <a title="190-lsi-1" href="../hunch_net-2006/hunch_net-2006-07-06-Branch_Prediction_Competition.html">190 hunch net-2006-07-06-Branch Prediction Competition</a></p>
<p>Introduction: Alan Fern  points out the  second branch prediction challenge  (due September 29) which is a follow up to the  first branch prediction competition .  Branch prediction is one of the fundamental learning problems of the computer age: without it our computers might run an order of magnitude slower.  This is a tough problem since there are sharp constraints on time and space complexity in an online environment.  For machine learning, the “idealistic track” may fit well.  Essentially, they remove these constraints to gain a weak upper bound on what might be done.</p><p>2 0.59286201 <a title="190-lsi-2" href="../hunch_net-2010/hunch_net-2010-12-02-Traffic_Prediction_Problem.html">418 hunch net-2010-12-02-Traffic Prediction Problem</a></p>
<p>Introduction: Slashdot  points out the  Traffic Prediction Challenge  which looks pretty fun.  The temporal aspect seems to be very common in many real-world problems and somewhat understudied.</p><p>3 0.51796836 <a title="190-lsi-3" href="../hunch_net-2005/hunch_net-2005-07-13-Text_Entailment_at_AAAI.html">94 hunch net-2005-07-13-Text Entailment at AAAI</a></p>
<p>Introduction: Rajat Raina  presented a paper on the technique they used for the  PASCAL   Recognizing Textual Entailment  challenge.  
 
“Text entailment” is the problem of deciding if one sentence implies another.  For example the previous sentence entails: 
  
 Text entailment is a decision problem. 
 One sentence can imply another. 
  
The challenge was of the form: given an original sentence and another sentence predict whether there was an entailment.  All current techniques for predicting correctness of an entailment are at the “flail” stage—accuracies of around 58% where humans could achieve near 100% accuracy, so there is much room to improve.   Apparently, there may be another PASCAL challenge on this problem in the near future.</p><p>4 0.46902019 <a title="190-lsi-4" href="../hunch_net-2011/hunch_net-2011-10-03-Monday_announcements.html">446 hunch net-2011-10-03-Monday announcements</a></p>
<p>Introduction: Various people want to use hunch.net to announce things.  I’ve generally resisted this because I feared hunch becoming a pure announcement zone while I am much more interested contentful posts and discussion personally.  Nevertheless there is clearly some value and announcements are easy, so I’m planning to summarize announcements on Mondays.
  
  D. Sculley  points out an interesting  Semisupervised feature learning  competition, with a deadline of October 17.  
  Lihong Li  points out the  webscope user interaction dataset  which is the first high quality exploration dataset I’m aware of that is publicly available. 
 Seth Rogers points out  CrossValidated  which looks similar in conception to  metaoptimize , but directly using the  stackoverflow  interface and with a bit more of a statistics twist.</p><p>5 0.46603647 <a title="190-lsi-5" href="../hunch_net-2012/hunch_net-2012-03-13-The_Submodularity_workshop_and_Lucca_Professorship.html">459 hunch net-2012-03-13-The Submodularity workshop and Lucca Professorship</a></p>
<p>Introduction: Nina  points out the  Submodularity Workshop   March 19-20  next week at  Georgia Tech .  Many people want to make Submodularity the new Convexity in machine learning, and it certainly seems worth exploring.
 
 Sara Olson  also points out a  tenured faculty position  at  IMT Lucca  with a deadline of  May 15th .  Lucca happens to be the ancestral home of 1/4 of my heritage</p><p>6 0.45309103 <a title="190-lsi-6" href="../hunch_net-2007/hunch_net-2007-10-19-Second_Annual_Reinforcement_Learning_Competition.html">268 hunch net-2007-10-19-Second Annual Reinforcement Learning Competition</a></p>
<p>7 0.45200869 <a title="190-lsi-7" href="../hunch_net-2005/hunch_net-2005-04-14-Families_of_Learning_Theory_Statements.html">56 hunch net-2005-04-14-Families of Learning Theory Statements</a></p>
<p>8 0.4435623 <a title="190-lsi-8" href="../hunch_net-2005/hunch_net-2005-11-07-Prediction_Competitions.html">129 hunch net-2005-11-07-Prediction Competitions</a></p>
<p>9 0.43016171 <a title="190-lsi-9" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>10 0.42806429 <a title="190-lsi-10" href="../hunch_net-2005/hunch_net-2005-02-28-Regularization.html">33 hunch net-2005-02-28-Regularization</a></p>
<p>11 0.42366642 <a title="190-lsi-11" href="../hunch_net-2006/hunch_net-2006-02-07-Pittsburgh_Mind_Reading_Competition.html">155 hunch net-2006-02-07-Pittsburgh Mind Reading Competition</a></p>
<p>12 0.41531983 <a title="190-lsi-12" href="../hunch_net-2005/hunch_net-2005-04-27-DARPA_project%3A_LAGR.html">63 hunch net-2005-04-27-DARPA project: LAGR</a></p>
<p>13 0.41428298 <a title="190-lsi-13" href="../hunch_net-2007/hunch_net-2007-06-24-Interesting_Papers_at_ICML_2007.html">251 hunch net-2007-06-24-Interesting Papers at ICML 2007</a></p>
<p>14 0.40415743 <a title="190-lsi-14" href="../hunch_net-2005/hunch_net-2005-02-02-Kolmogorov_Complexity_and_Googling.html">10 hunch net-2005-02-02-Kolmogorov Complexity and Googling</a></p>
<p>15 0.40303764 <a title="190-lsi-15" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>16 0.39126119 <a title="190-lsi-16" href="../hunch_net-2010/hunch_net-2010-05-20-Google_Predict.html">399 hunch net-2010-05-20-Google Predict</a></p>
<p>17 0.3905195 <a title="190-lsi-17" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>18 0.38109365 <a title="190-lsi-18" href="../hunch_net-2011/hunch_net-2011-03-20-KDD_Cup_2011.html">427 hunch net-2011-03-20-KDD Cup 2011</a></p>
<p>19 0.3804239 <a title="190-lsi-19" href="../hunch_net-2005/hunch_net-2005-09-14-The_Predictionist_Viewpoint.html">112 hunch net-2005-09-14-The Predictionist Viewpoint</a></p>
<p>20 0.37934792 <a title="190-lsi-20" href="../hunch_net-2005/hunch_net-2005-03-02-Prior%2C_%26%238220%3BPrior%26%238221%3B_and_Bias.html">34 hunch net-2005-03-02-Prior, &#8220;Prior&#8221; and Bias</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(20, 0.337), (27, 0.319), (55, 0.034), (94, 0.159)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.90394211 <a title="190-lda-1" href="../hunch_net-2006/hunch_net-2006-07-06-Branch_Prediction_Competition.html">190 hunch net-2006-07-06-Branch Prediction Competition</a></p>
<p>Introduction: Alan Fern  points out the  second branch prediction challenge  (due September 29) which is a follow up to the  first branch prediction competition .  Branch prediction is one of the fundamental learning problems of the computer age: without it our computers might run an order of magnitude slower.  This is a tough problem since there are sharp constraints on time and space complexity in an online environment.  For machine learning, the “idealistic track” may fit well.  Essentially, they remove these constraints to gain a weak upper bound on what might be done.</p><p>2 0.87008876 <a title="190-lda-2" href="../hunch_net-2005/hunch_net-2005-01-31-Watchword%3A_Assumption.html">7 hunch net-2005-01-31-Watchword: Assumption</a></p>
<p>Introduction: “Assumption” is another word to be careful with in machine learning because it is used in several ways.
  
  Assumption = Bias  There are several ways to see that some form of ‘bias’ (= preferring of one solution over another) is necessary.   This is obvious in an adversarial setting.  A good bit of work has been expended explaining this in other settings with “ no free lunch ” theorems.  This is a usage specialized to learning which is particularly common when talking about priors for Bayesian Learning. 
  Assumption = “if” of a theorem  The assumptions are the ‘if’ part of the ‘if-then’ in a theorem.  This is a fairly common usage. 
  Assumption = Axiom  The assumptions are the things that we assume are true, but which we cannot verify.  Examples are “the IID assumption” or “my problem is a DNF on a small number of bits”.  This is the usage which I prefer. 
  
One difficulty with any use of the word “assumption” is that you often encounter “if  assumption  then  conclusion  so if  no</p><p>3 0.83562326 <a title="190-lda-3" href="../hunch_net-2006/hunch_net-2006-09-18-What_is_missing_for_online_collaborative_research%3F.html">208 hunch net-2006-09-18-What is missing for online collaborative research?</a></p>
<p>Introduction: The internet has recently made the research process much smoother: papers are easy to obtain, citations are easy to follow, and unpublished “tutorials” are often available. Yet, new research fields can look very complicated to outsiders or newcomers. Every paper is like a small piece of an unfinished jigsaw puzzle: to understand just one publication, a researcher without experience in the field will typically have to follow several layers of citations, and many of the papers he encounters have a great deal of repeated information. Furthermore, from one publication to the next, notation and terminology may not be consistent which can further confuse the reader.
 
But the internet is now proving to be an extremely useful medium for collaboration and knowledge aggregation. Online forums allow users to ask and answer questions and to share ideas. The recent phenomenon of Wikipedia provides a proof-of-concept for the “anyone can edit” system. Can such models be used to facilitate research a</p><p>4 0.76311147 <a title="190-lda-4" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>Introduction: This post is partly meant as an advertisement for the  reductions tutorial   Alina ,  Bianca , and I are planning to do at  ICML .  Please come, if you are interested.
 
Many research programs can be thought of as finding and building new useful abstractions.  The running example I’ll use is  learning reductions  where I have experience.  The basic abstraction here is that we can build a learning algorithm capable of solving classification problems up to a small expected regret.   This is used repeatedly to solve more complex problems.
 
In working on a new abstraction, I think you typically run into many substantial problems of understanding, which make publishing particularly difficult.
  
 It is difficult to seriously discuss the reason behind or mechanism for abstraction in a conference paper with small page limits.  People rarely see such discussions and hence have little basis on which to think about new abstractions.    Another difficulty is that when building an abstraction, yo</p><p>5 0.70442766 <a title="190-lda-5" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>Introduction: Yahoo! laid off people .  Unlike every previous time there have been layoffs, this is serious for  Yahoo! Research .  
 
We had advanced warning from  Prabhakar  through the  simple act of leaving .  Yahoo! Research was a world class organization that Prabhakar recruited much of personally, so it is deeply implausible that he would spontaneously decide to leave.  My first thought when I saw the news was “Uhoh,  Rob  said that he knew it was serious when the head of ATnT Research left.”  In this case it was even more significant, because Prabhakar recruited me on the premise that Y!R was an experiment in how research should be done: via a combination of high quality people and high engagement with the company.  Prabhakar’s departure is a clear end to that experiment.
 
The result is ambiguous from a business perspective.  Y!R clearly was not capable of saving the company from its illnesses.  I’m not privy to the internal accounting of impact and this is the kind of subject where there c</p><p>6 0.70242471 <a title="190-lda-6" href="../hunch_net-2007/hunch_net-2007-07-01-Watchword%3A_Online_Learning.html">252 hunch net-2007-07-01-Watchword: Online Learning</a></p>
<p>7 0.69288707 <a title="190-lda-7" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>8 0.69192994 <a title="190-lda-8" href="../hunch_net-2007/hunch_net-2007-08-12-Exponentiated_Gradient.html">258 hunch net-2007-08-12-Exponentiated Gradient</a></p>
<p>9 0.68997741 <a title="190-lda-9" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<p>10 0.68809581 <a title="190-lda-10" href="../hunch_net-2005/hunch_net-2005-03-18-Binomial_Weighting.html">43 hunch net-2005-03-18-Binomial Weighting</a></p>
<p>11 0.68588859 <a title="190-lda-11" href="../hunch_net-2005/hunch_net-2005-03-22-Active_learning.html">45 hunch net-2005-03-22-Active learning</a></p>
<p>12 0.684533 <a title="190-lda-12" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>13 0.68310702 <a title="190-lda-13" href="../hunch_net-2009/hunch_net-2009-01-21-Nearly_all_natural_problems_require_nonlinearity.html">337 hunch net-2009-01-21-Nearly all natural problems require nonlinearity</a></p>
<p>14 0.68092018 <a title="190-lda-14" href="../hunch_net-2008/hunch_net-2008-03-23-Interactive_Machine_Learning.html">293 hunch net-2008-03-23-Interactive Machine Learning</a></p>
<p>15 0.67909336 <a title="190-lda-15" href="../hunch_net-2007/hunch_net-2007-01-26-Parallel_Machine_Learning_Problems.html">229 hunch net-2007-01-26-Parallel Machine Learning Problems</a></p>
<p>16 0.67897332 <a title="190-lda-16" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<p>17 0.6776436 <a title="190-lda-17" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>18 0.67750895 <a title="190-lda-18" href="../hunch_net-2006/hunch_net-2006-07-13-Regression_vs._Classification_as_a_Primitive.html">196 hunch net-2006-07-13-Regression vs. Classification as a Primitive</a></p>
<p>19 0.67625314 <a title="190-lda-19" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>20 0.67606461 <a title="190-lda-20" href="../hunch_net-2005/hunch_net-2005-03-15-The_State_of_Tight_Bounds.html">41 hunch net-2005-03-15-The State of Tight Bounds</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
