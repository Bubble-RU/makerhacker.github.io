<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>197 hunch net-2006-07-17-A Winner</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-197" href="#">hunch_net-2006-197</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>197 hunch net-2006-07-17-A Winner</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-197-html" href="http://hunch.net/?p=215">html</a></p><p>Introduction: Ed Snelsonwon thePredictive Uncertainty in Environmental Modelling
Competitionin the temp(erature) category usingthis algorithm. Some
characteristics of the algorithm are:Gradient descent… on about 600
parameters… with local minima… to solve regression.This bears a strong
resemblance to a neural network. The two main differences seem to be:The
system has a probabilistic interpretation (which may aid design).There are
(perhaps) fewer parameters than a typical neural network might have for the
same problem (aiding speed).</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('parameters', 0.306), ('neural', 0.293), ('ed', 0.267), ('environmental', 0.267), ('resemblance', 0.267), ('characteristics', 0.222), ('bears', 0.222), ('category', 0.213), ('minima', 0.213), ('interpretation', 0.206), ('differences', 0.194), ('uncertainty', 0.18), ('aid', 0.172), ('fewer', 0.169), ('main', 0.163), ('speed', 0.155), ('network', 0.153), ('local', 0.153), ('descent', 0.148), ('probabilistic', 0.146)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="197-tfidf-1" href="../hunch_net-2006/hunch_net-2006-07-17-A_Winner.html">197 hunch net-2006-07-17-A Winner</a></p>
<p>Introduction: Ed Snelsonwon thePredictive Uncertainty in Environmental Modelling
Competitionin the temp(erature) category usingthis algorithm. Some
characteristics of the algorithm are:Gradient descent… on about 600
parameters… with local minima… to solve regression.This bears a strong
resemblance to a neural network. The two main differences seem to be:The
system has a probabilistic interpretation (which may aid design).There are
(perhaps) fewer parameters than a typical neural network might have for the
same problem (aiding speed).</p><p>2 0.21931754 <a title="197-tfidf-2" href="../hunch_net-2005/hunch_net-2005-09-12-Fast_Gradient_Descent.html">111 hunch net-2005-09-12-Fast Gradient Descent</a></p>
<p>Introduction: Nic Schaudolphhas been developing a fast gradient descent algorithm
calledStochastic Meta-Descent(SMD).Gradient descent is currently untrendy in
the machine learning community, but there remains a large number of people
using gradient descent on neural networks or other architectures from when it
was trendy in the early 1990s. There are three problems with gradient
descent.Gradient descent does not necessarily produce easily reproduced
results. Typical algorithms start with "set the initial parameters to small
random values".The design of the representation that gradient descent is
applied to is often nontrivial. In particular, knowing exactly how to build a
large neural network so that it will perform well requires knowledge which has
not been made easily applicable.Gradient descent can be slow. Obviously,
taking infinitesimal steps in the direction of the gradient would take
forever, so some finite step size must be used. What exactly this step size
should be is unclear. Many people</p><p>3 0.1408582 <a title="197-tfidf-3" href="../hunch_net-2011/hunch_net-2011-07-11-Interesting_Neural_Network_Papers_at_ICML_2011.html">438 hunch net-2011-07-11-Interesting Neural Network Papers at ICML 2011</a></p>
<p>Introduction: Maybe it's too early to call, but with four separate Neural Network sessions
at this year'sICML, it looks like Neural Networks are making a comeback. Here
are my highlights of these sessions. In general, my feeling is that these
papers both demystify deep learning and show its broader applicability.The
first observation I made is that the once disreputable "Neural" nomenclature
is being used againin lieu of"deep learning". Maybe it's because Adam Coates
et al. showed that single layer networks can work surprisingly well.An
Analysis of Single-Layer Networks in Unsupervised Feature Learning,Adam
Coates,Honglak Lee,Andrew Y. Ng(AISTATS 2011)The Importance of Encoding Versus
Training with Sparse Coding and Vector Quantization,Adam Coates,Andrew Y.
Ng(ICML 2011)Another surprising result out of Andrew Ng's group comes from
Andrew Saxe et al. who show that certain convolutional pooling architectures
can obtain close to state-of-the-art performance with random weights (that is,
without actuall</p><p>4 0.12447947 <a title="197-tfidf-4" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>Introduction: Many learning algorithms used in practice are fairly simple. Viewed
representationally, many prediction algorithms either compute a linear
separator of basic features (perceptron, winnow, weighted majority, SVM) or
perhaps a linear separator of slightly more complex features (2-layer neural
networks or kernelized SVMs). Should we go beyond this, and start using "deep"
representations?What is deep learning?Intuitively, deep learning is about
learning to predict in ways which can involve complex dependencies between the
input (observed) features.Specifying this more rigorously turns out to be
rather difficult. Consider the following cases:SVM with Gaussian Kernel. This
is not considered deep learning, because an SVM with a gaussian kernel can't
succinctly represent certain decision surfaces. One ofYann LeCun's examples is
recognizing objects based on pixel values. An SVM will need a new support
vector for each significantly different background. Since the number of
distinct backgrounds i</p><p>5 0.11140273 <a title="197-tfidf-5" href="../hunch_net-2006/hunch_net-2006-03-27-Gradients_everywhere.html">167 hunch net-2006-03-27-Gradients everywhere</a></p>
<p>Introduction: One of the basic observations from theatomic learning workshopis that
gradient-based optimization is pervasive. For example, at least 7 (of 12)
speakers used the word 'gradient' in their talk and several others may be
approximating a gradient. The essential useful quality of a gradient is that
it decouples local updates from global optimization. Restated: Given a
gradient, we can determine how to change individual parameters of the system
so as to improve overall performance.It's easy to feel depressed about this
and think "nothing has happened", but that appears untrue. Many of the talks
were about clever techniques for computing gradients where your calculus
textbook breaks down.Sometimes there are clever approximations of the
gradient. (Simon Osindero)Sometimes we can compute constrained gradients via
iterated gradient/project steps. (Ben Taskar)Sometimes we can compute
gradients anyways over mildly nondifferentiable functions. (Drew Bagnell)Even
given a gradient, the choice of upda</p><p>6 0.098439053 <a title="197-tfidf-6" href="../hunch_net-2007/hunch_net-2007-10-17-Online_as_the_new_adjective.html">267 hunch net-2007-10-17-Online as the new adjective</a></p>
<p>7 0.095871575 <a title="197-tfidf-7" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">23 hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<p>8 0.089012653 <a title="197-tfidf-8" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<p>9 0.08624059 <a title="197-tfidf-9" href="../hunch_net-2007/hunch_net-2007-08-12-Exponentiated_Gradient.html">258 hunch net-2007-08-12-Exponentiated Gradient</a></p>
<p>10 0.082991906 <a title="197-tfidf-10" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>11 0.076749586 <a title="197-tfidf-11" href="../hunch_net-2005/hunch_net-2005-02-17-Learning_Research_Programs.html">21 hunch net-2005-02-17-Learning Research Programs</a></p>
<p>12 0.073885612 <a title="197-tfidf-12" href="../hunch_net-2006/hunch_net-2006-05-16-The_value_of_the_orthodox_view_of_Boosting.html">179 hunch net-2006-05-16-The value of the orthodox view of Boosting</a></p>
<p>13 0.072986051 <a title="197-tfidf-13" href="../hunch_net-2006/hunch_net-2006-01-30-Should_the_Input_Representation_be_a_Vector%3F.html">152 hunch net-2006-01-30-Should the Input Representation be a Vector?</a></p>
<p>14 0.067408711 <a title="197-tfidf-14" href="../hunch_net-2012/hunch_net-2012-06-05-ICML_acceptance_statistics.html">466 hunch net-2012-06-05-ICML acceptance statistics</a></p>
<p>15 0.066692375 <a title="197-tfidf-15" href="../hunch_net-2005/hunch_net-2005-02-09-Intuitions_from_applied_learning.html">16 hunch net-2005-02-09-Intuitions from applied learning</a></p>
<p>16 0.064096957 <a title="197-tfidf-16" href="../hunch_net-2006/hunch_net-2006-07-26-Two_more_UAI_papers_of_interest.html">199 hunch net-2006-07-26-Two more UAI papers of interest</a></p>
<p>17 0.06029027 <a title="197-tfidf-17" href="../hunch_net-2010/hunch_net-2010-04-24-COLT_Treasurer_is_now_Phil_Long.html">394 hunch net-2010-04-24-COLT Treasurer is now Phil Long</a></p>
<p>18 0.059927423 <a title="197-tfidf-18" href="../hunch_net-2006/hunch_net-2006-11-22-Explicit_Randomization_in_Learning_algorithms.html">219 hunch net-2006-11-22-Explicit Randomization in Learning algorithms</a></p>
<p>19 0.058765769 <a title="197-tfidf-19" href="../hunch_net-2010/hunch_net-2010-12-04-Vowpal_Wabbit%2C_version_5.0%2C_and_the_second_heresy.html">419 hunch net-2010-12-04-Vowpal Wabbit, version 5.0, and the second heresy</a></p>
<p>20 0.058653448 <a title="197-tfidf-20" href="../hunch_net-2007/hunch_net-2007-12-21-Vowpal_Wabbit_Code_Release.html">281 hunch net-2007-12-21-Vowpal Wabbit Code Release</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.105), (1, -0.047), (2, 0.012), (3, 0.001), (4, -0.06), (5, 0.131), (6, -0.045), (7, 0.004), (8, 0.016), (9, 0.096), (10, 0.173), (11, -0.097), (12, -0.078), (13, 0.03), (14, 0.0), (15, 0.043), (16, -0.035), (17, 0.004), (18, -0.007), (19, 0.031), (20, -0.04), (21, -0.017), (22, -0.021), (23, -0.038), (24, -0.035), (25, 0.136), (26, 0.057), (27, -0.064), (28, 0.045), (29, -0.008), (30, 0.048), (31, 0.068), (32, -0.03), (33, 0.025), (34, 0.086), (35, 0.057), (36, 0.064), (37, -0.129), (38, -0.007), (39, 0.082), (40, -0.033), (41, -0.087), (42, -0.092), (43, 0.006), (44, 0.003), (45, -0.001), (46, 0.095), (47, 0.04), (48, -0.02), (49, -0.017)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98505473 <a title="197-lsi-1" href="../hunch_net-2006/hunch_net-2006-07-17-A_Winner.html">197 hunch net-2006-07-17-A Winner</a></p>
<p>Introduction: Ed Snelsonwon thePredictive Uncertainty in Environmental Modelling
Competitionin the temp(erature) category usingthis algorithm. Some
characteristics of the algorithm are:Gradient descent… on about 600
parameters… with local minima… to solve regression.This bears a strong
resemblance to a neural network. The two main differences seem to be:The
system has a probabilistic interpretation (which may aid design).There are
(perhaps) fewer parameters than a typical neural network might have for the
same problem (aiding speed).</p><p>2 0.78278536 <a title="197-lsi-2" href="../hunch_net-2005/hunch_net-2005-09-12-Fast_Gradient_Descent.html">111 hunch net-2005-09-12-Fast Gradient Descent</a></p>
<p>Introduction: Nic Schaudolphhas been developing a fast gradient descent algorithm
calledStochastic Meta-Descent(SMD).Gradient descent is currently untrendy in
the machine learning community, but there remains a large number of people
using gradient descent on neural networks or other architectures from when it
was trendy in the early 1990s. There are three problems with gradient
descent.Gradient descent does not necessarily produce easily reproduced
results. Typical algorithms start with "set the initial parameters to small
random values".The design of the representation that gradient descent is
applied to is often nontrivial. In particular, knowing exactly how to build a
large neural network so that it will perform well requires knowledge which has
not been made easily applicable.Gradient descent can be slow. Obviously,
taking infinitesimal steps in the direction of the gradient would take
forever, so some finite step size must be used. What exactly this step size
should be is unclear. Many people</p><p>3 0.74727213 <a title="197-lsi-3" href="../hunch_net-2006/hunch_net-2006-03-27-Gradients_everywhere.html">167 hunch net-2006-03-27-Gradients everywhere</a></p>
<p>Introduction: One of the basic observations from theatomic learning workshopis that
gradient-based optimization is pervasive. For example, at least 7 (of 12)
speakers used the word 'gradient' in their talk and several others may be
approximating a gradient. The essential useful quality of a gradient is that
it decouples local updates from global optimization. Restated: Given a
gradient, we can determine how to change individual parameters of the system
so as to improve overall performance.It's easy to feel depressed about this
and think "nothing has happened", but that appears untrue. Many of the talks
were about clever techniques for computing gradients where your calculus
textbook breaks down.Sometimes there are clever approximations of the
gradient. (Simon Osindero)Sometimes we can compute constrained gradients via
iterated gradient/project steps. (Ben Taskar)Sometimes we can compute
gradients anyways over mildly nondifferentiable functions. (Drew Bagnell)Even
given a gradient, the choice of upda</p><p>4 0.55732417 <a title="197-lsi-4" href="../hunch_net-2006/hunch_net-2006-05-16-The_value_of_the_orthodox_view_of_Boosting.html">179 hunch net-2006-05-16-The value of the orthodox view of Boosting</a></p>
<p>Introduction: The term "boosting" comes from the idea of using a meta-algorithm which takes
"weak" learners (that may be able to only barely predict slightly better than
random) and turn them into strongly capable learners (which predict very
well).Adaboostin 1995 was the first widely used (and useful) boosting
algorithm, although there were theoretical boosting algorithms floating around
since 1990 (see the bottom ofthis page).Since then, many different
interpretations of why boosting works have arisen. There is significant
discussion about these different views in theannals of statistics, including
aresponsebyYoav FreundandRobert Schapire.I believe there is a great deal of
value to be found in the original view of boosting (meta-algorithm for
creating a strong learner from a weak learner). This is not a claim that one
particular viewpoint obviates the value of all others, but rather that no
other viewpoint seems to really capture important properties.Comparing with
all other views of boosting is t</p><p>5 0.52183497 <a title="197-lsi-5" href="../hunch_net-2011/hunch_net-2011-07-11-Interesting_Neural_Network_Papers_at_ICML_2011.html">438 hunch net-2011-07-11-Interesting Neural Network Papers at ICML 2011</a></p>
<p>Introduction: Maybe it's too early to call, but with four separate Neural Network sessions
at this year'sICML, it looks like Neural Networks are making a comeback. Here
are my highlights of these sessions. In general, my feeling is that these
papers both demystify deep learning and show its broader applicability.The
first observation I made is that the once disreputable "Neural" nomenclature
is being used againin lieu of"deep learning". Maybe it's because Adam Coates
et al. showed that single layer networks can work surprisingly well.An
Analysis of Single-Layer Networks in Unsupervised Feature Learning,Adam
Coates,Honglak Lee,Andrew Y. Ng(AISTATS 2011)The Importance of Encoding Versus
Training with Sparse Coding and Vector Quantization,Adam Coates,Andrew Y.
Ng(ICML 2011)Another surprising result out of Andrew Ng's group comes from
Andrew Saxe et al. who show that certain convolutional pooling architectures
can obtain close to state-of-the-art performance with random weights (that is,
without actuall</p><p>6 0.45127881 <a title="197-lsi-6" href="../hunch_net-2007/hunch_net-2007-08-12-Exponentiated_Gradient.html">258 hunch net-2007-08-12-Exponentiated Gradient</a></p>
<p>7 0.44242781 <a title="197-lsi-7" href="../hunch_net-2005/hunch_net-2005-02-09-Intuitions_from_applied_learning.html">16 hunch net-2005-02-09-Intuitions from applied learning</a></p>
<p>8 0.4397203 <a title="197-lsi-8" href="../hunch_net-2006/hunch_net-2006-11-22-Explicit_Randomization_in_Learning_algorithms.html">219 hunch net-2006-11-22-Explicit Randomization in Learning algorithms</a></p>
<p>9 0.42004216 <a title="197-lsi-9" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>10 0.41600493 <a title="197-lsi-10" href="../hunch_net-2005/hunch_net-2005-06-29-Not_EM_for_clustering_at_COLT.html">87 hunch net-2005-06-29-Not EM for clustering at COLT</a></p>
<p>11 0.41552496 <a title="197-lsi-11" href="../hunch_net-2006/hunch_net-2006-01-30-Should_the_Input_Representation_be_a_Vector%3F.html">152 hunch net-2006-01-30-Should the Input Representation be a Vector?</a></p>
<p>12 0.40268174 <a title="197-lsi-12" href="../hunch_net-2011/hunch_net-2011-08-15-Vowpal_Wabbit_6.0.html">441 hunch net-2011-08-15-Vowpal Wabbit 6.0</a></p>
<p>13 0.35938841 <a title="197-lsi-13" href="../hunch_net-2005/hunch_net-2005-12-09-Some_NIPS_papers.html">138 hunch net-2005-12-09-Some NIPS papers</a></p>
<p>14 0.35500669 <a title="197-lsi-14" href="../hunch_net-2005/hunch_net-2005-11-16-The_Everything_Ensemble_Edge.html">131 hunch net-2005-11-16-The Everything Ensemble Edge</a></p>
<p>15 0.3497946 <a title="197-lsi-15" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>16 0.34675038 <a title="197-lsi-16" href="../hunch_net-2005/hunch_net-2005-05-29-Maximum_Margin_Mismatch%3F.html">77 hunch net-2005-05-29-Maximum Margin Mismatch?</a></p>
<p>17 0.34075341 <a title="197-lsi-17" href="../hunch_net-2011/hunch_net-2011-04-18-A_paper_not_at_Snowbird.html">431 hunch net-2011-04-18-A paper not at Snowbird</a></p>
<p>18 0.33994004 <a title="197-lsi-18" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">23 hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<p>19 0.33459571 <a title="197-lsi-19" href="../hunch_net-2006/hunch_net-2006-07-05-more_icml_papers.html">189 hunch net-2006-07-05-more icml papers</a></p>
<p>20 0.32624519 <a title="197-lsi-20" href="../hunch_net-2008/hunch_net-2008-07-06-To_Dual_or_Not.html">308 hunch net-2008-07-06-To Dual or Not</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(29, 0.587), (35, 0.113), (42, 0.144)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.91065276 <a title="197-lda-1" href="../hunch_net-2006/hunch_net-2006-07-17-A_Winner.html">197 hunch net-2006-07-17-A Winner</a></p>
<p>Introduction: Ed Snelsonwon thePredictive Uncertainty in Environmental Modelling
Competitionin the temp(erature) category usingthis algorithm. Some
characteristics of the algorithm are:Gradient descent… on about 600
parameters… with local minima… to solve regression.This bears a strong
resemblance to a neural network. The two main differences seem to be:The
system has a probabilistic interpretation (which may aid design).There are
(perhaps) fewer parameters than a typical neural network might have for the
same problem (aiding speed).</p><p>2 0.85731488 <a title="197-lda-2" href="../hunch_net-2006/hunch_net-2006-10-02-%241M_Netflix_prediction_contest.html">211 hunch net-2006-10-02-$1M Netflix prediction contest</a></p>
<p>Introduction: Netflix isrunning a contestto improve recommender prediction systems. A 10%
improvement over their current system yields a $1M prize. Failing that, the
best smaller improvement yields a smaller $50K prize. This contest looks quite
real, and the $50K prize money is almost certainly achievable with a bit of
thought. The contest also comes with a dataset which is apparently 2 orders of
magnitude larger than any other public recommendation system datasets.</p><p>3 0.78864199 <a title="197-lda-3" href="../hunch_net-2009/hunch_net-2009-04-21-Interesting_Presentations_at_Snowbird.html">349 hunch net-2009-04-21-Interesting Presentations at Snowbird</a></p>
<p>Introduction: Here are a few of presentations interesting me at thesnowbird learningworkshop
(which, amusingly, was in Florida withAIStat).Thomas Breueldescribed machine
learning problems within OCR and an open sourceOCR software/researchplatform
with modular learning components as well has a 60Million size dataset derived
fromGoogle's scanned books.Kristen GraumanandFei-Fei Lidiscussed using active
learning with different cost labels and large datasets forimage ontology. Both
of them usedMechanical Turkas alabeling system, which looks to become routine,
at least for vision problems.Russ Tedrakediscussed using machine learning for
control, with a basic claim that it was the way to go for problems involving a
mediumReynold's numbersuch as in bird flight, where simulation is extremely
intense.Yann LeCunpresented a poster on anFPGA for convolutional neural
networksyielding a factor of 100 speedup in processing. In addition to the
graphics processor approachRajathas worked on, this seems like an effecti</p><p>4 0.65062362 <a title="197-lda-4" href="../hunch_net-2007/hunch_net-2007-04-13-What_to_do_with_an_unreasonable_conditional_accept.html">238 hunch net-2007-04-13-What to do with an unreasonable conditional accept</a></p>
<p>Introduction: Last year about this time, we received a conditional accept for thesearn
paper, which asked us to reference a paper that was not reasonable to cite
because there was strictly more relevant work by the same authors that we
already cited. We wrote a response explaining this, and didn't cite it in the
final draft, giving the SPC an excuse toreject the paper, leading to
unhappiness for all.Later,Sanjoy Dasguptasuggested that an alternative was to
talk to the PC chair instead, as soon as you see that a conditional accept is
unreasonable.William Cohenand I spoke about this by email, the relevant bit of
which is:If an SPC asks for a revision that is inappropriate, the
correctaction is to contact the chairs as soon as the decision is made,clearly
explaining what the problem is, so we can decide whether ornot to over-rule
the SPC. As you say, this is extra work for uschairs, but that's part of the
job, and we're willing to do that sortof work to improve the overall quality
of the reviewing proc</p><p>5 0.59919918 <a title="197-lda-5" href="../hunch_net-2008/hunch_net-2008-12-07-A_NIPS_paper.html">330 hunch net-2008-12-07-A NIPS paper</a></p>
<p>Introduction: I'm skipping NIPS this year in favor ofAda, but I wanted to point outthis
paperbyAndriy MnihandGeoff Hinton. The basic claim of the paper is that by
carefully but automatically constructing a binary tree over words, it's
possible to predict words well with huge computational resource savings over
unstructured approaches.I'm interested in this beyond the application to word
prediction because it is relevant to the general normalization problem: If you
want to predict the probability of one of a large number of events, often you
must compute a predicted score for all the events and then normalize, a
computationally inefficient operation. The problem comes up in many places
using probabilistic models, but I've run into it with high-dimensional
regression.There are a couple workarounds for this computational
bug:Approximate. There are many ways. Often the approximations are
uncontrolled (i.e. can be arbitrarily bad), and hence finicky in
application.Avoid. You don't really want a probabili</p><p>6 0.30748531 <a title="197-lda-6" href="../hunch_net-2010/hunch_net-2010-12-04-Vowpal_Wabbit%2C_version_5.0%2C_and_the_second_heresy.html">419 hunch net-2010-12-04-Vowpal Wabbit, version 5.0, and the second heresy</a></p>
<p>7 0.3067748 <a title="197-lda-7" href="../hunch_net-2012/hunch_net-2012-06-05-ICML_acceptance_statistics.html">466 hunch net-2012-06-05-ICML acceptance statistics</a></p>
<p>8 0.28761545 <a title="197-lda-8" href="../hunch_net-2005/hunch_net-2005-05-29-Maximum_Margin_Mismatch%3F.html">77 hunch net-2005-05-29-Maximum Margin Mismatch?</a></p>
<p>9 0.28552592 <a title="197-lda-9" href="../hunch_net-2007/hunch_net-2007-01-15-The_Machine_Learning_Department.html">228 hunch net-2007-01-15-The Machine Learning Department</a></p>
<p>10 0.28549325 <a title="197-lda-10" href="../hunch_net-2006/hunch_net-2006-05-16-The_value_of_the_orthodox_view_of_Boosting.html">179 hunch net-2006-05-16-The value of the orthodox view of Boosting</a></p>
<p>11 0.28511059 <a title="197-lda-11" href="../hunch_net-2005/hunch_net-2005-05-21-What_is_the_right_form_of_modularity_in_structured_prediction%3F.html">74 hunch net-2005-05-21-What is the right form of modularity in structured prediction?</a></p>
<p>12 0.28318068 <a title="197-lda-12" href="../hunch_net-2009/hunch_net-2009-09-21-Netflix_finishes_%28and_starts%29.html">371 hunch net-2009-09-21-Netflix finishes (and starts)</a></p>
<p>13 0.28311419 <a title="197-lda-13" href="../hunch_net-2006/hunch_net-2006-06-24-Online_convex_optimization_at_COLT.html">186 hunch net-2006-06-24-Online convex optimization at COLT</a></p>
<p>14 0.27970156 <a title="197-lda-14" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>15 0.27883852 <a title="197-lda-15" href="../hunch_net-2008/hunch_net-2008-05-23-Three_levels_of_addressing_the_Netflix_Prize.html">301 hunch net-2008-05-23-Three levels of addressing the Netflix Prize</a></p>
<p>16 0.27866244 <a title="197-lda-16" href="../hunch_net-2008/hunch_net-2008-04-30-Concerns_about_the_Large_Scale_Learning_Challenge.html">300 hunch net-2008-04-30-Concerns about the Large Scale Learning Challenge</a></p>
<p>17 0.27197981 <a title="197-lda-17" href="../hunch_net-2005/hunch_net-2005-06-13-Wikis_for_Summer_Schools_and_Workshops.html">81 hunch net-2005-06-13-Wikis for Summer Schools and Workshops</a></p>
<p>18 0.27154523 <a title="197-lda-18" href="../hunch_net-2010/hunch_net-2010-04-14-MLcomp%3A_a_website_for_objectively_comparing_ML_algorithms.html">393 hunch net-2010-04-14-MLcomp: a website for objectively comparing ML algorithms</a></p>
<p>19 0.26686999 <a title="197-lda-19" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>20 0.2666961 <a title="197-lda-20" href="../hunch_net-2009/hunch_net-2009-07-31-Vowpal_Wabbit_Open_Source_Project.html">365 hunch net-2009-07-31-Vowpal Wabbit Open Source Project</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
