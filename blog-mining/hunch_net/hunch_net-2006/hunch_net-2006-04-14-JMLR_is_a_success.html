<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>172 hunch net-2006-04-14-JMLR is a success</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-172" href="#">hunch_net-2006-172</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>172 hunch net-2006-04-14-JMLR is a success</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-172-html" href="http://hunch.net/?p=183">html</a></p><p>Introduction: In 2001, the "Journal of Machine Learning Research" was created in reaction to
unadaptive publisher policies atMLJ. Essentially, with the creation of the
internet, the bottleneck in publishing research shifted from publishing to
research. Thedeclaration of independenceaccompanying this move expresses the
reasons why in greater detail.MLJ has strongly changed its policy in reaction
to this. In particular, there is no longer an assignment of copyright to the
publisher (*), and MLJ regularly sponsors many student "best paper awards"
across several conferences with cash prizes. This is an advantage of MLJ over
JMLR: MLJ can afford to sponsor cash prizes for the machine learning
community. The remaining disadvantage is that reading papers in MLJ sometimes
requires searching for the author's website where the free version is
available. In contrast, JMLR articles are freely available to everyone off the
JMLR website. Whether or not this disadvantage cancels the advantage is
debatable, but ess</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In 2001, the "Journal of Machine Learning Research" was created in reaction to unadaptive publisher policies atMLJ. [sent-1, score-0.468]
</p><p>2 Essentially, with the creation of the internet, the bottleneck in publishing research shifted from publishing to research. [sent-2, score-0.65]
</p><p>3 Thedeclaration of independenceaccompanying this move expresses the reasons why in greater detail. [sent-3, score-0.168]
</p><p>4 MLJ has strongly changed its policy in reaction to this. [sent-4, score-0.198]
</p><p>5 In particular, there is no longer an assignment of copyright to the publisher (*), and MLJ regularly sponsors many student "best paper awards" across several conferences with cash prizes. [sent-5, score-0.723]
</p><p>6 This is an advantage of MLJ over JMLR: MLJ can afford to sponsor cash prizes for the machine learning community. [sent-6, score-0.54]
</p><p>7 The remaining disadvantage is that reading papers in MLJ sometimes requires searching for the author's website where the free version is available. [sent-7, score-0.412]
</p><p>8 In contrast, JMLR articles are freely available to everyone off the JMLR website. [sent-8, score-0.17]
</p><p>9 Whether or not this disadvantage cancels the advantage is debatable, but essentially no one working on machine learning argues with the following: the changes brought by the creation of JMLR have been positive for the general machine learning community. [sent-9, score-0.588]
</p><p>10 This model can and should be emulated in other areas of research where publishers are not behaving in a sufficiently constructive manner. [sent-10, score-0.305]
</p><p>11 Doing so requires two vital ingredients: a consensus of leaders to support a new journal and the willigness to spend the time and effort setting it up. [sent-11, score-0.657]
</p><p>12 Presumably, some lessons on how to do this have been learned by the editors of JMLR and they are willing to share it. [sent-12, score-0.256]
</p><p>13 (*) Back in the day, it was typical to be forced to sign over all rights to your journal paper, then ignore this and place it on your homepage. [sent-13, score-0.531]
</p><p>14 The natural act of placing your paper on your webpage is no longer illegal. [sent-14, score-0.422]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mlj', 0.447), ('jmlr', 0.434), ('journal', 0.23), ('reaction', 0.198), ('publisher', 0.198), ('creation', 0.174), ('cash', 0.165), ('disadvantage', 0.144), ('publishing', 0.124), ('longer', 0.121), ('requires', 0.102), ('advantage', 0.101), ('expresses', 0.099), ('behaving', 0.099), ('editors', 0.099), ('sponsor', 0.099), ('brought', 0.092), ('leaders', 0.092), ('rights', 0.092), ('afford', 0.092), ('regularly', 0.087), ('articles', 0.087), ('ingredients', 0.087), ('bottleneck', 0.087), ('awards', 0.087), ('consensus', 0.087), ('lessons', 0.087), ('searching', 0.083), ('remaining', 0.083), ('prizes', 0.083), ('freely', 0.083), ('vital', 0.079), ('placing', 0.079), ('paper', 0.078), ('essentially', 0.077), ('shifted', 0.074), ('assignment', 0.074), ('constructive', 0.072), ('act', 0.072), ('presumably', 0.072), ('webpage', 0.072), ('policies', 0.072), ('ignore', 0.07), ('willing', 0.07), ('sign', 0.07), ('forced', 0.069), ('move', 0.069), ('research', 0.067), ('sufficiently', 0.067), ('spend', 0.067)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="172-tfidf-1" href="../hunch_net-2006/hunch_net-2006-04-14-JMLR_is_a_success.html">172 hunch net-2006-04-14-JMLR is a success</a></p>
<p>Introduction: In 2001, the "Journal of Machine Learning Research" was created in reaction to
unadaptive publisher policies atMLJ. Essentially, with the creation of the
internet, the bottleneck in publishing research shifted from publishing to
research. Thedeclaration of independenceaccompanying this move expresses the
reasons why in greater detail.MLJ has strongly changed its policy in reaction
to this. In particular, there is no longer an assignment of copyright to the
publisher (*), and MLJ regularly sponsors many student "best paper awards"
across several conferences with cash prizes. This is an advantage of MLJ over
JMLR: MLJ can afford to sponsor cash prizes for the machine learning
community. The remaining disadvantage is that reading papers in MLJ sometimes
requires searching for the author's website where the free version is
available. In contrast, JMLR articles are freely available to everyone off the
JMLR website. Whether or not this disadvantage cancels the advantage is
debatable, but ess</p><p>2 0.099979371 <a title="172-tfidf-2" href="../hunch_net-2011/hunch_net-2011-03-20-KDD_Cup_2011.html">427 hunch net-2011-03-20-KDD Cup 2011</a></p>
<p>Introduction: Yehudapoints outKDD-Cup 2011whichMarkusandGideonhelped setup. This is a
prediction and recommendation contest for music. In addition to being a fun
chance to show your expertise, there are cash prizes of $5K/$2K/$1K.</p><p>3 0.098298483 <a title="172-tfidf-3" href="../hunch_net-2005/hunch_net-2005-06-28-The_cross_validation_problem%3A_cash_reward.html">86 hunch net-2005-06-28-The cross validation problem: cash reward</a></p>
<p>Introduction: I justpresentedthecross validationproblem atCOLT.The problem now has a cash
prize (up to $500) associated with it--see thepresentationfor details
.Thewrite-up for colt.</p><p>4 0.090171695 <a title="172-tfidf-4" href="../hunch_net-2009/hunch_net-2009-12-09-Future_Publication_Models_%40_NIPS.html">382 hunch net-2009-12-09-Future Publication Models @ NIPS</a></p>
<p>Introduction: Yesterday, there was a discussion aboutfuture publication models at
NIPS.YannandZoubinhave specific detailed proposals which I'll add links to
when I get them (Yann's proposalandZoubin's proposal).What struck me about the
discussion is that there are many simultaneous concerns as well as many
simultaneous proposals, which makes it difficult to keep all the distinctions
straight in a verbal conversation. It also seemed like people were serious
enough about this that we may see some real movement. Certainly, my personal
experience motivates that as I'veposted many timesabout the substantial flaws
in our review process, including some very poor personal experiences.Concerns
include the following:(Several) Reviewers are overloaded, boosting the noise
in decision making.(Yann) A new system should run with as little built-in
delay and friction to the process of research as possible.(Hanna
Wallach(updated)) Double-blind review is particularly important for people who
are unknown or from an un</p><p>5 0.087784924 <a title="172-tfidf-5" href="../hunch_net-2007/hunch_net-2007-11-02-The_Machine_Learning_Award_goes_to_%26%238230%3B.html">270 hunch net-2007-11-02-The Machine Learning Award goes to &#8230;</a></p>
<p>Introduction: Perhaps the biggest CS prize for research is theTuring Award, which has a
$0.25M cash prize associated with it. It appears none of the prizes so far
have been for anything like machine learning (the closest are perhaps database
awards).In CS theory, there is theGÃƒÂ¶del Prizewhich is smaller and newer,
offering a $5K prize along and perhaps (more importantly) recognition. One
such award has been given for Machine Learning, toRobert SchapireandYoav
Freundfor Adaboost.In Machine Learning, there seems to be no equivalent of
these sorts of prizes. There are several plausible reasons for this:There is
no coherent community.People drift in and out of the central conferences all
the time. Most of the author names from 10 years ago do not occur in the
conferences of today. In addition, the entire subject area is fairly new.There
are at least a core group of people who have stayed around.Machine Learning
work doesn't lastAlmost every paper is forgotten, because {the goals change,
there isn't an</p><p>6 0.08236403 <a title="172-tfidf-6" href="../hunch_net-2008/hunch_net-2008-02-10-Complexity_Illness.html">288 hunch net-2008-02-10-Complexity Illness</a></p>
<p>7 0.079191506 <a title="172-tfidf-7" href="../hunch_net-2005/hunch_net-2005-03-09-Bad_Reviewing.html">38 hunch net-2005-03-09-Bad Reviewing</a></p>
<p>8 0.071937039 <a title="172-tfidf-8" href="../hunch_net-2006/hunch_net-2006-01-06-MLTV.html">146 hunch net-2006-01-06-MLTV</a></p>
<p>9 0.069263294 <a title="172-tfidf-9" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>10 0.068438351 <a title="172-tfidf-10" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>11 0.05947201 <a title="172-tfidf-11" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>12 0.059131473 <a title="172-tfidf-12" href="../hunch_net-2006/hunch_net-2006-07-11-New_Models.html">194 hunch net-2006-07-11-New Models</a></p>
<p>13 0.056252893 <a title="172-tfidf-13" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>14 0.056183357 <a title="172-tfidf-14" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>15 0.055958822 <a title="172-tfidf-15" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>16 0.055098701 <a title="172-tfidf-16" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>17 0.053961836 <a title="172-tfidf-17" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>18 0.05259537 <a title="172-tfidf-18" href="../hunch_net-2010/hunch_net-2010-04-26-Compassionate_Reviewing.html">395 hunch net-2010-04-26-Compassionate Reviewing</a></p>
<p>19 0.051608682 <a title="172-tfidf-19" href="../hunch_net-2009/hunch_net-2009-07-09-The_Machine_Learning_Forum.html">363 hunch net-2009-07-09-The Machine Learning Forum</a></p>
<p>20 0.051392116 <a title="172-tfidf-20" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.118), (1, 0.067), (2, 0.013), (3, -0.022), (4, 0.015), (5, 0.001), (6, 0.034), (7, 0.019), (8, -0.016), (9, -0.037), (10, 0.008), (11, -0.013), (12, -0.001), (13, 0.026), (14, 0.015), (15, 0.025), (16, 0.038), (17, -0.026), (18, -0.064), (19, -0.027), (20, 0.01), (21, -0.05), (22, -0.005), (23, 0.021), (24, 0.011), (25, -0.034), (26, -0.017), (27, 0.01), (28, -0.04), (29, -0.053), (30, 0.012), (31, 0.036), (32, 0.082), (33, -0.021), (34, 0.015), (35, 0.021), (36, -0.002), (37, 0.026), (38, 0.042), (39, 0.087), (40, -0.025), (41, -0.078), (42, 0.034), (43, 0.002), (44, -0.01), (45, -0.101), (46, -0.028), (47, -0.011), (48, -0.055), (49, -0.038)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.90168828 <a title="172-lsi-1" href="../hunch_net-2006/hunch_net-2006-04-14-JMLR_is_a_success.html">172 hunch net-2006-04-14-JMLR is a success</a></p>
<p>Introduction: In 2001, the "Journal of Machine Learning Research" was created in reaction to
unadaptive publisher policies atMLJ. Essentially, with the creation of the
internet, the bottleneck in publishing research shifted from publishing to
research. Thedeclaration of independenceaccompanying this move expresses the
reasons why in greater detail.MLJ has strongly changed its policy in reaction
to this. In particular, there is no longer an assignment of copyright to the
publisher (*), and MLJ regularly sponsors many student "best paper awards"
across several conferences with cash prizes. This is an advantage of MLJ over
JMLR: MLJ can afford to sponsor cash prizes for the machine learning
community. The remaining disadvantage is that reading papers in MLJ sometimes
requires searching for the author's website where the free version is
available. In contrast, JMLR articles are freely available to everyone off the
JMLR website. Whether or not this disadvantage cancels the advantage is
debatable, but ess</p><p>2 0.63088888 <a title="172-lsi-2" href="../hunch_net-2007/hunch_net-2007-11-02-The_Machine_Learning_Award_goes_to_%26%238230%3B.html">270 hunch net-2007-11-02-The Machine Learning Award goes to &#8230;</a></p>
<p>Introduction: Perhaps the biggest CS prize for research is theTuring Award, which has a
$0.25M cash prize associated with it. It appears none of the prizes so far
have been for anything like machine learning (the closest are perhaps database
awards).In CS theory, there is theGÃƒÂ¶del Prizewhich is smaller and newer,
offering a $5K prize along and perhaps (more importantly) recognition. One
such award has been given for Machine Learning, toRobert SchapireandYoav
Freundfor Adaboost.In Machine Learning, there seems to be no equivalent of
these sorts of prizes. There are several plausible reasons for this:There is
no coherent community.People drift in and out of the central conferences all
the time. Most of the author names from 10 years ago do not occur in the
conferences of today. In addition, the entire subject area is fairly new.There
are at least a core group of people who have stayed around.Machine Learning
work doesn't lastAlmost every paper is forgotten, because {the goals change,
there isn't an</p><p>3 0.58096826 <a title="172-lsi-3" href="../hunch_net-2006/hunch_net-2006-09-18-What_is_missing_for_online_collaborative_research%3F.html">208 hunch net-2006-09-18-What is missing for online collaborative research?</a></p>
<p>Introduction: The internet has recently made the research process much smoother: papers are
easy to obtain, citations are easy to follow, and unpublished "tutorials" are
often available. Yet, new research fields can look very complicated to
outsiders or newcomers. Every paper is like a small piece of an unfinished
jigsaw puzzle: to understand just one publication, a researcher without
experience in the field will typically have to follow several layers of
citations, and many of the papers he encounters have a great deal of repeated
information. Furthermore, from one publication to the next, notation and
terminology may not be consistent which can further confuse the reader.But the
internet is now proving to be an extremely useful medium for collaboration and
knowledge aggregation. Online forums allow users to ask and answer questions
and to share ideas. The recent phenomenon of Wikipedia provides a proof-of-
concept for the "anyone can edit" system. Can such models be used to
facilitate research and</p><p>4 0.52380514 <a title="172-lsi-4" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>Introduction: The internet has significantly effected the way we do research but it's
capabilities have not yet been fully realized.First, let's acknowledge some
known effects.Self-publishingBy default, all researchers in machine learning
(and more generally computer science and physics) place their papers online
for anyone to download. The exact mechanism differs--physicists tend to use a
central repository (Arxiv) while computer scientists tend to place the papers
on their webpage. Arxiv has been slowly growing in subject breadth so it now
sometimes used by computer scientists.CollaborationEmail has enabled working
remotely with coauthors. This has allowed collaborationis which would not
otherwise have been possible and generally speeds research.Now, let's look at
attempts to go further.Blogs(like this one) allow public discussion about
topics which are not easily categorized as "a new idea in machine learning"
(like this topic).Organizationof some subfield of research. This
includesSatinder Singh</p><p>5 0.52368462 <a title="172-lsi-5" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>Introduction: One viewpoint on academia is that it is inherently adversarial: there are
finite research dollars, positions, and students to work with, implying a
zero-sum game between different participants. This is not a viewpoint that I
want to promote, as I consider it flawed. However, I know several people
believe strongly in this viewpoint, and I have found it to have substantial
explanatory power.For example:It explains why your paper was rejected based on
poor logic. The reviewer wasn't concerned with research quality, but rather
with rejecting a competitor.It explains why professors rarely work together.
The goal of a non-tenured professor (at least) is to get tenure, and a case
for tenure comes from a portfolio of work that is undisputably yours.It
explains why new research programs are not quickly adopted. Adopting a
competitor's program is impossible, if your career is based on the competitor
being wrong.Different academic groups subscribe to the adversarial viewpoint
in different degrees</p><p>6 0.50876397 <a title="172-lsi-6" href="../hunch_net-2007/hunch_net-2007-07-13-The_View_From_China.html">255 hunch net-2007-07-13-The View From China</a></p>
<p>7 0.48147115 <a title="172-lsi-7" href="../hunch_net-2007/hunch_net-2007-04-21-Videolectures.net.html">240 hunch net-2007-04-21-Videolectures.net</a></p>
<p>8 0.47988111 <a title="172-lsi-8" href="../hunch_net-2008/hunch_net-2008-07-02-Proprietary_Data_in_Academic_Research%3F.html">306 hunch net-2008-07-02-Proprietary Data in Academic Research?</a></p>
<p>9 0.47413352 <a title="172-lsi-9" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>10 0.47059429 <a title="172-lsi-10" href="../hunch_net-2005/hunch_net-2005-02-25-Why_Papers%3F.html">30 hunch net-2005-02-25-Why Papers?</a></p>
<p>11 0.46117067 <a title="172-lsi-11" href="../hunch_net-2011/hunch_net-2011-08-06-Interesting_thing_at_UAI_2011.html">440 hunch net-2011-08-06-Interesting thing at UAI 2011</a></p>
<p>12 0.45248476 <a title="172-lsi-12" href="../hunch_net-2009/hunch_net-2009-07-09-The_Machine_Learning_Forum.html">363 hunch net-2009-07-09-The Machine Learning Forum</a></p>
<p>13 0.44227672 <a title="172-lsi-13" href="../hunch_net-2005/hunch_net-2005-01-19-Why_I_decided_to_run_a_weblog..html">1 hunch net-2005-01-19-Why I decided to run a weblog.</a></p>
<p>14 0.44100904 <a title="172-lsi-14" href="../hunch_net-2010/hunch_net-2010-10-17-Partha_Niyogi_has_died.html">414 hunch net-2010-10-17-Partha Niyogi has died</a></p>
<p>15 0.43840614 <a title="172-lsi-15" href="../hunch_net-2006/hunch_net-2006-04-30-John_Langford_%26%238211%3B%3E_Yahoo_Research%2C_NY.html">175 hunch net-2006-04-30-John Langford &#8211;> Yahoo Research, NY</a></p>
<p>16 0.43626338 <a title="172-lsi-16" href="../hunch_net-2009/hunch_net-2009-12-09-Future_Publication_Models_%40_NIPS.html">382 hunch net-2009-12-09-Future Publication Models @ NIPS</a></p>
<p>17 0.43348211 <a title="172-lsi-17" href="../hunch_net-2006/hunch_net-2006-01-06-MLTV.html">146 hunch net-2006-01-06-MLTV</a></p>
<p>18 0.43345085 <a title="172-lsi-18" href="../hunch_net-2005/hunch_net-2005-03-09-Bad_Reviewing.html">38 hunch net-2005-03-09-Bad Reviewing</a></p>
<p>19 0.42897481 <a title="172-lsi-19" href="../hunch_net-2007/hunch_net-2007-02-16-The_Forgetting.html">233 hunch net-2007-02-16-The Forgetting</a></p>
<p>20 0.42509601 <a title="172-lsi-20" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">23 hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(6, 0.014), (22, 0.386), (35, 0.047), (42, 0.15), (61, 0.025), (68, 0.047), (73, 0.018), (74, 0.154), (88, 0.026), (95, 0.023)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.87395656 <a title="172-lda-1" href="../hunch_net-2006/hunch_net-2006-04-14-JMLR_is_a_success.html">172 hunch net-2006-04-14-JMLR is a success</a></p>
<p>Introduction: In 2001, the "Journal of Machine Learning Research" was created in reaction to
unadaptive publisher policies atMLJ. Essentially, with the creation of the
internet, the bottleneck in publishing research shifted from publishing to
research. Thedeclaration of independenceaccompanying this move expresses the
reasons why in greater detail.MLJ has strongly changed its policy in reaction
to this. In particular, there is no longer an assignment of copyright to the
publisher (*), and MLJ regularly sponsors many student "best paper awards"
across several conferences with cash prizes. This is an advantage of MLJ over
JMLR: MLJ can afford to sponsor cash prizes for the machine learning
community. The remaining disadvantage is that reading papers in MLJ sometimes
requires searching for the author's website where the free version is
available. In contrast, JMLR articles are freely available to everyone off the
JMLR website. Whether or not this disadvantage cancels the advantage is
debatable, but ess</p><p>2 0.80069524 <a title="172-lda-2" href="../hunch_net-2010/hunch_net-2010-09-28-Machined_Learnings.html">412 hunch net-2010-09-28-Machined Learnings</a></p>
<p>Introduction: Paul Mineirohas startedMachined Learningswhere he's seriously attempting to do
ML research in public. I personally need to read through in greater detail, as
much of it is learning reduction related, trying to deal with the sorts of
complex source problems that come up in practice.</p><p>3 0.49687165 <a title="172-lda-3" href="../hunch_net-2005/hunch_net-2005-03-09-Bad_Reviewing.html">38 hunch net-2005-03-09-Bad Reviewing</a></p>
<p>Introduction: This is a difficult subject to talk about for many reasons, but a discussion
may be helpful.Bad reviewing is a problem in academia. The first step in
understanding this is admitting to the problem, so here is a short list of
examples of bad reviewing.Reviewer disbelieves theorem proof (ICML), or
disbelieve theorem with a trivially false counterexample. (COLT)Reviewer
internally swaps quantifiers in a theorem, concludes it has been done before
and is trivial. (NIPS)Reviewer believes a technique will not work despite
experimental validation. (COLT)Reviewers fail to notice flaw in theorem
statement (CRYPTO).Reviewer erroneously claims that it has been done before
(NIPS, SODA, JMLR)--(complete with references!)Reviewer inverts the message of
a paper and concludes it says nothing important. (NIPS*2)Reviewer fails to
distinguish between a DAG and a tree (SODA).Reviewer is enthusiastic about
paper but clearly does not understand (ICML).Reviewer erroneously believe that
the "birthday paradox"</p><p>4 0.49261776 <a title="172-lda-4" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>Introduction: Unfortunately, I ended up sick for much of this ICML. I did manage to catch
one interesting paper:Richard Socher,Cliff Lin,Andrew Y. Ng, andChristopher D.
ManningParsing Natural Scenes and Natural Language with Recursive Neural
Networks.I invited Richard to share his list of interesting papers, so
hopefully we'll hear from him soon. In the meantime,PaulandHalhave posted some
lists.the futureJoelleand I are program chairs for ICML 2012 inEdinburgh,
which I previously enjoyed visiting in2005. This is a huge responsibility,
that we hope to accomplish well. A part of this (perhaps the most fun part),
is imagining how we can make ICML better. A key and critical constraint is
choosing things that can be accomplished. So far we have:Colocation. The first
thing we looked into was potential colocations. We quickly discovered that
many other conferences precomitted their location. For the future, getting a
colocation withACLorSIGIR, seems to require more advanced planning. If that
can be done, I</p><p>5 0.48866659 <a title="172-lda-5" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>Introduction: One of the confusing things about research is that progress is very hard to
measure. One of the consequences of being in a hard-to-measure environment is
that the wrong things are often measured.Lines of CodeThe classical example of
this phenomenon is the old lines-of-code-produced metric for programming. It
is easy to imagine systems for producing many lines of code with very little
work that accomplish very little.Paper countIn academia, a "paper count" is an
analog of "lines of code", and it suffers from the same failure modes. The
obvious failure mode here is that we end up with a large number of
uninteresting papers since people end up spending a lot of time optimizing
this metric.ComplexityAnother metric, is "complexity" (in the eye of a
reviewer) of a paper. There is a common temptation to make a method appear
more complex than it is in order for reviewers to judge it worthy of
publication. The failure mode here is unclean thinking. Simple effective
methods are often overlooked</p><p>6 0.48824531 <a title="172-lda-6" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>7 0.48433855 <a title="172-lda-7" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>8 0.48141691 <a title="172-lda-8" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>9 0.47902867 <a title="172-lda-9" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>10 0.47578755 <a title="172-lda-10" href="../hunch_net-2007/hunch_net-2007-01-26-Parallel_Machine_Learning_Problems.html">229 hunch net-2007-01-26-Parallel Machine Learning Problems</a></p>
<p>11 0.47484067 <a title="172-lda-11" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>12 0.47358865 <a title="172-lda-12" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>13 0.4733977 <a title="172-lda-13" href="../hunch_net-2006/hunch_net-2006-09-18-What_is_missing_for_online_collaborative_research%3F.html">208 hunch net-2006-09-18-What is missing for online collaborative research?</a></p>
<p>14 0.47256437 <a title="172-lda-14" href="../hunch_net-2006/hunch_net-2006-12-04-Structural_Problems_in_NIPS_Decision_Making.html">221 hunch net-2006-12-04-Structural Problems in NIPS Decision Making</a></p>
<p>15 0.47255948 <a title="172-lda-15" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>16 0.47202742 <a title="172-lda-16" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>17 0.47186235 <a title="172-lda-17" href="../hunch_net-2010/hunch_net-2010-08-23-Boosted_Decision_Trees_for_Deep_Learning.html">407 hunch net-2010-08-23-Boosted Decision Trees for Deep Learning</a></p>
<p>18 0.4717865 <a title="172-lda-18" href="../hunch_net-2010/hunch_net-2010-04-24-COLT_Treasurer_is_now_Phil_Long.html">394 hunch net-2010-04-24-COLT Treasurer is now Phil Long</a></p>
<p>19 0.47101954 <a title="172-lda-19" href="../hunch_net-2008/hunch_net-2008-09-03-Bidding_Problems.html">315 hunch net-2008-09-03-Bidding Problems</a></p>
<p>20 0.47074246 <a title="172-lda-20" href="../hunch_net-2005/hunch_net-2005-02-17-Learning_Research_Programs.html">21 hunch net-2005-02-17-Learning Research Programs</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
