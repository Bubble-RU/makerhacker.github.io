<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>172 hunch net-2006-04-14-JMLR is a success</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-172" href="#">hunch_net-2006-172</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>172 hunch net-2006-04-14-JMLR is a success</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-172-html" href="http://hunch.net/?p=183">html</a></p><p>Introduction: In 2001, the “ Journal of Machine Learning Research ” was created in reaction to unadaptive  publisher policies at  MLJ .  Essentially, with the creation of the internet, the bottleneck in publishing research shifted from publishing to research.  The  declaration of independence  accompanying this move expresses the reasons why in greater detail.
 
MLJ has strongly changed its policy in reaction to this.  In particular, there is no longer an assignment of copyright to the publisher (*), and MLJ regularly sponsors many student “best paper awards” across several conferences with cash prizes.  This is an advantage of MLJ over JMLR: MLJ can afford to sponsor cash prizes for the machine learning community.  The remaining disadvantage is that reading papers in MLJ sometimes requires searching for the author’s website where the free version is available.  In contrast, JMLR articles are freely available to everyone off the JMLR website.  Whether or not this disadvantage cancels the advantage i</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In 2001, the “ Journal of Machine Learning Research ” was created in reaction to unadaptive  publisher policies at  MLJ . [sent-1, score-0.438]
</p><p>2 Essentially, with the creation of the internet, the bottleneck in publishing research shifted from publishing to research. [sent-2, score-0.542]
</p><p>3 The  declaration of independence  accompanying this move expresses the reasons why in greater detail. [sent-3, score-0.307]
</p><p>4 MLJ has strongly changed its policy in reaction to this. [sent-4, score-0.186]
</p><p>5 In particular, there is no longer an assignment of copyright to the publisher (*), and MLJ regularly sponsors many student “best paper awards” across several conferences with cash prizes. [sent-5, score-0.673]
</p><p>6 This is an advantage of MLJ over JMLR: MLJ can afford to sponsor cash prizes for the machine learning community. [sent-6, score-0.5]
</p><p>7 The remaining disadvantage is that reading papers in MLJ sometimes requires searching for the author’s website where the free version is available. [sent-7, score-0.387]
</p><p>8 In contrast, JMLR articles are freely available to everyone off the JMLR website. [sent-8, score-0.159]
</p><p>9 Whether or not this disadvantage cancels the advantage is debatable, but essentially no one working on machine learning argues with the following: the changes brought by the creation of JMLR have been positive for the general machine learning community. [sent-9, score-0.551]
</p><p>10 This model can and should be emulated in other areas of research where publishers are not behaving in a sufficiently constructive manner. [sent-10, score-0.222]
</p><p>11 Doing so requires two vital ingredients: a consensus of leaders to support a new journal and the willigness to spend the time and effort setting it up. [sent-11, score-0.535]
</p><p>12 Presumably, some lessons on how to do this have been learned by the editors of JMLR and they are willing to share it. [sent-12, score-0.238]
</p><p>13 (*) Back in the day, it was typical to be forced to sign over all rights to your journal paper, then ignore this and place it on your homepage. [sent-13, score-0.476]
</p><p>14 The natural act of placing your paper on your webpage is no longer illegal. [sent-14, score-0.392]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mlj', 0.559), ('jmlr', 0.389), ('journal', 0.198), ('reaction', 0.186), ('publisher', 0.186), ('creation', 0.163), ('cash', 0.155), ('disadvantage', 0.136), ('publishing', 0.114), ('longer', 0.114), ('advantage', 0.095), ('requires', 0.095), ('expresses', 0.093), ('behaving', 0.093), ('editors', 0.093), ('accompanying', 0.086), ('brought', 0.086), ('leaders', 0.086), ('rights', 0.086), ('afford', 0.086), ('sponsor', 0.086), ('regularly', 0.081), ('articles', 0.081), ('ingredients', 0.081), ('bottleneck', 0.081), ('awards', 0.081), ('consensus', 0.081), ('lessons', 0.081), ('searching', 0.078), ('remaining', 0.078), ('prizes', 0.078), ('freely', 0.078), ('vital', 0.075), ('placing', 0.075), ('essentially', 0.071), ('shifted', 0.07), ('assignment', 0.07), ('act', 0.068), ('presumably', 0.068), ('webpage', 0.068), ('paper', 0.067), ('constructive', 0.066), ('policies', 0.066), ('ignore', 0.064), ('forced', 0.064), ('independence', 0.064), ('willing', 0.064), ('move', 0.064), ('sign', 0.064), ('sufficiently', 0.063)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999976 <a title="172-tfidf-1" href="../hunch_net-2006/hunch_net-2006-04-14-JMLR_is_a_success.html">172 hunch net-2006-04-14-JMLR is a success</a></p>
<p>Introduction: In 2001, the “ Journal of Machine Learning Research ” was created in reaction to unadaptive  publisher policies at  MLJ .  Essentially, with the creation of the internet, the bottleneck in publishing research shifted from publishing to research.  The  declaration of independence  accompanying this move expresses the reasons why in greater detail.
 
MLJ has strongly changed its policy in reaction to this.  In particular, there is no longer an assignment of copyright to the publisher (*), and MLJ regularly sponsors many student “best paper awards” across several conferences with cash prizes.  This is an advantage of MLJ over JMLR: MLJ can afford to sponsor cash prizes for the machine learning community.  The remaining disadvantage is that reading papers in MLJ sometimes requires searching for the author’s website where the free version is available.  In contrast, JMLR articles are freely available to everyone off the JMLR website.  Whether or not this disadvantage cancels the advantage i</p><p>2 0.11815628 <a title="172-tfidf-2" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>Introduction: Unfortunately, I ended up sick for much of this ICML.  I did manage to catch one interesting paper:
 
 Richard Socher ,  Cliff Lin ,  Andrew Y. Ng , and  Christopher D. Manning   Parsing Natural Scenes and Natural Language with Recursive Neural Networks .
 
I invited Richard to share his list of interesting papers, so hopefully we’ll hear from him soon.  In the meantime,  Paul  and  Hal  have posted some lists.
 
 the future 
 
 Joelle  and I are program chairs for ICML 2012 in  Edinburgh , which I previously enjoyed visiting in  2005 .  This is a huge responsibility, that we hope to accomplish well.  A part of this (perhaps the most fun part), is imagining how we can make ICML better.  A key and critical constraint is choosing things that can be accomplished.  So far we have:
  
  Colocation . The first thing we looked into was potential colocations.  We quickly discovered that many other conferences precomitted their location.  For the future, getting a colocation with  ACL  or  SIGI</p><p>3 0.080775939 <a title="172-tfidf-3" href="../hunch_net-2007/hunch_net-2007-11-02-The_Machine_Learning_Award_goes_to_%26%238230%3B.html">270 hunch net-2007-11-02-The Machine Learning Award goes to &#8230;</a></p>
<p>Introduction: Perhaps the biggest CS prize for research is the  Turing Award , which has a $0.25M cash prize associated with it.  It appears none of the prizes so far have been for anything like machine learning (the closest are perhaps database awards).
 
In CS theory, there is the  GÃƒÂ¶del Prize  which is smaller and newer, offering a $5K prize along and perhaps (more importantly) recognition.  One such award has been given for Machine Learning, to  Robert Schapire  and  Yoav Freund  for Adaboost.
 
In Machine Learning, there seems to be no equivalent of these sorts of prizes.  There are several plausible reasons for this:
  
 
 There is no coherent community. 
  People drift in and out of the central conferences all the time.  Most of the author names from 10 years ago do not occur in the conferences of today.  In addition, the entire subject area is fairly new. 
 There are at least a core group of people who have stayed around. 
 
 
 Machine Learning work doesn’t last 
 Almost every paper is fo</p><p>4 0.078303352 <a title="172-tfidf-4" href="../hunch_net-2005/hunch_net-2005-02-04-JMLG.html">13 hunch net-2005-02-04-JMLG</a></p>
<p>Introduction: The  Journal of Machine Learning Gossip  has some fine satire about learning research.  In particular, the  guides  are amusing and remarkably true.
 
As in all things, itâ&euro;&trade;s easy to criticize the way things are and harder to make them better.</p><p>5 0.072279662 <a title="172-tfidf-5" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>Introduction: Essentially everyone who writes research papers suffers rejections.  They always sting immediately, but upon further reflection many of these rejections come to seem reasonable.  Maybe the equations had too many typos or maybe the topic just isn’t as important as was originally thought.  A few rejections do not come to seem acceptable, and these form the basis of reviewing horror stories, a great material for conversations.  I’ve decided to share three of mine, now all safely a bit distant in the past.
  
  Prediction Theory for Classification Tutorial .  This is a tutorial about tight sample complexity bounds for classification that I submitted to  JMLR .  The first decision I heard was a reject which appeared quite unjust to me—for example one of the reviewers appeared to claim that all the content was in standard statistics books.  Upon further inquiry, several citations were given, none of which actually covered the content.  Later, I was shocked to hear the paper was accepted. App</p><p>6 0.072048739 <a title="172-tfidf-6" href="../hunch_net-2011/hunch_net-2011-03-20-KDD_Cup_2011.html">427 hunch net-2011-03-20-KDD Cup 2011</a></p>
<p>7 0.070720099 <a title="172-tfidf-7" href="../hunch_net-2009/hunch_net-2009-12-09-Future_Publication_Models_%40_NIPS.html">382 hunch net-2009-12-09-Future Publication Models @ NIPS</a></p>
<p>8 0.067578554 <a title="172-tfidf-8" href="../hunch_net-2008/hunch_net-2008-02-10-Complexity_Illness.html">288 hunch net-2008-02-10-Complexity Illness</a></p>
<p>9 0.066089392 <a title="172-tfidf-9" href="../hunch_net-2005/hunch_net-2005-06-28-The_cross_validation_problem%3A_cash_reward.html">86 hunch net-2005-06-28-The cross validation problem: cash reward</a></p>
<p>10 0.06363868 <a title="172-tfidf-10" href="../hunch_net-2006/hunch_net-2006-01-06-MLTV.html">146 hunch net-2006-01-06-MLTV</a></p>
<p>11 0.063013762 <a title="172-tfidf-11" href="../hunch_net-2005/hunch_net-2005-03-09-Bad_Reviewing.html">38 hunch net-2005-03-09-Bad Reviewing</a></p>
<p>12 0.062344197 <a title="172-tfidf-12" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>13 0.052768275 <a title="172-tfidf-13" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>14 0.052351311 <a title="172-tfidf-14" href="../hunch_net-2006/hunch_net-2006-07-11-New_Models.html">194 hunch net-2006-07-11-New Models</a></p>
<p>15 0.051887043 <a title="172-tfidf-15" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>16 0.049893826 <a title="172-tfidf-16" href="../hunch_net-2010/hunch_net-2010-04-26-Compassionate_Reviewing.html">395 hunch net-2010-04-26-Compassionate Reviewing</a></p>
<p>17 0.049440842 <a title="172-tfidf-17" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>18 0.047482625 <a title="172-tfidf-18" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>19 0.045764089 <a title="172-tfidf-19" href="../hunch_net-2009/hunch_net-2009-02-22-Effective_Research_Funding.html">344 hunch net-2009-02-22-Effective Research Funding</a></p>
<p>20 0.045691825 <a title="172-tfidf-20" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.108), (1, -0.05), (2, 0.0), (3, 0.046), (4, -0.028), (5, 0.002), (6, 0.014), (7, 0.002), (8, -0.025), (9, 0.023), (10, 0.017), (11, 0.016), (12, 0.017), (13, 0.016), (14, 0.012), (15, 0.009), (16, 0.018), (17, 0.019), (18, 0.022), (19, -0.003), (20, 0.002), (21, 0.012), (22, 0.006), (23, -0.032), (24, 0.012), (25, 0.058), (26, 0.008), (27, -0.001), (28, -0.035), (29, 0.009), (30, 0.006), (31, 0.02), (32, 0.022), (33, -0.025), (34, 0.027), (35, 0.037), (36, -0.032), (37, 0.016), (38, 0.051), (39, -0.013), (40, -0.057), (41, -0.032), (42, -0.026), (43, 0.046), (44, 0.029), (45, -0.057), (46, 0.052), (47, -0.088), (48, 0.057), (49, 0.007)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.90348613 <a title="172-lsi-1" href="../hunch_net-2006/hunch_net-2006-04-14-JMLR_is_a_success.html">172 hunch net-2006-04-14-JMLR is a success</a></p>
<p>Introduction: In 2001, the “ Journal of Machine Learning Research ” was created in reaction to unadaptive  publisher policies at  MLJ .  Essentially, with the creation of the internet, the bottleneck in publishing research shifted from publishing to research.  The  declaration of independence  accompanying this move expresses the reasons why in greater detail.
 
MLJ has strongly changed its policy in reaction to this.  In particular, there is no longer an assignment of copyright to the publisher (*), and MLJ regularly sponsors many student “best paper awards” across several conferences with cash prizes.  This is an advantage of MLJ over JMLR: MLJ can afford to sponsor cash prizes for the machine learning community.  The remaining disadvantage is that reading papers in MLJ sometimes requires searching for the author’s website where the free version is available.  In contrast, JMLR articles are freely available to everyone off the JMLR website.  Whether or not this disadvantage cancels the advantage i</p><p>2 0.56170368 <a title="172-lsi-2" href="../hunch_net-2007/hunch_net-2007-04-21-Videolectures.net.html">240 hunch net-2007-04-21-Videolectures.net</a></p>
<p>Introduction: Davor  has been working to setup  videolectures.net  which is the new site for the many lectures  mentioned here .  (Tragically, they seem to only be available in windows media format.) I went through  my own projects  and added a few links to the videos.  The day when every result is a set of {paper, slides, video} isn’t quite here yet, but it’s within sight.  (For many papers, of course, code is a 4th component.)</p><p>3 0.54763806 <a title="172-lsi-3" href="../hunch_net-2009/hunch_net-2009-12-09-Future_Publication_Models_%40_NIPS.html">382 hunch net-2009-12-09-Future Publication Models @ NIPS</a></p>
<p>Introduction: Yesterday, there was a discussion about  future publication models at NIPS .   Yann  and  Zoubin  have specific detailed proposals which I’ll add links to when I get them ( Yann’s proposal  and  Zoubin’s proposal ).
 
What struck me about the discussion is that there are many simultaneous concerns as well as many simultaneous proposals, which makes it difficult to keep all the distinctions straight in a verbal conversation.  It also seemed like people were serious enough about this that we may see some real movement.  Certainly, my personal experience motivates that as I’ve  posted many times  about the substantial flaws in our review process, including some very poor personal experiences.
 
Concerns include the following:
  
 (Several) Reviewers are overloaded, boosting the noise in decision making. 
 ( Yann ) A new system should run with as little built-in delay and friction to the process of research as possible. 
 ( Hanna Wallach (updated)) Double-blind review is particularly impor</p><p>4 0.52006835 <a title="172-lsi-4" href="../hunch_net-2009/hunch_net-2009-07-09-The_Machine_Learning_Forum.html">363 hunch net-2009-07-09-The Machine Learning Forum</a></p>
<p>Introduction: Dear Fellow Machine Learners,
 
For the past year or so I have become increasingly frustrated with the peer review system in our field. I constantly get asked to review papers in which I have no interest. At the same time, as an action editor in JMLR, I constantly have to harass people to review papers. When I send papers to conferences and to journals I often get rejected with reviews that, at least in my mind, make no sense. Finally, I have a very hard time keeping up with the best new work, because I don’t know where to look for it…
 
I decided to try an do something to improve the situation. I started a new web site, which I decided to call “The machine learning forum” the URL is  http://themachinelearningforum.org 
 
The main idea behind this web site is to remove anonymity from the review process. In this site, all opinions are attributed to the actual person that expressed them. I expect that this will improve the quality of the reviews. An obvious other effect is that there wil</p><p>5 0.51504976 <a title="172-lsi-5" href="../hunch_net-2007/hunch_net-2007-11-02-The_Machine_Learning_Award_goes_to_%26%238230%3B.html">270 hunch net-2007-11-02-The Machine Learning Award goes to &#8230;</a></p>
<p>Introduction: Perhaps the biggest CS prize for research is the  Turing Award , which has a $0.25M cash prize associated with it.  It appears none of the prizes so far have been for anything like machine learning (the closest are perhaps database awards).
 
In CS theory, there is the  GÃƒÂ¶del Prize  which is smaller and newer, offering a $5K prize along and perhaps (more importantly) recognition.  One such award has been given for Machine Learning, to  Robert Schapire  and  Yoav Freund  for Adaboost.
 
In Machine Learning, there seems to be no equivalent of these sorts of prizes.  There are several plausible reasons for this:
  
 
 There is no coherent community. 
  People drift in and out of the central conferences all the time.  Most of the author names from 10 years ago do not occur in the conferences of today.  In addition, the entire subject area is fairly new. 
 There are at least a core group of people who have stayed around. 
 
 
 Machine Learning work doesn’t last 
 Almost every paper is fo</p><p>6 0.51478589 <a title="172-lsi-6" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>7 0.49417096 <a title="172-lsi-7" href="../hunch_net-2006/hunch_net-2006-01-06-MLTV.html">146 hunch net-2006-01-06-MLTV</a></p>
<p>8 0.48873192 <a title="172-lsi-8" href="../hunch_net-2006/hunch_net-2006-09-18-What_is_missing_for_online_collaborative_research%3F.html">208 hunch net-2006-09-18-What is missing for online collaborative research?</a></p>
<p>9 0.48035187 <a title="172-lsi-9" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>10 0.47654617 <a title="172-lsi-10" href="../hunch_net-2005/hunch_net-2005-01-19-Why_I_decided_to_run_a_weblog..html">1 hunch net-2005-01-19-Why I decided to run a weblog.</a></p>
<p>11 0.46073031 <a title="172-lsi-11" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>12 0.4536103 <a title="172-lsi-12" href="../hunch_net-2013/hunch_net-2013-06-29-The_Benefits_of_Double-Blind_Review.html">485 hunch net-2013-06-29-The Benefits of Double-Blind Review</a></p>
<p>13 0.44605318 <a title="172-lsi-13" href="../hunch_net-2013/hunch_net-2013-06-10-The_Large_Scale_Learning_class_notes.html">483 hunch net-2013-06-10-The Large Scale Learning class notes</a></p>
<p>14 0.44303918 <a title="172-lsi-14" href="../hunch_net-2005/hunch_net-2005-02-02-Kolmogorov_Complexity_and_Googling.html">10 hunch net-2005-02-02-Kolmogorov Complexity and Googling</a></p>
<p>15 0.43906194 <a title="172-lsi-15" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>16 0.43706819 <a title="172-lsi-16" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>17 0.4361783 <a title="172-lsi-17" href="../hunch_net-2010/hunch_net-2010-10-17-Partha_Niyogi_has_died.html">414 hunch net-2010-10-17-Partha Niyogi has died</a></p>
<p>18 0.43428561 <a title="172-lsi-18" href="../hunch_net-2005/hunch_net-2005-09-05-Site_Update.html">107 hunch net-2005-09-05-Site Update</a></p>
<p>19 0.43402442 <a title="172-lsi-19" href="../hunch_net-2012/hunch_net-2012-05-02-ICML%3A_Behind_the_Scenes.html">463 hunch net-2012-05-02-ICML: Behind the Scenes</a></p>
<p>20 0.43034658 <a title="172-lsi-20" href="../hunch_net-2007/hunch_net-2007-10-24-Contextual_Bandits.html">269 hunch net-2007-10-24-Contextual Bandits</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(27, 0.724), (55, 0.054), (80, 0.04), (94, 0.022), (95, 0.042)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99940085 <a title="172-lda-1" href="../hunch_net-2006/hunch_net-2006-04-14-JMLR_is_a_success.html">172 hunch net-2006-04-14-JMLR is a success</a></p>
<p>Introduction: In 2001, the “ Journal of Machine Learning Research ” was created in reaction to unadaptive  publisher policies at  MLJ .  Essentially, with the creation of the internet, the bottleneck in publishing research shifted from publishing to research.  The  declaration of independence  accompanying this move expresses the reasons why in greater detail.
 
MLJ has strongly changed its policy in reaction to this.  In particular, there is no longer an assignment of copyright to the publisher (*), and MLJ regularly sponsors many student “best paper awards” across several conferences with cash prizes.  This is an advantage of MLJ over JMLR: MLJ can afford to sponsor cash prizes for the machine learning community.  The remaining disadvantage is that reading papers in MLJ sometimes requires searching for the author’s website where the free version is available.  In contrast, JMLR articles are freely available to everyone off the JMLR website.  Whether or not this disadvantage cancels the advantage i</p><p>2 0.99713224 <a title="172-lda-2" href="../hunch_net-2007/hunch_net-2007-06-14-Interesting_Papers_at_COLT_2007.html">247 hunch net-2007-06-14-Interesting Papers at COLT 2007</a></p>
<p>Introduction: Here are two papers that seem particularly interesting at this year’s COLT.
  
  Gilles Blanchard  and  FranÃƒÂ§ois Fleuret ,  Occam’s Hammer .  When we are interested in very tight bounds on the true error rate of a classifier, it is tempting to use a PAC-Bayes bound which can (empirically) be  quite tight .  A disadvantage of the PAC-Bayes bound is that it applies to a classifier which is randomized over a set of base classifiers rather than a single classifier.  This paper shows that a similar bound can be proved which holds for a single classifier drawn from the set.   The ability to safely use a single classifier is very nice.  This technique applies generically to any base bound, so it has other applications covered in the paper. 
  Adam Tauman Kalai .  Learning Nested Halfspaces and Uphill Decision Trees .  Classification PAC-learning, where you prove that any problem amongst some set is polytime learnable with respect to any distribution over the input  X  is extraordinarily ch</p><p>3 0.99562299 <a title="172-lda-3" href="../hunch_net-2007/hunch_net-2007-11-28-Computational_Consequences_of_Classification.html">274 hunch net-2007-11-28-Computational Consequences of Classification</a></p>
<p>Introduction: In the  regression vs classification debate , I’m adding a new “pro” to classification.  It seems there are computational shortcuts available for classification which simply aren’t available for regression.  This arises in several situations.
  
 In  active learning  it is sometimes possible to find an  e  error classifier with just  log(e)  labeled samples.    Only much more modest improvements appear to be achievable for squared loss regression.  The essential reason is that the loss function on many examples is flat with respect to large variations in the parameter spaces of a learned classifier, which implies that many of these classifiers do not need to be considered.  In contrast, for squared loss regression, most substantial variations in the parameter space influence the loss at most points. 
 In budgeted learning, where there is either a computational time constraint or a feature cost constraint, a classifier can sometimes be learned to very high accuracy under the constraints</p><p>4 0.99511331 <a title="172-lda-4" href="../hunch_net-2008/hunch_net-2008-02-10-Complexity_Illness.html">288 hunch net-2008-02-10-Complexity Illness</a></p>
<p>Introduction: One of the enduring stereotypes of academia is that people spend a great deal of intelligence, time, and effort finding complexity rather than simplicity.  This is at least anecdotally true in my experience.
  
  Math++  Several people have found that adding useless math makes their paper more publishable as evidenced by a reject-add-accept sequence. 
  8 page minimum  Who submitted a paper to  ICML  violating the 8 page minimum?  Every author fears that the reviewers won’t take their work seriously unless the allowed length is fully used.  The best minimum violation I know is  Adam ‘s paper at SODA on  generating random factored numbers , but this is deeply exceptional.  It’s a fair bet that 90% of papers submitted are exactly at the page limit.  We could imagine that this is because papers naturally take more space, but few people seem to be clamoring for more space. 
  Journalong   Has anyone been asked to review a 100 page journal paper?  I have.  Journal papers can be nice, becaus</p><p>5 0.9938103 <a title="172-lda-5" href="../hunch_net-2008/hunch_net-2008-07-06-To_Dual_or_Not.html">308 hunch net-2008-07-06-To Dual or Not</a></p>
<p>Introduction: Yoram  and  Shai ‘s  online learning tutorial  at  ICML  brings up a question for me, “Why use the  dual ?”
 
The basic setting is learning a weight vector  w i   so that the function  f(x)= sum i  w i  x i   optimizes some convex loss function.
 
The functional view of the dual is that instead of (or in addition to) keeping track of  w i   over the feature space, you keep track of a vector  a j   over the examples and define  w i  = sum j  a j  x ji  .
 
The above view of duality makes operating in the dual appear unnecessary, because in the end a weight vector is always used.  The tutorial suggests that thinking about the dual gives a unified algorithmic font for deriving online learning algorithms.  I haven’t worked with the dual representation much myself, but I have seen a few examples where it appears helpful.
  
  Noise  When doing online optimization (i.e. online learning where you are allowed to look at individual examples multiple times), the dual representation may be helpfu</p><p>6 0.99361807 <a title="172-lda-6" href="../hunch_net-2006/hunch_net-2006-03-24-NLPers.html">166 hunch net-2006-03-24-NLPers</a></p>
<p>7 0.99361807 <a title="172-lda-7" href="../hunch_net-2007/hunch_net-2007-06-13-Not_Posting.html">246 hunch net-2007-06-13-Not Posting</a></p>
<p>8 0.99361807 <a title="172-lda-8" href="../hunch_net-2010/hunch_net-2010-12-02-Traffic_Prediction_Problem.html">418 hunch net-2010-12-02-Traffic Prediction Problem</a></p>
<p>9 0.99288279 <a title="172-lda-9" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>10 0.99206793 <a title="172-lda-10" href="../hunch_net-2007/hunch_net-2007-05-12-Loss_Function_Semantics.html">245 hunch net-2007-05-12-Loss Function Semantics</a></p>
<p>11 0.99090606 <a title="172-lda-11" href="../hunch_net-2005/hunch_net-2005-03-22-Active_learning.html">45 hunch net-2005-03-22-Active learning</a></p>
<p>12 0.97953171 <a title="172-lda-12" href="../hunch_net-2005/hunch_net-2005-02-01-Watchword%3A_Loss.html">9 hunch net-2005-02-01-Watchword: Loss</a></p>
<p>13 0.97865605 <a title="172-lda-13" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<p>14 0.97700256 <a title="172-lda-14" href="../hunch_net-2009/hunch_net-2009-02-04-Optimal_Proxy_Loss_for_Classification.html">341 hunch net-2009-02-04-Optimal Proxy Loss for Classification</a></p>
<p>15 0.97664309 <a title="172-lda-15" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>16 0.96813178 <a title="172-lda-16" href="../hunch_net-2006/hunch_net-2006-07-13-Regression_vs._Classification_as_a_Primitive.html">196 hunch net-2006-07-13-Regression vs. Classification as a Primitive</a></p>
<p>17 0.95502895 <a title="172-lda-17" href="../hunch_net-2013/hunch_net-2013-06-10-The_Large_Scale_Learning_class_notes.html">483 hunch net-2013-06-10-The Large Scale Learning class notes</a></p>
<p>18 0.9510603 <a title="172-lda-18" href="../hunch_net-2008/hunch_net-2008-03-23-Interactive_Machine_Learning.html">293 hunch net-2008-03-23-Interactive Machine Learning</a></p>
<p>19 0.94897586 <a title="172-lda-19" href="../hunch_net-2007/hunch_net-2007-05-09-The_Missing_Bound.html">244 hunch net-2007-05-09-The Missing Bound</a></p>
<p>20 0.94043607 <a title="172-lda-20" href="../hunch_net-2005/hunch_net-2005-02-01-NIPS%3A_Online_Bayes.html">8 hunch net-2005-02-01-NIPS: Online Bayes</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
