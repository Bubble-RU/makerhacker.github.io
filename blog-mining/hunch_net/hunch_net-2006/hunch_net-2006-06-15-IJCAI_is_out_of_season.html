<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>184 hunch net-2006-06-15-IJCAI is out of season</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="../home/hunch_net-2006_home.html">hunch_net-2006</a> <a title="hunch_net-2006-184" href="#">hunch_net-2006-184</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>184 hunch net-2006-06-15-IJCAI is out of season</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2006-184-html" href="http://hunch.net/?p=196">html</a></p><p>Introduction: IJCAIis running January 6-12 in Hyderabad India rather than a more traditional
summer date. (Presumably, this is to avoid melting people in the Indian
summer.)The paper deadline(June 23 abstract / June 30 submission) are
particularly inconvenient if you attendCOLTorICML. But on the other hand, it's
a good excuse to visit India.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 IJCAIis running January 6-12 in Hyderabad India rather than a more traditional summer date. [sent-1, score-0.581]
</p><p>2 (Presumably, this is to avoid melting people in the Indian summer. [sent-2, score-0.179]
</p><p>3 )The paper deadline(June 23 abstract / June 30 submission) are particularly inconvenient if you attendCOLTorICML. [sent-3, score-0.349]
</p><p>4 But on the other hand, it's a good excuse to visit India. [sent-4, score-0.542]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('india', 0.509), ('june', 0.45), ('excuse', 0.27), ('indian', 0.27), ('visit', 0.225), ('january', 0.218), ('presumably', 0.212), ('submission', 0.201), ('traditional', 0.192), ('abstract', 0.178), ('deadline', 0.162), ('summer', 0.158), ('hand', 0.154), ('running', 0.148), ('avoid', 0.132), ('particularly', 0.094), ('rather', 0.083), ('paper', 0.077), ('good', 0.047), ('people', 0.047)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="184-tfidf-1" href="../hunch_net-2006/hunch_net-2006-06-15-IJCAI_is_out_of_season.html">184 hunch net-2006-06-15-IJCAI is out of season</a></p>
<p>Introduction: IJCAIis running January 6-12 in Hyderabad India rather than a more traditional
summer date. (Presumably, this is to avoid melting people in the Indian
summer.)The paper deadline(June 23 abstract / June 30 submission) are
particularly inconvenient if you attendCOLTorICML. But on the other hand, it's
a good excuse to visit India.</p><p>2 0.18792538 <a title="184-tfidf-2" href="../hunch_net-2008/hunch_net-2008-11-11-COLT_CFP.html">326 hunch net-2008-11-11-COLT CFP</a></p>
<p>Introduction: Adam Klivans, points out theCOLT call for papers. The important points are:Due
Feb 13.Montreal, June 18-21.This year, there is author feedback.</p><p>3 0.16483083 <a title="184-tfidf-3" href="../hunch_net-2009/hunch_net-2009-05-30-Many_ways_to_Learn_this_summer.html">357 hunch net-2009-05-30-Many ways to Learn this summer</a></p>
<p>Introduction: There are at least3summer schools related to machine learning this summer.The
firstis atUniversity of ChicagoJune 1-11 organized byMisha Belkin,Partha
Niyogi, andSteve Smale. Registration is closed for this one, meaning they met
their capacity limit. The format is essentially an extended Tutorial/Workshop.
I was particularly interested to seeValiantamongst the speakers. I'm also
presenting Saturday June 6, on logarithmic time prediction.Praveen
Srinivasanpoints out the second atPeking Universityin Beijing, China, July
20-27.This onediffers substantially, as it is about vision, machine learning,
and their intersection. The deadline for applications is June 10 or 15. This
is also another example of the growth of research in China, with active
support fromNSF.The third one is atCambridge, England, August 29-September 10.
It's in theMLSS series. Compared to the Chicago one, this one is more about
the Bayesian side of ML, although effort has been made to create a good cross
section of topic</p><p>4 0.12181559 <a title="184-tfidf-4" href="../hunch_net-2010/hunch_net-2010-01-19-Deadline_Season%2C_2010.html">387 hunch net-2010-01-19-Deadline Season, 2010</a></p>
<p>Introduction: Many conference deadlines are coming soon.DeadlineDouble Blind / Author
FeedbackTime/PlaceICMLJanuary 18((workshops) / February 1 (Papers) / February
13 (Tutorials)Y/YHaifa, Israel, June 21-25KDDFebruary 1(Workshops) / February
2&5 (Papers) / February 26 (Tutorials & Panels)) / April 17
(Demos)N/SWashington DC, July 25-28COLTJanuary 18 (Workshops) / February 19
(Papers)N/SHaifa, Israel, June 25-29UAIMarch 11 (Papers)N?/YCatalina Island,
California, July 8-11ICML continues to experiment with the reviewing process,
although perhaps less so than last year.The S "sort-of" for COLT is because
author feedback occurs only after decisions are made.KDD is notable for being
the most comprehensive in terms of {Tutorials, Workshops, Challenges, Panels,
Papers (two tracks), Demos}. The S for KDD is because there is sometimes
author feedback at the decision of the SPC.The (past) January 18 deadline for
workshops at ICML is nominal, as I (as workshop chair) almost missed it myself
and we have space f</p><p>5 0.12150655 <a title="184-tfidf-5" href="../hunch_net-2005/hunch_net-2005-05-14-NIPS.html">71 hunch net-2005-05-14-NIPS</a></p>
<p>Introduction: NIPSis the big winter conference of learning.Paper due date: June 3rd.
(Tweaked thanks toFei Sha.)Location: Vancouver (main program) Dec. 5-8 and
Whistler (workshops) Dec 9-10, BC, CanadaNIPS is larger than all of the other
learning conferences, partly because it's the only one at that time of year. I
recommend the workshops which are often quite interesting and energetic.</p><p>6 0.11733191 <a title="184-tfidf-6" href="../hunch_net-2012/hunch_net-2012-08-27-NYAS_ML_2012_and_ICML_2013.html">472 hunch net-2012-08-27-NYAS ML 2012 and ICML 2013</a></p>
<p>7 0.11295678 <a title="184-tfidf-7" href="../hunch_net-2008/hunch_net-2008-01-07-2008_Summer_Machine_Learning_Conference_Schedule.html">283 hunch net-2008-01-07-2008 Summer Machine Learning Conference Schedule</a></p>
<p>8 0.10133587 <a title="184-tfidf-8" href="../hunch_net-2005/hunch_net-2005-01-26-Summer_Schools.html">4 hunch net-2005-01-26-Summer Schools</a></p>
<p>9 0.088908523 <a title="184-tfidf-9" href="../hunch_net-2005/hunch_net-2005-02-10-Conferences%2C_Dates%2C_Locations.html">17 hunch net-2005-02-10-Conferences, Dates, Locations</a></p>
<p>10 0.088273704 <a title="184-tfidf-10" href="../hunch_net-2010/hunch_net-2010-11-18-ICML_2011_%26%238211%3B_Call_for_Tutorials.html">417 hunch net-2010-11-18-ICML 2011 &#8211; Call for Tutorials</a></p>
<p>11 0.086591437 <a title="184-tfidf-11" href="../hunch_net-2005/hunch_net-2005-12-29-Deadline_Season.html">145 hunch net-2005-12-29-Deadline Season</a></p>
<p>12 0.086566322 <a title="184-tfidf-12" href="../hunch_net-2007/hunch_net-2007-04-30-COLT_2007.html">242 hunch net-2007-04-30-COLT 2007</a></p>
<p>13 0.083898269 <a title="184-tfidf-13" href="../hunch_net-2013/hunch_net-2013-11-09-Graduates_and_Postdocs.html">490 hunch net-2013-11-09-Graduates and Postdocs</a></p>
<p>14 0.081214026 <a title="184-tfidf-14" href="../hunch_net-2007/hunch_net-2007-02-22-Create_Your_Own_ICML_Workshop.html">234 hunch net-2007-02-22-Create Your Own ICML Workshop</a></p>
<p>15 0.079879738 <a title="184-tfidf-15" href="../hunch_net-2006/hunch_net-2006-04-27-Conferences%2C_Workshops%2C_and_Tutorials.html">174 hunch net-2006-04-27-Conferences, Workshops, and Tutorials</a></p>
<p>16 0.073811777 <a title="184-tfidf-16" href="../hunch_net-2005/hunch_net-2005-02-25-Why_Papers%3F.html">30 hunch net-2005-02-25-Why Papers?</a></p>
<p>17 0.070746407 <a title="184-tfidf-17" href="../hunch_net-2008/hunch_net-2008-11-28-A_Bumper_Crop_of_Machine_Learning_Graduates.html">329 hunch net-2008-11-28-A Bumper Crop of Machine Learning Graduates</a></p>
<p>18 0.064182572 <a title="184-tfidf-18" href="../hunch_net-2009/hunch_net-2009-08-27-New_York_Area_Machine_Learning_Events.html">369 hunch net-2009-08-27-New York Area Machine Learning Events</a></p>
<p>19 0.063819684 <a title="184-tfidf-19" href="../hunch_net-2005/hunch_net-2005-07-11-AAAI_blog.html">92 hunch net-2005-07-11-AAAI blog</a></p>
<p>20 0.061257236 <a title="184-tfidf-20" href="../hunch_net-2009/hunch_net-2009-11-23-ICML_2009_Workshops_%28and_Tutorials%29.html">379 hunch net-2009-11-23-ICML 2009 Workshops (and Tutorials)</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.064), (1, 0.089), (2, 0.04), (3, 0.125), (4, 0.007), (5, -0.06), (6, -0.014), (7, 0.012), (8, 0.007), (9, -0.094), (10, 0.07), (11, -0.103), (12, -0.027), (13, -0.039), (14, 0.096), (15, -0.081), (16, 0.019), (17, -0.039), (18, -0.065), (19, -0.222), (20, 0.116), (21, 0.004), (22, 0.023), (23, 0.04), (24, 0.013), (25, 0.062), (26, -0.073), (27, -0.048), (28, 0.004), (29, 0.084), (30, -0.063), (31, 0.032), (32, 0.072), (33, -0.047), (34, 0.041), (35, 0.013), (36, -0.064), (37, 0.076), (38, 0.014), (39, -0.034), (40, -0.085), (41, 0.009), (42, -0.018), (43, -0.01), (44, -0.053), (45, 0.053), (46, -0.06), (47, 0.017), (48, -0.022), (49, -0.04)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98398185 <a title="184-lsi-1" href="../hunch_net-2006/hunch_net-2006-06-15-IJCAI_is_out_of_season.html">184 hunch net-2006-06-15-IJCAI is out of season</a></p>
<p>Introduction: IJCAIis running January 6-12 in Hyderabad India rather than a more traditional
summer date. (Presumably, this is to avoid melting people in the Indian
summer.)The paper deadline(June 23 abstract / June 30 submission) are
particularly inconvenient if you attendCOLTorICML. But on the other hand, it's
a good excuse to visit India.</p><p>2 0.65155345 <a title="184-lsi-2" href="../hunch_net-2009/hunch_net-2009-05-30-Many_ways_to_Learn_this_summer.html">357 hunch net-2009-05-30-Many ways to Learn this summer</a></p>
<p>Introduction: There are at least3summer schools related to machine learning this summer.The
firstis atUniversity of ChicagoJune 1-11 organized byMisha Belkin,Partha
Niyogi, andSteve Smale. Registration is closed for this one, meaning they met
their capacity limit. The format is essentially an extended Tutorial/Workshop.
I was particularly interested to seeValiantamongst the speakers. I'm also
presenting Saturday June 6, on logarithmic time prediction.Praveen
Srinivasanpoints out the second atPeking Universityin Beijing, China, July
20-27.This onediffers substantially, as it is about vision, machine learning,
and their intersection. The deadline for applications is June 10 or 15. This
is also another example of the growth of research in China, with active
support fromNSF.The third one is atCambridge, England, August 29-September 10.
It's in theMLSS series. Compared to the Chicago one, this one is more about
the Bayesian side of ML, although effort has been made to create a good cross
section of topic</p><p>3 0.60121465 <a title="184-lsi-3" href="../hunch_net-2005/hunch_net-2005-01-26-Summer_Schools.html">4 hunch net-2005-01-26-Summer Schools</a></p>
<p>Introduction: There are several summer schools related to machine learning.We are running a
two weekmachine learning summer schoolin Chicago, USA May 16-27.IPAM is
running a more focused three week summer school onIntelligent Extraction of
Information from Graphs and High Dimensional Datain Los Angeles, USA July
11-29.A broad one-week school onanalysis of patternswill be held in Erice,
Italy, Oct. 28-Nov 6.</p><p>4 0.59596753 <a title="184-lsi-4" href="../hunch_net-2005/hunch_net-2005-12-29-Deadline_Season.html">145 hunch net-2005-12-29-Deadline Season</a></p>
<p>Introduction: Many different paper deadlines are coming up soon so I made a little reference
table. Out of curiosity, I also computed the interval between submission
deadline and
conference.ConferenceLocationDateDeadlineintervalCOLTPittsburghJune
22-25January 21152ICMLPittsburghJune 26-28January 30/February 6140UAIMITJuly
13-16March 9/March 16119AAAIBostonJuly 16-20February
16/21145KDDPhiladelphiaAugust 23-26March 3/March 10166It looks like the
northeastern US is the big winner as far as location this year.</p><p>5 0.5514555 <a title="184-lsi-5" href="../hunch_net-2008/hunch_net-2008-11-11-COLT_CFP.html">326 hunch net-2008-11-11-COLT CFP</a></p>
<p>Introduction: Adam Klivans, points out theCOLT call for papers. The important points are:Due
Feb 13.Montreal, June 18-21.This year, there is author feedback.</p><p>6 0.49955398 <a title="184-lsi-6" href="../hunch_net-2005/hunch_net-2005-11-16-MLSS_2006.html">130 hunch net-2005-11-16-MLSS 2006</a></p>
<p>7 0.48371822 <a title="184-lsi-7" href="../hunch_net-2010/hunch_net-2010-01-19-Deadline_Season%2C_2010.html">387 hunch net-2010-01-19-Deadline Season, 2010</a></p>
<p>8 0.45639506 <a title="184-lsi-8" href="../hunch_net-2010/hunch_net-2010-11-18-ICML_2011_%26%238211%3B_Call_for_Tutorials.html">417 hunch net-2010-11-18-ICML 2011 &#8211; Call for Tutorials</a></p>
<p>9 0.4381831 <a title="184-lsi-9" href="../hunch_net-2012/hunch_net-2012-08-27-NYAS_ML_2012_and_ICML_2013.html">472 hunch net-2012-08-27-NYAS ML 2012 and ICML 2013</a></p>
<p>10 0.43132335 <a title="184-lsi-10" href="../hunch_net-2011/hunch_net-2011-01-16-2011_Summer_Conference_Deadline_Season.html">422 hunch net-2011-01-16-2011 Summer Conference Deadline Season</a></p>
<p>11 0.42755267 <a title="184-lsi-11" href="../hunch_net-2008/hunch_net-2008-01-07-2008_Summer_Machine_Learning_Conference_Schedule.html">283 hunch net-2008-01-07-2008 Summer Machine Learning Conference Schedule</a></p>
<p>12 0.41663808 <a title="184-lsi-12" href="../hunch_net-2012/hunch_net-2012-07-17-MUCMD_and_BayLearn.html">470 hunch net-2012-07-17-MUCMD and BayLearn</a></p>
<p>13 0.40861642 <a title="184-lsi-13" href="../hunch_net-2013/hunch_net-2013-11-09-Graduates_and_Postdocs.html">490 hunch net-2013-11-09-Graduates and Postdocs</a></p>
<p>14 0.3855767 <a title="184-lsi-14" href="../hunch_net-2005/hunch_net-2005-02-10-Conferences%2C_Dates%2C_Locations.html">17 hunch net-2005-02-10-Conferences, Dates, Locations</a></p>
<p>15 0.38211223 <a title="184-lsi-15" href="../hunch_net-2005/hunch_net-2005-07-11-AAAI_blog.html">92 hunch net-2005-07-11-AAAI blog</a></p>
<p>16 0.36280742 <a title="184-lsi-16" href="../hunch_net-2011/hunch_net-2011-10-03-Monday_announcements.html">446 hunch net-2011-10-03-Monday announcements</a></p>
<p>17 0.35805017 <a title="184-lsi-17" href="../hunch_net-2012/hunch_net-2012-03-13-The_Submodularity_workshop_and_Lucca_Professorship.html">459 hunch net-2012-03-13-The Submodularity workshop and Lucca Professorship</a></p>
<p>18 0.34963483 <a title="184-lsi-18" href="../hunch_net-2006/hunch_net-2006-05-21-NIPS_paper_evaluation_criteria.html">180 hunch net-2006-05-21-NIPS paper evaluation criteria</a></p>
<p>19 0.34293139 <a title="184-lsi-19" href="../hunch_net-2009/hunch_net-2009-11-23-ICML_2009_Workshops_%28and_Tutorials%29.html">379 hunch net-2009-11-23-ICML 2009 Workshops (and Tutorials)</a></p>
<p>20 0.33960834 <a title="184-lsi-20" href="../hunch_net-2007/hunch_net-2007-04-30-COLT_2007.html">242 hunch net-2007-04-30-COLT 2007</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(42, 0.795)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.99991876 <a title="184-lda-1" href="../hunch_net-2008/hunch_net-2008-04-27-Watchword%3A_Supervised_Learning.html">299 hunch net-2008-04-27-Watchword: Supervised Learning</a></p>
<p>Introduction: I recently discovered that supervised learning is a controversial term. The
two definitions are:Known LossSupervised learning corresponds to the situation
where you have unlabeled examples plus knowledge of the loss of each possible
predicted choice. This is the definition I'm familiar and comfortable with.
One reason to prefer this definition is that the analysis of sample complexity
for this class of learning problems are all pretty similar.Any kind of
signalSupervised learning corresponds to the situation where you have
unlabeled examples plus any source of side information about what the right
choice is. This notion of supervised learning seems to subsume reinforcement
learning, which makes me uncomfortable, because it means there are two words
for the same class. This also means there isn't a convenient word to describe
the first definition.Reviews suggest there are people who are dedicated to the
second definition out there, so it can be important to discriminate which you
mean.</p><p>2 0.99919683 <a title="184-lda-2" href="../hunch_net-2006/hunch_net-2006-01-23-On_Coding_via_Mutual_Information_%26%23038%3B_Bayes_Nets.html">150 hunch net-2006-01-23-On Coding via Mutual Information &#038; Bayes Nets</a></p>
<p>Introduction: Say we have two random variablesX,Ywith mutual informationI(X,Y). Let's say we
want to represent them with a bayes net of the formX< -M->Y, such that the
entropy ofMequals the mutual information, i.e.H(M)=I(X,Y). Intuitively, we
would like our hidden state to be as simple as possible (entropy wise). The
data processing inequality means thatH(M)>=I(X,Y), so the mutual information
is a lower bound on how simple theMcould be. Furthermore, if such a
construction existed it would have a nice coding interpretation -- one could
jointly codeXandYby first coding the mutual information, then codingXwith this
mutual info (withoutY) and codingYwith this mutual info (withoutX).It turns
out that such a construction does not exist in general (ThxAlina
Beygelzimerfor a counterexample! see below for the sketch).What are the
implications of this? Well, it's hard for me to say, but it does suggest to me
that the 'generative' model philosophy might be burdened with a harder
modeling task. If all we care a</p><p>3 0.99913281 <a title="184-lda-3" href="../hunch_net-2012/hunch_net-2012-10-26-ML_Symposium_and_Strata-Hadoop_World.html">475 hunch net-2012-10-26-ML Symposium and Strata-Hadoop World</a></p>
<p>Introduction: TheNew York ML symposiumwas last Friday. There were 303 registrations, up a
bit fromlast year. I particularly enjoyed talks byBill Freemanon vision and
ML,Jon Lenchneron strategy in Jeopardy, andTara N. Sainathand Brian Kingsbury
ondeep learning for speech recognition. If anyone has suggestions or thoughts
for next year, please speak up.I also attendedStrata + Hadoop Worldfor the
first time. This is primarily a trade conference rather than an academic
conference, but I found it pretty interesting as a first time attendee. This
is ground zero for theBig databuzzword, and I see now why. It's about data,
and the word "big" is so ambiguous that everyone can lay claim to it. There
were essentially zero academic talks. Instead, the focus was on war stories,
product announcements, and education. The general level of education is much
lower--explaining Machine Learning to the SQL educated is the primary
operating point. Nevertheless that's happening, and the fact that machine
learning is consi</p><p>4 0.9988516 <a title="184-lda-4" href="../hunch_net-2005/hunch_net-2005-05-06-Don%26%238217%3Bt_mix_the_solution_into_the_problem.html">67 hunch net-2005-05-06-Don&#8217;t mix the solution into the problem</a></p>
<p>Introduction: A common defect of many pieces of research is defining the problem in terms of
the solution. Here are some examples in learning:"The learning problem is
finding a good seperating hyperplane.""The goal of learning is to
minimize(y-p)2+ C w2wherey= the observation,p= the prediction andw= a
parameter vector."Defining thelossfunction to be the one that your algorithm
optimizes rather than the one imposed by the world.The fundamental reason why
this is a defect is that it creates artificial boundaries to problem solution.
Artificial boundaries lead to the possibility of being blind-sided. For
example, someone committing (1) or (2) above might find themselves themselves
surprised to find a decision tree working well on a problem. Example (3) might
result in someone else solving a learning problem better for real world
purposes, even if it's worse with respect to the algorithm optimization. This
defect should be avoided so as to not artificially limit your learning
kungfu.The way to avoid thi</p><p>5 0.99814397 <a title="184-lda-5" href="../hunch_net-2005/hunch_net-2005-12-09-Some_NIPS_papers.html">138 hunch net-2005-12-09-Some NIPS papers</a></p>
<p>Introduction: Here is a set of papers that I found interesting (and why).A PAC-Bayes
approach to the Set Covering Machineimproves the set covering machine. The set
covering machine approach is a new way to do classification characterized by a
very close connection between theory and algorithm. At this point, the
approach seems to be competing well with SVMs in about all dimensions: similar
computational speed, similar accuracy, stronger learning theory guarantees,
more general information source (a kernel has strictly more structure than a
metric), and more sparsity. Developing a classification algorithm is not very
easy, but the results so far are encouraging.Off-Road Obstacle Avoidance
through End-to-End LearningandLearning Depth from Single Monocular Imagesboth
effectively showed that depth information can be predicted from camera images
(using notably different techniques). This ability is strongly enabling
because cameras are cheap, tiny, light, and potentially provider longer range
distance in</p><p>same-blog 6 0.99804348 <a title="184-lda-6" href="../hunch_net-2006/hunch_net-2006-06-15-IJCAI_is_out_of_season.html">184 hunch net-2006-06-15-IJCAI is out of season</a></p>
<p>7 0.99746865 <a title="184-lda-7" href="../hunch_net-2005/hunch_net-2005-03-29-Academic_Mechanism_Design.html">48 hunch net-2005-03-29-Academic Mechanism Design</a></p>
<p>8 0.99571192 <a title="184-lda-8" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>9 0.9955017 <a title="184-lda-9" href="../hunch_net-2005/hunch_net-2005-07-07-The_Limits_of_Learning_Theory.html">90 hunch net-2005-07-07-The Limits of Learning Theory</a></p>
<p>10 0.98740375 <a title="184-lda-10" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>11 0.98612624 <a title="184-lda-11" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<p>12 0.97738785 <a title="184-lda-12" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>13 0.97117656 <a title="184-lda-13" href="../hunch_net-2005/hunch_net-2005-02-23-Problem%3A_Reinforcement_Learning_with_Classification.html">27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</a></p>
<p>14 0.96720743 <a title="184-lda-14" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>15 0.96312094 <a title="184-lda-15" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>16 0.95813882 <a title="184-lda-16" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>17 0.95636135 <a title="184-lda-17" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>18 0.9491021 <a title="184-lda-18" href="../hunch_net-2005/hunch_net-2005-03-21-Research_Styles_in_Machine_Learning.html">44 hunch net-2005-03-21-Research Styles in Machine Learning</a></p>
<p>19 0.94900185 <a title="184-lda-19" href="../hunch_net-2005/hunch_net-2005-05-16-Regret_minimizing_vs_error_limiting_reductions.html">72 hunch net-2005-05-16-Regret minimizing vs error limiting reductions</a></p>
<p>20 0.94723374 <a title="184-lda-20" href="../hunch_net-2006/hunch_net-2006-01-13-Benchmarks_for_RL.html">148 hunch net-2006-01-13-Benchmarks for RL</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
