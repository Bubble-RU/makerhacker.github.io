<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>448 hunch net-2011-10-24-2011 ML symposium and the bears</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2011" href="../home/hunch_net-2011_home.html">hunch_net-2011</a> <a title="hunch_net-2011-448" href="#">hunch_net-2011-448</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>448 hunch net-2011-10-24-2011 ML symposium and the bears</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2011-448-html" href="http://hunch.net/?p=2062">html</a></p><p>Introduction: The  New York ML symposium  was last Friday.  Attendance was 268, significantly larger than  last year .  My impression was that the event mostly still fit the space, although it was crowded.  If anyone has suggestions for next year, speak up.
 
The best student paper award went to  Sergiu Goschin  for a cool video of how his system learned to play video games (I can’t find the paper online yet).  Choosing amongst the submitted talks was pretty difficult this year, as there were many similarly good ones.
 
By coincidence all the invited talks were (at least potentially) about faster learning algorithms.   Stephen Boyd  talked about  ADMM .  Leon Bottou  spoke on single pass online learning via  averaged SGD .   Yoav Freund  talked about  parameter-free hedging .  In Yoav’s case the talk was mostly about a better theoretical learning algorithm, but it has the potential to unlock an exponential computational complexity improvement via oraclization of experts algorithms… but some serious</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Attendance was 268, significantly larger than  last year . [sent-2, score-0.218]
</p><p>2 My impression was that the event mostly still fit the space, although it was crowded. [sent-3, score-0.454]
</p><p>3 If anyone has suggestions for next year, speak up. [sent-4, score-0.282]
</p><p>4 The best student paper award went to  Sergiu Goschin  for a cool video of how his system learned to play video games (I can’t find the paper online yet). [sent-5, score-1.064]
</p><p>5 Choosing amongst the submitted talks was pretty difficult this year, as there were many similarly good ones. [sent-6, score-0.239]
</p><p>6 By coincidence all the invited talks were (at least potentially) about faster learning algorithms. [sent-7, score-0.276]
</p><p>7 Leon Bottou  spoke on single pass online learning via  averaged SGD . [sent-9, score-0.552]
</p><p>8 Yoav Freund  talked about  parameter-free hedging . [sent-10, score-0.359]
</p><p>9 In Yoav’s case the talk was mostly about a better theoretical learning algorithm, but it has the potential to unlock an exponential computational complexity improvement via oraclization of experts algorithms… but some serious thought needs to go in this direction. [sent-11, score-0.35]
</p><p>10 Unrelated, I found quite a bit of truth in Paul’s  talking bears  and  Xtranormal  always adds a dash of funny. [sent-12, score-0.406]
</p><p>11 My impression is that the ML job market has only become hotter  since 4 years ago . [sent-13, score-0.415]
</p><p>12 Anyone who is well trained can find work, with the key limiting factor being “well trained”. [sent-14, score-0.433]
</p><p>13 In this environment, efforts to make ML more automatic and more easily applied are greatly appreciated. [sent-15, score-0.186]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('trained', 0.241), ('talked', 0.221), ('yoav', 0.221), ('video', 0.201), ('ml', 0.187), ('impression', 0.179), ('mostly', 0.161), ('talks', 0.148), ('hotter', 0.138), ('hiring', 0.138), ('hedging', 0.138), ('averaged', 0.138), ('coincidence', 0.128), ('unrelated', 0.128), ('spoke', 0.128), ('year', 0.125), ('boyd', 0.121), ('stephen', 0.121), ('anyone', 0.114), ('still', 0.114), ('adds', 0.111), ('games', 0.111), ('bears', 0.111), ('sgd', 0.107), ('via', 0.105), ('bottou', 0.103), ('freund', 0.103), ('paul', 0.1), ('find', 0.099), ('market', 0.098), ('efforts', 0.095), ('environment', 0.095), ('leon', 0.095), ('award', 0.095), ('limiting', 0.093), ('pass', 0.093), ('play', 0.093), ('went', 0.093), ('truth', 0.093), ('last', 0.093), ('talking', 0.091), ('submitted', 0.091), ('automatic', 0.091), ('attendance', 0.089), ('online', 0.088), ('speak', 0.086), ('exponential', 0.084), ('cool', 0.083), ('symposium', 0.083), ('suggestions', 0.082)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999982 <a title="448-tfidf-1" href="../hunch_net-2011/hunch_net-2011-10-24-2011_ML_symposium_and_the_bears.html">448 hunch net-2011-10-24-2011 ML symposium and the bears</a></p>
<p>Introduction: The  New York ML symposium  was last Friday.  Attendance was 268, significantly larger than  last year .  My impression was that the event mostly still fit the space, although it was crowded.  If anyone has suggestions for next year, speak up.
 
The best student paper award went to  Sergiu Goschin  for a cool video of how his system learned to play video games (I can’t find the paper online yet).  Choosing amongst the submitted talks was pretty difficult this year, as there were many similarly good ones.
 
By coincidence all the invited talks were (at least potentially) about faster learning algorithms.   Stephen Boyd  talked about  ADMM .  Leon Bottou  spoke on single pass online learning via  averaged SGD .   Yoav Freund  talked about  parameter-free hedging .  In Yoav’s case the talk was mostly about a better theoretical learning algorithm, but it has the potential to unlock an exponential computational complexity improvement via oraclization of experts algorithms… but some serious</p><p>2 0.15643609 <a title="448-tfidf-2" href="../hunch_net-2011/hunch_net-2011-10-10-ML_Symposium_and_ICML_details.html">447 hunch net-2011-10-10-ML Symposium and ICML details</a></p>
<p>Introduction: Everyone should have received notice for  NY ML Symposium  abstracts.  Check carefully, as one was lost by our system.  
 
The event itself is October 21, next week.   Leon Bottou ,  Stephen Boyd , and  Yoav Freund  are giving the invited talks this year, and there are many spotlights on local work spread throughout the day.   Chris Wiggins  has setup 6(!) ML-interested startups to follow the symposium, which should be of substantial interest to the employment interested.
 
I also wanted to give an update on  ICML 2012 .  Unlike last year, our deadline is coordinated with  AIStat  (which is due this Friday).  The paper deadline for ICML has been pushed back to February 24 which should allow significant time for finishing up papers after the winter break.  Other details may interest people as well: 
  
 We settled on using  CMT  after checking out the possibilities.  I wasn’t looking for this, because I’ve  often found CMT clunky in terms of easy access to the right information.  Nevert</p><p>3 0.11894104 <a title="448-tfidf-3" href="../hunch_net-2012/hunch_net-2012-10-26-ML_Symposium_and_Strata-Hadoop_World.html">475 hunch net-2012-10-26-ML Symposium and Strata-Hadoop World</a></p>
<p>Introduction: The  New York ML symposium  was last Friday.  There were 303 registrations, up a bit from  last year .  I particularly enjoyed talks by  Bill Freeman  on vision and ML,  Jon Lenchner  on strategy in Jeopardy, and  Tara N. Sainath  and Brian Kingsbury on  deep learning for speech recognition .  If anyone has suggestions or thoughts for next year, please speak up.
 
I also attended  Strata + Hadoop World  for the first time.  This is primarily a trade conference rather than an academic conference, but I found it pretty interesting as a first time attendee.  This is ground zero for the  Big data  buzzword, and I see now why.  It’s about data, and the word “big” is so ambiguous that everyone can lay claim to it.  There were essentially zero academic talks.  Instead, the focus was on war stories, product announcements, and education.  The general level of education is much lower—explaining Machine Learning to the SQL educated is the primary operating point.  Nevertheless that’s happening, a</p><p>4 0.11688111 <a title="448-tfidf-4" href="../hunch_net-2009/hunch_net-2009-11-09-NYAS_ML_Symposium_this_year..html">377 hunch net-2009-11-09-NYAS ML Symposium this year.</a></p>
<p>Introduction: The  NYAS ML symposium  grew again this year to 170 participants, despite the need to outsmart or otherwise tunnel through  a crowd .  
 
Perhaps the most distinct talk was by Bob Bell on various aspects of the  Netflix prize  competition.  I also enjoyed several student posters including  Matt Hoffman ‘s cool examples of blind source separation for music.
 
I’m somewhat surprised how much the workshop has grown, as it is now comparable in size to a small conference, although in style more similar to a workshop.  At some point as an event grows, it becomes owned by the community rather than the organizers, so if anyone has suggestions on improving it, speak up and be heard.</p><p>5 0.10982668 <a title="448-tfidf-5" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>Introduction: The 2006 Machine Learning Summer School in Taipei, Taiwan ended on August 4, 2006. It has been a very exciting two weeks for a record crowd of 245 participants (including speakers and organizers) from 18 countries. We had a lineup of speakers that is hard to match up for other similar events (see our  WIKI  for more information). With this lineup, it is difficult for us as organizers to screw it up too bad.  Also, since we have pretty good infrastructure for international meetings and experienced staff at NTUST and Academia Sinica, plus the reputation established by previous MLSS series, it was relatively easy for us to attract registrations and simply enjoyed this two-week long party of machine learning.
 
In the end of MLSS we distributed a survey form for participants to fill in. I will report what we found from this survey, together with the registration data and word-of-mouth from participants.  
 
The first question is designed to find out how our participants learned about MLSS</p><p>6 0.10149378 <a title="448-tfidf-6" href="../hunch_net-2009/hunch_net-2009-12-27-Interesting_things_at_NIPS_2009.html">385 hunch net-2009-12-27-Interesting things at NIPS 2009</a></p>
<p>7 0.099067137 <a title="448-tfidf-7" href="../hunch_net-2012/hunch_net-2012-02-29-Key_Scientific_Challenges_and_the_Franklin_Symposium.html">457 hunch net-2012-02-29-Key Scientific Challenges and the Franklin Symposium</a></p>
<p>8 0.098259673 <a title="448-tfidf-8" href="../hunch_net-2010/hunch_net-2010-09-17-New_York_Area_Machine_Learning_Events.html">410 hunch net-2010-09-17-New York Area Machine Learning Events</a></p>
<p>9 0.095214911 <a title="448-tfidf-9" href="../hunch_net-2005/hunch_net-2005-05-29-Maximum_Margin_Mismatch%3F.html">77 hunch net-2005-05-29-Maximum Margin Mismatch?</a></p>
<p>10 0.093174323 <a title="448-tfidf-10" href="../hunch_net-2007/hunch_net-2007-11-02-The_Machine_Learning_Award_goes_to_%26%238230%3B.html">270 hunch net-2007-11-02-The Machine Learning Award goes to &#8230;</a></p>
<p>11 0.092305556 <a title="448-tfidf-11" href="../hunch_net-2009/hunch_net-2009-01-07-Interesting_Papers_at_SODA_2009.html">334 hunch net-2009-01-07-Interesting Papers at SODA 2009</a></p>
<p>12 0.090068579 <a title="448-tfidf-12" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>13 0.087785065 <a title="448-tfidf-13" href="../hunch_net-2010/hunch_net-2010-08-21-Rob_Schapire_at_NYC_ML_Meetup.html">405 hunch net-2010-08-21-Rob Schapire at NYC ML Meetup</a></p>
<p>14 0.08714895 <a title="448-tfidf-14" href="../hunch_net-2007/hunch_net-2007-04-21-Videolectures.net.html">240 hunch net-2007-04-21-Videolectures.net</a></p>
<p>15 0.08605057 <a title="448-tfidf-15" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>16 0.085386015 <a title="448-tfidf-16" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>17 0.084765166 <a title="448-tfidf-17" href="../hunch_net-2008/hunch_net-2008-11-28-A_Bumper_Crop_of_Machine_Learning_Graduates.html">329 hunch net-2008-11-28-A Bumper Crop of Machine Learning Graduates</a></p>
<p>18 0.082691275 <a title="448-tfidf-18" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>19 0.082261018 <a title="448-tfidf-19" href="../hunch_net-2009/hunch_net-2009-03-18-Parallel_ML_primitives.html">346 hunch net-2009-03-18-Parallel ML primitives</a></p>
<p>20 0.079744972 <a title="448-tfidf-20" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.196), (1, -0.053), (2, -0.095), (3, -0.021), (4, 0.021), (5, 0.009), (6, -0.1), (7, -0.058), (8, -0.137), (9, -0.104), (10, 0.077), (11, 0.0), (12, 0.056), (13, -0.032), (14, 0.069), (15, 0.033), (16, -0.001), (17, -0.001), (18, 0.129), (19, 0.027), (20, 0.046), (21, 0.037), (22, 0.057), (23, 0.042), (24, 0.116), (25, -0.091), (26, -0.063), (27, 0.045), (28, 0.099), (29, 0.037), (30, -0.066), (31, -0.069), (32, -0.029), (33, -0.028), (34, -0.023), (35, 0.01), (36, 0.062), (37, 0.044), (38, -0.029), (39, -0.038), (40, -0.048), (41, 0.013), (42, 0.03), (43, -0.006), (44, 0.05), (45, -0.031), (46, 0.014), (47, 0.089), (48, 0.048), (49, 0.014)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96997702 <a title="448-lsi-1" href="../hunch_net-2011/hunch_net-2011-10-24-2011_ML_symposium_and_the_bears.html">448 hunch net-2011-10-24-2011 ML symposium and the bears</a></p>
<p>Introduction: The  New York ML symposium  was last Friday.  Attendance was 268, significantly larger than  last year .  My impression was that the event mostly still fit the space, although it was crowded.  If anyone has suggestions for next year, speak up.
 
The best student paper award went to  Sergiu Goschin  for a cool video of how his system learned to play video games (I can’t find the paper online yet).  Choosing amongst the submitted talks was pretty difficult this year, as there were many similarly good ones.
 
By coincidence all the invited talks were (at least potentially) about faster learning algorithms.   Stephen Boyd  talked about  ADMM .  Leon Bottou  spoke on single pass online learning via  averaged SGD .   Yoav Freund  talked about  parameter-free hedging .  In Yoav’s case the talk was mostly about a better theoretical learning algorithm, but it has the potential to unlock an exponential computational complexity improvement via oraclization of experts algorithms… but some serious</p><p>2 0.66199309 <a title="448-lsi-2" href="../hunch_net-2010/hunch_net-2010-10-28-NY_ML_Symposium_2010.html">415 hunch net-2010-10-28-NY ML Symposium 2010</a></p>
<p>Introduction: About 200 people attended the  2010 NYAS ML Symposium  this year.  (It was  about 170 last year .)  I particularly enjoyed several talks.
  
  Yann  has a new live demo of (limited) real-time object recognition learning.  
  Sanjoy  gave a fairly convincing and comprehensible explanation of why a  modified form of single-linkage clustering  is consistent in higher dimensions, and why consistency is a critical feature for clustering algorithms.  I’m curious how well this algorithm works in practice. 
  Matt Hoffman ‘s poster covering online LDA seemed pretty convincing to me as an algorithmic improvement. 
  
This year, we allocated more time towards posters & poster spotlights.  
 
For next year, we are considering some further changes.  The format has traditionally been 4 invited Professor speakers, with posters and poster spotlight for students.  Demand from other parties to participate is growing, for example from postdocs and startups in the area.  Another growing concern is the fa</p><p>3 0.65633935 <a title="448-lsi-3" href="../hunch_net-2010/hunch_net-2010-08-21-Rob_Schapire_at_NYC_ML_Meetup.html">405 hunch net-2010-08-21-Rob Schapire at NYC ML Meetup</a></p>
<p>Introduction: I’ve been wanting to attend the  NYC ML Meetup  for some time and hope to make it  next week on the 25th .   Rob Schapire  is talking about “Playing Repeated Games”, which in my experience is far more relevant to machine learning than the title might indicate.</p><p>4 0.61051661 <a title="448-lsi-4" href="../hunch_net-2011/hunch_net-2011-10-10-ML_Symposium_and_ICML_details.html">447 hunch net-2011-10-10-ML Symposium and ICML details</a></p>
<p>Introduction: Everyone should have received notice for  NY ML Symposium  abstracts.  Check carefully, as one was lost by our system.  
 
The event itself is October 21, next week.   Leon Bottou ,  Stephen Boyd , and  Yoav Freund  are giving the invited talks this year, and there are many spotlights on local work spread throughout the day.   Chris Wiggins  has setup 6(!) ML-interested startups to follow the symposium, which should be of substantial interest to the employment interested.
 
I also wanted to give an update on  ICML 2012 .  Unlike last year, our deadline is coordinated with  AIStat  (which is due this Friday).  The paper deadline for ICML has been pushed back to February 24 which should allow significant time for finishing up papers after the winter break.  Other details may interest people as well: 
  
 We settled on using  CMT  after checking out the possibilities.  I wasn’t looking for this, because I’ve  often found CMT clunky in terms of easy access to the right information.  Nevert</p><p>5 0.61009759 <a title="448-lsi-5" href="../hunch_net-2009/hunch_net-2009-11-09-NYAS_ML_Symposium_this_year..html">377 hunch net-2009-11-09-NYAS ML Symposium this year.</a></p>
<p>Introduction: The  NYAS ML symposium  grew again this year to 170 participants, despite the need to outsmart or otherwise tunnel through  a crowd .  
 
Perhaps the most distinct talk was by Bob Bell on various aspects of the  Netflix prize  competition.  I also enjoyed several student posters including  Matt Hoffman ‘s cool examples of blind source separation for music.
 
I’m somewhat surprised how much the workshop has grown, as it is now comparable in size to a small conference, although in style more similar to a workshop.  At some point as an event grows, it becomes owned by the community rather than the organizers, so if anyone has suggestions on improving it, speak up and be heard.</p><p>6 0.57533282 <a title="448-lsi-6" href="../hunch_net-2013/hunch_net-2013-09-20-No_NY_ML_Symposium_in_2013%2C_and_some_good_news.html">489 hunch net-2013-09-20-No NY ML Symposium in 2013, and some good news</a></p>
<p>7 0.57523358 <a title="448-lsi-7" href="../hunch_net-2010/hunch_net-2010-09-17-New_York_Area_Machine_Learning_Events.html">410 hunch net-2010-09-17-New York Area Machine Learning Events</a></p>
<p>8 0.56412381 <a title="448-lsi-8" href="../hunch_net-2012/hunch_net-2012-10-26-ML_Symposium_and_Strata-Hadoop_World.html">475 hunch net-2012-10-26-ML Symposium and Strata-Hadoop World</a></p>
<p>9 0.54137546 <a title="448-lsi-9" href="../hunch_net-2008/hunch_net-2008-10-20-New_York%26%238217%3Bs_ML_Day.html">322 hunch net-2008-10-20-New York&#8217;s ML Day</a></p>
<p>10 0.54056841 <a title="448-lsi-10" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>11 0.52330542 <a title="448-lsi-11" href="../hunch_net-2008/hunch_net-2008-08-18-Radford_Neal_starts_a_blog.html">313 hunch net-2008-08-18-Radford Neal starts a blog</a></p>
<p>12 0.51199061 <a title="448-lsi-12" href="../hunch_net-2011/hunch_net-2011-09-28-Somebody%26%238217%3Bs_Eating_Your_Lunch.html">445 hunch net-2011-09-28-Somebody&#8217;s Eating Your Lunch</a></p>
<p>13 0.50275421 <a title="448-lsi-13" href="../hunch_net-2014/hunch_net-2014-02-16-Metacademy%3A_a_package_manager_for_knowledge.html">493 hunch net-2014-02-16-Metacademy: a package manager for knowledge</a></p>
<p>14 0.48752338 <a title="448-lsi-14" href="../hunch_net-2012/hunch_net-2012-10-18-7th_Annual_Machine_Learning_Symposium.html">474 hunch net-2012-10-18-7th Annual Machine Learning Symposium</a></p>
<p>15 0.47496402 <a title="448-lsi-15" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>16 0.46866569 <a title="448-lsi-16" href="../hunch_net-2013/hunch_net-2013-07-24-ICML_2012_videos_lost.html">487 hunch net-2013-07-24-ICML 2012 videos lost</a></p>
<p>17 0.46847063 <a title="448-lsi-17" href="../hunch_net-2005/hunch_net-2005-05-29-Maximum_Margin_Mismatch%3F.html">77 hunch net-2005-05-29-Maximum Margin Mismatch?</a></p>
<p>18 0.45695052 <a title="448-lsi-18" href="../hunch_net-2007/hunch_net-2007-11-02-The_Machine_Learning_Award_goes_to_%26%238230%3B.html">270 hunch net-2007-11-02-The Machine Learning Award goes to &#8230;</a></p>
<p>19 0.45642963 <a title="448-lsi-19" href="../hunch_net-2008/hunch_net-2008-02-27-The_Stats_Handicap.html">290 hunch net-2008-02-27-The Stats Handicap</a></p>
<p>20 0.45626768 <a title="448-lsi-20" href="../hunch_net-2008/hunch_net-2008-09-04-Fall_ML_Conferences.html">316 hunch net-2008-09-04-Fall ML Conferences</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(27, 0.134), (53, 0.015), (55, 0.693), (94, 0.035), (95, 0.028)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.9971537 <a title="448-lda-1" href="../hunch_net-2012/hunch_net-2012-08-27-NYAS_ML_2012_and_ICML_2013.html">472 hunch net-2012-08-27-NYAS ML 2012 and ICML 2013</a></p>
<p>Introduction: The  New York Machine Learning Symposium  is October 19 with a 2 page abstract deadline due September 13 via email with subject “Machine Learning Poster Submission” sent to physicalscience@nyas.org.  Everyone is welcome to submit.  Last year’s attendance was 246 and I expect more this year.
 
The primary experiment for  ICML 2013  is multiple paper submission deadlines with rolling review cycles.  The key dates are October 1, December 15, and February 15.  This is an attempt to shift ICML further towards a journal style review process and reduce peak load.   The “not for proceedings” experiment from this year’s ICML is not continuing.
 
Edit: Fixed second ICML deadline.</p><p>2 0.99417168 <a title="448-lda-2" href="../hunch_net-2011/hunch_net-2011-10-03-Monday_announcements.html">446 hunch net-2011-10-03-Monday announcements</a></p>
<p>Introduction: Various people want to use hunch.net to announce things.  I’ve generally resisted this because I feared hunch becoming a pure announcement zone while I am much more interested contentful posts and discussion personally.  Nevertheless there is clearly some value and announcements are easy, so I’m planning to summarize announcements on Mondays.
  
  D. Sculley  points out an interesting  Semisupervised feature learning  competition, with a deadline of October 17.  
  Lihong Li  points out the  webscope user interaction dataset  which is the first high quality exploration dataset I’m aware of that is publicly available. 
 Seth Rogers points out  CrossValidated  which looks similar in conception to  metaoptimize , but directly using the  stackoverflow  interface and with a bit more of a statistics twist.</p><p>3 0.99361432 <a title="448-lda-3" href="../hunch_net-2007/hunch_net-2007-11-05-CMU_wins_DARPA_Urban_Challenge.html">271 hunch net-2007-11-05-CMU wins DARPA Urban Challenge</a></p>
<p>Introduction: The  results have been posted , with  CMU first ,  Stanford second , and  Virginia Tech Third .
 
Considering that this was an open event (at least for people in the US), this was a very strong showing for research at universities (instead of defense contractors, for example).  Some details should become public at the  NIPS workshops .
 
 Slashdot  has a  post  with many comments.</p><p>4 0.99007577 <a title="448-lda-4" href="../hunch_net-2008/hunch_net-2008-05-25-Inappropriate_Mathematics_for_Machine_Learning.html">302 hunch net-2008-05-25-Inappropriate Mathematics for Machine Learning</a></p>
<p>Introduction: Reviewers and students are sometimes greatly concerned by the distinction between:
  
  An  open set  and a  closed set . 
 A  Supremum  and a  Maximum . 
 An event which happens with probability 1 and an event that always happens. 
  
I don’t appreciate this distinction in machine learning & learning theory.  All machine learning takes place (by definition) on a machine where every parameter has finite precision.  Consequently, every set is closed, a maximal element always exists, and probability 1 events always happen.
 
The fundamental issue here is that substantial parts of mathematics don’t appear well-matched to computation in the physical world, because the mathematics has concerns which are unphysical.  This mismatched mathematics makes irrelevant distinctions.  We can ask “what mathematics is appropriate to computation?”   Andrej  has convinced me that a pretty good answer to this question is  constructive mathematics .
 
So, here’s a basic challenge: Can anyone name a situati</p><p>same-blog 5 0.98807323 <a title="448-lda-5" href="../hunch_net-2011/hunch_net-2011-10-24-2011_ML_symposium_and_the_bears.html">448 hunch net-2011-10-24-2011 ML symposium and the bears</a></p>
<p>Introduction: The  New York ML symposium  was last Friday.  Attendance was 268, significantly larger than  last year .  My impression was that the event mostly still fit the space, although it was crowded.  If anyone has suggestions for next year, speak up.
 
The best student paper award went to  Sergiu Goschin  for a cool video of how his system learned to play video games (I can’t find the paper online yet).  Choosing amongst the submitted talks was pretty difficult this year, as there were many similarly good ones.
 
By coincidence all the invited talks were (at least potentially) about faster learning algorithms.   Stephen Boyd  talked about  ADMM .  Leon Bottou  spoke on single pass online learning via  averaged SGD .   Yoav Freund  talked about  parameter-free hedging .  In Yoav’s case the talk was mostly about a better theoretical learning algorithm, but it has the potential to unlock an exponential computational complexity improvement via oraclization of experts algorithms… but some serious</p><p>6 0.98756349 <a title="448-lda-6" href="../hunch_net-2005/hunch_net-2005-02-15-ESPgame_and_image_labeling.html">20 hunch net-2005-02-15-ESPgame and image labeling</a></p>
<p>7 0.97975618 <a title="448-lda-7" href="../hunch_net-2008/hunch_net-2008-11-11-COLT_CFP.html">326 hunch net-2008-11-11-COLT CFP</a></p>
<p>8 0.97975618 <a title="448-lda-8" href="../hunch_net-2012/hunch_net-2012-05-12-ICML_accepted_papers_and_early_registration.html">465 hunch net-2012-05-12-ICML accepted papers and early registration</a></p>
<p>9 0.96156353 <a title="448-lda-9" href="../hunch_net-2005/hunch_net-2005-07-07-The_Limits_of_Learning_Theory.html">90 hunch net-2005-07-07-The Limits of Learning Theory</a></p>
<p>10 0.94182819 <a title="448-lda-10" href="../hunch_net-2008/hunch_net-2008-12-12-Summer_Conferences.html">331 hunch net-2008-12-12-Summer Conferences</a></p>
<p>11 0.91646242 <a title="448-lda-11" href="../hunch_net-2007/hunch_net-2007-11-02-The_Machine_Learning_Award_goes_to_%26%238230%3B.html">270 hunch net-2007-11-02-The Machine Learning Award goes to &#8230;</a></p>
<p>12 0.90082556 <a title="448-lda-12" href="../hunch_net-2010/hunch_net-2010-01-19-Deadline_Season%2C_2010.html">387 hunch net-2010-01-19-Deadline Season, 2010</a></p>
<p>13 0.89790434 <a title="448-lda-13" href="../hunch_net-2010/hunch_net-2010-04-26-Compassionate_Reviewing.html">395 hunch net-2010-04-26-Compassionate Reviewing</a></p>
<p>14 0.88795257 <a title="448-lda-14" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<p>15 0.85437763 <a title="448-lda-15" href="../hunch_net-2005/hunch_net-2005-05-02-Reviewing_techniques_for_conferences.html">65 hunch net-2005-05-02-Reviewing techniques for conferences</a></p>
<p>16 0.82954043 <a title="448-lda-16" href="../hunch_net-2009/hunch_net-2009-05-24-2009_ICML_discussion_site.html">356 hunch net-2009-05-24-2009 ICML discussion site</a></p>
<p>17 0.82439184 <a title="448-lda-17" href="../hunch_net-2012/hunch_net-2012-02-29-Key_Scientific_Challenges_and_the_Franklin_Symposium.html">457 hunch net-2012-02-29-Key Scientific Challenges and the Franklin Symposium</a></p>
<p>18 0.81718647 <a title="448-lda-18" href="../hunch_net-2006/hunch_net-2006-11-02-2006_NIPS_workshops.html">216 hunch net-2006-11-02-2006 NIPS workshops</a></p>
<p>19 0.80557466 <a title="448-lda-19" href="../hunch_net-2005/hunch_net-2005-03-24-The_Role_of_Workshops.html">46 hunch net-2005-03-24-The Role of Workshops</a></p>
<p>20 0.79904807 <a title="448-lda-20" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
