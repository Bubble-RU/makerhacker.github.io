<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>448 hunch net-2011-10-24-2011 ML symposium and the bears</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2011" href="../home/hunch_net-2011_home.html">hunch_net-2011</a> <a title="hunch_net-2011-448" href="#">hunch_net-2011-448</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>448 hunch net-2011-10-24-2011 ML symposium and the bears</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2011-448-html" href="http://hunch.net/?p=2062">html</a></p><p>Introduction: TheNew York ML symposiumwas last Friday. Attendance was 268, significantly
larger thanlast year. My impression was that the event mostly still fit the
space, although it was crowded. If anyone has suggestions for next year, speak
up.The best student paper award went toSergiu Goschinfor a cool video of how
his system learned to play video games (I can't find the paper online yet).
Choosing amongst the submitted talks was pretty difficult this year, as there
were many similarly good ones.By coincidence all the invited talks were (at
least potentially) about faster learning algorithms.Stephen Boydtalked
aboutADMM.Leon Bottouspoke on single pass online learning viaaveraged SGD.Yoav
Freundtalked aboutparameter-free hedging. In Yoav's case the talk was mostly
about a better theoretical learning algorithm, but it has the potential to
unlock an exponential computational complexity improvement via oraclization of
experts algorithmsâ&euro;Ś but some serious thought needs to go in this
direction.Unrelat</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 My impression was that the event mostly still fit the space, although it was crowded. [sent-3, score-0.72]
</p><p>2 If anyone has suggestions for next year, speak up. [sent-4, score-0.338]
</p><p>3 The best student paper award went toSergiu Goschinfor a cool video of how his system learned to play video games (I can't find the paper online yet). [sent-5, score-1.374]
</p><p>4 Choosing amongst the submitted talks was pretty difficult this year, as there were many similarly good ones. [sent-6, score-0.369]
</p><p>5 By coincidence all the invited talks were (at least potentially) about faster learning algorithms. [sent-7, score-0.505]
</p><p>6 In Yoav's case the talk was mostly about a better theoretical learning algorithm, but it has the potential to unlock an exponential computational complexity improvement via oraclization of experts algorithmsâ&euro;Ś but some serious thought needs to go in this direction. [sent-11, score-0.56]
</p><p>7 Unrelated, I found quite a bit of truth in Paul'stalking bearsandXtranormalalways adds a dash of funny. [sent-12, score-0.247]
</p><p>8 My impression is that the ML job market has only become hottersince 4 years ago. [sent-13, score-0.422]
</p><p>9 Anyone who is well trained can find work, with the key limiting factor being "well trained". [sent-14, score-0.691]
</p><p>10 In this environment, efforts to make ML more automatic and more easily applied are greatly appreciated. [sent-15, score-0.228]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('trained', 0.284), ('video', 0.243), ('ml', 0.224), ('impression', 0.21), ('mostly', 0.189), ('talks', 0.173), ('hiring', 0.162), ('symposiumwas', 0.162), ('coincidence', 0.15), ('anyone', 0.137), ('adds', 0.135), ('still', 0.135), ('games', 0.13), ('find', 0.119), ('efforts', 0.118), ('market', 0.118), ('award', 0.118), ('limiting', 0.112), ('environment', 0.112), ('pass', 0.112), ('went', 0.112), ('truth', 0.112), ('play', 0.11), ('automatic', 0.11), ('submitted', 0.107), ('online', 0.107), ('attendance', 0.105), ('yahoo', 0.105), ('thenew', 0.103), ('speak', 0.103), ('year', 0.1), ('exponential', 0.099), ('suggestions', 0.098), ('cool', 0.098), ('experts', 0.096), ('event', 0.094), ('student', 0.094), ('yes', 0.094), ('job', 0.094), ('fit', 0.092), ('invited', 0.092), ('faster', 0.09), ('york', 0.09), ('choosing', 0.089), ('similarly', 0.089), ('improvement', 0.089), ('potentially', 0.089), ('factor', 0.089), ('key', 0.087), ('needs', 0.087)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000002 <a title="448-tfidf-1" href="../hunch_net-2011/hunch_net-2011-10-24-2011_ML_symposium_and_the_bears.html">448 hunch net-2011-10-24-2011 ML symposium and the bears</a></p>
<p>Introduction: TheNew York ML symposiumwas last Friday. Attendance was 268, significantly
larger thanlast year. My impression was that the event mostly still fit the
space, although it was crowded. If anyone has suggestions for next year, speak
up.The best student paper award went toSergiu Goschinfor a cool video of how
his system learned to play video games (I can't find the paper online yet).
Choosing amongst the submitted talks was pretty difficult this year, as there
were many similarly good ones.By coincidence all the invited talks were (at
least potentially) about faster learning algorithms.Stephen Boydtalked
aboutADMM.Leon Bottouspoke on single pass online learning viaaveraged SGD.Yoav
Freundtalked aboutparameter-free hedging. In Yoav's case the talk was mostly
about a better theoretical learning algorithm, but it has the potential to
unlock an exponential computational complexity improvement via oraclization of
experts algorithmsâ&euro;Ś but some serious thought needs to go in this
direction.Unrelat</p><p>2 0.14033949 <a title="448-tfidf-2" href="../hunch_net-2012/hunch_net-2012-10-26-ML_Symposium_and_Strata-Hadoop_World.html">475 hunch net-2012-10-26-ML Symposium and Strata-Hadoop World</a></p>
<p>Introduction: TheNew York ML symposiumwas last Friday. There were 303 registrations, up a
bit fromlast year. I particularly enjoyed talks byBill Freemanon vision and
ML,Jon Lenchneron strategy in Jeopardy, andTara N. Sainathand Brian Kingsbury
ondeep learning for speech recognition. If anyone has suggestions or thoughts
for next year, please speak up.I also attendedStrata + Hadoop Worldfor the
first time. This is primarily a trade conference rather than an academic
conference, but I found it pretty interesting as a first time attendee. This
is ground zero for theBig databuzzword, and I see now why. It's about data,
and the word "big" is so ambiguous that everyone can lay claim to it. There
were essentially zero academic talks. Instead, the focus was on war stories,
product announcements, and education. The general level of education is much
lower--explaining Machine Learning to the SQL educated is the primary
operating point. Nevertheless that's happening, and the fact that machine
learning is consi</p><p>3 0.12969384 <a title="448-tfidf-3" href="../hunch_net-2009/hunch_net-2009-11-09-NYAS_ML_Symposium_this_year..html">377 hunch net-2009-11-09-NYAS ML Symposium this year.</a></p>
<p>Introduction: TheNYAS ML symposiumgrew again this year to 170 participants, despite the need
to outsmart or otherwise tunnel througha crowd.Perhaps the most distinct talk
was by Bob Bell on various aspects of theNetflix prizecompetition. I also
enjoyed several student posters includingMatt Hoffman's cool examples of blind
source separation for music.I'm somewhat surprised how much the workshop has
grown, as it is now comparable in size to a small conference, although in
style more similar to a workshop. At some point as an event grows, it becomes
owned by the community rather than the organizers, so if anyone has
suggestions on improving it, speak up and be heard.</p><p>4 0.12821364 <a title="448-tfidf-4" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>Introduction: The 2006 Machine Learning Summer School in Taipei, Taiwan ended on August 4,
2006. It has been a very exciting two weeks for a record crowd of 245
participants (including speakers and organizers) from 18 countries. We had a
lineup of speakers that is hard to match up for other similar events (see
ourWIKIfor more information). With this lineup, it is difficult for us as
organizers to screw it up too bad. Also, since we have pretty good
infrastructure for international meetings and experienced staff at NTUST and
Academia Sinica, plus the reputation established by previous MLSS series, it
was relatively easy for us to attract registrations and simply enjoyed this
two-week long party of machine learning.In the end of MLSS we distributed a
survey form for participants to fill in. I will report what we found from this
survey, together with the registration data and word-of-mouth from
participants.The first question is designed to find out how our participants
learned about MLSS 2006 Taipei.</p><p>5 0.12700805 <a title="448-tfidf-5" href="../hunch_net-2007/hunch_net-2007-04-21-Videolectures.net.html">240 hunch net-2007-04-21-Videolectures.net</a></p>
<p>Introduction: Davorhas been working to setupvideolectures.netwhich is the new site for the
many lecturesmentioned here. (Tragically, they seem to only be available in
windows media format.) I went throughmy own projectsand added a few links to
the videos. The day when every result is a set of {paper, slides, video} isn't
quite here yet, but it's within sight. (For many papers, of course, code is a
4th component.)</p><p>6 0.10780671 <a title="448-tfidf-6" href="../hunch_net-2008/hunch_net-2008-11-28-A_Bumper_Crop_of_Machine_Learning_Graduates.html">329 hunch net-2008-11-28-A Bumper Crop of Machine Learning Graduates</a></p>
<p>7 0.1060367 <a title="448-tfidf-7" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>8 0.10599425 <a title="448-tfidf-8" href="../hunch_net-2010/hunch_net-2010-09-17-New_York_Area_Machine_Learning_Events.html">410 hunch net-2010-09-17-New York Area Machine Learning Events</a></p>
<p>9 0.10448331 <a title="448-tfidf-9" href="../hunch_net-2011/hunch_net-2011-10-10-ML_Symposium_and_ICML_details.html">447 hunch net-2011-10-10-ML Symposium and ICML details</a></p>
<p>10 0.10428247 <a title="448-tfidf-10" href="../hunch_net-2009/hunch_net-2009-01-07-Interesting_Papers_at_SODA_2009.html">334 hunch net-2009-01-07-Interesting Papers at SODA 2009</a></p>
<p>11 0.10091624 <a title="448-tfidf-11" href="../hunch_net-2009/hunch_net-2009-12-27-Interesting_things_at_NIPS_2009.html">385 hunch net-2009-12-27-Interesting things at NIPS 2009</a></p>
<p>12 0.10031781 <a title="448-tfidf-12" href="../hunch_net-2012/hunch_net-2012-10-18-7th_Annual_Machine_Learning_Symposium.html">474 hunch net-2012-10-18-7th Annual Machine Learning Symposium</a></p>
<p>13 0.09656933 <a title="448-tfidf-13" href="../hunch_net-2009/hunch_net-2009-06-24-Interesting_papers_at_UAICMOLT_2009.html">361 hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</a></p>
<p>14 0.096212901 <a title="448-tfidf-14" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>15 0.09576156 <a title="448-tfidf-15" href="../hunch_net-2005/hunch_net-2005-05-29-Maximum_Margin_Mismatch%3F.html">77 hunch net-2005-05-29-Maximum Margin Mismatch?</a></p>
<p>16 0.093887471 <a title="448-tfidf-16" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>17 0.093497708 <a title="448-tfidf-17" href="../hunch_net-2012/hunch_net-2012-02-29-Key_Scientific_Challenges_and_the_Franklin_Symposium.html">457 hunch net-2012-02-29-Key Scientific Challenges and the Franklin Symposium</a></p>
<p>18 0.09290456 <a title="448-tfidf-18" href="../hunch_net-2010/hunch_net-2010-12-04-Vowpal_Wabbit%2C_version_5.0%2C_and_the_second_heresy.html">419 hunch net-2010-12-04-Vowpal Wabbit, version 5.0, and the second heresy</a></p>
<p>19 0.092756636 <a title="448-tfidf-19" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>20 0.092334196 <a title="448-tfidf-20" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.213), (1, 0.049), (2, 0.08), (3, 0.097), (4, -0.031), (5, 0.057), (6, 0.105), (7, -0.026), (8, 0.099), (9, -0.143), (10, 0.046), (11, 0.06), (12, 0.041), (13, -0.036), (14, -0.134), (15, 0.099), (16, 0.011), (17, 0.07), (18, -0.011), (19, 0.036), (20, 0.029), (21, 0.022), (22, -0.045), (23, -0.024), (24, 0.054), (25, -0.075), (26, 0.069), (27, -0.018), (28, 0.011), (29, 0.033), (30, 0.038), (31, -0.039), (32, -0.07), (33, -0.026), (34, -0.019), (35, -0.031), (36, 0.071), (37, -0.09), (38, -0.136), (39, -0.086), (40, -0.081), (41, -0.037), (42, 0.074), (43, -0.061), (44, -0.076), (45, 0.017), (46, -0.063), (47, 0.021), (48, 0.061), (49, -0.069)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97297531 <a title="448-lsi-1" href="../hunch_net-2011/hunch_net-2011-10-24-2011_ML_symposium_and_the_bears.html">448 hunch net-2011-10-24-2011 ML symposium and the bears</a></p>
<p>Introduction: TheNew York ML symposiumwas last Friday. Attendance was 268, significantly
larger thanlast year. My impression was that the event mostly still fit the
space, although it was crowded. If anyone has suggestions for next year, speak
up.The best student paper award went toSergiu Goschinfor a cool video of how
his system learned to play video games (I can't find the paper online yet).
Choosing amongst the submitted talks was pretty difficult this year, as there
were many similarly good ones.By coincidence all the invited talks were (at
least potentially) about faster learning algorithms.Stephen Boydtalked
aboutADMM.Leon Bottouspoke on single pass online learning viaaveraged SGD.Yoav
Freundtalked aboutparameter-free hedging. In Yoav's case the talk was mostly
about a better theoretical learning algorithm, but it has the potential to
unlock an exponential computational complexity improvement via oraclization of
experts algorithmsâ&euro;Ś but some serious thought needs to go in this
direction.Unrelat</p><p>2 0.65615863 <a title="448-lsi-2" href="../hunch_net-2010/hunch_net-2010-10-28-NY_ML_Symposium_2010.html">415 hunch net-2010-10-28-NY ML Symposium 2010</a></p>
<p>Introduction: About 200 people attended the2010 NYAS ML Symposiumthis year. (It wasabout 170
last year.) I particularly enjoyed several talks.Yannhas a new live demo of
(limited) real-time object recognition learning.Sanjoygave a fairly convincing
and comprehensible explanation of why amodified form of single-linkage
clusteringis consistent in higher dimensions, and why consistency is a
critical feature for clustering algorithms. I'm curious how well this
algorithm works in practice.Matt Hoffman's poster covering online LDA seemed
pretty convincing to me as an algorithmic improvement.This year, we allocated
more time towards posters & poster spotlights.For next year, we are
considering some further changes. The format has traditionally been 4 invited
Professor speakers, with posters and poster spotlight for students. Demand
from other parties to participate is growing, for example from postdocs and
startups in the area. Another growing concern is the facility--the location is
exceptional, but fittin</p><p>3 0.62879938 <a title="448-lsi-3" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>Introduction: The 2006 Machine Learning Summer School in Taipei, Taiwan ended on August 4,
2006. It has been a very exciting two weeks for a record crowd of 245
participants (including speakers and organizers) from 18 countries. We had a
lineup of speakers that is hard to match up for other similar events (see
ourWIKIfor more information). With this lineup, it is difficult for us as
organizers to screw it up too bad. Also, since we have pretty good
infrastructure for international meetings and experienced staff at NTUST and
Academia Sinica, plus the reputation established by previous MLSS series, it
was relatively easy for us to attract registrations and simply enjoyed this
two-week long party of machine learning.In the end of MLSS we distributed a
survey form for participants to fill in. I will report what we found from this
survey, together with the registration data and word-of-mouth from
participants.The first question is designed to find out how our participants
learned about MLSS 2006 Taipei.</p><p>4 0.60445648 <a title="448-lsi-4" href="../hunch_net-2008/hunch_net-2008-10-20-New_York%26%238217%3Bs_ML_Day.html">322 hunch net-2008-10-20-New York&#8217;s ML Day</a></p>
<p>Introduction: I'm not as naturally exuberant asMuthu2orDavidaboutCS/Econday, but I believe
it andML daywere certainly successful.At the CS/Econ day, I particularly
enjoyedToumas Sandholm'stalk which showed a commanding depth of understanding
and application in automated auctions.For the machine learning day, I enjoyed
several talks and posters (I better, I helped pick them.). What stood out to
me was number of people attending: 158 registered, a level qualifying as
"scramble to find seats". My rule of thumb for workshops/conferences is that
the number of attendees is often something like the number of submissions.
That isn't the case here, where there were just 4 invited speakers and 30-or-
so posters. Presumably, the difference is due to a critical mass of Machine
Learning interested people in the area and the ease of their attendance.Are
there other areas where a local Machine Learning day would fly? It's easy to
imagine something working out in the San Francisco bay area and possibly
Germany or E</p><p>5 0.59655464 <a title="448-lsi-5" href="../hunch_net-2012/hunch_net-2012-10-26-ML_Symposium_and_Strata-Hadoop_World.html">475 hunch net-2012-10-26-ML Symposium and Strata-Hadoop World</a></p>
<p>Introduction: TheNew York ML symposiumwas last Friday. There were 303 registrations, up a
bit fromlast year. I particularly enjoyed talks byBill Freemanon vision and
ML,Jon Lenchneron strategy in Jeopardy, andTara N. Sainathand Brian Kingsbury
ondeep learning for speech recognition. If anyone has suggestions or thoughts
for next year, please speak up.I also attendedStrata + Hadoop Worldfor the
first time. This is primarily a trade conference rather than an academic
conference, but I found it pretty interesting as a first time attendee. This
is ground zero for theBig databuzzword, and I see now why. It's about data,
and the word "big" is so ambiguous that everyone can lay claim to it. There
were essentially zero academic talks. Instead, the focus was on war stories,
product announcements, and education. The general level of education is much
lower--explaining Machine Learning to the SQL educated is the primary
operating point. Nevertheless that's happening, and the fact that machine
learning is consi</p><p>6 0.57880402 <a title="448-lsi-6" href="../hunch_net-2009/hunch_net-2009-11-09-NYAS_ML_Symposium_this_year..html">377 hunch net-2009-11-09-NYAS ML Symposium this year.</a></p>
<p>7 0.49991909 <a title="448-lsi-7" href="../hunch_net-2011/hunch_net-2011-10-10-ML_Symposium_and_ICML_details.html">447 hunch net-2011-10-10-ML Symposium and ICML details</a></p>
<p>8 0.49566317 <a title="448-lsi-8" href="../hunch_net-2009/hunch_net-2009-01-07-Interesting_Papers_at_SODA_2009.html">334 hunch net-2009-01-07-Interesting Papers at SODA 2009</a></p>
<p>9 0.49305883 <a title="448-lsi-9" href="../hunch_net-2008/hunch_net-2008-08-18-Radford_Neal_starts_a_blog.html">313 hunch net-2008-08-18-Radford Neal starts a blog</a></p>
<p>10 0.48861358 <a title="448-lsi-10" href="../hunch_net-2010/hunch_net-2010-09-17-New_York_Area_Machine_Learning_Events.html">410 hunch net-2010-09-17-New York Area Machine Learning Events</a></p>
<p>11 0.48571521 <a title="448-lsi-11" href="../hunch_net-2012/hunch_net-2012-10-18-7th_Annual_Machine_Learning_Symposium.html">474 hunch net-2012-10-18-7th Annual Machine Learning Symposium</a></p>
<p>12 0.47798517 <a title="448-lsi-12" href="../hunch_net-2009/hunch_net-2009-08-27-New_York_Area_Machine_Learning_Events.html">369 hunch net-2009-08-27-New York Area Machine Learning Events</a></p>
<p>13 0.47551721 <a title="448-lsi-13" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>14 0.47471851 <a title="448-lsi-14" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>15 0.47302276 <a title="448-lsi-15" href="../hunch_net-2010/hunch_net-2010-08-21-Rob_Schapire_at_NYC_ML_Meetup.html">405 hunch net-2010-08-21-Rob Schapire at NYC ML Meetup</a></p>
<p>16 0.46035996 <a title="448-lsi-16" href="../hunch_net-2007/hunch_net-2007-04-21-Videolectures.net.html">240 hunch net-2007-04-21-Videolectures.net</a></p>
<p>17 0.45890063 <a title="448-lsi-17" href="../hunch_net-2010/hunch_net-2010-07-02-MetaOptimize.html">402 hunch net-2010-07-02-MetaOptimize</a></p>
<p>18 0.45346645 <a title="448-lsi-18" href="../hunch_net-2007/hunch_net-2007-02-10-Best_Practices_for_Collaboration.html">231 hunch net-2007-02-10-Best Practices for Collaboration</a></p>
<p>19 0.45013583 <a title="448-lsi-19" href="../hunch_net-2011/hunch_net-2011-08-01-Interesting_papers_at_COLT_2011.html">439 hunch net-2011-08-01-Interesting papers at COLT 2011</a></p>
<p>20 0.44679865 <a title="448-lsi-20" href="../hunch_net-2006/hunch_net-2006-05-08-Big_machine_learning.html">178 hunch net-2006-05-08-Big machine learning</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(27, 0.121), (35, 0.068), (42, 0.34), (45, 0.06), (68, 0.052), (69, 0.041), (74, 0.128), (82, 0.036), (91, 0.059)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97539377 <a title="448-lda-1" href="../hunch_net-2011/hunch_net-2011-10-24-2011_ML_symposium_and_the_bears.html">448 hunch net-2011-10-24-2011 ML symposium and the bears</a></p>
<p>Introduction: TheNew York ML symposiumwas last Friday. Attendance was 268, significantly
larger thanlast year. My impression was that the event mostly still fit the
space, although it was crowded. If anyone has suggestions for next year, speak
up.The best student paper award went toSergiu Goschinfor a cool video of how
his system learned to play video games (I can't find the paper online yet).
Choosing amongst the submitted talks was pretty difficult this year, as there
were many similarly good ones.By coincidence all the invited talks were (at
least potentially) about faster learning algorithms.Stephen Boydtalked
aboutADMM.Leon Bottouspoke on single pass online learning viaaveraged SGD.Yoav
Freundtalked aboutparameter-free hedging. In Yoav's case the talk was mostly
about a better theoretical learning algorithm, but it has the potential to
unlock an exponential computational complexity improvement via oraclization of
experts algorithmsâ&euro;Ś but some serious thought needs to go in this
direction.Unrelat</p><p>2 0.93183142 <a title="448-lda-2" href="../hunch_net-2007/hunch_net-2007-12-21-Vowpal_Wabbit_Code_Release.html">281 hunch net-2007-12-21-Vowpal Wabbit Code Release</a></p>
<p>Introduction: We are releasing theVowpal Wabbit (Fast Online Learning) codeas open source
under a BSD (revised) license. This is aproject at Yahoo! Researchto build a
useful large scale learning algorithm whichLihong Li,Alex Strehl, and I have
been working on.To appreciate the meaning of "large", it's useful to define
"small" and "medium". A "small" supervised learning problem is one where a
human could use a labeled dataset and come up with a reasonable predictor. A
"medium" supervised learning problem dataset fits into the RAM of a modern
desktop computer. A "large" supervised learning problem is one which does not
fit into the RAM of a normal machine. VW tackles large scale learning problems
by this definition of large. I'm not aware of any other open source Machine
Learning tools which can handle this scale (although they may exist). A few
close ones are:IBM's Parallel Machine Learning Toolboxisn't quite open source.
The approach used by this toolbox is essentially map-reduce style computation,</p><p>3 0.92675108 <a title="448-lda-3" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>Introduction: "Overfitting" is traditionally defined as training some flexible
representation so that it memorizes the data but fails to predict well in the
future. For this post, I will define overfitting more generally as over-
representing the performance of systems. There are two styles of general
overfitting: overrepresenting performance on particular datasets and
(implicitly) overrepresenting performance of a method on future datasets.We
should all be aware of these methods, avoid them where possible, and take them
into account otherwise. I have used "reproblem" and "old datasets", and may
have participated in "overfitting by review"--some of these are very difficult
to avoid.NameMethodExplanationRemedyTraditional overfittingTrain a complex
predictor on too-few examples.Hold out pristine examples for testing.Use a
simpler predictor.Get more training examples.Integrate over many
predictors.Reject papers which do this.Parameter tweak overfittingUse a
learning algorithm with many parameters. Choo</p><p>4 0.92527437 <a title="448-lda-4" href="../hunch_net-2006/hunch_net-2006-01-18-Is_Multitask_Learning_Black-Boxable%3F.html">149 hunch net-2006-01-18-Is Multitask Learning Black-Boxable?</a></p>
<p>Introduction: Multitask learning is the learning to predict multiple outputs given the same
input. Mathematically, we might think of this as trying to learn a functionf:X
-> {0,1}n. Structured learning is similar at this level of abstraction. Many
people have worked on solving multitask learning (for exampleRich Caruana)
using methods which share an internal representation. On other words, the the
computation and learning of theith prediction is shared with the computation
and learning of thejth prediction. Another way to ask this question is: can we
avoid sharing the internal representation?For example, itmightbe feasible to
solve multitask learning by some process feeding theith predictionf(x)iinto
thejth predictorf(x,f(x)i)j,If the answer is "no", then it implies we can not
take binary classification as a basic primitive in the process of solving
prediction problems. If the answer is "yes", then we can reuse binary
classification algorithms to solve multitask learning problems.Finding a
satisfyin</p><p>5 0.92298746 <a title="448-lda-5" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>Introduction: I've had serious conversations with several people who believe that the theory
in machine learning is "only useful for getting papers published". That's a
compelling statement, as I've seen many papers where the algorithm clearly
came first, and the theoretical justification for it came second, purely as a
perceived means to improve the chance of publication.Naturally, I disagree and
believe that learning theory has much more substantial applications.Even in
core learning algorithm design, I've found learning theory to be useful,
although it's application is more subtle than many realize. The most
straightforward applications can fail, because (as expectation suggests) worst
case bounds tend to be loose in practice (*). In my experience, considering
learning theory when designing an algorithm has two important effects in
practice:It can help make your algorithm behave right at a crude level of
analysis, leaving finer details to tuning or common sense. The best example I
have of this is</p><p>6 0.92280596 <a title="448-lda-6" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>7 0.92081004 <a title="448-lda-7" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<p>8 0.9202922 <a title="448-lda-8" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>9 0.91959548 <a title="448-lda-9" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>10 0.91919512 <a title="448-lda-10" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>11 0.91887146 <a title="448-lda-11" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>12 0.9183439 <a title="448-lda-12" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>13 0.91746509 <a title="448-lda-13" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>14 0.91634929 <a title="448-lda-14" href="../hunch_net-2005/hunch_net-2005-12-07-Is_the_Google_way_the_way_for_machine_learning%3F.html">136 hunch net-2005-12-07-Is the Google way the way for machine learning?</a></p>
<p>15 0.91551012 <a title="448-lda-15" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>16 0.91529918 <a title="448-lda-16" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>17 0.91512132 <a title="448-lda-17" href="../hunch_net-2006/hunch_net-2006-03-02-Why_do_people_count_for_learning%3F.html">160 hunch net-2006-03-02-Why do people count for learning?</a></p>
<p>18 0.91450381 <a title="448-lda-18" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>19 0.91416788 <a title="448-lda-19" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>20 0.91395426 <a title="448-lda-20" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
