<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>436 hunch net-2011-06-22-Ultra LDA</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2011" href="../home/hunch_net-2011_home.html">hunch_net-2011</a> <a title="hunch_net-2011-436" href="#">hunch_net-2011-436</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>436 hunch net-2011-06-22-Ultra LDA</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2011-436-html" href="http://hunch.net/?p=1848">html</a></p><p>Introduction: ShravanandAlex's LDA code isreleased. On a single machine, I'm not sure how it
currently compares to the online LDA inVW, but the ability to effectively
scale across very many machines is surely interesting.</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lda', 0.698), ('surely', 0.261), ('machines', 0.256), ('currently', 0.236), ('across', 0.227), ('effectively', 0.218), ('code', 0.213), ('scale', 0.197), ('single', 0.189), ('sure', 0.184), ('ability', 0.172), ('online', 0.137), ('interesting', 0.124), ('machine', 0.065), ('many', 0.054)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="436-tfidf-1" href="../hunch_net-2011/hunch_net-2011-06-22-Ultra_LDA.html">436 hunch net-2011-06-22-Ultra LDA</a></p>
<p>Introduction: ShravanandAlex's LDA code isreleased. On a single machine, I'm not sure how it
currently compares to the online LDA inVW, but the ability to effectively
scale across very many machines is surely interesting.</p><p>2 0.12969905 <a title="436-tfidf-2" href="../hunch_net-2009/hunch_net-2009-07-31-Vowpal_Wabbit_Open_Source_Project.html">365 hunch net-2009-07-31-Vowpal Wabbit Open Source Project</a></p>
<p>Introduction: Today brings a new release of theVowpal Wabbitfast online learning software.
This time, unlike the previous release, the project itself is going open
source, developing viagithub. For example, the lastest and greatest can be
downloaded via:git clone git://github.com/JohnLangford/vowpal_wabbit.gitIf you
aren't familiar withgit, it's a distributed version control system which
supports quick and easy branching, as well as reconciliation.This version of
the code is confirmed to compile without complaint on at least some flavors of
OSX as well as Linux boxes.As much of the point of this project is pushing the
limits of fast and effective machine learning, let me mention a few datapoints
from my experience.The program can effectively scale up to batch-style
training on sparse terafeature (i.e. 1012sparse feature) size datasets. The
limiting factor is typically i/o.I started using the the real datasets from
thelarge-scale learningworkshop as a convenient benchmark. The largest dataset
takes a</p><p>3 0.12864777 <a title="436-tfidf-3" href="../hunch_net-2010/hunch_net-2010-12-04-Vowpal_Wabbit%2C_version_5.0%2C_and_the_second_heresy.html">419 hunch net-2010-12-04-Vowpal Wabbit, version 5.0, and the second heresy</a></p>
<p>Introduction: I've releasedversion 5.0of theVowpal Wabbitonline learning software. The major
number has changed since thelast releasebecause I regard all earlier versions
as obsolete--there are several new algorithms & features including substantial
changes and upgrades to the default learning algorithm.The biggest changes are
new algorithms:Nikosand I improved the default algorithm. The basic update
rule still uses gradient descent, but the size of the update is carefully
controlled so that it's impossible to overrun the label. In addition, the
normalization has changed. Computationally, these changes are virtually free
and yield better results, sometimes much better. Less careful updates can be
reenabled with -loss_function classic, although results are still not
identical to previous due to normalization changes.Nikos also implemented the
per-feature learning rates as per thesetwopapers. Often, this works better
than the default algorithm. It isn't the default because it isn't (yet) as
adaptable</p><p>4 0.10263387 <a title="436-tfidf-4" href="../hunch_net-2008/hunch_net-2008-11-04-Rise_of_the_Machines.html">323 hunch net-2008-11-04-Rise of the Machines</a></p>
<p>Introduction: On theenduring topic of how people deal with intelligent machines, we have
this importantelection bulletin.</p><p>5 0.093915559 <a title="436-tfidf-5" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>Introduction: Many people in Machine Learning don't fully understand the impact of
computation, as demonstrated by a lack ofbig-Oanalysis of new learning
algorithms. This is important--some current active research programs are
fundamentally flawed w.r.t. computation, and other research programs are
directly motivated by it. When considering a learning algorithm, I think about
the following questions:How does the learning algorithm scale with the number
of examplesm? Any algorithm using all of the data is at leastO(m), but in many
cases this isO(m2)(naive nearest neighbor for self-prediction) or unknown
(k-means or many other optimization algorithms). The unknown case is very
common, and it can mean (for example) that the algorithm isn't convergent or
simply that the amount of computation isn't controlled.The above question can
also be asked for test cases. In some applications, test-time performance is
of great importance.How does the algorithm scale with the number of
featuresnper example? Many sec</p><p>6 0.078486621 <a title="436-tfidf-6" href="../hunch_net-2007/hunch_net-2007-12-21-Vowpal_Wabbit_Code_Release.html">281 hunch net-2007-12-21-Vowpal Wabbit Code Release</a></p>
<p>7 0.07712207 <a title="436-tfidf-7" href="../hunch_net-2011/hunch_net-2011-08-15-Vowpal_Wabbit_6.0.html">441 hunch net-2011-08-15-Vowpal Wabbit 6.0</a></p>
<p>8 0.075697266 <a title="436-tfidf-8" href="../hunch_net-2010/hunch_net-2010-04-14-MLcomp%3A_a_website_for_objectively_comparing_ML_algorithms.html">393 hunch net-2010-04-14-MLcomp: a website for objectively comparing ML algorithms</a></p>
<p>9 0.075686835 <a title="436-tfidf-9" href="../hunch_net-2011/hunch_net-2011-09-28-Somebody%26%238217%3Bs_Eating_Your_Lunch.html">445 hunch net-2011-09-28-Somebody&#8217;s Eating Your Lunch</a></p>
<p>10 0.073763087 <a title="436-tfidf-10" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>11 0.067320757 <a title="436-tfidf-11" href="../hunch_net-2010/hunch_net-2010-10-28-NY_ML_Symposium_2010.html">415 hunch net-2010-10-28-NY ML Symposium 2010</a></p>
<p>12 0.059925243 <a title="436-tfidf-12" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>13 0.058156855 <a title="436-tfidf-13" href="../hunch_net-2006/hunch_net-2006-07-05-more_icml_papers.html">189 hunch net-2006-07-05-more icml papers</a></p>
<p>14 0.056629688 <a title="436-tfidf-14" href="../hunch_net-2007/hunch_net-2007-06-14-Interesting_Papers_at_COLT_2007.html">247 hunch net-2007-06-14-Interesting Papers at COLT 2007</a></p>
<p>15 0.056426574 <a title="436-tfidf-15" href="../hunch_net-2007/hunch_net-2007-10-17-Online_as_the_new_adjective.html">267 hunch net-2007-10-17-Online as the new adjective</a></p>
<p>16 0.056182235 <a title="436-tfidf-16" href="../hunch_net-2005/hunch_net-2005-12-07-Is_the_Google_way_the_way_for_machine_learning%3F.html">136 hunch net-2005-12-07-Is the Google way the way for machine learning?</a></p>
<p>17 0.055018421 <a title="436-tfidf-17" href="../hunch_net-2012/hunch_net-2012-08-24-Patterns_for_research_in_machine_learning.html">471 hunch net-2012-08-24-Patterns for research in machine learning</a></p>
<p>18 0.053693712 <a title="436-tfidf-18" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>19 0.052809335 <a title="436-tfidf-19" href="../hunch_net-2005/hunch_net-2005-03-08-Fast_Physics_for_Learning.html">37 hunch net-2005-03-08-Fast Physics for Learning</a></p>
<p>20 0.052688874 <a title="436-tfidf-20" href="../hunch_net-2006/hunch_net-2006-04-02-Mad_%28Neuro%29science.html">168 hunch net-2006-04-02-Mad (Neuro)science</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.085), (1, -0.024), (2, 0.044), (3, 0.035), (4, -0.037), (5, 0.111), (6, 0.127), (7, 0.047), (8, 0.001), (9, 0.047), (10, 0.005), (11, -0.019), (12, -0.005), (13, -0.092), (14, -0.04), (15, 0.044), (16, 0.042), (17, 0.022), (18, 0.04), (19, -0.035), (20, 0.02), (21, -0.028), (22, -0.059), (23, -0.028), (24, -0.034), (25, -0.014), (26, 0.059), (27, 0.022), (28, -0.026), (29, -0.024), (30, -0.006), (31, 0.017), (32, 0.009), (33, 0.014), (34, -0.068), (35, 0.003), (36, 0.003), (37, 0.039), (38, 0.062), (39, 0.014), (40, 0.051), (41, 0.031), (42, -0.011), (43, 0.101), (44, -0.02), (45, -0.005), (46, 0.019), (47, 0.033), (48, -0.048), (49, 0.038)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97667313 <a title="436-lsi-1" href="../hunch_net-2011/hunch_net-2011-06-22-Ultra_LDA.html">436 hunch net-2011-06-22-Ultra LDA</a></p>
<p>Introduction: ShravanandAlex's LDA code isreleased. On a single machine, I'm not sure how it
currently compares to the online LDA inVW, but the ability to effectively
scale across very many machines is surely interesting.</p><p>2 0.74931067 <a title="436-lsi-2" href="../hunch_net-2009/hunch_net-2009-07-31-Vowpal_Wabbit_Open_Source_Project.html">365 hunch net-2009-07-31-Vowpal Wabbit Open Source Project</a></p>
<p>Introduction: Today brings a new release of theVowpal Wabbitfast online learning software.
This time, unlike the previous release, the project itself is going open
source, developing viagithub. For example, the lastest and greatest can be
downloaded via:git clone git://github.com/JohnLangford/vowpal_wabbit.gitIf you
aren't familiar withgit, it's a distributed version control system which
supports quick and easy branching, as well as reconciliation.This version of
the code is confirmed to compile without complaint on at least some flavors of
OSX as well as Linux boxes.As much of the point of this project is pushing the
limits of fast and effective machine learning, let me mention a few datapoints
from my experience.The program can effectively scale up to batch-style
training on sparse terafeature (i.e. 1012sparse feature) size datasets. The
limiting factor is typically i/o.I started using the the real datasets from
thelarge-scale learningworkshop as a convenient benchmark. The largest dataset
takes a</p><p>3 0.66738856 <a title="436-lsi-3" href="../hunch_net-2007/hunch_net-2007-12-21-Vowpal_Wabbit_Code_Release.html">281 hunch net-2007-12-21-Vowpal Wabbit Code Release</a></p>
<p>Introduction: We are releasing theVowpal Wabbit (Fast Online Learning) codeas open source
under a BSD (revised) license. This is aproject at Yahoo! Researchto build a
useful large scale learning algorithm whichLihong Li,Alex Strehl, and I have
been working on.To appreciate the meaning of "large", it's useful to define
"small" and "medium". A "small" supervised learning problem is one where a
human could use a labeled dataset and come up with a reasonable predictor. A
"medium" supervised learning problem dataset fits into the RAM of a modern
desktop computer. A "large" supervised learning problem is one which does not
fit into the RAM of a normal machine. VW tackles large scale learning problems
by this definition of large. I'm not aware of any other open source Machine
Learning tools which can handle this scale (although they may exist). A few
close ones are:IBM's Parallel Machine Learning Toolboxisn't quite open source.
The approach used by this toolbox is essentially map-reduce style computation,</p><p>4 0.66370356 <a title="436-lsi-4" href="../hunch_net-2011/hunch_net-2011-08-15-Vowpal_Wabbit_6.0.html">441 hunch net-2011-08-15-Vowpal Wabbit 6.0</a></p>
<p>Introduction: I just releasedVowpal Wabbit 6.0. Since the last version:VW is now 2-3 orders
of magnitude faster at linear learning, primarily thanks toAlekh. Given the
baseline, this is loads of fun, allowing us to easily deal with terafeature
datasets, and dwarfing the scale of any other open source projects. The core
improvement here comes from effective parallelization over kilonode clusters
(eitherHadoopor not). This code is highly scalable, so it even helps with
clusters of size 2 (and doesn't hurt for clusters of size 1). The core
allreduce technique appears widely and easily reused--we've already used it to
parallelize Conjugate Gradient, LBFGS, and two variants of online learning.
We'll be documenting how to do this more thoroughly, but for now
"README_cluster" and associated scripts should provide a good starting
point.The newLBFGScode fromMiroseems to commonly dominate the existing
conjugate gradient code in time/quality tradeoffs.The new matrix factorization
code fromJakeadds a core algor</p><p>5 0.58443987 <a title="436-lsi-5" href="../hunch_net-2008/hunch_net-2008-04-30-Concerns_about_the_Large_Scale_Learning_Challenge.html">300 hunch net-2008-04-30-Concerns about the Large Scale Learning Challenge</a></p>
<p>Introduction: Thelarge scale learning challengefor ICML interests me a great deal, although
I have concerns about the way it is structured.From theinstructions page,
several issues come up:Large DefinitionMy personal definition of dataset size
is:smallA dataset is small if a human could look at the dataset and plausibly
find a good solution.mediumA dataset is mediumsize if it fits in the RAM of a
reasonably priced computer.largeA large dataset does not fit in the RAM of a
reasonably priced computer.By this definition, all of the datasets are medium
sized. This might sound like a pissing match over dataset size, but I believe
it is more than that.The fundamental reason for these definitions is that they
correspond to transitions in the sorts of approaches which are feasible. From
small to medium, the ability to use a human as the learning algorithm
degrades. From medium to large, it becomes essential to have learning
algorithms that don't require random access to examples.No Loading TimeThe
medium sc</p><p>6 0.55205804 <a title="436-lsi-6" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>7 0.54487437 <a title="436-lsi-7" href="../hunch_net-2009/hunch_net-2009-03-18-Parallel_ML_primitives.html">346 hunch net-2009-03-18-Parallel ML primitives</a></p>
<p>8 0.51592177 <a title="436-lsi-8" href="../hunch_net-2013/hunch_net-2013-01-07-NYU_Large_Scale_Machine_Learning_Class.html">478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</a></p>
<p>9 0.51544845 <a title="436-lsi-9" href="../hunch_net-2011/hunch_net-2011-12-13-Vowpal_Wabbit_version_6.1_%26%23038%3B_the_NIPS_tutorial.html">451 hunch net-2011-12-13-Vowpal Wabbit version 6.1 &#038; the NIPS tutorial</a></p>
<p>10 0.49408939 <a title="436-lsi-10" href="../hunch_net-2007/hunch_net-2007-10-17-Online_as_the_new_adjective.html">267 hunch net-2007-10-17-Online as the new adjective</a></p>
<p>11 0.48948348 <a title="436-lsi-11" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>12 0.48793977 <a title="436-lsi-12" href="../hunch_net-2010/hunch_net-2010-12-04-Vowpal_Wabbit%2C_version_5.0%2C_and_the_second_heresy.html">419 hunch net-2010-12-04-Vowpal Wabbit, version 5.0, and the second heresy</a></p>
<p>13 0.4831253 <a title="436-lsi-13" href="../hunch_net-2010/hunch_net-2010-04-14-MLcomp%3A_a_website_for_objectively_comparing_ML_algorithms.html">393 hunch net-2010-04-14-MLcomp: a website for objectively comparing ML algorithms</a></p>
<p>14 0.47869936 <a title="436-lsi-14" href="../hunch_net-2005/hunch_net-2005-11-05-The_design_of_a_computing_cluster.html">128 hunch net-2005-11-05-The design of a computing cluster</a></p>
<p>15 0.47846505 <a title="436-lsi-15" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>16 0.46645346 <a title="436-lsi-16" href="../hunch_net-2007/hunch_net-2007-09-16-Optimizing_Machine_Learning_Programs.html">262 hunch net-2007-09-16-Optimizing Machine Learning Programs</a></p>
<p>17 0.46189418 <a title="436-lsi-17" href="../hunch_net-2007/hunch_net-2007-01-26-Parallel_Machine_Learning_Problems.html">229 hunch net-2007-01-26-Parallel Machine Learning Problems</a></p>
<p>18 0.44809195 <a title="436-lsi-18" href="../hunch_net-2005/hunch_net-2005-04-27-DARPA_project%3A_LAGR.html">63 hunch net-2005-04-27-DARPA project: LAGR</a></p>
<p>19 0.44202992 <a title="436-lsi-19" href="../hunch_net-2009/hunch_net-2009-08-03-Carbon_in_Computer_Science_Research.html">366 hunch net-2009-08-03-Carbon in Computer Science Research</a></p>
<p>20 0.43552101 <a title="436-lsi-20" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(42, 0.285), (67, 0.483)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.96690363 <a title="436-lda-1" href="../hunch_net-2005/hunch_net-2005-10-19-Workshop%3A_Atomic_Learning.html">124 hunch net-2005-10-19-Workshop: Atomic Learning</a></p>
<p>Introduction: We are planning to have a workshop on atomic learning Jan 7 & 8 at TTI-
Chicago.Details are here.The earlier request for interest ishere.The primary
deadline is abstracts due Nov. 20 to jl@tti-c.org.</p><p>2 0.8213225 <a title="436-lda-2" href="../hunch_net-2005/hunch_net-2005-04-16-Which_Assumptions_are_Reasonable%3F.html">57 hunch net-2005-04-16-Which Assumptions are Reasonable?</a></p>
<p>Introduction: One of the most confusing things about understanding learning theory is the
vast array of differing assumptions. Some critical thought about which of
these assumptions are reasonable for real-world problems may be useful.Before
we even start thinking about assumptions, it's important to realize that the
word hasmultiple meanings. The meaning used here is "assumption = axiom" (i.e.
something you can not verify).AssumptionReasonable?Which
analysis?Example/notesIndependent and Identically Distributed
DataSometimesPAC,ERM,Prediction bounds,statisticsTheKDD cup 2004 physics
datasetis plausibly IID data. There are a number of situations which are
"almost IID" in the sense that IID analysis results in correct intuitions.
Unreasonable in adversarial situations (stock market, war, etcâ&euro;Ś)Independently
Distributed DataMore than IID, but still only sometimesonline->batch
conversionLosing "identical" can be helpful in situations where you have a
cyclic process generating data.Finite exchangeability</p><p>same-blog 3 0.79006648 <a title="436-lda-3" href="../hunch_net-2011/hunch_net-2011-06-22-Ultra_LDA.html">436 hunch net-2011-06-22-Ultra LDA</a></p>
<p>Introduction: ShravanandAlex's LDA code isreleased. On a single machine, I'm not sure how it
currently compares to the online LDA inVW, but the ability to effectively
scale across very many machines is surely interesting.</p><p>4 0.62705308 <a title="436-lda-4" href="../hunch_net-2005/hunch_net-2005-03-21-Research_Styles_in_Machine_Learning.html">44 hunch net-2005-03-21-Research Styles in Machine Learning</a></p>
<p>Introduction: Machine Learning is a field with an impressively diverse set of reseearch
styles. Understanding this may be important in appreciating what you see at a
conference.Engineering. How can I solve this problem? People in the
engineering research style try to solve hard problems directly by any means
available and then describe how they did it. This is typical of problem-
specific conferences and communities.Scientific. What are the principles for
solving learning problems? People in this research style test techniques on
many different problems. This is fairly common at ICML and NIPS.Mathematical.
How can the learning problem be mathematically understood? People in this
research style prove theorems with implications for learning but often do not
implement (or test algorithms). COLT is a typical conference for this
style.Many people manage to cross these styles, and that is often
beneficial.Whenver we list a set of alternative, it becomes natural to think
"which is best?" In this case of le</p><p>5 0.59610939 <a title="436-lda-5" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>Introduction: Let's define a learning problem as making predictions given past data. There
are several ways to attack the learning problem which seem to be equivalent to
solving the learning problem.Find the InvariantThis viewpoint says that
learning is all about learning (or incorporating) transformations of objects
that do not change the correct prediction. The best possible invariant is the
one which says "all things of the same class are the same". Finding this is
equivalent to learning. This viewpoint is particularly common when working
with image features.Feature SelectionThis viewpoint says that the way to learn
is by finding the right features to input to a learning algorithm. The best
feature is the one which is the class to predict. Finding this is equivalent
to learning for all reasonable learning algorithms. This viewpoint is common
in several applications of machine learning. SeeGilad's and Bianca's
comments.Find the RepresentationThis is almost the same as feature selection,
except int</p><p>6 0.59041518 <a title="436-lda-6" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>7 0.52262402 <a title="436-lda-7" href="../hunch_net-2005/hunch_net-2005-05-06-Don%26%238217%3Bt_mix_the_solution_into_the_problem.html">67 hunch net-2005-05-06-Don&#8217;t mix the solution into the problem</a></p>
<p>8 0.51297849 <a title="436-lda-8" href="../hunch_net-2006/hunch_net-2006-01-18-Is_Multitask_Learning_Black-Boxable%3F.html">149 hunch net-2006-01-18-Is Multitask Learning Black-Boxable?</a></p>
<p>9 0.50882763 <a title="436-lda-9" href="../hunch_net-2008/hunch_net-2008-04-27-Watchword%3A_Supervised_Learning.html">299 hunch net-2008-04-27-Watchword: Supervised Learning</a></p>
<p>10 0.50846028 <a title="436-lda-10" href="../hunch_net-2006/hunch_net-2006-01-23-On_Coding_via_Mutual_Information_%26%23038%3B_Bayes_Nets.html">150 hunch net-2006-01-23-On Coding via Mutual Information &#038; Bayes Nets</a></p>
<p>11 0.50842768 <a title="436-lda-11" href="../hunch_net-2012/hunch_net-2012-10-26-ML_Symposium_and_Strata-Hadoop_World.html">475 hunch net-2012-10-26-ML Symposium and Strata-Hadoop World</a></p>
<p>12 0.5079245 <a title="436-lda-12" href="../hunch_net-2005/hunch_net-2005-12-09-Some_NIPS_papers.html">138 hunch net-2005-12-09-Some NIPS papers</a></p>
<p>13 0.50787336 <a title="436-lda-13" href="../hunch_net-2006/hunch_net-2006-06-15-IJCAI_is_out_of_season.html">184 hunch net-2006-06-15-IJCAI is out of season</a></p>
<p>14 0.50758088 <a title="436-lda-14" href="../hunch_net-2005/hunch_net-2005-03-29-Academic_Mechanism_Design.html">48 hunch net-2005-03-29-Academic Mechanism Design</a></p>
<p>15 0.50668693 <a title="436-lda-15" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>16 0.50657994 <a title="436-lda-16" href="../hunch_net-2005/hunch_net-2005-07-07-The_Limits_of_Learning_Theory.html">90 hunch net-2005-07-07-The Limits of Learning Theory</a></p>
<p>17 0.50520021 <a title="436-lda-17" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>18 0.50256717 <a title="436-lda-18" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>19 0.50245917 <a title="436-lda-19" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>20 0.50180906 <a title="436-lda-20" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
