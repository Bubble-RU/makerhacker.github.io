<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>431 hunch net-2011-04-18-A paper not at Snowbird</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2011" href="../home/hunch_net-2011_home.html">hunch_net-2011</a> <a title="hunch_net-2011-431" href="#">hunch_net-2011-431</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>431 hunch net-2011-04-18-A paper not at Snowbird</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2011-431-html" href="http://hunch.net/?p=1772">html</a></p><p>Introduction: Unfortunately, a scheduling failure meant I missed all ofAIStatand most of
thelearning workshop, otherwise known as Snowbird, when it's atSnowbird.At
snowbird, the talk onSum-Productnetworks byHoifung Poonstood out to me (Pedro
Domingosis a coauthor.). The basic point was that by appropriately
constructing networks based on sums and products, the normalization problem in
probabilistic models is eliminated, yielding a highly tractable yet flexible
representation+learning algorithm. As an algorithm, this is noticeably cleaner
than deep belief networks with a claim to being an order of magnitude faster
and working better on an image completion task.Snowbird doesn't have real
papers--just the abstract above. I look forward to seeing the paper. (added:
Rodrigo points out the deep learning workshopdraft.)</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Unfortunately, a scheduling failure meant I missed all ofAIStatand most of thelearning workshop, otherwise known as Snowbird, when it's atSnowbird. [sent-1, score-0.89]
</p><p>2 At snowbird, the talk onSum-Productnetworks byHoifung Poonstood out to me (Pedro Domingosis a coauthor. [sent-2, score-0.092]
</p><p>3 The basic point was that by appropriately constructing networks based on sums and products, the normalization problem in probabilistic models is eliminated, yielding a highly tractable yet flexible representation+learning algorithm. [sent-4, score-1.756]
</p><p>4 As an algorithm, this is noticeably cleaner than deep belief networks with a claim to being an order of magnitude faster and working better on an image completion task. [sent-5, score-1.619]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('snowbird', 0.458), ('networks', 0.23), ('cleaner', 0.203), ('products', 0.203), ('deep', 0.195), ('completion', 0.188), ('sums', 0.188), ('eliminated', 0.188), ('thelearning', 0.17), ('normalization', 0.17), ('seeing', 0.163), ('scheduling', 0.157), ('flexible', 0.152), ('constructing', 0.152), ('forward', 0.148), ('yielding', 0.148), ('image', 0.14), ('missed', 0.14), ('magnitude', 0.134), ('meant', 0.132), ('abstract', 0.124), ('added', 0.122), ('tractable', 0.117), ('faster', 0.113), ('probabilistic', 0.112), ('highly', 0.112), ('belief', 0.107), ('unfortunately', 0.107), ('failure', 0.107), ('representation', 0.106), ('claim', 0.105), ('otherwise', 0.101), ('points', 0.098), ('workshop', 0.095), ('talk', 0.092), ('look', 0.092), ('models', 0.088), ('known', 0.083), ('order', 0.08), ('working', 0.073), ('based', 0.068), ('yet', 0.065), ('point', 0.062), ('real', 0.061), ('basic', 0.054), ('better', 0.051), ('algorithm', 0.047), ('problem', 0.038), ('learning', 0.027)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="431-tfidf-1" href="../hunch_net-2011/hunch_net-2011-04-18-A_paper_not_at_Snowbird.html">431 hunch net-2011-04-18-A paper not at Snowbird</a></p>
<p>Introduction: Unfortunately, a scheduling failure meant I missed all ofAIStatand most of
thelearning workshop, otherwise known as Snowbird, when it's atSnowbird.At
snowbird, the talk onSum-Productnetworks byHoifung Poonstood out to me (Pedro
Domingosis a coauthor.). The basic point was that by appropriately
constructing networks based on sums and products, the normalization problem in
probabilistic models is eliminated, yielding a highly tractable yet flexible
representation+learning algorithm. As an algorithm, this is noticeably cleaner
than deep belief networks with a claim to being an order of magnitude faster
and working better on an image completion task.Snowbird doesn't have real
papers--just the abstract above. I look forward to seeing the paper. (added:
Rodrigo points out the deep learning workshopdraft.)</p><p>2 0.16975427 <a title="431-tfidf-2" href="../hunch_net-2010/hunch_net-2010-08-23-Boosted_Decision_Trees_for_Deep_Learning.html">407 hunch net-2010-08-23-Boosted Decision Trees for Deep Learning</a></p>
<p>Introduction: About 4 years ago, I speculated thatdecision trees qualify as a deep learning
algorithmbecause they can make decisions which are substantially nonlinear in
the input representation.Ping Lihasproved this correct, empiricallyatUAIby
showing that boosted decision trees can beat deep belief networks on versions
ofMnistwhich are artificially hardened so as to make them solvable only by
deep learning algorithms.This is an important point, because the ability to
solve these sorts of problems is probably the best objective definition of a
deep learning algorithm we have. I'm not that surprised. In my experience, if
you can accept the computational drawbacks of a boosted decision tree, they
can achieve pretty good performance.Geoff Hintononce told me that the great
thing about deep belief networks is that they work. I understand that Ping had
very substantial difficulty in getting this published, so I hope some
reviewers step up to the standard of valuing what works.</p><p>3 0.16121969 <a title="431-tfidf-3" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>Introduction: Many learning algorithms used in practice are fairly simple. Viewed
representationally, many prediction algorithms either compute a linear
separator of basic features (perceptron, winnow, weighted majority, SVM) or
perhaps a linear separator of slightly more complex features (2-layer neural
networks or kernelized SVMs). Should we go beyond this, and start using "deep"
representations?What is deep learning?Intuitively, deep learning is about
learning to predict in ways which can involve complex dependencies between the
input (observed) features.Specifying this more rigorously turns out to be
rather difficult. Consider the following cases:SVM with Gaussian Kernel. This
is not considered deep learning, because an SVM with a gaussian kernel can't
succinctly represent certain decision surfaces. One ofYann LeCun's examples is
recognizing objects based on pixel values. An SVM will need a new support
vector for each significantly different background. Since the number of
distinct backgrounds i</p><p>4 0.10708756 <a title="431-tfidf-4" href="../hunch_net-2007/hunch_net-2007-01-10-A_Deep_Belief_Net_Learning_Problem.html">227 hunch net-2007-01-10-A Deep Belief Net Learning Problem</a></p>
<p>Introduction: "Deep learning" is used to describe learning architectures which have
significant depth (as a circuit).One claimis that shallow architectures (one
or two layers) can not concisely represent some functions while a circuit with
more depth can concisely represent these same functions. Proving lower bounds
on the size of a circuit is substantially harder than upper bounds (which are
constructive), but some results are known.Luca Trevisan'sclass notesdetail how
XOR is not concisely representable by "AC0â&euro;ł (= constant depth unbounded fan-in
AND, OR, NOT gates). This doesn't quite prove that depth is necessary for the
representations commonly used in learning (such as a thresholded weighted
sum), but it is strongly suggestive that this is so.Examples like this are a
bit disheartening because existing algorithms for deep learning (deep belief
nets, gradient descent on deep neural networks, and a perhaps decision trees
depending on who you ask) can't learn XOR very easily. Evidence so far
sugges</p><p>5 0.10643557 <a title="431-tfidf-5" href="../hunch_net-2008/hunch_net-2008-12-07-A_NIPS_paper.html">330 hunch net-2008-12-07-A NIPS paper</a></p>
<p>Introduction: I'm skipping NIPS this year in favor ofAda, but I wanted to point outthis
paperbyAndriy MnihandGeoff Hinton. The basic claim of the paper is that by
carefully but automatically constructing a binary tree over words, it's
possible to predict words well with huge computational resource savings over
unstructured approaches.I'm interested in this beyond the application to word
prediction because it is relevant to the general normalization problem: If you
want to predict the probability of one of a large number of events, often you
must compute a predicted score for all the events and then normalize, a
computationally inefficient operation. The problem comes up in many places
using probabilistic models, but I've run into it with high-dimensional
regression.There are a couple workarounds for this computational
bug:Approximate. There are many ways. Often the approximations are
uncontrolled (i.e. can be arbitrarily bad), and hence finicky in
application.Avoid. You don't really want a probabili</p><p>6 0.10263363 <a title="431-tfidf-6" href="../hunch_net-2011/hunch_net-2011-07-11-Interesting_Neural_Network_Papers_at_ICML_2011.html">438 hunch net-2011-07-11-Interesting Neural Network Papers at ICML 2011</a></p>
<p>7 0.10092451 <a title="431-tfidf-7" href="../hunch_net-2011/hunch_net-2011-08-06-Interesting_thing_at_UAI_2011.html">440 hunch net-2011-08-06-Interesting thing at UAI 2011</a></p>
<p>8 0.092617676 <a title="431-tfidf-8" href="../hunch_net-2005/hunch_net-2005-02-09-Intuitions_from_applied_learning.html">16 hunch net-2005-02-09-Intuitions from applied learning</a></p>
<p>9 0.07275831 <a title="431-tfidf-9" href="../hunch_net-2013/hunch_net-2013-01-01-Deep_Learning_2012.html">477 hunch net-2013-01-01-Deep Learning 2012</a></p>
<p>10 0.069182113 <a title="431-tfidf-10" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>11 0.068724588 <a title="431-tfidf-11" href="../hunch_net-2005/hunch_net-2005-09-19-NIPS_Workshops.html">113 hunch net-2005-09-19-NIPS Workshops</a></p>
<p>12 0.067969993 <a title="431-tfidf-12" href="../hunch_net-2006/hunch_net-2006-12-12-Interesting_Papers_at_NIPS_2006.html">224 hunch net-2006-12-12-Interesting Papers at NIPS 2006</a></p>
<p>13 0.064502113 <a title="431-tfidf-13" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>14 0.063914835 <a title="431-tfidf-14" href="../hunch_net-2009/hunch_net-2009-12-27-Interesting_things_at_NIPS_2009.html">385 hunch net-2009-12-27-Interesting things at NIPS 2009</a></p>
<p>15 0.062139101 <a title="431-tfidf-15" href="../hunch_net-2006/hunch_net-2006-02-27-The_Peekaboom_Dataset.html">159 hunch net-2006-02-27-The Peekaboom Dataset</a></p>
<p>16 0.061182268 <a title="431-tfidf-16" href="../hunch_net-2007/hunch_net-2007-10-14-NIPS_workshp%3A_Learning_Problem_Design.html">265 hunch net-2007-10-14-NIPS workshp: Learning Problem Design</a></p>
<p>17 0.061140589 <a title="431-tfidf-17" href="../hunch_net-2005/hunch_net-2005-02-15-ESPgame_and_image_labeling.html">20 hunch net-2005-02-15-ESPgame and image labeling</a></p>
<p>18 0.060999524 <a title="431-tfidf-18" href="../hunch_net-2009/hunch_net-2009-04-21-Interesting_Presentations_at_Snowbird.html">349 hunch net-2009-04-21-Interesting Presentations at Snowbird</a></p>
<p>19 0.060652159 <a title="431-tfidf-19" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>20 0.059063196 <a title="431-tfidf-20" href="../hunch_net-2010/hunch_net-2010-12-04-Vowpal_Wabbit%2C_version_5.0%2C_and_the_second_heresy.html">419 hunch net-2010-12-04-Vowpal Wabbit, version 5.0, and the second heresy</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.111), (1, -0.018), (2, 0.024), (3, 0.051), (4, -0.097), (5, 0.072), (6, -0.088), (7, -0.031), (8, 0.056), (9, 0.064), (10, 0.167), (11, 0.003), (12, -0.081), (13, 0.145), (14, 0.028), (15, -0.013), (16, -0.063), (17, 0.071), (18, -0.006), (19, 0.049), (20, -0.016), (21, 0.061), (22, -0.049), (23, 0.019), (24, -0.036), (25, 0.002), (26, -0.069), (27, -0.066), (28, -0.013), (29, -0.002), (30, -0.022), (31, 0.001), (32, 0.073), (33, -0.053), (34, -0.037), (35, -0.056), (36, -0.005), (37, -0.022), (38, 0.027), (39, -0.0), (40, -0.001), (41, 0.049), (42, 0.025), (43, -0.102), (44, 0.093), (45, 0.018), (46, -0.072), (47, 0.069), (48, 0.022), (49, -0.074)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97668195 <a title="431-lsi-1" href="../hunch_net-2011/hunch_net-2011-04-18-A_paper_not_at_Snowbird.html">431 hunch net-2011-04-18-A paper not at Snowbird</a></p>
<p>Introduction: Unfortunately, a scheduling failure meant I missed all ofAIStatand most of
thelearning workshop, otherwise known as Snowbird, when it's atSnowbird.At
snowbird, the talk onSum-Productnetworks byHoifung Poonstood out to me (Pedro
Domingosis a coauthor.). The basic point was that by appropriately
constructing networks based on sums and products, the normalization problem in
probabilistic models is eliminated, yielding a highly tractable yet flexible
representation+learning algorithm. As an algorithm, this is noticeably cleaner
than deep belief networks with a claim to being an order of magnitude faster
and working better on an image completion task.Snowbird doesn't have real
papers--just the abstract above. I look forward to seeing the paper. (added:
Rodrigo points out the deep learning workshopdraft.)</p><p>2 0.7429235 <a title="431-lsi-2" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>Introduction: Many learning algorithms used in practice are fairly simple. Viewed
representationally, many prediction algorithms either compute a linear
separator of basic features (perceptron, winnow, weighted majority, SVM) or
perhaps a linear separator of slightly more complex features (2-layer neural
networks or kernelized SVMs). Should we go beyond this, and start using "deep"
representations?What is deep learning?Intuitively, deep learning is about
learning to predict in ways which can involve complex dependencies between the
input (observed) features.Specifying this more rigorously turns out to be
rather difficult. Consider the following cases:SVM with Gaussian Kernel. This
is not considered deep learning, because an SVM with a gaussian kernel can't
succinctly represent certain decision surfaces. One ofYann LeCun's examples is
recognizing objects based on pixel values. An SVM will need a new support
vector for each significantly different background. Since the number of
distinct backgrounds i</p><p>3 0.74259061 <a title="431-lsi-3" href="../hunch_net-2010/hunch_net-2010-08-23-Boosted_Decision_Trees_for_Deep_Learning.html">407 hunch net-2010-08-23-Boosted Decision Trees for Deep Learning</a></p>
<p>Introduction: About 4 years ago, I speculated thatdecision trees qualify as a deep learning
algorithmbecause they can make decisions which are substantially nonlinear in
the input representation.Ping Lihasproved this correct, empiricallyatUAIby
showing that boosted decision trees can beat deep belief networks on versions
ofMnistwhich are artificially hardened so as to make them solvable only by
deep learning algorithms.This is an important point, because the ability to
solve these sorts of problems is probably the best objective definition of a
deep learning algorithm we have. I'm not that surprised. In my experience, if
you can accept the computational drawbacks of a boosted decision tree, they
can achieve pretty good performance.Geoff Hintononce told me that the great
thing about deep belief networks is that they work. I understand that Ping had
very substantial difficulty in getting this published, so I hope some
reviewers step up to the standard of valuing what works.</p><p>4 0.69684589 <a title="431-lsi-4" href="../hunch_net-2011/hunch_net-2011-07-11-Interesting_Neural_Network_Papers_at_ICML_2011.html">438 hunch net-2011-07-11-Interesting Neural Network Papers at ICML 2011</a></p>
<p>Introduction: Maybe it's too early to call, but with four separate Neural Network sessions
at this year'sICML, it looks like Neural Networks are making a comeback. Here
are my highlights of these sessions. In general, my feeling is that these
papers both demystify deep learning and show its broader applicability.The
first observation I made is that the once disreputable "Neural" nomenclature
is being used againin lieu of"deep learning". Maybe it's because Adam Coates
et al. showed that single layer networks can work surprisingly well.An
Analysis of Single-Layer Networks in Unsupervised Feature Learning,Adam
Coates,Honglak Lee,Andrew Y. Ng(AISTATS 2011)The Importance of Encoding Versus
Training with Sparse Coding and Vector Quantization,Adam Coates,Andrew Y.
Ng(ICML 2011)Another surprising result out of Andrew Ng's group comes from
Andrew Saxe et al. who show that certain convolutional pooling architectures
can obtain close to state-of-the-art performance with random weights (that is,
without actuall</p><p>5 0.67842323 <a title="431-lsi-5" href="../hunch_net-2013/hunch_net-2013-01-01-Deep_Learning_2012.html">477 hunch net-2013-01-01-Deep Learning 2012</a></p>
<p>Introduction: 2012 was a tumultuous year for me, but it was undeniably a great year for deep
learning efforts. Signs of this include:Winning aKaggle competition.Wide
adoption ofdeep learning for speech recognition.Significantindustry
support.Gains inimagerecognition.This is a rare event in research: a
significant capability breakout. Congratulations are definitely in order for
those who managed to achieve it. At this point, deep learning algorithms seem
like a choice undeniably worth investigating for real applications with
significant data.</p><p>6 0.62345409 <a title="431-lsi-6" href="../hunch_net-2007/hunch_net-2007-01-10-A_Deep_Belief_Net_Learning_Problem.html">227 hunch net-2007-01-10-A Deep Belief Net Learning Problem</a></p>
<p>7 0.59481353 <a title="431-lsi-7" href="../hunch_net-2006/hunch_net-2006-12-12-Interesting_Papers_at_NIPS_2006.html">224 hunch net-2006-12-12-Interesting Papers at NIPS 2006</a></p>
<p>8 0.44956395 <a title="431-lsi-8" href="../hunch_net-2005/hunch_net-2005-02-09-Intuitions_from_applied_learning.html">16 hunch net-2005-02-09-Intuitions from applied learning</a></p>
<p>9 0.43018371 <a title="431-lsi-9" href="../hunch_net-2011/hunch_net-2011-08-06-Interesting_thing_at_UAI_2011.html">440 hunch net-2011-08-06-Interesting thing at UAI 2011</a></p>
<p>10 0.39404911 <a title="431-lsi-10" href="../hunch_net-2008/hunch_net-2008-11-28-A_Bumper_Crop_of_Machine_Learning_Graduates.html">329 hunch net-2008-11-28-A Bumper Crop of Machine Learning Graduates</a></p>
<p>11 0.3773002 <a title="431-lsi-11" href="../hunch_net-2006/hunch_net-2006-07-08-Some_recent_papers.html">192 hunch net-2006-07-08-Some recent papers</a></p>
<p>12 0.3694607 <a title="431-lsi-12" href="../hunch_net-2006/hunch_net-2006-11-22-Explicit_Randomization_in_Learning_algorithms.html">219 hunch net-2006-11-22-Explicit Randomization in Learning algorithms</a></p>
<p>13 0.36804056 <a title="431-lsi-13" href="../hunch_net-2009/hunch_net-2009-12-27-Interesting_things_at_NIPS_2009.html">385 hunch net-2009-12-27-Interesting things at NIPS 2009</a></p>
<p>14 0.36102843 <a title="431-lsi-14" href="../hunch_net-2007/hunch_net-2007-10-14-NIPS_workshp%3A_Learning_Problem_Design.html">265 hunch net-2007-10-14-NIPS workshp: Learning Problem Design</a></p>
<p>15 0.35468706 <a title="431-lsi-15" href="../hunch_net-2012/hunch_net-2012-02-24-ICML%2B50%25.html">456 hunch net-2012-02-24-ICML+50%</a></p>
<p>16 0.35183978 <a title="431-lsi-16" href="../hunch_net-2006/hunch_net-2006-01-30-Should_the_Input_Representation_be_a_Vector%3F.html">152 hunch net-2006-01-30-Should the Input Representation be a Vector?</a></p>
<p>17 0.35180536 <a title="431-lsi-17" href="../hunch_net-2006/hunch_net-2006-07-17-A_Winner.html">197 hunch net-2006-07-17-A Winner</a></p>
<p>18 0.34182706 <a title="431-lsi-18" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>19 0.34141421 <a title="431-lsi-19" href="../hunch_net-2006/hunch_net-2006-07-26-Two_more_UAI_papers_of_interest.html">199 hunch net-2006-07-26-Two more UAI papers of interest</a></p>
<p>20 0.32636899 <a title="431-lsi-20" href="../hunch_net-2009/hunch_net-2009-01-21-Nearly_all_natural_problems_require_nonlinearity.html">337 hunch net-2009-01-21-Nearly all natural problems require nonlinearity</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(35, 0.138), (39, 0.432), (42, 0.155), (68, 0.112), (74, 0.03)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.92986524 <a title="431-lda-1" href="../hunch_net-2005/hunch_net-2005-02-10-Conferences%2C_Dates%2C_Locations.html">17 hunch net-2005-02-10-Conferences, Dates, Locations</a></p>
<p>Introduction: ConferenceLocateDateCOLTBertinoro, ItalyJune 27-30AAAIPittsburgh, PA, USAJuly
9-13UAIEdinburgh, ScotlandJuly 26-29IJCAIEdinburgh, ScotlandJuly 30 - August
5ICMLBonn, GermanyAugust 7-11KDDChicago, IL, USAAugust 21-24The big winner
this year is Europe. This is partly a coincidence, and partly due to the
general internationalization of science over the last few years. Withcuts to
basic sciencein the US and increased hassle for visitors, conferences outside
the US become more attractive. Europe and Australia/New Zealand are the
immediate winners because they have the science, infrastructure, and english
in place. China and India are possible future winners.</p><p>same-blog 2 0.90111768 <a title="431-lda-2" href="../hunch_net-2011/hunch_net-2011-04-18-A_paper_not_at_Snowbird.html">431 hunch net-2011-04-18-A paper not at Snowbird</a></p>
<p>Introduction: Unfortunately, a scheduling failure meant I missed all ofAIStatand most of
thelearning workshop, otherwise known as Snowbird, when it's atSnowbird.At
snowbird, the talk onSum-Productnetworks byHoifung Poonstood out to me (Pedro
Domingosis a coauthor.). The basic point was that by appropriately
constructing networks based on sums and products, the normalization problem in
probabilistic models is eliminated, yielding a highly tractable yet flexible
representation+learning algorithm. As an algorithm, this is noticeably cleaner
than deep belief networks with a claim to being an order of magnitude faster
and working better on an image completion task.Snowbird doesn't have real
papers--just the abstract above. I look forward to seeing the paper. (added:
Rodrigo points out the deep learning workshopdraft.)</p><p>3 0.87094229 <a title="431-lda-3" href="../hunch_net-2005/hunch_net-2005-12-11-More_NIPS_Papers.html">139 hunch net-2005-12-11-More NIPS Papers</a></p>
<p>Introduction: Let me add to John's post with a few of my own favouritesfrom this year's
conference. First, let me say thatSanjoy's talk,Coarse Sample Complexity
Bounds for ActiveLearningwas also one of my favourites, as was theForgettron
paper.I also really enjoyed the last third ofChristos'talkon the complexity of
finding Nash equilibria.And, speaking of tagging, I thinkthe U.Mass Citeseer
replacement systemRexafrom the demo track is very cool.Finally, let me add my
recommendations for specific papers:Z. Ghahramani, K. Heller:Bayesian Sets[no
preprint](A very elegant probabilistic information retrieval style modelof
which objects are "most like" a given subset of objects.)T. Griffiths, Z.
Ghahramani:Infinite Latent Feature Models andthe Indian Buffet
Process[preprint](A Dirichlet style prior over infinite binary matrices
withbeautiful exchangeability properties.)K. Weinberger, J. Blitzer, L.
Saul:Distance Metric Learning forLarge Margin Nearest Neighbor
Classification[preprint](A nice idea about ho</p><p>4 0.86814362 <a title="431-lda-4" href="../hunch_net-2007/hunch_net-2007-07-13-The_View_From_China.html">255 hunch net-2007-07-13-The View From China</a></p>
<p>Introduction: I'm visiting Beijing for thePao-Lu Hsu Statistics Conferenceon Machine
Learning.I had several discussions about the state of Chinese research. Given
the large population and economy, you might expect substantial research--more
than has been observed at international conferences. The fundamental problem
seems to be theCultural Revolutionwhich lobotimized higher education, and the
research associated with it. There has been a process of slow recovery since
then, which has begun to be felt in the research world via increased
participation in international conferences and (now) conferences in China.The
amount of effort going into construction in Beijing is very impressive--people
are literally building a skyscraper at night outside the window of the hotel
I'm staying at (and this is not unusual). If a small fraction of this effort
is later focused onto supporting research, the effect could be very
substantial. General growth in China's research portfolio should be expected.</p><p>5 0.86434042 <a title="431-lda-5" href="../hunch_net-2006/hunch_net-2006-04-17-Rexa_is_live.html">173 hunch net-2006-04-17-Rexa is live</a></p>
<p>Introduction: Rexais now publicly available. Anyone can create an account and login.Rexa is
similar toCiteseerandGoogle Scholarin functionality with more emphasis on the
use of machine learning for intelligent information extraction. For example,
Rexa can automatically display a picture on an author's homepage when the
author is searched for.</p><p>6 0.83592671 <a title="431-lda-6" href="../hunch_net-2005/hunch_net-2005-05-14-NIPS.html">71 hunch net-2005-05-14-NIPS</a></p>
<p>7 0.70615882 <a title="431-lda-7" href="../hunch_net-2006/hunch_net-2006-01-25-1_year.html">151 hunch net-2006-01-25-1 year</a></p>
<p>8 0.58069277 <a title="431-lda-8" href="../hunch_net-2006/hunch_net-2006-06-30-ICML_papers.html">188 hunch net-2006-06-30-ICML papers</a></p>
<p>9 0.49361506 <a title="431-lda-9" href="../hunch_net-2006/hunch_net-2006-09-12-Incentive_Compatible_Reviewing.html">207 hunch net-2006-09-12-Incentive Compatible Reviewing</a></p>
<p>10 0.45874113 <a title="431-lda-10" href="../hunch_net-2007/hunch_net-2007-08-12-Exponentiated_Gradient.html">258 hunch net-2007-08-12-Exponentiated Gradient</a></p>
<p>11 0.45492586 <a title="431-lda-11" href="../hunch_net-2005/hunch_net-2005-06-13-Wikis_for_Summer_Schools_and_Workshops.html">81 hunch net-2005-06-13-Wikis for Summer Schools and Workshops</a></p>
<p>12 0.45076168 <a title="431-lda-12" href="../hunch_net-2005/hunch_net-2005-05-21-What_is_the_right_form_of_modularity_in_structured_prediction%3F.html">74 hunch net-2005-05-21-What is the right form of modularity in structured prediction?</a></p>
<p>13 0.45011875 <a title="431-lda-13" href="../hunch_net-2006/hunch_net-2006-05-16-The_value_of_the_orthodox_view_of_Boosting.html">179 hunch net-2006-05-16-The value of the orthodox view of Boosting</a></p>
<p>14 0.43781772 <a title="431-lda-14" href="../hunch_net-2005/hunch_net-2005-05-29-Maximum_Margin_Mismatch%3F.html">77 hunch net-2005-05-29-Maximum Margin Mismatch?</a></p>
<p>15 0.43689466 <a title="431-lda-15" href="../hunch_net-2007/hunch_net-2007-01-15-The_Machine_Learning_Department.html">228 hunch net-2007-01-15-The Machine Learning Department</a></p>
<p>16 0.4339346 <a title="431-lda-16" href="../hunch_net-2011/hunch_net-2011-07-11-Interesting_Neural_Network_Papers_at_ICML_2011.html">438 hunch net-2011-07-11-Interesting Neural Network Papers at ICML 2011</a></p>
<p>17 0.42693308 <a title="431-lda-17" href="../hunch_net-2006/hunch_net-2006-06-24-Online_convex_optimization_at_COLT.html">186 hunch net-2006-06-24-Online convex optimization at COLT</a></p>
<p>18 0.42561647 <a title="431-lda-18" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>19 0.42477334 <a title="431-lda-19" href="../hunch_net-2009/hunch_net-2009-02-04-Optimal_Proxy_Loss_for_Classification.html">341 hunch net-2009-02-04-Optimal Proxy Loss for Classification</a></p>
<p>20 0.42249811 <a title="431-lda-20" href="../hunch_net-2014/hunch_net-2014-02-16-Metacademy%3A_a_package_manager_for_knowledge.html">493 hunch net-2014-02-16-Metacademy: a package manager for knowledge</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
