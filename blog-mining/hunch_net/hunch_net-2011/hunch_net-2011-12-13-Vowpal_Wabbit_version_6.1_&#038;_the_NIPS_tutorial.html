<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>451 hunch net-2011-12-13-Vowpal Wabbit version 6.1 &#038; the NIPS tutorial</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2011" href="../home/hunch_net-2011_home.html">hunch_net-2011</a> <a title="hunch_net-2011-451" href="#">hunch_net-2011-451</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>451 hunch net-2011-12-13-Vowpal Wabbit version 6.1 &#038; the NIPS tutorial</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2011-451-html" href="http://hunch.net/?p=2159">html</a></p><p>Introduction: I just madeversion 6.1ofVowpal Wabbit. Relative to6.0, there are few new
features, but many refinements.The cluster parallel learning code better
supports multiple simultaneous runs, and other forms of parallelism have been
mostly removed. This incidentally significantly simplifies the learning
core.The online learning algorithms are more general, with support for l1(via
a truncated gradient variant) and l2regularization, and a generalized form of
variable metric learning.There is a solid persistent server mode which can
train online, as well as serve answers to many simultaneous queries, either in
text or binary.This should be a very good release if you are just getting
started, as we've made it compile more automatically out of the box, have
several newexamplesand updated documentation.Aspertradition, we're planning to
do a tutorial at NIPS during the break at theparallel learning workshopat 2pm
Spanish time Friday. I'll cover the basics, leaving the fun stuff for
others.Mirowill cov</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The cluster parallel learning code better supports multiple simultaneous runs, and other forms of parallelism have been mostly removed. [sent-5, score-1.086]
</p><p>2 The online learning algorithms are more general, with support for l1(via a truncated gradient variant) and l2regularization, and a generalized form of variable metric learning. [sent-7, score-0.472]
</p><p>3 There is a solid persistent server mode which can train online, as well as serve answers to many simultaneous queries, either in text or binary. [sent-8, score-1.225]
</p><p>4 This should be a very good release if you are just getting started, as we've made it compile more automatically out of the box, have several newexamplesand updated documentation. [sent-9, score-0.468]
</p><p>5 Aspertradition, we're planning to do a tutorial at NIPS during the break at theparallel learning workshopat 2pm Spanish time Friday. [sent-10, score-0.462]
</p><p>6 I'll cover the basics, leaving the fun stuff for others. [sent-11, score-0.645]
</p><p>7 We have found this works quite well amongst batch learning algorithms. [sent-13, score-0.101]
</p><p>8 If you have access to a large cluster, VW is orders of magnitude faster than any other public learning system accomplishing linear prediction. [sent-15, score-0.378]
</p><p>9 And if you are as impatient as I am, it is a real pleasure when the computers can keep up with you. [sent-16, score-0.191]
</p><p>10 This will be recorded, so it will hopefully be available for viewing online before too long. [sent-17, score-0.37]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cover', 0.292), ('simultaneous', 0.25), ('cluster', 0.202), ('parallel', 0.191), ('serve', 0.156), ('simplifies', 0.156), ('workshopat', 0.156), ('online', 0.154), ('persistent', 0.145), ('accomplishing', 0.145), ('stuff', 0.137), ('orders', 0.13), ('compile', 0.13), ('recorded', 0.13), ('parallelism', 0.13), ('box', 0.125), ('viewing', 0.125), ('supports', 0.125), ('server', 0.125), ('updated', 0.125), ('solid', 0.125), ('queries', 0.121), ('incidentally', 0.121), ('release', 0.117), ('runs', 0.117), ('leaving', 0.117), ('generalized', 0.114), ('break', 0.111), ('train', 0.111), ('vw', 0.111), ('mode', 0.108), ('text', 0.108), ('variant', 0.106), ('metric', 0.103), ('magnitude', 0.103), ('soon', 0.103), ('tutorial', 0.103), ('batch', 0.101), ('variable', 0.101), ('computers', 0.099), ('fun', 0.099), ('forms', 0.097), ('answers', 0.097), ('automatically', 0.096), ('keep', 0.092), ('planning', 0.092), ('mostly', 0.091), ('hopefully', 0.091), ('relative', 0.09), ('created', 0.087)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="451-tfidf-1" href="../hunch_net-2011/hunch_net-2011-12-13-Vowpal_Wabbit_version_6.1_%26%23038%3B_the_NIPS_tutorial.html">451 hunch net-2011-12-13-Vowpal Wabbit version 6.1 &#038; the NIPS tutorial</a></p>
<p>Introduction: I just madeversion 6.1ofVowpal Wabbit. Relative to6.0, there are few new
features, but many refinements.The cluster parallel learning code better
supports multiple simultaneous runs, and other forms of parallelism have been
mostly removed. This incidentally significantly simplifies the learning
core.The online learning algorithms are more general, with support for l1(via
a truncated gradient variant) and l2regularization, and a generalized form of
variable metric learning.There is a solid persistent server mode which can
train online, as well as serve answers to many simultaneous queries, either in
text or binary.This should be a very good release if you are just getting
started, as we've made it compile more automatically out of the box, have
several newexamplesand updated documentation.Aspertradition, we're planning to
do a tutorial at NIPS during the break at theparallel learning workshopat 2pm
Spanish time Friday. I'll cover the basics, leaving the fun stuff for
others.Mirowill cov</p><p>2 0.14216752 <a title="451-tfidf-2" href="../hunch_net-2011/hunch_net-2011-08-15-Vowpal_Wabbit_6.0.html">441 hunch net-2011-08-15-Vowpal Wabbit 6.0</a></p>
<p>Introduction: I just releasedVowpal Wabbit 6.0. Since the last version:VW is now 2-3 orders
of magnitude faster at linear learning, primarily thanks toAlekh. Given the
baseline, this is loads of fun, allowing us to easily deal with terafeature
datasets, and dwarfing the scale of any other open source projects. The core
improvement here comes from effective parallelization over kilonode clusters
(eitherHadoopor not). This code is highly scalable, so it even helps with
clusters of size 2 (and doesn't hurt for clusters of size 1). The core
allreduce technique appears widely and easily reused--we've already used it to
parallelize Conjugate Gradient, LBFGS, and two variants of online learning.
We'll be documenting how to do this more thoroughly, but for now
"README_cluster" and associated scripts should provide a good starting
point.The newLBFGScode fromMiroseems to commonly dominate the existing
conjugate gradient code in time/quality tradeoffs.The new matrix factorization
code fromJakeadds a core algor</p><p>3 0.13994578 <a title="451-tfidf-3" href="../hunch_net-2009/hunch_net-2009-12-07-Vowpal_Wabbit_version_4.0%2C_and_a_NIPS_heresy.html">381 hunch net-2009-12-07-Vowpal Wabbit version 4.0, and a NIPS heresy</a></p>
<p>Introduction: I'm releasingversion 4.0(tarball) ofVowpal Wabbit. The biggest change (by far)
in this release is experimental support for cluster parallelism, with notable
help fromDaniel Hsu.I also took advantage of the major version number to
introduce some incompatible changes, including switching tomurmurhash 2, and
other alterations to cachefiles. You'll need to delete and regenerate them. In
addition, the precise specification for a "tag" (i.e. string that can be used
to identify an example) changed--you can't have a space between the tag and
the '|' at the beginning of the feature namespace.And, of course, we made it
faster.For the future, I put up mytodo listoutlining the major future
improvements I want to see in the code. I'm planning to discuss the current
mechanism and results of the cluster parallel implementation at thelarge scale
machine learning workshopatNIPSlater this week. Several people have asked me
to do a tutorial/walkthrough of VW, which is arranged for friday 2pm in the
works</p><p>4 0.13868573 <a title="451-tfidf-4" href="../hunch_net-2009/hunch_net-2009-07-31-Vowpal_Wabbit_Open_Source_Project.html">365 hunch net-2009-07-31-Vowpal Wabbit Open Source Project</a></p>
<p>Introduction: Today brings a new release of theVowpal Wabbitfast online learning software.
This time, unlike the previous release, the project itself is going open
source, developing viagithub. For example, the lastest and greatest can be
downloaded via:git clone git://github.com/JohnLangford/vowpal_wabbit.gitIf you
aren't familiar withgit, it's a distributed version control system which
supports quick and easy branching, as well as reconciliation.This version of
the code is confirmed to compile without complaint on at least some flavors of
OSX as well as Linux boxes.As much of the point of this project is pushing the
limits of fast and effective machine learning, let me mention a few datapoints
from my experience.The program can effectively scale up to batch-style
training on sparse terafeature (i.e. 1012sparse feature) size datasets. The
limiting factor is typically i/o.I started using the the real datasets from
thelarge-scale learningworkshop as a convenient benchmark. The largest dataset
takes a</p><p>5 0.13593413 <a title="451-tfidf-5" href="../hunch_net-2009/hunch_net-2009-03-18-Parallel_ML_primitives.html">346 hunch net-2009-03-18-Parallel ML primitives</a></p>
<p>Introduction: Previously, we discussedparallel machine learninga bit. As parallel ML is
rather difficult, I'd like to describe my thinking at the moment, and ask for
advice from the rest of the world. This is particularly relevant right now, as
I'm attending a workshop tomorrow on parallel ML.Parallelizing slow algorithms
seems uncompelling. Parallelizing many algorithms also seems uncompelling,
because the effort required to parallelize is substantial. This leaves the
question: Which one fast algorithm is the best to parallelize? What is a
substantially different second?One compellingly fast simple algorithm is
online gradient descent on a linear representation. This is the core of
Leon'ssgdcode andVowpal Wabbit.Antoine Bordesshowed a variant was competitive
in thelarge scale learning challenge. It's also a decades old primitive which
has been reused in many algorithms, and continues to be reused. It also
applies to onlinelearningrather than just onlineoptimization, implying the
algorithm can be us</p><p>6 0.13426788 <a title="451-tfidf-6" href="../hunch_net-2013/hunch_net-2013-12-01-NIPS_tutorials_and_Vowpal_Wabbit_7.4.html">492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</a></p>
<p>7 0.13109206 <a title="451-tfidf-7" href="../hunch_net-2010/hunch_net-2010-08-20-The_Workshop_on_Cores%2C_Clusters%2C_and_Clouds.html">404 hunch net-2010-08-20-The Workshop on Cores, Clusters, and Clouds</a></p>
<p>8 0.12919192 <a title="451-tfidf-8" href="../hunch_net-2007/hunch_net-2007-10-17-Online_as_the_new_adjective.html">267 hunch net-2007-10-17-Online as the new adjective</a></p>
<p>9 0.11636557 <a title="451-tfidf-9" href="../hunch_net-2010/hunch_net-2010-12-04-Vowpal_Wabbit%2C_version_5.0%2C_and_the_second_heresy.html">419 hunch net-2010-12-04-Vowpal Wabbit, version 5.0, and the second heresy</a></p>
<p>10 0.11522712 <a title="451-tfidf-10" href="../hunch_net-2007/hunch_net-2007-12-21-Vowpal_Wabbit_Code_Release.html">281 hunch net-2007-12-21-Vowpal Wabbit Code Release</a></p>
<p>11 0.1144643 <a title="451-tfidf-11" href="../hunch_net-2007/hunch_net-2007-01-26-Parallel_Machine_Learning_Problems.html">229 hunch net-2007-01-26-Parallel Machine Learning Problems</a></p>
<p>12 0.1136548 <a title="451-tfidf-12" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>13 0.10289793 <a title="451-tfidf-13" href="../hunch_net-2011/hunch_net-2011-03-27-Vowpal_Wabbit%2C_v5.1.html">428 hunch net-2011-03-27-Vowpal Wabbit, v5.1</a></p>
<p>14 0.10224932 <a title="451-tfidf-14" href="../hunch_net-2011/hunch_net-2011-08-20-The_Large_Scale_Learning_Survey_Tutorial.html">442 hunch net-2011-08-20-The Large Scale Learning Survey Tutorial</a></p>
<p>15 0.098820195 <a title="451-tfidf-15" href="../hunch_net-2013/hunch_net-2013-01-07-NYU_Large_Scale_Machine_Learning_Class.html">478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</a></p>
<p>16 0.095668837 <a title="451-tfidf-16" href="../hunch_net-2009/hunch_net-2009-08-03-Carbon_in_Computer_Science_Research.html">366 hunch net-2009-08-03-Carbon in Computer Science Research</a></p>
<p>17 0.095470771 <a title="451-tfidf-17" href="../hunch_net-2012/hunch_net-2012-09-29-Vowpal_Wabbit%2C_version_7.0.html">473 hunch net-2012-09-29-Vowpal Wabbit, version 7.0</a></p>
<p>18 0.090403169 <a title="451-tfidf-18" href="../hunch_net-2009/hunch_net-2009-12-09-Future_Publication_Models_%40_NIPS.html">382 hunch net-2009-12-09-Future Publication Models @ NIPS</a></p>
<p>19 0.09014678 <a title="451-tfidf-19" href="../hunch_net-2007/hunch_net-2007-12-19-Cool_and_interesting_things_seen_at_NIPS.html">279 hunch net-2007-12-19-Cool and interesting things seen at NIPS</a></p>
<p>20 0.085441567 <a title="451-tfidf-20" href="../hunch_net-2008/hunch_net-2008-01-28-Sufficient_Computation.html">287 hunch net-2008-01-28-Sufficient Computation</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.164), (1, -0.028), (2, 0.084), (3, 0.05), (4, -0.069), (5, 0.205), (6, 0.191), (7, 0.044), (8, 0.026), (9, 0.103), (10, -0.002), (11, 0.005), (12, 0.008), (13, -0.113), (14, -0.057), (15, -0.039), (16, 0.029), (17, -0.067), (18, -0.017), (19, -0.054), (20, -0.034), (21, -0.071), (22, -0.052), (23, 0.154), (24, 0.028), (25, -0.065), (26, -0.073), (27, 0.045), (28, -0.025), (29, 0.007), (30, -0.044), (31, 0.037), (32, 0.079), (33, 0.05), (34, 0.043), (35, -0.031), (36, 0.02), (37, 0.059), (38, -0.066), (39, 0.028), (40, 0.037), (41, 0.027), (42, 0.107), (43, -0.086), (44, 0.026), (45, 0.044), (46, -0.114), (47, 0.094), (48, 0.024), (49, 0.0)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.93859798 <a title="451-lsi-1" href="../hunch_net-2011/hunch_net-2011-12-13-Vowpal_Wabbit_version_6.1_%26%23038%3B_the_NIPS_tutorial.html">451 hunch net-2011-12-13-Vowpal Wabbit version 6.1 &#038; the NIPS tutorial</a></p>
<p>Introduction: I just madeversion 6.1ofVowpal Wabbit. Relative to6.0, there are few new
features, but many refinements.The cluster parallel learning code better
supports multiple simultaneous runs, and other forms of parallelism have been
mostly removed. This incidentally significantly simplifies the learning
core.The online learning algorithms are more general, with support for l1(via
a truncated gradient variant) and l2regularization, and a generalized form of
variable metric learning.There is a solid persistent server mode which can
train online, as well as serve answers to many simultaneous queries, either in
text or binary.This should be a very good release if you are just getting
started, as we've made it compile more automatically out of the box, have
several newexamplesand updated documentation.Aspertradition, we're planning to
do a tutorial at NIPS during the break at theparallel learning workshopat 2pm
Spanish time Friday. I'll cover the basics, leaving the fun stuff for
others.Mirowill cov</p><p>2 0.77395177 <a title="451-lsi-2" href="../hunch_net-2009/hunch_net-2009-12-07-Vowpal_Wabbit_version_4.0%2C_and_a_NIPS_heresy.html">381 hunch net-2009-12-07-Vowpal Wabbit version 4.0, and a NIPS heresy</a></p>
<p>Introduction: I'm releasingversion 4.0(tarball) ofVowpal Wabbit. The biggest change (by far)
in this release is experimental support for cluster parallelism, with notable
help fromDaniel Hsu.I also took advantage of the major version number to
introduce some incompatible changes, including switching tomurmurhash 2, and
other alterations to cachefiles. You'll need to delete and regenerate them. In
addition, the precise specification for a "tag" (i.e. string that can be used
to identify an example) changed--you can't have a space between the tag and
the '|' at the beginning of the feature namespace.And, of course, we made it
faster.For the future, I put up mytodo listoutlining the major future
improvements I want to see in the code. I'm planning to discuss the current
mechanism and results of the cluster parallel implementation at thelarge scale
machine learning workshopatNIPSlater this week. Several people have asked me
to do a tutorial/walkthrough of VW, which is arranged for friday 2pm in the
works</p><p>3 0.68787014 <a title="451-lsi-3" href="../hunch_net-2010/hunch_net-2010-12-04-Vowpal_Wabbit%2C_version_5.0%2C_and_the_second_heresy.html">419 hunch net-2010-12-04-Vowpal Wabbit, version 5.0, and the second heresy</a></p>
<p>Introduction: I've releasedversion 5.0of theVowpal Wabbitonline learning software. The major
number has changed since thelast releasebecause I regard all earlier versions
as obsolete--there are several new algorithms & features including substantial
changes and upgrades to the default learning algorithm.The biggest changes are
new algorithms:Nikosand I improved the default algorithm. The basic update
rule still uses gradient descent, but the size of the update is carefully
controlled so that it's impossible to overrun the label. In addition, the
normalization has changed. Computationally, these changes are virtually free
and yield better results, sometimes much better. Less careful updates can be
reenabled with -loss_function classic, although results are still not
identical to previous due to normalization changes.Nikos also implemented the
per-feature learning rates as per thesetwopapers. Often, this works better
than the default algorithm. It isn't the default because it isn't (yet) as
adaptable</p><p>4 0.65884089 <a title="451-lsi-4" href="../hunch_net-2012/hunch_net-2012-09-29-Vowpal_Wabbit%2C_version_7.0.html">473 hunch net-2012-09-29-Vowpal Wabbit, version 7.0</a></p>
<p>Introduction: A new version ofVWisout. The primary changes are:Learning Reductions: I've
wanted to getlearning reductionsworking and we've finally done it. Not
everything is implemented yet, but VW now supports direct:Multiclass
Classification-oaaor-ect.Cost Sensitive Multiclass Classification-csoaaor-
wap.Contextual Bandit Classification-cb.Sequential Structured Prediction-
searnor-daggerIn addition, it is now easy to build your own custom learning
reductions for various plausible uses: feature diddling, custom structured
prediction problems, or alternate learning reductions. This effort is far from
done, but it is now in a generally useful state. Note that all learning
reductions inherit the ability to do cluster parallel learning.Library
interface: VW now has a basic library interface. The library provides most of
the functionality of VW, with the limitation that it is monolithic and
nonreentrant. These will be improved over time.Windows port: The priority of a
windows port jumped way up once we</p><p>5 0.65063196 <a title="451-lsi-5" href="../hunch_net-2011/hunch_net-2011-08-15-Vowpal_Wabbit_6.0.html">441 hunch net-2011-08-15-Vowpal Wabbit 6.0</a></p>
<p>Introduction: I just releasedVowpal Wabbit 6.0. Since the last version:VW is now 2-3 orders
of magnitude faster at linear learning, primarily thanks toAlekh. Given the
baseline, this is loads of fun, allowing us to easily deal with terafeature
datasets, and dwarfing the scale of any other open source projects. The core
improvement here comes from effective parallelization over kilonode clusters
(eitherHadoopor not). This code is highly scalable, so it even helps with
clusters of size 2 (and doesn't hurt for clusters of size 1). The core
allreduce technique appears widely and easily reused--we've already used it to
parallelize Conjugate Gradient, LBFGS, and two variants of online learning.
We'll be documenting how to do this more thoroughly, but for now
"README_cluster" and associated scripts should provide a good starting
point.The newLBFGScode fromMiroseems to commonly dominate the existing
conjugate gradient code in time/quality tradeoffs.The new matrix factorization
code fromJakeadds a core algor</p><p>6 0.6231522 <a title="451-lsi-6" href="../hunch_net-2011/hunch_net-2011-08-20-The_Large_Scale_Learning_Survey_Tutorial.html">442 hunch net-2011-08-20-The Large Scale Learning Survey Tutorial</a></p>
<p>7 0.61427784 <a title="451-lsi-7" href="../hunch_net-2013/hunch_net-2013-12-01-NIPS_tutorials_and_Vowpal_Wabbit_7.4.html">492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</a></p>
<p>8 0.61318201 <a title="451-lsi-8" href="../hunch_net-2009/hunch_net-2009-03-18-Parallel_ML_primitives.html">346 hunch net-2009-03-18-Parallel ML primitives</a></p>
<p>9 0.60780942 <a title="451-lsi-9" href="../hunch_net-2009/hunch_net-2009-07-31-Vowpal_Wabbit_Open_Source_Project.html">365 hunch net-2009-07-31-Vowpal Wabbit Open Source Project</a></p>
<p>10 0.58476561 <a title="451-lsi-10" href="../hunch_net-2007/hunch_net-2007-10-17-Online_as_the_new_adjective.html">267 hunch net-2007-10-17-Online as the new adjective</a></p>
<p>11 0.55697107 <a title="451-lsi-11" href="../hunch_net-2010/hunch_net-2010-08-20-The_Workshop_on_Cores%2C_Clusters%2C_and_Clouds.html">404 hunch net-2010-08-20-The Workshop on Cores, Clusters, and Clouds</a></p>
<p>12 0.5419383 <a title="451-lsi-12" href="../hunch_net-2007/hunch_net-2007-12-21-Vowpal_Wabbit_Code_Release.html">281 hunch net-2007-12-21-Vowpal Wabbit Code Release</a></p>
<p>13 0.51258612 <a title="451-lsi-13" href="../hunch_net-2011/hunch_net-2011-03-27-Vowpal_Wabbit%2C_v5.1.html">428 hunch net-2011-03-27-Vowpal Wabbit, v5.1</a></p>
<p>14 0.50016063 <a title="451-lsi-14" href="../hunch_net-2005/hunch_net-2005-04-08-Fast_SVMs.html">54 hunch net-2005-04-08-Fast SVMs</a></p>
<p>15 0.49623996 <a title="451-lsi-15" href="../hunch_net-2007/hunch_net-2007-01-26-Parallel_Machine_Learning_Problems.html">229 hunch net-2007-01-26-Parallel Machine Learning Problems</a></p>
<p>16 0.48153695 <a title="451-lsi-16" href="../hunch_net-2008/hunch_net-2008-04-30-Concerns_about_the_Large_Scale_Learning_Challenge.html">300 hunch net-2008-04-30-Concerns about the Large Scale Learning Challenge</a></p>
<p>17 0.47306216 <a title="451-lsi-17" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>18 0.45550671 <a title="451-lsi-18" href="../hunch_net-2011/hunch_net-2011-06-22-Ultra_LDA.html">436 hunch net-2011-06-22-Ultra LDA</a></p>
<p>19 0.42435279 <a title="451-lsi-19" href="../hunch_net-2013/hunch_net-2013-01-07-NYU_Large_Scale_Machine_Learning_Class.html">478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</a></p>
<p>20 0.4235594 <a title="451-lsi-20" href="../hunch_net-2009/hunch_net-2009-08-03-Carbon_in_Computer_Science_Research.html">366 hunch net-2009-08-03-Carbon in Computer Science Research</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(35, 0.08), (42, 0.141), (45, 0.096), (74, 0.121), (88, 0.431), (95, 0.032)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.93072313 <a title="451-lda-1" href="../hunch_net-2005/hunch_net-2005-04-08-Fast_SVMs.html">54 hunch net-2005-04-08-Fast SVMs</a></p>
<p>Introduction: There was apresentation at snowbirdabout parallelized support vector machines.
In many cases, people parallelize by ignoring serial operations, but that is
not what happened here--they parallelize with optimizations. Consequently,
this seems to be the fastest SVM in existence.There is a relatedpaper here.</p><p>2 0.92685926 <a title="451-lda-2" href="../hunch_net-2011/hunch_net-2011-04-06-COLT_open_questions.html">429 hunch net-2011-04-06-COLT open questions</a></p>
<p>Introduction: AlinaandJakepoint out the COLTCall for Open Questionsdue May 11. In general,
this is cool, and worth doing if you can come up with a crisp question. In my
case, I particularly enjoyedcrafting an open questionwith precisely a form
such that acritic targeting my paperswould be forced to confront their fallacy
or make a case for the reward. But less esoterically, this is a way to get the
attention of some very smart people focused on a problem that really matters,
which is the real value.</p><p>3 0.92008007 <a title="451-lda-3" href="../hunch_net-2012/hunch_net-2012-12-29-Simons_Institute_Big_Data_Program.html">476 hunch net-2012-12-29-Simons Institute Big Data Program</a></p>
<p>Introduction: Michael Jordansends the below:The newSimons Institute for the Theory of
Computingwill begin organizing semester-long programs starting in 2013.One of
our first programs, set for Fall 2013, will be on the "Theoretical
Foundationsof Big Data Analysis". The organizers of this program are Michael
Jordan (chair),Stephen Boyd, Peter Buehlmann, Ravi Kannan, Michael Mahoney,
and Muthu
Muthukrishnan.Seehttp://simons.berkeley.edu/program_bigdata2013.htmlfor more
information onthe program.The Simons Institute has created a number of
"Research Fellowships" for youngresearchers (within at most six years of the
award of their PhD) who wish toparticipate in Institute programs, including
the Big Data program. Individualswho already hold postdoctoral positions or
who are junior faculty are welcometo apply, as are finishing PhDs.Please note
that the application deadline is January 15, 2013. Further detailsare
available athttp://simons.berkeley.edu/fellows.html.Mike Jordan</p><p>4 0.90992665 <a title="451-lda-4" href="../hunch_net-2008/hunch_net-2008-11-11-COLT_CFP.html">326 hunch net-2008-11-11-COLT CFP</a></p>
<p>Introduction: Adam Klivans, points out theCOLT call for papers. The important points are:Due
Feb 13.Montreal, June 18-21.This year, there is author feedback.</p><p>same-blog 5 0.89475912 <a title="451-lda-5" href="../hunch_net-2011/hunch_net-2011-12-13-Vowpal_Wabbit_version_6.1_%26%23038%3B_the_NIPS_tutorial.html">451 hunch net-2011-12-13-Vowpal Wabbit version 6.1 &#038; the NIPS tutorial</a></p>
<p>Introduction: I just madeversion 6.1ofVowpal Wabbit. Relative to6.0, there are few new
features, but many refinements.The cluster parallel learning code better
supports multiple simultaneous runs, and other forms of parallelism have been
mostly removed. This incidentally significantly simplifies the learning
core.The online learning algorithms are more general, with support for l1(via
a truncated gradient variant) and l2regularization, and a generalized form of
variable metric learning.There is a solid persistent server mode which can
train online, as well as serve answers to many simultaneous queries, either in
text or binary.This should be a very good release if you are just getting
started, as we've made it compile more automatically out of the box, have
several newexamplesand updated documentation.Aspertradition, we're planning to
do a tutorial at NIPS during the break at theparallel learning workshopat 2pm
Spanish time Friday. I'll cover the basics, leaving the fun stuff for
others.Mirowill cov</p><p>6 0.8092227 <a title="451-lda-6" href="../hunch_net-2006/hunch_net-2006-07-06-Branch_Prediction_Competition.html">190 hunch net-2006-07-06-Branch Prediction Competition</a></p>
<p>7 0.79425031 <a title="451-lda-7" href="../hunch_net-2007/hunch_net-2007-05-08-Conditional_Tournaments_for_Multiclass_to_Binary.html">243 hunch net-2007-05-08-Conditional Tournaments for Multiclass to Binary</a></p>
<p>8 0.74722183 <a title="451-lda-8" href="../hunch_net-2006/hunch_net-2006-11-20-Context_and_the_calculation_misperception.html">218 hunch net-2006-11-20-Context and the calculation misperception</a></p>
<p>9 0.60576224 <a title="451-lda-9" href="../hunch_net-2013/hunch_net-2013-01-07-NYU_Large_Scale_Machine_Learning_Class.html">478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</a></p>
<p>10 0.5122782 <a title="451-lda-10" href="../hunch_net-2006/hunch_net-2006-05-23-What_is_the_best_regret_transform_reduction_from_multiclass_to_binary%3F.html">181 hunch net-2006-05-23-What is the best regret transform reduction from multiclass to binary?</a></p>
<p>11 0.51048386 <a title="451-lda-11" href="../hunch_net-2008/hunch_net-2008-03-15-COLT_Open_Problems.html">292 hunch net-2008-03-15-COLT Open Problems</a></p>
<p>12 0.50955939 <a title="451-lda-12" href="../hunch_net-2011/hunch_net-2011-08-15-Vowpal_Wabbit_6.0.html">441 hunch net-2011-08-15-Vowpal Wabbit 6.0</a></p>
<p>13 0.50287277 <a title="451-lda-13" href="../hunch_net-2012/hunch_net-2012-09-29-Vowpal_Wabbit%2C_version_7.0.html">473 hunch net-2012-09-29-Vowpal Wabbit, version 7.0</a></p>
<p>14 0.48578355 <a title="451-lda-14" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>15 0.48545414 <a title="451-lda-15" href="../hunch_net-2011/hunch_net-2011-03-27-Vowpal_Wabbit%2C_v5.1.html">428 hunch net-2011-03-27-Vowpal Wabbit, v5.1</a></p>
<p>16 0.48359734 <a title="451-lda-16" href="../hunch_net-2005/hunch_net-2005-05-16-Regret_minimizing_vs_error_limiting_reductions.html">72 hunch net-2005-05-16-Regret minimizing vs error limiting reductions</a></p>
<p>17 0.48029125 <a title="451-lda-17" href="../hunch_net-2009/hunch_net-2009-07-31-Vowpal_Wabbit_Open_Source_Project.html">365 hunch net-2009-07-31-Vowpal Wabbit Open Source Project</a></p>
<p>18 0.47496614 <a title="451-lda-18" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<p>19 0.46465176 <a title="451-lda-19" href="../hunch_net-2011/hunch_net-2011-02-17-What_does_Watson_mean%3F.html">424 hunch net-2011-02-17-What does Watson mean?</a></p>
<p>20 0.45477587 <a title="451-lda-20" href="../hunch_net-2008/hunch_net-2008-12-07-A_NIPS_paper.html">330 hunch net-2008-12-07-A NIPS paper</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
