<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>423 hunch net-2011-02-02-User preferences for search engines</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2011" href="../home/hunch_net-2011_home.html">hunch_net-2011</a> <a title="hunch_net-2011-423" href="#">hunch_net-2011-423</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>423 hunch net-2011-02-02-User preferences for search engines</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2011-423-html" href="http://hunch.net/?p=1660">html</a></p><p>Introduction: I want to comment on the "Bing copies Google" discussionhere,here, andhere,
because there are data-related issues which the general public may not
understand, and some of the framing seems substantially misleading to me.As a
not-distant-outsider, let me mention the sources of bias I may have. I work
atYahoo!, which has started usingBing. This might predispose me towards Bing,
but on the other hand I'm still at Yahoo!, and have been usingLinuxexclusively
as an OS for many years, including even a couple minor kernel patches. And,on
the gripping hand, I've spent quite a bit of time thinking about the
basicprinciples of incorporating user feedback in machine learning. Also note,
this post is not related to official Yahoo! policy, it's just my personal
view.The issueGoogle engineers inserted synthetic responses to synthetic
queries on google.com, then executed the synthetic searches on google.com
using Internet Explorer with the Bing toolbar and later noticed some synthetic
responses from B</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 And,on the gripping hand, I've spent quite a bit of time thinking about the basicprinciples of incorporating user feedback in machine learning. [sent-7, score-0.481]
</p><p>2 The issueGoogle engineers inserted synthetic responses to synthetic queries on google. [sent-10, score-0.499]
</p><p>3 com using Internet Explorer with the Bing toolbar and later noticed some synthetic responses from Bing with the synthetic queries. [sent-12, score-0.499]
</p><p>4 I'm sympathetic on this count, but also sympathetic to the counter argument, that the data collected has value and can enhance the results for all users. [sent-15, score-0.314]
</p><p>5 In the end, I think companies should simply do their best to accept a user's wishes, so those who want privacy can have it, and those who want to contribute their data towards improving a search engine can do so. [sent-16, score-0.808]
</p><p>6 What I believe happened was a user feedback process, where users queried Google, clicked on a result, informed Microsoft/Bing of the query and clicked result, and their preference was used to promote the search result within Bing. [sent-21, score-1.246]
</p><p>7 Should a user be allowed to:Reveal to their chosen search engine their preferred result? [sent-23, score-0.851]
</p><p>8 If you answer 'no' to the first, you are deeply against user freedom in a manner I can't sympathize with. [sent-25, score-0.448]
</p><p>9 If you answer 'yes' to the first, and 'no' to the second, then you are still somewhat against user freedom. [sent-26, score-0.469]
</p><p>10 This isn't too crazy a stance, as various people sell information and require of their users that it not be retransmitted. [sent-27, score-0.311]
</p><p>11 However, in all instances I'm aware of, users knowingly agree to a contract providing access to the information with limitations. [sent-29, score-0.462]
</p><p>12 You could argue that it's ok for Microsoft to take advantage of revealed user interaction, but it's still a matter of following rather than leading. [sent-33, score-0.504]
</p><p>13 A basic truth seen in many ways, is that the proper incorporation of new sources of information always improves results. [sent-35, score-0.321]
</p><p>14 More generally, it's true in basic knowledge engineering, where people fuse sources of information to create a better system, and I'm virtually certain it's true of the ranking algorithms behind Google and Bing, which are surely complex beasts taking into account many sources of information. [sent-37, score-0.66]
</p><p>15 If that's the case, Google will either follow Microsoft's lead taking into account user feedback as Microsoft does, or risk becoming obsolete. [sent-39, score-0.504]
</p><p>16 A basic truth, is that building a successful search engine is extraordinarily difficult. [sent-41, score-0.443]
</p><p>17 This is revealed by search market share, but also by simply thinking about the logistics involved. [sent-42, score-0.372]
</p><p>18 If we prefer a future where there is a healthy competition amongst search engines, then it's important to lower these barriers to entry so new people with new ideas can more easily test them out. [sent-44, score-0.409]
</p><p>19 One way to lower the barrier to entry is to accept that users can share their interaction, even with a competitor's search engine. [sent-45, score-0.743]
</p><p>20 I would be more sympathetic to a position for allowing users of Internet Explorer a built-in means to choose to share their search behavior with Google or other search engines on an equal footing. [sent-48, score-1.065]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('user', 0.337), ('bing', 0.322), ('search', 0.284), ('google', 0.263), ('synthetic', 0.208), ('users', 0.197), ('microsoft', 0.159), ('engine', 0.159), ('sources', 0.141), ('sympathetic', 0.132), ('privacy', 0.132), ('disagreement', 0.119), ('issue', 0.117), ('information', 0.114), ('internet', 0.102), ('result', 0.099), ('explorer', 0.095), ('clicked', 0.095), ('competitor', 0.095), ('revealed', 0.088), ('contract', 0.088), ('reveal', 0.088), ('share', 0.085), ('engines', 0.083), ('responses', 0.083), ('true', 0.081), ('incorporating', 0.079), ('still', 0.079), ('dealt', 0.076), ('informed', 0.074), ('entry', 0.074), ('preferred', 0.071), ('towards', 0.069), ('interaction', 0.066), ('truth', 0.066), ('feedback', 0.065), ('instances', 0.063), ('yahoo', 0.062), ('manner', 0.058), ('want', 0.056), ('yes', 0.055), ('argument', 0.055), ('entirely', 0.055), ('answer', 0.053), ('accept', 0.052), ('account', 0.052), ('lower', 0.051), ('taking', 0.05), ('hand', 0.05), ('results', 0.05)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="423-tfidf-1" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<p>Introduction: I want to comment on the "Bing copies Google" discussionhere,here, andhere,
because there are data-related issues which the general public may not
understand, and some of the framing seems substantially misleading to me.As a
not-distant-outsider, let me mention the sources of bias I may have. I work
atYahoo!, which has started usingBing. This might predispose me towards Bing,
but on the other hand I'm still at Yahoo!, and have been usingLinuxexclusively
as an OS for many years, including even a couple minor kernel patches. And,on
the gripping hand, I've spent quite a bit of time thinking about the
basicprinciples of incorporating user feedback in machine learning. Also note,
this post is not related to official Yahoo! policy, it's just my personal
view.The issueGoogle engineers inserted synthetic responses to synthetic
queries on google.com, then executed the synthetic searches on google.com
using Internet Explorer with the Bing toolbar and later noticed some synthetic
responses from B</p><p>2 0.20204161 <a title="423-tfidf-2" href="../hunch_net-2006/hunch_net-2006-08-03-AOL%26%238217%3Bs_data_drop.html">200 hunch net-2006-08-03-AOL&#8217;s data drop</a></p>
<p>Introduction: AOL hasreleasedseveral large search engine related datasets. This looks like a
pretty impressive data release, and it is a big opportunity for people
everywhere to worry about search engine related learning problems, if they
want.</p><p>3 0.17292199 <a title="423-tfidf-3" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>Introduction: "Search" is the other branch of AI research which has been succesful. Concrete
examples includeDeep Bluewhich beat the world chess champion andChinookthe
champion checkers program. A set of core search techniques exist including A*,
alpha-beta pruning, and others that can be applied to any of many different
search problems.Given this, it may be surprising to learn that there has been
relatively little succesful work on combining prediction and search. Given
also that humans typically solve search problems using a number of predictive
heuristics to narrow in on a solution, we might be surprised again. However,
the big successful search-based systems have typically not used "smart" search
algorithms. Insteady they have optimized for very fast search. This is not for
lack of tryingâ&euro;Ś many people have tried to synthesize search and prediction to
various degrees of success. For example,Knightcapachieves good-but-not-stellar
chess playing performance, andTD-gammonhas achieved near-optimal Bac</p><p>4 0.16620427 <a title="423-tfidf-4" href="../hunch_net-2010/hunch_net-2010-03-12-Netflix_Challenge_2_Canceled.html">390 hunch net-2010-03-12-Netflix Challenge 2 Canceled</a></p>
<p>Introduction: Thesecond Netflix prize is canceleddue toprivacy problems. I continue to
believe my original assessment of this paper, that the privacy break was
somewhat overstated. I still haven't seen any serious privacy failures on the
scale of theAOL search log release.I expect privacy concerns to continue to be
a big issue when dealing with data releases by companies or governments. The
theory of maintaining privacy while using data is improving, but it is not yet
in a state where the limits of what's possible are clear let alone how to
achieve these limits in a manner friendly to a prediction competition.</p><p>5 0.16128282 <a title="423-tfidf-5" href="../hunch_net-2007/hunch_net-2007-08-25-The_Privacy_Problem.html">260 hunch net-2007-08-25-The Privacy Problem</a></p>
<p>Introduction: Machine Learning is rising in importance because data is being collected for
all sorts of tasks where it either wasn't previously collected, or for tasks
that did not previously exist. While this is great for Machine Learning, it
has a downside--the massive data collection which is so useful can also lead
to substantial privacy problems.It's important to understand that this is a
much harder problem than many people appreciate. TheAOLdatareleaseis a good
example. To those doing machine learning, the following strategies might be
obvious:Just delete any names or other obviously personally identifiable
information. The logic here seems to be "if I can't easily find the person
then no one can". That doesn't work as demonstrated by the people who were
found circumstantially from the AOL data.… then just hash all the search
terms! The logic here is "if I can't read it, then no one can". It's also
trivially broken by a dictionary attack--just hash all the strings that might
be in the data an</p><p>6 0.15676092 <a title="423-tfidf-6" href="../hunch_net-2006/hunch_net-2006-02-11-Yahoo%26%238217%3Bs_Learning_Problems..html">156 hunch net-2006-02-11-Yahoo&#8217;s Learning Problems.</a></p>
<p>7 0.13048254 <a title="423-tfidf-7" href="../hunch_net-2010/hunch_net-2010-04-14-MLcomp%3A_a_website_for_objectively_comparing_ML_algorithms.html">393 hunch net-2010-04-14-MLcomp: a website for objectively comparing ML algorithms</a></p>
<p>8 0.1269158 <a title="423-tfidf-8" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>9 0.1150217 <a title="423-tfidf-9" href="../hunch_net-2007/hunch_net-2007-10-24-Contextual_Bandits.html">269 hunch net-2007-10-24-Contextual Bandits</a></p>
<p>10 0.11268613 <a title="423-tfidf-10" href="../hunch_net-2009/hunch_net-2009-07-11-Interesting_papers_at_KDD.html">364 hunch net-2009-07-11-Interesting papers at KDD</a></p>
<p>11 0.10688158 <a title="423-tfidf-11" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>12 0.10106245 <a title="423-tfidf-12" href="../hunch_net-2005/hunch_net-2005-02-09-Intuitions_from_applied_learning.html">16 hunch net-2005-02-09-Intuitions from applied learning</a></p>
<p>13 0.096721865 <a title="423-tfidf-13" href="../hunch_net-2005/hunch_net-2005-10-03-Not_ICML.html">117 hunch net-2005-10-03-Not ICML</a></p>
<p>14 0.096563354 <a title="423-tfidf-14" href="../hunch_net-2011/hunch_net-2011-10-03-Monday_announcements.html">446 hunch net-2011-10-03-Monday announcements</a></p>
<p>15 0.096555546 <a title="423-tfidf-15" href="../hunch_net-2010/hunch_net-2010-12-26-NIPS_2010.html">420 hunch net-2010-12-26-NIPS 2010</a></p>
<p>16 0.095907688 <a title="423-tfidf-16" href="../hunch_net-2009/hunch_net-2009-09-21-Netflix_finishes_%28and_starts%29.html">371 hunch net-2009-09-21-Netflix finishes (and starts)</a></p>
<p>17 0.094042942 <a title="423-tfidf-17" href="../hunch_net-2005/hunch_net-2005-02-02-Kolmogorov_Complexity_and_Googling.html">10 hunch net-2005-02-02-Kolmogorov Complexity and Googling</a></p>
<p>18 0.092756197 <a title="423-tfidf-18" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>19 0.088955618 <a title="423-tfidf-19" href="../hunch_net-2006/hunch_net-2006-09-18-What_is_missing_for_online_collaborative_research%3F.html">208 hunch net-2006-09-18-What is missing for online collaborative research?</a></p>
<p>20 0.087918244 <a title="423-tfidf-20" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.202), (1, 0.011), (2, 0.055), (3, -0.06), (4, 0.019), (5, 0.04), (6, -0.004), (7, -0.005), (8, 0.098), (9, -0.01), (10, -0.141), (11, 0.09), (12, 0.035), (13, 0.075), (14, 0.125), (15, -0.094), (16, -0.014), (17, 0.043), (18, -0.048), (19, -0.084), (20, -0.024), (21, -0.132), (22, -0.096), (23, -0.196), (24, -0.059), (25, -0.018), (26, -0.034), (27, 0.038), (28, 0.133), (29, -0.071), (30, -0.134), (31, -0.064), (32, -0.055), (33, -0.068), (34, -0.011), (35, 0.108), (36, -0.072), (37, -0.089), (38, -0.008), (39, 0.009), (40, -0.018), (41, 0.012), (42, 0.029), (43, -0.079), (44, -0.146), (45, 0.019), (46, 0.001), (47, 0.041), (48, 0.093), (49, 0.05)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97565401 <a title="423-lsi-1" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<p>Introduction: I want to comment on the "Bing copies Google" discussionhere,here, andhere,
because there are data-related issues which the general public may not
understand, and some of the framing seems substantially misleading to me.As a
not-distant-outsider, let me mention the sources of bias I may have. I work
atYahoo!, which has started usingBing. This might predispose me towards Bing,
but on the other hand I'm still at Yahoo!, and have been usingLinuxexclusively
as an OS for many years, including even a couple minor kernel patches. And,on
the gripping hand, I've spent quite a bit of time thinking about the
basicprinciples of incorporating user feedback in machine learning. Also note,
this post is not related to official Yahoo! policy, it's just my personal
view.The issueGoogle engineers inserted synthetic responses to synthetic
queries on google.com, then executed the synthetic searches on google.com
using Internet Explorer with the Bing toolbar and later noticed some synthetic
responses from B</p><p>2 0.79891837 <a title="423-lsi-2" href="../hunch_net-2006/hunch_net-2006-08-03-AOL%26%238217%3Bs_data_drop.html">200 hunch net-2006-08-03-AOL&#8217;s data drop</a></p>
<p>Introduction: AOL hasreleasedseveral large search engine related datasets. This looks like a
pretty impressive data release, and it is a big opportunity for people
everywhere to worry about search engine related learning problems, if they
want.</p><p>3 0.68229443 <a title="423-lsi-3" href="../hunch_net-2006/hunch_net-2006-02-11-Yahoo%26%238217%3Bs_Learning_Problems..html">156 hunch net-2006-02-11-Yahoo&#8217;s Learning Problems.</a></p>
<p>Introduction: I just visitedYahoo Researchwhich has several fundamental learning problems
near to (or beyond) the set of problems we know how to solve well. Here are 3
of them.RankingThis is the canonical problem of all search engines. It is made
extra difficult for several reasons.There is relatively little "good"
supervised learning data and a great deal of data with some signal (such as
click through rates).The learning must occur in a partially adversarial
environment. Many people very actively attempt to place themselves at the top
ofrankings.It is not even quite clear whether the problem should be posed as
'ranking' or as 'regression' which is then used to produce
aranking.Collaborative filteringYahoo has a large number of recommendation
systems for music, movies, etcâ&euro;Ś In these sorts of systems, users specify how
they liked a set of things, and then the system can (hopefully) find some more
examples of things they might likeby reasoning across multiple such
sets.Exploration with Generalization</p><p>4 0.67961836 <a title="423-lsi-4" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>Introduction: "Search" is the other branch of AI research which has been succesful. Concrete
examples includeDeep Bluewhich beat the world chess champion andChinookthe
champion checkers program. A set of core search techniques exist including A*,
alpha-beta pruning, and others that can be applied to any of many different
search problems.Given this, it may be surprising to learn that there has been
relatively little succesful work on combining prediction and search. Given
also that humans typically solve search problems using a number of predictive
heuristics to narrow in on a solution, we might be surprised again. However,
the big successful search-based systems have typically not used "smart" search
algorithms. Insteady they have optimized for very fast search. This is not for
lack of tryingâ&euro;Ś many people have tried to synthesize search and prediction to
various degrees of success. For example,Knightcapachieves good-but-not-stellar
chess playing performance, andTD-gammonhas achieved near-optimal Bac</p><p>5 0.65432996 <a title="423-lsi-5" href="../hunch_net-2010/hunch_net-2010-03-12-Netflix_Challenge_2_Canceled.html">390 hunch net-2010-03-12-Netflix Challenge 2 Canceled</a></p>
<p>Introduction: Thesecond Netflix prize is canceleddue toprivacy problems. I continue to
believe my original assessment of this paper, that the privacy break was
somewhat overstated. I still haven't seen any serious privacy failures on the
scale of theAOL search log release.I expect privacy concerns to continue to be
a big issue when dealing with data releases by companies or governments. The
theory of maintaining privacy while using data is improving, but it is not yet
in a state where the limits of what's possible are clear let alone how to
achieve these limits in a manner friendly to a prediction competition.</p><p>6 0.64530408 <a title="423-lsi-6" href="../hunch_net-2007/hunch_net-2007-08-25-The_Privacy_Problem.html">260 hunch net-2007-08-25-The Privacy Problem</a></p>
<p>7 0.59021813 <a title="423-lsi-7" href="../hunch_net-2006/hunch_net-2006-05-08-Big_machine_learning.html">178 hunch net-2006-05-08-Big machine learning</a></p>
<p>8 0.55478644 <a title="423-lsi-8" href="../hunch_net-2005/hunch_net-2005-10-20-Machine_Learning_in_the_News.html">125 hunch net-2005-10-20-Machine Learning in the News</a></p>
<p>9 0.52359599 <a title="423-lsi-9" href="../hunch_net-2005/hunch_net-2005-10-03-Not_ICML.html">117 hunch net-2005-10-03-Not ICML</a></p>
<p>10 0.50162458 <a title="423-lsi-10" href="../hunch_net-2009/hunch_net-2009-07-11-Interesting_papers_at_KDD.html">364 hunch net-2009-07-11-Interesting papers at KDD</a></p>
<p>11 0.47834215 <a title="423-lsi-11" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>12 0.46150193 <a title="423-lsi-12" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>13 0.45261556 <a title="423-lsi-13" href="../hunch_net-2011/hunch_net-2011-10-03-Monday_announcements.html">446 hunch net-2011-10-03-Monday announcements</a></p>
<p>14 0.4219175 <a title="423-lsi-14" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>15 0.41820505 <a title="423-lsi-15" href="../hunch_net-2010/hunch_net-2010-12-26-NIPS_2010.html">420 hunch net-2010-12-26-NIPS 2010</a></p>
<p>16 0.40973768 <a title="423-lsi-16" href="../hunch_net-2007/hunch_net-2007-10-24-Contextual_Bandits.html">269 hunch net-2007-10-24-Contextual Bandits</a></p>
<p>17 0.4078548 <a title="423-lsi-17" href="../hunch_net-2008/hunch_net-2008-08-04-Electoralmarkets.com.html">312 hunch net-2008-08-04-Electoralmarkets.com</a></p>
<p>18 0.40392429 <a title="423-lsi-18" href="../hunch_net-2011/hunch_net-2011-02-17-What_does_Watson_mean%3F.html">424 hunch net-2011-02-17-What does Watson mean?</a></p>
<p>19 0.38665339 <a title="423-lsi-19" href="../hunch_net-2005/hunch_net-2005-04-21-Dynamic_Programming_Generalizations_and_Their_Use.html">58 hunch net-2005-04-21-Dynamic Programming Generalizations and Their Use</a></p>
<p>20 0.3860811 <a title="423-lsi-20" href="../hunch_net-2006/hunch_net-2006-04-09-Progress_in_Machine_Translation.html">171 hunch net-2006-04-09-Progress in Machine Translation</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(6, 0.026), (7, 0.012), (35, 0.074), (42, 0.177), (44, 0.013), (45, 0.083), (68, 0.041), (69, 0.028), (74, 0.146), (82, 0.04), (95, 0.031), (97, 0.23)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.90955377 <a title="423-lda-1" href="../hunch_net-2007/hunch_net-2007-04-28-The_Coming_Patent_Apocalypse.html">241 hunch net-2007-04-28-The Coming Patent Apocalypse</a></p>
<p>Introduction: Many people in computer science believe that patents are problematic. The
truth is even worse--the patent system in the US is fundamentally broken in
ways that will require much more significant reform thanis being considered
now.The myth of the patent is the following: Patents are a mechanism for
inventors to be compensated according to the value of their inventions while
making the invention available to all. This myth sounds pretty desirable, but
the reality is a strange distortion slowly leading towards collapse.There are
many problems associated with patents, but I would like to focus on just two
of them:Patent TrollsThe way that patents have generally worked over the last
several decades is that they were a tool of large companies. Large companies
would amass a large number of patents and then cross-license each other's
patents--in effect saying "we agree to owe each other nothing". Smaller
companies would sometimes lose in this game, essentially because they didn't
have enough p</p><p>2 0.90408552 <a title="423-lda-2" href="../hunch_net-2009/hunch_net-2009-01-19-Netflix_prize_within_epsilon.html">336 hunch net-2009-01-19-Netflix prize within epsilon</a></p>
<p>Introduction: The competitors for theNetflix Prizeare tantalizingly close winning the
million dollar prize. This year,BellKorandCommendo Researchsent a combined
solution that won theprogress prize. Reading thewriteups2is instructive.
Several aspects of solutions are taken for granted including stochastic
gradient descent, ensemble prediction, and targeting residuals (a form of
boosting). Relatively to last year, it appears that many approaches have added
parameterizations, especially for the purpose of modeling through time.The big
question is: will they make the big prize? At this point, the level of
complexity in entering the competition is prohibitive, so perhaps only the
existing competitors will continue to try. (This equation might change
drastically if the teams open source their existing solutions, including
parameter settings.) One fear is that the progress is asymptoting on the wrong
side of the 10% threshold. In the first year, the teams progressed through
84.3% of the 10% gap, and in the</p><p>3 0.8964982 <a title="423-lda-3" href="../hunch_net-2012/hunch_net-2012-02-24-ICML%2B50%25.html">456 hunch net-2012-02-24-ICML+50%</a></p>
<p>Introduction: TheICMLpaper deadline has passed.Joelleand I were surprised to see the number
of submissions jump from last year by about 50% to around 900 submissions. A
tiny portion of these are immediate rejects(*), so this is a much larger set
of papers than expected. The number of workshop submissions also doubled
compared to last year, so ICML may grow significantly this year, if we can
manage to handle the load well. The prospect of making 900 good decisions is
fundamentally daunting, and success will rely heavily on theprogram
committeeandarea chairsat this point.For those who want to rubberneck a bit
more, here's a breakdown of submissions by primary topic of submitted
papers:66 Reinforcement Learning 52 Supervised Learning 51 Clustering 46
Kernel Methods 40 Optimization Algorithms 39 Feature Selection and
Dimensionality Reduction 33 Learning Theory 33 Graphical Models 33
Applications 29 Probabilistic Models 29 NN & Deep Learning 26 Transfer and
Multi-Task Learning 25 Online Learning 25 Activ</p><p>same-blog 4 0.87394083 <a title="423-lda-4" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<p>Introduction: I want to comment on the "Bing copies Google" discussionhere,here, andhere,
because there are data-related issues which the general public may not
understand, and some of the framing seems substantially misleading to me.As a
not-distant-outsider, let me mention the sources of bias I may have. I work
atYahoo!, which has started usingBing. This might predispose me towards Bing,
but on the other hand I'm still at Yahoo!, and have been usingLinuxexclusively
as an OS for many years, including even a couple minor kernel patches. And,on
the gripping hand, I've spent quite a bit of time thinking about the
basicprinciples of incorporating user feedback in machine learning. Also note,
this post is not related to official Yahoo! policy, it's just my personal
view.The issueGoogle engineers inserted synthetic responses to synthetic
queries on google.com, then executed the synthetic searches on google.com
using Internet Explorer with the Bing toolbar and later noticed some synthetic
responses from B</p><p>5 0.83345675 <a title="423-lda-5" href="../hunch_net-2008/hunch_net-2008-03-15-COLT_Open_Problems.html">292 hunch net-2008-03-15-COLT Open Problems</a></p>
<p>Introduction: COLT has acall for open problemsdue March 21. I encourage anyone with a
specifiable open problem to write it down and send it in. Just the effort of
specifying an open problem precisely and concisely has been very helpful for
my own solutions, and there is a substantial chance others will solve it. To
increase the chance someone will take it up, you can even put a bounty on the
solution. (Perhaps I should raise the$500 bountyon theK-fold cross-validation
problemas it hasn't yet been solved).</p><p>6 0.81844616 <a title="423-lda-6" href="../hunch_net-2006/hunch_net-2006-09-09-How_to_solve_an_NP_hard_problem_in_quadratic_time.html">206 hunch net-2006-09-09-How to solve an NP hard problem in quadratic time</a></p>
<p>7 0.77410007 <a title="423-lda-7" href="../hunch_net-2012/hunch_net-2012-06-05-ICML_acceptance_statistics.html">466 hunch net-2012-06-05-ICML acceptance statistics</a></p>
<p>8 0.71947229 <a title="423-lda-8" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>9 0.7093938 <a title="423-lda-9" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>10 0.70885235 <a title="423-lda-10" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>11 0.70791155 <a title="423-lda-11" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>12 0.70631117 <a title="423-lda-12" href="../hunch_net-2005/hunch_net-2005-02-17-Learning_Research_Programs.html">21 hunch net-2005-02-17-Learning Research Programs</a></p>
<p>13 0.70613956 <a title="423-lda-13" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>14 0.70603853 <a title="423-lda-14" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>15 0.70581931 <a title="423-lda-15" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>16 0.70256829 <a title="423-lda-16" href="../hunch_net-2009/hunch_net-2009-02-22-Effective_Research_Funding.html">344 hunch net-2009-02-22-Effective Research Funding</a></p>
<p>17 0.70170492 <a title="423-lda-17" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>18 0.70154971 <a title="423-lda-18" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>19 0.70145714 <a title="423-lda-19" href="../hunch_net-2005/hunch_net-2005-09-12-Fast_Gradient_Descent.html">111 hunch net-2005-09-12-Fast Gradient Descent</a></p>
<p>20 0.70100462 <a title="423-lda-20" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
