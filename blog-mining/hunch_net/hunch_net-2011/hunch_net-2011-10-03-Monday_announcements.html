<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>446 hunch net-2011-10-03-Monday announcements</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2011" href="../home/hunch_net-2011_home.html">hunch_net-2011</a> <a title="hunch_net-2011-446" href="#">hunch_net-2011-446</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>446 hunch net-2011-10-03-Monday announcements</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2011-446-html" href="http://hunch.net/?p=2043">html</a></p><p>Introduction: Various people want to use hunch.net to announce things. I've generally
resisted this because I feared hunch becoming a pure announcement zone while I
am much more interested contentful posts and discussion personally.
Nevertheless there is clearly some value and announcements are easy, so I'm
planning to summarize announcements on Mondays.D. Sculleypoints out an
interestingSemisupervised feature learningcompetition, with a deadline of
October 17.Lihong Lipoints out thewebscope user interaction datasetwhich is
the first high quality exploration dataset I'm aware of that is publicly
available.Seth Rogers points outCrossValidatedwhich looks similar in
conception tometaoptimize, but directly using thestackoverflowinterface and
with a bit more of a statistics twist.</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('announcements', 0.433), ('rogers', 0.234), ('announcement', 0.234), ('zone', 0.234), ('feared', 0.234), ('conception', 0.217), ('summarize', 0.204), ('announce', 0.204), ('publicly', 0.17), ('october', 0.17), ('user', 0.165), ('pure', 0.165), ('interaction', 0.161), ('posts', 0.158), ('becoming', 0.146), ('looks', 0.141), ('planning', 0.138), ('exploration', 0.138), ('deadline', 0.13), ('statistics', 0.128)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="446-tfidf-1" href="../hunch_net-2011/hunch_net-2011-10-03-Monday_announcements.html">446 hunch net-2011-10-03-Monday announcements</a></p>
<p>Introduction: Various people want to use hunch.net to announce things. I've generally
resisted this because I feared hunch becoming a pure announcement zone while I
am much more interested contentful posts and discussion personally.
Nevertheless there is clearly some value and announcements are easy, so I'm
planning to summarize announcements on Mondays.D. Sculleypoints out an
interestingSemisupervised feature learningcompetition, with a deadline of
October 17.Lihong Lipoints out thewebscope user interaction datasetwhich is
the first high quality exploration dataset I'm aware of that is publicly
available.Seth Rogers points outCrossValidatedwhich looks similar in
conception tometaoptimize, but directly using thestackoverflowinterface and
with a bit more of a statistics twist.</p><p>2 0.11247516 <a title="446-tfidf-2" href="../hunch_net-2005/hunch_net-2005-02-20-At_One_Month.html">25 hunch net-2005-02-20-At One Month</a></p>
<p>Introduction: This is near the one month point, so it seems appropriate to consider meta-
issues for the moment.The number of posts is a bit over 20.The number of
people speaking up in discussions is about 10.The number of people viewing the
site is somewhat more than 100.I am (naturally) dissatisfied with many
things.Many of thepotential useshaven't been realized. This is partly a matter
of opportunity (no conferences in the last month), partly a matter of will (no
open problems because it's hard to give them up), and partly a matter of
tradition. In academia, there is a strong tradition of trying to get
everything perfectly right before presentation. This is somewhat contradictory
to the nature of making many posts, and it's definitely contradictory to the
idea of doing "public research". If that sort of idea is to pay off, it must
be significantly more succesful than previous methods. In an effort to
continue experimenting, I'm going to use the next week as "open problems
week".Spam is a problem.</p><p>3 0.097049698 <a title="446-tfidf-3" href="../hunch_net-2010/hunch_net-2010-04-14-MLcomp%3A_a_website_for_objectively_comparing_ML_algorithms.html">393 hunch net-2010-04-14-MLcomp: a website for objectively comparing ML algorithms</a></p>
<p>Introduction: Much of the success and popularity of machine learning has been driven by its
practical impact. Of course, the evaluation of empirical work is an integral
part of the field. But are the existing mechanisms for evaluating algorithms
and comparing results good enough? We (PercyandJake) believe there are
currently a number of shortcomings:Incomplete Disclosure:You read a paper that
proposes Algorithm A which is shown to outperform SVMs on two datasets.
Great.  But what about on other datasets?  How sensitive is this result?
What about compute time - does the algorithm take two seconds on a laptop or
two weeks on a 100-node cluster?Lack of Standardization:Algorithm A beats
Algorithm B on one version of a dataset.  Algorithm B beats Algorithm A on
another version yet uses slightly different preprocessing.  Though doing a
head-on comparison would be ideal, it would be tedious since the programs
probably use different dataset formats and have a large array of options.  And
what if we wanted t</p><p>4 0.096563354 <a title="446-tfidf-4" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<p>Introduction: I want to comment on the "Bing copies Google" discussionhere,here, andhere,
because there are data-related issues which the general public may not
understand, and some of the framing seems substantially misleading to me.As a
not-distant-outsider, let me mention the sources of bias I may have. I work
atYahoo!, which has started usingBing. This might predispose me towards Bing,
but on the other hand I'm still at Yahoo!, and have been usingLinuxexclusively
as an OS for many years, including even a couple minor kernel patches. And,on
the gripping hand, I've spent quite a bit of time thinking about the
basicprinciples of incorporating user feedback in machine learning. Also note,
this post is not related to official Yahoo! policy, it's just my personal
view.The issueGoogle engineers inserted synthetic responses to synthetic
queries on google.com, then executed the synthetic searches on google.com
using Internet Explorer with the Bing toolbar and later noticed some synthetic
responses from B</p><p>5 0.085956961 <a title="446-tfidf-5" href="../hunch_net-2012/hunch_net-2012-10-26-ML_Symposium_and_Strata-Hadoop_World.html">475 hunch net-2012-10-26-ML Symposium and Strata-Hadoop World</a></p>
<p>Introduction: TheNew York ML symposiumwas last Friday. There were 303 registrations, up a
bit fromlast year. I particularly enjoyed talks byBill Freemanon vision and
ML,Jon Lenchneron strategy in Jeopardy, andTara N. Sainathand Brian Kingsbury
ondeep learning for speech recognition. If anyone has suggestions or thoughts
for next year, please speak up.I also attendedStrata + Hadoop Worldfor the
first time. This is primarily a trade conference rather than an academic
conference, but I found it pretty interesting as a first time attendee. This
is ground zero for theBig databuzzword, and I see now why. It's about data,
and the word "big" is so ambiguous that everyone can lay claim to it. There
were essentially zero academic talks. Instead, the focus was on war stories,
product announcements, and education. The general level of education is much
lower--explaining Machine Learning to the SQL educated is the primary
operating point. Nevertheless that's happening, and the fact that machine
learning is consi</p><p>6 0.085848205 <a title="446-tfidf-6" href="../hunch_net-2008/hunch_net-2008-03-23-Interactive_Machine_Learning.html">293 hunch net-2008-03-23-Interactive Machine Learning</a></p>
<p>7 0.078276381 <a title="446-tfidf-7" href="../hunch_net-2012/hunch_net-2012-08-27-NYAS_ML_2012_and_ICML_2013.html">472 hunch net-2012-08-27-NYAS ML 2012 and ICML 2013</a></p>
<p>8 0.076018497 <a title="446-tfidf-8" href="../hunch_net-2006/hunch_net-2006-01-25-1_year.html">151 hunch net-2006-01-25-1 year</a></p>
<p>9 0.075931787 <a title="446-tfidf-9" href="../hunch_net-2005/hunch_net-2005-10-19-Workshop%3A_Atomic_Learning.html">124 hunch net-2005-10-19-Workshop: Atomic Learning</a></p>
<p>10 0.071584739 <a title="446-tfidf-10" href="../hunch_net-2005/hunch_net-2005-07-11-AAAI_blog.html">92 hunch net-2005-07-11-AAAI blog</a></p>
<p>11 0.07150916 <a title="446-tfidf-11" href="../hunch_net-2007/hunch_net-2007-01-02-Retrospective.html">225 hunch net-2007-01-02-Retrospective</a></p>
<p>12 0.070364378 <a title="446-tfidf-12" href="../hunch_net-2006/hunch_net-2006-04-17-Rexa_is_live.html">173 hunch net-2006-04-17-Rexa is live</a></p>
<p>13 0.067091875 <a title="446-tfidf-13" href="../hunch_net-2009/hunch_net-2009-08-27-New_York_Area_Machine_Learning_Events.html">369 hunch net-2009-08-27-New York Area Machine Learning Events</a></p>
<p>14 0.065474302 <a title="446-tfidf-14" href="../hunch_net-2005/hunch_net-2005-12-09-Machine_Learning_Thoughts.html">137 hunch net-2005-12-09-Machine Learning Thoughts</a></p>
<p>15 0.064445041 <a title="446-tfidf-15" href="../hunch_net-2007/hunch_net-2007-09-30-NIPS_workshops_are_out..html">264 hunch net-2007-09-30-NIPS workshops are out.</a></p>
<p>16 0.062312171 <a title="446-tfidf-16" href="../hunch_net-2007/hunch_net-2007-06-21-Presentation_Preparation.html">249 hunch net-2007-06-21-Presentation Preparation</a></p>
<p>17 0.061767805 <a title="446-tfidf-17" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>18 0.061202042 <a title="446-tfidf-18" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>19 0.06051261 <a title="446-tfidf-19" href="../hunch_net-2012/hunch_net-2012-03-13-The_Submodularity_workshop_and_Lucca_Professorship.html">459 hunch net-2012-03-13-The Submodularity workshop and Lucca Professorship</a></p>
<p>20 0.058317281 <a title="446-tfidf-20" href="../hunch_net-2008/hunch_net-2008-11-11-COLT_CFP.html">326 hunch net-2008-11-11-COLT CFP</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.103), (1, 0.041), (2, 0.039), (3, 0.036), (4, -0.005), (5, 0.012), (6, 0.017), (7, -0.064), (8, 0.097), (9, 0.007), (10, -0.03), (11, 0.043), (12, 0.025), (13, 0.007), (14, 0.011), (15, -0.055), (16, -0.002), (17, 0.003), (18, -0.007), (19, -0.034), (20, 0.103), (21, -0.053), (22, -0.01), (23, -0.001), (24, 0.03), (25, 0.076), (26, 0.007), (27, -0.057), (28, 0.027), (29, -0.05), (30, -0.056), (31, -0.042), (32, 0.054), (33, -0.088), (34, -0.008), (35, 0.1), (36, 0.019), (37, 0.043), (38, -0.022), (39, -0.081), (40, 0.008), (41, -0.016), (42, -0.055), (43, 0.031), (44, -0.013), (45, 0.045), (46, -0.062), (47, 0.021), (48, 0.037), (49, -0.028)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98001379 <a title="446-lsi-1" href="../hunch_net-2011/hunch_net-2011-10-03-Monday_announcements.html">446 hunch net-2011-10-03-Monday announcements</a></p>
<p>Introduction: Various people want to use hunch.net to announce things. I've generally
resisted this because I feared hunch becoming a pure announcement zone while I
am much more interested contentful posts and discussion personally.
Nevertheless there is clearly some value and announcements are easy, so I'm
planning to summarize announcements on Mondays.D. Sculleypoints out an
interestingSemisupervised feature learningcompetition, with a deadline of
October 17.Lihong Lipoints out thewebscope user interaction datasetwhich is
the first high quality exploration dataset I'm aware of that is publicly
available.Seth Rogers points outCrossValidatedwhich looks similar in
conception tometaoptimize, but directly using thestackoverflowinterface and
with a bit more of a statistics twist.</p><p>2 0.51770234 <a title="446-lsi-2" href="../hunch_net-2012/hunch_net-2012-03-13-The_Submodularity_workshop_and_Lucca_Professorship.html">459 hunch net-2012-03-13-The Submodularity workshop and Lucca Professorship</a></p>
<p>Introduction: Ninapoints out theSubmodularity WorkshopMarch 19-20next week atGeorgia Tech.
Many people want to make Submodularity the new Convexity in machine learning,
and it certainly seems worth exploring.Sara Olsonalso points out atenured
faculty positionatIMT Luccawith a deadline ofMay 15th. Lucca happens to be the
ancestral home of 1/4 of my heritage</p><p>3 0.49941668 <a title="446-lsi-3" href="../hunch_net-2009/hunch_net-2009-08-27-New_York_Area_Machine_Learning_Events.html">369 hunch net-2009-08-27-New York Area Machine Learning Events</a></p>
<p>Introduction: Several events are happening in the NY area.Barriers in Computational Learning
Theory Workshop, Aug 28.That's tomorrow near Princeton. I'm looking forward to
speaking at this one on "Getting around Barriers in Learning Theory", but
several other talks are of interest, particularly to the CS theory
inclined.Claudia Perlichis running theINFORMS Data Mining Contestwith a
deadline of Sept. 25. This is a contest using real health record data (they
partnered withHealthCare Intelligence) to predict transfers and mortality. In
the current US health care reform debate, the case studies of high costs we
hear strongly suggest machine learning & statistics can save many billions.The
Singularity Summit October 3&4\. This is for the AIists out there. Several of
the talks look interesting, although unfortunately I'll miss it
forALT.Predictive Analytics World, Oct 20-21. This is stretching the
definition of "New York Area" a bit, but the train to DC is reasonable. This
is a conference of case studies</p><p>4 0.49445081 <a title="446-lsi-4" href="../hunch_net-2006/hunch_net-2006-12-06-The_Spam_Problem.html">223 hunch net-2006-12-06-The Spam Problem</a></p>
<p>Introduction: TheNew York Timeshas an article on thegrowth of spam. Interesting facts
include: 9/10 of all email is spam, spam source identification is nearly
useless due to botnet spam senders, and image based spam (emails which consist
of an image only) are on the growth.Estimates of the cost of spam are almost
certainly far to low, because they do not account for the cost in time lost by
people.The image based spam which is currently penetrating many filters should
be catchable with a more sophisticated application of machine learning
technology. For the spam I see, the rendered images come in only a few
formats, which would be easy to recognize via a support vector machine (with
RBF kernel), neural network, or even nearest-neighbor architecture. The
mechanics of setting this up to run efficiently is the only real challenge.
This is the next step in the spam war.The response to this system is to make
the image based spam even more random. We should (essentially) expect to
seeCaptchaspam, and our</p><p>5 0.49298942 <a title="446-lsi-5" href="../hunch_net-2005/hunch_net-2005-12-29-Deadline_Season.html">145 hunch net-2005-12-29-Deadline Season</a></p>
<p>Introduction: Many different paper deadlines are coming up soon so I made a little reference
table. Out of curiosity, I also computed the interval between submission
deadline and
conference.ConferenceLocationDateDeadlineintervalCOLTPittsburghJune
22-25January 21152ICMLPittsburghJune 26-28January 30/February 6140UAIMITJuly
13-16March 9/March 16119AAAIBostonJuly 16-20February
16/21145KDDPhiladelphiaAugust 23-26March 3/March 10166It looks like the
northeastern US is the big winner as far as location this year.</p><p>6 0.48462179 <a title="446-lsi-6" href="../hunch_net-2005/hunch_net-2005-02-20-At_One_Month.html">25 hunch net-2005-02-20-At One Month</a></p>
<p>7 0.44174042 <a title="446-lsi-7" href="../hunch_net-2009/hunch_net-2009-08-16-Centmail_comments.html">367 hunch net-2009-08-16-Centmail comments</a></p>
<p>8 0.43363103 <a title="446-lsi-8" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<p>9 0.43160146 <a title="446-lsi-9" href="../hunch_net-2005/hunch_net-2005-08-11-Why_Manifold-Based_Dimension_Reduction_Techniques%3F.html">102 hunch net-2005-08-11-Why Manifold-Based Dimension Reduction Techniques?</a></p>
<p>10 0.42845353 <a title="446-lsi-10" href="../hunch_net-2010/hunch_net-2010-04-14-MLcomp%3A_a_website_for_objectively_comparing_ML_algorithms.html">393 hunch net-2010-04-14-MLcomp: a website for objectively comparing ML algorithms</a></p>
<p>11 0.42838487 <a title="446-lsi-11" href="../hunch_net-2005/hunch_net-2005-12-09-Machine_Learning_Thoughts.html">137 hunch net-2005-12-09-Machine Learning Thoughts</a></p>
<p>12 0.41573301 <a title="446-lsi-12" href="../hunch_net-2006/hunch_net-2006-01-25-1_year.html">151 hunch net-2006-01-25-1 year</a></p>
<p>13 0.40487641 <a title="446-lsi-13" href="../hunch_net-2006/hunch_net-2006-06-15-IJCAI_is_out_of_season.html">184 hunch net-2006-06-15-IJCAI is out of season</a></p>
<p>14 0.39935625 <a title="446-lsi-14" href="../hunch_net-2005/hunch_net-2005-02-15-ESPgame_and_image_labeling.html">20 hunch net-2005-02-15-ESPgame and image labeling</a></p>
<p>15 0.39776626 <a title="446-lsi-15" href="../hunch_net-2009/hunch_net-2009-05-30-Many_ways_to_Learn_this_summer.html">357 hunch net-2009-05-30-Many ways to Learn this summer</a></p>
<p>16 0.39244893 <a title="446-lsi-16" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>17 0.38427985 <a title="446-lsi-17" href="../hunch_net-2006/hunch_net-2006-08-03-AOL%26%238217%3Bs_data_drop.html">200 hunch net-2006-08-03-AOL&#8217;s data drop</a></p>
<p>18 0.38393506 <a title="446-lsi-18" href="../hunch_net-2013/hunch_net-2013-03-22-I%26%238217%3Bm_a_bandit.html">480 hunch net-2013-03-22-I&#8217;m a bandit</a></p>
<p>19 0.37928259 <a title="446-lsi-19" href="../hunch_net-2007/hunch_net-2007-01-02-Retrospective.html">225 hunch net-2007-01-02-Retrospective</a></p>
<p>20 0.37608451 <a title="446-lsi-20" href="../hunch_net-2009/hunch_net-2009-02-16-KDNuggets.html">342 hunch net-2009-02-16-KDNuggets</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(29, 0.041), (42, 0.146), (45, 0.04), (51, 0.493), (74, 0.082), (88, 0.06)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.88758343 <a title="446-lda-1" href="../hunch_net-2011/hunch_net-2011-10-03-Monday_announcements.html">446 hunch net-2011-10-03-Monday announcements</a></p>
<p>Introduction: Various people want to use hunch.net to announce things. I've generally
resisted this because I feared hunch becoming a pure announcement zone while I
am much more interested contentful posts and discussion personally.
Nevertheless there is clearly some value and announcements are easy, so I'm
planning to summarize announcements on Mondays.D. Sculleypoints out an
interestingSemisupervised feature learningcompetition, with a deadline of
October 17.Lihong Lipoints out thewebscope user interaction datasetwhich is
the first high quality exploration dataset I'm aware of that is publicly
available.Seth Rogers points outCrossValidatedwhich looks similar in
conception tometaoptimize, but directly using thestackoverflowinterface and
with a bit more of a statistics twist.</p><p>2 0.62470019 <a title="446-lda-2" href="../hunch_net-2005/hunch_net-2005-10-12-The_unrealized_potential_of_the_research_lab.html">121 hunch net-2005-10-12-The unrealized potential of the research lab</a></p>
<p>Introduction: I attended theIBM research 60th anniversary. IBM research is, by any
reasonable account, the industrial research lab which has managed to bring the
most value to it's parent company over the long term. This can be seen by
simply counting the survivors: IBM research is the only older research lab
which has not gone through a period of massive firing. (Note that there are
alsonew research labs.)Despite this impressive record, IBM research has
failed, by far, to achieve it's potential. Examples which came up in this
meeting include:It took about a decade to produce DRAM after it was invented
in the lab. (In fact, Intel produced it first.)Relational databases and SQL
were invented and then languished. It was only under external competition that
IBM released it's own relational database. Why didn't IBM grow anOracle
division?An early lead in IP networking hardware did not result in IBM growing
aCisco division. Why not?And remember â&euro;Ś IBM research is a stark success story
compared to it's com</p><p>3 0.45198813 <a title="446-lda-3" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>Introduction: One of the confusing things about research is that progress is very hard to
measure. One of the consequences of being in a hard-to-measure environment is
that the wrong things are often measured.Lines of CodeThe classical example of
this phenomenon is the old lines-of-code-produced metric for programming. It
is easy to imagine systems for producing many lines of code with very little
work that accomplish very little.Paper countIn academia, a "paper count" is an
analog of "lines of code", and it suffers from the same failure modes. The
obvious failure mode here is that we end up with a large number of
uninteresting papers since people end up spending a lot of time optimizing
this metric.ComplexityAnother metric, is "complexity" (in the eye of a
reviewer) of a paper. There is a common temptation to make a method appear
more complex than it is in order for reviewers to judge it worthy of
publication. The failure mode here is unclean thinking. Simple effective
methods are often overlooked</p><p>4 0.32068214 <a title="446-lda-4" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>Introduction: "Search" is the other branch of AI research which has been succesful. Concrete
examples includeDeep Bluewhich beat the world chess champion andChinookthe
champion checkers program. A set of core search techniques exist including A*,
alpha-beta pruning, and others that can be applied to any of many different
search problems.Given this, it may be surprising to learn that there has been
relatively little succesful work on combining prediction and search. Given
also that humans typically solve search problems using a number of predictive
heuristics to narrow in on a solution, we might be surprised again. However,
the big successful search-based systems have typically not used "smart" search
algorithms. Insteady they have optimized for very fast search. This is not for
lack of tryingâ&euro;Ś many people have tried to synthesize search and prediction to
various degrees of success. For example,Knightcapachieves good-but-not-stellar
chess playing performance, andTD-gammonhas achieved near-optimal Bac</p><p>5 0.32040191 <a title="446-lda-5" href="../hunch_net-2013/hunch_net-2013-01-07-NYU_Large_Scale_Machine_Learning_Class.html">478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</a></p>
<p>Introduction: Yann LeCunand I are coteaching a class onLarge Scale Machine Learningstarting
late Januaryat NYU. This class will cover many tricks to get machine learning
working well on datasets with many features, examples, and classes, along with
several elements of deep learning and support systems enabling the
previous.This is not a beginning class--you really need to have taken a basic
machine learning class previously to follow along. Students will be able to
run and experiment with large scale learning algorithms sinceYahoo!has donated
servers which are being configured into a small scaleHadoopcluster. We are
planning to cover the frontier of research in scalable learning algorithms, so
good class projects could easily lead to papers.For me, this is a chance to
teach on many topics of past research. In general, it seems like researchers
should engage in at least occasional teaching of research, both as a proof of
teachability and to see their own research through that lens. More generally,
I</p><p>6 0.32001093 <a title="446-lda-6" href="../hunch_net-2008/hunch_net-2008-12-07-A_NIPS_paper.html">330 hunch net-2008-12-07-A NIPS paper</a></p>
<p>7 0.31948012 <a title="446-lda-7" href="../hunch_net-2005/hunch_net-2005-12-07-Is_the_Google_way_the_way_for_machine_learning%3F.html">136 hunch net-2005-12-07-Is the Google way the way for machine learning?</a></p>
<p>8 0.31799775 <a title="446-lda-8" href="../hunch_net-2011/hunch_net-2011-02-17-What_does_Watson_mean%3F.html">424 hunch net-2011-02-17-What does Watson mean?</a></p>
<p>9 0.31760809 <a title="446-lda-9" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>10 0.31657353 <a title="446-lda-10" href="../hunch_net-2005/hunch_net-2005-02-15-ESPgame_and_image_labeling.html">20 hunch net-2005-02-15-ESPgame and image labeling</a></p>
<p>11 0.31607363 <a title="446-lda-11" href="../hunch_net-2008/hunch_net-2008-11-10-ICML_Reviewing_Criteria.html">325 hunch net-2008-11-10-ICML Reviewing Criteria</a></p>
<p>12 0.31550869 <a title="446-lda-12" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>13 0.31530017 <a title="446-lda-13" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>14 0.31522012 <a title="446-lda-14" href="../hunch_net-2011/hunch_net-2011-12-13-Vowpal_Wabbit_version_6.1_%26%23038%3B_the_NIPS_tutorial.html">451 hunch net-2011-12-13-Vowpal Wabbit version 6.1 &#038; the NIPS tutorial</a></p>
<p>15 0.31432289 <a title="446-lda-15" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>16 0.314311 <a title="446-lda-16" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>17 0.31426847 <a title="446-lda-17" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>18 0.31422406 <a title="446-lda-18" href="../hunch_net-2009/hunch_net-2009-02-22-Effective_Research_Funding.html">344 hunch net-2009-02-22-Effective Research Funding</a></p>
<p>19 0.31414017 <a title="446-lda-19" href="../hunch_net-2005/hunch_net-2005-02-17-Learning_Research_Programs.html">21 hunch net-2005-02-17-Learning Research Programs</a></p>
<p>20 0.31396931 <a title="446-lda-20" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
