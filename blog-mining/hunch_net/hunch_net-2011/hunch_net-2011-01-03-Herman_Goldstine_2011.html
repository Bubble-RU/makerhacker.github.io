<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>421 hunch net-2011-01-03-Herman Goldstine 2011</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2011" href="../home/hunch_net-2011_home.html">hunch_net-2011</a> <a title="hunch_net-2011-421" href="#">hunch_net-2011-421</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>421 hunch net-2011-01-03-Herman Goldstine 2011</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2011-421-html" href="http://hunch.net/?p=1627">html</a></p><p>Introduction: Vikas  points out the  Herman Goldstine Fellowship  at  IBM .  I was a Herman Goldstine Fellow, and benefited from the experience a great deal—that’s where work on learning reductions started.  If you can do research independently, it’s recommended.  Applications are due January 6.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Vikas  points out the  Herman Goldstine Fellowship  at  IBM . [sent-1, score-0.113]
</p><p>2 I was a Herman Goldstine Fellow, and benefited from the experience a great deal—that’s where work on learning reductions started. [sent-2, score-0.66]
</p><p>3 If you can do research independently, it’s recommended. [sent-3, score-0.062]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('goldstine', 0.559), ('herman', 0.518), ('vikas', 0.28), ('benefited', 0.259), ('fellow', 0.244), ('independently', 0.233), ('january', 0.189), ('ibm', 0.185), ('reductions', 0.13), ('deal', 0.113), ('points', 0.113), ('experience', 0.11), ('applications', 0.11), ('due', 0.092), ('great', 0.085), ('research', 0.062), ('work', 0.059), ('learning', 0.017)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="421-tfidf-1" href="../hunch_net-2011/hunch_net-2011-01-03-Herman_Goldstine_2011.html">421 hunch net-2011-01-03-Herman Goldstine 2011</a></p>
<p>Introduction: Vikas  points out the  Herman Goldstine Fellowship  at  IBM .  I was a Herman Goldstine Fellow, and benefited from the experience a great deal—that’s where work on learning reductions started.  If you can do research independently, it’s recommended.  Applications are due January 6.</p><p>2 0.12532382 <a title="421-tfidf-2" href="../hunch_net-2011/hunch_net-2011-11-26-Giving_Thanks.html">449 hunch net-2011-11-26-Giving Thanks</a></p>
<p>Introduction: Thanksgiving is perhaps my favorite holiday, because pausing your life and giving thanks provides a needed moment of perspective.
 
As a researcher, I am most thankful for my education, without which I could not function.  I want to share this, because it provides some sense of how a researcher starts.
  
 My long term memory seems to function particularly well, which makes any education I get is particularly useful. 
 I am naturally obsessive, which makes me chase down details until I fully understand things.  Natural obsessiveness can go wrong, of course, but it’s a great ally when you absolutely must get things right. 
 My childhood was all in one hometown, which was a conscious sacrifice on the part of my father, implying disruptions from moving around were eliminated.  I’m not sure how important this was since travel has it’s own benefits, but it bears thought. 
 I had several great teachers in grade school, and naturally gravitated towards teachers over classmates, as they seemed</p><p>3 0.093490355 <a title="421-tfidf-3" href="../hunch_net-2005/hunch_net-2005-10-12-The_unrealized_potential_of_the_research_lab.html">121 hunch net-2005-10-12-The unrealized potential of the research lab</a></p>
<p>Introduction: I attended the  IBM research 60th anniversary .  IBM research is, by any reasonable account, the industrial research lab which has managed to bring the most value to it’s parent company over the long term.  This can be seen by simply counting the survivors: IBM research is the only older research lab which has not gone through a period of massive firing.  (Note that there are also  new research labs .)
 
Despite this impressive record, IBM research has failed, by far, to achieve it’s potential.  Examples which came up in this meeting include:
  
 It took about a decade to produce DRAM after it was invented in the lab.  (In fact, Intel produced it first.) 
 Relational databases and SQL were invented and then languished.  It was only under external competition that IBM released it’s own relational database.  Why didn’t IBM grow an  Oracle division ? 
 An early lead in IP networking hardware did not result in IBM growing a  Cisco division .  Why not? 
  
And remember … IBM research is a s</p><p>4 0.057299763 <a title="421-tfidf-4" href="../hunch_net-2006/hunch_net-2006-10-04-Health_of_Conferences_Wiki.html">212 hunch net-2006-10-04-Health of Conferences Wiki</a></p>
<p>Introduction: Aaron Hertzmann  points out the  health of conferences wiki , which has a great deal of information about how many different conferences function.</p><p>5 0.052778587 <a title="421-tfidf-5" href="../hunch_net-2011/hunch_net-2011-04-23-ICML_workshops_due.html">433 hunch net-2011-04-23-ICML workshops due</a></p>
<p>Introduction: Lihong  points out that  ICML   workshop  submissions are due April 29.</p><p>6 0.051153336 <a title="421-tfidf-6" href="../hunch_net-2008/hunch_net-2008-11-11-COLT_CFP.html">326 hunch net-2008-11-11-COLT CFP</a></p>
<p>7 0.051061533 <a title="421-tfidf-7" href="../hunch_net-2010/hunch_net-2010-11-18-ICML_2011_%26%238211%3B_Call_for_Tutorials.html">417 hunch net-2010-11-18-ICML 2011 &#8211; Call for Tutorials</a></p>
<p>8 0.05059148 <a title="421-tfidf-8" href="../hunch_net-2008/hunch_net-2008-01-23-Why_Workshop%3F.html">285 hunch net-2008-01-23-Why Workshop?</a></p>
<p>9 0.049916662 <a title="421-tfidf-9" href="../hunch_net-2005/hunch_net-2005-12-29-Deadline_Season.html">145 hunch net-2005-12-29-Deadline Season</a></p>
<p>10 0.046816118 <a title="421-tfidf-10" href="../hunch_net-2009/hunch_net-2009-11-23-ICML_2009_Workshops_%28and_Tutorials%29.html">379 hunch net-2009-11-23-ICML 2009 Workshops (and Tutorials)</a></p>
<p>11 0.044869985 <a title="421-tfidf-11" href="../hunch_net-2007/hunch_net-2007-03-15-Alternative_Machine_Learning_Reductions_Definitions.html">236 hunch net-2007-03-15-Alternative Machine Learning Reductions Definitions</a></p>
<p>12 0.043586574 <a title="421-tfidf-12" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>13 0.043507207 <a title="421-tfidf-13" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>14 0.043180414 <a title="421-tfidf-14" href="../hunch_net-2009/hunch_net-2009-10-26-NIPS_workshops.html">375 hunch net-2009-10-26-NIPS workshops</a></p>
<p>15 0.041585311 <a title="421-tfidf-15" href="../hunch_net-2005/hunch_net-2005-02-02-Paper_Deadlines.html">11 hunch net-2005-02-02-Paper Deadlines</a></p>
<p>16 0.04139927 <a title="421-tfidf-16" href="../hunch_net-2005/hunch_net-2005-06-17-Reopening_RL-%3EClassification.html">82 hunch net-2005-06-17-Reopening RL->Classification</a></p>
<p>17 0.040462289 <a title="421-tfidf-17" href="../hunch_net-2010/hunch_net-2010-01-19-Deadline_Season%2C_2010.html">387 hunch net-2010-01-19-Deadline Season, 2010</a></p>
<p>18 0.038671985 <a title="421-tfidf-18" href="../hunch_net-2012/hunch_net-2012-09-29-Vowpal_Wabbit%2C_version_7.0.html">473 hunch net-2012-09-29-Vowpal Wabbit, version 7.0</a></p>
<p>19 0.038469769 <a title="421-tfidf-19" href="../hunch_net-2011/hunch_net-2011-02-25-Yahoo%21_Machine_Learning_grant_due_March_11.html">425 hunch net-2011-02-25-Yahoo! Machine Learning grant due March 11</a></p>
<p>20 0.0378461 <a title="421-tfidf-20" href="../hunch_net-2005/hunch_net-2005-06-18-Lower_Bounds_for_Learning_Reductions.html">83 hunch net-2005-06-18-Lower Bounds for Learning Reductions</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.051), (1, -0.024), (2, -0.039), (3, -0.028), (4, -0.031), (5, -0.025), (6, 0.041), (7, 0.023), (8, -0.022), (9, 0.015), (10, 0.012), (11, -0.015), (12, -0.045), (13, -0.029), (14, -0.045), (15, 0.025), (16, 0.029), (17, -0.03), (18, -0.07), (19, -0.059), (20, -0.002), (21, 0.036), (22, -0.044), (23, -0.007), (24, 0.014), (25, 0.017), (26, 0.069), (27, 0.01), (28, -0.055), (29, 0.026), (30, 0.017), (31, -0.042), (32, 0.071), (33, 0.022), (34, -0.033), (35, -0.056), (36, -0.06), (37, 0.006), (38, -0.05), (39, 0.013), (40, -0.004), (41, 0.014), (42, -0.009), (43, 0.058), (44, -0.035), (45, -0.006), (46, -0.052), (47, 0.066), (48, -0.053), (49, 0.016)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95560825 <a title="421-lsi-1" href="../hunch_net-2011/hunch_net-2011-01-03-Herman_Goldstine_2011.html">421 hunch net-2011-01-03-Herman Goldstine 2011</a></p>
<p>Introduction: Vikas  points out the  Herman Goldstine Fellowship  at  IBM .  I was a Herman Goldstine Fellow, and benefited from the experience a great deal—that’s where work on learning reductions started.  If you can do research independently, it’s recommended.  Applications are due January 6.</p><p>2 0.53413683 <a title="421-lsi-2" href="../hunch_net-2012/hunch_net-2012-03-13-The_Submodularity_workshop_and_Lucca_Professorship.html">459 hunch net-2012-03-13-The Submodularity workshop and Lucca Professorship</a></p>
<p>Introduction: Nina  points out the  Submodularity Workshop   March 19-20  next week at  Georgia Tech .  Many people want to make Submodularity the new Convexity in machine learning, and it certainly seems worth exploring.
 
 Sara Olson  also points out a  tenured faculty position  at  IMT Lucca  with a deadline of  May 15th .  Lucca happens to be the ancestral home of 1/4 of my heritage</p><p>3 0.53239012 <a title="421-lsi-3" href="../hunch_net-2010/hunch_net-2010-09-13-AIStats.html">409 hunch net-2010-09-13-AIStats</a></p>
<p>Introduction: Geoff Gordon  points out  AIStats 2011  in Ft. Lauderdale, Florida.  The  call for papers  is now out, due Nov. 1.  The plan is to  experiment with the review process  to encourage quality in several ways.  I expect to submit a paper and would encourage others with good research to do likewise.</p><p>4 0.46697626 <a title="421-lsi-4" href="../hunch_net-2011/hunch_net-2011-04-23-ICML_workshops_due.html">433 hunch net-2011-04-23-ICML workshops due</a></p>
<p>Introduction: Lihong  points out that  ICML   workshop  submissions are due April 29.</p><p>5 0.45476043 <a title="421-lsi-5" href="../hunch_net-2009/hunch_net-2009-10-26-NIPS_workshops.html">375 hunch net-2009-10-26-NIPS workshops</a></p>
<p>Introduction: Many of the  NIPS workshops  have a deadline about now, and the NIPS  early registration deadline is Nov. 6 .  Several interest me:
  
  Adaptive Sensing, Active Learning, and Experimental Design  due 10/27. 
  Discrete Optimization in Machine Learning: Submodularity, Sparsity & Polyhedra , due Nov. 6. 
  Large-Scale Machine Learning: Parallelism and Massive Datasets , due 10/23 (i.e. past) 
  Analysis and Design of Algorithms for Interactive Machine Learning , due 10/30. 
  
And Iâ&euro;&trade;m sure many of the others interest others.   Workshops are great as a mechanism for research, so take a look if there is any chance you might be interested.</p><p>6 0.45109597 <a title="421-lsi-6" href="../hunch_net-2008/hunch_net-2008-11-11-COLT_CFP.html">326 hunch net-2008-11-11-COLT CFP</a></p>
<p>7 0.41729978 <a title="421-lsi-7" href="../hunch_net-2011/hunch_net-2011-10-03-Monday_announcements.html">446 hunch net-2011-10-03-Monday announcements</a></p>
<p>8 0.40190944 <a title="421-lsi-8" href="../hunch_net-2006/hunch_net-2006-07-06-Branch_Prediction_Competition.html">190 hunch net-2006-07-06-Branch Prediction Competition</a></p>
<p>9 0.39895099 <a title="421-lsi-9" href="../hunch_net-2005/hunch_net-2005-06-18-Lower_Bounds_for_Learning_Reductions.html">83 hunch net-2005-06-18-Lower Bounds for Learning Reductions</a></p>
<p>10 0.39427117 <a title="421-lsi-10" href="../hunch_net-2005/hunch_net-2005-10-12-The_unrealized_potential_of_the_research_lab.html">121 hunch net-2005-10-12-The unrealized potential of the research lab</a></p>
<p>11 0.3832145 <a title="421-lsi-11" href="../hunch_net-2010/hunch_net-2010-11-18-ICML_2011_%26%238211%3B_Call_for_Tutorials.html">417 hunch net-2010-11-18-ICML 2011 &#8211; Call for Tutorials</a></p>
<p>12 0.37370604 <a title="421-lsi-12" href="../hunch_net-2005/hunch_net-2005-03-05-Funding_Research.html">36 hunch net-2005-03-05-Funding Research</a></p>
<p>13 0.35848817 <a title="421-lsi-13" href="../hunch_net-2009/hunch_net-2009-12-24-Top_graduates_this_season.html">384 hunch net-2009-12-24-Top graduates this season</a></p>
<p>14 0.35187748 <a title="421-lsi-14" href="../hunch_net-2006/hunch_net-2006-02-04-Research_Budget_Changes.html">154 hunch net-2006-02-04-Research Budget Changes</a></p>
<p>15 0.35092783 <a title="421-lsi-15" href="../hunch_net-2010/hunch_net-2010-09-28-Machined_Learnings.html">412 hunch net-2010-09-28-Machined Learnings</a></p>
<p>16 0.35090145 <a title="421-lsi-16" href="../hunch_net-2005/hunch_net-2005-06-17-Reopening_RL-%3EClassification.html">82 hunch net-2005-06-17-Reopening RL->Classification</a></p>
<p>17 0.34774417 <a title="421-lsi-17" href="../hunch_net-2009/hunch_net-2009-02-22-Effective_Research_Funding.html">344 hunch net-2009-02-22-Effective Research Funding</a></p>
<p>18 0.34736502 <a title="421-lsi-18" href="../hunch_net-2011/hunch_net-2011-02-25-Yahoo%21_Machine_Learning_grant_due_March_11.html">425 hunch net-2011-02-25-Yahoo! Machine Learning grant due March 11</a></p>
<p>19 0.34414455 <a title="421-lsi-19" href="../hunch_net-2010/hunch_net-2010-05-20-Google_Predict.html">399 hunch net-2010-05-20-Google Predict</a></p>
<p>20 0.33923668 <a title="421-lsi-20" href="../hunch_net-2009/hunch_net-2009-05-30-Many_ways_to_Learn_this_summer.html">357 hunch net-2009-05-30-Many ways to Learn this summer</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(12, 0.619), (27, 0.156)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.96676344 <a title="421-lda-1" href="../hunch_net-2005/hunch_net-2005-02-19-Machine_learning_reading_groups.html">24 hunch net-2005-02-19-Machine learning reading groups</a></p>
<p>Introduction: Yaroslav collected an extensive list of  machine learning reading groups .</p><p>same-blog 2 0.90813154 <a title="421-lda-2" href="../hunch_net-2011/hunch_net-2011-01-03-Herman_Goldstine_2011.html">421 hunch net-2011-01-03-Herman Goldstine 2011</a></p>
<p>Introduction: Vikas  points out the  Herman Goldstine Fellowship  at  IBM .  I was a Herman Goldstine Fellow, and benefited from the experience a great deal—that’s where work on learning reductions started.  If you can do research independently, it’s recommended.  Applications are due January 6.</p><p>3 0.64990997 <a title="421-lda-3" href="../hunch_net-2013/hunch_net-2013-05-04-COLT_and_ICML_registration.html">482 hunch net-2013-05-04-COLT and ICML registration</a></p>
<p>Introduction: Sebastien Bubeck  points out  COLT   registration  with a May 13 early registration deadline.   The local organizers have done an admirable job of containing costs with a $300 registration fee.
 
 ICML   registration  is also available, at about an x3 higher cost.  My understanding is that this is partly due to the costs of a larger conference being harder to contain, partly due to ICML lasting twice as long with tutorials and workshops, and partly because the conference organizers were a bit over-conservative in various ways.</p><p>4 0.56721485 <a title="421-lda-4" href="../hunch_net-2011/hunch_net-2011-07-11-Interesting_Neural_Network_Papers_at_ICML_2011.html">438 hunch net-2011-07-11-Interesting Neural Network Papers at ICML 2011</a></p>
<p>Introduction: Maybe it’s too early to call, but with four separate Neural Network sessions at this year’s  ICML ,  it looks like Neural Networks are making a comeback. Here are my  highlights of these sessions. In general, my feeling is that these  papers both demystify deep learning and show its broader applicability.
 
The first observation I made is that the once disreputable “Neural” nomenclature is being used again  in lieu of  “deep learning”. Maybe it’s because Adam Coates et al. showed that single layer networks can work surprisingly well.
  
  An Analysis of Single-Layer Networks in Unsupervised Feature       Learning ,  Adam Coates ,  Honglak Lee ,  Andrew Y. Ng  (AISTATS 2011) 
  The Importance of Encoding Versus Training with Sparse Coding and Vector Quantization ,  Adam Coates ,  Andrew Y. Ng  (ICML 2011) 
  
Another surprising result out of Andrew Ng’s group comes from Andrew  Saxe et al. who show that certain convolutional pooling architectures  can obtain close to state-of-the-art pe</p><p>5 0.37958634 <a title="421-lda-5" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<p>Introduction: There were  two   papers  at ICML presenting learning algorithms for a  contextual bandit -style setting, where the loss for all labels is not known, but the loss for one label is known.  (The first might require a  exploration scavenging  viewpoint to understand if the experimental assignment was nonrandom.)  I strongly approve of these papers and further work in this setting and its variants, because I expect it to become more important than supervised learning.  As a quick review, we are thinking about situations where repeatedly:
  
 The world reveals feature values (aka context information). 
 A policy chooses an action. 
 The world provides a reward. 
  
Sometimes this is done in an online fashion where the policy can change based on immediate feedback and sometimes it’s done in a batch setting where many samples are collected before the policy can change.  If you haven’t spent time thinking about the setting, you might want to because there are many natural applications.
 
I’m g</p><p>6 0.3285293 <a title="421-lda-6" href="../hunch_net-2007/hunch_net-2007-08-19-Choice_of_Metrics.html">259 hunch net-2007-08-19-Choice of Metrics</a></p>
<p>7 0.24414885 <a title="421-lda-7" href="../hunch_net-2006/hunch_net-2006-03-24-NLPers.html">166 hunch net-2006-03-24-NLPers</a></p>
<p>8 0.24414885 <a title="421-lda-8" href="../hunch_net-2007/hunch_net-2007-06-13-Not_Posting.html">246 hunch net-2007-06-13-Not Posting</a></p>
<p>9 0.24414885 <a title="421-lda-9" href="../hunch_net-2010/hunch_net-2010-12-02-Traffic_Prediction_Problem.html">418 hunch net-2010-12-02-Traffic Prediction Problem</a></p>
<p>10 0.24389727 <a title="421-lda-10" href="../hunch_net-2007/hunch_net-2007-11-28-Computational_Consequences_of_Classification.html">274 hunch net-2007-11-28-Computational Consequences of Classification</a></p>
<p>11 0.2434949 <a title="421-lda-11" href="../hunch_net-2007/hunch_net-2007-06-14-Interesting_Papers_at_COLT_2007.html">247 hunch net-2007-06-14-Interesting Papers at COLT 2007</a></p>
<p>12 0.24294752 <a title="421-lda-12" href="../hunch_net-2008/hunch_net-2008-07-06-To_Dual_or_Not.html">308 hunch net-2008-07-06-To Dual or Not</a></p>
<p>13 0.24246424 <a title="421-lda-13" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>14 0.24235988 <a title="421-lda-14" href="../hunch_net-2007/hunch_net-2007-05-12-Loss_Function_Semantics.html">245 hunch net-2007-05-12-Loss Function Semantics</a></p>
<p>15 0.24231575 <a title="421-lda-15" href="../hunch_net-2006/hunch_net-2006-04-14-JMLR_is_a_success.html">172 hunch net-2006-04-14-JMLR is a success</a></p>
<p>16 0.24209632 <a title="421-lda-16" href="../hunch_net-2008/hunch_net-2008-02-10-Complexity_Illness.html">288 hunch net-2008-02-10-Complexity Illness</a></p>
<p>17 0.24081895 <a title="421-lda-17" href="../hunch_net-2005/hunch_net-2005-03-22-Active_learning.html">45 hunch net-2005-03-22-Active learning</a></p>
<p>18 0.23804487 <a title="421-lda-18" href="../hunch_net-2005/hunch_net-2005-02-01-Watchword%3A_Loss.html">9 hunch net-2005-02-01-Watchword: Loss</a></p>
<p>19 0.23673637 <a title="421-lda-19" href="../hunch_net-2009/hunch_net-2009-02-04-Optimal_Proxy_Loss_for_Classification.html">341 hunch net-2009-02-04-Optimal Proxy Loss for Classification</a></p>
<p>20 0.23630841 <a title="421-lda-20" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
