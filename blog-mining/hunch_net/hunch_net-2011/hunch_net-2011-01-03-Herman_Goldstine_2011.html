<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>421 hunch net-2011-01-03-Herman Goldstine 2011</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2011" href="../home/hunch_net-2011_home.html">hunch_net-2011</a> <a title="hunch_net-2011-421" href="#">hunch_net-2011-421</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>421 hunch net-2011-01-03-Herman Goldstine 2011</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2011-421-html" href="http://hunch.net/?p=1627">html</a></p><p>Introduction: Vikaspoints out theHerman Goldstine FellowshipatIBM. I was a Herman Goldstine
Fellow, and benefited from the experience a great deal--that's where work on
learning reductions started. If you can do research independently, it's
recommended. Applications are due January 6.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 I was a Herman Goldstine Fellow, and benefited from the experience a great deal--that's where work on learning reductions started. [sent-2, score-0.767]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('goldstine', 0.712), ('herman', 0.316), ('benefited', 0.293), ('fellow', 0.293), ('independently', 0.276), ('january', 0.237), ('reductions', 0.161), ('applications', 0.125), ('experience', 0.125), ('due', 0.111), ('great', 0.097), ('research', 0.072), ('work', 0.07), ('learning', 0.021)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="421-tfidf-1" href="../hunch_net-2011/hunch_net-2011-01-03-Herman_Goldstine_2011.html">421 hunch net-2011-01-03-Herman Goldstine 2011</a></p>
<p>Introduction: Vikaspoints out theHerman Goldstine FellowshipatIBM. I was a Herman Goldstine
Fellow, and benefited from the experience a great deal--that's where work on
learning reductions started. If you can do research independently, it's
recommended. Applications are due January 6.</p><p>2 0.073022582 <a title="421-tfidf-2" href="../hunch_net-2010/hunch_net-2010-11-18-ICML_2011_%26%238211%3B_Call_for_Tutorials.html">417 hunch net-2010-11-18-ICML 2011 &#8211; Call for Tutorials</a></p>
<p>Introduction: I would like to encourage people to consider giving a tutorial at next years
ICML. The ideal tutorial attracts a wide audience, provides a gentle and
easily taught introduction to the chosen research area, and also covers the
most important contributions in depth.Submissions are due January 14 Â (about
two weeks before paper
deadline).http://www.icml-2011.org/tutorials.phpRegards,Ulf</p><p>3 0.067261443 <a title="421-tfidf-3" href="../hunch_net-2009/hunch_net-2009-11-23-ICML_2009_Workshops_%28and_Tutorials%29.html">379 hunch net-2009-11-23-ICML 2009 Workshops (and Tutorials)</a></p>
<p>Introduction: I'm theworkshops chairforICMLthis year. As such, I would like to personally
encourage people to consider running a workshop.My general view of workshops
is that they are excellent as opportunities to discuss and develop research
directions--some of my best work has come from collaborations at workshops and
several workshops have substantially altered my thinking about various
problems. My experience running workshops is that setting them up and making
them fly often appears much harder than it actually is, and the workshops
often come off much better than expected in the end. Submissions are due
January 18, two weeks before papers.Similarly,Ben Taskaris looking for
goodtutorials, which is complementary. Workshops are about exploring a
subject, while a tutorial is about distilling it down into an easily taught
essence, a vital part of the research process. Tutorials are due February 13,
two weeks after papers.</p><p>4 0.062953256 <a title="421-tfidf-4" href="../hunch_net-2008/hunch_net-2008-01-23-Why_Workshop%3F.html">285 hunch net-2008-01-23-Why Workshop?</a></p>
<p>Introduction: I second thecall for workshops at ICML/COLT/UAI.Severaltimesbefore, details of
why and how to run a workshop have been mentioned.There is a simple reason to
prefer workshops here: attendance. The Helsinki colocation has placed
workshopsdirectly between ICML and COLT/UAI, which is optimal for getting
attendees from any conference. In addition,last year ICML had relatively few
workshopsand NIPS workshops were overloaded. In addition tothose that
happeneda similar number were rejected. The overload has strange consequences
--for example,the best attended workshopwasn't an official NIPS workshop.
Aside from intrinsic interest, the Deep Learning workshop benefited greatly
from being off schedule.</p><p>5 0.059074048 <a title="421-tfidf-5" href="../hunch_net-2005/hunch_net-2005-02-02-Paper_Deadlines.html">11 hunch net-2005-02-02-Paper Deadlines</a></p>
<p>Introduction: It's conference season, and smell of budding papers is in the air.IJCAI 2005,
January 21COLT 2005, February 2KDD 2005, February 18ICML 2005, March 8UAI
2005, March 16AAAI 2005, March 18</p><p>6 0.056065943 <a title="421-tfidf-6" href="../hunch_net-2005/hunch_net-2005-06-17-Reopening_RL-%3EClassification.html">82 hunch net-2005-06-17-Reopening RL->Classification</a></p>
<p>7 0.054612815 <a title="421-tfidf-7" href="../hunch_net-2008/hunch_net-2008-12-12-Summer_Conferences.html">331 hunch net-2008-12-12-Summer Conferences</a></p>
<p>8 0.051583622 <a title="421-tfidf-8" href="../hunch_net-2006/hunch_net-2006-06-15-IJCAI_is_out_of_season.html">184 hunch net-2006-06-15-IJCAI is out of season</a></p>
<p>9 0.050605372 <a title="421-tfidf-9" href="../hunch_net-2011/hunch_net-2011-02-25-Yahoo%21_Machine_Learning_grant_due_March_11.html">425 hunch net-2011-02-25-Yahoo! Machine Learning grant due March 11</a></p>
<p>10 0.048193473 <a title="421-tfidf-10" href="../hunch_net-2012/hunch_net-2012-09-29-Vowpal_Wabbit%2C_version_7.0.html">473 hunch net-2012-09-29-Vowpal Wabbit, version 7.0</a></p>
<p>11 0.047297459 <a title="421-tfidf-11" href="../hunch_net-2009/hunch_net-2009-10-26-NIPS_workshops.html">375 hunch net-2009-10-26-NIPS workshops</a></p>
<p>12 0.046164144 <a title="421-tfidf-12" href="../hunch_net-2011/hunch_net-2011-04-23-ICML_workshops_due.html">433 hunch net-2011-04-23-ICML workshops due</a></p>
<p>13 0.045827247 <a title="421-tfidf-13" href="../hunch_net-2007/hunch_net-2007-12-19-Cool_and_interesting_things_seen_at_NIPS.html">279 hunch net-2007-12-19-Cool and interesting things seen at NIPS</a></p>
<p>14 0.043193966 <a title="421-tfidf-14" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>15 0.041890737 <a title="421-tfidf-15" href="../hunch_net-2007/hunch_net-2007-03-15-Alternative_Machine_Learning_Reductions_Definitions.html">236 hunch net-2007-03-15-Alternative Machine Learning Reductions Definitions</a></p>
<p>16 0.037096564 <a title="421-tfidf-16" href="../hunch_net-2005/hunch_net-2005-06-18-Lower_Bounds_for_Learning_Reductions.html">83 hunch net-2005-06-18-Lower Bounds for Learning Reductions</a></p>
<p>17 0.033273842 <a title="421-tfidf-17" href="../hunch_net-2008/hunch_net-2008-10-14-Who_is_Responsible_for_a_Bad_Review%3F.html">320 hunch net-2008-10-14-Who is Responsible for a Bad Review?</a></p>
<p>18 0.032690365 <a title="421-tfidf-18" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>19 0.03139285 <a title="421-tfidf-19" href="../hunch_net-2012/hunch_net-2012-12-29-Simons_Institute_Big_Data_Program.html">476 hunch net-2012-12-29-Simons Institute Big Data Program</a></p>
<p>20 0.03052965 <a title="421-tfidf-20" href="../hunch_net-2005/hunch_net-2005-07-07-The_Limits_of_Learning_Theory.html">90 hunch net-2005-07-07-The Limits of Learning Theory</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.045), (1, 0.018), (2, 0.036), (3, 0.03), (4, 0.02), (5, -0.048), (6, -0.003), (7, 0.028), (8, -0.028), (9, 0.003), (10, 0.043), (11, -0.027), (12, -0.004), (13, 0.018), (14, 0.066), (15, -0.039), (16, -0.037), (17, -0.091), (18, 0.018), (19, -0.036), (20, -0.024), (21, -0.026), (22, 0.006), (23, -0.006), (24, 0.019), (25, -0.002), (26, -0.011), (27, 0.012), (28, 0.027), (29, -0.003), (30, 0.0), (31, -0.011), (32, 0.006), (33, 0.024), (34, 0.028), (35, -0.015), (36, 0.05), (37, -0.018), (38, -0.036), (39, -0.005), (40, 0.057), (41, 0.009), (42, -0.077), (43, -0.037), (44, 0.001), (45, -0.017), (46, -0.01), (47, 0.01), (48, -0.03), (49, -0.073)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.92309225 <a title="421-lsi-1" href="../hunch_net-2011/hunch_net-2011-01-03-Herman_Goldstine_2011.html">421 hunch net-2011-01-03-Herman Goldstine 2011</a></p>
<p>Introduction: Vikaspoints out theHerman Goldstine FellowshipatIBM. I was a Herman Goldstine
Fellow, and benefited from the experience a great deal--that's where work on
learning reductions started. If you can do research independently, it's
recommended. Applications are due January 6.</p><p>2 0.58398485 <a title="421-lsi-2" href="../hunch_net-2010/hunch_net-2010-09-13-AIStats.html">409 hunch net-2010-09-13-AIStats</a></p>
<p>Introduction: Geoff Gordonpoints outAIStats 2011in Ft. Lauderdale, Florida. Thecall for
papersis now out, due Nov. 1. The plan is toexperiment with the review
processto encourage quality in several ways. I expect to submit a paper and
would encourage others with good research to do likewise.</p><p>3 0.55030882 <a title="421-lsi-3" href="../hunch_net-2010/hunch_net-2010-11-18-ICML_2011_%26%238211%3B_Call_for_Tutorials.html">417 hunch net-2010-11-18-ICML 2011 &#8211; Call for Tutorials</a></p>
<p>Introduction: I would like to encourage people to consider giving a tutorial at next years
ICML. The ideal tutorial attracts a wide audience, provides a gentle and
easily taught introduction to the chosen research area, and also covers the
most important contributions in depth.Submissions are due January 14 Â (about
two weeks before paper
deadline).http://www.icml-2011.org/tutorials.phpRegards,Ulf</p><p>4 0.49712813 <a title="421-lsi-4" href="../hunch_net-2011/hunch_net-2011-02-25-Yahoo%21_Machine_Learning_grant_due_March_11.html">425 hunch net-2011-02-25-Yahoo! Machine Learning grant due March 11</a></p>
<p>Introduction: Yahoo!'sKey Scientific ChallengesforMachine Learninggrant applications are due
March 11. If you are a student working on relevant research, please consider
applying. It's for $5K of unrestricted funding.</p><p>5 0.49588946 <a title="421-lsi-5" href="../hunch_net-2009/hunch_net-2009-11-23-ICML_2009_Workshops_%28and_Tutorials%29.html">379 hunch net-2009-11-23-ICML 2009 Workshops (and Tutorials)</a></p>
<p>Introduction: I'm theworkshops chairforICMLthis year. As such, I would like to personally
encourage people to consider running a workshop.My general view of workshops
is that they are excellent as opportunities to discuss and develop research
directions--some of my best work has come from collaborations at workshops and
several workshops have substantially altered my thinking about various
problems. My experience running workshops is that setting them up and making
them fly often appears much harder than it actually is, and the workshops
often come off much better than expected in the end. Submissions are due
January 18, two weeks before papers.Similarly,Ben Taskaris looking for
goodtutorials, which is complementary. Workshops are about exploring a
subject, while a tutorial is about distilling it down into an easily taught
essence, a vital part of the research process. Tutorials are due February 13,
two weeks after papers.</p><p>6 0.44863948 <a title="421-lsi-6" href="../hunch_net-2011/hunch_net-2011-04-23-ICML_workshops_due.html">433 hunch net-2011-04-23-ICML workshops due</a></p>
<p>7 0.422079 <a title="421-lsi-7" href="../hunch_net-2005/hunch_net-2005-06-17-Reopening_RL-%3EClassification.html">82 hunch net-2005-06-17-Reopening RL->Classification</a></p>
<p>8 0.42175397 <a title="421-lsi-8" href="../hunch_net-2005/hunch_net-2005-05-03-Conference_attendance_is_mandatory.html">66 hunch net-2005-05-03-Conference attendance is mandatory</a></p>
<p>9 0.41607168 <a title="421-lsi-9" href="../hunch_net-2009/hunch_net-2009-10-26-NIPS_workshops.html">375 hunch net-2009-10-26-NIPS workshops</a></p>
<p>10 0.41076323 <a title="421-lsi-10" href="../hunch_net-2008/hunch_net-2008-07-02-Proprietary_Data_in_Academic_Research%3F.html">306 hunch net-2008-07-02-Proprietary Data in Academic Research?</a></p>
<p>11 0.3558079 <a title="421-lsi-11" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>12 0.35313442 <a title="421-lsi-12" href="../hunch_net-2012/hunch_net-2012-12-29-Simons_Institute_Big_Data_Program.html">476 hunch net-2012-12-29-Simons Institute Big Data Program</a></p>
<p>13 0.34304994 <a title="421-lsi-13" href="../hunch_net-2010/hunch_net-2010-01-13-Sam_Roweis_died.html">386 hunch net-2010-01-13-Sam Roweis died</a></p>
<p>14 0.34121269 <a title="421-lsi-14" href="../hunch_net-2013/hunch_net-2013-08-31-Extreme_Classification_workshop_at_NIPS.html">488 hunch net-2013-08-31-Extreme Classification workshop at NIPS</a></p>
<p>15 0.34001404 <a title="421-lsi-15" href="../hunch_net-2007/hunch_net-2007-03-15-Alternative_Machine_Learning_Reductions_Definitions.html">236 hunch net-2007-03-15-Alternative Machine Learning Reductions Definitions</a></p>
<p>16 0.33841836 <a title="421-lsi-16" href="../hunch_net-2005/hunch_net-2005-10-12-The_unrealized_potential_of_the_research_lab.html">121 hunch net-2005-10-12-The unrealized potential of the research lab</a></p>
<p>17 0.33503312 <a title="421-lsi-17" href="../hunch_net-2007/hunch_net-2007-10-15-NIPS_workshops_extended_to_3_days.html">266 hunch net-2007-10-15-NIPS workshops extended to 3 days</a></p>
<p>18 0.32479846 <a title="421-lsi-18" href="../hunch_net-2010/hunch_net-2010-10-17-Partha_Niyogi_has_died.html">414 hunch net-2010-10-17-Partha Niyogi has died</a></p>
<p>19 0.32404876 <a title="421-lsi-19" href="../hunch_net-2013/hunch_net-2013-11-09-Graduates_and_Postdocs.html">490 hunch net-2013-11-09-Graduates and Postdocs</a></p>
<p>20 0.32352698 <a title="421-lsi-20" href="../hunch_net-2005/hunch_net-2005-03-05-Funding_Research.html">36 hunch net-2005-03-05-Funding Research</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.747)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.97007519 <a title="421-lda-1" href="../hunch_net-2006/hunch_net-2006-10-13-David_Pennock_starts_Oddhead.html">214 hunch net-2006-10-13-David Pennock starts Oddhead</a></p>
<p>Introduction: his blog on information markets and other research topics.</p><p>same-blog 2 0.91483366 <a title="421-lda-2" href="../hunch_net-2011/hunch_net-2011-01-03-Herman_Goldstine_2011.html">421 hunch net-2011-01-03-Herman Goldstine 2011</a></p>
<p>Introduction: Vikaspoints out theHerman Goldstine FellowshipatIBM. I was a Herman Goldstine
Fellow, and benefited from the experience a great deal--that's where work on
learning reductions started. If you can do research independently, it's
recommended. Applications are due January 6.</p><p>3 0.85058117 <a title="421-lda-3" href="../hunch_net-2013/hunch_net-2013-04-15-NEML_II.html">481 hunch net-2013-04-15-NEML II</a></p>
<p>Introduction: Adam Kalaipoints out theNew England Machine Learning DayMay 1 at MSR New
England. There is a poster session with abstracts due April 19. I understand
last year'sNEMLwent well and it's great to meet your neighbors at regional
workshops like this.</p><p>4 0.82308996 <a title="421-lda-4" href="../hunch_net-2006/hunch_net-2006-02-04-Research_Budget_Changes.html">154 hunch net-2006-02-04-Research Budget Changes</a></p>
<p>Introduction: The announcement of an increase in funding for basic research in the US is
encouraging. There is some discussion of this at theComputing Research
Policyblog.One part of this discussion has a graph of NSF funding over time,
presumably in dollar budgets. I don't believe that dollar budgets are the
right way to judge the impact of funding changes on researchers. A better way
to judge seems to be in terms of dollar budget divided by GDP which provides a
measure of the relative emphasis on research.This graph was assembled by
dividing theNSF budgetby theUS GDP. For 2005 GDP, I used thecurrent
estimateand for 2006 and 2007 assumed an increase by a factor of 1.04 per
year. The 2007 number also uses the requested 2007 budget which is certain to
change.This graph makes it clear why researchers were upset: research funding
emphasis has fallen for 3 years in a row. The reality has been significantly
more severe due toDARPA decreasing fundingand industrial research labs (ATnT
and Lucent for exampl</p><p>5 0.78053075 <a title="421-lda-5" href="../hunch_net-2009/hunch_net-2009-06-26-Netflix_nearly_done.html">362 hunch net-2009-06-26-Netflix nearly done</a></p>
<p>Introduction: A $1M qualifying result was achieved on thepublic Netflix test setby a3-way
ensemble team. This is just in time forYehuda's presentation atKDD, which I'm
sure will be one of the best attended ever.This isn't quite over--there are a
few days for another super-conglomerate team to come together and there is
some small chance that the performance is nonrepresentative of the final test
set, but I expect not.Regardless of the final outcome, the biggest lesson for
ML from the Netflix contest has been the formidable performance edge of
ensemble methods.</p><p>6 0.46910998 <a title="421-lda-6" href="../hunch_net-2005/hunch_net-2005-03-05-Funding_Research.html">36 hunch net-2005-03-05-Funding Research</a></p>
<p>7 0.4484148 <a title="421-lda-7" href="../hunch_net-2006/hunch_net-2006-01-06-MLTV.html">146 hunch net-2006-01-06-MLTV</a></p>
<p>8 0.39967978 <a title="421-lda-8" href="../hunch_net-2005/hunch_net-2005-04-01-Basic_computer_science_research_takes_a_hit.html">50 hunch net-2005-04-01-Basic computer science research takes a hit</a></p>
<p>9 0.33782965 <a title="421-lda-9" href="../hunch_net-2007/hunch_net-2007-12-19-Cool_and_interesting_things_seen_at_NIPS.html">279 hunch net-2007-12-19-Cool and interesting things seen at NIPS</a></p>
<p>10 0.25279522 <a title="421-lda-10" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>11 0.10952414 <a title="421-lda-11" href="../hunch_net-2009/hunch_net-2009-02-22-Effective_Research_Funding.html">344 hunch net-2009-02-22-Effective Research Funding</a></p>
<p>12 0.099323437 <a title="421-lda-12" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>13 0.096614763 <a title="421-lda-13" href="../hunch_net-2008/hunch_net-2008-01-06-Research_Political_Issues.html">282 hunch net-2008-01-06-Research Political Issues</a></p>
<p>14 0.08954604 <a title="421-lda-14" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<p>15 0.086769626 <a title="421-lda-15" href="../hunch_net-2013/hunch_net-2013-06-29-The_Benefits_of_Double-Blind_Review.html">485 hunch net-2013-06-29-The Benefits of Double-Blind Review</a></p>
<p>16 0.081926584 <a title="421-lda-16" href="../hunch_net-2012/hunch_net-2012-06-05-ICML_acceptance_statistics.html">466 hunch net-2012-06-05-ICML acceptance statistics</a></p>
<p>17 0.074293986 <a title="421-lda-17" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>18 0.070128039 <a title="421-lda-18" href="../hunch_net-2010/hunch_net-2010-01-19-Deadline_Season%2C_2010.html">387 hunch net-2010-01-19-Deadline Season, 2010</a></p>
<p>19 0.069798276 <a title="421-lda-19" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>20 0.064268455 <a title="421-lda-20" href="../hunch_net-2006/hunch_net-2006-12-04-Structural_Problems_in_NIPS_Decision_Making.html">221 hunch net-2006-12-04-Structural Problems in NIPS Decision Making</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
