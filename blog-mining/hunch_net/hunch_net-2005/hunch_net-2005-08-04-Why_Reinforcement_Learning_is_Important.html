<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>100 hunch net-2005-08-04-Why Reinforcement Learning is Important</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-100" href="#">hunch_net-2005-100</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>100 hunch net-2005-08-04-Why Reinforcement Learning is Important</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-100-html" href="http://hunch.net/?p=107">html</a></p><p>Introduction: One prescription for solving a problem well is:
  
 State the problem, in the simplest way possible. In particular, this statement should involve no contamination with or anticipation of the solution. 
 Think about solutions to the stated problem. 
  
Stating a problem in a succinct and crisp manner tends to invite a simple elegant solution.  When a problem can not be stated succinctly, we  wonder if the problem is even understood. (And when a problem is not understood, we wonder if a solution can be meaningful.)
 
Reinforcement learning does step (1) well.  It provides a clean simple language to state general AI problems.  In reinforcement learning there is a set of actions  A , a set of observations  O , and a reward  r .  The reinforcement learning problem, in general, is defined by a conditional measure  D( o, r | (o,r,a) * )  which produces an observation  o  and a reward  r  given a history  (o,r,a) *  .  The goal in reinforcement learning is to find a policy  pi:(o,r,a) *  -> a</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 One prescription for solving a problem well is:     State the problem, in the simplest way possible. [sent-1, score-0.479]
</p><p>2 In particular, this statement should involve no contamination with or anticipation of the solution. [sent-2, score-0.08]
</p><p>3 Stating a problem in a succinct and crisp manner tends to invite a simple elegant solution. [sent-4, score-0.857]
</p><p>4 When a problem can not be stated succinctly, we  wonder if the problem is even understood. [sent-5, score-0.717]
</p><p>5 (And when a problem is not understood, we wonder if a solution can be meaningful. [sent-6, score-0.385]
</p><p>6 It provides a clean simple language to state general AI problems. [sent-8, score-0.312]
</p><p>7 In reinforcement learning there is a set of actions  A , a set of observations  O , and a reward  r . [sent-9, score-0.636]
</p><p>8 The reinforcement learning problem, in general, is defined by a conditional measure  D( o, r | (o,r,a) * )  which produces an observation  o  and a reward  r  given a history  (o,r,a) *  . [sent-10, score-0.826]
</p><p>9 The goal in reinforcement learning is to find a policy  pi:(o,r,a) *  -> a  mapping histories to actions so as to maximize (or approximately maximize) the expected sum of observed rewards. [sent-11, score-0.884]
</p><p>10 This formulation is capable of capturing almost any (all? [sent-12, score-0.477]
</p><p>11 (Are there any other formulations capable of capturing a similar generality? [sent-14, score-0.486]
</p><p>12 ) I donâ&euro;&trade;t believe we yet have good RL solutions from step (2), but that is unsurprising given the generality of the problem. [sent-15, score-0.698]
</p><p>13 Note that solving RL in this generality is impossible (for example, it can encode classification). [sent-16, score-0.591]
</p><p>14 It is very common to consider the restricted problem where the history is summarized by the previous observation. [sent-18, score-0.574]
</p><p>15 Think about relativized solutions (such as reductions). [sent-22, score-0.195]
</p><p>16 Both methods are options are under active investigation. [sent-23, score-0.097]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('generality', 0.283), ('reinforcement', 0.245), ('capturing', 0.22), ('maximize', 0.21), ('wonder', 0.202), ('solutions', 0.195), ('history', 0.189), ('problem', 0.183), ('rl', 0.163), ('actions', 0.163), ('reward', 0.157), ('ai', 0.152), ('stated', 0.149), ('capable', 0.14), ('encode', 0.126), ('formulations', 0.126), ('prescription', 0.126), ('restrictions', 0.126), ('simplify', 0.126), ('succinctly', 0.126), ('state', 0.117), ('crisp', 0.117), ('formulation', 0.117), ('succinct', 0.117), ('step', 0.115), ('aka', 0.11), ('summarized', 0.11), ('clean', 0.105), ('unsurprising', 0.105), ('mapping', 0.101), ('stating', 0.097), ('options', 0.097), ('solving', 0.093), ('invite', 0.092), ('elegant', 0.092), ('investigation', 0.092), ('produces', 0.092), ('restricted', 0.092), ('simple', 0.09), ('impossible', 0.089), ('approximately', 0.089), ('tends', 0.089), ('involve', 0.08), ('manner', 0.077), ('simplest', 0.077), ('sum', 0.076), ('markov', 0.075), ('defined', 0.072), ('conditional', 0.071), ('observations', 0.071)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="100-tfidf-1" href="../hunch_net-2005/hunch_net-2005-08-04-Why_Reinforcement_Learning_is_Important.html">100 hunch net-2005-08-04-Why Reinforcement Learning is Important</a></p>
<p>Introduction: One prescription for solving a problem well is:
  
 State the problem, in the simplest way possible. In particular, this statement should involve no contamination with or anticipation of the solution. 
 Think about solutions to the stated problem. 
  
Stating a problem in a succinct and crisp manner tends to invite a simple elegant solution.  When a problem can not be stated succinctly, we  wonder if the problem is even understood. (And when a problem is not understood, we wonder if a solution can be meaningful.)
 
Reinforcement learning does step (1) well.  It provides a clean simple language to state general AI problems.  In reinforcement learning there is a set of actions  A , a set of observations  O , and a reward  r .  The reinforcement learning problem, in general, is defined by a conditional measure  D( o, r | (o,r,a) * )  which produces an observation  o  and a reward  r  given a history  (o,r,a) *  .  The goal in reinforcement learning is to find a policy  pi:(o,r,a) *  -> a</p><p>2 0.18470865 <a title="100-tfidf-2" href="../hunch_net-2005/hunch_net-2005-02-23-Problem%3A_Reinforcement_Learning_with_Classification.html">27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</a></p>
<p>Introduction: At an intuitive level, the question here is “Can reinforcement learning be solved with classification?”  
 
 Problem  Construct a reinforcement learning algorithm with near-optimal expected sum of rewards in the  direct experience model  given access to a classifier learning algorithm which has a small error rate or regret on all posed classification problems.  The definition of “posed” here is slightly murky.  I consider a problem “posed” if there is an algorithm for constructing labeled classification examples.
 
 Past Work 
  
 There exists a  reduction of reinforcement learning to classification given a generative model.   A generative model is an inherently stronger assumption than the direct experience model. 
 Other  work on learning reductions  may be important. 
 Several algorithms for solving reinforcement learning in the direct experience model exist.  Most, such as  E 3  ,  Factored-E 3  , and  metric-E 3   and  Rmax  require that the observation be the state.  Recent work</p><p>3 0.15829334 <a title="100-tfidf-3" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>Introduction: One thing which is clear on a little reflection is that there exists a single master learning problem capable of encoding essentially all learning problems.  This problem is of course a very general sort of reinforcement learning where the world interacts with an agent as:
  
 The world announces an observation  x . 
 The agent makes a choice  a . 
 The world announces a reward  r . 
  
The goal here is to maximize the sum of the rewards over the time of the agent.  No particular structure relating  x  to  a  or  a  to  r  is implied by this setting so we do not know effective general algorithms for the agent.  It’s very easy to prove lower bounds showing that an agent cannot hope to succeed here—just consider the case where actions are unrelated to rewards.  Nevertheless, there is a real sense in which essentially all forms of life are agents operating in this setting, somehow succeeding.  The gap between these observations drives research—How can we find tractable specializations of</p><p>4 0.15216465 <a title="100-tfidf-4" href="../hunch_net-2006/hunch_net-2006-04-05-What_is_state%3F.html">169 hunch net-2006-04-05-What is state?</a></p>
<p>Introduction: In reinforcement learning (and sometimes other settings), there is a notion of “state”.   Based upon the state various predictions are made such as “Which action should be taken next?” or “How much cumulative reward do I expect if I take some action from this state?”  Given the importance of state, it is important to examine the meaning.   There are actually several distinct options and it turns out the definition variation is very important in motivating different pieces of work.
  
 Newtonian State.  State is the physical pose of the world.  Under this definition, there are  very  many states, often too many for explicit representation.  This is also the definition typically used in games. 
 Abstracted State.  State is an abstracted physical state of the world.  “Is the door open or closed?” “Are you in room A or not?” The number of states is much smaller here.  A basic issue here is: “How do you compute the state from observations?” 
 Mathematical State.  State is a sufficient stati</p><p>5 0.13296539 <a title="100-tfidf-5" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>Introduction: This post is about a general technique for problem solving which I’ve never seen taught (in full generality), but which I’ve found very useful.
 
Many problems in computer science turn out to be discretely difficult.  The best known version of such problems are NP-hard problems, but I mean ‘discretely difficult’ in a much more general way, which I only know how to capture by examples.
  
  ERM  In empirical risk minimization, you choose a minimum error rate classifier from a set of classifiers.  This is NP hard for common sets, but it can be much harder, depending on the set. 
  Experts  In the online learning with experts setting, you try to predict well so as to compete with a set of (adversarial) experts.  Here the alternating quantifiers of you and an adversary playing out a game can yield a dynamic programming problem that grows exponentially. 
  Policy Iteration  The problem with policy iteration is that you learn a new policy with respect to an old policy, which implies that sim</p><p>6 0.13143535 <a title="100-tfidf-6" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>7 0.12363958 <a title="100-tfidf-7" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>8 0.12299822 <a title="100-tfidf-8" href="../hunch_net-2005/hunch_net-2005-01-24-Holy_grails_of_machine_learning%3F.html">2 hunch net-2005-01-24-Holy grails of machine learning?</a></p>
<p>9 0.11759627 <a title="100-tfidf-9" href="../hunch_net-2009/hunch_net-2009-11-29-AI_Safety.html">380 hunch net-2009-11-29-AI Safety</a></p>
<p>10 0.11600762 <a title="100-tfidf-10" href="../hunch_net-2005/hunch_net-2005-06-17-Reopening_RL-%3EClassification.html">82 hunch net-2005-06-17-Reopening RL->Classification</a></p>
<p>11 0.11482451 <a title="100-tfidf-11" href="../hunch_net-2009/hunch_net-2009-05-08-Computability_in_Artificial_Intelligence.html">353 hunch net-2009-05-08-Computability in Artificial Intelligence</a></p>
<p>12 0.1064685 <a title="100-tfidf-12" href="../hunch_net-2008/hunch_net-2008-09-12-How_do_we_get_weak_action_dependence_for_learning_with_partial_observations%3F.html">317 hunch net-2008-09-12-How do we get weak action dependence for learning with partial observations?</a></p>
<p>13 0.10043487 <a title="100-tfidf-13" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>14 0.096863501 <a title="100-tfidf-14" href="../hunch_net-2006/hunch_net-2006-01-13-Benchmarks_for_RL.html">148 hunch net-2006-01-13-Benchmarks for RL</a></p>
<p>15 0.096562803 <a title="100-tfidf-15" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<p>16 0.096351221 <a title="100-tfidf-16" href="../hunch_net-2005/hunch_net-2005-05-06-Don%26%238217%3Bt_mix_the_solution_into_the_problem.html">67 hunch net-2005-05-06-Don&#8217;t mix the solution into the problem</a></p>
<p>17 0.095313221 <a title="100-tfidf-17" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>18 0.093816184 <a title="100-tfidf-18" href="../hunch_net-2012/hunch_net-2012-03-06-COLT-ICML_Open_Questions_and_ICML_Instructions.html">458 hunch net-2012-03-06-COLT-ICML Open Questions and ICML Instructions</a></p>
<p>19 0.093261749 <a title="100-tfidf-19" href="../hunch_net-2005/hunch_net-2005-08-08-Apprenticeship_Reinforcement_Learning_for_Control.html">101 hunch net-2005-08-08-Apprenticeship Reinforcement Learning for Control</a></p>
<p>20 0.090867966 <a title="100-tfidf-20" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.171), (1, 0.094), (2, 0.008), (3, 0.022), (4, 0.026), (5, -0.068), (6, 0.12), (7, 0.036), (8, 0.008), (9, -0.02), (10, -0.003), (11, -0.065), (12, -0.03), (13, 0.187), (14, -0.028), (15, 0.027), (16, 0.113), (17, -0.094), (18, 0.028), (19, 0.058), (20, -0.116), (21, 0.062), (22, -0.084), (23, -0.022), (24, -0.024), (25, 0.081), (26, -0.108), (27, -0.005), (28, 0.012), (29, 0.097), (30, 0.028), (31, -0.044), (32, -0.034), (33, -0.016), (34, 0.052), (35, 0.082), (36, 0.043), (37, 0.108), (38, 0.099), (39, -0.014), (40, 0.074), (41, -0.047), (42, 0.056), (43, -0.035), (44, -0.034), (45, 0.059), (46, -0.071), (47, 0.05), (48, -0.002), (49, -0.08)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96793115 <a title="100-lsi-1" href="../hunch_net-2005/hunch_net-2005-08-04-Why_Reinforcement_Learning_is_Important.html">100 hunch net-2005-08-04-Why Reinforcement Learning is Important</a></p>
<p>Introduction: One prescription for solving a problem well is:
  
 State the problem, in the simplest way possible. In particular, this statement should involve no contamination with or anticipation of the solution. 
 Think about solutions to the stated problem. 
  
Stating a problem in a succinct and crisp manner tends to invite a simple elegant solution.  When a problem can not be stated succinctly, we  wonder if the problem is even understood. (And when a problem is not understood, we wonder if a solution can be meaningful.)
 
Reinforcement learning does step (1) well.  It provides a clean simple language to state general AI problems.  In reinforcement learning there is a set of actions  A , a set of observations  O , and a reward  r .  The reinforcement learning problem, in general, is defined by a conditional measure  D( o, r | (o,r,a) * )  which produces an observation  o  and a reward  r  given a history  (o,r,a) *  .  The goal in reinforcement learning is to find a policy  pi:(o,r,a) *  -> a</p><p>2 0.7045188 <a title="100-lsi-2" href="../hunch_net-2006/hunch_net-2006-04-05-What_is_state%3F.html">169 hunch net-2006-04-05-What is state?</a></p>
<p>Introduction: In reinforcement learning (and sometimes other settings), there is a notion of “state”.   Based upon the state various predictions are made such as “Which action should be taken next?” or “How much cumulative reward do I expect if I take some action from this state?”  Given the importance of state, it is important to examine the meaning.   There are actually several distinct options and it turns out the definition variation is very important in motivating different pieces of work.
  
 Newtonian State.  State is the physical pose of the world.  Under this definition, there are  very  many states, often too many for explicit representation.  This is also the definition typically used in games. 
 Abstracted State.  State is an abstracted physical state of the world.  “Is the door open or closed?” “Are you in room A or not?” The number of states is much smaller here.  A basic issue here is: “How do you compute the state from observations?” 
 Mathematical State.  State is a sufficient stati</p><p>3 0.686387 <a title="100-lsi-3" href="../hunch_net-2005/hunch_net-2005-02-23-Problem%3A_Reinforcement_Learning_with_Classification.html">27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</a></p>
<p>Introduction: At an intuitive level, the question here is “Can reinforcement learning be solved with classification?”  
 
 Problem  Construct a reinforcement learning algorithm with near-optimal expected sum of rewards in the  direct experience model  given access to a classifier learning algorithm which has a small error rate or regret on all posed classification problems.  The definition of “posed” here is slightly murky.  I consider a problem “posed” if there is an algorithm for constructing labeled classification examples.
 
 Past Work 
  
 There exists a  reduction of reinforcement learning to classification given a generative model.   A generative model is an inherently stronger assumption than the direct experience model. 
 Other  work on learning reductions  may be important. 
 Several algorithms for solving reinforcement learning in the direct experience model exist.  Most, such as  E 3  ,  Factored-E 3  , and  metric-E 3   and  Rmax  require that the observation be the state.  Recent work</p><p>4 0.64779574 <a title="100-lsi-4" href="../hunch_net-2005/hunch_net-2005-06-17-Reopening_RL-%3EClassification.html">82 hunch net-2005-06-17-Reopening RL->Classification</a></p>
<p>Introduction: In research, it’s often the case that solving a problem helps you realize that it wasn’t the right problem to solve.  This is the case for the “ reduce RL to classification ” problem with the solution hinted at  here  and turned into a paper  here .  
 
The essential difficulty is that the method of stating and analyzing reductions ends up being nonalgorithmic (unlike previous reductions) unless you work with learning from teleoperated robots as  Greg Grudic  does.  The difficulty here is due to the reduction being dependent on the optimal policy (which a human teleoperator might simulate, but which is otherwise unavailable).
 
So, this  problem  is “open” again with the caveat that this time we want a more algorithmic solution.  
 
Whether or not this is feasible at all is still unclear and evidence in either direction would greatly interest me.  A positive answer might have many practical implications in the long run.</p><p>5 0.58106387 <a title="100-lsi-5" href="../hunch_net-2009/hunch_net-2009-06-03-Functionally_defined_Nonlinear_Dynamic_Models.html">359 hunch net-2009-06-03-Functionally defined Nonlinear Dynamic Models</a></p>
<p>Introduction: Suppose we have a set of observations over time  x 1 ,x 2 ,…,x t   and want to predict some future event  y t+1  .  An inevitable problem arises, because learning a predictor  h(x 1 ,…,x t )  of  y t+1   is generically intractable due to the size of the input.  To make this problem tractable, what’s necessary is a method for summarizing the relevant information in past observations for the purpose of prediction in the future.  In other words, state is required.
 
Existing approaches for deriving state have some limitations.
  
  Hidden Markov models  learned with EM suffer from local minima, use tabular learning approaches which provide dubious generalization ability, and often require substantial a.priori specification of the observations. 
  Kalman Filters  and  Particle Filters  are very parametric in the sense that substantial information must be specified up front. 
 Dynamic Bayesian Networks ( graphical models  through time) require substantial a.priori specification and often re</p><p>6 0.579611 <a title="100-lsi-6" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>7 0.56012136 <a title="100-lsi-7" href="../hunch_net-2005/hunch_net-2005-08-08-Apprenticeship_Reinforcement_Learning_for_Control.html">101 hunch net-2005-08-08-Apprenticeship Reinforcement Learning for Control</a></p>
<p>8 0.55486554 <a title="100-lsi-8" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>9 0.53987849 <a title="100-lsi-9" href="../hunch_net-2006/hunch_net-2006-01-13-Benchmarks_for_RL.html">148 hunch net-2006-01-13-Benchmarks for RL</a></p>
<p>10 0.53328329 <a title="100-lsi-10" href="../hunch_net-2005/hunch_net-2005-05-10-Learning_Reductions_are_Reductionist.html">68 hunch net-2005-05-10-Learning Reductions are Reductionist</a></p>
<p>11 0.53150582 <a title="100-lsi-11" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>12 0.51866251 <a title="100-lsi-12" href="../hunch_net-2007/hunch_net-2007-10-24-Contextual_Bandits.html">269 hunch net-2007-10-24-Contextual Bandits</a></p>
<p>13 0.48747027 <a title="100-lsi-13" href="../hunch_net-2005/hunch_net-2005-02-26-Problem%3A_Reductions_and_Relative_Ranking_Metrics.html">31 hunch net-2005-02-26-Problem: Reductions and Relative Ranking Metrics</a></p>
<p>14 0.48651677 <a title="100-lsi-14" href="../hunch_net-2012/hunch_net-2012-03-06-COLT-ICML_Open_Questions_and_ICML_Instructions.html">458 hunch net-2012-03-06-COLT-ICML Open Questions and ICML Instructions</a></p>
<p>15 0.47926846 <a title="100-lsi-15" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>16 0.47921947 <a title="100-lsi-16" href="../hunch_net-2005/hunch_net-2005-01-24-Holy_grails_of_machine_learning%3F.html">2 hunch net-2005-01-24-Holy grails of machine learning?</a></p>
<p>17 0.47297937 <a title="100-lsi-17" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>18 0.47288755 <a title="100-lsi-18" href="../hunch_net-2010/hunch_net-2010-03-15-The_Efficient_Robust_Conditional_Probability_Estimation_Problem.html">391 hunch net-2010-03-15-The Efficient Robust Conditional Probability Estimation Problem</a></p>
<p>19 0.47140631 <a title="100-lsi-19" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<p>20 0.46323717 <a title="100-lsi-20" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(27, 0.245), (28, 0.355), (38, 0.013), (53, 0.105), (55, 0.038), (77, 0.093), (95, 0.039)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.90501946 <a title="100-lda-1" href="../hunch_net-2005/hunch_net-2005-08-04-Why_Reinforcement_Learning_is_Important.html">100 hunch net-2005-08-04-Why Reinforcement Learning is Important</a></p>
<p>Introduction: One prescription for solving a problem well is:
  
 State the problem, in the simplest way possible. In particular, this statement should involve no contamination with or anticipation of the solution. 
 Think about solutions to the stated problem. 
  
Stating a problem in a succinct and crisp manner tends to invite a simple elegant solution.  When a problem can not be stated succinctly, we  wonder if the problem is even understood. (And when a problem is not understood, we wonder if a solution can be meaningful.)
 
Reinforcement learning does step (1) well.  It provides a clean simple language to state general AI problems.  In reinforcement learning there is a set of actions  A , a set of observations  O , and a reward  r .  The reinforcement learning problem, in general, is defined by a conditional measure  D( o, r | (o,r,a) * )  which produces an observation  o  and a reward  r  given a history  (o,r,a) *  .  The goal in reinforcement learning is to find a policy  pi:(o,r,a) *  -> a</p><p>2 0.80352581 <a title="100-lda-2" href="../hunch_net-2005/hunch_net-2005-10-19-Workshop%3A_Atomic_Learning.html">124 hunch net-2005-10-19-Workshop: Atomic Learning</a></p>
<p>Introduction: We are planning to have a workshop on atomic learning Jan 7 & 8 at TTI-Chicago.  
 
 Details are here .
 
The earlier request for interest is  here .
 
The primary deadline is abstracts due Nov. 20 to jl@tti-c.org.</p><p>3 0.59184611 <a title="100-lda-3" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>Introduction: One thing which is clear on a little reflection is that there exists a single master learning problem capable of encoding essentially all learning problems.  This problem is of course a very general sort of reinforcement learning where the world interacts with an agent as:
  
 The world announces an observation  x . 
 The agent makes a choice  a . 
 The world announces a reward  r . 
  
The goal here is to maximize the sum of the rewards over the time of the agent.  No particular structure relating  x  to  a  or  a  to  r  is implied by this setting so we do not know effective general algorithms for the agent.  It’s very easy to prove lower bounds showing that an agent cannot hope to succeed here—just consider the case where actions are unrelated to rewards.  Nevertheless, there is a real sense in which essentially all forms of life are agents operating in this setting, somehow succeeding.  The gap between these observations drives research—How can we find tractable specializations of</p><p>4 0.58756649 <a title="100-lda-4" href="../hunch_net-2008/hunch_net-2008-09-12-How_do_we_get_weak_action_dependence_for_learning_with_partial_observations%3F.html">317 hunch net-2008-09-12-How do we get weak action dependence for learning with partial observations?</a></p>
<p>Introduction: This post is about contextual bandit problems where, repeatedly:
  
 The world chooses features  x  and rewards for each action  r 1 ,…,r k   then announces the features  x  (but not the rewards). 
 A policy chooses an action  a . 
 The world announces the reward  r a   
  
The goal in these situations is to learn a policy which maximizes  r a   in expectation efficiently.  I’m thinking about all situations which fit the above setting, whether they are drawn IID or adversarially from round to round and whether they involve past logged data or rapidly learning via interaction.
 
One common drawback of all algorithms for solving this setting, is that they have a poor dependence on the number of actions.  For example if  k  is the number of actions,  EXP4 (page 66)  has a dependence on  k 0.5  ,  epoch-greedy  (and the simpler epsilon greedy) have a dependence on  k 1/3  , and the  offset tree  has a dependence on  k-1 .  These results aren’t directly comparable because different things a</p><p>5 0.58604354 <a title="100-lda-5" href="../hunch_net-2005/hunch_net-2005-04-23-Advantages_and_Disadvantages_of_Bayesian_Learning.html">60 hunch net-2005-04-23-Advantages and Disadvantages of Bayesian Learning</a></p>
<p>Introduction: I don’t consider myself a “Bayesian”, but I do try hard to understand why Bayesian learning works.  For the purposes of this post, Bayesian learning is a simple process of:
  
 Specify a prior over world models. 
 Integrate using Bayes law with respect to all observed information to compute a posterior over world models. 
 Predict according to the posterior. 
  
Bayesian learning has many advantages over other learning programs:
  
  Interpolation  Bayesian learning methods interpolate all the way to pure engineering.  When faced with any learning problem, there is a choice of how much time and effort a human vs. a computer puts in.  (For example, the mars rover pathfinding algorithms are almost entirely engineered.)  When creating an engineered system, you build a model of the world and then find a good controller in that model.  Bayesian methods interpolate to this extreme because the Bayesian prior can be a delta function on one model of the world.  What this means is that a recipe</p><p>6 0.58234257 <a title="100-lda-6" href="../hunch_net-2013/hunch_net-2013-06-10-The_Large_Scale_Learning_class_notes.html">483 hunch net-2013-06-10-The Large Scale Learning class notes</a></p>
<p>7 0.57962674 <a title="100-lda-7" href="../hunch_net-2006/hunch_net-2006-03-23-The_Approximation_Argument.html">165 hunch net-2006-03-23-The Approximation Argument</a></p>
<p>8 0.57823408 <a title="100-lda-8" href="../hunch_net-2005/hunch_net-2005-02-03-Learning_Theory%2C_by_assumption.html">12 hunch net-2005-02-03-Learning Theory, by assumption</a></p>
<p>9 0.5738976 <a title="100-lda-9" href="../hunch_net-2005/hunch_net-2005-08-08-Apprenticeship_Reinforcement_Learning_for_Control.html">101 hunch net-2005-08-08-Apprenticeship Reinforcement Learning for Control</a></p>
<p>10 0.57180279 <a title="100-lda-10" href="../hunch_net-2013/hunch_net-2013-01-07-NYU_Large_Scale_Machine_Learning_Class.html">478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</a></p>
<p>11 0.57053953 <a title="100-lda-11" href="../hunch_net-2006/hunch_net-2006-09-09-How_to_solve_an_NP_hard_problem_in_quadratic_time.html">206 hunch net-2006-09-09-How to solve an NP hard problem in quadratic time</a></p>
<p>12 0.57031643 <a title="100-lda-12" href="../hunch_net-2007/hunch_net-2007-10-24-Contextual_Bandits.html">269 hunch net-2007-10-24-Contextual Bandits</a></p>
<p>13 0.56948924 <a title="100-lda-13" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>14 0.56824321 <a title="100-lda-14" href="../hunch_net-2005/hunch_net-2005-11-16-The_Everything_Ensemble_Edge.html">131 hunch net-2005-11-16-The Everything Ensemble Edge</a></p>
<p>15 0.56739753 <a title="100-lda-15" href="../hunch_net-2005/hunch_net-2005-02-01-Watchword%3A_Loss.html">9 hunch net-2005-02-01-Watchword: Loss</a></p>
<p>16 0.56724638 <a title="100-lda-16" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>17 0.56663811 <a title="100-lda-17" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<p>18 0.56583524 <a title="100-lda-18" href="../hunch_net-2007/hunch_net-2007-01-10-A_Deep_Belief_Net_Learning_Problem.html">227 hunch net-2007-01-10-A Deep Belief Net Learning Problem</a></p>
<p>19 0.56359673 <a title="100-lda-19" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>20 0.5628652 <a title="100-lda-20" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
