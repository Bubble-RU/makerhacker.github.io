<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>63 hunch net-2005-04-27-DARPA project: LAGR</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-63" href="#">hunch_net-2005-63</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>63 hunch net-2005-04-27-DARPA project: LAGR</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-63-html" href="http://hunch.net/?p=68">html</a></p><p>Introduction: Larry Jackal has set up the  LAGR  (“Learning Applied to Ground Robotics”) project (and competition) which seems to be quite well designed.  Features include:
  
 Many participants (8 going on 12?) 
 Standardized hardware.  In the  DARPA grand challenge  contestants entering with motorcycles are at a severe disadvantage to those entering with a Hummer.  Similarly, contestants using more powerful sensors can gain huge advantages. 
 Monthly contests, with full feedback (but since the hardware is standardized, only code is shipped).  One of the premises of the program is that robust systems are desired.  Monthly evaluations at different locations can help measure this and provide data. 
 Attacks a known hard problem.  (cross country driving)</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Larry Jackal has set up the  LAGR  (“Learning Applied to Ground Robotics”) project (and competition) which seems to be quite well designed. [sent-1, score-0.19]
</p><p>2 Features include:     Many participants (8 going on 12? [sent-2, score-0.174]
</p><p>3 In the  DARPA grand challenge  contestants entering with motorcycles are at a severe disadvantage to those entering with a Hummer. [sent-4, score-1.392]
</p><p>4 Similarly, contestants using more powerful sensors can gain huge advantages. [sent-5, score-0.834]
</p><p>5 Monthly contests, with full feedback (but since the hardware is standardized, only code is shipped). [sent-6, score-0.458]
</p><p>6 One of the premises of the program is that robust systems are desired. [sent-7, score-0.235]
</p><p>7 Monthly evaluations at different locations can help measure this and provide data. [sent-8, score-0.606]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('monthly', 0.389), ('contestants', 0.32), ('entering', 0.302), ('standardized', 0.302), ('contests', 0.173), ('evaluations', 0.173), ('shipped', 0.173), ('lagr', 0.16), ('sensors', 0.16), ('attacks', 0.151), ('country', 0.151), ('larry', 0.151), ('locations', 0.151), ('grand', 0.144), ('ground', 0.144), ('hardware', 0.144), ('robotics', 0.129), ('disadvantage', 0.125), ('darpa', 0.122), ('gain', 0.119), ('driving', 0.116), ('cross', 0.109), ('severe', 0.109), ('powerful', 0.106), ('full', 0.102), ('competition', 0.102), ('participants', 0.1), ('robust', 0.099), ('project', 0.097), ('similarly', 0.092), ('measure', 0.09), ('challenge', 0.09), ('huge', 0.087), ('code', 0.084), ('include', 0.083), ('feedback', 0.079), ('provide', 0.078), ('going', 0.074), ('features', 0.073), ('applied', 0.073), ('known', 0.071), ('help', 0.07), ('systems', 0.069), ('program', 0.067), ('hard', 0.057), ('since', 0.049), ('quite', 0.048), ('set', 0.045), ('different', 0.044), ('using', 0.042)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="63-tfidf-1" href="../hunch_net-2005/hunch_net-2005-04-27-DARPA_project%3A_LAGR.html">63 hunch net-2005-04-27-DARPA project: LAGR</a></p>
<p>Introduction: Larry Jackal has set up the  LAGR  (“Learning Applied to Ground Robotics”) project (and competition) which seems to be quite well designed.  Features include:
  
 Many participants (8 going on 12?) 
 Standardized hardware.  In the  DARPA grand challenge  contestants entering with motorcycles are at a severe disadvantage to those entering with a Hummer.  Similarly, contestants using more powerful sensors can gain huge advantages. 
 Monthly contests, with full feedback (but since the hardware is standardized, only code is shipped).  One of the premises of the program is that robust systems are desired.  Monthly evaluations at different locations can help measure this and provide data. 
 Attacks a known hard problem.  (cross country driving)</p><p>2 0.095207922 <a title="63-tfidf-2" href="../hunch_net-2009/hunch_net-2009-07-31-Vowpal_Wabbit_Open_Source_Project.html">365 hunch net-2009-07-31-Vowpal Wabbit Open Source Project</a></p>
<p>Introduction: Today brings a new release of the  Vowpal Wabbit  fast online learning software.  This time, unlike the previous release, the project itself is going open source, developing via  github .  For example, the lastest and greatest can be downloaded via:
  
git clone git://github.com/JohnLangford/vowpal_wabbit.git
  
If you aren’t familiar with  git , it’s a distributed version control system which supports quick and easy branching, as well as reconciliation.
 
This version of the code is confirmed to compile without complaint on at least some flavors of OSX as well as Linux boxes.
 
As much of the point of this project is pushing the limits of fast and effective machine learning, let me mention a few datapoints from my experience.
  
 The program can effectively scale up to batch-style training on sparse terafeature (i.e. 10 12  sparse feature) size datasets.  The limiting factor is typically i/o. 
 I started using the the real datasets from the  large-scale learning  workshop as a conve</p><p>3 0.077059038 <a title="63-tfidf-3" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>Introduction: One of the common trends in machine learning has been an emphasis on the use of unlabeled data.  The argument goes something like “there aren’t many labeled web pages out there, but there are a  huge  number of web pages, so we must find a way to take advantage of them.”  There are several standard approaches for doing this:
  
  Unsupervised Learning .  You use only unlabeled data.  In a typical application, you cluster the data and hope that the clusters somehow correspond to what you care about. 
 Semisupervised Learning.  You use both unlabeled and labeled data to build a predictor.  The unlabeled data influences the learned predictor in some way. 
  Active Learning . You have unlabeled data and access to a labeling oracle.  You interactively choose which examples to label so as to optimize prediction accuracy. 
  
It seems there is a fourth approach worth serious investigation—automated labeling.  The approach goes as follows:
  
 Identify some subset of observed values to predict</p><p>4 0.074187927 <a title="63-tfidf-4" href="../hunch_net-2009/hunch_net-2009-09-21-Netflix_finishes_%28and_starts%29.html">371 hunch net-2009-09-21-Netflix finishes (and starts)</a></p>
<p>Introduction: I attended the  Netflix prize  ceremony this morning.  The press conference part is  covered fine elsewhere , with the basic outcome being that  BellKor’s Pragmatic Chaos  won over  The Ensemble  by 15-20  minutes , because they were tied in performance on the ultimate holdout set.  I’m sure the individual participants will have many chances to speak about the solution.  One of these is Bell at the  NYAS ML symposium on Nov. 6 .
 
Several additional details may interest ML people.
  
 The degree of overfitting exhibited by the difference in performance on the  leaderboard test set  and the ultimate hold out set was small, but determining at .02 to .03%. 
 A tie was possible, because the rules cut off measurements below the fourth digit based on significance concerns.  In actuality, of course, the scores do differ before rounding, but everyone I spoke to claimed not to know how.  The complete dataset has been  released on UCI , so each team could compute their own score to whatever accu</p><p>5 0.067814857 <a title="63-tfidf-5" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>Introduction: One view of machine learning is that it’s about how to program computers to predict well.  This suggests a broader research program centered around the more pervasive goal of simply predicting well. 
There are many distinct strands of this broader research program which are only partially unified.  Here are the ones that I know of:
  
  Learning Theory .  Learning theory focuses on several topics related to the dynamics and process of prediction.  Convergence bounds like the  VC bound   give an intellectual foundation to many learning algorithms.  Online learning algorithms like  Weighted Majority  provide an alternate purely game theoretic foundation for learning.   Boosting algorithms  yield algorithms for purifying prediction abiliity.   Reduction algorithms  provide means for changing esoteric problems into well known ones. 
  Machine Learning .  A great deal of experience has accumulated in practical algorithm design from a mixture of paradigms, including bayesian, biological, opt</p><p>6 0.06034857 <a title="63-tfidf-6" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>7 0.059444945 <a title="63-tfidf-7" href="../hunch_net-2005/hunch_net-2005-02-21-Problem%3A_Cross_Validation.html">26 hunch net-2005-02-21-Problem: Cross Validation</a></p>
<p>8 0.058356628 <a title="63-tfidf-8" href="../hunch_net-2007/hunch_net-2007-10-19-Second_Annual_Reinforcement_Learning_Competition.html">268 hunch net-2007-10-19-Second Annual Reinforcement Learning Competition</a></p>
<p>9 0.058074109 <a title="63-tfidf-9" href="../hunch_net-2007/hunch_net-2007-11-14-BellKor_wins_Netflix.html">272 hunch net-2007-11-14-BellKor wins Netflix</a></p>
<p>10 0.055930078 <a title="63-tfidf-10" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>11 0.055772882 <a title="63-tfidf-11" href="../hunch_net-2006/hunch_net-2006-02-07-Pittsburgh_Mind_Reading_Competition.html">155 hunch net-2006-02-07-Pittsburgh Mind Reading Competition</a></p>
<p>12 0.049494397 <a title="63-tfidf-12" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>13 0.048822179 <a title="63-tfidf-13" href="../hunch_net-2007/hunch_net-2007-01-26-Parallel_Machine_Learning_Problems.html">229 hunch net-2007-01-26-Parallel Machine Learning Problems</a></p>
<p>14 0.047231767 <a title="63-tfidf-14" href="../hunch_net-2012/hunch_net-2012-05-12-ICML_accepted_papers_and_early_registration.html">465 hunch net-2012-05-12-ICML accepted papers and early registration</a></p>
<p>15 0.047186792 <a title="63-tfidf-15" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>16 0.046996631 <a title="63-tfidf-16" href="../hunch_net-2005/hunch_net-2005-06-13-Wikis_for_Summer_Schools_and_Workshops.html">81 hunch net-2005-06-13-Wikis for Summer Schools and Workshops</a></p>
<p>17 0.046478834 <a title="63-tfidf-17" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>18 0.046193503 <a title="63-tfidf-18" href="../hunch_net-2009/hunch_net-2009-01-19-Netflix_prize_within_epsilon.html">336 hunch net-2009-01-19-Netflix prize within epsilon</a></p>
<p>19 0.04552589 <a title="63-tfidf-19" href="../hunch_net-2006/hunch_net-2006-07-06-Branch_Prediction_Competition.html">190 hunch net-2006-07-06-Branch Prediction Competition</a></p>
<p>20 0.044971276 <a title="63-tfidf-20" href="../hunch_net-2012/hunch_net-2012-06-15-Normal_Deviate_and_the_UCSC_Machine_Learning_Summer_School.html">467 hunch net-2012-06-15-Normal Deviate and the UCSC Machine Learning Summer School</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.083), (1, 0.004), (2, -0.03), (3, 0.002), (4, 0.005), (5, 0.0), (6, -0.033), (7, 0.008), (8, -0.026), (9, 0.005), (10, -0.04), (11, 0.06), (12, 0.006), (13, -0.036), (14, -0.017), (15, -0.03), (16, 0.045), (17, 0.005), (18, 0.016), (19, 0.028), (20, 0.053), (21, -0.004), (22, 0.009), (23, -0.054), (24, -0.017), (25, -0.02), (26, 0.005), (27, -0.004), (28, -0.023), (29, 0.035), (30, 0.024), (31, 0.026), (32, 0.056), (33, -0.047), (34, -0.081), (35, 0.027), (36, -0.027), (37, 0.049), (38, 0.04), (39, -0.049), (40, 0.014), (41, 0.038), (42, 0.024), (43, -0.036), (44, -0.044), (45, 0.074), (46, 0.037), (47, 0.045), (48, 0.095), (49, 0.036)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9603554 <a title="63-lsi-1" href="../hunch_net-2005/hunch_net-2005-04-27-DARPA_project%3A_LAGR.html">63 hunch net-2005-04-27-DARPA project: LAGR</a></p>
<p>Introduction: Larry Jackal has set up the  LAGR  (“Learning Applied to Ground Robotics”) project (and competition) which seems to be quite well designed.  Features include:
  
 Many participants (8 going on 12?) 
 Standardized hardware.  In the  DARPA grand challenge  contestants entering with motorcycles are at a severe disadvantage to those entering with a Hummer.  Similarly, contestants using more powerful sensors can gain huge advantages. 
 Monthly contests, with full feedback (but since the hardware is standardized, only code is shipped).  One of the premises of the program is that robust systems are desired.  Monthly evaluations at different locations can help measure this and provide data. 
 Attacks a known hard problem.  (cross country driving)</p><p>2 0.55003595 <a title="63-lsi-2" href="../hunch_net-2006/hunch_net-2006-02-07-Pittsburgh_Mind_Reading_Competition.html">155 hunch net-2006-02-07-Pittsburgh Mind Reading Competition</a></p>
<p>Introduction: Francisco Pereira  points out a fun  Prediction Competition .   Francisco says:
 
DARPA is sponsoring a competition to analyze data from an unusual functional Magnetic Resonance Imaging experiment. Subjects watch videos inside the scanner while fMRI data are acquired. Unbeknownst to these subjects, the videos have been seen by a panel of other subjects that labeled each instant with labels in categories such as representation (are there tools, body parts, motion, sound), location, presence of actors, emotional content, etc.
 
The challenge is to predict all of these different labels on an instant-by-instant basis from the fMRI data. A few reasons why this is particularly interesting:
  
  This is beyond the current state of the art, but not inconceivably hard. 
  This is a new type of experiment design current analysis methods cannot deal with. 
  This is an opportunity to work with a heavily examined and preprocessed neuroimaging dataset. 
  DARPA is offering prizes!</p><p>3 0.49222538 <a title="63-lsi-3" href="../hunch_net-2005/hunch_net-2005-04-14-Families_of_Learning_Theory_Statements.html">56 hunch net-2005-04-14-Families of Learning Theory Statements</a></p>
<p>Introduction: The diagram above shows a very broad viewpoint of learning theory. 
  
 
 arrow 
 Typical statement 
 Examples 
 
 
 Past->Past 
  Some prediction algorithm  A  does almost as well as any of a set of algorithms. 
 Weighted Majority 
 
 
 Past->Future 
 Assuming independent samples, past performance predicts future performance. 
 PAC analysis, ERM analysis 
 
 
 Future->Future 
 Future prediction performance on subproblems implies future prediction performance using algorithm  A . 
 ECOC, Probing 
 
  
A basic question is: Are there other varieties of statements of this type?   Avrim  noted that there are also “arrows between arrows”: generic methods for transforming between Past->Past statements and Past->Future statements.  Are there others?</p><p>4 0.48492187 <a title="63-lsi-4" href="../hunch_net-2006/hunch_net-2006-04-09-Progress_in_Machine_Translation.html">171 hunch net-2006-04-09-Progress in Machine Translation</a></p>
<p>Introduction: I just visited  ISI  where  Daniel Marcu  and others are working on machine translation.  Apparently, machine translation is rapidly improving.   A particularly dramatic year was 2002->2003 when systems switched from word-based translation to phrase-based translation.  From a (now famous) slide by Charles Wayne at  DARPA  (which funds much of the work on machine translation) here is some anecdotal evidence:
  
 
 2002 
 2003 
 
 
 insistent Wednesday may recurred her trips to Libya tomorrow for flying.

 Cairo 6-4 ( AFP ) – An official announced today in the Egyptian lines company for flying  Tuesday is a company “insistent for flying” may resumed a consideration of a day Wednesday tomorrow her trips to Libya of Security Council decision trace international the imposed ban comment.


 And said the official “the institution sent a speech to Ministry of Foreign Affairs of lifting on Libya air, a situation her recieving replying are so a trip will pull to Libya a morning Wednesday.”

 
 E</p><p>5 0.48098105 <a title="63-lsi-5" href="../hunch_net-2007/hunch_net-2007-10-19-Second_Annual_Reinforcement_Learning_Competition.html">268 hunch net-2007-10-19-Second Annual Reinforcement Learning Competition</a></p>
<p>Introduction: The Second Annual Reinforcement Learning Competition is about to get started.  The aim of the competition is to facilitate direct comparisons between various learning methods on important and realistic domains.  This yearâ&euro;&trade;s event will feature well-known benchmark domains as well as more challenging problems of real-world complexity, such as helicopter control and robot soccer keepaway.
 
The competition begins on November 1st, 2007 when training software is released.   Results must be submitted by July 1st, 2008.  The competition will culminate in an event at ICML-08 in Helsinki, Finland, at which the winners will be announced.
 
For more information, visit   the competition website.</p><p>6 0.44561607 <a title="63-lsi-6" href="../hunch_net-2005/hunch_net-2005-06-13-Wikis_for_Summer_Schools_and_Workshops.html">81 hunch net-2005-06-13-Wikis for Summer Schools and Workshops</a></p>
<p>7 0.41339973 <a title="63-lsi-7" href="../hunch_net-2006/hunch_net-2006-07-06-Branch_Prediction_Competition.html">190 hunch net-2006-07-06-Branch Prediction Competition</a></p>
<p>8 0.4093149 <a title="63-lsi-8" href="../hunch_net-2009/hunch_net-2009-07-31-Vowpal_Wabbit_Open_Source_Project.html">365 hunch net-2009-07-31-Vowpal Wabbit Open Source Project</a></p>
<p>9 0.40533885 <a title="63-lsi-9" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>10 0.40194535 <a title="63-lsi-10" href="../hunch_net-2013/hunch_net-2013-07-24-ICML_2012_videos_lost.html">487 hunch net-2013-07-24-ICML 2012 videos lost</a></p>
<p>11 0.39354053 <a title="63-lsi-11" href="../hunch_net-2005/hunch_net-2005-09-14-The_Predictionist_Viewpoint.html">112 hunch net-2005-09-14-The Predictionist Viewpoint</a></p>
<p>12 0.38302138 <a title="63-lsi-12" href="../hunch_net-2005/hunch_net-2005-12-09-Some_NIPS_papers.html">138 hunch net-2005-12-09-Some NIPS papers</a></p>
<p>13 0.38270429 <a title="63-lsi-13" href="../hunch_net-2010/hunch_net-2010-02-26-Yahoo%21_ML_events.html">389 hunch net-2010-02-26-Yahoo! ML events</a></p>
<p>14 0.37981817 <a title="63-lsi-14" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>15 0.36585405 <a title="63-lsi-15" href="../hunch_net-2007/hunch_net-2007-09-16-Optimizing_Machine_Learning_Programs.html">262 hunch net-2007-09-16-Optimizing Machine Learning Programs</a></p>
<p>16 0.36107183 <a title="63-lsi-16" href="../hunch_net-2005/hunch_net-2005-02-27-Antilearning%3A_When_proximity_goes_bad.html">32 hunch net-2005-02-27-Antilearning: When proximity goes bad</a></p>
<p>17 0.35296732 <a title="63-lsi-17" href="../hunch_net-2005/hunch_net-2005-05-28-Running_A_Machine_Learning_Summer_School.html">75 hunch net-2005-05-28-Running A Machine Learning Summer School</a></p>
<p>18 0.34620416 <a title="63-lsi-18" href="../hunch_net-2005/hunch_net-2005-10-08-We_have_a_winner.html">119 hunch net-2005-10-08-We have a winner</a></p>
<p>19 0.34023607 <a title="63-lsi-19" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>20 0.33484811 <a title="63-lsi-20" href="../hunch_net-2006/hunch_net-2006-09-28-Programming_Languages_for_Machine_Learning_Implementations.html">210 hunch net-2006-09-28-Programming Languages for Machine Learning Implementations</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(27, 0.063), (37, 0.571), (53, 0.037), (55, 0.056), (94, 0.14)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95122838 <a title="63-lda-1" href="../hunch_net-2005/hunch_net-2005-04-27-DARPA_project%3A_LAGR.html">63 hunch net-2005-04-27-DARPA project: LAGR</a></p>
<p>Introduction: Larry Jackal has set up the  LAGR  (“Learning Applied to Ground Robotics”) project (and competition) which seems to be quite well designed.  Features include:
  
 Many participants (8 going on 12?) 
 Standardized hardware.  In the  DARPA grand challenge  contestants entering with motorcycles are at a severe disadvantage to those entering with a Hummer.  Similarly, contestants using more powerful sensors can gain huge advantages. 
 Monthly contests, with full feedback (but since the hardware is standardized, only code is shipped).  One of the premises of the program is that robust systems are desired.  Monthly evaluations at different locations can help measure this and provide data. 
 Attacks a known hard problem.  (cross country driving)</p><p>2 0.82850236 <a title="63-lda-2" href="../hunch_net-2011/hunch_net-2011-04-18-A_paper_not_at_Snowbird.html">431 hunch net-2011-04-18-A paper not at Snowbird</a></p>
<p>Introduction: Unfortunately, a scheduling failure meant I missed all of  AIStat  and most of the  learning workshop , otherwise known as Snowbird, when it’s at  Snowbird .  
 
At snowbird, the talk on  Sum-Product  networks by  Hoifung Poon  stood out to me ( Pedro Domingos  is a coauthor.).  The basic point was that by appropriately constructing networks based on sums and products, the normalization problem in probabilistic models is eliminated, yielding a highly tractable yet flexible representation+learning algorithm.  As an algorithm, this is noticeably cleaner than deep belief networks with a claim to being an order of magnitude faster and working better on an image completion task.
 
Snowbird doesn’t have real papers—just the abstract above.  I look forward to seeing the paper.  (added: Rodrigo points out the deep learning workshop  draft .)</p><p>3 0.58293182 <a title="63-lda-3" href="../hunch_net-2009/hunch_net-2009-08-26-Another_10-year_paper_in_Machine_Learning.html">368 hunch net-2009-08-26-Another 10-year paper in Machine Learning</a></p>
<p>Introduction: When I was thinking about the best “10 year paper” for  ICML , I also took a look at a few other conferences.  Here is one from 10 years ago that interested me:
 
 David McAllester   PAC-Bayesian Model Averaging ,  COLT  1999.    2001 Journal Draft . 
 
Prior to this paper, the only mechanism known for controlling or estimating the necessary sample complexity for learning over continuously parameterized predictors was VC theory and variants, all of which suffered from a basic problem: they were incredibly pessimistic in practice.  This meant that only very gross guidance could be provided for learning algorithm design.  The PAC-Bayes bound provided an alternative approach to sample complexity bounds which was radically tighter, quantitatively.  It also imported and explained many of the motivations for Bayesian learning in a way that learning theory and perhaps optimization people might appreciate.  Since this paper came out, there have been a number of moderately successful attempts t</p><p>4 0.55471241 <a title="63-lda-4" href="../hunch_net-2005/hunch_net-2005-01-19-Why_I_decided_to_run_a_weblog..html">1 hunch net-2005-01-19-Why I decided to run a weblog.</a></p>
<p>Introduction: I have decided to run a weblog on machine learning and learning theory research.  Here are some reasons:
 
1) Weblogs enable new functionality:
  
 Public comment on papers.  No mechanism for this exists at conferences and most journals.  I have encountered it once for a  science  paper.   Some communities have mailing lists supporting this, but not machine learning or learning theory.  I have often read papers and found myself wishing there was some method to consider other’s questions and read the replies. 
 Conference shortlists.  One of the most common conversations at a conference is “what did you find interesting?”  There is no explicit mechanism for sharing this information at conferences, and it’s easy to imagine that it would be handy to do so. 
 Evaluation and comment on research directions.  Papers are almost exclusively about new research, rather than evaluation (and consideration) of research directions.  This last role is satisfied by funding agencies to some extent, but</p><p>5 0.5503543 <a title="63-lda-5" href="../hunch_net-2005/hunch_net-2005-12-09-Some_NIPS_papers.html">138 hunch net-2005-12-09-Some NIPS papers</a></p>
<p>Introduction: Here is a set of papers that I found interesting (and why).
  
  A PAC-Bayes approach to the Set Covering Machine  improves the set covering machine.  The set covering machine approach is a new way to do classification characterized by a very close connection between theory and algorithm.  At this point, the approach seems to be competing well with SVMs in about all dimensions: similar computational speed, similar accuracy, stronger learning theory guarantees, more general information source (a kernel has strictly more structure than a metric), and more sparsity.  Developing a classification algorithm is not very easy, but the results so far are encouraging. 
  Off-Road Obstacle Avoidance through End-to-End Learning  and  Learning Depth from Single Monocular Images  both effectively showed that depth information can be predicted from camera images (using notably different techniques).  This ability is strongly enabling because cameras are cheap, tiny, light, and potentially provider lo</p><p>6 0.42829683 <a title="63-lda-6" href="../hunch_net-2005/hunch_net-2005-02-17-Learning_Research_Programs.html">21 hunch net-2005-02-17-Learning Research Programs</a></p>
<p>7 0.3099511 <a title="63-lda-7" href="../hunch_net-2006/hunch_net-2006-07-11-New_Models.html">194 hunch net-2006-07-11-New Models</a></p>
<p>8 0.30633503 <a title="63-lda-8" href="../hunch_net-2005/hunch_net-2005-12-28-Yet_more_nips_thoughts.html">144 hunch net-2005-12-28-Yet more nips thoughts</a></p>
<p>9 0.28137037 <a title="63-lda-9" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>10 0.27436322 <a title="63-lda-10" href="../hunch_net-2005/hunch_net-2005-06-13-Wikis_for_Summer_Schools_and_Workshops.html">81 hunch net-2005-06-13-Wikis for Summer Schools and Workshops</a></p>
<p>11 0.27380276 <a title="63-lda-11" href="../hunch_net-2005/hunch_net-2005-09-26-Prediction_Bounds_as_the_Mathematics_of_Science.html">115 hunch net-2005-09-26-Prediction Bounds as the Mathematics of Science</a></p>
<p>12 0.26602969 <a title="63-lda-12" href="../hunch_net-2005/hunch_net-2005-03-17-Going_all_the_Way%2C_Sometimes.html">42 hunch net-2005-03-17-Going all the Way, Sometimes</a></p>
<p>13 0.26532036 <a title="63-lda-13" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>14 0.26487941 <a title="63-lda-14" href="../hunch_net-2009/hunch_net-2009-03-18-Parallel_ML_primitives.html">346 hunch net-2009-03-18-Parallel ML primitives</a></p>
<p>15 0.26387656 <a title="63-lda-15" href="../hunch_net-2005/hunch_net-2005-03-04-The_Big_O_and_Constants_in_Learning.html">35 hunch net-2005-03-04-The Big O and Constants in Learning</a></p>
<p>16 0.26151109 <a title="63-lda-16" href="../hunch_net-2006/hunch_net-2006-12-04-Structural_Problems_in_NIPS_Decision_Making.html">221 hunch net-2006-12-04-Structural Problems in NIPS Decision Making</a></p>
<p>17 0.25199634 <a title="63-lda-17" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>18 0.23974031 <a title="63-lda-18" href="../hunch_net-2007/hunch_net-2007-01-26-Parallel_Machine_Learning_Problems.html">229 hunch net-2007-01-26-Parallel Machine Learning Problems</a></p>
<p>19 0.23524919 <a title="63-lda-19" href="../hunch_net-2005/hunch_net-2005-12-07-Is_the_Google_way_the_way_for_machine_learning%3F.html">136 hunch net-2005-12-07-Is the Google way the way for machine learning?</a></p>
<p>20 0.23465149 <a title="63-lda-20" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
