<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>63 hunch net-2005-04-27-DARPA project: LAGR</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-63" href="#">hunch_net-2005-63</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>63 hunch net-2005-04-27-DARPA project: LAGR</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-63-html" href="http://hunch.net/?p=68">html</a></p><p>Introduction: Larry Jackal has set up theLAGR("Learning Applied to Ground Robotics") project
(and competition) which seems to be quite well designed. Features include:Many
participants (8 going on 12?)Standardized hardware. In theDARPA grand
challengecontestants entering with motorcycles are at a severe disadvantage to
those entering with a Hummer. Similarly, contestants using more powerful
sensors can gain huge advantages.Monthly contests, with full feedback (but
since the hardware is standardized, only code is shipped). One of the premises
of the program is that robust systems are desired. Monthly evaluations at
different locations can help measure this and provide data.Attacks a known
hard problem. (cross country driving)</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('entering', 0.343), ('standardized', 0.343), ('contests', 0.196), ('evaluations', 0.196), ('shipped', 0.196), ('thedarpa', 0.196), ('thelagr', 0.196), ('contestants', 0.182), ('sensors', 0.182), ('country', 0.171), ('hardware', 0.171), ('larry', 0.171), ('locations', 0.171), ('grand', 0.163), ('ground', 0.163), ('robotics', 0.151), ('disadvantage', 0.143), ('gain', 0.135), ('driving', 0.132), ('cross', 0.129)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="63-tfidf-1" href="../hunch_net-2005/hunch_net-2005-04-27-DARPA_project%3A_LAGR.html">63 hunch net-2005-04-27-DARPA project: LAGR</a></p>
<p>Introduction: Larry Jackal has set up theLAGR("Learning Applied to Ground Robotics") project
(and competition) which seems to be quite well designed. Features include:Many
participants (8 going on 12?)Standardized hardware. In theDARPA grand
challengecontestants entering with motorcycles are at a severe disadvantage to
those entering with a Hummer. Similarly, contestants using more powerful
sensors can gain huge advantages.Monthly contests, with full feedback (but
since the hardware is standardized, only code is shipped). One of the premises
of the program is that robust systems are desired. Monthly evaluations at
different locations can help measure this and provide data.Attacks a known
hard problem. (cross country driving)</p><p>2 0.10284136 <a title="63-tfidf-2" href="../hunch_net-2009/hunch_net-2009-07-31-Vowpal_Wabbit_Open_Source_Project.html">365 hunch net-2009-07-31-Vowpal Wabbit Open Source Project</a></p>
<p>Introduction: Today brings a new release of theVowpal Wabbitfast online learning software.
This time, unlike the previous release, the project itself is going open
source, developing viagithub. For example, the lastest and greatest can be
downloaded via:git clone git://github.com/JohnLangford/vowpal_wabbit.gitIf you
aren't familiar withgit, it's a distributed version control system which
supports quick and easy branching, as well as reconciliation.This version of
the code is confirmed to compile without complaint on at least some flavors of
OSX as well as Linux boxes.As much of the point of this project is pushing the
limits of fast and effective machine learning, let me mention a few datapoints
from my experience.The program can effectively scale up to batch-style
training on sparse terafeature (i.e. 1012sparse feature) size datasets. The
limiting factor is typically i/o.I started using the the real datasets from
thelarge-scale learningworkshop as a convenient benchmark. The largest dataset
takes a</p><p>3 0.088061586 <a title="63-tfidf-3" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>Introduction: One of the common trends in machine learning has been an emphasis on the use
of unlabeled data. The argument goes something like "there aren't many labeled
web pages out there, but there are ahugenumber of web pages, so we must find a
way to take advantage of them." There are several standard approaches for
doing this:Unsupervised Learning. You use only unlabeled data. In a typical
application, you cluster the data and hope that the clusters somehow
correspond to what you care about.Semisupervised Learning. You use both
unlabeled and labeled data to build a predictor. The unlabeled data influences
the learned predictor in some way.Active Learning. You have unlabeled data and
access to a labeling oracle. You interactively choose which examples to label
so as to optimize prediction accuracy.It seems there is a fourth approach
worth serious investigation--automated labeling. The approach goes as
follows:Identify some subset of observed values to predict from the
others.Build a predictor.U</p><p>4 0.084860653 <a title="63-tfidf-4" href="../hunch_net-2009/hunch_net-2009-09-21-Netflix_finishes_%28and_starts%29.html">371 hunch net-2009-09-21-Netflix finishes (and starts)</a></p>
<p>Introduction: I attended theNetflix prizeceremony this morning. The press conference part
iscovered fine elsewhere, with the basic outcome being thatBellKor's Pragmatic
Chaoswon overThe Ensembleby 15-20minutes, because they were tied in
performance on the ultimate holdout set. I'm sure the individual participants
will have many chances to speak about the solution. One of these is Bell at
theNYAS ML symposium on Nov. 6.Several additional details may interest ML
people.The degree of overfitting exhibited by the difference in performance on
theleaderboard test setand the ultimate hold out set was small, but
determining at .02 to .03%.A tie was possible, because the rules cut off
measurements below the fourth digit based on significance concerns. In
actuality, of course, the scores do differ before rounding, but everyone I
spoke to claimed not to know how. The complete dataset has beenreleased on
UCI, so each team could compute their own score to whatever accuracy desired.I
was impressed by the slick sy</p><p>5 0.082990438 <a title="63-tfidf-5" href="../hunch_net-2005/hunch_net-2005-02-21-Problem%3A_Cross_Validation.html">26 hunch net-2005-02-21-Problem: Cross Validation</a></p>
<p>Introduction: The essential problem here is the large gap between experimental observation
and theoretical understanding.MethodK-fold cross validation is a commonly used
technique which takes a set ofmexamples and partitions them intoKsets
("folds") of sizem/K. For each fold, a classifier is trained on the other
folds and then test on the fold.ProblemAssume only independent samples. Derive
a classifier from the K classifiers with a small bound on the true error
rate.Past Work(I'll add more as I remember/learn.)Devroye, Rogers, and Wagner
analyzed cross validation and found algorithm specific bounds. Not all of this
is online, but here is onepaper.Michael KearnsandDana Ronanalyzed cross
validationand found that under additional stability assumptions the bound for
the classifier which learns on all the data is not much worse than for a test
set of sizem/K.Avrim Blum,Adam Kalai, andmyselfanalyzed cross validationand
found that you can do at least as well as a test set of sizem/Kwith no
additional assum</p><p>6 0.075435527 <a title="63-tfidf-6" href="../hunch_net-2007/hunch_net-2007-10-19-Second_Annual_Reinforcement_Learning_Competition.html">268 hunch net-2007-10-19-Second Annual Reinforcement Learning Competition</a></p>
<p>7 0.074246675 <a title="63-tfidf-7" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>8 0.072711751 <a title="63-tfidf-8" href="../hunch_net-2012/hunch_net-2012-05-12-ICML_accepted_papers_and_early_registration.html">465 hunch net-2012-05-12-ICML accepted papers and early registration</a></p>
<p>9 0.070630535 <a title="63-tfidf-9" href="../hunch_net-2012/hunch_net-2012-06-15-Normal_Deviate_and_the_UCSC_Machine_Learning_Summer_School.html">467 hunch net-2012-06-15-Normal Deviate and the UCSC Machine Learning Summer School</a></p>
<p>10 0.068125404 <a title="63-tfidf-10" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>11 0.063471265 <a title="63-tfidf-11" href="../hunch_net-2007/hunch_net-2007-01-26-Parallel_Machine_Learning_Problems.html">229 hunch net-2007-01-26-Parallel Machine Learning Problems</a></p>
<p>12 0.062654994 <a title="63-tfidf-12" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>13 0.057918981 <a title="63-tfidf-13" href="../hunch_net-2009/hunch_net-2009-01-19-Netflix_prize_within_epsilon.html">336 hunch net-2009-01-19-Netflix prize within epsilon</a></p>
<p>14 0.057115659 <a title="63-tfidf-14" href="../hunch_net-2005/hunch_net-2005-06-13-Wikis_for_Summer_Schools_and_Workshops.html">81 hunch net-2005-06-13-Wikis for Summer Schools and Workshops</a></p>
<p>15 0.054360185 <a title="63-tfidf-15" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>16 0.051919606 <a title="63-tfidf-16" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>17 0.050352853 <a title="63-tfidf-17" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>18 0.046214458 <a title="63-tfidf-18" href="../hunch_net-2011/hunch_net-2011-10-10-ML_Symposium_and_ICML_details.html">447 hunch net-2011-10-10-ML Symposium and ICML details</a></p>
<p>19 0.043472826 <a title="63-tfidf-19" href="../hunch_net-2007/hunch_net-2007-06-23-Machine_Learning_Jobs_are_Growing_on_Trees.html">250 hunch net-2007-06-23-Machine Learning Jobs are Growing on Trees</a></p>
<p>20 0.042179722 <a title="63-tfidf-20" href="../hunch_net-2006/hunch_net-2006-02-27-The_Peekaboom_Dataset.html">159 hunch net-2006-02-27-The Peekaboom Dataset</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.093), (1, -0.004), (2, 0.021), (3, 0.016), (4, -0.011), (5, 0.046), (6, 0.017), (7, 0.027), (8, 0.045), (9, 0.013), (10, -0.056), (11, -0.018), (12, -0.035), (13, -0.046), (14, 0.014), (15, 0.036), (16, 0.013), (17, 0.066), (18, 0.028), (19, -0.068), (20, 0.037), (21, 0.024), (22, 0.042), (23, -0.037), (24, -0.047), (25, 0.006), (26, -0.016), (27, 0.025), (28, 0.002), (29, 0.022), (30, -0.011), (31, -0.023), (32, -0.051), (33, -0.051), (34, -0.064), (35, -0.003), (36, 0.025), (37, -0.063), (38, 0.095), (39, -0.008), (40, -0.019), (41, 0.022), (42, 0.011), (43, -0.018), (44, 0.125), (45, -0.043), (46, 0.002), (47, 0.039), (48, -0.051), (49, 0.041)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96041012 <a title="63-lsi-1" href="../hunch_net-2005/hunch_net-2005-04-27-DARPA_project%3A_LAGR.html">63 hunch net-2005-04-27-DARPA project: LAGR</a></p>
<p>Introduction: Larry Jackal has set up theLAGR("Learning Applied to Ground Robotics") project
(and competition) which seems to be quite well designed. Features include:Many
participants (8 going on 12?)Standardized hardware. In theDARPA grand
challengecontestants entering with motorcycles are at a severe disadvantage to
those entering with a Hummer. Similarly, contestants using more powerful
sensors can gain huge advantages.Monthly contests, with full feedback (but
since the hardware is standardized, only code is shipped). One of the premises
of the program is that robust systems are desired. Monthly evaluations at
different locations can help measure this and provide data.Attacks a known
hard problem. (cross country driving)</p><p>2 0.53906566 <a title="63-lsi-2" href="../hunch_net-2005/hunch_net-2005-06-13-Wikis_for_Summer_Schools_and_Workshops.html">81 hunch net-2005-06-13-Wikis for Summer Schools and Workshops</a></p>
<p>Introduction: Chicago '05ended a couple of weeks ago. This was the sixthMachine Learning
Summer School, and the second one that used awiki. (The first was Berder '04,
thanks to Gunnar Raetsch.) Wikis are relatively easy to set up, greatly aid
social interaction, and should be used a lot more at summer schools and
workshops. They can even be used as the meeting's webpage, as a permanent
record of its participants' collaborations -- see for example the wiki/website
for last year'sNVO Summer School.A basic wiki is a collection of editable
webpages, maintained by software called awiki engine. The engine used at both
Berder and Chicago wasTikiWiki-- it is well documented and gets you something
running fast. It uses PHP and MySQL, but doesn't require you to know either.
Tikiwiki has far more features than most wikis, as it is really a fullContent
Management System. (My thanks to Sebastian Stark for pointing this out.) Here
are the features we found most useful:Bulletin boards, or forums. The most-
used on</p><p>3 0.49935105 <a title="63-lsi-3" href="../hunch_net-2005/hunch_net-2005-12-09-Some_NIPS_papers.html">138 hunch net-2005-12-09-Some NIPS papers</a></p>
<p>Introduction: Here is a set of papers that I found interesting (and why).A PAC-Bayes
approach to the Set Covering Machineimproves the set covering machine. The set
covering machine approach is a new way to do classification characterized by a
very close connection between theory and algorithm. At this point, the
approach seems to be competing well with SVMs in about all dimensions: similar
computational speed, similar accuracy, stronger learning theory guarantees,
more general information source (a kernel has strictly more structure than a
metric), and more sparsity. Developing a classification algorithm is not very
easy, but the results so far are encouraging.Off-Road Obstacle Avoidance
through End-to-End LearningandLearning Depth from Single Monocular Imagesboth
effectively showed that depth information can be predicted from camera images
(using notably different techniques). This ability is strongly enabling
because cameras are cheap, tiny, light, and potentially provider longer range
distance in</p><p>4 0.49821699 <a title="63-lsi-4" href="../hunch_net-2007/hunch_net-2007-09-16-Optimizing_Machine_Learning_Programs.html">262 hunch net-2007-09-16-Optimizing Machine Learning Programs</a></p>
<p>Introduction: Machine learning is often computationally bounded which implies that the
ability to write fast code becomes important if you ever want to implement a
machine learning algorithm. Basic tactical optimizations are covered
wellelsewhere, but I haven't seen a reasonable guide to higher level
optimizations, which are the most important in my experience. Here are some of
the higher level optimizations I've often found useful.Algorithmic Improvement
First. This is Hard, but it is the most important consideration, and typically
yields the most benefits. Good optimizations here are publishable. In the
context of machine learning, you should be familiar with the arguments for
online vs. batch learning.Choice of Language. There are many arguments about
thechoice of language. Sometimes you don't have a choice when interfacing with
other people. Personally, I favor C/C++ when I want to write fast code. This
(admittedly) makes me a slower programmer than when using higher level
languages. (Sometimes</p><p>5 0.48458463 <a title="63-lsi-5" href="../hunch_net-2009/hunch_net-2009-07-31-Vowpal_Wabbit_Open_Source_Project.html">365 hunch net-2009-07-31-Vowpal Wabbit Open Source Project</a></p>
<p>Introduction: Today brings a new release of theVowpal Wabbitfast online learning software.
This time, unlike the previous release, the project itself is going open
source, developing viagithub. For example, the lastest and greatest can be
downloaded via:git clone git://github.com/JohnLangford/vowpal_wabbit.gitIf you
aren't familiar withgit, it's a distributed version control system which
supports quick and easy branching, as well as reconciliation.This version of
the code is confirmed to compile without complaint on at least some flavors of
OSX as well as Linux boxes.As much of the point of this project is pushing the
limits of fast and effective machine learning, let me mention a few datapoints
from my experience.The program can effectively scale up to batch-style
training on sparse terafeature (i.e. 1012sparse feature) size datasets. The
limiting factor is typically i/o.I started using the the real datasets from
thelarge-scale learningworkshop as a convenient benchmark. The largest dataset
takes a</p><p>6 0.46313494 <a title="63-lsi-6" href="../hunch_net-2008/hunch_net-2008-08-24-Mass_Customized_Medicine_in_the_Future%3F.html">314 hunch net-2008-08-24-Mass Customized Medicine in the Future?</a></p>
<p>7 0.45323047 <a title="63-lsi-7" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>8 0.44964138 <a title="63-lsi-8" href="../hunch_net-2012/hunch_net-2012-06-15-Normal_Deviate_and_the_UCSC_Machine_Learning_Summer_School.html">467 hunch net-2012-06-15-Normal Deviate and the UCSC Machine Learning Summer School</a></p>
<p>9 0.44723377 <a title="63-lsi-9" href="../hunch_net-2006/hunch_net-2006-04-09-Progress_in_Machine_Translation.html">171 hunch net-2006-04-09-Progress in Machine Translation</a></p>
<p>10 0.43019426 <a title="63-lsi-10" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>11 0.42318186 <a title="63-lsi-11" href="../hunch_net-2005/hunch_net-2005-11-05-The_design_of_a_computing_cluster.html">128 hunch net-2005-11-05-The design of a computing cluster</a></p>
<p>12 0.41396582 <a title="63-lsi-12" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>13 0.40294921 <a title="63-lsi-13" href="../hunch_net-2005/hunch_net-2005-01-26-Summer_Schools.html">4 hunch net-2005-01-26-Summer Schools</a></p>
<p>14 0.39806387 <a title="63-lsi-14" href="../hunch_net-2007/hunch_net-2007-01-26-Parallel_Machine_Learning_Problems.html">229 hunch net-2007-01-26-Parallel Machine Learning Problems</a></p>
<p>15 0.39290288 <a title="63-lsi-15" href="../hunch_net-2009/hunch_net-2009-09-21-Netflix_finishes_%28and_starts%29.html">371 hunch net-2009-09-21-Netflix finishes (and starts)</a></p>
<p>16 0.39217541 <a title="63-lsi-16" href="../hunch_net-2005/hunch_net-2005-05-28-Running_A_Machine_Learning_Summer_School.html">75 hunch net-2005-05-28-Running A Machine Learning Summer School</a></p>
<p>17 0.38900712 <a title="63-lsi-17" href="../hunch_net-2008/hunch_net-2008-04-30-Concerns_about_the_Large_Scale_Learning_Challenge.html">300 hunch net-2008-04-30-Concerns about the Large Scale Learning Challenge</a></p>
<p>18 0.38504711 <a title="63-lsi-18" href="../hunch_net-2005/hunch_net-2005-02-04-JMLG.html">13 hunch net-2005-02-04-JMLG</a></p>
<p>19 0.38316262 <a title="63-lsi-19" href="../hunch_net-2012/hunch_net-2012-05-12-ICML_accepted_papers_and_early_registration.html">465 hunch net-2012-05-12-ICML accepted papers and early registration</a></p>
<p>20 0.37618306 <a title="63-lsi-20" href="../hunch_net-2010/hunch_net-2010-04-14-MLcomp%3A_a_website_for_objectively_comparing_ML_algorithms.html">393 hunch net-2010-04-14-MLcomp: a website for objectively comparing ML algorithms</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(42, 0.205), (48, 0.586), (74, 0.076)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9122687 <a title="63-lda-1" href="../hunch_net-2005/hunch_net-2005-04-27-DARPA_project%3A_LAGR.html">63 hunch net-2005-04-27-DARPA project: LAGR</a></p>
<p>Introduction: Larry Jackal has set up theLAGR("Learning Applied to Ground Robotics") project
(and competition) which seems to be quite well designed. Features include:Many
participants (8 going on 12?)Standardized hardware. In theDARPA grand
challengecontestants entering with motorcycles are at a severe disadvantage to
those entering with a Hummer. Similarly, contestants using more powerful
sensors can gain huge advantages.Monthly contests, with full feedback (but
since the hardware is standardized, only code is shipped). One of the premises
of the program is that robust systems are desired. Monthly evaluations at
different locations can help measure this and provide data.Attacks a known
hard problem. (cross country driving)</p><p>2 0.76135993 <a title="63-lda-2" href="../hunch_net-2005/hunch_net-2005-04-14-Families_of_Learning_Theory_Statements.html">56 hunch net-2005-04-14-Families of Learning Theory Statements</a></p>
<p>Introduction: The diagram above shows a very broad viewpoint of learning theory.arrowTypical
statementExamplesPast->PastSome prediction algorithmAdoes almost as well as
any of a set of algorithms.Weighted MajorityPast->FutureAssuming independent
samples, past performance predicts future performance.PAC analysis, ERM
analysisFuture->FutureFuture prediction performance on subproblems implies
future prediction performance using algorithmA.ECOC, ProbingA basic question
is: Are there other varieties of statements of this type?Avrimnoted that there
are also "arrows between arrows": generic methods for transforming between
Past->Past statements and Past->Future statements. Are there others?</p><p>3 0.68743229 <a title="63-lda-3" href="../hunch_net-2005/hunch_net-2005-09-14-The_Predictionist_Viewpoint.html">112 hunch net-2005-09-14-The Predictionist Viewpoint</a></p>
<p>Introduction: Virtually every discipline of significant human endeavor has a way explaining
itself as fundamental and important. In all the cases I know of, they are both
right (they are vital) and wrong (they are not solely vital).Politics. This is
the one that everyone is familiar with at the moment. "What could be more
important than the process of making decisions?"Science and Technology. This
is the one that we-the-academics are familiar with. "The loss of modern
science and technology would be catastrophic."Military. "Without the military,
a nation will be invaded and destroyed."(insert your favorite here)Within
science and technology, the same thing happens again.Mathematics. "What could
be more important than a precise language for establishing truths?"Physics.
"Nothing is more fundamental than the laws which govern the universe.
Understanding them is the key to understanding everything else."Biology.
"Without life, we wouldn't be here, so clearly the study of life is
fundamental."Computer S</p><p>4 0.66258967 <a title="63-lda-4" href="../hunch_net-2007/hunch_net-2007-09-16-Optimizing_Machine_Learning_Programs.html">262 hunch net-2007-09-16-Optimizing Machine Learning Programs</a></p>
<p>Introduction: Machine learning is often computationally bounded which implies that the
ability to write fast code becomes important if you ever want to implement a
machine learning algorithm. Basic tactical optimizations are covered
wellelsewhere, but I haven't seen a reasonable guide to higher level
optimizations, which are the most important in my experience. Here are some of
the higher level optimizations I've often found useful.Algorithmic Improvement
First. This is Hard, but it is the most important consideration, and typically
yields the most benefits. Good optimizations here are publishable. In the
context of machine learning, you should be familiar with the arguments for
online vs. batch learning.Choice of Language. There are many arguments about
thechoice of language. Sometimes you don't have a choice when interfacing with
other people. Personally, I favor C/C++ when I want to write fast code. This
(admittedly) makes me a slower programmer than when using higher level
languages. (Sometimes</p><p>5 0.64758718 <a title="63-lda-5" href="../hunch_net-2005/hunch_net-2005-05-17-A_Short_Guide_to_PhD_Graduate_Study.html">73 hunch net-2005-05-17-A Short Guide to PhD Graduate Study</a></p>
<p>Introduction: Graduate study is a mysterious and uncertain process. This easiest way to see
this is by noting that a very old advisor/student mechanism is preferred.
There is no known succesful mechanism for "mass producing" PhDs as is done (in
some sense) for undergraduate and masters study. Here are a few hints that
might be useful to prospective or current students based on my own
experience.Masters or PhD(a) You want a PhD if you want to do research. (b)
You want a masters if you want to make money. People wanting (b) will be
manifestly unhappy with (a) because it typically means years of low pay.
People wanting (a) should try to avoid (b) because it prolongs an already long
process.Attitude.Manystudents struggle for awhile with the wrong attitude
towards research. Most students come into graduate school with 16-19 years of
schooling where the principle means of success is proving that you know
something via assignments, tests, etcâ&euro;Ś Research doesnotwork this way. Research
is what a PhD is about.</p><p>6 0.57833308 <a title="63-lda-6" href="../hunch_net-2009/hunch_net-2009-08-03-Carbon_in_Computer_Science_Research.html">366 hunch net-2009-08-03-Carbon in Computer Science Research</a></p>
<p>7 0.38337556 <a title="63-lda-7" href="../hunch_net-2006/hunch_net-2006-09-28-Programming_Languages_for_Machine_Learning_Implementations.html">210 hunch net-2006-09-28-Programming Languages for Machine Learning Implementations</a></p>
<p>8 0.36686516 <a title="63-lda-8" href="../hunch_net-2012/hunch_net-2012-09-29-Vowpal_Wabbit%2C_version_7.0.html">473 hunch net-2012-09-29-Vowpal Wabbit, version 7.0</a></p>
<p>9 0.3664518 <a title="63-lda-9" href="../hunch_net-2005/hunch_net-2005-09-10-%26%238220%3BFailure%26%238221%3B_is_an_option.html">110 hunch net-2005-09-10-&#8220;Failure&#8221; is an option</a></p>
<p>10 0.34689039 <a title="63-lda-10" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">23 hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<p>11 0.34676626 <a title="63-lda-11" href="../hunch_net-2011/hunch_net-2011-11-26-Giving_Thanks.html">449 hunch net-2011-11-26-Giving Thanks</a></p>
<p>12 0.34643504 <a title="63-lda-12" href="../hunch_net-2009/hunch_net-2009-02-22-Effective_Research_Funding.html">344 hunch net-2009-02-22-Effective Research Funding</a></p>
<p>13 0.34557548 <a title="63-lda-13" href="../hunch_net-2009/hunch_net-2009-11-29-AI_Safety.html">380 hunch net-2009-11-29-AI Safety</a></p>
<p>14 0.34111008 <a title="63-lda-14" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>15 0.34072357 <a title="63-lda-15" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>16 0.34008431 <a title="63-lda-16" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>17 0.33986241 <a title="63-lda-17" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>18 0.33976915 <a title="63-lda-18" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>19 0.33941472 <a title="63-lda-19" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>20 0.33937666 <a title="63-lda-20" href="../hunch_net-2005/hunch_net-2005-03-21-Research_Styles_in_Machine_Learning.html">44 hunch net-2005-03-21-Research Styles in Machine Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
