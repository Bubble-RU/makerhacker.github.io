<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>64 hunch net-2005-04-28-Science Fiction and Research</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-64" href="#">hunch_net-2005-64</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>64 hunch net-2005-04-28-Science Fiction and Research</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-64-html" href="http://hunch.net/?p=69">html</a></p><p>Introduction: A big part of doing research is imagining how things could be different, and then trying to figure out how to get there.  
 
A big part of science fiction is imagining how things could be different, and then working through the implications.  
 
Because of the similarity here, reading science fiction can sometimes be helpful in understanding and doing research.  (And, hey, it’s fun.)  Here’s some list of science fiction books I enjoyed which seem particularly relevant to computer science and (sometimes) learning systems:
  
 Vernor Vinge, “True Names”, “A Fire Upon the Deep” 
 Marc Stiegler, “David’s Sling”, “Earthweb” 
 Charles Stross, “Singularity Sky” 
 Greg Egan, “Diaspora” 
 Joe Haldeman, “Forever Peace” 
  
(There are surely many others.)  
 
Incidentally, the nature of science fiction itself has changed.  Decades ago, science fiction projected great increases in the power humans control (example: E.E. Smith Lensman series).  That didn’t really happen in the last 50 years.  Inste</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 A big part of doing research is imagining how things could be different, and then trying to figure out how to get there. [sent-1, score-0.614]
</p><p>2 A big part of science fiction is imagining how things could be different, and then working through the implications. [sent-2, score-1.553]
</p><p>3 Because of the similarity here, reading science fiction can sometimes be helpful in understanding and doing research. [sent-3, score-1.247]
</p><p>4 )     Incidentally, the nature of science fiction itself has changed. [sent-6, score-1.085]
</p><p>5 Decades ago, science fiction projected great increases in the power humans control (example: E. [sent-7, score-1.496]
</p><p>6 Instead, we gradually refined the degree to which we can control various kinds of power. [sent-11, score-0.469]
</p><p>7 This can be understood as a shift from physics-based progress to engineering or computer science based progress. [sent-13, score-0.813]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('fiction', 0.635), ('science', 0.387), ('imagining', 0.228), ('control', 0.142), ('progress', 0.115), ('joe', 0.114), ('diaspora', 0.114), ('forever', 0.114), ('marc', 0.114), ('projected', 0.114), ('smith', 0.114), ('books', 0.106), ('greg', 0.106), ('computer', 0.101), ('charles', 0.1), ('singularity', 0.1), ('decades', 0.095), ('fire', 0.095), ('gradually', 0.095), ('refined', 0.095), ('part', 0.094), ('reflect', 0.091), ('similarity', 0.091), ('names', 0.088), ('increases', 0.085), ('incidentally', 0.085), ('big', 0.084), ('figure', 0.083), ('series', 0.077), ('engineering', 0.075), ('changed', 0.075), ('shift', 0.072), ('didn', 0.072), ('sometimes', 0.072), ('surely', 0.071), ('degree', 0.069), ('humans', 0.069), ('kinds', 0.068), ('could', 0.065), ('enjoyed', 0.064), ('power', 0.064), ('nature', 0.063), ('understood', 0.063), ('reading', 0.062), ('ago', 0.061), ('david', 0.061), ('upon', 0.06), ('things', 0.06), ('happen', 0.059), ('different', 0.059)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="64-tfidf-1" href="../hunch_net-2005/hunch_net-2005-04-28-Science_Fiction_and_Research.html">64 hunch net-2005-04-28-Science Fiction and Research</a></p>
<p>Introduction: A big part of doing research is imagining how things could be different, and then trying to figure out how to get there.  
 
A big part of science fiction is imagining how things could be different, and then working through the implications.  
 
Because of the similarity here, reading science fiction can sometimes be helpful in understanding and doing research.  (And, hey, it’s fun.)  Here’s some list of science fiction books I enjoyed which seem particularly relevant to computer science and (sometimes) learning systems:
  
 Vernor Vinge, “True Names”, “A Fire Upon the Deep” 
 Marc Stiegler, “David’s Sling”, “Earthweb” 
 Charles Stross, “Singularity Sky” 
 Greg Egan, “Diaspora” 
 Joe Haldeman, “Forever Peace” 
  
(There are surely many others.)  
 
Incidentally, the nature of science fiction itself has changed.  Decades ago, science fiction projected great increases in the power humans control (example: E.E. Smith Lensman series).  That didn’t really happen in the last 50 years.  Inste</p><p>2 0.12644678 <a title="64-tfidf-2" href="../hunch_net-2007/hunch_net-2007-01-15-The_Machine_Learning_Department.html">228 hunch net-2007-01-15-The Machine Learning Department</a></p>
<p>Introduction: Carnegie Mellon   School of Computer Science  has the first academic  Machine Learning department .  This department already existed as the  Center for Automated Learning and Discovery , but recently changed it’s name.  
 
The reason for changing the name is obvious: very few people think of themselves as “Automated Learner and Discoverers”, but there are number of people who think of themselves as “Machine Learners”.  Machine learning is both more succinct and recognizable—good properties for a name.
 
A more interesting question is “Should there be a Machine Learning Department?”.    Tom Mitchell  has a relevant  whitepaper  claiming that machine learning  is answering a different question than other fields or departments.  The fundamental debate here is “Is machine learning different from statistics?”  
 
At a cultural level, there is no real debate: they are different.  Machine learning is characterized by several very active large peer reviewed conferences, operating in a computer</p><p>3 0.10635213 <a title="64-tfidf-3" href="../hunch_net-2008/hunch_net-2008-01-06-Research_Political_Issues.html">282 hunch net-2008-01-06-Research Political Issues</a></p>
<p>Introduction: I’ve avoided discussing politics here, although not for lack of interest.  The problem with discussing politics is that it’s customary for people to say much based upon little information.  Nevertheless, politics can have a substantial impact on science (and we might hope for the vice-versa).  It’s primary election time in the United States, so the topic is timely, although the issues are not.
 
There are several policy decisions which substantially effect development of science and technology in the US.
  
  Education  The US has great contrasts in education.  The top universities are very good places, yet the grade school education system produces mediocre results.  For me, the contrast between a  public education  and  Caltech  was bracing.  For many others attending Caltech, it clearly was not.  Upgrading the k-12 education system in the US is a long-standing chronic problem which I know relatively little about.  My own experience is that a basic attitude of “no child unrealized” i</p><p>4 0.10191292 <a title="64-tfidf-4" href="../hunch_net-2011/hunch_net-2011-11-26-Giving_Thanks.html">449 hunch net-2011-11-26-Giving Thanks</a></p>
<p>Introduction: Thanksgiving is perhaps my favorite holiday, because pausing your life and giving thanks provides a needed moment of perspective.
 
As a researcher, I am most thankful for my education, without which I could not function.  I want to share this, because it provides some sense of how a researcher starts.
  
 My long term memory seems to function particularly well, which makes any education I get is particularly useful. 
 I am naturally obsessive, which makes me chase down details until I fully understand things.  Natural obsessiveness can go wrong, of course, but it’s a great ally when you absolutely must get things right. 
 My childhood was all in one hometown, which was a conscious sacrifice on the part of my father, implying disruptions from moving around were eliminated.  I’m not sure how important this was since travel has it’s own benefits, but it bears thought. 
 I had several great teachers in grade school, and naturally gravitated towards teachers over classmates, as they seemed</p><p>5 0.089465827 <a title="64-tfidf-5" href="../hunch_net-2009/hunch_net-2009-01-28-Nielsen%26%238217%3Bs_talk.html">340 hunch net-2009-01-28-Nielsen&#8217;s talk</a></p>
<p>Introduction: I wanted to point to   Michael Nielsenâ&euro;&trade;s talk  about blogging science, which I found interesting.</p><p>6 0.087652594 <a title="64-tfidf-6" href="../hunch_net-2009/hunch_net-2009-08-03-Carbon_in_Computer_Science_Research.html">366 hunch net-2009-08-03-Carbon in Computer Science Research</a></p>
<p>7 0.084184438 <a title="64-tfidf-7" href="../hunch_net-2005/hunch_net-2005-09-04-Science_in_the_Government.html">106 hunch net-2005-09-04-Science in the Government</a></p>
<p>8 0.083430693 <a title="64-tfidf-8" href="../hunch_net-2005/hunch_net-2005-09-14-The_Predictionist_Viewpoint.html">112 hunch net-2005-09-14-The Predictionist Viewpoint</a></p>
<p>9 0.082564637 <a title="64-tfidf-9" href="../hunch_net-2008/hunch_net-2008-04-21-The_Science_2.0_article.html">296 hunch net-2008-04-21-The Science 2.0 article</a></p>
<p>10 0.082154371 <a title="64-tfidf-10" href="../hunch_net-2008/hunch_net-2008-04-12-It_Doesn%26%238217%3Bt_Stop.html">295 hunch net-2008-04-12-It Doesn&#8217;t Stop</a></p>
<p>11 0.079131074 <a title="64-tfidf-11" href="../hunch_net-2005/hunch_net-2005-02-10-Conferences%2C_Dates%2C_Locations.html">17 hunch net-2005-02-10-Conferences, Dates, Locations</a></p>
<p>12 0.071720771 <a title="64-tfidf-12" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>13 0.068697706 <a title="64-tfidf-13" href="../hunch_net-2010/hunch_net-2010-10-17-Partha_Niyogi_has_died.html">414 hunch net-2010-10-17-Partha Niyogi has died</a></p>
<p>14 0.067920491 <a title="64-tfidf-14" href="../hunch_net-2005/hunch_net-2005-02-02-Kolmogorov_Complexity_and_Googling.html">10 hunch net-2005-02-02-Kolmogorov Complexity and Googling</a></p>
<p>15 0.065498598 <a title="64-tfidf-15" href="../hunch_net-2008/hunch_net-2008-11-28-A_Bumper_Crop_of_Machine_Learning_Graduates.html">329 hunch net-2008-11-28-A Bumper Crop of Machine Learning Graduates</a></p>
<p>16 0.060593344 <a title="64-tfidf-16" href="../hunch_net-2005/hunch_net-2005-05-10-Learning_Reductions_are_Reductionist.html">68 hunch net-2005-05-10-Learning Reductions are Reductionist</a></p>
<p>17 0.054398909 <a title="64-tfidf-17" href="../hunch_net-2005/hunch_net-2005-03-28-Open_Problems_for_Colt.html">47 hunch net-2005-03-28-Open Problems for Colt</a></p>
<p>18 0.052873902 <a title="64-tfidf-18" href="../hunch_net-2009/hunch_net-2009-05-19-CI_Fellows.html">355 hunch net-2009-05-19-CI Fellows</a></p>
<p>19 0.051952615 <a title="64-tfidf-19" href="../hunch_net-2010/hunch_net-2010-04-28-CI_Fellows_program_renewed.html">396 hunch net-2010-04-28-CI Fellows program renewed</a></p>
<p>20 0.051700268 <a title="64-tfidf-20" href="../hunch_net-2011/hunch_net-2011-09-28-Somebody%26%238217%3Bs_Eating_Your_Lunch.html">445 hunch net-2011-09-28-Somebody&#8217;s Eating Your Lunch</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.1), (1, -0.024), (2, -0.062), (3, 0.071), (4, -0.038), (5, -0.042), (6, 0.016), (7, 0.043), (8, 0.014), (9, -0.001), (10, 0.055), (11, -0.014), (12, -0.05), (13, -0.054), (14, -0.037), (15, -0.053), (16, 0.054), (17, 0.049), (18, 0.03), (19, 0.029), (20, 0.02), (21, 0.004), (22, -0.06), (23, 0.06), (24, -0.043), (25, -0.074), (26, 0.074), (27, 0.046), (28, 0.039), (29, 0.106), (30, -0.033), (31, 0.096), (32, -0.014), (33, -0.013), (34, 0.076), (35, 0.016), (36, -0.076), (37, 0.048), (38, -0.044), (39, 0.067), (40, 0.028), (41, 0.058), (42, -0.043), (43, 0.053), (44, 0.027), (45, 0.043), (46, 0.006), (47, -0.012), (48, 0.08), (49, -0.056)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98813391 <a title="64-lsi-1" href="../hunch_net-2005/hunch_net-2005-04-28-Science_Fiction_and_Research.html">64 hunch net-2005-04-28-Science Fiction and Research</a></p>
<p>Introduction: A big part of doing research is imagining how things could be different, and then trying to figure out how to get there.  
 
A big part of science fiction is imagining how things could be different, and then working through the implications.  
 
Because of the similarity here, reading science fiction can sometimes be helpful in understanding and doing research.  (And, hey, it’s fun.)  Here’s some list of science fiction books I enjoyed which seem particularly relevant to computer science and (sometimes) learning systems:
  
 Vernor Vinge, “True Names”, “A Fire Upon the Deep” 
 Marc Stiegler, “David’s Sling”, “Earthweb” 
 Charles Stross, “Singularity Sky” 
 Greg Egan, “Diaspora” 
 Joe Haldeman, “Forever Peace” 
  
(There are surely many others.)  
 
Incidentally, the nature of science fiction itself has changed.  Decades ago, science fiction projected great increases in the power humans control (example: E.E. Smith Lensman series).  That didn’t really happen in the last 50 years.  Inste</p><p>2 0.65442008 <a title="64-lsi-2" href="../hunch_net-2005/hunch_net-2005-09-14-The_Predictionist_Viewpoint.html">112 hunch net-2005-09-14-The Predictionist Viewpoint</a></p>
<p>Introduction: Virtually every discipline of significant human endeavor has a way explaining itself as fundamental and important.  In all the cases I know of, they are both right (they are vital) and wrong (they are not solely vital).
  
 Politics.  This is the one that everyone is familiar with at the moment.  “What could be more important than the process of making decisions?” 
 Science and Technology.  This is the one that we-the-academics are familiar with.  “The loss of modern science and technology would be catastrophic.” 
 Military.  “Without the military, a nation will be invaded and destroyed.” 
 (insert your favorite here) 
  
Within science and technology, the same thing happens again.
  
 Mathematics. “What could be more important than a precise language for establishing truths?” 
 Physics.  “Nothing is more fundamental than the laws which govern the universe.  Understanding them is the key to understanding everything else.” 
 Biology.  “Without life, we wouldn’t be here, so clearly the s</p><p>3 0.63899028 <a title="64-lsi-3" href="../hunch_net-2008/hunch_net-2008-01-06-Research_Political_Issues.html">282 hunch net-2008-01-06-Research Political Issues</a></p>
<p>Introduction: I’ve avoided discussing politics here, although not for lack of interest.  The problem with discussing politics is that it’s customary for people to say much based upon little information.  Nevertheless, politics can have a substantial impact on science (and we might hope for the vice-versa).  It’s primary election time in the United States, so the topic is timely, although the issues are not.
 
There are several policy decisions which substantially effect development of science and technology in the US.
  
  Education  The US has great contrasts in education.  The top universities are very good places, yet the grade school education system produces mediocre results.  For me, the contrast between a  public education  and  Caltech  was bracing.  For many others attending Caltech, it clearly was not.  Upgrading the k-12 education system in the US is a long-standing chronic problem which I know relatively little about.  My own experience is that a basic attitude of “no child unrealized” i</p><p>4 0.6098361 <a title="64-lsi-4" href="../hunch_net-2005/hunch_net-2005-09-04-Science_in_the_Government.html">106 hunch net-2005-09-04-Science in the Government</a></p>
<p>Introduction: I found the article on “ Political Science ” at the  New York Times  interesting.  Essentially the article is about allegations that the US government has been systematically distorting scientific views.   With a  petition  by some  7000+ scientists  alleging such behavior this is clearly a significant concern.
 
One thing not mentioned explicitly in this discussion is that there are fundamental cultural differences between academic research and the rest of the world.  In academic research, careful, clear thought is valued.  This value is achieved by both formal and informal mechanisms.  One example of a formal mechanism is peer review.
 
In contrast, in the land of politics, the basic value is agreement.  It is only with some amount of agreement that a new law can be passed or other actions can be taken.  Since Science (with a capitol ‘S’) has accomplished many things, it can be a significant tool in persuading people.  This makes it compelling for a politician to use science as a mec</p><p>5 0.59507859 <a title="64-lsi-5" href="../hunch_net-2009/hunch_net-2009-01-28-Nielsen%26%238217%3Bs_talk.html">340 hunch net-2009-01-28-Nielsen&#8217;s talk</a></p>
<p>Introduction: I wanted to point to   Michael Nielsenâ&euro;&trade;s talk  about blogging science, which I found interesting.</p><p>6 0.58376282 <a title="64-lsi-6" href="../hunch_net-2007/hunch_net-2007-01-15-The_Machine_Learning_Department.html">228 hunch net-2007-01-15-The Machine Learning Department</a></p>
<p>7 0.5648061 <a title="64-lsi-7" href="../hunch_net-2010/hunch_net-2010-10-17-Partha_Niyogi_has_died.html">414 hunch net-2010-10-17-Partha Niyogi has died</a></p>
<p>8 0.55255932 <a title="64-lsi-8" href="../hunch_net-2009/hunch_net-2009-08-03-Carbon_in_Computer_Science_Research.html">366 hunch net-2009-08-03-Carbon in Computer Science Research</a></p>
<p>9 0.54750127 <a title="64-lsi-9" href="../hunch_net-2005/hunch_net-2005-02-02-Kolmogorov_Complexity_and_Googling.html">10 hunch net-2005-02-02-Kolmogorov Complexity and Googling</a></p>
<p>10 0.51309866 <a title="64-lsi-10" href="../hunch_net-2008/hunch_net-2008-04-21-The_Science_2.0_article.html">296 hunch net-2008-04-21-The Science 2.0 article</a></p>
<p>11 0.44574296 <a title="64-lsi-11" href="../hunch_net-2010/hunch_net-2010-01-13-Sam_Roweis_died.html">386 hunch net-2010-01-13-Sam Roweis died</a></p>
<p>12 0.44453356 <a title="64-lsi-12" href="../hunch_net-2008/hunch_net-2008-11-28-A_Bumper_Crop_of_Machine_Learning_Graduates.html">329 hunch net-2008-11-28-A Bumper Crop of Machine Learning Graduates</a></p>
<p>13 0.42663959 <a title="64-lsi-13" href="../hunch_net-2008/hunch_net-2008-04-12-It_Doesn%26%238217%3Bt_Stop.html">295 hunch net-2008-04-12-It Doesn&#8217;t Stop</a></p>
<p>14 0.42592531 <a title="64-lsi-14" href="../hunch_net-2009/hunch_net-2009-05-19-CI_Fellows.html">355 hunch net-2009-05-19-CI Fellows</a></p>
<p>15 0.42091247 <a title="64-lsi-15" href="../hunch_net-2005/hunch_net-2005-09-06-A_link.html">108 hunch net-2005-09-06-A link</a></p>
<p>16 0.39934489 <a title="64-lsi-16" href="../hunch_net-2008/hunch_net-2008-08-24-Mass_Customized_Medicine_in_the_Future%3F.html">314 hunch net-2008-08-24-Mass Customized Medicine in the Future?</a></p>
<p>17 0.39838874 <a title="64-lsi-17" href="../hunch_net-2006/hunch_net-2006-01-06-MLTV.html">146 hunch net-2006-01-06-MLTV</a></p>
<p>18 0.39548159 <a title="64-lsi-18" href="../hunch_net-2005/hunch_net-2005-04-01-Basic_computer_science_research_takes_a_hit.html">50 hunch net-2005-04-01-Basic computer science research takes a hit</a></p>
<p>19 0.3895703 <a title="64-lsi-19" href="../hunch_net-2005/hunch_net-2005-05-11-Visa_Casualties.html">69 hunch net-2005-05-11-Visa Casualties</a></p>
<p>20 0.38917142 <a title="64-lsi-20" href="../hunch_net-2005/hunch_net-2005-05-10-Learning_Reductions_are_Reductionist.html">68 hunch net-2005-05-10-Learning Reductions are Reductionist</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(3, 0.02), (27, 0.153), (53, 0.087), (55, 0.039), (94, 0.09), (99, 0.479)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.87562537 <a title="64-lda-1" href="../hunch_net-2005/hunch_net-2005-04-28-Science_Fiction_and_Research.html">64 hunch net-2005-04-28-Science Fiction and Research</a></p>
<p>Introduction: A big part of doing research is imagining how things could be different, and then trying to figure out how to get there.  
 
A big part of science fiction is imagining how things could be different, and then working through the implications.  
 
Because of the similarity here, reading science fiction can sometimes be helpful in understanding and doing research.  (And, hey, it’s fun.)  Here’s some list of science fiction books I enjoyed which seem particularly relevant to computer science and (sometimes) learning systems:
  
 Vernor Vinge, “True Names”, “A Fire Upon the Deep” 
 Marc Stiegler, “David’s Sling”, “Earthweb” 
 Charles Stross, “Singularity Sky” 
 Greg Egan, “Diaspora” 
 Joe Haldeman, “Forever Peace” 
  
(There are surely many others.)  
 
Incidentally, the nature of science fiction itself has changed.  Decades ago, science fiction projected great increases in the power humans control (example: E.E. Smith Lensman series).  That didn’t really happen in the last 50 years.  Inste</p><p>2 0.70439339 <a title="64-lda-2" href="../hunch_net-2011/hunch_net-2011-12-13-Vowpal_Wabbit_version_6.1_%26%23038%3B_the_NIPS_tutorial.html">451 hunch net-2011-12-13-Vowpal Wabbit version 6.1 &#038; the NIPS tutorial</a></p>
<p>Introduction: I just made  version 6.1  of  Vowpal Wabbit .  Relative to  6.0 , there are few new features, but many refinements. 
  
 The cluster parallel learning code better supports multiple simultaneous runs, and other forms of parallelism have been mostly removed.  This incidentally significantly simplifies the learning core. 
 The online learning algorithms are more general, with support for l 1  (via a truncated gradient variant) and l 2  regularization, and a generalized form of variable metric learning. 
 There is a solid persistent server mode which can train online, as well as serve answers to many simultaneous queries, either in text or binary. 
  
This should be a very good release if you are just getting started, as we’ve made it compile more automatically out of the box, have several new  examples  and updated documentation.
 
As  per   tradition , we’re planning to do a tutorial at NIPS during the break at the  parallel learning workshop  at 2pm Spanish time Friday.  I’ll cover the</p><p>3 0.37520522 <a title="64-lda-3" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>Introduction: Many people in Machine Learning don’t fully understand the impact of computation, as demonstrated by a lack of  big-O  analysis of new learning algorithms.  This is important—some current active research programs are fundamentally flawed w.r.t. computation, and other research programs are directly motivated by it.  When considering a learning algorithm, I think about the following questions:
  
 How does the learning algorithm scale with the number of examples  m ?  Any algorithm using all of the data is at least  O(m) , but in many cases this is  O(m 2 )  (naive nearest neighbor for self-prediction) or unknown (k-means or many other optimization algorithms).  The unknown case is very common, and it can mean (for example) that the algorithm isn’t convergent or simply that the amount of computation isn’t controlled. 
 The above question can also be asked for test cases.  In some applications, test-time performance is of great importance. 
 How does the algorithm scale with the number of</p><p>4 0.37140268 <a title="64-lda-4" href="../hunch_net-2009/hunch_net-2009-08-03-Carbon_in_Computer_Science_Research.html">366 hunch net-2009-08-03-Carbon in Computer Science Research</a></p>
<p>Introduction: Al Gore ‘s  film  and gradually more assertive and thorough science has managed to mostly shift the debate on climate change from “Is it happening?” to “What should be done?”  In that context, it’s worthwhile to think a bit about what can be done within computer science research.
 
There are two things we can think about:
  
  Doing Research  At a cartoon level, computer science research consists of some combination of commuting to&from; work, writing programs, running them on computers, writing papers, and presenting them at conferences.  A typical computer has a power usage on the order of 100 Watts, which works out to 2.4 kiloWatt-hours/day.  Looking up  David MacKay ‘s  reference on power usage per person , it becomes clear that this is a relatively minor part of the lifestyle, although it could become substantial if many more computers are required.  Much larger costs are associated with commuting (which is in common with many people) and attending conferences.  Since local commuti</p><p>5 0.3708863 <a title="64-lda-5" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>Introduction: Muthu  invited me to the workshop on  algorithms in the field , with the goal of providing a sense of where near-term research should go.  When the time came though, I bargained for a post instead, which provides a chance for many other people to comment.
 
There are several things I didn’t fully understand when I went to Yahoo! about 5 years ago.  I’d like to repeat them as people in academia may not yet understand them intuitively.
  
 Almost all the big impact algorithms operate in pseudo-linear or better time.  Think about caching, hashing, sorting, filtering, etc… and you have a sense of what some of the most heavily used algorithms are.  This matters quite a bit to Machine Learning research, because people often work with superlinear time algorithms and languages.  Two very common examples of this are graphical models, where inference is often a superlinear operation—think about the  n 2   dependence on the number of states in a  Hidden Markov Model  and Kernelized  Support Vecto</p><p>6 0.36961383 <a title="64-lda-6" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>7 0.36694735 <a title="64-lda-7" href="../hunch_net-2007/hunch_net-2007-07-06-Idempotent-capable_Predictors.html">253 hunch net-2007-07-06-Idempotent-capable Predictors</a></p>
<p>8 0.36634281 <a title="64-lda-8" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>9 0.36615291 <a title="64-lda-9" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>10 0.36552161 <a title="64-lda-10" href="../hunch_net-2009/hunch_net-2009-06-03-Functionally_defined_Nonlinear_Dynamic_Models.html">359 hunch net-2009-06-03-Functionally defined Nonlinear Dynamic Models</a></p>
<p>11 0.36522657 <a title="64-lda-11" href="../hunch_net-2009/hunch_net-2009-01-21-Nearly_all_natural_problems_require_nonlinearity.html">337 hunch net-2009-01-21-Nearly all natural problems require nonlinearity</a></p>
<p>12 0.36475888 <a title="64-lda-12" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>13 0.36462373 <a title="64-lda-13" href="../hunch_net-2009/hunch_net-2009-06-01-Multitask_Poisoning.html">358 hunch net-2009-06-01-Multitask Poisoning</a></p>
<p>14 0.36442968 <a title="64-lda-14" href="../hunch_net-2005/hunch_net-2005-07-21-Six_Months.html">96 hunch net-2005-07-21-Six Months</a></p>
<p>15 0.36442265 <a title="64-lda-15" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>16 0.3641876 <a title="64-lda-16" href="../hunch_net-2007/hunch_net-2007-08-12-Exponentiated_Gradient.html">258 hunch net-2007-08-12-Exponentiated Gradient</a></p>
<p>17 0.36382568 <a title="64-lda-17" href="../hunch_net-2006/hunch_net-2006-01-30-Should_the_Input_Representation_be_a_Vector%3F.html">152 hunch net-2006-01-30-Should the Input Representation be a Vector?</a></p>
<p>18 0.36333829 <a title="64-lda-18" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>19 0.3619408 <a title="64-lda-19" href="../hunch_net-2005/hunch_net-2005-06-06-Exact_Online_Learning_for_Classification.html">78 hunch net-2005-06-06-Exact Online Learning for Classification</a></p>
<p>20 0.36182928 <a title="64-lda-20" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
