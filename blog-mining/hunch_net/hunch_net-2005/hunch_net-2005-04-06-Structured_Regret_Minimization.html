<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>53 hunch net-2005-04-06-Structured Regret Minimization</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-53" href="#">hunch_net-2005-53</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>53 hunch net-2005-04-06-Structured Regret Minimization</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-53-html" href="http://hunch.net/?p=58">html</a></p><p>Introduction: Geoff Gordon made an interesting presentation at the  snowbird learning workshop  discussing the use of no-regret algorithms for the use of several robot-related learning problems.    There seems to be a draft  here .  This seems interesting in two ways:
  
  Drawback Removal  One of the significant problems with these online algorithms is that they can’t cope with structure very easily.  This drawback is addressed for certain structures. 
  Experiments  One criticism of such algorithms is that they are too “worst case”.   Several experiments suggest that protecting yourself against this worst case does not necessarily incur a great loss.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Geoff Gordon made an interesting presentation at the  snowbird learning workshop  discussing the use of no-regret algorithms for the use of several robot-related learning problems. [sent-1, score-1.435]
</p><p>2 This seems interesting in two ways:      Drawback Removal  One of the significant problems with these online algorithms is that they can’t cope with structure very easily. [sent-3, score-1.03]
</p><p>3 Experiments  One criticism of such algorithms is that they are too “worst case”. [sent-5, score-0.371]
</p><p>4 Several experiments suggest that protecting yourself against this worst case does not necessarily incur a great loss. [sent-6, score-1.422]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('drawback', 0.329), ('worst', 0.317), ('experiments', 0.281), ('incur', 0.259), ('gordon', 0.227), ('removal', 0.227), ('snowbird', 0.216), ('cope', 0.194), ('criticism', 0.194), ('case', 0.184), ('geoff', 0.183), ('draft', 0.179), ('algorithms', 0.177), ('discussing', 0.175), ('addressed', 0.175), ('certain', 0.153), ('interesting', 0.152), ('suggest', 0.151), ('necessarily', 0.151), ('presentation', 0.142), ('structure', 0.119), ('use', 0.11), ('seems', 0.105), ('loss', 0.103), ('workshop', 0.101), ('several', 0.099), ('ways', 0.095), ('made', 0.091), ('significant', 0.085), ('online', 0.082), ('great', 0.079), ('two', 0.059), ('one', 0.059), ('problems', 0.057), ('learning', 0.031)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="53-tfidf-1" href="../hunch_net-2005/hunch_net-2005-04-06-Structured_Regret_Minimization.html">53 hunch net-2005-04-06-Structured Regret Minimization</a></p>
<p>Introduction: Geoff Gordon made an interesting presentation at the  snowbird learning workshop  discussing the use of no-regret algorithms for the use of several robot-related learning problems.    There seems to be a draft  here .  This seems interesting in two ways:
  
  Drawback Removal  One of the significant problems with these online algorithms is that they can’t cope with structure very easily.  This drawback is addressed for certain structures. 
  Experiments  One criticism of such algorithms is that they are too “worst case”.   Several experiments suggest that protecting yourself against this worst case does not necessarily incur a great loss.</p><p>2 0.15391311 <a title="53-tfidf-2" href="../hunch_net-2011/hunch_net-2011-04-18-A_paper_not_at_Snowbird.html">431 hunch net-2011-04-18-A paper not at Snowbird</a></p>
<p>Introduction: Unfortunately, a scheduling failure meant I missed all of  AIStat  and most of the  learning workshop , otherwise known as Snowbird, when it’s at  Snowbird .  
 
At snowbird, the talk on  Sum-Product  networks by  Hoifung Poon  stood out to me ( Pedro Domingos  is a coauthor.).  The basic point was that by appropriately constructing networks based on sums and products, the normalization problem in probabilistic models is eliminated, yielding a highly tractable yet flexible representation+learning algorithm.  As an algorithm, this is noticeably cleaner than deep belief networks with a claim to being an order of magnitude faster and working better on an image completion task.
 
Snowbird doesn’t have real papers—just the abstract above.  I look forward to seeing the paper.  (added: Rodrigo points out the deep learning workshop  draft .)</p><p>3 0.12958115 <a title="53-tfidf-3" href="../hunch_net-2010/hunch_net-2010-09-13-AIStats.html">409 hunch net-2010-09-13-AIStats</a></p>
<p>Introduction: Geoff Gordon  points out  AIStats 2011  in Ft. Lauderdale, Florida.  The  call for papers  is now out, due Nov. 1.  The plan is to  experiment with the review process  to encourage quality in several ways.  I expect to submit a paper and would encourage others with good research to do likewise.</p><p>4 0.12121099 <a title="53-tfidf-4" href="../hunch_net-2008/hunch_net-2008-07-02-Proprietary_Data_in_Academic_Research%3F.html">306 hunch net-2008-07-02-Proprietary Data in Academic Research?</a></p>
<p>Introduction: Should results of experiments on proprietary datasets be in the academic research literature?
 
The arguments I can imagine in the “against” column are: 
  
  Experiments are not repeatable.  Repeatability in experiments is essential to science because it allows others to compare new methods with old and discover which is better. 
  It’s unfair.  Academics who don’t have insider access to proprietary data are at a substantial disadvantage when competing with others who do. 
  
I’m unsympathetic to argument (2).  To me, it looks like their are simply some resource constraints, and these should not prevent research progress.  For example, we wouldn’t prevent publishing about particle accelerator experiments by physicists at  CERN  because physicists at  CMU  couldn’t run their own experiments.  
 
Argument (1) seems like a real issue.
 
The argument for is: 
  
 Yes, they are another form of evidence that an algorithm is good. The degree to which they are evidence is less than for public</p><p>5 0.10699689 <a title="53-tfidf-5" href="../hunch_net-2005/hunch_net-2005-06-06-Exact_Online_Learning_for_Classification.html">78 hunch net-2005-06-06-Exact Online Learning for Classification</a></p>
<p>Introduction: Jacob Abernethy and I have found a computationally tractable method for computing an optimal (or near optimal depending on setting) master algorithm combining expert predictions addressing  this open problem .  A draft is  here .  
 
The effect of this improvement seems to be about a factor of  2  decrease in the regret (= error rate minus best possible error rate) for the low error rate situation.  (At large error rates, there may be no significant difference.)  
 
There are some unfinished details still to consider:
  
 When we remove all of the approximation slack from online learning, is the result a satisfying learning algorithm, in practice?  I consider online learning is one of the more compelling methods of analyzing and deriving algorithms, but that expectation must be either met or not by this algorithm 
 Some extra details: The algorithm is optimal given a small amount of side information ( k  in the draft).  What is the best way to remove this side information?  The removal</p><p>6 0.10267436 <a title="53-tfidf-6" href="../hunch_net-2005/hunch_net-2005-11-02-Progress_in_Active_Learning.html">127 hunch net-2005-11-02-Progress in Active Learning</a></p>
<p>7 0.09821108 <a title="53-tfidf-7" href="../hunch_net-2005/hunch_net-2005-05-02-Reviewing_techniques_for_conferences.html">65 hunch net-2005-05-02-Reviewing techniques for conferences</a></p>
<p>8 0.095508121 <a title="53-tfidf-8" href="../hunch_net-2005/hunch_net-2005-06-10-Workshops_are_not_Conferences.html">80 hunch net-2005-06-10-Workshops are not Conferences</a></p>
<p>9 0.093406558 <a title="53-tfidf-9" href="../hunch_net-2005/hunch_net-2005-04-08-Fast_SVMs.html">54 hunch net-2005-04-08-Fast SVMs</a></p>
<p>10 0.09015549 <a title="53-tfidf-10" href="../hunch_net-2006/hunch_net-2006-05-05-An_ICML_reject.html">177 hunch net-2006-05-05-An ICML reject</a></p>
<p>11 0.086489998 <a title="53-tfidf-11" href="../hunch_net-2006/hunch_net-2006-01-13-Benchmarks_for_RL.html">148 hunch net-2006-01-13-Benchmarks for RL</a></p>
<p>12 0.086425275 <a title="53-tfidf-12" href="../hunch_net-2009/hunch_net-2009-09-21-Netflix_finishes_%28and_starts%29.html">371 hunch net-2009-09-21-Netflix finishes (and starts)</a></p>
<p>13 0.084555231 <a title="53-tfidf-13" href="../hunch_net-2005/hunch_net-2005-02-01-Watchword%3A_Loss.html">9 hunch net-2005-02-01-Watchword: Loss</a></p>
<p>14 0.083756879 <a title="53-tfidf-14" href="../hunch_net-2005/hunch_net-2005-09-08-Online_Learning_as_the_Mathematics_of_Accountability.html">109 hunch net-2005-09-08-Online Learning as the Mathematics of Accountability</a></p>
<p>15 0.083450668 <a title="53-tfidf-15" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>16 0.081772149 <a title="53-tfidf-16" href="../hunch_net-2008/hunch_net-2008-07-04-More_Presentation_Preparation.html">307 hunch net-2008-07-04-More Presentation Preparation</a></p>
<p>17 0.081507459 <a title="53-tfidf-17" href="../hunch_net-2005/hunch_net-2005-06-08-Question%3A_%26%238220%3BWhen_is_the_right_time_to_insert_the_loss_function%3F%26%238221%3B.html">79 hunch net-2005-06-08-Question: &#8220;When is the right time to insert the loss function?&#8221;</a></p>
<p>18 0.074548408 <a title="53-tfidf-18" href="../hunch_net-2007/hunch_net-2007-05-12-Loss_Function_Semantics.html">245 hunch net-2007-05-12-Loss Function Semantics</a></p>
<p>19 0.074001424 <a title="53-tfidf-19" href="../hunch_net-2005/hunch_net-2005-09-26-Prediction_Bounds_as_the_Mathematics_of_Science.html">115 hunch net-2005-09-26-Prediction Bounds as the Mathematics of Science</a></p>
<p>20 0.07152234 <a title="53-tfidf-20" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.143), (1, 0.041), (2, 0.004), (3, -0.07), (4, 0.006), (5, 0.089), (6, -0.038), (7, -0.021), (8, 0.02), (9, 0.061), (10, 0.018), (11, -0.028), (12, 0.034), (13, -0.017), (14, 0.03), (15, 0.025), (16, 0.037), (17, 0.025), (18, -0.151), (19, -0.005), (20, -0.031), (21, -0.013), (22, -0.061), (23, 0.041), (24, 0.066), (25, 0.018), (26, 0.061), (27, -0.011), (28, -0.02), (29, 0.098), (30, -0.138), (31, -0.012), (32, -0.03), (33, 0.09), (34, 0.118), (35, -0.088), (36, 0.088), (37, 0.001), (38, 0.016), (39, -0.005), (40, -0.018), (41, 0.086), (42, -0.077), (43, 0.147), (44, 0.068), (45, 0.021), (46, 0.063), (47, 0.057), (48, 0.043), (49, 0.08)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95923388 <a title="53-lsi-1" href="../hunch_net-2005/hunch_net-2005-04-06-Structured_Regret_Minimization.html">53 hunch net-2005-04-06-Structured Regret Minimization</a></p>
<p>Introduction: Geoff Gordon made an interesting presentation at the  snowbird learning workshop  discussing the use of no-regret algorithms for the use of several robot-related learning problems.    There seems to be a draft  here .  This seems interesting in two ways:
  
  Drawback Removal  One of the significant problems with these online algorithms is that they can’t cope with structure very easily.  This drawback is addressed for certain structures. 
  Experiments  One criticism of such algorithms is that they are too “worst case”.   Several experiments suggest that protecting yourself against this worst case does not necessarily incur a great loss.</p><p>2 0.61747289 <a title="53-lsi-2" href="../hunch_net-2011/hunch_net-2011-04-18-A_paper_not_at_Snowbird.html">431 hunch net-2011-04-18-A paper not at Snowbird</a></p>
<p>Introduction: Unfortunately, a scheduling failure meant I missed all of  AIStat  and most of the  learning workshop , otherwise known as Snowbird, when it’s at  Snowbird .  
 
At snowbird, the talk on  Sum-Product  networks by  Hoifung Poon  stood out to me ( Pedro Domingos  is a coauthor.).  The basic point was that by appropriately constructing networks based on sums and products, the normalization problem in probabilistic models is eliminated, yielding a highly tractable yet flexible representation+learning algorithm.  As an algorithm, this is noticeably cleaner than deep belief networks with a claim to being an order of magnitude faster and working better on an image completion task.
 
Snowbird doesn’t have real papers—just the abstract above.  I look forward to seeing the paper.  (added: Rodrigo points out the deep learning workshop  draft .)</p><p>3 0.59903616 <a title="53-lsi-3" href="../hunch_net-2005/hunch_net-2005-04-08-Fast_SVMs.html">54 hunch net-2005-04-08-Fast SVMs</a></p>
<p>Introduction: There was a  presentation at snowbird  about parallelized support vector machines.  In many cases, people parallelize by ignoring serial operations, but that is not what happened hereâ&euro;&rdquo;they parallelize with optimizations.  Consequently, this seems to be the fastest SVM in existence.
 
There is a related  paper here .</p><p>4 0.4693813 <a title="53-lsi-4" href="../hunch_net-2009/hunch_net-2009-03-18-Parallel_ML_primitives.html">346 hunch net-2009-03-18-Parallel ML primitives</a></p>
<p>Introduction: Previously, we discussed  parallel machine learning  a bit.  As parallel ML is rather difficult, I’d like to describe my thinking at the moment, and ask for advice from the rest of the world.  This is particularly relevant right now, as I’m attending a workshop tomorrow on parallel ML.
 
Parallelizing slow algorithms seems uncompelling.  Parallelizing many algorithms also seems uncompelling, because the effort required to parallelize is substantial.  This leaves the question: Which one fast algorithm is the best to parallelize?  What is a substantially different second?
 
One compellingly fast simple algorithm is online gradient descent on a linear representation.  This is the core of Leon’s  sgd  code and  Vowpal Wabbit .   Antoine Bordes  showed a variant was competitive in the  large scale learning challenge .  It’s also a decades old primitive which has been reused in many algorithms, and continues to be reused.  It also applies to online  learning  rather than just online  optimiz</p><p>5 0.4657926 <a title="53-lsi-5" href="../hunch_net-2005/hunch_net-2005-06-10-Workshops_are_not_Conferences.html">80 hunch net-2005-06-10-Workshops are not Conferences</a></p>
<p>Introduction: … and you should use that fact.
 
A workshop differs from a conference in that it is about a focused group of people worrying about a focused topic.  It also differs in that a workshop is typically a “one-time affair” rather than a series.  (The  Snowbird learning workshop  counts as a conference in this respect.)  
 
A common failure mode of both organizers and speakers at a workshop is to treat it as a conference.  This is “ok”, but it is not really taking advantage of the situation.  Here are some things I’ve learned:
  
 For speakers: A smaller audience means it can be more interactive.  Interactive means a better chance to avoid losing your audience and a more interesting presentation (because you can adapt to your audience).  Greater focus amongst the participants means you can get to the heart of the matter more easily, and discuss tradeoffs more carefully.  Unlike conferences, relevance is more valued than newness. 
 For organizers: Not everything needs to be in a conference st</p><p>6 0.45084038 <a title="53-lsi-6" href="../hunch_net-2005/hunch_net-2005-09-08-Online_Learning_as_the_Mathematics_of_Accountability.html">109 hunch net-2005-09-08-Online Learning as the Mathematics of Accountability</a></p>
<p>7 0.44803885 <a title="53-lsi-7" href="../hunch_net-2010/hunch_net-2010-08-20-The_Workshop_on_Cores%2C_Clusters%2C_and_Clouds.html">404 hunch net-2010-08-20-The Workshop on Cores, Clusters, and Clouds</a></p>
<p>8 0.4453356 <a title="53-lsi-8" href="../hunch_net-2009/hunch_net-2009-01-07-Interesting_Papers_at_SODA_2009.html">334 hunch net-2009-01-07-Interesting Papers at SODA 2009</a></p>
<p>9 0.4219574 <a title="53-lsi-9" href="../hunch_net-2008/hunch_net-2008-07-04-More_Presentation_Preparation.html">307 hunch net-2008-07-04-More Presentation Preparation</a></p>
<p>10 0.42152449 <a title="53-lsi-10" href="../hunch_net-2005/hunch_net-2005-10-26-Fallback_Analysis_is_a_Secret_to_Useful_Algorithms.html">126 hunch net-2005-10-26-Fallback Analysis is a Secret to Useful Algorithms</a></p>
<p>11 0.41781765 <a title="53-lsi-11" href="../hunch_net-2011/hunch_net-2011-12-13-Vowpal_Wabbit_version_6.1_%26%23038%3B_the_NIPS_tutorial.html">451 hunch net-2011-12-13-Vowpal Wabbit version 6.1 &#038; the NIPS tutorial</a></p>
<p>12 0.41695312 <a title="53-lsi-12" href="../hunch_net-2007/hunch_net-2007-01-26-Parallel_Machine_Learning_Problems.html">229 hunch net-2007-01-26-Parallel Machine Learning Problems</a></p>
<p>13 0.41677019 <a title="53-lsi-13" href="../hunch_net-2011/hunch_net-2011-08-20-The_Large_Scale_Learning_Survey_Tutorial.html">442 hunch net-2011-08-20-The Large Scale Learning Survey Tutorial</a></p>
<p>14 0.40495113 <a title="53-lsi-14" href="../hunch_net-2005/hunch_net-2005-02-25-Problem%3A_Online_Learning.html">28 hunch net-2005-02-25-Problem: Online Learning</a></p>
<p>15 0.39798012 <a title="53-lsi-15" href="../hunch_net-2008/hunch_net-2008-07-02-Proprietary_Data_in_Academic_Research%3F.html">306 hunch net-2008-07-02-Proprietary Data in Academic Research?</a></p>
<p>16 0.39405072 <a title="53-lsi-16" href="../hunch_net-2006/hunch_net-2006-01-13-Benchmarks_for_RL.html">148 hunch net-2006-01-13-Benchmarks for RL</a></p>
<p>17 0.39076951 <a title="53-lsi-17" href="../hunch_net-2006/hunch_net-2006-11-22-Explicit_Randomization_in_Learning_algorithms.html">219 hunch net-2006-11-22-Explicit Randomization in Learning algorithms</a></p>
<p>18 0.39025646 <a title="53-lsi-18" href="../hunch_net-2006/hunch_net-2006-03-12-Online_learning_or_online_preservation_of_learning%3F.html">163 hunch net-2006-03-12-Online learning or online preservation of learning?</a></p>
<p>19 0.38836795 <a title="53-lsi-19" href="../hunch_net-2007/hunch_net-2007-02-22-Create_Your_Own_ICML_Workshop.html">234 hunch net-2007-02-22-Create Your Own ICML Workshop</a></p>
<p>20 0.38589972 <a title="53-lsi-20" href="../hunch_net-2009/hunch_net-2009-10-10-ALT_2009.html">374 hunch net-2009-10-10-ALT 2009</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(27, 0.198), (94, 0.102), (96, 0.547)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.80683768 <a title="53-lda-1" href="../hunch_net-2005/hunch_net-2005-04-06-Structured_Regret_Minimization.html">53 hunch net-2005-04-06-Structured Regret Minimization</a></p>
<p>Introduction: Geoff Gordon made an interesting presentation at the  snowbird learning workshop  discussing the use of no-regret algorithms for the use of several robot-related learning problems.    There seems to be a draft  here .  This seems interesting in two ways:
  
  Drawback Removal  One of the significant problems with these online algorithms is that they can’t cope with structure very easily.  This drawback is addressed for certain structures. 
  Experiments  One criticism of such algorithms is that they are too “worst case”.   Several experiments suggest that protecting yourself against this worst case does not necessarily incur a great loss.</p><p>2 0.78150928 <a title="53-lda-2" href="../hunch_net-2006/hunch_net-2006-04-30-John_Langford_%26%238211%3B%3E_Yahoo_Research%2C_NY.html">175 hunch net-2006-04-30-John Langford &#8211;> Yahoo Research, NY</a></p>
<p>Introduction: I will join  Yahoo Research  (in New York) after my contract ends at  TTI-Chicago .
 
The deciding reasons are:
  
 Yahoo is running into many hard learning problems.  This is precisely the situation where basic research might hope to have the greatest impact. 
 Yahoo Research understands research including publishing, conferences, etc… 
 Yahoo Research is growing, so there is a chance I can help it grow well. 
 Yahoo understands the internet, including (but not at all limited to) experimenting with research blogs. 
  
In the end, Yahoo Research seems like the place where I might have a chance to make the greatest difference.  
 
Yahoo (as a company) has made a strong bet on Yahoo Research.  We-the-researchers all hope that bet will pay off, and this seems plausible.  I’ll certainly have fun trying.</p><p>3 0.64292634 <a title="53-lda-3" href="../hunch_net-2005/hunch_net-2005-08-22-Do_you_believe_in_induction%3F.html">104 hunch net-2005-08-22-Do you believe in induction?</a></p>
<p>Introduction: Foster Provost  gave a talk at the ICML  metalearning workshop  on “metalearning” and the “no free lunch theorem” which seems worth summarizing.
 
As a review: the no free lunch theorem is the most complicated way we know of to say that a  bias  is required in order to learn.  The simplest way to see this is in a nonprobabilistic setting.  If you are given examples of the form  (x,y)  and you wish to predict  y  from  x  then any prediction mechanism errs half the time in expectation over all sequences of examples.  The proof of this is very simple: on every example a predictor must make some prediction and by symmetry over the set of sequences it will be wrong half the time and right half the time.  The basic idea of this proof has been applied to many other settings.
 
The simplistic interpretation of this theorem which many people jump to is “machine learning is dead” since there can be no single learning algorithm which can solve all learning problems.  This is the wrong way to thi</p><p>4 0.55296057 <a title="53-lda-4" href="../hunch_net-2011/hunch_net-2011-09-03-Fall_Machine_Learning_Events.html">443 hunch net-2011-09-03-Fall Machine Learning Events</a></p>
<p>Introduction: Many Machine Learning related events are coming up this fall.
  
  September 9 ,  abstracts for the New York Machine Learning Symposium  are due.  Send a 2 page pdf, if interested, and note that we:
 
 widened submissions to be from anybody rather than students. 
 set aside a larger fraction of time for contributed submissions.  
 
 
  September 15 , there is a  machine learning meetup , where I’ll be discussing terascale learning at AOL. 
  September 16 , there is a  CS&Econ; day  at New York Academy of Sciences.  This is not ML focused, but it’s easy to imagine interest. 
  September 23 and later   NIPS workshop  submissions start coming due.  As usual, there are too many good ones, so I won’t be able to attend all those that interest me.  I do hope some workshop makers consider ICML this coming summer, as we are increasing to a 2 day format for you.  Here are a few that interest me:
 
  Big Learning  is about dealing with lots of data.  Abstracts are due  September 30 . 
 The  Bayes</p><p>5 0.40415826 <a title="53-lda-5" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<p>Introduction: Martin Pool  and I recently discussed the similarities and differences between academia and open source programming.   
 
Similarities:
  
  Cost profile   Research and programming share approximately the same cost profile: A large upfront effort is required to produce something useful, and then “anyone” can use it.  (The “anyone” is not quite right for either group because only sufficiently technical people could use it.) 
  Wealth profile  A “wealthy” academic or open source programmer is someone who has contributed a lot to other people in research or programs.  Much of academia is a “gift culture”: whoever gives the most is most respected. 
  Problems   Both academia and open source programming suffer from similar problems.
 
 Whether or not (and which) open source program is used are perhaps too-often personality driven rather than driven by capability or usefulness.  Similar phenomena can happen in academia with respect to directions of research. 
 Funding is often a problem for</p><p>6 0.37207279 <a title="53-lda-6" href="../hunch_net-2013/hunch_net-2013-01-07-NYU_Large_Scale_Machine_Learning_Class.html">478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</a></p>
<p>7 0.35843837 <a title="53-lda-7" href="../hunch_net-2007/hunch_net-2007-07-01-Watchword%3A_Online_Learning.html">252 hunch net-2007-07-01-Watchword: Online Learning</a></p>
<p>8 0.35363013 <a title="53-lda-8" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>9 0.3533814 <a title="53-lda-9" href="../hunch_net-2007/hunch_net-2007-08-12-Exponentiated_Gradient.html">258 hunch net-2007-08-12-Exponentiated Gradient</a></p>
<p>10 0.35264114 <a title="53-lda-10" href="../hunch_net-2005/hunch_net-2005-03-22-Active_learning.html">45 hunch net-2005-03-22-Active learning</a></p>
<p>11 0.35241076 <a title="53-lda-11" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<p>12 0.35124403 <a title="53-lda-12" href="../hunch_net-2005/hunch_net-2005-03-18-Binomial_Weighting.html">43 hunch net-2005-03-18-Binomial Weighting</a></p>
<p>13 0.34897545 <a title="53-lda-13" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>14 0.34799266 <a title="53-lda-14" href="../hunch_net-2006/hunch_net-2006-07-06-Branch_Prediction_Competition.html">190 hunch net-2006-07-06-Branch Prediction Competition</a></p>
<p>15 0.34742156 <a title="53-lda-15" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>16 0.34725186 <a title="53-lda-16" href="../hunch_net-2007/hunch_net-2007-05-09-The_Missing_Bound.html">244 hunch net-2007-05-09-The Missing Bound</a></p>
<p>17 0.34633002 <a title="53-lda-17" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<p>18 0.34624517 <a title="53-lda-18" href="../hunch_net-2009/hunch_net-2009-01-21-Nearly_all_natural_problems_require_nonlinearity.html">337 hunch net-2009-01-21-Nearly all natural problems require nonlinearity</a></p>
<p>19 0.34621409 <a title="53-lda-19" href="../hunch_net-2005/hunch_net-2005-03-15-The_State_of_Tight_Bounds.html">41 hunch net-2005-03-15-The State of Tight Bounds</a></p>
<p>20 0.34563732 <a title="53-lda-20" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
