<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>44 hunch net-2005-03-21-Research Styles in Machine Learning</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-44" href="#">hunch_net-2005-44</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>44 hunch net-2005-03-21-Research Styles in Machine Learning</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-44-html" href="http://hunch.net/?p=48">html</a></p><p>Introduction: Machine Learning is a field with an impressively diverse set of reseearch styles.  Understanding this may be important in appreciating what you see at a conference.
  
  Engineering . How can I solve this problem?  People in the engineering research style try to solve hard problems directly by any means available and then describe how they did it.  This is typical of problem-specific conferences and communities. 
  Scientific . What are the principles for solving learning problems? People in this research style test techniques on many different problems.  This is fairly common at ICML and NIPS. 
  Mathematical . How can the learning problem be mathematically understood?  People in this research style prove theorems with implications for learning but often do not implement (or test algorithms).  COLT is a typical conference for this style. 
  
Many people manage to cross these styles, and that is often beneficial.  
 
Whenver we list a set of alternative, it becomes natural to think â&euro;&oelig;wh</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Machine Learning is a field with an impressively diverse set of reseearch styles. [sent-1, score-0.332]
</p><p>2 Understanding this may be important in appreciating what you see at a conference. [sent-2, score-0.309]
</p><p>3 People in the engineering research style try to solve hard problems directly by any means available and then describe how they did it. [sent-5, score-1.377]
</p><p>4 This is typical of problem-specific conferences and communities. [sent-6, score-0.273]
</p><p>5 People in this research style test techniques on many different problems. [sent-9, score-0.677]
</p><p>6 People in this research style prove theorems with implications for learning but often do not implement (or test algorithms). [sent-13, score-1.175]
</p><p>7 Many people manage to cross these styles, and that is often beneficial. [sent-15, score-0.469]
</p><p>8 Whenver we list a set of alternative, it becomes natural to think â&euro;&oelig;which is best? [sent-16, score-0.367]
</p><p>9 In this case of learning it seems that each of these styles is useful, and can lead to new useful discoveries. [sent-18, score-0.676]
</p><p>10 I sometimes see failures to appreciate the other approaches, which is a shame. [sent-19, score-0.384]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('styles', 0.353), ('style', 0.291), ('engineering', 0.267), ('appreciating', 0.202), ('shame', 0.202), ('typical', 0.196), ('test', 0.169), ('mathematically', 0.168), ('principles', 0.168), ('failures', 0.156), ('scientific', 0.147), ('implement', 0.143), ('solve', 0.141), ('manage', 0.133), ('implications', 0.133), ('research', 0.133), ('useful', 0.13), ('cross', 0.128), ('people', 0.125), ('alternative', 0.123), ('describe', 0.123), ('diverse', 0.123), ('lead', 0.121), ('appreciate', 0.121), ('theorems', 0.116), ('understood', 0.111), ('see', 0.107), ('prove', 0.107), ('set', 0.106), ('fairly', 0.103), ('mathematical', 0.103), ('field', 0.103), ('becomes', 0.097), ('directly', 0.09), ('problems', 0.088), ('list', 0.088), ('available', 0.088), ('techniques', 0.084), ('try', 0.083), ('colt', 0.083), ('often', 0.083), ('approaches', 0.079), ('conferences', 0.077), ('natural', 0.076), ('solving', 0.075), ('problem', 0.073), ('means', 0.073), ('case', 0.072), ('understanding', 0.071), ('conference', 0.068)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="44-tfidf-1" href="../hunch_net-2005/hunch_net-2005-03-21-Research_Styles_in_Machine_Learning.html">44 hunch net-2005-03-21-Research Styles in Machine Learning</a></p>
<p>Introduction: Machine Learning is a field with an impressively diverse set of reseearch styles.  Understanding this may be important in appreciating what you see at a conference.
  
  Engineering . How can I solve this problem?  People in the engineering research style try to solve hard problems directly by any means available and then describe how they did it.  This is typical of problem-specific conferences and communities. 
  Scientific . What are the principles for solving learning problems? People in this research style test techniques on many different problems.  This is fairly common at ICML and NIPS. 
  Mathematical . How can the learning problem be mathematically understood?  People in this research style prove theorems with implications for learning but often do not implement (or test algorithms).  COLT is a typical conference for this style. 
  
Many people manage to cross these styles, and that is often beneficial.  
 
Whenver we list a set of alternative, it becomes natural to think â&euro;&oelig;wh</p><p>2 0.15376213 <a title="44-tfidf-2" href="../hunch_net-2005/hunch_net-2005-02-17-Learning_Research_Programs.html">21 hunch net-2005-02-17-Learning Research Programs</a></p>
<p>Introduction: This is an attempt to organize the broad research programs related to machine learning currently underway.  This isn’t easy—this map is partial, the categories often overlap, and there are many details left out.  Nevertheless, it is (perhaps) helpful to have some map of what is happening where.  The word ‘typical’ should not be construed narrowly here.
  
  Learning Theory  Focuses on analyzing mathematical models of learning, essentially no experiments.  Typical conference: COLT. 
  Bayesian Learning  Bayes law is always used. Focus on methods of speeding up or approximating integration, new probabilistic models, and practical applications.  Typical conferences: NIPS,UAI 
  Structured learning  Predicting complex structured outputs, some applications. Typiical conferences: NIPS, UAI, others 
  Reinforcement Learning  Focused on ‘agent-in-the-world’ learning problems where the goal is optimizing reward.  Typical conferences: ICML 
  Unsupervised Learning/Clustering/Dimensionality Reduc</p><p>3 0.15326475 <a title="44-tfidf-3" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>Introduction: I want to try to describe what doing research means, especially from the point of view of an undergraduate.  The shift from a class-taking mentality to a research mentality is very significant and not easy.  
  
  Problem Posing  Posing the right problem is often as important as solving them.  Many people can get by in research by solving problems others have posed, but that’s not sufficient for really inspiring research.  For learning in particular, there is a strong feeling that we just haven’t figured out which questions are the right ones to ask.  You can see this, because the answers we have do not seem convincing. 
  Gambling your life  When you do research, you think very hard about new ways of solving problems, new problems, and new solutions.  Many conversations are of the form “I wonder what would happen if…” These processes can be short (days or weeks) or years-long endeavours.  The worst part is that you’ll only know if you were succesful at the end of the process (and some</p><p>4 0.11990934 <a title="44-tfidf-4" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>Introduction: One of the remarkable things about machine learning is how diverse it is.  The viewpoints of Bayesian learning, reinforcement learning, graphical models, supervised learning, unsupervised learning, genetic programming, etc… share little enough overlap that many people can and do make their careers within one without touching, or even necessarily understanding the others.
 
There are two fundamental reasons why this is possible.
  
 For many problems, many approaches work in the sense that they do something useful.  This is true empirically, where for many problems we can observe that many different approaches yield better performance than any constant predictor.  It’s also true in theory, where we know that for any set of predictors representable in a finite amount of RAM, minimizing training error over the set of predictors does something nontrivial when there are a sufficient number of examples. 
 There is nothing like a unifying problem defining the field.  In many other areas there</p><p>5 0.11805895 <a title="44-tfidf-5" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>Introduction: Normally, I don’t indulge in posters for  ICML , but this year is naturally an exception for me.   If you want one, there are a small number  left here , if you sign up before February.
 
It also seems worthwhile to give some sense of the scope and reviewing criteria for ICML for authors considering submitting papers.  At ICML, the (very large) program committee does the reviewing which informs final decisions by area chairs on most papers.  Program chairs setup the process, deal with exceptions or disagreements, and provide advice for the reviewing process.  Providing advice is tricky (and easily misleading) because a conference is a community, and in the end the aggregate interests of the community determine the conference.  Nevertheless, as a program chair this year it seems worthwhile to state the overall philosophy I have and what I plan to encourage (and occasionally discourage).
 
At the highest level, I believe ICML exists to further research into machine learning, which I gene</p><p>6 0.11693382 <a title="44-tfidf-6" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>7 0.11085439 <a title="44-tfidf-7" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>8 0.10951069 <a title="44-tfidf-8" href="../hunch_net-2005/hunch_net-2005-02-21-Problem%3A_Cross_Validation.html">26 hunch net-2005-02-21-Problem: Cross Validation</a></p>
<p>9 0.10764377 <a title="44-tfidf-9" href="../hunch_net-2005/hunch_net-2005-05-06-Don%26%238217%3Bt_mix_the_solution_into_the_problem.html">67 hunch net-2005-05-06-Don&#8217;t mix the solution into the problem</a></p>
<p>10 0.10435769 <a title="44-tfidf-10" href="../hunch_net-2005/hunch_net-2005-06-28-The_cross_validation_problem%3A_cash_reward.html">86 hunch net-2005-06-28-The cross validation problem: cash reward</a></p>
<p>11 0.1018981 <a title="44-tfidf-11" href="../hunch_net-2005/hunch_net-2005-07-04-The_Health_of_COLT.html">89 hunch net-2005-07-04-The Health of COLT</a></p>
<p>12 0.10102537 <a title="44-tfidf-12" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>13 0.10042904 <a title="44-tfidf-13" href="../hunch_net-2005/hunch_net-2005-09-26-Prediction_Bounds_as_the_Mathematics_of_Science.html">115 hunch net-2005-09-26-Prediction Bounds as the Mathematics of Science</a></p>
<p>14 0.099788383 <a title="44-tfidf-14" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>15 0.098267354 <a title="44-tfidf-15" href="../hunch_net-2006/hunch_net-2006-07-11-New_Models.html">194 hunch net-2006-07-11-New Models</a></p>
<p>16 0.096640244 <a title="44-tfidf-16" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>17 0.095262185 <a title="44-tfidf-17" href="../hunch_net-2005/hunch_net-2005-03-05-Funding_Research.html">36 hunch net-2005-03-05-Funding Research</a></p>
<p>18 0.094475247 <a title="44-tfidf-18" href="../hunch_net-2005/hunch_net-2005-03-17-Going_all_the_Way%2C_Sometimes.html">42 hunch net-2005-03-17-Going all the Way, Sometimes</a></p>
<p>19 0.090468891 <a title="44-tfidf-19" href="../hunch_net-2007/hunch_net-2007-07-20-Motivation_should_be_the_Responsibility_of_the_Reviewer.html">256 hunch net-2007-07-20-Motivation should be the Responsibility of the Reviewer</a></p>
<p>20 0.088316873 <a title="44-tfidf-20" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.223), (1, -0.007), (2, -0.036), (3, 0.049), (4, -0.048), (5, -0.072), (6, 0.041), (7, 0.065), (8, 0.041), (9, 0.033), (10, 0.002), (11, 0.063), (12, -0.003), (13, 0.109), (14, 0.113), (15, 0.03), (16, 0.098), (17, 0.011), (18, -0.135), (19, -0.006), (20, 0.063), (21, -0.037), (22, 0.011), (23, -0.054), (24, 0.038), (25, -0.009), (26, -0.066), (27, -0.004), (28, -0.131), (29, -0.008), (30, -0.069), (31, 0.107), (32, -0.052), (33, 0.023), (34, 0.007), (35, 0.108), (36, 0.028), (37, 0.03), (38, -0.038), (39, 0.027), (40, 0.0), (41, -0.002), (42, -0.043), (43, 0.024), (44, -0.031), (45, 0.046), (46, -0.1), (47, -0.035), (48, 0.042), (49, -0.088)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96229327 <a title="44-lsi-1" href="../hunch_net-2005/hunch_net-2005-03-21-Research_Styles_in_Machine_Learning.html">44 hunch net-2005-03-21-Research Styles in Machine Learning</a></p>
<p>Introduction: Machine Learning is a field with an impressively diverse set of reseearch styles.  Understanding this may be important in appreciating what you see at a conference.
  
  Engineering . How can I solve this problem?  People in the engineering research style try to solve hard problems directly by any means available and then describe how they did it.  This is typical of problem-specific conferences and communities. 
  Scientific . What are the principles for solving learning problems? People in this research style test techniques on many different problems.  This is fairly common at ICML and NIPS. 
  Mathematical . How can the learning problem be mathematically understood?  People in this research style prove theorems with implications for learning but often do not implement (or test algorithms).  COLT is a typical conference for this style. 
  
Many people manage to cross these styles, and that is often beneficial.  
 
Whenver we list a set of alternative, it becomes natural to think â&euro;&oelig;wh</p><p>2 0.70094812 <a title="44-lsi-2" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>Introduction: I want to try to describe what doing research means, especially from the point of view of an undergraduate.  The shift from a class-taking mentality to a research mentality is very significant and not easy.  
  
  Problem Posing  Posing the right problem is often as important as solving them.  Many people can get by in research by solving problems others have posed, but that’s not sufficient for really inspiring research.  For learning in particular, there is a strong feeling that we just haven’t figured out which questions are the right ones to ask.  You can see this, because the answers we have do not seem convincing. 
  Gambling your life  When you do research, you think very hard about new ways of solving problems, new problems, and new solutions.  Many conversations are of the form “I wonder what would happen if…” These processes can be short (days or weeks) or years-long endeavours.  The worst part is that you’ll only know if you were succesful at the end of the process (and some</p><p>3 0.65559375 <a title="44-lsi-3" href="../hunch_net-2008/hunch_net-2008-07-04-More_Presentation_Preparation.html">307 hunch net-2008-07-04-More Presentation Preparation</a></p>
<p>Introduction: We’ve discussed  presentation preparation before , but I have one more thing to add:  transitioning .  For a research presentation, it is substantially helpful for the audience if transitions are clear.  A common outline for a research presentation in machine leanring is:
  
  The problem .  Presentations which don’t describe the problem almost immediately lose people, because the context is missing to understand the detail. 
  Prior relevant work .  In many cases, a paper builds on some previous bit of work which must be understood in order to understand what the paper does.  A common failure mode seems to be spending too much time on prior work.  Discuss just the relevant aspects of prior work in the language of your work.  Sometimes this is missing when unneeded. 
  What we did . For theory papers in particular, it is often not possible to really cover the details.  Prioritizing what you present can be very important. 
  How it worked .  Many papers in Machine Learning have some sor</p><p>4 0.62428117 <a title="44-lsi-4" href="../hunch_net-2005/hunch_net-2005-03-17-Going_all_the_Way%2C_Sometimes.html">42 hunch net-2005-03-17-Going all the Way, Sometimes</a></p>
<p>Introduction: At many points in research, you face a choice: should I keep on improving some old piece of technology or should I do something new?  For example: 
  
 Should I refine bounds to make them tighter? 
 Should I take some learning theory and turn it into a learning algorithm? 
 Should I implement the learning algorithm? 
 Should I test the learning algorithm widely? 
 Should I release the algorithm as source code? 
 Should I go see what problems people actually need to solve? 
  
The universal temptation of people attracted to research is doing something new.  That is sometimes the right decision, but is also often not.  I’d like to discuss some reasons why not.
  
  Expertise  Once expertise are developed on some subject, you are the right person to refine them. 
  What is the real problem?  Continually improving a piece of technology is a mechanism forcing you to confront this question.  In many cases, this confrontation is uncomfortable because you discover that your method has fundamen</p><p>5 0.62025613 <a title="44-lsi-5" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>Introduction: One of the remarkable things about machine learning is how diverse it is.  The viewpoints of Bayesian learning, reinforcement learning, graphical models, supervised learning, unsupervised learning, genetic programming, etc… share little enough overlap that many people can and do make their careers within one without touching, or even necessarily understanding the others.
 
There are two fundamental reasons why this is possible.
  
 For many problems, many approaches work in the sense that they do something useful.  This is true empirically, where for many problems we can observe that many different approaches yield better performance than any constant predictor.  It’s also true in theory, where we know that for any set of predictors representable in a finite amount of RAM, minimizing training error over the set of predictors does something nontrivial when there are a sufficient number of examples. 
 There is nothing like a unifying problem defining the field.  In many other areas there</p><p>6 0.61127329 <a title="44-lsi-6" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>7 0.60870409 <a title="44-lsi-7" href="../hunch_net-2005/hunch_net-2005-02-17-Learning_Research_Programs.html">21 hunch net-2005-02-17-Learning Research Programs</a></p>
<p>8 0.59448659 <a title="44-lsi-8" href="../hunch_net-2007/hunch_net-2007-06-21-Presentation_Preparation.html">249 hunch net-2007-06-21-Presentation Preparation</a></p>
<p>9 0.58926672 <a title="44-lsi-9" href="../hunch_net-2005/hunch_net-2005-02-26-Problem%3A_Reductions_and_Relative_Ranking_Metrics.html">31 hunch net-2005-02-26-Problem: Reductions and Relative Ranking Metrics</a></p>
<p>10 0.5877611 <a title="44-lsi-10" href="../hunch_net-2006/hunch_net-2006-03-09-Use_of_Notation.html">162 hunch net-2006-03-09-Use of Notation</a></p>
<p>11 0.5857783 <a title="44-lsi-11" href="../hunch_net-2005/hunch_net-2005-07-04-The_Health_of_COLT.html">89 hunch net-2005-07-04-The Health of COLT</a></p>
<p>12 0.57336509 <a title="44-lsi-12" href="../hunch_net-2005/hunch_net-2005-05-17-A_Short_Guide_to_PhD_Graduate_Study.html">73 hunch net-2005-05-17-A Short Guide to PhD Graduate Study</a></p>
<p>13 0.57331431 <a title="44-lsi-13" href="../hunch_net-2005/hunch_net-2005-08-22-Do_you_believe_in_induction%3F.html">104 hunch net-2005-08-22-Do you believe in induction?</a></p>
<p>14 0.56893677 <a title="44-lsi-14" href="../hunch_net-2009/hunch_net-2009-06-01-Multitask_Poisoning.html">358 hunch net-2009-06-01-Multitask Poisoning</a></p>
<p>15 0.56869638 <a title="44-lsi-15" href="../hunch_net-2006/hunch_net-2006-01-06-MLTV.html">146 hunch net-2006-01-06-MLTV</a></p>
<p>16 0.56396788 <a title="44-lsi-16" href="../hunch_net-2006/hunch_net-2006-06-25-Presentation_of_Proofs_is_Hard..html">187 hunch net-2006-06-25-Presentation of Proofs is Hard.</a></p>
<p>17 0.56167847 <a title="44-lsi-17" href="../hunch_net-2006/hunch_net-2006-01-13-Benchmarks_for_RL.html">148 hunch net-2006-01-13-Benchmarks for RL</a></p>
<p>18 0.55518234 <a title="44-lsi-18" href="../hunch_net-2006/hunch_net-2006-08-10-Precision_is_not_accuracy.html">202 hunch net-2006-08-10-Precision is not accuracy</a></p>
<p>19 0.54925644 <a title="44-lsi-19" href="../hunch_net-2005/hunch_net-2005-01-24-Holy_grails_of_machine_learning%3F.html">2 hunch net-2005-01-24-Holy grails of machine learning?</a></p>
<p>20 0.54452544 <a title="44-lsi-20" href="../hunch_net-2005/hunch_net-2005-05-06-Don%26%238217%3Bt_mix_the_solution_into_the_problem.html">67 hunch net-2005-05-06-Don&#8217;t mix the solution into the problem</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(27, 0.229), (38, 0.086), (53, 0.072), (55, 0.156), (70, 0.233), (94, 0.108)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.92829245 <a title="44-lda-1" href="../hunch_net-2011/hunch_net-2011-08-06-Interesting_thing_at_UAI_2011.html">440 hunch net-2011-08-06-Interesting thing at UAI 2011</a></p>
<p>Introduction: I had a chance to attend  UAI  this year, where several papers interested me, including:
  
  Hoifung Poon  and  Pedro Domingos   Sum-Product Networks: A New Deep Architecture .  We’ve  already discussed this one , but in a nutshell, they identify a large class of efficiently normalizable distributions and do learning with it. 
  Yao-Liang Yu  and  Dale Schuurmans ,  Rank/norm regularization with closed-form solutions: Application to subspace clustering .  This paper is about matrices, and in particular they prove that certain matrices are the solution of matrix optimizations.  I’m not matrix inclined enough to fully appreciate this one, but I believe many others may be, and anytime closed form solutions come into play, you get 2 order of magnitude speedups, as they show experimentally. 
  Laurent Charlin ,  Richard Zemel  and  Craig Boutilier ,  A Framework for Optimizing Paper Matching .  This is about what works in matching papers to reviewers, as has been tested at several previous</p><p>same-blog 2 0.89551198 <a title="44-lda-2" href="../hunch_net-2005/hunch_net-2005-03-21-Research_Styles_in_Machine_Learning.html">44 hunch net-2005-03-21-Research Styles in Machine Learning</a></p>
<p>Introduction: Machine Learning is a field with an impressively diverse set of reseearch styles.  Understanding this may be important in appreciating what you see at a conference.
  
  Engineering . How can I solve this problem?  People in the engineering research style try to solve hard problems directly by any means available and then describe how they did it.  This is typical of problem-specific conferences and communities. 
  Scientific . What are the principles for solving learning problems? People in this research style test techniques on many different problems.  This is fairly common at ICML and NIPS. 
  Mathematical . How can the learning problem be mathematically understood?  People in this research style prove theorems with implications for learning but often do not implement (or test algorithms).  COLT is a typical conference for this style. 
  
Many people manage to cross these styles, and that is often beneficial.  
 
Whenver we list a set of alternative, it becomes natural to think â&euro;&oelig;wh</p><p>3 0.77909952 <a title="44-lda-3" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>Introduction: Unfortunately, I ended up sick for much of this ICML.  I did manage to catch one interesting paper:
 
 Richard Socher ,  Cliff Lin ,  Andrew Y. Ng , and  Christopher D. Manning   Parsing Natural Scenes and Natural Language with Recursive Neural Networks .
 
I invited Richard to share his list of interesting papers, so hopefully we’ll hear from him soon.  In the meantime,  Paul  and  Hal  have posted some lists.
 
 the future 
 
 Joelle  and I are program chairs for ICML 2012 in  Edinburgh , which I previously enjoyed visiting in  2005 .  This is a huge responsibility, that we hope to accomplish well.  A part of this (perhaps the most fun part), is imagining how we can make ICML better.  A key and critical constraint is choosing things that can be accomplished.  So far we have:
  
  Colocation . The first thing we looked into was potential colocations.  We quickly discovered that many other conferences precomitted their location.  For the future, getting a colocation with  ACL  or  SIGI</p><p>4 0.77451944 <a title="44-lda-4" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>Introduction: Bob Williamson  and I are the learning theory PC members at  NIPS  this year.  This is some attempt to state the standards and tests I applied to the papers.  I think it is a good idea to talk about this for two reasons:
  
 Making community standards a matter of public record seems healthy.  It give us a chance to debate what is and is not the right standard.  It might even give us a bit more consistency across the years. 
 It may save us all time.  There are a number of papers submitted which just aren’t there yet.  Avoiding submitting is the right decision in this case. 
  
There are several criteria for judging a paper.  All of these were active this year.  Some criteria are uncontroversial while others may be so.
  
 The paper must have a theorem establishing something new for which it is possible to derive high confidence in the correctness of the results.  A surprising number of papers fail this test.  This criteria seems essential to the definition of “theory”.
 
  Missing theo</p><p>5 0.76619458 <a title="44-lda-5" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>Introduction: At the  last ICML ,  Tom Dietterich  asked me to look into systems for commenting on papers.  I’ve been slow getting to this, but it’s relevant now.
 
The essential observation is that we now have many tools for online collaboration, but they are not yet much used in academic research.  If we can find the right way to use them, then perhaps great things might happen, with extra kudos to the first conference that manages to really create an online community.  Various conferences have been poking at this.  For example,  UAI has setup a wiki , COLT has  started using   Joomla , with some dynamic content, and AAAI has been setting up a “ student blog “.  Similarly,  Dinoj Surendran  setup a twiki for the  Chicago Machine Learning Summer School , which was quite useful for coordinating events and other things.
 
I believe the most important thing is a willingness to experiment.  A good place to start seems to be enhancing existing conference websites.  For example, the  ICML 2007 papers pag</p><p>6 0.76610237 <a title="44-lda-6" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>7 0.76528013 <a title="44-lda-7" href="../hunch_net-2009/hunch_net-2009-11-23-ICML_2009_Workshops_%28and_Tutorials%29.html">379 hunch net-2009-11-23-ICML 2009 Workshops (and Tutorials)</a></p>
<p>8 0.76253706 <a title="44-lda-8" href="../hunch_net-2007/hunch_net-2007-07-20-Motivation_should_be_the_Responsibility_of_the_Reviewer.html">256 hunch net-2007-07-20-Motivation should be the Responsibility of the Reviewer</a></p>
<p>9 0.76182044 <a title="44-lda-9" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>10 0.76145387 <a title="44-lda-10" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>11 0.76135349 <a title="44-lda-11" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>12 0.76059818 <a title="44-lda-12" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<p>13 0.75948066 <a title="44-lda-13" href="../hunch_net-2005/hunch_net-2005-03-13-Avoiding_Bad_Reviewing.html">40 hunch net-2005-03-13-Avoiding Bad Reviewing</a></p>
<p>14 0.7592544 <a title="44-lda-14" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>15 0.75893319 <a title="44-lda-15" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>16 0.75743091 <a title="44-lda-16" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>17 0.75739151 <a title="44-lda-17" href="../hunch_net-2008/hunch_net-2008-10-14-Who_is_Responsible_for_a_Bad_Review%3F.html">320 hunch net-2008-10-14-Who is Responsible for a Bad Review?</a></p>
<p>18 0.75584453 <a title="44-lda-18" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>19 0.75525576 <a title="44-lda-19" href="../hunch_net-2005/hunch_net-2005-07-21-Six_Months.html">96 hunch net-2005-07-21-Six Months</a></p>
<p>20 0.75380689 <a title="44-lda-20" href="../hunch_net-2008/hunch_net-2008-11-10-ICML_Reviewing_Criteria.html">325 hunch net-2008-11-10-ICML Reviewing Criteria</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
