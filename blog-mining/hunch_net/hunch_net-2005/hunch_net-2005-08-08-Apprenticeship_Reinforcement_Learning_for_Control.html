<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>101 hunch net-2005-08-08-Apprenticeship Reinforcement Learning for Control</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-101" href="#">hunch_net-2005-101</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>101 hunch net-2005-08-08-Apprenticeship Reinforcement Learning for Control</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-101-html" href="http://hunch.net/?p=108">html</a></p><p>Introduction: Pieter Abbeelpresented a paper withAndrew NgatICMLonExploration and
Apprenticeship Learning in Reinforcement Learning. The basic idea of this
algorithm is:Collect data from a human controlling a machine.Build a
transition model based upon the experience.Build a policy which optimizes the
transition model.Evaluate the policy. If it works well, halt, otherwise add
the experience into the pool and go to (2).The paper proves that this
technique will converge to some policy with expected performance near human
expected performance assuming the world fits certain assumptions (MDP or
linear dynamics).This general idea of apprenticeship learning (i.e.
incorporating data from an expert) seems very compelling because (a) humans
often learn this way and (b) much harder problems can be solved. For (a), the
notion of teaching is about transferring knowledge from an expert to novices,
often via demonstration. To see (b), note that we can create intricate
reinforcement learning problems where a parti</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sequence', 0.303), ('apprenticeship', 0.268), ('policy', 0.218), ('transition', 0.215), ('approximate', 0.181), ('expert', 0.181), ('occurs', 0.177), ('reinforcement', 0.14), ('exploited', 0.134), ('flying', 0.134), ('helicopter', 0.134), ('memorize', 0.134), ('upside', 0.134), ('human', 0.13), ('expected', 0.126), ('achieve', 0.125), ('optimizes', 0.124), ('demonstration', 0.124), ('fuse', 0.124), ('incorporating', 0.112)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="101-tfidf-1" href="../hunch_net-2005/hunch_net-2005-08-08-Apprenticeship_Reinforcement_Learning_for_Control.html">101 hunch net-2005-08-08-Apprenticeship Reinforcement Learning for Control</a></p>
<p>Introduction: Pieter Abbeelpresented a paper withAndrew NgatICMLonExploration and
Apprenticeship Learning in Reinforcement Learning. The basic idea of this
algorithm is:Collect data from a human controlling a machine.Build a
transition model based upon the experience.Build a policy which optimizes the
transition model.Evaluate the policy. If it works well, halt, otherwise add
the experience into the pool and go to (2).The paper proves that this
technique will converge to some policy with expected performance near human
expected performance assuming the world fits certain assumptions (MDP or
linear dynamics).This general idea of apprenticeship learning (i.e.
incorporating data from an expert) seems very compelling because (a) humans
often learn this way and (b) much harder problems can be solved. For (a), the
notion of teaching is about transferring knowledge from an expert to novices,
often via demonstration. To see (b), note that we can create intricate
reinforcement learning problems where a parti</p><p>2 0.14531246 <a title="101-tfidf-2" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>Introduction: This post is about a general technique for problem solving which I've never
seen taught (in full generality), but which I've found very useful.Many
problems in computer science turn out to be discretely difficult. The best
known version of such problems are NP-hard problems, but I mean 'discretely
difficult' in a much more general way, which I only know how to capture by
examples.ERMIn empirical risk minimization, you choose a minimum error rate
classifier from a set of classifiers. This is NP hard for common sets, but it
can be much harder, depending on the set.ExpertsIn the online learning with
experts setting, you try to predict well so as to compete with a set of
(adversarial) experts. Here the alternating quantifiers of you and an
adversary playing out a game can yield a dynamic programming problem that
grows exponentially.Policy IterationThe problem with policy iteration is that
you learn a new policy with respect to an old policy, which implies that
simply adopting the new polic</p><p>3 0.13694654 <a title="101-tfidf-3" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>Introduction: One thing which is clear on a little reflection is that there exists a single
master learning problem capable of encoding essentially all learning problems.
This problem is of course a very general sort of reinforcement learning where
the world interacts with an agent as:The world announces an observationx.The
agent makes a choicea.The world announces a rewardr.The goal here is to
maximize the sum of the rewards over the time of the agent. No particular
structure relatingxtoaoratoris implied by this setting so we do not know
effective general algorithms for the agent. It's very easy to prove lower
bounds showing that an agent cannot hope to succeed here--just consider the
case where actions are unrelated to rewards. Nevertheless, there is a real
sense in which essentially all forms of life are agents operating in this
setting, somehow succeeding. The gap between these observations drives
research--How can we find tractable specializations of the master problem
general enough to provide</p><p>4 0.13319781 <a title="101-tfidf-4" href="../hunch_net-2005/hunch_net-2005-02-23-Problem%3A_Reinforcement_Learning_with_Classification.html">27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</a></p>
<p>Introduction: At an intuitive level, the question here is "Can reinforcement learning be
solved with classification?"ProblemConstruct a reinforcement learning
algorithm with near-optimal expected sum of rewards in thedirect experience
modelgiven access to a classifier learning algorithm which has a small error
rate or regret on all posed classification problems. The definition of "posed"
here is slightly murky. I consider a problem "posed" if there is an algorithm
for constructing labeled classification examples.Past WorkThere exists
areduction of reinforcement learning to classification given a generative
model.A generative model is an inherently stronger assumption than the direct
experience model.Otherwork on learning reductionsmay be important.Several
algorithms for solving reinforcement learning in the direct experience model
exist. Most, such asE3,Factored-E3, andmetric-E3andRmaxrequire that the
observation be the state. Recent workextends this approach to POMDPs.This
problem is related topred</p><p>5 0.12565595 <a title="101-tfidf-5" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>Introduction: What?Reductions are machines which turn solvers for one problem into solvers
for another problem.Why?Reductions are useful for several reasons.Laziness.
Reducing a problem to classification make at least 10 learning algorithms
available to solve a problem. Inventing 10 learning algorithms is quite a bit
of work. Similarly, programming a reduction is often trivial, while
programming a learning algorithm is a great deal of work.Crystallization. The
problems we often want to solve in learning are worst-case-impossible, but
average case feasible. By reducing all problems onto one or a few primitives,
we can fine tune these primitives to perform well on real-world problems with
greater precision due to the greater number of problems to validate
on.Theoretical Organization. By studying what reductions are easy vs. hard vs.
impossible, we can learn which problems are roughly equivalent in difficulty
and which are much harder.What we know now.Typesafe reductions. In the
beginning, there was th</p><p>6 0.11379465 <a title="101-tfidf-6" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>7 0.11101937 <a title="101-tfidf-7" href="../hunch_net-2005/hunch_net-2005-03-22-Active_learning.html">45 hunch net-2005-03-22-Active learning</a></p>
<p>8 0.10372695 <a title="101-tfidf-8" href="../hunch_net-2005/hunch_net-2005-05-29-Maximum_Margin_Mismatch%3F.html">77 hunch net-2005-05-29-Maximum Margin Mismatch?</a></p>
<p>9 0.096569903 <a title="101-tfidf-9" href="../hunch_net-2005/hunch_net-2005-11-28-A_question_of_quantification.html">133 hunch net-2005-11-28-A question of quantification</a></p>
<p>10 0.095304817 <a title="101-tfidf-10" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>11 0.095298171 <a title="101-tfidf-11" href="../hunch_net-2005/hunch_net-2005-07-23-Interesting_papers_at_ACL.html">97 hunch net-2005-07-23-Interesting papers at ACL</a></p>
<p>12 0.091999263 <a title="101-tfidf-12" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>13 0.089946322 <a title="101-tfidf-13" href="../hunch_net-2005/hunch_net-2005-01-24-The_Humanloop_Spectrum_of_Machine_Learning.html">3 hunch net-2005-01-24-The Humanloop Spectrum of Machine Learning</a></p>
<p>14 0.088794 <a title="101-tfidf-14" href="../hunch_net-2007/hunch_net-2007-10-24-Contextual_Bandits.html">269 hunch net-2007-10-24-Contextual Bandits</a></p>
<p>15 0.088268079 <a title="101-tfidf-15" href="../hunch_net-2006/hunch_net-2006-01-13-Benchmarks_for_RL.html">148 hunch net-2006-01-13-Benchmarks for RL</a></p>
<p>16 0.087645106 <a title="101-tfidf-16" href="../hunch_net-2008/hunch_net-2008-01-28-Sufficient_Computation.html">287 hunch net-2008-01-28-Sufficient Computation</a></p>
<p>17 0.087293863 <a title="101-tfidf-17" href="../hunch_net-2006/hunch_net-2006-03-23-The_Approximation_Argument.html">165 hunch net-2006-03-23-The Approximation Argument</a></p>
<p>18 0.083342925 <a title="101-tfidf-18" href="../hunch_net-2006/hunch_net-2006-06-30-ICML_papers.html">188 hunch net-2006-06-30-ICML papers</a></p>
<p>19 0.082949191 <a title="101-tfidf-19" href="../hunch_net-2005/hunch_net-2005-09-14-The_Predictionist_Viewpoint.html">112 hunch net-2005-09-14-The Predictionist Viewpoint</a></p>
<p>20 0.08208549 <a title="101-tfidf-20" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">23 hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.204), (1, -0.068), (2, -0.021), (3, -0.06), (4, -0.075), (5, -0.012), (6, -0.047), (7, -0.005), (8, -0.012), (9, -0.037), (10, 0.022), (11, -0.042), (12, 0.024), (13, -0.005), (14, 0.066), (15, 0.047), (16, 0.035), (17, -0.05), (18, -0.096), (19, 0.002), (20, -0.03), (21, -0.016), (22, -0.015), (23, 0.013), (24, -0.026), (25, -0.119), (26, -0.028), (27, 0.028), (28, 0.03), (29, 0.027), (30, -0.03), (31, -0.0), (32, 0.076), (33, -0.001), (34, -0.125), (35, -0.033), (36, 0.095), (37, -0.032), (38, 0.003), (39, -0.089), (40, -0.065), (41, -0.146), (42, -0.042), (43, 0.04), (44, 0.008), (45, -0.004), (46, -0.101), (47, -0.054), (48, -0.064), (49, 0.001)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95252478 <a title="101-lsi-1" href="../hunch_net-2005/hunch_net-2005-08-08-Apprenticeship_Reinforcement_Learning_for_Control.html">101 hunch net-2005-08-08-Apprenticeship Reinforcement Learning for Control</a></p>
<p>Introduction: Pieter Abbeelpresented a paper withAndrew NgatICMLonExploration and
Apprenticeship Learning in Reinforcement Learning. The basic idea of this
algorithm is:Collect data from a human controlling a machine.Build a
transition model based upon the experience.Build a policy which optimizes the
transition model.Evaluate the policy. If it works well, halt, otherwise add
the experience into the pool and go to (2).The paper proves that this
technique will converge to some policy with expected performance near human
expected performance assuming the world fits certain assumptions (MDP or
linear dynamics).This general idea of apprenticeship learning (i.e.
incorporating data from an expert) seems very compelling because (a) humans
often learn this way and (b) much harder problems can be solved. For (a), the
notion of teaching is about transferring knowledge from an expert to novices,
often via demonstration. To see (b), note that we can create intricate
reinforcement learning problems where a parti</p><p>2 0.61768907 <a title="101-lsi-2" href="../hunch_net-2005/hunch_net-2005-02-23-Problem%3A_Reinforcement_Learning_with_Classification.html">27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</a></p>
<p>Introduction: At an intuitive level, the question here is "Can reinforcement learning be
solved with classification?"ProblemConstruct a reinforcement learning
algorithm with near-optimal expected sum of rewards in thedirect experience
modelgiven access to a classifier learning algorithm which has a small error
rate or regret on all posed classification problems. The definition of "posed"
here is slightly murky. I consider a problem "posed" if there is an algorithm
for constructing labeled classification examples.Past WorkThere exists
areduction of reinforcement learning to classification given a generative
model.A generative model is an inherently stronger assumption than the direct
experience model.Otherwork on learning reductionsmay be important.Several
algorithms for solving reinforcement learning in the direct experience model
exist. Most, such asE3,Factored-E3, andmetric-E3andRmaxrequire that the
observation be the state. Recent workextends this approach to POMDPs.This
problem is related topred</p><p>3 0.59060782 <a title="101-lsi-3" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>Introduction: Exploration is one of the big unsolved problems in machine learning. This
isn't for lack of trying--there are many models of exploration which have been
analyzed in many different ways by many different groups of people. At some
point, it is worthwhile to sit back and see what has been done across these
many models.Reinforcement Learning(1). Reinforcement learning has
traditionally focused on Markov Decision Processes where the next states'is
given by a conditional distributionP(s'|s,a)given the current statesand
actiona. The typical result here is that certain specific algorithms
controlling an agent can behave withineof optimal for horizonTexcept
forpoly(1/e,T,S,A)"wasted" experiences (with high probability). This started
withE3bySatinder SinghandMichael Kearns.Sham Kakade's thesishas significant
discussion. Extensions have typically been of the form "under extra
assumptions, we can prove more", for exampleFactored-E3andMetric-E3. (It turns
out that the number of wasted samples can b</p><p>4 0.58706874 <a title="101-lsi-4" href="../hunch_net-2006/hunch_net-2006-04-05-What_is_state%3F.html">169 hunch net-2006-04-05-What is state?</a></p>
<p>Introduction: In reinforcement learning (and sometimes other settings), there is a notion of
"state". Based upon the state various predictions are made such as "Which
action should be taken next?" or "How much cumulative reward do I expect if I
take some action from this state?" Given the importance of state, it is
important to examine the meaning. There are actually several distinct options
and it turns out the definition variation is very important in motivating
different pieces of work.Newtonian State. State is the physical pose of the
world. Under this definition, there areverymany states, often too many for
explicit representation. This is also the definition typically used in
games.Abstracted State. State is an abstracted physical state of the world.
"Is the door open or closed?" "Are you in room A or not?" The number of states
is much smaller here. A basic issue here is: "How do you compute the state
from observations?"Mathematical State. State is a sufficient statistic of
observations for ma</p><p>5 0.57193285 <a title="101-lsi-5" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>Introduction: This post is about a general technique for problem solving which I've never
seen taught (in full generality), but which I've found very useful.Many
problems in computer science turn out to be discretely difficult. The best
known version of such problems are NP-hard problems, but I mean 'discretely
difficult' in a much more general way, which I only know how to capture by
examples.ERMIn empirical risk minimization, you choose a minimum error rate
classifier from a set of classifiers. This is NP hard for common sets, but it
can be much harder, depending on the set.ExpertsIn the online learning with
experts setting, you try to predict well so as to compete with a set of
(adversarial) experts. Here the alternating quantifiers of you and an
adversary playing out a game can yield a dynamic programming problem that
grows exponentially.Policy IterationThe problem with policy iteration is that
you learn a new policy with respect to an old policy, which implies that
simply adopting the new polic</p><p>6 0.56708908 <a title="101-lsi-6" href="../hunch_net-2005/hunch_net-2005-01-24-The_Humanloop_Spectrum_of_Machine_Learning.html">3 hunch net-2005-01-24-The Humanloop Spectrum of Machine Learning</a></p>
<p>7 0.55588084 <a title="101-lsi-7" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>8 0.54995954 <a title="101-lsi-8" href="../hunch_net-2006/hunch_net-2006-07-08-Some_recent_papers.html">192 hunch net-2006-07-08-Some recent papers</a></p>
<p>9 0.54994082 <a title="101-lsi-9" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>10 0.53016764 <a title="101-lsi-10" href="../hunch_net-2005/hunch_net-2005-08-04-Why_Reinforcement_Learning_is_Important.html">100 hunch net-2005-08-04-Why Reinforcement Learning is Important</a></p>
<p>11 0.51893878 <a title="101-lsi-11" href="../hunch_net-2005/hunch_net-2005-04-25-Embeddings%3A_what_are_they_good_for%3F.html">61 hunch net-2005-04-25-Embeddings: what are they good for?</a></p>
<p>12 0.50447875 <a title="101-lsi-12" href="../hunch_net-2005/hunch_net-2005-03-10-Breaking_Abstractions.html">39 hunch net-2005-03-10-Breaking Abstractions</a></p>
<p>13 0.50383174 <a title="101-lsi-13" href="../hunch_net-2005/hunch_net-2005-10-07-On-line_learning_of_regular_decision_rules.html">118 hunch net-2005-10-07-On-line learning of regular decision rules</a></p>
<p>14 0.50234997 <a title="101-lsi-14" href="../hunch_net-2005/hunch_net-2005-05-29-Maximum_Margin_Mismatch%3F.html">77 hunch net-2005-05-29-Maximum Margin Mismatch?</a></p>
<p>15 0.50212204 <a title="101-lsi-15" href="../hunch_net-2006/hunch_net-2006-01-13-Benchmarks_for_RL.html">148 hunch net-2006-01-13-Benchmarks for RL</a></p>
<p>16 0.49448875 <a title="101-lsi-16" href="../hunch_net-2011/hunch_net-2011-08-06-Interesting_thing_at_UAI_2011.html">440 hunch net-2011-08-06-Interesting thing at UAI 2011</a></p>
<p>17 0.48892343 <a title="101-lsi-17" href="../hunch_net-2006/hunch_net-2006-02-07-Pittsburgh_Mind_Reading_Competition.html">155 hunch net-2006-02-07-Pittsburgh Mind Reading Competition</a></p>
<p>18 0.48174137 <a title="101-lsi-18" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>19 0.47633737 <a title="101-lsi-19" href="../hunch_net-2006/hunch_net-2006-02-02-Introspectionism_as_a_Disease.html">153 hunch net-2006-02-02-Introspectionism as a Disease</a></p>
<p>20 0.47238094 <a title="101-lsi-20" href="../hunch_net-2006/hunch_net-2006-06-30-ICML_papers.html">188 hunch net-2006-06-30-ICML papers</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(6, 0.41), (35, 0.09), (42, 0.272), (68, 0.081), (74, 0.041)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.95182741 <a title="101-lda-1" href="../hunch_net-2008/hunch_net-2008-10-01-NIPS_2008_workshop_on_%26%238216%3BLearning_over_Empirical_Hypothesis_Spaces%26%238217%3B.html">319 hunch net-2008-10-01-NIPS 2008 workshop on &#8216;Learning over Empirical Hypothesis Spaces&#8217;</a></p>
<p>Introduction: This workshop asks for insights how far we may/can push the theoretical
boundary of using data in the design of learning machines. Can we express our
classification rule in terms of the sample, or do we have to stick to a core
assumption of classical statistical learning theory, namely that the
hypothesis space is to be defined independent from the sample? This workshop
is particularly interested in - but not restricted to - the 'luckiness
framework' and the recently introduced notion of 'compatibility functions' in
a semi-supervised learning context (more information can be found
athttp://www.kuleuven.be/wehys).</p><p>2 0.95176953 <a title="101-lda-2" href="../hunch_net-2009/hunch_net-2009-02-16-KDNuggets.html">342 hunch net-2009-02-16-KDNuggets</a></p>
<p>Introduction: Eric Zaetsch points outKDNuggetswhich is a well-developed mailing list/news
site with aKDDflavor. This might particularly interest people looking for
industrial jobs in machine learning, as the mailing list has many such.</p><p>3 0.94593233 <a title="101-lda-3" href="../hunch_net-2005/hunch_net-2005-06-17-Reopening_RL-%3EClassification.html">82 hunch net-2005-06-17-Reopening RL->Classification</a></p>
<p>Introduction: In research, it's often the case that solving a problem helps you realize that
it wasn't the right problem to solve. This is the case for the "reduce RL to
classification" problem with the solution hinted athereand turned into a
paperhere.The essential difficulty is that the method of stating and analyzing
reductions ends up being nonalgorithmic (unlike previous reductions) unless
you work with learning from teleoperated robots asGreg Grudicdoes. The
difficulty here is due to the reduction being dependent on the optimal policy
(which a human teleoperator might simulate, but which is otherwise
unavailable).So, thisproblemis "open" again with the caveat that this time we
want a more algorithmic solution.Whether or not this is feasible at all is
still unclear and evidence in either direction would greatly interest me. A
positive answer might have many practical implications in the long run.</p><p>4 0.94184268 <a title="101-lda-4" href="../hunch_net-2005/hunch_net-2005-10-07-On-line_learning_of_regular_decision_rules.html">118 hunch net-2005-10-07-On-line learning of regular decision rules</a></p>
<p>Introduction: Many decision problems can be represented in the formFORn=1,2,â&euro;Ś:-- Reality
chooses a datumxn.-- Decision Maker chooses his decisiondn.-- Reality chooses
an observationyn.-- Decision Maker suffers lossL(yn,dn).END FOR.The
observationyncan be, for example, tomorrow's stock price and the decisiondnthe
number of shares Decision Maker chooses to buy. The datumxnideally contains
all information that might be relevant in making this decision. We do not want
to assume anything about the way Reality generates the observations and
data.Suppose there is a good and not too complex decision ruleDmapping each
datumxto a decisionD(x). Can we perform as well, or almost as well, asD,
without knowing it? This is essentially a special case of the problem ofon-
line learning.This is a simple result of this kind. Suppose the dataxnare
taken from [0,1] andL(y,d)=|y-d|. A norm ||h|| of a functionhon [0,1] is
defined by||h||2= (Integral01h(t)dt)2+ Integral01(h'(t))2dt.Decision Maker has
a strategy that guaran</p><p>5 0.91477734 <a title="101-lda-5" href="../hunch_net-2005/hunch_net-2005-06-22-Languages__of_Learning.html">84 hunch net-2005-06-22-Languages  of Learning</a></p>
<p>Introduction: A language is a set of primitives which can be combined to succesfully create
complex objects. Languages arise in all sorts of situations: mechanical
construction, martial arts, communication, etcâ&euro;Ś Languages appear to be the key
to succesfully creating complex objects--it is difficult to come up with any
convincing example of a complex object which is not built using some language.
Since languages are so crucial to success, it is interesting to organize
various machine learning research programs by language.The most common
language in machine learning are languages for representing the solution to
machine learning. This includes:Bayes Nets and Graphical ModelsA language for
representing probability distributions. The key concept supporting modularity
is conditional independence.Michael Kearnshas been working on extending this
to game theory.Kernelized Linear ClassifiersA language for representing linear
separators, possibly in a large space. The key form of modularity here is
kerneliza</p><p>6 0.90827179 <a title="101-lda-6" href="../hunch_net-2006/hunch_net-2006-12-06-The_Spam_Problem.html">223 hunch net-2006-12-06-The Spam Problem</a></p>
<p>same-blog 7 0.89788789 <a title="101-lda-7" href="../hunch_net-2005/hunch_net-2005-08-08-Apprenticeship_Reinforcement_Learning_for_Control.html">101 hunch net-2005-08-08-Apprenticeship Reinforcement Learning for Control</a></p>
<p>8 0.88757008 <a title="101-lda-8" href="../hunch_net-2008/hunch_net-2008-02-10-Complexity_Illness.html">288 hunch net-2008-02-10-Complexity Illness</a></p>
<p>9 0.6780293 <a title="101-lda-9" href="../hunch_net-2005/hunch_net-2005-02-15-ESPgame_and_image_labeling.html">20 hunch net-2005-02-15-ESPgame and image labeling</a></p>
<p>10 0.63785529 <a title="101-lda-10" href="../hunch_net-2005/hunch_net-2005-05-16-Regret_minimizing_vs_error_limiting_reductions.html">72 hunch net-2005-05-16-Regret minimizing vs error limiting reductions</a></p>
<p>11 0.6301173 <a title="101-lda-11" href="../hunch_net-2005/hunch_net-2005-02-25-Problem%3A_Online_Learning.html">28 hunch net-2005-02-25-Problem: Online Learning</a></p>
<p>12 0.62359774 <a title="101-lda-12" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<p>13 0.62331355 <a title="101-lda-13" href="../hunch_net-2006/hunch_net-2006-09-28-Programming_Languages_for_Machine_Learning_Implementations.html">210 hunch net-2006-09-28-Programming Languages for Machine Learning Implementations</a></p>
<p>14 0.62127882 <a title="101-lda-14" href="../hunch_net-2007/hunch_net-2007-10-24-Contextual_Bandits.html">269 hunch net-2007-10-24-Contextual Bandits</a></p>
<p>15 0.61745864 <a title="101-lda-15" href="../hunch_net-2005/hunch_net-2005-03-10-Breaking_Abstractions.html">39 hunch net-2005-03-10-Breaking Abstractions</a></p>
<p>16 0.61086857 <a title="101-lda-16" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>17 0.60913175 <a title="101-lda-17" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>18 0.60740143 <a title="101-lda-18" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>19 0.60689145 <a title="101-lda-19" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>20 0.60545737 <a title="101-lda-20" href="../hunch_net-2010/hunch_net-2010-05-02-What%26%238217%3Bs_the_difference_between_gambling_and_rewarding_good_prediction%3F.html">397 hunch net-2010-05-02-What&#8217;s the difference between gambling and rewarding good prediction?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
