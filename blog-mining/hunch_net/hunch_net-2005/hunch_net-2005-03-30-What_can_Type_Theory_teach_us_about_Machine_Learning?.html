<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-49" href="#">hunch_net-2005-49</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-49-html" href="http://hunch.net/?p=54">html</a></p><p>Introduction: This post is some combination of belaboring the obvious and speculating wildly
about the future. The basic issue to be addressed is how to think about
machine learning in terms given to us from Programming Language theory.Types
and ReductionsJohn's research programme (I feel this should be in British
spelling to reflect the grandiousness of the ideaâ&euro;Ś) of machine learning
reductionsStateOfReductionis at some essential level type-theoretic in nature.
The fundamental elements are the classifier, a function f: alpha -> beta, and
the corresponding classifier trainer g: List of (alpha,beta) -> (alpha ->
beta). The research goal is to create *combinators* that produce new f's and
g's given existing ones. John (probably quite rightly) seems unwilling at the
moment to commit to any notion stronger than these combinators are correctly
typed. One way to see the result of a reduction is something typed like: (For
those denied the joy of the Hindly-Milner type system, "simple" is probably
wildly wr</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 One way to see the result of a reduction is something typed like: (For those denied the joy of the Hindly-Milner type system, "simple" is probably wildly wrong. [sent-7, score-0.737]
</p><p>2 )g': List of (gamma,delta) -> (List of (alpha,beta) -> (alpha -> beta)) -> (gamma -> delta)Perhaps another is to think of the reduction itself type-theoretically as a combinator that takes something typed like g above and spits out a new classifier trainer. [sent-8, score-0.652]
</p><p>3 a reduction "lifts" a lower-level function up to operate on some type it wasn't originally designed for. [sent-11, score-0.47]
</p><p>4 Some reductions (including, I would argue, some that have been discussed seriously) are "trivial" reductions in the sense that the antecedent never holds. [sent-16, score-0.453]
</p><p>5 Can we somehow type this kind of thing so we can separate good from bad reductions, where we try to define bad as meaning something like "creates impossible subproblems where it was possible to do otherwise"? [sent-18, score-0.659]
</p><p>6 RLBenchhints at the power that formalizing interfaces for things like reductions can be. [sent-19, score-0.717]
</p><p>7 Types and Probabilistic ModelsArguably the graphical models/Bayesian community has just as grandiose plans as John, but here the reductions are to learning probabilities in the sense "beliefs". [sent-22, score-0.369]
</p><p>8 Graphical models form a language that allows us to compose together little subgraphs that express our beliefs about some subsystem. [sent-23, score-0.339]
</p><p>9 It's interesting to explore what type theory can tell us about this as well. [sent-25, score-0.378]
</p><p>10 Allison uses Haskell to type a number of ideas includingMML and some classifiers, and can check many things statically. [sent-27, score-0.437]
</p><p>11 )It may still make a huge amount of sense to think about things in something like the Haskell type system and then translate them to the capable (but gross) type system of, say C++. [sent-33, score-1.238]
</p><p>12 Understanding polymorphism and type classes and there relation with Machine Learning may be a real fundamental breakthrough to making ML widely useful. [sent-34, score-0.377]
</p><p>13 Contributions flowing towards PL theoryRight now, when push comes to shove, all good interfaces between systems basically amount to invoking functions or closures. [sent-35, score-0.46]
</p><p>14 I think of graphical models as tools for expressing future interfaces because they preserve uncertainty across boundaries. [sent-38, score-0.738]
</p><p>15 My guess is that these interfaces are basically multi-directional (unlike functional interfaces). [sent-42, score-0.532]
</p><p>16 I can resolve something ambigious about, say a phoneme, by understanding the higher level language model. [sent-45, score-0.425]
</p><p>17 To get the phoneme parsing right I have to feedback the results from language layer. [sent-46, score-0.311]
</p><p>18 In this sense, interfaces need to preserve uncertainty and probably pass information both ways. [sent-47, score-0.647]
</p><p>19 How to start (small)I think a serious effort should be made to explain things like reductions in terms of PL theory- even if it means that, like Allison, you have to do some explaining type systems first. [sent-48, score-0.785]
</p><p>20 )We should write our libraries to have super-clean functional interfaces and make them as (parametrically) polymorphic as reasonable. [sent-51, score-0.525]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('interfaces', 0.395), ('type', 0.313), ('allison', 0.222), ('pl', 0.222), ('reductions', 0.176), ('beta', 0.167), ('combinators', 0.167), ('haskell', 0.167), ('phoneme', 0.167), ('alpha', 0.148), ('language', 0.144), ('classifier', 0.116), ('combinator', 0.111), ('sense', 0.101), ('reduction', 0.1), ('uncertainty', 0.1), ('typed', 0.099), ('graphical', 0.092), ('ml', 0.091), ('say', 0.09), ('john', 0.085), ('preserve', 0.082), ('like', 0.081), ('wildly', 0.079), ('something', 0.076), ('beliefs', 0.076), ('functional', 0.072), ('seriously', 0.072), ('impossible', 0.07), ('probably', 0.07), ('think', 0.069), ('thoughts', 0.068), ('modeling', 0.068), ('list', 0.068), ('system', 0.065), ('basically', 0.065), ('recognition', 0.065), ('things', 0.065), ('us', 0.065), ('classes', 0.064), ('level', 0.062), ('define', 0.06), ('recent', 0.059), ('check', 0.059), ('kind', 0.059), ('write', 0.058), ('designed', 0.057), ('together', 0.054), ('understanding', 0.053), ('unfortunately', 0.052)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="49-tfidf-1" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>Introduction: This post is some combination of belaboring the obvious and speculating wildly
about the future. The basic issue to be addressed is how to think about
machine learning in terms given to us from Programming Language theory.Types
and ReductionsJohn's research programme (I feel this should be in British
spelling to reflect the grandiousness of the ideaâ&euro;Ś) of machine learning
reductionsStateOfReductionis at some essential level type-theoretic in nature.
The fundamental elements are the classifier, a function f: alpha -> beta, and
the corresponding classifier trainer g: List of (alpha,beta) -> (alpha ->
beta). The research goal is to create *combinators* that produce new f's and
g's given existing ones. John (probably quite rightly) seems unwilling at the
moment to commit to any notion stronger than these combinators are correctly
typed. One way to see the result of a reduction is something typed like: (For
those denied the joy of the Hindly-Milner type system, "simple" is probably
wildly wr</p><p>2 0.17695345 <a title="49-tfidf-2" href="../hunch_net-2006/hunch_net-2006-09-28-Programming_Languages_for_Machine_Learning_Implementations.html">210 hunch net-2006-09-28-Programming Languages for Machine Learning Implementations</a></p>
<p>Introduction: Machine learning algorithms have a much better chance of being widely adopted
if they are implemented in some easy-to-use code. There are several important
concerns associated with machine learning which stress programming languages
on the ease-of-use vs. speed frontier.SpeedThe rate at which data sources are
growing seems to be outstripping the rate at which computational power is
growing, so it is important that we be able to eak out every bit of
computational power. Garbage collected languages (java,ocaml,perlandpython)
often have several issues here.Garbage collection often implies that floating
point numbers are "boxed": every float is represented by a pointer to a float.
Boxing can cause an order of magnitude slowdown because an extra nonlocalized
memory reference is made, and accesses to main memory can are many CPU cycles
long.Garbage collection often implies that considerably more memory is used
than is necessary. This has a variable effect. In some circumstances it
results in</p><p>3 0.13780296 <a title="49-tfidf-3" href="../hunch_net-2005/hunch_net-2005-06-18-Lower_Bounds_for_Learning_Reductions.html">83 hunch net-2005-06-18-Lower Bounds for Learning Reductions</a></p>
<p>Introduction: Learning reductionstransform a solver of one type of learning problem into a
solver of another type of learning problem. When we analyze these for
robustness we can make statement of the form "ReductionRhas the property that
regretr(or loss) on subproblems of typeAimplies regret at mostf ( r )on the
original problem of typeB".A lower bound for a learning reduction would have
the form "for all reductionsR, there exists a learning problem of typeBand
learning algorithm for problems of typeAwhere regretron induced problems
impliesat leastregretf ( r )forB".The pursuit of lower bounds is often
questionable because, unlike upper bounds, they do not yield practical
algorithms. Nevertheless, they may be helpful as a tool for thinking about
what is learnable and how learnable it is. This has already come
uphereandhere.At the moment, there is no coherent theory of lower bounds for
learning reductions, and we have little understanding of how feasible they are
or which techniques may be useful in</p><p>4 0.1186008 <a title="49-tfidf-4" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>Introduction: There are several different flavors of Machine Learning classes. Many classes
are of the 'zoo' sort: many different learning algorithms are presented.
Others avoid the zoo by not covering the full scope of machine learning.This
is my view of what makes a good machine learning class, along with why. I'd
like to specifically invite comment on whether things are missing,
misemphasized, or misplaced.PhaseSubjectWhy?IntroductionWhat is a machine
learning problem?A good understanding of the characteristics of machine
learning problems seems essential. Characteristics include: a data source,
some hope the data is predictive, and a need for generalization. This is
probably best taught in a case study manner: lay out the specifics of some
problem and then ask "Is this a machine learning problem?"IntroductionMachine
Learning Problem IdentificationIdentification and recognition of the type of
learning problems is (obviously) a very important step in solving such
problems. People need to be famili</p><p>5 0.107968 <a title="49-tfidf-5" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>Introduction: Muthuinvited me to the workshop onalgorithms in the field, with the goal of
providing a sense of where near-term research should go. When the time came
though, I bargained for a post instead, which provides a chance for many other
people to comment.There are several things I didn't fully understand when I
went to Yahoo! about 5 years ago. I'd like to repeat them as people in
academia may not yet understand them intuitively.Almost all the big impact
algorithms operate in pseudo-linear or better time. Think about caching,
hashing, sorting, filtering, etcâ&euro;Ś and you have a sense of what some of the
most heavily used algorithms are. This matters quite a bit to Machine Learning
research, because people often work with superlinear time algorithms and
languages. Two very common examples of this are graphical models, where
inference is often a superlinear operation--think about then2dependence on the
number of states in aHidden Markov Modeland KernelizedSupport Vector
Machineswhere optimization</p><p>6 0.10688768 <a title="49-tfidf-6" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<p>7 0.10180847 <a title="49-tfidf-7" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>8 0.10131393 <a title="49-tfidf-8" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>9 0.10068142 <a title="49-tfidf-9" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>10 0.099892877 <a title="49-tfidf-10" href="../hunch_net-2013/hunch_net-2013-12-01-NIPS_tutorials_and_Vowpal_Wabbit_7.4.html">492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</a></p>
<p>11 0.099314131 <a title="49-tfidf-11" href="../hunch_net-2007/hunch_net-2007-03-15-Alternative_Machine_Learning_Reductions_Definitions.html">236 hunch net-2007-03-15-Alternative Machine Learning Reductions Definitions</a></p>
<p>12 0.085679203 <a title="49-tfidf-12" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>13 0.084525883 <a title="49-tfidf-13" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>14 0.084448263 <a title="49-tfidf-14" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>15 0.083161771 <a title="49-tfidf-15" href="../hunch_net-2006/hunch_net-2006-08-10-Precision_is_not_accuracy.html">202 hunch net-2006-08-10-Precision is not accuracy</a></p>
<p>16 0.082703546 <a title="49-tfidf-16" href="../hunch_net-2006/hunch_net-2006-05-23-What_is_the_best_regret_transform_reduction_from_multiclass_to_binary%3F.html">181 hunch net-2006-05-23-What is the best regret transform reduction from multiclass to binary?</a></p>
<p>17 0.081806041 <a title="49-tfidf-17" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>18 0.081430204 <a title="49-tfidf-18" href="../hunch_net-2008/hunch_net-2008-04-12-It_Doesn%26%238217%3Bt_Stop.html">295 hunch net-2008-04-12-It Doesn&#8217;t Stop</a></p>
<p>19 0.080987178 <a title="49-tfidf-19" href="../hunch_net-2005/hunch_net-2005-10-16-Complexity%3A_It%26%238217%3Bs_all_in_your_head.html">123 hunch net-2005-10-16-Complexity: It&#8217;s all in your head</a></p>
<p>20 0.080582708 <a title="49-tfidf-20" href="../hunch_net-2005/hunch_net-2005-05-16-Regret_minimizing_vs_error_limiting_reductions.html">72 hunch net-2005-05-16-Regret minimizing vs error limiting reductions</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.229), (1, -0.039), (2, 0.042), (3, -0.041), (4, 0.025), (5, -0.075), (6, -0.016), (7, -0.007), (8, 0.095), (9, 0.078), (10, 0.012), (11, 0.068), (12, 0.05), (13, -0.003), (14, -0.009), (15, 0.063), (16, 0.093), (17, -0.057), (18, -0.009), (19, -0.07), (20, -0.011), (21, 0.011), (22, 0.001), (23, 0.068), (24, 0.001), (25, -0.024), (26, 0.003), (27, 0.082), (28, -0.006), (29, -0.006), (30, 0.127), (31, -0.018), (32, -0.012), (33, 0.003), (34, 0.025), (35, 0.02), (36, 0.033), (37, -0.09), (38, 0.023), (39, -0.01), (40, 0.071), (41, -0.026), (42, -0.082), (43, -0.042), (44, 0.061), (45, 0.045), (46, 0.13), (47, 0.033), (48, 0.047), (49, 0.018)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94254446 <a title="49-lsi-1" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>Introduction: This post is some combination of belaboring the obvious and speculating wildly
about the future. The basic issue to be addressed is how to think about
machine learning in terms given to us from Programming Language theory.Types
and ReductionsJohn's research programme (I feel this should be in British
spelling to reflect the grandiousness of the ideaâ&euro;Ś) of machine learning
reductionsStateOfReductionis at some essential level type-theoretic in nature.
The fundamental elements are the classifier, a function f: alpha -> beta, and
the corresponding classifier trainer g: List of (alpha,beta) -> (alpha ->
beta). The research goal is to create *combinators* that produce new f's and
g's given existing ones. John (probably quite rightly) seems unwilling at the
moment to commit to any notion stronger than these combinators are correctly
typed. One way to see the result of a reduction is something typed like: (For
those denied the joy of the Hindly-Milner type system, "simple" is probably
wildly wr</p><p>2 0.64469969 <a title="49-lsi-2" href="../hunch_net-2006/hunch_net-2006-09-28-Programming_Languages_for_Machine_Learning_Implementations.html">210 hunch net-2006-09-28-Programming Languages for Machine Learning Implementations</a></p>
<p>Introduction: Machine learning algorithms have a much better chance of being widely adopted
if they are implemented in some easy-to-use code. There are several important
concerns associated with machine learning which stress programming languages
on the ease-of-use vs. speed frontier.SpeedThe rate at which data sources are
growing seems to be outstripping the rate at which computational power is
growing, so it is important that we be able to eak out every bit of
computational power. Garbage collected languages (java,ocaml,perlandpython)
often have several issues here.Garbage collection often implies that floating
point numbers are "boxed": every float is represented by a pointer to a float.
Boxing can cause an order of magnitude slowdown because an extra nonlocalized
memory reference is made, and accesses to main memory can are many CPU cycles
long.Garbage collection often implies that considerably more memory is used
than is necessary. This has a variable effect. In some circumstances it
results in</p><p>3 0.56994253 <a title="49-lsi-3" href="../hunch_net-2005/hunch_net-2005-05-29-Maximum_Margin_Mismatch%3F.html">77 hunch net-2005-05-29-Maximum Margin Mismatch?</a></p>
<p>Introduction: John makes a fascinating point about structured classification (and slightly
scooped my post!). Maximum Margin Markov Networks (M3N) are an interesting
example of the second class of structured classifiers (where the
classification of one label depends on the others), and one of my favorite
papers. I'm not alone: the paper won the best student paper award at NIPS in
2003.There are some things I find odd about the paper. For instance, it says
of probabilistic models"cannot handle high dimensional feature spaces and lack
strong theoretical guarrantees."I'm aware of no such limitations.
Also:"Unfortunately, even probabilistic graphical models that are trained
discriminatively do not achieve the same level of performance as SVMs,
especially when kernel features are used."This is quite interesting and
contradicts my own experience as well as that of a number of
peopleIgreatlyrespect. I wonder what the root cause is: perhaps there is
something different about the data Ben+Carlos were working</p><p>4 0.54174256 <a title="49-lsi-4" href="../hunch_net-2006/hunch_net-2006-02-02-Introspectionism_as_a_Disease.html">153 hunch net-2006-02-02-Introspectionism as a Disease</a></p>
<p>Introduction: In the AI-related parts of machine learning, it is often tempting to examine
howyoudo things in order to imagine how a machine should do things. This is
introspection, and it can easily go awry. I will call introspection gone awry
introspectionism.Introspectionism is almost unique to AI (and the AI-related
parts of machine learning) and it can lead to huge wasted effort in research.
It's easiest to show how introspectionism arises by an example.Suppose we want
to solve the problem of navigating a robot from point A to point B given a
camera. Then, the following research action plan might seem natural when you
examine your own capabilities:Build an edge detector for still images.Build an
object recognition system given the edge detector.Build a system to predict
distance and orientation to objects given the object recognition system.Build
a system to plan a path through the scene you construct from {object
identification, distance, orientation} predictions.As you execute the above,
cons</p><p>5 0.52924192 <a title="49-lsi-5" href="../hunch_net-2011/hunch_net-2011-02-17-What_does_Watson_mean%3F.html">424 hunch net-2011-02-17-What does Watson mean?</a></p>
<p>Introduction: Watsonconvincingly beat the best championJeopardy!players. The apparent
significance of this varies hugely, depending on your background knowledge
about the related machine learning, NLP, and search technology. For a random
person, this might seem evidence of serious machine intelligence, while for
people working on the system itself, it probably seems like a reasonably good
assemblage of existing technologies with several twists to make the entire
system work.Above all, I think we should congratulate the people who managed
to put together and execute this project--many years of effort by a diverse
set of highly skilled people were needed to make this happen. In academia,
it's pretty difficult for one professor to assemble that quantity of talent,
and in industry it's rarely the case that such a capable group has both a
worthwhile project and the support needed to pursue something like this for
several years before success.Alinainvited me to the Jeopardy watching party
atIBM, which was</p><p>6 0.5262289 <a title="49-lsi-6" href="../hunch_net-2007/hunch_net-2007-09-16-Optimizing_Machine_Learning_Programs.html">262 hunch net-2007-09-16-Optimizing Machine Learning Programs</a></p>
<p>7 0.52565974 <a title="49-lsi-7" href="../hunch_net-2008/hunch_net-2008-11-26-Efficient_Reinforcement_Learning_in_MDPs.html">328 hunch net-2008-11-26-Efficient Reinforcement Learning in MDPs</a></p>
<p>8 0.52342957 <a title="49-lsi-8" href="../hunch_net-2013/hunch_net-2013-12-01-NIPS_tutorials_and_Vowpal_Wabbit_7.4.html">492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</a></p>
<p>9 0.51653719 <a title="49-lsi-9" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>10 0.50707483 <a title="49-lsi-10" href="../hunch_net-2010/hunch_net-2010-09-28-Machined_Learnings.html">412 hunch net-2010-09-28-Machined Learnings</a></p>
<p>11 0.50528073 <a title="49-lsi-11" href="../hunch_net-2007/hunch_net-2007-12-20-Cool_and_Interesting_things_at_NIPS%2C_take_three.html">280 hunch net-2007-12-20-Cool and Interesting things at NIPS, take three</a></p>
<p>12 0.49998605 <a title="49-lsi-12" href="../hunch_net-2011/hunch_net-2011-08-06-Interesting_thing_at_UAI_2011.html">440 hunch net-2011-08-06-Interesting thing at UAI 2011</a></p>
<p>13 0.49452469 <a title="49-lsi-13" href="../hunch_net-2005/hunch_net-2005-05-12-Math_on_the_Web.html">70 hunch net-2005-05-12-Math on the Web</a></p>
<p>14 0.48738113 <a title="49-lsi-14" href="../hunch_net-2005/hunch_net-2005-06-22-Languages__of_Learning.html">84 hunch net-2005-06-22-Languages  of Learning</a></p>
<p>15 0.48735887 <a title="49-lsi-15" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>16 0.48656374 <a title="49-lsi-16" href="../hunch_net-2006/hunch_net-2006-07-09-The_Stock_Prediction_Machine_Learning_Problem.html">193 hunch net-2006-07-09-The Stock Prediction Machine Learning Problem</a></p>
<p>17 0.48413536 <a title="49-lsi-17" href="../hunch_net-2006/hunch_net-2006-04-09-Progress_in_Machine_Translation.html">171 hunch net-2006-04-09-Progress in Machine Translation</a></p>
<p>18 0.48250476 <a title="49-lsi-18" href="../hunch_net-2005/hunch_net-2005-06-17-Reopening_RL-%3EClassification.html">82 hunch net-2005-06-17-Reopening RL->Classification</a></p>
<p>19 0.48222005 <a title="49-lsi-19" href="../hunch_net-2005/hunch_net-2005-11-05-The_design_of_a_computing_cluster.html">128 hunch net-2005-11-05-The design of a computing cluster</a></p>
<p>20 0.48196915 <a title="49-lsi-20" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(4, 0.01), (6, 0.031), (15, 0.011), (35, 0.056), (42, 0.587), (45, 0.028), (48, 0.019), (50, 0.01), (52, 0.014), (56, 0.024), (74, 0.094), (92, 0.015), (95, 0.025)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99765569 <a title="49-lda-1" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>Introduction: This post is some combination of belaboring the obvious and speculating wildly
about the future. The basic issue to be addressed is how to think about
machine learning in terms given to us from Programming Language theory.Types
and ReductionsJohn's research programme (I feel this should be in British
spelling to reflect the grandiousness of the ideaâ&euro;Ś) of machine learning
reductionsStateOfReductionis at some essential level type-theoretic in nature.
The fundamental elements are the classifier, a function f: alpha -> beta, and
the corresponding classifier trainer g: List of (alpha,beta) -> (alpha ->
beta). The research goal is to create *combinators* that produce new f's and
g's given existing ones. John (probably quite rightly) seems unwilling at the
moment to commit to any notion stronger than these combinators are correctly
typed. One way to see the result of a reduction is something typed like: (For
those denied the joy of the Hindly-Milner type system, "simple" is probably
wildly wr</p><p>2 0.99313819 <a title="49-lda-2" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<p>Introduction: This post is by Daniel Hsu and John Langford.In selective sampling style
active learning, a learning algorithm chooses which examples to label. We now
have an active learning algorithm that is:Efficientin label complexity,
unlabeled complexity, and computational complexity.Competitivewith supervised
learning anywhere that supervised learning works.Compatiblewith online
learning, with any optimization-based learning algorithm, with any loss
function, with offline testing, and even with changing learning
algorithms.Empiricallyeffective.The basic idea is to combinedisagreement
region-based samplingwithimportance weighting: an example is selected to be
labeled with probability proportional to how useful it is for distinguishing
among near-optimal classifiers, and labeled examples are importance-weighted
by the inverse of these probabilities. The combination of these simple ideas
removes thesampling biasproblem that has plagued many previous heuristics for
active learning, and yet leads to</p><p>3 0.99026924 <a title="49-lda-3" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>Introduction: If you search for "online learning" with anymajorsearchengine, it's
interesting to note that zero of the results are for online machine learning.
This may not be a mistake if you are committed to a global ordering. In other
words, the number of people specifically interested in the least interesting
top-10 online human learning result might exceed the number of people
interested in online machine learning, even given the presence of the other 9
results. The essential observation here is that the process of human learning
is a big business (around 5% of GDP) effecting virtually everyone.The internet
is changing this dramatically, by altering the economics of teaching. Consider
two possibilities:The classroom-style teaching environment continues as is,
with many teachers for the same subject.All the teachers for one subject get
together, along with perhaps a factor of 2 more people who are experts in
online delivery. They spend a factor of 4 more time designing the perfect
lecture & lear</p><p>4 0.9896993 <a title="49-lda-4" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>Introduction: There are several different flavors of Machine Learning classes. Many classes
are of the 'zoo' sort: many different learning algorithms are presented.
Others avoid the zoo by not covering the full scope of machine learning.This
is my view of what makes a good machine learning class, along with why. I'd
like to specifically invite comment on whether things are missing,
misemphasized, or misplaced.PhaseSubjectWhy?IntroductionWhat is a machine
learning problem?A good understanding of the characteristics of machine
learning problems seems essential. Characteristics include: a data source,
some hope the data is predictive, and a need for generalization. This is
probably best taught in a case study manner: lay out the specifics of some
problem and then ask "Is this a machine learning problem?"IntroductionMachine
Learning Problem IdentificationIdentification and recognition of the type of
learning problems is (obviously) a very important step in solving such
problems. People need to be famili</p><p>5 0.98854506 <a title="49-lda-5" href="../hunch_net-2005/hunch_net-2005-07-07-The_Limits_of_Learning_Theory.html">90 hunch net-2005-07-07-The Limits of Learning Theory</a></p>
<p>Introduction: Suppose we had an infinitely powerful mathematician sitting in a room and
proving theorems about learning. Could he solve machine learning?The answer is
"no". This answer is both obvious and sometimes underappreciated.There are
several ways to conclude that somebiasis necessary in order to succesfully
learn. For example, suppose we are trying to solve classification. At
prediction time, we observe some featuresXand want to make a prediction of
either0or1. Bias is what makes us prefer one answer over the other based on
past experience. In order to learn we must:Have a bias. Always predicting0is
as likely as1is useless.Have the "right" bias. Predicting1when the answer
is0is also not helpful.The implication of "have a bias" is that we can not
design effective learning algorithms with "a uniform prior over all
possibilities". The implication of "have the 'right' bias" is that our
mathematician fails since "right" is defined with respect to the solutions to
problems encountered in the real</p><p>6 0.98780173 <a title="49-lda-6" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>7 0.9876312 <a title="49-lda-7" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>8 0.9857825 <a title="49-lda-8" href="../hunch_net-2005/hunch_net-2005-12-09-Some_NIPS_papers.html">138 hunch net-2005-12-09-Some NIPS papers</a></p>
<p>9 0.98576069 <a title="49-lda-9" href="../hunch_net-2005/hunch_net-2005-03-29-Academic_Mechanism_Design.html">48 hunch net-2005-03-29-Academic Mechanism Design</a></p>
<p>10 0.98536915 <a title="49-lda-10" href="../hunch_net-2006/hunch_net-2006-06-15-IJCAI_is_out_of_season.html">184 hunch net-2006-06-15-IJCAI is out of season</a></p>
<p>11 0.98439747 <a title="49-lda-11" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>12 0.98339701 <a title="49-lda-12" href="../hunch_net-2005/hunch_net-2005-05-06-Don%26%238217%3Bt_mix_the_solution_into_the_problem.html">67 hunch net-2005-05-06-Don&#8217;t mix the solution into the problem</a></p>
<p>13 0.98332059 <a title="49-lda-13" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>14 0.98189342 <a title="49-lda-14" href="../hunch_net-2012/hunch_net-2012-10-26-ML_Symposium_and_Strata-Hadoop_World.html">475 hunch net-2012-10-26-ML Symposium and Strata-Hadoop World</a></p>
<p>15 0.98010796 <a title="49-lda-15" href="../hunch_net-2006/hunch_net-2006-01-23-On_Coding_via_Mutual_Information_%26%23038%3B_Bayes_Nets.html">150 hunch net-2006-01-23-On Coding via Mutual Information &#038; Bayes Nets</a></p>
<p>16 0.97794783 <a title="49-lda-16" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>17 0.97739512 <a title="49-lda-17" href="../hunch_net-2005/hunch_net-2005-02-23-Problem%3A_Reinforcement_Learning_with_Classification.html">27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</a></p>
<p>18 0.97737747 <a title="49-lda-18" href="../hunch_net-2008/hunch_net-2008-04-27-Watchword%3A_Supervised_Learning.html">299 hunch net-2008-04-27-Watchword: Supervised Learning</a></p>
<p>19 0.97379684 <a title="49-lda-19" href="../hunch_net-2005/hunch_net-2005-03-21-Research_Styles_in_Machine_Learning.html">44 hunch net-2005-03-21-Research Styles in Machine Learning</a></p>
<p>20 0.97225213 <a title="49-lda-20" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
