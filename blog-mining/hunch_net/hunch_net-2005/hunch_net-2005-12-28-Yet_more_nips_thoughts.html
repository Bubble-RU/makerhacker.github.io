<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>144 hunch net-2005-12-28-Yet more nips thoughts</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-144" href="#">hunch_net-2005-144</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>144 hunch net-2005-12-28-Yet more nips thoughts</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-144-html" href="http://hunch.net/?p=155">html</a></p><p>Introduction: I only managed to make it out to the NIPS workshops this year soI'll give my
comments on what I saw there.The Learing and Robotics workshops lives again. I
hope itcontinues and gets more high quality papers in the future. Themost
interesting talk for me was Larry Jackel's on the LAGRprogram (see John's
previous post on said program). I got someideas as to what progress has been
made. Larry really explainedthe types of benchmarks and the tradeoffs that had
to be made tomake the goals achievable but challenging.Hal Daume gave a very
interesting talk about structuredprediction using RL techniques, something
near and dear to my ownheart. He achieved rather impressive results using only
a verygreedy search.The non-parametric Bayes workshop was great. I enjoyed the
entiremorning session I spent there, and particularly (the usuallydesultory)
discussion periods. One interesting topic was theGibbs/Variational inference
divide. I won't try to summarizeespecially as no conclusion was reached. It</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bibtex', 0.616), ('pdf', 0.547), ('larry', 0.12), ('dirichlet', 0.109), ('variants', 0.092), ('interesting', 0.081), ('workshops', 0.07), ('olivier', 0.068), ('thrun', 0.068), ('aaron', 0.068), ('alan', 0.068), ('benchmarks', 0.068), ('dear', 0.068), ('dynamical', 0.068), ('freeman', 0.068), ('gibbs', 0.068), ('le', 0.068), ('modelsdavid', 0.068), ('partha', 0.068), ('sudderth', 0.068)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="144-tfidf-1" href="../hunch_net-2005/hunch_net-2005-12-28-Yet_more_nips_thoughts.html">144 hunch net-2005-12-28-Yet more nips thoughts</a></p>
<p>Introduction: I only managed to make it out to the NIPS workshops this year soI'll give my
comments on what I saw there.The Learing and Robotics workshops lives again. I
hope itcontinues and gets more high quality papers in the future. Themost
interesting talk for me was Larry Jackel's on the LAGRprogram (see John's
previous post on said program). I got someideas as to what progress has been
made. Larry really explainedthe types of benchmarks and the tradeoffs that had
to be made tomake the goals achievable but challenging.Hal Daume gave a very
interesting talk about structuredprediction using RL techniques, something
near and dear to my ownheart. He achieved rather impressive results using only
a verygreedy search.The non-parametric Bayes workshop was great. I enjoyed the
entiremorning session I spent there, and particularly (the usuallydesultory)
discussion periods. One interesting topic was theGibbs/Variational inference
divide. I won't try to summarizeespecially as no conclusion was reached. It</p><p>2 0.077186041 <a title="144-tfidf-2" href="../hunch_net-2007/hunch_net-2007-12-20-Cool_and_Interesting_things_at_NIPS%2C_take_three.html">280 hunch net-2007-12-20-Cool and Interesting things at NIPS, take three</a></p>
<p>Introduction: Following up on Hal Daume's post and John's post on cool and interesting
things seen at NIPS I'll post my own little list of neat papers here as well.
Of course it's going to be biased towards what I think is interesting. Also, I
have to say that I hadn't been able to see many papers this year at nips due
to myself being too busy, so please feel free to contribute the papers that
you liked1. P. Mudigonda, V. Kolmogorov, P. Torr. An Analysis of Convex
Relaxations for MAP Estimation. A surprising paper which shows that many of
the more sophisticated convex relaxations that had been proposed recently
turns out to be subsumed by the simplest LP relaxation. Be careful next time
you try a cool new convex relaxation!2. D. Sontag, T. Jaakkola. New Outer
Bounds on the Marginal Polytope. The title says it all. The marginal polytope
is the set of local marginal distributions over subsets of variables that are
globally consistent in the sense that there is at least one distribution over
all the va</p><p>3 0.070085056 <a title="144-tfidf-3" href="../hunch_net-2011/hunch_net-2011-07-11-Interesting_Neural_Network_Papers_at_ICML_2011.html">438 hunch net-2011-07-11-Interesting Neural Network Papers at ICML 2011</a></p>
<p>Introduction: Maybe it's too early to call, but with four separate Neural Network sessions
at this year'sICML, it looks like Neural Networks are making a comeback. Here
are my highlights of these sessions. In general, my feeling is that these
papers both demystify deep learning and show its broader applicability.The
first observation I made is that the once disreputable "Neural" nomenclature
is being used againin lieu of"deep learning". Maybe it's because Adam Coates
et al. showed that single layer networks can work surprisingly well.An
Analysis of Single-Layer Networks in Unsupervised Feature Learning,Adam
Coates,Honglak Lee,Andrew Y. Ng(AISTATS 2011)The Importance of Encoding Versus
Training with Sparse Coding and Vector Quantization,Adam Coates,Andrew Y.
Ng(ICML 2011)Another surprising result out of Andrew Ng's group comes from
Andrew Saxe et al. who show that certain convolutional pooling architectures
can obtain close to state-of-the-art performance with random weights (that is,
without actuall</p><p>4 0.064831048 <a title="144-tfidf-4" href="../hunch_net-2005/hunch_net-2005-12-14-More_NIPS_Papers_II.html">140 hunch net-2005-12-14-More NIPS Papers II</a></p>
<p>Introduction: I thought this was a very good NIPS with many excellent papers. The following
are a few NIPS papers which I liked and I hope to study more carefully when I
get the chance. The list is not exhaustive and in no particular
order…Preconditioner Approximations for Probabilistic Graphical
Models.Pradeeep Ravikumar and John Lafferty.I thought the use of
preconditioner methods from solving linear systems in the context of
approximate inference was novel and interesting. The results look good and I'd
like to understand the limitations.Rodeo: Sparse nonparametric regression in
high dimensions.John Lafferty and Larry Wasserman.A very interesting approach
to feature selection in nonparametric regression from a frequentist framework.
The use of lengthscale variables in each dimension reminds me a lot of
'Automatic Relevance Determination' in Gaussian process regression -- it would
be interesting to compare Rodeo to ARD in GPs.Interpolating between types and
tokens by estimating power law generators</p><p>5 0.064064533 <a title="144-tfidf-5" href="../hunch_net-2011/hunch_net-2011-09-03-Fall_Machine_Learning_Events.html">443 hunch net-2011-09-03-Fall Machine Learning Events</a></p>
<p>Introduction: Many Machine Learning related events are coming up this fall.September
9,abstracts for the New York Machine Learning Symposiumare due. Send a 2 page
pdf, if interested, and note that we:widened submissions to be from anybody
rather than students.set aside a larger fraction of time for contributed
submissions.September 15, there is amachine learning meetup, where I'll be
discussing terascale learning at AOL.September 16, there is aCS&Econ; dayat New
York Academy of Sciences. This is not ML focused, but it's easy to imagine
interest.September 23 and laterNIPS workshopsubmissions start coming due. As
usual, there are too many good ones, so I won't be able to attend all those
that interest me. I do hope some workshop makers consider ICML this coming
summer, as we are increasing to a 2 day format for you. Here are a few that
interest me:Big Learningis about dealing with lots of data. Abstracts are
dueSeptember 30.TheBayes Banditsworkshop. Abstracts are dueSeptember
23.ThePersonalized Medicin</p><p>6 0.061388075 <a title="144-tfidf-6" href="../hunch_net-2005/hunch_net-2005-12-09-Machine_Learning_Thoughts.html">137 hunch net-2005-12-09-Machine Learning Thoughts</a></p>
<p>7 0.055653058 <a title="144-tfidf-7" href="../hunch_net-2010/hunch_net-2010-10-17-Partha_Niyogi_has_died.html">414 hunch net-2010-10-17-Partha Niyogi has died</a></p>
<p>8 0.052687943 <a title="144-tfidf-8" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>9 0.052588291 <a title="144-tfidf-9" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>10 0.052485961 <a title="144-tfidf-10" href="../hunch_net-2012/hunch_net-2012-06-15-Normal_Deviate_and_the_UCSC_Machine_Learning_Summer_School.html">467 hunch net-2012-06-15-Normal Deviate and the UCSC Machine Learning Summer School</a></p>
<p>11 0.052364122 <a title="144-tfidf-11" href="../hunch_net-2012/hunch_net-2012-12-29-Simons_Institute_Big_Data_Program.html">476 hunch net-2012-12-29-Simons Institute Big Data Program</a></p>
<p>12 0.051714346 <a title="144-tfidf-12" href="../hunch_net-2006/hunch_net-2006-07-05-more_icml_papers.html">189 hunch net-2006-07-05-more icml papers</a></p>
<p>13 0.051639397 <a title="144-tfidf-13" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>14 0.051382132 <a title="144-tfidf-14" href="../hunch_net-2006/hunch_net-2006-07-08-Some_recent_papers.html">192 hunch net-2006-07-08-Some recent papers</a></p>
<p>15 0.048735328 <a title="144-tfidf-15" href="../hunch_net-2005/hunch_net-2005-02-20-At_One_Month.html">25 hunch net-2005-02-20-At One Month</a></p>
<p>16 0.048375219 <a title="144-tfidf-16" href="../hunch_net-2009/hunch_net-2009-12-27-Interesting_things_at_NIPS_2009.html">385 hunch net-2009-12-27-Interesting things at NIPS 2009</a></p>
<p>17 0.045875937 <a title="144-tfidf-17" href="../hunch_net-2005/hunch_net-2005-03-24-The_Role_of_Workshops.html">46 hunch net-2005-03-24-The Role of Workshops</a></p>
<p>18 0.043708961 <a title="144-tfidf-18" href="../hunch_net-2008/hunch_net-2008-07-15-Interesting_papers_at_COLT_%28and_a_bit_of_UAI_%26%23038%3B_workshops%29.html">310 hunch net-2008-07-15-Interesting papers at COLT (and a bit of UAI &#038; workshops)</a></p>
<p>19 0.043396939 <a title="144-tfidf-19" href="../hunch_net-2009/hunch_net-2009-11-23-ICML_2009_Workshops_%28and_Tutorials%29.html">379 hunch net-2009-11-23-ICML 2009 Workshops (and Tutorials)</a></p>
<p>20 0.043134399 <a title="144-tfidf-20" href="../hunch_net-2005/hunch_net-2005-12-11-More_NIPS_Papers.html">139 hunch net-2005-12-11-More NIPS Papers</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.09), (1, 0.023), (2, 0.002), (3, 0.082), (4, -0.009), (5, -0.01), (6, -0.047), (7, -0.064), (8, -0.008), (9, 0.03), (10, 0.015), (11, 0.018), (12, -0.06), (13, 0.008), (14, -0.009), (15, -0.023), (16, 0.046), (17, -0.021), (18, 0.015), (19, 0.023), (20, 0.019), (21, 0.034), (22, -0.005), (23, -0.031), (24, -0.035), (25, -0.06), (26, -0.016), (27, 0.058), (28, -0.003), (29, -0.038), (30, 0.053), (31, -0.033), (32, 0.025), (33, -0.024), (34, -0.014), (35, -0.015), (36, 0.014), (37, -0.04), (38, 0.079), (39, -0.032), (40, 0.07), (41, -0.029), (42, -0.074), (43, 0.054), (44, -0.003), (45, -0.015), (46, 0.023), (47, 0.034), (48, 0.006), (49, 0.019)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96202052 <a title="144-lsi-1" href="../hunch_net-2005/hunch_net-2005-12-28-Yet_more_nips_thoughts.html">144 hunch net-2005-12-28-Yet more nips thoughts</a></p>
<p>Introduction: I only managed to make it out to the NIPS workshops this year soI'll give my
comments on what I saw there.The Learing and Robotics workshops lives again. I
hope itcontinues and gets more high quality papers in the future. Themost
interesting talk for me was Larry Jackel's on the LAGRprogram (see John's
previous post on said program). I got someideas as to what progress has been
made. Larry really explainedthe types of benchmarks and the tradeoffs that had
to be made tomake the goals achievable but challenging.Hal Daume gave a very
interesting talk about structuredprediction using RL techniques, something
near and dear to my ownheart. He achieved rather impressive results using only
a verygreedy search.The non-parametric Bayes workshop was great. I enjoyed the
entiremorning session I spent there, and particularly (the usuallydesultory)
discussion periods. One interesting topic was theGibbs/Variational inference
divide. I won't try to summarizeespecially as no conclusion was reached. It</p><p>2 0.59840405 <a title="144-lsi-2" href="../hunch_net-2007/hunch_net-2007-12-20-Cool_and_Interesting_things_at_NIPS%2C_take_three.html">280 hunch net-2007-12-20-Cool and Interesting things at NIPS, take three</a></p>
<p>Introduction: Following up on Hal Daume's post and John's post on cool and interesting
things seen at NIPS I'll post my own little list of neat papers here as well.
Of course it's going to be biased towards what I think is interesting. Also, I
have to say that I hadn't been able to see many papers this year at nips due
to myself being too busy, so please feel free to contribute the papers that
you liked1. P. Mudigonda, V. Kolmogorov, P. Torr. An Analysis of Convex
Relaxations for MAP Estimation. A surprising paper which shows that many of
the more sophisticated convex relaxations that had been proposed recently
turns out to be subsumed by the simplest LP relaxation. Be careful next time
you try a cool new convex relaxation!2. D. Sontag, T. Jaakkola. New Outer
Bounds on the Marginal Polytope. The title says it all. The marginal polytope
is the set of local marginal distributions over subsets of variables that are
globally consistent in the sense that there is at least one distribution over
all the va</p><p>3 0.57565767 <a title="144-lsi-3" href="../hunch_net-2005/hunch_net-2005-12-14-More_NIPS_Papers_II.html">140 hunch net-2005-12-14-More NIPS Papers II</a></p>
<p>Introduction: I thought this was a very good NIPS with many excellent papers. The following
are a few NIPS papers which I liked and I hope to study more carefully when I
get the chance. The list is not exhaustive and in no particular
order…Preconditioner Approximations for Probabilistic Graphical
Models.Pradeeep Ravikumar and John Lafferty.I thought the use of
preconditioner methods from solving linear systems in the context of
approximate inference was novel and interesting. The results look good and I'd
like to understand the limitations.Rodeo: Sparse nonparametric regression in
high dimensions.John Lafferty and Larry Wasserman.A very interesting approach
to feature selection in nonparametric regression from a frequentist framework.
The use of lengthscale variables in each dimension reminds me a lot of
'Automatic Relevance Determination' in Gaussian process regression -- it would
be interesting to compare Rodeo to ARD in GPs.Interpolating between types and
tokens by estimating power law generators</p><p>4 0.53108281 <a title="144-lsi-4" href="../hunch_net-2005/hunch_net-2005-05-29-Maximum_Margin_Mismatch%3F.html">77 hunch net-2005-05-29-Maximum Margin Mismatch?</a></p>
<p>Introduction: John makes a fascinating point about structured classification (and slightly
scooped my post!). Maximum Margin Markov Networks (M3N) are an interesting
example of the second class of structured classifiers (where the
classification of one label depends on the others), and one of my favorite
papers. I'm not alone: the paper won the best student paper award at NIPS in
2003.There are some things I find odd about the paper. For instance, it says
of probabilistic models"cannot handle high dimensional feature spaces and lack
strong theoretical guarrantees."I'm aware of no such limitations.
Also:"Unfortunately, even probabilistic graphical models that are trained
discriminatively do not achieve the same level of performance as SVMs,
especially when kernel features are used."This is quite interesting and
contradicts my own experience as well as that of a number of
peopleIgreatlyrespect. I wonder what the root cause is: perhaps there is
something different about the data Ben+Carlos were working</p><p>5 0.51635766 <a title="144-lsi-5" href="../hunch_net-2006/hunch_net-2006-06-16-Regularization_%3D_Robustness.html">185 hunch net-2006-06-16-Regularization = Robustness</a></p>
<p>Introduction: The Gibbs-Jaynes theorem is a classical result that tells us that the highest
entropy distribution (most uncertain, least committed, etc.) subject to
expectation constraints on a set of features is an exponential family
distribution with the features as sufficient statistics. In math,argmax_p
H(p)s.t. E_p[f_i] = c_iis given by e^{\sum \lambda_i f_i}/Z. (Z here is the
necessary normalization constraint, and the lambdas are free parameters we set
to meet the expectation constraints).A great deal of statistical mechanics
flows from this result, and it has proven very fruitful in learning as well.
(Motivating work in models in text learning and Conditional Random Fields, for
instance. ) The result has been demonstrated a number of ways. One of the most
elegant is the Ã¢â‚¬Å“geometricÃ¢â‚¬Â versionhere.In the case when the
expectation constraints come from data, this tells us that the maximum entropy
distribution is exactly the maximum likelihood distribution in the exponential
family. ItÃ</p><p>6 0.50395179 <a title="144-lsi-6" href="../hunch_net-2005/hunch_net-2005-03-24-The_Role_of_Workshops.html">46 hunch net-2005-03-24-The Role of Workshops</a></p>
<p>7 0.49693543 <a title="144-lsi-7" href="../hunch_net-2005/hunch_net-2005-12-11-More_NIPS_Papers.html">139 hunch net-2005-12-11-More NIPS Papers</a></p>
<p>8 0.49193409 <a title="144-lsi-8" href="../hunch_net-2008/hunch_net-2008-01-23-Why_Workshop%3F.html">285 hunch net-2008-01-23-Why Workshop?</a></p>
<p>9 0.4877772 <a title="144-lsi-9" href="../hunch_net-2007/hunch_net-2007-10-15-NIPS_workshops_extended_to_3_days.html">266 hunch net-2007-10-15-NIPS workshops extended to 3 days</a></p>
<p>10 0.4514485 <a title="144-lsi-10" href="../hunch_net-2006/hunch_net-2006-07-08-Some_recent_papers.html">192 hunch net-2006-07-08-Some recent papers</a></p>
<p>11 0.4503113 <a title="144-lsi-11" href="../hunch_net-2006/hunch_net-2006-11-02-2006_NIPS_workshops.html">216 hunch net-2006-11-02-2006 NIPS workshops</a></p>
<p>12 0.41180298 <a title="144-lsi-12" href="../hunch_net-2007/hunch_net-2007-07-12-ICML_Trends.html">254 hunch net-2007-07-12-ICML Trends</a></p>
<p>13 0.41161621 <a title="144-lsi-13" href="../hunch_net-2005/hunch_net-2005-05-14-NIPS.html">71 hunch net-2005-05-14-NIPS</a></p>
<p>14 0.40815869 <a title="144-lsi-14" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>15 0.40016523 <a title="144-lsi-15" href="../hunch_net-2006/hunch_net-2006-01-23-On_Coding_via_Mutual_Information_%26%23038%3B_Bayes_Nets.html">150 hunch net-2006-01-23-On Coding via Mutual Information &#038; Bayes Nets</a></p>
<p>16 0.39607185 <a title="144-lsi-16" href="../hunch_net-2006/hunch_net-2006-02-07-Pittsburgh_Mind_Reading_Competition.html">155 hunch net-2006-02-07-Pittsburgh Mind Reading Competition</a></p>
<p>17 0.39144412 <a title="144-lsi-17" href="../hunch_net-2005/hunch_net-2005-02-01-NIPS%3A_Online_Bayes.html">8 hunch net-2005-02-01-NIPS: Online Bayes</a></p>
<p>18 0.38530606 <a title="144-lsi-18" href="../hunch_net-2011/hunch_net-2011-07-11-Interesting_Neural_Network_Papers_at_ICML_2011.html">438 hunch net-2011-07-11-Interesting Neural Network Papers at ICML 2011</a></p>
<p>19 0.37859797 <a title="144-lsi-19" href="../hunch_net-2007/hunch_net-2007-09-18-It%26%238217%3Bs_MDL_Jim%2C_but_not_as_we_know_it%26%238230%3B%28on_Bayes%2C_MDL_and_consistency%29.html">263 hunch net-2007-09-18-It&#8217;s MDL Jim, but not as we know it&#8230;(on Bayes, MDL and consistency)</a></p>
<p>20 0.378014 <a title="144-lsi-20" href="../hunch_net-2005/hunch_net-2005-09-19-NIPS_Workshops.html">113 hunch net-2005-09-19-NIPS Workshops</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(6, 0.033), (26, 0.046), (35, 0.047), (42, 0.113), (45, 0.018), (47, 0.018), (53, 0.015), (62, 0.039), (68, 0.037), (73, 0.318), (74, 0.085), (76, 0.022), (82, 0.018), (88, 0.032), (95, 0.039)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.85451877 <a title="144-lda-1" href="../hunch_net-2005/hunch_net-2005-12-28-Yet_more_nips_thoughts.html">144 hunch net-2005-12-28-Yet more nips thoughts</a></p>
<p>Introduction: I only managed to make it out to the NIPS workshops this year soI'll give my
comments on what I saw there.The Learing and Robotics workshops lives again. I
hope itcontinues and gets more high quality papers in the future. Themost
interesting talk for me was Larry Jackel's on the LAGRprogram (see John's
previous post on said program). I got someideas as to what progress has been
made. Larry really explainedthe types of benchmarks and the tradeoffs that had
to be made tomake the goals achievable but challenging.Hal Daume gave a very
interesting talk about structuredprediction using RL techniques, something
near and dear to my ownheart. He achieved rather impressive results using only
a verygreedy search.The non-parametric Bayes workshop was great. I enjoyed the
entiremorning session I spent there, and particularly (the usuallydesultory)
discussion periods. One interesting topic was theGibbs/Variational inference
divide. I won't try to summarizeespecially as no conclusion was reached. It</p><p>2 0.84558594 <a title="144-lda-2" href="../hunch_net-2013/hunch_net-2013-06-10-The_Large_Scale_Learning_class_notes.html">483 hunch net-2013-06-10-The Large Scale Learning class notes</a></p>
<p>Introduction: Thelarge scale machine learning classI taught withYann LeCunhas finished. As I
expected, it took quite a bit of time. We had about 25 people attending in
person on average and 400 regularly watching therecorded lectureswhich is
substantially more sustained interest than I expected for an advanced ML
class. We also had some fun with class projects--I'm hopeful that several will
eventually turn into papers.I expect there are a number of professors
interested in lecturing on this and related topics. Everyone will have their
personal taste in subjects of course, but hopefully there will be some
convergence to common course materials as well. To help with this, I am making
thesources to my presentations available. Feel free to
use/improve/embelish/ridicule/etcâ&euro;Ś in the pursuit of the perfect course.</p><p>3 0.8364588 <a title="144-lda-3" href="../hunch_net-2005/hunch_net-2005-07-13-Text_Entailment_at_AAAI.html">94 hunch net-2005-07-13-Text Entailment at AAAI</a></p>
<p>Introduction: Rajat Rainapresented a paper on the technique they used for
thePASCALRecognizing Textual Entailmentchallenge."Text entailment" is the
problem of deciding if one sentence implies another. For example the previous
sentence entails:Text entailment is a decision problem.One sentence can imply
another.The challenge was of the form: given an original sentence and another
sentence predict whether there was an entailment. All current techniques for
predicting correctness of an entailment are at the "flail" stage--accuracies
of around 58% where humans could achieve near 100% accuracy, so there is much
room to improve. Apparently, there may be another PASCAL challenge on this
problem in the near future.</p><p>4 0.75339031 <a title="144-lda-4" href="../hunch_net-2007/hunch_net-2007-11-16-MLSS_2008.html">273 hunch net-2007-11-16-MLSS 2008</a></p>
<p>Introduction: â&euro;Ś is in Kioloa, Australia from March 3 to March 14. It's a great chance to
learn something about Machine Learning and I've enjoyed severalprevious
Machine Learning Summer Schools.Thewebsite has many more details, but
registration is open now for the first 80 to sign up.</p><p>5 0.72391737 <a title="144-lda-5" href="../hunch_net-2008/hunch_net-2008-10-20-New_York%26%238217%3Bs_ML_Day.html">322 hunch net-2008-10-20-New York&#8217;s ML Day</a></p>
<p>Introduction: I'm not as naturally exuberant asMuthu2orDavidaboutCS/Econday, but I believe
it andML daywere certainly successful.At the CS/Econ day, I particularly
enjoyedToumas Sandholm'stalk which showed a commanding depth of understanding
and application in automated auctions.For the machine learning day, I enjoyed
several talks and posters (I better, I helped pick them.). What stood out to
me was number of people attending: 158 registered, a level qualifying as
"scramble to find seats". My rule of thumb for workshops/conferences is that
the number of attendees is often something like the number of submissions.
That isn't the case here, where there were just 4 invited speakers and 30-or-
so posters. Presumably, the difference is due to a critical mass of Machine
Learning interested people in the area and the ease of their attendance.Are
there other areas where a local Machine Learning day would fly? It's easy to
imagine something working out in the San Francisco bay area and possibly
Germany or E</p><p>6 0.71024942 <a title="144-lda-6" href="../hunch_net-2011/hunch_net-2011-04-11-The_Heritage_Health_Prize.html">430 hunch net-2011-04-11-The Heritage Health Prize</a></p>
<p>7 0.56814247 <a title="144-lda-7" href="../hunch_net-2005/hunch_net-2005-03-15-The_State_of_Tight_Bounds.html">41 hunch net-2005-03-15-The State of Tight Bounds</a></p>
<p>8 0.47101107 <a title="144-lda-8" href="../hunch_net-2005/hunch_net-2005-09-26-Prediction_Bounds_as_the_Mathematics_of_Science.html">115 hunch net-2005-09-26-Prediction Bounds as the Mathematics of Science</a></p>
<p>9 0.44267878 <a title="144-lda-9" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>10 0.43562126 <a title="144-lda-10" href="../hunch_net-2010/hunch_net-2010-10-29-To_Vidoelecture_or_not.html">416 hunch net-2010-10-29-To Vidoelecture or not</a></p>
<p>11 0.43158996 <a title="144-lda-11" href="../hunch_net-2005/hunch_net-2005-11-05-The_design_of_a_computing_cluster.html">128 hunch net-2005-11-05-The design of a computing cluster</a></p>
<p>12 0.43089956 <a title="144-lda-12" href="../hunch_net-2007/hunch_net-2007-12-20-Cool_and_Interesting_things_at_NIPS%2C_take_three.html">280 hunch net-2007-12-20-Cool and Interesting things at NIPS, take three</a></p>
<p>13 0.42831382 <a title="144-lda-13" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<p>14 0.427349 <a title="144-lda-14" href="../hunch_net-2011/hunch_net-2011-02-17-What_does_Watson_mean%3F.html">424 hunch net-2011-02-17-What does Watson mean?</a></p>
<p>15 0.42642918 <a title="144-lda-15" href="../hunch_net-2010/hunch_net-2010-10-17-Partha_Niyogi_has_died.html">414 hunch net-2010-10-17-Partha Niyogi has died</a></p>
<p>16 0.42366409 <a title="144-lda-16" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>17 0.42273274 <a title="144-lda-17" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>18 0.42090255 <a title="144-lda-18" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>19 0.41962424 <a title="144-lda-19" href="../hunch_net-2011/hunch_net-2011-09-03-Fall_Machine_Learning_Events.html">443 hunch net-2011-09-03-Fall Machine Learning Events</a></p>
<p>20 0.41947713 <a title="144-lda-20" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
