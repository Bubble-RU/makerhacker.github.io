<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>140 hunch net-2005-12-14-More NIPS Papers II</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-140" href="#">hunch_net-2005-140</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>140 hunch net-2005-12-14-More NIPS Papers II</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-140-html" href="http://hunch.net/?p=150">html</a></p><p>Introduction: I thought this was a very good NIPS with many excellent papers. The following are a few NIPS papers which I liked and I hope to study more carefully when I get the chance. The list is not exhaustive and in no particular order…
  
 Preconditioner Approximations for Probabilistic Graphical Models. 
Pradeeep Ravikumar and John Lafferty. 
I thought the use of preconditioner methods from solving linear systems in the context of approximate inference was novel and interesting. The results look good and I’d like to understand the limitations.
 
 Rodeo: Sparse nonparametric regression in high dimensions. 
John Lafferty and Larry Wasserman. 
A very interesting approach to feature selection in nonparametric regression from a frequentist framework. The use of lengthscale variables in each dimension reminds me a lot of  ‘Automatic Relevance Determination’ in Gaussian process regression — it would be interesting to compare Rodeo to ARD in GPs.
 
 Interpolating between types and tokens by estimating</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 I thought this was a very good NIPS with many excellent papers. [sent-1, score-0.089]
</p><p>2 The following are a few NIPS papers which I liked and I hope to study more carefully when I get the chance. [sent-2, score-0.148]
</p><p>3 The list is not exhaustive and in no particular order…     Preconditioner Approximations for Probabilistic Graphical Models. [sent-3, score-0.107]
</p><p>4 I thought the use of preconditioner methods from solving linear systems in the context of approximate inference was novel and interesting. [sent-5, score-0.43]
</p><p>5 The results look good and I’d like to understand the limitations. [sent-6, score-0.073]
</p><p>6 Rodeo: Sparse nonparametric regression in high dimensions. [sent-7, score-0.335]
</p><p>7 A very interesting approach to feature selection in nonparametric regression from a frequentist framework. [sent-9, score-0.784]
</p><p>8 The use of lengthscale variables in each dimension reminds me a lot of  ‘Automatic Relevance Determination’ in Gaussian process regression — it would be interesting to compare Rodeo to ARD in GPs. [sent-10, score-0.616]
</p><p>9 Interpolating between types and tokens by estimating power law generators. [sent-11, score-0.281]
</p><p>10 I had wondered how Chinese restaurant processes and Pitman-Yor processes related to Zipf’s plots and power laws for word frequencies. [sent-16, score-0.794]
</p><p>11 When I first learned about spatial scan statistics I wondered what a Bayesian counterpart would be. [sent-23, score-0.699]
</p><p>12 I liked the fact they their method was simple, more accurate, and much  faster  than the usual frequentist method. [sent-24, score-0.52]
</p><p>13 A very interesting application of sub-modular function optimization to clustering. [sent-30, score-0.095]
</p><p>14 It’s useful for Gaussian process practitioners to know that their approaches don’t do silly things when viewed from a worst-case frequentist setting. [sent-37, score-0.573]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('frequentist', 0.282), ('preconditioner', 0.242), ('rodeo', 0.242), ('scan', 0.242), ('spatial', 0.242), ('gaussian', 0.217), ('wondered', 0.215), ('regression', 0.174), ('nonparametric', 0.161), ('liked', 0.148), ('processes', 0.129), ('power', 0.12), ('john', 0.113), ('matthias', 0.107), ('griffiths', 0.107), ('determination', 0.107), ('exhaustive', 0.107), ('gregory', 0.107), ('narasimhan', 0.107), ('plots', 0.107), ('ravikumar', 0.107), ('seeger', 0.107), ('reminds', 0.099), ('novel', 0.099), ('chinese', 0.099), ('lafferty', 0.099), ('feels', 0.099), ('practitioners', 0.099), ('silly', 0.099), ('interesting', 0.095), ('larry', 0.094), ('laws', 0.094), ('process', 0.093), ('usual', 0.09), ('relevance', 0.09), ('hot', 0.09), ('thought', 0.089), ('bayesian', 0.089), ('nips', 0.087), ('approximations', 0.086), ('dean', 0.086), ('estimating', 0.083), ('dimension', 0.083), ('kakade', 0.078), ('types', 0.078), ('accurate', 0.074), ('results', 0.073), ('variables', 0.072), ('selection', 0.072), ('sham', 0.072)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="140-tfidf-1" href="../hunch_net-2005/hunch_net-2005-12-14-More_NIPS_Papers_II.html">140 hunch net-2005-12-14-More NIPS Papers II</a></p>
<p>Introduction: I thought this was a very good NIPS with many excellent papers. The following are a few NIPS papers which I liked and I hope to study more carefully when I get the chance. The list is not exhaustive and in no particular order…
  
 Preconditioner Approximations for Probabilistic Graphical Models. 
Pradeeep Ravikumar and John Lafferty. 
I thought the use of preconditioner methods from solving linear systems in the context of approximate inference was novel and interesting. The results look good and I’d like to understand the limitations.
 
 Rodeo: Sparse nonparametric regression in high dimensions. 
John Lafferty and Larry Wasserman. 
A very interesting approach to feature selection in nonparametric regression from a frequentist framework. The use of lengthscale variables in each dimension reminds me a lot of  ‘Automatic Relevance Determination’ in Gaussian process regression — it would be interesting to compare Rodeo to ARD in GPs.
 
 Interpolating between types and tokens by estimating</p><p>2 0.16161557 <a title="140-tfidf-2" href="../hunch_net-2005/hunch_net-2005-01-26-Watchword%3A_Probability.html">5 hunch net-2005-01-26-Watchword: Probability</a></p>
<p>Introduction: Probability is one of the most confusingly used words in machine learning.  There are at least 3 distinct ways the word is used. 
  
  Bayesian  The Bayesian notion of probability is a ‘degree of belief’.   The degree of belief that some event (i.e. “stock goes up” or “stock goes down”) occurs can be measured by asking a sequence of questions of the form “Would you bet the stock goes up or down at  Y  to 1 odds?” A consistent better will switch from ‘for’ to ‘against’ at some single value of  Y .  The probability is then  Y/(Y+1) .  Bayesian probabilities express lack of knowledge rather than randomization.  They are useful in learning because we often lack knowledge and expressing that lack flexibly makes the learning algorithms work better.  Bayesian Learning uses ‘probability’ in this way exclusively. 
  Frequentist  The Frequentist notion of probability is a rate of occurence.  A rate of occurrence can be measured by doing an experiment many times.  If an event occurs  k  times in</p><p>3 0.11935037 <a title="140-tfidf-3" href="../hunch_net-2007/hunch_net-2007-09-18-It%26%238217%3Bs_MDL_Jim%2C_but_not_as_we_know_it%26%238230%3B%28on_Bayes%2C_MDL_and_consistency%29.html">263 hunch net-2007-09-18-It&#8217;s MDL Jim, but not as we know it&#8230;(on Bayes, MDL and consistency)</a></p>
<p>Introduction: I have recently completed  a 500+ page-book on MDL , the first comprehensive overview of the field (yes, this is a sneak advertisement    ). 
 Chapter 17  compares MDL to a menagerie of other methods and paradigms for learning and statistics. By far the most time (20 pages) is spent on the relation between MDL and Bayes. My two main points here are:
  
  In sharp contrast to Bayes, MDL is by definition based on designing universal codes for the data relative to some given (parametric or nonparametric) probabilistic model M. By some theorems due to  Andrew Barron , MDL inference  must  therefore be statistically consistent, and it is immune to Bayesian inconsistency results such as those by Diaconis, Freedman and Barron (I explain what I mean by “inconsistency” further below).  Hence, MDL must be different from Bayes! 
 In contrast to what has sometimes been claimed, practical MDL algorithms do have a subjective component (which in many, but not all cases, may be implemented by somethin</p><p>4 0.097882144 <a title="140-tfidf-4" href="../hunch_net-2009/hunch_net-2009-06-24-Interesting_papers_at_UAICMOLT_2009.html">361 hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</a></p>
<p>Introduction: Here’s a list of papers that I found interesting at  ICML / COLT / UAI  in 2009.
  
  Elad Hazan  and  Comandur Seshadhri   Efficient learning algorithms for changing environments  at ICML.  This paper shows how to adapt learning algorithms that compete with fixed predictors to compete with changing policies.  The definition of regret they deal with seems particularly useful in many situation. 
  Hal Daume ,  Unsupervised Search-based Structured Prediction  at ICML.  This paper shows a technique for reducing unsupervised learning to supervised learning which (a) make a fast unsupervised learning algorithm and (b)  makes semisupervised learning both easy and highly effective.  
 There were two papers with similar results on active learning in the KWIK framework for linear regression, both reducing the sample complexity to .  One was  Nicolo Cesa-Bianchi ,  Claudio Gentile , and  Francesco Orabona   Robust Bounds for Classification via Selective Sampling  at ICML and the other was  Thoma</p><p>5 0.093975388 <a title="140-tfidf-5" href="../hunch_net-2012/hunch_net-2012-06-05-ICML_acceptance_statistics.html">466 hunch net-2012-06-05-ICML acceptance statistics</a></p>
<p>Introduction: People are naturally interested in slicing the ICML acceptance statistics in various ways.  Here’s a rundown for the top categories.
  
 
 18/66 = 0.27 
 in (0.18,0.36) 
  Reinforcement Learning 
 
 
 10/52 = 0.19 
  in (0.17,0.37) 
  Supervised Learning 
 
 
  9/51 = 0.18 
   not in (0.18, 0.37)  
  Clustering 
 
 
  12/46 = 0.26 
  in (0.17, 0.37) 
  Kernel Methods 
 
 
  11/40 = 0.28 
  in (0.15, 0.4) 
  Optimization Algorithms 
 
 
  8/33 = 0.24 
  in (0.15, 0.39) 
  Learning Theory 
 
 
  14/33 = 0.42 
   not in (0.15, 0.39)  
  Graphical Models 
 
 
  10/32 = 0.31 
  in (0.15, 0.41) 
  Applications (+5 invited) 
 
 
  8/29 = 0.28 
  in (0.14, 0.41]) 
 Probabilistic Models 
 
 
  13/29 = 0.45 
  not in (0.14, 0.41)  
  NN & Deep Learning 
 
 
   8/26 = 0.31 
  in (0.12, 0.42) 
  Transfer and Multi-Task Learning 
 
 
  13/25 = 0.52 
  not in (0.12, 0.44)  
  Online Learning 
 
 
  5/25 = 0.20 
  in (0.12, 0.44) 
  Active Learning 
 
 
  6/22 = 0.27 
  in (0.14, 0.41) 
  Semi-Superv</p><p>6 0.0930392 <a title="140-tfidf-6" href="../hunch_net-2007/hunch_net-2007-12-20-Cool_and_Interesting_things_at_NIPS%2C_take_three.html">280 hunch net-2007-12-20-Cool and Interesting things at NIPS, take three</a></p>
<p>7 0.090480916 <a title="140-tfidf-7" href="../hunch_net-2006/hunch_net-2006-07-13-Regression_vs._Classification_as_a_Primitive.html">196 hunch net-2006-07-13-Regression vs. Classification as a Primitive</a></p>
<p>8 0.08808089 <a title="140-tfidf-8" href="../hunch_net-2005/hunch_net-2005-02-01-NIPS%3A_Online_Bayes.html">8 hunch net-2005-02-01-NIPS: Online Bayes</a></p>
<p>9 0.084947459 <a title="140-tfidf-9" href="../hunch_net-2008/hunch_net-2008-11-16-Observations_on_Linearity_for_Reductions_to_Regression.html">327 hunch net-2008-11-16-Observations on Linearity for Reductions to Regression</a></p>
<p>10 0.079750746 <a title="140-tfidf-10" href="../hunch_net-2012/hunch_net-2012-02-24-ICML%2B50%25.html">456 hunch net-2012-02-24-ICML+50%</a></p>
<p>11 0.077129096 <a title="140-tfidf-11" href="../hunch_net-2005/hunch_net-2005-04-25-Embeddings%3A_what_are_they_good_for%3F.html">61 hunch net-2005-04-25-Embeddings: what are they good for?</a></p>
<p>12 0.077034764 <a title="140-tfidf-12" href="../hunch_net-2005/hunch_net-2005-03-10-Breaking_Abstractions.html">39 hunch net-2005-03-10-Breaking Abstractions</a></p>
<p>13 0.076217405 <a title="140-tfidf-13" href="../hunch_net-2006/hunch_net-2006-07-05-more_icml_papers.html">189 hunch net-2006-07-05-more icml papers</a></p>
<p>14 0.068979204 <a title="140-tfidf-14" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>15 0.068964764 <a title="140-tfidf-15" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>16 0.068852365 <a title="140-tfidf-16" href="../hunch_net-2005/hunch_net-2005-12-28-Yet_more_nips_thoughts.html">144 hunch net-2005-12-28-Yet more nips thoughts</a></p>
<p>17 0.066465609 <a title="140-tfidf-17" href="../hunch_net-2005/hunch_net-2005-04-23-Advantages_and_Disadvantages_of_Bayesian_Learning.html">60 hunch net-2005-04-23-Advantages and Disadvantages of Bayesian Learning</a></p>
<p>18 0.066373736 <a title="140-tfidf-18" href="../hunch_net-2005/hunch_net-2005-05-29-Maximum_Margin_Mismatch%3F.html">77 hunch net-2005-05-29-Maximum Margin Mismatch?</a></p>
<p>19 0.065137744 <a title="140-tfidf-19" href="../hunch_net-2009/hunch_net-2009-01-21-Nearly_all_natural_problems_require_nonlinearity.html">337 hunch net-2009-01-21-Nearly all natural problems require nonlinearity</a></p>
<p>20 0.06333854 <a title="140-tfidf-20" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.141), (1, 0.032), (2, 0.013), (3, -0.028), (4, 0.061), (5, 0.041), (6, 0.012), (7, -0.008), (8, 0.079), (9, -0.059), (10, 0.023), (11, -0.055), (12, -0.083), (13, -0.07), (14, 0.023), (15, -0.031), (16, -0.096), (17, 0.063), (18, 0.086), (19, -0.107), (20, 0.017), (21, 0.027), (22, 0.0), (23, 0.025), (24, -0.011), (25, 0.015), (26, -0.022), (27, -0.021), (28, -0.047), (29, 0.03), (30, -0.043), (31, -0.01), (32, 0.017), (33, -0.03), (34, -0.013), (35, 0.065), (36, -0.013), (37, 0.029), (38, 0.026), (39, 0.068), (40, 0.04), (41, 0.036), (42, 0.071), (43, 0.07), (44, -0.052), (45, 0.073), (46, 0.015), (47, 0.076), (48, -0.087), (49, 0.052)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96984261 <a title="140-lsi-1" href="../hunch_net-2005/hunch_net-2005-12-14-More_NIPS_Papers_II.html">140 hunch net-2005-12-14-More NIPS Papers II</a></p>
<p>Introduction: I thought this was a very good NIPS with many excellent papers. The following are a few NIPS papers which I liked and I hope to study more carefully when I get the chance. The list is not exhaustive and in no particular order…
  
 Preconditioner Approximations for Probabilistic Graphical Models. 
Pradeeep Ravikumar and John Lafferty. 
I thought the use of preconditioner methods from solving linear systems in the context of approximate inference was novel and interesting. The results look good and I’d like to understand the limitations.
 
 Rodeo: Sparse nonparametric regression in high dimensions. 
John Lafferty and Larry Wasserman. 
A very interesting approach to feature selection in nonparametric regression from a frequentist framework. The use of lengthscale variables in each dimension reminds me a lot of  ‘Automatic Relevance Determination’ in Gaussian process regression — it would be interesting to compare Rodeo to ARD in GPs.
 
 Interpolating between types and tokens by estimating</p><p>2 0.63378155 <a title="140-lsi-2" href="../hunch_net-2005/hunch_net-2005-05-29-Maximum_Margin_Mismatch%3F.html">77 hunch net-2005-05-29-Maximum Margin Mismatch?</a></p>
<p>Introduction: John makes a fascinating point about structured classification (and slightly scooped my post!). Maximum Margin Markov Networks (M3N) are an interesting example of the second class of structured classifiers (where the classification of one label depends on the others), and one of my favorite papers. I’m not alone: the paper won the best student paper award at NIPS in 2003. 
 
There are some things I find odd about the paper. For instance, it says of probabilistic models 
  
“cannot handle high dimensional feature spaces and lack strong theoretical guarrantees.” 
  
I’m aware of no such limitations. Also: 
  

“Unfortunately, even probabilistic graphical models that are trained discriminatively do not achieve the same level of performance as SVMs, especially when kernel features are used.”

  
This is quite interesting and contradicts my own experience as well as that of a number of people  I   greatly  
 respect . I wonder what the root cause is: perhaps there is something different abo</p><p>3 0.62690318 <a title="140-lsi-3" href="../hunch_net-2005/hunch_net-2005-12-11-More_NIPS_Papers.html">139 hunch net-2005-12-11-More NIPS Papers</a></p>
<p>Introduction: Let me add to John’s post with a few of my own favourites 
from this year’s conference. First, let me say that 
Sanjoy’s talk,  Coarse Sample Complexity Bounds for Active 
Learning  was also one of my favourites, as was the 
  
Forgettron paper .
 

I also really enjoyed the last third of 
 Christos’  talk 
on the complexity of finding Nash equilibria.

 

And, speaking of tagging, I think 
the U.Mass Citeseer replacement system 
 Rexa  from the demo track is very cool.

 

Finally, let me add my recommendations for specific papers:
  
  Z. Ghahramani, K. Heller:  Bayesian Sets  
[no preprint] 
(A very elegant probabilistic information retrieval style model 
of which objects are “most like” a given subset of objects.)
 
 T. Griffiths, Z. Ghahramani:  Infinite Latent Feature Models and 
the Indian Buffet Process  
[  
preprint ] 
(A Dirichlet style prior over infinite binary matrices with 
beautiful exchangeability properties.)
 
 K. Weinberger, J. Blitzer, L. Saul:  Distance Metric Lea</p><p>4 0.62361908 <a title="140-lsi-4" href="../hunch_net-2007/hunch_net-2007-12-20-Cool_and_Interesting_things_at_NIPS%2C_take_three.html">280 hunch net-2007-12-20-Cool and Interesting things at NIPS, take three</a></p>
<p>Introduction: Following up on Hal Daume’s post and John’s post on cool and interesting things seen at NIPS I’ll post my own little list of neat papers here as well.  Of course it’s going to be biased towards what I think is interesting.  Also, I have to say that I hadn’t been able to see many papers this year at nips due to myself being too busy, so please feel free to contribute the papers that you liked   
 
1. P. Mudigonda, V. Kolmogorov, P. Torr.  An Analysis of Convex Relaxations for MAP Estimation.  A surprising paper which shows that many of the more sophisticated convex relaxations that had been proposed recently turns out to be subsumed by the simplest LP relaxation.  Be careful next time you try a cool new convex relaxation!
 
2. D. Sontag, T. Jaakkola.  New Outer Bounds on the Marginal Polytope.  The title says it all.  The marginal polytope is the set of local marginal distributions over subsets of variables that are globally consistent in the sense that there is at least one distributio</p><p>5 0.59573233 <a title="140-lsi-5" href="../hunch_net-2005/hunch_net-2005-01-26-Watchword%3A_Probability.html">5 hunch net-2005-01-26-Watchword: Probability</a></p>
<p>Introduction: Probability is one of the most confusingly used words in machine learning.  There are at least 3 distinct ways the word is used. 
  
  Bayesian  The Bayesian notion of probability is a ‘degree of belief’.   The degree of belief that some event (i.e. “stock goes up” or “stock goes down”) occurs can be measured by asking a sequence of questions of the form “Would you bet the stock goes up or down at  Y  to 1 odds?” A consistent better will switch from ‘for’ to ‘against’ at some single value of  Y .  The probability is then  Y/(Y+1) .  Bayesian probabilities express lack of knowledge rather than randomization.  They are useful in learning because we often lack knowledge and expressing that lack flexibly makes the learning algorithms work better.  Bayesian Learning uses ‘probability’ in this way exclusively. 
  Frequentist  The Frequentist notion of probability is a rate of occurence.  A rate of occurrence can be measured by doing an experiment many times.  If an event occurs  k  times in</p><p>6 0.59011543 <a title="140-lsi-6" href="../hunch_net-2007/hunch_net-2007-09-18-It%26%238217%3Bs_MDL_Jim%2C_but_not_as_we_know_it%26%238230%3B%28on_Bayes%2C_MDL_and_consistency%29.html">263 hunch net-2007-09-18-It&#8217;s MDL Jim, but not as we know it&#8230;(on Bayes, MDL and consistency)</a></p>
<p>7 0.58454841 <a title="140-lsi-7" href="../hunch_net-2006/hunch_net-2006-07-05-more_icml_papers.html">189 hunch net-2006-07-05-more icml papers</a></p>
<p>8 0.56786036 <a title="140-lsi-8" href="../hunch_net-2005/hunch_net-2005-12-28-Yet_more_nips_thoughts.html">144 hunch net-2005-12-28-Yet more nips thoughts</a></p>
<p>9 0.55843371 <a title="140-lsi-9" href="../hunch_net-2008/hunch_net-2008-12-07-A_NIPS_paper.html">330 hunch net-2008-12-07-A NIPS paper</a></p>
<p>10 0.54097152 <a title="140-lsi-10" href="../hunch_net-2007/hunch_net-2007-06-19-How_is_Compressed_Sensing_going_to_change_Machine_Learning_%3F.html">248 hunch net-2007-06-19-How is Compressed Sensing going to change Machine Learning ?</a></p>
<p>11 0.52248579 <a title="140-lsi-11" href="../hunch_net-2005/hunch_net-2005-10-16-Complexity%3A_It%26%238217%3Bs_all_in_your_head.html">123 hunch net-2005-10-16-Complexity: It&#8217;s all in your head</a></p>
<p>12 0.47700387 <a title="140-lsi-12" href="../hunch_net-2005/hunch_net-2005-04-25-Embeddings%3A_what_are_they_good_for%3F.html">61 hunch net-2005-04-25-Embeddings: what are they good for?</a></p>
<p>13 0.47574511 <a title="140-lsi-13" href="../hunch_net-2006/hunch_net-2006-01-23-On_Coding_via_Mutual_Information_%26%23038%3B_Bayes_Nets.html">150 hunch net-2006-01-23-On Coding via Mutual Information &#038; Bayes Nets</a></p>
<p>14 0.47387481 <a title="140-lsi-14" href="../hunch_net-2005/hunch_net-2005-07-23-Interesting_papers_at_ACL.html">97 hunch net-2005-07-23-Interesting papers at ACL</a></p>
<p>15 0.46526557 <a title="140-lsi-15" href="../hunch_net-2011/hunch_net-2011-08-06-Interesting_thing_at_UAI_2011.html">440 hunch net-2011-08-06-Interesting thing at UAI 2011</a></p>
<p>16 0.46355492 <a title="140-lsi-16" href="../hunch_net-2006/hunch_net-2006-06-16-Regularization_%3D_Robustness.html">185 hunch net-2006-06-16-Regularization = Robustness</a></p>
<p>17 0.44771105 <a title="140-lsi-17" href="../hunch_net-2009/hunch_net-2009-06-24-Interesting_papers_at_UAICMOLT_2009.html">361 hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</a></p>
<p>18 0.44394511 <a title="140-lsi-18" href="../hunch_net-2005/hunch_net-2005-10-07-On-line_learning_of_regular_decision_rules.html">118 hunch net-2005-10-07-On-line learning of regular decision rules</a></p>
<p>19 0.43553829 <a title="140-lsi-19" href="../hunch_net-2005/hunch_net-2005-04-26-To_calibrate_or_not%3F.html">62 hunch net-2005-04-26-To calibrate or not?</a></p>
<p>20 0.42734134 <a title="140-lsi-20" href="../hunch_net-2012/hunch_net-2012-06-05-ICML_acceptance_statistics.html">466 hunch net-2012-06-05-ICML acceptance statistics</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.079), (3, 0.029), (7, 0.239), (10, 0.029), (24, 0.018), (27, 0.157), (30, 0.052), (33, 0.026), (44, 0.037), (48, 0.019), (53, 0.077), (55, 0.079), (84, 0.011), (94, 0.036), (95, 0.013)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.90118295 <a title="140-lda-1" href="../hunch_net-2005/hunch_net-2005-12-14-More_NIPS_Papers_II.html">140 hunch net-2005-12-14-More NIPS Papers II</a></p>
<p>Introduction: I thought this was a very good NIPS with many excellent papers. The following are a few NIPS papers which I liked and I hope to study more carefully when I get the chance. The list is not exhaustive and in no particular order…
  
 Preconditioner Approximations for Probabilistic Graphical Models. 
Pradeeep Ravikumar and John Lafferty. 
I thought the use of preconditioner methods from solving linear systems in the context of approximate inference was novel and interesting. The results look good and I’d like to understand the limitations.
 
 Rodeo: Sparse nonparametric regression in high dimensions. 
John Lafferty and Larry Wasserman. 
A very interesting approach to feature selection in nonparametric regression from a frequentist framework. The use of lengthscale variables in each dimension reminds me a lot of  ‘Automatic Relevance Determination’ in Gaussian process regression — it would be interesting to compare Rodeo to ARD in GPs.
 
 Interpolating between types and tokens by estimating</p><p>2 0.86343551 <a title="140-lda-2" href="../hunch_net-2009/hunch_net-2009-05-19-CI_Fellows.html">355 hunch net-2009-05-19-CI Fellows</a></p>
<p>Introduction: Lev Reyzin  points out the  CI Fellows Project .  Essentially,  NSF  is funding 60 postdocs in computer science for graduates from a wide array of US places to a wide array of US places.  This is particularly welcome given a tough year for new hires.  I expect some fraction of these postdocs will be in ML.  The time frame is quite short, so those interested should look it over immediately.</p><p>3 0.78652471 <a title="140-lda-3" href="../hunch_net-2006/hunch_net-2006-06-15-IJCAI_is_out_of_season.html">184 hunch net-2006-06-15-IJCAI is out of season</a></p>
<p>Introduction: IJCAI  is running January 6-12 in Hyderabad India rather than a more traditional summer date. (Presumably, this is to avoid melting people in the Indian summer.)
 
The paper deadline(June 23 abstract / June 30 submission) are particularly inconvenient if you attend  COLT  or  ICML .  But on the other hand, itâ&euro;&trade;s a good excuse to visit India.</p><p>4 0.74257696 <a title="140-lda-4" href="../hunch_net-2006/hunch_net-2006-09-19-Luis_von_Ahn_is_awarded_a_MacArthur_fellowship..html">209 hunch net-2006-09-19-Luis von Ahn is awarded a MacArthur fellowship.</a></p>
<p>Introduction: For  his  work on the subject of human computation including  ESPGame ,  Peekaboom , and  Phetch .  The  new MacArthur fellows .</p><p>5 0.62521416 <a title="140-lda-5" href="../hunch_net-2005/hunch_net-2005-03-10-Breaking_Abstractions.html">39 hunch net-2005-03-10-Breaking Abstractions</a></p>
<p>Introduction: Sam Roweis ‘s comment reminds me of a more general issue that comes up in doing research: abstractions always break.  
  
 Real number’s aren’t.  Most real numbers can not be represented with any machine.  One implication of this is that many real-number based algorithms have difficulties when implemented with floating point numbers. 
 The box on your desk is not a turing machine. A turing machine can compute anything computable, given sufficient time.  A typical computer fails terribly when the state required for the computation exceeds some limit. 
 Nash equilibria aren’t equilibria.  This comes up when trying to predict human behavior based on the result of the equilibria computation.  Often, it doesn’t work. 
 The  probability  isn’t.  Probability is an abstraction expressing either our lack of knowledge (the Bayesian viewpoint) or fundamental randomization (the frequentist viewpoint).  From the frequentist viewpoint the lack of knowledge typically precludes actually knowing the fu</p><p>6 0.62437415 <a title="140-lda-6" href="../hunch_net-2008/hunch_net-2008-08-24-Mass_Customized_Medicine_in_the_Future%3F.html">314 hunch net-2008-08-24-Mass Customized Medicine in the Future?</a></p>
<p>7 0.60435683 <a title="140-lda-7" href="../hunch_net-2005/hunch_net-2005-05-29-Bad_ideas.html">76 hunch net-2005-05-29-Bad ideas</a></p>
<p>8 0.59053111 <a title="140-lda-8" href="../hunch_net-2007/hunch_net-2007-01-02-Retrospective.html">225 hunch net-2007-01-02-Retrospective</a></p>
<p>9 0.58374208 <a title="140-lda-9" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>10 0.5834291 <a title="140-lda-10" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>11 0.57819885 <a title="140-lda-11" href="../hunch_net-2006/hunch_net-2006-09-12-Incentive_Compatible_Reviewing.html">207 hunch net-2006-09-12-Incentive Compatible Reviewing</a></p>
<p>12 0.57811373 <a title="140-lda-12" href="../hunch_net-2005/hunch_net-2005-07-21-Six_Months.html">96 hunch net-2005-07-21-Six Months</a></p>
<p>13 0.57782745 <a title="140-lda-13" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>14 0.5764659 <a title="140-lda-14" href="../hunch_net-2005/hunch_net-2005-01-26-Watchword%3A_Probability.html">5 hunch net-2005-01-26-Watchword: Probability</a></p>
<p>15 0.57625771 <a title="140-lda-15" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>16 0.57610804 <a title="140-lda-16" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>17 0.57471669 <a title="140-lda-17" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>18 0.57416809 <a title="140-lda-18" href="../hunch_net-2013/hunch_net-2013-01-07-NYU_Large_Scale_Machine_Learning_Class.html">478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</a></p>
<p>19 0.5732525 <a title="140-lda-19" href="../hunch_net-2005/hunch_net-2005-03-13-Avoiding_Bad_Reviewing.html">40 hunch net-2005-03-13-Avoiding Bad Reviewing</a></p>
<p>20 0.57301807 <a title="140-lda-20" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
