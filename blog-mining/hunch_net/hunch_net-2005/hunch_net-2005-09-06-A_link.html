<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>108 hunch net-2005-09-06-A link</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-108" href="#">hunch_net-2005-108</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>108 hunch net-2005-09-06-A link</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-108-html" href="http://hunch.net/?p=116">html</a></p><p>Introduction: I read through some of the essays ofMichael Nielsentoday, and recommend
them.Principles of Effective ResearchandExtreme Thinkingare both relevant to
several discussions here.</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('essays', 0.615), ('recommend', 0.436), ('discussions', 0.406), ('read', 0.313), ('effective', 0.281), ('relevant', 0.273), ('several', 0.123)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="108-tfidf-1" href="../hunch_net-2005/hunch_net-2005-09-06-A_link.html">108 hunch net-2005-09-06-A link</a></p>
<p>Introduction: I read through some of the essays ofMichael Nielsentoday, and recommend
them.Principles of Effective ResearchandExtreme Thinkingare both relevant to
several discussions here.</p><p>2 0.15317656 <a title="108-tfidf-2" href="../hunch_net-2006/hunch_net-2006-11-02-2006_NIPS_workshops.html">216 hunch net-2006-11-02-2006 NIPS workshops</a></p>
<p>Introduction: I expect theNIPS 2006 workshopsto be quite interesting, and recommend going
for anyone interested in machine learning research. (Most or all of the
workshops webpages can be found two links deep.)</p><p>3 0.10775065 <a title="108-tfidf-3" href="../hunch_net-2005/hunch_net-2005-05-14-NIPS.html">71 hunch net-2005-05-14-NIPS</a></p>
<p>Introduction: NIPSis the big winter conference of learning.Paper due date: June 3rd.
(Tweaked thanks toFei Sha.)Location: Vancouver (main program) Dec. 5-8 and
Whistler (workshops) Dec 9-10, BC, CanadaNIPS is larger than all of the other
learning conferences, partly because it's the only one at that time of year. I
recommend the workshops which are often quite interesting and energetic.</p><p>4 0.074564643 <a title="108-tfidf-4" href="../hunch_net-2010/hunch_net-2010-04-28-CI_Fellows_program_renewed.html">396 hunch net-2010-04-28-CI Fellows program renewed</a></p>
<p>Introduction: Lev Reyzinpoints out theCI Fellows program is renewed. CI Fellows are
essentiallyNSFfunded computer science postdocs for universities and industry
research labs. I've been lucky and happy to have Lev visit me for a year
underlast year's program, so I strongly recommend participating if it suits
you.As with last year, the application timeline is very short, with everything
due by May 23.</p><p>5 0.073270686 <a title="108-tfidf-5" href="../hunch_net-2005/hunch_net-2005-09-05-Site_Update.html">107 hunch net-2005-09-05-Site Update</a></p>
<p>Introduction: I tweaked the site in a number of ways today, including:Updating
toWordPress1.5.Installing and heavily tweaking theGeeknichetheme. Update: I
switched back to a tweaked version of the old theme.Adding theCustomizable
Post Listingsplugin.Installing theStatTraqplugin.Updating some of the links. I
particularly recommend looking at thecomputer research
policyblog.Addingthreaded comments. This doesn't thread old comments
obviously, but the extra structure may be helpful for new ones.Overall, I
think this is an improvement, and it addresses a few of myearlier problems. If
you have any difficulties or anything seems "not quite right", please speak
up. A few other tweaks to the site may happen in the near future.</p><p>6 0.073206119 <a title="108-tfidf-6" href="../hunch_net-2005/hunch_net-2005-05-29-Bad_ideas.html">76 hunch net-2005-05-29-Bad ideas</a></p>
<p>7 0.066188484 <a title="108-tfidf-7" href="../hunch_net-2010/hunch_net-2010-09-28-Machined_Learnings.html">412 hunch net-2010-09-28-Machined Learnings</a></p>
<p>8 0.063368179 <a title="108-tfidf-8" href="../hunch_net-2007/hunch_net-2007-10-14-NIPS_workshp%3A_Learning_Problem_Design.html">265 hunch net-2007-10-14-NIPS workshp: Learning Problem Design</a></p>
<p>9 0.063237548 <a title="108-tfidf-9" href="../hunch_net-2005/hunch_net-2005-02-25-Why_Papers%3F.html">30 hunch net-2005-02-25-Why Papers?</a></p>
<p>10 0.059214838 <a title="108-tfidf-10" href="../hunch_net-2007/hunch_net-2007-06-23-Machine_Learning_Jobs_are_Growing_on_Trees.html">250 hunch net-2007-06-23-Machine Learning Jobs are Growing on Trees</a></p>
<p>11 0.058381449 <a title="108-tfidf-11" href="../hunch_net-2011/hunch_net-2011-02-25-Yahoo%21_Machine_Learning_grant_due_March_11.html">425 hunch net-2011-02-25-Yahoo! Machine Learning grant due March 11</a></p>
<p>12 0.05362853 <a title="108-tfidf-12" href="../hunch_net-2007/hunch_net-2007-07-13-The_View_From_China.html">255 hunch net-2007-07-13-The View From China</a></p>
<p>13 0.04514268 <a title="108-tfidf-13" href="../hunch_net-2010/hunch_net-2010-08-21-Rob_Schapire_at_NYC_ML_Meetup.html">405 hunch net-2010-08-21-Rob Schapire at NYC ML Meetup</a></p>
<p>14 0.044924721 <a title="108-tfidf-14" href="../hunch_net-2005/hunch_net-2005-02-20-At_One_Month.html">25 hunch net-2005-02-20-At One Month</a></p>
<p>15 0.044463433 <a title="108-tfidf-15" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>16 0.044293478 <a title="108-tfidf-16" href="../hunch_net-2006/hunch_net-2006-03-05-%26%238220%3BStructural%26%238221%3B_Learning.html">161 hunch net-2006-03-05-&#8220;Structural&#8221; Learning</a></p>
<p>17 0.04346865 <a title="108-tfidf-17" href="../hunch_net-2005/hunch_net-2005-06-10-Workshops_are_not_Conferences.html">80 hunch net-2005-06-10-Workshops are not Conferences</a></p>
<p>18 0.042228557 <a title="108-tfidf-18" href="../hunch_net-2006/hunch_net-2006-07-11-New_Models.html">194 hunch net-2006-07-11-New Models</a></p>
<p>19 0.040063195 <a title="108-tfidf-19" href="../hunch_net-2007/hunch_net-2007-01-15-The_Machine_Learning_Department.html">228 hunch net-2007-01-15-The Machine Learning Department</a></p>
<p>20 0.039165121 <a title="108-tfidf-20" href="../hunch_net-2013/hunch_net-2013-12-01-NIPS_tutorials_and_Vowpal_Wabbit_7.4.html">492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.039), (1, 0.023), (2, 0.026), (3, 0.011), (4, 0.007), (5, -0.003), (6, 0.01), (7, -0.024), (8, -0.032), (9, 0.016), (10, 0.007), (11, 0.019), (12, -0.007), (13, -0.007), (14, 0.064), (15, -0.002), (16, 0.043), (17, -0.05), (18, -0.015), (19, 0.067), (20, -0.015), (21, 0.035), (22, -0.015), (23, -0.052), (24, 0.029), (25, 0.046), (26, -0.015), (27, 0.073), (28, -0.043), (29, -0.092), (30, 0.015), (31, 0.027), (32, -0.055), (33, -0.047), (34, -0.036), (35, -0.038), (36, -0.018), (37, -0.004), (38, 0.043), (39, -0.025), (40, -0.078), (41, 0.085), (42, 0.04), (43, 0.038), (44, -0.041), (45, 0.016), (46, -0.064), (47, -0.014), (48, -0.068), (49, -0.076)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98772216 <a title="108-lsi-1" href="../hunch_net-2005/hunch_net-2005-09-06-A_link.html">108 hunch net-2005-09-06-A link</a></p>
<p>Introduction: I read through some of the essays ofMichael Nielsentoday, and recommend
them.Principles of Effective ResearchandExtreme Thinkingare both relevant to
several discussions here.</p><p>2 0.59653908 <a title="108-lsi-2" href="../hunch_net-2006/hunch_net-2006-11-02-2006_NIPS_workshops.html">216 hunch net-2006-11-02-2006 NIPS workshops</a></p>
<p>Introduction: I expect theNIPS 2006 workshopsto be quite interesting, and recommend going
for anyone interested in machine learning research. (Most or all of the
workshops webpages can be found two links deep.)</p><p>3 0.43487671 <a title="108-lsi-3" href="../hunch_net-2005/hunch_net-2005-05-14-NIPS.html">71 hunch net-2005-05-14-NIPS</a></p>
<p>Introduction: NIPSis the big winter conference of learning.Paper due date: June 3rd.
(Tweaked thanks toFei Sha.)Location: Vancouver (main program) Dec. 5-8 and
Whistler (workshops) Dec 9-10, BC, CanadaNIPS is larger than all of the other
learning conferences, partly because it's the only one at that time of year. I
recommend the workshops which are often quite interesting and energetic.</p><p>4 0.36448875 <a title="108-lsi-4" href="../hunch_net-2009/hunch_net-2009-05-19-CI_Fellows.html">355 hunch net-2009-05-19-CI Fellows</a></p>
<p>Introduction: Lev Reyzinpoints out theCI Fellows Project. Essentially,NSFis funding 60
postdocs in computer science for graduates from a wide array of US places to a
wide array of US places. This is particularly welcome given a tough year for
new hires. I expect some fraction of these postdocs will be in ML. The time
frame is quite short, so those interested should look it over immediately.</p><p>5 0.35871717 <a title="108-lsi-5" href="../hunch_net-2005/hunch_net-2005-09-05-Site_Update.html">107 hunch net-2005-09-05-Site Update</a></p>
<p>Introduction: I tweaked the site in a number of ways today, including:Updating
toWordPress1.5.Installing and heavily tweaking theGeeknichetheme. Update: I
switched back to a tweaked version of the old theme.Adding theCustomizable
Post Listingsplugin.Installing theStatTraqplugin.Updating some of the links. I
particularly recommend looking at thecomputer research
policyblog.Addingthreaded comments. This doesn't thread old comments
obviously, but the extra structure may be helpful for new ones.Overall, I
think this is an improvement, and it addresses a few of myearlier problems. If
you have any difficulties or anything seems "not quite right", please speak
up. A few other tweaks to the site may happen in the near future.</p><p>6 0.35177913 <a title="108-lsi-6" href="../hunch_net-2005/hunch_net-2005-02-25-Why_Papers%3F.html">30 hunch net-2005-02-25-Why Papers?</a></p>
<p>7 0.34704769 <a title="108-lsi-7" href="../hunch_net-2007/hunch_net-2007-10-15-NIPS_workshops_extended_to_3_days.html">266 hunch net-2007-10-15-NIPS workshops extended to 3 days</a></p>
<p>8 0.33268252 <a title="108-lsi-8" href="../hunch_net-2010/hunch_net-2010-04-28-CI_Fellows_program_renewed.html">396 hunch net-2010-04-28-CI Fellows program renewed</a></p>
<p>9 0.33164382 <a title="108-lsi-9" href="../hunch_net-2007/hunch_net-2007-09-30-NIPS_workshops_are_out..html">264 hunch net-2007-09-30-NIPS workshops are out.</a></p>
<p>10 0.33084518 <a title="108-lsi-10" href="../hunch_net-2009/hunch_net-2009-01-28-Nielsen%26%238217%3Bs_talk.html">340 hunch net-2009-01-28-Nielsen&#8217;s talk</a></p>
<p>11 0.3239406 <a title="108-lsi-11" href="../hunch_net-2008/hunch_net-2008-11-10-ICML_Reviewing_Criteria.html">325 hunch net-2008-11-10-ICML Reviewing Criteria</a></p>
<p>12 0.32225305 <a title="108-lsi-12" href="../hunch_net-2010/hunch_net-2010-04-14-MLcomp%3A_a_website_for_objectively_comparing_ML_algorithms.html">393 hunch net-2010-04-14-MLcomp: a website for objectively comparing ML algorithms</a></p>
<p>13 0.29027256 <a title="108-lsi-13" href="../hunch_net-2011/hunch_net-2011-05-09-CI_Fellows%2C_again.html">434 hunch net-2011-05-09-CI Fellows, again</a></p>
<p>14 0.29017645 <a title="108-lsi-14" href="../hunch_net-2010/hunch_net-2010-05-20-Google_Predict.html">399 hunch net-2010-05-20-Google Predict</a></p>
<p>15 0.28701934 <a title="108-lsi-15" href="../hunch_net-2009/hunch_net-2009-06-03-Functionally_defined_Nonlinear_Dynamic_Models.html">359 hunch net-2009-06-03-Functionally defined Nonlinear Dynamic Models</a></p>
<p>16 0.2856738 <a title="108-lsi-16" href="../hunch_net-2009/hunch_net-2009-11-23-ICML_2009_Workshops_%28and_Tutorials%29.html">379 hunch net-2009-11-23-ICML 2009 Workshops (and Tutorials)</a></p>
<p>17 0.28532583 <a title="108-lsi-17" href="../hunch_net-2007/hunch_net-2007-04-21-Videolectures.net.html">240 hunch net-2007-04-21-Videolectures.net</a></p>
<p>18 0.28401673 <a title="108-lsi-18" href="../hunch_net-2005/hunch_net-2005-01-19-Why_I_decided_to_run_a_weblog..html">1 hunch net-2005-01-19-Why I decided to run a weblog.</a></p>
<p>19 0.27883768 <a title="108-lsi-19" href="../hunch_net-2009/hunch_net-2009-07-11-Interesting_papers_at_KDD.html">364 hunch net-2009-07-11-Interesting papers at KDD</a></p>
<p>20 0.27717432 <a title="108-lsi-20" href="../hunch_net-2006/hunch_net-2006-01-08-Debugging_Your_Brain.html">147 hunch net-2006-01-08-Debugging Your Brain</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(24, 0.713)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95682311 <a title="108-lda-1" href="../hunch_net-2005/hunch_net-2005-09-06-A_link.html">108 hunch net-2005-09-06-A link</a></p>
<p>Introduction: I read through some of the essays ofMichael Nielsentoday, and recommend
them.Principles of Effective ResearchandExtreme Thinkingare both relevant to
several discussions here.</p><p>2 0.4060728 <a title="108-lda-2" href="../hunch_net-2007/hunch_net-2007-02-10-Best_Practices_for_Collaboration.html">231 hunch net-2007-02-10-Best Practices for Collaboration</a></p>
<p>Introduction: Many people, especially students, haven't had an opportunity to collaborate
with other researchers. Collaboration, especially with remote people can be
tricky. Here are some observations of what has worked for me on collaborations
involving a few people.Travel and DiscussAlmost all collaborations start with
in-person discussion. This implies that travel is often necessary. We can hope
that in the future we'll have better systems for starting collaborations
remotely (such as blogs), but we aren't quite there yet.Enable your
collaborator. A collaboration can fall apart because one collaborator disables
another. This sounds stupid (and it is), but it's far easier than you might
think.Avoid Duplication. Discovering that you and a collaborator have been
editing the same thing and now need to waste time reconciling changes is
annoying. The best way to avoid this to be explicit about who has write
permission to what. Most of the time, a write lock is held for the entire
document, just to be s</p><p>3 0.10480542 <a title="108-lda-3" href="../hunch_net-2005/hunch_net-2005-06-08-Question%3A_%26%238220%3BWhen_is_the_right_time_to_insert_the_loss_function%3F%26%238221%3B.html">79 hunch net-2005-06-08-Question: &#8220;When is the right time to insert the loss function?&#8221;</a></p>
<p>Introduction: Halasksa very good question: "When is the right time to insert the loss
function?" In particular, should it be used at testing time or at training
time?When the world imposes a loss on us, the standard Bayesian recipe is to
predict the (conditional) probability of each possibility and then choose the
possibility which minimizes the expected loss. In contrast, as
theconfusionover "loss = money lost" or "loss = the thing you optimize" might
indicate, many people ignore the Bayesian approach and simply optimize their
loss (or a close proxy for their loss) over the representation on the training
set.The best answer I can give is "it's unclear, but I prefer optimizing the
loss at training time". My experience is that optimizing the loss in the most
direct manner possible typically yields best performance. This question is
related to a basic principle which bothYann LeCun(applied) andVladimir
Vapnik(theoretical) advocate: "solve the simplest prediction problem that
solves the problem". (One</p><p>4 0.0 <a title="108-lda-4" href="../hunch_net-2005/hunch_net-2005-01-19-Why_I_decided_to_run_a_weblog..html">1 hunch net-2005-01-19-Why I decided to run a weblog.</a></p>
<p>Introduction: I have decided to run a weblog on machine learning and learning theory
research. Here are some reasons:1) Weblogs enable new functionality:Public
comment on papers. No mechanism for this exists at conferences and most
journals. I have encountered it once for asciencepaper. Some communities have
mailing lists supporting this, but not machine learning or learning theory. I
have often read papers and found myself wishing there was some method to
consider other's questions and read the replies.Conference shortlists. One of
the most common conversations at a conference is "what did you find
interesting?" There is no explicit mechanism for sharing this information at
conferences, and it's easy to imagine that it would be handy to do
so.Evaluation and comment on research directions. Papers are almost
exclusively about new research, rather than evaluation (and consideration) of
research directions. This last role is satisfied by funding agencies to some
extent, but that is a private debate of</p><p>5 0.0 <a title="108-lda-5" href="../hunch_net-2005/hunch_net-2005-01-24-Holy_grails_of_machine_learning%3F.html">2 hunch net-2005-01-24-Holy grails of machine learning?</a></p>
<p>Introduction: Let me kick things off by posing this question to ML researchers:What do you
think are some important holy grails of machine learning?For example:- "A
classifier with SVM-level performance but much more scalable"- "Practical
confidence bounds (or learning bounds) for classification"- "A reinforcement
learning algorithm that can handle the ___ problem"- "Understanding
theoretically why ___ works so well in practice"etc.I pose this question
because I believe that when goals are stated explicitly and well (thus
providing clarity as well as opening up the problems to more people), rather
than left implicit, they are likely to be achieved much more quickly. I would
also like to know more about the internal goals of the various machine
learning sub-areas (theory, kernel methods, graphical models, reinforcement
learning, etc) as stated by people in these respective areas. This could help
people cross sub-areas.</p><p>6 0.0 <a title="108-lda-6" href="../hunch_net-2005/hunch_net-2005-01-24-The_Humanloop_Spectrum_of_Machine_Learning.html">3 hunch net-2005-01-24-The Humanloop Spectrum of Machine Learning</a></p>
<p>7 0.0 <a title="108-lda-7" href="../hunch_net-2005/hunch_net-2005-01-26-Summer_Schools.html">4 hunch net-2005-01-26-Summer Schools</a></p>
<p>8 0.0 <a title="108-lda-8" href="../hunch_net-2005/hunch_net-2005-01-26-Watchword%3A_Probability.html">5 hunch net-2005-01-26-Watchword: Probability</a></p>
<p>9 0.0 <a title="108-lda-9" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>10 0.0 <a title="108-lda-10" href="../hunch_net-2005/hunch_net-2005-01-31-Watchword%3A_Assumption.html">7 hunch net-2005-01-31-Watchword: Assumption</a></p>
<p>11 0.0 <a title="108-lda-11" href="../hunch_net-2005/hunch_net-2005-02-01-NIPS%3A_Online_Bayes.html">8 hunch net-2005-02-01-NIPS: Online Bayes</a></p>
<p>12 0.0 <a title="108-lda-12" href="../hunch_net-2005/hunch_net-2005-02-01-Watchword%3A_Loss.html">9 hunch net-2005-02-01-Watchword: Loss</a></p>
<p>13 0.0 <a title="108-lda-13" href="../hunch_net-2005/hunch_net-2005-02-02-Kolmogorov_Complexity_and_Googling.html">10 hunch net-2005-02-02-Kolmogorov Complexity and Googling</a></p>
<p>14 0.0 <a title="108-lda-14" href="../hunch_net-2005/hunch_net-2005-02-02-Paper_Deadlines.html">11 hunch net-2005-02-02-Paper Deadlines</a></p>
<p>15 0.0 <a title="108-lda-15" href="../hunch_net-2005/hunch_net-2005-02-03-Learning_Theory%2C_by_assumption.html">12 hunch net-2005-02-03-Learning Theory, by assumption</a></p>
<p>16 0.0 <a title="108-lda-16" href="../hunch_net-2005/hunch_net-2005-02-04-JMLG.html">13 hunch net-2005-02-04-JMLG</a></p>
<p>17 0.0 <a title="108-lda-17" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>18 0.0 <a title="108-lda-18" href="../hunch_net-2005/hunch_net-2005-02-08-Some_Links.html">15 hunch net-2005-02-08-Some Links</a></p>
<p>19 0.0 <a title="108-lda-19" href="../hunch_net-2005/hunch_net-2005-02-09-Intuitions_from_applied_learning.html">16 hunch net-2005-02-09-Intuitions from applied learning</a></p>
<p>20 0.0 <a title="108-lda-20" href="../hunch_net-2005/hunch_net-2005-02-10-Conferences%2C_Dates%2C_Locations.html">17 hunch net-2005-02-10-Conferences, Dates, Locations</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
