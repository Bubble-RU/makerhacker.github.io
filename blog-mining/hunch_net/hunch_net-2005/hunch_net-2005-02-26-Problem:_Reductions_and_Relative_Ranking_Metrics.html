<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>31 hunch net-2005-02-26-Problem: Reductions and Relative Ranking Metrics</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-31" href="#">hunch_net-2005-31</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>31 hunch net-2005-02-26-Problem: Reductions and Relative Ranking Metrics</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-31-html" href="http://hunch.net/?p=34">html</a></p><p>Introduction: This, again, is something of a research direction rather than a single problem.
 
There are several metrics people care about which depend upon the relative ranking of examples and there are sometimes good reasons to care about such metrics.  Examples include  AROC , “F1″, the proportion of the time that the top ranked element is in some class, the proportion of the top 10 examples in some class ( google ‘s problem),  the lowest ranked example of some class, and the “sort distance” from a predicted ranking to a correct ranking.  See  here  for an example of some of these.  
 
 Problem  What does the ability to classify well imply about performance under these metrics?
 
 Past Work 
  
  Probabilistic classification under squared error  can be solved with a classifier.  A counterexample shows this does not imply a good AROC. 
 Sample complexity bounds for  AROC  (and  here ). 
 A paper on “ Learning to Order Things “. 
  
 Difficulty  Several of these may be easy.  Some of them may be h</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 This, again, is something of a research direction rather than a single problem. [sent-1, score-0.162]
</p><p>2 There are several metrics people care about which depend upon the relative ranking of examples and there are sometimes good reasons to care about such metrics. [sent-2, score-1.446]
</p><p>3 Examples include  AROC , “F1″, the proportion of the time that the top ranked element is in some class, the proportion of the top 10 examples in some class ( google ‘s problem),  the lowest ranked example of some class, and the “sort distance” from a predicted ranking to a correct ranking. [sent-3, score-2.933]
</p><p>4 Problem  What does the ability to classify well imply about performance under these metrics? [sent-5, score-0.46]
</p><p>5 Past Work       Probabilistic classification under squared error  can be solved with a classifier. [sent-6, score-0.305]
</p><p>6 A counterexample shows this does not imply a good AROC. [sent-7, score-0.425]
</p><p>7 Sample complexity bounds for  AROC  (and  here ). [sent-8, score-0.136]
</p><p>8 Impact  Positive or negative results will broaden our understanding of the relationship between different learning goals. [sent-12, score-0.221]
</p><p>9 It might also yield new algorithms (via the reduction) for solving these problems. [sent-13, score-0.15]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ranked', 0.369), ('aroc', 0.328), ('proportion', 0.303), ('metrics', 0.253), ('top', 0.238), ('class', 0.218), ('ranking', 0.212), ('care', 0.191), ('imply', 0.175), ('counterexample', 0.164), ('classify', 0.152), ('depend', 0.152), ('examples', 0.142), ('relationship', 0.127), ('distance', 0.113), ('element', 0.111), ('predicted', 0.111), ('squared', 0.097), ('positive', 0.094), ('negative', 0.094), ('google', 0.093), ('relative', 0.091), ('direction', 0.09), ('yield', 0.089), ('past', 0.088), ('probabilistic', 0.086), ('upon', 0.086), ('shows', 0.086), ('correct', 0.085), ('sort', 0.081), ('impact', 0.08), ('include', 0.079), ('reduction', 0.079), ('solved', 0.078), ('difficulty', 0.075), ('sample', 0.075), ('bounds', 0.073), ('single', 0.072), ('ability', 0.068), ('classification', 0.067), ('reasons', 0.065), ('may', 0.065), ('performance', 0.065), ('order', 0.065), ('error', 0.063), ('complexity', 0.063), ('several', 0.063), ('example', 0.062), ('via', 0.062), ('solving', 0.061)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="31-tfidf-1" href="../hunch_net-2005/hunch_net-2005-02-26-Problem%3A_Reductions_and_Relative_Ranking_Metrics.html">31 hunch net-2005-02-26-Problem: Reductions and Relative Ranking Metrics</a></p>
<p>Introduction: This, again, is something of a research direction rather than a single problem.
 
There are several metrics people care about which depend upon the relative ranking of examples and there are sometimes good reasons to care about such metrics.  Examples include  AROC , “F1″, the proportion of the time that the top ranked element is in some class, the proportion of the top 10 examples in some class ( google ‘s problem),  the lowest ranked example of some class, and the “sort distance” from a predicted ranking to a correct ranking.  See  here  for an example of some of these.  
 
 Problem  What does the ability to classify well imply about performance under these metrics?
 
 Past Work 
  
  Probabilistic classification under squared error  can be solved with a classifier.  A counterexample shows this does not imply a good AROC. 
 Sample complexity bounds for  AROC  (and  here ). 
 A paper on “ Learning to Order Things “. 
  
 Difficulty  Several of these may be easy.  Some of them may be h</p><p>2 0.16255432 <a title="31-tfidf-2" href="../hunch_net-2005/hunch_net-2005-02-12-ROC_vs._Accuracy_vs._AROC.html">18 hunch net-2005-02-12-ROC vs. Accuracy vs. AROC</a></p>
<p>Introduction: Foster Provost  and I discussed the merits of ROC curves vs. accuracy estimation.  Here is a quick summary of our discussion.
 
The “Receiver Operating Characteristic” (ROC) curve is an alternative to accuracy for the evaluation of learning algorithms on natural datasets.  The ROC curve is a  curve  and not a single number statistic.  In particular, this means that the comparison of two algorithms on a dataset does not always produce an obvious order.
 
Accuracy (= 1 – error rate) is a standard method used to evaluate learning algorithms.  It is a single-number summary of performance.
 
AROC is the area under the ROC curve.  It is a single number summary of performance.
 
The comparison of these metrics is a subtle affair, because in machine learning, they are compared on different natural datasets.  This makes some sense if we accept the hypothesis “Performance on past learning problems (roughly) predicts performance on future learning problems.”
 
The ROC vs. accuracy discussion is o</p><p>3 0.12451397 <a title="31-tfidf-3" href="../hunch_net-2013/hunch_net-2013-01-07-NYU_Large_Scale_Machine_Learning_Class.html">478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</a></p>
<p>Introduction: Yann LeCun  and I are coteaching a class on  Large Scale Machine Learning  starting late January  at NYU .  This class will cover many tricks to get machine learning working well on datasets with many features, examples, and classes, along with several elements of deep learning and support systems enabling the previous.
 
This is not a beginning class—you really need to have taken a basic machine learning class previously to follow along.  Students will be able to run and experiment with large scale learning algorithms since  Yahoo!  has donated servers which are being configured into a small scale  Hadoop  cluster.   We are planning to cover the frontier of research in scalable learning algorithms, so good class projects could easily lead to papers.
 
For me, this is a chance to teach on many topics of past research.  In general, it seems like researchers should engage in at least occasional teaching of research, both as a proof of teachability and to see their own research through th</p><p>4 0.12255333 <a title="31-tfidf-4" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>Introduction: Essentially everyone who writes research papers suffers rejections.  They always sting immediately, but upon further reflection many of these rejections come to seem reasonable.  Maybe the equations had too many typos or maybe the topic just isn’t as important as was originally thought.  A few rejections do not come to seem acceptable, and these form the basis of reviewing horror stories, a great material for conversations.  I’ve decided to share three of mine, now all safely a bit distant in the past.
  
  Prediction Theory for Classification Tutorial .  This is a tutorial about tight sample complexity bounds for classification that I submitted to  JMLR .  The first decision I heard was a reject which appeared quite unjust to me—for example one of the reviewers appeared to claim that all the content was in standard statistics books.  Upon further inquiry, several citations were given, none of which actually covered the content.  Later, I was shocked to hear the paper was accepted. App</p><p>5 0.11176896 <a title="31-tfidf-5" href="../hunch_net-2006/hunch_net-2006-07-13-Regression_vs._Classification_as_a_Primitive.html">196 hunch net-2006-07-13-Regression vs. Classification as a Primitive</a></p>
<p>Introduction: For  learning reductions  we have been concentrating on reducing various complex learning problems to binary classification.  This choice needs to be actively questioned, because it was not carefully considered.
 
Binary clasification is learning a classifier  c:X -> {0,1}  so as to minimize the probability of being wrong,  Pr x,y~D (c(x)   y)  .
 
The primary alternative candidate seems to be squared error regression.  In squared error regression, you learn a regressor  s:X -> [0,1]  so as to minimize squared error,  E x,y~D  (s(x)-y) 2  .
 
It is difficult to judge one primitive against another.  The judgement must at least partially be made on nontheoretical grounds because (essentially) we are evaluating a choice between two axioms/assumptions.
 
These two primitives are significantly related.  Classification can be reduced to regression in the obvious way: you use the regressor to predict  D(y=1|x) , then threshold at  0.5 .   For this simple reduction a squared error regret of  r</p><p>6 0.1089903 <a title="31-tfidf-6" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>7 0.10565121 <a title="31-tfidf-7" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>8 0.094201066 <a title="31-tfidf-8" href="../hunch_net-2005/hunch_net-2005-03-15-The_State_of_Tight_Bounds.html">41 hunch net-2005-03-15-The State of Tight Bounds</a></p>
<p>9 0.092467137 <a title="31-tfidf-9" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>10 0.088345282 <a title="31-tfidf-10" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>11 0.087954856 <a title="31-tfidf-11" href="../hunch_net-2005/hunch_net-2005-02-03-Learning_Theory%2C_by_assumption.html">12 hunch net-2005-02-03-Learning Theory, by assumption</a></p>
<p>12 0.087548159 <a title="31-tfidf-12" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>13 0.081015088 <a title="31-tfidf-13" href="../hunch_net-2005/hunch_net-2005-06-17-Reopening_RL-%3EClassification.html">82 hunch net-2005-06-17-Reopening RL->Classification</a></p>
<p>14 0.079973683 <a title="31-tfidf-14" href="../hunch_net-2006/hunch_net-2006-02-11-Yahoo%26%238217%3Bs_Learning_Problems..html">156 hunch net-2006-02-11-Yahoo&#8217;s Learning Problems.</a></p>
<p>15 0.076796979 <a title="31-tfidf-15" href="../hunch_net-2008/hunch_net-2008-07-15-Interesting_papers_at_COLT_%28and_a_bit_of_UAI_%26%23038%3B_workshops%29.html">310 hunch net-2008-07-15-Interesting papers at COLT (and a bit of UAI &#038; workshops)</a></p>
<p>16 0.076111965 <a title="31-tfidf-16" href="../hunch_net-2005/hunch_net-2005-11-28-A_question_of_quantification.html">133 hunch net-2005-11-28-A question of quantification</a></p>
<p>17 0.073395833 <a title="31-tfidf-17" href="../hunch_net-2006/hunch_net-2006-09-09-How_to_solve_an_NP_hard_problem_in_quadratic_time.html">206 hunch net-2006-09-09-How to solve an NP hard problem in quadratic time</a></p>
<p>18 0.072929792 <a title="31-tfidf-18" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>19 0.07277064 <a title="31-tfidf-19" href="../hunch_net-2010/hunch_net-2010-03-15-The_Efficient_Robust_Conditional_Probability_Estimation_Problem.html">391 hunch net-2010-03-15-The Efficient Robust Conditional Probability Estimation Problem</a></p>
<p>20 0.071144894 <a title="31-tfidf-20" href="../hunch_net-2007/hunch_net-2007-11-28-Computational_Consequences_of_Classification.html">274 hunch net-2007-11-28-Computational Consequences of Classification</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.175), (1, 0.078), (2, 0.037), (3, -0.0), (4, -0.027), (5, -0.027), (6, 0.074), (7, 0.01), (8, -0.026), (9, -0.025), (10, -0.014), (11, 0.035), (12, 0.002), (13, -0.002), (14, 0.011), (15, 0.037), (16, 0.021), (17, -0.02), (18, -0.008), (19, -0.008), (20, 0.073), (21, 0.013), (22, -0.013), (23, -0.054), (24, 0.016), (25, -0.014), (26, 0.009), (27, 0.008), (28, -0.04), (29, -0.02), (30, 0.008), (31, -0.041), (32, -0.065), (33, -0.067), (34, 0.033), (35, 0.027), (36, 0.06), (37, 0.064), (38, 0.071), (39, 0.02), (40, 0.049), (41, -0.098), (42, -0.004), (43, 0.024), (44, 0.086), (45, 0.041), (46, -0.033), (47, -0.019), (48, 0.043), (49, -0.041)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95046794 <a title="31-lsi-1" href="../hunch_net-2005/hunch_net-2005-02-26-Problem%3A_Reductions_and_Relative_Ranking_Metrics.html">31 hunch net-2005-02-26-Problem: Reductions and Relative Ranking Metrics</a></p>
<p>Introduction: This, again, is something of a research direction rather than a single problem.
 
There are several metrics people care about which depend upon the relative ranking of examples and there are sometimes good reasons to care about such metrics.  Examples include  AROC , “F1″, the proportion of the time that the top ranked element is in some class, the proportion of the top 10 examples in some class ( google ‘s problem),  the lowest ranked example of some class, and the “sort distance” from a predicted ranking to a correct ranking.  See  here  for an example of some of these.  
 
 Problem  What does the ability to classify well imply about performance under these metrics?
 
 Past Work 
  
  Probabilistic classification under squared error  can be solved with a classifier.  A counterexample shows this does not imply a good AROC. 
 Sample complexity bounds for  AROC  (and  here ). 
 A paper on “ Learning to Order Things “. 
  
 Difficulty  Several of these may be easy.  Some of them may be h</p><p>2 0.79700631 <a title="31-lsi-2" href="../hunch_net-2005/hunch_net-2005-02-12-ROC_vs._Accuracy_vs._AROC.html">18 hunch net-2005-02-12-ROC vs. Accuracy vs. AROC</a></p>
<p>Introduction: Foster Provost  and I discussed the merits of ROC curves vs. accuracy estimation.  Here is a quick summary of our discussion.
 
The “Receiver Operating Characteristic” (ROC) curve is an alternative to accuracy for the evaluation of learning algorithms on natural datasets.  The ROC curve is a  curve  and not a single number statistic.  In particular, this means that the comparison of two algorithms on a dataset does not always produce an obvious order.
 
Accuracy (= 1 – error rate) is a standard method used to evaluate learning algorithms.  It is a single-number summary of performance.
 
AROC is the area under the ROC curve.  It is a single number summary of performance.
 
The comparison of these metrics is a subtle affair, because in machine learning, they are compared on different natural datasets.  This makes some sense if we accept the hypothesis “Performance on past learning problems (roughly) predicts performance on future learning problems.”
 
The ROC vs. accuracy discussion is o</p><p>3 0.68543732 <a title="31-lsi-3" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>Introduction: What?  Reductions are machines which turn solvers for one problem into solvers for another problem. 
 Why?  Reductions are useful for several reasons.
  
  Laziness .  Reducing a problem to classification make at least 10 learning algorithms available to solve a problem.  Inventing 10 learning algorithms is quite a bit of work.  Similarly, programming a reduction is often trivial, while programming a learning algorithm is a great deal of work. 
  Crystallization .  The problems we often want to solve in learning are worst-case-impossible, but average case feasible.  By reducing all problems onto one or a few primitives, we can fine tune these primitives to perform well on real-world problems with greater precision due to the greater number of problems to validate on. 
  Theoretical Organization .  By studying what reductions are easy vs. hard vs. impossible, we can learn which problems are roughly equivalent in difficulty and which are much harder. 
  
 What we know now .
 
 Typesafe r</p><p>4 0.65170044 <a title="31-lsi-4" href="../hunch_net-2005/hunch_net-2005-02-23-Problem%3A_Reinforcement_Learning_with_Classification.html">27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</a></p>
<p>Introduction: At an intuitive level, the question here is “Can reinforcement learning be solved with classification?”  
 
 Problem  Construct a reinforcement learning algorithm with near-optimal expected sum of rewards in the  direct experience model  given access to a classifier learning algorithm which has a small error rate or regret on all posed classification problems.  The definition of “posed” here is slightly murky.  I consider a problem “posed” if there is an algorithm for constructing labeled classification examples.
 
 Past Work 
  
 There exists a  reduction of reinforcement learning to classification given a generative model.   A generative model is an inherently stronger assumption than the direct experience model. 
 Other  work on learning reductions  may be important. 
 Several algorithms for solving reinforcement learning in the direct experience model exist.  Most, such as  E 3  ,  Factored-E 3  , and  metric-E 3   and  Rmax  require that the observation be the state.  Recent work</p><p>5 0.62696296 <a title="31-lsi-5" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>Introduction: One of the remarkable things about machine learning is how diverse it is.  The viewpoints of Bayesian learning, reinforcement learning, graphical models, supervised learning, unsupervised learning, genetic programming, etc… share little enough overlap that many people can and do make their careers within one without touching, or even necessarily understanding the others.
 
There are two fundamental reasons why this is possible.
  
 For many problems, many approaches work in the sense that they do something useful.  This is true empirically, where for many problems we can observe that many different approaches yield better performance than any constant predictor.  It’s also true in theory, where we know that for any set of predictors representable in a finite amount of RAM, minimizing training error over the set of predictors does something nontrivial when there are a sufficient number of examples. 
 There is nothing like a unifying problem defining the field.  In many other areas there</p><p>6 0.62319297 <a title="31-lsi-6" href="../hunch_net-2007/hunch_net-2007-06-14-Interesting_Papers_at_COLT_2007.html">247 hunch net-2007-06-14-Interesting Papers at COLT 2007</a></p>
<p>7 0.614838 <a title="31-lsi-7" href="../hunch_net-2005/hunch_net-2005-05-06-Don%26%238217%3Bt_mix_the_solution_into_the_problem.html">67 hunch net-2005-05-06-Don&#8217;t mix the solution into the problem</a></p>
<p>8 0.6103099 <a title="31-lsi-8" href="../hunch_net-2005/hunch_net-2005-12-09-Some_NIPS_papers.html">138 hunch net-2005-12-09-Some NIPS papers</a></p>
<p>9 0.60972553 <a title="31-lsi-9" href="../hunch_net-2006/hunch_net-2006-09-09-How_to_solve_an_NP_hard_problem_in_quadratic_time.html">206 hunch net-2006-09-09-How to solve an NP hard problem in quadratic time</a></p>
<p>10 0.6096276 <a title="31-lsi-10" href="../hunch_net-2006/hunch_net-2006-05-05-An_ICML_reject.html">177 hunch net-2006-05-05-An ICML reject</a></p>
<p>11 0.6083504 <a title="31-lsi-11" href="../hunch_net-2005/hunch_net-2005-08-08-Apprenticeship_Reinforcement_Learning_for_Control.html">101 hunch net-2005-08-08-Apprenticeship Reinforcement Learning for Control</a></p>
<p>12 0.60216868 <a title="31-lsi-12" href="../hunch_net-2006/hunch_net-2006-01-13-Benchmarks_for_RL.html">148 hunch net-2006-01-13-Benchmarks for RL</a></p>
<p>13 0.59634364 <a title="31-lsi-13" href="../hunch_net-2005/hunch_net-2005-11-28-A_question_of_quantification.html">133 hunch net-2005-11-28-A question of quantification</a></p>
<p>14 0.58049905 <a title="31-lsi-14" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>15 0.57379544 <a title="31-lsi-15" href="../hunch_net-2005/hunch_net-2005-08-22-Do_you_believe_in_induction%3F.html">104 hunch net-2005-08-22-Do you believe in induction?</a></p>
<p>16 0.57121336 <a title="31-lsi-16" href="../hunch_net-2005/hunch_net-2005-01-24-Holy_grails_of_machine_learning%3F.html">2 hunch net-2005-01-24-Holy grails of machine learning?</a></p>
<p>17 0.56032914 <a title="31-lsi-17" href="../hunch_net-2005/hunch_net-2005-11-16-The_Everything_Ensemble_Edge.html">131 hunch net-2005-11-16-The Everything Ensemble Edge</a></p>
<p>18 0.55964589 <a title="31-lsi-18" href="../hunch_net-2006/hunch_net-2006-07-13-Regression_vs._Classification_as_a_Primitive.html">196 hunch net-2006-07-13-Regression vs. Classification as a Primitive</a></p>
<p>19 0.55894488 <a title="31-lsi-19" href="../hunch_net-2005/hunch_net-2005-03-17-Going_all_the_Way%2C_Sometimes.html">42 hunch net-2005-03-17-Going all the Way, Sometimes</a></p>
<p>20 0.55785179 <a title="31-lsi-20" href="../hunch_net-2007/hunch_net-2007-02-02-Thoughts_regarding_%26%238220%3BIs_machine_learning_different_from_statistics%3F%26%238221%3B.html">230 hunch net-2007-02-02-Thoughts regarding &#8220;Is machine learning different from statistics?&#8221;</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(3, 0.435), (27, 0.28), (53, 0.101), (55, 0.061)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.96955544 <a title="31-lda-1" href="../hunch_net-2007/hunch_net-2007-05-08-Conditional_Tournaments_for_Multiclass_to_Binary.html">243 hunch net-2007-05-08-Conditional Tournaments for Multiclass to Binary</a></p>
<p>Introduction: This  problem  has been cracked (but not quite completely solved) by  Alina ,  Pradeep , and  I .  The problem is essentially finding a better way to reduce multiclass classification to binary classification.  The solution is to use a carefully crafted tournament,  the simplest version of which is a  single elimination tournament  where the “players” are the different classes.  An example of the structure is here: 
    
For the single elimination tournament, we can prove that: 
 For all multiclass problems  D , for all learned binary classifiers  c , the regret of an induced multiclass classifier is bounded by the regret of the binary classifier times  log 2  k .  Restated:  
  reg multiclass (D,Filter_tree_test(c)) <= reg binary  (Filter_tree_train(D),c)    
Here:
  
   Filter_tree_train(D)  is the induced binary classification problem 
   Filter_tree_test(c)  is the induced multiclass classifier. 
   reg multiclass   is the multiclass regret (= difference between error rate and minim</p><p>2 0.93632704 <a title="31-lda-2" href="../hunch_net-2010/hunch_net-2010-06-20-2010_ICML_discussion_site.html">401 hunch net-2010-06-20-2010 ICML discussion site</a></p>
<p>Introduction: A substantial difficulty with the 2009 and 2008  ICML discussion system  was a communication vacuum, where authors were not informed of comments, and commenters were not informed of responses to their comments without explicit monitoring.   Mark Reid  has setup a  new discussion system for 2010  with the goal of addressing this.
 
Mark didn’t want to make it to intrusive, so you must opt-in.  As an author,  find your paper  and “Subscribe by email” to the comments.  As a commenter, you have the option of providing an email for follow-up notification.</p><p>3 0.92825127 <a title="31-lda-3" href="../hunch_net-2008/hunch_net-2008-04-26-Eliminating_the_Birthday_Paradox_for_Universal_Features.html">298 hunch net-2008-04-26-Eliminating the Birthday Paradox for Universal Features</a></p>
<p>Introduction: I want to expand on  this post  which describes one of the core tricks for making  Vowpal Wabbit  fast and easy to use when learning from text.  
 
The central trick is converting a word (or any other parseable quantity) into a number via a hash function.   Kishore  tells me this is a relatively old trick in NLP land, but it has some added advantages when doing online learning, because you can learn directly from the existing data without preprocessing the data to create features (destroying the online property) or using an expensive hashtable lookup (slowing things down).
 
A central concern for this approach is collisions, which create a loss of information.  If you use  m  features in an index space of size  n  the birthday paradox suggests a collision if  m > n 0.5  , essentially because there are  m 2   pairs.   This is pretty bad, because it says that with a vocabulary of  10 5   features, you might need to have  10 10   entries in your table.
 
It turns out that redundancy is gr</p><p>same-blog 4 0.90731889 <a title="31-lda-4" href="../hunch_net-2005/hunch_net-2005-02-26-Problem%3A_Reductions_and_Relative_Ranking_Metrics.html">31 hunch net-2005-02-26-Problem: Reductions and Relative Ranking Metrics</a></p>
<p>Introduction: This, again, is something of a research direction rather than a single problem.
 
There are several metrics people care about which depend upon the relative ranking of examples and there are sometimes good reasons to care about such metrics.  Examples include  AROC , “F1″, the proportion of the time that the top ranked element is in some class, the proportion of the top 10 examples in some class ( google ‘s problem),  the lowest ranked example of some class, and the “sort distance” from a predicted ranking to a correct ranking.  See  here  for an example of some of these.  
 
 Problem  What does the ability to classify well imply about performance under these metrics?
 
 Past Work 
  
  Probabilistic classification under squared error  can be solved with a classifier.  A counterexample shows this does not imply a good AROC. 
 Sample complexity bounds for  AROC  (and  here ). 
 A paper on “ Learning to Order Things “. 
  
 Difficulty  Several of these may be easy.  Some of them may be h</p><p>5 0.87851042 <a title="31-lda-5" href="../hunch_net-2006/hunch_net-2006-10-08-Incompatibilities_between_classical_confidence_intervals_and_learning..html">213 hunch net-2006-10-08-Incompatibilities between classical confidence intervals and learning.</a></p>
<p>Introduction: Classical confidence intervals satisfy a theorem of the form: For some data sources  D , 
  Pr S ~ D (f(D) > g(S)) > 1-d   
where  f  is some function of the distribution (such as the mean) and  g  is some function of the observed sample  S .   The constraints on  D  can vary between “Independent and identically distributed (IID) samples from a gaussian with an unknown mean” to “IID samples from an arbitrary distribution  D “.  There are even some confidence intervals which do not require IID samples.
 
Classical confidence intervals often confuse people.  They do  not  say “with high probability, for my observed sample, the bounds holds”.   Instead, they tell you that if you reason according to the confidence interval in the future (and the constraints on  D  are satisfied), then you are not often wrong.  Restated, they tell you something about what a safe procedure is in a stochastic world where  d  is the safety parameter.
 
There are a number of results in theoretical machine learn</p><p>6 0.83829004 <a title="31-lda-6" href="../hunch_net-2010/hunch_net-2010-03-15-The_Efficient_Robust_Conditional_Probability_Estimation_Problem.html">391 hunch net-2010-03-15-The Efficient Robust Conditional Probability Estimation Problem</a></p>
<p>7 0.79551071 <a title="31-lda-7" href="../hunch_net-2008/hunch_net-2008-02-17-The_Meaning_of_Confidence.html">289 hunch net-2008-02-17-The Meaning of Confidence</a></p>
<p>8 0.7424711 <a title="31-lda-8" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>9 0.70083576 <a title="31-lda-9" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>10 0.66589165 <a title="31-lda-10" href="../hunch_net-2005/hunch_net-2005-03-02-Prior%2C_%26%238220%3BPrior%26%238221%3B_and_Bias.html">34 hunch net-2005-03-02-Prior, &#8220;Prior&#8221; and Bias</a></p>
<p>11 0.64034474 <a title="31-lda-11" href="../hunch_net-2007/hunch_net-2007-01-10-A_Deep_Belief_Net_Learning_Problem.html">227 hunch net-2007-01-10-A Deep Belief Net Learning Problem</a></p>
<p>12 0.63552415 <a title="31-lda-12" href="../hunch_net-2005/hunch_net-2005-05-06-Don%26%238217%3Bt_mix_the_solution_into_the_problem.html">67 hunch net-2005-05-06-Don&#8217;t mix the solution into the problem</a></p>
<p>13 0.62923682 <a title="31-lda-13" href="../hunch_net-2005/hunch_net-2005-11-07-Prediction_Competitions.html">129 hunch net-2005-11-07-Prediction Competitions</a></p>
<p>14 0.62522566 <a title="31-lda-14" href="../hunch_net-2012/hunch_net-2012-04-09-ICML_author_feedback_is_open.html">461 hunch net-2012-04-09-ICML author feedback is open</a></p>
<p>15 0.61854887 <a title="31-lda-15" href="../hunch_net-2009/hunch_net-2009-02-04-Optimal_Proxy_Loss_for_Classification.html">341 hunch net-2009-02-04-Optimal Proxy Loss for Classification</a></p>
<p>16 0.61818266 <a title="31-lda-16" href="../hunch_net-2005/hunch_net-2005-06-06-Exact_Online_Learning_for_Classification.html">78 hunch net-2005-06-06-Exact Online Learning for Classification</a></p>
<p>17 0.60973966 <a title="31-lda-17" href="../hunch_net-2005/hunch_net-2005-03-15-The_State_of_Tight_Bounds.html">41 hunch net-2005-03-15-The State of Tight Bounds</a></p>
<p>18 0.60722798 <a title="31-lda-18" href="../hunch_net-2005/hunch_net-2005-02-01-Watchword%3A_Loss.html">9 hunch net-2005-02-01-Watchword: Loss</a></p>
<p>19 0.60143882 <a title="31-lda-19" href="../hunch_net-2007/hunch_net-2007-08-19-Choice_of_Metrics.html">259 hunch net-2007-08-19-Choice of Metrics</a></p>
<p>20 0.5989024 <a title="31-lda-20" href="../hunch_net-2008/hunch_net-2008-10-14-Who_is_Responsible_for_a_Bad_Review%3F.html">320 hunch net-2008-10-14-Who is Responsible for a Bad Review?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
