<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>47 hunch net-2005-03-28-Open Problems for Colt</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-47" href="#">hunch_net-2005-47</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>47 hunch net-2005-03-28-Open Problems for Colt</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-47-html" href="http://hunch.net/?p=51">html</a></p><p>Introduction: Adam KlivansandRocco Servedioare looking foropen (learning theory)
problemsforCOLT. This is a good idea in the same way that the KDDcup challenge
is a good idea: crisp problem definitions that anyone can attack yield
solutions that advance science.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 This is a good idea in the same way that the KDDcup challenge is a good idea: crisp problem definitions that anyone can attack yield solutions that advance science. [sent-2, score-2.954]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('crisp', 0.398), ('adam', 0.375), ('attack', 0.312), ('definitions', 0.312), ('idea', 0.292), ('advance', 0.278), ('challenge', 0.25), ('yield', 0.239), ('looking', 0.224), ('solutions', 0.224), ('science', 0.188), ('anyone', 0.182), ('good', 0.139), ('theory', 0.131), ('way', 0.108), ('problem', 0.081), ('learning', 0.028)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="47-tfidf-1" href="../hunch_net-2005/hunch_net-2005-03-28-Open_Problems_for_Colt.html">47 hunch net-2005-03-28-Open Problems for Colt</a></p>
<p>Introduction: Adam KlivansandRocco Servedioare looking foropen (learning theory)
problemsforCOLT. This is a good idea in the same way that the KDDcup challenge
is a good idea: crisp problem definitions that anyone can attack yield
solutions that advance science.</p><p>2 0.17731228 <a title="47-tfidf-2" href="../hunch_net-2008/hunch_net-2008-11-11-COLT_CFP.html">326 hunch net-2008-11-11-COLT CFP</a></p>
<p>Introduction: Adam Klivans, points out theCOLT call for papers. The important points are:Due
Feb 13.Montreal, June 18-21.This year, there is author feedback.</p><p>3 0.13466215 <a title="47-tfidf-3" href="../hunch_net-2011/hunch_net-2011-04-06-COLT_open_questions.html">429 hunch net-2011-04-06-COLT open questions</a></p>
<p>Introduction: AlinaandJakepoint out the COLTCall for Open Questionsdue May 11. In general,
this is cool, and worth doing if you can come up with a crisp question. In my
case, I particularly enjoyedcrafting an open questionwith precisely a form
such that acritic targeting my paperswould be forced to confront their fallacy
or make a case for the reward. But less esoterically, this is a way to get the
attention of some very smart people focused on a problem that really matters,
which is the real value.</p><p>4 0.12360549 <a title="47-tfidf-4" href="../hunch_net-2013/hunch_net-2013-04-15-NEML_II.html">481 hunch net-2013-04-15-NEML II</a></p>
<p>Introduction: Adam Kalaipoints out theNew England Machine Learning DayMay 1 at MSR New
England. There is a poster session with abstracts due April 19. I understand
last year'sNEMLwent well and it's great to meet your neighbors at regional
workshops like this.</p><p>5 0.11623172 <a title="47-tfidf-5" href="../hunch_net-2005/hunch_net-2005-08-04-Why_Reinforcement_Learning_is_Important.html">100 hunch net-2005-08-04-Why Reinforcement Learning is Important</a></p>
<p>Introduction: One prescription for solving a problem well is:State the problem, in the
simplest way possible. In particular, this statement should involve no
contamination with or anticipation of the solution.Think about solutions to
the stated problem.Stating a problem in a succinct and crisp manner tends to
invite a simple elegant solution. When a problem can not be stated succinctly,
we wonder if the problem is even understood. (And when a problem is not
understood, we wonder if a solution can be meaningful.)Reinforcement learning
does step (1) well. It provides a clean simple language to state general AI
problems. In reinforcement learning there is a set of actionsA, a set of
observationsO, and a rewardr. The reinforcement learning problem, in general,
is defined by a conditional measureD( o, r | (o,r,a)*)which produces an
observationoand a rewardrgiven a history(o,r,a)*. The goal in reinforcement
learning is to find a policypi:(o,r,a)*-> amapping histories to actions so as
to maximize (or appro</p><p>6 0.094399743 <a title="47-tfidf-6" href="../hunch_net-2007/hunch_net-2007-04-18-%2450K_Spock_Challenge.html">239 hunch net-2007-04-18-$50K Spock Challenge</a></p>
<p>7 0.083900452 <a title="47-tfidf-7" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>8 0.076227754 <a title="47-tfidf-8" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>9 0.075921632 <a title="47-tfidf-9" href="../hunch_net-2005/hunch_net-2005-04-25-Embeddings%3A_what_are_they_good_for%3F.html">61 hunch net-2005-04-25-Embeddings: what are they good for?</a></p>
<p>10 0.075528465 <a title="47-tfidf-10" href="../hunch_net-2005/hunch_net-2005-04-28-Science_Fiction_and_Research.html">64 hunch net-2005-04-28-Science Fiction and Research</a></p>
<p>11 0.073493175 <a title="47-tfidf-11" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>12 0.069470167 <a title="47-tfidf-12" href="../hunch_net-2013/hunch_net-2013-08-31-Extreme_Classification_workshop_at_NIPS.html">488 hunch net-2013-08-31-Extreme Classification workshop at NIPS</a></p>
<p>13 0.066977814 <a title="47-tfidf-13" href="../hunch_net-2008/hunch_net-2008-03-15-COLT_Open_Problems.html">292 hunch net-2008-03-15-COLT Open Problems</a></p>
<p>14 0.065613702 <a title="47-tfidf-14" href="../hunch_net-2005/hunch_net-2005-07-21-Six_Months.html">96 hunch net-2005-07-21-Six Months</a></p>
<p>15 0.064162828 <a title="47-tfidf-15" href="../hunch_net-2006/hunch_net-2006-03-09-Use_of_Notation.html">162 hunch net-2006-03-09-Use of Notation</a></p>
<p>16 0.060988192 <a title="47-tfidf-16" href="../hunch_net-2005/hunch_net-2005-09-14-The_Predictionist_Viewpoint.html">112 hunch net-2005-09-14-The Predictionist Viewpoint</a></p>
<p>17 0.055803787 <a title="47-tfidf-17" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>18 0.053688325 <a title="47-tfidf-18" href="../hunch_net-2007/hunch_net-2007-11-29-The_Netflix_Crack.html">275 hunch net-2007-11-29-The Netflix Crack</a></p>
<p>19 0.053537138 <a title="47-tfidf-19" href="../hunch_net-2011/hunch_net-2011-04-11-The_Heritage_Health_Prize.html">430 hunch net-2011-04-11-The Heritage Health Prize</a></p>
<p>20 0.053422444 <a title="47-tfidf-20" href="../hunch_net-2006/hunch_net-2006-04-27-Conferences%2C_Workshops%2C_and_Tutorials.html">174 hunch net-2006-04-27-Conferences, Workshops, and Tutorials</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.096), (1, 0.012), (2, 0.047), (3, -0.012), (4, 0.022), (5, -0.046), (6, -0.043), (7, -0.02), (8, 0.019), (9, -0.037), (10, -0.027), (11, -0.017), (12, 0.056), (13, 0.021), (14, 0.083), (15, 0.015), (16, -0.031), (17, -0.021), (18, -0.017), (19, 0.015), (20, 0.02), (21, 0.095), (22, -0.015), (23, -0.077), (24, -0.04), (25, -0.007), (26, -0.0), (27, -0.044), (28, -0.082), (29, 0.183), (30, -0.001), (31, -0.058), (32, 0.058), (33, 0.003), (34, 0.107), (35, -0.102), (36, 0.074), (37, 0.053), (38, 0.075), (39, 0.135), (40, -0.008), (41, 0.029), (42, -0.084), (43, -0.029), (44, -0.096), (45, 0.127), (46, -0.203), (47, -0.049), (48, -0.042), (49, 0.045)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96817458 <a title="47-lsi-1" href="../hunch_net-2005/hunch_net-2005-03-28-Open_Problems_for_Colt.html">47 hunch net-2005-03-28-Open Problems for Colt</a></p>
<p>Introduction: Adam KlivansandRocco Servedioare looking foropen (learning theory)
problemsforCOLT. This is a good idea in the same way that the KDDcup challenge
is a good idea: crisp problem definitions that anyone can attack yield
solutions that advance science.</p><p>2 0.59647346 <a title="47-lsi-2" href="../hunch_net-2011/hunch_net-2011-04-06-COLT_open_questions.html">429 hunch net-2011-04-06-COLT open questions</a></p>
<p>Introduction: AlinaandJakepoint out the COLTCall for Open Questionsdue May 11. In general,
this is cool, and worth doing if you can come up with a crisp question. In my
case, I particularly enjoyedcrafting an open questionwith precisely a form
such that acritic targeting my paperswould be forced to confront their fallacy
or make a case for the reward. But less esoterically, this is a way to get the
attention of some very smart people focused on a problem that really matters,
which is the real value.</p><p>3 0.50032687 <a title="47-lsi-3" href="../hunch_net-2005/hunch_net-2005-07-13-Text_Entailment_at_AAAI.html">94 hunch net-2005-07-13-Text Entailment at AAAI</a></p>
<p>Introduction: Rajat Rainapresented a paper on the technique they used for
thePASCALRecognizing Textual Entailmentchallenge."Text entailment" is the
problem of deciding if one sentence implies another. For example the previous
sentence entails:Text entailment is a decision problem.One sentence can imply
another.The challenge was of the form: given an original sentence and another
sentence predict whether there was an entailment. All current techniques for
predicting correctness of an entailment are at the "flail" stage--accuracies
of around 58% where humans could achieve near 100% accuracy, so there is much
room to improve. Apparently, there may be another PASCAL challenge on this
problem in the near future.</p><p>4 0.50013477 <a title="47-lsi-4" href="../hunch_net-2008/hunch_net-2008-11-11-COLT_CFP.html">326 hunch net-2008-11-11-COLT CFP</a></p>
<p>Introduction: Adam Klivans, points out theCOLT call for papers. The important points are:Due
Feb 13.Montreal, June 18-21.This year, there is author feedback.</p><p>5 0.42053455 <a title="47-lsi-5" href="../hunch_net-2008/hunch_net-2008-01-06-Research_Political_Issues.html">282 hunch net-2008-01-06-Research Political Issues</a></p>
<p>Introduction: I've avoided discussing politics here, although not for lack of interest. The
problem with discussing politics is that it's customary for people to say much
based upon little information. Nevertheless, politics can have a substantial
impact on science (and we might hope for the vice-versa). It's primary
election time in the United States, so the topic is timely, although the
issues are not.There are several policy decisions which substantially effect
development of science and technology in the US.EducationThe US has great
contrasts in education. The top universities are very good places, yet the
grade school education system produces mediocre results. For me, the contrast
between apublic educationandCaltechwas bracing. For many others attending
Caltech, it clearly was not. Upgrading the k-12 education system in the US is
a long-standing chronic problem which I know relatively little about. My own
experience is that a basic attitude of "no child unrealized" is better than
"no child lef</p><p>6 0.41270518 <a title="47-lsi-6" href="../hunch_net-2005/hunch_net-2005-09-14-The_Predictionist_Viewpoint.html">112 hunch net-2005-09-14-The Predictionist Viewpoint</a></p>
<p>7 0.41075426 <a title="47-lsi-7" href="../hunch_net-2007/hunch_net-2007-02-10-Best_Practices_for_Collaboration.html">231 hunch net-2007-02-10-Best Practices for Collaboration</a></p>
<p>8 0.40962836 <a title="47-lsi-8" href="../hunch_net-2005/hunch_net-2005-04-28-Science_Fiction_and_Research.html">64 hunch net-2005-04-28-Science Fiction and Research</a></p>
<p>9 0.40478817 <a title="47-lsi-9" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>10 0.39678413 <a title="47-lsi-10" href="../hunch_net-2005/hunch_net-2005-04-25-Embeddings%3A_what_are_they_good_for%3F.html">61 hunch net-2005-04-25-Embeddings: what are they good for?</a></p>
<p>11 0.37444347 <a title="47-lsi-11" href="../hunch_net-2005/hunch_net-2005-06-17-Reopening_RL-%3EClassification.html">82 hunch net-2005-06-17-Reopening RL->Classification</a></p>
<p>12 0.36724758 <a title="47-lsi-12" href="../hunch_net-2012/hunch_net-2012-03-06-COLT-ICML_Open_Questions_and_ICML_Instructions.html">458 hunch net-2012-03-06-COLT-ICML Open Questions and ICML Instructions</a></p>
<p>13 0.36694878 <a title="47-lsi-13" href="../hunch_net-2009/hunch_net-2009-01-28-Nielsen%26%238217%3Bs_talk.html">340 hunch net-2009-01-28-Nielsen&#8217;s talk</a></p>
<p>14 0.36671948 <a title="47-lsi-14" href="../hunch_net-2005/hunch_net-2005-08-11-Why_Manifold-Based_Dimension_Reduction_Techniques%3F.html">102 hunch net-2005-08-11-Why Manifold-Based Dimension Reduction Techniques?</a></p>
<p>15 0.36341584 <a title="47-lsi-15" href="../hunch_net-2008/hunch_net-2008-03-15-COLT_Open_Problems.html">292 hunch net-2008-03-15-COLT Open Problems</a></p>
<p>16 0.35148278 <a title="47-lsi-16" href="../hunch_net-2005/hunch_net-2005-08-04-Why_Reinforcement_Learning_is_Important.html">100 hunch net-2005-08-04-Why Reinforcement Learning is Important</a></p>
<p>17 0.34540775 <a title="47-lsi-17" href="../hunch_net-2006/hunch_net-2006-05-01-A_conversation_between_Theo_and_Pat.html">176 hunch net-2006-05-01-A conversation between Theo and Pat</a></p>
<p>18 0.34296736 <a title="47-lsi-18" href="../hunch_net-2011/hunch_net-2011-04-11-The_Heritage_Health_Prize.html">430 hunch net-2011-04-11-The Heritage Health Prize</a></p>
<p>19 0.33419475 <a title="47-lsi-19" href="../hunch_net-2010/hunch_net-2010-05-02-What%26%238217%3Bs_the_difference_between_gambling_and_rewarding_good_prediction%3F.html">397 hunch net-2010-05-02-What&#8217;s the difference between gambling and rewarding good prediction?</a></p>
<p>20 0.31499735 <a title="47-lsi-20" href="../hunch_net-2007/hunch_net-2007-11-29-The_Netflix_Crack.html">275 hunch net-2007-11-29-The Netflix Crack</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(42, 0.206), (89, 0.588)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.84361857 <a title="47-lda-1" href="../hunch_net-2005/hunch_net-2005-03-28-Open_Problems_for_Colt.html">47 hunch net-2005-03-28-Open Problems for Colt</a></p>
<p>Introduction: Adam KlivansandRocco Servedioare looking foropen (learning theory)
problemsforCOLT. This is a good idea in the same way that the KDDcup challenge
is a good idea: crisp problem definitions that anyone can attack yield
solutions that advance science.</p><p>2 0.63329303 <a title="47-lda-2" href="../hunch_net-2005/hunch_net-2005-07-13-%26%238220%3BSister_Conference%26%238221%3B_presentations.html">93 hunch net-2005-07-13-&#8220;Sister Conference&#8221; presentations</a></p>
<p>Introduction: Some of the "sister conference" presentations atAAAIhave been great. Roughly
speaking, the conference organizers asked other conference organizers to come
give a summary of their conference. Many different AI-related conferences
accepted. The presenters typically discuss some of the background and goals of
the conference then mention the results from a few papers they liked. This is
great because it provides a mechanism to get a digested overview of the work
of several thousand researchers--something which is simply available nowhere
else.Based on these presentations, it looks like there is a significant
component of (and opportunity for) applied machine learning inAIIDE,IUI,
andACL.There was also some discussion of having a super-colocation event
similar toFCRC, but centered on AI & Learning. This seems like a fine idea.
The field is fractured across so many different conferences that the mixing of
a supercolocation seems likely helpful for research.</p><p>3 0.3306984 <a title="47-lda-3" href="../hunch_net-2008/hunch_net-2008-04-27-Watchword%3A_Supervised_Learning.html">299 hunch net-2008-04-27-Watchword: Supervised Learning</a></p>
<p>Introduction: I recently discovered that supervised learning is a controversial term. The
two definitions are:Known LossSupervised learning corresponds to the situation
where you have unlabeled examples plus knowledge of the loss of each possible
predicted choice. This is the definition I'm familiar and comfortable with.
One reason to prefer this definition is that the analysis of sample complexity
for this class of learning problems are all pretty similar.Any kind of
signalSupervised learning corresponds to the situation where you have
unlabeled examples plus any source of side information about what the right
choice is. This notion of supervised learning seems to subsume reinforcement
learning, which makes me uncomfortable, because it means there are two words
for the same class. This also means there isn't a convenient word to describe
the first definition.Reviews suggest there are people who are dedicated to the
second definition out there, so it can be important to discriminate which you
mean.</p><p>4 0.33045965 <a title="47-lda-4" href="../hunch_net-2006/hunch_net-2006-01-23-On_Coding_via_Mutual_Information_%26%23038%3B_Bayes_Nets.html">150 hunch net-2006-01-23-On Coding via Mutual Information &#038; Bayes Nets</a></p>
<p>Introduction: Say we have two random variablesX,Ywith mutual informationI(X,Y). Let's say we
want to represent them with a bayes net of the formX< -M->Y, such that the
entropy ofMequals the mutual information, i.e.H(M)=I(X,Y). Intuitively, we
would like our hidden state to be as simple as possible (entropy wise). The
data processing inequality means thatH(M)>=I(X,Y), so the mutual information
is a lower bound on how simple theMcould be. Furthermore, if such a
construction existed it would have a nice coding interpretation -- one could
jointly codeXandYby first coding the mutual information, then codingXwith this
mutual info (withoutY) and codingYwith this mutual info (withoutX).It turns
out that such a construction does not exist in general (ThxAlina
Beygelzimerfor a counterexample! see below for the sketch).What are the
implications of this? Well, it's hard for me to say, but it does suggest to me
that the 'generative' model philosophy might be burdened with a harder
modeling task. If all we care a</p><p>5 0.33043849 <a title="47-lda-5" href="../hunch_net-2012/hunch_net-2012-10-26-ML_Symposium_and_Strata-Hadoop_World.html">475 hunch net-2012-10-26-ML Symposium and Strata-Hadoop World</a></p>
<p>Introduction: TheNew York ML symposiumwas last Friday. There were 303 registrations, up a
bit fromlast year. I particularly enjoyed talks byBill Freemanon vision and
ML,Jon Lenchneron strategy in Jeopardy, andTara N. Sainathand Brian Kingsbury
ondeep learning for speech recognition. If anyone has suggestions or thoughts
for next year, please speak up.I also attendedStrata + Hadoop Worldfor the
first time. This is primarily a trade conference rather than an academic
conference, but I found it pretty interesting as a first time attendee. This
is ground zero for theBig databuzzword, and I see now why. It's about data,
and the word "big" is so ambiguous that everyone can lay claim to it. There
were essentially zero academic talks. Instead, the focus was on war stories,
product announcements, and education. The general level of education is much
lower--explaining Machine Learning to the SQL educated is the primary
operating point. Nevertheless that's happening, and the fact that machine
learning is consi</p><p>6 0.33034548 <a title="47-lda-6" href="../hunch_net-2005/hunch_net-2005-05-06-Don%26%238217%3Bt_mix_the_solution_into_the_problem.html">67 hunch net-2005-05-06-Don&#8217;t mix the solution into the problem</a></p>
<p>7 0.33011144 <a title="47-lda-7" href="../hunch_net-2005/hunch_net-2005-12-09-Some_NIPS_papers.html">138 hunch net-2005-12-09-Some NIPS papers</a></p>
<p>8 0.33007821 <a title="47-lda-8" href="../hunch_net-2006/hunch_net-2006-06-15-IJCAI_is_out_of_season.html">184 hunch net-2006-06-15-IJCAI is out of season</a></p>
<p>9 0.32988811 <a title="47-lda-9" href="../hunch_net-2005/hunch_net-2005-03-29-Academic_Mechanism_Design.html">48 hunch net-2005-03-29-Academic Mechanism Design</a></p>
<p>10 0.32930711 <a title="47-lda-10" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>11 0.32923758 <a title="47-lda-11" href="../hunch_net-2005/hunch_net-2005-07-07-The_Limits_of_Learning_Theory.html">90 hunch net-2005-07-07-The Limits of Learning Theory</a></p>
<p>12 0.32655939 <a title="47-lda-12" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>13 0.32613689 <a title="47-lda-13" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<p>14 0.32324687 <a title="47-lda-14" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>15 0.32119265 <a title="47-lda-15" href="../hunch_net-2005/hunch_net-2005-02-23-Problem%3A_Reinforcement_Learning_with_Classification.html">27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</a></p>
<p>16 0.31987995 <a title="47-lda-16" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>17 0.31852844 <a title="47-lda-17" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>18 0.31688073 <a title="47-lda-18" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>19 0.31629288 <a title="47-lda-19" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>20 0.31389207 <a title="47-lda-20" href="../hunch_net-2005/hunch_net-2005-03-21-Research_Styles_in_Machine_Learning.html">44 hunch net-2005-03-21-Research Styles in Machine Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
