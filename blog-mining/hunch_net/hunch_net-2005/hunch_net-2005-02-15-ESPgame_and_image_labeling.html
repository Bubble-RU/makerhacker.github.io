<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>20 hunch net-2005-02-15-ESPgame and image labeling</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-20" href="#">hunch_net-2005-20</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>20 hunch net-2005-02-15-ESPgame and image labeling</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-20-html" href="http://hunch.net/?p=23">html</a></p><p>Introduction: Luis von Ahnhas been running theespgamefor awhile now. The espgame provides a
picture to two randomly paired people across the web, and asks them to agree
on a label. It hasn't managed to label the web yet, but it has produced alarge
datasetof (image, label) pairs. I organized the dataset so you couldexplore
the implied bipartite graph(requires much bandwidth).Relative to other image
datasets, this one is quite large--67000 images, 358,000 labels (average of
5/image with variation from 1 to 19), and 22,000 unique labels (one every 3
images). The dataset is also very 'natural', consisting of images spidered
from the internet. The multiple label characteristic is intriguing because
'learning to learn' and metalearning techniques may be applicable. The
'natural' quality means that this dataset varies greatly in difficulty from
easy (predicting "red") to hard (predicting "funny") and potentially more
rewarding to tackle.The open problem here is, of course, to make an internet
image labelin</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The espgame provides a picture to two randomly paired people across the web, and asks them to agree on a label. [sent-2, score-0.814]
</p><p>2 It hasn't managed to label the web yet, but it has produced alarge datasetof (image, label) pairs. [sent-3, score-0.75]
</p><p>3 I organized the dataset so you couldexplore the implied bipartite graph(requires much bandwidth). [sent-4, score-0.597]
</p><p>4 Relative to other image datasets, this one is quite large--67000 images, 358,000 labels (average of 5/image with variation from 1 to 19), and 22,000 unique labels (one every 3 images). [sent-5, score-0.94]
</p><p>5 The dataset is also very 'natural', consisting of images spidered from the internet. [sent-6, score-0.687]
</p><p>6 The multiple label characteristic is intriguing because 'learning to learn' and metalearning techniques may be applicable. [sent-7, score-0.693]
</p><p>7 The 'natural' quality means that this dataset varies greatly in difficulty from easy (predicting "red") to hard (predicting "funny") and potentially more rewarding to tackle. [sent-8, score-0.675]
</p><p>8 The open problem here is, of course, to make an internet image labeling program. [sent-9, score-0.576]
</p><p>9 At a minimum this might be useful for blind people and image search. [sent-10, score-0.565]
</p><p>10 Solving this problem well seems likely to require new learning methods. [sent-11, score-0.142]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('image', 0.397), ('images', 0.323), ('dataset', 0.231), ('label', 0.222), ('web', 0.186), ('labels', 0.162), ('predicting', 0.152), ('alarge', 0.144), ('bipartite', 0.144), ('espgame', 0.144), ('funny', 0.144), ('intriguing', 0.144), ('paired', 0.144), ('consisting', 0.133), ('luis', 0.133), ('metalearning', 0.133), ('red', 0.133), ('characteristic', 0.126), ('rewarding', 0.126), ('variation', 0.12), ('asks', 0.12), ('bandwidth', 0.12), ('implied', 0.12), ('picture', 0.12), ('von', 0.12), ('randomly', 0.111), ('produced', 0.105), ('labeling', 0.102), ('organized', 0.102), ('unique', 0.099), ('varies', 0.099), ('agree', 0.097), ('awhile', 0.097), ('graph', 0.097), ('managed', 0.093), ('minimum', 0.086), ('blind', 0.082), ('potentially', 0.079), ('across', 0.078), ('internet', 0.077), ('datasets', 0.075), ('requires', 0.074), ('running', 0.073), ('average', 0.073), ('require', 0.071), ('likely', 0.071), ('greatly', 0.071), ('difficulty', 0.069), ('multiple', 0.068), ('course', 0.066)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="20-tfidf-1" href="../hunch_net-2005/hunch_net-2005-02-15-ESPgame_and_image_labeling.html">20 hunch net-2005-02-15-ESPgame and image labeling</a></p>
<p>Introduction: Luis von Ahnhas been running theespgamefor awhile now. The espgame provides a
picture to two randomly paired people across the web, and asks them to agree
on a label. It hasn't managed to label the web yet, but it has produced alarge
datasetof (image, label) pairs. I organized the dataset so you couldexplore
the implied bipartite graph(requires much bandwidth).Relative to other image
datasets, this one is quite large--67000 images, 358,000 labels (average of
5/image with variation from 1 to 19), and 22,000 unique labels (one every 3
images). The dataset is also very 'natural', consisting of images spidered
from the internet. The multiple label characteristic is intriguing because
'learning to learn' and metalearning techniques may be applicable. The
'natural' quality means that this dataset varies greatly in difficulty from
easy (predicting "red") to hard (predicting "funny") and potentially more
rewarding to tackle.The open problem here is, of course, to make an internet
image labelin</p><p>2 0.18997753 <a title="20-tfidf-2" href="../hunch_net-2006/hunch_net-2006-02-27-The_Peekaboom_Dataset.html">159 hunch net-2006-02-27-The Peekaboom Dataset</a></p>
<p>Introduction: Luis von Ahn'sPeekaboom projecthas yieldeddata(830MB).Peekaboom is the second
attempt (afterEspgame) to produce a dataset which is useful for learning to
solve vision problems based on voluntary game play. As a second attempt, it is
meant to address all of the shortcomings of the first attempt. In
particular:The locations of specific objects are provided by the data.The data
collection is far more complete and extensive.The data consists of:The source
images. (1 file per image, just short of 60K images.)The in-game events. (1
file per image, in a lispy syntax.)A description of the event language.There
is a great deal of very specific and relevant data here so the hope that this
will help solve vision problems seems quite reasonable.</p><p>3 0.18916063 <a title="20-tfidf-3" href="../hunch_net-2005/hunch_net-2005-08-01-Peekaboom.html">99 hunch net-2005-08-01-Peekaboom</a></p>
<p>Introduction: Luishas releasedPeekabooma successor toESPgame(game site). The purpose of the
game is similar--using the actions of people playing a game to gather data
helpful in solving AI.Peekaboom gathers more detailed, and perhaps more
useful, data about vision. For ESPgame, the byproduct of the game was mutually
agreed upon labels for common images. For Peekaboom, the location of the
subimage generating the label is revealed by the game as well. Given knowledge
about what portion of the image is related to a label it may be more feasible
learn to recognize the appropriate parts.There isn't a dataset yet available
for this game as there is for ESPgame, but hopefully a significant number of
people will play and we'll have one to work wtih soon.</p><p>4 0.10682716 <a title="20-tfidf-4" href="../hunch_net-2006/hunch_net-2006-12-06-The_Spam_Problem.html">223 hunch net-2006-12-06-The Spam Problem</a></p>
<p>Introduction: TheNew York Timeshas an article on thegrowth of spam. Interesting facts
include: 9/10 of all email is spam, spam source identification is nearly
useless due to botnet spam senders, and image based spam (emails which consist
of an image only) are on the growth.Estimates of the cost of spam are almost
certainly far to low, because they do not account for the cost in time lost by
people.The image based spam which is currently penetrating many filters should
be catchable with a more sophisticated application of machine learning
technology. For the spam I see, the rendered images come in only a few
formats, which would be easy to recognize via a support vector machine (with
RBF kernel), neural network, or even nearest-neighbor architecture. The
mechanics of setting this up to run efficiently is the only real challenge.
This is the next step in the spam war.The response to this system is to make
the image based spam even more random. We should (essentially) expect to
seeCaptchaspam, and our</p><p>5 0.096779056 <a title="20-tfidf-5" href="../hunch_net-2010/hunch_net-2010-04-14-MLcomp%3A_a_website_for_objectively_comparing_ML_algorithms.html">393 hunch net-2010-04-14-MLcomp: a website for objectively comparing ML algorithms</a></p>
<p>Introduction: Much of the success and popularity of machine learning has been driven by its
practical impact. Of course, the evaluation of empirical work is an integral
part of the field. But are the existing mechanisms for evaluating algorithms
and comparing results good enough? We (PercyandJake) believe there are
currently a number of shortcomings:Incomplete Disclosure:You read a paper that
proposes Algorithm A which is shown to outperform SVMs on two datasets.
Great.  But what about on other datasets?  How sensitive is this result?
What about compute time - does the algorithm take two seconds on a laptop or
two weeks on a 100-node cluster?Lack of Standardization:Algorithm A beats
Algorithm B on one version of a dataset.  Algorithm B beats Algorithm A on
another version yet uses slightly different preprocessing.  Though doing a
head-on comparison would be ideal, it would be tedious since the programs
probably use different dataset formats and have a large array of options.  And
what if we wanted t</p><p>6 0.093295023 <a title="20-tfidf-6" href="../hunch_net-2005/hunch_net-2005-08-11-Why_Manifold-Based_Dimension_Reduction_Techniques%3F.html">102 hunch net-2005-08-11-Why Manifold-Based Dimension Reduction Techniques?</a></p>
<p>7 0.093218878 <a title="20-tfidf-7" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>8 0.091526605 <a title="20-tfidf-8" href="../hunch_net-2007/hunch_net-2007-12-12-Workshop_Summary%26%238212%3BPrinciples_of_Learning_Problem_Design.html">277 hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</a></p>
<p>9 0.081583351 <a title="20-tfidf-9" href="../hunch_net-2005/hunch_net-2005-05-12-Math_on_the_Web.html">70 hunch net-2005-05-12-Math on the Web</a></p>
<p>10 0.079290882 <a title="20-tfidf-10" href="../hunch_net-2008/hunch_net-2008-04-30-Concerns_about_the_Large_Scale_Learning_Challenge.html">300 hunch net-2008-04-30-Concerns about the Large Scale Learning Challenge</a></p>
<p>11 0.076970175 <a title="20-tfidf-11" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>12 0.076314047 <a title="20-tfidf-12" href="../hunch_net-2005/hunch_net-2005-03-22-Active_learning.html">45 hunch net-2005-03-22-Active learning</a></p>
<p>13 0.070179977 <a title="20-tfidf-13" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>14 0.064739071 <a title="20-tfidf-14" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>15 0.064669631 <a title="20-tfidf-15" href="../hunch_net-2009/hunch_net-2009-04-21-Interesting_Presentations_at_Snowbird.html">349 hunch net-2009-04-21-Interesting Presentations at Snowbird</a></p>
<p>16 0.063868485 <a title="20-tfidf-16" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>17 0.063386872 <a title="20-tfidf-17" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>18 0.061140589 <a title="20-tfidf-18" href="../hunch_net-2011/hunch_net-2011-04-18-A_paper_not_at_Snowbird.html">431 hunch net-2011-04-18-A paper not at Snowbird</a></p>
<p>19 0.0590543 <a title="20-tfidf-19" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>20 0.058892243 <a title="20-tfidf-20" href="../hunch_net-2007/hunch_net-2007-06-24-Interesting_Papers_at_ICML_2007.html">251 hunch net-2007-06-24-Interesting Papers at ICML 2007</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.131), (1, -0.013), (2, 0.021), (3, -0.017), (4, -0.02), (5, 0.029), (6, 0.015), (7, 0.014), (8, 0.041), (9, 0.057), (10, -0.044), (11, 0.025), (12, 0.021), (13, 0.037), (14, 0.047), (15, -0.126), (16, 0.018), (17, 0.023), (18, 0.013), (19, -0.02), (20, 0.063), (21, 0.107), (22, 0.05), (23, -0.063), (24, -0.001), (25, 0.008), (26, -0.086), (27, -0.109), (28, -0.099), (29, -0.013), (30, 0.075), (31, 0.017), (32, 0.095), (33, -0.029), (34, -0.142), (35, 0.06), (36, 0.097), (37, 0.053), (38, -0.066), (39, 0.02), (40, -0.083), (41, 0.01), (42, -0.078), (43, 0.031), (44, 0.018), (45, -0.067), (46, -0.008), (47, 0.173), (48, 0.043), (49, -0.02)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97026414 <a title="20-lsi-1" href="../hunch_net-2005/hunch_net-2005-02-15-ESPgame_and_image_labeling.html">20 hunch net-2005-02-15-ESPgame and image labeling</a></p>
<p>Introduction: Luis von Ahnhas been running theespgamefor awhile now. The espgame provides a
picture to two randomly paired people across the web, and asks them to agree
on a label. It hasn't managed to label the web yet, but it has produced alarge
datasetof (image, label) pairs. I organized the dataset so you couldexplore
the implied bipartite graph(requires much bandwidth).Relative to other image
datasets, this one is quite large--67000 images, 358,000 labels (average of
5/image with variation from 1 to 19), and 22,000 unique labels (one every 3
images). The dataset is also very 'natural', consisting of images spidered
from the internet. The multiple label characteristic is intriguing because
'learning to learn' and metalearning techniques may be applicable. The
'natural' quality means that this dataset varies greatly in difficulty from
easy (predicting "red") to hard (predicting "funny") and potentially more
rewarding to tackle.The open problem here is, of course, to make an internet
image labelin</p><p>2 0.83919472 <a title="20-lsi-2" href="../hunch_net-2005/hunch_net-2005-08-01-Peekaboom.html">99 hunch net-2005-08-01-Peekaboom</a></p>
<p>Introduction: Luishas releasedPeekabooma successor toESPgame(game site). The purpose of the
game is similar--using the actions of people playing a game to gather data
helpful in solving AI.Peekaboom gathers more detailed, and perhaps more
useful, data about vision. For ESPgame, the byproduct of the game was mutually
agreed upon labels for common images. For Peekaboom, the location of the
subimage generating the label is revealed by the game as well. Given knowledge
about what portion of the image is related to a label it may be more feasible
learn to recognize the appropriate parts.There isn't a dataset yet available
for this game as there is for ESPgame, but hopefully a significant number of
people will play and we'll have one to work wtih soon.</p><p>3 0.76077247 <a title="20-lsi-3" href="../hunch_net-2006/hunch_net-2006-02-27-The_Peekaboom_Dataset.html">159 hunch net-2006-02-27-The Peekaboom Dataset</a></p>
<p>Introduction: Luis von Ahn'sPeekaboom projecthas yieldeddata(830MB).Peekaboom is the second
attempt (afterEspgame) to produce a dataset which is useful for learning to
solve vision problems based on voluntary game play. As a second attempt, it is
meant to address all of the shortcomings of the first attempt. In
particular:The locations of specific objects are provided by the data.The data
collection is far more complete and extensive.The data consists of:The source
images. (1 file per image, just short of 60K images.)The in-game events. (1
file per image, in a lispy syntax.)A description of the event language.There
is a great deal of very specific and relevant data here so the hope that this
will help solve vision problems seems quite reasonable.</p><p>4 0.57330811 <a title="20-lsi-4" href="../hunch_net-2006/hunch_net-2006-12-06-The_Spam_Problem.html">223 hunch net-2006-12-06-The Spam Problem</a></p>
<p>Introduction: TheNew York Timeshas an article on thegrowth of spam. Interesting facts
include: 9/10 of all email is spam, spam source identification is nearly
useless due to botnet spam senders, and image based spam (emails which consist
of an image only) are on the growth.Estimates of the cost of spam are almost
certainly far to low, because they do not account for the cost in time lost by
people.The image based spam which is currently penetrating many filters should
be catchable with a more sophisticated application of machine learning
technology. For the spam I see, the rendered images come in only a few
formats, which would be easy to recognize via a support vector machine (with
RBF kernel), neural network, or even nearest-neighbor architecture. The
mechanics of setting this up to run efficiently is the only real challenge.
This is the next step in the spam war.The response to this system is to make
the image based spam even more random. We should (essentially) expect to
seeCaptchaspam, and our</p><p>5 0.51204914 <a title="20-lsi-5" href="../hunch_net-2005/hunch_net-2005-08-11-Why_Manifold-Based_Dimension_Reduction_Techniques%3F.html">102 hunch net-2005-08-11-Why Manifold-Based Dimension Reduction Techniques?</a></p>
<p>Introduction: Manifold based dimension-reduction algorithms share the following general
outline.Given: a metricd()and a set of pointsSConstruct a graph with a point
in every node and every edge connecting to the node of one of thek-nearest
neighbors. Associate with the edge a weight which is the distance between the
points in the connected nodes.Digest the graph. This might include computing
the shortest path between all points or figuring out how to linearly
interpolate the point from it's neighbors.Find a set of points in a low
dimensional space which preserve the digested properties.Examples include LLE,
Isomap (which I worked on), Hessian-LLE, SDE, and many others. The hope with
these algorithms is that they can recover the low dimensional structure of
point sets in high dimensional spaces. Many of them can be shown to work in
interesting ways producing various compelling pictures.Despite doing some
early work in this direction, I suffer from a motivational problem: Why do we
want to recover the</p><p>6 0.44757852 <a title="20-lsi-6" href="../hunch_net-2009/hunch_net-2009-04-21-Interesting_Presentations_at_Snowbird.html">349 hunch net-2009-04-21-Interesting Presentations at Snowbird</a></p>
<p>7 0.42102757 <a title="20-lsi-7" href="../hunch_net-2006/hunch_net-2006-05-21-NIPS_paper_evaluation_criteria.html">180 hunch net-2006-05-21-NIPS paper evaluation criteria</a></p>
<p>8 0.40223491 <a title="20-lsi-8" href="../hunch_net-2005/hunch_net-2005-09-20-Workshop_Proposal%3A_Atomic_Learning.html">114 hunch net-2005-09-20-Workshop Proposal: Atomic Learning</a></p>
<p>9 0.40036672 <a title="20-lsi-9" href="../hunch_net-2006/hunch_net-2006-02-07-Pittsburgh_Mind_Reading_Competition.html">155 hunch net-2006-02-07-Pittsburgh Mind Reading Competition</a></p>
<p>10 0.39556032 <a title="20-lsi-10" href="../hunch_net-2011/hunch_net-2011-10-03-Monday_announcements.html">446 hunch net-2011-10-03-Monday announcements</a></p>
<p>11 0.39495042 <a title="20-lsi-11" href="../hunch_net-2005/hunch_net-2005-06-13-Wikis_for_Summer_Schools_and_Workshops.html">81 hunch net-2005-06-13-Wikis for Summer Schools and Workshops</a></p>
<p>12 0.390926 <a title="20-lsi-12" href="../hunch_net-2006/hunch_net-2006-01-30-Should_the_Input_Representation_be_a_Vector%3F.html">152 hunch net-2006-01-30-Should the Input Representation be a Vector?</a></p>
<p>13 0.38482192 <a title="20-lsi-13" href="../hunch_net-2006/hunch_net-2006-03-05-%26%238220%3BStructural%26%238221%3B_Learning.html">161 hunch net-2006-03-05-&#8220;Structural&#8221; Learning</a></p>
<p>14 0.38296151 <a title="20-lsi-14" href="../hunch_net-2007/hunch_net-2007-12-12-Workshop_Summary%26%238212%3BPrinciples_of_Learning_Problem_Design.html">277 hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</a></p>
<p>15 0.37949955 <a title="20-lsi-15" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>16 0.37302044 <a title="20-lsi-16" href="../hunch_net-2005/hunch_net-2005-05-29-Maximum_Margin_Mismatch%3F.html">77 hunch net-2005-05-29-Maximum Margin Mismatch?</a></p>
<p>17 0.35527095 <a title="20-lsi-17" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>18 0.34866983 <a title="20-lsi-18" href="../hunch_net-2006/hunch_net-2006-11-20-Context_and_the_calculation_misperception.html">218 hunch net-2006-11-20-Context and the calculation misperception</a></p>
<p>19 0.33645815 <a title="20-lsi-19" href="../hunch_net-2009/hunch_net-2009-12-09-Future_Publication_Models_%40_NIPS.html">382 hunch net-2009-12-09-Future Publication Models @ NIPS</a></p>
<p>20 0.33220124 <a title="20-lsi-20" href="../hunch_net-2010/hunch_net-2010-04-14-MLcomp%3A_a_website_for_objectively_comparing_ML_algorithms.html">393 hunch net-2010-04-14-MLcomp: a website for objectively comparing ML algorithms</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(6, 0.117), (18, 0.017), (39, 0.016), (42, 0.203), (63, 0.106), (68, 0.03), (69, 0.046), (74, 0.182), (77, 0.013), (82, 0.011), (96, 0.142)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.89393979 <a title="20-lda-1" href="../hunch_net-2005/hunch_net-2005-02-15-ESPgame_and_image_labeling.html">20 hunch net-2005-02-15-ESPgame and image labeling</a></p>
<p>Introduction: Luis von Ahnhas been running theespgamefor awhile now. The espgame provides a
picture to two randomly paired people across the web, and asks them to agree
on a label. It hasn't managed to label the web yet, but it has produced alarge
datasetof (image, label) pairs. I organized the dataset so you couldexplore
the implied bipartite graph(requires much bandwidth).Relative to other image
datasets, this one is quite large--67000 images, 358,000 labels (average of
5/image with variation from 1 to 19), and 22,000 unique labels (one every 3
images). The dataset is also very 'natural', consisting of images spidered
from the internet. The multiple label characteristic is intriguing because
'learning to learn' and metalearning techniques may be applicable. The
'natural' quality means that this dataset varies greatly in difficulty from
easy (predicting "red") to hard (predicting "funny") and potentially more
rewarding to tackle.The open problem here is, of course, to make an internet
image labelin</p><p>2 0.85775918 <a title="20-lda-2" href="../hunch_net-2006/hunch_net-2006-03-05-%26%238220%3BStructural%26%238221%3B_Learning.html">161 hunch net-2006-03-05-&#8220;Structural&#8221; Learning</a></p>
<p>Introduction: Fernando Pereirapointed outAndo andZhang'spaperon "structural" learning.
Structural learning is multitask learning on subproblems created from
unlabeled data.The basic idea is to take a look at the unlabeled data and
create many supervised problems. On text data, which they test on, these
subproblems might be of the form "Given surrounding words predict the middle
word". The hope here is that successfully predicting on these subproblems is
relevant to the prediction of your core problem.In the long run, the precise
mechanism used (essentially, linear predictors with parameters tied by a
common matrix) and the precise problems formed may not be critical. What seems
critical is that the hope is realized: the technique provides a significant
edge in practice.Some basic questions about this approach are:Are there
effective automated mechanisms for creating the subproblems?Is it necessary to
use a shared representation?</p><p>3 0.79558074 <a title="20-lda-3" href="../hunch_net-2008/hunch_net-2008-01-06-Research_Political_Issues.html">282 hunch net-2008-01-06-Research Political Issues</a></p>
<p>Introduction: I've avoided discussing politics here, although not for lack of interest. The
problem with discussing politics is that it's customary for people to say much
based upon little information. Nevertheless, politics can have a substantial
impact on science (and we might hope for the vice-versa). It's primary
election time in the United States, so the topic is timely, although the
issues are not.There are several policy decisions which substantially effect
development of science and technology in the US.EducationThe US has great
contrasts in education. The top universities are very good places, yet the
grade school education system produces mediocre results. For me, the contrast
between apublic educationandCaltechwas bracing. For many others attending
Caltech, it clearly was not. Upgrading the k-12 education system in the US is
a long-standing chronic problem which I know relatively little about. My own
experience is that a basic attitude of "no child unrealized" is better than
"no child lef</p><p>4 0.79444629 <a title="20-lda-4" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>Introduction: The papers which interested me most atICMLandCOLT2010 were:Thomas
Walsh,Kaushik Subramanian,Michael LittmanandCarlos DiukGeneralizing
Apprenticeship Learning across Hypothesis Classes. This paper formalizes and
provides algorithms with guarantees for mixed-mode apprenticeship and
traditional reinforcement learning algorithms, allowing RL algorithms that
perform better than for either setting alone.István SzitaandCsaba
SzepesváriModel-based reinforcement learning with nearly tight exploration
complexity bounds. This paper andanotherrepresent the frontier of best-known
algorithm for Reinforcement Learning in a Markov Decision Process.James
MartensDeep learning via Hessian-free optimization. About a new not-quite-
online second order gradient algorithm for learning deep functional
structures. Potentially this is very powerful because while people have often
talked about end-to-end learning, it has rarely worked in practice.Chrisoph
Sawade,Niels Landwehr,Steffen Bickel. andTobias SchefferA</p><p>5 0.78408253 <a title="20-lda-5" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>Introduction: Few would mistake the process of academic paper review for a fair process, but
sometimes the unfairness seems particularly striking. This is most easily seen
by comparison:PaperBanditronOffset TreeNotesProblem ScopeMulticlass problems
where only the loss of one choice can be probed.Strictly greater: Cost
sensitive multiclass problems where only the loss of one choice can be
probed.Often generalizations don't matter. That's not the case here, since
every plausible application I've thought of involves loss functions
substantially different from 0/1.What's newAnalysis and ExperimentsAlgorithm,
Analysis, and ExperimentsAs far as I know, the essence of the more general
problem was first stated and analyzed with theEXP4 algorithm (page 16)(1998).
It's also the time horizon 1 simplification of the Reinforcement Learning
setting for therandom trajectory method (page 15)(2002). The Banditron
algorithm itself is functionally identical toOne-Step RL with Traces (page
122)(2003) inBianca's thesis</p><p>6 0.78253329 <a title="20-lda-6" href="../hunch_net-2008/hunch_net-2008-02-10-Complexity_Illness.html">288 hunch net-2008-02-10-Complexity Illness</a></p>
<p>7 0.7821489 <a title="20-lda-7" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>8 0.78158587 <a title="20-lda-8" href="../hunch_net-2005/hunch_net-2005-06-22-Languages__of_Learning.html">84 hunch net-2005-06-22-Languages  of Learning</a></p>
<p>9 0.78045732 <a title="20-lda-9" href="../hunch_net-2006/hunch_net-2006-09-12-Incentive_Compatible_Reviewing.html">207 hunch net-2006-09-12-Incentive Compatible Reviewing</a></p>
<p>10 0.77930373 <a title="20-lda-10" href="../hunch_net-2008/hunch_net-2008-04-21-The_Science_2.0_article.html">296 hunch net-2008-04-21-The Science 2.0 article</a></p>
<p>11 0.77763307 <a title="20-lda-11" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>12 0.77716345 <a title="20-lda-12" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>13 0.77665639 <a title="20-lda-13" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>14 0.77628011 <a title="20-lda-14" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>15 0.77436525 <a title="20-lda-15" href="../hunch_net-2010/hunch_net-2010-08-20-The_Workshop_on_Cores%2C_Clusters%2C_and_Clouds.html">404 hunch net-2010-08-20-The Workshop on Cores, Clusters, and Clouds</a></p>
<p>16 0.76920068 <a title="20-lda-16" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>17 0.76886839 <a title="20-lda-17" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>18 0.76358187 <a title="20-lda-18" href="../hunch_net-2005/hunch_net-2005-02-17-Learning_Research_Programs.html">21 hunch net-2005-02-17-Learning Research Programs</a></p>
<p>19 0.76321858 <a title="20-lda-19" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>20 0.75960928 <a title="20-lda-20" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
