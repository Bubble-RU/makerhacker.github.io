<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>114 hunch net-2005-09-20-Workshop Proposal: Atomic Learning</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-114" href="#">hunch_net-2005-114</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>114 hunch net-2005-09-20-Workshop Proposal: Atomic Learning</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-114-html" href="http://hunch.net/?p=123">html</a></p><p>Introduction: This is a proposal for a workshop.  It may or may not happen depending on the level of interest.  If you are interested, feel free to indicate so (by email or comments).
 
Description: 
Assume(*) that any system for solving large difficult learning problems must decompose into repeated use of basic elements (i.e. atoms).  There are many basic questions which remain:
  
  What are the viable basic elements? 
  What makes a basic element viable? 
  What are the viable principles for the composition of these basic elements? 
  What are the viable principles for learning in such systems? 
  What problems can this approach handle? 
  
Hal Daume adds:
  
 Can composition of atoms be (semi-) automatically constructed[?] 
 When atoms are constructed through reductions, is there some notion of the “naturalness” of the created leaning problems? 
 Other than Markov fields/graphical models/Bayes nets, is there a good language for representing atoms and their compositions? 
  
The answer to these a</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 It may or may not happen depending on the level of interest. [sent-2, score-0.209]
</p><p>2 If you are interested, feel free to indicate so (by email or comments). [sent-3, score-0.167]
</p><p>3 Description:  Assume(*) that any system for solving large difficult learning problems must decompose into repeated use of basic elements (i. [sent-4, score-0.903]
</p><p>4 There are many basic questions which remain:      What are the viable basic elements? [sent-7, score-0.835]
</p><p>5 What are the viable principles for the composition of these basic elements? [sent-9, score-0.962]
</p><p>6 What are the viable principles for learning in such systems? [sent-10, score-0.559]
</p><p>7 Hal Daume adds:     Can composition of atoms be (semi-) automatically constructed[? [sent-12, score-0.759]
</p><p>8 ]   When atoms are constructed through reductions, is there some notion of the “naturalness” of the created leaning problems? [sent-13, score-0.722]
</p><p>9 Other than Markov fields/graphical models/Bayes nets, is there a good language for representing atoms and their compositions? [sent-14, score-0.579]
</p><p>10 The answer to these and related questions remain unclear to me. [sent-15, score-0.318]
</p><p>11 A workshop gives us a chance to pool what we have learned from some very different approaches to tackling this same basic goal. [sent-16, score-0.577]
</p><p>12 (*) As a general principle, it’s very difficult to conceive of any system for solving any large problem which does not decompose. [sent-17, score-0.238]
</p><p>13 Plan Sketch:     A two day workshop with unhurried presentations and discussion seems appropriate, especially given the diversity of approaches. [sent-18, score-0.383]
</p><p>14 The above two points suggest having a workshop on a {Friday, Saturday} or {Saturday, Sunday} at TTI-Chicago. [sent-20, score-0.205]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('atoms', 0.47), ('viable', 0.363), ('saturday', 0.235), ('composition', 0.218), ('elements', 0.202), ('principles', 0.196), ('constructed', 0.188), ('basic', 0.185), ('remain', 0.149), ('workshop', 0.137), ('naturalness', 0.117), ('sunday', 0.117), ('representing', 0.109), ('diversity', 0.109), ('decompose', 0.103), ('tackling', 0.103), ('questions', 0.102), ('indicate', 0.098), ('nets', 0.098), ('repeated', 0.098), ('sketch', 0.094), ('adds', 0.094), ('daume', 0.094), ('friday', 0.094), ('proposal', 0.091), ('solving', 0.087), ('pool', 0.085), ('principle', 0.083), ('element', 0.079), ('problems', 0.077), ('system', 0.076), ('presentations', 0.076), ('hal', 0.076), ('difficult', 0.075), ('assume', 0.072), ('description', 0.072), ('handle', 0.071), ('automatically', 0.071), ('plan', 0.071), ('may', 0.07), ('email', 0.069), ('depending', 0.069), ('markov', 0.069), ('suggest', 0.068), ('unclear', 0.067), ('gives', 0.067), ('appropriate', 0.065), ('created', 0.064), ('comments', 0.061), ('day', 0.061)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="114-tfidf-1" href="../hunch_net-2005/hunch_net-2005-09-20-Workshop_Proposal%3A_Atomic_Learning.html">114 hunch net-2005-09-20-Workshop Proposal: Atomic Learning</a></p>
<p>Introduction: This is a proposal for a workshop.  It may or may not happen depending on the level of interest.  If you are interested, feel free to indicate so (by email or comments).
 
Description: 
Assume(*) that any system for solving large difficult learning problems must decompose into repeated use of basic elements (i.e. atoms).  There are many basic questions which remain:
  
  What are the viable basic elements? 
  What makes a basic element viable? 
  What are the viable principles for the composition of these basic elements? 
  What are the viable principles for learning in such systems? 
  What problems can this approach handle? 
  
Hal Daume adds:
  
 Can composition of atoms be (semi-) automatically constructed[?] 
 When atoms are constructed through reductions, is there some notion of the “naturalness” of the created leaning problems? 
 Other than Markov fields/graphical models/Bayes nets, is there a good language for representing atoms and their compositions? 
  
The answer to these a</p><p>2 0.11438545 <a title="114-tfidf-2" href="../hunch_net-2006/hunch_net-2006-03-24-NLPers.html">166 hunch net-2006-03-24-NLPers</a></p>
<p>Introduction: Hal Daume  has started the  NLPers  blog to discuss learning for language problems.</p><p>3 0.092401378 <a title="114-tfidf-3" href="../hunch_net-2007/hunch_net-2007-12-12-Workshop_Summary%26%238212%3BPrinciples_of_Learning_Problem_Design.html">277 hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</a></p>
<p>Introduction: This is a summary of the  workshop on Learning Problem Design  which  Alina  and I ran at  NIPS  this year.
 
The first question many people have is “What is learning problem design?”  This workshop is about admitting that solving learning problems does not start with labeled data, but rather somewhere before.  When humans are hired to produce labels, this is usually not a serious problem because you can tell them precisely what semantics you want the labels to have, and we can fix some set of features in advance.  However, when other methods are used this becomes more problematic.  This focus is important for Machine Learning because there are very large quantities of data which are not labeled by a hired human.
 
The title of the workshop was a bit ambitious, because a workshop is not long enough to synthesize a diversity of approaches into a coherent set of principles.  For me, the posters at the end of the workshop were quite helpful in getting approaches to gel.
 
Here are some an</p><p>4 0.086370572 <a title="114-tfidf-4" href="../hunch_net-2007/hunch_net-2007-02-22-Create_Your_Own_ICML_Workshop.html">234 hunch net-2007-02-22-Create Your Own ICML Workshop</a></p>
<p>Introduction: As usual  ICML 2007  will be hosting a  workshop program  to be held this year on June 24th. The success of the program depends on having researchers like you propose interesting workshop topics and then organize the workshops. I’d like to encourage all of you to consider sending a workshop proposal. The proposal deadline has been extended to March 5. See the workshop web-site for details. 
 
Organizing a workshop is a unique way to gather an international group of researchers together to focus for an entire day on a topic of your choosing. I’ve always found that the cost of organizing a workshop is not so large, and very low compared to the benefits. The topic and format of a workshop are limited only by your imagination (and the attractiveness to potential participants) and need not follow the usual model of a mini-conference on a particular ML sub-area.  Hope to see some interesting proposals rolling in.</p><p>5 0.086085066 <a title="114-tfidf-5" href="../hunch_net-2005/hunch_net-2005-03-24-The_Role_of_Workshops.html">46 hunch net-2005-03-24-The Role of Workshops</a></p>
<p>Introduction: A good workshop is often far more interesting than the papers at a conference.  This happens because a workshop has a much tighter focus than a conference.  Since you choose the workshops fitting your interest, the increased relevance can greatly enhance the level of your interest and attention.  Roughly speaking, a workshop program consists of elements related to a subject of your interest.  The main conference program consists of elements related to someoneâ&euro;&trade;s interest (which is rarely your own).  Workshops are more about doing research while conferences are more about presenting research.  
 
Several conferences have associated workshop programs, some with deadlines due shortly.
  
 
  ICML workshops  
 Due April 1 
 
 
  IJCAI workshops  
 Deadlines Vary 
 
 
 KDD workshops 
 Not yet finalized 
 
  
Anyone going to these conferences should examine the workshops and see if any are of interest.  (If none are, then maybe you should organize one next year.)</p><p>6 0.083765492 <a title="114-tfidf-6" href="../hunch_net-2008/hunch_net-2008-12-07-A_NIPS_paper.html">330 hunch net-2008-12-07-A NIPS paper</a></p>
<p>7 0.078657828 <a title="114-tfidf-7" href="../hunch_net-2005/hunch_net-2005-06-08-Question%3A_%26%238220%3BWhen_is_the_right_time_to_insert_the_loss_function%3F%26%238221%3B.html">79 hunch net-2005-06-08-Question: &#8220;When is the right time to insert the loss function?&#8221;</a></p>
<p>8 0.074380204 <a title="114-tfidf-8" href="../hunch_net-2005/hunch_net-2005-06-22-Languages__of_Learning.html">84 hunch net-2005-06-22-Languages  of Learning</a></p>
<p>9 0.072221413 <a title="114-tfidf-9" href="../hunch_net-2007/hunch_net-2007-10-14-NIPS_workshp%3A_Learning_Problem_Design.html">265 hunch net-2007-10-14-NIPS workshp: Learning Problem Design</a></p>
<p>10 0.06878072 <a title="114-tfidf-10" href="../hunch_net-2005/hunch_net-2005-09-06-A_link.html">108 hunch net-2005-09-06-A link</a></p>
<p>11 0.067419283 <a title="114-tfidf-11" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>12 0.063213274 <a title="114-tfidf-12" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>13 0.063044138 <a title="114-tfidf-13" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>14 0.062183402 <a title="114-tfidf-14" href="../hunch_net-2010/hunch_net-2010-06-20-2010_ICML_discussion_site.html">401 hunch net-2010-06-20-2010 ICML discussion site</a></p>
<p>15 0.061201952 <a title="114-tfidf-15" href="../hunch_net-2011/hunch_net-2011-02-17-What_does_Watson_mean%3F.html">424 hunch net-2011-02-17-What does Watson mean?</a></p>
<p>16 0.060634598 <a title="114-tfidf-16" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>17 0.05937615 <a title="114-tfidf-17" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>18 0.05914994 <a title="114-tfidf-18" href="../hunch_net-2009/hunch_net-2009-11-29-AI_Safety.html">380 hunch net-2009-11-29-AI Safety</a></p>
<p>19 0.058919191 <a title="114-tfidf-19" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>20 0.058473893 <a title="114-tfidf-20" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.135), (1, 0.001), (2, -0.067), (3, -0.006), (4, 0.004), (5, 0.043), (6, 0.041), (7, 0.001), (8, 0.009), (9, -0.011), (10, -0.045), (11, -0.095), (12, -0.046), (13, 0.071), (14, -0.018), (15, -0.044), (16, -0.025), (17, 0.013), (18, -0.069), (19, -0.004), (20, -0.076), (21, -0.032), (22, -0.029), (23, -0.012), (24, 0.042), (25, 0.055), (26, 0.013), (27, -0.061), (28, -0.006), (29, 0.073), (30, 0.006), (31, -0.006), (32, 0.026), (33, 0.012), (34, -0.004), (35, 0.025), (36, -0.052), (37, 0.013), (38, -0.037), (39, -0.019), (40, 0.02), (41, -0.033), (42, -0.058), (43, 0.016), (44, 0.075), (45, -0.026), (46, -0.04), (47, -0.005), (48, 0.058), (49, 0.002)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96474934 <a title="114-lsi-1" href="../hunch_net-2005/hunch_net-2005-09-20-Workshop_Proposal%3A_Atomic_Learning.html">114 hunch net-2005-09-20-Workshop Proposal: Atomic Learning</a></p>
<p>Introduction: This is a proposal for a workshop.  It may or may not happen depending on the level of interest.  If you are interested, feel free to indicate so (by email or comments).
 
Description: 
Assume(*) that any system for solving large difficult learning problems must decompose into repeated use of basic elements (i.e. atoms).  There are many basic questions which remain:
  
  What are the viable basic elements? 
  What makes a basic element viable? 
  What are the viable principles for the composition of these basic elements? 
  What are the viable principles for learning in such systems? 
  What problems can this approach handle? 
  
Hal Daume adds:
  
 Can composition of atoms be (semi-) automatically constructed[?] 
 When atoms are constructed through reductions, is there some notion of the “naturalness” of the created leaning problems? 
 Other than Markov fields/graphical models/Bayes nets, is there a good language for representing atoms and their compositions? 
  
The answer to these a</p><p>2 0.67781222 <a title="114-lsi-2" href="../hunch_net-2007/hunch_net-2007-10-14-NIPS_workshp%3A_Learning_Problem_Design.html">265 hunch net-2007-10-14-NIPS workshp: Learning Problem Design</a></p>
<p>Introduction: Alina  and I are organizing a workshop on  Learning Problem Design  at  NIPS .  
 
 What is learning problem design?  Itâ&euro;&trade;s about being clever in creating learning problems from otherwise unlabeled data.  Read the webpage above for examples.
 
 I want to participate!  Email us before Nov. 1 with a description of what you want to talk about.</p><p>3 0.63243723 <a title="114-lsi-3" href="../hunch_net-2007/hunch_net-2007-12-12-Workshop_Summary%26%238212%3BPrinciples_of_Learning_Problem_Design.html">277 hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</a></p>
<p>Introduction: This is a summary of the  workshop on Learning Problem Design  which  Alina  and I ran at  NIPS  this year.
 
The first question many people have is “What is learning problem design?”  This workshop is about admitting that solving learning problems does not start with labeled data, but rather somewhere before.  When humans are hired to produce labels, this is usually not a serious problem because you can tell them precisely what semantics you want the labels to have, and we can fix some set of features in advance.  However, when other methods are used this becomes more problematic.  This focus is important for Machine Learning because there are very large quantities of data which are not labeled by a hired human.
 
The title of the workshop was a bit ambitious, because a workshop is not long enough to synthesize a diversity of approaches into a coherent set of principles.  For me, the posters at the end of the workshop were quite helpful in getting approaches to gel.
 
Here are some an</p><p>4 0.56854057 <a title="114-lsi-4" href="../hunch_net-2007/hunch_net-2007-02-22-Create_Your_Own_ICML_Workshop.html">234 hunch net-2007-02-22-Create Your Own ICML Workshop</a></p>
<p>Introduction: As usual  ICML 2007  will be hosting a  workshop program  to be held this year on June 24th. The success of the program depends on having researchers like you propose interesting workshop topics and then organize the workshops. I’d like to encourage all of you to consider sending a workshop proposal. The proposal deadline has been extended to March 5. See the workshop web-site for details. 
 
Organizing a workshop is a unique way to gather an international group of researchers together to focus for an entire day on a topic of your choosing. I’ve always found that the cost of organizing a workshop is not so large, and very low compared to the benefits. The topic and format of a workshop are limited only by your imagination (and the attractiveness to potential participants) and need not follow the usual model of a mini-conference on a particular ML sub-area.  Hope to see some interesting proposals rolling in.</p><p>5 0.55204958 <a title="114-lsi-5" href="../hunch_net-2012/hunch_net-2012-03-13-The_Submodularity_workshop_and_Lucca_Professorship.html">459 hunch net-2012-03-13-The Submodularity workshop and Lucca Professorship</a></p>
<p>Introduction: Nina  points out the  Submodularity Workshop   March 19-20  next week at  Georgia Tech .  Many people want to make Submodularity the new Convexity in machine learning, and it certainly seems worth exploring.
 
 Sara Olson  also points out a  tenured faculty position  at  IMT Lucca  with a deadline of  May 15th .  Lucca happens to be the ancestral home of 1/4 of my heritage</p><p>6 0.54915601 <a title="114-lsi-6" href="../hunch_net-2009/hunch_net-2009-04-21-Interesting_Presentations_at_Snowbird.html">349 hunch net-2009-04-21-Interesting Presentations at Snowbird</a></p>
<p>7 0.53853935 <a title="114-lsi-7" href="../hunch_net-2005/hunch_net-2005-05-10-Learning_Reductions_are_Reductionist.html">68 hunch net-2005-05-10-Learning Reductions are Reductionist</a></p>
<p>8 0.52715778 <a title="114-lsi-8" href="../hunch_net-2008/hunch_net-2008-07-04-More_Presentation_Preparation.html">307 hunch net-2008-07-04-More Presentation Preparation</a></p>
<p>9 0.50988477 <a title="114-lsi-9" href="../hunch_net-2010/hunch_net-2010-08-20-The_Workshop_on_Cores%2C_Clusters%2C_and_Clouds.html">404 hunch net-2010-08-20-The Workshop on Cores, Clusters, and Clouds</a></p>
<p>10 0.50702572 <a title="114-lsi-10" href="../hunch_net-2009/hunch_net-2009-12-07-Vowpal_Wabbit_version_4.0%2C_and_a_NIPS_heresy.html">381 hunch net-2009-12-07-Vowpal Wabbit version 4.0, and a NIPS heresy</a></p>
<p>11 0.49865207 <a title="114-lsi-11" href="../hunch_net-2013/hunch_net-2013-12-01-NIPS_tutorials_and_Vowpal_Wabbit_7.4.html">492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</a></p>
<p>12 0.49427518 <a title="114-lsi-12" href="../hunch_net-2005/hunch_net-2005-06-17-Reopening_RL-%3EClassification.html">82 hunch net-2005-06-17-Reopening RL->Classification</a></p>
<p>13 0.48734123 <a title="114-lsi-13" href="../hunch_net-2011/hunch_net-2011-04-18-A_paper_not_at_Snowbird.html">431 hunch net-2011-04-18-A paper not at Snowbird</a></p>
<p>14 0.46942452 <a title="114-lsi-14" href="../hunch_net-2005/hunch_net-2005-08-18-SVM_Adaptability.html">103 hunch net-2005-08-18-SVM Adaptability</a></p>
<p>15 0.46874616 <a title="114-lsi-15" href="../hunch_net-2005/hunch_net-2005-03-08-Fast_Physics_for_Learning.html">37 hunch net-2005-03-08-Fast Physics for Learning</a></p>
<p>16 0.46836358 <a title="114-lsi-16" href="../hunch_net-2006/hunch_net-2006-07-25-Upcoming_conference.html">198 hunch net-2006-07-25-Upcoming conference</a></p>
<p>17 0.46590963 <a title="114-lsi-17" href="../hunch_net-2009/hunch_net-2009-06-01-Multitask_Poisoning.html">358 hunch net-2009-06-01-Multitask Poisoning</a></p>
<p>18 0.46155715 <a title="114-lsi-18" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>19 0.46116266 <a title="114-lsi-19" href="../hunch_net-2009/hunch_net-2009-06-03-Functionally_defined_Nonlinear_Dynamic_Models.html">359 hunch net-2009-06-03-Functionally defined Nonlinear Dynamic Models</a></p>
<p>20 0.46079671 <a title="114-lsi-20" href="../hunch_net-2006/hunch_net-2006-07-08-Some_recent_papers.html">192 hunch net-2006-07-08-Some recent papers</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(22, 0.461), (27, 0.217), (38, 0.029), (53, 0.056), (55, 0.03), (94, 0.015), (95, 0.078)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.88291919 <a title="114-lda-1" href="../hunch_net-2005/hunch_net-2005-09-20-Workshop_Proposal%3A_Atomic_Learning.html">114 hunch net-2005-09-20-Workshop Proposal: Atomic Learning</a></p>
<p>Introduction: This is a proposal for a workshop.  It may or may not happen depending on the level of interest.  If you are interested, feel free to indicate so (by email or comments).
 
Description: 
Assume(*) that any system for solving large difficult learning problems must decompose into repeated use of basic elements (i.e. atoms).  There are many basic questions which remain:
  
  What are the viable basic elements? 
  What makes a basic element viable? 
  What are the viable principles for the composition of these basic elements? 
  What are the viable principles for learning in such systems? 
  What problems can this approach handle? 
  
Hal Daume adds:
  
 Can composition of atoms be (semi-) automatically constructed[?] 
 When atoms are constructed through reductions, is there some notion of the “naturalness” of the created leaning problems? 
 Other than Markov fields/graphical models/Bayes nets, is there a good language for representing atoms and their compositions? 
  
The answer to these a</p><p>2 0.73710978 <a title="114-lda-2" href="../hunch_net-2005/hunch_net-2005-09-19-NIPS_Workshops.html">113 hunch net-2005-09-19-NIPS Workshops</a></p>
<p>Introduction: Attendance at the  NIPS workshops  is highly recommended for both research and learning.   Unfortunately, there does not yet appear to be a public list of workshops. However, I found the following workshop webpages of interest:
  
  Machine Learning in Finance  
  Learning to Rank  
  Foundations of Active Learning  
  Machine Learning Based Robotics in Unstructured Environments  
  
There are  many  more workshops.  In fact, there are so many that it is not plausible anyone can attend every workshop they are interested in.  Maybe in future years the organizers can spread them out over more days to reduce overlap. 
 
Many of these workshops are accepting presentation proposals (due mid-October).</p><p>3 0.71586674 <a title="114-lda-3" href="../hunch_net-2010/hunch_net-2010-09-17-New_York_Area_Machine_Learning_Events.html">410 hunch net-2010-09-17-New York Area Machine Learning Events</a></p>
<p>Introduction: On Sept 21, there is another  machine learning meetup  where I’ll be speaking.  Although the topic is contextual bandits, I think of it as “the future of machine learning”.  In particular, it’s all about how to learn in an interactive environment, such as for ad display, trading, news recommendation, etc…
 
On Sept 24, abstracts for the  New York Machine Learning Symposium  are due.  This is the largest Machine Learning event in the area, so it’s a great way to have a conversation with other people.
 
On Oct 22, the NY ML Symposium actually happens.  This year, we are expanding the spotlights, and trying to have more time for posters.  In addition, we have a strong set of invited speakers:  David Blei ,  Sanjoy Dasgupta ,  Tommi Jaakkola , and  Yann LeCun .  After the meeting, a late  hackNY  related event is planned where students and startups can meet.
 
I’d also like to point out the related  CS/Econ symposium  as I have interests there as well.</p><p>4 0.62253636 <a title="114-lda-4" href="../hunch_net-2009/hunch_net-2009-06-01-Multitask_Poisoning.html">358 hunch net-2009-06-01-Multitask Poisoning</a></p>
<p>Introduction: There are many ways that interesting research gets done.  For example it’s common at a conference for someone to discuss a problem with a partial solution, and for someone else to know how to solve a piece of it, resulting in a paper.  In some sense,  these are the easiest results we can achieve, so we should ask: Can all research be this easy?  
 
The answer is certainly no for fields where research inherently requires  experimentation to discover how the real world works.  However, mathematics, including parts of physics, computer science, statistics, etc… which are effectively mathematics don’t require experimentation. In effect, a paper can be simply a pure expression of thinking.  Can all mathematical-style research be this easy?
 
What’s going on here is research-by-communication.  Someone knows something, someone knows something else, and as soon as someone knows both things, a problem is solved.  The interesting thing about research-by-communication is that it is becoming radic</p><p>5 0.55471939 <a title="114-lda-5" href="../hunch_net-2005/hunch_net-2005-06-08-Question%3A_%26%238220%3BWhen_is_the_right_time_to_insert_the_loss_function%3F%26%238221%3B.html">79 hunch net-2005-06-08-Question: &#8220;When is the right time to insert the loss function?&#8221;</a></p>
<p>Introduction: Hal  asks   a very good question: “When is the right time to insert the loss function?”  In particular, should it be used at testing time or at training time?
 
When the world imposes a loss on us, the standard Bayesian recipe is to predict the (conditional) probability of each possibility and then choose the possibility which minimizes the expected loss.  In contrast, as the  confusion  over “loss = money lost” or “loss = the thing you optimize” might indicate, many people ignore the Bayesian approach and simply optimize their loss (or a close proxy for their loss) over the representation on the training set.
 
The best answer I can give is “it’s unclear, but I prefer optimizing the loss at training time”.  My experience is that optimizing the loss in the most direct manner possible typically yields best performance.  This question is related to a basic principle which both  Yann LeCun (applied) and  Vladimir Vapnik (theoretical) advocate: “solve the simplest prediction problem that s</p><p>6 0.45144323 <a title="114-lda-6" href="../hunch_net-2013/hunch_net-2013-06-10-The_Large_Scale_Learning_class_notes.html">483 hunch net-2013-06-10-The Large Scale Learning class notes</a></p>
<p>7 0.44996712 <a title="114-lda-7" href="../hunch_net-2005/hunch_net-2005-02-03-Learning_Theory%2C_by_assumption.html">12 hunch net-2005-02-03-Learning Theory, by assumption</a></p>
<p>8 0.44984347 <a title="114-lda-8" href="../hunch_net-2013/hunch_net-2013-01-07-NYU_Large_Scale_Machine_Learning_Class.html">478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</a></p>
<p>9 0.44469115 <a title="114-lda-9" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>10 0.44129002 <a title="114-lda-10" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<p>11 0.44014895 <a title="114-lda-11" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>12 0.43957117 <a title="114-lda-12" href="../hunch_net-2006/hunch_net-2006-07-11-New_Models.html">194 hunch net-2006-07-11-New Models</a></p>
<p>13 0.43956298 <a title="114-lda-13" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>14 0.43881726 <a title="114-lda-14" href="../hunch_net-2007/hunch_net-2007-02-02-Thoughts_regarding_%26%238220%3BIs_machine_learning_different_from_statistics%3F%26%238221%3B.html">230 hunch net-2007-02-02-Thoughts regarding &#8220;Is machine learning different from statistics?&#8221;</a></p>
<p>15 0.43786997 <a title="114-lda-15" href="../hunch_net-2005/hunch_net-2005-02-01-Watchword%3A_Loss.html">9 hunch net-2005-02-01-Watchword: Loss</a></p>
<p>16 0.4371025 <a title="114-lda-16" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<p>17 0.4370791 <a title="114-lda-17" href="../hunch_net-2006/hunch_net-2006-07-13-Regression_vs._Classification_as_a_Primitive.html">196 hunch net-2006-07-13-Regression vs. Classification as a Primitive</a></p>
<p>18 0.43676978 <a title="114-lda-18" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>19 0.43652076 <a title="114-lda-19" href="../hunch_net-2005/hunch_net-2005-02-01-NIPS%3A_Online_Bayes.html">8 hunch net-2005-02-01-NIPS: Online Bayes</a></p>
<p>20 0.43614033 <a title="114-lda-20" href="../hunch_net-2005/hunch_net-2005-03-15-The_State_of_Tight_Bounds.html">41 hunch net-2005-03-15-The State of Tight Bounds</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
