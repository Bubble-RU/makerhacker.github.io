<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>120 hunch net-2005-10-10-Predictive Search is Coming</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-120" href="#">hunch_net-2005-120</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>120 hunch net-2005-10-10-Predictive Search is Coming</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-120-html" href="http://hunch.net/?p=131">html</a></p><p>Introduction: "Search" is the other branch of AI research which has been succesful. Concrete
examples includeDeep Bluewhich beat the world chess champion andChinookthe
champion checkers program. A set of core search techniques exist including A*,
alpha-beta pruning, and others that can be applied to any of many different
search problems.Given this, it may be surprising to learn that there has been
relatively little succesful work on combining prediction and search. Given
also that humans typically solve search problems using a number of predictive
heuristics to narrow in on a solution, we might be surprised again. However,
the big successful search-based systems have typically not used "smart" search
algorithms. Insteady they have optimized for very fast search. This is not for
lack of tryingâ&euro;Ś many people have tried to synthesize search and prediction to
various degrees of success. For example,Knightcapachieves good-but-not-stellar
chess playing performance, andTD-gammonhas achieved near-optimal Bac</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('search', 0.441), ('execution', 0.305), ('computers', 0.221), ('humans', 0.142), ('speed', 0.135), ('exemplified', 0.131), ('successes', 0.131), ('distribution', 0.128), ('exposing', 0.116), ('champion', 0.116), ('parallelizable', 0.116), ('strengths', 0.116), ('chess', 0.108), ('instruction', 0.108), ('processors', 0.102), ('classification', 0.1), ('sequential', 0.09), ('advantage', 0.089), ('term', 0.085), ('memory', 0.084)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="120-tfidf-1" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>Introduction: "Search" is the other branch of AI research which has been succesful. Concrete
examples includeDeep Bluewhich beat the world chess champion andChinookthe
champion checkers program. A set of core search techniques exist including A*,
alpha-beta pruning, and others that can be applied to any of many different
search problems.Given this, it may be surprising to learn that there has been
relatively little succesful work on combining prediction and search. Given
also that humans typically solve search problems using a number of predictive
heuristics to narrow in on a solution, we might be surprised again. However,
the big successful search-based systems have typically not used "smart" search
algorithms. Insteady they have optimized for very fast search. This is not for
lack of tryingâ&euro;Ś many people have tried to synthesize search and prediction to
various degrees of success. For example,Knightcapachieves good-but-not-stellar
chess playing performance, andTD-gammonhas achieved near-optimal Bac</p><p>2 0.17292199 <a title="120-tfidf-2" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<p>Introduction: I want to comment on the "Bing copies Google" discussionhere,here, andhere,
because there are data-related issues which the general public may not
understand, and some of the framing seems substantially misleading to me.As a
not-distant-outsider, let me mention the sources of bias I may have. I work
atYahoo!, which has started usingBing. This might predispose me towards Bing,
but on the other hand I'm still at Yahoo!, and have been usingLinuxexclusively
as an OS for many years, including even a couple minor kernel patches. And,on
the gripping hand, I've spent quite a bit of time thinking about the
basicprinciples of incorporating user feedback in machine learning. Also note,
this post is not related to official Yahoo! policy, it's just my personal
view.The issueGoogle engineers inserted synthetic responses to synthetic
queries on google.com, then executed the synthetic searches on google.com
using Internet Explorer with the Bing toolbar and later noticed some synthetic
responses from B</p><p>3 0.17136765 <a title="120-tfidf-3" href="../hunch_net-2006/hunch_net-2006-02-11-Yahoo%26%238217%3Bs_Learning_Problems..html">156 hunch net-2006-02-11-Yahoo&#8217;s Learning Problems.</a></p>
<p>Introduction: I just visitedYahoo Researchwhich has several fundamental learning problems
near to (or beyond) the set of problems we know how to solve well. Here are 3
of them.RankingThis is the canonical problem of all search engines. It is made
extra difficult for several reasons.There is relatively little "good"
supervised learning data and a great deal of data with some signal (such as
click through rates).The learning must occur in a partially adversarial
environment. Many people very actively attempt to place themselves at the top
ofrankings.It is not even quite clear whether the problem should be posed as
'ranking' or as 'regression' which is then used to produce
aranking.Collaborative filteringYahoo has a large number of recommendation
systems for music, movies, etcâ&euro;Ś In these sorts of systems, users specify how
they liked a set of things, and then the system can (hopefully) find some more
examples of things they might likeby reasoning across multiple such
sets.Exploration with Generalization</p><p>4 0.16815276 <a title="120-tfidf-4" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>Introduction: One view of machine learning is that it's about how to program computers to
predict well. This suggests a broader research program centered around the
more pervasive goal of simply predicting well.There are many distinct strands
of this broader research program which are only partially unified. Here are
the ones that I know of:Learning Theory. Learning theory focuses on several
topics related to the dynamics and process of prediction. Convergence bounds
like theVC boundgive an intellectual foundation to many learning algorithms.
Online learning algorithms likeWeighted Majorityprovide an alternate purely
game theoretic foundation for learning.Boosting algorithmsyield algorithms for
purifying prediction abiliity.Reduction algorithmsprovide means for changing
esoteric problems into well known ones.Machine Learning. A great deal of
experience has accumulated in practical algorithm design from a mixture of
paradigms, including bayesian, biological, optimization, and
theoretical.Mechanism De</p><p>5 0.16623095 <a title="120-tfidf-5" href="../hunch_net-2006/hunch_net-2006-08-03-AOL%26%238217%3Bs_data_drop.html">200 hunch net-2006-08-03-AOL&#8217;s data drop</a></p>
<p>Introduction: AOL hasreleasedseveral large search engine related datasets. This looks like a
pretty impressive data release, and it is a big opportunity for people
everywhere to worry about search engine related learning problems, if they
want.</p><p>6 0.14597203 <a title="120-tfidf-6" href="../hunch_net-2011/hunch_net-2011-02-17-What_does_Watson_mean%3F.html">424 hunch net-2011-02-17-What does Watson mean?</a></p>
<p>7 0.13854656 <a title="120-tfidf-7" href="../hunch_net-2007/hunch_net-2007-01-26-Parallel_Machine_Learning_Problems.html">229 hunch net-2007-01-26-Parallel Machine Learning Problems</a></p>
<p>8 0.12022847 <a title="120-tfidf-8" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<p>9 0.11509494 <a title="120-tfidf-9" href="../hunch_net-2010/hunch_net-2010-12-26-NIPS_2010.html">420 hunch net-2010-12-26-NIPS 2010</a></p>
<p>10 0.11496329 <a title="120-tfidf-10" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>11 0.1101978 <a title="120-tfidf-11" href="../hunch_net-2005/hunch_net-2005-02-02-Kolmogorov_Complexity_and_Googling.html">10 hunch net-2005-02-02-Kolmogorov Complexity and Googling</a></p>
<p>12 0.10850815 <a title="120-tfidf-12" href="../hunch_net-2007/hunch_net-2007-10-24-Contextual_Bandits.html">269 hunch net-2007-10-24-Contextual Bandits</a></p>
<p>13 0.1078784 <a title="120-tfidf-13" href="../hunch_net-2005/hunch_net-2005-04-21-Dynamic_Programming_Generalizations_and_Their_Use.html">58 hunch net-2005-04-21-Dynamic Programming Generalizations and Their Use</a></p>
<p>14 0.10679332 <a title="120-tfidf-14" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>15 0.10374425 <a title="120-tfidf-15" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>16 0.10355823 <a title="120-tfidf-16" href="../hunch_net-2006/hunch_net-2006-07-06-Branch_Prediction_Competition.html">190 hunch net-2006-07-06-Branch Prediction Competition</a></p>
<p>17 0.10249696 <a title="120-tfidf-17" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>18 0.10101689 <a title="120-tfidf-18" href="../hunch_net-2008/hunch_net-2008-04-30-Concerns_about_the_Large_Scale_Learning_Challenge.html">300 hunch net-2008-04-30-Concerns about the Large Scale Learning Challenge</a></p>
<p>19 0.10068902 <a title="120-tfidf-19" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>20 0.096726768 <a title="120-tfidf-20" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.271), (1, -0.096), (2, 0.056), (3, -0.043), (4, 0.021), (5, -0.016), (6, -0.003), (7, 0.059), (8, 0.1), (9, 0.061), (10, -0.071), (11, 0.07), (12, 0.041), (13, -0.032), (14, 0.147), (15, 0.016), (16, -0.035), (17, 0.01), (18, -0.008), (19, -0.103), (20, -0.095), (21, -0.044), (22, -0.086), (23, -0.072), (24, -0.036), (25, -0.015), (26, 0.018), (27, -0.018), (28, 0.14), (29, -0.089), (30, -0.054), (31, -0.021), (32, -0.024), (33, 0.047), (34, 0.02), (35, 0.087), (36, -0.045), (37, -0.043), (38, -0.014), (39, 0.057), (40, 0.023), (41, -0.007), (42, -0.038), (43, -0.067), (44, -0.067), (45, 0.029), (46, 0.019), (47, 0.104), (48, -0.023), (49, 0.042)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94809991 <a title="120-lsi-1" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>Introduction: "Search" is the other branch of AI research which has been succesful. Concrete
examples includeDeep Bluewhich beat the world chess champion andChinookthe
champion checkers program. A set of core search techniques exist including A*,
alpha-beta pruning, and others that can be applied to any of many different
search problems.Given this, it may be surprising to learn that there has been
relatively little succesful work on combining prediction and search. Given
also that humans typically solve search problems using a number of predictive
heuristics to narrow in on a solution, we might be surprised again. However,
the big successful search-based systems have typically not used "smart" search
algorithms. Insteady they have optimized for very fast search. This is not for
lack of tryingâ&euro;Ś many people have tried to synthesize search and prediction to
various degrees of success. For example,Knightcapachieves good-but-not-stellar
chess playing performance, andTD-gammonhas achieved near-optimal Bac</p><p>2 0.77386785 <a title="120-lsi-2" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<p>Introduction: I want to comment on the "Bing copies Google" discussionhere,here, andhere,
because there are data-related issues which the general public may not
understand, and some of the framing seems substantially misleading to me.As a
not-distant-outsider, let me mention the sources of bias I may have. I work
atYahoo!, which has started usingBing. This might predispose me towards Bing,
but on the other hand I'm still at Yahoo!, and have been usingLinuxexclusively
as an OS for many years, including even a couple minor kernel patches. And,on
the gripping hand, I've spent quite a bit of time thinking about the
basicprinciples of incorporating user feedback in machine learning. Also note,
this post is not related to official Yahoo! policy, it's just my personal
view.The issueGoogle engineers inserted synthetic responses to synthetic
queries on google.com, then executed the synthetic searches on google.com
using Internet Explorer with the Bing toolbar and later noticed some synthetic
responses from B</p><p>3 0.67968667 <a title="120-lsi-3" href="../hunch_net-2006/hunch_net-2006-08-03-AOL%26%238217%3Bs_data_drop.html">200 hunch net-2006-08-03-AOL&#8217;s data drop</a></p>
<p>Introduction: AOL hasreleasedseveral large search engine related datasets. This looks like a
pretty impressive data release, and it is a big opportunity for people
everywhere to worry about search engine related learning problems, if they
want.</p><p>4 0.64689589 <a title="120-lsi-4" href="../hunch_net-2006/hunch_net-2006-02-11-Yahoo%26%238217%3Bs_Learning_Problems..html">156 hunch net-2006-02-11-Yahoo&#8217;s Learning Problems.</a></p>
<p>Introduction: I just visitedYahoo Researchwhich has several fundamental learning problems
near to (or beyond) the set of problems we know how to solve well. Here are 3
of them.RankingThis is the canonical problem of all search engines. It is made
extra difficult for several reasons.There is relatively little "good"
supervised learning data and a great deal of data with some signal (such as
click through rates).The learning must occur in a partially adversarial
environment. Many people very actively attempt to place themselves at the top
ofrankings.It is not even quite clear whether the problem should be posed as
'ranking' or as 'regression' which is then used to produce
aranking.Collaborative filteringYahoo has a large number of recommendation
systems for music, movies, etcâ&euro;Ś In these sorts of systems, users specify how
they liked a set of things, and then the system can (hopefully) find some more
examples of things they might likeby reasoning across multiple such
sets.Exploration with Generalization</p><p>5 0.61333191 <a title="120-lsi-5" href="../hunch_net-2011/hunch_net-2011-02-17-What_does_Watson_mean%3F.html">424 hunch net-2011-02-17-What does Watson mean?</a></p>
<p>Introduction: Watsonconvincingly beat the best championJeopardy!players. The apparent
significance of this varies hugely, depending on your background knowledge
about the related machine learning, NLP, and search technology. For a random
person, this might seem evidence of serious machine intelligence, while for
people working on the system itself, it probably seems like a reasonably good
assemblage of existing technologies with several twists to make the entire
system work.Above all, I think we should congratulate the people who managed
to put together and execute this project--many years of effort by a diverse
set of highly skilled people were needed to make this happen. In academia,
it's pretty difficult for one professor to assemble that quantity of talent,
and in industry it's rarely the case that such a capable group has both a
worthwhile project and the support needed to pursue something like this for
several years before success.Alinainvited me to the Jeopardy watching party
atIBM, which was</p><p>6 0.60644704 <a title="120-lsi-6" href="../hunch_net-2006/hunch_net-2006-05-08-Big_machine_learning.html">178 hunch net-2006-05-08-Big machine learning</a></p>
<p>7 0.60514492 <a title="120-lsi-7" href="../hunch_net-2007/hunch_net-2007-08-25-The_Privacy_Problem.html">260 hunch net-2007-08-25-The Privacy Problem</a></p>
<p>8 0.59740615 <a title="120-lsi-8" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>9 0.58300096 <a title="120-lsi-9" href="../hunch_net-2005/hunch_net-2005-04-21-Dynamic_Programming_Generalizations_and_Their_Use.html">58 hunch net-2005-04-21-Dynamic Programming Generalizations and Their Use</a></p>
<p>10 0.54193091 <a title="120-lsi-10" href="../hunch_net-2005/hunch_net-2005-11-05-The_design_of_a_computing_cluster.html">128 hunch net-2005-11-05-The design of a computing cluster</a></p>
<p>11 0.53182715 <a title="120-lsi-11" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<p>12 0.52601594 <a title="120-lsi-12" href="../hunch_net-2009/hunch_net-2009-08-03-Carbon_in_Computer_Science_Research.html">366 hunch net-2009-08-03-Carbon in Computer Science Research</a></p>
<p>13 0.51462269 <a title="120-lsi-13" href="../hunch_net-2010/hunch_net-2010-12-26-NIPS_2010.html">420 hunch net-2010-12-26-NIPS 2010</a></p>
<p>14 0.50574493 <a title="120-lsi-14" href="../hunch_net-2006/hunch_net-2006-02-02-Introspectionism_as_a_Disease.html">153 hunch net-2006-02-02-Introspectionism as a Disease</a></p>
<p>15 0.50432611 <a title="120-lsi-15" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>16 0.50051236 <a title="120-lsi-16" href="../hunch_net-2006/hunch_net-2006-04-09-Progress_in_Machine_Translation.html">171 hunch net-2006-04-09-Progress in Machine Translation</a></p>
<p>17 0.49959743 <a title="120-lsi-17" href="../hunch_net-2005/hunch_net-2005-10-20-Machine_Learning_in_the_News.html">125 hunch net-2005-10-20-Machine Learning in the News</a></p>
<p>18 0.49600416 <a title="120-lsi-18" href="../hunch_net-2007/hunch_net-2007-09-16-Optimizing_Machine_Learning_Programs.html">262 hunch net-2007-09-16-Optimizing Machine Learning Programs</a></p>
<p>19 0.49191868 <a title="120-lsi-19" href="../hunch_net-2008/hunch_net-2008-01-28-Sufficient_Computation.html">287 hunch net-2008-01-28-Sufficient Computation</a></p>
<p>20 0.49075705 <a title="120-lsi-20" href="../hunch_net-2007/hunch_net-2007-10-24-Contextual_Bandits.html">269 hunch net-2007-10-24-Contextual Bandits</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(35, 0.025), (42, 0.233), (45, 0.089), (53, 0.027), (61, 0.016), (68, 0.066), (69, 0.036), (74, 0.133), (88, 0.05), (92, 0.204), (95, 0.041)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.94810504 <a title="120-lda-1" href="../hunch_net-2007/hunch_net-2007-06-19-How_is_Compressed_Sensing_going_to_change_Machine_Learning_%3F.html">248 hunch net-2007-06-19-How is Compressed Sensing going to change Machine Learning ?</a></p>
<p>Introduction: Compressed Sensing(CS) is a new framework developed byEmmanuel Candes,Terry
TaoandDavid Donoho. To summarize, if you acquire a signal in some basis that
is incoherent with the basis in which you know the signal to be sparse in, it
is very likely you will be able to reconstruct the signal from these
incoherent projections.Terry Tao, the recentFields medalist, does a very nice
job at explaining the frameworkhere. He goes further in the theory description
in thispostwhere he mentions the central issue of the Uniform Uncertainty
Principle. It so happens that random projections are on average incoherent,
within the UUP meaning, with most known basis (sines, polynomials, splines,
wavelets, curvelets …) and are therefore an ideal basis for Compressed
Sensing. [ For more in-depth information on the subject, the Rice group has
done a very good job at providing a central library of papers relevant to the
growing subject:http://www.dsp.ece.rice.edu/cs/]The Machine Learning community
has looked at</p><p>2 0.93892229 <a title="120-lda-2" href="../hunch_net-2005/hunch_net-2005-05-12-Math_on_the_Web.html">70 hunch net-2005-05-12-Math on the Web</a></p>
<p>Introduction: Andrej Bauer has setup aMathematics and ComputationBlog. As a first step he
has tried to address the persistent and annoying problem of math on the web.
As a basic tool for precisely stating and transfering understanding of
technical subjects, mathematics is very necessary. Despite this necessity,
every mechanism for expressing mathematics on the web seems unnaturally
clumsy. Here are some of the methods and their drawbacks:MathMLThis was
supposed to be the answer, but it has two severe drawbacks: "Internet
Explorer" doesn't read it and the language is an example of push-XML-to-the-
limit which no one would ever consider writing in. (In contrast, html is easy
to write in.) It's also very annoying that math fonts must be installed
independent of the browser, even for mozilla based browsers.Create inline
images. This has several big drawbacks: font size is fixed for all viewers,
you can't cut & paste inside the images, and you can't hyperlink from (say)
symbol to definition.Math Worldis</p><p>3 0.93171966 <a title="120-lda-3" href="../hunch_net-2005/hunch_net-2005-12-14-More_NIPS_Papers_II.html">140 hunch net-2005-12-14-More NIPS Papers II</a></p>
<p>Introduction: I thought this was a very good NIPS with many excellent papers. The following
are a few NIPS papers which I liked and I hope to study more carefully when I
get the chance. The list is not exhaustive and in no particular
order…Preconditioner Approximations for Probabilistic Graphical
Models.Pradeeep Ravikumar and John Lafferty.I thought the use of
preconditioner methods from solving linear systems in the context of
approximate inference was novel and interesting. The results look good and I'd
like to understand the limitations.Rodeo: Sparse nonparametric regression in
high dimensions.John Lafferty and Larry Wasserman.A very interesting approach
to feature selection in nonparametric regression from a frequentist framework.
The use of lengthscale variables in each dimension reminds me a lot of
'Automatic Relevance Determination' in Gaussian process regression -- it would
be interesting to compare Rodeo to ARD in GPs.Interpolating between types and
tokens by estimating power law generators</p><p>same-blog 4 0.90447032 <a title="120-lda-4" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>Introduction: "Search" is the other branch of AI research which has been succesful. Concrete
examples includeDeep Bluewhich beat the world chess champion andChinookthe
champion checkers program. A set of core search techniques exist including A*,
alpha-beta pruning, and others that can be applied to any of many different
search problems.Given this, it may be surprising to learn that there has been
relatively little succesful work on combining prediction and search. Given
also that humans typically solve search problems using a number of predictive
heuristics to narrow in on a solution, we might be surprised again. However,
the big successful search-based systems have typically not used "smart" search
algorithms. Insteady they have optimized for very fast search. This is not for
lack of tryingâ&euro;Ś many people have tried to synthesize search and prediction to
various degrees of success. For example,Knightcapachieves good-but-not-stellar
chess playing performance, andTD-gammonhas achieved near-optimal Bac</p><p>5 0.83386815 <a title="120-lda-5" href="../hunch_net-2005/hunch_net-2005-03-22-Active_learning.html">45 hunch net-2005-03-22-Active learning</a></p>
<p>Introduction: Often, unlabeled data is easy to come by but labels are expensive. For
instance, if you're building a speech recognizer, it's easy enough to get raw
speech samples -- just walk around with a microphone -- but labeling even one
of these samples is a tedious process in which a human must examine the speech
signal and carefully segment it into phonemes. In the field of active
learning, the goal is as usual to construct an accurate classifier, but the
labels of the data points are initially hidden and there is a charge for each
label you want revealed. The hope is that by intelligent adaptive querying,
you can get away with significantly fewer labels than you would need in a
regular supervised learning framework.Here's an example. Suppose the data lie
on the real line, and the classifiers are simple thresholding functions, H =
{hw}:hw(x) = 1 if x > w, and 0 otherwise.VC theory tells us that if the
underlying distribution P can be classified perfectly by some hypothesis in H
(called thereal</p><p>6 0.80624509 <a title="120-lda-6" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>7 0.78208828 <a title="120-lda-7" href="../hunch_net-2005/hunch_net-2005-03-17-Going_all_the_Way%2C_Sometimes.html">42 hunch net-2005-03-17-Going all the Way, Sometimes</a></p>
<p>8 0.78115863 <a title="120-lda-8" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>9 0.77967113 <a title="120-lda-9" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<p>10 0.77912569 <a title="120-lda-10" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>11 0.7787047 <a title="120-lda-11" href="../hunch_net-2005/hunch_net-2005-12-07-Is_the_Google_way_the_way_for_machine_learning%3F.html">136 hunch net-2005-12-07-Is the Google way the way for machine learning?</a></p>
<p>12 0.77857834 <a title="120-lda-12" href="../hunch_net-2005/hunch_net-2005-09-12-Fast_Gradient_Descent.html">111 hunch net-2005-09-12-Fast Gradient Descent</a></p>
<p>13 0.7782172 <a title="120-lda-13" href="../hunch_net-2005/hunch_net-2005-08-11-Why_Manifold-Based_Dimension_Reduction_Techniques%3F.html">102 hunch net-2005-08-11-Why Manifold-Based Dimension Reduction Techniques?</a></p>
<p>14 0.77784401 <a title="120-lda-14" href="../hunch_net-2011/hunch_net-2011-11-26-Giving_Thanks.html">449 hunch net-2011-11-26-Giving Thanks</a></p>
<p>15 0.77695578 <a title="120-lda-15" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>16 0.77664179 <a title="120-lda-16" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>17 0.77576488 <a title="120-lda-17" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>18 0.77553511 <a title="120-lda-18" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>19 0.77530462 <a title="120-lda-19" href="../hunch_net-2010/hunch_net-2010-12-26-NIPS_2010.html">420 hunch net-2010-12-26-NIPS 2010</a></p>
<p>20 0.77444774 <a title="120-lda-20" href="../hunch_net-2009/hunch_net-2009-08-03-Carbon_in_Computer_Science_Research.html">366 hunch net-2009-08-03-Carbon in Computer Science Research</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
