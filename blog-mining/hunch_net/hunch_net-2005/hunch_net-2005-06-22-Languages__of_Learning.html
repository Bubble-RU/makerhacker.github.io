<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>84 hunch net-2005-06-22-Languages  of Learning</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-84" href="#">hunch_net-2005-84</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>84 hunch net-2005-06-22-Languages  of Learning</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-84-html" href="http://hunch.net/?p=90">html</a></p><p>Introduction: A language is a set of primitives which can be combined to succesfully create
complex objects. Languages arise in all sorts of situations: mechanical
construction, martial arts, communication, etcâ&euro;Ś Languages appear to be the key
to succesfully creating complex objects--it is difficult to come up with any
convincing example of a complex object which is not built using some language.
Since languages are so crucial to success, it is interesting to organize
various machine learning research programs by language.The most common
language in machine learning are languages for representing the solution to
machine learning. This includes:Bayes Nets and Graphical ModelsA language for
representing probability distributions. The key concept supporting modularity
is conditional independence.Michael Kearnshas been working on extending this
to game theory.Kernelized Linear ClassifiersA language for representing linear
separators, possibly in a large space. The key form of modularity here is
kerneliza</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 A language is a set of primitives which can be combined to succesfully create complex objects. [sent-1, score-0.795]
</p><p>2 Languages arise in all sorts of situations: mechanical construction, martial arts, communication, etcâ&euro;Ś Languages appear to be the key to succesfully creating complex objects--it is difficult to come up with any convincing example of a complex object which is not built using some language. [sent-2, score-0.842]
</p><p>3 Since languages are so crucial to success, it is interesting to organize various machine learning research programs by language. [sent-3, score-0.695]
</p><p>4 The most common language in machine learning are languages for representing the solution to machine learning. [sent-4, score-1.212]
</p><p>5 This includes:Bayes Nets and Graphical ModelsA language for representing probability distributions. [sent-5, score-0.71]
</p><p>6 Michael Kearnshas been working on extending this to game theory. [sent-7, score-0.133]
</p><p>7 Kernelized Linear ClassifiersA language for representing linear separators, possibly in a large space. [sent-8, score-0.835]
</p><p>8 (Yann LeCungave some very impressive demos at theChicago MLSS. [sent-12, score-0.125]
</p><p>9 The key concept supporting modularity is partitioning the input space. [sent-14, score-0.866]
</p><p>10 In addition there are languages related to various aspects of learning. [sent-16, score-0.664]
</p><p>11 ReductionsA language for translating between varying real-world losses and core learning algorithm optimizations. [sent-17, score-0.55]
</p><p>12 Feature LanguagesExactly how features are specified varies from on learning algorithm to another. [sent-18, score-0.184]
</p><p>13 Several people have been working on languages for features that cope with sparsity or the cross-product nature of databases. [sent-19, score-0.763]
</p><p>14 Data interaction languagesThestatistical query modelof learning algorithms provides a standardized interface between data and learning algorithm. [sent-20, score-0.265]
</p><p>15 These lists surely miss some languages--feel free to point them out below. [sent-21, score-0.12]
</p><p>16 With respect to research "interesting" language-related questions include:For what aspects of learning is a language missing? [sent-22, score-0.52]
</p><p>17 Are any of these languages fundamentally flawed or fundamentally advantageous with respect to another language? [sent-25, score-0.817]
</p><p>18 What are the most easy to use and effective primitives for these languages? [sent-26, score-0.13]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('languages', 0.502), ('language', 0.355), ('representing', 0.355), ('modularity', 0.251), ('key', 0.217), ('supporting', 0.203), ('concept', 0.195), ('succesfully', 0.135), ('primitives', 0.13), ('complex', 0.118), ('aspects', 0.101), ('fundamentally', 0.094), ('mechanical', 0.081), ('modelof', 0.081), ('translating', 0.081), ('separators', 0.075), ('crucial', 0.075), ('extending', 0.075), ('standardized', 0.071), ('anytime', 0.071), ('sparsity', 0.071), ('features', 0.071), ('linear', 0.07), ('demos', 0.068), ('thechicago', 0.068), ('nets', 0.068), ('respect', 0.064), ('lists', 0.063), ('yann', 0.063), ('construction', 0.063), ('flawed', 0.063), ('various', 0.061), ('cope', 0.061), ('arise', 0.061), ('varying', 0.059), ('working', 0.058), ('communication', 0.057), ('miss', 0.057), ('specified', 0.057), ('organize', 0.057), ('query', 0.057), ('convincing', 0.057), ('combined', 0.057), ('impressive', 0.057), ('varies', 0.056), ('interaction', 0.056), ('encountered', 0.055), ('losses', 0.055), ('possibly', 0.055), ('built', 0.055)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="84-tfidf-1" href="../hunch_net-2005/hunch_net-2005-06-22-Languages__of_Learning.html">84 hunch net-2005-06-22-Languages  of Learning</a></p>
<p>Introduction: A language is a set of primitives which can be combined to succesfully create
complex objects. Languages arise in all sorts of situations: mechanical
construction, martial arts, communication, etcâ&euro;Ś Languages appear to be the key
to succesfully creating complex objects--it is difficult to come up with any
convincing example of a complex object which is not built using some language.
Since languages are so crucial to success, it is interesting to organize
various machine learning research programs by language.The most common
language in machine learning are languages for representing the solution to
machine learning. This includes:Bayes Nets and Graphical ModelsA language for
representing probability distributions. The key concept supporting modularity
is conditional independence.Michael Kearnshas been working on extending this
to game theory.Kernelized Linear ClassifiersA language for representing linear
separators, possibly in a large space. The key form of modularity here is
kerneliza</p><p>2 0.3217085 <a title="84-tfidf-2" href="../hunch_net-2006/hunch_net-2006-09-28-Programming_Languages_for_Machine_Learning_Implementations.html">210 hunch net-2006-09-28-Programming Languages for Machine Learning Implementations</a></p>
<p>Introduction: Machine learning algorithms have a much better chance of being widely adopted
if they are implemented in some easy-to-use code. There are several important
concerns associated with machine learning which stress programming languages
on the ease-of-use vs. speed frontier.SpeedThe rate at which data sources are
growing seems to be outstripping the rate at which computational power is
growing, so it is important that we be able to eak out every bit of
computational power. Garbage collected languages (java,ocaml,perlandpython)
often have several issues here.Garbage collection often implies that floating
point numbers are "boxed": every float is represented by a pointer to a float.
Boxing can cause an order of magnitude slowdown because an extra nonlocalized
memory reference is made, and accesses to main memory can are many CPU cycles
long.Garbage collection often implies that considerably more memory is used
than is necessary. This has a variable effect. In some circumstances it
results in</p><p>3 0.13009882 <a title="84-tfidf-3" href="../hunch_net-2007/hunch_net-2007-12-12-Workshop_Summary%26%238212%3BPrinciples_of_Learning_Problem_Design.html">277 hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</a></p>
<p>Introduction: This is a summary of theworkshop on Learning Problem DesignwhichAlinaand I ran
atNIPSthis year.The first question many people have is "What is learning
problem design?" This workshop is about admitting that solving learning
problems does not start with labeled data, but rather somewhere before. When
humans are hired to produce labels, this is usually not a serious problem
because you can tell them precisely what semantics you want the labels to
have, and we can fix some set of features in advance. However, when other
methods are used this becomes more problematic. This focus is important for
Machine Learning because there are very large quantities of data which are not
labeled by a hired human.The title of the workshop was a bit ambitious,
because a workshop is not long enough to synthesize a diversity of approaches
into a coherent set of principles. For me, the posters at the end of the
workshop were quite helpful in getting approaches to gel.Here are some answers
to "where do the lab</p><p>4 0.11226404 <a title="84-tfidf-4" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>Introduction: Muthuinvited me to the workshop onalgorithms in the field, with the goal of
providing a sense of where near-term research should go. When the time came
though, I bargained for a post instead, which provides a chance for many other
people to comment.There are several things I didn't fully understand when I
went to Yahoo! about 5 years ago. I'd like to repeat them as people in
academia may not yet understand them intuitively.Almost all the big impact
algorithms operate in pseudo-linear or better time. Think about caching,
hashing, sorting, filtering, etcâ&euro;Ś and you have a sense of what some of the
most heavily used algorithms are. This matters quite a bit to Machine Learning
research, because people often work with superlinear time algorithms and
languages. Two very common examples of this are graphical models, where
inference is often a superlinear operation--think about then2dependence on the
number of states in aHidden Markov Modeland KernelizedSupport Vector
Machineswhere optimization</p><p>5 0.11200996 <a title="84-tfidf-5" href="../hunch_net-2009/hunch_net-2009-04-23-Jonathan_Chang_at_Slycoder.html">350 hunch net-2009-04-23-Jonathan Chang at Slycoder</a></p>
<p>Introduction: Jonathan Changhas aresearch blogon aspects of machine learning.</p><p>6 0.10927676 <a title="84-tfidf-6" href="../hunch_net-2006/hunch_net-2006-03-24-NLPers.html">166 hunch net-2006-03-24-NLPers</a></p>
<p>7 0.10304868 <a title="84-tfidf-7" href="../hunch_net-2005/hunch_net-2005-05-21-What_is_the_right_form_of_modularity_in_structured_prediction%3F.html">74 hunch net-2005-05-21-What is the right form of modularity in structured prediction?</a></p>
<p>8 0.099235445 <a title="84-tfidf-8" href="../hunch_net-2005/hunch_net-2005-03-04-The_Big_O_and_Constants_in_Learning.html">35 hunch net-2005-03-04-The Big O and Constants in Learning</a></p>
<p>9 0.092752025 <a title="84-tfidf-9" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>10 0.085910074 <a title="84-tfidf-10" href="../hunch_net-2009/hunch_net-2009-01-21-Nearly_all_natural_problems_require_nonlinearity.html">337 hunch net-2009-01-21-Nearly all natural problems require nonlinearity</a></p>
<p>11 0.083583884 <a title="84-tfidf-11" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>12 0.07648854 <a title="84-tfidf-12" href="../hunch_net-2006/hunch_net-2006-01-30-Should_the_Input_Representation_be_a_Vector%3F.html">152 hunch net-2006-01-30-Should the Input Representation be a Vector?</a></p>
<p>13 0.074712127 <a title="84-tfidf-13" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>14 0.074519813 <a title="84-tfidf-14" href="../hunch_net-2005/hunch_net-2005-09-20-Workshop_Proposal%3A_Atomic_Learning.html">114 hunch net-2005-09-20-Workshop Proposal: Atomic Learning</a></p>
<p>15 0.073155448 <a title="84-tfidf-15" href="../hunch_net-2005/hunch_net-2005-04-23-Advantages_and_Disadvantages_of_Bayesian_Learning.html">60 hunch net-2005-04-23-Advantages and Disadvantages of Bayesian Learning</a></p>
<p>16 0.069878921 <a title="84-tfidf-16" href="../hunch_net-2006/hunch_net-2006-10-22-Exemplar_programming.html">215 hunch net-2006-10-22-Exemplar programming</a></p>
<p>17 0.069067478 <a title="84-tfidf-17" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>18 0.064623252 <a title="84-tfidf-18" href="../hunch_net-2005/hunch_net-2005-10-13-Site_tweak.html">122 hunch net-2005-10-13-Site tweak</a></p>
<p>19 0.063032649 <a title="84-tfidf-19" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>20 0.061915636 <a title="84-tfidf-20" href="../hunch_net-2007/hunch_net-2007-01-15-The_Machine_Learning_Department.html">228 hunch net-2007-01-15-The Machine Learning Department</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.14), (1, -0.053), (2, 0.065), (3, -0.013), (4, -0.021), (5, 0.021), (6, -0.084), (7, 0.025), (8, 0.002), (9, 0.08), (10, 0.006), (11, 0.032), (12, 0.055), (13, -0.044), (14, -0.054), (15, -0.014), (16, 0.055), (17, -0.028), (18, 0.07), (19, -0.034), (20, -0.005), (21, -0.025), (22, 0.077), (23, -0.026), (24, 0.001), (25, 0.016), (26, -0.074), (27, 0.008), (28, -0.023), (29, -0.051), (30, 0.159), (31, -0.056), (32, -0.046), (33, -0.12), (34, -0.026), (35, 0.084), (36, -0.193), (37, 0.014), (38, 0.124), (39, -0.033), (40, 0.076), (41, -0.056), (42, -0.097), (43, -0.111), (44, -0.048), (45, -0.013), (46, 0.098), (47, -0.016), (48, 0.136), (49, -0.138)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94784826 <a title="84-lsi-1" href="../hunch_net-2005/hunch_net-2005-06-22-Languages__of_Learning.html">84 hunch net-2005-06-22-Languages  of Learning</a></p>
<p>Introduction: A language is a set of primitives which can be combined to succesfully create
complex objects. Languages arise in all sorts of situations: mechanical
construction, martial arts, communication, etcâ&euro;Ś Languages appear to be the key
to succesfully creating complex objects--it is difficult to come up with any
convincing example of a complex object which is not built using some language.
Since languages are so crucial to success, it is interesting to organize
various machine learning research programs by language.The most common
language in machine learning are languages for representing the solution to
machine learning. This includes:Bayes Nets and Graphical ModelsA language for
representing probability distributions. The key concept supporting modularity
is conditional independence.Michael Kearnshas been working on extending this
to game theory.Kernelized Linear ClassifiersA language for representing linear
separators, possibly in a large space. The key form of modularity here is
kerneliza</p><p>2 0.8246277 <a title="84-lsi-2" href="../hunch_net-2006/hunch_net-2006-09-28-Programming_Languages_for_Machine_Learning_Implementations.html">210 hunch net-2006-09-28-Programming Languages for Machine Learning Implementations</a></p>
<p>Introduction: Machine learning algorithms have a much better chance of being widely adopted
if they are implemented in some easy-to-use code. There are several important
concerns associated with machine learning which stress programming languages
on the ease-of-use vs. speed frontier.SpeedThe rate at which data sources are
growing seems to be outstripping the rate at which computational power is
growing, so it is important that we be able to eak out every bit of
computational power. Garbage collected languages (java,ocaml,perlandpython)
often have several issues here.Garbage collection often implies that floating
point numbers are "boxed": every float is represented by a pointer to a float.
Boxing can cause an order of magnitude slowdown because an extra nonlocalized
memory reference is made, and accesses to main memory can are many CPU cycles
long.Garbage collection often implies that considerably more memory is used
than is necessary. This has a variable effect. In some circumstances it
results in</p><p>3 0.7463184 <a title="84-lsi-3" href="../hunch_net-2006/hunch_net-2006-03-24-NLPers.html">166 hunch net-2006-03-24-NLPers</a></p>
<p>Introduction: Hal Daumehas started theNLPersblog to discuss learning for language problems.</p><p>4 0.56962764 <a title="84-lsi-4" href="../hunch_net-2005/hunch_net-2005-05-12-Math_on_the_Web.html">70 hunch net-2005-05-12-Math on the Web</a></p>
<p>Introduction: Andrej Bauer has setup aMathematics and ComputationBlog. As a first step he
has tried to address the persistent and annoying problem of math on the web.
As a basic tool for precisely stating and transfering understanding of
technical subjects, mathematics is very necessary. Despite this necessity,
every mechanism for expressing mathematics on the web seems unnaturally
clumsy. Here are some of the methods and their drawbacks:MathMLThis was
supposed to be the answer, but it has two severe drawbacks: "Internet
Explorer" doesn't read it and the language is an example of push-XML-to-the-
limit which no one would ever consider writing in. (In contrast, html is easy
to write in.) It's also very annoying that math fonts must be installed
independent of the browser, even for mozilla based browsers.Create inline
images. This has several big drawbacks: font size is fixed for all viewers,
you can't cut & paste inside the images, and you can't hyperlink from (say)
symbol to definition.Math Worldis</p><p>5 0.55081755 <a title="84-lsi-5" href="../hunch_net-2006/hunch_net-2006-10-22-Exemplar_programming.html">215 hunch net-2006-10-22-Exemplar programming</a></p>
<p>Introduction: There are many different abstractions for problem definition and solution.
Here are a few examples:Functional programming: a set of functions are
defined. The composed execution of these functions yields the solution.Linear
programming: a set of constraints and a linear objective function are defined.
An LP solver finds the constrained optimum.Quadratic programming: Like linear
programming, but the language is a little more flexible (and the solution
slower).Convex programming: like quadratic programming, but the language is
more flexible (and the solutions even slower).Dynamic programming: a recursive
definition of the problem is defined and then solved efficiently via caching
tricks.SAT programming: A problem is specified as a satisfiability involving a
conjunction of a disjunction of boolean variables. A general engine attempts
to find a good satisfying assignment. For exampleKautz'sblackboxplanner.These
abstractions have different tradeoffs between ease of use, generality, and the</p><p>6 0.5097608 <a title="84-lsi-6" href="../hunch_net-2007/hunch_net-2007-09-16-Optimizing_Machine_Learning_Programs.html">262 hunch net-2007-09-16-Optimizing Machine Learning Programs</a></p>
<p>7 0.50323081 <a title="84-lsi-7" href="../hunch_net-2009/hunch_net-2009-01-21-Nearly_all_natural_problems_require_nonlinearity.html">337 hunch net-2009-01-21-Nearly all natural problems require nonlinearity</a></p>
<p>8 0.47348291 <a title="84-lsi-8" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>9 0.47071278 <a title="84-lsi-9" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>10 0.4674938 <a title="84-lsi-10" href="../hunch_net-2009/hunch_net-2009-04-23-Jonathan_Chang_at_Slycoder.html">350 hunch net-2009-04-23-Jonathan Chang at Slycoder</a></p>
<p>11 0.46183446 <a title="84-lsi-11" href="../hunch_net-2005/hunch_net-2005-03-04-The_Big_O_and_Constants_in_Learning.html">35 hunch net-2005-03-04-The Big O and Constants in Learning</a></p>
<p>12 0.45461366 <a title="84-lsi-12" href="../hunch_net-2007/hunch_net-2007-12-12-Workshop_Summary%26%238212%3BPrinciples_of_Learning_Problem_Design.html">277 hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</a></p>
<p>13 0.44825691 <a title="84-lsi-13" href="../hunch_net-2006/hunch_net-2006-08-10-Precision_is_not_accuracy.html">202 hunch net-2006-08-10-Precision is not accuracy</a></p>
<p>14 0.43155831 <a title="84-lsi-14" href="../hunch_net-2005/hunch_net-2005-10-13-Site_tweak.html">122 hunch net-2005-10-13-Site tweak</a></p>
<p>15 0.41913551 <a title="84-lsi-15" href="../hunch_net-2006/hunch_net-2006-03-09-Use_of_Notation.html">162 hunch net-2006-03-09-Use of Notation</a></p>
<p>16 0.41459164 <a title="84-lsi-16" href="../hunch_net-2010/hunch_net-2010-10-17-Partha_Niyogi_has_died.html">414 hunch net-2010-10-17-Partha Niyogi has died</a></p>
<p>17 0.40155229 <a title="84-lsi-17" href="../hunch_net-2006/hunch_net-2006-01-30-Should_the_Input_Representation_be_a_Vector%3F.html">152 hunch net-2006-01-30-Should the Input Representation be a Vector?</a></p>
<p>18 0.398536 <a title="84-lsi-18" href="../hunch_net-2005/hunch_net-2005-02-28-Regularization.html">33 hunch net-2005-02-28-Regularization</a></p>
<p>19 0.39550352 <a title="84-lsi-19" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>20 0.39252028 <a title="84-lsi-20" href="../hunch_net-2010/hunch_net-2010-05-10-Aggregation_of_estimators%2C_sparsity_in_high_dimension_and_computational_feasibility.html">398 hunch net-2010-05-10-Aggregation of estimators, sparsity in high dimension and computational feasibility</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(6, 0.398), (42, 0.204), (45, 0.036), (64, 0.01), (68, 0.043), (74, 0.112), (76, 0.021), (82, 0.013), (95, 0.038)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.93713796 <a title="84-lda-1" href="../hunch_net-2005/hunch_net-2005-06-17-Reopening_RL-%3EClassification.html">82 hunch net-2005-06-17-Reopening RL->Classification</a></p>
<p>Introduction: In research, it's often the case that solving a problem helps you realize that
it wasn't the right problem to solve. This is the case for the "reduce RL to
classification" problem with the solution hinted athereand turned into a
paperhere.The essential difficulty is that the method of stating and analyzing
reductions ends up being nonalgorithmic (unlike previous reductions) unless
you work with learning from teleoperated robots asGreg Grudicdoes. The
difficulty here is due to the reduction being dependent on the optimal policy
(which a human teleoperator might simulate, but which is otherwise
unavailable).So, thisproblemis "open" again with the caveat that this time we
want a more algorithmic solution.Whether or not this is feasible at all is
still unclear and evidence in either direction would greatly interest me. A
positive answer might have many practical implications in the long run.</p><p>2 0.93369365 <a title="84-lda-2" href="../hunch_net-2006/hunch_net-2006-12-06-The_Spam_Problem.html">223 hunch net-2006-12-06-The Spam Problem</a></p>
<p>Introduction: TheNew York Timeshas an article on thegrowth of spam. Interesting facts
include: 9/10 of all email is spam, spam source identification is nearly
useless due to botnet spam senders, and image based spam (emails which consist
of an image only) are on the growth.Estimates of the cost of spam are almost
certainly far to low, because they do not account for the cost in time lost by
people.The image based spam which is currently penetrating many filters should
be catchable with a more sophisticated application of machine learning
technology. For the spam I see, the rendered images come in only a few
formats, which would be easy to recognize via a support vector machine (with
RBF kernel), neural network, or even nearest-neighbor architecture. The
mechanics of setting this up to run efficiently is the only real challenge.
This is the next step in the spam war.The response to this system is to make
the image based spam even more random. We should (essentially) expect to
seeCaptchaspam, and our</p><p>3 0.92799246 <a title="84-lda-3" href="../hunch_net-2008/hunch_net-2008-10-01-NIPS_2008_workshop_on_%26%238216%3BLearning_over_Empirical_Hypothesis_Spaces%26%238217%3B.html">319 hunch net-2008-10-01-NIPS 2008 workshop on &#8216;Learning over Empirical Hypothesis Spaces&#8217;</a></p>
<p>Introduction: This workshop asks for insights how far we may/can push the theoretical
boundary of using data in the design of learning machines. Can we express our
classification rule in terms of the sample, or do we have to stick to a core
assumption of classical statistical learning theory, namely that the
hypothesis space is to be defined independent from the sample? This workshop
is particularly interested in - but not restricted to - the 'luckiness
framework' and the recently introduced notion of 'compatibility functions' in
a semi-supervised learning context (more information can be found
athttp://www.kuleuven.be/wehys).</p><p>4 0.92045105 <a title="84-lda-4" href="../hunch_net-2009/hunch_net-2009-02-16-KDNuggets.html">342 hunch net-2009-02-16-KDNuggets</a></p>
<p>Introduction: Eric Zaetsch points outKDNuggetswhich is a well-developed mailing list/news
site with aKDDflavor. This might particularly interest people looking for
industrial jobs in machine learning, as the mailing list has many such.</p><p>same-blog 5 0.91452962 <a title="84-lda-5" href="../hunch_net-2005/hunch_net-2005-06-22-Languages__of_Learning.html">84 hunch net-2005-06-22-Languages  of Learning</a></p>
<p>Introduction: A language is a set of primitives which can be combined to succesfully create
complex objects. Languages arise in all sorts of situations: mechanical
construction, martial arts, communication, etcâ&euro;Ś Languages appear to be the key
to succesfully creating complex objects--it is difficult to come up with any
convincing example of a complex object which is not built using some language.
Since languages are so crucial to success, it is interesting to organize
various machine learning research programs by language.The most common
language in machine learning are languages for representing the solution to
machine learning. This includes:Bayes Nets and Graphical ModelsA language for
representing probability distributions. The key concept supporting modularity
is conditional independence.Michael Kearnshas been working on extending this
to game theory.Kernelized Linear ClassifiersA language for representing linear
separators, possibly in a large space. The key form of modularity here is
kerneliza</p><p>6 0.90948182 <a title="84-lda-6" href="../hunch_net-2005/hunch_net-2005-10-07-On-line_learning_of_regular_decision_rules.html">118 hunch net-2005-10-07-On-line learning of regular decision rules</a></p>
<p>7 0.8701697 <a title="84-lda-7" href="../hunch_net-2008/hunch_net-2008-02-10-Complexity_Illness.html">288 hunch net-2008-02-10-Complexity Illness</a></p>
<p>8 0.82095718 <a title="84-lda-8" href="../hunch_net-2005/hunch_net-2005-08-08-Apprenticeship_Reinforcement_Learning_for_Control.html">101 hunch net-2005-08-08-Apprenticeship Reinforcement Learning for Control</a></p>
<p>9 0.68247986 <a title="84-lda-9" href="../hunch_net-2005/hunch_net-2005-02-15-ESPgame_and_image_labeling.html">20 hunch net-2005-02-15-ESPgame and image labeling</a></p>
<p>10 0.6115334 <a title="84-lda-10" href="../hunch_net-2006/hunch_net-2006-09-28-Programming_Languages_for_Machine_Learning_Implementations.html">210 hunch net-2006-09-28-Programming Languages for Machine Learning Implementations</a></p>
<p>11 0.57271057 <a title="84-lda-11" href="../hunch_net-2005/hunch_net-2005-02-25-Problem%3A_Online_Learning.html">28 hunch net-2005-02-25-Problem: Online Learning</a></p>
<p>12 0.57015997 <a title="84-lda-12" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>13 0.56893462 <a title="84-lda-13" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<p>14 0.56491995 <a title="84-lda-14" href="../hunch_net-2010/hunch_net-2010-05-02-What%26%238217%3Bs_the_difference_between_gambling_and_rewarding_good_prediction%3F.html">397 hunch net-2010-05-02-What&#8217;s the difference between gambling and rewarding good prediction?</a></p>
<p>15 0.56403285 <a title="84-lda-15" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>16 0.55673081 <a title="84-lda-16" href="../hunch_net-2006/hunch_net-2006-06-30-ICML_papers.html">188 hunch net-2006-06-30-ICML papers</a></p>
<p>17 0.55660653 <a title="84-lda-17" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>18 0.55435461 <a title="84-lda-18" href="../hunch_net-2006/hunch_net-2006-03-09-Use_of_Notation.html">162 hunch net-2006-03-09-Use of Notation</a></p>
<p>19 0.55253351 <a title="84-lda-19" href="../hunch_net-2005/hunch_net-2005-02-17-Learning_Research_Programs.html">21 hunch net-2005-02-17-Learning Research Programs</a></p>
<p>20 0.55205858 <a title="84-lda-20" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
