<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>52 hunch net-2005-04-04-Grounds for Rejection</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-52" href="#">hunch_net-2005-52</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>52 hunch net-2005-04-04-Grounds for Rejection</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-52-html" href="http://hunch.net/?p=57">html</a></p><p>Introduction: It's reviewing season right now, so I thought I would list (at a high level)
the sorts of problems which I see in papers. Hopefully, this will help us all
write better papers.The following flaws are fatal to any paper:Incorrect
theorem or lemma statementsA typo might be "ok", if it can be understood. Any
theorem or lemma which indicates an incorrect understanding of reality must be
rejected. Not doing so would severely harm the integrity of the conference. A
paper rejected for this reason must be fixed.Lack of UnderstandingIf a paper
is understood by none of the (typically 3) reviewers then it must be rejected
for the same reason. This is more controversial than it sounds because there
are some people who maximize paper complexity in the hope of impressing the
reviewer. The tactic sometimes succeeds with some reviewers (but not with
me).As a reviewer, I sometimes get lost for stupid reasons. This is why an
anonymizedcommunication channelwith the author can be very helpful.Bad
ideaRarel</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 It's reviewing season right now, so I thought I would list (at a high level) the sorts of problems which I see in papers. [sent-1, score-0.318]
</p><p>2 Hopefully, this will help us all write better papers. [sent-2, score-0.077]
</p><p>3 The following flaws are fatal to any paper:Incorrect theorem or lemma statementsA typo might be "ok", if it can be understood. [sent-3, score-0.817]
</p><p>4 Any theorem or lemma which indicates an incorrect understanding of reality must be rejected. [sent-4, score-0.981]
</p><p>5 Not doing so would severely harm the integrity of the conference. [sent-5, score-0.376]
</p><p>6 Lack of UnderstandingIf a paper is understood by none of the (typically 3) reviewers then it must be rejected for the same reason. [sent-7, score-0.826]
</p><p>7 This is more controversial than it sounds because there are some people who maximize paper complexity in the hope of impressing the reviewer. [sent-8, score-0.656]
</p><p>8 The tactic sometimes succeeds with some reviewers (but not with me). [sent-9, score-0.327]
</p><p>9 As a reviewer, I sometimes get lost for stupid reasons. [sent-10, score-0.298]
</p><p>10 Bad ideaRarely, a paper comes along with an obviously bad idea. [sent-12, score-0.289]
</p><p>11 These also must be rejected for the integrity of scienceThe following flaws have a strong negative impact on my opinion of the paper. [sent-13, score-1.092]
</p><p>12 "Kneecapping the giants" papers take a previously published idea, cripple it, and then come up with an improvement on the crippled version. [sent-15, score-0.157]
</p><p>13 This often looks great experimentally, but is unconvincing because it does not improve on the state of the art. [sent-16, score-0.193]
</p><p>14 The paper emphasizes experimental evidence on datasets specially created to show the good performance of their algorithm. [sent-18, score-0.424]
</p><p>15 Unfortunately, because learning is worst-case-impossible, I have little trust that performing well on a toy dataset implies good performance on real-world datasets. [sent-19, score-0.427]
</p><p>16 My actual standard for reviewing is quite low, and I'm happy to approve of incremental improvements. [sent-20, score-0.645]
</p><p>17 Unfortunately, even that standard is such that I suggest rejection on most reviewed papers. [sent-21, score-0.376]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('integrity', 0.262), ('rejected', 0.249), ('incorrect', 0.242), ('lemma', 0.242), ('flaws', 0.209), ('paper', 0.206), ('must', 0.166), ('unfortunately', 0.138), ('approve', 0.131), ('giants', 0.131), ('impressing', 0.131), ('stupid', 0.131), ('typo', 0.131), ('theorem', 0.127), ('reviewing', 0.126), ('reviewers', 0.122), ('experimentally', 0.121), ('succeeds', 0.121), ('emphasizes', 0.114), ('harm', 0.114), ('unconvincing', 0.114), ('season', 0.109), ('toy', 0.109), ('indicates', 0.109), ('maximize', 0.109), ('performing', 0.109), ('following', 0.108), ('controversial', 0.105), ('sounds', 0.105), ('trust', 0.105), ('performance', 0.104), ('standard', 0.103), ('rejection', 0.101), ('incremental', 0.101), ('happy', 0.098), ('opinion', 0.098), ('reality', 0.095), ('reviewed', 0.093), ('ok', 0.086), ('actual', 0.086), ('sometimes', 0.084), ('none', 0.083), ('lost', 0.083), ('along', 0.083), ('sorts', 0.083), ('published', 0.081), ('suggest', 0.079), ('looks', 0.079), ('write', 0.077), ('previously', 0.076)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="52-tfidf-1" href="../hunch_net-2005/hunch_net-2005-04-04-Grounds_for_Rejection.html">52 hunch net-2005-04-04-Grounds for Rejection</a></p>
<p>Introduction: It's reviewing season right now, so I thought I would list (at a high level)
the sorts of problems which I see in papers. Hopefully, this will help us all
write better papers.The following flaws are fatal to any paper:Incorrect
theorem or lemma statementsA typo might be "ok", if it can be understood. Any
theorem or lemma which indicates an incorrect understanding of reality must be
rejected. Not doing so would severely harm the integrity of the conference. A
paper rejected for this reason must be fixed.Lack of UnderstandingIf a paper
is understood by none of the (typically 3) reviewers then it must be rejected
for the same reason. This is more controversial than it sounds because there
are some people who maximize paper complexity in the hope of impressing the
reviewer. The tactic sometimes succeeds with some reviewers (but not with
me).As a reviewer, I sometimes get lost for stupid reasons. This is why an
anonymizedcommunication channelwith the author can be very helpful.Bad
ideaRarel</p><p>2 0.17831057 <a title="52-tfidf-2" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>Introduction: When thinking about how best to review papers, it seems helpful to have some
conception of what good reviewing is. As far as I can tell, this is almost
always only discussed in the specific context of a paper (i.e. your rejected
paper), or at most an area (i.e. what a "good paper" looks like for that area)
rather than general principles. Neither individual papers or areas are
sufficiently general for a large conference--every paper differs in the
details, and what if you want to build a new area and/or cross areas?An
unavoidable reason for reviewing is that the community of research is too
large. In particular, it is not possible for a researcher to read every paper
which someone thinks might be of interest. This reason for reviewing exists
independent of constraints on rooms or scheduling formats of individual
conferences. Indeed, history suggests that physical constraints are relatively
meaningless over the long term -- growing conferences simply use more rooms
and/or change formats</p><p>3 0.16899827 <a title="52-tfidf-3" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>Introduction: Few would mistake the process of academic paper review for a fair process, but
sometimes the unfairness seems particularly striking. This is most easily seen
by comparison:PaperBanditronOffset TreeNotesProblem ScopeMulticlass problems
where only the loss of one choice can be probed.Strictly greater: Cost
sensitive multiclass problems where only the loss of one choice can be
probed.Often generalizations don't matter. That's not the case here, since
every plausible application I've thought of involves loss functions
substantially different from 0/1.What's newAnalysis and ExperimentsAlgorithm,
Analysis, and ExperimentsAs far as I know, the essence of the more general
problem was first stated and analyzed with theEXP4 algorithm (page 16)(1998).
It's also the time horizon 1 simplification of the Reinforcement Learning
setting for therandom trajectory method (page 15)(2002). The Banditron
algorithm itself is functionally identical toOne-Step RL with Traces (page
122)(2003) inBianca's thesis</p><p>4 0.16072413 <a title="52-tfidf-4" href="../hunch_net-2005/hunch_net-2005-03-09-Bad_Reviewing.html">38 hunch net-2005-03-09-Bad Reviewing</a></p>
<p>Introduction: This is a difficult subject to talk about for many reasons, but a discussion
may be helpful.Bad reviewing is a problem in academia. The first step in
understanding this is admitting to the problem, so here is a short list of
examples of bad reviewing.Reviewer disbelieves theorem proof (ICML), or
disbelieve theorem with a trivially false counterexample. (COLT)Reviewer
internally swaps quantifiers in a theorem, concludes it has been done before
and is trivial. (NIPS)Reviewer believes a technique will not work despite
experimental validation. (COLT)Reviewers fail to notice flaw in theorem
statement (CRYPTO).Reviewer erroneously claims that it has been done before
(NIPS, SODA, JMLR)--(complete with references!)Reviewer inverts the message of
a paper and concludes it says nothing important. (NIPS*2)Reviewer fails to
distinguish between a DAG and a tree (SODA).Reviewer is enthusiastic about
paper but clearly does not understand (ICML).Reviewer erroneously believe that
the "birthday paradox"</p><p>5 0.15411946 <a title="52-tfidf-5" href="../hunch_net-2008/hunch_net-2008-10-14-Who_is_Responsible_for_a_Bad_Review%3F.html">320 hunch net-2008-10-14-Who is Responsible for a Bad Review?</a></p>
<p>Introduction: Although I'm greatly interested in machine learning, I think it must be
admitted that there is a large amount of low quality logic being used in
reviews. The problem is bad enough that sometimes I wonder if theByzantine
generalslimit has been exceeded. For example, I've seen recent reviews where
the given reasons for rejecting are:[NIPS] Theorem A is uninteresting because
Theorem B is uninteresting.[UAI] When you learn by memorization, the problem
addressed is trivial.[NIPS] The proof is in the appendix.[NIPS] This has been
done before. (â&euro;Ś but not giving any relevant citations)Just for the record I
want to point out what's wrong with these reviews. A future world in which
such reasons never come up again would be great, but I'm sure these errors
will be committed many times more in the future.This is nonsense. A theorem
should be evaluated based on it's merits, rather than the merits of another
theorem.Learning by memorization requires an exponentially larger sample
complexity than man</p><p>6 0.15311454 <a title="52-tfidf-6" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>7 0.13931702 <a title="52-tfidf-7" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>8 0.1381259 <a title="52-tfidf-8" href="../hunch_net-2010/hunch_net-2010-04-26-Compassionate_Reviewing.html">395 hunch net-2010-04-26-Compassionate Reviewing</a></p>
<p>9 0.13229853 <a title="52-tfidf-9" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>10 0.12935732 <a title="52-tfidf-10" href="../hunch_net-2008/hunch_net-2008-09-03-Bidding_Problems.html">315 hunch net-2008-09-03-Bidding Problems</a></p>
<p>11 0.12590468 <a title="52-tfidf-11" href="../hunch_net-2006/hunch_net-2006-09-12-Incentive_Compatible_Reviewing.html">207 hunch net-2006-09-12-Incentive Compatible Reviewing</a></p>
<p>12 0.12230846 <a title="52-tfidf-12" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>13 0.1198884 <a title="52-tfidf-13" href="../hunch_net-2005/hunch_net-2005-03-13-Avoiding_Bad_Reviewing.html">40 hunch net-2005-03-13-Avoiding Bad Reviewing</a></p>
<p>14 0.11699018 <a title="52-tfidf-14" href="../hunch_net-2007/hunch_net-2007-02-16-The_Forgetting.html">233 hunch net-2007-02-16-The Forgetting</a></p>
<p>15 0.11564527 <a title="52-tfidf-15" href="../hunch_net-2006/hunch_net-2006-05-05-An_ICML_reject.html">177 hunch net-2006-05-05-An ICML reject</a></p>
<p>16 0.11349264 <a title="52-tfidf-16" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>17 0.11327197 <a title="52-tfidf-17" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>18 0.11258402 <a title="52-tfidf-18" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">23 hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<p>19 0.10816288 <a title="52-tfidf-19" href="../hunch_net-2012/hunch_net-2012-04-09-ICML_author_feedback_is_open.html">461 hunch net-2012-04-09-ICML author feedback is open</a></p>
<p>20 0.10618731 <a title="52-tfidf-20" href="../hunch_net-2012/hunch_net-2012-05-02-ICML%3A_Behind_the_Scenes.html">463 hunch net-2012-05-02-ICML: Behind the Scenes</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.205), (1, 0.125), (2, -0.177), (3, -0.078), (4, -0.021), (5, 0.031), (6, 0.007), (7, -0.011), (8, 0.003), (9, -0.018), (10, -0.018), (11, 0.061), (12, -0.016), (13, 0.018), (14, -0.027), (15, 0.013), (16, -0.038), (17, -0.136), (18, -0.029), (19, -0.067), (20, -0.002), (21, 0.063), (22, -0.007), (23, 0.013), (24, 0.023), (25, 0.046), (26, -0.01), (27, 0.026), (28, -0.019), (29, -0.03), (30, -0.022), (31, 0.069), (32, -0.024), (33, -0.019), (34, -0.033), (35, -0.021), (36, 0.027), (37, 0.041), (38, 0.057), (39, -0.024), (40, -0.085), (41, 0.027), (42, -0.02), (43, 0.012), (44, 0.029), (45, -0.071), (46, -0.014), (47, -0.002), (48, 0.003), (49, -0.018)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97700596 <a title="52-lsi-1" href="../hunch_net-2005/hunch_net-2005-04-04-Grounds_for_Rejection.html">52 hunch net-2005-04-04-Grounds for Rejection</a></p>
<p>Introduction: It's reviewing season right now, so I thought I would list (at a high level)
the sorts of problems which I see in papers. Hopefully, this will help us all
write better papers.The following flaws are fatal to any paper:Incorrect
theorem or lemma statementsA typo might be "ok", if it can be understood. Any
theorem or lemma which indicates an incorrect understanding of reality must be
rejected. Not doing so would severely harm the integrity of the conference. A
paper rejected for this reason must be fixed.Lack of UnderstandingIf a paper
is understood by none of the (typically 3) reviewers then it must be rejected
for the same reason. This is more controversial than it sounds because there
are some people who maximize paper complexity in the hope of impressing the
reviewer. The tactic sometimes succeeds with some reviewers (but not with
me).As a reviewer, I sometimes get lost for stupid reasons. This is why an
anonymizedcommunication channelwith the author can be very helpful.Bad
ideaRarel</p><p>2 0.82982486 <a title="52-lsi-2" href="../hunch_net-2008/hunch_net-2008-09-03-Bidding_Problems.html">315 hunch net-2008-09-03-Bidding Problems</a></p>
<p>Introduction: One way that many conferences in machine learning assign reviewers to papers
is via bidding, which has steps something like:Invite people to reviewAccept
papersReviewers look at title and abstract and state the papers they are
interested in reviewing.Some massaging happens, but reviewers often get
approximately the papers they bid for.At the ICML business meeting,Andrew
McCallumsuggested getting rid of bidding for papers. A couple reasons were
given:PrivacyThe title and abstract of the entire set of papers is visible to
every participating reviewer. Some authors might be uncomfortable about this
for submitted papers. I'm not sympathetic to this reason: the point of
submitting a paper to review is to publish it, so the value (if any) of not
publishing a part of it a little bit earlier seems limited.CliquesA bidding
system is gameable. If you have 3 buddies and you inform each other of your
submissions, you can each bid for your friend's papers and express a
disinterest in others. There</p><p>3 0.80892313 <a title="52-lsi-3" href="../hunch_net-2005/hunch_net-2005-03-09-Bad_Reviewing.html">38 hunch net-2005-03-09-Bad Reviewing</a></p>
<p>Introduction: This is a difficult subject to talk about for many reasons, but a discussion
may be helpful.Bad reviewing is a problem in academia. The first step in
understanding this is admitting to the problem, so here is a short list of
examples of bad reviewing.Reviewer disbelieves theorem proof (ICML), or
disbelieve theorem with a trivially false counterexample. (COLT)Reviewer
internally swaps quantifiers in a theorem, concludes it has been done before
and is trivial. (NIPS)Reviewer believes a technique will not work despite
experimental validation. (COLT)Reviewers fail to notice flaw in theorem
statement (CRYPTO).Reviewer erroneously claims that it has been done before
(NIPS, SODA, JMLR)--(complete with references!)Reviewer inverts the message of
a paper and concludes it says nothing important. (NIPS*2)Reviewer fails to
distinguish between a DAG and a tree (SODA).Reviewer is enthusiastic about
paper but clearly does not understand (ICML).Reviewer erroneously believe that
the "birthday paradox"</p><p>4 0.79575384 <a title="52-lsi-4" href="../hunch_net-2008/hunch_net-2008-10-14-Who_is_Responsible_for_a_Bad_Review%3F.html">320 hunch net-2008-10-14-Who is Responsible for a Bad Review?</a></p>
<p>Introduction: Although I'm greatly interested in machine learning, I think it must be
admitted that there is a large amount of low quality logic being used in
reviews. The problem is bad enough that sometimes I wonder if theByzantine
generalslimit has been exceeded. For example, I've seen recent reviews where
the given reasons for rejecting are:[NIPS] Theorem A is uninteresting because
Theorem B is uninteresting.[UAI] When you learn by memorization, the problem
addressed is trivial.[NIPS] The proof is in the appendix.[NIPS] This has been
done before. (â&euro;Ś but not giving any relevant citations)Just for the record I
want to point out what's wrong with these reviews. A future world in which
such reasons never come up again would be great, but I'm sure these errors
will be committed many times more in the future.This is nonsense. A theorem
should be evaluated based on it's merits, rather than the merits of another
theorem.Learning by memorization requires an exponentially larger sample
complexity than man</p><p>5 0.78666151 <a title="52-lsi-5" href="../hunch_net-2012/hunch_net-2012-05-02-ICML%3A_Behind_the_Scenes.html">463 hunch net-2012-05-02-ICML: Behind the Scenes</a></p>
<p>Introduction: This is a rather long post, detailing the ICML 2012 review process. The goal
is to make the process more transparent, help authors understand how we came
to a decision, and discuss the strengths and weaknesses of this process for
future conference organizers.Microsoft’s Conference Management Toolkit (CMT)We
chose to useCMTover other conference management software mainly because of its
rich toolkit. The interface is sub-optimal (to say the least!) but it has
extensive capabilities (to handle bids, author response, resubmissions, etc.),
good import/export mechanisms (to process the data elsewhere), excellent
technical support (to answer late night emails, add new functionalities).
Overall, it was the right choice, although we hope a designer will look at
that interface sometime soon!Toronto Matching System (TMS)TMSis now being used
by many major conferences in our field (including NIPS and UAI). It is an
automated system (developed byLaurent CharlinandRich Zemelat U. Toronto) to
match re</p><p>6 0.75748378 <a title="52-lsi-6" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>7 0.75212932 <a title="52-lsi-7" href="../hunch_net-2006/hunch_net-2006-12-04-Structural_Problems_in_NIPS_Decision_Making.html">221 hunch net-2006-12-04-Structural Problems in NIPS Decision Making</a></p>
<p>8 0.73580712 <a title="52-lsi-8" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>9 0.72498244 <a title="52-lsi-9" href="../hunch_net-2007/hunch_net-2007-02-16-The_Forgetting.html">233 hunch net-2007-02-16-The Forgetting</a></p>
<p>10 0.71937877 <a title="52-lsi-10" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>11 0.70317554 <a title="52-lsi-11" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>12 0.70115525 <a title="52-lsi-12" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>13 0.69749469 <a title="52-lsi-13" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>14 0.68405694 <a title="52-lsi-14" href="../hunch_net-2006/hunch_net-2006-09-12-Incentive_Compatible_Reviewing.html">207 hunch net-2006-09-12-Incentive Compatible Reviewing</a></p>
<p>15 0.67832148 <a title="52-lsi-15" href="../hunch_net-2007/hunch_net-2007-04-13-What_to_do_with_an_unreasonable_conditional_accept.html">238 hunch net-2007-04-13-What to do with an unreasonable conditional accept</a></p>
<p>16 0.66706741 <a title="52-lsi-16" href="../hunch_net-2005/hunch_net-2005-03-13-Avoiding_Bad_Reviewing.html">40 hunch net-2005-03-13-Avoiding Bad Reviewing</a></p>
<p>17 0.66593748 <a title="52-lsi-17" href="../hunch_net-2005/hunch_net-2005-02-25-Why_Papers%3F.html">30 hunch net-2005-02-25-Why Papers?</a></p>
<p>18 0.65491861 <a title="52-lsi-18" href="../hunch_net-2009/hunch_net-2009-07-09-The_Machine_Learning_Forum.html">363 hunch net-2009-07-09-The Machine Learning Forum</a></p>
<p>19 0.65352035 <a title="52-lsi-19" href="../hunch_net-2008/hunch_net-2008-11-10-ICML_Reviewing_Criteria.html">325 hunch net-2008-11-10-ICML Reviewing Criteria</a></p>
<p>20 0.63720131 <a title="52-lsi-20" href="../hunch_net-2012/hunch_net-2012-04-09-ICML_author_feedback_is_open.html">461 hunch net-2012-04-09-ICML author feedback is open</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(29, 0.022), (42, 0.259), (68, 0.078), (74, 0.12), (82, 0.037), (83, 0.36), (95, 0.028)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.95226532 <a title="52-lda-1" href="../hunch_net-2013/hunch_net-2013-01-31-Remote_large_scale_learning_class_participation.html">479 hunch net-2013-01-31-Remote large scale learning class participation</a></p>
<p>Introduction: Yann and I have arranged so that people who are interested in ourlarge scale
machine learning classand not able to attend in person can follow along via
two methods.Videoswill be posted with about a 1 day delay ontechtalks. This is
a side-by-side capture of video+slides fromWeyond.We are experimenting
withPiazzaas a discussion forum. Anyone is welcome to subscribe to Piazza and
ask questions there, where I will be monitoring things.update2: Sign
uphere.The first lecture is up now, including therevised version of the
slideswhich fixes a few typos and rounds out references.</p><p>2 0.92810774 <a title="52-lda-2" href="../hunch_net-2012/hunch_net-2012-06-15-Normal_Deviate_and_the_UCSC_Machine_Learning_Summer_School.html">467 hunch net-2012-06-15-Normal Deviate and the UCSC Machine Learning Summer School</a></p>
<p>Introduction: Larry Wassermanhas started theNormal Deviateblog which I added to the blogroll
on the right.Manfred Warmuthpoints out theUCSC machine learning summer
schoolrunning July 9-20 which may be of particular interest to those in
silicon valley.</p><p>same-blog 3 0.90098637 <a title="52-lda-3" href="../hunch_net-2005/hunch_net-2005-04-04-Grounds_for_Rejection.html">52 hunch net-2005-04-04-Grounds for Rejection</a></p>
<p>Introduction: It's reviewing season right now, so I thought I would list (at a high level)
the sorts of problems which I see in papers. Hopefully, this will help us all
write better papers.The following flaws are fatal to any paper:Incorrect
theorem or lemma statementsA typo might be "ok", if it can be understood. Any
theorem or lemma which indicates an incorrect understanding of reality must be
rejected. Not doing so would severely harm the integrity of the conference. A
paper rejected for this reason must be fixed.Lack of UnderstandingIf a paper
is understood by none of the (typically 3) reviewers then it must be rejected
for the same reason. This is more controversial than it sounds because there
are some people who maximize paper complexity in the hope of impressing the
reviewer. The tactic sometimes succeeds with some reviewers (but not with
me).As a reviewer, I sometimes get lost for stupid reasons. This is why an
anonymizedcommunication channelwith the author can be very helpful.Bad
ideaRarel</p><p>4 0.90005928 <a title="52-lda-4" href="../hunch_net-2009/hunch_net-2009-05-30-Many_ways_to_Learn_this_summer.html">357 hunch net-2009-05-30-Many ways to Learn this summer</a></p>
<p>Introduction: There are at least3summer schools related to machine learning this summer.The
firstis atUniversity of ChicagoJune 1-11 organized byMisha Belkin,Partha
Niyogi, andSteve Smale. Registration is closed for this one, meaning they met
their capacity limit. The format is essentially an extended Tutorial/Workshop.
I was particularly interested to seeValiantamongst the speakers. I'm also
presenting Saturday June 6, on logarithmic time prediction.Praveen
Srinivasanpoints out the second atPeking Universityin Beijing, China, July
20-27.This onediffers substantially, as it is about vision, machine learning,
and their intersection. The deadline for applications is June 10 or 15. This
is also another example of the growth of research in China, with active
support fromNSF.The third one is atCambridge, England, August 29-September 10.
It's in theMLSS series. Compared to the Chicago one, this one is more about
the Bayesian side of ML, although effort has been made to create a good cross
section of topic</p><p>5 0.89521098 <a title="52-lda-5" href="../hunch_net-2007/hunch_net-2007-11-02-The_Machine_Learning_Award_goes_to_%26%238230%3B.html">270 hunch net-2007-11-02-The Machine Learning Award goes to &#8230;</a></p>
<p>Introduction: Perhaps the biggest CS prize for research is theTuring Award, which has a
$0.25M cash prize associated with it. It appears none of the prizes so far
have been for anything like machine learning (the closest are perhaps database
awards).In CS theory, there is theGÃƒÂ¶del Prizewhich is smaller and newer,
offering a $5K prize along and perhaps (more importantly) recognition. One
such award has been given for Machine Learning, toRobert SchapireandYoav
Freundfor Adaboost.In Machine Learning, there seems to be no equivalent of
these sorts of prizes. There are several plausible reasons for this:There is
no coherent community.People drift in and out of the central conferences all
the time. Most of the author names from 10 years ago do not occur in the
conferences of today. In addition, the entire subject area is fairly new.There
are at least a core group of people who have stayed around.Machine Learning
work doesn't lastAlmost every paper is forgotten, because {the goals change,
there isn't an</p><p>6 0.85326385 <a title="52-lda-6" href="../hunch_net-2009/hunch_net-2009-05-08-Computability_in_Artificial_Intelligence.html">353 hunch net-2009-05-08-Computability in Artificial Intelligence</a></p>
<p>7 0.7315293 <a title="52-lda-7" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>8 0.6407302 <a title="52-lda-8" href="../hunch_net-2005/hunch_net-2005-03-17-Going_all_the_Way%2C_Sometimes.html">42 hunch net-2005-03-17-Going all the Way, Sometimes</a></p>
<p>9 0.63761818 <a title="52-lda-9" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>10 0.62921429 <a title="52-lda-10" href="../hunch_net-2009/hunch_net-2009-06-01-Multitask_Poisoning.html">358 hunch net-2009-06-01-Multitask Poisoning</a></p>
<p>11 0.62306577 <a title="52-lda-11" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>12 0.62275428 <a title="52-lda-12" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>13 0.62264788 <a title="52-lda-13" href="../hunch_net-2005/hunch_net-2005-09-10-%26%238220%3BFailure%26%238221%3B_is_an_option.html">110 hunch net-2005-09-10-&#8220;Failure&#8221; is an option</a></p>
<p>14 0.62029159 <a title="52-lda-14" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>15 0.61958617 <a title="52-lda-15" href="../hunch_net-2005/hunch_net-2005-06-08-Question%3A_%26%238220%3BWhen_is_the_right_time_to_insert_the_loss_function%3F%26%238221%3B.html">79 hunch net-2005-06-08-Question: &#8220;When is the right time to insert the loss function?&#8221;</a></p>
<p>16 0.61834449 <a title="52-lda-16" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>17 0.61785859 <a title="52-lda-17" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>18 0.61458606 <a title="52-lda-18" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>19 0.61306095 <a title="52-lda-19" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>20 0.61305529 <a title="52-lda-20" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
