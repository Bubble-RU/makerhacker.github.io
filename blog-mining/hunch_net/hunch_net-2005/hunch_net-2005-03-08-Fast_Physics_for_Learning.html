<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>37 hunch net-2005-03-08-Fast Physics for Learning</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-37" href="#">hunch_net-2005-37</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>37 hunch net-2005-03-08-Fast Physics for Learning</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-37-html" href="http://hunch.net/?p=41">html</a></p><p>Introduction: While everyone is silently working on ICML submissions, I found this discussion about a  fast physics simulator  chip interesting from a learning viewpoint.  In many cases, learning attempts to predict the outcome of physical processes.  Access to a fast simulator for these processes might be quite helpful in predicting the outcome.  Bayesian learning in particular may directly benefit while many other algorithms (like support vector machines) might have their speed greatly increased.
 
The biggest drawback is that writing software for these odd architectures is always difficult and time consuming, but a several-orders-of-magnitude speedup might make that worthwhile.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 While everyone is silently working on ICML submissions, I found this discussion about a  fast physics simulator  chip interesting from a learning viewpoint. [sent-1, score-1.708]
</p><p>2 In many cases, learning attempts to predict the outcome of physical processes. [sent-2, score-0.658]
</p><p>3 Access to a fast simulator for these processes might be quite helpful in predicting the outcome. [sent-3, score-1.193]
</p><p>4 Bayesian learning in particular may directly benefit while many other algorithms (like support vector machines) might have their speed greatly increased. [sent-4, score-1.133]
</p><p>5 The biggest drawback is that writing software for these odd architectures is always difficult and time consuming, but a several-orders-of-magnitude speedup might make that worthwhile. [sent-5, score-1.556]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('simulator', 0.424), ('consuming', 0.229), ('silently', 0.229), ('chip', 0.212), ('speedup', 0.212), ('fast', 0.211), ('architectures', 0.177), ('biggest', 0.177), ('physical', 0.177), ('odd', 0.171), ('physics', 0.158), ('outcome', 0.154), ('might', 0.149), ('software', 0.148), ('attempts', 0.145), ('benefit', 0.145), ('drawback', 0.145), ('worthwhile', 0.14), ('submissions', 0.14), ('processes', 0.137), ('machines', 0.133), ('speed', 0.131), ('writing', 0.126), ('vector', 0.124), ('access', 0.121), ('predicting', 0.114), ('greatly', 0.11), ('cases', 0.109), ('everyone', 0.109), ('support', 0.105), ('directly', 0.102), ('bayesian', 0.095), ('helpful', 0.095), ('discussion', 0.091), ('predict', 0.085), ('found', 0.084), ('working', 0.082), ('always', 0.081), ('particular', 0.073), ('difficult', 0.073), ('icml', 0.071), ('interesting', 0.067), ('quite', 0.063), ('many', 0.056), ('algorithms', 0.052), ('make', 0.051), ('time', 0.046), ('may', 0.045), ('like', 0.045), ('learning', 0.041)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="37-tfidf-1" href="../hunch_net-2005/hunch_net-2005-03-08-Fast_Physics_for_Learning.html">37 hunch net-2005-03-08-Fast Physics for Learning</a></p>
<p>Introduction: While everyone is silently working on ICML submissions, I found this discussion about a  fast physics simulator  chip interesting from a learning viewpoint.  In many cases, learning attempts to predict the outcome of physical processes.  Access to a fast simulator for these processes might be quite helpful in predicting the outcome.  Bayesian learning in particular may directly benefit while many other algorithms (like support vector machines) might have their speed greatly increased.
 
The biggest drawback is that writing software for these odd architectures is always difficult and time consuming, but a several-orders-of-magnitude speedup might make that worthwhile.</p><p>2 0.074346513 <a title="37-tfidf-2" href="../hunch_net-2011/hunch_net-2011-04-23-ICML_workshops_due.html">433 hunch net-2011-04-23-ICML workshops due</a></p>
<p>Introduction: Lihong  points out that  ICML   workshop  submissions are due April 29.</p><p>3 0.073090494 <a title="37-tfidf-3" href="../hunch_net-2005/hunch_net-2005-12-07-Is_the_Google_way_the_way_for_machine_learning%3F.html">136 hunch net-2005-12-07-Is the Google way the way for machine learning?</a></p>
<p>Introduction: Urs Hoelzle  from  Google  gave an invited presentation at  NIPS .  In the presentation, he strongly advocates interacting with data in a particular scalable manner which is something like the following:
  
 Make a cluster of machines. 
 Build a unified filesystem.  (Google uses GFS, but NFS or other approaches work reasonably well for smaller clusters.) 
 Interact with data via  MapReduce . 
  
Creating a cluster of machines is, by this point, relatively straightforward.  
 
Unified filesystems are a little bit tricky—GFS is capable by design of essentially unlimited speed throughput to disk.  NFS can bottleneck because all of the data has to move through one machine.  Nevertheless,  this may not be a limiting factor for smaller clusters.
 
MapReduce is a programming paradigm.  Essentially, it is a combination of a data element transform (map) and an agreggator/selector (reduce).  These operations are highly parallelizable and the claim is that they support the forms of data interacti</p><p>4 0.072648749 <a title="37-tfidf-4" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>Introduction: Normally, I don’t indulge in posters for  ICML , but this year is naturally an exception for me.   If you want one, there are a small number  left here , if you sign up before February.
 
It also seems worthwhile to give some sense of the scope and reviewing criteria for ICML for authors considering submitting papers.  At ICML, the (very large) program committee does the reviewing which informs final decisions by area chairs on most papers.  Program chairs setup the process, deal with exceptions or disagreements, and provide advice for the reviewing process.  Providing advice is tricky (and easily misleading) because a conference is a community, and in the end the aggregate interests of the community determine the conference.  Nevertheless, as a program chair this year it seems worthwhile to state the overall philosophy I have and what I plan to encourage (and occasionally discourage).
 
At the highest level, I believe ICML exists to further research into machine learning, which I gene</p><p>5 0.071891606 <a title="37-tfidf-5" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>Introduction: I wanted to expand on this  post  and some of the previous  problems/research directions  about where learning theory might make large strides.  
  
  Why theory?   The essential reason for theory is “intuition extension”.  A very good applied learning person can master some particular application domain yielding the best computer algorithms for solving that problem.  A very good theory can take the intuitions discovered by this and other applied learning people and extend them to new domains in a relatively automatic fashion.  To do this, we take these basic intuitions and try to find a mathematical model that:
 
 Explains the basic intuitions. 
 Makes new testable predictions about how to learn. 
 Succeeds in so learning. 
 

This is “intuition extension”: taking what we have learned somewhere else and applying it in new domains.  It is fundamentally useful to everyone because it increases the level of automation in solving problems.

 
  Where next for learning theory?  I like the a</p><p>6 0.070701174 <a title="37-tfidf-6" href="../hunch_net-2006/hunch_net-2006-01-30-Should_the_Input_Representation_be_a_Vector%3F.html">152 hunch net-2006-01-30-Should the Input Representation be a Vector?</a></p>
<p>7 0.070512429 <a title="37-tfidf-7" href="../hunch_net-2005/hunch_net-2005-04-23-Advantages_and_Disadvantages_of_Bayesian_Learning.html">60 hunch net-2005-04-23-Advantages and Disadvantages of Bayesian Learning</a></p>
<p>8 0.068349361 <a title="37-tfidf-8" href="../hunch_net-2005/hunch_net-2005-04-06-Structured_Regret_Minimization.html">53 hunch net-2005-04-06-Structured Regret Minimization</a></p>
<p>9 0.066193305 <a title="37-tfidf-9" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>10 0.062798992 <a title="37-tfidf-10" href="../hunch_net-2005/hunch_net-2005-05-02-Reviewing_techniques_for_conferences.html">65 hunch net-2005-05-02-Reviewing techniques for conferences</a></p>
<p>11 0.062742069 <a title="37-tfidf-11" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>12 0.060985077 <a title="37-tfidf-12" href="../hunch_net-2007/hunch_net-2007-12-12-Workshop_Summary%26%238212%3BPrinciples_of_Learning_Problem_Design.html">277 hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</a></p>
<p>13 0.060766354 <a title="37-tfidf-13" href="../hunch_net-2009/hunch_net-2009-06-26-Netflix_nearly_done.html">362 hunch net-2009-06-26-Netflix nearly done</a></p>
<p>14 0.0607436 <a title="37-tfidf-14" href="../hunch_net-2006/hunch_net-2006-09-28-Programming_Languages_for_Machine_Learning_Implementations.html">210 hunch net-2006-09-28-Programming Languages for Machine Learning Implementations</a></p>
<p>15 0.060548373 <a title="37-tfidf-15" href="../hunch_net-2008/hunch_net-2008-07-06-To_Dual_or_Not.html">308 hunch net-2008-07-06-To Dual or Not</a></p>
<p>16 0.060325492 <a title="37-tfidf-16" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>17 0.059988823 <a title="37-tfidf-17" href="../hunch_net-2008/hunch_net-2008-08-24-Mass_Customized_Medicine_in_the_Future%3F.html">314 hunch net-2008-08-24-Mass Customized Medicine in the Future?</a></p>
<p>18 0.059139539 <a title="37-tfidf-18" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>19 0.058454916 <a title="37-tfidf-19" href="../hunch_net-2009/hunch_net-2009-03-18-Parallel_ML_primitives.html">346 hunch net-2009-03-18-Parallel ML primitives</a></p>
<p>20 0.058179088 <a title="37-tfidf-20" href="../hunch_net-2005/hunch_net-2005-04-08-Fast_SVMs.html">54 hunch net-2005-04-08-Fast SVMs</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.132), (1, -0.007), (2, -0.027), (3, 0.012), (4, 0.03), (5, 0.009), (6, -0.032), (7, 0.015), (8, 0.048), (9, 0.0), (10, -0.053), (11, -0.059), (12, -0.022), (13, -0.025), (14, 0.059), (15, 0.014), (16, -0.028), (17, -0.066), (18, -0.009), (19, -0.017), (20, -0.013), (21, -0.018), (22, -0.038), (23, 0.071), (24, 0.026), (25, -0.048), (26, -0.011), (27, -0.025), (28, -0.054), (29, 0.058), (30, -0.064), (31, -0.015), (32, -0.006), (33, 0.051), (34, -0.038), (35, -0.01), (36, -0.052), (37, 0.001), (38, -0.028), (39, -0.03), (40, 0.058), (41, -0.065), (42, -0.037), (43, 0.072), (44, -0.036), (45, -0.027), (46, 0.035), (47, -0.044), (48, 0.022), (49, 0.046)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9422816 <a title="37-lsi-1" href="../hunch_net-2005/hunch_net-2005-03-08-Fast_Physics_for_Learning.html">37 hunch net-2005-03-08-Fast Physics for Learning</a></p>
<p>Introduction: While everyone is silently working on ICML submissions, I found this discussion about a  fast physics simulator  chip interesting from a learning viewpoint.  In many cases, learning attempts to predict the outcome of physical processes.  Access to a fast simulator for these processes might be quite helpful in predicting the outcome.  Bayesian learning in particular may directly benefit while many other algorithms (like support vector machines) might have their speed greatly increased.
 
The biggest drawback is that writing software for these odd architectures is always difficult and time consuming, but a several-orders-of-magnitude speedup might make that worthwhile.</p><p>2 0.51751572 <a title="37-lsi-2" href="../hunch_net-2009/hunch_net-2009-03-18-Parallel_ML_primitives.html">346 hunch net-2009-03-18-Parallel ML primitives</a></p>
<p>Introduction: Previously, we discussed  parallel machine learning  a bit.  As parallel ML is rather difficult, I’d like to describe my thinking at the moment, and ask for advice from the rest of the world.  This is particularly relevant right now, as I’m attending a workshop tomorrow on parallel ML.
 
Parallelizing slow algorithms seems uncompelling.  Parallelizing many algorithms also seems uncompelling, because the effort required to parallelize is substantial.  This leaves the question: Which one fast algorithm is the best to parallelize?  What is a substantially different second?
 
One compellingly fast simple algorithm is online gradient descent on a linear representation.  This is the core of Leon’s  sgd  code and  Vowpal Wabbit .   Antoine Bordes  showed a variant was competitive in the  large scale learning challenge .  It’s also a decades old primitive which has been reused in many algorithms, and continues to be reused.  It also applies to online  learning  rather than just online  optimiz</p><p>3 0.5120948 <a title="37-lsi-3" href="../hunch_net-2006/hunch_net-2006-01-30-Should_the_Input_Representation_be_a_Vector%3F.html">152 hunch net-2006-01-30-Should the Input Representation be a Vector?</a></p>
<p>Introduction: Let’s suppose that we are trying to create a general purpose machine learning box.  The box is fed many examples of the function it is supposed to learn and (hopefully) succeeds.
 
To date, most such attempts to produce a box of this form take a vector as input.  The elements of the vector might be bits, real numbers, or ‘categorical’ data (a discrete set of values).
 
On the other hand, there are a number of succesful applications of machine learning which do not seem to use a vector representation as input.  For example, in vision,   convolutional neural networks  have been used to solve several vision problems.  The input to the convolutional neural network is essentially the raw camera image as a  matrix .  In learning for natural languages, several people have had success on problems like parts-of-speech tagging using predictors restricted to a window surrounding the word to be predicted.  
 
A vector window and a matrix both imply a notion of locality which is being actively and</p><p>4 0.51163965 <a title="37-lsi-4" href="../hunch_net-2006/hunch_net-2006-09-28-Programming_Languages_for_Machine_Learning_Implementations.html">210 hunch net-2006-09-28-Programming Languages for Machine Learning Implementations</a></p>
<p>Introduction: Machine learning algorithms have a much better chance of being widely adopted if they are implemented in some easy-to-use code.  There are several important concerns associated with machine learning which stress programming languages on the ease-of-use vs. speed frontier.
  
  Speed   The rate at which data sources are growing seems to be outstripping the rate at which computational power is growing, so it is important that we be able to eak out every bit of computational power.  Garbage collected languages ( java ,  ocaml ,  perl  and  python ) often have several issues here.
 
 Garbage collection often implies that floating point numbers are “boxed”: every float is represented by a pointer to a float.  Boxing can cause an order of magnitude slowdown because an extra nonlocalized memory reference is made, and accesses to main memory can are many CPU cycles long. 
 Garbage collection often implies that considerably more memory is used than is necessary.   This has a variable effect.  I</p><p>5 0.51023811 <a title="37-lsi-5" href="../hunch_net-2007/hunch_net-2007-09-16-Optimizing_Machine_Learning_Programs.html">262 hunch net-2007-09-16-Optimizing Machine Learning Programs</a></p>
<p>Introduction: Machine learning is often computationally bounded which implies that the ability to write fast code becomes important if you ever want to implement a machine learning algorithm.  Basic tactical optimizations are covered well  elsewhere , but I haven’t seen a reasonable guide to higher level optimizations, which are the most important in my experience.  Here are some of the higher level optimizations I’ve often found useful.
  
  Algorithmic Improvement First . This is Hard, but it is the most important consideration, and typically yields the most benefits.  Good optimizations here are publishable.  In the context of machine learning, you should be familiar with the arguments for online vs. batch learning. 
  Choice of Language . There are many arguments about the  choice of language .  Sometimes you don’t have a choice when interfacing with other people.  Personally, I favor C/C++ when I want to write fast code.  This (admittedly) makes me a slower programmer than when using higher lev</p><p>6 0.4985114 <a title="37-lsi-6" href="../hunch_net-2007/hunch_net-2007-07-12-ICML_Trends.html">254 hunch net-2007-07-12-ICML Trends</a></p>
<p>7 0.49842852 <a title="37-lsi-7" href="../hunch_net-2012/hunch_net-2012-07-09-Videolectures.html">469 hunch net-2012-07-09-Videolectures</a></p>
<p>8 0.49608383 <a title="37-lsi-8" href="../hunch_net-2005/hunch_net-2005-02-04-JMLG.html">13 hunch net-2005-02-04-JMLG</a></p>
<p>9 0.49246815 <a title="37-lsi-9" href="../hunch_net-2007/hunch_net-2007-06-23-Machine_Learning_Jobs_are_Growing_on_Trees.html">250 hunch net-2007-06-23-Machine Learning Jobs are Growing on Trees</a></p>
<p>10 0.49132812 <a title="37-lsi-10" href="../hunch_net-2005/hunch_net-2005-07-07-The_Limits_of_Learning_Theory.html">90 hunch net-2005-07-07-The Limits of Learning Theory</a></p>
<p>11 0.48959053 <a title="37-lsi-11" href="../hunch_net-2008/hunch_net-2008-06-30-ICML_has_a_comment_system.html">305 hunch net-2008-06-30-ICML has a comment system</a></p>
<p>12 0.48926032 <a title="37-lsi-12" href="../hunch_net-2009/hunch_net-2009-08-03-Carbon_in_Computer_Science_Research.html">366 hunch net-2009-08-03-Carbon in Computer Science Research</a></p>
<p>13 0.48141384 <a title="37-lsi-13" href="../hunch_net-2005/hunch_net-2005-06-22-Languages__of_Learning.html">84 hunch net-2005-06-22-Languages  of Learning</a></p>
<p>14 0.48088104 <a title="37-lsi-14" href="../hunch_net-2007/hunch_net-2007-01-26-Parallel_Machine_Learning_Problems.html">229 hunch net-2007-01-26-Parallel Machine Learning Problems</a></p>
<p>15 0.47541368 <a title="37-lsi-15" href="../hunch_net-2005/hunch_net-2005-09-20-Workshop_Proposal%3A_Atomic_Learning.html">114 hunch net-2005-09-20-Workshop Proposal: Atomic Learning</a></p>
<p>16 0.47055468 <a title="37-lsi-16" href="../hunch_net-2009/hunch_net-2009-04-02-Asymmophobia.html">348 hunch net-2009-04-02-Asymmophobia</a></p>
<p>17 0.47045434 <a title="37-lsi-17" href="../hunch_net-2008/hunch_net-2008-08-24-Mass_Customized_Medicine_in_the_Future%3F.html">314 hunch net-2008-08-24-Mass Customized Medicine in the Future?</a></p>
<p>18 0.45985553 <a title="37-lsi-18" href="../hunch_net-2009/hunch_net-2009-12-09-Future_Publication_Models_%40_NIPS.html">382 hunch net-2009-12-09-Future Publication Models @ NIPS</a></p>
<p>19 0.45891929 <a title="37-lsi-19" href="../hunch_net-2006/hunch_net-2006-04-02-Mad_%28Neuro%29science.html">168 hunch net-2006-04-02-Mad (Neuro)science</a></p>
<p>20 0.45848659 <a title="37-lsi-20" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(27, 0.156), (49, 0.295), (53, 0.09), (55, 0.141), (94, 0.099), (95, 0.09)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.92714459 <a title="37-lda-1" href="../hunch_net-2006/hunch_net-2006-12-12-Interesting_Papers_at_NIPS_2006.html">224 hunch net-2006-12-12-Interesting Papers at NIPS 2006</a></p>
<p>Introduction: Here are some papers that I found surprisingly interesting.
  
  Yoshua Bengio , Pascal Lamblin, Dan Popovici, Hugo Larochelle,  Greedy Layer-wise Training of Deep Networks . Empirically investigates some of the design choices behind deep belief networks.
 
  Long Zhu , Yuanhao Chen,  Alan Yuille  Unsupervised Learning of a Probabilistic Grammar for Object Detection and Parsing.  An unsupervised method for detecting objects using simple feature filters that works remarkably well on the (supervised)  caltech-101 dataset . 
  Shai Ben-David ,  John Blitzer ,  Koby Crammer , and  Fernando Pereira ,  Analysis of Representations for Domain Adaptation .  This is the first analysis I’ve seen of learning with respect to samples drawn differently from the evaluation distribution which depends on reasonable measurable quantities. 
  
All of these papers turn out to have a common theme—the power of unlabeled data to do generically useful things.</p><p>same-blog 2 0.9072817 <a title="37-lda-2" href="../hunch_net-2005/hunch_net-2005-03-08-Fast_Physics_for_Learning.html">37 hunch net-2005-03-08-Fast Physics for Learning</a></p>
<p>Introduction: While everyone is silently working on ICML submissions, I found this discussion about a  fast physics simulator  chip interesting from a learning viewpoint.  In many cases, learning attempts to predict the outcome of physical processes.  Access to a fast simulator for these processes might be quite helpful in predicting the outcome.  Bayesian learning in particular may directly benefit while many other algorithms (like support vector machines) might have their speed greatly increased.
 
The biggest drawback is that writing software for these odd architectures is always difficult and time consuming, but a several-orders-of-magnitude speedup might make that worthwhile.</p><p>3 0.90163279 <a title="37-lda-3" href="../hunch_net-2005/hunch_net-2005-10-13-Site_tweak.html">122 hunch net-2005-10-13-Site tweak</a></p>
<p>Introduction: Several people have had difficulty with comments which seem to have an allowed language significantly poorer than posts.  The set of allowed html tags has been increased and the  markdown filter  has been put in place to try to make commenting easier.  Iâ&euro;&trade;ll put some examples into the comments of this post.</p><p>4 0.87748474 <a title="37-lda-4" href="../hunch_net-2009/hunch_net-2009-07-31-Vowpal_Wabbit_Open_Source_Project.html">365 hunch net-2009-07-31-Vowpal Wabbit Open Source Project</a></p>
<p>Introduction: Today brings a new release of the  Vowpal Wabbit  fast online learning software.  This time, unlike the previous release, the project itself is going open source, developing via  github .  For example, the lastest and greatest can be downloaded via:
  
git clone git://github.com/JohnLangford/vowpal_wabbit.git
  
If you aren’t familiar with  git , it’s a distributed version control system which supports quick and easy branching, as well as reconciliation.
 
This version of the code is confirmed to compile without complaint on at least some flavors of OSX as well as Linux boxes.
 
As much of the point of this project is pushing the limits of fast and effective machine learning, let me mention a few datapoints from my experience.
  
 The program can effectively scale up to batch-style training on sparse terafeature (i.e. 10 12  sparse feature) size datasets.  The limiting factor is typically i/o. 
 I started using the the real datasets from the  large-scale learning  workshop as a conve</p><p>5 0.82169586 <a title="37-lda-5" href="../hunch_net-2009/hunch_net-2009-01-23-An_Active_Learning_Survey.html">338 hunch net-2009-01-23-An Active Learning Survey</a></p>
<p>Introduction: Burr Settles  wrote a fairly comprehensive  survey of active learning .  He intends to maintain and update the survey, so send him any suggestions you have.</p><p>6 0.78751218 <a title="37-lda-6" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">23 hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<p>7 0.78142238 <a title="37-lda-7" href="../hunch_net-2009/hunch_net-2009-04-02-Asymmophobia.html">348 hunch net-2009-04-02-Asymmophobia</a></p>
<p>8 0.69835031 <a title="37-lda-8" href="../hunch_net-2009/hunch_net-2009-06-03-Functionally_defined_Nonlinear_Dynamic_Models.html">359 hunch net-2009-06-03-Functionally defined Nonlinear Dynamic Models</a></p>
<p>9 0.65536308 <a title="37-lda-9" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>10 0.65111059 <a title="37-lda-10" href="../hunch_net-2011/hunch_net-2011-07-11-Interesting_Neural_Network_Papers_at_ICML_2011.html">438 hunch net-2011-07-11-Interesting Neural Network Papers at ICML 2011</a></p>
<p>11 0.6468969 <a title="37-lda-11" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>12 0.63679421 <a title="37-lda-12" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>13 0.62764937 <a title="37-lda-13" href="../hunch_net-2005/hunch_net-2005-05-28-Running_A_Machine_Learning_Summer_School.html">75 hunch net-2005-05-28-Running A Machine Learning Summer School</a></p>
<p>14 0.62722743 <a title="37-lda-14" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<p>15 0.62631553 <a title="37-lda-15" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>16 0.62534517 <a title="37-lda-16" href="../hunch_net-2010/hunch_net-2010-10-29-To_Vidoelecture_or_not.html">416 hunch net-2010-10-29-To Vidoelecture or not</a></p>
<p>17 0.62270093 <a title="37-lda-17" href="../hunch_net-2006/hunch_net-2006-01-25-1_year.html">151 hunch net-2006-01-25-1 year</a></p>
<p>18 0.62182522 <a title="37-lda-18" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>19 0.62150902 <a title="37-lda-19" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<p>20 0.62010199 <a title="37-lda-20" href="../hunch_net-2005/hunch_net-2005-03-13-Avoiding_Bad_Reviewing.html">40 hunch net-2005-03-13-Avoiding Bad Reviewing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
