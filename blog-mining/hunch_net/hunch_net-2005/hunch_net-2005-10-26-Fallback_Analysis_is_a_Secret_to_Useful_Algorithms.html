<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>126 hunch net-2005-10-26-Fallback Analysis is a Secret to Useful Algorithms</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-126" href="#">hunch_net-2005-126</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>126 hunch net-2005-10-26-Fallback Analysis is a Secret to Useful Algorithms</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-126-html" href="http://hunch.net/?p=137">html</a></p><p>Introduction: The ideal of theoretical algorithm analysis is to construct an algorithm with
accompanying optimality theorems proving that it is a useful algorithm. This
ideal often fails, particularly for learning algorithms and theory. The
general form of a theorem is:IfpreconditionsThenpostconditionsWhen we design
learning algorithms it is very common to come up with precondition assumptions
such as "the data is IID", "the learning problem is drawn from a known
distribution over learning problems", or "there is a perfect classifier". All
of these example preconditions can be false for real-world problems in ways
that are not easily detectable. This means that algorithms derived and
justified by these very common forms of analysis may be prone to catastrophic
failure in routine (mis)application.Wecanhope for better. Several different
kinds of learning algorithm analysis have been developed some of which have
fewer preconditions. Simply demanding that these forms of analysis be used may
be too stron</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The ideal of theoretical algorithm analysis is to construct an algorithm with accompanying optimality theorems proving that it is a useful algorithm. [sent-1, score-1.007]
</p><p>2 This ideal often fails, particularly for learning algorithms and theory. [sent-2, score-0.252]
</p><p>3 All of these example preconditions can be false for real-world problems in ways that are not easily detectable. [sent-4, score-0.374]
</p><p>4 This means that algorithms derived and justified by these very common forms of analysis may be prone to catastrophic failure in routine (mis)application. [sent-5, score-0.969]
</p><p>5 Several different kinds of learning algorithm analysis have been developed some of which have fewer preconditions. [sent-7, score-0.413]
</p><p>6 Simply demanding that these forms of analysis be used may be too strong--there is an unresolved criticism that these algorithm may be "too worst case". [sent-8, score-0.745]
</p><p>7 Nevertheless, it is possible to have a learning algorithm that simultaneously provides strong postconditions given strong preconditions, reasonable postconditions given reasonable preconditions, and weak postconditions given weak preconditions. [sent-9, score-2.058]
</p><p>8 Thecover treewhich creates anO(n)datastructure for nearest neighbor queries while simultaneously guaranteeingO(log(n))query time when the metric obeys a dimensionality constraint. [sent-11, score-0.351]
</p><p>9 The basic claim is that algorithms with a good fallback analysis are significantly more likely to achieve the theoretical algorithm analysis ideal. [sent-12, score-1.198]
</p><p>10 Both of the above algorithms have been tested in practice and found capable. [sent-13, score-0.193]
</p><p>11 Several significant difficulties occur for anyone working on fallback analysis. [sent-14, score-0.385]
</p><p>12 This is probably the most valid reason--people have limited time to do things. [sent-16, score-0.076]
</p><p>13 Nevertheless, it is reasonable to hope that the core techniques used by many people have had this effort put into them. [sent-17, score-0.157]
</p><p>14 It is psychologically difficult to both assume and not assume a precondition, for a researcher. [sent-18, score-0.415]
</p><p>15 A critical valuable resource here is observing multiple forms of analysis. [sent-19, score-0.247]
</p><p>16 It is psychologically difficult for a reviewer to appreciate the value of both assuming and not assuming some precondition. [sent-20, score-0.443]
</p><p>17 In particular, theoretically inclined people 1) get great joy from showing that something new ispossibleand 1) routinely work on papers of the form "here is a better algorithm to do X given the same assumptions". [sent-23, score-0.571]
</p><p>18 A fallback analysis requires a change in assumption invalidating (2) and the new thing that it shows for (1) is subtle: that two existing guarantees can hold for the same algorithm. [sent-24, score-0.665]
</p><p>19 My hope here is that this subtlety becomes better appreciated in time--making useful algorithms has a fundamental sexiness of it's own. [sent-25, score-0.27]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('fallback', 0.309), ('postconditions', 0.309), ('preconditions', 0.309), ('analysis', 0.264), ('precondition', 0.206), ('psychologically', 0.183), ('forms', 0.171), ('algorithm', 0.149), ('assuming', 0.13), ('algorithms', 0.128), ('ideal', 0.124), ('simultaneously', 0.121), ('assume', 0.116), ('given', 0.109), ('weak', 0.102), ('assumptions', 0.099), ('accompanying', 0.092), ('invalidating', 0.092), ('justified', 0.092), ('obeys', 0.092), ('sexy', 0.092), ('unresolved', 0.092), ('reasonable', 0.091), ('joy', 0.085), ('catastrophic', 0.085), ('inclined', 0.085), ('prone', 0.085), ('theoretical', 0.084), ('ano', 0.08), ('optimality', 0.08), ('nevertheless', 0.08), ('occur', 0.076), ('appreciated', 0.076), ('datastructure', 0.076), ('mis', 0.076), ('resource', 0.076), ('routinely', 0.076), ('valid', 0.076), ('strong', 0.074), ('routine', 0.073), ('queries', 0.071), ('derived', 0.071), ('criticism', 0.069), ('neither', 0.069), ('theoretically', 0.067), ('dimensionality', 0.067), ('hope', 0.066), ('false', 0.065), ('proving', 0.065), ('tested', 0.065)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="126-tfidf-1" href="../hunch_net-2005/hunch_net-2005-10-26-Fallback_Analysis_is_a_Secret_to_Useful_Algorithms.html">126 hunch net-2005-10-26-Fallback Analysis is a Secret to Useful Algorithms</a></p>
<p>Introduction: The ideal of theoretical algorithm analysis is to construct an algorithm with
accompanying optimality theorems proving that it is a useful algorithm. This
ideal often fails, particularly for learning algorithms and theory. The
general form of a theorem is:IfpreconditionsThenpostconditionsWhen we design
learning algorithms it is very common to come up with precondition assumptions
such as "the data is IID", "the learning problem is drawn from a known
distribution over learning problems", or "there is a perfect classifier". All
of these example preconditions can be false for real-world problems in ways
that are not easily detectable. This means that algorithms derived and
justified by these very common forms of analysis may be prone to catastrophic
failure in routine (mis)application.Wecanhope for better. Several different
kinds of learning algorithm analysis have been developed some of which have
fewer preconditions. Simply demanding that these forms of analysis be used may
be too stron</p><p>2 0.13536531 <a title="126-tfidf-2" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>Introduction: One thing which is clear on a little reflection is that there exists a single
master learning problem capable of encoding essentially all learning problems.
This problem is of course a very general sort of reinforcement learning where
the world interacts with an agent as:The world announces an observationx.The
agent makes a choicea.The world announces a rewardr.The goal here is to
maximize the sum of the rewards over the time of the agent. No particular
structure relatingxtoaoratoris implied by this setting so we do not know
effective general algorithms for the agent. It's very easy to prove lower
bounds showing that an agent cannot hope to succeed here--just consider the
case where actions are unrelated to rewards. Nevertheless, there is a real
sense in which essentially all forms of life are agents operating in this
setting, somehow succeeding. The gap between these observations drives
research--How can we find tractable specializations of the master problem
general enough to provide</p><p>3 0.13205081 <a title="126-tfidf-3" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>Introduction: I've had serious conversations with several people who believe that the theory
in machine learning is "only useful for getting papers published". That's a
compelling statement, as I've seen many papers where the algorithm clearly
came first, and the theoretical justification for it came second, purely as a
perceived means to improve the chance of publication.Naturally, I disagree and
believe that learning theory has much more substantial applications.Even in
core learning algorithm design, I've found learning theory to be useful,
although it's application is more subtle than many realize. The most
straightforward applications can fail, because (as expectation suggests) worst
case bounds tend to be loose in practice (*). In my experience, considering
learning theory when designing an algorithm has two important effects in
practice:It can help make your algorithm behave right at a crude level of
analysis, leaving finer details to tuning or common sense. The best example I
have of this is</p><p>4 0.12028793 <a title="126-tfidf-4" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>Introduction: Few would mistake the process of academic paper review for a fair process, but
sometimes the unfairness seems particularly striking. This is most easily seen
by comparison:PaperBanditronOffset TreeNotesProblem ScopeMulticlass problems
where only the loss of one choice can be probed.Strictly greater: Cost
sensitive multiclass problems where only the loss of one choice can be
probed.Often generalizations don't matter. That's not the case here, since
every plausible application I've thought of involves loss functions
substantially different from 0/1.What's newAnalysis and ExperimentsAlgorithm,
Analysis, and ExperimentsAs far as I know, the essence of the more general
problem was first stated and analyzed with theEXP4 algorithm (page 16)(1998).
It's also the time horizon 1 simplification of the Reinforcement Learning
setting for therandom trajectory method (page 15)(2002). The Banditron
algorithm itself is functionally identical toOne-Step RL with Traces (page
122)(2003) inBianca's thesis</p><p>5 0.1179626 <a title="126-tfidf-5" href="../hunch_net-2009/hunch_net-2009-01-07-Interesting_Papers_at_SODA_2009.html">334 hunch net-2009-01-07-Interesting Papers at SODA 2009</a></p>
<p>Introduction: Several talks seem potentially interesting to ML folks at this year's SODA
.Maria-Florina Balcan,Avrim Blum, andAnupam Gupta,Approximate Clustering
without the Approximation. This paper gives reasonable algorithms with
provable approximation guarantees for k-median and other notions of
clustering. It's conceptually interesting, because it's the second example
I've seen where NP hardness is subverted by changing the problem definition
subtle but reasonable way. Essentially, they show that if any near-
approximation to an optimal solution is good, then it's computationally easy
to find a near-optimal solution. This subtle shift bears serious thought. A
similar one occurred inour ranking paperwith respect to minimum feedback
arcset. With two known examples, it suggests that many more NP-complete
problems might be finessed into irrelevance in this style.Yury
LifshitsandShengyu Zhang,Combinatorial Algorithms for Nearest Neighbors, Near-
Duplicates, and Small-World Design. The basic idea of</p><p>6 0.11302476 <a title="126-tfidf-6" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>7 0.10981103 <a title="126-tfidf-7" href="../hunch_net-2005/hunch_net-2005-01-31-Watchword%3A_Assumption.html">7 hunch net-2005-01-31-Watchword: Assumption</a></p>
<p>8 0.10891486 <a title="126-tfidf-8" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>9 0.10610657 <a title="126-tfidf-9" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>10 0.10534442 <a title="126-tfidf-10" href="../hunch_net-2005/hunch_net-2005-02-25-Problem%3A_Online_Learning.html">28 hunch net-2005-02-25-Problem: Online Learning</a></p>
<p>11 0.10418551 <a title="126-tfidf-11" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>12 0.10399514 <a title="126-tfidf-12" href="../hunch_net-2006/hunch_net-2006-05-05-An_ICML_reject.html">177 hunch net-2006-05-05-An ICML reject</a></p>
<p>13 0.10093176 <a title="126-tfidf-13" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>14 0.10078014 <a title="126-tfidf-14" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>15 0.098362863 <a title="126-tfidf-15" href="../hunch_net-2005/hunch_net-2005-03-04-The_Big_O_and_Constants_in_Learning.html">35 hunch net-2005-03-04-The Big O and Constants in Learning</a></p>
<p>16 0.09783271 <a title="126-tfidf-16" href="../hunch_net-2005/hunch_net-2005-04-16-Which_Assumptions_are_Reasonable%3F.html">57 hunch net-2005-04-16-Which Assumptions are Reasonable?</a></p>
<p>17 0.094687283 <a title="126-tfidf-17" href="../hunch_net-2005/hunch_net-2005-12-07-Is_the_Google_way_the_way_for_machine_learning%3F.html">136 hunch net-2005-12-07-Is the Google way the way for machine learning?</a></p>
<p>18 0.092774644 <a title="126-tfidf-18" href="../hunch_net-2005/hunch_net-2005-11-02-Progress_in_Active_Learning.html">127 hunch net-2005-11-02-Progress in Active Learning</a></p>
<p>19 0.09209048 <a title="126-tfidf-19" href="../hunch_net-2006/hunch_net-2006-05-16-The_value_of_the_orthodox_view_of_Boosting.html">179 hunch net-2006-05-16-The value of the orthodox view of Boosting</a></p>
<p>20 0.091896683 <a title="126-tfidf-20" href="../hunch_net-2005/hunch_net-2005-03-22-Active_learning.html">45 hunch net-2005-03-22-Active learning</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.218), (1, -0.083), (2, -0.022), (3, -0.03), (4, -0.066), (5, -0.006), (6, 0.001), (7, 0.002), (8, -0.07), (9, -0.008), (10, -0.015), (11, -0.044), (12, 0.004), (13, -0.023), (14, -0.032), (15, -0.038), (16, -0.02), (17, -0.04), (18, 0.047), (19, 0.068), (20, 0.043), (21, -0.032), (22, 0.064), (23, -0.03), (24, 0.029), (25, 0.04), (26, 0.049), (27, 0.037), (28, 0.096), (29, -0.007), (30, 0.011), (31, 0.058), (32, -0.039), (33, 0.004), (34, -0.019), (35, -0.056), (36, -0.061), (37, 0.068), (38, 0.023), (39, 0.018), (40, -0.129), (41, -0.03), (42, -0.011), (43, 0.032), (44, 0.029), (45, 0.007), (46, -0.012), (47, -0.02), (48, 0.067), (49, 0.017)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96652359 <a title="126-lsi-1" href="../hunch_net-2005/hunch_net-2005-10-26-Fallback_Analysis_is_a_Secret_to_Useful_Algorithms.html">126 hunch net-2005-10-26-Fallback Analysis is a Secret to Useful Algorithms</a></p>
<p>Introduction: The ideal of theoretical algorithm analysis is to construct an algorithm with
accompanying optimality theorems proving that it is a useful algorithm. This
ideal often fails, particularly for learning algorithms and theory. The
general form of a theorem is:IfpreconditionsThenpostconditionsWhen we design
learning algorithms it is very common to come up with precondition assumptions
such as "the data is IID", "the learning problem is drawn from a known
distribution over learning problems", or "there is a perfect classifier". All
of these example preconditions can be false for real-world problems in ways
that are not easily detectable. This means that algorithms derived and
justified by these very common forms of analysis may be prone to catastrophic
failure in routine (mis)application.Wecanhope for better. Several different
kinds of learning algorithm analysis have been developed some of which have
fewer preconditions. Simply demanding that these forms of analysis be used may
be too stron</p><p>2 0.78258139 <a title="126-lsi-2" href="../hunch_net-2005/hunch_net-2005-04-16-Which_Assumptions_are_Reasonable%3F.html">57 hunch net-2005-04-16-Which Assumptions are Reasonable?</a></p>
<p>Introduction: One of the most confusing things about understanding learning theory is the
vast array of differing assumptions. Some critical thought about which of
these assumptions are reasonable for real-world problems may be useful.Before
we even start thinking about assumptions, it's important to realize that the
word hasmultiple meanings. The meaning used here is "assumption = axiom" (i.e.
something you can not verify).AssumptionReasonable?Which
analysis?Example/notesIndependent and Identically Distributed
DataSometimesPAC,ERM,Prediction bounds,statisticsTheKDD cup 2004 physics
datasetis plausibly IID data. There are a number of situations which are
"almost IID" in the sense that IID analysis results in correct intuitions.
Unreasonable in adversarial situations (stock market, war, etcâ&euro;Ś)Independently
Distributed DataMore than IID, but still only sometimesonline->batch
conversionLosing "identical" can be helpful in situations where you have a
cyclic process generating data.Finite exchangeability</p><p>3 0.77872485 <a title="126-lsi-3" href="../hunch_net-2005/hunch_net-2005-01-31-Watchword%3A_Assumption.html">7 hunch net-2005-01-31-Watchword: Assumption</a></p>
<p>Introduction: "Assumption" is another word to be careful with in machine learning because it
is used in several ways.Assumption = BiasThere are several ways to see that
some form of 'bias' (= preferring of one solution over another) is necessary.
This is obvious in an adversarial setting. A good bit of work has been
expended explaining this in other settings with "no free lunch" theorems. This
is a usage specialized to learning which is particularly common when talking
about priors for Bayesian Learning.Assumption = "if" of a theoremThe
assumptions are the 'if' part of the 'if-then' in a theorem. This is a fairly
common usage.Assumption = AxiomThe assumptions are the things that we assume
are true, but which we cannot verify. Examples are "the IID assumption" or "my
problem is a DNF on a small number of bits". This is the usage which I
prefer.One difficulty with any use of the word "assumption" is that you often
encounter "ifassumptionthenconclusionso ifnot assumptionthennot conclusion".
This is inc</p><p>4 0.72879612 <a title="126-lsi-4" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>Introduction: Many people in Machine Learning don't fully understand the impact of
computation, as demonstrated by a lack ofbig-Oanalysis of new learning
algorithms. This is important--some current active research programs are
fundamentally flawed w.r.t. computation, and other research programs are
directly motivated by it. When considering a learning algorithm, I think about
the following questions:How does the learning algorithm scale with the number
of examplesm? Any algorithm using all of the data is at leastO(m), but in many
cases this isO(m2)(naive nearest neighbor for self-prediction) or unknown
(k-means or many other optimization algorithms). The unknown case is very
common, and it can mean (for example) that the algorithm isn't convergent or
simply that the amount of computation isn't controlled.The above question can
also be asked for test cases. In some applications, test-time performance is
of great importance.How does the algorithm scale with the number of
featuresnper example? Many sec</p><p>5 0.69897592 <a title="126-lsi-5" href="../hunch_net-2005/hunch_net-2005-08-22-Do_you_believe_in_induction%3F.html">104 hunch net-2005-08-22-Do you believe in induction?</a></p>
<p>Introduction: Foster Provostgave a talk at the ICMLmetalearning workshopon "metalearning"
and the "no free lunch theorem" which seems worth summarizing.As a review: the
no free lunch theorem is the most complicated way we know of to say that
abiasis required in order to learn. The simplest way to see this is in a
nonprobabilistic setting. If you are given examples of the form(x,y)and you
wish to predictyfromxthen any prediction mechanism errs half the time in
expectation over all sequences of examples. The proof of this is very simple:
on every example a predictor must make some prediction and by symmetry over
the set of sequences it will be wrong half the time and right half the time.
The basic idea of this proof has been applied to many other settings.The
simplistic interpretation of this theorem which many people jump to is
"machine learning is dead" since there can be no single learning algorithm
which can solve all learning problems. This is the wrong way to think about
it. In the real world, w</p><p>6 0.68798459 <a title="126-lsi-6" href="../hunch_net-2005/hunch_net-2005-04-06-Structured_Regret_Minimization.html">53 hunch net-2005-04-06-Structured Regret Minimization</a></p>
<p>7 0.684937 <a title="126-lsi-7" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>8 0.67750061 <a title="126-lsi-8" href="../hunch_net-2005/hunch_net-2005-12-07-Is_the_Google_way_the_way_for_machine_learning%3F.html">136 hunch net-2005-12-07-Is the Google way the way for machine learning?</a></p>
<p>9 0.67626303 <a title="126-lsi-9" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>10 0.67271906 <a title="126-lsi-10" href="../hunch_net-2007/hunch_net-2007-07-06-Idempotent-capable_Predictors.html">253 hunch net-2007-07-06-Idempotent-capable Predictors</a></p>
<p>11 0.66293931 <a title="126-lsi-11" href="../hunch_net-2006/hunch_net-2006-11-22-Explicit_Randomization_in_Learning_algorithms.html">219 hunch net-2006-11-22-Explicit Randomization in Learning algorithms</a></p>
<p>12 0.65186256 <a title="126-lsi-12" href="../hunch_net-2009/hunch_net-2009-01-07-Interesting_Papers_at_SODA_2009.html">334 hunch net-2009-01-07-Interesting Papers at SODA 2009</a></p>
<p>13 0.64395326 <a title="126-lsi-13" href="../hunch_net-2005/hunch_net-2005-02-25-Problem%3A_Online_Learning.html">28 hunch net-2005-02-25-Problem: Online Learning</a></p>
<p>14 0.64357579 <a title="126-lsi-14" href="../hunch_net-2009/hunch_net-2009-04-02-Asymmophobia.html">348 hunch net-2009-04-02-Asymmophobia</a></p>
<p>15 0.63812351 <a title="126-lsi-15" href="../hunch_net-2005/hunch_net-2005-03-04-The_Big_O_and_Constants_in_Learning.html">35 hunch net-2005-03-04-The Big O and Constants in Learning</a></p>
<p>16 0.63311911 <a title="126-lsi-16" href="../hunch_net-2006/hunch_net-2006-01-13-Benchmarks_for_RL.html">148 hunch net-2006-01-13-Benchmarks for RL</a></p>
<p>17 0.63010663 <a title="126-lsi-17" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>18 0.62576371 <a title="126-lsi-18" href="../hunch_net-2006/hunch_net-2006-05-16-The_value_of_the_orthodox_view_of_Boosting.html">179 hunch net-2006-05-16-The value of the orthodox view of Boosting</a></p>
<p>19 0.6226123 <a title="126-lsi-19" href="../hunch_net-2006/hunch_net-2006-01-30-Should_the_Input_Representation_be_a_Vector%3F.html">152 hunch net-2006-01-30-Should the Input Representation be a Vector?</a></p>
<p>20 0.61968273 <a title="126-lsi-20" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(35, 0.043), (42, 0.275), (60, 0.305), (61, 0.049), (68, 0.055), (74, 0.137), (76, 0.018), (95, 0.032)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.95694822 <a title="126-lda-1" href="../hunch_net-2005/hunch_net-2005-05-29-Bad_ideas.html">76 hunch net-2005-05-29-Bad ideas</a></p>
<p>Introduction: I found these two essays on bad ideas interesting. Neither of these is written
from the viewpoint of research, but they are both highly relevant.Why smart
people have bad ideasby Paul GrahamWhy smart people defend bad ideasby Scott
Berkun (which appeared onslashdot)In my experience, bad ideas are
commonandover confidence in ideas is common. This overconfidence can take
either the form of excessive condemnation or excessive praise. Some of this is
necessary to the process of research. For example, some overconfidence in the
value of your own research is expected and probably necessary to motivate your
own investigation. Since research is a rather risky business, much of it does
not pan out. Learning to accept when something does not pan out is a critical
skill which is sometimes never acquired.Excessive condemnation can be a real
ill when it's encountered. This has two effects:When the penalty for being
wrong is too large, it means people have a great investment in defending
"their" ide</p><p>2 0.91463476 <a title="126-lda-2" href="../hunch_net-2012/hunch_net-2012-07-09-Videolectures.html">469 hunch net-2012-07-09-Videolectures</a></p>
<p>Introduction: Yaserpoints out some nicelyvideotaped machine learning lecturesatCaltech.
Yaser taught me machine learning, and I always found the lectures clear and
interesting, so I expect many people can benefit from watching. Relative
toAndrew Ng'sML classthere are somewhat different areas of emphasis but the
topic is the same, so picking and choosing the union may be helpful.</p><p>same-blog 3 0.86143434 <a title="126-lda-3" href="../hunch_net-2005/hunch_net-2005-10-26-Fallback_Analysis_is_a_Secret_to_Useful_Algorithms.html">126 hunch net-2005-10-26-Fallback Analysis is a Secret to Useful Algorithms</a></p>
<p>Introduction: The ideal of theoretical algorithm analysis is to construct an algorithm with
accompanying optimality theorems proving that it is a useful algorithm. This
ideal often fails, particularly for learning algorithms and theory. The
general form of a theorem is:IfpreconditionsThenpostconditionsWhen we design
learning algorithms it is very common to come up with precondition assumptions
such as "the data is IID", "the learning problem is drawn from a known
distribution over learning problems", or "there is a perfect classifier". All
of these example preconditions can be false for real-world problems in ways
that are not easily detectable. This means that algorithms derived and
justified by these very common forms of analysis may be prone to catastrophic
failure in routine (mis)application.Wecanhope for better. Several different
kinds of learning algorithm analysis have been developed some of which have
fewer preconditions. Simply demanding that these forms of analysis be used may
be too stron</p><p>4 0.84445822 <a title="126-lda-4" href="../hunch_net-2006/hunch_net-2006-07-25-Upcoming_conference.html">198 hunch net-2006-07-25-Upcoming conference</a></p>
<p>Introduction: The Workshop for Women in Machine Learning will be held in San Diego on
October 4, 2006.For details see the workshop
website:http://www.seas.upenn.edu/~wiml/</p><p>5 0.75831026 <a title="126-lda-5" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>Introduction: Claireasked me to be on the SODA program committee this year, which was quite
a bit of work.I had a relatively light load--merely 49 theory papers. Many of
these papers were not on subjects that I was expert about, so (as is common
for theory conferences) I found various reviewers that I trusted to help
review the papers. I ended up reviewing about 1/3 personally. There were a
couple instances where I ended up overruling a subreviewer whose logic seemed
off, but otherwise I generally let their reviews stand.There are some
differences in standards for paper reviews between the machine learning and
theory communities. In machine learning it is expected that a review be
detailed, while in the theory community this is often not the case. Every
paper given to me ended up with a review varying between somewhat and very
detailed.I'm sure not every author was happy with the outcome. While we did
our best to make good decisions, they were difficult decisions to make. For
example, if there is a</p><p>6 0.70970494 <a title="126-lda-6" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>7 0.70509762 <a title="126-lda-7" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>8 0.70290464 <a title="126-lda-8" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>9 0.70272022 <a title="126-lda-9" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>10 0.70257378 <a title="126-lda-10" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>11 0.70196813 <a title="126-lda-11" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>12 0.70150924 <a title="126-lda-12" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>13 0.69939721 <a title="126-lda-13" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>14 0.69922262 <a title="126-lda-14" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>15 0.69794768 <a title="126-lda-15" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>16 0.69718957 <a title="126-lda-16" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>17 0.69628274 <a title="126-lda-17" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>18 0.69599712 <a title="126-lda-18" href="../hunch_net-2006/hunch_net-2006-05-05-An_ICML_reject.html">177 hunch net-2006-05-05-An ICML reject</a></p>
<p>19 0.69554198 <a title="126-lda-19" href="../hunch_net-2005/hunch_net-2005-03-17-Going_all_the_Way%2C_Sometimes.html">42 hunch net-2005-03-17-Going all the Way, Sometimes</a></p>
<p>20 0.69415474 <a title="126-lda-20" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">23 hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
