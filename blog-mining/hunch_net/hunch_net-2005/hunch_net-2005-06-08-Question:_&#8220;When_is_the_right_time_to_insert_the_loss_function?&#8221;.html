<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>79 hunch net-2005-06-08-Question: &#8220;When is the right time to insert the loss function?&#8221;</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-79" href="#">hunch_net-2005-79</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>79 hunch net-2005-06-08-Question: &#8220;When is the right time to insert the loss function?&#8221;</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-79-html" href="http://hunch.net/?p=85">html</a></p><p>Introduction: Hal  asks   a very good question: “When is the right time to insert the loss function?”  In particular, should it be used at testing time or at training time?
 
When the world imposes a loss on us, the standard Bayesian recipe is to predict the (conditional) probability of each possibility and then choose the possibility which minimizes the expected loss.  In contrast, as the  confusion  over “loss = money lost” or “loss = the thing you optimize” might indicate, many people ignore the Bayesian approach and simply optimize their loss (or a close proxy for their loss) over the representation on the training set.
 
The best answer I can give is “it’s unclear, but I prefer optimizing the loss at training time”.  My experience is that optimizing the loss in the most direct manner possible typically yields best performance.  This question is related to a basic principle which both  Yann LeCun (applied) and  Vladimir Vapnik (theoretical) advocate: “solve the simplest prediction problem that s</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Hal  asks   a very good question: “When is the right time to insert the loss function? [sent-1, score-0.838]
</p><p>2 ”  In particular, should it be used at testing time or at training time? [sent-2, score-0.345]
</p><p>3 When the world imposes a loss on us, the standard Bayesian recipe is to predict the (conditional) probability of each possibility and then choose the possibility which minimizes the expected loss. [sent-3, score-1.321]
</p><p>4 In contrast, as the  confusion  over “loss = money lost” or “loss = the thing you optimize” might indicate, many people ignore the Bayesian approach and simply optimize their loss (or a close proxy for their loss) over the representation on the training set. [sent-4, score-1.381]
</p><p>5 The best answer I can give is “it’s unclear, but I prefer optimizing the loss at training time”. [sent-5, score-1.041]
</p><p>6 My experience is that optimizing the loss in the most direct manner possible typically yields best performance. [sent-6, score-1.0]
</p><p>7 This question is related to a basic principle which both  Yann LeCun (applied) and  Vladimir Vapnik (theoretical) advocate: “solve the simplest prediction problem that solves the problem”. [sent-7, score-0.503]
</p><p>8 (One difficulty with this principle is that ‘simplest’ is difficult to define in a satisfying way. [sent-8, score-0.336]
</p><p>9 )   One reason why it’s unclear is that optimizing an arbitrary loss is not an easy thing for a learning algorithm to cope with. [sent-9, score-1.102]
</p><p>10 Learning reductions  (which I am a big fan of) give a mechanism for doing this, but they are new and relatively untried. [sent-10, score-0.231]
</p><p>11 Drew Bagnell adds: Another approach to integrating loss functions into learning is to try to re-derive ideas about probability theory appropriate for other loss functions. [sent-11, score-1.36]
</p><p>12 Dawid present a variant on  maximum entropy learning . [sent-14, score-0.251]
</p><p>13 Unfortunately, it’s even less clear how often these approaches lead to efficient algorithms. [sent-15, score-0.074]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('loss', 0.543), ('optimizing', 0.223), ('principle', 0.175), ('training', 0.168), ('possibility', 0.163), ('simplest', 0.151), ('optimize', 0.146), ('unclear', 0.142), ('fan', 0.124), ('minimizes', 0.115), ('vapnik', 0.115), ('recipe', 0.115), ('imposes', 0.115), ('probability', 0.107), ('give', 0.107), ('vladimir', 0.103), ('proxy', 0.103), ('indicate', 0.103), ('bayesian', 0.103), ('thing', 0.101), ('time', 0.1), ('asks', 0.099), ('bagnell', 0.099), ('adds', 0.099), ('solves', 0.099), ('insert', 0.096), ('cope', 0.093), ('integrating', 0.093), ('drew', 0.09), ('entropy', 0.088), ('peter', 0.088), ('satisfying', 0.088), ('confusion', 0.085), ('lecun', 0.085), ('ignore', 0.085), ('variant', 0.084), ('instance', 0.082), ('hal', 0.08), ('yann', 0.079), ('maximum', 0.079), ('direct', 0.079), ('yields', 0.079), ('question', 0.078), ('lost', 0.077), ('testing', 0.077), ('manner', 0.076), ('money', 0.076), ('lead', 0.074), ('approach', 0.074), ('define', 0.073)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999976 <a title="79-tfidf-1" href="../hunch_net-2005/hunch_net-2005-06-08-Question%3A_%26%238220%3BWhen_is_the_right_time_to_insert_the_loss_function%3F%26%238221%3B.html">79 hunch net-2005-06-08-Question: &#8220;When is the right time to insert the loss function?&#8221;</a></p>
<p>Introduction: Hal  asks   a very good question: “When is the right time to insert the loss function?”  In particular, should it be used at testing time or at training time?
 
When the world imposes a loss on us, the standard Bayesian recipe is to predict the (conditional) probability of each possibility and then choose the possibility which minimizes the expected loss.  In contrast, as the  confusion  over “loss = money lost” or “loss = the thing you optimize” might indicate, many people ignore the Bayesian approach and simply optimize their loss (or a close proxy for their loss) over the representation on the training set.
 
The best answer I can give is “it’s unclear, but I prefer optimizing the loss at training time”.  My experience is that optimizing the loss in the most direct manner possible typically yields best performance.  This question is related to a basic principle which both  Yann LeCun (applied) and  Vladimir Vapnik (theoretical) advocate: “solve the simplest prediction problem that s</p><p>2 0.45249856 <a title="79-tfidf-2" href="../hunch_net-2005/hunch_net-2005-02-01-Watchword%3A_Loss.html">9 hunch net-2005-02-01-Watchword: Loss</a></p>
<p>Introduction: A loss function is some function which, for any example, takes a prediction and the correct prediction, and determines how much loss is incurred.  (People sometimes attempt to optimize functions of more than one example such as “area under the ROC curve” or “harmonic mean of precision and recall”.)  Typically we try to find predictors that minimize loss.  
 
There seems to be a strong dichotomy between two views of what “loss” means in learning.
  
  Loss is determined by the problem.  Loss is a part of the specification of the learning problem.  Examples of problems specified by the loss function include “binary classification”, “multiclass classification”, “importance weighted classification”, “l 2  regression”, etc…  This is the decision theory view of what loss means, and the view that I prefer. 
  Loss is determined by the solution.  To solve a problem, you optimize some particular loss function  not  given by the problem.  Examples of these loss functions are “hinge loss” (for SV</p><p>3 0.41569108 <a title="79-tfidf-3" href="../hunch_net-2009/hunch_net-2009-02-04-Optimal_Proxy_Loss_for_Classification.html">341 hunch net-2009-02-04-Optimal Proxy Loss for Classification</a></p>
<p>Introduction: Many people in machine learning take advantage of the notion of a proxy loss: A loss function which is much easier to optimize computationally than the loss function imposed by the world.  A canonical example is when we want to learn a weight vector  w  and predict according to a dot product  f w (x)= sum i  w i x i   
where optimizing squared loss  (y-f w (x)) 2   over many samples is much more tractable than optimizing 0-1 loss  I(y = Threshold(f w (x) – 0.5)) .
 
While the computational advantages of optimizing a proxy loss are substantial, we are curious: which proxy loss is best?  The answer of course depends on what the real loss imposed by the world is.  For 0-1 loss classification, there are adherents to many choices:
  
 Log loss.  If we confine the prediction to  [0,1] , we can treat it as a predicted probability that the label is  1 , and measure loss according to  log 1/p’(y|x)  where  p’(y|x)  is the predicted probability of the observed label.  A standard method for confi</p><p>4 0.36889857 <a title="79-tfidf-4" href="../hunch_net-2007/hunch_net-2007-05-12-Loss_Function_Semantics.html">245 hunch net-2007-05-12-Loss Function Semantics</a></p>
<p>Introduction: Some loss functions have a meaning, which can be understood in a manner independent of the loss function itself.  
  
 Optimizing squared loss  l sq (y,y’)=(y-y’) 2   means predicting the (conditional) mean of  y . 
 Optimizing absolute value loss  l av (y,y’)=|y-y’|  means predicting the (conditional) median of  y .  Variants can  handle other quantiles .  0/1 loss for classification is a special case. 
 Optimizing log loss  l log (y,y’)=log (1/Pr z~y’ (z=y))  means minimizing the description length of  y . 
  
The semantics (= meaning) of the loss are made explicit by a theorem in each case.  For squared loss, we can prove a theorem of the form: 
For all distributions  D  over  Y , if    y’ = arg min y’  E y ~ D  l sq  (y,y’)   then   y’ = E y~D  y  
 
Similar theorems hold for the other examples above, and they can all be extended to predictors of  y’  for distributions  D  over a context  X  and a value  Y .
 
There are 3 points to this post.
  
 Everyone doing general machine lear</p><p>5 0.25213522 <a title="79-tfidf-5" href="../hunch_net-2007/hunch_net-2007-11-28-Computational_Consequences_of_Classification.html">274 hunch net-2007-11-28-Computational Consequences of Classification</a></p>
<p>Introduction: In the  regression vs classification debate , I’m adding a new “pro” to classification.  It seems there are computational shortcuts available for classification which simply aren’t available for regression.  This arises in several situations.
  
 In  active learning  it is sometimes possible to find an  e  error classifier with just  log(e)  labeled samples.    Only much more modest improvements appear to be achievable for squared loss regression.  The essential reason is that the loss function on many examples is flat with respect to large variations in the parameter spaces of a learned classifier, which implies that many of these classifiers do not need to be considered.  In contrast, for squared loss regression, most substantial variations in the parameter space influence the loss at most points. 
 In budgeted learning, where there is either a computational time constraint or a feature cost constraint, a classifier can sometimes be learned to very high accuracy under the constraints</p><p>6 0.23878857 <a title="79-tfidf-6" href="../hunch_net-2007/hunch_net-2007-08-19-Choice_of_Metrics.html">259 hunch net-2007-08-19-Choice of Metrics</a></p>
<p>7 0.19773073 <a title="79-tfidf-7" href="../hunch_net-2009/hunch_net-2009-10-10-ALT_2009.html">374 hunch net-2009-10-10-ALT 2009</a></p>
<p>8 0.19392745 <a title="79-tfidf-8" href="../hunch_net-2005/hunch_net-2005-05-21-What_is_the_right_form_of_modularity_in_structured_prediction%3F.html">74 hunch net-2005-05-21-What is the right form of modularity in structured prediction?</a></p>
<p>9 0.18082498 <a title="79-tfidf-9" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>10 0.17174271 <a title="79-tfidf-10" href="../hunch_net-2005/hunch_net-2005-11-07-Prediction_Competitions.html">129 hunch net-2005-11-07-Prediction Competitions</a></p>
<p>11 0.16365212 <a title="79-tfidf-11" href="../hunch_net-2007/hunch_net-2007-03-15-Alternative_Machine_Learning_Reductions_Definitions.html">236 hunch net-2007-03-15-Alternative Machine Learning Reductions Definitions</a></p>
<p>12 0.1606624 <a title="79-tfidf-12" href="../hunch_net-2005/hunch_net-2005-08-18-SVM_Adaptability.html">103 hunch net-2005-08-18-SVM Adaptability</a></p>
<p>13 0.15502939 <a title="79-tfidf-13" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>14 0.13535511 <a title="79-tfidf-14" href="../hunch_net-2006/hunch_net-2006-03-23-The_Approximation_Argument.html">165 hunch net-2006-03-23-The Approximation Argument</a></p>
<p>15 0.1350348 <a title="79-tfidf-15" href="../hunch_net-2010/hunch_net-2010-12-26-NIPS_2010.html">420 hunch net-2010-12-26-NIPS 2010</a></p>
<p>16 0.13168116 <a title="79-tfidf-16" href="../hunch_net-2006/hunch_net-2006-05-05-An_ICML_reject.html">177 hunch net-2006-05-05-An ICML reject</a></p>
<p>17 0.13023402 <a title="79-tfidf-17" href="../hunch_net-2005/hunch_net-2005-05-06-Don%26%238217%3Bt_mix_the_solution_into_the_problem.html">67 hunch net-2005-05-06-Don&#8217;t mix the solution into the problem</a></p>
<p>18 0.12865824 <a title="79-tfidf-18" href="../hunch_net-2005/hunch_net-2005-09-08-Online_Learning_as_the_Mathematics_of_Accountability.html">109 hunch net-2005-09-08-Online Learning as the Mathematics of Accountability</a></p>
<p>19 0.12852216 <a title="79-tfidf-19" href="../hunch_net-2009/hunch_net-2009-09-21-Netflix_finishes_%28and_starts%29.html">371 hunch net-2009-09-21-Netflix finishes (and starts)</a></p>
<p>20 0.12427854 <a title="79-tfidf-20" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">23 hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.229), (1, 0.213), (2, 0.129), (3, -0.167), (4, -0.376), (5, 0.173), (6, -0.184), (7, 0.05), (8, 0.116), (9, 0.019), (10, 0.083), (11, -0.102), (12, -0.053), (13, 0.063), (14, 0.001), (15, 0.013), (16, 0.023), (17, -0.011), (18, -0.019), (19, 0.009), (20, 0.025), (21, 0.001), (22, -0.064), (23, -0.017), (24, 0.029), (25, 0.037), (26, -0.015), (27, 0.013), (28, 0.024), (29, 0.054), (30, 0.014), (31, 0.015), (32, 0.004), (33, -0.01), (34, -0.055), (35, -0.008), (36, 0.008), (37, -0.004), (38, -0.039), (39, 0.003), (40, 0.019), (41, -0.001), (42, -0.01), (43, 0.028), (44, 0.07), (45, -0.051), (46, 0.016), (47, -0.026), (48, -0.036), (49, 0.028)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99013561 <a title="79-lsi-1" href="../hunch_net-2005/hunch_net-2005-06-08-Question%3A_%26%238220%3BWhen_is_the_right_time_to_insert_the_loss_function%3F%26%238221%3B.html">79 hunch net-2005-06-08-Question: &#8220;When is the right time to insert the loss function?&#8221;</a></p>
<p>Introduction: Hal  asks   a very good question: “When is the right time to insert the loss function?”  In particular, should it be used at testing time or at training time?
 
When the world imposes a loss on us, the standard Bayesian recipe is to predict the (conditional) probability of each possibility and then choose the possibility which minimizes the expected loss.  In contrast, as the  confusion  over “loss = money lost” or “loss = the thing you optimize” might indicate, many people ignore the Bayesian approach and simply optimize their loss (or a close proxy for their loss) over the representation on the training set.
 
The best answer I can give is “it’s unclear, but I prefer optimizing the loss at training time”.  My experience is that optimizing the loss in the most direct manner possible typically yields best performance.  This question is related to a basic principle which both  Yann LeCun (applied) and  Vladimir Vapnik (theoretical) advocate: “solve the simplest prediction problem that s</p><p>2 0.9620173 <a title="79-lsi-2" href="../hunch_net-2005/hunch_net-2005-02-01-Watchword%3A_Loss.html">9 hunch net-2005-02-01-Watchword: Loss</a></p>
<p>Introduction: A loss function is some function which, for any example, takes a prediction and the correct prediction, and determines how much loss is incurred.  (People sometimes attempt to optimize functions of more than one example such as “area under the ROC curve” or “harmonic mean of precision and recall”.)  Typically we try to find predictors that minimize loss.  
 
There seems to be a strong dichotomy between two views of what “loss” means in learning.
  
  Loss is determined by the problem.  Loss is a part of the specification of the learning problem.  Examples of problems specified by the loss function include “binary classification”, “multiclass classification”, “importance weighted classification”, “l 2  regression”, etc…  This is the decision theory view of what loss means, and the view that I prefer. 
  Loss is determined by the solution.  To solve a problem, you optimize some particular loss function  not  given by the problem.  Examples of these loss functions are “hinge loss” (for SV</p><p>3 0.93550909 <a title="79-lsi-3" href="../hunch_net-2009/hunch_net-2009-02-04-Optimal_Proxy_Loss_for_Classification.html">341 hunch net-2009-02-04-Optimal Proxy Loss for Classification</a></p>
<p>Introduction: Many people in machine learning take advantage of the notion of a proxy loss: A loss function which is much easier to optimize computationally than the loss function imposed by the world.  A canonical example is when we want to learn a weight vector  w  and predict according to a dot product  f w (x)= sum i  w i x i   
where optimizing squared loss  (y-f w (x)) 2   over many samples is much more tractable than optimizing 0-1 loss  I(y = Threshold(f w (x) – 0.5)) .
 
While the computational advantages of optimizing a proxy loss are substantial, we are curious: which proxy loss is best?  The answer of course depends on what the real loss imposed by the world is.  For 0-1 loss classification, there are adherents to many choices:
  
 Log loss.  If we confine the prediction to  [0,1] , we can treat it as a predicted probability that the label is  1 , and measure loss according to  log 1/p’(y|x)  where  p’(y|x)  is the predicted probability of the observed label.  A standard method for confi</p><p>4 0.93016523 <a title="79-lsi-4" href="../hunch_net-2007/hunch_net-2007-05-12-Loss_Function_Semantics.html">245 hunch net-2007-05-12-Loss Function Semantics</a></p>
<p>Introduction: Some loss functions have a meaning, which can be understood in a manner independent of the loss function itself.  
  
 Optimizing squared loss  l sq (y,y’)=(y-y’) 2   means predicting the (conditional) mean of  y . 
 Optimizing absolute value loss  l av (y,y’)=|y-y’|  means predicting the (conditional) median of  y .  Variants can  handle other quantiles .  0/1 loss for classification is a special case. 
 Optimizing log loss  l log (y,y’)=log (1/Pr z~y’ (z=y))  means minimizing the description length of  y . 
  
The semantics (= meaning) of the loss are made explicit by a theorem in each case.  For squared loss, we can prove a theorem of the form: 
For all distributions  D  over  Y , if    y’ = arg min y’  E y ~ D  l sq  (y,y’)   then   y’ = E y~D  y  
 
Similar theorems hold for the other examples above, and they can all be extended to predictors of  y’  for distributions  D  over a context  X  and a value  Y .
 
There are 3 points to this post.
  
 Everyone doing general machine lear</p><p>5 0.85998559 <a title="79-lsi-5" href="../hunch_net-2007/hunch_net-2007-08-19-Choice_of_Metrics.html">259 hunch net-2007-08-19-Choice of Metrics</a></p>
<p>Introduction: How do we judge success in Machine Learning?  As  Aaron   notes , the best way is to use the loss imposed on you by the world.  This turns out to be infeasible sometimes for various reasons.  The ones I’ve seen are:
  
 The learned prediction is used in some complicated process that does not give the feedback necessary to understand the prediction’s impact on the loss.  
 The prediction is used by some other system which expects some semantics to the predicted value.  This is similar to the previous example, except that the issue is design modularity rather than engineering modularity. 
 The correct loss function is simply unknown (and perhaps unknowable, except by experimentation). 
  
In these situations, it’s unclear what metric for evaluation should be chosen.  This post has some design advice for this murkier case.  I’m using the word “metric” here to distinguish the fact that we are considering methods for  evaluating  predictive systems rather than a loss imposed by the real wor</p><p>6 0.80312675 <a title="79-lsi-6" href="../hunch_net-2009/hunch_net-2009-10-10-ALT_2009.html">374 hunch net-2009-10-10-ALT 2009</a></p>
<p>7 0.78992271 <a title="79-lsi-7" href="../hunch_net-2007/hunch_net-2007-11-28-Computational_Consequences_of_Classification.html">274 hunch net-2007-11-28-Computational Consequences of Classification</a></p>
<p>8 0.78907299 <a title="79-lsi-8" href="../hunch_net-2005/hunch_net-2005-05-21-What_is_the_right_form_of_modularity_in_structured_prediction%3F.html">74 hunch net-2005-05-21-What is the right form of modularity in structured prediction?</a></p>
<p>9 0.74380994 <a title="79-lsi-9" href="../hunch_net-2005/hunch_net-2005-11-07-Prediction_Competitions.html">129 hunch net-2005-11-07-Prediction Competitions</a></p>
<p>10 0.59568322 <a title="79-lsi-10" href="../hunch_net-2005/hunch_net-2005-08-18-SVM_Adaptability.html">103 hunch net-2005-08-18-SVM Adaptability</a></p>
<p>11 0.57097489 <a title="79-lsi-11" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>12 0.5219177 <a title="79-lsi-12" href="../hunch_net-2007/hunch_net-2007-03-15-Alternative_Machine_Learning_Reductions_Definitions.html">236 hunch net-2007-03-15-Alternative Machine Learning Reductions Definitions</a></p>
<p>13 0.50667679 <a title="79-lsi-13" href="../hunch_net-2005/hunch_net-2005-05-06-Don%26%238217%3Bt_mix_the_solution_into_the_problem.html">67 hunch net-2005-05-06-Don&#8217;t mix the solution into the problem</a></p>
<p>14 0.50388074 <a title="79-lsi-14" href="../hunch_net-2009/hunch_net-2009-09-21-Netflix_finishes_%28and_starts%29.html">371 hunch net-2009-09-21-Netflix finishes (and starts)</a></p>
<p>15 0.47704318 <a title="79-lsi-15" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">23 hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<p>16 0.47034109 <a title="79-lsi-16" href="../hunch_net-2008/hunch_net-2008-04-27-Watchword%3A_Supervised_Learning.html">299 hunch net-2008-04-27-Watchword: Supervised Learning</a></p>
<p>17 0.45353493 <a title="79-lsi-17" href="../hunch_net-2005/hunch_net-2005-09-08-Online_Learning_as_the_Mathematics_of_Accountability.html">109 hunch net-2005-09-08-Online Learning as the Mathematics of Accountability</a></p>
<p>18 0.44384736 <a title="79-lsi-18" href="../hunch_net-2006/hunch_net-2006-03-23-The_Approximation_Argument.html">165 hunch net-2006-03-23-The Approximation Argument</a></p>
<p>19 0.4436996 <a title="79-lsi-19" href="../hunch_net-2010/hunch_net-2010-12-26-NIPS_2010.html">420 hunch net-2010-12-26-NIPS 2010</a></p>
<p>20 0.4225218 <a title="79-lsi-20" href="../hunch_net-2010/hunch_net-2010-05-10-Aggregation_of_estimators%2C_sparsity_in_high_dimension_and_computational_feasibility.html">398 hunch net-2010-05-10-Aggregation of estimators, sparsity in high dimension and computational feasibility</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.019), (3, 0.045), (10, 0.044), (22, 0.163), (24, 0.012), (27, 0.271), (38, 0.071), (50, 0.01), (53, 0.076), (55, 0.046), (92, 0.019), (94, 0.084), (95, 0.046)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.95035154 <a title="79-lda-1" href="../hunch_net-2005/hunch_net-2005-09-20-Workshop_Proposal%3A_Atomic_Learning.html">114 hunch net-2005-09-20-Workshop Proposal: Atomic Learning</a></p>
<p>Introduction: This is a proposal for a workshop.  It may or may not happen depending on the level of interest.  If you are interested, feel free to indicate so (by email or comments).
 
Description: 
Assume(*) that any system for solving large difficult learning problems must decompose into repeated use of basic elements (i.e. atoms).  There are many basic questions which remain:
  
  What are the viable basic elements? 
  What makes a basic element viable? 
  What are the viable principles for the composition of these basic elements? 
  What are the viable principles for learning in such systems? 
  What problems can this approach handle? 
  
Hal Daume adds:
  
 Can composition of atoms be (semi-) automatically constructed[?] 
 When atoms are constructed through reductions, is there some notion of the “naturalness” of the created leaning problems? 
 Other than Markov fields/graphical models/Bayes nets, is there a good language for representing atoms and their compositions? 
  
The answer to these a</p><p>2 0.9358843 <a title="79-lda-2" href="../hunch_net-2009/hunch_net-2009-06-01-Multitask_Poisoning.html">358 hunch net-2009-06-01-Multitask Poisoning</a></p>
<p>Introduction: There are many ways that interesting research gets done.  For example it’s common at a conference for someone to discuss a problem with a partial solution, and for someone else to know how to solve a piece of it, resulting in a paper.  In some sense,  these are the easiest results we can achieve, so we should ask: Can all research be this easy?  
 
The answer is certainly no for fields where research inherently requires  experimentation to discover how the real world works.  However, mathematics, including parts of physics, computer science, statistics, etc… which are effectively mathematics don’t require experimentation. In effect, a paper can be simply a pure expression of thinking.  Can all mathematical-style research be this easy?
 
What’s going on here is research-by-communication.  Someone knows something, someone knows something else, and as soon as someone knows both things, a problem is solved.  The interesting thing about research-by-communication is that it is becoming radic</p><p>same-blog 3 0.93397075 <a title="79-lda-3" href="../hunch_net-2005/hunch_net-2005-06-08-Question%3A_%26%238220%3BWhen_is_the_right_time_to_insert_the_loss_function%3F%26%238221%3B.html">79 hunch net-2005-06-08-Question: &#8220;When is the right time to insert the loss function?&#8221;</a></p>
<p>Introduction: Hal  asks   a very good question: “When is the right time to insert the loss function?”  In particular, should it be used at testing time or at training time?
 
When the world imposes a loss on us, the standard Bayesian recipe is to predict the (conditional) probability of each possibility and then choose the possibility which minimizes the expected loss.  In contrast, as the  confusion  over “loss = money lost” or “loss = the thing you optimize” might indicate, many people ignore the Bayesian approach and simply optimize their loss (or a close proxy for their loss) over the representation on the training set.
 
The best answer I can give is “it’s unclear, but I prefer optimizing the loss at training time”.  My experience is that optimizing the loss in the most direct manner possible typically yields best performance.  This question is related to a basic principle which both  Yann LeCun (applied) and  Vladimir Vapnik (theoretical) advocate: “solve the simplest prediction problem that s</p><p>4 0.87690628 <a title="79-lda-4" href="../hunch_net-2005/hunch_net-2005-09-19-NIPS_Workshops.html">113 hunch net-2005-09-19-NIPS Workshops</a></p>
<p>Introduction: Attendance at the  NIPS workshops  is highly recommended for both research and learning.   Unfortunately, there does not yet appear to be a public list of workshops. However, I found the following workshop webpages of interest:
  
  Machine Learning in Finance  
  Learning to Rank  
  Foundations of Active Learning  
  Machine Learning Based Robotics in Unstructured Environments  
  
There are  many  more workshops.  In fact, there are so many that it is not plausible anyone can attend every workshop they are interested in.  Maybe in future years the organizers can spread them out over more days to reduce overlap. 
 
Many of these workshops are accepting presentation proposals (due mid-October).</p><p>5 0.86525524 <a title="79-lda-5" href="../hunch_net-2005/hunch_net-2005-02-03-Learning_Theory%2C_by_assumption.html">12 hunch net-2005-02-03-Learning Theory, by assumption</a></p>
<p>Introduction: One way to organize learning theory is by assumption (in the  assumption = axiom sense ), from no assumptions to many assumptions.   As you travel down this list, the statements become stronger, but the scope of applicability decreases.
  
   No assumptions 
 
  Online learning  There exist a meta prediction algorithm which compete well with the best element of any set of prediction algorithms. 
  Universal Learning  Using a “bias” of 2 - description length of turing machine  in learning is equivalent to all other computable biases up to some constant. 
  Reductions  The ability to predict well on classification problems is equivalent to the ability to predict well on many other learning problems.
 
 
 
  Independent and Identically Distributed (IID) Data 
 
  Performance Prediction  Based upon past performance, you can predict future performance. 
  Uniform Convergence  Performance prediction works even after choosing classifiers based on the data from large sets of classifiers.</p><p>6 0.86395156 <a title="79-lda-6" href="../hunch_net-2005/hunch_net-2005-03-15-The_State_of_Tight_Bounds.html">41 hunch net-2005-03-15-The State of Tight Bounds</a></p>
<p>7 0.86197472 <a title="79-lda-7" href="../hunch_net-2007/hunch_net-2007-01-10-A_Deep_Belief_Net_Learning_Problem.html">227 hunch net-2007-01-10-A Deep Belief Net Learning Problem</a></p>
<p>8 0.86108637 <a title="79-lda-8" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>9 0.85961366 <a title="79-lda-9" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>10 0.8594594 <a title="79-lda-10" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>11 0.85921812 <a title="79-lda-11" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>12 0.85828424 <a title="79-lda-12" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>13 0.85772485 <a title="79-lda-13" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>14 0.85765034 <a title="79-lda-14" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>15 0.8572666 <a title="79-lda-15" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>16 0.85593289 <a title="79-lda-16" href="../hunch_net-2007/hunch_net-2007-08-12-Exponentiated_Gradient.html">258 hunch net-2007-08-12-Exponentiated Gradient</a></p>
<p>17 0.85458016 <a title="79-lda-17" href="../hunch_net-2007/hunch_net-2007-02-02-Thoughts_regarding_%26%238220%3BIs_machine_learning_different_from_statistics%3F%26%238221%3B.html">230 hunch net-2007-02-02-Thoughts regarding &#8220;Is machine learning different from statistics?&#8221;</a></p>
<p>18 0.85299867 <a title="79-lda-18" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>19 0.8527686 <a title="79-lda-19" href="../hunch_net-2005/hunch_net-2005-11-16-The_Everything_Ensemble_Edge.html">131 hunch net-2005-11-16-The Everything Ensemble Edge</a></p>
<p>20 0.85233718 <a title="79-lda-20" href="../hunch_net-2009/hunch_net-2009-06-03-Functionally_defined_Nonlinear_Dynamic_Models.html">359 hunch net-2009-06-03-Functionally defined Nonlinear Dynamic Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
