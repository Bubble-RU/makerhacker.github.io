<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>111 hunch net-2005-09-12-Fast Gradient Descent</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-111" href="#">hunch_net-2005-111</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>111 hunch net-2005-09-12-Fast Gradient Descent</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-111-html" href="http://hunch.net/?p=119">html</a></p><p>Introduction: Nic Schaudolph  has been developing a fast gradient descent algorithm called  Stochastic Meta-Descent  (SMD).
 
Gradient descent is currently untrendy in the machine learning community, but there remains a large number of people using gradient descent on neural networks or other architectures from when it was trendy in the early 1990s.  There are three problems with gradient descent.
  
 Gradient descent does not necessarily produce easily reproduced results.  Typical algorithms start with “set the initial parameters to small random values”. 
 The design of the representation that gradient descent is applied to is often nontrivial.  In particular, knowing exactly how to build a large neural network so that it will perform well requires knowledge which has not been made easily applicable. 
 Gradient descent can be slow.  Obviously, taking infinitesimal steps in the direction of the gradient would take forever, so some finite step size must be used.  What exactly this step size should be</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Nic Schaudolph  has been developing a fast gradient descent algorithm called  Stochastic Meta-Descent  (SMD). [sent-1, score-1.14]
</p><p>2 Gradient descent is currently untrendy in the machine learning community, but there remains a large number of people using gradient descent on neural networks or other architectures from when it was trendy in the early 1990s. [sent-2, score-1.833]
</p><p>3 Gradient descent does not necessarily produce easily reproduced results. [sent-4, score-0.686]
</p><p>4 Typical algorithms start with “set the initial parameters to small random values”. [sent-5, score-0.225]
</p><p>5 The design of the representation that gradient descent is applied to is often nontrivial. [sent-6, score-0.955]
</p><p>6 In particular, knowing exactly how to build a large neural network so that it will perform well requires knowledge which has not been made easily applicable. [sent-7, score-0.465]
</p><p>7 Obviously, taking infinitesimal steps in the direction of the gradient would take forever, so some finite step size must be used. [sent-9, score-1.105]
</p><p>8 What exactly this step size should be is unclear. [sent-10, score-0.57]
</p><p>9 Many people have developed many algorithms for adjusting the step size (and to some extent the step direction). [sent-11, score-0.911]
</p><p>10 Unfortunately, many of the more sophisticated algorithms are not robust to noise, scale badly with the number of parameters (Anything worse than  O(n)  is unacceptable for big applications) or both. [sent-12, score-0.702]
</p><p>11 Consequently, many people simply use gradient descent where the step size is adjusted by a simple momentum heuristic. [sent-13, score-1.57]
</p><p>12 Many people would add point (4): gradient descent on many architectures does not result in a global optima. [sent-14, score-1.429]
</p><p>13 The goal is good performance on future examples in learning rather than achieving a global optima on the training set. [sent-16, score-0.333]
</p><p>14 It is an  O(n)  algorithm for gradient descent that can compete with the sophisticed methods where the sophisticated methods work but remains fairly robust to noise. [sent-18, score-1.6]
</p><p>15 Exactly how well it addresses point (3) is not entirely clear, but a few interesting problems have been solved with the algorithm, and perhaps we will see more evidence in the near future. [sent-19, score-0.225]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('gradient', 0.491), ('descent', 0.464), ('step', 0.214), ('smd', 0.212), ('size', 0.181), ('exactly', 0.175), ('sophisticated', 0.15), ('architectures', 0.145), ('addresses', 0.141), ('global', 0.119), ('remains', 0.113), ('robust', 0.108), ('parameters', 0.106), ('direction', 0.103), ('neural', 0.098), ('forever', 0.094), ('adjusted', 0.094), ('nic', 0.094), ('reproduced', 0.094), ('unacceptable', 0.094), ('optima', 0.087), ('point', 0.084), ('methods', 0.073), ('easily', 0.073), ('future', 0.071), ('many', 0.068), ('confusion', 0.065), ('compete', 0.065), ('developing', 0.065), ('algorithms', 0.064), ('knowing', 0.063), ('algorithm', 0.063), ('steps', 0.061), ('anything', 0.058), ('stochastic', 0.058), ('badly', 0.058), ('people', 0.058), ('goals', 0.057), ('called', 0.057), ('values', 0.057), ('extent', 0.056), ('developed', 0.056), ('noise', 0.056), ('perform', 0.056), ('consequently', 0.056), ('achieving', 0.056), ('finite', 0.055), ('necessarily', 0.055), ('initial', 0.055), ('worse', 0.054)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="111-tfidf-1" href="../hunch_net-2005/hunch_net-2005-09-12-Fast_Gradient_Descent.html">111 hunch net-2005-09-12-Fast Gradient Descent</a></p>
<p>Introduction: Nic Schaudolph  has been developing a fast gradient descent algorithm called  Stochastic Meta-Descent  (SMD).
 
Gradient descent is currently untrendy in the machine learning community, but there remains a large number of people using gradient descent on neural networks or other architectures from when it was trendy in the early 1990s.  There are three problems with gradient descent.
  
 Gradient descent does not necessarily produce easily reproduced results.  Typical algorithms start with “set the initial parameters to small random values”. 
 The design of the representation that gradient descent is applied to is often nontrivial.  In particular, knowing exactly how to build a large neural network so that it will perform well requires knowledge which has not been made easily applicable. 
 Gradient descent can be slow.  Obviously, taking infinitesimal steps in the direction of the gradient would take forever, so some finite step size must be used.  What exactly this step size should be</p><p>2 0.25377661 <a title="111-tfidf-2" href="../hunch_net-2007/hunch_net-2007-08-12-Exponentiated_Gradient.html">258 hunch net-2007-08-12-Exponentiated Gradient</a></p>
<p>Introduction: The  Exponentiated Gradient  algorithm by  Manfred Warmuth  and  Jyrki Kivinen  came out just as I was starting graduate school, so I missed it both at a conference and in class.  It’s a fine algorithm which has a remarkable theoretical statement accompanying it.
 
The essential statement holds in the “online learning with an adversary” setting.  Initially, there are of set of  n  weights, which might have values  (1/n,…,1/n) , (or any other values from a probability distribution).  Everything happens in a round-by-round fashion.  On each round, the following happens:
  
 The world reveals a set of features  x in {0,1} n  .  In the online learning with an adversary literature, the features are called “experts” and thought of as subpredictors, but this interpretation isn’t necessary—you can just use feature values as experts (or maybe the feature value and the negation of the feature value as two experts). 
 EG makes a prediction according to  y’ = w . x  (dot product). 
 The world reve</p><p>3 0.20667326 <a title="111-tfidf-3" href="../hunch_net-2006/hunch_net-2006-03-27-Gradients_everywhere.html">167 hunch net-2006-03-27-Gradients everywhere</a></p>
<p>Introduction: One of the basic observations from the  atomic learning workshop  is that gradient-based optimization is pervasive.   For example, at least 7 (of 12) speakers used the word ‘gradient’ in their talk and several others may be approximating a gradient.  The essential useful quality of a gradient is that it decouples local updates from global optimization.  Restated: Given a gradient, we can determine how to change individual parameters of the system so as to improve overall performance.
 
It’s easy to feel depressed about this and think “nothing has happened”, but that appears untrue.  Many of the talks were about clever techniques for computing gradients where your calculus textbook breaks down.
  
 Sometimes there are clever approximations of the gradient. ( Simon Osindero ) 
 Sometimes we can compute constrained gradients via iterated gradient/project steps. ( Ben Taskar ) 
 Sometimes we can compute gradients anyways over mildly nondifferentiable functions. ( Drew Bagnell ) 
 Even give</p><p>4 0.2052519 <a title="111-tfidf-4" href="../hunch_net-2006/hunch_net-2006-07-17-A_Winner.html">197 hunch net-2006-07-17-A Winner</a></p>
<p>Introduction: Ed Snelson  won the  Predictive Uncertainty in Environmental Modelling Competition  in the temp(erature) category using  this algorithm .  Some characteristics of the algorithm are:
  
 Gradient descent 
 … on about 600 parameters 
 … with local minima 
 … to solve regression. 
  
This bears a strong resemblance to a neural network.  The two main differences seem to be:
  
 The system has a probabilistic interpretation (which may aid design). 
 There are (perhaps) fewer parameters than a typical neural network might have for the same problem (aiding speed).</p><p>5 0.19082719 <a title="111-tfidf-5" href="../hunch_net-2007/hunch_net-2007-12-21-Vowpal_Wabbit_Code_Release.html">281 hunch net-2007-12-21-Vowpal Wabbit Code Release</a></p>
<p>Introduction: We are releasing the  Vowpal Wabbit (Fast Online Learning) code  as open source under a BSD (revised) license.  This is a  project at Yahoo! Research  to build a useful large scale learning algorithm which  Lihong Li ,  Alex Strehl , and I have been working on.
 
To appreciate the meaning of “large”, it’s useful to define “small” and “medium”.  A “small” supervised learning problem is one where a human could use a labeled dataset and come up with a reasonable predictor.  A “medium” supervised learning problem dataset fits into the RAM of a modern desktop computer.  A “large” supervised learning problem is one which does not fit into the RAM of a normal machine.  VW tackles large scale learning problems by this definition of large.  I’m not aware of any other open source Machine Learning tools which can handle this scale (although they may exist).  A few close ones are:
  
  IBM’s Parallel Machine Learning Toolbox   isn’t quite open source .  The approach used by this toolbox is essenti</p><p>6 0.18106331 <a title="111-tfidf-6" href="../hunch_net-2006/hunch_net-2006-05-16-The_value_of_the_orthodox_view_of_Boosting.html">179 hunch net-2006-05-16-The value of the orthodox view of Boosting</a></p>
<p>7 0.17234617 <a title="111-tfidf-7" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>8 0.16683687 <a title="111-tfidf-8" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">23 hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<p>9 0.158875 <a title="111-tfidf-9" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>10 0.1503468 <a title="111-tfidf-10" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>11 0.14265883 <a title="111-tfidf-11" href="../hunch_net-2006/hunch_net-2006-06-24-Online_convex_optimization_at_COLT.html">186 hunch net-2006-06-24-Online convex optimization at COLT</a></p>
<p>12 0.1117909 <a title="111-tfidf-12" href="../hunch_net-2011/hunch_net-2011-08-15-Vowpal_Wabbit_6.0.html">441 hunch net-2011-08-15-Vowpal Wabbit 6.0</a></p>
<p>13 0.10733838 <a title="111-tfidf-13" href="../hunch_net-2009/hunch_net-2009-03-18-Parallel_ML_primitives.html">346 hunch net-2009-03-18-Parallel ML primitives</a></p>
<p>14 0.10702182 <a title="111-tfidf-14" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>15 0.10636838 <a title="111-tfidf-15" href="../hunch_net-2010/hunch_net-2010-12-04-Vowpal_Wabbit%2C_version_5.0%2C_and_the_second_heresy.html">419 hunch net-2010-12-04-Vowpal Wabbit, version 5.0, and the second heresy</a></p>
<p>16 0.098253794 <a title="111-tfidf-16" href="../hunch_net-2007/hunch_net-2007-12-19-Cool_and_interesting_things_seen_at_NIPS.html">279 hunch net-2007-12-19-Cool and interesting things seen at NIPS</a></p>
<p>17 0.098195516 <a title="111-tfidf-17" href="../hunch_net-2011/hunch_net-2011-07-11-Interesting_Neural_Network_Papers_at_ICML_2011.html">438 hunch net-2011-07-11-Interesting Neural Network Papers at ICML 2011</a></p>
<p>18 0.097717106 <a title="111-tfidf-18" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>19 0.097623557 <a title="111-tfidf-19" href="../hunch_net-2007/hunch_net-2007-10-17-Online_as_the_new_adjective.html">267 hunch net-2007-10-17-Online as the new adjective</a></p>
<p>20 0.095351979 <a title="111-tfidf-20" href="../hunch_net-2006/hunch_net-2006-05-05-An_ICML_reject.html">177 hunch net-2006-05-05-An ICML reject</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.187), (1, 0.092), (2, -0.03), (3, -0.005), (4, 0.1), (5, 0.012), (6, -0.165), (7, -0.017), (8, -0.025), (9, 0.143), (10, -0.076), (11, -0.039), (12, 0.002), (13, -0.133), (14, 0.091), (15, 0.144), (16, 0.069), (17, -0.005), (18, -0.042), (19, 0.011), (20, -0.078), (21, 0.031), (22, 0.001), (23, -0.011), (24, -0.084), (25, 0.045), (26, -0.075), (27, 0.028), (28, -0.008), (29, -0.098), (30, -0.116), (31, 0.034), (32, -0.187), (33, -0.127), (34, -0.121), (35, -0.005), (36, -0.213), (37, -0.069), (38, 0.052), (39, 0.046), (40, 0.055), (41, -0.135), (42, 0.047), (43, -0.089), (44, 0.016), (45, 0.087), (46, 0.063), (47, -0.036), (48, 0.05), (49, 0.057)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98376119 <a title="111-lsi-1" href="../hunch_net-2005/hunch_net-2005-09-12-Fast_Gradient_Descent.html">111 hunch net-2005-09-12-Fast Gradient Descent</a></p>
<p>Introduction: Nic Schaudolph  has been developing a fast gradient descent algorithm called  Stochastic Meta-Descent  (SMD).
 
Gradient descent is currently untrendy in the machine learning community, but there remains a large number of people using gradient descent on neural networks or other architectures from when it was trendy in the early 1990s.  There are three problems with gradient descent.
  
 Gradient descent does not necessarily produce easily reproduced results.  Typical algorithms start with “set the initial parameters to small random values”. 
 The design of the representation that gradient descent is applied to is often nontrivial.  In particular, knowing exactly how to build a large neural network so that it will perform well requires knowledge which has not been made easily applicable. 
 Gradient descent can be slow.  Obviously, taking infinitesimal steps in the direction of the gradient would take forever, so some finite step size must be used.  What exactly this step size should be</p><p>2 0.89777547 <a title="111-lsi-2" href="../hunch_net-2006/hunch_net-2006-03-27-Gradients_everywhere.html">167 hunch net-2006-03-27-Gradients everywhere</a></p>
<p>Introduction: One of the basic observations from the  atomic learning workshop  is that gradient-based optimization is pervasive.   For example, at least 7 (of 12) speakers used the word ‘gradient’ in their talk and several others may be approximating a gradient.  The essential useful quality of a gradient is that it decouples local updates from global optimization.  Restated: Given a gradient, we can determine how to change individual parameters of the system so as to improve overall performance.
 
It’s easy to feel depressed about this and think “nothing has happened”, but that appears untrue.  Many of the talks were about clever techniques for computing gradients where your calculus textbook breaks down.
  
 Sometimes there are clever approximations of the gradient. ( Simon Osindero ) 
 Sometimes we can compute constrained gradients via iterated gradient/project steps. ( Ben Taskar ) 
 Sometimes we can compute gradients anyways over mildly nondifferentiable functions. ( Drew Bagnell ) 
 Even give</p><p>3 0.84369224 <a title="111-lsi-3" href="../hunch_net-2006/hunch_net-2006-07-17-A_Winner.html">197 hunch net-2006-07-17-A Winner</a></p>
<p>Introduction: Ed Snelson  won the  Predictive Uncertainty in Environmental Modelling Competition  in the temp(erature) category using  this algorithm .  Some characteristics of the algorithm are:
  
 Gradient descent 
 … on about 600 parameters 
 … with local minima 
 … to solve regression. 
  
This bears a strong resemblance to a neural network.  The two main differences seem to be:
  
 The system has a probabilistic interpretation (which may aid design). 
 There are (perhaps) fewer parameters than a typical neural network might have for the same problem (aiding speed).</p><p>4 0.74697232 <a title="111-lsi-4" href="../hunch_net-2006/hunch_net-2006-05-16-The_value_of_the_orthodox_view_of_Boosting.html">179 hunch net-2006-05-16-The value of the orthodox view of Boosting</a></p>
<p>Introduction: The term “boosting” comes from the idea of using a meta-algorithm which takes “weak” learners (that may be able to only barely predict slightly better than random) and turn them into strongly capable learners (which predict very well).    Adaboost  in 1995 was the first widely used (and useful) boosting algorithm, although there were theoretical boosting algorithms floating around since 1990 (see the bottom of  this page ).
 
Since then, many different interpretations of why boosting works have arisen.  There is significant discussion about these different views in the  annals of statistics , including a  response  by  Yoav Freund  and  Robert Schapire .
 
I believe there is a great deal of value to be found in the original view of boosting (meta-algorithm for creating a strong learner from a weak learner).  This is not a claim that one particular viewpoint obviates the value of all others, but rather that no other viewpoint seems to really capture important properties.
 
Comparing wit</p><p>5 0.70832235 <a title="111-lsi-5" href="../hunch_net-2007/hunch_net-2007-08-12-Exponentiated_Gradient.html">258 hunch net-2007-08-12-Exponentiated Gradient</a></p>
<p>Introduction: The  Exponentiated Gradient  algorithm by  Manfred Warmuth  and  Jyrki Kivinen  came out just as I was starting graduate school, so I missed it both at a conference and in class.  It’s a fine algorithm which has a remarkable theoretical statement accompanying it.
 
The essential statement holds in the “online learning with an adversary” setting.  Initially, there are of set of  n  weights, which might have values  (1/n,…,1/n) , (or any other values from a probability distribution).  Everything happens in a round-by-round fashion.  On each round, the following happens:
  
 The world reveals a set of features  x in {0,1} n  .  In the online learning with an adversary literature, the features are called “experts” and thought of as subpredictors, but this interpretation isn’t necessary—you can just use feature values as experts (or maybe the feature value and the negation of the feature value as two experts). 
 EG makes a prediction according to  y’ = w . x  (dot product). 
 The world reve</p><p>6 0.57303762 <a title="111-lsi-6" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>7 0.54787046 <a title="111-lsi-7" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>8 0.54547822 <a title="111-lsi-8" href="../hunch_net-2007/hunch_net-2007-12-21-Vowpal_Wabbit_Code_Release.html">281 hunch net-2007-12-21-Vowpal Wabbit Code Release</a></p>
<p>9 0.46782523 <a title="111-lsi-9" href="../hunch_net-2006/hunch_net-2006-06-24-Online_convex_optimization_at_COLT.html">186 hunch net-2006-06-24-Online convex optimization at COLT</a></p>
<p>10 0.46665359 <a title="111-lsi-10" href="../hunch_net-2009/hunch_net-2009-04-02-Asymmophobia.html">348 hunch net-2009-04-02-Asymmophobia</a></p>
<p>11 0.45999226 <a title="111-lsi-11" href="../hunch_net-2006/hunch_net-2006-09-07-Objective_and_subjective_interpretations_of_probability.html">205 hunch net-2006-09-07-Objective and subjective interpretations of probability</a></p>
<p>12 0.45765463 <a title="111-lsi-12" href="../hunch_net-2009/hunch_net-2009-03-18-Parallel_ML_primitives.html">346 hunch net-2009-03-18-Parallel ML primitives</a></p>
<p>13 0.4511528 <a title="111-lsi-13" href="../hunch_net-2005/hunch_net-2005-12-09-Some_NIPS_papers.html">138 hunch net-2005-12-09-Some NIPS papers</a></p>
<p>14 0.44725269 <a title="111-lsi-14" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">23 hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<p>15 0.43222627 <a title="111-lsi-15" href="../hunch_net-2011/hunch_net-2011-08-15-Vowpal_Wabbit_6.0.html">441 hunch net-2011-08-15-Vowpal Wabbit 6.0</a></p>
<p>16 0.43074065 <a title="111-lsi-16" href="../hunch_net-2005/hunch_net-2005-02-27-Antilearning%3A_When_proximity_goes_bad.html">32 hunch net-2005-02-27-Antilearning: When proximity goes bad</a></p>
<p>17 0.42082062 <a title="111-lsi-17" href="../hunch_net-2005/hunch_net-2005-11-16-The_Everything_Ensemble_Edge.html">131 hunch net-2005-11-16-The Everything Ensemble Edge</a></p>
<p>18 0.42024153 <a title="111-lsi-18" href="../hunch_net-2010/hunch_net-2010-12-04-Vowpal_Wabbit%2C_version_5.0%2C_and_the_second_heresy.html">419 hunch net-2010-12-04-Vowpal Wabbit, version 5.0, and the second heresy</a></p>
<p>19 0.41910881 <a title="111-lsi-19" href="../hunch_net-2006/hunch_net-2006-11-22-Explicit_Randomization_in_Learning_algorithms.html">219 hunch net-2006-11-22-Explicit Randomization in Learning algorithms</a></p>
<p>20 0.41545707 <a title="111-lsi-20" href="../hunch_net-2009/hunch_net-2009-07-31-Vowpal_Wabbit_Open_Source_Project.html">365 hunch net-2009-07-31-Vowpal Wabbit Open Source Project</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(3, 0.031), (27, 0.186), (38, 0.044), (53, 0.125), (55, 0.027), (94, 0.063), (95, 0.065), (98, 0.346)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.94313049 <a title="111-lda-1" href="../hunch_net-2006/hunch_net-2006-03-27-Gradients_everywhere.html">167 hunch net-2006-03-27-Gradients everywhere</a></p>
<p>Introduction: One of the basic observations from the  atomic learning workshop  is that gradient-based optimization is pervasive.   For example, at least 7 (of 12) speakers used the word ‘gradient’ in their talk and several others may be approximating a gradient.  The essential useful quality of a gradient is that it decouples local updates from global optimization.  Restated: Given a gradient, we can determine how to change individual parameters of the system so as to improve overall performance.
 
It’s easy to feel depressed about this and think “nothing has happened”, but that appears untrue.  Many of the talks were about clever techniques for computing gradients where your calculus textbook breaks down.
  
 Sometimes there are clever approximations of the gradient. ( Simon Osindero ) 
 Sometimes we can compute constrained gradients via iterated gradient/project steps. ( Ben Taskar ) 
 Sometimes we can compute gradients anyways over mildly nondifferentiable functions. ( Drew Bagnell ) 
 Even give</p><p>2 0.91536665 <a title="111-lda-2" href="../hunch_net-2008/hunch_net-2008-10-20-New_York%26%238217%3Bs_ML_Day.html">322 hunch net-2008-10-20-New York&#8217;s ML Day</a></p>
<p>Introduction: I’m not as naturally exuberant as  Muthu   2  or  David  about  CS/Econ  day, but I believe it and  ML day  were certainly successful.
 
At the CS/Econ day, I particularly enjoyed  Toumas Sandholm’s  talk which showed a commanding depth of understanding and application in automated auctions.
 
For the machine learning day, I enjoyed several talks and posters (I better, I helped pick them.).  What stood out to me was number of people attending: 158 registered, a level qualifying as “scramble to find seats”.  My rule of thumb for workshops/conferences is that the number of attendees is often something like the number of submissions.  That isn’t the case here, where there were just 4 invited speakers and 30-or-so posters.  Presumably, the difference is due to a critical mass of Machine Learning interested people in the area and the ease of their attendance.  
 
Are there other areas where a local Machine Learning day would fly?  It’s easy to imagine something working out in the San Franci</p><p>same-blog 3 0.88778782 <a title="111-lda-3" href="../hunch_net-2005/hunch_net-2005-09-12-Fast_Gradient_Descent.html">111 hunch net-2005-09-12-Fast Gradient Descent</a></p>
<p>Introduction: Nic Schaudolph  has been developing a fast gradient descent algorithm called  Stochastic Meta-Descent  (SMD).
 
Gradient descent is currently untrendy in the machine learning community, but there remains a large number of people using gradient descent on neural networks or other architectures from when it was trendy in the early 1990s.  There are three problems with gradient descent.
  
 Gradient descent does not necessarily produce easily reproduced results.  Typical algorithms start with “set the initial parameters to small random values”. 
 The design of the representation that gradient descent is applied to is often nontrivial.  In particular, knowing exactly how to build a large neural network so that it will perform well requires knowledge which has not been made easily applicable. 
 Gradient descent can be slow.  Obviously, taking infinitesimal steps in the direction of the gradient would take forever, so some finite step size must be used.  What exactly this step size should be</p><p>4 0.87032104 <a title="111-lda-4" href="../hunch_net-2006/hunch_net-2006-10-02-%241M_Netflix_prediction_contest.html">211 hunch net-2006-10-02-$1M Netflix prediction contest</a></p>
<p>Introduction: Netflix is  running a contest  to improve recommender prediction systems.   A 10% improvement over their current system yields a $1M prize.  Failing that, the best smaller improvement yields a smaller $50K prize.  This contest looks quite real, and the $50K prize money is almost certainly achievable with a bit of thought.  The contest also comes with a dataset which is apparently 2 orders of magnitude larger than any other public recommendation system datasets.</p><p>5 0.86902082 <a title="111-lda-5" href="../hunch_net-2007/hunch_net-2007-02-10-Best_Practices_for_Collaboration.html">231 hunch net-2007-02-10-Best Practices for Collaboration</a></p>
<p>Introduction: Many people, especially students, haven’t had an opportunity to collaborate with other researchers.  Collaboration, especially with remote people can be tricky.  Here are some observations of what has worked for me on collaborations involving a few people.
  
  Travel and Discuss  Almost all collaborations start with in-person discussion.  This implies that travel is often necessary.  We can hope that in the future we’ll have better systems for starting collaborations remotely (such as blogs), but we aren’t quite there yet. 
  Enable your collaborator .  A collaboration can fall apart because one collaborator disables another.  This sounds stupid (and it is), but it’s far easier than you might think.
 
  Avoid Duplication .  Discovering that you and a collaborator have been editing the same thing and now need to waste time reconciling changes is annoying.  The best way to avoid this to be explicit about who has write permission to what.  Most of the time, a write lock is held for the e</p><p>6 0.56523883 <a title="111-lda-6" href="../hunch_net-2013/hunch_net-2013-01-07-NYU_Large_Scale_Machine_Learning_Class.html">478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</a></p>
<p>7 0.55637002 <a title="111-lda-7" href="../hunch_net-2005/hunch_net-2005-02-03-Learning_Theory%2C_by_assumption.html">12 hunch net-2005-02-03-Learning Theory, by assumption</a></p>
<p>8 0.55478483 <a title="111-lda-8" href="../hunch_net-2007/hunch_net-2007-01-10-A_Deep_Belief_Net_Learning_Problem.html">227 hunch net-2007-01-10-A Deep Belief Net Learning Problem</a></p>
<p>9 0.55391419 <a title="111-lda-9" href="../hunch_net-2009/hunch_net-2009-11-23-ICML_2009_Workshops_%28and_Tutorials%29.html">379 hunch net-2009-11-23-ICML 2009 Workshops (and Tutorials)</a></p>
<p>10 0.55354214 <a title="111-lda-10" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>11 0.55234993 <a title="111-lda-11" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>12 0.54719901 <a title="111-lda-12" href="../hunch_net-2005/hunch_net-2005-11-16-The_Everything_Ensemble_Edge.html">131 hunch net-2005-11-16-The Everything Ensemble Edge</a></p>
<p>13 0.54637498 <a title="111-lda-13" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>14 0.54549628 <a title="111-lda-14" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>15 0.54445833 <a title="111-lda-15" href="../hunch_net-2005/hunch_net-2005-04-23-Advantages_and_Disadvantages_of_Bayesian_Learning.html">60 hunch net-2005-04-23-Advantages and Disadvantages of Bayesian Learning</a></p>
<p>16 0.54293364 <a title="111-lda-16" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>17 0.54282546 <a title="111-lda-17" href="../hunch_net-2005/hunch_net-2005-03-15-The_State_of_Tight_Bounds.html">41 hunch net-2005-03-15-The State of Tight Bounds</a></p>
<p>18 0.54151928 <a title="111-lda-18" href="../hunch_net-2006/hunch_net-2006-01-30-Should_the_Input_Representation_be_a_Vector%3F.html">152 hunch net-2006-01-30-Should the Input Representation be a Vector?</a></p>
<p>19 0.53901684 <a title="111-lda-19" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>20 0.53883517 <a title="111-lda-20" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
