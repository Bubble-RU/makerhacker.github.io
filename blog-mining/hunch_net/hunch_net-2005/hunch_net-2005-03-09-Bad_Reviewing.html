<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>38 hunch net-2005-03-09-Bad Reviewing</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-38" href="#">hunch_net-2005-38</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>38 hunch net-2005-03-09-Bad Reviewing</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-38-html" href="http://hunch.net/?p=42">html</a></p><p>Introduction: This is a difficult subject to talk about for many reasons, but a discussion
may be helpful.Bad reviewing is a problem in academia. The first step in
understanding this is admitting to the problem, so here is a short list of
examples of bad reviewing.Reviewer disbelieves theorem proof (ICML), or
disbelieve theorem with a trivially false counterexample. (COLT)Reviewer
internally swaps quantifiers in a theorem, concludes it has been done before
and is trivial. (NIPS)Reviewer believes a technique will not work despite
experimental validation. (COLT)Reviewers fail to notice flaw in theorem
statement (CRYPTO).Reviewer erroneously claims that it has been done before
(NIPS, SODA, JMLR)--(complete with references!)Reviewer inverts the message of
a paper and concludes it says nothing important. (NIPS*2)Reviewer fails to
distinguish between a DAG and a tree (SODA).Reviewer is enthusiastic about
paper but clearly does not understand (ICML).Reviewer erroneously believe that
the "birthday paradox"</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 This is a difficult subject to talk about for many reasons, but a discussion may be helpful. [sent-1, score-0.103]
</p><p>2 The first step in understanding this is admitting to the problem, so here is a short list of examples of bad reviewing. [sent-3, score-0.335]
</p><p>3 Reviewer disbelieves theorem proof (ICML), or disbelieve theorem with a trivially false counterexample. [sent-4, score-0.704]
</p><p>4 (COLT)Reviewer internally swaps quantifiers in a theorem, concludes it has been done before and is trivial. [sent-5, score-0.567]
</p><p>5 (NIPS)Reviewer believes a technique will not work despite experimental validation. [sent-6, score-0.094]
</p><p>6 (COLT)Reviewers fail to notice flaw in theorem statement (CRYPTO). [sent-7, score-0.5]
</p><p>7 Reviewer erroneously claims that it has been done before (NIPS, SODA, JMLR)--(complete with references! [sent-8, score-0.326]
</p><p>8 )Reviewer inverts the message of a paper and concludes it says nothing important. [sent-9, score-0.485]
</p><p>9 (NIPS*2)Reviewer fails to distinguish between a DAG and a tree (SODA). [sent-10, score-0.076]
</p><p>10 Reviewer is enthusiastic about paper but clearly does not understand (ICML). [sent-11, score-0.36]
</p><p>11 Reviewer erroneously believe that the "birthday paradox" is relevant (CCS). [sent-12, score-0.243]
</p><p>12 The above is only for cases where there was sufficient reviewer comments to actually understand reviewer failure modes. [sent-13, score-0.985]
</p><p>13 Many reviewers fail to leave sufficient comments and it's easy to imagine they commit similar mistakes. [sent-14, score-0.558]
</p><p>14 Bad reviewing should be clearly distinguished from rejections--note that some of the above examples are actually accepts. [sent-15, score-0.476]
</p><p>15 The standard psychological reaction to any rejected paper is trying to find fault with the reviewers. [sent-16, score-0.344]
</p><p>16 You, as a paper writer, have invested significant work (weeks? [sent-17, score-0.228]
</p><p>17 ) in the process of creating a paper, so it is extremely difficult to step back and read the reviews objectively. [sent-20, score-0.203]
</p><p>18 One distinguishing characteristic of a bad review from a rejection is that it bothers you years later. [sent-21, score-0.519]
</p><p>19 If we accept that bad reviewing happens and want to address the issue, we are left with a very difficult problem. [sent-22, score-0.407]
</p><p>20 Many smart people have thought about improving this process, yielding the system we observe now. [sent-23, score-0.078]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('reviewer', 0.333), ('concludes', 0.243), ('erroneously', 0.243), ('theorem', 0.21), ('soda', 0.18), ('nips', 0.161), ('reviewing', 0.155), ('bad', 0.149), ('paper', 0.142), ('fail', 0.126), ('clearly', 0.118), ('comments', 0.117), ('colt', 0.114), ('commit', 0.108), ('dag', 0.108), ('disbelieve', 0.108), ('distinguished', 0.108), ('internally', 0.108), ('quantifiers', 0.108), ('reaction', 0.108), ('swaps', 0.108), ('sufficient', 0.107), ('difficult', 0.103), ('reviewers', 0.1), ('step', 0.1), ('birthday', 0.1), ('enthusiastic', 0.1), ('message', 0.1), ('paradox', 0.1), ('references', 0.1), ('trivially', 0.1), ('writer', 0.1), ('years', 0.099), ('actually', 0.095), ('characteristic', 0.094), ('believes', 0.094), ('crypto', 0.094), ('distinguishing', 0.094), ('fault', 0.094), ('jmlr', 0.094), ('icml', 0.089), ('admitting', 0.086), ('invested', 0.086), ('rejection', 0.083), ('claims', 0.083), ('notice', 0.083), ('flaw', 0.081), ('yielding', 0.078), ('false', 0.076), ('distinguish', 0.076)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="38-tfidf-1" href="../hunch_net-2005/hunch_net-2005-03-09-Bad_Reviewing.html">38 hunch net-2005-03-09-Bad Reviewing</a></p>
<p>Introduction: This is a difficult subject to talk about for many reasons, but a discussion
may be helpful.Bad reviewing is a problem in academia. The first step in
understanding this is admitting to the problem, so here is a short list of
examples of bad reviewing.Reviewer disbelieves theorem proof (ICML), or
disbelieve theorem with a trivially false counterexample. (COLT)Reviewer
internally swaps quantifiers in a theorem, concludes it has been done before
and is trivial. (NIPS)Reviewer believes a technique will not work despite
experimental validation. (COLT)Reviewers fail to notice flaw in theorem
statement (CRYPTO).Reviewer erroneously claims that it has been done before
(NIPS, SODA, JMLR)--(complete with references!)Reviewer inverts the message of
a paper and concludes it says nothing important. (NIPS*2)Reviewer fails to
distinguish between a DAG and a tree (SODA).Reviewer is enthusiastic about
paper but clearly does not understand (ICML).Reviewer erroneously believe that
the "birthday paradox"</p><p>2 0.24891502 <a title="38-tfidf-2" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>Introduction: Few would mistake the process of academic paper review for a fair process, but
sometimes the unfairness seems particularly striking. This is most easily seen
by comparison:PaperBanditronOffset TreeNotesProblem ScopeMulticlass problems
where only the loss of one choice can be probed.Strictly greater: Cost
sensitive multiclass problems where only the loss of one choice can be
probed.Often generalizations don't matter. That's not the case here, since
every plausible application I've thought of involves loss functions
substantially different from 0/1.What's newAnalysis and ExperimentsAlgorithm,
Analysis, and ExperimentsAs far as I know, the essence of the more general
problem was first stated and analyzed with theEXP4 algorithm (page 16)(1998).
It's also the time horizon 1 simplification of the Reinforcement Learning
setting for therandom trajectory method (page 15)(2002). The Banditron
algorithm itself is functionally identical toOne-Step RL with Traces (page
122)(2003) inBianca's thesis</p><p>3 0.24408288 <a title="38-tfidf-3" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>Introduction: When thinking about how best to review papers, it seems helpful to have some
conception of what good reviewing is. As far as I can tell, this is almost
always only discussed in the specific context of a paper (i.e. your rejected
paper), or at most an area (i.e. what a "good paper" looks like for that area)
rather than general principles. Neither individual papers or areas are
sufficiently general for a large conference--every paper differs in the
details, and what if you want to build a new area and/or cross areas?An
unavoidable reason for reviewing is that the community of research is too
large. In particular, it is not possible for a researcher to read every paper
which someone thinks might be of interest. This reason for reviewing exists
independent of constraints on rooms or scheduling formats of individual
conferences. Indeed, history suggests that physical constraints are relatively
meaningless over the long term -- growing conferences simply use more rooms
and/or change formats</p><p>4 0.22699963 <a title="38-tfidf-4" href="../hunch_net-2008/hunch_net-2008-10-14-Who_is_Responsible_for_a_Bad_Review%3F.html">320 hunch net-2008-10-14-Who is Responsible for a Bad Review?</a></p>
<p>Introduction: Although I'm greatly interested in machine learning, I think it must be
admitted that there is a large amount of low quality logic being used in
reviews. The problem is bad enough that sometimes I wonder if theByzantine
generalslimit has been exceeded. For example, I've seen recent reviews where
the given reasons for rejecting are:[NIPS] Theorem A is uninteresting because
Theorem B is uninteresting.[UAI] When you learn by memorization, the problem
addressed is trivial.[NIPS] The proof is in the appendix.[NIPS] This has been
done before. (â&euro;Ś but not giving any relevant citations)Just for the record I
want to point out what's wrong with these reviews. A future world in which
such reasons never come up again would be great, but I'm sure these errors
will be committed many times more in the future.This is nonsense. A theorem
should be evaluated based on it's merits, rather than the merits of another
theorem.Learning by memorization requires an exponentially larger sample
complexity than man</p><p>5 0.19374356 <a title="38-tfidf-5" href="../hunch_net-2005/hunch_net-2005-03-13-Avoiding_Bad_Reviewing.html">40 hunch net-2005-03-13-Avoiding Bad Reviewing</a></p>
<p>Introduction: If we accept that bad reviewing often occurs and want to fix it, the question
is "how"?Reviewing is done by paper writers just like yourself, so a good
proxy for this question is asking "How can I be a better reviewer?" Here are a
few things I've learned by trial (and error), as a paper writer, and as a
reviewer.The secret ingredient is careful thought. There is no good
substitution for a deep and careful understanding.Avoid reviewing papers that
you feel competitive about. You almost certainly will be asked to review
papers that feel competitive if you work on subjects of common interest. But,
the feeling of competition can easily lead to bad judgement.If you feel biased
for some other reason, then you should avoid reviewing. For exampleâ&euro;ŚFeeling
angry or threatened by a paper is a form of bias. See above.Double blind
yourself (avoid looking at the name even in a single-blind situation). The
significant effect of a name you recognize is making you pay close attention
to a paper. Since</p><p>6 0.19065453 <a title="38-tfidf-6" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>7 0.17209068 <a title="38-tfidf-7" href="../hunch_net-2006/hunch_net-2006-09-12-Incentive_Compatible_Reviewing.html">207 hunch net-2006-09-12-Incentive Compatible Reviewing</a></p>
<p>8 0.16788457 <a title="38-tfidf-8" href="../hunch_net-2008/hunch_net-2008-09-03-Bidding_Problems.html">315 hunch net-2008-09-03-Bidding Problems</a></p>
<p>9 0.16072413 <a title="38-tfidf-9" href="../hunch_net-2005/hunch_net-2005-04-04-Grounds_for_Rejection.html">52 hunch net-2005-04-04-Grounds for Rejection</a></p>
<p>10 0.15811436 <a title="38-tfidf-10" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>11 0.15168606 <a title="38-tfidf-11" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>12 0.15061381 <a title="38-tfidf-12" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>13 0.14734858 <a title="38-tfidf-13" href="../hunch_net-2012/hunch_net-2012-04-09-ICML_author_feedback_is_open.html">461 hunch net-2012-04-09-ICML author feedback is open</a></p>
<p>14 0.14542864 <a title="38-tfidf-14" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<p>15 0.13882999 <a title="38-tfidf-15" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>16 0.13419716 <a title="38-tfidf-16" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>17 0.13397411 <a title="38-tfidf-17" href="../hunch_net-2010/hunch_net-2010-04-26-Compassionate_Reviewing.html">395 hunch net-2010-04-26-Compassionate Reviewing</a></p>
<p>18 0.13326845 <a title="38-tfidf-18" href="../hunch_net-2012/hunch_net-2012-05-02-ICML%3A_Behind_the_Scenes.html">463 hunch net-2012-05-02-ICML: Behind the Scenes</a></p>
<p>19 0.12981331 <a title="38-tfidf-19" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>20 0.12376624 <a title="38-tfidf-20" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">23 hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.215), (1, 0.202), (2, -0.223), (3, -0.101), (4, -0.014), (5, 0.024), (6, 0.022), (7, -0.013), (8, -0.018), (9, 0.019), (10, 0.028), (11, 0.045), (12, 0.025), (13, -0.004), (14, -0.049), (15, 0.004), (16, -0.05), (17, -0.105), (18, -0.054), (19, 0.045), (20, -0.048), (21, 0.054), (22, -0.027), (23, 0.053), (24, 0.031), (25, 0.006), (26, -0.018), (27, -0.028), (28, 0.009), (29, -0.006), (30, -0.052), (31, 0.035), (32, 0.005), (33, -0.011), (34, 0.055), (35, -0.002), (36, -0.098), (37, 0.026), (38, 0.057), (39, 0.087), (40, -0.051), (41, 0.024), (42, 0.022), (43, 0.008), (44, 0.027), (45, -0.028), (46, -0.033), (47, 0.019), (48, -0.011), (49, -0.003)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98742348 <a title="38-lsi-1" href="../hunch_net-2005/hunch_net-2005-03-09-Bad_Reviewing.html">38 hunch net-2005-03-09-Bad Reviewing</a></p>
<p>Introduction: This is a difficult subject to talk about for many reasons, but a discussion
may be helpful.Bad reviewing is a problem in academia. The first step in
understanding this is admitting to the problem, so here is a short list of
examples of bad reviewing.Reviewer disbelieves theorem proof (ICML), or
disbelieve theorem with a trivially false counterexample. (COLT)Reviewer
internally swaps quantifiers in a theorem, concludes it has been done before
and is trivial. (NIPS)Reviewer believes a technique will not work despite
experimental validation. (COLT)Reviewers fail to notice flaw in theorem
statement (CRYPTO).Reviewer erroneously claims that it has been done before
(NIPS, SODA, JMLR)--(complete with references!)Reviewer inverts the message of
a paper and concludes it says nothing important. (NIPS*2)Reviewer fails to
distinguish between a DAG and a tree (SODA).Reviewer is enthusiastic about
paper but clearly does not understand (ICML).Reviewer erroneously believe that
the "birthday paradox"</p><p>2 0.86206388 <a title="38-lsi-2" href="../hunch_net-2008/hunch_net-2008-09-03-Bidding_Problems.html">315 hunch net-2008-09-03-Bidding Problems</a></p>
<p>Introduction: One way that many conferences in machine learning assign reviewers to papers
is via bidding, which has steps something like:Invite people to reviewAccept
papersReviewers look at title and abstract and state the papers they are
interested in reviewing.Some massaging happens, but reviewers often get
approximately the papers they bid for.At the ICML business meeting,Andrew
McCallumsuggested getting rid of bidding for papers. A couple reasons were
given:PrivacyThe title and abstract of the entire set of papers is visible to
every participating reviewer. Some authors might be uncomfortable about this
for submitted papers. I'm not sympathetic to this reason: the point of
submitting a paper to review is to publish it, so the value (if any) of not
publishing a part of it a little bit earlier seems limited.CliquesA bidding
system is gameable. If you have 3 buddies and you inform each other of your
submissions, you can each bid for your friend's papers and express a
disinterest in others. There</p><p>3 0.85140371 <a title="38-lsi-3" href="../hunch_net-2008/hunch_net-2008-10-14-Who_is_Responsible_for_a_Bad_Review%3F.html">320 hunch net-2008-10-14-Who is Responsible for a Bad Review?</a></p>
<p>Introduction: Although I'm greatly interested in machine learning, I think it must be
admitted that there is a large amount of low quality logic being used in
reviews. The problem is bad enough that sometimes I wonder if theByzantine
generalslimit has been exceeded. For example, I've seen recent reviews where
the given reasons for rejecting are:[NIPS] Theorem A is uninteresting because
Theorem B is uninteresting.[UAI] When you learn by memorization, the problem
addressed is trivial.[NIPS] The proof is in the appendix.[NIPS] This has been
done before. (â&euro;Ś but not giving any relevant citations)Just for the record I
want to point out what's wrong with these reviews. A future world in which
such reasons never come up again would be great, but I'm sure these errors
will be committed many times more in the future.This is nonsense. A theorem
should be evaluated based on it's merits, rather than the merits of another
theorem.Learning by memorization requires an exponentially larger sample
complexity than man</p><p>4 0.83443719 <a title="38-lsi-4" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>Introduction: When thinking about how best to review papers, it seems helpful to have some
conception of what good reviewing is. As far as I can tell, this is almost
always only discussed in the specific context of a paper (i.e. your rejected
paper), or at most an area (i.e. what a "good paper" looks like for that area)
rather than general principles. Neither individual papers or areas are
sufficiently general for a large conference--every paper differs in the
details, and what if you want to build a new area and/or cross areas?An
unavoidable reason for reviewing is that the community of research is too
large. In particular, it is not possible for a researcher to read every paper
which someone thinks might be of interest. This reason for reviewing exists
independent of constraints on rooms or scheduling formats of individual
conferences. Indeed, history suggests that physical constraints are relatively
meaningless over the long term -- growing conferences simply use more rooms
and/or change formats</p><p>5 0.80638206 <a title="38-lsi-5" href="../hunch_net-2012/hunch_net-2012-05-02-ICML%3A_Behind_the_Scenes.html">463 hunch net-2012-05-02-ICML: Behind the Scenes</a></p>
<p>Introduction: This is a rather long post, detailing the ICML 2012 review process. The goal
is to make the process more transparent, help authors understand how we came
to a decision, and discuss the strengths and weaknesses of this process for
future conference organizers.Microsoft’s Conference Management Toolkit (CMT)We
chose to useCMTover other conference management software mainly because of its
rich toolkit. The interface is sub-optimal (to say the least!) but it has
extensive capabilities (to handle bids, author response, resubmissions, etc.),
good import/export mechanisms (to process the data elsewhere), excellent
technical support (to answer late night emails, add new functionalities).
Overall, it was the right choice, although we hope a designer will look at
that interface sometime soon!Toronto Matching System (TMS)TMSis now being used
by many major conferences in our field (including NIPS and UAI). It is an
automated system (developed byLaurent CharlinandRich Zemelat U. Toronto) to
match re</p><p>6 0.79547435 <a title="38-lsi-6" href="../hunch_net-2005/hunch_net-2005-04-04-Grounds_for_Rejection.html">52 hunch net-2005-04-04-Grounds for Rejection</a></p>
<p>7 0.77952051 <a title="38-lsi-7" href="../hunch_net-2006/hunch_net-2006-12-04-Structural_Problems_in_NIPS_Decision_Making.html">221 hunch net-2006-12-04-Structural Problems in NIPS Decision Making</a></p>
<p>8 0.76138437 <a title="38-lsi-8" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>9 0.75513709 <a title="38-lsi-9" href="../hunch_net-2012/hunch_net-2012-04-09-ICML_author_feedback_is_open.html">461 hunch net-2012-04-09-ICML author feedback is open</a></p>
<p>10 0.73488802 <a title="38-lsi-10" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>11 0.72803146 <a title="38-lsi-11" href="../hunch_net-2005/hunch_net-2005-03-13-Avoiding_Bad_Reviewing.html">40 hunch net-2005-03-13-Avoiding Bad Reviewing</a></p>
<p>12 0.72762525 <a title="38-lsi-12" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>13 0.72607023 <a title="38-lsi-13" href="../hunch_net-2006/hunch_net-2006-09-12-Incentive_Compatible_Reviewing.html">207 hunch net-2006-09-12-Incentive Compatible Reviewing</a></p>
<p>14 0.66337687 <a title="38-lsi-14" href="../hunch_net-2007/hunch_net-2007-04-13-What_to_do_with_an_unreasonable_conditional_accept.html">238 hunch net-2007-04-13-What to do with an unreasonable conditional accept</a></p>
<p>15 0.66233438 <a title="38-lsi-15" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>16 0.64361429 <a title="38-lsi-16" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>17 0.64117509 <a title="38-lsi-17" href="../hunch_net-2009/hunch_net-2009-07-09-The_Machine_Learning_Forum.html">363 hunch net-2009-07-09-The Machine Learning Forum</a></p>
<p>18 0.63513303 <a title="38-lsi-18" href="../hunch_net-2012/hunch_net-2012-06-29-ICML_survey_and_comments.html">468 hunch net-2012-06-29-ICML survey and comments</a></p>
<p>19 0.61566645 <a title="38-lsi-19" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>20 0.61330569 <a title="38-lsi-20" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(23, 0.325), (35, 0.065), (42, 0.121), (45, 0.026), (50, 0.016), (68, 0.055), (74, 0.233), (95, 0.07)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.86837041 <a title="38-lda-1" href="../hunch_net-2005/hunch_net-2005-03-09-Bad_Reviewing.html">38 hunch net-2005-03-09-Bad Reviewing</a></p>
<p>Introduction: This is a difficult subject to talk about for many reasons, but a discussion
may be helpful.Bad reviewing is a problem in academia. The first step in
understanding this is admitting to the problem, so here is a short list of
examples of bad reviewing.Reviewer disbelieves theorem proof (ICML), or
disbelieve theorem with a trivially false counterexample. (COLT)Reviewer
internally swaps quantifiers in a theorem, concludes it has been done before
and is trivial. (NIPS)Reviewer believes a technique will not work despite
experimental validation. (COLT)Reviewers fail to notice flaw in theorem
statement (CRYPTO).Reviewer erroneously claims that it has been done before
(NIPS, SODA, JMLR)--(complete with references!)Reviewer inverts the message of
a paper and concludes it says nothing important. (NIPS*2)Reviewer fails to
distinguish between a DAG and a tree (SODA).Reviewer is enthusiastic about
paper but clearly does not understand (ICML).Reviewer erroneously believe that
the "birthday paradox"</p><p>2 0.62536329 <a title="38-lda-2" href="../hunch_net-2008/hunch_net-2008-10-14-Who_is_Responsible_for_a_Bad_Review%3F.html">320 hunch net-2008-10-14-Who is Responsible for a Bad Review?</a></p>
<p>Introduction: Although I'm greatly interested in machine learning, I think it must be
admitted that there is a large amount of low quality logic being used in
reviews. The problem is bad enough that sometimes I wonder if theByzantine
generalslimit has been exceeded. For example, I've seen recent reviews where
the given reasons for rejecting are:[NIPS] Theorem A is uninteresting because
Theorem B is uninteresting.[UAI] When you learn by memorization, the problem
addressed is trivial.[NIPS] The proof is in the appendix.[NIPS] This has been
done before. (â&euro;Ś but not giving any relevant citations)Just for the record I
want to point out what's wrong with these reviews. A future world in which
such reasons never come up again would be great, but I'm sure these errors
will be committed many times more in the future.This is nonsense. A theorem
should be evaluated based on it's merits, rather than the merits of another
theorem.Learning by memorization requires an exponentially larger sample
complexity than man</p><p>3 0.62281203 <a title="38-lda-3" href="../hunch_net-2007/hunch_net-2007-09-18-It%26%238217%3Bs_MDL_Jim%2C_but_not_as_we_know_it%26%238230%3B%28on_Bayes%2C_MDL_and_consistency%29.html">263 hunch net-2007-09-18-It&#8217;s MDL Jim, but not as we know it&#8230;(on Bayes, MDL and consistency)</a></p>
<p>Introduction: I have recently completeda 500+ page-book on MDL, the first comprehensive
overview of the field (yes, this is a sneak advertisement).Chapter 17compares
MDL to a menagerie of other methods and paradigms for learning and statistics.
By far the most time (20 pages) is spent on the relation between MDL and
Bayes. My two main points here are:In sharp contrast to Bayes, MDL is by
definition based on designing universal codes for the data relative to some
given (parametric or nonparametric) probabilistic model M. By some theorems
due toAndrew Barron, MDL inferencemusttherefore be statistically consistent,
and it is immune to Bayesian inconsistency results such as those by Diaconis,
Freedman and Barron (I explain what I mean by "inconsistency" further below).
Hence, MDL must be different from Bayes!In contrast to what has sometimes been
claimed, practical MDL algorithms do have a subjective component (which in
many, but not all cases, may be implemented by something similar to a Bayesian
prior</p><p>4 0.62131846 <a title="38-lda-4" href="../hunch_net-2006/hunch_net-2006-07-11-New_Models.html">194 hunch net-2006-07-11-New Models</a></p>
<p>Introduction: How should we, as researchers in machine learning, organize ourselves?The most
immediate measurable objective of computer science research is publishing a
paper. The most difficult aspect of publishing a paper is having reviewers
accept and recommend it for publication. The simplest mechanism for doing this
is to show theoretical progress on some standard, well-known easily understood
problem.In doing this, we often fall into a local minima of the research
process. The basic problem in machine learning is that it is very unclear that
the mathematical model is the right one for the (or some) real problem. A good
mathematical model in machine learning should have one fundamental trait: it
should aid the design of effective learning algorithms. To date, our ability
to solve interesting learning problems (speech recognition, machine
translation, object recognition, etcâ&euro;Ś) remains limited (although improving),
so the "rightness" of our models is in doubt.If our mathematical models are
bad, t</p><p>5 0.61727715 <a title="38-lda-5" href="../hunch_net-2005/hunch_net-2005-05-28-Running_A_Machine_Learning_Summer_School.html">75 hunch net-2005-05-28-Running A Machine Learning Summer School</a></p>
<p>Introduction: We just finished theChicago 2005 Machine Learning Summer School. The school
was 2 weeks long with about 130 (or 140 counting the speakers) participants.
For perspective, this is perhaps the largest graduate level machine learning
class I am aware of anywhere and anytime (previousMLSSs have been close).
Overall, it seemed to go well, although the students are the real authority on
this. For those who missed it, DVDs will be available from our Slovenian
friends. EmailMrs Spela Sitarof the Jozsef Stefan Institute for details.The
following are some notes for future planning and those interested.Good
DecisionsAcquiring the larger-than-necessary "Assembly Hall" atInternational
House. Our attendance came in well above our expectations, so this was a
critical early decision that made a huge difference.The invited speakers were
key. They made a huge difference in the quality of the content.Delegating
early and often was important. One key difficulty here is gauging how much a
volunteer can (or</p><p>6 0.61690068 <a title="38-lda-6" href="../hunch_net-2006/hunch_net-2006-09-18-What_is_missing_for_online_collaborative_research%3F.html">208 hunch net-2006-09-18-What is missing for online collaborative research?</a></p>
<p>7 0.61651289 <a title="38-lda-7" href="../hunch_net-2005/hunch_net-2005-07-04-The_Health_of_COLT.html">89 hunch net-2005-07-04-The Health of COLT</a></p>
<p>8 0.61356115 <a title="38-lda-8" href="../hunch_net-2012/hunch_net-2012-05-02-ICML%3A_Behind_the_Scenes.html">463 hunch net-2012-05-02-ICML: Behind the Scenes</a></p>
<p>9 0.61343861 <a title="38-lda-9" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>10 0.61173475 <a title="38-lda-10" href="../hunch_net-2007/hunch_net-2007-01-26-Parallel_Machine_Learning_Problems.html">229 hunch net-2007-01-26-Parallel Machine Learning Problems</a></p>
<p>11 0.60393226 <a title="38-lda-11" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>12 0.59965712 <a title="38-lda-12" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>13 0.59392118 <a title="38-lda-13" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>14 0.59272498 <a title="38-lda-14" href="../hunch_net-2006/hunch_net-2006-04-30-John_Langford_%26%238211%3B%3E_Yahoo_Research%2C_NY.html">175 hunch net-2006-04-30-John Langford &#8211;> Yahoo Research, NY</a></p>
<p>15 0.59243923 <a title="38-lda-15" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>16 0.58923787 <a title="38-lda-16" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>17 0.58781093 <a title="38-lda-17" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>18 0.57975167 <a title="38-lda-18" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>19 0.57716399 <a title="38-lda-19" href="../hunch_net-2008/hunch_net-2008-09-03-Bidding_Problems.html">315 hunch net-2008-09-03-Bidding Problems</a></p>
<p>20 0.57389414 <a title="38-lda-20" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
