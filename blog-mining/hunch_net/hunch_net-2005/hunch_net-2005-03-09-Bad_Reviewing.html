<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>38 hunch net-2005-03-09-Bad Reviewing</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-38" href="#">hunch_net-2005-38</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>38 hunch net-2005-03-09-Bad Reviewing</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-38-html" href="http://hunch.net/?p=42">html</a></p><p>Introduction: This is a difficult subject to talk about for many reasons, but a discussion may be helpful.
 
Bad reviewing is a problem in academia.  The first step in understanding this is admitting to the problem, so here is a short list of examples of bad reviewing. 
  
 Reviewer disbelieves theorem proof (ICML),  or disbelieve theorem with a trivially false counterexample. (COLT)  
 Reviewer internally swaps quantifiers in a theorem, concludes it has been done before and is trivial. (NIPS) 
 Reviewer believes a technique will not work despite experimental validation. (COLT) 
 Reviewers fail to notice flaw in theorem statement (CRYPTO).   
 Reviewer erroneously claims that it has been done before (NIPS, SODA, JMLR)—(complete with references!) 
 Reviewer inverts the message of a paper and concludes it says nothing important. (NIPS*2) 
 Reviewer fails to distinguish between a DAG and a tree (SODA). 
 Reviewer is enthusiastic about paper but clearly does not understand (ICML). 
 Reviewer erroneously</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 This is a difficult subject to talk about for many reasons, but a discussion may be helpful. [sent-1, score-0.095]
</p><p>2 The first step in understanding this is admitting to the problem, so here is a short list of examples of bad reviewing. [sent-3, score-0.39]
</p><p>3 Reviewer disbelieves theorem proof (ICML),  or disbelieve theorem with a trivially false counterexample. [sent-4, score-0.57]
</p><p>4 (COLT)    Reviewer internally swaps quantifiers in a theorem, concludes it has been done before and is trivial. [sent-5, score-0.519]
</p><p>5 (NIPS)   Reviewer believes a technique will not work despite experimental validation. [sent-6, score-0.086]
</p><p>6 (COLT)   Reviewers fail to notice flaw in theorem statement (CRYPTO). [sent-7, score-0.455]
</p><p>7 Reviewer erroneously claims that it has been done before (NIPS, SODA, JMLR)—(complete with references! [sent-8, score-0.298]
</p><p>8 )   Reviewer inverts the message of a paper and concludes it says nothing important. [sent-9, score-0.425]
</p><p>9 (NIPS*2)   Reviewer fails to distinguish between a DAG and a tree (SODA). [sent-10, score-0.07]
</p><p>10 Reviewer is enthusiastic about paper but clearly does not understand (ICML). [sent-11, score-0.313]
</p><p>11 Reviewer erroneously believe that the “birthday paradox” is relevant (CCS). [sent-12, score-0.222]
</p><p>12 The above is only for cases where there was sufficient reviewer comments to actually understand reviewer failure modes. [sent-13, score-1.291]
</p><p>13 Many reviewers fail to leave sufficient comments and it’s easy to imagine they commit similar mistakes. [sent-14, score-0.574]
</p><p>14 Bad reviewing should be clearly distinguished from rejections—note that some of the above examples are actually accepts. [sent-15, score-0.421]
</p><p>15 The standard psychological reaction to any rejected paper is trying to find fault with the reviewers. [sent-16, score-0.302]
</p><p>16 You, as a paper writer, have invested significant work (weeks? [sent-17, score-0.196]
</p><p>17 ) in the process of creating a paper, so it is extremely difficult to step back and read the reviews objectively. [sent-20, score-0.185]
</p><p>18 One distinguishing characteristic of a bad review from a rejection is that it bothers you years later. [sent-21, score-0.559]
</p><p>19 If we accept that bad reviewing happens and want to address the issue, we are left with a very difficult problem. [sent-22, score-0.454]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('reviewer', 0.502), ('concludes', 0.222), ('erroneously', 0.222), ('bad', 0.221), ('theorem', 0.19), ('soda', 0.158), ('reviewing', 0.138), ('nips', 0.12), ('paper', 0.117), ('fail', 0.115), ('clearly', 0.105), ('comments', 0.103), ('commit', 0.099), ('dag', 0.099), ('disbelieve', 0.099), ('internally', 0.099), ('quantifiers', 0.099), ('reaction', 0.099), ('swaps', 0.099), ('sufficient', 0.097), ('difficult', 0.095), ('reviewers', 0.092), ('birthday', 0.091), ('distinguished', 0.091), ('enthusiastic', 0.091), ('paradox', 0.091), ('references', 0.091), ('trivially', 0.091), ('writer', 0.091), ('step', 0.09), ('years', 0.09), ('actually', 0.087), ('characteristic', 0.086), ('believes', 0.086), ('distinguishing', 0.086), ('fault', 0.086), ('message', 0.086), ('jmlr', 0.082), ('rejections', 0.082), ('colt', 0.081), ('admitting', 0.079), ('crypto', 0.079), ('invested', 0.079), ('rejection', 0.076), ('claims', 0.076), ('notice', 0.076), ('flaw', 0.074), ('distinguish', 0.07), ('months', 0.068), ('leave', 0.068)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="38-tfidf-1" href="../hunch_net-2005/hunch_net-2005-03-09-Bad_Reviewing.html">38 hunch net-2005-03-09-Bad Reviewing</a></p>
<p>Introduction: This is a difficult subject to talk about for many reasons, but a discussion may be helpful.
 
Bad reviewing is a problem in academia.  The first step in understanding this is admitting to the problem, so here is a short list of examples of bad reviewing. 
  
 Reviewer disbelieves theorem proof (ICML),  or disbelieve theorem with a trivially false counterexample. (COLT)  
 Reviewer internally swaps quantifiers in a theorem, concludes it has been done before and is trivial. (NIPS) 
 Reviewer believes a technique will not work despite experimental validation. (COLT) 
 Reviewers fail to notice flaw in theorem statement (CRYPTO).   
 Reviewer erroneously claims that it has been done before (NIPS, SODA, JMLR)—(complete with references!) 
 Reviewer inverts the message of a paper and concludes it says nothing important. (NIPS*2) 
 Reviewer fails to distinguish between a DAG and a tree (SODA). 
 Reviewer is enthusiastic about paper but clearly does not understand (ICML). 
 Reviewer erroneously</p><p>2 0.28989938 <a title="38-tfidf-2" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>Introduction: Few would mistake the process of academic paper review for a fair process, but sometimes the unfairness seems particularly striking.  This is most easily seen by comparison:
  
 
 Paper 
  Banditron  
  Offset Tree  
 Notes 
 
 
 Problem Scope 
 Multiclass problems where only the loss of one choice can be probed. 
 Strictly greater: Cost sensitive multiclass problems where only the loss of one choice can be probed. 
 Often generalizations don’t matter.  That’s not the case here, since every plausible application I’ve thought of involves loss functions substantially different from 0/1. 
 
 
 What’s new 
 Analysis and Experiments 
 Algorithm, Analysis, and Experiments 
  As far as I know, the essence of the more general problem was first stated and analyzed with the  EXP4 algorithm (page 16)  (1998).  It’s also the time horizon 1 simplification of the Reinforcement Learning setting for the  random trajectory method (page 15)  (2002).  The Banditron algorithm itself is functionally identi</p><p>3 0.26740739 <a title="38-tfidf-3" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>Introduction: When thinking about how best to review papers, it seems helpful to have some conception of what good reviewing is.  As far as I can tell, this is almost always only discussed in the specific context of a paper (i.e. your rejected paper), or at most an area (i.e. what a “good paper” looks like for that area) rather than general principles.  Neither individual papers or areas are sufficiently general for a large conference—every paper differs in the details, and what if you want to build a new area and/or cross areas?
 
An unavoidable reason for reviewing is that the community of research is too large.  In particular, it is not possible for a researcher to read every paper which someone thinks might be of interest.  This reason for reviewing exists independent of constraints on rooms or scheduling formats of individual conferences.  Indeed, history suggests that physical constraints are relatively meaningless over the long term — growing conferences simply use more rooms and/or change fo</p><p>4 0.23255721 <a title="38-tfidf-4" href="../hunch_net-2008/hunch_net-2008-10-14-Who_is_Responsible_for_a_Bad_Review%3F.html">320 hunch net-2008-10-14-Who is Responsible for a Bad Review?</a></p>
<p>Introduction: Although I’m greatly interested in machine learning, I think it must be admitted that there is a large amount of low quality logic being used in reviews.  The problem is bad enough that sometimes I wonder if the  Byzantine generals  limit has been exceeded.  For example, I’ve seen recent reviews where the given reasons for rejecting are:
  
 [ NIPS ] Theorem A is uninteresting because Theorem B is uninteresting. 
 [ UAI ] When you learn by memorization, the problem addressed is trivial. 
 [NIPS] The proof is in the appendix.  
 [NIPS] This has been done before.  (… but not giving any relevant citations)  
  
Just for the record I want to point out what’s wrong with these reviews.  A future world in which such reasons never come up again would be great, but I’m sure these errors will be committed many times more in the future.
  
 This is nonsense.  A theorem should be evaluated based on it’s merits, rather than the merits of another theorem. 
 Learning by memorization requires an expon</p><p>5 0.22150099 <a title="38-tfidf-5" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>Introduction: Essentially everyone who writes research papers suffers rejections.  They always sting immediately, but upon further reflection many of these rejections come to seem reasonable.  Maybe the equations had too many typos or maybe the topic just isn’t as important as was originally thought.  A few rejections do not come to seem acceptable, and these form the basis of reviewing horror stories, a great material for conversations.  I’ve decided to share three of mine, now all safely a bit distant in the past.
  
  Prediction Theory for Classification Tutorial .  This is a tutorial about tight sample complexity bounds for classification that I submitted to  JMLR .  The first decision I heard was a reject which appeared quite unjust to me—for example one of the reviewers appeared to claim that all the content was in standard statistics books.  Upon further inquiry, several citations were given, none of which actually covered the content.  Later, I was shocked to hear the paper was accepted. App</p><p>6 0.21151029 <a title="38-tfidf-6" href="../hunch_net-2006/hunch_net-2006-09-12-Incentive_Compatible_Reviewing.html">207 hunch net-2006-09-12-Incentive Compatible Reviewing</a></p>
<p>7 0.19883481 <a title="38-tfidf-7" href="../hunch_net-2008/hunch_net-2008-09-03-Bidding_Problems.html">315 hunch net-2008-09-03-Bidding Problems</a></p>
<p>8 0.19827494 <a title="38-tfidf-8" href="../hunch_net-2005/hunch_net-2005-03-13-Avoiding_Bad_Reviewing.html">40 hunch net-2005-03-13-Avoiding Bad Reviewing</a></p>
<p>9 0.15660539 <a title="38-tfidf-9" href="../hunch_net-2005/hunch_net-2005-04-04-Grounds_for_Rejection.html">52 hunch net-2005-04-04-Grounds for Rejection</a></p>
<p>10 0.1539274 <a title="38-tfidf-10" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>11 0.14696799 <a title="38-tfidf-11" href="../hunch_net-2010/hunch_net-2010-04-26-Compassionate_Reviewing.html">395 hunch net-2010-04-26-Compassionate Reviewing</a></p>
<p>12 0.14324453 <a title="38-tfidf-12" href="../hunch_net-2012/hunch_net-2012-04-09-ICML_author_feedback_is_open.html">461 hunch net-2012-04-09-ICML author feedback is open</a></p>
<p>13 0.14193313 <a title="38-tfidf-13" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>14 0.14020979 <a title="38-tfidf-14" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>15 0.13978164 <a title="38-tfidf-15" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>16 0.12602957 <a title="38-tfidf-16" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>17 0.12373362 <a title="38-tfidf-17" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>18 0.12331507 <a title="38-tfidf-18" href="../hunch_net-2012/hunch_net-2012-05-02-ICML%3A_Behind_the_Scenes.html">463 hunch net-2012-05-02-ICML: Behind the Scenes</a></p>
<p>19 0.11321685 <a title="38-tfidf-19" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>20 0.11320861 <a title="38-tfidf-20" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.205), (1, -0.165), (2, 0.258), (3, 0.111), (4, 0.035), (5, 0.085), (6, 0.026), (7, 0.009), (8, -0.004), (9, 0.021), (10, 0.012), (11, -0.031), (12, 0.061), (13, -0.004), (14, -0.051), (15, 0.004), (16, 0.026), (17, 0.014), (18, 0.016), (19, -0.003), (20, -0.074), (21, 0.044), (22, -0.02), (23, -0.098), (24, -0.044), (25, 0.038), (26, 0.028), (27, -0.061), (28, 0.076), (29, -0.056), (30, -0.009), (31, 0.023), (32, -0.057), (33, 0.063), (34, -0.043), (35, 0.017), (36, -0.057), (37, 0.076), (38, -0.04), (39, -0.054), (40, -0.026), (41, -0.01), (42, -0.037), (43, -0.036), (44, 0.056), (45, -0.098), (46, -0.022), (47, 0.024), (48, 0.047), (49, -0.076)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98764127 <a title="38-lsi-1" href="../hunch_net-2005/hunch_net-2005-03-09-Bad_Reviewing.html">38 hunch net-2005-03-09-Bad Reviewing</a></p>
<p>Introduction: This is a difficult subject to talk about for many reasons, but a discussion may be helpful.
 
Bad reviewing is a problem in academia.  The first step in understanding this is admitting to the problem, so here is a short list of examples of bad reviewing. 
  
 Reviewer disbelieves theorem proof (ICML),  or disbelieve theorem with a trivially false counterexample. (COLT)  
 Reviewer internally swaps quantifiers in a theorem, concludes it has been done before and is trivial. (NIPS) 
 Reviewer believes a technique will not work despite experimental validation. (COLT) 
 Reviewers fail to notice flaw in theorem statement (CRYPTO).   
 Reviewer erroneously claims that it has been done before (NIPS, SODA, JMLR)—(complete with references!) 
 Reviewer inverts the message of a paper and concludes it says nothing important. (NIPS*2) 
 Reviewer fails to distinguish between a DAG and a tree (SODA). 
 Reviewer is enthusiastic about paper but clearly does not understand (ICML). 
 Reviewer erroneously</p><p>2 0.85563385 <a title="38-lsi-2" href="../hunch_net-2008/hunch_net-2008-10-14-Who_is_Responsible_for_a_Bad_Review%3F.html">320 hunch net-2008-10-14-Who is Responsible for a Bad Review?</a></p>
<p>Introduction: Although I’m greatly interested in machine learning, I think it must be admitted that there is a large amount of low quality logic being used in reviews.  The problem is bad enough that sometimes I wonder if the  Byzantine generals  limit has been exceeded.  For example, I’ve seen recent reviews where the given reasons for rejecting are:
  
 [ NIPS ] Theorem A is uninteresting because Theorem B is uninteresting. 
 [ UAI ] When you learn by memorization, the problem addressed is trivial. 
 [NIPS] The proof is in the appendix.  
 [NIPS] This has been done before.  (… but not giving any relevant citations)  
  
Just for the record I want to point out what’s wrong with these reviews.  A future world in which such reasons never come up again would be great, but I’m sure these errors will be committed many times more in the future.
  
 This is nonsense.  A theorem should be evaluated based on it’s merits, rather than the merits of another theorem. 
 Learning by memorization requires an expon</p><p>3 0.82675511 <a title="38-lsi-3" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>Introduction: When thinking about how best to review papers, it seems helpful to have some conception of what good reviewing is.  As far as I can tell, this is almost always only discussed in the specific context of a paper (i.e. your rejected paper), or at most an area (i.e. what a “good paper” looks like for that area) rather than general principles.  Neither individual papers or areas are sufficiently general for a large conference—every paper differs in the details, and what if you want to build a new area and/or cross areas?
 
An unavoidable reason for reviewing is that the community of research is too large.  In particular, it is not possible for a researcher to read every paper which someone thinks might be of interest.  This reason for reviewing exists independent of constraints on rooms or scheduling formats of individual conferences.  Indeed, history suggests that physical constraints are relatively meaningless over the long term — growing conferences simply use more rooms and/or change fo</p><p>4 0.81951034 <a title="38-lsi-4" href="../hunch_net-2008/hunch_net-2008-09-03-Bidding_Problems.html">315 hunch net-2008-09-03-Bidding Problems</a></p>
<p>Introduction: One way that many conferences in machine learning assign reviewers to papers is via bidding, which has steps something like:
  
 Invite people to review 
 Accept papers 
 Reviewers look at title and abstract and state the papers they are interested in reviewing. 
 Some massaging happens, but reviewers often get approximately the papers they bid for. 
  
At the ICML business meeting,  Andrew McCallum  suggested getting rid of bidding for papers.  A couple reasons were given:
  
  Privacy  The title and abstract of the entire set of papers is visible to every participating reviewer.  Some authors might be uncomfortable about this for submitted papers.  I’m not sympathetic to this reason: the point of submitting a paper to review is to publish it, so the value (if any) of not publishing a part of it a little bit earlier seems limited. 
  Cliques   A bidding system is gameable.  If you have 3 buddies and you inform each other of your submissions, you can each bid for your friend’s papers a</p><p>5 0.77705866 <a title="38-lsi-5" href="../hunch_net-2005/hunch_net-2005-04-04-Grounds_for_Rejection.html">52 hunch net-2005-04-04-Grounds for Rejection</a></p>
<p>Introduction: It’s reviewing season right now, so I thought I would list (at a high level) the sorts of problems which I see in papers.  Hopefully, this will help us all write better papers.
 
The following flaws are fatal to any paper:
  
  Incorrect theorem or lemma statements  A typo might be “ok”, if it can be understood.  Any theorem or lemma which indicates an incorrect understanding of reality must be rejected.  Not doing so would severely harm the integrity of the conference.  A paper rejected for this reason must be fixed. 
  Lack of Understanding  If a paper is understood by none of the (typically 3) reviewers then it must be rejected for the same reason.  This is more controversial than it sounds because there are some people who maximize paper complexity in the hope of impressing the reviewer.  The tactic sometimes succeeds with some reviewers (but not with me).

As a reviewer, I sometimes get lost for stupid reasons.  This is why an anonymized  communication channel  with the author can</p><p>6 0.77188188 <a title="38-lsi-6" href="../hunch_net-2012/hunch_net-2012-04-09-ICML_author_feedback_is_open.html">461 hunch net-2012-04-09-ICML author feedback is open</a></p>
<p>7 0.76665235 <a title="38-lsi-7" href="../hunch_net-2012/hunch_net-2012-05-02-ICML%3A_Behind_the_Scenes.html">463 hunch net-2012-05-02-ICML: Behind the Scenes</a></p>
<p>8 0.75442028 <a title="38-lsi-8" href="../hunch_net-2006/hunch_net-2006-09-12-Incentive_Compatible_Reviewing.html">207 hunch net-2006-09-12-Incentive Compatible Reviewing</a></p>
<p>9 0.74815059 <a title="38-lsi-9" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>10 0.73236352 <a title="38-lsi-10" href="../hunch_net-2007/hunch_net-2007-04-13-What_to_do_with_an_unreasonable_conditional_accept.html">238 hunch net-2007-04-13-What to do with an unreasonable conditional accept</a></p>
<p>11 0.72518468 <a title="38-lsi-11" href="../hunch_net-2006/hunch_net-2006-12-04-Structural_Problems_in_NIPS_Decision_Making.html">221 hunch net-2006-12-04-Structural Problems in NIPS Decision Making</a></p>
<p>12 0.72347248 <a title="38-lsi-12" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>13 0.67733383 <a title="38-lsi-13" href="../hunch_net-2005/hunch_net-2005-03-13-Avoiding_Bad_Reviewing.html">40 hunch net-2005-03-13-Avoiding Bad Reviewing</a></p>
<p>14 0.65808362 <a title="38-lsi-14" href="../hunch_net-2013/hunch_net-2013-06-29-The_Benefits_of_Double-Blind_Review.html">485 hunch net-2013-06-29-The Benefits of Double-Blind Review</a></p>
<p>15 0.64933366 <a title="38-lsi-15" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>16 0.6329754 <a title="38-lsi-16" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>17 0.6015777 <a title="38-lsi-17" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>18 0.57062477 <a title="38-lsi-18" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>19 0.55918998 <a title="38-lsi-19" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>20 0.55500275 <a title="38-lsi-20" href="../hunch_net-2010/hunch_net-2010-04-26-Compassionate_Reviewing.html">395 hunch net-2010-04-26-Compassionate Reviewing</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(10, 0.427), (27, 0.174), (38, 0.025), (53, 0.06), (55, 0.135), (94, 0.046), (95, 0.038)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.99065375 <a title="38-lda-1" href="../hunch_net-2012/hunch_net-2012-10-18-7th_Annual_Machine_Learning_Symposium.html">474 hunch net-2012-10-18-7th Annual Machine Learning Symposium</a></p>
<p>Introduction: A reminder that the  New York Academy of Sciences  will be hosting the  7th Annual Machine Learning Symposium  tomorrow from 9:30am.
 
The main program will feature invited talks from  Peter Bartlett ,  William Freeman , and  Vladimir Vapnik , along with numerous spotlight talks and a poster session. Following the main program,  hackNY  and  Microsoft Research  are sponsoring a networking hour with talks from machine learning practitioners at NYC startups (specifically  bit.ly ,  Buzzfeed ,  Chartbeat , and  Sense Networks ,  Visual Revenue ).  This should be of great interest to everyone considering working in machine learning.</p><p>same-blog 2 0.96716893 <a title="38-lda-2" href="../hunch_net-2005/hunch_net-2005-03-09-Bad_Reviewing.html">38 hunch net-2005-03-09-Bad Reviewing</a></p>
<p>Introduction: This is a difficult subject to talk about for many reasons, but a discussion may be helpful.
 
Bad reviewing is a problem in academia.  The first step in understanding this is admitting to the problem, so here is a short list of examples of bad reviewing. 
  
 Reviewer disbelieves theorem proof (ICML),  or disbelieve theorem with a trivially false counterexample. (COLT)  
 Reviewer internally swaps quantifiers in a theorem, concludes it has been done before and is trivial. (NIPS) 
 Reviewer believes a technique will not work despite experimental validation. (COLT) 
 Reviewers fail to notice flaw in theorem statement (CRYPTO).   
 Reviewer erroneously claims that it has been done before (NIPS, SODA, JMLR)—(complete with references!) 
 Reviewer inverts the message of a paper and concludes it says nothing important. (NIPS*2) 
 Reviewer fails to distinguish between a DAG and a tree (SODA). 
 Reviewer is enthusiastic about paper but clearly does not understand (ICML). 
 Reviewer erroneously</p><p>3 0.91661441 <a title="38-lda-3" href="../hunch_net-2006/hunch_net-2006-07-26-Two_more_UAI_papers_of_interest.html">199 hunch net-2006-07-26-Two more UAI papers of interest</a></p>
<p>Introduction: In addition to Ed Snelson’s paper, there were (at least) two other papers that caught my eye at UAI.
 
One was  this paper  by Sanjoy Dasgupta, Daniel Hsu and Nakul Verma at UCSD which shows in a surprisingly general and strong way that almost all linear projections of any jointly distributed vector random variable with finite first and second moments look sphereical and unimodal (in fact look like a scale mixture of Gaussians). Great result, as you’d expect from Sanjoy.
 
The other paper which I found intriguing but which I just haven’t groked yet is  this beast  by Manfred and Dima Kuzmin. 
You can check out the (beautiful)  slides  
if that helps. I feel like there is something deep here, but my brain is too small to understand it. The COLT and last NIPS papers/slides are also on Manfred’s page. Hopefully someone here can illuminate.</p><p>4 0.90617234 <a title="38-lda-4" href="../hunch_net-2005/hunch_net-2005-04-10-Is_the_Goal_Understanding_or_Prediction%3F.html">55 hunch net-2005-04-10-Is the Goal Understanding or Prediction?</a></p>
<p>Introduction: Steve Smale  and I have a debate about goals of learning theory.
 
Steve likes theorems with a dependence on unobservable quantities.  For example, if  D  is a distribution over a space  X x [0,1] , you can state a theorem about the error rate dependent on the variance,  E (x,y)~D  (y-E y’~D|x [y']) 2  .
 
I dislike this, because I want to use the theorems to produce code solving learning problems.  Since I don’t know (and can’t measure) the variance, a theorem depending on the variance does not help me—I would not know what variance to plug into the learning algorithm.
 
Recast more broadly, this is a debate between “declarative” and “operative” mathematics.  A strong example of “declarative” mathematics is  “a new kind of science” .  Roughly speaking, the goal of this kind of approach seems to be finding a way to explain the observations we make.  Examples include “some things are unpredictable”, “a phase transition exists”, etc…
 
“Operative” mathematics helps you make predictions a</p><p>5 0.90344119 <a title="38-lda-5" href="../hunch_net-2011/hunch_net-2011-05-09-CI_Fellows%2C_again.html">434 hunch net-2011-05-09-CI Fellows, again</a></p>
<p>Introduction: Lev  and  Hal  point out the  CI Fellows  program is on again for this year.  Lev visited me for a year under this program, and I quite enjoyed it.  Due May 31.</p><p>6 0.85849673 <a title="38-lda-6" href="../hunch_net-2006/hunch_net-2006-06-05-Server_Shift%2C_Site_Tweaks%2C_Suggestions%3F.html">182 hunch net-2006-06-05-Server Shift, Site Tweaks, Suggestions?</a></p>
<p>7 0.85268563 <a title="38-lda-7" href="../hunch_net-2007/hunch_net-2007-04-21-Videolectures.net.html">240 hunch net-2007-04-21-Videolectures.net</a></p>
<p>8 0.70671099 <a title="38-lda-8" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>9 0.67594278 <a title="38-lda-9" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>10 0.57618964 <a title="38-lda-10" href="../hunch_net-2006/hunch_net-2006-09-12-Incentive_Compatible_Reviewing.html">207 hunch net-2006-09-12-Incentive Compatible Reviewing</a></p>
<p>11 0.56425905 <a title="38-lda-11" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>12 0.56043851 <a title="38-lda-12" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>13 0.55004388 <a title="38-lda-13" href="../hunch_net-2005/hunch_net-2005-03-13-Avoiding_Bad_Reviewing.html">40 hunch net-2005-03-13-Avoiding Bad Reviewing</a></p>
<p>14 0.54313147 <a title="38-lda-14" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>15 0.53817081 <a title="38-lda-15" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>16 0.53557563 <a title="38-lda-16" href="../hunch_net-2009/hunch_net-2009-07-09-The_Machine_Learning_Forum.html">363 hunch net-2009-07-09-The Machine Learning Forum</a></p>
<p>17 0.53481859 <a title="38-lda-17" href="../hunch_net-2007/hunch_net-2007-12-19-Cool_and_interesting_things_seen_at_NIPS.html">279 hunch net-2007-12-19-Cool and interesting things seen at NIPS</a></p>
<p>18 0.53381848 <a title="38-lda-18" href="../hunch_net-2008/hunch_net-2008-10-14-Who_is_Responsible_for_a_Bad_Review%3F.html">320 hunch net-2008-10-14-Who is Responsible for a Bad Review?</a></p>
<p>19 0.53332353 <a title="38-lda-19" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>20 0.52959591 <a title="38-lda-20" href="../hunch_net-2012/hunch_net-2012-05-02-ICML%3A_Behind_the_Scenes.html">463 hunch net-2012-05-02-ICML: Behind the Scenes</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
