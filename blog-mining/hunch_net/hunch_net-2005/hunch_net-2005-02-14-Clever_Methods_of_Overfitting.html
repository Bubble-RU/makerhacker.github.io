<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>19 hunch net-2005-02-14-Clever Methods of Overfitting</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-19" href="#">hunch_net-2005-19</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>19 hunch net-2005-02-14-Clever Methods of Overfitting</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-19-html" href="http://hunch.net/?p=22">html</a></p><p>Introduction: "Overfitting" is traditionally defined as training some flexible
representation so that it memorizes the data but fails to predict well in the
future. For this post, I will define overfitting more generally as over-
representing the performance of systems. There are two styles of general
overfitting: overrepresenting performance on particular datasets and
(implicitly) overrepresenting performance of a method on future datasets.We
should all be aware of these methods, avoid them where possible, and take them
into account otherwise. I have used "reproblem" and "old datasets", and may
have participated in "overfitting by review"--some of these are very difficult
to avoid.NameMethodExplanationRemedyTraditional overfittingTrain a complex
predictor on too-few examples.Hold out pristine examples for testing.Use a
simpler predictor.Get more training examples.Integrate over many
predictors.Reject papers which do this.Parameter tweak overfittingUse a
learning algorithm with many parameters. Choo</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 For this post, I will define overfitting more generally as over- representing the performance of systems. [sent-2, score-0.664]
</p><p>2 There are two styles of general overfitting: overrepresenting performance on particular datasets and (implicitly) overrepresenting performance of a method on future datasets. [sent-3, score-1.317]
</p><p>3 Choose the parameters based on the test set performance. [sent-13, score-0.332]
</p><p>4 For example, choosing the features so as to optimize test set performance can achieve this. [sent-14, score-0.663]
</p><p>5 Brittle measureUse a measure of performance which is especially brittle to overfitting. [sent-16, score-0.543]
</p><p>6 One common example is pretending that cross validation performance is drawn from an i. [sent-21, score-0.682]
</p><p>7 Choice of measureChoose the best of Accuracy, error rate, (A)ROC, F1, percent improvement on the previous best, percent improvement of error rate, etc. [sent-29, score-0.446]
</p><p>8 For example, the performance measure directly motivated by the problem. [sent-35, score-0.411]
</p><p>9 Incomplete PredictionInstead of (say) making a multiclass prediction, make a set of binary predictions, then compute the optimal multiclass prediction. [sent-36, score-0.276]
</p><p>10 Use a human as part of a learning algorithm and don't take into account overfitting by the entire human/computer interaction. [sent-40, score-0.658]
</p><p>11 One example is a human using a clustering algorithm (on training and test examples) to guide learning algorithm choice. [sent-42, score-0.857]
</p><p>12 Make sure test examples are not available to the human. [sent-43, score-0.455]
</p><p>13 Data set selectionChose to report results on some subset of datasets where your algorithm performs well. [sent-44, score-0.606]
</p><p>14 The reason why we test on natural datasets is because we believe there is some structure captured by the past problems that helps on future problems. [sent-45, score-0.621]
</p><p>15 For example, take a time series dataset and use cross validation. [sent-51, score-0.346]
</p><p>16 Old datasetsCreate an algorithm for the purpose of improving performance on old datasets. [sent-56, score-0.536]
</p><p>17 After a dataset has been released, algorithms can be made to perform well on the dataset using a process of feedback design, indicating better performance than we might expect in the future. [sent-57, score-0.796]
</p><p>18 Some conferences have canonical datasets that have been used for a decade…Prefer simplicity in algorithm design. [sent-58, score-0.551]
</p><p>19 Making test examples not publicly available for datasets slows the feedback design process but does not eliminate it. [sent-60, score-0.773]
</p><p>20 Be more pessimistic of confidence statements by papers at high rejection rate conferences. [sent-65, score-0.352]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('performance', 0.331), ('datasets', 0.315), ('overfitting', 0.267), ('test', 0.23), ('overrepresenting', 0.17), ('cross', 0.15), ('percent', 0.14), ('brittle', 0.132), ('algorithm', 0.123), ('dataset', 0.121), ('canonical', 0.113), ('human', 0.11), ('validation', 0.11), ('papers', 0.104), ('training', 0.102), ('set', 0.102), ('examples', 0.093), ('confidence', 0.091), ('example', 0.091), ('standard', 0.089), ('methods', 0.088), ('prefer', 0.088), ('rate', 0.087), ('multiclass', 0.087), ('account', 0.083), ('improvement', 0.083), ('old', 0.082), ('measure', 0.08), ('using', 0.078), ('captured', 0.076), ('indicating', 0.076), ('participated', 0.076), ('take', 0.075), ('difficult', 0.072), ('ambiguous', 0.07), ('asymmetric', 0.07), ('bonus', 0.07), ('conjunction', 0.07), ('detect', 0.07), ('doubts', 0.07), ('excessive', 0.07), ('mutual', 0.07), ('pessimistic', 0.07), ('systemic', 0.07), ('feedback', 0.069), ('sure', 0.066), ('available', 0.066), ('comparisons', 0.066), ('report', 0.066), ('representing', 0.066)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999976 <a title="19-tfidf-1" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>Introduction: "Overfitting" is traditionally defined as training some flexible
representation so that it memorizes the data but fails to predict well in the
future. For this post, I will define overfitting more generally as over-
representing the performance of systems. There are two styles of general
overfitting: overrepresenting performance on particular datasets and
(implicitly) overrepresenting performance of a method on future datasets.We
should all be aware of these methods, avoid them where possible, and take them
into account otherwise. I have used "reproblem" and "old datasets", and may
have participated in "overfitting by review"--some of these are very difficult
to avoid.NameMethodExplanationRemedyTraditional overfittingTrain a complex
predictor on too-few examples.Hold out pristine examples for testing.Use a
simpler predictor.Get more training examples.Integrate over many
predictors.Reject papers which do this.Parameter tweak overfittingUse a
learning algorithm with many parameters. Choo</p><p>2 0.24594155 <a title="19-tfidf-2" href="../hunch_net-2005/hunch_net-2005-11-16-The_Everything_Ensemble_Edge.html">131 hunch net-2005-11-16-The Everything Ensemble Edge</a></p>
<p>Introduction: Rich Caruana,Alexandru Niculescu, Geoff Crew, and Alex Ksikes have donea lot
of empirical testingwhich shows thatusing all methods to make a predictionis
more powerful than using any single method. This is in rough agreement with
the Bayesian way of solving problems, but based upon a different (essentially
empirical) motivation. A rough summary is:Take all of {decision trees, boosted
decision trees, bagged decision trees, boosted decision stumps, K nearest
neighbors, neural networks, SVM} with all reasonable parameter settings.Run
the methods on each problem of 8 problems with a large test set, calibrating
margins using eithersigmoid fittingorisotonic regression.For each loss of
{accuracy, area under the ROC curve, cross entropy, squared error, etcâ&euro;Ś}
evaluate the average performance of the method.A series of conclusions can be
drawn from the observations.(Calibrated) boosted decision trees appear to
perform best, in general although support vector machines and neural networks
give cred</p><p>3 0.22125924 <a title="19-tfidf-3" href="../hunch_net-2005/hunch_net-2005-02-21-Problem%3A_Cross_Validation.html">26 hunch net-2005-02-21-Problem: Cross Validation</a></p>
<p>Introduction: The essential problem here is the large gap between experimental observation
and theoretical understanding.MethodK-fold cross validation is a commonly used
technique which takes a set ofmexamples and partitions them intoKsets
("folds") of sizem/K. For each fold, a classifier is trained on the other
folds and then test on the fold.ProblemAssume only independent samples. Derive
a classifier from the K classifiers with a small bound on the true error
rate.Past Work(I'll add more as I remember/learn.)Devroye, Rogers, and Wagner
analyzed cross validation and found algorithm specific bounds. Not all of this
is online, but here is onepaper.Michael KearnsandDana Ronanalyzed cross
validationand found that under additional stability assumptions the bound for
the classifier which learns on all the data is not much worse than for a test
set of sizem/K.Avrim Blum,Adam Kalai, andmyselfanalyzed cross validationand
found that you can do at least as well as a test set of sizem/Kwith no
additional assum</p><p>4 0.16921018 <a title="19-tfidf-4" href="../hunch_net-2007/hunch_net-2007-06-24-Interesting_Papers_at_ICML_2007.html">251 hunch net-2007-06-24-Interesting Papers at ICML 2007</a></p>
<p>Introduction: Here are a few of the papers I enjoyed at ICML.Steffen Bickel, Michael
BrÃƒÂ¼eckner,Tobias Scheffer,Discriminative Learning for Differing Training
and Test DistributionsThere is a nice trick in this paper: they predict the
probability that an unlabeled sample is in the training set vs. the test set,
and then use this prediction to importance weight labeled samples in the
training set. This paper uses a specific parametric model, but the approach is
easily generalized.Steve HannekeA Bound on the Label Complexity of Agnostic
Active LearningThis paper bounds the number of labels required by the
A2algorithm for active learning in the agnostic case. Last year we figured out
agnostic active learning was possible. This year, it's quantified. Hopefull
soon, it will be practical.Sylvian Gelly,David SilverCombining Online and
Offline Knowledge in UCT. This paper is about techniques for improvingMoGowith
various sorts of learning. MoGo has a fair claim at being the world's best Go
algorithm.There</p><p>5 0.16193101 <a title="19-tfidf-5" href="../hunch_net-2005/hunch_net-2005-04-14-Families_of_Learning_Theory_Statements.html">56 hunch net-2005-04-14-Families of Learning Theory Statements</a></p>
<p>Introduction: The diagram above shows a very broad viewpoint of learning theory.arrowTypical
statementExamplesPast->PastSome prediction algorithmAdoes almost as well as
any of a set of algorithms.Weighted MajorityPast->FutureAssuming independent
samples, past performance predicts future performance.PAC analysis, ERM
analysisFuture->FutureFuture prediction performance on subproblems implies
future prediction performance using algorithmA.ECOC, ProbingA basic question
is: Are there other varieties of statements of this type?Avrimnoted that there
are also "arrows between arrows": generic methods for transforming between
Past->Past statements and Past->Future statements. Are there others?</p><p>6 0.1599731 <a title="19-tfidf-6" href="../hunch_net-2009/hunch_net-2009-06-26-Netflix_nearly_done.html">362 hunch net-2009-06-26-Netflix nearly done</a></p>
<p>7 0.15547711 <a title="19-tfidf-7" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>8 0.15092729 <a title="19-tfidf-8" href="../hunch_net-2006/hunch_net-2006-05-05-An_ICML_reject.html">177 hunch net-2006-05-05-An ICML reject</a></p>
<p>9 0.14766495 <a title="19-tfidf-9" href="../hunch_net-2008/hunch_net-2008-04-30-Concerns_about_the_Large_Scale_Learning_Challenge.html">300 hunch net-2008-04-30-Concerns about the Large Scale Learning Challenge</a></p>
<p>10 0.14732179 <a title="19-tfidf-10" href="../hunch_net-2010/hunch_net-2010-04-14-MLcomp%3A_a_website_for_objectively_comparing_ML_algorithms.html">393 hunch net-2010-04-14-MLcomp: a website for objectively comparing ML algorithms</a></p>
<p>11 0.1458905 <a title="19-tfidf-11" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>12 0.14378092 <a title="19-tfidf-12" href="../hunch_net-2005/hunch_net-2005-03-15-The_State_of_Tight_Bounds.html">41 hunch net-2005-03-15-The State of Tight Bounds</a></p>
<p>13 0.13619979 <a title="19-tfidf-13" href="../hunch_net-2006/hunch_net-2006-03-12-Online_learning_or_online_preservation_of_learning%3F.html">163 hunch net-2006-03-12-Online learning or online preservation of learning?</a></p>
<p>14 0.13596697 <a title="19-tfidf-14" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<p>15 0.13431542 <a title="19-tfidf-15" href="../hunch_net-2005/hunch_net-2005-02-12-ROC_vs._Accuracy_vs._AROC.html">18 hunch net-2005-02-12-ROC vs. Accuracy vs. AROC</a></p>
<p>16 0.13236019 <a title="19-tfidf-16" href="../hunch_net-2005/hunch_net-2005-03-21-Research_Styles_in_Machine_Learning.html">44 hunch net-2005-03-21-Research Styles in Machine Learning</a></p>
<p>17 0.12915663 <a title="19-tfidf-17" href="../hunch_net-2007/hunch_net-2007-08-19-Choice_of_Metrics.html">259 hunch net-2007-08-19-Choice of Metrics</a></p>
<p>18 0.12773673 <a title="19-tfidf-18" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>19 0.12628824 <a title="19-tfidf-19" href="../hunch_net-2009/hunch_net-2009-09-21-Netflix_finishes_%28and_starts%29.html">371 hunch net-2009-09-21-Netflix finishes (and starts)</a></p>
<p>20 0.12150776 <a title="19-tfidf-20" href="../hunch_net-2005/hunch_net-2005-11-07-Prediction_Competitions.html">129 hunch net-2005-11-07-Prediction Competitions</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.293), (1, -0.117), (2, -0.097), (3, -0.015), (4, -0.067), (5, -0.007), (6, 0.025), (7, 0.066), (8, 0.112), (9, 0.02), (10, -0.142), (11, -0.079), (12, -0.213), (13, -0.072), (14, 0.017), (15, 0.018), (16, -0.112), (17, -0.039), (18, 0.06), (19, 0.012), (20, 0.104), (21, 0.076), (22, 0.104), (23, -0.028), (24, -0.034), (25, 0.033), (26, 0.059), (27, -0.027), (28, -0.071), (29, -0.084), (30, -0.056), (31, 0.083), (32, -0.05), (33, 0.007), (34, -0.063), (35, 0.14), (36, 0.048), (37, 0.057), (38, 0.075), (39, -0.101), (40, -0.002), (41, 0.059), (42, -0.019), (43, -0.111), (44, -0.021), (45, -0.027), (46, 0.086), (47, 0.011), (48, -0.149), (49, 0.043)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98439527 <a title="19-lsi-1" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>Introduction: "Overfitting" is traditionally defined as training some flexible
representation so that it memorizes the data but fails to predict well in the
future. For this post, I will define overfitting more generally as over-
representing the performance of systems. There are two styles of general
overfitting: overrepresenting performance on particular datasets and
(implicitly) overrepresenting performance of a method on future datasets.We
should all be aware of these methods, avoid them where possible, and take them
into account otherwise. I have used "reproblem" and "old datasets", and may
have participated in "overfitting by review"--some of these are very difficult
to avoid.NameMethodExplanationRemedyTraditional overfittingTrain a complex
predictor on too-few examples.Hold out pristine examples for testing.Use a
simpler predictor.Get more training examples.Integrate over many
predictors.Reject papers which do this.Parameter tweak overfittingUse a
learning algorithm with many parameters. Choo</p><p>2 0.7858963 <a title="19-lsi-2" href="../hunch_net-2005/hunch_net-2005-11-16-The_Everything_Ensemble_Edge.html">131 hunch net-2005-11-16-The Everything Ensemble Edge</a></p>
<p>Introduction: Rich Caruana,Alexandru Niculescu, Geoff Crew, and Alex Ksikes have donea lot
of empirical testingwhich shows thatusing all methods to make a predictionis
more powerful than using any single method. This is in rough agreement with
the Bayesian way of solving problems, but based upon a different (essentially
empirical) motivation. A rough summary is:Take all of {decision trees, boosted
decision trees, bagged decision trees, boosted decision stumps, K nearest
neighbors, neural networks, SVM} with all reasonable parameter settings.Run
the methods on each problem of 8 problems with a large test set, calibrating
margins using eithersigmoid fittingorisotonic regression.For each loss of
{accuracy, area under the ROC curve, cross entropy, squared error, etcâ&euro;Ś}
evaluate the average performance of the method.A series of conclusions can be
drawn from the observations.(Calibrated) boosted decision trees appear to
perform best, in general although support vector machines and neural networks
give cred</p><p>3 0.77928865 <a title="19-lsi-3" href="../hunch_net-2005/hunch_net-2005-02-21-Problem%3A_Cross_Validation.html">26 hunch net-2005-02-21-Problem: Cross Validation</a></p>
<p>Introduction: The essential problem here is the large gap between experimental observation
and theoretical understanding.MethodK-fold cross validation is a commonly used
technique which takes a set ofmexamples and partitions them intoKsets
("folds") of sizem/K. For each fold, a classifier is trained on the other
folds and then test on the fold.ProblemAssume only independent samples. Derive
a classifier from the K classifiers with a small bound on the true error
rate.Past Work(I'll add more as I remember/learn.)Devroye, Rogers, and Wagner
analyzed cross validation and found algorithm specific bounds. Not all of this
is online, but here is onepaper.Michael KearnsandDana Ronanalyzed cross
validationand found that under additional stability assumptions the bound for
the classifier which learns on all the data is not much worse than for a test
set of sizem/K.Avrim Blum,Adam Kalai, andmyselfanalyzed cross validationand
found that you can do at least as well as a test set of sizem/Kwith no
additional assum</p><p>4 0.68464881 <a title="19-lsi-4" href="../hunch_net-2005/hunch_net-2005-02-12-ROC_vs._Accuracy_vs._AROC.html">18 hunch net-2005-02-12-ROC vs. Accuracy vs. AROC</a></p>
<p>Introduction: Foster Provostand I discussed the merits of ROC curves vs. accuracy
estimation. Here is a quick summary of our discussion.The "Receiver Operating
Characteristic" (ROC) curve is an alternative to accuracy for the evaluation
of learning algorithms on natural datasets. The ROC curve is acurveand not a
single number statistic. In particular, this means that the comparison of two
algorithms on a dataset does not always produce an obvious order.Accuracy (= 1
- error rate) is a standard method used to evaluate learning algorithms. It is
a single-number summary of performance.AROC is the area under the ROC curve.
It is a single number summary of performance.The comparison of these metrics
is a subtle affair, because in machine learning, they are compared on
different natural datasets. This makes some sense if we accept the hypothesis
"Performance on past learning problems (roughly) predicts performance on
future learning problems."The ROC vs. accuracy discussion is often conflated
with "is the</p><p>5 0.67724073 <a title="19-lsi-5" href="../hunch_net-2005/hunch_net-2005-04-14-Families_of_Learning_Theory_Statements.html">56 hunch net-2005-04-14-Families of Learning Theory Statements</a></p>
<p>Introduction: The diagram above shows a very broad viewpoint of learning theory.arrowTypical
statementExamplesPast->PastSome prediction algorithmAdoes almost as well as
any of a set of algorithms.Weighted MajorityPast->FutureAssuming independent
samples, past performance predicts future performance.PAC analysis, ERM
analysisFuture->FutureFuture prediction performance on subproblems implies
future prediction performance using algorithmA.ECOC, ProbingA basic question
is: Are there other varieties of statements of this type?Avrimnoted that there
are also "arrows between arrows": generic methods for transforming between
Past->Past statements and Past->Future statements. Are there others?</p><p>6 0.6113615 <a title="19-lsi-6" href="../hunch_net-2009/hunch_net-2009-06-26-Netflix_nearly_done.html">362 hunch net-2009-06-26-Netflix nearly done</a></p>
<p>7 0.60739058 <a title="19-lsi-7" href="../hunch_net-2006/hunch_net-2006-05-05-An_ICML_reject.html">177 hunch net-2006-05-05-An ICML reject</a></p>
<p>8 0.59355873 <a title="19-lsi-8" href="../hunch_net-2010/hunch_net-2010-04-14-MLcomp%3A_a_website_for_objectively_comparing_ML_algorithms.html">393 hunch net-2010-04-14-MLcomp: a website for objectively comparing ML algorithms</a></p>
<p>9 0.56122184 <a title="19-lsi-9" href="../hunch_net-2005/hunch_net-2005-03-18-Binomial_Weighting.html">43 hunch net-2005-03-18-Binomial Weighting</a></p>
<p>10 0.55930233 <a title="19-lsi-10" href="../hunch_net-2008/hunch_net-2008-08-24-Mass_Customized_Medicine_in_the_Future%3F.html">314 hunch net-2008-08-24-Mass Customized Medicine in the Future?</a></p>
<p>11 0.55454749 <a title="19-lsi-11" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<p>12 0.5459649 <a title="19-lsi-12" href="../hunch_net-2005/hunch_net-2005-04-27-DARPA_project%3A_LAGR.html">63 hunch net-2005-04-27-DARPA project: LAGR</a></p>
<p>13 0.54510069 <a title="19-lsi-13" href="../hunch_net-2006/hunch_net-2006-03-12-Online_learning_or_online_preservation_of_learning%3F.html">163 hunch net-2006-03-12-Online learning or online preservation of learning?</a></p>
<p>14 0.537287 <a title="19-lsi-14" href="../hunch_net-2007/hunch_net-2007-04-18-%2450K_Spock_Challenge.html">239 hunch net-2007-04-18-$50K Spock Challenge</a></p>
<p>15 0.5316959 <a title="19-lsi-15" href="../hunch_net-2008/hunch_net-2008-04-30-Concerns_about_the_Large_Scale_Learning_Challenge.html">300 hunch net-2008-04-30-Concerns about the Large Scale Learning Challenge</a></p>
<p>16 0.52042753 <a title="19-lsi-16" href="../hunch_net-2005/hunch_net-2005-03-15-The_State_of_Tight_Bounds.html">41 hunch net-2005-03-15-The State of Tight Bounds</a></p>
<p>17 0.5197019 <a title="19-lsi-17" href="../hunch_net-2005/hunch_net-2005-02-28-Regularization.html">33 hunch net-2005-02-28-Regularization</a></p>
<p>18 0.50035703 <a title="19-lsi-18" href="../hunch_net-2007/hunch_net-2007-09-16-Optimizing_Machine_Learning_Programs.html">262 hunch net-2007-09-16-Optimizing Machine Learning Programs</a></p>
<p>19 0.49851403 <a title="19-lsi-19" href="../hunch_net-2006/hunch_net-2006-03-02-Why_do_people_count_for_learning%3F.html">160 hunch net-2006-03-02-Why do people count for learning?</a></p>
<p>20 0.49651271 <a title="19-lsi-20" href="../hunch_net-2005/hunch_net-2005-05-06-Don%26%238217%3Bt_mix_the_solution_into_the_problem.html">67 hunch net-2005-05-06-Don&#8217;t mix the solution into the problem</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(33, 0.193), (35, 0.05), (42, 0.317), (45, 0.016), (68, 0.076), (69, 0.054), (74, 0.138), (76, 0.042), (82, 0.014)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.91042954 <a title="19-lda-1" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>Introduction: "Overfitting" is traditionally defined as training some flexible
representation so that it memorizes the data but fails to predict well in the
future. For this post, I will define overfitting more generally as over-
representing the performance of systems. There are two styles of general
overfitting: overrepresenting performance on particular datasets and
(implicitly) overrepresenting performance of a method on future datasets.We
should all be aware of these methods, avoid them where possible, and take them
into account otherwise. I have used "reproblem" and "old datasets", and may
have participated in "overfitting by review"--some of these are very difficult
to avoid.NameMethodExplanationRemedyTraditional overfittingTrain a complex
predictor on too-few examples.Hold out pristine examples for testing.Use a
simpler predictor.Get more training examples.Integrate over many
predictors.Reject papers which do this.Parameter tweak overfittingUse a
learning algorithm with many parameters. Choo</p><p>2 0.86267972 <a title="19-lda-2" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>Introduction: I wanted to expand on thispostand some of the previousproblems/research
directionsabout where learning theory might make large strides.Why theory?The
essential reason for theory is "intuition extension". A very good applied
learning person can master some particular application domain yielding the
best computer algorithms for solving that problem. A very good theory can take
the intuitions discovered by this and other applied learning people and extend
them to new domains in a relatively automatic fashion. To do this, we take
these basic intuitions and try to find a mathematical model that:Explains the
basic intuitions.Makes new testable predictions about how to learn.Succeeds in
so learning.This is "intuition extension": taking what we have learned
somewhere else and applying it in new domains. It is fundamentally useful to
everyone because it increases the level of automation in solving
problems.Where next for learning theory?I like the analogy with physics. Back
before we-the-humans</p><p>3 0.86161029 <a title="19-lda-3" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>Introduction: Exploration is one of the big unsolved problems in machine learning. This
isn't for lack of trying--there are many models of exploration which have been
analyzed in many different ways by many different groups of people. At some
point, it is worthwhile to sit back and see what has been done across these
many models.Reinforcement Learning(1). Reinforcement learning has
traditionally focused on Markov Decision Processes where the next states'is
given by a conditional distributionP(s'|s,a)given the current statesand
actiona. The typical result here is that certain specific algorithms
controlling an agent can behave withineof optimal for horizonTexcept
forpoly(1/e,T,S,A)"wasted" experiences (with high probability). This started
withE3bySatinder SinghandMichael Kearns.Sham Kakade's thesishas significant
discussion. Extensions have typically been of the form "under extra
assumptions, we can prove more", for exampleFactored-E3andMetric-E3. (It turns
out that the number of wasted samples can b</p><p>4 0.86042976 <a title="19-lda-4" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>Introduction: Researchers are typically confronted with big problems that they have no idea
how to solve. In trying to come up with a solution, a natural approach is to
decompose the big problem into a set of subproblems whose solution yields a
solution to the larger problem. This approach can go wrong in several
ways.Decomposition failure. The solution to the decomposition does not in fact
yield a solution to the overall problem.Artificial hardness. The subproblems
created are sufficient if solved to solve the overall problem, but they are
harder than necessary.As you can see, computational complexity forms a
relatively new (in research-history) razor by which to judge an approach
sufficient but not necessary.In my experience, the artificial hardness problem
is very common. Many researchers abdicate the responsibility of choosing a
problem to work on to other people. This process starts very naturally as a
graduate student, when an incoming student might have relatively little idea
about how to do</p><p>5 0.85888571 <a title="19-lda-5" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>Introduction: Normally, I don't indulge in posters forICML, but this year is naturally an
exception for me. If you want one, there are a small numberleft here, if you
sign up before February.It also seems worthwhile to give some sense of the
scope and reviewing criteria for ICML for authors considering submitting
papers. At ICML, the (very large) program committee does the reviewing which
informs final decisions by area chairs on most papers. Program chairs setup
the process, deal with exceptions or disagreements, and provide advice for the
reviewing process. Providing advice is tricky (and easily misleading) because
a conference is a community, and in the end the aggregate interests of the
community determine the conference. Nevertheless, as a program chair this year
it seems worthwhile to state the overall philosophy I have and what I plan to
encourage (and occasionally discourage).At the highest level, I believe ICML
exists to further research into machine learning, which I generally think of
as</p><p>6 0.85483348 <a title="19-lda-6" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>7 0.85462254 <a title="19-lda-7" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>8 0.85396153 <a title="19-lda-8" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>9 0.85340071 <a title="19-lda-9" href="../hunch_net-2011/hunch_net-2011-10-24-2011_ML_symposium_and_the_bears.html">448 hunch net-2011-10-24-2011 ML symposium and the bears</a></p>
<p>10 0.85292035 <a title="19-lda-10" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>11 0.85091299 <a title="19-lda-11" href="../hunch_net-2006/hunch_net-2006-03-02-Why_do_people_count_for_learning%3F.html">160 hunch net-2006-03-02-Why do people count for learning?</a></p>
<p>12 0.84983224 <a title="19-lda-12" href="../hunch_net-2005/hunch_net-2005-01-24-The_Humanloop_Spectrum_of_Machine_Learning.html">3 hunch net-2005-01-24-The Humanloop Spectrum of Machine Learning</a></p>
<p>13 0.84963524 <a title="19-lda-13" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<p>14 0.84879404 <a title="19-lda-14" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>15 0.84830302 <a title="19-lda-15" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>16 0.84796995 <a title="19-lda-16" href="../hunch_net-2011/hunch_net-2011-02-17-What_does_Watson_mean%3F.html">424 hunch net-2011-02-17-What does Watson mean?</a></p>
<p>17 0.84578156 <a title="19-lda-17" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<p>18 0.84569615 <a title="19-lda-18" href="../hunch_net-2006/hunch_net-2006-05-05-An_ICML_reject.html">177 hunch net-2006-05-05-An ICML reject</a></p>
<p>19 0.84504008 <a title="19-lda-19" href="../hunch_net-2011/hunch_net-2011-11-26-Giving_Thanks.html">449 hunch net-2011-11-26-Giving Thanks</a></p>
<p>20 0.84481531 <a title="19-lda-20" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
