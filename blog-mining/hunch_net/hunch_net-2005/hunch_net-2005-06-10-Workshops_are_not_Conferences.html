<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>80 hunch net-2005-06-10-Workshops are not Conferences</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-80" href="#">hunch_net-2005-80</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>80 hunch net-2005-06-10-Workshops are not Conferences</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-80-html" href="http://hunch.net/?p=86">html</a></p><p>Introduction: â&euro;Ś and you should use that fact.A workshop differs from a conference in that it
is about a focused group of people worrying about a focused topic. It also
differs in that a workshop is typically a "one-time affair" rather than a
series. (TheSnowbird learning workshopcounts as a conference in this
respect.)A common failure mode of both organizers and speakers at a workshop
is to treat it as a conference. This is "ok", but it is not really taking
advantage of the situation. Here are some things I've learned:For speakers: A
smaller audience means it can be more interactive. Interactive means a better
chance to avoid losing your audience and a more interesting presentation
(because you can adapt to your audience). Greater focus amongst the
participants means you can get to the heart of the matter more easily, and
discuss tradeoffs more carefully. Unlike conferences, relevance is more valued
than newness.For organizers: Not everything needs to be in a conference style
presentation format (i.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 A workshop differs from a conference in that it is about a focused group of people worrying about a focused topic. [sent-2, score-1.281]
</p><p>2 It also differs in that a workshop is typically a "one-time affair" rather than a series. [sent-3, score-0.476]
</p><p>3 (TheSnowbird learning workshopcounts as a conference in this respect. [sent-4, score-0.17]
</p><p>4 )A common failure mode of both organizers and speakers at a workshop is to treat it as a conference. [sent-5, score-0.989]
</p><p>5 This is "ok", but it is not really taking advantage of the situation. [sent-6, score-0.167]
</p><p>6 Here are some things I've learned:For speakers: A smaller audience means it can be more interactive. [sent-7, score-0.598]
</p><p>7 Interactive means a better chance to avoid losing your audience and a more interesting presentation (because you can adapt to your audience). [sent-8, score-1.095]
</p><p>8 Greater focus amongst the participants means you can get to the heart of the matter more easily, and discuss tradeoffs more carefully. [sent-9, score-0.905]
</p><p>9 For organizers: Not everything needs to be in a conference style presentation format (i. [sent-11, score-0.703]
</p><p>10 Significant (and variable) question time, different talk durations, flexible rescheduling, and panel discussions can all work well. [sent-14, score-0.463]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('audience', 0.328), ('differs', 0.25), ('workshop', 0.226), ('speakers', 0.214), ('focused', 0.202), ('organizers', 0.202), ('presentation', 0.183), ('means', 0.176), ('conference', 0.17), ('panel', 0.162), ('thesnowbird', 0.15), ('duration', 0.15), ('treat', 0.15), ('valued', 0.15), ('worrying', 0.142), ('affair', 0.142), ('heart', 0.142), ('regularly', 0.142), ('tradeoffs', 0.142), ('relevance', 0.135), ('minute', 0.135), ('adapt', 0.135), ('losing', 0.125), ('flexible', 0.121), ('mode', 0.112), ('interactive', 0.109), ('discussions', 0.107), ('ok', 0.107), ('variable', 0.105), ('unlike', 0.101), ('format', 0.097), ('discuss', 0.096), ('smaller', 0.094), ('participants', 0.094), ('greater', 0.093), ('focus', 0.089), ('group', 0.089), ('needs', 0.087), ('everything', 0.087), ('talks', 0.087), ('matter', 0.085), ('failure', 0.085), ('taking', 0.085), ('advantage', 0.082), ('amongst', 0.081), ('style', 0.079), ('chance', 0.075), ('avoid', 0.073), ('learned', 0.073), ('talk', 0.073)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999982 <a title="80-tfidf-1" href="../hunch_net-2005/hunch_net-2005-06-10-Workshops_are_not_Conferences.html">80 hunch net-2005-06-10-Workshops are not Conferences</a></p>
<p>Introduction: â&euro;Ś and you should use that fact.A workshop differs from a conference in that it
is about a focused group of people worrying about a focused topic. It also
differs in that a workshop is typically a "one-time affair" rather than a
series. (TheSnowbird learning workshopcounts as a conference in this
respect.)A common failure mode of both organizers and speakers at a workshop
is to treat it as a conference. This is "ok", but it is not really taking
advantage of the situation. Here are some things I've learned:For speakers: A
smaller audience means it can be more interactive. Interactive means a better
chance to avoid losing your audience and a more interesting presentation
(because you can adapt to your audience). Greater focus amongst the
participants means you can get to the heart of the matter more easily, and
discuss tradeoffs more carefully. Unlike conferences, relevance is more valued
than newness.For organizers: Not everything needs to be in a conference style
presentation format (i.</p><p>2 0.25171494 <a title="80-tfidf-2" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>Introduction: Founding a successful new conference is extraordinarily difficult. As a
conference founder, you must manage to attract a significant number of good
papers--enough to entice the participants into participating next year and to
(generally) to grow the conference. For someone choosing to participate in a
new conference, there is a very significant decision to make: do you send a
paper to some new conference with no guarantee that the conference will work
out? Or do you send it to another (possibly less related) conference that you
are sure will work?The conference founding problem is a joint agreement
problem with a very significant barrier. Workshops are a way around this
problem, and workshops attached to conferences are a particularly effective
means for this. A workshop at a conference is sure to have people available to
speak and attend and is sure to have a large audience available. Presenting
work at a workshop is not generally exclusive: it can also be presented at a
conference. F</p><p>3 0.18036108 <a title="80-tfidf-3" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>Introduction: The 2006 Machine Learning Summer School in Taipei, Taiwan ended on August 4,
2006. It has been a very exciting two weeks for a record crowd of 245
participants (including speakers and organizers) from 18 countries. We had a
lineup of speakers that is hard to match up for other similar events (see
ourWIKIfor more information). With this lineup, it is difficult for us as
organizers to screw it up too bad. Also, since we have pretty good
infrastructure for international meetings and experienced staff at NTUST and
Academia Sinica, plus the reputation established by previous MLSS series, it
was relatively easy for us to attract registrations and simply enjoyed this
two-week long party of machine learning.In the end of MLSS we distributed a
survey form for participants to fill in. I will report what we found from this
survey, together with the registration data and word-of-mouth from
participants.The first question is designed to find out how our participants
learned about MLSS 2006 Taipei.</p><p>4 0.17263274 <a title="80-tfidf-4" href="../hunch_net-2007/hunch_net-2007-02-22-Create_Your_Own_ICML_Workshop.html">234 hunch net-2007-02-22-Create Your Own ICML Workshop</a></p>
<p>Introduction: As usualICML 2007will be hosting aworkshop programto be held this year on June
24th. The success of the program depends on having researchers like you
propose interesting workshop topics and then organize the workshops. I'd like
to encourage all of you to consider sending a workshop proposal. The proposal
deadline has been extended to March 5. See the workshop web-site for
details.Organizing a workshop is a unique way to gather an international group
of researchers together to focus for an entire day on a topic of your
choosing. I've always found that the cost of organizing a workshop is not so
large, and very low compared to the benefits. The topic and format of a
workshop are limited only by your imagination (and the attractiveness to
potential participants) and need not follow the usual model of a mini-
conference on a particular ML sub-area. Hope to see some interesting proposals
rolling in.</p><p>5 0.16943671 <a title="80-tfidf-5" href="../hunch_net-2008/hunch_net-2008-07-04-More_Presentation_Preparation.html">307 hunch net-2008-07-04-More Presentation Preparation</a></p>
<p>Introduction: We've discussedpresentation preparation before, but I have one more thing to
add:transitioning. For a research presentation, it is substantially helpful
for the audience if transitions are clear. A common outline for a research
presentation in machine leanring is:The problem. Presentations which don't
describe the problem almost immediately lose people, because the context is
missing to understand the detail.Prior relevant work. In many cases, a paper
builds on some previous bit of work which must be understood in order to
understand what the paper does. A common failure mode seems to be spending too
much time on prior work. Discuss just the relevant aspects of prior work in
the language of your work. Sometimes this is missing when unneeded.What we
did. For theory papers in particular, it is often not possible to really cover
the details. Prioritizing what you present can be very important.How it
worked. Many papers in Machine Learning have some sort of experimental test of
the algorit</p><p>6 0.14402492 <a title="80-tfidf-6" href="../hunch_net-2005/hunch_net-2005-03-24-The_Role_of_Workshops.html">46 hunch net-2005-03-24-The Role of Workshops</a></p>
<p>7 0.14312309 <a title="80-tfidf-7" href="../hunch_net-2005/hunch_net-2005-05-28-Running_A_Machine_Learning_Summer_School.html">75 hunch net-2005-05-28-Running A Machine Learning Summer School</a></p>
<p>8 0.1426129 <a title="80-tfidf-8" href="../hunch_net-2005/hunch_net-2005-07-13-%26%238220%3BSister_Conference%26%238221%3B_presentations.html">93 hunch net-2005-07-13-&#8220;Sister Conference&#8221; presentations</a></p>
<p>9 0.13893588 <a title="80-tfidf-9" href="../hunch_net-2007/hunch_net-2007-06-21-Presentation_Preparation.html">249 hunch net-2007-06-21-Presentation Preparation</a></p>
<p>10 0.11962794 <a title="80-tfidf-10" href="../hunch_net-2005/hunch_net-2005-09-19-NIPS_Workshops.html">113 hunch net-2005-09-19-NIPS Workshops</a></p>
<p>11 0.11133704 <a title="80-tfidf-11" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>12 0.10473092 <a title="80-tfidf-12" href="../hunch_net-2009/hunch_net-2009-11-09-NYAS_ML_Symposium_this_year..html">377 hunch net-2009-11-09-NYAS ML Symposium this year.</a></p>
<p>13 0.10109808 <a title="80-tfidf-13" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>14 0.095067628 <a title="80-tfidf-14" href="../hunch_net-2010/hunch_net-2010-05-20-Google_Predict.html">399 hunch net-2010-05-20-Google Predict</a></p>
<p>15 0.08849287 <a title="80-tfidf-15" href="../hunch_net-2013/hunch_net-2013-05-04-COLT_and_ICML_registration.html">482 hunch net-2013-05-04-COLT and ICML registration</a></p>
<p>16 0.087593019 <a title="80-tfidf-16" href="../hunch_net-2006/hunch_net-2006-07-25-Upcoming_conference.html">198 hunch net-2006-07-25-Upcoming conference</a></p>
<p>17 0.087546855 <a title="80-tfidf-17" href="../hunch_net-2005/hunch_net-2005-04-06-Structured_Regret_Minimization.html">53 hunch net-2005-04-06-Structured Regret Minimization</a></p>
<p>18 0.086393625 <a title="80-tfidf-18" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>19 0.085393049 <a title="80-tfidf-19" href="../hunch_net-2007/hunch_net-2007-09-30-NIPS_workshops_are_out..html">264 hunch net-2007-09-30-NIPS workshops are out.</a></p>
<p>20 0.083005309 <a title="80-tfidf-20" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.148), (1, 0.108), (2, 0.09), (3, 0.161), (4, 0.026), (5, -0.055), (6, -0.056), (7, 0.01), (8, -0.127), (9, 0.147), (10, -0.107), (11, 0.096), (12, -0.008), (13, 0.066), (14, -0.159), (15, 0.097), (16, -0.068), (17, 0.174), (18, -0.021), (19, -0.0), (20, -0.035), (21, 0.028), (22, 0.061), (23, 0.104), (24, -0.058), (25, 0.014), (26, -0.086), (27, -0.104), (28, 0.076), (29, -0.118), (30, 0.06), (31, -0.048), (32, -0.067), (33, 0.039), (34, -0.042), (35, -0.037), (36, 0.061), (37, 0.06), (38, -0.048), (39, -0.032), (40, -0.037), (41, -0.028), (42, 0.028), (43, -0.122), (44, 0.113), (45, 0.04), (46, -0.081), (47, -0.03), (48, 0.07), (49, -0.052)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98894417 <a title="80-lsi-1" href="../hunch_net-2005/hunch_net-2005-06-10-Workshops_are_not_Conferences.html">80 hunch net-2005-06-10-Workshops are not Conferences</a></p>
<p>Introduction: â&euro;Ś and you should use that fact.A workshop differs from a conference in that it
is about a focused group of people worrying about a focused topic. It also
differs in that a workshop is typically a "one-time affair" rather than a
series. (TheSnowbird learning workshopcounts as a conference in this
respect.)A common failure mode of both organizers and speakers at a workshop
is to treat it as a conference. This is "ok", but it is not really taking
advantage of the situation. Here are some things I've learned:For speakers: A
smaller audience means it can be more interactive. Interactive means a better
chance to avoid losing your audience and a more interesting presentation
(because you can adapt to your audience). Greater focus amongst the
participants means you can get to the heart of the matter more easily, and
discuss tradeoffs more carefully. Unlike conferences, relevance is more valued
than newness.For organizers: Not everything needs to be in a conference style
presentation format (i.</p><p>2 0.70970654 <a title="80-lsi-2" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>Introduction: Founding a successful new conference is extraordinarily difficult. As a
conference founder, you must manage to attract a significant number of good
papers--enough to entice the participants into participating next year and to
(generally) to grow the conference. For someone choosing to participate in a
new conference, there is a very significant decision to make: do you send a
paper to some new conference with no guarantee that the conference will work
out? Or do you send it to another (possibly less related) conference that you
are sure will work?The conference founding problem is a joint agreement
problem with a very significant barrier. Workshops are a way around this
problem, and workshops attached to conferences are a particularly effective
means for this. A workshop at a conference is sure to have people available to
speak and attend and is sure to have a large audience available. Presenting
work at a workshop is not generally exclusive: it can also be presented at a
conference. F</p><p>3 0.6761784 <a title="80-lsi-3" href="../hunch_net-2009/hunch_net-2009-11-09-NYAS_ML_Symposium_this_year..html">377 hunch net-2009-11-09-NYAS ML Symposium this year.</a></p>
<p>Introduction: TheNYAS ML symposiumgrew again this year to 170 participants, despite the need
to outsmart or otherwise tunnel througha crowd.Perhaps the most distinct talk
was by Bob Bell on various aspects of theNetflix prizecompetition. I also
enjoyed several student posters includingMatt Hoffman's cool examples of blind
source separation for music.I'm somewhat surprised how much the workshop has
grown, as it is now comparable in size to a small conference, although in
style more similar to a workshop. At some point as an event grows, it becomes
owned by the community rather than the organizers, so if anyone has
suggestions on improving it, speak up and be heard.</p><p>4 0.61944723 <a title="80-lsi-4" href="../hunch_net-2005/hunch_net-2005-07-13-%26%238220%3BSister_Conference%26%238221%3B_presentations.html">93 hunch net-2005-07-13-&#8220;Sister Conference&#8221; presentations</a></p>
<p>Introduction: Some of the "sister conference" presentations atAAAIhave been great. Roughly
speaking, the conference organizers asked other conference organizers to come
give a summary of their conference. Many different AI-related conferences
accepted. The presenters typically discuss some of the background and goals of
the conference then mention the results from a few papers they liked. This is
great because it provides a mechanism to get a digested overview of the work
of several thousand researchers--something which is simply available nowhere
else.Based on these presentations, it looks like there is a significant
component of (and opportunity for) applied machine learning inAIIDE,IUI,
andACL.There was also some discussion of having a super-colocation event
similar toFCRC, but centered on AI & Learning. This seems like a fine idea.
The field is fractured across so many different conferences that the mixing of
a supercolocation seems likely helpful for research.</p><p>5 0.60447615 <a title="80-lsi-5" href="../hunch_net-2007/hunch_net-2007-06-21-Presentation_Preparation.html">249 hunch net-2007-06-21-Presentation Preparation</a></p>
<p>Introduction: A big part of doing research is presenting it at a conference. Since many
people start out shy of public presentations, this can be a substantial
challenge. Here are a few notes which might be helpful when thinking about
preparing a presentation on research.Motivate. Talks which don't start by
describing the problem to solve cause many people to zone out.Prioritize. It
is typical that you have more things to say than time to say them, and many
presenters fall into the failure mode of trying to say too much. This is an
easy-to-understand failure mode as it's very natural to want to include
everything. A basic fact is: you can't. Example of this are:Your slides are so
densely full of equations and words that you can't cover them.Your talk runs
over and a moderator prioritizes for you by cutting you off.You motor-mouth
through the presentation, and the information absorption rate of the audience
prioritizes in some uncontrolled fashion.The rate of flow of concepts simply
exceeds the infor</p><p>6 0.56808174 <a title="80-lsi-6" href="../hunch_net-2007/hunch_net-2007-02-22-Create_Your_Own_ICML_Workshop.html">234 hunch net-2007-02-22-Create Your Own ICML Workshop</a></p>
<p>7 0.56761014 <a title="80-lsi-7" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>8 0.5142768 <a title="80-lsi-8" href="../hunch_net-2006/hunch_net-2006-01-06-MLTV.html">146 hunch net-2006-01-06-MLTV</a></p>
<p>9 0.5106771 <a title="80-lsi-9" href="../hunch_net-2006/hunch_net-2006-07-25-Upcoming_conference.html">198 hunch net-2006-07-25-Upcoming conference</a></p>
<p>10 0.51034367 <a title="80-lsi-10" href="../hunch_net-2010/hunch_net-2010-10-29-To_Vidoelecture_or_not.html">416 hunch net-2010-10-29-To Vidoelecture or not</a></p>
<p>11 0.48959523 <a title="80-lsi-11" href="../hunch_net-2008/hunch_net-2008-07-04-More_Presentation_Preparation.html">307 hunch net-2008-07-04-More Presentation Preparation</a></p>
<p>12 0.46622467 <a title="80-lsi-12" href="../hunch_net-2005/hunch_net-2005-03-24-The_Role_of_Workshops.html">46 hunch net-2005-03-24-The Role of Workshops</a></p>
<p>13 0.44514447 <a title="80-lsi-13" href="../hunch_net-2005/hunch_net-2005-09-20-Workshop_Proposal%3A_Atomic_Learning.html">114 hunch net-2005-09-20-Workshop Proposal: Atomic Learning</a></p>
<p>14 0.44082671 <a title="80-lsi-14" href="../hunch_net-2005/hunch_net-2005-09-19-NIPS_Workshops.html">113 hunch net-2005-09-19-NIPS Workshops</a></p>
<p>15 0.43922397 <a title="80-lsi-15" href="../hunch_net-2005/hunch_net-2005-05-28-Running_A_Machine_Learning_Summer_School.html">75 hunch net-2005-05-28-Running A Machine Learning Summer School</a></p>
<p>16 0.42824361 <a title="80-lsi-16" href="../hunch_net-2007/hunch_net-2007-02-11-24.html">232 hunch net-2007-02-11-24</a></p>
<p>17 0.41256949 <a title="80-lsi-17" href="../hunch_net-2007/hunch_net-2007-10-14-NIPS_workshp%3A_Learning_Problem_Design.html">265 hunch net-2007-10-14-NIPS workshp: Learning Problem Design</a></p>
<p>18 0.39921162 <a title="80-lsi-18" href="../hunch_net-2005/hunch_net-2005-07-10-Thinking_the_Unthought.html">91 hunch net-2005-07-10-Thinking the Unthought</a></p>
<p>19 0.39256755 <a title="80-lsi-19" href="../hunch_net-2007/hunch_net-2007-12-12-Workshop_Summary%26%238212%3BPrinciples_of_Learning_Problem_Design.html">277 hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</a></p>
<p>20 0.38312742 <a title="80-lsi-20" href="../hunch_net-2005/hunch_net-2005-07-01-The_Role_of_Impromptu_Talks.html">88 hunch net-2005-07-01-The Role of Impromptu Talks</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(42, 0.184), (68, 0.544), (74, 0.031), (95, 0.125)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95961487 <a title="80-lda-1" href="../hunch_net-2005/hunch_net-2005-06-10-Workshops_are_not_Conferences.html">80 hunch net-2005-06-10-Workshops are not Conferences</a></p>
<p>Introduction: â&euro;Ś and you should use that fact.A workshop differs from a conference in that it
is about a focused group of people worrying about a focused topic. It also
differs in that a workshop is typically a "one-time affair" rather than a
series. (TheSnowbird learning workshopcounts as a conference in this
respect.)A common failure mode of both organizers and speakers at a workshop
is to treat it as a conference. This is "ok", but it is not really taking
advantage of the situation. Here are some things I've learned:For speakers: A
smaller audience means it can be more interactive. Interactive means a better
chance to avoid losing your audience and a more interesting presentation
(because you can adapt to your audience). Greater focus amongst the
participants means you can get to the heart of the matter more easily, and
discuss tradeoffs more carefully. Unlike conferences, relevance is more valued
than newness.For organizers: Not everything needs to be in a conference style
presentation format (i.</p><p>2 0.94560528 <a title="80-lda-2" href="../hunch_net-2005/hunch_net-2005-02-09-Intuitions_from_applied_learning.html">16 hunch net-2005-02-09-Intuitions from applied learning</a></p>
<p>Introduction: Since learning is far from an exact science, it's good to pay attention to
basic intuitions of applied learning. Here are a few I've
collected.IntegrationIn Bayesian learning, the posterior is computed by an
integral, and the optimal thing to do is to predict according to this
integral. This phenomena seems to be far more general. Bagging, Boosting,
SVMs, and Neural Networks all take advantage of this idea to some extent. The
phenomena is more general: you can average over many differentclassification
predictorsto improve performance.
Sources:Zoubin,CaruanaDifferentiationDifferent pieces of an average should
differentiate to achieve good performance by different methods. This is know
as the 'symmetry breaking' problem for neural networks, and it's why weights
are initialized randomly. Boosting explicitly attempts to achieve good
differentiation by creating new, different, learning problems. Sources:Yann
LeCun,Phil LongDeep RepresentationHaving a deep representation is necessary
for hav</p><p>3 0.93453109 <a title="80-lda-3" href="../hunch_net-2012/hunch_net-2012-03-13-The_Submodularity_workshop_and_Lucca_Professorship.html">459 hunch net-2012-03-13-The Submodularity workshop and Lucca Professorship</a></p>
<p>Introduction: Ninapoints out theSubmodularity WorkshopMarch 19-20next week atGeorgia Tech.
Many people want to make Submodularity the new Convexity in machine learning,
and it certainly seems worth exploring.Sara Olsonalso points out atenured
faculty positionatIMT Luccawith a deadline ofMay 15th. Lucca happens to be the
ancestral home of 1/4 of my heritage</p><p>4 0.89878964 <a title="80-lda-4" href="../hunch_net-2009/hunch_net-2009-02-04-Optimal_Proxy_Loss_for_Classification.html">341 hunch net-2009-02-04-Optimal Proxy Loss for Classification</a></p>
<p>Introduction: Many people in machine learning take advantage of the notion of a proxy loss:
A loss function which is much easier to optimize computationally than the loss
function imposed by the world. A canonical example is when we want to learn a
weight vectorwand predict according to a dot productfw(x)= sumiwixiwhere
optimizing squared loss(y-fw(x))2over many samples is much more tractable than
optimizing 0-1 lossI(y = Threshold(fw(x) - 0.5)).While the computational
advantages of optimizing a proxy loss are substantial, we are curious: which
proxy loss is best? The answer of course depends on what the real loss imposed
by the world is. For 0-1 loss classification, there are adherents to many
choices:Log loss. If we confine the prediction to[0,1], we can treat it as a
predicted probability that the label is1, and measure loss according tolog
1/p'(y|x)wherep'(y|x)is the predicted probability of the observed label. A
standard method for confining the prediction to[0,1]islogistic regressionwhich
expo</p><p>5 0.89499593 <a title="80-lda-5" href="../hunch_net-2006/hunch_net-2006-07-08-Some_recent_papers.html">192 hunch net-2006-07-08-Some recent papers</a></p>
<p>Introduction: It was a fine time for learning in Pittsburgh. John and Sam mentioned some of
my favorites. Here's a few more worth checking out:Online Multitask
LearningOfer Dekel, Phil Long, Yoram SingerThis is on my reading list.
Definitely an area I'm interested in.Maximum Entropy Distribution Estimation
with Generalized RegularizationMiroslav DudÃƒÂ­k, Robert E. SchapireLearning
near-optimal policies with Bellman-residual minimization based fitted policy
iteration and a single sample pathAndrÃƒÂ¡s Antos, Csaba SzepesvÃƒÂ¡ri,
RÃƒÂ©mi MunosAgain, on the list to read. I saw Csaba and Remi talk about this
and related work at an ICML Workshop on Kernel Reinforcement Learning. The big
question in my head is how this compares/contrasts with existing work
inreductions to reinforcement learning.Are there
advantages/disadvantages?Higher Order Learning On Graphs>by Sameer Agarwal,
Kristin Branson, and Serge Belongie, looks to be interesteding. They seem to
poo-poo "tensorization" of existing graph algorithm</p><p>6 0.87592244 <a title="80-lda-6" href="../hunch_net-2009/hunch_net-2009-01-21-Nearly_all_natural_problems_require_nonlinearity.html">337 hunch net-2009-01-21-Nearly all natural problems require nonlinearity</a></p>
<p>7 0.85041797 <a title="80-lda-7" href="../hunch_net-2012/hunch_net-2012-03-24-David_Waltz.html">460 hunch net-2012-03-24-David Waltz</a></p>
<p>8 0.80026531 <a title="80-lda-8" href="../hunch_net-2008/hunch_net-2008-04-12-It_Doesn%26%238217%3Bt_Stop.html">295 hunch net-2008-04-12-It Doesn&#8217;t Stop</a></p>
<p>9 0.77151775 <a title="80-lda-9" href="../hunch_net-2005/hunch_net-2005-08-22-Do_you_believe_in_induction%3F.html">104 hunch net-2005-08-22-Do you believe in induction?</a></p>
<p>10 0.73444909 <a title="80-lda-10" href="../hunch_net-2007/hunch_net-2007-05-12-Loss_Function_Semantics.html">245 hunch net-2007-05-12-Loss Function Semantics</a></p>
<p>11 0.67533678 <a title="80-lda-11" href="../hunch_net-2005/hunch_net-2005-09-10-%26%238220%3BFailure%26%238221%3B_is_an_option.html">110 hunch net-2005-09-10-&#8220;Failure&#8221; is an option</a></p>
<p>12 0.67377424 <a title="80-lda-12" href="../hunch_net-2011/hunch_net-2011-07-11-Interesting_Neural_Network_Papers_at_ICML_2011.html">438 hunch net-2011-07-11-Interesting Neural Network Papers at ICML 2011</a></p>
<p>13 0.65492952 <a title="80-lda-13" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>14 0.65016037 <a title="80-lda-14" href="../hunch_net-2007/hunch_net-2007-08-12-Exponentiated_Gradient.html">258 hunch net-2007-08-12-Exponentiated Gradient</a></p>
<p>15 0.64299512 <a title="80-lda-15" href="../hunch_net-2005/hunch_net-2005-11-07-Prediction_Competitions.html">129 hunch net-2005-11-07-Prediction Competitions</a></p>
<p>16 0.6286931 <a title="80-lda-16" href="../hunch_net-2007/hunch_net-2007-11-28-Computational_Consequences_of_Classification.html">274 hunch net-2007-11-28-Computational Consequences of Classification</a></p>
<p>17 0.61878121 <a title="80-lda-17" href="../hunch_net-2005/hunch_net-2005-02-01-Watchword%3A_Loss.html">9 hunch net-2005-02-01-Watchword: Loss</a></p>
<p>18 0.60243356 <a title="80-lda-18" href="../hunch_net-2005/hunch_net-2005-06-08-Question%3A_%26%238220%3BWhen_is_the_right_time_to_insert_the_loss_function%3F%26%238221%3B.html">79 hunch net-2005-06-08-Question: &#8220;When is the right time to insert the loss function?&#8221;</a></p>
<p>19 0.57804495 <a title="80-lda-19" href="../hunch_net-2009/hunch_net-2009-10-10-ALT_2009.html">374 hunch net-2009-10-10-ALT 2009</a></p>
<p>20 0.55158663 <a title="80-lda-20" href="../hunch_net-2008/hunch_net-2008-09-12-How_do_we_get_weak_action_dependence_for_learning_with_partial_observations%3F.html">317 hunch net-2008-09-12-How do we get weak action dependence for learning with partial observations?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
