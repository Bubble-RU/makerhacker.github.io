<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>102 hunch net-2005-08-11-Why Manifold-Based Dimension Reduction Techniques?</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-102" href="#">hunch_net-2005-102</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>102 hunch net-2005-08-11-Why Manifold-Based Dimension Reduction Techniques?</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-102-html" href="http://hunch.net/?p=109">html</a></p><p>Introduction: Manifold based dimension-reduction algorithms share the following general
outline.Given: a metricd()and a set of pointsSConstruct a graph with a point
in every node and every edge connecting to the node of one of thek-nearest
neighbors. Associate with the edge a weight which is the distance between the
points in the connected nodes.Digest the graph. This might include computing
the shortest path between all points or figuring out how to linearly
interpolate the point from it's neighbors.Find a set of points in a low
dimensional space which preserve the digested properties.Examples include LLE,
Isomap (which I worked on), Hessian-LLE, SDE, and many others. The hope with
these algorithms is that they can recover the low dimensional structure of
point sets in high dimensional spaces. Many of them can be shown to work in
interesting ways producing various compelling pictures.Despite doing some
early work in this direction, I suffer from a motivational problem: Why do we
want to recover the</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('dimensional', 0.335), ('low', 0.233), ('manifold', 0.202), ('actuator', 0.182), ('distance', 0.167), ('shortest', 0.162), ('approximation', 0.154), ('visualization', 0.15), ('transitions', 0.15), ('space', 0.148), ('projection', 0.141), ('compelling', 0.137), ('nearby', 0.135), ('path', 0.129), ('node', 0.129), ('recover', 0.129), ('robot', 0.129), ('position', 0.125), ('dimension', 0.125), ('binary', 0.125)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000002 <a title="102-tfidf-1" href="../hunch_net-2005/hunch_net-2005-08-11-Why_Manifold-Based_Dimension_Reduction_Techniques%3F.html">102 hunch net-2005-08-11-Why Manifold-Based Dimension Reduction Techniques?</a></p>
<p>Introduction: Manifold based dimension-reduction algorithms share the following general
outline.Given: a metricd()and a set of pointsSConstruct a graph with a point
in every node and every edge connecting to the node of one of thek-nearest
neighbors. Associate with the edge a weight which is the distance between the
points in the connected nodes.Digest the graph. This might include computing
the shortest path between all points or figuring out how to linearly
interpolate the point from it's neighbors.Find a set of points in a low
dimensional space which preserve the digested properties.Examples include LLE,
Isomap (which I worked on), Hessian-LLE, SDE, and many others. The hope with
these algorithms is that they can recover the low dimensional structure of
point sets in high dimensional spaces. Many of them can be shown to work in
interesting ways producing various compelling pictures.Despite doing some
early work in this direction, I suffer from a motivational problem: Why do we
want to recover the</p><p>2 0.16653711 <a title="102-tfidf-2" href="../hunch_net-2005/hunch_net-2005-06-29-Not_EM_for_clustering_at_COLT.html">87 hunch net-2005-06-29-Not EM for clustering at COLT</a></p>
<p>Introduction: One standard approach for clustering data with a set of gaussians is using EM.
Roughly speaking, you pick a set ofkrandom guassians and then use alternating
expectation maximization to (hopefully) find a set of guassians that "explain"
the data well. This process is difficult to work with because EM can become
"stuck" in local optima. There are various hacks like "rerun withtdifferent
random starting points".One cool observation is that this can often be solved
via other algorithm which donotsuffer from local optima. This is an
earlypaperwhich shows this. Ravi Kannan presented anew papershowing this is
possible in a much more adaptive setting.A very rough summary of these papers
is that by projecting into a lower dimensional space, it is computationally
tractable to pick out the gross structure of the data. It is unclear how well
these algorithms work in practice, but they might be effective, especially if
used as a subroutine of the form:Project to low dimensional space.Pick out
gross</p><p>3 0.14876108 <a title="102-tfidf-3" href="../hunch_net-2005/hunch_net-2005-04-25-Embeddings%3A_what_are_they_good_for%3F.html">61 hunch net-2005-04-25-Embeddings: what are they good for?</a></p>
<p>Introduction: I've been looking at some recent embeddings work, and am struck by how
beautiful the theory and algorithms are. It also makes me wonder, what are
embeddings good for?A few things immediately come to mind:(1) For
visualization of high-dimensional data sets.In this case, one would like good
algorithms for embedding specifically into 2- and 3-dimensional Euclidean
spaces.(2) For nonparametric modeling.The usual nonparametric models
(histograms, nearest neighbor) often require resources which are exponential
in the dimension. So if the data actually lie close to some low-
dimensionalsurface, it might be a good idea to first identify this surface and
embed the data before applying the model.Incidentally, for applications like
these, it's important to have a functional mapping from high to low dimension,
which some techniques do not yield up.(3) As a prelude to classifier
learning.The hope here is presumably that learning will be easier in the low-
dimensional space, because of (i) better ge</p><p>4 0.14756373 <a title="102-tfidf-4" href="../hunch_net-2005/hunch_net-2005-04-21-Dynamic_Programming_Generalizations_and_Their_Use.html">58 hunch net-2005-04-21-Dynamic Programming Generalizations and Their Use</a></p>
<p>Introduction: David Mcallestergave a talk about thispaper(withPedro Felzenszwalb). I'll try
to give a high level summary of why it's interesting.Dynamic programming is
most familiar as instantiated by Viterbi decoding in a hidden markov model. It
is a general paradigm for problem solving where subproblems are solved and
used to solve larger problems. In the Viterbi decoding example, the subproblem
is "What is the most probable path ending at each state at timestept?", and
the larger problem is the same except at timestept+1. There are a few
optimizations you can do here:Dynamic Programming -> queued Dynamic
Programming. Keep track of the "cost so far" (or "most probable path") and
(carefully) only look at extensions to paths likely to yield the shortest
path. "Carefully" here is defined byDijkstra's shortest path algorithm.queued
Dynamic programming -> A*Add a lower bound on the cost to complete a path (or
an upper bound on the probability of a completion) for the priority queue of
Dijkstra's shorte</p><p>5 0.11750237 <a title="102-tfidf-5" href="../hunch_net-2006/hunch_net-2006-02-02-Introspectionism_as_a_Disease.html">153 hunch net-2006-02-02-Introspectionism as a Disease</a></p>
<p>Introduction: In the AI-related parts of machine learning, it is often tempting to examine
howyoudo things in order to imagine how a machine should do things. This is
introspection, and it can easily go awry. I will call introspection gone awry
introspectionism.Introspectionism is almost unique to AI (and the AI-related
parts of machine learning) and it can lead to huge wasted effort in research.
It's easiest to show how introspectionism arises by an example.Suppose we want
to solve the problem of navigating a robot from point A to point B given a
camera. Then, the following research action plan might seem natural when you
examine your own capabilities:Build an edge detector for still images.Build an
object recognition system given the edge detector.Build a system to predict
distance and orientation to objects given the object recognition system.Build
a system to plan a path through the scene you construct from {object
identification, distance, orientation} predictions.As you execute the above,
cons</p><p>6 0.11376613 <a title="102-tfidf-6" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>7 0.1134271 <a title="102-tfidf-7" href="../hunch_net-2009/hunch_net-2009-06-24-Interesting_papers_at_UAICMOLT_2009.html">361 hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</a></p>
<p>8 0.1095932 <a title="102-tfidf-8" href="../hunch_net-2007/hunch_net-2007-06-19-How_is_Compressed_Sensing_going_to_change_Machine_Learning_%3F.html">248 hunch net-2007-06-19-How is Compressed Sensing going to change Machine Learning ?</a></p>
<p>9 0.10793331 <a title="102-tfidf-9" href="../hunch_net-2006/hunch_net-2006-03-23-The_Approximation_Argument.html">165 hunch net-2006-03-23-The Approximation Argument</a></p>
<p>10 0.10342957 <a title="102-tfidf-10" href="../hunch_net-2006/hunch_net-2006-05-23-What_is_the_best_regret_transform_reduction_from_multiclass_to_binary%3F.html">181 hunch net-2006-05-23-What is the best regret transform reduction from multiclass to binary?</a></p>
<p>11 0.10296414 <a title="102-tfidf-11" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>12 0.098007694 <a title="102-tfidf-12" href="../hunch_net-2009/hunch_net-2009-01-07-Interesting_Papers_at_SODA_2009.html">334 hunch net-2009-01-07-Interesting Papers at SODA 2009</a></p>
<p>13 0.093295023 <a title="102-tfidf-13" href="../hunch_net-2005/hunch_net-2005-02-15-ESPgame_and_image_labeling.html">20 hunch net-2005-02-15-ESPgame and image labeling</a></p>
<p>14 0.090778247 <a title="102-tfidf-14" href="../hunch_net-2009/hunch_net-2009-01-21-Nearly_all_natural_problems_require_nonlinearity.html">337 hunch net-2009-01-21-Nearly all natural problems require nonlinearity</a></p>
<p>15 0.087595709 <a title="102-tfidf-15" href="../hunch_net-2006/hunch_net-2006-03-02-Why_do_people_count_for_learning%3F.html">160 hunch net-2006-03-02-Why do people count for learning?</a></p>
<p>16 0.087080359 <a title="102-tfidf-16" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>17 0.086421691 <a title="102-tfidf-17" href="../hunch_net-2005/hunch_net-2005-12-11-More_NIPS_Papers.html">139 hunch net-2005-12-11-More NIPS Papers</a></p>
<p>18 0.085086517 <a title="102-tfidf-18" href="../hunch_net-2005/hunch_net-2005-01-24-The_Humanloop_Spectrum_of_Machine_Learning.html">3 hunch net-2005-01-24-The Humanloop Spectrum of Machine Learning</a></p>
<p>19 0.083738677 <a title="102-tfidf-19" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>20 0.08296635 <a title="102-tfidf-20" href="../hunch_net-2006/hunch_net-2006-09-09-How_to_solve_an_NP_hard_problem_in_quadratic_time.html">206 hunch net-2006-09-09-How to solve an NP hard problem in quadratic time</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.204), (1, -0.065), (2, -0.012), (3, -0.001), (4, -0.038), (5, -0.066), (6, -0.026), (7, 0.013), (8, 0.102), (9, 0.04), (10, -0.005), (11, 0.054), (12, 0.034), (13, 0.017), (14, 0.037), (15, -0.026), (16, 0.084), (17, 0.027), (18, 0.01), (19, -0.076), (20, 0.034), (21, 0.048), (22, 0.055), (23, -0.076), (24, 0.028), (25, 0.001), (26, -0.029), (27, -0.092), (28, 0.115), (29, 0.097), (30, 0.072), (31, -0.008), (32, 0.089), (33, 0.001), (34, 0.042), (35, 0.08), (36, 0.143), (37, 0.015), (38, -0.002), (39, -0.008), (40, -0.091), (41, -0.018), (42, -0.065), (43, 0.02), (44, 0.087), (45, 0.028), (46, -0.093), (47, 0.05), (48, 0.009), (49, 0.015)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96683127 <a title="102-lsi-1" href="../hunch_net-2005/hunch_net-2005-08-11-Why_Manifold-Based_Dimension_Reduction_Techniques%3F.html">102 hunch net-2005-08-11-Why Manifold-Based Dimension Reduction Techniques?</a></p>
<p>Introduction: Manifold based dimension-reduction algorithms share the following general
outline.Given: a metricd()and a set of pointsSConstruct a graph with a point
in every node and every edge connecting to the node of one of thek-nearest
neighbors. Associate with the edge a weight which is the distance between the
points in the connected nodes.Digest the graph. This might include computing
the shortest path between all points or figuring out how to linearly
interpolate the point from it's neighbors.Find a set of points in a low
dimensional space which preserve the digested properties.Examples include LLE,
Isomap (which I worked on), Hessian-LLE, SDE, and many others. The hope with
these algorithms is that they can recover the low dimensional structure of
point sets in high dimensional spaces. Many of them can be shown to work in
interesting ways producing various compelling pictures.Despite doing some
early work in this direction, I suffer from a motivational problem: Why do we
want to recover the</p><p>2 0.68092817 <a title="102-lsi-2" href="../hunch_net-2005/hunch_net-2005-06-29-Not_EM_for_clustering_at_COLT.html">87 hunch net-2005-06-29-Not EM for clustering at COLT</a></p>
<p>Introduction: One standard approach for clustering data with a set of gaussians is using EM.
Roughly speaking, you pick a set ofkrandom guassians and then use alternating
expectation maximization to (hopefully) find a set of guassians that "explain"
the data well. This process is difficult to work with because EM can become
"stuck" in local optima. There are various hacks like "rerun withtdifferent
random starting points".One cool observation is that this can often be solved
via other algorithm which donotsuffer from local optima. This is an
earlypaperwhich shows this. Ravi Kannan presented anew papershowing this is
possible in a much more adaptive setting.A very rough summary of these papers
is that by projecting into a lower dimensional space, it is computationally
tractable to pick out the gross structure of the data. It is unclear how well
these algorithms work in practice, but they might be effective, especially if
used as a subroutine of the form:Project to low dimensional space.Pick out
gross</p><p>3 0.66313213 <a title="102-lsi-3" href="../hunch_net-2005/hunch_net-2005-04-25-Embeddings%3A_what_are_they_good_for%3F.html">61 hunch net-2005-04-25-Embeddings: what are they good for?</a></p>
<p>Introduction: I've been looking at some recent embeddings work, and am struck by how
beautiful the theory and algorithms are. It also makes me wonder, what are
embeddings good for?A few things immediately come to mind:(1) For
visualization of high-dimensional data sets.In this case, one would like good
algorithms for embedding specifically into 2- and 3-dimensional Euclidean
spaces.(2) For nonparametric modeling.The usual nonparametric models
(histograms, nearest neighbor) often require resources which are exponential
in the dimension. So if the data actually lie close to some low-
dimensionalsurface, it might be a good idea to first identify this surface and
embed the data before applying the model.Incidentally, for applications like
these, it's important to have a functional mapping from high to low dimension,
which some techniques do not yield up.(3) As a prelude to classifier
learning.The hope here is presumably that learning will be easier in the low-
dimensional space, because of (i) better ge</p><p>4 0.56977665 <a title="102-lsi-4" href="../hunch_net-2005/hunch_net-2005-02-15-ESPgame_and_image_labeling.html">20 hunch net-2005-02-15-ESPgame and image labeling</a></p>
<p>Introduction: Luis von Ahnhas been running theespgamefor awhile now. The espgame provides a
picture to two randomly paired people across the web, and asks them to agree
on a label. It hasn't managed to label the web yet, but it has produced alarge
datasetof (image, label) pairs. I organized the dataset so you couldexplore
the implied bipartite graph(requires much bandwidth).Relative to other image
datasets, this one is quite large--67000 images, 358,000 labels (average of
5/image with variation from 1 to 19), and 22,000 unique labels (one every 3
images). The dataset is also very 'natural', consisting of images spidered
from the internet. The multiple label characteristic is intriguing because
'learning to learn' and metalearning techniques may be applicable. The
'natural' quality means that this dataset varies greatly in difficulty from
easy (predicting "red") to hard (predicting "funny") and potentially more
rewarding to tackle.The open problem here is, of course, to make an internet
image labelin</p><p>5 0.54187185 <a title="102-lsi-5" href="../hunch_net-2006/hunch_net-2006-09-09-How_to_solve_an_NP_hard_problem_in_quadratic_time.html">206 hunch net-2006-09-09-How to solve an NP hard problem in quadratic time</a></p>
<p>Introduction: This title is a lie, but it is a special lie which has a bit of
truth.Ifnplayers each play each other, you have a tournament. How do you order
the players from weakest to strongest?The standard first attempt is "find the
ordering which agrees with the tournament on as many player pairs as
possible". This is called the "minimum feedback arcset" problem in the CS
theory literature and it is a well known NP-hard problem. A basic guarantee
holds for the solution to this problem: if there is some "true" intrinsic
ordering, and the outcome of the tournament disagreesktimes (due to noise for
instance), then the output ordering will disagree with the original ordering
on at most2kedges (and no solution can be better).One standard approach to
tractably solving an NP-hard problem is to find another algorithm with an
approximation guarantee. For example,Don Coppersmith,Lisa FleischerandAtri
Rudraproved thatordering players according to the number of wins is a
5-approximation to the NP-hard proble</p><p>6 0.52570939 <a title="102-lsi-6" href="../hunch_net-2005/hunch_net-2005-12-09-Some_NIPS_papers.html">138 hunch net-2005-12-09-Some NIPS papers</a></p>
<p>7 0.51514632 <a title="102-lsi-7" href="../hunch_net-2006/hunch_net-2006-02-02-Introspectionism_as_a_Disease.html">153 hunch net-2006-02-02-Introspectionism as a Disease</a></p>
<p>8 0.49785089 <a title="102-lsi-8" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>9 0.49663544 <a title="102-lsi-9" href="../hunch_net-2005/hunch_net-2005-02-27-Antilearning%3A_When_proximity_goes_bad.html">32 hunch net-2005-02-27-Antilearning: When proximity goes bad</a></p>
<p>10 0.4962143 <a title="102-lsi-10" href="../hunch_net-2005/hunch_net-2005-04-21-Dynamic_Programming_Generalizations_and_Their_Use.html">58 hunch net-2005-04-21-Dynamic Programming Generalizations and Their Use</a></p>
<p>11 0.4776243 <a title="102-lsi-11" href="../hunch_net-2005/hunch_net-2005-12-11-More_NIPS_Papers.html">139 hunch net-2005-12-11-More NIPS Papers</a></p>
<p>12 0.47689644 <a title="102-lsi-12" href="../hunch_net-2005/hunch_net-2005-08-01-Peekaboom.html">99 hunch net-2005-08-01-Peekaboom</a></p>
<p>13 0.47653943 <a title="102-lsi-13" href="../hunch_net-2005/hunch_net-2005-05-29-Maximum_Margin_Mismatch%3F.html">77 hunch net-2005-05-29-Maximum Margin Mismatch?</a></p>
<p>14 0.47614208 <a title="102-lsi-14" href="../hunch_net-2006/hunch_net-2006-11-06-Data_Linkage_Problems.html">217 hunch net-2006-11-06-Data Linkage Problems</a></p>
<p>15 0.47388598 <a title="102-lsi-15" href="../hunch_net-2011/hunch_net-2011-10-03-Monday_announcements.html">446 hunch net-2011-10-03-Monday announcements</a></p>
<p>16 0.46435523 <a title="102-lsi-16" href="../hunch_net-2006/hunch_net-2006-03-23-The_Approximation_Argument.html">165 hunch net-2006-03-23-The Approximation Argument</a></p>
<p>17 0.46367407 <a title="102-lsi-17" href="../hunch_net-2006/hunch_net-2006-02-27-The_Peekaboom_Dataset.html">159 hunch net-2006-02-27-The Peekaboom Dataset</a></p>
<p>18 0.45656079 <a title="102-lsi-18" href="../hunch_net-2009/hunch_net-2009-01-07-Interesting_Papers_at_SODA_2009.html">334 hunch net-2009-01-07-Interesting Papers at SODA 2009</a></p>
<p>19 0.44959784 <a title="102-lsi-19" href="../hunch_net-2006/hunch_net-2006-07-05-more_icml_papers.html">189 hunch net-2006-07-05-more icml papers</a></p>
<p>20 0.44372436 <a title="102-lsi-20" href="../hunch_net-2007/hunch_net-2007-09-18-It%26%238217%3Bs_MDL_Jim%2C_but_not_as_we_know_it%26%238230%3B%28on_Bayes%2C_MDL_and_consistency%29.html">263 hunch net-2007-09-18-It&#8217;s MDL Jim, but not as we know it&#8230;(on Bayes, MDL and consistency)</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(6, 0.036), (38, 0.02), (42, 0.214), (45, 0.067), (68, 0.108), (69, 0.03), (74, 0.137), (76, 0.045), (80, 0.218), (88, 0.023), (95, 0.028)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.89350218 <a title="102-lda-1" href="../hunch_net-2005/hunch_net-2005-08-11-Why_Manifold-Based_Dimension_Reduction_Techniques%3F.html">102 hunch net-2005-08-11-Why Manifold-Based Dimension Reduction Techniques?</a></p>
<p>Introduction: Manifold based dimension-reduction algorithms share the following general
outline.Given: a metricd()and a set of pointsSConstruct a graph with a point
in every node and every edge connecting to the node of one of thek-nearest
neighbors. Associate with the edge a weight which is the distance between the
points in the connected nodes.Digest the graph. This might include computing
the shortest path between all points or figuring out how to linearly
interpolate the point from it's neighbors.Find a set of points in a low
dimensional space which preserve the digested properties.Examples include LLE,
Isomap (which I worked on), Hessian-LLE, SDE, and many others. The hope with
these algorithms is that they can recover the low dimensional structure of
point sets in high dimensional spaces. Many of them can be shown to work in
interesting ways producing various compelling pictures.Despite doing some
early work in this direction, I suffer from a motivational problem: Why do we
want to recover the</p><p>2 0.86941731 <a title="102-lda-2" href="../hunch_net-2011/hunch_net-2011-07-11-Interesting_Neural_Network_Papers_at_ICML_2011.html">438 hunch net-2011-07-11-Interesting Neural Network Papers at ICML 2011</a></p>
<p>Introduction: Maybe it's too early to call, but with four separate Neural Network sessions
at this year'sICML, it looks like Neural Networks are making a comeback. Here
are my highlights of these sessions. In general, my feeling is that these
papers both demystify deep learning and show its broader applicability.The
first observation I made is that the once disreputable "Neural" nomenclature
is being used againin lieu of"deep learning". Maybe it's because Adam Coates
et al. showed that single layer networks can work surprisingly well.An
Analysis of Single-Layer Networks in Unsupervised Feature Learning,Adam
Coates,Honglak Lee,Andrew Y. Ng(AISTATS 2011)The Importance of Encoding Versus
Training with Sparse Coding and Vector Quantization,Adam Coates,Andrew Y.
Ng(ICML 2011)Another surprising result out of Andrew Ng's group comes from
Andrew Saxe et al. who show that certain convolutional pooling architectures
can obtain close to state-of-the-art performance with random weights (that is,
without actuall</p><p>3 0.84950876 <a title="102-lda-3" href="../hunch_net-2005/hunch_net-2005-10-13-Site_tweak.html">122 hunch net-2005-10-13-Site tweak</a></p>
<p>Introduction: Several people have had difficulty with comments which seem to have an allowed
language significantly poorer than posts. The set of allowed html tags has
been increased and themarkdown filterhas been put in place to try to make
commenting easier. I'll put some examples into the comments of this post.</p><p>4 0.83259815 <a title="102-lda-4" href="../hunch_net-2009/hunch_net-2009-05-19-CI_Fellows.html">355 hunch net-2009-05-19-CI Fellows</a></p>
<p>Introduction: Lev Reyzinpoints out theCI Fellows Project. Essentially,NSFis funding 60
postdocs in computer science for graduates from a wide array of US places to a
wide array of US places. This is particularly welcome given a tough year for
new hires. I expect some fraction of these postdocs will be in ML. The time
frame is quite short, so those interested should look it over immediately.</p><p>5 0.80854911 <a title="102-lda-5" href="../hunch_net-2005/hunch_net-2005-03-17-Going_all_the_Way%2C_Sometimes.html">42 hunch net-2005-03-17-Going all the Way, Sometimes</a></p>
<p>Introduction: At many points in research, you face a choice: should I keep on improving some
old piece of technology or should I do something new? For example:Should I
refine bounds to make them tighter?Should I take some learning theory and turn
it into a learning algorithm?Should I implement the learning algorithm?Should
I test the learning algorithm widely?Should I release the algorithm as source
code?Should I go see what problems people actually need to solve?The universal
temptation of people attracted to research is doing something new. That is
sometimes the right decision, but is also often not. I'd like to discuss some
reasons why not.ExpertiseOnce expertise are developed on some subject, you are
the right person to refine them.What is the real problem?Continually improving
a piece of technology is a mechanism forcing you to confront this question. In
many cases, this confrontation is uncomfortable because you discover that your
method has fundamental flaws with respect to solving the real p</p><p>6 0.76659203 <a title="102-lda-6" href="../hunch_net-2011/hunch_net-2011-08-20-The_Large_Scale_Learning_Survey_Tutorial.html">442 hunch net-2011-08-20-The Large Scale Learning Survey Tutorial</a></p>
<p>7 0.75219548 <a title="102-lda-7" href="../hunch_net-2005/hunch_net-2005-09-10-%26%238220%3BFailure%26%238221%3B_is_an_option.html">110 hunch net-2005-09-10-&#8220;Failure&#8221; is an option</a></p>
<p>8 0.74400902 <a title="102-lda-8" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>9 0.74368304 <a title="102-lda-9" href="../hunch_net-2008/hunch_net-2008-04-12-It_Doesn%26%238217%3Bt_Stop.html">295 hunch net-2008-04-12-It Doesn&#8217;t Stop</a></p>
<p>10 0.74302751 <a title="102-lda-10" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>11 0.74035072 <a title="102-lda-11" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>12 0.73882669 <a title="102-lda-12" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>13 0.73757714 <a title="102-lda-13" href="../hunch_net-2011/hunch_net-2011-11-26-Giving_Thanks.html">449 hunch net-2011-11-26-Giving Thanks</a></p>
<p>14 0.73580396 <a title="102-lda-14" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>15 0.73397797 <a title="102-lda-15" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>16 0.73389536 <a title="102-lda-16" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>17 0.73315609 <a title="102-lda-17" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>18 0.73307335 <a title="102-lda-18" href="../hunch_net-2005/hunch_net-2005-06-08-Question%3A_%26%238220%3BWhen_is_the_right_time_to_insert_the_loss_function%3F%26%238221%3B.html">79 hunch net-2005-06-08-Question: &#8220;When is the right time to insert the loss function?&#8221;</a></p>
<p>19 0.732333 <a title="102-lda-19" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>20 0.73069364 <a title="102-lda-20" href="../hunch_net-2008/hunch_net-2008-07-02-Proprietary_Data_in_Academic_Research%3F.html">306 hunch net-2008-07-02-Proprietary Data in Academic Research?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
