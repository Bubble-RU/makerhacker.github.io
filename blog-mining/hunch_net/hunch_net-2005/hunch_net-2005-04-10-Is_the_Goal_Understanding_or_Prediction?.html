<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>55 hunch net-2005-04-10-Is the Goal Understanding or Prediction?</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-55" href="#">hunch_net-2005-55</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>55 hunch net-2005-04-10-Is the Goal Understanding or Prediction?</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-55-html" href="http://hunch.net/?p=60">html</a></p><p>Introduction: Steve Smaleand I have a debate about goals of learning theory.Steve likes
theorems with a dependence on unobservable quantities. For example, ifDis a
distribution over a spaceX x [0,1], you can state a theorem about the error
rate dependent on the variance,E(x,y)~D(y-Ey'~D|x[y'])2.I dislike this,
because I want to use the theorems to produce code solving learning problems.
Since I don't know (and can't measure) the variance, a theorem depending on
the variance does not help me--I would not know what variance to plug into the
learning algorithm.Recast more broadly, this is a debate between "declarative"
and "operative" mathematics. A strong example of "declarative" mathematics
is"a new kind of science". Roughly speaking, the goal of this kind of approach
seems to be finding a way to explain the observations we make. Examples
include "some things are unpredictable", "a phase transition exists",
etcâ&euro;Ś"Operative" mathematics helps you make predictions about the world. A
strong example of op</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Steve Smaleand I have a debate about goals of learning theory. [sent-1, score-0.237]
</p><p>2 For example, ifDis a distribution over a spaceX x [0,1], you can state a theorem about the error rate dependent on the variance,E(x,y)~D(y-Ey'~D|x[y'])2. [sent-3, score-0.236]
</p><p>3 I dislike this, because I want to use the theorems to produce code solving learning problems. [sent-4, score-0.421]
</p><p>4 Since I don't know (and can't measure) the variance, a theorem depending on the variance does not help me--I would not know what variance to plug into the learning algorithm. [sent-5, score-0.975]
</p><p>5 Recast more broadly, this is a debate between "declarative" and "operative" mathematics. [sent-6, score-0.167]
</p><p>6 A strong example of "declarative" mathematics is"a new kind of science". [sent-7, score-0.609]
</p><p>7 Roughly speaking, the goal of this kind of approach seems to be finding a way to explain the observations we make. [sent-8, score-0.34]
</p><p>8 Examples include "some things are unpredictable", "a phase transition exists", etcâ&euro;Ś"Operative" mathematics helps you make predictions about the world. [sent-9, score-0.736]
</p><p>9 A strong example of operative mathematics is Newtonian mechanics in physics: it's a great tool to help you predict what is going to happen in the world. [sent-10, score-1.448]
</p><p>10 In addition to the "I want to do things" motivation for operative mathematics, I find it less arbitrary. [sent-11, score-0.848]
</p><p>11 In particular, two reasonable people can each be convinced they understand a topic in ways so different that they do not understand the viewpoint. [sent-12, score-0.312]
</p><p>12 If these understandings are operative, the rest of us on the sidelines can better appreciate which understanding is "best". [sent-13, score-0.248]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('operative', 0.646), ('mathematics', 0.31), ('declarative', 0.258), ('variance', 0.232), ('debate', 0.167), ('kind', 0.138), ('theorems', 0.132), ('plug', 0.115), ('steve', 0.115), ('understandings', 0.115), ('theorem', 0.112), ('dislike', 0.1), ('convinced', 0.1), ('mechanics', 0.1), ('phase', 0.096), ('help', 0.093), ('strong', 0.092), ('transition', 0.092), ('explain', 0.081), ('physics', 0.079), ('tool', 0.079), ('broadly', 0.077), ('understand', 0.077), ('dependence', 0.074), ('motivation', 0.074), ('speaking', 0.071), ('goals', 0.07), ('dependent', 0.07), ('appreciate', 0.069), ('depending', 0.069), ('example', 0.069), ('want', 0.067), ('roughly', 0.067), ('observations', 0.067), ('produce', 0.064), ('rest', 0.064), ('helps', 0.062), ('addition', 0.061), ('know', 0.061), ('measure', 0.061), ('things', 0.06), ('happen', 0.059), ('predictions', 0.058), ('code', 0.058), ('topic', 0.058), ('include', 0.058), ('state', 0.054), ('finding', 0.054), ('exists', 0.053), ('ca', 0.053)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="55-tfidf-1" href="../hunch_net-2005/hunch_net-2005-04-10-Is_the_Goal_Understanding_or_Prediction%3F.html">55 hunch net-2005-04-10-Is the Goal Understanding or Prediction?</a></p>
<p>Introduction: Steve Smaleand I have a debate about goals of learning theory.Steve likes
theorems with a dependence on unobservable quantities. For example, ifDis a
distribution over a spaceX x [0,1], you can state a theorem about the error
rate dependent on the variance,E(x,y)~D(y-Ey'~D|x[y'])2.I dislike this,
because I want to use the theorems to produce code solving learning problems.
Since I don't know (and can't measure) the variance, a theorem depending on
the variance does not help me--I would not know what variance to plug into the
learning algorithm.Recast more broadly, this is a debate between "declarative"
and "operative" mathematics. A strong example of "declarative" mathematics
is"a new kind of science". Roughly speaking, the goal of this kind of approach
seems to be finding a way to explain the observations we make. Examples
include "some things are unpredictable", "a phase transition exists",
etcâ&euro;Ś"Operative" mathematics helps you make predictions about the world. A
strong example of op</p><p>2 0.17437117 <a title="55-tfidf-2" href="../hunch_net-2008/hunch_net-2008-05-25-Inappropriate_Mathematics_for_Machine_Learning.html">302 hunch net-2008-05-25-Inappropriate Mathematics for Machine Learning</a></p>
<p>Introduction: Reviewers and students are sometimes greatly concerned by the distinction
between:Anopen setand aclosed set.ASupremumand aMaximum.An event which happens
with probability 1 and an event that always happens.I don't appreciate this
distinction in machine learning & learning theory. All machine learning takes
place (by definition) on a machine where every parameter has finite precision.
Consequently, every set is closed, a maximal element always exists, and
probability 1 events always happen.The fundamental issue here is that
substantial parts of mathematics don't appear well-matched to computation in
the physical world, because the mathematics has concerns which are unphysical.
This mismatched mathematics makes irrelevant distinctions. We can ask "what
mathematics is appropriate to computation?"Andrejhas convinced me that a
pretty good answer to this question isconstructive mathematics.So, here's a
basic challenge: Can anyone name a situation where any of the distinctions
above (or simila</p><p>3 0.13980648 <a title="55-tfidf-3" href="../hunch_net-2007/hunch_net-2007-05-09-The_Missing_Bound.html">244 hunch net-2007-05-09-The Missing Bound</a></p>
<p>Introduction: Sham Kakadepoints out that we are missing a bound.Suppose we
havemsamplesxdrawn IID from some distributionD. Through the magic of
exponential moment method we know that:If the range ofxis bounded by an
interval of sizeI, aChernoff/Hoeffding style boundgives us a bound on the
deviations likeO(I/m0.5)(at least in crude form). A proof is on page 9here.If
the range ofxis bounded, and the variance (or a bound on the variance) is
known, thenBennett's boundcan give tighter results (*). This can be a huge
improvment when the true variance small.What's missing here is a bound that
depends on the observed variance rather than a bound on the variance. This
means that many people attempt to use Bennett's bound (incorrectly) by
plugging the observed variance in as the true variance, invalidating the bound
application. Most of the time, they get away with it, but this is a dangerous
move when doing machine learning. In machine learning, we are typically trying
to find a predictor with 0 expected los</p><p>4 0.1181477 <a title="55-tfidf-4" href="../hunch_net-2005/hunch_net-2005-07-07-The_Limits_of_Learning_Theory.html">90 hunch net-2005-07-07-The Limits of Learning Theory</a></p>
<p>Introduction: Suppose we had an infinitely powerful mathematician sitting in a room and
proving theorems about learning. Could he solve machine learning?The answer is
"no". This answer is both obvious and sometimes underappreciated.There are
several ways to conclude that somebiasis necessary in order to succesfully
learn. For example, suppose we are trying to solve classification. At
prediction time, we observe some featuresXand want to make a prediction of
either0or1. Bias is what makes us prefer one answer over the other based on
past experience. In order to learn we must:Have a bias. Always predicting0is
as likely as1is useless.Have the "right" bias. Predicting1when the answer
is0is also not helpful.The implication of "have a bias" is that we can not
design effective learning algorithms with "a uniform prior over all
possibilities". The implication of "have the 'right' bias" is that our
mathematician fails since "right" is defined with respect to the solutions to
problems encountered in the real</p><p>5 0.097193286 <a title="55-tfidf-5" href="../hunch_net-2005/hunch_net-2005-11-28-A_question_of_quantification.html">133 hunch net-2005-11-28-A question of quantification</a></p>
<p>Introduction: This is about methods for phrasing and think about the scope of some theorems
in learning theory. The basic claim is that there are several different ways
of quantifying the scope which sound different yet are essentially the
same.For all sequences of examples. This is the standard quantification in
online learning analysis. Standard theorems would say something like "for all
sequences of predictions by experts, the algorithm A will perform almost as
well as the best expert."For all training sets. This is the standard
quantification for boosting analysis such asadaboostormulticlass
boosting.Standard theorems have the form "for all training sets the error rate
inequalities … hold".For all distributions over examples. This is the one that
we have been using for reductions analysis. Standard theorem statements have
the form "For all distributions over examples, the error rate inequalities …
hold".It is not quite true that each of these is equivalent. For example, in
the online learning se</p><p>6 0.090586141 <a title="55-tfidf-6" href="../hunch_net-2005/hunch_net-2005-05-12-Math_on_the_Web.html">70 hunch net-2005-05-12-Math on the Web</a></p>
<p>7 0.076103456 <a title="55-tfidf-7" href="../hunch_net-2007/hunch_net-2007-07-20-Motivation_should_be_the_Responsibility_of_the_Reviewer.html">256 hunch net-2007-07-20-Motivation should be the Responsibility of the Reviewer</a></p>
<p>8 0.076078907 <a title="55-tfidf-8" href="../hunch_net-2005/hunch_net-2005-02-02-Kolmogorov_Complexity_and_Googling.html">10 hunch net-2005-02-02-Kolmogorov Complexity and Googling</a></p>
<p>9 0.075210907 <a title="55-tfidf-9" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>10 0.069885582 <a title="55-tfidf-10" href="../hunch_net-2010/hunch_net-2010-03-26-A_Variance_only_Deviation_Bound.html">392 hunch net-2010-03-26-A Variance only Deviation Bound</a></p>
<p>11 0.065357134 <a title="55-tfidf-11" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>12 0.064171568 <a title="55-tfidf-12" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>13 0.064000927 <a title="55-tfidf-13" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>14 0.063812621 <a title="55-tfidf-14" href="../hunch_net-2010/hunch_net-2010-03-15-The_Efficient_Robust_Conditional_Probability_Estimation_Problem.html">391 hunch net-2010-03-15-The Efficient Robust Conditional Probability Estimation Problem</a></p>
<p>15 0.063443758 <a title="55-tfidf-15" href="../hunch_net-2008/hunch_net-2008-09-12-How_do_we_get_weak_action_dependence_for_learning_with_partial_observations%3F.html">317 hunch net-2008-09-12-How do we get weak action dependence for learning with partial observations?</a></p>
<p>16 0.060158234 <a title="55-tfidf-16" href="../hunch_net-2006/hunch_net-2006-04-05-What_is_state%3F.html">169 hunch net-2006-04-05-What is state?</a></p>
<p>17 0.056235276 <a title="55-tfidf-17" href="../hunch_net-2005/hunch_net-2005-03-15-The_State_of_Tight_Bounds.html">41 hunch net-2005-03-15-The State of Tight Bounds</a></p>
<p>18 0.056229267 <a title="55-tfidf-18" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>19 0.055066202 <a title="55-tfidf-19" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>20 0.054243643 <a title="55-tfidf-20" href="../hunch_net-2006/hunch_net-2006-08-10-Precision_is_not_accuracy.html">202 hunch net-2006-08-10-Precision is not accuracy</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.132), (1, -0.038), (2, 0.01), (3, -0.053), (4, 0.017), (5, -0.075), (6, -0.022), (7, 0.013), (8, 0.002), (9, -0.026), (10, 0.006), (11, 0.011), (12, 0.002), (13, -0.076), (14, -0.05), (15, 0.034), (16, -0.003), (17, 0.031), (18, -0.005), (19, 0.013), (20, 0.02), (21, -0.012), (22, -0.093), (23, -0.042), (24, -0.003), (25, 0.041), (26, -0.044), (27, 0.021), (28, -0.068), (29, -0.033), (30, 0.019), (31, 0.037), (32, -0.06), (33, -0.049), (34, -0.034), (35, -0.088), (36, -0.047), (37, 0.015), (38, 0.017), (39, 0.147), (40, -0.055), (41, 0.019), (42, 0.061), (43, 0.133), (44, 0.024), (45, 0.004), (46, 0.003), (47, 0.021), (48, 0.005), (49, 0.05)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94892961 <a title="55-lsi-1" href="../hunch_net-2005/hunch_net-2005-04-10-Is_the_Goal_Understanding_or_Prediction%3F.html">55 hunch net-2005-04-10-Is the Goal Understanding or Prediction?</a></p>
<p>Introduction: Steve Smaleand I have a debate about goals of learning theory.Steve likes
theorems with a dependence on unobservable quantities. For example, ifDis a
distribution over a spaceX x [0,1], you can state a theorem about the error
rate dependent on the variance,E(x,y)~D(y-Ey'~D|x[y'])2.I dislike this,
because I want to use the theorems to produce code solving learning problems.
Since I don't know (and can't measure) the variance, a theorem depending on
the variance does not help me--I would not know what variance to plug into the
learning algorithm.Recast more broadly, this is a debate between "declarative"
and "operative" mathematics. A strong example of "declarative" mathematics
is"a new kind of science". Roughly speaking, the goal of this kind of approach
seems to be finding a way to explain the observations we make. Examples
include "some things are unpredictable", "a phase transition exists",
etcâ&euro;Ś"Operative" mathematics helps you make predictions about the world. A
strong example of op</p><p>2 0.64711267 <a title="55-lsi-2" href="../hunch_net-2008/hunch_net-2008-05-25-Inappropriate_Mathematics_for_Machine_Learning.html">302 hunch net-2008-05-25-Inappropriate Mathematics for Machine Learning</a></p>
<p>Introduction: Reviewers and students are sometimes greatly concerned by the distinction
between:Anopen setand aclosed set.ASupremumand aMaximum.An event which happens
with probability 1 and an event that always happens.I don't appreciate this
distinction in machine learning & learning theory. All machine learning takes
place (by definition) on a machine where every parameter has finite precision.
Consequently, every set is closed, a maximal element always exists, and
probability 1 events always happen.The fundamental issue here is that
substantial parts of mathematics don't appear well-matched to computation in
the physical world, because the mathematics has concerns which are unphysical.
This mismatched mathematics makes irrelevant distinctions. We can ask "what
mathematics is appropriate to computation?"Andrejhas convinced me that a
pretty good answer to this question isconstructive mathematics.So, here's a
basic challenge: Can anyone name a situation where any of the distinctions
above (or simila</p><p>3 0.63331378 <a title="55-lsi-3" href="../hunch_net-2005/hunch_net-2005-07-07-The_Limits_of_Learning_Theory.html">90 hunch net-2005-07-07-The Limits of Learning Theory</a></p>
<p>Introduction: Suppose we had an infinitely powerful mathematician sitting in a room and
proving theorems about learning. Could he solve machine learning?The answer is
"no". This answer is both obvious and sometimes underappreciated.There are
several ways to conclude that somebiasis necessary in order to succesfully
learn. For example, suppose we are trying to solve classification. At
prediction time, we observe some featuresXand want to make a prediction of
either0or1. Bias is what makes us prefer one answer over the other based on
past experience. In order to learn we must:Have a bias. Always predicting0is
as likely as1is useless.Have the "right" bias. Predicting1when the answer
is0is also not helpful.The implication of "have a bias" is that we can not
design effective learning algorithms with "a uniform prior over all
possibilities". The implication of "have the 'right' bias" is that our
mathematician fails since "right" is defined with respect to the solutions to
problems encountered in the real</p><p>4 0.53126574 <a title="55-lsi-4" href="../hunch_net-2005/hunch_net-2005-11-28-A_question_of_quantification.html">133 hunch net-2005-11-28-A question of quantification</a></p>
<p>Introduction: This is about methods for phrasing and think about the scope of some theorems
in learning theory. The basic claim is that there are several different ways
of quantifying the scope which sound different yet are essentially the
same.For all sequences of examples. This is the standard quantification in
online learning analysis. Standard theorems would say something like "for all
sequences of predictions by experts, the algorithm A will perform almost as
well as the best expert."For all training sets. This is the standard
quantification for boosting analysis such asadaboostormulticlass
boosting.Standard theorems have the form "for all training sets the error rate
inequalities … hold".For all distributions over examples. This is the one that
we have been using for reductions analysis. Standard theorem statements have
the form "For all distributions over examples, the error rate inequalities …
hold".It is not quite true that each of these is equivalent. For example, in
the online learning se</p><p>5 0.52350742 <a title="55-lsi-5" href="../hunch_net-2007/hunch_net-2007-05-09-The_Missing_Bound.html">244 hunch net-2007-05-09-The Missing Bound</a></p>
<p>Introduction: Sham Kakadepoints out that we are missing a bound.Suppose we
havemsamplesxdrawn IID from some distributionD. Through the magic of
exponential moment method we know that:If the range ofxis bounded by an
interval of sizeI, aChernoff/Hoeffding style boundgives us a bound on the
deviations likeO(I/m0.5)(at least in crude form). A proof is on page 9here.If
the range ofxis bounded, and the variance (or a bound on the variance) is
known, thenBennett's boundcan give tighter results (*). This can be a huge
improvment when the true variance small.What's missing here is a bound that
depends on the observed variance rather than a bound on the variance. This
means that many people attempt to use Bennett's bound (incorrectly) by
plugging the observed variance in as the true variance, invalidating the bound
application. Most of the time, they get away with it, but this is a dangerous
move when doing machine learning. In machine learning, we are typically trying
to find a predictor with 0 expected los</p><p>6 0.52273721 <a title="55-lsi-6" href="../hunch_net-2010/hunch_net-2010-10-08-An_easy_proof_of_the_Chernoff-Hoeffding_bound.html">413 hunch net-2010-10-08-An easy proof of the Chernoff-Hoeffding bound</a></p>
<p>7 0.51834774 <a title="55-lsi-7" href="../hunch_net-2006/hunch_net-2006-08-10-Precision_is_not_accuracy.html">202 hunch net-2006-08-10-Precision is not accuracy</a></p>
<p>8 0.5079999 <a title="55-lsi-8" href="../hunch_net-2006/hunch_net-2006-06-25-Presentation_of_Proofs_is_Hard..html">187 hunch net-2006-06-25-Presentation of Proofs is Hard.</a></p>
<p>9 0.49467364 <a title="55-lsi-9" href="../hunch_net-2005/hunch_net-2005-08-22-Do_you_believe_in_induction%3F.html">104 hunch net-2005-08-22-Do you believe in induction?</a></p>
<p>10 0.49172544 <a title="55-lsi-10" href="../hunch_net-2005/hunch_net-2005-02-02-Kolmogorov_Complexity_and_Googling.html">10 hunch net-2005-02-02-Kolmogorov Complexity and Googling</a></p>
<p>11 0.49162981 <a title="55-lsi-11" href="../hunch_net-2006/hunch_net-2006-12-05-Recruitment_Conferences.html">222 hunch net-2006-12-05-Recruitment Conferences</a></p>
<p>12 0.48914918 <a title="55-lsi-12" href="../hunch_net-2010/hunch_net-2010-03-26-A_Variance_only_Deviation_Bound.html">392 hunch net-2010-03-26-A Variance only Deviation Bound</a></p>
<p>13 0.48185551 <a title="55-lsi-13" href="../hunch_net-2007/hunch_net-2007-01-15-The_Machine_Learning_Department.html">228 hunch net-2007-01-15-The Machine Learning Department</a></p>
<p>14 0.47611943 <a title="55-lsi-14" href="../hunch_net-2006/hunch_net-2006-03-17-Multitask_learning_is_Black-Boxable.html">164 hunch net-2006-03-17-Multitask learning is Black-Boxable</a></p>
<p>15 0.46864578 <a title="55-lsi-15" href="../hunch_net-2008/hunch_net-2008-08-24-Mass_Customized_Medicine_in_the_Future%3F.html">314 hunch net-2008-08-24-Mass Customized Medicine in the Future?</a></p>
<p>16 0.46418107 <a title="55-lsi-16" href="../hunch_net-2009/hunch_net-2009-04-02-Asymmophobia.html">348 hunch net-2009-04-02-Asymmophobia</a></p>
<p>17 0.46363565 <a title="55-lsi-17" href="../hunch_net-2007/hunch_net-2007-07-20-Motivation_should_be_the_Responsibility_of_the_Reviewer.html">256 hunch net-2007-07-20-Motivation should be the Responsibility of the Reviewer</a></p>
<p>18 0.46134186 <a title="55-lsi-18" href="../hunch_net-2006/hunch_net-2006-03-09-Use_of_Notation.html">162 hunch net-2006-03-09-Use of Notation</a></p>
<p>19 0.46088764 <a title="55-lsi-19" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>20 0.44926074 <a title="55-lsi-20" href="../hunch_net-2006/hunch_net-2006-01-18-Is_Multitask_Learning_Black-Boxable%3F.html">149 hunch net-2006-01-18-Is Multitask Learning Black-Boxable?</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(28, 0.293), (35, 0.1), (42, 0.304), (69, 0.022), (74, 0.098), (82, 0.024), (95, 0.032)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.88780034 <a title="55-lda-1" href="../hunch_net-2005/hunch_net-2005-04-10-Is_the_Goal_Understanding_or_Prediction%3F.html">55 hunch net-2005-04-10-Is the Goal Understanding or Prediction?</a></p>
<p>Introduction: Steve Smaleand I have a debate about goals of learning theory.Steve likes
theorems with a dependence on unobservable quantities. For example, ifDis a
distribution over a spaceX x [0,1], you can state a theorem about the error
rate dependent on the variance,E(x,y)~D(y-Ey'~D|x[y'])2.I dislike this,
because I want to use the theorems to produce code solving learning problems.
Since I don't know (and can't measure) the variance, a theorem depending on
the variance does not help me--I would not know what variance to plug into the
learning algorithm.Recast more broadly, this is a debate between "declarative"
and "operative" mathematics. A strong example of "declarative" mathematics
is"a new kind of science". Roughly speaking, the goal of this kind of approach
seems to be finding a way to explain the observations we make. Examples
include "some things are unpredictable", "a phase transition exists",
etcâ&euro;Ś"Operative" mathematics helps you make predictions about the world. A
strong example of op</p><p>2 0.79102576 <a title="55-lda-2" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>Introduction: Machine learning has a new kind of "scaling to larger problems" to worry
about: scaling with the amount of contextual information. The standard
development path for a machine learning application in practice seems to be
the following:Marginal. In the beginning, there was "majority vote". At this
stage, it isn't necessary to understand that you have a prediction problem.
People just realize that one answer is right sometimes and another answer
other times. In machine learning terms, this corresponds to making a
prediction without side information.First context. A clever person realizes
that some bit of informationx1could be helpful. Ifx1is discrete, they
condition on it and make a predictorh(x1), typically by counting. If they are
clever, then they also do some smoothing. Ifx1is some real valued parameter,
it's very common to make a threshold cutoff. Often, these tasks are simply
done by hand.Second. Another clever person (or perhaps the same one) realizes
that some other bit of informa</p><p>3 0.72997463 <a title="55-lda-3" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>Introduction: I want to try to describe what doing research means, especially from the point
of view of an undergraduate. The shift from a class-taking mentality to a
research mentality is very significant and not easy.Problem PosingPosing the
right problem is often as important as solving them. Many people can get by in
research by solving problems others have posed, but that's not sufficient for
really inspiring research. For learning in particular, there is a strong
feeling that we just haven't figured out which questions are the right ones to
ask. You can see this, because the answers we have do not seem
convincing.Gambling your lifeWhen you do research, you think very hard about
new ways of solving problems, new problems, and new solutions. Many
conversations are of the form "I wonder what would happen if…" These processes
can be short (days or weeks) or years-long endeavours. The worst part is that
you'll only know if you were succesful at the end of the process (and
sometimes not even then be</p><p>4 0.7266351 <a title="55-lda-4" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>Introduction: Yahoo! laid off people. Unlike every previous time there have been layoffs,
this is serious forYahoo! Research.We had advanced warning
fromPrabhakarthrough thesimple act of leaving. Yahoo! Research was a world
class organization that Prabhakar recruited much of personally, so it is
deeply implausible that he would spontaneously decide to leave. My first
thought when I saw the news was "Uhoh,Robsaid that he knew it was serious when
the head of ATnT Research left." In this case it was even more significant,
because Prabhakar recruited me on the premise that Y!R was an experiment in
how research should be done: via a combination of high quality people and high
engagement with the company. Prabhakar's departure is a clear end to that
experiment.The result is ambiguous from a business perspective. Y!R clearly
was not capable of saving the company from its illnesses. I'm not privy to the
internal accounting of impact and this is the kind of subject where there can
easily be great disagreemen</p><p>5 0.72614503 <a title="55-lda-5" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>Introduction: I've had serious conversations with several people who believe that the theory
in machine learning is "only useful for getting papers published". That's a
compelling statement, as I've seen many papers where the algorithm clearly
came first, and the theoretical justification for it came second, purely as a
perceived means to improve the chance of publication.Naturally, I disagree and
believe that learning theory has much more substantial applications.Even in
core learning algorithm design, I've found learning theory to be useful,
although it's application is more subtle than many realize. The most
straightforward applications can fail, because (as expectation suggests) worst
case bounds tend to be loose in practice (*). In my experience, considering
learning theory when designing an algorithm has two important effects in
practice:It can help make your algorithm behave right at a crude level of
analysis, leaving finer details to tuning or common sense. The best example I
have of this is</p><p>6 0.72578335 <a title="55-lda-6" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>7 0.72529495 <a title="55-lda-7" href="../hunch_net-2013/hunch_net-2013-12-01-NIPS_tutorials_and_Vowpal_Wabbit_7.4.html">492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</a></p>
<p>8 0.725236 <a title="55-lda-8" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>9 0.72518027 <a title="55-lda-9" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>10 0.7243557 <a title="55-lda-10" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>11 0.72422779 <a title="55-lda-11" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>12 0.72414947 <a title="55-lda-12" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>13 0.72407061 <a title="55-lda-13" href="../hunch_net-2005/hunch_net-2005-02-25-Problem%3A_Online_Learning.html">28 hunch net-2005-02-25-Problem: Online Learning</a></p>
<p>14 0.72223824 <a title="55-lda-14" href="../hunch_net-2006/hunch_net-2006-01-13-Benchmarks_for_RL.html">148 hunch net-2006-01-13-Benchmarks for RL</a></p>
<p>15 0.72193843 <a title="55-lda-15" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>16 0.72172004 <a title="55-lda-16" href="../hunch_net-2006/hunch_net-2006-05-05-An_ICML_reject.html">177 hunch net-2006-05-05-An ICML reject</a></p>
<p>17 0.72163802 <a title="55-lda-17" href="../hunch_net-2006/hunch_net-2006-02-11-Yahoo%26%238217%3Bs_Learning_Problems..html">156 hunch net-2006-02-11-Yahoo&#8217;s Learning Problems.</a></p>
<p>18 0.72129643 <a title="55-lda-18" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>19 0.72116613 <a title="55-lda-19" href="../hunch_net-2007/hunch_net-2007-10-17-Online_as_the_new_adjective.html">267 hunch net-2007-10-17-Online as the new adjective</a></p>
<p>20 0.72023189 <a title="55-lda-20" href="../hunch_net-2007/hunch_net-2007-01-02-Retrospective.html">225 hunch net-2007-01-02-Retrospective</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
