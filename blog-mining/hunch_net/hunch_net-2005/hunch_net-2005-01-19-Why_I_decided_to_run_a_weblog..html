<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1 hunch net-2005-01-19-Why I decided to run a weblog.</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-1" href="#">hunch_net-2005-1</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1 hunch net-2005-01-19-Why I decided to run a weblog.</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-1-html" href="http://hunch.net/?p=4">html</a></p><p>Introduction: I have decided to run a weblog on machine learning and learning theory research.  Here are some reasons:
 
1) Weblogs enable new functionality:
  
 Public comment on papers.  No mechanism for this exists at conferences and most journals.  I have encountered it once for a  science  paper.   Some communities have mailing lists supporting this, but not machine learning or learning theory.  I have often read papers and found myself wishing there was some method to consider other’s questions and read the replies. 
 Conference shortlists.  One of the most common conversations at a conference is “what did you find interesting?”  There is no explicit mechanism for sharing this information at conferences, and it’s easy to imagine that it would be handy to do so. 
 Evaluation and comment on research directions.  Papers are almost exclusively about new research, rather than evaluation (and consideration) of research directions.  This last role is satisfied by funding agencies to some extent, but</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 I have decided to run a weblog on machine learning and learning theory research. [sent-1, score-0.284]
</p><p>2 Some communities have mailing lists supporting this, but not machine learning or learning theory. [sent-5, score-0.338]
</p><p>3 I have often read papers and found myself wishing there was some method to consider other’s questions and read the replies. [sent-6, score-0.213]
</p><p>4 ”  There is no explicit mechanism for sharing this information at conferences, and it’s easy to imagine that it would be handy to do so. [sent-9, score-0.436]
</p><p>5 Papers are almost exclusively about new research, rather than evaluation (and consideration) of research directions. [sent-11, score-0.236]
</p><p>6 It’s easy to imagine that a public debate would be more thorough and thoughtful, producing better decisions. [sent-13, score-0.338]
</p><p>7 It may be feasible to use a weblog as a mechanism for public research on a scale less than a paper. [sent-15, score-0.626]
</p><p>8 Weblogs provide a natural generalization where anyone who is interested may be able to contribute. [sent-17, score-0.181]
</p><p>9 Weblogs provide new capabilities, and it is natural to miss the impact of these capabilities until a number of people have thought about and used them. [sent-19, score-0.296]
</p><p>10 mechanism   speed   scope   permanency   information filtration       journal papers   6 months to years. [sent-23, score-0.918]
</p><p>11 Very permanent   reviewed       conference papers   4-6 months   Attendees (and often any with interest). [sent-25, score-0.669]
</p><p>12 Permanent   reviewed       workshops   1-6 months   Attendees   Typically Transient   inspected       mailing lists   a few days   Anyone subscribed (or reading archives). [sent-26, score-0.794]
</p><p>13 Semipermanent (with archives)   inspected       personal discussion   thought speed   Whoever is there then. [sent-27, score-0.4]
</p><p>14 Transient   not reviewed       weblog   thought speed   Anyone with interest   Semipermaent   not reviewed       Weblogs achieve “best we can imagine” in every category except permanency and quality control. [sent-28, score-1.166]
</p><p>15 Permalinks  are the equivalent of a citation, providing a semipermanent pointer to a piece of content. [sent-30, score-0.278]
</p><p>16 This is only ‘semi’ becuase the _author_ of the content can typically revise the content at any moment in the future and the pointer is only permanet up to the permanence of the website. [sent-31, score-0.34]
</p><p>17 Trackback  is an explicit method for creating the reverse lookup table of citations: who cites this? [sent-32, score-0.225]
</p><p>18 In addition, there are several mechanisms for information filtration such as “post is reposted in another weblog” and experimental moderation schemes. [sent-33, score-0.379]
</p><p>19 The same forces driving academia into desiring permanent indelible records and very careful information filtration exist for blogs. [sent-34, score-0.677]
</p><p>20 These forces may produce the ‘missing pieces’, making weblogs very compelling for academic purposes. [sent-35, score-0.567]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('weblogs', 0.425), ('weblog', 0.284), ('filtration', 0.213), ('reviewed', 0.201), ('permanent', 0.177), ('inspected', 0.16), ('permanency', 0.16), ('semipermanent', 0.16), ('transient', 0.16), ('months', 0.147), ('public', 0.144), ('archives', 0.142), ('forces', 0.142), ('speed', 0.122), ('mechanism', 0.12), ('pointer', 0.118), ('thought', 0.118), ('anyone', 0.117), ('capabilities', 0.114), ('attendees', 0.11), ('lists', 0.11), ('mailing', 0.11), ('debate', 0.103), ('imagine', 0.091), ('evaluation', 0.087), ('information', 0.083), ('explicit', 0.083), ('mechanisms', 0.083), ('content', 0.08), ('interest', 0.08), ('research', 0.078), ('comment', 0.077), ('papers', 0.073), ('conference', 0.071), ('exclusively', 0.071), ('fortnow', 0.071), ('lookup', 0.071), ('reverse', 0.071), ('read', 0.07), ('subscribed', 0.066), ('whoever', 0.066), ('provide', 0.064), ('records', 0.062), ('revise', 0.062), ('weaknesses', 0.062), ('communicating', 0.059), ('communities', 0.059), ('sharing', 0.059), ('supporting', 0.059), ('thoughtful', 0.059)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="1-tfidf-1" href="../hunch_net-2005/hunch_net-2005-01-19-Why_I_decided_to_run_a_weblog..html">1 hunch net-2005-01-19-Why I decided to run a weblog.</a></p>
<p>Introduction: I have decided to run a weblog on machine learning and learning theory research.  Here are some reasons:
 
1) Weblogs enable new functionality:
  
 Public comment on papers.  No mechanism for this exists at conferences and most journals.  I have encountered it once for a  science  paper.   Some communities have mailing lists supporting this, but not machine learning or learning theory.  I have often read papers and found myself wishing there was some method to consider other’s questions and read the replies. 
 Conference shortlists.  One of the most common conversations at a conference is “what did you find interesting?”  There is no explicit mechanism for sharing this information at conferences, and it’s easy to imagine that it would be handy to do so. 
 Evaluation and comment on research directions.  Papers are almost exclusively about new research, rather than evaluation (and consideration) of research directions.  This last role is satisfied by funding agencies to some extent, but</p><p>2 0.11793315 <a title="1-tfidf-2" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>Introduction: Unfortunately, I ended up sick for much of this ICML.  I did manage to catch one interesting paper:
 
 Richard Socher ,  Cliff Lin ,  Andrew Y. Ng , and  Christopher D. Manning   Parsing Natural Scenes and Natural Language with Recursive Neural Networks .
 
I invited Richard to share his list of interesting papers, so hopefully we’ll hear from him soon.  In the meantime,  Paul  and  Hal  have posted some lists.
 
 the future 
 
 Joelle  and I are program chairs for ICML 2012 in  Edinburgh , which I previously enjoyed visiting in  2005 .  This is a huge responsibility, that we hope to accomplish well.  A part of this (perhaps the most fun part), is imagining how we can make ICML better.  A key and critical constraint is choosing things that can be accomplished.  So far we have:
  
  Colocation . The first thing we looked into was potential colocations.  We quickly discovered that many other conferences precomitted their location.  For the future, getting a colocation with  ACL  or  SIGI</p><p>3 0.11159392 <a title="1-tfidf-3" href="../hunch_net-2005/hunch_net-2005-02-25-Solution%3A_Reinforcement_Learning_with_Classification.html">29 hunch net-2005-02-25-Solution: Reinforcement Learning with Classification</a></p>
<p>Introduction: I realized that the tools needed to solve the  problem just posted  were just created.  I tried to sketch out the solution  here  (also in  .lyx  and  .tex ).  It is still quite sketchy (and probably only the few people who understand reductions well can follow).
 
One of the reasons why I started this weblog was to experiment with “research in the open”, and this is an opportunity to do so.  Over the next few days, I’ll be filling in details and trying to get things to make sense.  If you have additions or ideas, please propose them.</p><p>4 0.10597612 <a title="1-tfidf-4" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>Introduction: I want to try to describe what doing research means, especially from the point of view of an undergraduate.  The shift from a class-taking mentality to a research mentality is very significant and not easy.  
  
  Problem Posing  Posing the right problem is often as important as solving them.  Many people can get by in research by solving problems others have posed, but that’s not sufficient for really inspiring research.  For learning in particular, there is a strong feeling that we just haven’t figured out which questions are the right ones to ask.  You can see this, because the answers we have do not seem convincing. 
  Gambling your life  When you do research, you think very hard about new ways of solving problems, new problems, and new solutions.  Many conversations are of the form “I wonder what would happen if…” These processes can be short (days or weeks) or years-long endeavours.  The worst part is that you’ll only know if you were succesful at the end of the process (and some</p><p>5 0.096406303 <a title="1-tfidf-5" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>Introduction: Conferences exist as part of the process of doing research.  They provide many roles including “announcing research”, “meeting people”, and  “point of reference”.  Not all conferences are alike so a basic question is: “to what extent do individual conferences attempt to aid research?”  This question is very difficult to answer in any satisfying way.  What we can do is compare details of the process across multiple conferences.
  
  Comments   The average quality of comments across conferences can vary dramatically.  At one extreme, the tradition in CS theory conferences is to provide essentially zero feedback.  At the other extreme, some conferences have a strong tradition of providing detailed constructive feedback.  Detailed feedback can give authors significant guidance about how to improve research.  This is the most subjective entry. 
  Blind  Virtually all conferences offer single blind review where authors do not know reviewers.  Some also provide  double blind  review where rev</p><p>6 0.095410451 <a title="1-tfidf-6" href="../hunch_net-2005/hunch_net-2005-02-25-Why_Papers%3F.html">30 hunch net-2005-02-25-Why Papers?</a></p>
<p>7 0.094155602 <a title="1-tfidf-7" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>8 0.093142457 <a title="1-tfidf-8" href="../hunch_net-2008/hunch_net-2008-04-21-The_Science_2.0_article.html">296 hunch net-2008-04-21-The Science 2.0 article</a></p>
<p>9 0.085451066 <a title="1-tfidf-9" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>10 0.085070267 <a title="1-tfidf-10" href="../hunch_net-2009/hunch_net-2009-02-16-KDNuggets.html">342 hunch net-2009-02-16-KDNuggets</a></p>
<p>11 0.084844172 <a title="1-tfidf-11" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>12 0.079720482 <a title="1-tfidf-12" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>13 0.079699256 <a title="1-tfidf-13" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>14 0.079623379 <a title="1-tfidf-14" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>15 0.07742741 <a title="1-tfidf-15" href="../hunch_net-2006/hunch_net-2006-09-18-What_is_missing_for_online_collaborative_research%3F.html">208 hunch net-2006-09-18-What is missing for online collaborative research?</a></p>
<p>16 0.077032708 <a title="1-tfidf-16" href="../hunch_net-2008/hunch_net-2008-11-10-ICML_Reviewing_Criteria.html">325 hunch net-2008-11-10-ICML Reviewing Criteria</a></p>
<p>17 0.07561969 <a title="1-tfidf-17" href="../hunch_net-2005/hunch_net-2005-03-29-Academic_Mechanism_Design.html">48 hunch net-2005-03-29-Academic Mechanism Design</a></p>
<p>18 0.073084421 <a title="1-tfidf-18" href="../hunch_net-2012/hunch_net-2012-06-29-ICML_survey_and_comments.html">468 hunch net-2012-06-29-ICML survey and comments</a></p>
<p>19 0.071938314 <a title="1-tfidf-19" href="../hunch_net-2005/hunch_net-2005-03-05-Funding_Research.html">36 hunch net-2005-03-05-Funding Research</a></p>
<p>20 0.071725465 <a title="1-tfidf-20" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.169), (1, -0.089), (2, -0.04), (3, 0.061), (4, -0.045), (5, 0.042), (6, 0.062), (7, -0.008), (8, 0.022), (9, 0.056), (10, -0.025), (11, 0.012), (12, 0.002), (13, -0.006), (14, 0.022), (15, 0.008), (16, -0.019), (17, 0.0), (18, 0.007), (19, -0.004), (20, 0.032), (21, -0.009), (22, -0.022), (23, -0.031), (24, -0.021), (25, -0.052), (26, -0.009), (27, 0.082), (28, -0.046), (29, -0.051), (30, 0.001), (31, 0.077), (32, 0.058), (33, -0.019), (34, -0.005), (35, -0.014), (36, 0.002), (37, 0.037), (38, 0.01), (39, 0.026), (40, -0.041), (41, 0.042), (42, 0.034), (43, -0.024), (44, 0.012), (45, -0.062), (46, -0.06), (47, 0.014), (48, 0.052), (49, 0.01)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95451623 <a title="1-lsi-1" href="../hunch_net-2005/hunch_net-2005-01-19-Why_I_decided_to_run_a_weblog..html">1 hunch net-2005-01-19-Why I decided to run a weblog.</a></p>
<p>Introduction: I have decided to run a weblog on machine learning and learning theory research.  Here are some reasons:
 
1) Weblogs enable new functionality:
  
 Public comment on papers.  No mechanism for this exists at conferences and most journals.  I have encountered it once for a  science  paper.   Some communities have mailing lists supporting this, but not machine learning or learning theory.  I have often read papers and found myself wishing there was some method to consider other’s questions and read the replies. 
 Conference shortlists.  One of the most common conversations at a conference is “what did you find interesting?”  There is no explicit mechanism for sharing this information at conferences, and it’s easy to imagine that it would be handy to do so. 
 Evaluation and comment on research directions.  Papers are almost exclusively about new research, rather than evaluation (and consideration) of research directions.  This last role is satisfied by funding agencies to some extent, but</p><p>2 0.7475161 <a title="1-lsi-2" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>Introduction: The internet has significantly effected the way we do research but it’s capabilities have not yet been fully realized.
 
First, let’s acknowledge some known effects.
  
  Self-publishing  By default, all researchers in machine learning (and more generally computer science and physics) place their papers online for anyone to download.  The exact mechanism differs—physicists tend to use a central repository ( Arxiv ) while computer scientists tend to place the papers on their webpage.  Arxiv has been slowly growing in subject breadth so it now sometimes used by computer scientists. 
  Collaboration  Email has enabled working remotely with coauthors.  This has allowed collaborationis which would not otherwise have been possible and generally speeds research. 
  
Now, let’s look at attempts to go further.
  
  Blogs  (like this one) allow public discussion about topics which are not easily categorized as “a new idea in machine learning” (like this topic). 
  Organization  of some subfield</p><p>3 0.69138312 <a title="1-lsi-3" href="../hunch_net-2006/hunch_net-2006-09-18-What_is_missing_for_online_collaborative_research%3F.html">208 hunch net-2006-09-18-What is missing for online collaborative research?</a></p>
<p>Introduction: The internet has recently made the research process much smoother: papers are easy to obtain, citations are easy to follow, and unpublished “tutorials” are often available. Yet, new research fields can look very complicated to outsiders or newcomers. Every paper is like a small piece of an unfinished jigsaw puzzle: to understand just one publication, a researcher without experience in the field will typically have to follow several layers of citations, and many of the papers he encounters have a great deal of repeated information. Furthermore, from one publication to the next, notation and terminology may not be consistent which can further confuse the reader.
 
But the internet is now proving to be an extremely useful medium for collaboration and knowledge aggregation. Online forums allow users to ask and answer questions and to share ideas. The recent phenomenon of Wikipedia provides a proof-of-concept for the “anyone can edit” system. Can such models be used to facilitate research a</p><p>4 0.6589973 <a title="1-lsi-4" href="../hunch_net-2005/hunch_net-2005-02-25-Why_Papers%3F.html">30 hunch net-2005-02-25-Why Papers?</a></p>
<p>Introduction: Makc asked a good  question  in comments—”Why bother to make a paper, at all?”  There are several reasons for writing papers which may not be immediately obvious to people not in academia.
 
The basic idea is that papers have considerably more utility than the obvious “present an idea”.
  
 Papers are a formalized units of work. Academics (especially young ones) are often judged on the number of papers they produce. 
 Papers have a formalized method of citing and crediting other—the bibliography.  Academics (especially older ones) are often judged on the number of citations they receive. 
 Papers enable a “more fair” anonymous review.  Conferences receive  many  papers, from which a subset are selected.  Discussion forums are inherently not anonymous for anyone who wants to build a reputation for good work. 
 Papers are an excuse to meet your friends.  Papers are the content of conferences, but much of what you do is talk to friends about interesting problems while there.  Sometimes yo</p><p>5 0.65594596 <a title="1-lsi-5" href="../hunch_net-2007/hunch_net-2007-02-16-The_Forgetting.html">233 hunch net-2007-02-16-The Forgetting</a></p>
<p>Introduction: How many papers do you remember from 2006? 2005? 2002? 1997? 1987? 1967?  One way to judge this would be to look at the citations of the papers you write—how many came from which year?  For myself, the answers on recent papers are:
  
 
 year 
 2006 
 2005 
 2002 
 1997 
 1987 
 1967 
 
 
 count 
 4 
 10 
 5 
 1 
 0 
 0 
 
  
This spectrum is fairly typical of papers in general.  There are many reasons that citations are focused on recent papers.
  
 The number of papers being published continues to grow.  This is not a very significant effect, because the rate of publication has not grown nearly as fast. 
 Dead men don’t reject your papers for not citing them.  This reason seems lame, because it’s a distortion from the ideal of science.  Nevertheless, it must be stated because the effect can be significant. 
 In 1997, I started as a PhD student.  Naturally, papers after 1997 are better remembered because they were absorbed in real time.  A large fraction of people writing papers and a</p><p>6 0.65455723 <a title="1-lsi-6" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>7 0.61972225 <a title="1-lsi-7" href="../hunch_net-2008/hunch_net-2008-04-21-The_Science_2.0_article.html">296 hunch net-2008-04-21-The Science 2.0 article</a></p>
<p>8 0.59994435 <a title="1-lsi-8" href="../hunch_net-2005/hunch_net-2005-03-29-Academic_Mechanism_Design.html">48 hunch net-2005-03-29-Academic Mechanism Design</a></p>
<p>9 0.59768355 <a title="1-lsi-9" href="../hunch_net-2008/hunch_net-2008-02-10-Complexity_Illness.html">288 hunch net-2008-02-10-Complexity Illness</a></p>
<p>10 0.59282559 <a title="1-lsi-10" href="../hunch_net-2007/hunch_net-2007-02-10-Best_Practices_for_Collaboration.html">231 hunch net-2007-02-10-Best Practices for Collaboration</a></p>
<p>11 0.59134007 <a title="1-lsi-11" href="../hunch_net-2006/hunch_net-2006-01-06-MLTV.html">146 hunch net-2006-01-06-MLTV</a></p>
<p>12 0.58854097 <a title="1-lsi-12" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>13 0.58220589 <a title="1-lsi-13" href="../hunch_net-2007/hunch_net-2007-11-02-The_Machine_Learning_Award_goes_to_%26%238230%3B.html">270 hunch net-2007-11-02-The Machine Learning Award goes to &#8230;</a></p>
<p>14 0.57830638 <a title="1-lsi-14" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>15 0.57449096 <a title="1-lsi-15" href="../hunch_net-2005/hunch_net-2005-09-04-Science_in_the_Government.html">106 hunch net-2005-09-04-Science in the Government</a></p>
<p>16 0.57341844 <a title="1-lsi-16" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>17 0.56962317 <a title="1-lsi-17" href="../hunch_net-2006/hunch_net-2006-08-10-Precision_is_not_accuracy.html">202 hunch net-2006-08-10-Precision is not accuracy</a></p>
<p>18 0.56623203 <a title="1-lsi-18" href="../hunch_net-2009/hunch_net-2009-06-01-Multitask_Poisoning.html">358 hunch net-2009-06-01-Multitask Poisoning</a></p>
<p>19 0.56322515 <a title="1-lsi-19" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>20 0.56181628 <a title="1-lsi-20" href="../hunch_net-2005/hunch_net-2005-03-17-Going_all_the_Way%2C_Sometimes.html">42 hunch net-2005-03-17-Going all the Way, Sometimes</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(3, 0.03), (13, 0.027), (27, 0.122), (29, 0.098), (37, 0.187), (48, 0.022), (53, 0.074), (55, 0.087), (58, 0.021), (68, 0.021), (89, 0.01), (94, 0.095), (95, 0.079)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.91748959 <a title="1-lda-1" href="../hunch_net-2005/hunch_net-2005-01-19-Why_I_decided_to_run_a_weblog..html">1 hunch net-2005-01-19-Why I decided to run a weblog.</a></p>
<p>Introduction: I have decided to run a weblog on machine learning and learning theory research.  Here are some reasons:
 
1) Weblogs enable new functionality:
  
 Public comment on papers.  No mechanism for this exists at conferences and most journals.  I have encountered it once for a  science  paper.   Some communities have mailing lists supporting this, but not machine learning or learning theory.  I have often read papers and found myself wishing there was some method to consider other’s questions and read the replies. 
 Conference shortlists.  One of the most common conversations at a conference is “what did you find interesting?”  There is no explicit mechanism for sharing this information at conferences, and it’s easy to imagine that it would be handy to do so. 
 Evaluation and comment on research directions.  Papers are almost exclusively about new research, rather than evaluation (and consideration) of research directions.  This last role is satisfied by funding agencies to some extent, but</p><p>2 0.88113725 <a title="1-lda-2" href="../hunch_net-2011/hunch_net-2011-04-18-A_paper_not_at_Snowbird.html">431 hunch net-2011-04-18-A paper not at Snowbird</a></p>
<p>Introduction: Unfortunately, a scheduling failure meant I missed all of  AIStat  and most of the  learning workshop , otherwise known as Snowbird, when it’s at  Snowbird .  
 
At snowbird, the talk on  Sum-Product  networks by  Hoifung Poon  stood out to me ( Pedro Domingos  is a coauthor.).  The basic point was that by appropriately constructing networks based on sums and products, the normalization problem in probabilistic models is eliminated, yielding a highly tractable yet flexible representation+learning algorithm.  As an algorithm, this is noticeably cleaner than deep belief networks with a claim to being an order of magnitude faster and working better on an image completion task.
 
Snowbird doesn’t have real papers—just the abstract above.  I look forward to seeing the paper.  (added: Rodrigo points out the deep learning workshop  draft .)</p><p>3 0.85813165 <a title="1-lda-3" href="../hunch_net-2005/hunch_net-2005-04-27-DARPA_project%3A_LAGR.html">63 hunch net-2005-04-27-DARPA project: LAGR</a></p>
<p>Introduction: Larry Jackal has set up the  LAGR  (“Learning Applied to Ground Robotics”) project (and competition) which seems to be quite well designed.  Features include:
  
 Many participants (8 going on 12?) 
 Standardized hardware.  In the  DARPA grand challenge  contestants entering with motorcycles are at a severe disadvantage to those entering with a Hummer.  Similarly, contestants using more powerful sensors can gain huge advantages. 
 Monthly contests, with full feedback (but since the hardware is standardized, only code is shipped).  One of the premises of the program is that robust systems are desired.  Monthly evaluations at different locations can help measure this and provide data. 
 Attacks a known hard problem.  (cross country driving)</p><p>4 0.78342408 <a title="1-lda-4" href="../hunch_net-2005/hunch_net-2005-12-09-Some_NIPS_papers.html">138 hunch net-2005-12-09-Some NIPS papers</a></p>
<p>Introduction: Here is a set of papers that I found interesting (and why).
  
  A PAC-Bayes approach to the Set Covering Machine  improves the set covering machine.  The set covering machine approach is a new way to do classification characterized by a very close connection between theory and algorithm.  At this point, the approach seems to be competing well with SVMs in about all dimensions: similar computational speed, similar accuracy, stronger learning theory guarantees, more general information source (a kernel has strictly more structure than a metric), and more sparsity.  Developing a classification algorithm is not very easy, but the results so far are encouraging. 
  Off-Road Obstacle Avoidance through End-to-End Learning  and  Learning Depth from Single Monocular Images  both effectively showed that depth information can be predicted from camera images (using notably different techniques).  This ability is strongly enabling because cameras are cheap, tiny, light, and potentially provider lo</p><p>5 0.75301939 <a title="1-lda-5" href="../hunch_net-2009/hunch_net-2009-08-26-Another_10-year_paper_in_Machine_Learning.html">368 hunch net-2009-08-26-Another 10-year paper in Machine Learning</a></p>
<p>Introduction: When I was thinking about the best “10 year paper” for  ICML , I also took a look at a few other conferences.  Here is one from 10 years ago that interested me:
 
 David McAllester   PAC-Bayesian Model Averaging ,  COLT  1999.    2001 Journal Draft . 
 
Prior to this paper, the only mechanism known for controlling or estimating the necessary sample complexity for learning over continuously parameterized predictors was VC theory and variants, all of which suffered from a basic problem: they were incredibly pessimistic in practice.  This meant that only very gross guidance could be provided for learning algorithm design.  The PAC-Bayes bound provided an alternative approach to sample complexity bounds which was radically tighter, quantitatively.  It also imported and explained many of the motivations for Bayesian learning in a way that learning theory and perhaps optimization people might appreciate.  Since this paper came out, there have been a number of moderately successful attempts t</p><p>6 0.70045519 <a title="1-lda-6" href="../hunch_net-2005/hunch_net-2005-02-17-Learning_Research_Programs.html">21 hunch net-2005-02-17-Learning Research Programs</a></p>
<p>7 0.68359554 <a title="1-lda-7" href="../hunch_net-2006/hunch_net-2006-07-11-New_Models.html">194 hunch net-2006-07-11-New Models</a></p>
<p>8 0.66103679 <a title="1-lda-8" href="../hunch_net-2009/hunch_net-2009-05-30-Many_ways_to_Learn_this_summer.html">357 hunch net-2009-05-30-Many ways to Learn this summer</a></p>
<p>9 0.65841073 <a title="1-lda-9" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>10 0.6528396 <a title="1-lda-10" href="../hunch_net-2005/hunch_net-2005-02-25-Problem%3A_Online_Learning.html">28 hunch net-2005-02-25-Problem: Online Learning</a></p>
<p>11 0.64268416 <a title="1-lda-11" href="../hunch_net-2011/hunch_net-2011-09-28-Somebody%26%238217%3Bs_Eating_Your_Lunch.html">445 hunch net-2011-09-28-Somebody&#8217;s Eating Your Lunch</a></p>
<p>12 0.64132965 <a title="1-lda-12" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<p>13 0.64113724 <a title="1-lda-13" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>14 0.63988847 <a title="1-lda-14" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<p>15 0.63846749 <a title="1-lda-15" href="../hunch_net-2010/hunch_net-2010-10-29-To_Vidoelecture_or_not.html">416 hunch net-2010-10-29-To Vidoelecture or not</a></p>
<p>16 0.63739401 <a title="1-lda-16" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>17 0.63456875 <a title="1-lda-17" href="../hunch_net-2006/hunch_net-2006-01-06-MLTV.html">146 hunch net-2006-01-06-MLTV</a></p>
<p>18 0.63415754 <a title="1-lda-18" href="../hunch_net-2005/hunch_net-2005-05-28-Running_A_Machine_Learning_Summer_School.html">75 hunch net-2005-05-28-Running A Machine Learning Summer School</a></p>
<p>19 0.63377517 <a title="1-lda-19" href="../hunch_net-2012/hunch_net-2012-04-09-ICML_author_feedback_is_open.html">461 hunch net-2012-04-09-ICML author feedback is open</a></p>
<p>20 0.63251877 <a title="1-lda-20" href="../hunch_net-2006/hunch_net-2006-01-25-1_year.html">151 hunch net-2006-01-25-1 year</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
