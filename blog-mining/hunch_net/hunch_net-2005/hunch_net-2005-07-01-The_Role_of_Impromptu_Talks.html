<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>88 hunch net-2005-07-01-The Role of Impromptu Talks</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-88" href="#">hunch_net-2005-88</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>88 hunch net-2005-07-01-The Role of Impromptu Talks</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-88-html" href="http://hunch.net/?p=94">html</a></p><p>Introduction: COLThad an impromptu session which seemed as interesting or more interesting
than any other single technical session (despite being only an hour long).
There are several roles that an impromptu session can play
including:Announcing new work since the paper deadline. Letting this happen
now rather than later helps aid the process of research.Discussing a paper
that was rejected. Reviewers err sometimes and an impromptu session provides a
means to remedy that.Entertainment. We all like to have a bit of fun.For
design, the following seem important:Impromptu speakers should not have much
time. At COLT, it was 8 minutes, but I have seen even 5 work well.The entire
impromptu session should not last too long because the format is dense and
promotes restlessness. A half hour or hour can work well.Impromptu talks are a
mechanism to let a little bit of chaos into the schedule. They will be chaotic
in content, presentation, and usefulness. The fundamental advantage of this
chaos is that it provid</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 COLThad an impromptu session which seemed as interesting or more interesting than any other single technical session (despite being only an hour long). [sent-1, score-2.061]
</p><p>2 There are several roles that an impromptu session can play including:Announcing new work since the paper deadline. [sent-2, score-1.326]
</p><p>3 Letting this happen now rather than later helps aid the process of research. [sent-3, score-0.284]
</p><p>4 Reviewers err sometimes and an impromptu session provides a means to remedy that. [sent-5, score-1.402]
</p><p>5 For design, the following seem important:Impromptu speakers should not have much time. [sent-8, score-0.081]
</p><p>6 At COLT, it was 8 minutes, but I have seen even 5 work well. [sent-9, score-0.134]
</p><p>7 The entire impromptu session should not last too long because the format is dense and promotes restlessness. [sent-10, score-1.452]
</p><p>8 Impromptu talks are a mechanism to let a little bit of chaos into the schedule. [sent-12, score-0.546]
</p><p>9 The fundamental advantage of this chaos is that it provides a means for covering material that the planned program did not (or could not). [sent-14, score-0.839]
</p><p>10 This seems like a "bargain use of time" considering the short duration. [sent-15, score-0.126]
</p><p>11 One caveat is that it is unclear how well this mechanism can scale to large conferences. [sent-16, score-0.322]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('impromptu', 0.565), ('session', 0.421), ('hour', 0.305), ('chaos', 0.244), ('dense', 0.122), ('announcing', 0.113), ('planned', 0.113), ('remedy', 0.113), ('roles', 0.113), ('provides', 0.107), ('letting', 0.107), ('err', 0.107), ('promotes', 0.107), ('mechanism', 0.103), ('minutes', 0.102), ('caveat', 0.091), ('material', 0.091), ('means', 0.089), ('long', 0.089), ('half', 0.084), ('play', 0.082), ('work', 0.081), ('despite', 0.081), ('covering', 0.081), ('speakers', 0.081), ('aid', 0.079), ('later', 0.076), ('technical', 0.075), ('entire', 0.075), ('bit', 0.074), ('seemed', 0.073), ('format', 0.073), ('interesting', 0.073), ('unclear', 0.071), ('content', 0.07), ('presentation', 0.069), ('considering', 0.067), ('helps', 0.066), ('talks', 0.065), ('colt', 0.064), ('paper', 0.064), ('happen', 0.063), ('advantage', 0.062), ('let', 0.06), ('short', 0.059), ('scale', 0.057), ('reviewers', 0.057), ('single', 0.055), ('seen', 0.053), ('fundamental', 0.052)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="88-tfidf-1" href="../hunch_net-2005/hunch_net-2005-07-01-The_Role_of_Impromptu_Talks.html">88 hunch net-2005-07-01-The Role of Impromptu Talks</a></p>
<p>Introduction: COLThad an impromptu session which seemed as interesting or more interesting
than any other single technical session (despite being only an hour long).
There are several roles that an impromptu session can play
including:Announcing new work since the paper deadline. Letting this happen
now rather than later helps aid the process of research.Discussing a paper
that was rejected. Reviewers err sometimes and an impromptu session provides a
means to remedy that.Entertainment. We all like to have a bit of fun.For
design, the following seem important:Impromptu speakers should not have much
time. At COLT, it was 8 minutes, but I have seen even 5 work well.The entire
impromptu session should not last too long because the format is dense and
promotes restlessness. A half hour or hour can work well.Impromptu talks are a
mechanism to let a little bit of chaos into the schedule. They will be chaotic
in content, presentation, and usefulness. The fundamental advantage of this
chaos is that it provid</p><p>2 0.17003414 <a title="88-tfidf-2" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>Introduction: Unfortunately, I ended up sick for much of this ICML. I did manage to catch
one interesting paper:Richard Socher,Cliff Lin,Andrew Y. Ng, andChristopher D.
ManningParsing Natural Scenes and Natural Language with Recursive Neural
Networks.I invited Richard to share his list of interesting papers, so
hopefully we'll hear from him soon. In the meantime,PaulandHalhave posted some
lists.the futureJoelleand I are program chairs for ICML 2012 inEdinburgh,
which I previously enjoyed visiting in2005. This is a huge responsibility,
that we hope to accomplish well. A part of this (perhaps the most fun part),
is imagining how we can make ICML better. A key and critical constraint is
choosing things that can be accomplished. So far we have:Colocation. The first
thing we looked into was potential colocations. We quickly discovered that
many other conferences precomitted their location. For the future, getting a
colocation withACLorSIGIR, seems to require more advanced planning. If that
can be done, I</p><p>3 0.15740916 <a title="88-tfidf-3" href="../hunch_net-2011/hunch_net-2011-08-01-Interesting_papers_at_COLT_2011.html">439 hunch net-2011-08-01-Interesting papers at COLT 2011</a></p>
<p>Introduction: SinceJohndid not attendCOLTthis year, I have been volunteered to report back
on the hot stuff at this year's meeting. The conference seemed to have pretty
high quality stuff this year, and I found plenty of interesting papers on all
the three days. I'm gonna pick some of my favorites going through the program
in a chronological order.The first session on matrices seemed interesting for
two reasons. First, the papers were quite nice. But more interestingly, this
is a topic that has had a lot of presence in Statistics and Compressed sensing
literature recently. So it was good to see high-dimensional matrices finally
make their entry at COLT. The paper ofOhadandShaionCollaborative Filtering
with the Trace Norm: Learning, Bounding, and Transducingprovides non-trivial
guarantees on trace norm regularization in an agnostic setup, while Rina
andNatishow how Rademacher averages can be used to get sharper results for
matrix completion problems in their paperConcentration-Based Guarantees for
Lo</p><p>4 0.12191591 <a title="88-tfidf-4" href="../hunch_net-2013/hunch_net-2013-04-15-NEML_II.html">481 hunch net-2013-04-15-NEML II</a></p>
<p>Introduction: Adam Kalaipoints out theNew England Machine Learning DayMay 1 at MSR New
England. There is a poster session with abstracts due April 19. I understand
last year'sNEMLwent well and it's great to meet your neighbors at regional
workshops like this.</p><p>5 0.12138167 <a title="88-tfidf-5" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>Introduction: Founding a successful new conference is extraordinarily difficult. As a
conference founder, you must manage to attract a significant number of good
papers--enough to entice the participants into participating next year and to
(generally) to grow the conference. For someone choosing to participate in a
new conference, there is a very significant decision to make: do you send a
paper to some new conference with no guarantee that the conference will work
out? Or do you send it to another (possibly less related) conference that you
are sure will work?The conference founding problem is a joint agreement
problem with a very significant barrier. Workshops are a way around this
problem, and workshops attached to conferences are a particularly effective
means for this. A workshop at a conference is sure to have people available to
speak and attend and is sure to have a large audience available. Presenting
work at a workshop is not generally exclusive: it can also be presented at a
conference. F</p><p>6 0.096894793 <a title="88-tfidf-6" href="../hunch_net-2012/hunch_net-2012-10-18-7th_Annual_Machine_Learning_Symposium.html">474 hunch net-2012-10-18-7th Annual Machine Learning Symposium</a></p>
<p>7 0.092403777 <a title="88-tfidf-7" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<p>8 0.088514693 <a title="88-tfidf-8" href="../hunch_net-2012/hunch_net-2012-03-06-COLT-ICML_Open_Questions_and_ICML_Instructions.html">458 hunch net-2012-03-06-COLT-ICML Open Questions and ICML Instructions</a></p>
<p>9 0.087196387 <a title="88-tfidf-9" href="../hunch_net-2005/hunch_net-2005-05-28-Running_A_Machine_Learning_Summer_School.html">75 hunch net-2005-05-28-Running A Machine Learning Summer School</a></p>
<p>10 0.07858295 <a title="88-tfidf-10" href="../hunch_net-2008/hunch_net-2008-07-04-More_Presentation_Preparation.html">307 hunch net-2008-07-04-More Presentation Preparation</a></p>
<p>11 0.076868415 <a title="88-tfidf-11" href="../hunch_net-2005/hunch_net-2005-06-10-Workshops_are_not_Conferences.html">80 hunch net-2005-06-10-Workshops are not Conferences</a></p>
<p>12 0.07558354 <a title="88-tfidf-12" href="../hunch_net-2005/hunch_net-2005-07-10-Thinking_the_Unthought.html">91 hunch net-2005-07-10-Thinking the Unthought</a></p>
<p>13 0.075578749 <a title="88-tfidf-13" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>14 0.070583642 <a title="88-tfidf-14" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>15 0.070475377 <a title="88-tfidf-15" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>16 0.065431744 <a title="88-tfidf-16" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>17 0.064513586 <a title="88-tfidf-17" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>18 0.062244229 <a title="88-tfidf-18" href="../hunch_net-2006/hunch_net-2006-01-06-MLTV.html">146 hunch net-2006-01-06-MLTV</a></p>
<p>19 0.060614217 <a title="88-tfidf-19" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>20 0.059863217 <a title="88-tfidf-20" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.129), (1, 0.097), (2, -0.009), (3, 0.042), (4, 0.008), (5, -0.019), (6, -0.012), (7, -0.008), (8, -0.024), (9, 0.014), (10, 0.014), (11, 0.051), (12, 0.021), (13, -0.012), (14, -0.073), (15, 0.107), (16, 0.022), (17, 0.051), (18, 0.008), (19, 0.021), (20, -0.005), (21, 0.032), (22, 0.012), (23, -0.058), (24, 0.034), (25, 0.036), (26, -0.054), (27, -0.109), (28, -0.027), (29, 0.014), (30, -0.007), (31, -0.079), (32, -0.017), (33, 0.041), (34, 0.014), (35, -0.029), (36, 0.04), (37, 0.011), (38, -0.009), (39, 0.003), (40, 0.016), (41, 0.033), (42, -0.003), (43, 0.029), (44, -0.037), (45, -0.072), (46, 0.065), (47, 0.0), (48, -0.084), (49, 0.015)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96328914 <a title="88-lsi-1" href="../hunch_net-2005/hunch_net-2005-07-01-The_Role_of_Impromptu_Talks.html">88 hunch net-2005-07-01-The Role of Impromptu Talks</a></p>
<p>Introduction: COLThad an impromptu session which seemed as interesting or more interesting
than any other single technical session (despite being only an hour long).
There are several roles that an impromptu session can play
including:Announcing new work since the paper deadline. Letting this happen
now rather than later helps aid the process of research.Discussing a paper
that was rejected. Reviewers err sometimes and an impromptu session provides a
means to remedy that.Entertainment. We all like to have a bit of fun.For
design, the following seem important:Impromptu speakers should not have much
time. At COLT, it was 8 minutes, but I have seen even 5 work well.The entire
impromptu session should not last too long because the format is dense and
promotes restlessness. A half hour or hour can work well.Impromptu talks are a
mechanism to let a little bit of chaos into the schedule. They will be chaotic
in content, presentation, and usefulness. The fundamental advantage of this
chaos is that it provid</p><p>2 0.58731586 <a title="88-lsi-2" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>Introduction: Unfortunately, I ended up sick for much of this ICML. I did manage to catch
one interesting paper:Richard Socher,Cliff Lin,Andrew Y. Ng, andChristopher D.
ManningParsing Natural Scenes and Natural Language with Recursive Neural
Networks.I invited Richard to share his list of interesting papers, so
hopefully we'll hear from him soon. In the meantime,PaulandHalhave posted some
lists.the futureJoelleand I are program chairs for ICML 2012 inEdinburgh,
which I previously enjoyed visiting in2005. This is a huge responsibility,
that we hope to accomplish well. A part of this (perhaps the most fun part),
is imagining how we can make ICML better. A key and critical constraint is
choosing things that can be accomplished. So far we have:Colocation. The first
thing we looked into was potential colocations. We quickly discovered that
many other conferences precomitted their location. For the future, getting a
colocation withACLorSIGIR, seems to require more advanced planning. If that
can be done, I</p><p>3 0.55142909 <a title="88-lsi-3" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<p>Introduction: ByShieandNatiFollowing John's advertisement for submitting to ICML, we thought
it appropriate to highlight the advantages of COLT, and the reasons it is
often the best place for theory papers. We would like to emphasize that we
both respect ICML, and are active in ICML, both as authors and as area chairs,
and certainly are not arguing that ICML is a bad place for your papers. For
many papers, ICML is the best venue. But for many theory papers, COLT is a
better and more appropriate place.Why should you submit to COLT?By-and-large,
theory papers go to COLT. This is the tradition of the field and most theory
papers are sent to COLT. This is the place to present your ground-breaking
theorems and new models that will shape the theory of machine learning. COLT
is more focused then ICML with a single track session. Unlike ICML, the norm
in COLT is for people to sit through most sessions, and hear most of the talks
presented. There is also often a lively discussion following paper
presentation</p><p>4 0.54755783 <a title="88-lsi-4" href="../hunch_net-2007/hunch_net-2007-07-20-Motivation_should_be_the_Responsibility_of_the_Reviewer.html">256 hunch net-2007-07-20-Motivation should be the Responsibility of the Reviewer</a></p>
<p>Introduction: The prevailing wisdom in machine learning seems to be that motivating a paper
is the responsibility of the author. I think this is a harmful view--instead,
it's healthier for the community to regard this as the responsibility of the
reviewer.There are lots of reasons to prefer a reviewer-responsibility
approach.Authors are the most biased possible source of information about the
motivation of the paper. Systems which rely upon very biased sources of
information are inherently unreliable.Authors are highly variable in their
ability and desire to express motivation for their work. This adds greatly to
variance on acceptance of an idea, and it can systematically discriminate or
accentuate careers. It's great if you have a career accentuated by awesome
wording choice, but wise decision making by reviewers is important for the
field.The motivation section in a paper doesn'tdoanything in some sense--it's
there to get the paper in. Reading the motivation of a paper is of little use
in helping</p><p>5 0.539666 <a title="88-lsi-5" href="../hunch_net-2008/hunch_net-2008-11-09-A_Healthy__COLT.html">324 hunch net-2008-11-09-A Healthy  COLT</a></p>
<p>Introduction: Awhile ago, we discussed the health ofCOLT.COLT 2008substantially addressed my
concerns. The papers were diverse and several were interesting. Attendance was
up, which is particularly notable in Europe. In my opinion, the colocation
with UAI and ICML was the best colocation since 1998.And, perhaps best of all,
registration ended up being free for all students due to various grants from
theAcademy of Finland,Google,IBM, andYahoo.A basic question is: what went
right? There seem to be several answers.Cost-wise, COLT had sufficient grants
to alleviate the high cost of the Euro and location at a university
substantially reduces the cost compared to a hotel.Organization-wise, the
Finns were great with hordes of volunteers helping set everything up. Having
too many volunteers is a good failure mode.Organization-wise, it was clear
that all 3 program chairs were cooperating in designing the program
.Facilities-wise, proximity in time and space made the colocation much more
real than many others</p><p>6 0.53045011 <a title="88-lsi-6" href="../hunch_net-2011/hunch_net-2011-08-01-Interesting_papers_at_COLT_2011.html">439 hunch net-2011-08-01-Interesting papers at COLT 2011</a></p>
<p>7 0.52327931 <a title="88-lsi-7" href="../hunch_net-2006/hunch_net-2006-07-26-Two_more_UAI_papers_of_interest.html">199 hunch net-2006-07-26-Two more UAI papers of interest</a></p>
<p>8 0.51849711 <a title="88-lsi-8" href="../hunch_net-2006/hunch_net-2006-12-04-Structural_Problems_in_NIPS_Decision_Making.html">221 hunch net-2006-12-04-Structural Problems in NIPS Decision Making</a></p>
<p>9 0.4979423 <a title="88-lsi-9" href="../hunch_net-2005/hunch_net-2005-07-04-The_Health_of_COLT.html">89 hunch net-2005-07-04-The Health of COLT</a></p>
<p>10 0.4977704 <a title="88-lsi-10" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>11 0.49641183 <a title="88-lsi-11" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>12 0.49394706 <a title="88-lsi-12" href="../hunch_net-2008/hunch_net-2008-07-04-More_Presentation_Preparation.html">307 hunch net-2008-07-04-More Presentation Preparation</a></p>
<p>13 0.49068102 <a title="88-lsi-13" href="../hunch_net-2005/hunch_net-2005-02-25-Why_Papers%3F.html">30 hunch net-2005-02-25-Why Papers?</a></p>
<p>14 0.48703858 <a title="88-lsi-14" href="../hunch_net-2006/hunch_net-2006-01-08-Debugging_Your_Brain.html">147 hunch net-2006-01-08-Debugging Your Brain</a></p>
<p>15 0.48586398 <a title="88-lsi-15" href="../hunch_net-2005/hunch_net-2005-05-28-Running_A_Machine_Learning_Summer_School.html">75 hunch net-2005-05-28-Running A Machine Learning Summer School</a></p>
<p>16 0.4854528 <a title="88-lsi-16" href="../hunch_net-2007/hunch_net-2007-02-16-The_Forgetting.html">233 hunch net-2007-02-16-The Forgetting</a></p>
<p>17 0.480755 <a title="88-lsi-17" href="../hunch_net-2011/hunch_net-2011-10-10-ML_Symposium_and_ICML_details.html">447 hunch net-2011-10-10-ML Symposium and ICML details</a></p>
<p>18 0.47252312 <a title="88-lsi-18" href="../hunch_net-2007/hunch_net-2007-02-10-Best_Practices_for_Collaboration.html">231 hunch net-2007-02-10-Best Practices for Collaboration</a></p>
<p>19 0.47201177 <a title="88-lsi-19" href="../hunch_net-2008/hunch_net-2008-10-14-Who_is_Responsible_for_a_Bad_Review%3F.html">320 hunch net-2008-10-14-Who is Responsible for a Bad Review?</a></p>
<p>20 0.46935323 <a title="88-lsi-20" href="../hunch_net-2005/hunch_net-2005-07-10-Thinking_the_Unthought.html">91 hunch net-2005-07-10-Thinking the Unthought</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(42, 0.181), (53, 0.401), (68, 0.039), (74, 0.207), (95, 0.039)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.9622646 <a title="88-lda-1" href="../hunch_net-2010/hunch_net-2010-04-28-CI_Fellows_program_renewed.html">396 hunch net-2010-04-28-CI Fellows program renewed</a></p>
<p>Introduction: Lev Reyzinpoints out theCI Fellows program is renewed. CI Fellows are
essentiallyNSFfunded computer science postdocs for universities and industry
research labs. I've been lucky and happy to have Lev visit me for a year
underlast year's program, so I strongly recommend participating if it suits
you.As with last year, the application timeline is very short, with everything
due by May 23.</p><p>2 0.9386394 <a title="88-lda-2" href="../hunch_net-2009/hunch_net-2009-12-09-Inherent_Uncertainty.html">383 hunch net-2009-12-09-Inherent Uncertainty</a></p>
<p>Introduction: I'd like to point outInherent Uncertainty, which I've added to the ML blog
post scanner on the right. My understanding fromJakeis that the intention is
to have a multiauthor blog which is more specialized towards learning
theory/game theory than this one. Nevertheless, several of the posts seem to
be of wider interest.</p><p>same-blog 3 0.90200335 <a title="88-lda-3" href="../hunch_net-2005/hunch_net-2005-07-01-The_Role_of_Impromptu_Talks.html">88 hunch net-2005-07-01-The Role of Impromptu Talks</a></p>
<p>Introduction: COLThad an impromptu session which seemed as interesting or more interesting
than any other single technical session (despite being only an hour long).
There are several roles that an impromptu session can play
including:Announcing new work since the paper deadline. Letting this happen
now rather than later helps aid the process of research.Discussing a paper
that was rejected. Reviewers err sometimes and an impromptu session provides a
means to remedy that.Entertainment. We all like to have a bit of fun.For
design, the following seem important:Impromptu speakers should not have much
time. At COLT, it was 8 minutes, but I have seen even 5 work well.The entire
impromptu session should not last too long because the format is dense and
promotes restlessness. A half hour or hour can work well.Impromptu talks are a
mechanism to let a little bit of chaos into the schedule. They will be chaotic
in content, presentation, and usefulness. The fundamental advantage of this
chaos is that it provid</p><p>4 0.8501668 <a title="88-lda-4" href="../hunch_net-2005/hunch_net-2005-09-19-NIPS_Workshops.html">113 hunch net-2005-09-19-NIPS Workshops</a></p>
<p>Introduction: Attendance at theNIPS workshopsis highly recommended for both research and
learning. Unfortunately, there does not yet appear to be a public list of
workshops. However, I found the following workshop webpages of
interest:Machine Learning in FinanceLearning to RankFoundations of Active
LearningMachine Learning Based Robotics in Unstructured EnvironmentsThere
aremanymore workshops. In fact, there are so many that it is not plausible
anyone can attend every workshop they are interested in. Maybe in future years
the organizers can spread them out over more days to reduce overlap.Many of
these workshops are accepting presentation proposals (due mid-October).</p><p>5 0.84473145 <a title="88-lda-5" href="../hunch_net-2006/hunch_net-2006-09-07-Objective_and_subjective_interpretations_of_probability.html">205 hunch net-2006-09-07-Objective and subjective interpretations of probability</a></p>
<p>Introduction: An amusing tidbit (reproduced without permission) from Herman Chernoff's
delightful monograph, "Sequential analysis and optimal design":The use of
randomization raises a philosophical question which is articulated by the
following probably apocryphal anecdote.The metallurgist told his friend the
statistician how he planned to test the effect of heat on the strength of a
metal bar by sawing the bar into six pieces. The first two would go into the
hot oven, the next two into the medium oven, and the last two into the cool
oven. The statistician, horrified, explained how he should randomize to avoid
the effect of a possible gradient of strength in the metal bar. The method of
randomization was applied, and it turned out that the randomized experiment
called for putting the first two pieces into the hot oven, the next two into
the medium oven, and the last two into the cool oven. "Obviously, we can't do
that," said the metallurgist. "On the contrary, you have to do that," said the
statisti</p><p>6 0.66449207 <a title="88-lda-6" href="../hunch_net-2009/hunch_net-2009-07-31-Vowpal_Wabbit_Open_Source_Project.html">365 hunch net-2009-07-31-Vowpal Wabbit Open Source Project</a></p>
<p>7 0.57381797 <a title="88-lda-7" href="../hunch_net-2005/hunch_net-2005-02-03-Learning_Theory%2C_by_assumption.html">12 hunch net-2005-02-03-Learning Theory, by assumption</a></p>
<p>8 0.56398308 <a title="88-lda-8" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>9 0.56045824 <a title="88-lda-9" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>10 0.55816466 <a title="88-lda-10" href="../hunch_net-2006/hunch_net-2006-12-04-Structural_Problems_in_NIPS_Decision_Making.html">221 hunch net-2006-12-04-Structural Problems in NIPS Decision Making</a></p>
<p>11 0.55756015 <a title="88-lda-11" href="../hunch_net-2007/hunch_net-2007-01-26-Parallel_Machine_Learning_Problems.html">229 hunch net-2007-01-26-Parallel Machine Learning Problems</a></p>
<p>12 0.55641294 <a title="88-lda-12" href="../hunch_net-2006/hunch_net-2006-09-18-What_is_missing_for_online_collaborative_research%3F.html">208 hunch net-2006-09-18-What is missing for online collaborative research?</a></p>
<p>13 0.55372393 <a title="88-lda-13" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>14 0.55100024 <a title="88-lda-14" href="../hunch_net-2006/hunch_net-2006-07-11-New_Models.html">194 hunch net-2006-07-11-New Models</a></p>
<p>15 0.54947156 <a title="88-lda-15" href="../hunch_net-2005/hunch_net-2005-05-28-Running_A_Machine_Learning_Summer_School.html">75 hunch net-2005-05-28-Running A Machine Learning Summer School</a></p>
<p>16 0.5486812 <a title="88-lda-16" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>17 0.54657495 <a title="88-lda-17" href="../hunch_net-2005/hunch_net-2005-07-04-The_Health_of_COLT.html">89 hunch net-2005-07-04-The Health of COLT</a></p>
<p>18 0.54602724 <a title="88-lda-18" href="../hunch_net-2008/hunch_net-2008-10-14-Who_is_Responsible_for_a_Bad_Review%3F.html">320 hunch net-2008-10-14-Who is Responsible for a Bad Review?</a></p>
<p>19 0.54485071 <a title="88-lda-19" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>20 0.54082656 <a title="88-lda-20" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
