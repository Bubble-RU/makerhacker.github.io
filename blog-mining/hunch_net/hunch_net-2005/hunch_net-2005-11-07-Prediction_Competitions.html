<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>129 hunch net-2005-11-07-Prediction Competitions</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-129" href="#">hunch_net-2005-129</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>129 hunch net-2005-11-07-Prediction Competitions</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-129-html" href="http://hunch.net/?p=140">html</a></p><p>Introduction: There are two prediction competitions currently in the air.ThePerformance
Prediction ChallengebyIsabelle Guyon. Good entries minimize a weighted 0/1
loss + the difference between a prediction of this loss and the observed truth
on 5 datasets. Isabelle tells me all of the problems are "real world" and the
test datasets are large enough (17K minimum) that the winner should be well
determined by ability rather than luck. This is due March 1.ThePredictive
Uncertainty ChallengebyGavin Cawley. Good entries minimize log loss on real
valued output variables for one synthetic and 3 "real" datasets related to
atmospheric prediction. The use of log loss (which can be infinite and hence
is never convergent) and smaller test sets of size 1K to 7K examples makes the
winner of this contest more luck dependent. Nevertheless, the contest may be
of some interest particularly to the branch of learning (typically Bayes
learning) which prefers to optimize log loss.May the best predictor win.</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('loss', 0.28), ('entries', 0.276), ('log', 0.257), ('contest', 0.251), ('winner', 0.238), ('minimize', 0.223), ('datasets', 0.18), ('convergent', 0.173), ('prediction', 0.169), ('valued', 0.16), ('competitions', 0.16), ('luck', 0.16), ('prefers', 0.16), ('real', 0.155), ('determined', 0.151), ('synthetic', 0.151), ('branch', 0.151), ('test', 0.15), ('infinite', 0.138), ('win', 0.126)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="129-tfidf-1" href="../hunch_net-2005/hunch_net-2005-11-07-Prediction_Competitions.html">129 hunch net-2005-11-07-Prediction Competitions</a></p>
<p>Introduction: There are two prediction competitions currently in the air.ThePerformance
Prediction ChallengebyIsabelle Guyon. Good entries minimize a weighted 0/1
loss + the difference between a prediction of this loss and the observed truth
on 5 datasets. Isabelle tells me all of the problems are "real world" and the
test datasets are large enough (17K minimum) that the winner should be well
determined by ability rather than luck. This is due March 1.ThePredictive
Uncertainty ChallengebyGavin Cawley. Good entries minimize log loss on real
valued output variables for one synthetic and 3 "real" datasets related to
atmospheric prediction. The use of log loss (which can be infinite and hence
is never convergent) and smaller test sets of size 1K to 7K examples makes the
winner of this contest more luck dependent. Nevertheless, the contest may be
of some interest particularly to the branch of learning (typically Bayes
learning) which prefers to optimize log loss.May the best predictor win.</p><p>2 0.33624369 <a title="129-tfidf-2" href="../hunch_net-2005/hunch_net-2005-02-01-Watchword%3A_Loss.html">9 hunch net-2005-02-01-Watchword: Loss</a></p>
<p>Introduction: A loss function is some function which, for any example, takes a prediction
and the correct prediction, and determines how much loss is incurred. (People
sometimes attempt to optimize functions of more than one example such as "area
under the ROC curve" or "harmonic mean of precision and recall".) Typically we
try to find predictors that minimize loss.There seems to be a strong dichotomy
between two views of what "loss" means in learning.Loss is determined by the
problem.Loss is a part of the specification of the learning problem. Examples
of problems specified by the loss function include "binary classification",
"multiclass classification", "importance weighted classification",
"l2regression", etcâ&euro;Ś This is the decision theory view of what loss means, and
the view that I prefer.Loss is determined by the solution.To solve a problem,
you optimize some particular loss functionnotgiven by the problem. Examples of
these loss functions are "hinge loss" (for SVMs), "log loss" (common in
Baye</p><p>3 0.25372466 <a title="129-tfidf-3" href="../hunch_net-2009/hunch_net-2009-02-04-Optimal_Proxy_Loss_for_Classification.html">341 hunch net-2009-02-04-Optimal Proxy Loss for Classification</a></p>
<p>Introduction: Many people in machine learning take advantage of the notion of a proxy loss:
A loss function which is much easier to optimize computationally than the loss
function imposed by the world. A canonical example is when we want to learn a
weight vectorwand predict according to a dot productfw(x)= sumiwixiwhere
optimizing squared loss(y-fw(x))2over many samples is much more tractable than
optimizing 0-1 lossI(y = Threshold(fw(x) - 0.5)).While the computational
advantages of optimizing a proxy loss are substantial, we are curious: which
proxy loss is best? The answer of course depends on what the real loss imposed
by the world is. For 0-1 loss classification, there are adherents to many
choices:Log loss. If we confine the prediction to[0,1], we can treat it as a
predicted probability that the label is1, and measure loss according tolog
1/p'(y|x)wherep'(y|x)is the predicted probability of the observed label. A
standard method for confining the prediction to[0,1]islogistic regressionwhich
expo</p><p>4 0.22381322 <a title="129-tfidf-4" href="../hunch_net-2007/hunch_net-2007-08-19-Choice_of_Metrics.html">259 hunch net-2007-08-19-Choice of Metrics</a></p>
<p>Introduction: How do we judge success in Machine Learning? AsAaronnotes, the best way is to
use the loss imposed on you by the world. This turns out to be infeasible
sometimes for various reasons. The ones I've seen are:The learned prediction
is used in some complicated process that does not give the feedback necessary
to understand the prediction's impact on the loss.The prediction is used by
some other system which expects some semantics to the predicted value. This is
similar to the previous example, except that the issue is design modularity
rather than engineering modularity.The correct loss function is simply unknown
(and perhaps unknowable, except by experimentation).In these situations, it's
unclear what metric for evaluation should be chosen. This post has some design
advice for this murkier case. I'm using the word "metric" here to distinguish
the fact that we are considering methods forevaluatingpredictive systems
rather than a loss imposed by the real world or a loss which is optimized b</p><p>5 0.1987461 <a title="129-tfidf-5" href="../hunch_net-2009/hunch_net-2009-09-21-Netflix_finishes_%28and_starts%29.html">371 hunch net-2009-09-21-Netflix finishes (and starts)</a></p>
<p>Introduction: I attended theNetflix prizeceremony this morning. The press conference part
iscovered fine elsewhere, with the basic outcome being thatBellKor's Pragmatic
Chaoswon overThe Ensembleby 15-20minutes, because they were tied in
performance on the ultimate holdout set. I'm sure the individual participants
will have many chances to speak about the solution. One of these is Bell at
theNYAS ML symposium on Nov. 6.Several additional details may interest ML
people.The degree of overfitting exhibited by the difference in performance on
theleaderboard test setand the ultimate hold out set was small, but
determining at .02 to .03%.A tie was possible, because the rules cut off
measurements below the fourth digit based on significance concerns. In
actuality, of course, the scores do differ before rounding, but everyone I
spoke to claimed not to know how. The complete dataset has beenreleased on
UCI, so each team could compute their own score to whatever accuracy desired.I
was impressed by the slick sy</p><p>6 0.19276857 <a title="129-tfidf-6" href="../hunch_net-2005/hunch_net-2005-06-08-Question%3A_%26%238220%3BWhen_is_the_right_time_to_insert_the_loss_function%3F%26%238221%3B.html">79 hunch net-2005-06-08-Question: &#8220;When is the right time to insert the loss function?&#8221;</a></p>
<p>7 0.17665206 <a title="129-tfidf-7" href="../hunch_net-2006/hunch_net-2006-10-02-%241M_Netflix_prediction_contest.html">211 hunch net-2006-10-02-$1M Netflix prediction contest</a></p>
<p>8 0.17531668 <a title="129-tfidf-8" href="../hunch_net-2007/hunch_net-2007-05-12-Loss_Function_Semantics.html">245 hunch net-2007-05-12-Loss Function Semantics</a></p>
<p>9 0.16120906 <a title="129-tfidf-9" href="../hunch_net-2011/hunch_net-2011-04-11-The_Heritage_Health_Prize.html">430 hunch net-2011-04-11-The Heritage Health Prize</a></p>
<p>10 0.12936534 <a title="129-tfidf-10" href="../hunch_net-2006/hunch_net-2006-07-06-Branch_Prediction_Competition.html">190 hunch net-2006-07-06-Branch Prediction Competition</a></p>
<p>11 0.12485824 <a title="129-tfidf-11" href="../hunch_net-2009/hunch_net-2009-10-10-ALT_2009.html">374 hunch net-2009-10-10-ALT 2009</a></p>
<p>12 0.12150776 <a title="129-tfidf-12" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>13 0.11798654 <a title="129-tfidf-13" href="../hunch_net-2007/hunch_net-2007-11-28-Computational_Consequences_of_Classification.html">274 hunch net-2007-11-28-Computational Consequences of Classification</a></p>
<p>14 0.11745038 <a title="129-tfidf-14" href="../hunch_net-2005/hunch_net-2005-05-21-What_is_the_right_form_of_modularity_in_structured_prediction%3F.html">74 hunch net-2005-05-21-What is the right form of modularity in structured prediction?</a></p>
<p>15 0.11475816 <a title="129-tfidf-15" href="../hunch_net-2011/hunch_net-2011-03-20-KDD_Cup_2011.html">427 hunch net-2011-03-20-KDD Cup 2011</a></p>
<p>16 0.1117682 <a title="129-tfidf-16" href="../hunch_net-2007/hunch_net-2007-03-15-Alternative_Machine_Learning_Reductions_Definitions.html">236 hunch net-2007-03-15-Alternative Machine Learning Reductions Definitions</a></p>
<p>17 0.10946774 <a title="129-tfidf-17" href="../hunch_net-2008/hunch_net-2008-04-30-Concerns_about_the_Large_Scale_Learning_Challenge.html">300 hunch net-2008-04-30-Concerns about the Large Scale Learning Challenge</a></p>
<p>18 0.10299113 <a title="129-tfidf-18" href="../hunch_net-2005/hunch_net-2005-09-08-Online_Learning_as_the_Mathematics_of_Accountability.html">109 hunch net-2005-09-08-Online Learning as the Mathematics of Accountability</a></p>
<p>19 0.099634059 <a title="129-tfidf-19" href="../hunch_net-2010/hunch_net-2010-05-10-Aggregation_of_estimators%2C_sparsity_in_high_dimension_and_computational_feasibility.html">398 hunch net-2010-05-10-Aggregation of estimators, sparsity in high dimension and computational feasibility</a></p>
<p>20 0.094543152 <a title="129-tfidf-20" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.171), (1, -0.159), (2, -0.101), (3, 0.173), (4, 0.286), (5, 0.15), (6, -0.048), (7, -0.017), (8, 0.047), (9, -0.081), (10, -0.113), (11, -0.05), (12, -0.066), (13, 0.008), (14, 0.061), (15, -0.016), (16, -0.1), (17, -0.083), (18, 0.02), (19, -0.028), (20, -0.018), (21, 0.08), (22, 0.056), (23, 0.056), (24, -0.041), (25, 0.038), (26, -0.009), (27, -0.051), (28, 0.009), (29, -0.073), (30, 0.009), (31, 0.058), (32, 0.081), (33, -0.007), (34, 0.041), (35, -0.042), (36, 0.048), (37, -0.018), (38, 0.02), (39, -0.043), (40, 0.119), (41, 0.028), (42, -0.0), (43, 0.028), (44, 0.037), (45, 0.027), (46, 0.015), (47, -0.023), (48, 0.035), (49, -0.02)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9799279 <a title="129-lsi-1" href="../hunch_net-2005/hunch_net-2005-11-07-Prediction_Competitions.html">129 hunch net-2005-11-07-Prediction Competitions</a></p>
<p>Introduction: There are two prediction competitions currently in the air.ThePerformance
Prediction ChallengebyIsabelle Guyon. Good entries minimize a weighted 0/1
loss + the difference between a prediction of this loss and the observed truth
on 5 datasets. Isabelle tells me all of the problems are "real world" and the
test datasets are large enough (17K minimum) that the winner should be well
determined by ability rather than luck. This is due March 1.ThePredictive
Uncertainty ChallengebyGavin Cawley. Good entries minimize log loss on real
valued output variables for one synthetic and 3 "real" datasets related to
atmospheric prediction. The use of log loss (which can be infinite and hence
is never convergent) and smaller test sets of size 1K to 7K examples makes the
winner of this contest more luck dependent. Nevertheless, the contest may be
of some interest particularly to the branch of learning (typically Bayes
learning) which prefers to optimize log loss.May the best predictor win.</p><p>2 0.77563465 <a title="129-lsi-2" href="../hunch_net-2007/hunch_net-2007-08-19-Choice_of_Metrics.html">259 hunch net-2007-08-19-Choice of Metrics</a></p>
<p>Introduction: How do we judge success in Machine Learning? AsAaronnotes, the best way is to
use the loss imposed on you by the world. This turns out to be infeasible
sometimes for various reasons. The ones I've seen are:The learned prediction
is used in some complicated process that does not give the feedback necessary
to understand the prediction's impact on the loss.The prediction is used by
some other system which expects some semantics to the predicted value. This is
similar to the previous example, except that the issue is design modularity
rather than engineering modularity.The correct loss function is simply unknown
(and perhaps unknowable, except by experimentation).In these situations, it's
unclear what metric for evaluation should be chosen. This post has some design
advice for this murkier case. I'm using the word "metric" here to distinguish
the fact that we are considering methods forevaluatingpredictive systems
rather than a loss imposed by the real world or a loss which is optimized b</p><p>3 0.73386997 <a title="129-lsi-3" href="../hunch_net-2009/hunch_net-2009-02-04-Optimal_Proxy_Loss_for_Classification.html">341 hunch net-2009-02-04-Optimal Proxy Loss for Classification</a></p>
<p>Introduction: Many people in machine learning take advantage of the notion of a proxy loss:
A loss function which is much easier to optimize computationally than the loss
function imposed by the world. A canonical example is when we want to learn a
weight vectorwand predict according to a dot productfw(x)= sumiwixiwhere
optimizing squared loss(y-fw(x))2over many samples is much more tractable than
optimizing 0-1 lossI(y = Threshold(fw(x) - 0.5)).While the computational
advantages of optimizing a proxy loss are substantial, we are curious: which
proxy loss is best? The answer of course depends on what the real loss imposed
by the world is. For 0-1 loss classification, there are adherents to many
choices:Log loss. If we confine the prediction to[0,1], we can treat it as a
predicted probability that the label is1, and measure loss according tolog
1/p'(y|x)wherep'(y|x)is the predicted probability of the observed label. A
standard method for confining the prediction to[0,1]islogistic regressionwhich
expo</p><p>4 0.7254703 <a title="129-lsi-4" href="../hunch_net-2005/hunch_net-2005-02-01-Watchword%3A_Loss.html">9 hunch net-2005-02-01-Watchword: Loss</a></p>
<p>Introduction: A loss function is some function which, for any example, takes a prediction
and the correct prediction, and determines how much loss is incurred. (People
sometimes attempt to optimize functions of more than one example such as "area
under the ROC curve" or "harmonic mean of precision and recall".) Typically we
try to find predictors that minimize loss.There seems to be a strong dichotomy
between two views of what "loss" means in learning.Loss is determined by the
problem.Loss is a part of the specification of the learning problem. Examples
of problems specified by the loss function include "binary classification",
"multiclass classification", "importance weighted classification",
"l2regression", etcâ&euro;Ś This is the decision theory view of what loss means, and
the view that I prefer.Loss is determined by the solution.To solve a problem,
you optimize some particular loss functionnotgiven by the problem. Examples of
these loss functions are "hinge loss" (for SVMs), "log loss" (common in
Baye</p><p>5 0.72334582 <a title="129-lsi-5" href="../hunch_net-2005/hunch_net-2005-06-08-Question%3A_%26%238220%3BWhen_is_the_right_time_to_insert_the_loss_function%3F%26%238221%3B.html">79 hunch net-2005-06-08-Question: &#8220;When is the right time to insert the loss function?&#8221;</a></p>
<p>Introduction: Halasksa very good question: "When is the right time to insert the loss
function?" In particular, should it be used at testing time or at training
time?When the world imposes a loss on us, the standard Bayesian recipe is to
predict the (conditional) probability of each possibility and then choose the
possibility which minimizes the expected loss. In contrast, as
theconfusionover "loss = money lost" or "loss = the thing you optimize" might
indicate, many people ignore the Bayesian approach and simply optimize their
loss (or a close proxy for their loss) over the representation on the training
set.The best answer I can give is "it's unclear, but I prefer optimizing the
loss at training time". My experience is that optimizing the loss in the most
direct manner possible typically yields best performance. This question is
related to a basic principle which bothYann LeCun(applied) andVladimir
Vapnik(theoretical) advocate: "solve the simplest prediction problem that
solves the problem". (One</p><p>6 0.6826427 <a title="129-lsi-6" href="../hunch_net-2007/hunch_net-2007-05-12-Loss_Function_Semantics.html">245 hunch net-2007-05-12-Loss Function Semantics</a></p>
<p>7 0.67142057 <a title="129-lsi-7" href="../hunch_net-2009/hunch_net-2009-09-21-Netflix_finishes_%28and_starts%29.html">371 hunch net-2009-09-21-Netflix finishes (and starts)</a></p>
<p>8 0.61026442 <a title="129-lsi-8" href="../hunch_net-2005/hunch_net-2005-05-21-What_is_the_right_form_of_modularity_in_structured_prediction%3F.html">74 hunch net-2005-05-21-What is the right form of modularity in structured prediction?</a></p>
<p>9 0.60685194 <a title="129-lsi-9" href="../hunch_net-2007/hunch_net-2007-11-28-Computational_Consequences_of_Classification.html">274 hunch net-2007-11-28-Computational Consequences of Classification</a></p>
<p>10 0.59991252 <a title="129-lsi-10" href="../hunch_net-2011/hunch_net-2011-04-11-The_Heritage_Health_Prize.html">430 hunch net-2011-04-11-The Heritage Health Prize</a></p>
<p>11 0.57602274 <a title="129-lsi-11" href="../hunch_net-2009/hunch_net-2009-10-10-ALT_2009.html">374 hunch net-2009-10-10-ALT 2009</a></p>
<p>12 0.55911273 <a title="129-lsi-12" href="../hunch_net-2006/hunch_net-2006-10-02-%241M_Netflix_prediction_contest.html">211 hunch net-2006-10-02-$1M Netflix prediction contest</a></p>
<p>13 0.48641858 <a title="129-lsi-13" href="../hunch_net-2007/hunch_net-2007-03-15-Alternative_Machine_Learning_Reductions_Definitions.html">236 hunch net-2007-03-15-Alternative Machine Learning Reductions Definitions</a></p>
<p>14 0.47318879 <a title="129-lsi-14" href="../hunch_net-2006/hunch_net-2006-07-06-Branch_Prediction_Competition.html">190 hunch net-2006-07-06-Branch Prediction Competition</a></p>
<p>15 0.44932419 <a title="129-lsi-15" href="../hunch_net-2011/hunch_net-2011-03-20-KDD_Cup_2011.html">427 hunch net-2011-03-20-KDD Cup 2011</a></p>
<p>16 0.41457647 <a title="129-lsi-16" href="../hunch_net-2010/hunch_net-2010-05-10-Aggregation_of_estimators%2C_sparsity_in_high_dimension_and_computational_feasibility.html">398 hunch net-2010-05-10-Aggregation of estimators, sparsity in high dimension and computational feasibility</a></p>
<p>17 0.38134834 <a title="129-lsi-17" href="../hunch_net-2005/hunch_net-2005-08-18-SVM_Adaptability.html">103 hunch net-2005-08-18-SVM Adaptability</a></p>
<p>18 0.37571254 <a title="129-lsi-18" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>19 0.37384313 <a title="129-lsi-19" href="../hunch_net-2006/hunch_net-2006-03-23-The_Approximation_Argument.html">165 hunch net-2006-03-23-The Approximation Argument</a></p>
<p>20 0.36777404 <a title="129-lsi-20" href="../hunch_net-2005/hunch_net-2005-09-08-Online_Learning_as_the_Mathematics_of_Accountability.html">109 hunch net-2005-09-08-Online Learning as the Mathematics of Accountability</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(14, 0.44), (42, 0.193), (68, 0.173), (74, 0.071)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.83108056 <a title="129-lda-1" href="../hunch_net-2005/hunch_net-2005-11-07-Prediction_Competitions.html">129 hunch net-2005-11-07-Prediction Competitions</a></p>
<p>Introduction: There are two prediction competitions currently in the air.ThePerformance
Prediction ChallengebyIsabelle Guyon. Good entries minimize a weighted 0/1
loss + the difference between a prediction of this loss and the observed truth
on 5 datasets. Isabelle tells me all of the problems are "real world" and the
test datasets are large enough (17K minimum) that the winner should be well
determined by ability rather than luck. This is due March 1.ThePredictive
Uncertainty ChallengebyGavin Cawley. Good entries minimize log loss on real
valued output variables for one synthetic and 3 "real" datasets related to
atmospheric prediction. The use of log loss (which can be infinite and hence
is never convergent) and smaller test sets of size 1K to 7K examples makes the
winner of this contest more luck dependent. Nevertheless, the contest may be
of some interest particularly to the branch of learning (typically Bayes
learning) which prefers to optimize log loss.May the best predictor win.</p><p>2 0.74954975 <a title="129-lda-2" href="../hunch_net-2007/hunch_net-2007-11-14-BellKor_wins_Netflix.html">272 hunch net-2007-11-14-BellKor wins Netflix</a></p>
<p>Introduction: â&euro;Ś but only the little prize. TheBellKor teamfocused on integrating predictions
from many different methods. The base methods consist of:Nearest Neighbor
MethodsMatrix Factorization Methods (asymmetric and symmetric)Linear
Regression on various feature spacesRestricted Boltzman MachinesThe final
predictor was an ensemble (as was reasonable to expect), although it's a
little bit more complicated than just a weighted average--it's essentially a
customized learning algorithm. Base approaches (1)-(3) seem like relatively
well-known approaches (although I haven't seen the asymmetric factorization
variant before). RBMs are the new approach.Thewriteupis pretty clear for more
details.The contestants are close to reaching the big prize, but the last 1.5%
is probably at least as hard as what's been done. A few new structurally
different methods for making predictions may need to be discovered and added
into the mixture. In other words, research may be required.</p><p>3 0.70015877 <a title="129-lda-3" href="../hunch_net-2005/hunch_net-2005-09-20-Workshop_Proposal%3A_Atomic_Learning.html">114 hunch net-2005-09-20-Workshop Proposal: Atomic Learning</a></p>
<p>Introduction: This is a proposal for a workshop. It may or may not happen depending on the
level of interest. If you are interested, feel free to indicate so (by email
or comments).Description:Assume(*) that any system for solving large difficult
learning problems must decompose into repeated use of basic elements (i.e.
atoms). There are many basic questions which remain:What are the viable basic
elements?What makes a basic element viable?What are the viable principles for
the composition of these basic elements?What are the viable principles for
learning in such systems?What problems can this approach handle?Hal Daume
adds:Can composition of atoms be (semi-) automatically constructed[?]When
atoms are constructed through reductions, is there some notion of the
"naturalness" of the created leaning problems?Other than Markov
fields/graphical models/Bayes nets, is there a good language for representing
atoms and their compositions?The answer to these and related questions remain
unclear to me. A worksh</p><p>4 0.5811457 <a title="129-lda-4" href="../hunch_net-2007/hunch_net-2007-09-30-NIPS_workshops_are_out..html">264 hunch net-2007-09-30-NIPS workshops are out.</a></p>
<p>Introduction: Here. I'm particularly interested in theWeb Search,Efficient ML, and (of
course)Learning Problem Designworkshops but there are many others to check out
as well. Workshops are a great chance to make progress on or learn about a
topic. Relevance and interaction amongst diverse people can sometimes be
magical.</p><p>5 0.57894409 <a title="129-lda-5" href="../hunch_net-2008/hunch_net-2008-12-12-Summer_Conferences.html">331 hunch net-2008-12-12-Summer Conferences</a></p>
<p>Introduction: Here's a handy table for the summer conferences.ConferenceDeadlineReviewer
TargetingDouble BlindAuthor FeedbackLocationDateICML(wrong ICML)January
26YesYesYesMontreal, CanadaJune 14-17COLTFebruary 13NoNoYesMontrealJune
19-21UAIMarch 13NoYesNoMontrealJune 19-21KDDFebruary 2/6NoNoNoParis,
FranceJune 28-July 1Reviewer targeting is new this year. The idea is that many
poor decisions happen because the papers go to reviewers who are unqualified,
and the hope is that allowing authors to point out who is qualified results in
better decisions. In my experience, this is a reasonable idea to test.Both UAI
and COLT are experimenting this year as well with double blind and author
feedback, respectively. Of the two, I believe author feedback is more
important, as I've seen it make a difference. However, I still consider double
blind reviewing a net win, as it's a substantial public commitment to
fairness.</p><p>6 0.5191887 <a title="129-lda-6" href="../hunch_net-2012/hunch_net-2012-03-24-David_Waltz.html">460 hunch net-2012-03-24-David Waltz</a></p>
<p>7 0.51585877 <a title="129-lda-7" href="../hunch_net-2009/hunch_net-2009-01-21-Nearly_all_natural_problems_require_nonlinearity.html">337 hunch net-2009-01-21-Nearly all natural problems require nonlinearity</a></p>
<p>8 0.51464802 <a title="129-lda-8" href="../hunch_net-2008/hunch_net-2008-04-12-It_Doesn%26%238217%3Bt_Stop.html">295 hunch net-2008-04-12-It Doesn&#8217;t Stop</a></p>
<p>9 0.51127052 <a title="129-lda-9" href="../hunch_net-2005/hunch_net-2005-08-22-Do_you_believe_in_induction%3F.html">104 hunch net-2005-08-22-Do you believe in induction?</a></p>
<p>10 0.50673842 <a title="129-lda-10" href="../hunch_net-2009/hunch_net-2009-02-04-Optimal_Proxy_Loss_for_Classification.html">341 hunch net-2009-02-04-Optimal Proxy Loss for Classification</a></p>
<p>11 0.49931645 <a title="129-lda-11" href="../hunch_net-2005/hunch_net-2005-09-10-%26%238220%3BFailure%26%238221%3B_is_an_option.html">110 hunch net-2005-09-10-&#8220;Failure&#8221; is an option</a></p>
<p>12 0.49166191 <a title="129-lda-12" href="../hunch_net-2005/hunch_net-2005-02-09-Intuitions_from_applied_learning.html">16 hunch net-2005-02-09-Intuitions from applied learning</a></p>
<p>13 0.49140391 <a title="129-lda-13" href="../hunch_net-2006/hunch_net-2006-07-08-Some_recent_papers.html">192 hunch net-2006-07-08-Some recent papers</a></p>
<p>14 0.49084294 <a title="129-lda-14" href="../hunch_net-2012/hunch_net-2012-03-13-The_Submodularity_workshop_and_Lucca_Professorship.html">459 hunch net-2012-03-13-The Submodularity workshop and Lucca Professorship</a></p>
<p>15 0.48857671 <a title="129-lda-15" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>16 0.48706546 <a title="129-lda-16" href="../hunch_net-2005/hunch_net-2005-06-10-Workshops_are_not_Conferences.html">80 hunch net-2005-06-10-Workshops are not Conferences</a></p>
<p>17 0.48447239 <a title="129-lda-17" href="../hunch_net-2007/hunch_net-2007-05-12-Loss_Function_Semantics.html">245 hunch net-2007-05-12-Loss Function Semantics</a></p>
<p>18 0.4801656 <a title="129-lda-18" href="../hunch_net-2005/hunch_net-2005-02-01-Watchword%3A_Loss.html">9 hunch net-2005-02-01-Watchword: Loss</a></p>
<p>19 0.47798344 <a title="129-lda-19" href="../hunch_net-2005/hunch_net-2005-06-08-Question%3A_%26%238220%3BWhen_is_the_right_time_to_insert_the_loss_function%3F%26%238221%3B.html">79 hunch net-2005-06-08-Question: &#8220;When is the right time to insert the loss function?&#8221;</a></p>
<p>20 0.47726321 <a title="129-lda-20" href="../hunch_net-2007/hunch_net-2007-08-12-Exponentiated_Gradient.html">258 hunch net-2007-08-12-Exponentiated Gradient</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
