<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>56 hunch net-2005-04-14-Families of Learning Theory Statements</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-56" href="#">hunch_net-2005-56</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>56 hunch net-2005-04-14-Families of Learning Theory Statements</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-56-html" href="http://hunch.net/?p=61">html</a></p><p>Introduction: The diagram above shows a very broad viewpoint of learning theory.arrowTypical
statementExamplesPast->PastSome prediction algorithmAdoes almost as well as
any of a set of algorithms.Weighted MajorityPast->FutureAssuming independent
samples, past performance predicts future performance.PAC analysis, ERM
analysisFuture->FutureFuture prediction performance on subproblems implies
future prediction performance using algorithmA.ECOC, ProbingA basic question
is: Are there other varieties of statements of this type?Avrimnoted that there
are also "arrows between arrows": generic methods for transforming between
Past->Past statements and Past->Future statements. Are there others?</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('arrows', 0.529), ('statements', 0.287), ('performance', 0.281), ('future', 0.272), ('past', 0.258), ('varieties', 0.235), ('prediction', 0.23), ('erm', 0.205), ('generic', 0.182), ('predicts', 0.176), ('subproblems', 0.171), ('broad', 0.149), ('type', 0.149), ('independent', 0.135), ('viewpoint', 0.133), ('shows', 0.127), ('samples', 0.12), ('analysis', 0.097), ('methods', 0.091), ('implies', 0.091)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="56-tfidf-1" href="../hunch_net-2005/hunch_net-2005-04-14-Families_of_Learning_Theory_Statements.html">56 hunch net-2005-04-14-Families of Learning Theory Statements</a></p>
<p>Introduction: The diagram above shows a very broad viewpoint of learning theory.arrowTypical
statementExamplesPast->PastSome prediction algorithmAdoes almost as well as
any of a set of algorithms.Weighted MajorityPast->FutureAssuming independent
samples, past performance predicts future performance.PAC analysis, ERM
analysisFuture->FutureFuture prediction performance on subproblems implies
future prediction performance using algorithmA.ECOC, ProbingA basic question
is: Are there other varieties of statements of this type?Avrimnoted that there
are also "arrows between arrows": generic methods for transforming between
Past->Past statements and Past->Future statements. Are there others?</p><p>2 0.16193101 <a title="56-tfidf-2" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>Introduction: "Overfitting" is traditionally defined as training some flexible
representation so that it memorizes the data but fails to predict well in the
future. For this post, I will define overfitting more generally as over-
representing the performance of systems. There are two styles of general
overfitting: overrepresenting performance on particular datasets and
(implicitly) overrepresenting performance of a method on future datasets.We
should all be aware of these methods, avoid them where possible, and take them
into account otherwise. I have used "reproblem" and "old datasets", and may
have participated in "overfitting by review"--some of these are very difficult
to avoid.NameMethodExplanationRemedyTraditional overfittingTrain a complex
predictor on too-few examples.Hold out pristine examples for testing.Use a
simpler predictor.Get more training examples.Integrate over many
predictors.Reject papers which do this.Parameter tweak overfittingUse a
learning algorithm with many parameters. Choo</p><p>3 0.13161935 <a title="56-tfidf-3" href="../hunch_net-2005/hunch_net-2005-02-03-Learning_Theory%2C_by_assumption.html">12 hunch net-2005-02-03-Learning Theory, by assumption</a></p>
<p>Introduction: One way to organize learning theory is by assumption (in theassumption = axiom
sense), from no assumptions to many assumptions. As you travel down this list,
the statements become stronger, but the scope of applicability decreases.No
assumptionsOnline learningThere exist a meta prediction algorithm which
compete well with the best element of any set of prediction
algorithms.Universal LearningUsing a "bias" of 2- description length of turing
machinein learning is equivalent to all other computable biases up to some
constant.ReductionsThe ability to predict well on classification problems is
equivalent to the ability to predict well on many other learning
problems.Independent and Identically Distributed (IID) DataPerformance
PredictionBased upon past performance, you can predict future
performance.Uniform ConvergencePerformance prediction works even after
choosing classifiers based on the data from large sets of classifiers.IID and
partial constraints on the data sourcePAC LearningThere</p><p>4 0.1080034 <a title="56-tfidf-4" href="../hunch_net-2006/hunch_net-2006-03-05-%26%238220%3BStructural%26%238221%3B_Learning.html">161 hunch net-2006-03-05-&#8220;Structural&#8221; Learning</a></p>
<p>Introduction: Fernando Pereirapointed outAndo andZhang'spaperon "structural" learning.
Structural learning is multitask learning on subproblems created from
unlabeled data.The basic idea is to take a look at the unlabeled data and
create many supervised problems. On text data, which they test on, these
subproblems might be of the form "Given surrounding words predict the middle
word". The hope here is that successfully predicting on these subproblems is
relevant to the prediction of your core problem.In the long run, the precise
mechanism used (essentially, linear predictors with parameters tied by a
common matrix) and the precise problems formed may not be critical. What seems
critical is that the hope is realized: the technique provides a significant
edge in practice.Some basic questions about this approach are:Are there
effective automated mechanisms for creating the subproblems?Is it necessary to
use a shared representation?</p><p>5 0.10054033 <a title="56-tfidf-5" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>Introduction: What?Reductions are machines which turn solvers for one problem into solvers
for another problem.Why?Reductions are useful for several reasons.Laziness.
Reducing a problem to classification make at least 10 learning algorithms
available to solve a problem. Inventing 10 learning algorithms is quite a bit
of work. Similarly, programming a reduction is often trivial, while
programming a learning algorithm is a great deal of work.Crystallization. The
problems we often want to solve in learning are worst-case-impossible, but
average case feasible. By reducing all problems onto one or a few primitives,
we can fine tune these primitives to perform well on real-world problems with
greater precision due to the greater number of problems to validate
on.Theoretical Organization. By studying what reductions are easy vs. hard vs.
impossible, we can learn which problems are roughly equivalent in difficulty
and which are much harder.What we know now.Typesafe reductions. In the
beginning, there was th</p><p>6 0.088612475 <a title="56-tfidf-6" href="../hunch_net-2005/hunch_net-2005-11-16-The_Everything_Ensemble_Edge.html">131 hunch net-2005-11-16-The Everything Ensemble Edge</a></p>
<p>7 0.088212416 <a title="56-tfidf-7" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>8 0.080825686 <a title="56-tfidf-8" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>9 0.079894498 <a title="56-tfidf-9" href="../hunch_net-2006/hunch_net-2006-05-05-An_ICML_reject.html">177 hunch net-2006-05-05-An ICML reject</a></p>
<p>10 0.074804038 <a title="56-tfidf-10" href="../hunch_net-2005/hunch_net-2005-04-26-To_calibrate_or_not%3F.html">62 hunch net-2005-04-26-To calibrate or not?</a></p>
<p>11 0.07351952 <a title="56-tfidf-11" href="../hunch_net-2005/hunch_net-2005-06-18-Lower_Bounds_for_Learning_Reductions.html">83 hunch net-2005-06-18-Lower Bounds for Learning Reductions</a></p>
<p>12 0.073163249 <a title="56-tfidf-12" href="../hunch_net-2005/hunch_net-2005-06-28-A_COLT_paper.html">85 hunch net-2005-06-28-A COLT paper</a></p>
<p>13 0.07108254 <a title="56-tfidf-13" href="../hunch_net-2011/hunch_net-2011-08-01-Interesting_papers_at_COLT_2011.html">439 hunch net-2011-08-01-Interesting papers at COLT 2011</a></p>
<p>14 0.067546435 <a title="56-tfidf-14" href="../hunch_net-2009/hunch_net-2009-06-26-Netflix_nearly_done.html">362 hunch net-2009-06-26-Netflix nearly done</a></p>
<p>15 0.06713745 <a title="56-tfidf-15" href="../hunch_net-2008/hunch_net-2008-08-24-Mass_Customized_Medicine_in_the_Future%3F.html">314 hunch net-2008-08-24-Mass Customized Medicine in the Future?</a></p>
<p>16 0.065741055 <a title="56-tfidf-16" href="../hunch_net-2006/hunch_net-2006-03-02-Why_do_people_count_for_learning%3F.html">160 hunch net-2006-03-02-Why do people count for learning?</a></p>
<p>17 0.063730597 <a title="56-tfidf-17" href="../hunch_net-2005/hunch_net-2005-03-15-The_State_of_Tight_Bounds.html">41 hunch net-2005-03-15-The State of Tight Bounds</a></p>
<p>18 0.063575655 <a title="56-tfidf-18" href="../hunch_net-2006/hunch_net-2006-03-12-Online_learning_or_online_preservation_of_learning%3F.html">163 hunch net-2006-03-12-Online learning or online preservation of learning?</a></p>
<p>19 0.061898276 <a title="56-tfidf-19" href="../hunch_net-2009/hunch_net-2009-10-26-NIPS_workshops.html">375 hunch net-2009-10-26-NIPS workshops</a></p>
<p>20 0.06139271 <a title="56-tfidf-20" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.111), (1, -0.085), (2, -0.028), (3, 0.01), (4, -0.02), (5, -0.032), (6, -0.025), (7, 0.026), (8, 0.073), (9, -0.005), (10, -0.08), (11, -0.034), (12, -0.051), (13, -0.038), (14, 0.019), (15, 0.046), (16, -0.084), (17, -0.021), (18, 0.02), (19, -0.028), (20, 0.082), (21, -0.001), (22, 0.09), (23, -0.016), (24, -0.041), (25, -0.083), (26, -0.02), (27, 0.017), (28, -0.079), (29, -0.124), (30, -0.062), (31, 0.047), (32, -0.033), (33, 0.059), (34, 0.115), (35, 0.01), (36, 0.03), (37, -0.04), (38, -0.019), (39, -0.038), (40, -0.001), (41, 0.058), (42, -0.124), (43, -0.031), (44, 0.005), (45, -0.022), (46, 0.102), (47, -0.016), (48, -0.034), (49, 0.068)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9887594 <a title="56-lsi-1" href="../hunch_net-2005/hunch_net-2005-04-14-Families_of_Learning_Theory_Statements.html">56 hunch net-2005-04-14-Families of Learning Theory Statements</a></p>
<p>Introduction: The diagram above shows a very broad viewpoint of learning theory.arrowTypical
statementExamplesPast->PastSome prediction algorithmAdoes almost as well as
any of a set of algorithms.Weighted MajorityPast->FutureAssuming independent
samples, past performance predicts future performance.PAC analysis, ERM
analysisFuture->FutureFuture prediction performance on subproblems implies
future prediction performance using algorithmA.ECOC, ProbingA basic question
is: Are there other varieties of statements of this type?Avrimnoted that there
are also "arrows between arrows": generic methods for transforming between
Past->Past statements and Past->Future statements. Are there others?</p><p>2 0.63408089 <a title="56-lsi-2" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>Introduction: "Overfitting" is traditionally defined as training some flexible
representation so that it memorizes the data but fails to predict well in the
future. For this post, I will define overfitting more generally as over-
representing the performance of systems. There are two styles of general
overfitting: overrepresenting performance on particular datasets and
(implicitly) overrepresenting performance of a method on future datasets.We
should all be aware of these methods, avoid them where possible, and take them
into account otherwise. I have used "reproblem" and "old datasets", and may
have participated in "overfitting by review"--some of these are very difficult
to avoid.NameMethodExplanationRemedyTraditional overfittingTrain a complex
predictor on too-few examples.Hold out pristine examples for testing.Use a
simpler predictor.Get more training examples.Integrate over many
predictors.Reject papers which do this.Parameter tweak overfittingUse a
learning algorithm with many parameters. Choo</p><p>3 0.5751397 <a title="56-lsi-3" href="../hunch_net-2005/hunch_net-2005-02-12-ROC_vs._Accuracy_vs._AROC.html">18 hunch net-2005-02-12-ROC vs. Accuracy vs. AROC</a></p>
<p>Introduction: Foster Provostand I discussed the merits of ROC curves vs. accuracy
estimation. Here is a quick summary of our discussion.The "Receiver Operating
Characteristic" (ROC) curve is an alternative to accuracy for the evaluation
of learning algorithms on natural datasets. The ROC curve is acurveand not a
single number statistic. In particular, this means that the comparison of two
algorithms on a dataset does not always produce an obvious order.Accuracy (= 1
- error rate) is a standard method used to evaluate learning algorithms. It is
a single-number summary of performance.AROC is the area under the ROC curve.
It is a single number summary of performance.The comparison of these metrics
is a subtle affair, because in machine learning, they are compared on
different natural datasets. This makes some sense if we accept the hypothesis
"Performance on past learning problems (roughly) predicts performance on
future learning problems."The ROC vs. accuracy discussion is often conflated
with "is the</p><p>4 0.54041851 <a title="56-lsi-4" href="../hunch_net-2005/hunch_net-2005-11-16-The_Everything_Ensemble_Edge.html">131 hunch net-2005-11-16-The Everything Ensemble Edge</a></p>
<p>Introduction: Rich Caruana,Alexandru Niculescu, Geoff Crew, and Alex Ksikes have donea lot
of empirical testingwhich shows thatusing all methods to make a predictionis
more powerful than using any single method. This is in rough agreement with
the Bayesian way of solving problems, but based upon a different (essentially
empirical) motivation. A rough summary is:Take all of {decision trees, boosted
decision trees, bagged decision trees, boosted decision stumps, K nearest
neighbors, neural networks, SVM} with all reasonable parameter settings.Run
the methods on each problem of 8 problems with a large test set, calibrating
margins using eithersigmoid fittingorisotonic regression.For each loss of
{accuracy, area under the ROC curve, cross entropy, squared error, etcâ&euro;Ś}
evaluate the average performance of the method.A series of conclusions can be
drawn from the observations.(Calibrated) boosted decision trees appear to
perform best, in general although support vector machines and neural networks
give cred</p><p>5 0.52510375 <a title="56-lsi-5" href="../hunch_net-2005/hunch_net-2005-02-03-Learning_Theory%2C_by_assumption.html">12 hunch net-2005-02-03-Learning Theory, by assumption</a></p>
<p>Introduction: One way to organize learning theory is by assumption (in theassumption = axiom
sense), from no assumptions to many assumptions. As you travel down this list,
the statements become stronger, but the scope of applicability decreases.No
assumptionsOnline learningThere exist a meta prediction algorithm which
compete well with the best element of any set of prediction
algorithms.Universal LearningUsing a "bias" of 2- description length of turing
machinein learning is equivalent to all other computable biases up to some
constant.ReductionsThe ability to predict well on classification problems is
equivalent to the ability to predict well on many other learning
problems.Independent and Identically Distributed (IID) DataPerformance
PredictionBased upon past performance, you can predict future
performance.Uniform ConvergencePerformance prediction works even after
choosing classifiers based on the data from large sets of classifiers.IID and
partial constraints on the data sourcePAC LearningThere</p><p>6 0.48451349 <a title="56-lsi-6" href="../hunch_net-2006/hunch_net-2006-07-06-Branch_Prediction_Competition.html">190 hunch net-2006-07-06-Branch Prediction Competition</a></p>
<p>7 0.47157907 <a title="56-lsi-7" href="../hunch_net-2005/hunch_net-2005-03-02-Prior%2C_%26%238220%3BPrior%26%238221%3B_and_Bias.html">34 hunch net-2005-03-02-Prior, &#8220;Prior&#8221; and Bias</a></p>
<p>8 0.46401194 <a title="56-lsi-8" href="../hunch_net-2006/hunch_net-2006-03-12-Online_learning_or_online_preservation_of_learning%3F.html">163 hunch net-2006-03-12-Online learning or online preservation of learning?</a></p>
<p>9 0.45843402 <a title="56-lsi-9" href="../hunch_net-2005/hunch_net-2005-02-28-Regularization.html">33 hunch net-2005-02-28-Regularization</a></p>
<p>10 0.45329756 <a title="56-lsi-10" href="../hunch_net-2009/hunch_net-2009-06-26-Netflix_nearly_done.html">362 hunch net-2009-06-26-Netflix nearly done</a></p>
<p>11 0.4528642 <a title="56-lsi-11" href="../hunch_net-2008/hunch_net-2008-08-24-Mass_Customized_Medicine_in_the_Future%3F.html">314 hunch net-2008-08-24-Mass Customized Medicine in the Future?</a></p>
<p>12 0.45096201 <a title="56-lsi-12" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>13 0.446123 <a title="56-lsi-13" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>14 0.44520873 <a title="56-lsi-14" href="../hunch_net-2005/hunch_net-2005-01-24-Holy_grails_of_machine_learning%3F.html">2 hunch net-2005-01-24-Holy grails of machine learning?</a></p>
<p>15 0.43964493 <a title="56-lsi-15" href="../hunch_net-2005/hunch_net-2005-06-18-Lower_Bounds_for_Learning_Reductions.html">83 hunch net-2005-06-18-Lower Bounds for Learning Reductions</a></p>
<p>16 0.41953191 <a title="56-lsi-16" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>17 0.41729304 <a title="56-lsi-17" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>18 0.41603184 <a title="56-lsi-18" href="../hunch_net-2005/hunch_net-2005-11-28-A_question_of_quantification.html">133 hunch net-2005-11-28-A question of quantification</a></p>
<p>19 0.41190371 <a title="56-lsi-19" href="../hunch_net-2005/hunch_net-2005-03-15-The_State_of_Tight_Bounds.html">41 hunch net-2005-03-15-The State of Tight Bounds</a></p>
<p>20 0.41109115 <a title="56-lsi-20" href="../hunch_net-2006/hunch_net-2006-05-05-An_ICML_reject.html">177 hunch net-2006-05-05-An ICML reject</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(42, 0.215), (48, 0.569), (68, 0.038)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.90732628 <a title="56-lda-1" href="../hunch_net-2005/hunch_net-2005-04-27-DARPA_project%3A_LAGR.html">63 hunch net-2005-04-27-DARPA project: LAGR</a></p>
<p>Introduction: Larry Jackal has set up theLAGR("Learning Applied to Ground Robotics") project
(and competition) which seems to be quite well designed. Features include:Many
participants (8 going on 12?)Standardized hardware. In theDARPA grand
challengecontestants entering with motorcycles are at a severe disadvantage to
those entering with a Hummer. Similarly, contestants using more powerful
sensors can gain huge advantages.Monthly contests, with full feedback (but
since the hardware is standardized, only code is shipped). One of the premises
of the program is that robust systems are desired. Monthly evaluations at
different locations can help measure this and provide data.Attacks a known
hard problem. (cross country driving)</p><p>same-blog 2 0.77305943 <a title="56-lda-2" href="../hunch_net-2005/hunch_net-2005-04-14-Families_of_Learning_Theory_Statements.html">56 hunch net-2005-04-14-Families of Learning Theory Statements</a></p>
<p>Introduction: The diagram above shows a very broad viewpoint of learning theory.arrowTypical
statementExamplesPast->PastSome prediction algorithmAdoes almost as well as
any of a set of algorithms.Weighted MajorityPast->FutureAssuming independent
samples, past performance predicts future performance.PAC analysis, ERM
analysisFuture->FutureFuture prediction performance on subproblems implies
future prediction performance using algorithmA.ECOC, ProbingA basic question
is: Are there other varieties of statements of this type?Avrimnoted that there
are also "arrows between arrows": generic methods for transforming between
Past->Past statements and Past->Future statements. Are there others?</p><p>3 0.69313872 <a title="56-lda-3" href="../hunch_net-2005/hunch_net-2005-09-14-The_Predictionist_Viewpoint.html">112 hunch net-2005-09-14-The Predictionist Viewpoint</a></p>
<p>Introduction: Virtually every discipline of significant human endeavor has a way explaining
itself as fundamental and important. In all the cases I know of, they are both
right (they are vital) and wrong (they are not solely vital).Politics. This is
the one that everyone is familiar with at the moment. "What could be more
important than the process of making decisions?"Science and Technology. This
is the one that we-the-academics are familiar with. "The loss of modern
science and technology would be catastrophic."Military. "Without the military,
a nation will be invaded and destroyed."(insert your favorite here)Within
science and technology, the same thing happens again.Mathematics. "What could
be more important than a precise language for establishing truths?"Physics.
"Nothing is more fundamental than the laws which govern the universe.
Understanding them is the key to understanding everything else."Biology.
"Without life, we wouldn't be here, so clearly the study of life is
fundamental."Computer S</p><p>4 0.65486979 <a title="56-lda-4" href="../hunch_net-2007/hunch_net-2007-09-16-Optimizing_Machine_Learning_Programs.html">262 hunch net-2007-09-16-Optimizing Machine Learning Programs</a></p>
<p>Introduction: Machine learning is often computationally bounded which implies that the
ability to write fast code becomes important if you ever want to implement a
machine learning algorithm. Basic tactical optimizations are covered
wellelsewhere, but I haven't seen a reasonable guide to higher level
optimizations, which are the most important in my experience. Here are some of
the higher level optimizations I've often found useful.Algorithmic Improvement
First. This is Hard, but it is the most important consideration, and typically
yields the most benefits. Good optimizations here are publishable. In the
context of machine learning, you should be familiar with the arguments for
online vs. batch learning.Choice of Language. There are many arguments about
thechoice of language. Sometimes you don't have a choice when interfacing with
other people. Personally, I favor C/C++ when I want to write fast code. This
(admittedly) makes me a slower programmer than when using higher level
languages. (Sometimes</p><p>5 0.62910622 <a title="56-lda-5" href="../hunch_net-2005/hunch_net-2005-05-17-A_Short_Guide_to_PhD_Graduate_Study.html">73 hunch net-2005-05-17-A Short Guide to PhD Graduate Study</a></p>
<p>Introduction: Graduate study is a mysterious and uncertain process. This easiest way to see
this is by noting that a very old advisor/student mechanism is preferred.
There is no known succesful mechanism for "mass producing" PhDs as is done (in
some sense) for undergraduate and masters study. Here are a few hints that
might be useful to prospective or current students based on my own
experience.Masters or PhD(a) You want a PhD if you want to do research. (b)
You want a masters if you want to make money. People wanting (b) will be
manifestly unhappy with (a) because it typically means years of low pay.
People wanting (a) should try to avoid (b) because it prolongs an already long
process.Attitude.Manystudents struggle for awhile with the wrong attitude
towards research. Most students come into graduate school with 16-19 years of
schooling where the principle means of success is proving that you know
something via assignments, tests, etcâ&euro;Ś Research doesnotwork this way. Research
is what a PhD is about.</p><p>6 0.56331211 <a title="56-lda-6" href="../hunch_net-2009/hunch_net-2009-08-03-Carbon_in_Computer_Science_Research.html">366 hunch net-2009-08-03-Carbon in Computer Science Research</a></p>
<p>7 0.37335813 <a title="56-lda-7" href="../hunch_net-2005/hunch_net-2005-09-10-%26%238220%3BFailure%26%238221%3B_is_an_option.html">110 hunch net-2005-09-10-&#8220;Failure&#8221; is an option</a></p>
<p>8 0.36136234 <a title="56-lda-8" href="../hunch_net-2006/hunch_net-2006-09-28-Programming_Languages_for_Machine_Learning_Implementations.html">210 hunch net-2006-09-28-Programming Languages for Machine Learning Implementations</a></p>
<p>9 0.35374072 <a title="56-lda-9" href="../hunch_net-2006/hunch_net-2006-01-23-On_Coding_via_Mutual_Information_%26%23038%3B_Bayes_Nets.html">150 hunch net-2006-01-23-On Coding via Mutual Information &#038; Bayes Nets</a></p>
<p>10 0.35337552 <a title="56-lda-10" href="../hunch_net-2008/hunch_net-2008-04-27-Watchword%3A_Supervised_Learning.html">299 hunch net-2008-04-27-Watchword: Supervised Learning</a></p>
<p>11 0.35335714 <a title="56-lda-11" href="../hunch_net-2012/hunch_net-2012-10-26-ML_Symposium_and_Strata-Hadoop_World.html">475 hunch net-2012-10-26-ML Symposium and Strata-Hadoop World</a></p>
<p>12 0.35288846 <a title="56-lda-12" href="../hunch_net-2005/hunch_net-2005-03-29-Academic_Mechanism_Design.html">48 hunch net-2005-03-29-Academic Mechanism Design</a></p>
<p>13 0.35219818 <a title="56-lda-13" href="../hunch_net-2005/hunch_net-2005-05-06-Don%26%238217%3Bt_mix_the_solution_into_the_problem.html">67 hunch net-2005-05-06-Don&#8217;t mix the solution into the problem</a></p>
<p>14 0.35194868 <a title="56-lda-14" href="../hunch_net-2005/hunch_net-2005-12-09-Some_NIPS_papers.html">138 hunch net-2005-12-09-Some NIPS papers</a></p>
<p>15 0.35191324 <a title="56-lda-15" href="../hunch_net-2006/hunch_net-2006-06-15-IJCAI_is_out_of_season.html">184 hunch net-2006-06-15-IJCAI is out of season</a></p>
<p>16 0.35187376 <a title="56-lda-16" href="../hunch_net-2005/hunch_net-2005-07-07-The_Limits_of_Learning_Theory.html">90 hunch net-2005-07-07-The Limits of Learning Theory</a></p>
<p>17 0.35136774 <a title="56-lda-17" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<p>18 0.35109112 <a title="56-lda-18" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>19 0.35105762 <a title="56-lda-19" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>20 0.34647742 <a title="56-lda-20" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
