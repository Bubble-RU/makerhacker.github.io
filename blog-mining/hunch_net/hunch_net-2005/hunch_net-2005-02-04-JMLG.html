<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>13 hunch net-2005-02-04-JMLG</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-13" href="#">hunch_net-2005-13</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>13 hunch net-2005-02-04-JMLG</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-13-html" href="http://hunch.net/?p=16">html</a></p><p>Introduction: TheJournal of Machine Learning Gossiphas some fine satire about learning
research. In particular, theguidesare amusing and remarkably true.As in all
things, it's easy to criticize the way things are and harder to make them
better.</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('amusing', 0.575), ('remarkably', 0.445), ('fine', 0.388), ('harder', 0.304), ('things', 0.303), ('particular', 0.186), ('easy', 0.173), ('way', 0.145), ('better', 0.144), ('make', 0.129), ('machine', 0.089), ('learning', 0.075)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="13-tfidf-1" href="../hunch_net-2005/hunch_net-2005-02-04-JMLG.html">13 hunch net-2005-02-04-JMLG</a></p>
<p>Introduction: TheJournal of Machine Learning Gossiphas some fine satire about learning
research. In particular, theguidesare amusing and remarkably true.As in all
things, it's easy to criticize the way things are and harder to make them
better.</p><p>2 0.095169678 <a title="13-tfidf-2" href="../hunch_net-2006/hunch_net-2006-12-12-Interesting_Papers_at_NIPS_2006.html">224 hunch net-2006-12-12-Interesting Papers at NIPS 2006</a></p>
<p>Introduction: Here are some papers that I found surprisingly interesting.Yoshua Bengio,
Pascal Lamblin, Dan Popovici, Hugo Larochelle,Greedy Layer-wise Training of
Deep Networks. Empirically investigates some of the design choices behind deep
belief networks.Long Zhu, Yuanhao Chen,Alan YuilleUnsupervised Learning of a
Probabilistic Grammar for Object Detection and Parsing. An unsupervised method
for detecting objects using simple feature filters that works remarkably well
on the (supervised)caltech-101 dataset.Shai Ben-David,John Blitzer,Koby
Crammer, andFernando Pereira,Analysis of Representations for Domain
Adaptation. This is the first analysis I've seen of learning with respect to
samples drawn differently from the evaluation distribution which depends on
reasonable measurable quantities.All of these papers turn out to have a common
theme--the power of unlabeled data to do generically useful things.</p><p>3 0.069779418 <a title="13-tfidf-3" href="../hunch_net-2005/hunch_net-2005-04-23-Advantages_and_Disadvantages_of_Bayesian_Learning.html">60 hunch net-2005-04-23-Advantages and Disadvantages of Bayesian Learning</a></p>
<p>Introduction: I don't consider myself a "Bayesian", but I do try hard to understand why
Bayesian learning works. For the purposes of this post, Bayesian learning is a
simple process of:Specify a prior over world models.Integrate using Bayes law
with respect to all observed information to compute a posterior over world
models.Predict according to the posterior.Bayesian learning has many
advantages over other learning programs:InterpolationBayesian learning methods
interpolate all the way to pure engineering. When faced with any learning
problem, there is a choice of how much time and effort a human vs. a computer
puts in. (For example, the mars rover pathfinding algorithms are almost
entirely engineered.) When creating an engineered system, you build a model of
the world and then find a good controller in that model. Bayesian methods
interpolate to this extreme because the Bayesian prior can be a delta function
on one model of the world. What this means is that a recipe of "think harder"
(about speci</p><p>4 0.069125846 <a title="13-tfidf-4" href="../hunch_net-2010/hunch_net-2010-12-26-NIPS_2010.html">420 hunch net-2010-12-26-NIPS 2010</a></p>
<p>Introduction: I enjoyed attendingNIPSthis year, with several things interesting me. For the
conference itself:Peter Welinder,Steve Branson,Serge Belongie, andPietro
Perona,The Multidimensional Wisdom of Crowds. This paper is about
usingmechanical turkto get label information, with results superior to a
majority vote approach.David McAllester,Tamir Hazan, andJoseph KeshetDirect
Loss Minimization for Structured Prediction. This is about another technique
for directly optimizing the loss in structured prediction, with an application
to speech recognition.Mohammad SaberianandNuno VasconcelosBoosting Classifier
Cascades. This is about an algorithm for simultaneously optimizing loss and
computation in a classifier cascade construction. There were several other
papers on cascades which are worth looking at if interested.Alan FernandPrasad
Tadepalli,A Computational Decision Theory for Interactive Assistants. This
paper carves out some forms of natural not-MDP problems and shows their RL-
style solution is t</p><p>5 0.062905543 <a title="13-tfidf-5" href="../hunch_net-2011/hunch_net-2011-09-28-Somebody%26%238217%3Bs_Eating_Your_Lunch.html">445 hunch net-2011-09-28-Somebody&#8217;s Eating Your Lunch</a></p>
<p>Introduction: Since we last discussedthe other online learning,Stanfordhas very visibly
started pushing mass teaching inAI,Machine Learning, andDatabases. In
retrospect, it's not too surprising that the next step up in serious online
teaching experiments are occurring at the computer science department of a
university embedded in the land of startups. Numbers on the order of100000are
quite significant--similar in scale to the number ofcomputer science
undergraduate students/yearin the US. Although these populations surely
differ, the fact that theycouldoverlap is worth considering for the
future.It's too soon to say how successful these classes will be and there are
many easy criticisms to make:Registration != Learning… but if only 1/10th
complete these classes, the scale of teaching still surpasses the scale of any
traditional process.1st year excitement != nth year routine… but if only
1/10th take future classes, the scale of teaching still surpasses the scale of
any traditional process.Hello, che</p><p>6 0.06179478 <a title="13-tfidf-6" href="../hunch_net-2008/hunch_net-2008-08-18-Radford_Neal_starts_a_blog.html">313 hunch net-2008-08-18-Radford Neal starts a blog</a></p>
<p>7 0.060167942 <a title="13-tfidf-7" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>8 0.059488349 <a title="13-tfidf-8" href="../hunch_net-2005/hunch_net-2005-08-08-Apprenticeship_Reinforcement_Learning_for_Control.html">101 hunch net-2005-08-08-Apprenticeship Reinforcement Learning for Control</a></p>
<p>9 0.058678925 <a title="13-tfidf-9" href="../hunch_net-2005/hunch_net-2005-07-13-%26%238220%3BSister_Conference%26%238221%3B_presentations.html">93 hunch net-2005-07-13-&#8220;Sister Conference&#8221; presentations</a></p>
<p>10 0.054560009 <a title="13-tfidf-10" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>11 0.053745773 <a title="13-tfidf-11" href="../hunch_net-2005/hunch_net-2005-05-28-Running_A_Machine_Learning_Summer_School.html">75 hunch net-2005-05-28-Running A Machine Learning Summer School</a></p>
<p>12 0.052103128 <a title="13-tfidf-12" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>13 0.047151197 <a title="13-tfidf-13" href="../hunch_net-2005/hunch_net-2005-09-08-Online_Learning_as_the_Mathematics_of_Accountability.html">109 hunch net-2005-09-08-Online Learning as the Mathematics of Accountability</a></p>
<p>14 0.045110527 <a title="13-tfidf-14" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>15 0.044606365 <a title="13-tfidf-15" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>16 0.044462532 <a title="13-tfidf-16" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>17 0.043433663 <a title="13-tfidf-17" href="../hunch_net-2006/hunch_net-2006-09-07-Objective_and_subjective_interpretations_of_probability.html">205 hunch net-2006-09-07-Objective and subjective interpretations of probability</a></p>
<p>18 0.042861465 <a title="13-tfidf-18" href="../hunch_net-2006/hunch_net-2006-08-10-Precision_is_not_accuracy.html">202 hunch net-2006-08-10-Precision is not accuracy</a></p>
<p>19 0.042444639 <a title="13-tfidf-19" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>20 0.042244367 <a title="13-tfidf-20" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.087), (1, -0.0), (2, 0.04), (3, -0.022), (4, 0.004), (5, 0.001), (6, -0.017), (7, -0.001), (8, -0.003), (9, 0.002), (10, 0.029), (11, -0.003), (12, 0.008), (13, -0.01), (14, -0.052), (15, 0.021), (16, -0.012), (17, 0.0), (18, -0.021), (19, -0.037), (20, 0.032), (21, -0.018), (22, 0.004), (23, -0.02), (24, -0.002), (25, -0.029), (26, 0.006), (27, 0.028), (28, 0.005), (29, 0.014), (30, 0.033), (31, -0.038), (32, -0.013), (33, -0.028), (34, -0.087), (35, -0.043), (36, 0.011), (37, 0.015), (38, 0.005), (39, -0.026), (40, 0.057), (41, 0.007), (42, 0.012), (43, 0.0), (44, 0.029), (45, 0.035), (46, -0.029), (47, -0.035), (48, -0.02), (49, 0.014)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.83621323 <a title="13-lsi-1" href="../hunch_net-2005/hunch_net-2005-02-04-JMLG.html">13 hunch net-2005-02-04-JMLG</a></p>
<p>Introduction: TheJournal of Machine Learning Gossiphas some fine satire about learning
research. In particular, theguidesare amusing and remarkably true.As in all
things, it's easy to criticize the way things are and harder to make them
better.</p><p>2 0.57486373 <a title="13-lsi-2" href="../hunch_net-2014/hunch_net-2014-02-16-Metacademy%3A_a_package_manager_for_knowledge.html">493 hunch net-2014-02-16-Metacademy: a package manager for knowledge</a></p>
<p>Introduction: In recent years, there’s been an explosion of free educational resources that
make high-level knowledge and skills accessible to an ever-wider group of
people. In your own field, you probably have a good idea of where to look for
the answer to any particular question. But outside your areas of expertise,
sifting through textbooks, Wikipedia articles, research papers, and online
lectures can be bewildering (unless you’re fortunate enough to have a
knowledgeable colleague to consult). What are the key concepts in the field,
how do they relate to each other, which ones should you learn, and where
should you learn them?Courses are a major vehicle for packaging educational
materials for a broad audience. The trouble is that they’re typically meant to
be consumed linearly, regardless of your specific background or goals. Also,
unless thousands of other people have had the same background and learning
goals, there may not even be a course that fits your needs. Recently, we
(Roger GrosseandCol</p><p>3 0.535285 <a title="13-lsi-3" href="../hunch_net-2011/hunch_net-2011-09-28-Somebody%26%238217%3Bs_Eating_Your_Lunch.html">445 hunch net-2011-09-28-Somebody&#8217;s Eating Your Lunch</a></p>
<p>Introduction: Since we last discussedthe other online learning,Stanfordhas very visibly
started pushing mass teaching inAI,Machine Learning, andDatabases. In
retrospect, it's not too surprising that the next step up in serious online
teaching experiments are occurring at the computer science department of a
university embedded in the land of startups. Numbers on the order of100000are
quite significant--similar in scale to the number ofcomputer science
undergraduate students/yearin the US. Although these populations surely
differ, the fact that theycouldoverlap is worth considering for the
future.It's too soon to say how successful these classes will be and there are
many easy criticisms to make:Registration != Learning… but if only 1/10th
complete these classes, the scale of teaching still surpasses the scale of any
traditional process.1st year excitement != nth year routine… but if only
1/10th take future classes, the scale of teaching still surpasses the scale of
any traditional process.Hello, che</p><p>4 0.51680493 <a title="13-lsi-4" href="../hunch_net-2006/hunch_net-2006-08-10-Precision_is_not_accuracy.html">202 hunch net-2006-08-10-Precision is not accuracy</a></p>
<p>Introduction: In my experience, there are two different groups of people who believe the
same thing: the mathematics encountered in typical machine learning conference
papers is often of questionable value.The two groups who agree on this are
applied machine learning people who have given up on math, and mature
theoreticians who understand the limits of theory.Partly, this is just a
statement about where we are with respect to machine learning. In particular,
we have no mechanism capable of generating a prescription for how to solve all
learning problems. In the absence of such certainty, people try to come up
with formalisms that partially describe and motivate how and why they do
things. This is natural and healthy--we might hope that it will eventually
lead to just such a mechanism.But, part of this is simply an emphasis on
complexity over clarity. A very natural and simple theoretical statement is
often obscured by complexifications. Common sources of complexification
include:GeneralizationBy tr</p><p>5 0.51527679 <a title="13-lsi-5" href="../hunch_net-2013/hunch_net-2013-01-07-NYU_Large_Scale_Machine_Learning_Class.html">478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</a></p>
<p>Introduction: Yann LeCunand I are coteaching a class onLarge Scale Machine Learningstarting
late Januaryat NYU. This class will cover many tricks to get machine learning
working well on datasets with many features, examples, and classes, along with
several elements of deep learning and support systems enabling the
previous.This is not a beginning class--you really need to have taken a basic
machine learning class previously to follow along. Students will be able to
run and experiment with large scale learning algorithms sinceYahoo!has donated
servers which are being configured into a small scaleHadoopcluster. We are
planning to cover the frontier of research in scalable learning algorithms, so
good class projects could easily lead to papers.For me, this is a chance to
teach on many topics of past research. In general, it seems like researchers
should engage in at least occasional teaching of research, both as a proof of
teachability and to see their own research through that lens. More generally,
I</p><p>6 0.49756318 <a title="13-lsi-6" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>7 0.48585179 <a title="13-lsi-7" href="../hunch_net-2009/hunch_net-2009-01-08-Predictive_Analytics_World.html">335 hunch net-2009-01-08-Predictive Analytics World</a></p>
<p>8 0.47998244 <a title="13-lsi-8" href="../hunch_net-2005/hunch_net-2005-03-17-Going_all_the_Way%2C_Sometimes.html">42 hunch net-2005-03-17-Going all the Way, Sometimes</a></p>
<p>9 0.47516066 <a title="13-lsi-9" href="../hunch_net-2007/hunch_net-2007-06-21-Presentation_Preparation.html">249 hunch net-2007-06-21-Presentation Preparation</a></p>
<p>10 0.47383338 <a title="13-lsi-10" href="../hunch_net-2006/hunch_net-2006-03-09-Use_of_Notation.html">162 hunch net-2006-03-09-Use of Notation</a></p>
<p>11 0.46949211 <a title="13-lsi-11" href="../hunch_net-2005/hunch_net-2005-01-19-Why_I_decided_to_run_a_weblog..html">1 hunch net-2005-01-19-Why I decided to run a weblog.</a></p>
<p>12 0.46704561 <a title="13-lsi-12" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>13 0.46497053 <a title="13-lsi-13" href="../hunch_net-2005/hunch_net-2005-08-08-Apprenticeship_Reinforcement_Learning_for_Control.html">101 hunch net-2005-08-08-Apprenticeship Reinforcement Learning for Control</a></p>
<p>14 0.46029943 <a title="13-lsi-14" href="../hunch_net-2006/hunch_net-2006-02-02-Introspectionism_as_a_Disease.html">153 hunch net-2006-02-02-Introspectionism as a Disease</a></p>
<p>15 0.45726681 <a title="13-lsi-15" href="../hunch_net-2012/hunch_net-2012-08-24-Patterns_for_research_in_machine_learning.html">471 hunch net-2012-08-24-Patterns for research in machine learning</a></p>
<p>16 0.45657831 <a title="13-lsi-16" href="../hunch_net-2007/hunch_net-2007-09-18-It%26%238217%3Bs_MDL_Jim%2C_but_not_as_we_know_it%26%238230%3B%28on_Bayes%2C_MDL_and_consistency%29.html">263 hunch net-2007-09-18-It&#8217;s MDL Jim, but not as we know it&#8230;(on Bayes, MDL and consistency)</a></p>
<p>17 0.45582646 <a title="13-lsi-17" href="../hunch_net-2008/hunch_net-2008-07-04-More_Presentation_Preparation.html">307 hunch net-2008-07-04-More Presentation Preparation</a></p>
<p>18 0.45141199 <a title="13-lsi-18" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>19 0.45059124 <a title="13-lsi-19" href="../hunch_net-2005/hunch_net-2005-04-25-Embeddings%3A_what_are_they_good_for%3F.html">61 hunch net-2005-04-25-Embeddings: what are they good for?</a></p>
<p>20 0.44990668 <a title="13-lsi-20" href="../hunch_net-2009/hunch_net-2009-01-21-Nearly_all_natural_problems_require_nonlinearity.html">337 hunch net-2009-01-21-Nearly all natural problems require nonlinearity</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(25, 0.36), (42, 0.264), (74, 0.131)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.86549664 <a title="13-lda-1" href="../hunch_net-2011/hunch_net-2011-01-16-2011_Summer_Conference_Deadline_Season.html">422 hunch net-2011-01-16-2011 Summer Conference Deadline Season</a></p>
<p>Introduction: Machine learning always welcomes the new year with paper deadlines for summer
conferences. This year, we have:ConferencePaper DeadlineWhen/WhereDouble
blind?Author Feedback?NotesICMLFebruary 1June 28-July 2, Bellevue, Washington,
USAYYWeak colocation withACLCOLTFebruary 11July 9-July 11, Budapest,
HungaryNNcolocated withFOCMKDDFebruary 11/18August 21-24, San Diego,
California, USANNUAIMarch 18July 14-17, Barcelona, SpainYNThe larger
conferences are on the west coast in the United States, while the smaller ones
are in Europe.</p><p>2 0.86502039 <a title="13-lda-2" href="../hunch_net-2005/hunch_net-2005-01-26-Watchword%3A_Probability.html">5 hunch net-2005-01-26-Watchword: Probability</a></p>
<p>Introduction: Probability is one of the most confusingly used words in machine learning.
There are at least 3 distinct ways the word is used.BayesianThe Bayesian
notion of probability is a 'degree of belief'. The degree of belief that some
event (i.e. "stock goes up" or "stock goes down") occurs can be measured by
asking a sequence of questions of the form "Would you bet the stock goes up or
down atYto 1 odds?" A consistent better will switch from 'for' to 'against' at
some single value ofY. The probability is thenY/(Y+1). Bayesian probabilities
express lack of knowledge rather than randomization. They are useful in
learning because we often lack knowledge and expressing that lack flexibly
makes the learning algorithms work better. Bayesian Learning uses
'probability' in this way exclusively.FrequentistThe Frequentist notion of
probability is a rate of occurence. A rate of occurrence can be measured by
doing an experiment many times. If an event occursktimes innexperiments then
it has probability ab</p><p>same-blog 3 0.85628915 <a title="13-lda-3" href="../hunch_net-2005/hunch_net-2005-02-04-JMLG.html">13 hunch net-2005-02-04-JMLG</a></p>
<p>Introduction: TheJournal of Machine Learning Gossiphas some fine satire about learning
research. In particular, theguidesare amusing and remarkably true.As in all
things, it's easy to criticize the way things are and harder to make them
better.</p><p>4 0.72121346 <a title="13-lda-4" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">23 hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<p>Introduction: This is apaperby Yann LeCun and Fu Jie Huang published atAISTAT 2005. I found
this paper very difficult to read, but it does have some point about a
computational shortcut.This paper takes for granted that the method of solving
a problem is gradient descent on parameters. Given this assumption, the
question arises: Do you want to do gradient descent on a probabilistic model
or something else?All (conditional) probabilistic models have the formp(y|x) =
f(x,y)/Z(x)whereZ(x) = sumyf(x,y)(the paper calls- log f(x,y)an "energy").
Iffis parameterized by somew, the gradient has a term forZ(x), and hence for
every value ofy. The paper claims, that such models can be optimized for
classification purposes using only the correctyand the othery' not ywhich
maximizesf(x,y). This can even be done on unnormalizable models. The paper
further claims that this can be done with an approximate maximum. These claims
are plausible based on experimental results and intuition.It wouldn't surprise
me to learn</p><p>5 0.61776292 <a title="13-lda-5" href="../hunch_net-2008/hunch_net-2008-11-10-ICML_Reviewing_Criteria.html">325 hunch net-2008-11-10-ICML Reviewing Criteria</a></p>
<p>Introduction: Michael LittmanandLeon Bottouhave decided to use a franchise program chair
approach toreviewing at ICMLthis year. I'll be one of the area chairs, so I
wanted to mention a few things if you are thinking about naming me.I take
reviewing seriously. That means papers to be reviewed are read, the
implications are considered, and decisions are only made after that. I do my
best to be fair, and there are zero subjects that I consider categorical
rejects. I don't consider severalarguments for rejection-not-on-the-merits
reasonable.I am generally interested in papers that (a) analyze new models of
machine learning, (b) provide new algorithms, and (c) show that they work
empirically on plausibly real problems. If a paper has the trifecta, I'm
particularly interested. With 2 out of 3, I might be interested. I often find
papers with only one element harder to accept, including papers with just
(a).I'm a bit tough. I rarely jump-up-and-down about a paper, because I
believe that great progress is ra</p><p>6 0.6150251 <a title="13-lda-6" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>7 0.61461294 <a title="13-lda-7" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>8 0.61428535 <a title="13-lda-8" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>9 0.61389995 <a title="13-lda-9" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>10 0.61112654 <a title="13-lda-10" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>11 0.61079705 <a title="13-lda-11" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>12 0.60790426 <a title="13-lda-12" href="../hunch_net-2005/hunch_net-2005-06-08-Question%3A_%26%238220%3BWhen_is_the_right_time_to_insert_the_loss_function%3F%26%238221%3B.html">79 hunch net-2005-06-08-Question: &#8220;When is the right time to insert the loss function?&#8221;</a></p>
<p>13 0.60788679 <a title="13-lda-13" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>14 0.60680062 <a title="13-lda-14" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>15 0.60626888 <a title="13-lda-15" href="../hunch_net-2006/hunch_net-2006-04-05-What_is_state%3F.html">169 hunch net-2006-04-05-What is state?</a></p>
<p>16 0.60564572 <a title="13-lda-16" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>17 0.60485137 <a title="13-lda-17" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>18 0.60450542 <a title="13-lda-18" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>19 0.60449982 <a title="13-lda-19" href="../hunch_net-2005/hunch_net-2005-01-24-Holy_grails_of_machine_learning%3F.html">2 hunch net-2005-01-24-Holy grails of machine learning?</a></p>
<p>20 0.60437703 <a title="13-lda-20" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
