<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>61 hunch net-2005-04-25-Embeddings: what are they good for?</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-61" href="#">hunch_net-2005-61</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>61 hunch net-2005-04-25-Embeddings: what are they good for?</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-61-html" href="http://hunch.net/?p=66">html</a></p><p>Introduction: I’ve been looking at some recent embeddings work, and am struck by how beautiful the theory and algorithms are. It also makes me wonder, what are embeddings good for?
 
A few things immediately come to mind:
 
(1) For visualization of high-dimensional data sets.
 
In this case, one would like good algorithms for embedding specifically into 2- and 3-dimensional Euclidean spaces.
 
(2) For nonparametric modeling.
 
The usual nonparametric models (histograms, nearest neighbor) often require resources which are exponential in the dimension. So if the data actually lie close to some low-dimensional 
surface, it might be a good idea to first identify this surface and embed the data before applying the model.
 
Incidentally, for applications like these, it’s important to have a functional mapping from high to low dimension, which some techniques do not yield up.
 
(3) As a prelude to classifier learning.
 
The hope here is presumably that learning will be easier in the low-dimensional space,</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 I’ve been looking at some recent embeddings work, and am struck by how beautiful the theory and algorithms are. [sent-1, score-1.029]
</p><p>2 It also makes me wonder, what are embeddings good for? [sent-2, score-0.562]
</p><p>3 A few things immediately come to mind:   (1) For visualization of high-dimensional data sets. [sent-3, score-0.373]
</p><p>4 In this case, one would like good algorithms for embedding specifically into 2- and 3-dimensional Euclidean spaces. [sent-4, score-0.494]
</p><p>5 The usual nonparametric models (histograms, nearest neighbor) often require resources which are exponential in the dimension. [sent-6, score-0.848]
</p><p>6 So if the data actually lie close to some low-dimensional  surface, it might be a good idea to first identify this surface and embed the data before applying the model. [sent-7, score-1.335]
</p><p>7 Incidentally, for applications like these, it’s important to have a functional mapping from high to low dimension, which some techniques do not yield up. [sent-8, score-0.664]
</p><p>8 The hope here is presumably that learning will be easier in the low-dimensional space, because of (i) better generalization and (ii) a more “natural” layout of the data. [sent-10, score-0.466]
</p><p>9 I’d be curious to know of other uses for embeddings. [sent-11, score-0.207]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('embeddings', 0.485), ('surface', 0.299), ('nonparametric', 0.242), ('embed', 0.162), ('embedding', 0.162), ('ii', 0.162), ('layout', 0.162), ('struck', 0.162), ('visualization', 0.15), ('lie', 0.15), ('euclidean', 0.141), ('usual', 0.135), ('curious', 0.135), ('beautiful', 0.129), ('wonder', 0.129), ('mapping', 0.129), ('resources', 0.129), ('dimension', 0.125), ('identify', 0.125), ('data', 0.124), ('incidentally', 0.121), ('presumably', 0.117), ('specifically', 0.117), ('functional', 0.114), ('generalization', 0.109), ('applying', 0.109), ('neighbor', 0.104), ('nearest', 0.099), ('exponential', 0.099), ('immediately', 0.099), ('mind', 0.097), ('recent', 0.096), ('close', 0.094), ('yield', 0.087), ('looking', 0.083), ('easier', 0.078), ('require', 0.078), ('good', 0.077), ('low', 0.076), ('classifier', 0.074), ('algorithms', 0.074), ('uses', 0.072), ('space', 0.072), ('actually', 0.071), ('techniques', 0.067), ('models', 0.066), ('high', 0.064), ('like', 0.064), ('applications', 0.063), ('natural', 0.061)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="61-tfidf-1" href="../hunch_net-2005/hunch_net-2005-04-25-Embeddings%3A_what_are_they_good_for%3F.html">61 hunch net-2005-04-25-Embeddings: what are they good for?</a></p>
<p>Introduction: I’ve been looking at some recent embeddings work, and am struck by how beautiful the theory and algorithms are. It also makes me wonder, what are embeddings good for?
 
A few things immediately come to mind:
 
(1) For visualization of high-dimensional data sets.
 
In this case, one would like good algorithms for embedding specifically into 2- and 3-dimensional Euclidean spaces.
 
(2) For nonparametric modeling.
 
The usual nonparametric models (histograms, nearest neighbor) often require resources which are exponential in the dimension. So if the data actually lie close to some low-dimensional 
surface, it might be a good idea to first identify this surface and embed the data before applying the model.
 
Incidentally, for applications like these, it’s important to have a functional mapping from high to low dimension, which some techniques do not yield up.
 
(3) As a prelude to classifier learning.
 
The hope here is presumably that learning will be easier in the low-dimensional space,</p><p>2 0.10825843 <a title="61-tfidf-2" href="../hunch_net-2005/hunch_net-2005-03-22-Active_learning.html">45 hunch net-2005-03-22-Active learning</a></p>
<p>Introduction: Often, unlabeled data is easy to come by but labels are expensive. For instance, if you’re building a speech recognizer, it’s easy enough to get raw speech samples — just walk around with a microphone — but labeling even one of these samples is a tedious process in which a human must examine the speech signal and carefully segment it into phonemes. In the field of active learning, the goal is as usual to construct an accurate classifier, but the labels of the data points are initially hidden and there is a charge for each label you want revealed. The hope is that by intelligent adaptive querying, you can get away with significantly fewer labels than you would need in a regular supervised learning framework.
 
Here’s an example. Suppose the data lie on the real line, and the classifiers are simple thresholding functions, H = {h w }: 
  h w (x) = 1 if x > w, and 0 otherwise.  
 
VC theory tells us that if the underlying distribution P can be classified perfectly by some hypothesis in H (</p><p>3 0.10240769 <a title="61-tfidf-3" href="../hunch_net-2005/hunch_net-2005-08-11-Why_Manifold-Based_Dimension_Reduction_Techniques%3F.html">102 hunch net-2005-08-11-Why Manifold-Based Dimension Reduction Techniques?</a></p>
<p>Introduction: Manifold based dimension-reduction algorithms share the following general outline.  
 
Given: a metric  d()  and a set of points  S 
  
 Construct a graph with a point in every node and every edge connecting to the node of one of the  k -nearest neighbors.  Associate with the edge a weight which is the distance between the points in the connected nodes. 
 Digest the graph.  This might include computing the shortest path between all points or figuring out how to linearly interpolate the point from it’s neighbors. 
 Find a set of points in a low dimensional space which preserve the digested properties. 
  
Examples include LLE, Isomap (which I worked on), Hessian-LLE, SDE, and many others.  The hope with these algorithms is that they can recover the low dimensional structure of point sets in high dimensional spaces.  Many of them can be shown to work in interesting ways producing various compelling pictures.
 
Despite doing some early work in this direction, I suffer from a motivational</p><p>4 0.077129096 <a title="61-tfidf-4" href="../hunch_net-2005/hunch_net-2005-12-14-More_NIPS_Papers_II.html">140 hunch net-2005-12-14-More NIPS Papers II</a></p>
<p>Introduction: I thought this was a very good NIPS with many excellent papers. The following are a few NIPS papers which I liked and I hope to study more carefully when I get the chance. The list is not exhaustive and in no particular order…
  
 Preconditioner Approximations for Probabilistic Graphical Models. 
Pradeeep Ravikumar and John Lafferty. 
I thought the use of preconditioner methods from solving linear systems in the context of approximate inference was novel and interesting. The results look good and I’d like to understand the limitations.
 
 Rodeo: Sparse nonparametric regression in high dimensions. 
John Lafferty and Larry Wasserman. 
A very interesting approach to feature selection in nonparametric regression from a frequentist framework. The use of lengthscale variables in each dimension reminds me a lot of  ‘Automatic Relevance Determination’ in Gaussian process regression — it would be interesting to compare Rodeo to ARD in GPs.
 
 Interpolating between types and tokens by estimating</p><p>5 0.075116053 <a title="61-tfidf-5" href="../hunch_net-2009/hunch_net-2009-06-03-Functionally_defined_Nonlinear_Dynamic_Models.html">359 hunch net-2009-06-03-Functionally defined Nonlinear Dynamic Models</a></p>
<p>Introduction: Suppose we have a set of observations over time  x 1 ,x 2 ,…,x t   and want to predict some future event  y t+1  .  An inevitable problem arises, because learning a predictor  h(x 1 ,…,x t )  of  y t+1   is generically intractable due to the size of the input.  To make this problem tractable, what’s necessary is a method for summarizing the relevant information in past observations for the purpose of prediction in the future.  In other words, state is required.
 
Existing approaches for deriving state have some limitations.
  
  Hidden Markov models  learned with EM suffer from local minima, use tabular learning approaches which provide dubious generalization ability, and often require substantial a.priori specification of the observations. 
  Kalman Filters  and  Particle Filters  are very parametric in the sense that substantial information must be specified up front. 
 Dynamic Bayesian Networks ( graphical models  through time) require substantial a.priori specification and often re</p><p>6 0.071586087 <a title="61-tfidf-6" href="../hunch_net-2005/hunch_net-2005-01-24-The_Humanloop_Spectrum_of_Machine_Learning.html">3 hunch net-2005-01-24-The Humanloop Spectrum of Machine Learning</a></p>
<p>7 0.069397599 <a title="61-tfidf-7" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>8 0.060687404 <a title="61-tfidf-8" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>9 0.060011253 <a title="61-tfidf-9" href="../hunch_net-2005/hunch_net-2005-07-10-Thinking_the_Unthought.html">91 hunch net-2005-07-10-Thinking the Unthought</a></p>
<p>10 0.057890326 <a title="61-tfidf-10" href="../hunch_net-2007/hunch_net-2007-06-19-How_is_Compressed_Sensing_going_to_change_Machine_Learning_%3F.html">248 hunch net-2007-06-19-How is Compressed Sensing going to change Machine Learning ?</a></p>
<p>11 0.057863772 <a title="61-tfidf-11" href="../hunch_net-2007/hunch_net-2007-09-18-It%26%238217%3Bs_MDL_Jim%2C_but_not_as_we_know_it%26%238230%3B%28on_Bayes%2C_MDL_and_consistency%29.html">263 hunch net-2007-09-18-It&#8217;s MDL Jim, but not as we know it&#8230;(on Bayes, MDL and consistency)</a></p>
<p>12 0.057144411 <a title="61-tfidf-12" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>13 0.056782681 <a title="61-tfidf-13" href="../hunch_net-2007/hunch_net-2007-02-22-Create_Your_Own_ICML_Workshop.html">234 hunch net-2007-02-22-Create Your Own ICML Workshop</a></p>
<p>14 0.056047693 <a title="61-tfidf-14" href="../hunch_net-2009/hunch_net-2009-01-07-Interesting_Papers_at_SODA_2009.html">334 hunch net-2009-01-07-Interesting Papers at SODA 2009</a></p>
<p>15 0.05572373 <a title="61-tfidf-15" href="../hunch_net-2005/hunch_net-2005-03-02-Prior%2C_%26%238220%3BPrior%26%238221%3B_and_Bias.html">34 hunch net-2005-03-02-Prior, &#8220;Prior&#8221; and Bias</a></p>
<p>16 0.055492707 <a title="61-tfidf-16" href="../hunch_net-2005/hunch_net-2005-03-28-Open_Problems_for_Colt.html">47 hunch net-2005-03-28-Open Problems for Colt</a></p>
<p>17 0.053872 <a title="61-tfidf-17" href="../hunch_net-2005/hunch_net-2005-07-04-The_Health_of_COLT.html">89 hunch net-2005-07-04-The Health of COLT</a></p>
<p>18 0.052430976 <a title="61-tfidf-18" href="../hunch_net-2005/hunch_net-2005-12-07-Is_the_Google_way_the_way_for_machine_learning%3F.html">136 hunch net-2005-12-07-Is the Google way the way for machine learning?</a></p>
<p>19 0.051254086 <a title="61-tfidf-19" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>20 0.051004447 <a title="61-tfidf-20" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.122), (1, 0.032), (2, -0.01), (3, 0.001), (4, 0.049), (5, -0.022), (6, 0.013), (7, 0.003), (8, 0.044), (9, -0.06), (10, 0.013), (11, 0.0), (12, -0.042), (13, 0.016), (14, 0.011), (15, -0.03), (16, 0.018), (17, 0.045), (18, 0.007), (19, -0.051), (20, 0.048), (21, 0.018), (22, -0.022), (23, -0.006), (24, 0.019), (25, 0.003), (26, 0.012), (27, 0.043), (28, -0.014), (29, -0.062), (30, -0.012), (31, -0.093), (32, -0.01), (33, 0.011), (34, -0.008), (35, 0.003), (36, 0.026), (37, 0.019), (38, -0.002), (39, -0.073), (40, 0.028), (41, 0.068), (42, 0.008), (43, -0.008), (44, -0.046), (45, 0.152), (46, 0.067), (47, 0.081), (48, -0.066), (49, -0.037)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95513427 <a title="61-lsi-1" href="../hunch_net-2005/hunch_net-2005-04-25-Embeddings%3A_what_are_they_good_for%3F.html">61 hunch net-2005-04-25-Embeddings: what are they good for?</a></p>
<p>Introduction: I’ve been looking at some recent embeddings work, and am struck by how beautiful the theory and algorithms are. It also makes me wonder, what are embeddings good for?
 
A few things immediately come to mind:
 
(1) For visualization of high-dimensional data sets.
 
In this case, one would like good algorithms for embedding specifically into 2- and 3-dimensional Euclidean spaces.
 
(2) For nonparametric modeling.
 
The usual nonparametric models (histograms, nearest neighbor) often require resources which are exponential in the dimension. So if the data actually lie close to some low-dimensional 
surface, it might be a good idea to first identify this surface and embed the data before applying the model.
 
Incidentally, for applications like these, it’s important to have a functional mapping from high to low dimension, which some techniques do not yield up.
 
(3) As a prelude to classifier learning.
 
The hope here is presumably that learning will be easier in the low-dimensional space,</p><p>2 0.58177495 <a title="61-lsi-2" href="../hunch_net-2010/hunch_net-2010-08-24-Alex_Smola_starts_a_blog.html">408 hunch net-2010-08-24-Alex Smola starts a blog</a></p>
<p>Introduction: Adventures in Data Land .</p><p>3 0.58143061 <a title="61-lsi-3" href="../hunch_net-2005/hunch_net-2005-08-11-Why_Manifold-Based_Dimension_Reduction_Techniques%3F.html">102 hunch net-2005-08-11-Why Manifold-Based Dimension Reduction Techniques?</a></p>
<p>Introduction: Manifold based dimension-reduction algorithms share the following general outline.  
 
Given: a metric  d()  and a set of points  S 
  
 Construct a graph with a point in every node and every edge connecting to the node of one of the  k -nearest neighbors.  Associate with the edge a weight which is the distance between the points in the connected nodes. 
 Digest the graph.  This might include computing the shortest path between all points or figuring out how to linearly interpolate the point from it’s neighbors. 
 Find a set of points in a low dimensional space which preserve the digested properties. 
  
Examples include LLE, Isomap (which I worked on), Hessian-LLE, SDE, and many others.  The hope with these algorithms is that they can recover the low dimensional structure of point sets in high dimensional spaces.  Many of them can be shown to work in interesting ways producing various compelling pictures.
 
Despite doing some early work in this direction, I suffer from a motivational</p><p>4 0.52469581 <a title="61-lsi-4" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>Introduction: At  KDD  I enjoyed  Stephen Boyd ‘s invited talk about optimization quite a bit.  However, the most interesting talk for me was  David Haussler ‘s.  His talk started out with a formidable load of biological complexity.  About half-way through you start wondering, “can this be used to help with cancer?”  And at the end he connects it directly to use with a call to arms for the audience: cure cancer.  The core thesis here is that cancer is a complex set of diseases which can be distentangled via genetic assays, allowing attacking the specific signature of individual cancers.  However, the data quantity and complex dependencies within the data require systematic and relatively automatic prediction and analysis algorithms of the kind that we are best familiar with.
 
Some of the papers which interested me are:
  
  Kai-Wei Chang  and  Dan Roth ,  Selective Block Minimization for Faster Convergence of Limited Memory Large-Scale Linear Models , which is about effectively using a hard-example</p><p>5 0.52193379 <a title="61-lsi-5" href="../hunch_net-2005/hunch_net-2005-12-14-More_NIPS_Papers_II.html">140 hunch net-2005-12-14-More NIPS Papers II</a></p>
<p>Introduction: I thought this was a very good NIPS with many excellent papers. The following are a few NIPS papers which I liked and I hope to study more carefully when I get the chance. The list is not exhaustive and in no particular order…
  
 Preconditioner Approximations for Probabilistic Graphical Models. 
Pradeeep Ravikumar and John Lafferty. 
I thought the use of preconditioner methods from solving linear systems in the context of approximate inference was novel and interesting. The results look good and I’d like to understand the limitations.
 
 Rodeo: Sparse nonparametric regression in high dimensions. 
John Lafferty and Larry Wasserman. 
A very interesting approach to feature selection in nonparametric regression from a frequentist framework. The use of lengthscale variables in each dimension reminds me a lot of  ‘Automatic Relevance Determination’ in Gaussian process regression — it would be interesting to compare Rodeo to ARD in GPs.
 
 Interpolating between types and tokens by estimating</p><p>6 0.5131954 <a title="61-lsi-6" href="../hunch_net-2005/hunch_net-2005-01-24-The_Humanloop_Spectrum_of_Machine_Learning.html">3 hunch net-2005-01-24-The Humanloop Spectrum of Machine Learning</a></p>
<p>7 0.50720185 <a title="61-lsi-7" href="../hunch_net-2005/hunch_net-2005-06-29-Not_EM_for_clustering_at_COLT.html">87 hunch net-2005-06-29-Not EM for clustering at COLT</a></p>
<p>8 0.50336862 <a title="61-lsi-8" href="../hunch_net-2005/hunch_net-2005-03-22-Active_learning.html">45 hunch net-2005-03-22-Active learning</a></p>
<p>9 0.48887438 <a title="61-lsi-9" href="../hunch_net-2007/hunch_net-2007-12-20-Cool_and_Interesting_things_at_NIPS%2C_take_three.html">280 hunch net-2007-12-20-Cool and Interesting things at NIPS, take three</a></p>
<p>10 0.48778152 <a title="61-lsi-10" href="../hunch_net-2007/hunch_net-2007-09-18-It%26%238217%3Bs_MDL_Jim%2C_but_not_as_we_know_it%26%238230%3B%28on_Bayes%2C_MDL_and_consistency%29.html">263 hunch net-2007-09-18-It&#8217;s MDL Jim, but not as we know it&#8230;(on Bayes, MDL and consistency)</a></p>
<p>11 0.48604968 <a title="61-lsi-11" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>12 0.47530392 <a title="61-lsi-12" href="../hunch_net-2005/hunch_net-2005-12-07-Is_the_Google_way_the_way_for_machine_learning%3F.html">136 hunch net-2005-12-07-Is the Google way the way for machine learning?</a></p>
<p>13 0.47340214 <a title="61-lsi-13" href="../hunch_net-2005/hunch_net-2005-12-11-More_NIPS_Papers.html">139 hunch net-2005-12-11-More NIPS Papers</a></p>
<p>14 0.47248381 <a title="61-lsi-14" href="../hunch_net-2010/hunch_net-2010-05-02-What%26%238217%3Bs_the_difference_between_gambling_and_rewarding_good_prediction%3F.html">397 hunch net-2010-05-02-What&#8217;s the difference between gambling and rewarding good prediction?</a></p>
<p>15 0.46803519 <a title="61-lsi-15" href="../hunch_net-2006/hunch_net-2006-01-23-On_Coding_via_Mutual_Information_%26%23038%3B_Bayes_Nets.html">150 hunch net-2006-01-23-On Coding via Mutual Information &#038; Bayes Nets</a></p>
<p>16 0.46433586 <a title="61-lsi-16" href="../hunch_net-2008/hunch_net-2008-11-10-ICML_Reviewing_Criteria.html">325 hunch net-2008-11-10-ICML Reviewing Criteria</a></p>
<p>17 0.46243718 <a title="61-lsi-17" href="../hunch_net-2007/hunch_net-2007-08-25-The_Privacy_Problem.html">260 hunch net-2007-08-25-The Privacy Problem</a></p>
<p>18 0.44743007 <a title="61-lsi-18" href="../hunch_net-2005/hunch_net-2005-05-29-Maximum_Margin_Mismatch%3F.html">77 hunch net-2005-05-29-Maximum Margin Mismatch?</a></p>
<p>19 0.44286969 <a title="61-lsi-19" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>20 0.44238809 <a title="61-lsi-20" href="../hunch_net-2012/hunch_net-2012-08-24-Patterns_for_research_in_machine_learning.html">471 hunch net-2012-08-24-Patterns for research in machine learning</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(27, 0.159), (33, 0.463), (38, 0.039), (53, 0.082), (55, 0.035), (94, 0.101)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.89034861 <a title="61-lda-1" href="../hunch_net-2005/hunch_net-2005-04-25-Embeddings%3A_what_are_they_good_for%3F.html">61 hunch net-2005-04-25-Embeddings: what are they good for?</a></p>
<p>Introduction: I’ve been looking at some recent embeddings work, and am struck by how beautiful the theory and algorithms are. It also makes me wonder, what are embeddings good for?
 
A few things immediately come to mind:
 
(1) For visualization of high-dimensional data sets.
 
In this case, one would like good algorithms for embedding specifically into 2- and 3-dimensional Euclidean spaces.
 
(2) For nonparametric modeling.
 
The usual nonparametric models (histograms, nearest neighbor) often require resources which are exponential in the dimension. So if the data actually lie close to some low-dimensional 
surface, it might be a good idea to first identify this surface and embed the data before applying the model.
 
Incidentally, for applications like these, it’s important to have a functional mapping from high to low dimension, which some techniques do not yield up.
 
(3) As a prelude to classifier learning.
 
The hope here is presumably that learning will be easier in the low-dimensional space,</p><p>2 0.83340657 <a title="61-lda-2" href="../hunch_net-2009/hunch_net-2009-04-23-Jonathan_Chang_at_Slycoder.html">350 hunch net-2009-04-23-Jonathan Chang at Slycoder</a></p>
<p>Introduction: Jonathan Chang  has a  research blog  on aspects of machine learning.</p><p>3 0.77685553 <a title="61-lda-3" href="../hunch_net-2009/hunch_net-2009-12-24-Top_graduates_this_season.html">384 hunch net-2009-12-24-Top graduates this season</a></p>
<p>Introduction: I would like to point out 3 graduates this season as having my confidence they are capable of doing great things. 
  
  Daniel Hsu  has diverse papers with diverse coauthors on {active learning, mulitlabeling, temporal learning, …} each covering new algorithms and methods of analysis.  He is also a capable programmer, having helped me with some nitty-gritty details of cluster parallel  Vowpal Wabbit  this summer.  He has an excellent tendency to just get things done. 
  Nicolas Lambert  doesn’t nominally work in machine learning, but I’ve found his work in  elicitation  relevant nevertheless.  In essence, elicitable properties are closely related to learnable properties, and the elicitation complexity is related to a notion of learning complexity.  See the  Surrogate regret bounds paper  for some related discussion.  Few people successfully work at such a general level that it crosses fields, but he’s one of them. 
  Yisong Yue  is deeply focused on interactive learning, which he has a</p><p>4 0.66258687 <a title="61-lda-4" href="../hunch_net-2014/hunch_net-2014-02-16-Metacademy%3A_a_package_manager_for_knowledge.html">493 hunch net-2014-02-16-Metacademy: a package manager for knowledge</a></p>
<p>Introduction: In recent years, there’s been an explosion of free educational resources that make high-level knowledge and skills accessible to an ever-wider group of people. In your own field, you probably have a good idea of where to look for the answer to any particular question. But outside your areas of expertise, sifting through textbooks, Wikipedia articles, research papers, and online lectures can be bewildering (unless you’re fortunate enough to have a knowledgeable colleague to consult). What are the key concepts in the field, how do they relate to each other, which ones should you learn, and where should you learn them?
 
Courses are a major vehicle for packaging educational materials for a broad audience. The trouble is that they’re typically meant to be consumed linearly, regardless of your specific background or goals. Also, unless thousands of other people have had the same background and learning goals, there may not even be a course that fits your needs. Recently, we ( Roger Grosse</p><p>5 0.55273199 <a title="61-lda-5" href="../hunch_net-2007/hunch_net-2007-02-22-Create_Your_Own_ICML_Workshop.html">234 hunch net-2007-02-22-Create Your Own ICML Workshop</a></p>
<p>Introduction: As usual  ICML 2007  will be hosting a  workshop program  to be held this year on June 24th. The success of the program depends on having researchers like you propose interesting workshop topics and then organize the workshops. I’d like to encourage all of you to consider sending a workshop proposal. The proposal deadline has been extended to March 5. See the workshop web-site for details. 
 
Organizing a workshop is a unique way to gather an international group of researchers together to focus for an entire day on a topic of your choosing. I’ve always found that the cost of organizing a workshop is not so large, and very low compared to the benefits. The topic and format of a workshop are limited only by your imagination (and the attractiveness to potential participants) and need not follow the usual model of a mini-conference on a particular ML sub-area.  Hope to see some interesting proposals rolling in.</p><p>6 0.44475627 <a title="61-lda-6" href="../hunch_net-2013/hunch_net-2013-01-07-NYU_Large_Scale_Machine_Learning_Class.html">478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</a></p>
<p>7 0.42665738 <a title="61-lda-7" href="../hunch_net-2007/hunch_net-2007-06-19-How_is_Compressed_Sensing_going_to_change_Machine_Learning_%3F.html">248 hunch net-2007-06-19-How is Compressed Sensing going to change Machine Learning ?</a></p>
<p>8 0.40630099 <a title="61-lda-8" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>9 0.3984046 <a title="61-lda-9" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>10 0.39814451 <a title="61-lda-10" href="../hunch_net-2009/hunch_net-2009-06-03-Functionally_defined_Nonlinear_Dynamic_Models.html">359 hunch net-2009-06-03-Functionally defined Nonlinear Dynamic Models</a></p>
<p>11 0.39666879 <a title="61-lda-11" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>12 0.39609721 <a title="61-lda-12" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>13 0.3958264 <a title="61-lda-13" href="../hunch_net-2007/hunch_net-2007-07-06-Idempotent-capable_Predictors.html">253 hunch net-2007-07-06-Idempotent-capable Predictors</a></p>
<p>14 0.39545706 <a title="61-lda-14" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>15 0.39532194 <a title="61-lda-15" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>16 0.39341521 <a title="61-lda-16" href="../hunch_net-2006/hunch_net-2006-03-12-Online_learning_or_online_preservation_of_learning%3F.html">163 hunch net-2006-03-12-Online learning or online preservation of learning?</a></p>
<p>17 0.39340046 <a title="61-lda-17" href="../hunch_net-2005/hunch_net-2005-11-16-The_Everything_Ensemble_Edge.html">131 hunch net-2005-11-16-The Everything Ensemble Edge</a></p>
<p>18 0.39337906 <a title="61-lda-18" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>19 0.39057049 <a title="61-lda-19" href="../hunch_net-2009/hunch_net-2009-06-01-Multitask_Poisoning.html">358 hunch net-2009-06-01-Multitask Poisoning</a></p>
<p>20 0.39038482 <a title="61-lda-20" href="../hunch_net-2005/hunch_net-2005-03-15-The_State_of_Tight_Bounds.html">41 hunch net-2005-03-15-The State of Tight Bounds</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
