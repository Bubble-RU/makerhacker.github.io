<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>61 hunch net-2005-04-25-Embeddings: what are they good for?</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-61" href="#">hunch_net-2005-61</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>61 hunch net-2005-04-25-Embeddings: what are they good for?</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-61-html" href="http://hunch.net/?p=66">html</a></p><p>Introduction: I've been looking at some recent embeddings work, and am struck by how
beautiful the theory and algorithms are. It also makes me wonder, what are
embeddings good for?A few things immediately come to mind:(1) For
visualization of high-dimensional data sets.In this case, one would like good
algorithms for embedding specifically into 2- and 3-dimensional Euclidean
spaces.(2) For nonparametric modeling.The usual nonparametric models
(histograms, nearest neighbor) often require resources which are exponential
in the dimension. So if the data actually lie close to some low-
dimensionalsurface, it might be a good idea to first identify this surface and
embed the data before applying the model.Incidentally, for applications like
these, it's important to have a functional mapping from high to low dimension,
which some techniques do not yield up.(3) As a prelude to classifier
learning.The hope here is presumably that learning will be easier in the low-
dimensional space, because of (i) better ge</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('embeddings', 0.498), ('nonparametric', 0.248), ('surface', 0.166), ('embed', 0.166), ('embedding', 0.166), ('ii', 0.166), ('layout', 0.166), ('struck', 0.166), ('visualization', 0.154), ('lie', 0.154), ('euclidean', 0.145), ('mapping', 0.145), ('beautiful', 0.138), ('usual', 0.138), ('curious', 0.138), ('data', 0.133), ('wonder', 0.133), ('resources', 0.133), ('dimension', 0.128), ('identify', 0.128)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000002 <a title="61-tfidf-1" href="../hunch_net-2005/hunch_net-2005-04-25-Embeddings%3A_what_are_they_good_for%3F.html">61 hunch net-2005-04-25-Embeddings: what are they good for?</a></p>
<p>Introduction: I've been looking at some recent embeddings work, and am struck by how
beautiful the theory and algorithms are. It also makes me wonder, what are
embeddings good for?A few things immediately come to mind:(1) For
visualization of high-dimensional data sets.In this case, one would like good
algorithms for embedding specifically into 2- and 3-dimensional Euclidean
spaces.(2) For nonparametric modeling.The usual nonparametric models
(histograms, nearest neighbor) often require resources which are exponential
in the dimension. So if the data actually lie close to some low-
dimensionalsurface, it might be a good idea to first identify this surface and
embed the data before applying the model.Incidentally, for applications like
these, it's important to have a functional mapping from high to low dimension,
which some techniques do not yield up.(3) As a prelude to classifier
learning.The hope here is presumably that learning will be easier in the low-
dimensional space, because of (i) better ge</p><p>2 0.14876108 <a title="61-tfidf-2" href="../hunch_net-2005/hunch_net-2005-08-11-Why_Manifold-Based_Dimension_Reduction_Techniques%3F.html">102 hunch net-2005-08-11-Why Manifold-Based Dimension Reduction Techniques?</a></p>
<p>Introduction: Manifold based dimension-reduction algorithms share the following general
outline.Given: a metricd()and a set of pointsSConstruct a graph with a point
in every node and every edge connecting to the node of one of thek-nearest
neighbors. Associate with the edge a weight which is the distance between the
points in the connected nodes.Digest the graph. This might include computing
the shortest path between all points or figuring out how to linearly
interpolate the point from it's neighbors.Find a set of points in a low
dimensional space which preserve the digested properties.Examples include LLE,
Isomap (which I worked on), Hessian-LLE, SDE, and many others. The hope with
these algorithms is that they can recover the low dimensional structure of
point sets in high dimensional spaces. Many of them can be shown to work in
interesting ways producing various compelling pictures.Despite doing some
early work in this direction, I suffer from a motivational problem: Why do we
want to recover the</p><p>3 0.097824618 <a title="61-tfidf-3" href="../hunch_net-2005/hunch_net-2005-03-22-Active_learning.html">45 hunch net-2005-03-22-Active learning</a></p>
<p>Introduction: Often, unlabeled data is easy to come by but labels are expensive. For
instance, if you're building a speech recognizer, it's easy enough to get raw
speech samples -- just walk around with a microphone -- but labeling even one
of these samples is a tedious process in which a human must examine the speech
signal and carefully segment it into phonemes. In the field of active
learning, the goal is as usual to construct an accurate classifier, but the
labels of the data points are initially hidden and there is a charge for each
label you want revealed. The hope is that by intelligent adaptive querying,
you can get away with significantly fewer labels than you would need in a
regular supervised learning framework.Here's an example. Suppose the data lie
on the real line, and the classifiers are simple thresholding functions, H =
{hw}:hw(x) = 1 if x > w, and 0 otherwise.VC theory tells us that if the
underlying distribution P can be classified perfectly by some hypothesis in H
(called thereal</p><p>4 0.083199173 <a title="61-tfidf-4" href="../hunch_net-2005/hunch_net-2005-12-14-More_NIPS_Papers_II.html">140 hunch net-2005-12-14-More NIPS Papers II</a></p>
<p>Introduction: I thought this was a very good NIPS with many excellent papers. The following
are a few NIPS papers which I liked and I hope to study more carefully when I
get the chance. The list is not exhaustive and in no particular
order…Preconditioner Approximations for Probabilistic Graphical
Models.Pradeeep Ravikumar and John Lafferty.I thought the use of
preconditioner methods from solving linear systems in the context of
approximate inference was novel and interesting. The results look good and I'd
like to understand the limitations.Rodeo: Sparse nonparametric regression in
high dimensions.John Lafferty and Larry Wasserman.A very interesting approach
to feature selection in nonparametric regression from a frequentist framework.
The use of lengthscale variables in each dimension reminds me a lot of
'Automatic Relevance Determination' in Gaussian process regression -- it would
be interesting to compare Rodeo to ARD in GPs.Interpolating between types and
tokens by estimating power law generators</p><p>5 0.080658257 <a title="61-tfidf-5" href="../hunch_net-2009/hunch_net-2009-06-03-Functionally_defined_Nonlinear_Dynamic_Models.html">359 hunch net-2009-06-03-Functionally defined Nonlinear Dynamic Models</a></p>
<p>Introduction: Suppose we have a set of observations over timex1,x2,…,xtand want to predict
some future eventyt+1. An inevitable problem arises, because learning a
predictorh(x1,…,xt)ofyt+1is generically intractable due to the size of the
input. To make this problem tractable, what's necessary is a method for
summarizing the relevant information in past observations for the purpose of
prediction in the future. In other words, state is required.Existing
approaches for deriving state have some limitations.Hidden Markov
modelslearned with EM suffer from local minima, use tabular learning
approaches which provide dubious generalization ability, and often require
substantial a.priori specification of the observations.Kalman
FiltersandParticle Filtersare very parametric in the sense that substantial
information must be specified up front.Dynamic Bayesian Networks (graphical
modelsthrough time) require substantial a.priori specification and often
require the solution of difficult computational problems to u</p><p>6 0.075921632 <a title="61-tfidf-6" href="../hunch_net-2005/hunch_net-2005-03-28-Open_Problems_for_Colt.html">47 hunch net-2005-03-28-Open Problems for Colt</a></p>
<p>7 0.068781398 <a title="61-tfidf-7" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>8 0.067322314 <a title="61-tfidf-8" href="../hunch_net-2005/hunch_net-2005-06-29-Not_EM_for_clustering_at_COLT.html">87 hunch net-2005-06-29-Not EM for clustering at COLT</a></p>
<p>9 0.066929221 <a title="61-tfidf-9" href="../hunch_net-2005/hunch_net-2005-01-24-The_Humanloop_Spectrum_of_Machine_Learning.html">3 hunch net-2005-01-24-The Humanloop Spectrum of Machine Learning</a></p>
<p>10 0.066359356 <a title="61-tfidf-10" href="../hunch_net-2007/hunch_net-2007-09-18-It%26%238217%3Bs_MDL_Jim%2C_but_not_as_we_know_it%26%238230%3B%28on_Bayes%2C_MDL_and_consistency%29.html">263 hunch net-2007-09-18-It&#8217;s MDL Jim, but not as we know it&#8230;(on Bayes, MDL and consistency)</a></p>
<p>11 0.064509794 <a title="61-tfidf-11" href="../hunch_net-2005/hunch_net-2005-07-10-Thinking_the_Unthought.html">91 hunch net-2005-07-10-Thinking the Unthought</a></p>
<p>12 0.063631296 <a title="61-tfidf-12" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>13 0.063498631 <a title="61-tfidf-13" href="../hunch_net-2007/hunch_net-2007-06-19-How_is_Compressed_Sensing_going_to_change_Machine_Learning_%3F.html">248 hunch net-2007-06-19-How is Compressed Sensing going to change Machine Learning ?</a></p>
<p>14 0.062359199 <a title="61-tfidf-14" href="../hunch_net-2009/hunch_net-2009-01-07-Interesting_Papers_at_SODA_2009.html">334 hunch net-2009-01-07-Interesting Papers at SODA 2009</a></p>
<p>15 0.059916124 <a title="61-tfidf-15" href="../hunch_net-2011/hunch_net-2011-07-11-Interesting_Neural_Network_Papers_at_ICML_2011.html">438 hunch net-2011-07-11-Interesting Neural Network Papers at ICML 2011</a></p>
<p>16 0.059714001 <a title="61-tfidf-16" href="../hunch_net-2005/hunch_net-2005-12-07-Is_the_Google_way_the_way_for_machine_learning%3F.html">136 hunch net-2005-12-07-Is the Google way the way for machine learning?</a></p>
<p>17 0.059459087 <a title="61-tfidf-17" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>18 0.057744011 <a title="61-tfidf-18" href="../hunch_net-2005/hunch_net-2005-05-29-Maximum_Margin_Mismatch%3F.html">77 hunch net-2005-05-29-Maximum Margin Mismatch?</a></p>
<p>19 0.057405226 <a title="61-tfidf-19" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>20 0.056882028 <a title="61-tfidf-20" href="../hunch_net-2006/hunch_net-2006-04-06-Bounds_greater_than_1.html">170 hunch net-2006-04-06-Bounds greater than 1</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.128), (1, -0.034), (2, -0.0), (3, -0.001), (4, -0.062), (5, -0.013), (6, -0.053), (7, -0.031), (8, 0.019), (9, -0.029), (10, -0.019), (11, 0.048), (12, -0.016), (13, 0.059), (14, -0.005), (15, 0.007), (16, 0.048), (17, -0.008), (18, 0.053), (19, -0.046), (20, 0.029), (21, -0.021), (22, 0.027), (23, -0.053), (24, 0.054), (25, -0.002), (26, -0.008), (27, -0.004), (28, 0.08), (29, 0.104), (30, 0.009), (31, 0.002), (32, 0.08), (33, -0.012), (34, -0.03), (35, -0.046), (36, 0.118), (37, -0.025), (38, 0.038), (39, 0.005), (40, -0.001), (41, 0.005), (42, -0.052), (43, -0.011), (44, 0.019), (45, 0.035), (46, -0.056), (47, -0.009), (48, -0.039), (49, 0.026)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95411867 <a title="61-lsi-1" href="../hunch_net-2005/hunch_net-2005-04-25-Embeddings%3A_what_are_they_good_for%3F.html">61 hunch net-2005-04-25-Embeddings: what are they good for?</a></p>
<p>Introduction: I've been looking at some recent embeddings work, and am struck by how
beautiful the theory and algorithms are. It also makes me wonder, what are
embeddings good for?A few things immediately come to mind:(1) For
visualization of high-dimensional data sets.In this case, one would like good
algorithms for embedding specifically into 2- and 3-dimensional Euclidean
spaces.(2) For nonparametric modeling.The usual nonparametric models
(histograms, nearest neighbor) often require resources which are exponential
in the dimension. So if the data actually lie close to some low-
dimensionalsurface, it might be a good idea to first identify this surface and
embed the data before applying the model.Incidentally, for applications like
these, it's important to have a functional mapping from high to low dimension,
which some techniques do not yield up.(3) As a prelude to classifier
learning.The hope here is presumably that learning will be easier in the low-
dimensional space, because of (i) better ge</p><p>2 0.70887476 <a title="61-lsi-2" href="../hunch_net-2005/hunch_net-2005-08-11-Why_Manifold-Based_Dimension_Reduction_Techniques%3F.html">102 hunch net-2005-08-11-Why Manifold-Based Dimension Reduction Techniques?</a></p>
<p>Introduction: Manifold based dimension-reduction algorithms share the following general
outline.Given: a metricd()and a set of pointsSConstruct a graph with a point
in every node and every edge connecting to the node of one of thek-nearest
neighbors. Associate with the edge a weight which is the distance between the
points in the connected nodes.Digest the graph. This might include computing
the shortest path between all points or figuring out how to linearly
interpolate the point from it's neighbors.Find a set of points in a low
dimensional space which preserve the digested properties.Examples include LLE,
Isomap (which I worked on), Hessian-LLE, SDE, and many others. The hope with
these algorithms is that they can recover the low dimensional structure of
point sets in high dimensional spaces. Many of them can be shown to work in
interesting ways producing various compelling pictures.Despite doing some
early work in this direction, I suffer from a motivational problem: Why do we
want to recover the</p><p>3 0.64023447 <a title="61-lsi-3" href="../hunch_net-2010/hunch_net-2010-08-24-Alex_Smola_starts_a_blog.html">408 hunch net-2010-08-24-Alex Smola starts a blog</a></p>
<p>Introduction: Adventures in Data Land.</p><p>4 0.62221223 <a title="61-lsi-4" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>Introduction: One of the common trends in machine learning has been an emphasis on the use
of unlabeled data. The argument goes something like "there aren't many labeled
web pages out there, but there are ahugenumber of web pages, so we must find a
way to take advantage of them." There are several standard approaches for
doing this:Unsupervised Learning. You use only unlabeled data. In a typical
application, you cluster the data and hope that the clusters somehow
correspond to what you care about.Semisupervised Learning. You use both
unlabeled and labeled data to build a predictor. The unlabeled data influences
the learned predictor in some way.Active Learning. You have unlabeled data and
access to a labeling oracle. You interactively choose which examples to label
so as to optimize prediction accuracy.It seems there is a fourth approach
worth serious investigation--automated labeling. The approach goes as
follows:Identify some subset of observed values to predict from the
others.Build a predictor.U</p><p>5 0.61894912 <a title="61-lsi-5" href="../hunch_net-2005/hunch_net-2005-06-29-Not_EM_for_clustering_at_COLT.html">87 hunch net-2005-06-29-Not EM for clustering at COLT</a></p>
<p>Introduction: One standard approach for clustering data with a set of gaussians is using EM.
Roughly speaking, you pick a set ofkrandom guassians and then use alternating
expectation maximization to (hopefully) find a set of guassians that "explain"
the data well. This process is difficult to work with because EM can become
"stuck" in local optima. There are various hacks like "rerun withtdifferent
random starting points".One cool observation is that this can often be solved
via other algorithm which donotsuffer from local optima. This is an
earlypaperwhich shows this. Ravi Kannan presented anew papershowing this is
possible in a much more adaptive setting.A very rough summary of these papers
is that by projecting into a lower dimensional space, it is computationally
tractable to pick out the gross structure of the data. It is unclear how well
these algorithms work in practice, but they might be effective, especially if
used as a subroutine of the form:Project to low dimensional space.Pick out
gross</p><p>6 0.61415792 <a title="61-lsi-6" href="../hunch_net-2007/hunch_net-2007-09-18-It%26%238217%3Bs_MDL_Jim%2C_but_not_as_we_know_it%26%238230%3B%28on_Bayes%2C_MDL_and_consistency%29.html">263 hunch net-2007-09-18-It&#8217;s MDL Jim, but not as we know it&#8230;(on Bayes, MDL and consistency)</a></p>
<p>7 0.58686262 <a title="61-lsi-7" href="../hunch_net-2006/hunch_net-2006-02-07-Pittsburgh_Mind_Reading_Competition.html">155 hunch net-2006-02-07-Pittsburgh Mind Reading Competition</a></p>
<p>8 0.5505926 <a title="61-lsi-8" href="../hunch_net-2005/hunch_net-2005-12-14-More_NIPS_Papers_II.html">140 hunch net-2005-12-14-More NIPS Papers II</a></p>
<p>9 0.53227723 <a title="61-lsi-9" href="../hunch_net-2005/hunch_net-2005-12-09-Some_NIPS_papers.html">138 hunch net-2005-12-09-Some NIPS papers</a></p>
<p>10 0.52895719 <a title="61-lsi-10" href="../hunch_net-2007/hunch_net-2007-06-19-How_is_Compressed_Sensing_going_to_change_Machine_Learning_%3F.html">248 hunch net-2007-06-19-How is Compressed Sensing going to change Machine Learning ?</a></p>
<p>11 0.52029067 <a title="61-lsi-11" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>12 0.50521815 <a title="61-lsi-12" href="../hunch_net-2008/hunch_net-2008-11-10-ICML_Reviewing_Criteria.html">325 hunch net-2008-11-10-ICML Reviewing Criteria</a></p>
<p>13 0.50500143 <a title="61-lsi-13" href="../hunch_net-2005/hunch_net-2005-08-08-Apprenticeship_Reinforcement_Learning_for_Control.html">101 hunch net-2005-08-08-Apprenticeship Reinforcement Learning for Control</a></p>
<p>14 0.50279963 <a title="61-lsi-14" href="../hunch_net-2005/hunch_net-2005-02-27-Antilearning%3A_When_proximity_goes_bad.html">32 hunch net-2005-02-27-Antilearning: When proximity goes bad</a></p>
<p>15 0.50095046 <a title="61-lsi-15" href="../hunch_net-2007/hunch_net-2007-08-25-The_Privacy_Problem.html">260 hunch net-2007-08-25-The Privacy Problem</a></p>
<p>16 0.49750993 <a title="61-lsi-16" href="../hunch_net-2005/hunch_net-2005-09-26-Prediction_Bounds_as_the_Mathematics_of_Science.html">115 hunch net-2005-09-26-Prediction Bounds as the Mathematics of Science</a></p>
<p>17 0.49630195 <a title="61-lsi-17" href="../hunch_net-2006/hunch_net-2006-07-05-more_icml_papers.html">189 hunch net-2006-07-05-more icml papers</a></p>
<p>18 0.49589202 <a title="61-lsi-18" href="../hunch_net-2005/hunch_net-2005-12-07-Is_the_Google_way_the_way_for_machine_learning%3F.html">136 hunch net-2005-12-07-Is the Google way the way for machine learning?</a></p>
<p>19 0.494688 <a title="61-lsi-19" href="../hunch_net-2005/hunch_net-2005-03-22-Active_learning.html">45 hunch net-2005-03-22-Active learning</a></p>
<p>20 0.48841605 <a title="61-lsi-20" href="../hunch_net-2007/hunch_net-2007-12-20-Cool_and_Interesting_things_at_NIPS%2C_take_three.html">280 hunch net-2007-12-20-Cool and Interesting things at NIPS, take three</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(21, 0.482), (42, 0.205), (68, 0.08), (69, 0.035), (74, 0.078)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.84828103 <a title="61-lda-1" href="../hunch_net-2005/hunch_net-2005-04-25-Embeddings%3A_what_are_they_good_for%3F.html">61 hunch net-2005-04-25-Embeddings: what are they good for?</a></p>
<p>Introduction: I've been looking at some recent embeddings work, and am struck by how
beautiful the theory and algorithms are. It also makes me wonder, what are
embeddings good for?A few things immediately come to mind:(1) For
visualization of high-dimensional data sets.In this case, one would like good
algorithms for embedding specifically into 2- and 3-dimensional Euclidean
spaces.(2) For nonparametric modeling.The usual nonparametric models
(histograms, nearest neighbor) often require resources which are exponential
in the dimension. So if the data actually lie close to some low-
dimensionalsurface, it might be a good idea to first identify this surface and
embed the data before applying the model.Incidentally, for applications like
these, it's important to have a functional mapping from high to low dimension,
which some techniques do not yield up.(3) As a prelude to classifier
learning.The hope here is presumably that learning will be easier in the low-
dimensional space, because of (i) better ge</p><p>2 0.63724059 <a title="61-lda-2" href="../hunch_net-2012/hunch_net-2012-09-29-Vowpal_Wabbit%2C_version_7.0.html">473 hunch net-2012-09-29-Vowpal Wabbit, version 7.0</a></p>
<p>Introduction: A new version ofVWisout. The primary changes are:Learning Reductions: I've
wanted to getlearning reductionsworking and we've finally done it. Not
everything is implemented yet, but VW now supports direct:Multiclass
Classification-oaaor-ect.Cost Sensitive Multiclass Classification-csoaaor-
wap.Contextual Bandit Classification-cb.Sequential Structured Prediction-
searnor-daggerIn addition, it is now easy to build your own custom learning
reductions for various plausible uses: feature diddling, custom structured
prediction problems, or alternate learning reductions. This effort is far from
done, but it is now in a generally useful state. Note that all learning
reductions inherit the ability to do cluster parallel learning.Library
interface: VW now has a basic library interface. The library provides most of
the functionality of VW, with the limitation that it is monolithic and
nonreentrant. These will be improved over time.Windows port: The priority of a
windows port jumped way up once we</p><p>3 0.43166819 <a title="61-lda-3" href="../hunch_net-2005/hunch_net-2005-09-10-%26%238220%3BFailure%26%238221%3B_is_an_option.html">110 hunch net-2005-09-10-&#8220;Failure&#8221; is an option</a></p>
<p>Introduction: This is about the hard choices that graduate students must make.The cultural
definition of success in academic research is to:Produce good research which
many other people appreciate.Produce many students who go on to do the
same.There are fundamental reasons why this is success in the local culture.
Good research appreciated by others means access to jobs. Many students
succesful in the same way implies that there are a number of people who think
in a similar way and appreciate your work.In order to graduate, a phd student
must live in an academic culture for a period of several years. It is common
to adopt the culture's definition of success during this time. It's also
common for many phd students discover they are not suited to an academic
research lifestyle. This collision of values and abilities naturally results
in depression.The most fundamental advice when this happens is: change
something. Pick a new advisor. Pick a new research topic. Or leave the program
(and do something el</p><p>4 0.42746377 <a title="61-lda-4" href="../hunch_net-2005/hunch_net-2005-06-08-Question%3A_%26%238220%3BWhen_is_the_right_time_to_insert_the_loss_function%3F%26%238221%3B.html">79 hunch net-2005-06-08-Question: &#8220;When is the right time to insert the loss function?&#8221;</a></p>
<p>Introduction: Halasksa very good question: "When is the right time to insert the loss
function?" In particular, should it be used at testing time or at training
time?When the world imposes a loss on us, the standard Bayesian recipe is to
predict the (conditional) probability of each possibility and then choose the
possibility which minimizes the expected loss. In contrast, as
theconfusionover "loss = money lost" or "loss = the thing you optimize" might
indicate, many people ignore the Bayesian approach and simply optimize their
loss (or a close proxy for their loss) over the representation on the training
set.The best answer I can give is "it's unclear, but I prefer optimizing the
loss at training time". My experience is that optimizing the loss in the most
direct manner possible typically yields best performance. This question is
related to a basic principle which bothYann LeCun(applied) andVladimir
Vapnik(theoretical) advocate: "solve the simplest prediction problem that
solves the problem". (One</p><p>5 0.42664939 <a title="61-lda-5" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>Introduction: "Overfitting" is traditionally defined as training some flexible
representation so that it memorizes the data but fails to predict well in the
future. For this post, I will define overfitting more generally as over-
representing the performance of systems. There are two styles of general
overfitting: overrepresenting performance on particular datasets and
(implicitly) overrepresenting performance of a method on future datasets.We
should all be aware of these methods, avoid them where possible, and take them
into account otherwise. I have used "reproblem" and "old datasets", and may
have participated in "overfitting by review"--some of these are very difficult
to avoid.NameMethodExplanationRemedyTraditional overfittingTrain a complex
predictor on too-few examples.Hold out pristine examples for testing.Use a
simpler predictor.Get more training examples.Integrate over many
predictors.Reject papers which do this.Parameter tweak overfittingUse a
learning algorithm with many parameters. Choo</p><p>6 0.425017 <a title="61-lda-6" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>7 0.42424202 <a title="61-lda-7" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>8 0.42393777 <a title="61-lda-8" href="../hunch_net-2005/hunch_net-2005-03-22-Active_learning.html">45 hunch net-2005-03-22-Active learning</a></p>
<p>9 0.42382759 <a title="61-lda-9" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>10 0.42055827 <a title="61-lda-10" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<p>11 0.42051393 <a title="61-lda-11" href="../hunch_net-2005/hunch_net-2005-02-01-Watchword%3A_Loss.html">9 hunch net-2005-02-01-Watchword: Loss</a></p>
<p>12 0.41900709 <a title="61-lda-12" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>13 0.41886979 <a title="61-lda-13" href="../hunch_net-2007/hunch_net-2007-11-28-Computational_Consequences_of_Classification.html">274 hunch net-2007-11-28-Computational Consequences of Classification</a></p>
<p>14 0.41872951 <a title="61-lda-14" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>15 0.41762042 <a title="61-lda-15" href="../hunch_net-2005/hunch_net-2005-08-22-Do_you_believe_in_induction%3F.html">104 hunch net-2005-08-22-Do you believe in induction?</a></p>
<p>16 0.41707993 <a title="61-lda-16" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>17 0.4169482 <a title="61-lda-17" href="../hunch_net-2008/hunch_net-2008-04-12-It_Doesn%26%238217%3Bt_Stop.html">295 hunch net-2008-04-12-It Doesn&#8217;t Stop</a></p>
<p>18 0.41551024 <a title="61-lda-18" href="../hunch_net-2005/hunch_net-2005-01-24-The_Humanloop_Spectrum_of_Machine_Learning.html">3 hunch net-2005-01-24-The Humanloop Spectrum of Machine Learning</a></p>
<p>19 0.41546294 <a title="61-lda-19" href="../hunch_net-2005/hunch_net-2005-08-11-Why_Manifold-Based_Dimension_Reduction_Techniques%3F.html">102 hunch net-2005-08-11-Why Manifold-Based Dimension Reduction Techniques?</a></p>
<p>20 0.41443273 <a title="61-lda-20" href="../hunch_net-2011/hunch_net-2011-11-26-Giving_Thanks.html">449 hunch net-2011-11-26-Giving Thanks</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
