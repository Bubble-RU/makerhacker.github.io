<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>3 hunch net-2005-01-24-The Humanloop Spectrum of Machine Learning</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-3" href="#">hunch_net-2005-3</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>3 hunch net-2005-01-24-The Humanloop Spectrum of Machine Learning</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-3-html" href="http://hunch.net/?p=5">html</a></p><p>Introduction: All branches of machine learning seem to be united in the idea of using data
to make predictions. However, people disagree to some extent about what this
means. One way to categorize these different goals is on an axis, where one
extreme is "tools to aid a human in using data to do prediction" and the other
extreme is "tools to do prediction with no human intervention". Here is my
estimate of where various elements of machine learning fall on this
spectrum.Human NecessaryHuman partially necessaryHuman unnecessaryClustering,
data visualizationBayesian Learning, Probabilistic Models, Graphical
ModelsKernel Learning (SVM's, etc..)Decision Trees?Reinforcement LearningThe
exact position of each element is of course debatable. My reasoning is that
clustering and data visualization are nearly useless for prediction without a
human in the loop. Bayesian/probabilistic models/graphical models generally
require a human to sit and think about what is a good prior/structure. Kernel
learning approac</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 All branches of machine learning seem to be united in the idea of using data to make predictions. [sent-1, score-0.458]
</p><p>2 However, people disagree to some extent about what this means. [sent-2, score-0.181]
</p><p>3 One way to categorize these different goals is on an axis, where one extreme is "tools to aid a human in using data to do prediction" and the other extreme is "tools to do prediction with no human intervention". [sent-3, score-1.292]
</p><p>4 Here is my estimate of where various elements of machine learning fall on this spectrum. [sent-4, score-0.259]
</p><p>5 Human NecessaryHuman partially necessaryHuman unnecessaryClustering, data visualizationBayesian Learning, Probabilistic Models, Graphical ModelsKernel Learning (SVM's, etc. [sent-5, score-0.146]
</p><p>6 Reinforcement LearningThe exact position of each element is of course debatable. [sent-8, score-0.394]
</p><p>7 My reasoning is that clustering and data visualization are nearly useless for prediction without a human in the loop. [sent-9, score-1.056]
</p><p>8 Bayesian/probabilistic models/graphical models generally require a human to sit and think about what is a good prior/structure. [sent-10, score-0.496]
</p><p>9 Kernel learning approaches have a few standard kernels which often work on simple problems, although sometimes significant kernel engineering is required. [sent-11, score-0.349]
</p><p>10 I've been impressed of late how 'black box' decision trees or boosted decision trees are. [sent-12, score-1.12]
</p><p>11 The goal of reinforcement learning (rather than perhaps the reality) is designing completely automated agents. [sent-13, score-0.402]
</p><p>12 The position in this spectrum provides some idea of what the state of progress is. [sent-14, score-0.417]
</p><p>13 Things at the 'human necessary' end have been succesfully used by many people to solve many learning problems. [sent-15, score-0.295]
</p><p>14 At the 'human unnecessary' end, the systems are finicky and often just won't work well. [sent-16, score-0.119]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('necessaryhuman', 0.307), ('human', 0.265), ('trees', 0.246), ('unnecessary', 0.227), ('position', 0.21), ('end', 0.181), ('tools', 0.164), ('extreme', 0.156), ('kernel', 0.152), ('data', 0.146), ('reinforcement', 0.142), ('decision', 0.142), ('prediction', 0.133), ('axis', 0.126), ('boosted', 0.126), ('impressed', 0.126), ('learningthe', 0.126), ('visualization', 0.126), ('finicky', 0.119), ('models', 0.117), ('branches', 0.114), ('sit', 0.114), ('spectrum', 0.114), ('succesfully', 0.114), ('box', 0.109), ('kernels', 0.109), ('reasoning', 0.109), ('united', 0.105), ('clustering', 0.102), ('disagree', 0.099), ('reality', 0.099), ('fall', 0.096), ('svm', 0.096), ('useless', 0.096), ('idea', 0.093), ('element', 0.092), ('exact', 0.092), ('late', 0.092), ('wo', 0.092), ('completely', 0.09), ('aid', 0.088), ('designing', 0.088), ('engineering', 0.088), ('graphical', 0.085), ('estimate', 0.085), ('goals', 0.083), ('extent', 0.082), ('automated', 0.082), ('nearly', 0.079), ('elements', 0.078)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="3-tfidf-1" href="../hunch_net-2005/hunch_net-2005-01-24-The_Humanloop_Spectrum_of_Machine_Learning.html">3 hunch net-2005-01-24-The Humanloop Spectrum of Machine Learning</a></p>
<p>Introduction: All branches of machine learning seem to be united in the idea of using data
to make predictions. However, people disagree to some extent about what this
means. One way to categorize these different goals is on an axis, where one
extreme is "tools to aid a human in using data to do prediction" and the other
extreme is "tools to do prediction with no human intervention". Here is my
estimate of where various elements of machine learning fall on this
spectrum.Human NecessaryHuman partially necessaryHuman unnecessaryClustering,
data visualizationBayesian Learning, Probabilistic Models, Graphical
ModelsKernel Learning (SVM's, etc..)Decision Trees?Reinforcement LearningThe
exact position of each element is of course debatable. My reasoning is that
clustering and data visualization are nearly useless for prediction without a
human in the loop. Bayesian/probabilistic models/graphical models generally
require a human to sit and think about what is a good prior/structure. Kernel
learning approac</p><p>2 0.14890289 <a title="3-tfidf-2" href="../hunch_net-2005/hunch_net-2005-11-16-The_Everything_Ensemble_Edge.html">131 hunch net-2005-11-16-The Everything Ensemble Edge</a></p>
<p>Introduction: Rich Caruana,Alexandru Niculescu, Geoff Crew, and Alex Ksikes have donea lot
of empirical testingwhich shows thatusing all methods to make a predictionis
more powerful than using any single method. This is in rough agreement with
the Bayesian way of solving problems, but based upon a different (essentially
empirical) motivation. A rough summary is:Take all of {decision trees, boosted
decision trees, bagged decision trees, boosted decision stumps, K nearest
neighbors, neural networks, SVM} with all reasonable parameter settings.Run
the methods on each problem of 8 problems with a large test set, calibrating
margins using eithersigmoid fittingorisotonic regression.For each loss of
{accuracy, area under the ROC curve, cross entropy, squared error, etcâ&euro;Ś}
evaluate the average performance of the method.A series of conclusions can be
drawn from the observations.(Calibrated) boosted decision trees appear to
perform best, in general although support vector machines and neural networks
give cred</p><p>3 0.1357594 <a title="3-tfidf-3" href="../hunch_net-2010/hunch_net-2010-08-23-Boosted_Decision_Trees_for_Deep_Learning.html">407 hunch net-2010-08-23-Boosted Decision Trees for Deep Learning</a></p>
<p>Introduction: About 4 years ago, I speculated thatdecision trees qualify as a deep learning
algorithmbecause they can make decisions which are substantially nonlinear in
the input representation.Ping Lihasproved this correct, empiricallyatUAIby
showing that boosted decision trees can beat deep belief networks on versions
ofMnistwhich are artificially hardened so as to make them solvable only by
deep learning algorithms.This is an important point, because the ability to
solve these sorts of problems is probably the best objective definition of a
deep learning algorithm we have. I'm not that surprised. In my experience, if
you can accept the computational drawbacks of a boosted decision tree, they
can achieve pretty good performance.Geoff Hintononce told me that the great
thing about deep belief networks is that they work. I understand that Ping had
very substantial difficulty in getting this published, so I hope some
reviewers step up to the standard of valuing what works.</p><p>4 0.12422654 <a title="3-tfidf-4" href="../hunch_net-2005/hunch_net-2005-01-24-Holy_grails_of_machine_learning%3F.html">2 hunch net-2005-01-24-Holy grails of machine learning?</a></p>
<p>Introduction: Let me kick things off by posing this question to ML researchers:What do you
think are some important holy grails of machine learning?For example:- "A
classifier with SVM-level performance but much more scalable"- "Practical
confidence bounds (or learning bounds) for classification"- "A reinforcement
learning algorithm that can handle the ___ problem"- "Understanding
theoretically why ___ works so well in practice"etc.I pose this question
because I believe that when goals are stated explicitly and well (thus
providing clarity as well as opening up the problems to more people), rather
than left implicit, they are likely to be achieved much more quickly. I would
also like to know more about the internal goals of the various machine
learning sub-areas (theory, kernel methods, graphical models, reinforcement
learning, etc) as stated by people in these respective areas. This could help
people cross sub-areas.</p><p>5 0.12020013 <a title="3-tfidf-5" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>Introduction: Many learning algorithms used in practice are fairly simple. Viewed
representationally, many prediction algorithms either compute a linear
separator of basic features (perceptron, winnow, weighted majority, SVM) or
perhaps a linear separator of slightly more complex features (2-layer neural
networks or kernelized SVMs). Should we go beyond this, and start using "deep"
representations?What is deep learning?Intuitively, deep learning is about
learning to predict in ways which can involve complex dependencies between the
input (observed) features.Specifying this more rigorously turns out to be
rather difficult. Consider the following cases:SVM with Gaussian Kernel. This
is not considered deep learning, because an SVM with a gaussian kernel can't
succinctly represent certain decision surfaces. One ofYann LeCun's examples is
recognizing objects based on pixel values. An SVM will need a new support
vector for each significantly different background. Since the number of
distinct backgrounds i</p><p>6 0.11437831 <a title="3-tfidf-6" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>7 0.11113199 <a title="3-tfidf-7" href="../hunch_net-2005/hunch_net-2005-03-10-Breaking_Abstractions.html">39 hunch net-2005-03-10-Breaking Abstractions</a></p>
<p>8 0.11064669 <a title="3-tfidf-8" href="../hunch_net-2006/hunch_net-2006-07-11-New_Models.html">194 hunch net-2006-07-11-New Models</a></p>
<p>9 0.10314174 <a title="3-tfidf-9" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>10 0.10026094 <a title="3-tfidf-10" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>11 0.099746451 <a title="3-tfidf-11" href="../hunch_net-2011/hunch_net-2011-02-17-What_does_Watson_mean%3F.html">424 hunch net-2011-02-17-What does Watson mean?</a></p>
<p>12 0.098765649 <a title="3-tfidf-12" href="../hunch_net-2005/hunch_net-2005-12-07-Is_the_Google_way_the_way_for_machine_learning%3F.html">136 hunch net-2005-12-07-Is the Google way the way for machine learning?</a></p>
<p>13 0.098267242 <a title="3-tfidf-13" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>14 0.096039072 <a title="3-tfidf-14" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>15 0.095992699 <a title="3-tfidf-15" href="../hunch_net-2008/hunch_net-2008-01-28-Sufficient_Computation.html">287 hunch net-2008-01-28-Sufficient Computation</a></p>
<p>16 0.094498746 <a title="3-tfidf-16" href="../hunch_net-2005/hunch_net-2005-02-23-Problem%3A_Reinforcement_Learning_with_Classification.html">27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</a></p>
<p>17 0.092345394 <a title="3-tfidf-17" href="../hunch_net-2006/hunch_net-2006-01-13-Benchmarks_for_RL.html">148 hunch net-2006-01-13-Benchmarks for RL</a></p>
<p>18 0.091569133 <a title="3-tfidf-18" href="../hunch_net-2006/hunch_net-2006-09-19-Luis_von_Ahn_is_awarded_a_MacArthur_fellowship..html">209 hunch net-2006-09-19-Luis von Ahn is awarded a MacArthur fellowship.</a></p>
<p>19 0.089946322 <a title="3-tfidf-19" href="../hunch_net-2005/hunch_net-2005-08-08-Apprenticeship_Reinforcement_Learning_for_Control.html">101 hunch net-2005-08-08-Apprenticeship Reinforcement Learning for Control</a></p>
<p>20 0.087874554 <a title="3-tfidf-20" href="../hunch_net-2012/hunch_net-2012-02-24-ICML%2B50%25.html">456 hunch net-2012-02-24-ICML+50%</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.197), (1, -0.061), (2, 0.043), (3, -0.051), (4, -0.063), (5, 0.033), (6, -0.197), (7, 0.019), (8, 0.062), (9, 0.05), (10, 0.019), (11, 0.016), (12, 0.002), (13, 0.033), (14, 0.081), (15, 0.059), (16, -0.019), (17, -0.019), (18, 0.02), (19, -0.004), (20, 0.016), (21, 0.019), (22, -0.006), (23, 0.008), (24, -0.003), (25, -0.074), (26, 0.038), (27, 0.057), (28, 0.004), (29, 0.042), (30, -0.092), (31, -0.032), (32, 0.062), (33, 0.058), (34, -0.036), (35, 0.007), (36, -0.008), (37, -0.104), (38, -0.042), (39, -0.07), (40, -0.052), (41, -0.013), (42, -0.024), (43, -0.007), (44, 0.063), (45, 0.035), (46, -0.043), (47, -0.062), (48, -0.067), (49, 0.078)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.93535364 <a title="3-lsi-1" href="../hunch_net-2005/hunch_net-2005-01-24-The_Humanloop_Spectrum_of_Machine_Learning.html">3 hunch net-2005-01-24-The Humanloop Spectrum of Machine Learning</a></p>
<p>Introduction: All branches of machine learning seem to be united in the idea of using data
to make predictions. However, people disagree to some extent about what this
means. One way to categorize these different goals is on an axis, where one
extreme is "tools to aid a human in using data to do prediction" and the other
extreme is "tools to do prediction with no human intervention". Here is my
estimate of where various elements of machine learning fall on this
spectrum.Human NecessaryHuman partially necessaryHuman unnecessaryClustering,
data visualizationBayesian Learning, Probabilistic Models, Graphical
ModelsKernel Learning (SVM's, etc..)Decision Trees?Reinforcement LearningThe
exact position of each element is of course debatable. My reasoning is that
clustering and data visualization are nearly useless for prediction without a
human in the loop. Bayesian/probabilistic models/graphical models generally
require a human to sit and think about what is a good prior/structure. Kernel
learning approac</p><p>2 0.65401059 <a title="3-lsi-2" href="../hunch_net-2005/hunch_net-2005-08-08-Apprenticeship_Reinforcement_Learning_for_Control.html">101 hunch net-2005-08-08-Apprenticeship Reinforcement Learning for Control</a></p>
<p>Introduction: Pieter Abbeelpresented a paper withAndrew NgatICMLonExploration and
Apprenticeship Learning in Reinforcement Learning. The basic idea of this
algorithm is:Collect data from a human controlling a machine.Build a
transition model based upon the experience.Build a policy which optimizes the
transition model.Evaluate the policy. If it works well, halt, otherwise add
the experience into the pool and go to (2).The paper proves that this
technique will converge to some policy with expected performance near human
expected performance assuming the world fits certain assumptions (MDP or
linear dynamics).This general idea of apprenticeship learning (i.e.
incorporating data from an expert) seems very compelling because (a) humans
often learn this way and (b) much harder problems can be solved. For (a), the
notion of teaching is about transferring knowledge from an expert to novices,
often via demonstration. To see (b), note that we can create intricate
reinforcement learning problems where a parti</p><p>3 0.62596595 <a title="3-lsi-3" href="../hunch_net-2005/hunch_net-2005-02-27-Antilearning%3A_When_proximity_goes_bad.html">32 hunch net-2005-02-27-Antilearning: When proximity goes bad</a></p>
<p>Introduction: Joel Preddmentioned"Antilearning" byAdam Kowalczyk, which is interesting from
a foundational intuitions viewpoint.There is a pervasive intuition that
"nearby things tend to have the same label". This intuition is instantiated in
SVMs, nearest neighbor classifiers, decision trees, and neural networks. It
turns out there are natural problems where this intuition is opposite of the
truth.One natural situation where this occurs is in competition. For example,
whenIntelfails to meet its earnings estimate, is this evidence thatAMDis doing
badly also? Or evidence that AMD is doing well?This violation of the proximity
intuition means that when the number of examples is few,negatinga classifier
which attempts to exploit proximity can provide predictive power (thus, the
term "antilearning").</p><p>4 0.59160161 <a title="3-lsi-4" href="../hunch_net-2006/hunch_net-2006-02-02-Introspectionism_as_a_Disease.html">153 hunch net-2006-02-02-Introspectionism as a Disease</a></p>
<p>Introduction: In the AI-related parts of machine learning, it is often tempting to examine
howyoudo things in order to imagine how a machine should do things. This is
introspection, and it can easily go awry. I will call introspection gone awry
introspectionism.Introspectionism is almost unique to AI (and the AI-related
parts of machine learning) and it can lead to huge wasted effort in research.
It's easiest to show how introspectionism arises by an example.Suppose we want
to solve the problem of navigating a robot from point A to point B given a
camera. Then, the following research action plan might seem natural when you
examine your own capabilities:Build an edge detector for still images.Build an
object recognition system given the edge detector.Build a system to predict
distance and orientation to objects given the object recognition system.Build
a system to plan a path through the scene you construct from {object
identification, distance, orientation} predictions.As you execute the above,
cons</p><p>5 0.56581247 <a title="3-lsi-5" href="../hunch_net-2005/hunch_net-2005-02-23-Problem%3A_Reinforcement_Learning_with_Classification.html">27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</a></p>
<p>Introduction: At an intuitive level, the question here is "Can reinforcement learning be
solved with classification?"ProblemConstruct a reinforcement learning
algorithm with near-optimal expected sum of rewards in thedirect experience
modelgiven access to a classifier learning algorithm which has a small error
rate or regret on all posed classification problems. The definition of "posed"
here is slightly murky. I consider a problem "posed" if there is an algorithm
for constructing labeled classification examples.Past WorkThere exists
areduction of reinforcement learning to classification given a generative
model.A generative model is an inherently stronger assumption than the direct
experience model.Otherwork on learning reductionsmay be important.Several
algorithms for solving reinforcement learning in the direct experience model
exist. Most, such asE3,Factored-E3, andmetric-E3andRmaxrequire that the
observation be the state. Recent workextends this approach to POMDPs.This
problem is related topred</p><p>6 0.56128281 <a title="3-lsi-6" href="../hunch_net-2011/hunch_net-2011-02-17-What_does_Watson_mean%3F.html">424 hunch net-2011-02-17-What does Watson mean?</a></p>
<p>7 0.55565488 <a title="3-lsi-7" href="../hunch_net-2005/hunch_net-2005-01-24-Holy_grails_of_machine_learning%3F.html">2 hunch net-2005-01-24-Holy grails of machine learning?</a></p>
<p>8 0.55440551 <a title="3-lsi-8" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<p>9 0.54528743 <a title="3-lsi-9" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>10 0.53394306 <a title="3-lsi-10" href="../hunch_net-2005/hunch_net-2005-04-25-Embeddings%3A_what_are_they_good_for%3F.html">61 hunch net-2005-04-25-Embeddings: what are they good for?</a></p>
<p>11 0.52869046 <a title="3-lsi-11" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>12 0.52768785 <a title="3-lsi-12" href="../hunch_net-2011/hunch_net-2011-08-06-Interesting_thing_at_UAI_2011.html">440 hunch net-2011-08-06-Interesting thing at UAI 2011</a></p>
<p>13 0.52744746 <a title="3-lsi-13" href="../hunch_net-2006/hunch_net-2006-01-13-Benchmarks_for_RL.html">148 hunch net-2006-01-13-Benchmarks for RL</a></p>
<p>14 0.52658165 <a title="3-lsi-14" href="../hunch_net-2005/hunch_net-2005-02-03-Learning_Theory%2C_by_assumption.html">12 hunch net-2005-02-03-Learning Theory, by assumption</a></p>
<p>15 0.52389604 <a title="3-lsi-15" href="../hunch_net-2005/hunch_net-2005-11-16-The_Everything_Ensemble_Edge.html">131 hunch net-2005-11-16-The Everything Ensemble Edge</a></p>
<p>16 0.52146339 <a title="3-lsi-16" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>17 0.51911879 <a title="3-lsi-17" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>18 0.50644785 <a title="3-lsi-18" href="../hunch_net-2005/hunch_net-2005-02-09-Intuitions_from_applied_learning.html">16 hunch net-2005-02-09-Intuitions from applied learning</a></p>
<p>19 0.50493562 <a title="3-lsi-19" href="../hunch_net-2010/hunch_net-2010-08-23-Boosted_Decision_Trees_for_Deep_Learning.html">407 hunch net-2010-08-23-Boosted Decision Trees for Deep Learning</a></p>
<p>20 0.49994814 <a title="3-lsi-20" href="../hunch_net-2008/hunch_net-2008-01-28-Sufficient_Computation.html">287 hunch net-2008-01-28-Sufficient Computation</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(6, 0.046), (35, 0.037), (42, 0.272), (44, 0.211), (68, 0.078), (69, 0.022), (74, 0.158), (76, 0.054), (88, 0.024)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.94026774 <a title="3-lda-1" href="../hunch_net-2011/hunch_net-2011-03-27-Vowpal_Wabbit%2C_v5.1.html">428 hunch net-2011-03-27-Vowpal Wabbit, v5.1</a></p>
<p>Introduction: I just createdversion 5.1ofvowpal wabbit. This almost entirely a bugfix
release, so it's an easy upgrade from v5.0.In addition:There is now amailing
list, which I and several other developers are subscribed to.The main website
has shifted to the wiki on github. This means that anyone with a github
account can now edit it.I'm planning to give a tutorial tomorrow on it
ateHarmony/the LA machine learning meetupat 10am. Drop by if you're
interested.The status of VW amongst other open source projects has changed.
When VW first came out, it was relatively unique amongst existing projects in
terms of features. At this point, many other projects have started to
appreciate the value of the design choices here. This includes:Mahout, which
now has an SGD implementation.Shogun, whereSoerenis keen onincorporating
features.LibLinear, where they won the KDD best paper award forout-of-core
learning.This is expected--any open source approach which works well should be
widely adopted. None of these othe</p><p>2 0.93133545 <a title="3-lda-2" href="../hunch_net-2008/hunch_net-2008-10-19-NIPS_2008_workshop_on_Kernel_Learning.html">321 hunch net-2008-10-19-NIPS 2008 workshop on Kernel Learning</a></p>
<p>Introduction: We'd like to invite hunch.net readers to participate in the NIPS 2008 workshop
on kernel learning. While the main focus is on automatically learning kernels
from data, we are also also looking at the broader questions of feature
selection, multi-task learning and multi-view learning. There are no
restrictions on the learning problem being addressed (regression,
classification, etc), and both theoretical and applied work will be
considered. The deadline for submissions isOctober 24.More detail can be
foundhere.Corinna Cortes, Arthur Gretton, Gert Lanckriet, Mehryar Mohri,
Afshin Rostamizadeh</p><p>same-blog 3 0.92960918 <a title="3-lda-3" href="../hunch_net-2005/hunch_net-2005-01-24-The_Humanloop_Spectrum_of_Machine_Learning.html">3 hunch net-2005-01-24-The Humanloop Spectrum of Machine Learning</a></p>
<p>Introduction: All branches of machine learning seem to be united in the idea of using data
to make predictions. However, people disagree to some extent about what this
means. One way to categorize these different goals is on an axis, where one
extreme is "tools to aid a human in using data to do prediction" and the other
extreme is "tools to do prediction with no human intervention". Here is my
estimate of where various elements of machine learning fall on this
spectrum.Human NecessaryHuman partially necessaryHuman unnecessaryClustering,
data visualizationBayesian Learning, Probabilistic Models, Graphical
ModelsKernel Learning (SVM's, etc..)Decision Trees?Reinforcement LearningThe
exact position of each element is of course debatable. My reasoning is that
clustering and data visualization are nearly useless for prediction without a
human in the loop. Bayesian/probabilistic models/graphical models generally
require a human to sit and think about what is a good prior/structure. Kernel
learning approac</p><p>4 0.89669263 <a title="3-lda-4" href="../hunch_net-2008/hunch_net-2008-01-23-Why_Workshop%3F.html">285 hunch net-2008-01-23-Why Workshop?</a></p>
<p>Introduction: I second thecall for workshops at ICML/COLT/UAI.Severaltimesbefore, details of
why and how to run a workshop have been mentioned.There is a simple reason to
prefer workshops here: attendance. The Helsinki colocation has placed
workshopsdirectly between ICML and COLT/UAI, which is optimal for getting
attendees from any conference. In addition,last year ICML had relatively few
workshopsand NIPS workshops were overloaded. In addition tothose that
happeneda similar number were rejected. The overload has strange consequences
--for example,the best attended workshopwasn't an official NIPS workshop.
Aside from intrinsic interest, the Deep Learning workshop benefited greatly
from being off schedule.</p><p>5 0.88756704 <a title="3-lda-5" href="../hunch_net-2008/hunch_net-2008-04-30-Concerns_about_the_Large_Scale_Learning_Challenge.html">300 hunch net-2008-04-30-Concerns about the Large Scale Learning Challenge</a></p>
<p>Introduction: Thelarge scale learning challengefor ICML interests me a great deal, although
I have concerns about the way it is structured.From theinstructions page,
several issues come up:Large DefinitionMy personal definition of dataset size
is:smallA dataset is small if a human could look at the dataset and plausibly
find a good solution.mediumA dataset is mediumsize if it fits in the RAM of a
reasonably priced computer.largeA large dataset does not fit in the RAM of a
reasonably priced computer.By this definition, all of the datasets are medium
sized. This might sound like a pissing match over dataset size, but I believe
it is more than that.The fundamental reason for these definitions is that they
correspond to transitions in the sorts of approaches which are feasible. From
small to medium, the ability to use a human as the learning algorithm
degrades. From medium to large, it becomes essential to have learning
algorithms that don't require random access to examples.No Loading TimeThe
medium sc</p><p>6 0.88425738 <a title="3-lda-6" href="../hunch_net-2008/hunch_net-2008-09-12-How_do_we_get_weak_action_dependence_for_learning_with_partial_observations%3F.html">317 hunch net-2008-09-12-How do we get weak action dependence for learning with partial observations?</a></p>
<p>7 0.81413764 <a title="3-lda-7" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>8 0.81403267 <a title="3-lda-8" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>9 0.81300688 <a title="3-lda-9" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>10 0.81132281 <a title="3-lda-10" href="../hunch_net-2007/hunch_net-2007-12-21-Vowpal_Wabbit_Code_Release.html">281 hunch net-2007-12-21-Vowpal Wabbit Code Release</a></p>
<p>11 0.81020409 <a title="3-lda-11" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>12 0.81015295 <a title="3-lda-12" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>13 0.80745757 <a title="3-lda-13" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>14 0.80573171 <a title="3-lda-14" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>15 0.80271113 <a title="3-lda-15" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>16 0.80233306 <a title="3-lda-16" href="../hunch_net-2005/hunch_net-2005-08-11-Why_Manifold-Based_Dimension_Reduction_Techniques%3F.html">102 hunch net-2005-08-11-Why Manifold-Based Dimension Reduction Techniques?</a></p>
<p>17 0.79843336 <a title="3-lda-17" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>18 0.79617095 <a title="3-lda-18" href="../hunch_net-2005/hunch_net-2005-02-01-Watchword%3A_Loss.html">9 hunch net-2005-02-01-Watchword: Loss</a></p>
<p>19 0.79517376 <a title="3-lda-19" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>20 0.79515803 <a title="3-lda-20" href="../hunch_net-2005/hunch_net-2005-02-15-ESPgame_and_image_labeling.html">20 hunch net-2005-02-15-ESPgame and image labeling</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
