<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>5 hunch net-2005-01-26-Watchword: Probability</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-5" href="#">hunch_net-2005-5</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>5 hunch net-2005-01-26-Watchword: Probability</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-5-html" href="http://hunch.net/?p=8">html</a></p><p>Introduction: Probability is one of the most confusingly used words in machine learning.
There are at least 3 distinct ways the word is used.BayesianThe Bayesian
notion of probability is a 'degree of belief'. The degree of belief that some
event (i.e. "stock goes up" or "stock goes down") occurs can be measured by
asking a sequence of questions of the form "Would you bet the stock goes up or
down atYto 1 odds?" A consistent better will switch from 'for' to 'against' at
some single value ofY. The probability is thenY/(Y+1). Bayesian probabilities
express lack of knowledge rather than randomization. They are useful in
learning because we often lack knowledge and expressing that lack flexibly
makes the learning algorithms work better. Bayesian Learning uses
'probability' in this way exclusively.FrequentistThe Frequentist notion of
probability is a rate of occurence. A rate of occurrence can be measured by
doing an experiment many times. If an event occursktimes innexperiments then
it has probability ab</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Probability is one of the most confusingly used words in machine learning. [sent-1, score-0.154]
</p><p>2 There are at least 3 distinct ways the word is used. [sent-2, score-0.207]
</p><p>3 BayesianThe Bayesian notion of probability is a 'degree of belief'. [sent-3, score-0.653]
</p><p>4 "stock goes up" or "stock goes down") occurs can be measured by asking a sequence of questions of the form "Would you bet the stock goes up or down atYto 1 odds? [sent-6, score-1.373]
</p><p>5 " A consistent better will switch from 'for' to 'against' at some single value ofY. [sent-7, score-0.241]
</p><p>6 Bayesian probabilities express lack of knowledge rather than randomization. [sent-9, score-0.669]
</p><p>7 They are useful in learning because we often lack knowledge and expressing that lack flexibly makes the learning algorithms work better. [sent-10, score-0.53]
</p><p>8 FrequentistThe Frequentist notion of probability is a rate of occurence. [sent-12, score-0.737]
</p><p>9 A rate of occurrence can be measured by doing an experiment many times. [sent-13, score-0.364]
</p><p>10 If an event occursktimes innexperiments then it has probability aboutk/n. [sent-14, score-0.555]
</p><p>11 Frequentist probabilities can be used to measure how sure you are about something. [sent-15, score-0.344]
</p><p>12 They may be appropriate in a learning context for measuring confidence in various predictors. [sent-16, score-0.294]
</p><p>13 The frequentist notion of probability is common in physics, other sciences, and computer science theory. [sent-17, score-1.035]
</p><p>14 EstimatedThe estimated notion of probability is measured by running some learning algorithm which predicts the probability of events rather than events. [sent-18, score-1.602]
</p><p>15 I tend to dislike this use of the word because it confuses the world with the model of the world. [sent-19, score-0.292]
</p><p>16 To avoid confusion, you should be careful to understand what other people mean for this word. [sent-20, score-0.128]
</p><p>17 It is helpful to always be explicit about which variables are randomized and which are constant whenever probability is used because Bayesian and Frequentist probabilities commonly switch this role. [sent-21, score-1.391]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('probability', 0.428), ('frequentist', 0.382), ('probabilities', 0.253), ('stock', 0.238), ('notion', 0.225), ('measured', 0.217), ('goes', 0.204), ('bayesian', 0.192), ('switch', 0.169), ('lack', 0.165), ('word', 0.131), ('event', 0.127), ('measuring', 0.109), ('sciences', 0.109), ('knowledge', 0.109), ('bet', 0.096), ('dislike', 0.096), ('whenever', 0.096), ('estimated', 0.091), ('expressing', 0.091), ('used', 0.091), ('confusion', 0.088), ('randomized', 0.084), ('rate', 0.084), ('odds', 0.082), ('predicts', 0.082), ('express', 0.079), ('role', 0.077), ('variables', 0.077), ('asking', 0.076), ('distinct', 0.076), ('physics', 0.076), ('consistent', 0.072), ('occurs', 0.072), ('careful', 0.069), ('events', 0.068), ('constant', 0.067), ('confidence', 0.066), ('degree', 0.066), ('explicit', 0.065), ('tend', 0.065), ('experiment', 0.063), ('words', 0.063), ('rather', 0.063), ('sequence', 0.062), ('appropriate', 0.061), ('commonly', 0.061), ('mean', 0.059), ('context', 0.058), ('belief', 0.058)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="5-tfidf-1" href="../hunch_net-2005/hunch_net-2005-01-26-Watchword%3A_Probability.html">5 hunch net-2005-01-26-Watchword: Probability</a></p>
<p>Introduction: Probability is one of the most confusingly used words in machine learning.
There are at least 3 distinct ways the word is used.BayesianThe Bayesian
notion of probability is a 'degree of belief'. The degree of belief that some
event (i.e. "stock goes up" or "stock goes down") occurs can be measured by
asking a sequence of questions of the form "Would you bet the stock goes up or
down atYto 1 odds?" A consistent better will switch from 'for' to 'against' at
some single value ofY. The probability is thenY/(Y+1). Bayesian probabilities
express lack of knowledge rather than randomization. They are useful in
learning because we often lack knowledge and expressing that lack flexibly
makes the learning algorithms work better. Bayesian Learning uses
'probability' in this way exclusively.FrequentistThe Frequentist notion of
probability is a rate of occurence. A rate of occurrence can be measured by
doing an experiment many times. If an event occursktimes innexperiments then
it has probability ab</p><p>2 0.20572926 <a title="5-tfidf-2" href="../hunch_net-2005/hunch_net-2005-04-26-To_calibrate_or_not%3F.html">62 hunch net-2005-04-26-To calibrate or not?</a></p>
<p>Introduction: A calibrated predictor is one which predicts the probability of a binary event
with the property: For all predictionsp, the proportion of the time that1is
observed isp.Since there are infinitely manyp, this definition must be
"softened" to make sense for any finite number of samples. The standard method
for "softening" is to consider all predictions in a small neighborhood about
each possiblep.A great deal of effort has been devoted to strategies for
achieving calibrated (such ashere) prediction. With statements like: (under
minimal conditions) you can always make calibrated predictions.Given the
strength of these statements, we might conclude we are done, but that would be
a "confusion of ends". A confusion of ends arises in the following way:We want
good probabilistic predictions.Good probabilistic predictions are
calibrated.Therefore, we want calibrated predictions.The "Therefore" step
misses the fact that calibration is a necessary but not
asufficientcharacterization of good probab</p><p>3 0.19585945 <a title="5-tfidf-3" href="../hunch_net-2005/hunch_net-2005-03-10-Breaking_Abstractions.html">39 hunch net-2005-03-10-Breaking Abstractions</a></p>
<p>Introduction: Sam Roweis's comment reminds me of a more general issue that comes up in doing
research: abstractions always break.Real number's aren't. Most real numbers
can not be represented with any machine. One implication of this is that many
real-number based algorithms have difficulties when implemented with floating
point numbers.The box on your desk is not a turing machine. A turing machine
can compute anything computable, given sufficient time. A typical computer
fails terribly when the state required for the computation exceeds some
limit.Nash equilibria aren't equilibria. This comes up when trying to predict
human behavior based on the result of the equilibria computation. Often, it
doesn't work.Theprobabilityisn't. Probability is an abstraction expressing
either our lack of knowledge (the Bayesian viewpoint) or fundamental
randomization (the frequentist viewpoint). From the frequentist viewpoint the
lack of knowledge typically precludes actually knowing the fundamental
randomization. Fro</p><p>4 0.17223407 <a title="5-tfidf-4" href="../hunch_net-2006/hunch_net-2006-11-20-Context_and_the_calculation_misperception.html">218 hunch net-2006-11-20-Context and the calculation misperception</a></p>
<p>Introduction: This post is really for peoplenotin machine learning (or related fields). It
is about a common misperception which affects people who have not thought
about the process of trying to predict somethinng. Hopefully, by precisely
stating it, we can remove it.Suppose we have a set of events, each described
by a vector of features.01011101011101000111110011000101110Suppose we want to
predict the value of the first feature given the others. One approach is to
bin the data byonefeature. For the above example, we might partition the data
according to feature 2, then observe that when feature 2 is 0 the label
(feature 1) is mostly 1. On the other hand, when feature 2 is 1, the label
(feature 1) is mostly 0. Using this simple rule we get an observed error rate
of 3/7.There are two issues here. The first is that this is really a training
error rate, and (hence) may be an overoptimistic prediction. This is not a
very serious issue as long as there are a reasonable number of representative
examples.</p><p>5 0.1553098 <a title="5-tfidf-5" href="../hunch_net-2005/hunch_net-2005-12-14-More_NIPS_Papers_II.html">140 hunch net-2005-12-14-More NIPS Papers II</a></p>
<p>Introduction: I thought this was a very good NIPS with many excellent papers. The following
are a few NIPS papers which I liked and I hope to study more carefully when I
get the chance. The list is not exhaustive and in no particular
order…Preconditioner Approximations for Probabilistic Graphical
Models.Pradeeep Ravikumar and John Lafferty.I thought the use of
preconditioner methods from solving linear systems in the context of
approximate inference was novel and interesting. The results look good and I'd
like to understand the limitations.Rodeo: Sparse nonparametric regression in
high dimensions.John Lafferty and Larry Wasserman.A very interesting approach
to feature selection in nonparametric regression from a frequentist framework.
The use of lengthscale variables in each dimension reminds me a lot of
'Automatic Relevance Determination' in Gaussian process regression -- it would
be interesting to compare Rodeo to ARD in GPs.Interpolating between types and
tokens by estimating power law generators</p><p>6 0.14803158 <a title="5-tfidf-6" href="../hunch_net-2008/hunch_net-2008-12-07-A_NIPS_paper.html">330 hunch net-2008-12-07-A NIPS paper</a></p>
<p>7 0.147287 <a title="5-tfidf-7" href="../hunch_net-2005/hunch_net-2005-03-02-Prior%2C_%26%238220%3BPrior%26%238221%3B_and_Bias.html">34 hunch net-2005-03-02-Prior, &#8220;Prior&#8221; and Bias</a></p>
<p>8 0.14248063 <a title="5-tfidf-8" href="../hunch_net-2008/hunch_net-2008-02-17-The_Meaning_of_Confidence.html">289 hunch net-2008-02-17-The Meaning of Confidence</a></p>
<p>9 0.13796321 <a title="5-tfidf-9" href="../hunch_net-2005/hunch_net-2005-10-07-On-line_learning_of_regular_decision_rules.html">118 hunch net-2005-10-07-On-line learning of regular decision rules</a></p>
<p>10 0.13330476 <a title="5-tfidf-10" href="../hunch_net-2005/hunch_net-2005-10-16-Complexity%3A_It%26%238217%3Bs_all_in_your_head.html">123 hunch net-2005-10-16-Complexity: It&#8217;s all in your head</a></p>
<p>11 0.12074279 <a title="5-tfidf-11" href="../hunch_net-2008/hunch_net-2008-05-25-Inappropriate_Mathematics_for_Machine_Learning.html">302 hunch net-2008-05-25-Inappropriate Mathematics for Machine Learning</a></p>
<p>12 0.11733112 <a title="5-tfidf-12" href="../hunch_net-2005/hunch_net-2005-04-23-Advantages_and_Disadvantages_of_Bayesian_Learning.html">60 hunch net-2005-04-23-Advantages and Disadvantages of Bayesian Learning</a></p>
<p>13 0.09363649 <a title="5-tfidf-13" href="../hunch_net-2005/hunch_net-2005-09-14-The_Predictionist_Viewpoint.html">112 hunch net-2005-09-14-The Predictionist Viewpoint</a></p>
<p>14 0.089042768 <a title="5-tfidf-14" href="../hunch_net-2006/hunch_net-2006-10-08-Incompatibilities_between_classical_confidence_intervals_and_learning..html">213 hunch net-2006-10-08-Incompatibilities between classical confidence intervals and learning.</a></p>
<p>15 0.087940134 <a title="5-tfidf-15" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>16 0.086280078 <a title="5-tfidf-16" href="../hunch_net-2005/hunch_net-2005-02-28-Regularization.html">33 hunch net-2005-02-28-Regularization</a></p>
<p>17 0.080722094 <a title="5-tfidf-17" href="../hunch_net-2005/hunch_net-2005-06-08-Question%3A_%26%238220%3BWhen_is_the_right_time_to_insert_the_loss_function%3F%26%238221%3B.html">79 hunch net-2005-06-08-Question: &#8220;When is the right time to insert the loss function?&#8221;</a></p>
<p>18 0.080098584 <a title="5-tfidf-18" href="../hunch_net-2011/hunch_net-2011-08-06-Interesting_thing_at_UAI_2011.html">440 hunch net-2011-08-06-Interesting thing at UAI 2011</a></p>
<p>19 0.078234836 <a title="5-tfidf-19" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>20 0.077038124 <a title="5-tfidf-20" href="../hunch_net-2007/hunch_net-2007-01-10-A_Deep_Belief_Net_Learning_Problem.html">227 hunch net-2007-01-10-A Deep Belief Net Learning Problem</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.153), (1, -0.099), (2, -0.018), (3, -0.028), (4, 0.048), (5, -0.059), (6, -0.143), (7, -0.046), (8, 0.02), (9, -0.08), (10, 0.004), (11, 0.001), (12, -0.081), (13, -0.146), (14, -0.085), (15, -0.155), (16, 0.096), (17, 0.107), (18, -0.155), (19, 0.012), (20, -0.052), (21, -0.007), (22, -0.137), (23, 0.108), (24, 0.104), (25, -0.037), (26, -0.009), (27, 0.008), (28, -0.121), (29, -0.026), (30, -0.105), (31, -0.005), (32, -0.097), (33, -0.045), (34, 0.006), (35, -0.006), (36, -0.033), (37, 0.076), (38, -0.039), (39, -0.112), (40, 0.011), (41, -0.097), (42, -0.122), (43, 0.011), (44, 0.057), (45, -0.118), (46, -0.003), (47, -0.029), (48, -0.056), (49, 0.076)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97354805 <a title="5-lsi-1" href="../hunch_net-2005/hunch_net-2005-01-26-Watchword%3A_Probability.html">5 hunch net-2005-01-26-Watchword: Probability</a></p>
<p>Introduction: Probability is one of the most confusingly used words in machine learning.
There are at least 3 distinct ways the word is used.BayesianThe Bayesian
notion of probability is a 'degree of belief'. The degree of belief that some
event (i.e. "stock goes up" or "stock goes down") occurs can be measured by
asking a sequence of questions of the form "Would you bet the stock goes up or
down atYto 1 odds?" A consistent better will switch from 'for' to 'against' at
some single value ofY. The probability is thenY/(Y+1). Bayesian probabilities
express lack of knowledge rather than randomization. They are useful in
learning because we often lack knowledge and expressing that lack flexibly
makes the learning algorithms work better. Bayesian Learning uses
'probability' in this way exclusively.FrequentistThe Frequentist notion of
probability is a rate of occurence. A rate of occurrence can be measured by
doing an experiment many times. If an event occursktimes innexperiments then
it has probability ab</p><p>2 0.75783294 <a title="5-lsi-2" href="../hunch_net-2005/hunch_net-2005-04-26-To_calibrate_or_not%3F.html">62 hunch net-2005-04-26-To calibrate or not?</a></p>
<p>Introduction: A calibrated predictor is one which predicts the probability of a binary event
with the property: For all predictionsp, the proportion of the time that1is
observed isp.Since there are infinitely manyp, this definition must be
"softened" to make sense for any finite number of samples. The standard method
for "softening" is to consider all predictions in a small neighborhood about
each possiblep.A great deal of effort has been devoted to strategies for
achieving calibrated (such ashere) prediction. With statements like: (under
minimal conditions) you can always make calibrated predictions.Given the
strength of these statements, we might conclude we are done, but that would be
a "confusion of ends". A confusion of ends arises in the following way:We want
good probabilistic predictions.Good probabilistic predictions are
calibrated.Therefore, we want calibrated predictions.The "Therefore" step
misses the fact that calibration is a necessary but not
asufficientcharacterization of good probab</p><p>3 0.65456951 <a title="5-lsi-3" href="../hunch_net-2005/hunch_net-2005-10-07-On-line_learning_of_regular_decision_rules.html">118 hunch net-2005-10-07-On-line learning of regular decision rules</a></p>
<p>Introduction: Many decision problems can be represented in the formFORn=1,2,â&euro;Ś:-- Reality
chooses a datumxn.-- Decision Maker chooses his decisiondn.-- Reality chooses
an observationyn.-- Decision Maker suffers lossL(yn,dn).END FOR.The
observationyncan be, for example, tomorrow's stock price and the decisiondnthe
number of shares Decision Maker chooses to buy. The datumxnideally contains
all information that might be relevant in making this decision. We do not want
to assume anything about the way Reality generates the observations and
data.Suppose there is a good and not too complex decision ruleDmapping each
datumxto a decisionD(x). Can we perform as well, or almost as well, asD,
without knowing it? This is essentially a special case of the problem ofon-
line learning.This is a simple result of this kind. Suppose the dataxnare
taken from [0,1] andL(y,d)=|y-d|. A norm ||h|| of a functionhon [0,1] is
defined by||h||2= (Integral01h(t)dt)2+ Integral01(h'(t))2dt.Decision Maker has
a strategy that guaran</p><p>4 0.62976402 <a title="5-lsi-4" href="../hunch_net-2005/hunch_net-2005-03-10-Breaking_Abstractions.html">39 hunch net-2005-03-10-Breaking Abstractions</a></p>
<p>Introduction: Sam Roweis's comment reminds me of a more general issue that comes up in doing
research: abstractions always break.Real number's aren't. Most real numbers
can not be represented with any machine. One implication of this is that many
real-number based algorithms have difficulties when implemented with floating
point numbers.The box on your desk is not a turing machine. A turing machine
can compute anything computable, given sufficient time. A typical computer
fails terribly when the state required for the computation exceeds some
limit.Nash equilibria aren't equilibria. This comes up when trying to predict
human behavior based on the result of the equilibria computation. Often, it
doesn't work.Theprobabilityisn't. Probability is an abstraction expressing
either our lack of knowledge (the Bayesian viewpoint) or fundamental
randomization (the frequentist viewpoint). From the frequentist viewpoint the
lack of knowledge typically precludes actually knowing the fundamental
randomization. Fro</p><p>5 0.62264949 <a title="5-lsi-5" href="../hunch_net-2006/hunch_net-2006-11-20-Context_and_the_calculation_misperception.html">218 hunch net-2006-11-20-Context and the calculation misperception</a></p>
<p>Introduction: This post is really for peoplenotin machine learning (or related fields). It
is about a common misperception which affects people who have not thought
about the process of trying to predict somethinng. Hopefully, by precisely
stating it, we can remove it.Suppose we have a set of events, each described
by a vector of features.01011101011101000111110011000101110Suppose we want to
predict the value of the first feature given the others. One approach is to
bin the data byonefeature. For the above example, we might partition the data
according to feature 2, then observe that when feature 2 is 0 the label
(feature 1) is mostly 1. On the other hand, when feature 2 is 1, the label
(feature 1) is mostly 0. Using this simple rule we get an observed error rate
of 3/7.There are two issues here. The first is that this is really a training
error rate, and (hence) may be an overoptimistic prediction. This is not a
very serious issue as long as there are a reasonable number of representative
examples.</p><p>6 0.60893697 <a title="5-lsi-6" href="../hunch_net-2008/hunch_net-2008-12-07-A_NIPS_paper.html">330 hunch net-2008-12-07-A NIPS paper</a></p>
<p>7 0.58421856 <a title="5-lsi-7" href="../hunch_net-2010/hunch_net-2010-10-08-An_easy_proof_of_the_Chernoff-Hoeffding_bound.html">413 hunch net-2010-10-08-An easy proof of the Chernoff-Hoeffding bound</a></p>
<p>8 0.53140581 <a title="5-lsi-8" href="../hunch_net-2005/hunch_net-2005-03-02-Prior%2C_%26%238220%3BPrior%26%238221%3B_and_Bias.html">34 hunch net-2005-03-02-Prior, &#8220;Prior&#8221; and Bias</a></p>
<p>9 0.52727658 <a title="5-lsi-9" href="../hunch_net-2005/hunch_net-2005-10-16-Complexity%3A_It%26%238217%3Bs_all_in_your_head.html">123 hunch net-2005-10-16-Complexity: It&#8217;s all in your head</a></p>
<p>10 0.52001518 <a title="5-lsi-10" href="../hunch_net-2008/hunch_net-2008-02-17-The_Meaning_of_Confidence.html">289 hunch net-2008-02-17-The Meaning of Confidence</a></p>
<p>11 0.50113291 <a title="5-lsi-11" href="../hunch_net-2008/hunch_net-2008-05-25-Inappropriate_Mathematics_for_Machine_Learning.html">302 hunch net-2008-05-25-Inappropriate Mathematics for Machine Learning</a></p>
<p>12 0.45400822 <a title="5-lsi-12" href="../hunch_net-2009/hunch_net-2009-09-29-Machine_Learning_Protests_at_the_G20.html">372 hunch net-2009-09-29-Machine Learning Protests at the G20</a></p>
<p>13 0.44501719 <a title="5-lsi-13" href="../hunch_net-2005/hunch_net-2005-12-14-More_NIPS_Papers_II.html">140 hunch net-2005-12-14-More NIPS Papers II</a></p>
<p>14 0.43446547 <a title="5-lsi-14" href="../hunch_net-2005/hunch_net-2005-02-28-Regularization.html">33 hunch net-2005-02-28-Regularization</a></p>
<p>15 0.42590916 <a title="5-lsi-15" href="../hunch_net-2005/hunch_net-2005-04-23-Advantages_and_Disadvantages_of_Bayesian_Learning.html">60 hunch net-2005-04-23-Advantages and Disadvantages of Bayesian Learning</a></p>
<p>16 0.42346627 <a title="5-lsi-16" href="../hunch_net-2006/hunch_net-2006-02-18-Multiplication_of_Learned_Probabilities_is_Dangerous.html">157 hunch net-2006-02-18-Multiplication of Learned Probabilities is Dangerous</a></p>
<p>17 0.41205263 <a title="5-lsi-17" href="../hunch_net-2006/hunch_net-2006-03-02-Why_do_people_count_for_learning%3F.html">160 hunch net-2006-03-02-Why do people count for learning?</a></p>
<p>18 0.40352896 <a title="5-lsi-18" href="../hunch_net-2006/hunch_net-2006-10-08-Incompatibilities_between_classical_confidence_intervals_and_learning..html">213 hunch net-2006-10-08-Incompatibilities between classical confidence intervals and learning.</a></p>
<p>19 0.40060651 <a title="5-lsi-19" href="../hunch_net-2011/hunch_net-2011-08-06-Interesting_thing_at_UAI_2011.html">440 hunch net-2011-08-06-Interesting thing at UAI 2011</a></p>
<p>20 0.388841 <a title="5-lsi-20" href="../hunch_net-2007/hunch_net-2007-10-19-Second_Annual_Reinforcement_Learning_Competition.html">268 hunch net-2007-10-19-Second Annual Reinforcement Learning Competition</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(6, 0.011), (25, 0.338), (35, 0.099), (42, 0.267), (45, 0.019), (48, 0.012), (68, 0.011), (74, 0.084), (76, 0.02), (82, 0.026)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.91072649 <a title="5-lda-1" href="../hunch_net-2005/hunch_net-2005-01-26-Watchword%3A_Probability.html">5 hunch net-2005-01-26-Watchword: Probability</a></p>
<p>Introduction: Probability is one of the most confusingly used words in machine learning.
There are at least 3 distinct ways the word is used.BayesianThe Bayesian
notion of probability is a 'degree of belief'. The degree of belief that some
event (i.e. "stock goes up" or "stock goes down") occurs can be measured by
asking a sequence of questions of the form "Would you bet the stock goes up or
down atYto 1 odds?" A consistent better will switch from 'for' to 'against' at
some single value ofY. The probability is thenY/(Y+1). Bayesian probabilities
express lack of knowledge rather than randomization. They are useful in
learning because we often lack knowledge and expressing that lack flexibly
makes the learning algorithms work better. Bayesian Learning uses
'probability' in this way exclusively.FrequentistThe Frequentist notion of
probability is a rate of occurence. A rate of occurrence can be measured by
doing an experiment many times. If an event occursktimes innexperiments then
it has probability ab</p><p>2 0.83357859 <a title="5-lda-2" href="../hunch_net-2005/hunch_net-2005-02-04-JMLG.html">13 hunch net-2005-02-04-JMLG</a></p>
<p>Introduction: TheJournal of Machine Learning Gossiphas some fine satire about learning
research. In particular, theguidesare amusing and remarkably true.As in all
things, it's easy to criticize the way things are and harder to make them
better.</p><p>3 0.83079219 <a title="5-lda-3" href="../hunch_net-2011/hunch_net-2011-01-16-2011_Summer_Conference_Deadline_Season.html">422 hunch net-2011-01-16-2011 Summer Conference Deadline Season</a></p>
<p>Introduction: Machine learning always welcomes the new year with paper deadlines for summer
conferences. This year, we have:ConferencePaper DeadlineWhen/WhereDouble
blind?Author Feedback?NotesICMLFebruary 1June 28-July 2, Bellevue, Washington,
USAYYWeak colocation withACLCOLTFebruary 11July 9-July 11, Budapest,
HungaryNNcolocated withFOCMKDDFebruary 11/18August 21-24, San Diego,
California, USANNUAIMarch 18July 14-17, Barcelona, SpainYNThe larger
conferences are on the west coast in the United States, while the smaller ones
are in Europe.</p><p>4 0.77087831 <a title="5-lda-4" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">23 hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<p>Introduction: This is apaperby Yann LeCun and Fu Jie Huang published atAISTAT 2005. I found
this paper very difficult to read, but it does have some point about a
computational shortcut.This paper takes for granted that the method of solving
a problem is gradient descent on parameters. Given this assumption, the
question arises: Do you want to do gradient descent on a probabilistic model
or something else?All (conditional) probabilistic models have the formp(y|x) =
f(x,y)/Z(x)whereZ(x) = sumyf(x,y)(the paper calls- log f(x,y)an "energy").
Iffis parameterized by somew, the gradient has a term forZ(x), and hence for
every value ofy. The paper claims, that such models can be optimized for
classification purposes using only the correctyand the othery' not ywhich
maximizesf(x,y). This can even be done on unnormalizable models. The paper
further claims that this can be done with an approximate maximum. These claims
are plausible based on experimental results and intuition.It wouldn't surprise
me to learn</p><p>5 0.65428483 <a title="5-lda-5" href="../hunch_net-2005/hunch_net-2005-03-10-Breaking_Abstractions.html">39 hunch net-2005-03-10-Breaking Abstractions</a></p>
<p>Introduction: Sam Roweis's comment reminds me of a more general issue that comes up in doing
research: abstractions always break.Real number's aren't. Most real numbers
can not be represented with any machine. One implication of this is that many
real-number based algorithms have difficulties when implemented with floating
point numbers.The box on your desk is not a turing machine. A turing machine
can compute anything computable, given sufficient time. A typical computer
fails terribly when the state required for the computation exceeds some
limit.Nash equilibria aren't equilibria. This comes up when trying to predict
human behavior based on the result of the equilibria computation. Often, it
doesn't work.Theprobabilityisn't. Probability is an abstraction expressing
either our lack of knowledge (the Bayesian viewpoint) or fundamental
randomization (the frequentist viewpoint). From the frequentist viewpoint the
lack of knowledge typically precludes actually knowing the fundamental
randomization. Fro</p><p>6 0.64012235 <a title="5-lda-6" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>7 0.63797361 <a title="5-lda-7" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>8 0.63784128 <a title="5-lda-8" href="../hunch_net-2007/hunch_net-2007-10-17-Online_as_the_new_adjective.html">267 hunch net-2007-10-17-Online as the new adjective</a></p>
<p>9 0.6365124 <a title="5-lda-9" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>10 0.63575715 <a title="5-lda-10" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>11 0.63523239 <a title="5-lda-11" href="../hunch_net-2006/hunch_net-2006-11-06-Data_Linkage_Problems.html">217 hunch net-2006-11-06-Data Linkage Problems</a></p>
<p>12 0.63475484 <a title="5-lda-12" href="../hunch_net-2011/hunch_net-2011-10-24-2011_ML_symposium_and_the_bears.html">448 hunch net-2011-10-24-2011 ML symposium and the bears</a></p>
<p>13 0.63454998 <a title="5-lda-13" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>14 0.63399124 <a title="5-lda-14" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<p>15 0.63390779 <a title="5-lda-15" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>16 0.63335109 <a title="5-lda-16" href="../hunch_net-2006/hunch_net-2006-03-02-Why_do_people_count_for_learning%3F.html">160 hunch net-2006-03-02-Why do people count for learning?</a></p>
<p>17 0.63308072 <a title="5-lda-17" href="../hunch_net-2007/hunch_net-2007-01-02-Retrospective.html">225 hunch net-2007-01-02-Retrospective</a></p>
<p>18 0.6328733 <a title="5-lda-18" href="../hunch_net-2006/hunch_net-2006-05-05-An_ICML_reject.html">177 hunch net-2006-05-05-An ICML reject</a></p>
<p>19 0.63278973 <a title="5-lda-19" href="../hunch_net-2007/hunch_net-2007-01-15-The_Machine_Learning_Department.html">228 hunch net-2007-01-15-The Machine Learning Department</a></p>
<p>20 0.63250345 <a title="5-lda-20" href="../hunch_net-2007/hunch_net-2007-01-10-A_Deep_Belief_Net_Learning_Problem.html">227 hunch net-2007-01-10-A Deep Belief Net Learning Problem</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
