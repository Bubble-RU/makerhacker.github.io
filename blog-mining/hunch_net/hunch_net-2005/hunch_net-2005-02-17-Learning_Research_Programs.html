<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>21 hunch net-2005-02-17-Learning Research Programs</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-21" href="#">hunch_net-2005-21</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>21 hunch net-2005-02-17-Learning Research Programs</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-21-html" href="http://hunch.net/?p=24">html</a></p><p>Introduction: This is an attempt to organize the broad research programs related to machine learning currently underway.  This isn’t easy—this map is partial, the categories often overlap, and there are many details left out.  Nevertheless, it is (perhaps) helpful to have some map of what is happening where.  The word ‘typical’ should not be construed narrowly here.
  
  Learning Theory  Focuses on analyzing mathematical models of learning, essentially no experiments.  Typical conference: COLT. 
  Bayesian Learning  Bayes law is always used. Focus on methods of speeding up or approximating integration, new probabilistic models, and practical applications.  Typical conferences: NIPS,UAI 
  Structured learning  Predicting complex structured outputs, some applications. Typiical conferences: NIPS, UAI, others 
  Reinforcement Learning  Focused on ‘agent-in-the-world’ learning problems where the goal is optimizing reward.  Typical conferences: ICML 
  Unsupervised Learning/Clustering/Dimensionality Reduc</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 This is an attempt to organize the broad research programs related to machine learning currently underway. [sent-1, score-0.533]
</p><p>2 This isn’t easy—this map is partial, the categories often overlap, and there are many details left out. [sent-2, score-0.402]
</p><p>3 Nevertheless, it is (perhaps) helpful to have some map of what is happening where. [sent-3, score-0.304]
</p><p>4 The word ‘typical’ should not be construed narrowly here. [sent-4, score-0.229]
</p><p>5 Learning Theory  Focuses on analyzing mathematical models of learning, essentially no experiments. [sent-5, score-0.303]
</p><p>6 Focus on methods of speeding up or approximating integration, new probabilistic models, and practical applications. [sent-8, score-0.493]
</p><p>7 Typical conferences: NIPS,UAI    Structured learning  Predicting complex structured outputs, some applications. [sent-9, score-0.25]
</p><p>8 Typiical conferences: NIPS, UAI, others    Reinforcement Learning  Focused on ‘agent-in-the-world’ learning problems where the goal is optimizing reward. [sent-10, score-0.17]
</p><p>9 Typicaly conferences: Many (each with a somewhat different viewpoint)    Applied Learning  Worries about cost sensitive learning, what to do on very large datasets, applications, etc. [sent-12, score-0.093]
</p><p>10 Typical conference: KDD    Supervised Leanring  Chief concern is making practical algorithms for simpler predictions. [sent-14, score-0.352]
</p><p>11 Typical conference: ICML     Please comment on any missing pieces—it would be good to build up a better understanding of what are the focuses and where they are. [sent-16, score-0.496]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('typical', 0.438), ('focuses', 0.25), ('conferences', 0.23), ('map', 0.203), ('focused', 0.178), ('structured', 0.17), ('conference', 0.151), ('worries', 0.15), ('practical', 0.144), ('chief', 0.139), ('leanring', 0.139), ('narrowly', 0.139), ('outputs', 0.139), ('speeding', 0.139), ('approximating', 0.131), ('models', 0.123), ('simpler', 0.116), ('categories', 0.112), ('overlap', 0.109), ('organize', 0.106), ('integration', 0.106), ('analyzing', 0.104), ('unsupervised', 0.104), ('happening', 0.101), ('partial', 0.099), ('law', 0.097), ('broad', 0.095), ('sensitive', 0.093), ('icml', 0.093), ('concern', 0.092), ('word', 0.09), ('optimizing', 0.09), ('pieces', 0.089), ('left', 0.087), ('programs', 0.087), ('currently', 0.084), ('missing', 0.084), ('viewpoint', 0.082), ('bayes', 0.082), ('focus', 0.082), ('comment', 0.081), ('attempt', 0.081), ('uai', 0.081), ('build', 0.081), ('learning', 0.08), ('probabilistic', 0.079), ('datasets', 0.077), ('kdd', 0.076), ('please', 0.076), ('mathematical', 0.076)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="21-tfidf-1" href="../hunch_net-2005/hunch_net-2005-02-17-Learning_Research_Programs.html">21 hunch net-2005-02-17-Learning Research Programs</a></p>
<p>Introduction: This is an attempt to organize the broad research programs related to machine learning currently underway.  This isn’t easy—this map is partial, the categories often overlap, and there are many details left out.  Nevertheless, it is (perhaps) helpful to have some map of what is happening where.  The word ‘typical’ should not be construed narrowly here.
  
  Learning Theory  Focuses on analyzing mathematical models of learning, essentially no experiments.  Typical conference: COLT. 
  Bayesian Learning  Bayes law is always used. Focus on methods of speeding up or approximating integration, new probabilistic models, and practical applications.  Typical conferences: NIPS,UAI 
  Structured learning  Predicting complex structured outputs, some applications. Typiical conferences: NIPS, UAI, others 
  Reinforcement Learning  Focused on ‘agent-in-the-world’ learning problems where the goal is optimizing reward.  Typical conferences: ICML 
  Unsupervised Learning/Clustering/Dimensionality Reduc</p><p>2 0.16939621 <a title="21-tfidf-2" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>Introduction: Here’s a quick reference for summer ML-related conferences sorted by due date:
  
 
 Conference 
 Due date 
 Location 
 Reviewing 
 
 
  KDD  
 Feb 10 
 August 12-16, Beijing, China 
 Single Blind 
 
 
  COLT  
 Feb 14 
 June 25-June 27, Edinburgh, Scotland 
 Single Blind? (historically) 
 
 
  ICML  
 Feb 24 
 June 26-July 1, Edinburgh, Scotland 
 Double Blind, author response, zero  SPOF  
 
 
  UAI  
 March 30 
 August 15-17, Catalina Islands, California 
 Double Blind, author response 
 
  
Geographically, this is greatly dispersed and the UAI/KDD conflict is unfortunate.
 
Machine Learning conferences are triannual now, between  NIPS ,  AIStat , and  ICML .  This has not always been the case: the academic default is annual summer conferences, then NIPS started with a December conference, and now AIStat has grown into an April conference.  
 
However, the first claim is not quite correct.  NIPS and AIStat have few competing venues while ICML implicitly competes with many other conf</p><p>3 0.15376213 <a title="21-tfidf-3" href="../hunch_net-2005/hunch_net-2005-03-21-Research_Styles_in_Machine_Learning.html">44 hunch net-2005-03-21-Research Styles in Machine Learning</a></p>
<p>Introduction: Machine Learning is a field with an impressively diverse set of reseearch styles.  Understanding this may be important in appreciating what you see at a conference.
  
  Engineering . How can I solve this problem?  People in the engineering research style try to solve hard problems directly by any means available and then describe how they did it.  This is typical of problem-specific conferences and communities. 
  Scientific . What are the principles for solving learning problems? People in this research style test techniques on many different problems.  This is fairly common at ICML and NIPS. 
  Mathematical . How can the learning problem be mathematically understood?  People in this research style prove theorems with implications for learning but often do not implement (or test algorithms).  COLT is a typical conference for this style. 
  
Many people manage to cross these styles, and that is often beneficial.  
 
Whenver we list a set of alternative, it becomes natural to think â&euro;&oelig;wh</p><p>4 0.14214717 <a title="21-tfidf-4" href="../hunch_net-2005/hunch_net-2005-07-04-The_Health_of_COLT.html">89 hunch net-2005-07-04-The Health of COLT</a></p>
<p>Introduction: The health of  COLT  (Conference on Learning Theory or Computational Learning Theory depending on who you ask) has been questioned over the last few years.  Low points for the conference occurred when  EuroCOLT  merged with COLT in 2001, and the attendance at the 2002 Sydney COLT fell to a new low.  This occurred in the general context of machine learning conferences rising in both number and size over the last decade.
 
Any discussion of  why  COLT has had difficulties is inherently controversial as is any story about well-intentioned people making the wrong decisions.   Nevertheless, this may be worth discussing in the hope of avoiding problems in the future and general understanding.  In any such discussion there is a strong tendency to identify with a conference/community in a patriotic manner that is detrimental to thinking.  Keep in mind that conferences exist to further research.
 
My understanding (I wasn’t around) is that COLT started as a subcommunity of the computer science</p><p>5 0.13671103 <a title="21-tfidf-5" href="../hunch_net-2010/hunch_net-2010-10-29-To_Vidoelecture_or_not.html">416 hunch net-2010-10-29-To Vidoelecture or not</a></p>
<p>Introduction: (update:  cross-posted  on  CACM )
 
For the first time in several years,  ICML 2010  did not have  videolectures  attending.  Luckily, the  tutorial on exploration and learning  which  Alina  and I put together can  be viewed , since we also presented at  KDD 2010 , which included videolecture support. 
 
ICML didn’t cover the cost of a videolecture, because  PASCAL  didn’t provide a grant for it this year.  On the other hand, KDD covered it out of registration costs.  The cost of videolectures isn’t cheap.  For  a workshop  the baseline quote we have is 270 euro per hour, plus a similar cost for the cameraman’s travel and accomodation.  This can be reduced substantially by having a volunteer with a camera handle the cameraman duties, uploading the video and slides to be processed for a quoted 216 euro per hour.
 
 Youtube  is the most predominant free video site with a cost of $0, but it turns out to be a poor alternative.   15 minute upload limits  do not match typical talk lengths.</p><p>6 0.12042181 <a title="21-tfidf-6" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>7 0.11107171 <a title="21-tfidf-7" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>8 0.10904435 <a title="21-tfidf-8" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>9 0.10742541 <a title="21-tfidf-9" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>10 0.10530731 <a title="21-tfidf-10" href="../hunch_net-2005/hunch_net-2005-03-24-The_Role_of_Workshops.html">46 hunch net-2005-03-24-The Role of Workshops</a></p>
<p>11 0.10440481 <a title="21-tfidf-11" href="../hunch_net-2005/hunch_net-2005-08-18-SVM_Adaptability.html">103 hunch net-2005-08-18-SVM Adaptability</a></p>
<p>12 0.1038985 <a title="21-tfidf-12" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>13 0.10335962 <a title="21-tfidf-13" href="../hunch_net-2012/hunch_net-2012-02-24-ICML%2B50%25.html">456 hunch net-2012-02-24-ICML+50%</a></p>
<p>14 0.096718356 <a title="21-tfidf-14" href="../hunch_net-2005/hunch_net-2005-07-13-%26%238220%3BSister_Conference%26%238221%3B_presentations.html">93 hunch net-2005-07-13-&#8220;Sister Conference&#8221; presentations</a></p>
<p>15 0.096049726 <a title="21-tfidf-15" href="../hunch_net-2006/hunch_net-2006-04-27-Conferences%2C_Workshops%2C_and_Tutorials.html">174 hunch net-2006-04-27-Conferences, Workshops, and Tutorials</a></p>
<p>16 0.094156705 <a title="21-tfidf-16" href="../hunch_net-2005/hunch_net-2005-02-03-Learning_Theory%2C_by_assumption.html">12 hunch net-2005-02-03-Learning Theory, by assumption</a></p>
<p>17 0.093587048 <a title="21-tfidf-17" href="../hunch_net-2006/hunch_net-2006-10-04-Health_of_Conferences_Wiki.html">212 hunch net-2006-10-04-Health of Conferences Wiki</a></p>
<p>18 0.092725039 <a title="21-tfidf-18" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>19 0.090827435 <a title="21-tfidf-19" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>20 0.089955866 <a title="21-tfidf-20" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.209), (1, -0.047), (2, -0.023), (3, -0.059), (4, 0.029), (5, -0.062), (6, 0.035), (7, 0.004), (8, 0.077), (9, 0.027), (10, -0.015), (11, -0.028), (12, -0.055), (13, 0.097), (14, 0.094), (15, 0.014), (16, 0.046), (17, 0.006), (18, -0.044), (19, -0.072), (20, 0.1), (21, -0.17), (22, 0.044), (23, 0.013), (24, -0.056), (25, -0.066), (26, -0.08), (27, 0.032), (28, -0.047), (29, 0.032), (30, 0.018), (31, 0.142), (32, -0.063), (33, -0.077), (34, 0.068), (35, -0.002), (36, -0.007), (37, -0.064), (38, -0.004), (39, 0.056), (40, -0.069), (41, -0.049), (42, 0.033), (43, 0.131), (44, -0.118), (45, -0.011), (46, -0.028), (47, -0.031), (48, -0.047), (49, 0.04)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96687448 <a title="21-lsi-1" href="../hunch_net-2005/hunch_net-2005-02-17-Learning_Research_Programs.html">21 hunch net-2005-02-17-Learning Research Programs</a></p>
<p>Introduction: This is an attempt to organize the broad research programs related to machine learning currently underway.  This isn’t easy—this map is partial, the categories often overlap, and there are many details left out.  Nevertheless, it is (perhaps) helpful to have some map of what is happening where.  The word ‘typical’ should not be construed narrowly here.
  
  Learning Theory  Focuses on analyzing mathematical models of learning, essentially no experiments.  Typical conference: COLT. 
  Bayesian Learning  Bayes law is always used. Focus on methods of speeding up or approximating integration, new probabilistic models, and practical applications.  Typical conferences: NIPS,UAI 
  Structured learning  Predicting complex structured outputs, some applications. Typiical conferences: NIPS, UAI, others 
  Reinforcement Learning  Focused on ‘agent-in-the-world’ learning problems where the goal is optimizing reward.  Typical conferences: ICML 
  Unsupervised Learning/Clustering/Dimensionality Reduc</p><p>2 0.64862967 <a title="21-lsi-2" href="../hunch_net-2009/hunch_net-2009-01-08-Predictive_Analytics_World.html">335 hunch net-2009-01-08-Predictive Analytics World</a></p>
<p>Introduction: Carla Vicens and  Eric Siegel  contacted me about  Predictive Analytics World  in San Francisco February 18&19, which I wasn’t familiar with.  A quick look at the  agenda  reveals several people I know working on applications of machine learning in businesses, covering deployed applications topics.  It’s interesting to see a business-focused machine learning conference, as it says that we are succeeding as a field.  If you are interested in deployed applications, you might attend.
 
Eric and I did a quick interview by email.
 
John > 
I’ve mostly published and participated in academic machine learning conferences like ICML, COLT, and NIPS.   When I look at the  set of speakers and subjects  for your conference  I think “machine learning for business”.  Is that your understanding of things? What I’m trying to ask is: what do you view as the primary goal for this conference?
 
Eric > 
 You got it.  This is the business event focused on the commercial deployment of technology developed at</p><p>3 0.63855112 <a title="21-lsi-3" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>Introduction: Here’s a quick reference for summer ML-related conferences sorted by due date:
  
 
 Conference 
 Due date 
 Location 
 Reviewing 
 
 
  KDD  
 Feb 10 
 August 12-16, Beijing, China 
 Single Blind 
 
 
  COLT  
 Feb 14 
 June 25-June 27, Edinburgh, Scotland 
 Single Blind? (historically) 
 
 
  ICML  
 Feb 24 
 June 26-July 1, Edinburgh, Scotland 
 Double Blind, author response, zero  SPOF  
 
 
  UAI  
 March 30 
 August 15-17, Catalina Islands, California 
 Double Blind, author response 
 
  
Geographically, this is greatly dispersed and the UAI/KDD conflict is unfortunate.
 
Machine Learning conferences are triannual now, between  NIPS ,  AIStat , and  ICML .  This has not always been the case: the academic default is annual summer conferences, then NIPS started with a December conference, and now AIStat has grown into an April conference.  
 
However, the first claim is not quite correct.  NIPS and AIStat have few competing venues while ICML implicitly competes with many other conf</p><p>4 0.6258586 <a title="21-lsi-4" href="../hunch_net-2005/hunch_net-2005-07-13-%26%238220%3BSister_Conference%26%238221%3B_presentations.html">93 hunch net-2005-07-13-&#8220;Sister Conference&#8221; presentations</a></p>
<p>Introduction: Some of the “sister conference” presentations at  AAAI  have been great.  Roughly speaking, the conference organizers asked other conference organizers to come give a summary of their conference.  Many different AI-related conferences accepted.  The presenters typically discuss some of the background and goals of the conference then mention the results from a few papers they liked.  This is great because it provides a mechanism to get a digested overview of the work of several thousand researchers—something which is simply available nowhere else.
 
Based on these presentations, it looks like there is a significant component of (and opportunity for) applied machine learning in  AIIDE ,  IUI , and  ACL .
 
There was also some discussion of having a super-colocation event similar to  FCRC , but centered on AI & Learning.  This seems like a fine idea.  The field is fractured across so many different conferences that the mixing of a supercolocation seems likely helpful for research.</p><p>5 0.6164906 <a title="21-lsi-5" href="../hunch_net-2007/hunch_net-2007-02-11-24.html">232 hunch net-2007-02-11-24</a></p>
<p>Introduction: To commemorate the  Twenty Fourth Annual International Conference on Machine  Learning  (ICML-07), the FOX Network has decided to  launch a new spin-off series in prime time.  Through unofficial  sources, I have obtained the  story arc  for the first season, which appears frighteningly realistic.</p><p>6 0.57709211 <a title="21-lsi-6" href="../hunch_net-2005/hunch_net-2005-03-21-Research_Styles_in_Machine_Learning.html">44 hunch net-2005-03-21-Research Styles in Machine Learning</a></p>
<p>7 0.53763103 <a title="21-lsi-7" href="../hunch_net-2010/hunch_net-2010-10-29-To_Vidoelecture_or_not.html">416 hunch net-2010-10-29-To Vidoelecture or not</a></p>
<p>8 0.53482348 <a title="21-lsi-8" href="../hunch_net-2005/hunch_net-2005-07-04-The_Health_of_COLT.html">89 hunch net-2005-07-04-The Health of COLT</a></p>
<p>9 0.52269953 <a title="21-lsi-9" href="../hunch_net-2006/hunch_net-2006-01-06-MLTV.html">146 hunch net-2006-01-06-MLTV</a></p>
<p>10 0.51353019 <a title="21-lsi-10" href="../hunch_net-2012/hunch_net-2012-02-24-ICML%2B50%25.html">456 hunch net-2012-02-24-ICML+50%</a></p>
<p>11 0.50028044 <a title="21-lsi-11" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>12 0.49081701 <a title="21-lsi-12" href="../hunch_net-2005/hunch_net-2005-08-18-SVM_Adaptability.html">103 hunch net-2005-08-18-SVM Adaptability</a></p>
<p>13 0.49050504 <a title="21-lsi-13" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>14 0.48895285 <a title="21-lsi-14" href="../hunch_net-2005/hunch_net-2005-01-19-Why_I_decided_to_run_a_weblog..html">1 hunch net-2005-01-19-Why I decided to run a weblog.</a></p>
<p>15 0.48688772 <a title="21-lsi-15" href="../hunch_net-2009/hunch_net-2009-12-24-Top_graduates_this_season.html">384 hunch net-2009-12-24-Top graduates this season</a></p>
<p>16 0.48556378 <a title="21-lsi-16" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>17 0.4807792 <a title="21-lsi-17" href="../hunch_net-2007/hunch_net-2007-06-23-Machine_Learning_Jobs_are_Growing_on_Trees.html">250 hunch net-2007-06-23-Machine Learning Jobs are Growing on Trees</a></p>
<p>18 0.47605196 <a title="21-lsi-18" href="../hunch_net-2007/hunch_net-2007-07-13-The_View_From_China.html">255 hunch net-2007-07-13-The View From China</a></p>
<p>19 0.47416705 <a title="21-lsi-19" href="../hunch_net-2006/hunch_net-2006-04-27-Conferences%2C_Workshops%2C_and_Tutorials.html">174 hunch net-2006-04-27-Conferences, Workshops, and Tutorials</a></p>
<p>20 0.47413826 <a title="21-lsi-20" href="../hunch_net-2007/hunch_net-2007-01-15-The_Machine_Learning_Department.html">228 hunch net-2007-01-15-The Machine Learning Department</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(9, 0.013), (27, 0.121), (37, 0.238), (38, 0.109), (53, 0.25), (55, 0.095), (89, 0.052), (95, 0.02)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.91207087 <a title="21-lda-1" href="../hunch_net-2005/hunch_net-2005-02-17-Learning_Research_Programs.html">21 hunch net-2005-02-17-Learning Research Programs</a></p>
<p>Introduction: This is an attempt to organize the broad research programs related to machine learning currently underway.  This isn’t easy—this map is partial, the categories often overlap, and there are many details left out.  Nevertheless, it is (perhaps) helpful to have some map of what is happening where.  The word ‘typical’ should not be construed narrowly here.
  
  Learning Theory  Focuses on analyzing mathematical models of learning, essentially no experiments.  Typical conference: COLT. 
  Bayesian Learning  Bayes law is always used. Focus on methods of speeding up or approximating integration, new probabilistic models, and practical applications.  Typical conferences: NIPS,UAI 
  Structured learning  Predicting complex structured outputs, some applications. Typiical conferences: NIPS, UAI, others 
  Reinforcement Learning  Focused on ‘agent-in-the-world’ learning problems where the goal is optimizing reward.  Typical conferences: ICML 
  Unsupervised Learning/Clustering/Dimensionality Reduc</p><p>2 0.88631701 <a title="21-lda-2" href="../hunch_net-2011/hunch_net-2011-04-18-A_paper_not_at_Snowbird.html">431 hunch net-2011-04-18-A paper not at Snowbird</a></p>
<p>Introduction: Unfortunately, a scheduling failure meant I missed all of  AIStat  and most of the  learning workshop , otherwise known as Snowbird, when it’s at  Snowbird .  
 
At snowbird, the talk on  Sum-Product  networks by  Hoifung Poon  stood out to me ( Pedro Domingos  is a coauthor.).  The basic point was that by appropriately constructing networks based on sums and products, the normalization problem in probabilistic models is eliminated, yielding a highly tractable yet flexible representation+learning algorithm.  As an algorithm, this is noticeably cleaner than deep belief networks with a claim to being an order of magnitude faster and working better on an image completion task.
 
Snowbird doesn’t have real papers—just the abstract above.  I look forward to seeing the paper.  (added: Rodrigo points out the deep learning workshop  draft .)</p><p>3 0.72816563 <a title="21-lda-3" href="../hunch_net-2009/hunch_net-2009-08-16-Centmail_comments.html">367 hunch net-2009-08-16-Centmail comments</a></p>
<p>Introduction: Centmail  is a scheme which makes charity donations have a secondary value, as a stamp for email.  When discussed on  newscientist ,  slashdot , and others, some of the comments make the academic review process appear thoughtful   .  Some prominent fallacies are:
  
 Costing money fallacy.  Some commenters appear to believe the system charges money per email.  Instead, the basic idea is that users get an extra benefit from donations to a charity and participation is strictly voluntary.  The solution to this fallacy is simply reading  the details . 
 Single solution fallacy.  Some commenters seem to think this is proposed as a complete solution to spam, and since not everyone will opt to participate, it won’t work.  But a complete solution is not at all necessary or even possible given the  flag-day problem .  Deployed machine learning systems for fighting spam are great at taking advantage of a partial solution.  The solution to this fallacy is learning about machine learning.  In the</p><p>4 0.71760428 <a title="21-lda-4" href="../hunch_net-2005/hunch_net-2005-01-24-Holy_grails_of_machine_learning%3F.html">2 hunch net-2005-01-24-Holy grails of machine learning?</a></p>
<p>Introduction: Let me kick things off by posing this question to ML researchers:
 
   What do you think are some important holy grails of machine learning?
 
For example: 
 – “A classifier with SVM-level performance but much more scalable” 
 – “Practical confidence bounds (or learning bounds) for classification” 
 – “A reinforcement learning algorithm that can handle the ___ problem” 
 – “Understanding theoretically why ___ works so well in practice” 
etc.
 
I pose this question because I believe that when goals are stated explicitly and well (thus providing clarity as well as opening up the problems to more people), rather than left implicit, they are likely to be achieved much more quickly.  I would also like to know more about the internal goals of the various machine learning sub-areas (theory, kernel methods, graphical models, reinforcement learning, etc) as stated by people in these respective areas.  This could help people cross sub-areas.</p><p>5 0.71672529 <a title="21-lda-5" href="../hunch_net-2005/hunch_net-2005-02-09-Intuitions_from_applied_learning.html">16 hunch net-2005-02-09-Intuitions from applied learning</a></p>
<p>Introduction: Since learning is far from an exact science, it’s good to pay attention to basic intuitions of applied learning.  Here are a few I’ve collected.
  
  Integration   In Bayesian learning, the posterior is computed by an integral, and the optimal thing to do is to predict according to this integral.  This phenomena seems to be far more general.  Bagging, Boosting, SVMs, and Neural Networks all take advantage of this idea to some extent.  The phenomena is more general: you can average over many different  classification predictors  to improve performance.  Sources:  Zoubin ,  Caruana  
  Differentiation  Different pieces of an average should differentiate to achieve good performance by different methods.  This is know as the ‘symmetry breaking’ problem for neural networks, and it’s why weights are initialized randomly.   Boosting explicitly attempts to achieve good differentiation by creating new, different, learning problems.  Sources:  Yann LeCun ,  Phil Long  
  Deep Representation   Ha</p><p>6 0.71498728 <a title="21-lda-6" href="../hunch_net-2005/hunch_net-2005-07-10-Thinking_the_Unthought.html">91 hunch net-2005-07-10-Thinking the Unthought</a></p>
<p>7 0.70976734 <a title="21-lda-7" href="../hunch_net-2005/hunch_net-2005-04-27-DARPA_project%3A_LAGR.html">63 hunch net-2005-04-27-DARPA project: LAGR</a></p>
<p>8 0.70753694 <a title="21-lda-8" href="../hunch_net-2005/hunch_net-2005-04-14-Families_of_Learning_Theory_Statements.html">56 hunch net-2005-04-14-Families of Learning Theory Statements</a></p>
<p>9 0.69890153 <a title="21-lda-9" href="../hunch_net-2005/hunch_net-2005-12-29-Deadline_Season.html">145 hunch net-2005-12-29-Deadline Season</a></p>
<p>10 0.69873238 <a title="21-lda-10" href="../hunch_net-2005/hunch_net-2005-09-05-Site_Update.html">107 hunch net-2005-09-05-Site Update</a></p>
<p>11 0.69344538 <a title="21-lda-11" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>12 0.68061632 <a title="21-lda-12" href="../hunch_net-2009/hunch_net-2009-08-26-Another_10-year_paper_in_Machine_Learning.html">368 hunch net-2009-08-26-Another 10-year paper in Machine Learning</a></p>
<p>13 0.67955196 <a title="21-lda-13" href="../hunch_net-2005/hunch_net-2005-01-19-Why_I_decided_to_run_a_weblog..html">1 hunch net-2005-01-19-Why I decided to run a weblog.</a></p>
<p>14 0.63725662 <a title="21-lda-14" href="../hunch_net-2005/hunch_net-2005-12-09-Some_NIPS_papers.html">138 hunch net-2005-12-09-Some NIPS papers</a></p>
<p>15 0.61597008 <a title="21-lda-15" href="../hunch_net-2006/hunch_net-2006-01-25-1_year.html">151 hunch net-2006-01-25-1 year</a></p>
<p>16 0.61505777 <a title="21-lda-16" href="../hunch_net-2006/hunch_net-2006-07-08-MaxEnt_contradicts_Bayes_Rule%3F.html">191 hunch net-2006-07-08-MaxEnt contradicts Bayes Rule?</a></p>
<p>17 0.61402458 <a title="21-lda-17" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>18 0.6129334 <a title="21-lda-18" href="../hunch_net-2005/hunch_net-2005-04-23-Advantages_and_Disadvantages_of_Bayesian_Learning.html">60 hunch net-2005-04-23-Advantages and Disadvantages of Bayesian Learning</a></p>
<p>19 0.60327584 <a title="21-lda-19" href="../hunch_net-2010/hunch_net-2010-10-29-To_Vidoelecture_or_not.html">416 hunch net-2010-10-29-To Vidoelecture or not</a></p>
<p>20 0.60308677 <a title="21-lda-20" href="../hunch_net-2006/hunch_net-2006-07-11-New_Models.html">194 hunch net-2006-07-11-New Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
