<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>25 hunch net-2005-02-20-At One Month</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-25" href="#">hunch_net-2005-25</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>25 hunch net-2005-02-20-At One Month</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-25-html" href="http://hunch.net/?p=28">html</a></p><p>Introduction: This is near the one month point, so it seems appropriate to consider meta-
issues for the moment.The number of posts is a bit over 20.The number of
people speaking up in discussions is about 10.The number of people viewing the
site is somewhat more than 100.I am (naturally) dissatisfied with many
things.Many of thepotential useshaven't been realized. This is partly a matter
of opportunity (no conferences in the last month), partly a matter of will (no
open problems because it's hard to give them up), and partly a matter of
tradition. In academia, there is a strong tradition of trying to get
everything perfectly right before presentation. This is somewhat contradictory
to the nature of making many posts, and it's definitely contradictory to the
idea of doing "public research". If that sort of idea is to pay off, it must
be significantly more succesful than previous methods. In an effort to
continue experimenting, I'm going to use the next week as "open problems
week".Spam is a problem.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 This is near the one month point, so it seems appropriate to consider meta- issues for the moment. [sent-1, score-0.375]
</p><p>2 This is partly a matter of opportunity (no conferences in the last month), partly a matter of will (no open problems because it's hard to give them up), and partly a matter of tradition. [sent-7, score-0.958]
</p><p>3 This is somewhat contradictory to the nature of making many posts, and it's definitely contradictory to the idea of doing "public research". [sent-9, score-0.488]
</p><p>4 In an effort to continue experimenting, I'm going to use the next week as "open problems week". [sent-11, score-0.218]
</p><p>5 WordPress allows you to block specific posts by match, but there seems to be some minor bug (or maybe a misuse) in how it matches. [sent-13, score-0.621]
</p><p>6 This resulted in everything being blocked pending approval, which is highly unnatural for any conversation. [sent-14, score-0.594]
</p><p>7 I approved all posts by real people, and I think the 'everything blocked pending approval' problem has been solved. [sent-15, score-0.9]
</p><p>8 A site discussing learning ought to have a better system for coping with what is spam and what is not. [sent-16, score-0.38]
</p><p>9 (It's not clear this is research instead of just engineering, but it is clear that it would be very valuable here and in many other places. [sent-18, score-0.253]
</p><p>10 Threading would be helpful in comments because it would help localize discussion to particular contexts. [sent-21, score-0.455]
</p><p>11 Tagging of posts with categories seems inadequate because it's hard to anticipate all the ways something might be thought about. [sent-22, score-0.85]
</p><p>12 Idealy, the sequence of posts would create a well-organized virtual site. [sent-24, score-0.659]
</p><p>13 In many cases there are very good comments and it seems altering the post to summarize the comments is appropriate, but doing so leaves the comments out of context. [sent-25, score-0.872]
</p><p>14 Some mechanism of refinement which avoids this problem would be great. [sent-26, score-0.267]
</p><p>15 Many comments develop into something that should (essentially) be their own post on a new topic. [sent-27, score-0.217]
</p><p>16 Doing so is currently cumbersome, and a mechanism for making that shift would be helpful. [sent-28, score-0.275]
</p><p>17 Making a stream of good posts is hard and takes awhile. [sent-30, score-0.695]
</p><p>18 Naturally, some were (and even still are) stored up, but that store is finite, and eventually will be exhausted. [sent-31, score-0.252]
</p><p>19 Since I'm unwilling to compromise quality, this means the rate of posts may eventually fall. [sent-32, score-0.798]
</p><p>20 Several of the discussions have been quite interesting, and I often find that the process of writing posts helps clarify my understanding. [sent-39, score-0.72]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('posts', 0.54), ('comments', 0.217), ('approval', 0.18), ('blocked', 0.18), ('pending', 0.18), ('contradictory', 0.16), ('partly', 0.142), ('spam', 0.128), ('matter', 0.127), ('month', 0.12), ('would', 0.119), ('week', 0.116), ('discussions', 0.106), ('eventually', 0.104), ('continue', 0.102), ('site', 0.098), ('appropriate', 0.089), ('making', 0.088), ('everything', 0.086), ('consider', 0.085), ('seems', 0.081), ('naturally', 0.081), ('hard', 0.081), ('archives', 0.08), ('compromise', 0.08), ('cumbersome', 0.08), ('dissatisfied', 0.08), ('ought', 0.08), ('refinement', 0.08), ('threading', 0.08), ('somewhat', 0.08), ('anticipate', 0.074), ('clarify', 0.074), ('coping', 0.074), ('inadequate', 0.074), ('resulted', 0.074), ('store', 0.074), ('stored', 0.074), ('stream', 0.074), ('unnatural', 0.074), ('unwilling', 0.074), ('altering', 0.07), ('summarize', 0.07), ('wordpress', 0.07), ('open', 0.07), ('mechanism', 0.068), ('clear', 0.067), ('commitment', 0.067), ('jl', 0.067), ('perfectly', 0.067)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="25-tfidf-1" href="../hunch_net-2005/hunch_net-2005-02-20-At_One_Month.html">25 hunch net-2005-02-20-At One Month</a></p>
<p>Introduction: This is near the one month point, so it seems appropriate to consider meta-
issues for the moment.The number of posts is a bit over 20.The number of
people speaking up in discussions is about 10.The number of people viewing the
site is somewhat more than 100.I am (naturally) dissatisfied with many
things.Many of thepotential useshaven't been realized. This is partly a matter
of opportunity (no conferences in the last month), partly a matter of will (no
open problems because it's hard to give them up), and partly a matter of
tradition. In academia, there is a strong tradition of trying to get
everything perfectly right before presentation. This is somewhat contradictory
to the nature of making many posts, and it's definitely contradictory to the
idea of doing "public research". If that sort of idea is to pay off, it must
be significantly more succesful than previous methods. In an effort to
continue experimenting, I'm going to use the next week as "open problems
week".Spam is a problem.</p><p>2 0.2732017 <a title="25-tfidf-2" href="../hunch_net-2007/hunch_net-2007-01-02-Retrospective.html">225 hunch net-2007-01-02-Retrospective</a></p>
<p>Introduction: It's been almost two years since this blog began. In that time, I've learned
enough to shift my expectations in several ways.Initially, the idea was for a
general purpose ML blog where different people could contribute posts. What
has actually happened is most posts come from me, with a few guest posts that
I greatly value. There are a few reasons I see for this.Overload. A couple
years ago, I had not fully appreciated just how busy life gets for a
researcher. Making a post is not simply a matter of getting to it, but rather
of prioritizing between {writing a grant, finishing an overdue review, writing
a paper, teaching a class, writing a program, etcâ&euro;Ś}. This is a substantial
transition away from what life as a graduate student is like. At some point
the question is not "when will I get to it?" but rather "will I get to it?"
and the answer starts to become "no" most of the time.Feedback failure. This
blog currently receives about 3K unique visitors per day from about 13K unique
sites p</p><p>3 0.22879022 <a title="25-tfidf-3" href="../hunch_net-2005/hunch_net-2005-12-09-Machine_Learning_Thoughts.html">137 hunch net-2005-12-09-Machine Learning Thoughts</a></p>
<p>Introduction: I added a link to Olivier Bousquet'smachine learning thoughtsblog. Several of
the posts may be of interest.</p><p>4 0.20405242 <a title="25-tfidf-4" href="../hunch_net-2006/hunch_net-2006-01-25-1_year.html">151 hunch net-2006-01-25-1 year</a></p>
<p>Introduction: At the one year (+5 days) anniversary, the natural question is: "Was it
helpful for research?"Answer: Yes, and so it shall continue.Some evidence is
provided by noticing that I am about a factor of 2 more overloaded with paper
ideas than I've ever previously been. It is always hard to estimate
counterfactual worlds, but I expect that this is also a factor of 2 more than
"What if I had not started the blog?"As for "Why?", there seem to be two
primary effects.A blog is a mechanism for connecting with people who either
think like you or are interested in the same problems. This allows for
concentration of thinking which is very helpful in solving problems.The
process of stating things you don't understand publicly is very helpful in
understanding them. Sometimes you are simply forced to express them in a way
which aids understanding. Sometimes someone else says something which helps.
And sometimes you discover that someone else has already solved the
problem.There are drawbacks which shou</p><p>5 0.15196541 <a title="25-tfidf-5" href="../hunch_net-2009/hunch_net-2009-12-09-Inherent_Uncertainty.html">383 hunch net-2009-12-09-Inherent Uncertainty</a></p>
<p>Introduction: I'd like to point outInherent Uncertainty, which I've added to the ML blog
post scanner on the right. My understanding fromJakeis that the intention is
to have a multiauthor blog which is more specialized towards learning
theory/game theory than this one. Nevertheless, several of the posts seem to
be of wider interest.</p><p>6 0.13467616 <a title="25-tfidf-6" href="../hunch_net-2005/hunch_net-2005-12-22-Yes_%2C_I_am_applying.html">142 hunch net-2005-12-22-Yes , I am applying</a></p>
<p>7 0.12492974 <a title="25-tfidf-7" href="../hunch_net-2009/hunch_net-2009-05-17-Server_Update.html">354 hunch net-2009-05-17-Server Update</a></p>
<p>8 0.12450218 <a title="25-tfidf-8" href="../hunch_net-2006/hunch_net-2006-12-06-The_Spam_Problem.html">223 hunch net-2006-12-06-The Spam Problem</a></p>
<p>9 0.1232079 <a title="25-tfidf-9" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>10 0.11964326 <a title="25-tfidf-10" href="../hunch_net-2005/hunch_net-2005-09-05-Site_Update.html">107 hunch net-2005-09-05-Site Update</a></p>
<p>11 0.11247516 <a title="25-tfidf-11" href="../hunch_net-2011/hunch_net-2011-10-03-Monday_announcements.html">446 hunch net-2011-10-03-Monday announcements</a></p>
<p>12 0.10450982 <a title="25-tfidf-12" href="../hunch_net-2009/hunch_net-2009-08-16-Centmail_comments.html">367 hunch net-2009-08-16-Centmail comments</a></p>
<p>13 0.10157287 <a title="25-tfidf-13" href="../hunch_net-2005/hunch_net-2005-10-13-Site_tweak.html">122 hunch net-2005-10-13-Site tweak</a></p>
<p>14 0.10055675 <a title="25-tfidf-14" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>15 0.09852232 <a title="25-tfidf-15" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>16 0.097950175 <a title="25-tfidf-16" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>17 0.092248678 <a title="25-tfidf-17" href="../hunch_net-2010/hunch_net-2010-06-20-2010_ICML_discussion_site.html">401 hunch net-2010-06-20-2010 ICML discussion site</a></p>
<p>18 0.09160734 <a title="25-tfidf-18" href="../hunch_net-2007/hunch_net-2007-02-02-Thoughts_regarding_%26%238220%3BIs_machine_learning_different_from_statistics%3F%26%238221%3B.html">230 hunch net-2007-02-02-Thoughts regarding &#8220;Is machine learning different from statistics?&#8221;</a></p>
<p>19 0.085247919 <a title="25-tfidf-19" href="../hunch_net-2005/hunch_net-2005-07-21-Six_Months.html">96 hunch net-2005-07-21-Six Months</a></p>
<p>20 0.084696807 <a title="25-tfidf-20" href="../hunch_net-2007/hunch_net-2007-11-05-CMU_wins_DARPA_Urban_Challenge.html">271 hunch net-2007-11-05-CMU wins DARPA Urban Challenge</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.202), (1, 0.075), (2, 0.086), (3, -0.098), (4, 0.081), (5, -0.027), (6, 0.064), (7, -0.216), (8, 0.074), (9, 0.123), (10, -0.01), (11, -0.072), (12, 0.041), (13, 0.035), (14, -0.036), (15, -0.08), (16, 0.081), (17, -0.067), (18, -0.05), (19, 0.163), (20, 0.151), (21, 0.077), (22, -0.043), (23, -0.041), (24, -0.051), (25, 0.172), (26, 0.104), (27, -0.062), (28, 0.078), (29, -0.103), (30, -0.052), (31, -0.005), (32, 0.081), (33, -0.06), (34, 0.005), (35, -0.023), (36, -0.065), (37, -0.025), (38, -0.065), (39, -0.045), (40, 0.036), (41, 0.002), (42, -0.001), (43, 0.071), (44, 0.039), (45, -0.03), (46, -0.059), (47, 0.009), (48, 0.013), (49, -0.02)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96216846 <a title="25-lsi-1" href="../hunch_net-2005/hunch_net-2005-02-20-At_One_Month.html">25 hunch net-2005-02-20-At One Month</a></p>
<p>Introduction: This is near the one month point, so it seems appropriate to consider meta-
issues for the moment.The number of posts is a bit over 20.The number of
people speaking up in discussions is about 10.The number of people viewing the
site is somewhat more than 100.I am (naturally) dissatisfied with many
things.Many of thepotential useshaven't been realized. This is partly a matter
of opportunity (no conferences in the last month), partly a matter of will (no
open problems because it's hard to give them up), and partly a matter of
tradition. In academia, there is a strong tradition of trying to get
everything perfectly right before presentation. This is somewhat contradictory
to the nature of making many posts, and it's definitely contradictory to the
idea of doing "public research". If that sort of idea is to pay off, it must
be significantly more succesful than previous methods. In an effort to
continue experimenting, I'm going to use the next week as "open problems
week".Spam is a problem.</p><p>2 0.73379391 <a title="25-lsi-2" href="../hunch_net-2007/hunch_net-2007-01-02-Retrospective.html">225 hunch net-2007-01-02-Retrospective</a></p>
<p>Introduction: It's been almost two years since this blog began. In that time, I've learned
enough to shift my expectations in several ways.Initially, the idea was for a
general purpose ML blog where different people could contribute posts. What
has actually happened is most posts come from me, with a few guest posts that
I greatly value. There are a few reasons I see for this.Overload. A couple
years ago, I had not fully appreciated just how busy life gets for a
researcher. Making a post is not simply a matter of getting to it, but rather
of prioritizing between {writing a grant, finishing an overdue review, writing
a paper, teaching a class, writing a program, etcâ&euro;Ś}. This is a substantial
transition away from what life as a graduate student is like. At some point
the question is not "when will I get to it?" but rather "will I get to it?"
and the answer starts to become "no" most of the time.Feedback failure. This
blog currently receives about 3K unique visitors per day from about 13K unique
sites p</p><p>3 0.71346611 <a title="25-lsi-3" href="../hunch_net-2005/hunch_net-2005-12-09-Machine_Learning_Thoughts.html">137 hunch net-2005-12-09-Machine Learning Thoughts</a></p>
<p>Introduction: I added a link to Olivier Bousquet'smachine learning thoughtsblog. Several of
the posts may be of interest.</p><p>4 0.67495084 <a title="25-lsi-4" href="../hunch_net-2006/hunch_net-2006-01-25-1_year.html">151 hunch net-2006-01-25-1 year</a></p>
<p>Introduction: At the one year (+5 days) anniversary, the natural question is: "Was it
helpful for research?"Answer: Yes, and so it shall continue.Some evidence is
provided by noticing that I am about a factor of 2 more overloaded with paper
ideas than I've ever previously been. It is always hard to estimate
counterfactual worlds, but I expect that this is also a factor of 2 more than
"What if I had not started the blog?"As for "Why?", there seem to be two
primary effects.A blog is a mechanism for connecting with people who either
think like you or are interested in the same problems. This allows for
concentration of thinking which is very helpful in solving problems.The
process of stating things you don't understand publicly is very helpful in
understanding them. Sometimes you are simply forced to express them in a way
which aids understanding. Sometimes someone else says something which helps.
And sometimes you discover that someone else has already solved the
problem.There are drawbacks which shou</p><p>5 0.6170764 <a title="25-lsi-5" href="../hunch_net-2009/hunch_net-2009-12-09-Inherent_Uncertainty.html">383 hunch net-2009-12-09-Inherent Uncertainty</a></p>
<p>Introduction: I'd like to point outInherent Uncertainty, which I've added to the ML blog
post scanner on the right. My understanding fromJakeis that the intention is
to have a multiauthor blog which is more specialized towards learning
theory/game theory than this one. Nevertheless, several of the posts seem to
be of wider interest.</p><p>6 0.61633146 <a title="25-lsi-6" href="../hunch_net-2009/hunch_net-2009-05-17-Server_Update.html">354 hunch net-2009-05-17-Server Update</a></p>
<p>7 0.60044265 <a title="25-lsi-7" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>8 0.57909733 <a title="25-lsi-8" href="../hunch_net-2009/hunch_net-2009-08-16-Centmail_comments.html">367 hunch net-2009-08-16-Centmail comments</a></p>
<p>9 0.56641859 <a title="25-lsi-9" href="../hunch_net-2005/hunch_net-2005-10-13-Site_tweak.html">122 hunch net-2005-10-13-Site tweak</a></p>
<p>10 0.56605053 <a title="25-lsi-10" href="../hunch_net-2005/hunch_net-2005-09-05-Site_Update.html">107 hunch net-2005-09-05-Site Update</a></p>
<p>11 0.55438 <a title="25-lsi-11" href="../hunch_net-2006/hunch_net-2006-12-06-The_Spam_Problem.html">223 hunch net-2006-12-06-The Spam Problem</a></p>
<p>12 0.53268945 <a title="25-lsi-12" href="../hunch_net-2006/hunch_net-2006-06-05-Server_Shift%2C_Site_Tweaks%2C_Suggestions%3F.html">182 hunch net-2006-06-05-Server Shift, Site Tweaks, Suggestions?</a></p>
<p>13 0.51990974 <a title="25-lsi-13" href="../hunch_net-2010/hunch_net-2010-06-20-2010_ICML_discussion_site.html">401 hunch net-2010-06-20-2010 ICML discussion site</a></p>
<p>14 0.51220948 <a title="25-lsi-14" href="../hunch_net-2011/hunch_net-2011-10-03-Monday_announcements.html">446 hunch net-2011-10-03-Monday announcements</a></p>
<p>15 0.49703041 <a title="25-lsi-15" href="../hunch_net-2005/hunch_net-2005-12-22-Yes_%2C_I_am_applying.html">142 hunch net-2005-12-22-Yes , I am applying</a></p>
<p>16 0.4676218 <a title="25-lsi-16" href="../hunch_net-2005/hunch_net-2005-07-21-Six_Months.html">96 hunch net-2005-07-21-Six Months</a></p>
<p>17 0.41932729 <a title="25-lsi-17" href="../hunch_net-2005/hunch_net-2005-03-29-Academic_Mechanism_Design.html">48 hunch net-2005-03-29-Academic Mechanism Design</a></p>
<p>18 0.41595486 <a title="25-lsi-18" href="../hunch_net-2005/hunch_net-2005-05-12-Math_on_the_Web.html">70 hunch net-2005-05-12-Math on the Web</a></p>
<p>19 0.41335335 <a title="25-lsi-19" href="../hunch_net-2007/hunch_net-2007-11-05-CMU_wins_DARPA_Urban_Challenge.html">271 hunch net-2007-11-05-CMU wins DARPA Urban Challenge</a></p>
<p>20 0.40142033 <a title="25-lsi-20" href="../hunch_net-2008/hunch_net-2008-04-21-The_Science_2.0_article.html">296 hunch net-2008-04-21-The Science 2.0 article</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(6, 0.028), (35, 0.044), (39, 0.029), (42, 0.195), (45, 0.076), (61, 0.017), (68, 0.043), (69, 0.013), (74, 0.085), (94, 0.321), (95, 0.06)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.83766276 <a title="25-lda-1" href="../hunch_net-2009/hunch_net-2009-11-29-AI_Safety.html">380 hunch net-2009-11-29-AI Safety</a></p>
<p>Introduction: Dan Reevesintroduced me toMichael Vassarwho ran theSingularity Summitand
educated me a bit on the subject of AI safety which theSingularity
Institutehassmall grants for.I still believe thatinterstellar space travel is
necessary for long term civilization survival, and the AI is necessary for
interstellar space travel. On these grounds alone, we could judge that
developing AI is much more safe than not. Nevertheless, there is a basic
reasonable fear, as expressed by some commenters, that AI could go bad.A basic
scenario starts with someone inventing an AI and telling it to make as much
money as possible. The AI promptly starts trading in various markets to make
money. To improve, it crafts a virus that takes over most of the world's
computers using it as a surveillance network so that it can always make the
right decision. The AI also branches out into any form of distance work,
taking over the entire outsourcing process for all jobs that are entirely
digital. To further improve, the AI</p><p>same-blog 2 0.82540005 <a title="25-lda-2" href="../hunch_net-2005/hunch_net-2005-02-20-At_One_Month.html">25 hunch net-2005-02-20-At One Month</a></p>
<p>Introduction: This is near the one month point, so it seems appropriate to consider meta-
issues for the moment.The number of posts is a bit over 20.The number of
people speaking up in discussions is about 10.The number of people viewing the
site is somewhat more than 100.I am (naturally) dissatisfied with many
things.Many of thepotential useshaven't been realized. This is partly a matter
of opportunity (no conferences in the last month), partly a matter of will (no
open problems because it's hard to give them up), and partly a matter of
tradition. In academia, there is a strong tradition of trying to get
everything perfectly right before presentation. This is somewhat contradictory
to the nature of making many posts, and it's definitely contradictory to the
idea of doing "public research". If that sort of idea is to pay off, it must
be significantly more succesful than previous methods. In an effort to
continue experimenting, I'm going to use the next week as "open problems
week".Spam is a problem.</p><p>3 0.7940557 <a title="25-lda-3" href="../hunch_net-2008/hunch_net-2008-01-18-Datasets.html">284 hunch net-2008-01-18-Datasets</a></p>
<p>Introduction: David Pennocknotes the impressiveset of datasetsatdatawrangling.</p><p>4 0.7176773 <a title="25-lda-4" href="../hunch_net-2005/hunch_net-2005-02-28-Regularization.html">33 hunch net-2005-02-28-Regularization</a></p>
<p>Introduction: Yaroslav Bulatovsays that we should think about regularization a bit. It's a
complex topic which I only partially understand, so I'll try to explain from a
couple viewpoints.Functionally. Regularization is optimizing some
representation to fit the dataandminimize some notion of predictor complexity.
This notion of complexity is often the l1or l2norm on a set of parameters, but
the term can be used much more generally. Empirically, this often works much
better than simply fitting the data.Statistical Learning
ViewpointRegularization is about the failiure of statistical learning to
adequately predict generalization error. Lete(c,D)be the expected error rate
with respect toDof classifiercande(c,S)the observed error rate on a sampleS.
There are numerous bounds of the form: assuming i.i.d. samples, with high
probability over the drawn samplesS,e(c,D) less than e(c,S) +
f(complexity)wherecomplexityis some measure of the size of a set of functions.
Unfortunately, we have never convincingly na</p><p>5 0.58074391 <a title="25-lda-5" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>Introduction: Attempts to abstract and study machine learning are within some given
framework or mathematical model. It turns out that all of these models are
significantly flawed for the purpose of studying machine learning. I've
created a table (below) outlining the major flaws in some common models of
machine learning.The point here is not simply "woe unto us". There are several
implications which seem important.The multitude of models is a point of
continuing confusion. It is common for people to learn about machine learning
within one framework which often becomes there "home framework" through which
they attempt to filter all machine learning. (Have you met people who can only
think in terms of kernels? Only via Bayes Law? Only via PAC Learning?)
Explicitly understanding the existence of these other frameworks can help
resolve the confusion. This is particularly important when reviewing and
particularly important for students.Algorithms which conform to multiple
approaches can have substantial</p><p>6 0.57859725 <a title="25-lda-6" href="../hunch_net-2005/hunch_net-2005-09-12-Fast_Gradient_Descent.html">111 hunch net-2005-09-12-Fast Gradient Descent</a></p>
<p>7 0.57722104 <a title="25-lda-7" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>8 0.57639033 <a title="25-lda-8" href="../hunch_net-2006/hunch_net-2006-09-28-Programming_Languages_for_Machine_Learning_Implementations.html">210 hunch net-2006-09-28-Programming Languages for Machine Learning Implementations</a></p>
<p>9 0.57610554 <a title="25-lda-9" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>10 0.57552165 <a title="25-lda-10" href="../hunch_net-2005/hunch_net-2005-03-17-Going_all_the_Way%2C_Sometimes.html">42 hunch net-2005-03-17-Going all the Way, Sometimes</a></p>
<p>11 0.57372421 <a title="25-lda-11" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>12 0.57101452 <a title="25-lda-12" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>13 0.56998825 <a title="25-lda-13" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>14 0.5693171 <a title="25-lda-14" href="../hunch_net-2010/hunch_net-2010-12-26-NIPS_2010.html">420 hunch net-2010-12-26-NIPS 2010</a></p>
<p>15 0.56786591 <a title="25-lda-15" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>16 0.56706691 <a title="25-lda-16" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>17 0.5664869 <a title="25-lda-17" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<p>18 0.56599975 <a title="25-lda-18" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>19 0.5652287 <a title="25-lda-19" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>20 0.56510472 <a title="25-lda-20" href="../hunch_net-2013/hunch_net-2013-12-01-NIPS_tutorials_and_Vowpal_Wabbit_7.4.html">492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
