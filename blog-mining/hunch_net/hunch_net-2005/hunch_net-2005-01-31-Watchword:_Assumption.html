<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>7 hunch net-2005-01-31-Watchword: Assumption</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-7" href="#">hunch_net-2005-7</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>7 hunch net-2005-01-31-Watchword: Assumption</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-7-html" href="http://hunch.net/?p=10">html</a></p><p>Introduction: "Assumption" is another word to be careful with in machine learning because it
is used in several ways.Assumption = BiasThere are several ways to see that
some form of 'bias' (= preferring of one solution over another) is necessary.
This is obvious in an adversarial setting. A good bit of work has been
expended explaining this in other settings with "no free lunch" theorems. This
is a usage specialized to learning which is particularly common when talking
about priors for Bayesian Learning.Assumption = "if" of a theoremThe
assumptions are the 'if' part of the 'if-then' in a theorem. This is a fairly
common usage.Assumption = AxiomThe assumptions are the things that we assume
are true, but which we cannot verify. Examples are "the IID assumption" or "my
problem is a DNF on a small number of bits". This is the usage which I
prefer.One difficulty with any use of the word "assumption" is that you often
encounter "ifassumptionthenconclusionso ifnot assumptionthennot conclusion".
This is inc</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 "Assumption" is another word to be careful with in machine learning because it is used in several ways. [sent-1, score-0.509]
</p><p>2 Assumption = BiasThere are several ways to see that some form of 'bias' (= preferring of one solution over another) is necessary. [sent-2, score-0.475]
</p><p>3 A good bit of work has been expended explaining this in other settings with "no free lunch" theorems. [sent-4, score-0.63]
</p><p>4 This is a usage specialized to learning which is particularly common when talking about priors for Bayesian Learning. [sent-5, score-0.892]
</p><p>5 Assumption = "if" of a theoremThe assumptions are the 'if' part of the 'if-then' in a theorem. [sent-6, score-0.265]
</p><p>6 Assumption = AxiomThe assumptions are the things that we assume are true, but which we cannot verify. [sent-8, score-0.305]
</p><p>7 Examples are "the IID assumption" or "my problem is a DNF on a small number of bits". [sent-9, score-0.061]
</p><p>8 One difficulty with any use of the word "assumption" is that you often encounter "ifassumptionthenconclusionso ifnot assumptionthennot conclusion". [sent-11, score-0.418]
</p><p>9 For example, with variant (1), "the assumption of my prior is not met so the algorithm will not learn". [sent-13, score-0.928]
</p><p>10 Or, with variant (3), "the data is not IID, so my learning algorithm designed for IID data will not work". [sent-14, score-0.616]
</p><p>11 In each of these cases "will" must be replaced with "may" for correctness. [sent-15, score-0.289]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('assumption', 0.39), ('usage', 0.329), ('iid', 0.292), ('variant', 0.24), ('word', 0.213), ('assumptions', 0.192), ('expended', 0.177), ('incorrect', 0.164), ('lunch', 0.164), ('preferring', 0.164), ('conclusion', 0.148), ('correctness', 0.148), ('priors', 0.148), ('replaced', 0.148), ('met', 0.133), ('specialized', 0.133), ('explaining', 0.129), ('encounter', 0.12), ('talking', 0.12), ('careful', 0.113), ('assume', 0.113), ('bits', 0.113), ('another', 0.112), ('adversarial', 0.111), ('settings', 0.109), ('common', 0.105), ('designed', 0.103), ('data', 0.095), ('fairly', 0.09), ('cases', 0.085), ('difficulty', 0.085), ('obvious', 0.084), ('algorithm', 0.083), ('free', 0.082), ('prior', 0.082), ('work', 0.079), ('bayesian', 0.078), ('true', 0.075), ('part', 0.073), ('several', 0.071), ('solution', 0.067), ('ways', 0.065), ('small', 0.061), ('learn', 0.058), ('form', 0.057), ('particularly', 0.057), ('must', 0.056), ('examples', 0.054), ('bit', 0.054), ('see', 0.051)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="7-tfidf-1" href="../hunch_net-2005/hunch_net-2005-01-31-Watchword%3A_Assumption.html">7 hunch net-2005-01-31-Watchword: Assumption</a></p>
<p>Introduction: "Assumption" is another word to be careful with in machine learning because it
is used in several ways.Assumption = BiasThere are several ways to see that
some form of 'bias' (= preferring of one solution over another) is necessary.
This is obvious in an adversarial setting. A good bit of work has been
expended explaining this in other settings with "no free lunch" theorems. This
is a usage specialized to learning which is particularly common when talking
about priors for Bayesian Learning.Assumption = "if" of a theoremThe
assumptions are the 'if' part of the 'if-then' in a theorem. This is a fairly
common usage.Assumption = AxiomThe assumptions are the things that we assume
are true, but which we cannot verify. Examples are "the IID assumption" or "my
problem is a DNF on a small number of bits". This is the usage which I
prefer.One difficulty with any use of the word "assumption" is that you often
encounter "ifassumptionthenconclusionso ifnot assumptionthennot conclusion".
This is inc</p><p>2 0.21098369 <a title="7-tfidf-2" href="../hunch_net-2005/hunch_net-2005-11-02-Progress_in_Active_Learning.html">127 hunch net-2005-11-02-Progress in Active Learning</a></p>
<p>Introduction: Several bits of progress have been made sinceSanjoypointed out the
significantlack of theoretical understanding of active learning. This is an
update on the progress I know of. As a refresher, active learning as meant
here is:There is a source of unlabeled data.There is an oracle from which
labels can be requested for unlabeled data produced by the source.The goal is
to perform well with minimal use of the oracle.Here is what I've
learned:Sanjoy has developed sufficient and semi-necessary conditions for
active learning given the assumptions of IID data and "realizability" (that
one of the classifiers is a correct classifier).Nina,Alina, and I developed an
algorithm for active learning relying on only the assumption of IID data. A
draft ishere.Nicolo,Claudio, andLucashowed that it is possible to do active
learning in an entirely adversarial setting for linear threshold
classifiershere. This was published a year or two ago and I recently learned
about it.All of these results are relative</p><p>3 0.18609881 <a title="7-tfidf-3" href="../hunch_net-2005/hunch_net-2005-04-16-Which_Assumptions_are_Reasonable%3F.html">57 hunch net-2005-04-16-Which Assumptions are Reasonable?</a></p>
<p>Introduction: One of the most confusing things about understanding learning theory is the
vast array of differing assumptions. Some critical thought about which of
these assumptions are reasonable for real-world problems may be useful.Before
we even start thinking about assumptions, it's important to realize that the
word hasmultiple meanings. The meaning used here is "assumption = axiom" (i.e.
something you can not verify).AssumptionReasonable?Which
analysis?Example/notesIndependent and Identically Distributed
DataSometimesPAC,ERM,Prediction bounds,statisticsTheKDD cup 2004 physics
datasetis plausibly IID data. There are a number of situations which are
"almost IID" in the sense that IID analysis results in correct intuitions.
Unreasonable in adversarial situations (stock market, war, etcâ&euro;Ś)Independently
Distributed DataMore than IID, but still only sometimesonline->batch
conversionLosing "identical" can be helpful in situations where you have a
cyclic process generating data.Finite exchangeability</p><p>4 0.14701104 <a title="7-tfidf-4" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>Introduction: One of the remarkable things about machine learning is how diverse it is. The
viewpoints of Bayesian learning, reinforcement learning, graphical models,
supervised learning, unsupervised learning, genetic programming, etc… share
little enough overlap that many people can and do make their careers within
one without touching, or even necessarily understanding the others.There are
two fundamental reasons why this is possible.For many problems, many
approaches work in the sense that they do something useful. This is true
empirically, where for many problems we can observe that many different
approaches yield better performance than any constant predictor. It's also
true in theory, where we know that for any set of predictors representable in
a finite amount of RAM, minimizing training error over the set of predictors
does something nontrivial when there are a sufficient number of examples.There
is nothing like a unifying problem defining the field. In many other areas
there are unifying p</p><p>5 0.13166228 <a title="7-tfidf-5" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>Introduction: Attempts to abstract and study machine learning are within some given
framework or mathematical model. It turns out that all of these models are
significantly flawed for the purpose of studying machine learning. I've
created a table (below) outlining the major flaws in some common models of
machine learning.The point here is not simply "woe unto us". There are several
implications which seem important.The multitude of models is a point of
continuing confusion. It is common for people to learn about machine learning
within one framework which often becomes there "home framework" through which
they attempt to filter all machine learning. (Have you met people who can only
think in terms of kernels? Only via Bayes Law? Only via PAC Learning?)
Explicitly understanding the existence of these other frameworks can help
resolve the confusion. This is particularly important when reviewing and
particularly important for students.Algorithms which conform to multiple
approaches can have substantial</p><p>6 0.1294124 <a title="7-tfidf-6" href="../hunch_net-2005/hunch_net-2005-02-03-Learning_Theory%2C_by_assumption.html">12 hunch net-2005-02-03-Learning Theory, by assumption</a></p>
<p>7 0.10981103 <a title="7-tfidf-7" href="../hunch_net-2005/hunch_net-2005-10-26-Fallback_Analysis_is_a_Secret_to_Useful_Algorithms.html">126 hunch net-2005-10-26-Fallback Analysis is a Secret to Useful Algorithms</a></p>
<p>8 0.10969524 <a title="7-tfidf-8" href="../hunch_net-2005/hunch_net-2005-03-15-The_State_of_Tight_Bounds.html">41 hunch net-2005-03-15-The State of Tight Bounds</a></p>
<p>9 0.10708982 <a title="7-tfidf-9" href="../hunch_net-2005/hunch_net-2005-08-22-Do_you_believe_in_induction%3F.html">104 hunch net-2005-08-22-Do you believe in induction?</a></p>
<p>10 0.10539543 <a title="7-tfidf-10" href="../hunch_net-2005/hunch_net-2005-04-23-Advantages_and_Disadvantages_of_Bayesian_Learning.html">60 hunch net-2005-04-23-Advantages and Disadvantages of Bayesian Learning</a></p>
<p>11 0.10267964 <a title="7-tfidf-11" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>12 0.096849337 <a title="7-tfidf-12" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>13 0.091312557 <a title="7-tfidf-13" href="../hunch_net-2006/hunch_net-2006-05-01-A_conversation_between_Theo_and_Pat.html">176 hunch net-2006-05-01-A conversation between Theo and Pat</a></p>
<p>14 0.085206278 <a title="7-tfidf-14" href="../hunch_net-2008/hunch_net-2008-04-26-Eliminating_the_Birthday_Paradox_for_Universal_Features.html">298 hunch net-2008-04-26-Eliminating the Birthday Paradox for Universal Features</a></p>
<p>15 0.083089352 <a title="7-tfidf-15" href="../hunch_net-2009/hunch_net-2009-08-03-Carbon_in_Computer_Science_Research.html">366 hunch net-2009-08-03-Carbon in Computer Science Research</a></p>
<p>16 0.08291205 <a title="7-tfidf-16" href="../hunch_net-2005/hunch_net-2005-03-02-Prior%2C_%26%238220%3BPrior%26%238221%3B_and_Bias.html">34 hunch net-2005-03-02-Prior, &#8220;Prior&#8221; and Bias</a></p>
<p>17 0.080850065 <a title="7-tfidf-17" href="../hunch_net-2005/hunch_net-2005-03-18-Binomial_Weighting.html">43 hunch net-2005-03-18-Binomial Weighting</a></p>
<p>18 0.080189787 <a title="7-tfidf-18" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>19 0.079942912 <a title="7-tfidf-19" href="../hunch_net-2005/hunch_net-2005-09-26-Prediction_Bounds_as_the_Mathematics_of_Science.html">115 hunch net-2005-09-26-Prediction Bounds as the Mathematics of Science</a></p>
<p>20 0.078979775 <a title="7-tfidf-20" href="../hunch_net-2007/hunch_net-2007-01-10-A_Deep_Belief_Net_Learning_Problem.html">227 hunch net-2007-01-10-A Deep Belief Net Learning Problem</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.165), (1, -0.102), (2, -0.002), (3, -0.034), (4, -0.069), (5, -0.049), (6, -0.081), (7, -0.024), (8, -0.073), (9, -0.051), (10, -0.07), (11, 0.01), (12, 0.003), (13, -0.051), (14, -0.087), (15, -0.158), (16, 0.049), (17, -0.034), (18, 0.012), (19, 0.051), (20, 0.031), (21, 0.014), (22, 0.132), (23, 0.045), (24, 0.044), (25, 0.039), (26, 0.057), (27, 0.092), (28, 0.094), (29, 0.031), (30, -0.006), (31, 0.035), (32, 0.056), (33, 0.116), (34, -0.062), (35, -0.056), (36, -0.066), (37, 0.038), (38, 0.043), (39, 0.112), (40, -0.102), (41, -0.049), (42, 0.052), (43, -0.014), (44, -0.051), (45, 0.029), (46, 0.054), (47, 0.016), (48, 0.017), (49, -0.04)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95644438 <a title="7-lsi-1" href="../hunch_net-2005/hunch_net-2005-01-31-Watchword%3A_Assumption.html">7 hunch net-2005-01-31-Watchword: Assumption</a></p>
<p>Introduction: "Assumption" is another word to be careful with in machine learning because it
is used in several ways.Assumption = BiasThere are several ways to see that
some form of 'bias' (= preferring of one solution over another) is necessary.
This is obvious in an adversarial setting. A good bit of work has been
expended explaining this in other settings with "no free lunch" theorems. This
is a usage specialized to learning which is particularly common when talking
about priors for Bayesian Learning.Assumption = "if" of a theoremThe
assumptions are the 'if' part of the 'if-then' in a theorem. This is a fairly
common usage.Assumption = AxiomThe assumptions are the things that we assume
are true, but which we cannot verify. Examples are "the IID assumption" or "my
problem is a DNF on a small number of bits". This is the usage which I
prefer.One difficulty with any use of the word "assumption" is that you often
encounter "ifassumptionthenconclusionso ifnot assumptionthennot conclusion".
This is inc</p><p>2 0.797414 <a title="7-lsi-2" href="../hunch_net-2005/hunch_net-2005-04-16-Which_Assumptions_are_Reasonable%3F.html">57 hunch net-2005-04-16-Which Assumptions are Reasonable?</a></p>
<p>Introduction: One of the most confusing things about understanding learning theory is the
vast array of differing assumptions. Some critical thought about which of
these assumptions are reasonable for real-world problems may be useful.Before
we even start thinking about assumptions, it's important to realize that the
word hasmultiple meanings. The meaning used here is "assumption = axiom" (i.e.
something you can not verify).AssumptionReasonable?Which
analysis?Example/notesIndependent and Identically Distributed
DataSometimesPAC,ERM,Prediction bounds,statisticsTheKDD cup 2004 physics
datasetis plausibly IID data. There are a number of situations which are
"almost IID" in the sense that IID analysis results in correct intuitions.
Unreasonable in adversarial situations (stock market, war, etcâ&euro;Ś)Independently
Distributed DataMore than IID, but still only sometimesonline->batch
conversionLosing "identical" can be helpful in situations where you have a
cyclic process generating data.Finite exchangeability</p><p>3 0.66216129 <a title="7-lsi-3" href="../hunch_net-2005/hunch_net-2005-11-02-Progress_in_Active_Learning.html">127 hunch net-2005-11-02-Progress in Active Learning</a></p>
<p>Introduction: Several bits of progress have been made sinceSanjoypointed out the
significantlack of theoretical understanding of active learning. This is an
update on the progress I know of. As a refresher, active learning as meant
here is:There is a source of unlabeled data.There is an oracle from which
labels can be requested for unlabeled data produced by the source.The goal is
to perform well with minimal use of the oracle.Here is what I've
learned:Sanjoy has developed sufficient and semi-necessary conditions for
active learning given the assumptions of IID data and "realizability" (that
one of the classifiers is a correct classifier).Nina,Alina, and I developed an
algorithm for active learning relying on only the assumption of IID data. A
draft ishere.Nicolo,Claudio, andLucashowed that it is possible to do active
learning in an entirely adversarial setting for linear threshold
classifiershere. This was published a year or two ago and I recently learned
about it.All of these results are relative</p><p>4 0.62529427 <a title="7-lsi-4" href="../hunch_net-2005/hunch_net-2005-10-26-Fallback_Analysis_is_a_Secret_to_Useful_Algorithms.html">126 hunch net-2005-10-26-Fallback Analysis is a Secret to Useful Algorithms</a></p>
<p>Introduction: The ideal of theoretical algorithm analysis is to construct an algorithm with
accompanying optimality theorems proving that it is a useful algorithm. This
ideal often fails, particularly for learning algorithms and theory. The
general form of a theorem is:IfpreconditionsThenpostconditionsWhen we design
learning algorithms it is very common to come up with precondition assumptions
such as "the data is IID", "the learning problem is drawn from a known
distribution over learning problems", or "there is a perfect classifier". All
of these example preconditions can be false for real-world problems in ways
that are not easily detectable. This means that algorithms derived and
justified by these very common forms of analysis may be prone to catastrophic
failure in routine (mis)application.Wecanhope for better. Several different
kinds of learning algorithm analysis have been developed some of which have
fewer preconditions. Simply demanding that these forms of analysis be used may
be too stron</p><p>5 0.62241817 <a title="7-lsi-5" href="../hunch_net-2006/hunch_net-2006-02-18-Multiplication_of_Learned_Probabilities_is_Dangerous.html">157 hunch net-2006-02-18-Multiplication of Learned Probabilities is Dangerous</a></p>
<p>Introduction: This is about a design flaw in several learning algorithms such as the Naive
Bayes classifier and Hidden Markov Models. A number of people are aware of it,
but it seems that not everyone is.Several learning systems have the property
that they estimate some conditional probabilitiesP(event | other events)either
explicitly or implicitly. Then, at prediction time, these learned
probabilities are multiplied together according to some formula to produce a
final prediction. The Naive Bayes classifier for binary data is the simplest
of these, so it seems like a good example.When Naive Bayes is used, a set of
probabilities of the formPr'(feature i | label)are estimated via counting
statistics and some prior. Predictions are made according to the label
maximizing:Pr'(label) * Productfeatures iPr'(feature i | label)(ThePr'notation
indicates these are estimated values.)There is nothing wrong with this method
as long as (a) the prior for the sample counts is very strong and (b) the
prior (on the c</p><p>6 0.61387593 <a title="7-lsi-6" href="../hunch_net-2007/hunch_net-2007-09-18-It%26%238217%3Bs_MDL_Jim%2C_but_not_as_we_know_it%26%238230%3B%28on_Bayes%2C_MDL_and_consistency%29.html">263 hunch net-2007-09-18-It&#8217;s MDL Jim, but not as we know it&#8230;(on Bayes, MDL and consistency)</a></p>
<p>7 0.57960588 <a title="7-lsi-7" href="../hunch_net-2005/hunch_net-2005-03-18-Binomial_Weighting.html">43 hunch net-2005-03-18-Binomial Weighting</a></p>
<p>8 0.56947261 <a title="7-lsi-8" href="../hunch_net-2006/hunch_net-2006-03-23-The_Approximation_Argument.html">165 hunch net-2006-03-23-The Approximation Argument</a></p>
<p>9 0.56266981 <a title="7-lsi-9" href="../hunch_net-2005/hunch_net-2005-02-03-Learning_Theory%2C_by_assumption.html">12 hunch net-2005-02-03-Learning Theory, by assumption</a></p>
<p>10 0.55585867 <a title="7-lsi-10" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>11 0.54232228 <a title="7-lsi-11" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>12 0.53782004 <a title="7-lsi-12" href="../hunch_net-2005/hunch_net-2005-12-07-Is_the_Google_way_the_way_for_machine_learning%3F.html">136 hunch net-2005-12-07-Is the Google way the way for machine learning?</a></p>
<p>13 0.5217979 <a title="7-lsi-13" href="../hunch_net-2012/hunch_net-2012-02-20-Berkeley_Streaming_Data_Workshop.html">455 hunch net-2012-02-20-Berkeley Streaming Data Workshop</a></p>
<p>14 0.51634765 <a title="7-lsi-14" href="../hunch_net-2006/hunch_net-2006-07-08-MaxEnt_contradicts_Bayes_Rule%3F.html">191 hunch net-2006-07-08-MaxEnt contradicts Bayes Rule?</a></p>
<p>15 0.49166292 <a title="7-lsi-15" href="../hunch_net-2005/hunch_net-2005-04-23-Advantages_and_Disadvantages_of_Bayesian_Learning.html">60 hunch net-2005-04-23-Advantages and Disadvantages of Bayesian Learning</a></p>
<p>16 0.48368219 <a title="7-lsi-16" href="../hunch_net-2006/hunch_net-2006-05-01-A_conversation_between_Theo_and_Pat.html">176 hunch net-2006-05-01-A conversation between Theo and Pat</a></p>
<p>17 0.48062211 <a title="7-lsi-17" href="../hunch_net-2006/hunch_net-2006-03-02-Why_do_people_count_for_learning%3F.html">160 hunch net-2006-03-02-Why do people count for learning?</a></p>
<p>18 0.47356057 <a title="7-lsi-18" href="../hunch_net-2005/hunch_net-2005-08-22-Do_you_believe_in_induction%3F.html">104 hunch net-2005-08-22-Do you believe in induction?</a></p>
<p>19 0.45121139 <a title="7-lsi-19" href="../hunch_net-2007/hunch_net-2007-12-12-Workshop_Summary%26%238212%3BPrinciples_of_Learning_Problem_Design.html">277 hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</a></p>
<p>20 0.4442862 <a title="7-lsi-20" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(42, 0.278), (50, 0.463), (68, 0.048), (74, 0.072), (95, 0.013)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.97621423 <a title="7-lda-1" href="../hunch_net-2007/hunch_net-2007-11-29-The_Netflix_Crack.html">275 hunch net-2007-11-29-The Netflix Crack</a></p>
<p>Introduction: A couple security researchersclaim to have cracked the netflix dataset. The
claims of success appear somewhat overstated to me, but the method of attack
is valid and could plausibly be substantially improved so as to reveal the
movie preferences of a small fraction of Netflix users.The basic idea is to
use a heuristic similarity function between ratings in a public database (from
IMDB) and an anonymized database (Netflix) to link ratings in the private
database to public identities (in IMDB). They claim to have linked two of a
few dozen IMDB users to anonymized netflix users.The claims seem a bit
inflated to me, because (a) knowing the IMDB identity isn't equivalent to
knowing the person and (b) the claims of statistical significance are with
respect to a model of the world they created (rather than one they
created).Overall, this is another example showing that completeprivacy is
hard. It may be worth remembering that there are some substantial benefits
from the Netflix challenge as w</p><p>same-blog 2 0.92360669 <a title="7-lda-2" href="../hunch_net-2005/hunch_net-2005-01-31-Watchword%3A_Assumption.html">7 hunch net-2005-01-31-Watchword: Assumption</a></p>
<p>Introduction: "Assumption" is another word to be careful with in machine learning because it
is used in several ways.Assumption = BiasThere are several ways to see that
some form of 'bias' (= preferring of one solution over another) is necessary.
This is obvious in an adversarial setting. A good bit of work has been
expended explaining this in other settings with "no free lunch" theorems. This
is a usage specialized to learning which is particularly common when talking
about priors for Bayesian Learning.Assumption = "if" of a theoremThe
assumptions are the 'if' part of the 'if-then' in a theorem. This is a fairly
common usage.Assumption = AxiomThe assumptions are the things that we assume
are true, but which we cannot verify. Examples are "the IID assumption" or "my
problem is a DNF on a small number of bits". This is the usage which I
prefer.One difficulty with any use of the word "assumption" is that you often
encounter "ifassumptionthenconclusionso ifnot assumptionthennot conclusion".
This is inc</p><p>3 0.91726506 <a title="7-lda-3" href="../hunch_net-2005/hunch_net-2005-12-04-Watchword%3A_model.html">135 hunch net-2005-12-04-Watchword: model</a></p>
<p>Introduction: In everyday use a model is a system which explains the behavior of some
system, hopefully at the level where some alteration of the model predicts
some alteration of the real-world system. In machine learning "model" has
several variant definitions.Everyday. The common definition is sometimes
used.Parameterized. Sometimes model is a short-hand for "parameterized model".
Here, it refers to a model with unspecified free parameters. In the Bayesian
learning approach, you typically have a prior over (everyday)
models.Predictive. Even further from everyday use is the predictive model.
Examples of this are "my model is a decision tree" or "my model is a support
vector machine". Here, there is no real sense in which an SVM explains the
underlying process. For example, an SVM tells us nothing in particular about
how alterations to the real-world system would create a change.Which
definition is being used at any particular time is important information. For
example, if it's a parameterized or p</p><p>4 0.90522873 <a title="7-lda-4" href="../hunch_net-2006/hunch_net-2006-06-25-Presentation_of_Proofs_is_Hard..html">187 hunch net-2006-06-25-Presentation of Proofs is Hard.</a></p>
<p>Introduction: When presenting part of theReinforcement Learning theory tutorialatICML 2006,
I was forcibly reminded of this.There are several difficulties.When creating
the presentation, the correct level of detail is tricky. With too much detail,
the proof takes too much time and people may be lost to boredom. With too
little detail, the steps of the proof involve too-great a jump. This is very
difficult to judge.What may be an easy step in the careful thought of a quiet
room is not so easy when you are occupied by the process of presentation.What
may be easy after having gone over this (and other) proofs is not so easy to
follow in the first pass by a viewer.These problems seem only correctable by
process of repeated test-and-revise.When presenting the proof, simply speaking
with sufficient precision is substantially harder than in normal conversation
(where precision is not so critical). Practice can help here.When presenting
the proof, going at the right pace for understanding is difficult. When</p><p>5 0.89339262 <a title="7-lda-5" href="../hunch_net-2009/hunch_net-2009-08-27-New_York_Area_Machine_Learning_Events.html">369 hunch net-2009-08-27-New York Area Machine Learning Events</a></p>
<p>Introduction: Several events are happening in the NY area.Barriers in Computational Learning
Theory Workshop, Aug 28.That's tomorrow near Princeton. I'm looking forward to
speaking at this one on "Getting around Barriers in Learning Theory", but
several other talks are of interest, particularly to the CS theory
inclined.Claudia Perlichis running theINFORMS Data Mining Contestwith a
deadline of Sept. 25. This is a contest using real health record data (they
partnered withHealthCare Intelligence) to predict transfers and mortality. In
the current US health care reform debate, the case studies of high costs we
hear strongly suggest machine learning & statistics can save many billions.The
Singularity Summit October 3&4\. This is for the AIists out there. Several of
the talks look interesting, although unfortunately I'll miss it
forALT.Predictive Analytics World, Oct 20-21. This is stretching the
definition of "New York Area" a bit, but the train to DC is reasonable. This
is a conference of case studies</p><p>6 0.88626242 <a title="7-lda-6" href="../hunch_net-2005/hunch_net-2005-07-23-Interesting_papers_at_ACL.html">97 hunch net-2005-07-23-Interesting papers at ACL</a></p>
<p>7 0.81121713 <a title="7-lda-7" href="../hunch_net-2008/hunch_net-2008-11-28-A_Bumper_Crop_of_Machine_Learning_Graduates.html">329 hunch net-2008-11-28-A Bumper Crop of Machine Learning Graduates</a></p>
<p>8 0.70177138 <a title="7-lda-8" href="../hunch_net-2007/hunch_net-2007-02-02-Thoughts_regarding_%26%238220%3BIs_machine_learning_different_from_statistics%3F%26%238221%3B.html">230 hunch net-2007-02-02-Thoughts regarding &#8220;Is machine learning different from statistics?&#8221;</a></p>
<p>9 0.60021585 <a title="7-lda-9" href="../hunch_net-2005/hunch_net-2005-06-08-Question%3A_%26%238220%3BWhen_is_the_right_time_to_insert_the_loss_function%3F%26%238221%3B.html">79 hunch net-2005-06-08-Question: &#8220;When is the right time to insert the loss function?&#8221;</a></p>
<p>10 0.57719713 <a title="7-lda-10" href="../hunch_net-2008/hunch_net-2008-05-23-Three_levels_of_addressing_the_Netflix_Prize.html">301 hunch net-2008-05-23-Three levels of addressing the Netflix Prize</a></p>
<p>11 0.57575828 <a title="7-lda-11" href="../hunch_net-2005/hunch_net-2005-11-02-Progress_in_Active_Learning.html">127 hunch net-2005-11-02-Progress in Active Learning</a></p>
<p>12 0.57361132 <a title="7-lda-12" href="../hunch_net-2006/hunch_net-2006-05-01-A_conversation_between_Theo_and_Pat.html">176 hunch net-2006-05-01-A conversation between Theo and Pat</a></p>
<p>13 0.56897676 <a title="7-lda-13" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>14 0.56495345 <a title="7-lda-14" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>15 0.56325227 <a title="7-lda-15" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>16 0.56220782 <a title="7-lda-16" href="../hunch_net-2005/hunch_net-2005-03-15-The_State_of_Tight_Bounds.html">41 hunch net-2005-03-15-The State of Tight Bounds</a></p>
<p>17 0.56042713 <a title="7-lda-17" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>18 0.55951363 <a title="7-lda-18" href="../hunch_net-2006/hunch_net-2006-07-13-Regression_vs._Classification_as_a_Primitive.html">196 hunch net-2006-07-13-Regression vs. Classification as a Primitive</a></p>
<p>19 0.55916578 <a title="7-lda-19" href="../hunch_net-2005/hunch_net-2005-09-26-Prediction_Bounds_as_the_Mathematics_of_Science.html">115 hunch net-2005-09-26-Prediction Bounds as the Mathematics of Science</a></p>
<p>20 0.55901557 <a title="7-lda-20" href="../hunch_net-2006/hunch_net-2006-03-17-Multitask_learning_is_Black-Boxable.html">164 hunch net-2006-03-17-Multitask learning is Black-Boxable</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
