<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>43 hunch net-2005-03-18-Binomial Weighting</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-43" href="#">hunch_net-2005-43</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>43 hunch net-2005-03-18-Binomial Weighting</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-43-html" href="http://hunch.net/?p=47">html</a></p><p>Introduction: Suppose we have a set of classifiers  c  making binary predictions from an input  x  and we see examples in an online fashion.  In particular, we repeatedly see an unlabeled example  x , make a prediction  y’ (possibly based on the classifiers  c ), and then see the correct label  y .  
 
When one of these classifiers is perfect, there is a great algorithm available: predict according to the majority vote over every classifier  consistent with every previous example.  This is called the Halving algorithm.  It makes at most  log 2  |c|  mistakes since on any mistake, at least half of the classifiers are eliminated.
 
Obviously, we can’t generally hope that the there exists a classifier which never errs.  The  Binomial Weighting algorithm  is an elegant technique allowing a variant Halving algorithm  to cope with errors by creating a set of virtual classifiers for every classifier which occasionally disagree with the original classifier.  The Halving algorithm on this set of virtual clas</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Suppose we have a set of classifiers  c  making binary predictions from an input  x  and we see examples in an online fashion. [sent-1, score-0.567]
</p><p>2 In particular, we repeatedly see an unlabeled example  x , make a prediction  y’ (possibly based on the classifiers  c ), and then see the correct label  y . [sent-2, score-0.657]
</p><p>3 When one of these classifiers is perfect, there is a great algorithm available: predict according to the majority vote over every classifier  consistent with every previous example. [sent-3, score-1.164]
</p><p>4 It makes at most  log 2  |c|  mistakes since on any mistake, at least half of the classifiers are eliminated. [sent-5, score-0.681]
</p><p>5 Obviously, we can’t generally hope that the there exists a classifier which never errs. [sent-6, score-0.165]
</p><p>6 The  Binomial Weighting algorithm  is an elegant technique allowing a variant Halving algorithm  to cope with errors by creating a set of virtual classifiers for every classifier which occasionally disagree with the original classifier. [sent-7, score-2.023]
</p><p>7 By introducing a “prior” over the number of mistakes, it can be made parameter free. [sent-9, score-0.652]
</p><p>8 Similarly, introducing a “prior” over the set of classifiers is easy and makes the algorithm sufficiently flexible for common use. [sent-10, score-1.196]
</p><p>9 The minimal value of  f()  is  2  times the number of errors of any classifier, regardless of the number of classifiers. [sent-12, score-0.803]
</p><p>10 This is frustrating because a parameter-free learning algorithm taking an arbitrary “prior” and achieving good performance on an arbitrary (not even IID) set of examples is compelling for implementation and use,  if  we had a good technique for removing the factor of  2 . [sent-13, score-0.927]
</p><p>11 See the  weighted majority algorithm  for an example of a similar algorithm which can remove a factor of 2 using randomization and at the expense of introducing a parameter. [sent-15, score-1.081]
</p><p>12 There are known techniques for eliminating this parameter, but they appear not as tight (and therefore practically useful) as introducing a “prior” over the number of errors. [sent-16, score-0.666]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('classifiers', 0.354), ('introducing', 0.332), ('errors', 0.309), ('halving', 0.249), ('binomial', 0.235), ('weighting', 0.201), ('mistakes', 0.201), ('algorithm', 0.201), ('parameter', 0.171), ('classifier', 0.165), ('prior', 0.162), ('virtual', 0.149), ('number', 0.149), ('minimal', 0.127), ('set', 0.118), ('majority', 0.114), ('arbitrary', 0.103), ('factor', 0.098), ('every', 0.098), ('technique', 0.096), ('see', 0.095), ('maximal', 0.078), ('frustrating', 0.075), ('mistake', 0.075), ('occasionally', 0.075), ('removing', 0.075), ('vote', 0.075), ('randomization', 0.072), ('regardless', 0.069), ('satisfies', 0.069), ('cope', 0.067), ('flexible', 0.067), ('disagree', 0.065), ('elegant', 0.065), ('min', 0.065), ('makes', 0.064), ('eliminating', 0.063), ('remove', 0.063), ('therefore', 0.062), ('half', 0.062), ('perfect', 0.062), ('variant', 0.06), ('possibly', 0.06), ('tight', 0.06), ('sufficiently', 0.06), ('consistent', 0.059), ('implementation', 0.058), ('repeatedly', 0.058), ('called', 0.055), ('unlabeled', 0.055)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000002 <a title="43-tfidf-1" href="../hunch_net-2005/hunch_net-2005-03-18-Binomial_Weighting.html">43 hunch net-2005-03-18-Binomial Weighting</a></p>
<p>Introduction: Suppose we have a set of classifiers  c  making binary predictions from an input  x  and we see examples in an online fashion.  In particular, we repeatedly see an unlabeled example  x , make a prediction  y’ (possibly based on the classifiers  c ), and then see the correct label  y .  
 
When one of these classifiers is perfect, there is a great algorithm available: predict according to the majority vote over every classifier  consistent with every previous example.  This is called the Halving algorithm.  It makes at most  log 2  |c|  mistakes since on any mistake, at least half of the classifiers are eliminated.
 
Obviously, we can’t generally hope that the there exists a classifier which never errs.  The  Binomial Weighting algorithm  is an elegant technique allowing a variant Halving algorithm  to cope with errors by creating a set of virtual classifiers for every classifier which occasionally disagree with the original classifier.  The Halving algorithm on this set of virtual clas</p><p>2 0.21092767 <a title="43-tfidf-2" href="../hunch_net-2005/hunch_net-2005-02-25-Problem%3A_Online_Learning.html">28 hunch net-2005-02-25-Problem: Online Learning</a></p>
<p>Introduction: Despite my best intentions, this is not a fully specified problem, but rather a research direction.  
 
Competitive online learning is one of the more compelling pieces of learning theory because typical statements of the form “this algorithm will perform almost as well as a large set of other algorithms” rely only on fully-observable quantities, and are therefore applicable in many situations.  Examples include  Winnow ,  Weighted Majority , and  Binomial Weighting .  Algorithms with this property haven’t taken over the world yet.  Here might be some reasons:
  
  Lack of caring .  Many people working on learning theory don’t care about particular applications much.  This means constants in the algorithm are not optimized, usable code is often not produced, and empirical studies aren’t done. 
  Inefficiency . Viewed from the perspective of other learning algorithms, online learning is terribly inefficient.  It requires that every hypothesis (called an expert in the online learning set</p><p>3 0.14081267 <a title="43-tfidf-3" href="../hunch_net-2005/hunch_net-2005-05-16-Regret_minimizing_vs_error_limiting_reductions.html">72 hunch net-2005-05-16-Regret minimizing vs error limiting reductions</a></p>
<p>Introduction: This post is about a reductions-related problem that I find mysterious.  There are two kinds of reductions analysis currently under consideration.
  
 Error limiting reductions.  Here, the goal is to bound the error rate of the created classifier in terms of the error rate of the binary classifiers that you reduce to.  A very simple example of this is that  error correcting output codes  where it is possible to prove that for certain codes, the multiclass error rate is at most 4 * the binary classifier error rate. 
 Regret minimizing reductions.  Here, the goal is to bound the  regret  of the created classifier in terms of the  regret  of the binary classifiers reduced to.  The regret is the error rate minus the minimum error rate.  When the learning problem is noisy the minimum error rate may not be  0 .  An analagous result for reget is that for a  probabilistic error correcting output code , multiclass regret is at most 4 * (binary regret) 0.5 .  
  
The use of “regret” is more desi</p><p>4 0.13427861 <a title="43-tfidf-4" href="../hunch_net-2006/hunch_net-2006-05-05-An_ICML_reject.html">177 hunch net-2006-05-05-An ICML reject</a></p>
<p>Introduction: Hal ,  Daniel , and I have been working on the algorithm  Searn  for structured prediction.  This was just conditionally accepted and then rejected from ICML, and we were quite surprised.  By any reasonable criteria, it seems this is an interesting algorithm.
  
 Prediction Performance: Searn performed better than any other algorithm on all the problems we tested against using the same feature set.  This is true even using the numbers reported by authors in their papers. 
 Theoretical underpinning.  Searn is a reduction which comes with a reduction guarantee: the good performance on a base classifiers implies good performance for the overall system.  No other theorem of this type has been made for other structured prediction algorithms, as far as we know. 
 Speed. Searn has no problem handling much larger datasets than other algorithms we tested against. 
 Simplicity.  Given code for a binary classifier and a problem-specific search algorithm, only a few tens of lines are necessary to</p><p>5 0.1298968 <a title="43-tfidf-5" href="../hunch_net-2007/hunch_net-2007-11-28-Computational_Consequences_of_Classification.html">274 hunch net-2007-11-28-Computational Consequences of Classification</a></p>
<p>Introduction: In the  regression vs classification debate , I’m adding a new “pro” to classification.  It seems there are computational shortcuts available for classification which simply aren’t available for regression.  This arises in several situations.
  
 In  active learning  it is sometimes possible to find an  e  error classifier with just  log(e)  labeled samples.    Only much more modest improvements appear to be achievable for squared loss regression.  The essential reason is that the loss function on many examples is flat with respect to large variations in the parameter spaces of a learned classifier, which implies that many of these classifiers do not need to be considered.  In contrast, for squared loss regression, most substantial variations in the parameter space influence the loss at most points. 
 In budgeted learning, where there is either a computational time constraint or a feature cost constraint, a classifier can sometimes be learned to very high accuracy under the constraints</p><p>6 0.12621137 <a title="43-tfidf-6" href="../hunch_net-2006/hunch_net-2006-03-02-Why_do_people_count_for_learning%3F.html">160 hunch net-2006-03-02-Why do people count for learning?</a></p>
<p>7 0.12437008 <a title="43-tfidf-7" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>8 0.12414001 <a title="43-tfidf-8" href="../hunch_net-2005/hunch_net-2005-11-02-Progress_in_Active_Learning.html">127 hunch net-2005-11-02-Progress in Active Learning</a></p>
<p>9 0.12374199 <a title="43-tfidf-9" href="../hunch_net-2005/hunch_net-2005-03-02-Prior%2C_%26%238220%3BPrior%26%238221%3B_and_Bias.html">34 hunch net-2005-03-02-Prior, &#8220;Prior&#8221; and Bias</a></p>
<p>10 0.11741754 <a title="43-tfidf-10" href="../hunch_net-2006/hunch_net-2006-02-18-Multiplication_of_Learned_Probabilities_is_Dangerous.html">157 hunch net-2006-02-18-Multiplication of Learned Probabilities is Dangerous</a></p>
<p>11 0.1157499 <a title="43-tfidf-11" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>12 0.11442 <a title="43-tfidf-12" href="../hunch_net-2007/hunch_net-2007-06-14-Interesting_Papers_at_COLT_2007.html">247 hunch net-2007-06-14-Interesting Papers at COLT 2007</a></p>
<p>13 0.11396057 <a title="43-tfidf-13" href="../hunch_net-2005/hunch_net-2005-02-21-Problem%3A_Cross_Validation.html">26 hunch net-2005-02-21-Problem: Cross Validation</a></p>
<p>14 0.097817086 <a title="43-tfidf-14" href="../hunch_net-2007/hunch_net-2007-10-17-Online_as_the_new_adjective.html">267 hunch net-2007-10-17-Online as the new adjective</a></p>
<p>15 0.096487686 <a title="43-tfidf-15" href="../hunch_net-2005/hunch_net-2005-09-08-Online_Learning_as_the_Mathematics_of_Accountability.html">109 hunch net-2005-09-08-Online Learning as the Mathematics of Accountability</a></p>
<p>16 0.095629387 <a title="43-tfidf-16" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>17 0.095500603 <a title="43-tfidf-17" href="../hunch_net-2008/hunch_net-2008-06-09-The_Minimum_Sample_Complexity_of_Importance_Weighting.html">303 hunch net-2008-06-09-The Minimum Sample Complexity of Importance Weighting</a></p>
<p>18 0.093889445 <a title="43-tfidf-18" href="../hunch_net-2005/hunch_net-2005-02-03-Learning_Theory%2C_by_assumption.html">12 hunch net-2005-02-03-Learning Theory, by assumption</a></p>
<p>19 0.090774223 <a title="43-tfidf-19" href="../hunch_net-2006/hunch_net-2006-03-12-Online_learning_or_online_preservation_of_learning%3F.html">163 hunch net-2006-03-12-Online learning or online preservation of learning?</a></p>
<p>20 0.089848101 <a title="43-tfidf-20" href="../hunch_net-2005/hunch_net-2005-08-22-Do_you_believe_in_induction%3F.html">104 hunch net-2005-08-22-Do you believe in induction?</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.177), (1, 0.152), (2, 0.053), (3, -0.049), (4, 0.032), (5, -0.037), (6, 0.05), (7, 0.008), (8, -0.005), (9, 0.015), (10, -0.014), (11, 0.042), (12, 0.117), (13, -0.103), (14, 0.052), (15, -0.042), (16, -0.047), (17, -0.001), (18, -0.018), (19, 0.027), (20, -0.035), (21, -0.07), (22, 0.121), (23, -0.017), (24, 0.047), (25, -0.037), (26, 0.083), (27, 0.068), (28, 0.048), (29, -0.094), (30, -0.016), (31, -0.007), (32, 0.03), (33, 0.029), (34, 0.096), (35, 0.063), (36, -0.013), (37, 0.029), (38, 0.067), (39, 0.027), (40, 0.007), (41, -0.065), (42, -0.05), (43, -0.104), (44, 0.05), (45, -0.01), (46, -0.032), (47, 0.026), (48, 0.03), (49, 0.033)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97795475 <a title="43-lsi-1" href="../hunch_net-2005/hunch_net-2005-03-18-Binomial_Weighting.html">43 hunch net-2005-03-18-Binomial Weighting</a></p>
<p>Introduction: Suppose we have a set of classifiers  c  making binary predictions from an input  x  and we see examples in an online fashion.  In particular, we repeatedly see an unlabeled example  x , make a prediction  y’ (possibly based on the classifiers  c ), and then see the correct label  y .  
 
When one of these classifiers is perfect, there is a great algorithm available: predict according to the majority vote over every classifier  consistent with every previous example.  This is called the Halving algorithm.  It makes at most  log 2  |c|  mistakes since on any mistake, at least half of the classifiers are eliminated.
 
Obviously, we can’t generally hope that the there exists a classifier which never errs.  The  Binomial Weighting algorithm  is an elegant technique allowing a variant Halving algorithm  to cope with errors by creating a set of virtual classifiers for every classifier which occasionally disagree with the original classifier.  The Halving algorithm on this set of virtual clas</p><p>2 0.71536958 <a title="43-lsi-2" href="../hunch_net-2006/hunch_net-2006-02-18-Multiplication_of_Learned_Probabilities_is_Dangerous.html">157 hunch net-2006-02-18-Multiplication of Learned Probabilities is Dangerous</a></p>
<p>Introduction: This is about a design flaw in several learning algorithms such as the Naive Bayes classifier and Hidden Markov Models.  A number of people are aware of it, but it seems that not everyone is.
 
Several learning systems have the property that they estimate some conditional probabilities  P(event | other events)  either explicitly or implicitly.  Then, at prediction time, these learned probabilities are multiplied together according to some formula to produce a final prediction.  The Naive Bayes classifier for binary data is the simplest of these, so it seems like a good example.  
 
When Naive Bayes is used, a set of probabilities of the form  Pr’(feature i | label)  are estimated via counting statistics and some prior.  Predictions are made according to the label maximizing: 
  Pr’(label) * Product features i  Pr’(feature i | label)  
 
(The  Pr’  notation indicates these are estimated values.) 
 
There is nothing wrong with this method as long as (a) the prior for the sample counts is</p><p>3 0.6994648 <a title="43-lsi-3" href="../hunch_net-2006/hunch_net-2006-03-02-Why_do_people_count_for_learning%3F.html">160 hunch net-2006-03-02-Why do people count for learning?</a></p>
<p>Introduction: This post is about a confusion of mine with respect to many commonly used machine learning algorithms.   
 
A simple example where this comes up is Bayes net prediction.  A Bayes net where a directed acyclic graph over a set of nodes where each node is associated with a variable and the edges indicate dependence.  The joint probability distribution over the variables is given by a set of conditional probabilities.  For example, a very simple Bayes net might express: 
 P(A,B,C) = P(A | B,C)P(B)P(C) 
 
What I don’t understand is the mechanism commonly used to estimate  P(A | B, C) .  If we let  N(A,B,C)  be the number of instances of  A,B,C  then people sometimes form an estimate according to: 
  P’(A | B,C) = N(A,B,C) / N /[N(B)/N * N(C)/N] = N(A,B,C) N /[N(B)  N(C)]   
… in other words, people just estimate  P’(A | B,C)  according to observed relative frequencies.  This is a reasonable technique when you have a large number of samples compared to the size space  A x B x C , but it (nat</p><p>4 0.60584092 <a title="43-lsi-4" href="../hunch_net-2005/hunch_net-2005-11-28-A_question_of_quantification.html">133 hunch net-2005-11-28-A question of quantification</a></p>
<p>Introduction: This is about methods for phrasing and think about the scope of some theorems in learning theory.  The basic claim is that there are several different ways of quantifying the scope which sound different yet are essentially the same.
  
  For all sequences of examples .  This is the standard quantification in online learning analysis.  Standard theorems would say something like “for all sequences of predictions by experts, the algorithm A will perform almost as well as the best expert.” 
  For all training sets . This is the standard quantification for boosting analysis such as  adaboost  or  multiclass boosting . 
Standard theorems have the form “for all training sets the error rate inequalities … hold”.  
  For all distributions over examples .  This is the one that we have been using for reductions analysis.  Standard theorem statements have the form “For all distributions over examples, the error rate inequalities … hold”. 
  
It is not quite true that each of these is equivalent. F</p><p>5 0.58808619 <a title="43-lsi-5" href="../hunch_net-2005/hunch_net-2005-03-02-Prior%2C_%26%238220%3BPrior%26%238221%3B_and_Bias.html">34 hunch net-2005-03-02-Prior, &#8220;Prior&#8221; and Bias</a></p>
<p>Introduction: Many different ways of reasoning about learning exist, and many of these suggest that some method of saying “I prefer this predictor to that predictor” is useful and necessary.  Examples include Bayesian reasoning, prediction bounds, and online learning.   One difficulty which arises is that the manner and meaning of saying “I prefer this predictor to that predictor” differs.
  
  Prior  (Bayesian) A prior is a probability distribution over a set of distributions which expresses a belief in the probability that some distribution is the distribution generating the data. 
  “Prior”  (Prediction bounds & online learning) The “prior” is a measure over a set of classifiers which expresses the degree to which you hope the classifier will predict well. 
  Bias  (Regularization, Early termination of neural network training, etc…)  The bias is some (often implicitly specified by an algorithm) way of preferring one predictor to another. 
  
This only scratches the surface—there are yet more subt</p><p>6 0.58430594 <a title="43-lsi-6" href="../hunch_net-2006/hunch_net-2006-09-09-How_to_solve_an_NP_hard_problem_in_quadratic_time.html">206 hunch net-2006-09-09-How to solve an NP hard problem in quadratic time</a></p>
<p>7 0.5829125 <a title="43-lsi-7" href="../hunch_net-2005/hunch_net-2005-02-21-Problem%3A_Cross_Validation.html">26 hunch net-2005-02-21-Problem: Cross Validation</a></p>
<p>8 0.57170475 <a title="43-lsi-8" href="../hunch_net-2005/hunch_net-2005-02-27-Antilearning%3A_When_proximity_goes_bad.html">32 hunch net-2005-02-27-Antilearning: When proximity goes bad</a></p>
<p>9 0.56985414 <a title="43-lsi-9" href="../hunch_net-2005/hunch_net-2005-02-25-Problem%3A_Online_Learning.html">28 hunch net-2005-02-25-Problem: Online Learning</a></p>
<p>10 0.56678808 <a title="43-lsi-10" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>11 0.55444694 <a title="43-lsi-11" href="../hunch_net-2005/hunch_net-2005-02-12-ROC_vs._Accuracy_vs._AROC.html">18 hunch net-2005-02-12-ROC vs. Accuracy vs. AROC</a></p>
<p>12 0.55158085 <a title="43-lsi-12" href="../hunch_net-2005/hunch_net-2005-06-06-Exact_Online_Learning_for_Classification.html">78 hunch net-2005-06-06-Exact Online Learning for Classification</a></p>
<p>13 0.55149019 <a title="43-lsi-13" href="../hunch_net-2006/hunch_net-2006-03-12-Online_learning_or_online_preservation_of_learning%3F.html">163 hunch net-2006-03-12-Online learning or online preservation of learning?</a></p>
<p>14 0.54950392 <a title="43-lsi-14" href="../hunch_net-2010/hunch_net-2010-03-15-The_Efficient_Robust_Conditional_Probability_Estimation_Problem.html">391 hunch net-2010-03-15-The Efficient Robust Conditional Probability Estimation Problem</a></p>
<p>15 0.54925138 <a title="43-lsi-15" href="../hunch_net-2005/hunch_net-2005-11-16-The_Everything_Ensemble_Edge.html">131 hunch net-2005-11-16-The Everything Ensemble Edge</a></p>
<p>16 0.54185754 <a title="43-lsi-16" href="../hunch_net-2005/hunch_net-2005-01-31-Watchword%3A_Assumption.html">7 hunch net-2005-01-31-Watchword: Assumption</a></p>
<p>17 0.54128957 <a title="43-lsi-17" href="../hunch_net-2005/hunch_net-2005-08-22-Do_you_believe_in_induction%3F.html">104 hunch net-2005-08-22-Do you believe in induction?</a></p>
<p>18 0.54027951 <a title="43-lsi-18" href="../hunch_net-2006/hunch_net-2006-11-20-Context_and_the_calculation_misperception.html">218 hunch net-2006-11-20-Context and the calculation misperception</a></p>
<p>19 0.53271621 <a title="43-lsi-19" href="../hunch_net-2005/hunch_net-2005-02-03-Learning_Theory%2C_by_assumption.html">12 hunch net-2005-02-03-Learning Theory, by assumption</a></p>
<p>20 0.53247172 <a title="43-lsi-20" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.025), (26, 0.245), (27, 0.299), (53, 0.055), (55, 0.062), (77, 0.044), (94, 0.12), (95, 0.041)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.93467653 <a title="43-lda-1" href="../hunch_net-2005/hunch_net-2005-07-23-Interesting_papers_at_ACL.html">97 hunch net-2005-07-23-Interesting papers at ACL</a></p>
<p>Introduction: A  recent discussion  indicated that one goal of this blog might be to allow people to post comments about recent papers that they liked.  I think this could potentially be very useful, especially for those with diverse interests but only finite time to read through conference proceedings.   ACL 2005  recently completed, and here are four papers from that conference that I thought were either good or perhaps of interest to a machine learning audience.
 
David Chiang,   A Hierarchical Phrase-Based Model for Statistical Machine Translation  . (Best paper award.) This paper takes the standard phrase-based MT model that is popular in our field (basically, translate a sentence by individually translating phrases and reordering them according to a complicated statistical model) and extends it to take into account hierarchy in phrases, so that you can learn things like “X ‘s Y” -> “Y de X” in chinese, where X and Y are arbitrary phrases. This takes a step toward linguistic syntax for MT, whic</p><p>same-blog 2 0.92533797 <a title="43-lda-2" href="../hunch_net-2005/hunch_net-2005-03-18-Binomial_Weighting.html">43 hunch net-2005-03-18-Binomial Weighting</a></p>
<p>Introduction: Suppose we have a set of classifiers  c  making binary predictions from an input  x  and we see examples in an online fashion.  In particular, we repeatedly see an unlabeled example  x , make a prediction  y’ (possibly based on the classifiers  c ), and then see the correct label  y .  
 
When one of these classifiers is perfect, there is a great algorithm available: predict according to the majority vote over every classifier  consistent with every previous example.  This is called the Halving algorithm.  It makes at most  log 2  |c|  mistakes since on any mistake, at least half of the classifiers are eliminated.
 
Obviously, we can’t generally hope that the there exists a classifier which never errs.  The  Binomial Weighting algorithm  is an elegant technique allowing a variant Halving algorithm  to cope with errors by creating a set of virtual classifiers for every classifier which occasionally disagree with the original classifier.  The Halving algorithm on this set of virtual clas</p><p>3 0.9117468 <a title="43-lda-3" href="../hunch_net-2005/hunch_net-2005-02-20-At_One_Month.html">25 hunch net-2005-02-20-At One Month</a></p>
<p>Introduction: This is near the one month point, so it seems appropriate to consider meta-issues for the moment.
 
The number of posts is a bit over 20. 
The number of people speaking up in discussions is about 10. 
The number of people viewing the site is somewhat more than 100.
 
I am (naturally) dissatisfied with many things.
  
 Many of the  potential uses  haven’t been realized.  This is partly a matter of opportunity (no conferences in the last month), partly a matter of will (no open problems because it’s hard to give them up), and partly a matter of tradition.  In academia, there is a strong tradition of trying to get everything perfectly right before presentation.  This is somewhat contradictory to the nature of making many posts, and it’s definitely contradictory to the idea of doing “public research”.  If that sort of idea is to pay off, it must be significantly more succesful than previous methods. In an effort to continue experimenting, I’m going to use the next week as “open problems we</p><p>4 0.87463355 <a title="43-lda-4" href="../hunch_net-2010/hunch_net-2010-10-08-An_easy_proof_of_the_Chernoff-Hoeffding_bound.html">413 hunch net-2010-10-08-An easy proof of the Chernoff-Hoeffding bound</a></p>
<p>Introduction: Textbooks invariably seem to carry the proof that uses Markov’s inequality, moment-generating functions, and Taylor approximations. Here’s an easier way.
 
For  , let   be the KL divergence between a coin of bias   and one of bias  :  
 
 Theorem:  Suppose you do   independent tosses of a coin of bias  . The probability of seeing   heads or more, for  , is at most  . So is the probability of seeing   heads or less, for  .
 
 Remark:  By Pinsker’s inequality,  .
 
 Proof  Let’s do the   case; the other is identical.
 
Let   be the distribution over   induced by a coin of bias  , and likewise   for a coin of bias  . Let   be the set of all sequences of   tosses which contain   heads or more. We’d like to show that   is unlikely under  .
 
Pick any  , with say   heads. Then: 
 
 
Since   for every  , we have   and we’re done.</p><p>5 0.87289971 <a title="43-lda-5" href="../hunch_net-2006/hunch_net-2006-04-09-Progress_in_Machine_Translation.html">171 hunch net-2006-04-09-Progress in Machine Translation</a></p>
<p>Introduction: I just visited  ISI  where  Daniel Marcu  and others are working on machine translation.  Apparently, machine translation is rapidly improving.   A particularly dramatic year was 2002->2003 when systems switched from word-based translation to phrase-based translation.  From a (now famous) slide by Charles Wayne at  DARPA  (which funds much of the work on machine translation) here is some anecdotal evidence:
  
 
 2002 
 2003 
 
 
 insistent Wednesday may recurred her trips to Libya tomorrow for flying.

 Cairo 6-4 ( AFP ) – An official announced today in the Egyptian lines company for flying  Tuesday is a company “insistent for flying” may resumed a consideration of a day Wednesday tomorrow her trips to Libya of Security Council decision trace international the imposed ban comment.


 And said the official “the institution sent a speech to Ministry of Foreign Affairs of lifting on Libya air, a situation her recieving replying are so a trip will pull to Libya a morning Wednesday.”

 
 E</p><p>6 0.79389799 <a title="43-lda-6" href="../hunch_net-2007/hunch_net-2007-08-12-Exponentiated_Gradient.html">258 hunch net-2007-08-12-Exponentiated Gradient</a></p>
<p>7 0.79076469 <a title="43-lda-7" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>8 0.78620368 <a title="43-lda-8" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>9 0.78446263 <a title="43-lda-9" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>10 0.78393483 <a title="43-lda-10" href="../hunch_net-2007/hunch_net-2007-07-01-Watchword%3A_Online_Learning.html">252 hunch net-2007-07-01-Watchword: Online Learning</a></p>
<p>11 0.78317463 <a title="43-lda-11" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>12 0.78252536 <a title="43-lda-12" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>13 0.78138113 <a title="43-lda-13" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>14 0.78127688 <a title="43-lda-14" href="../hunch_net-2009/hunch_net-2009-01-21-Nearly_all_natural_problems_require_nonlinearity.html">337 hunch net-2009-01-21-Nearly all natural problems require nonlinearity</a></p>
<p>15 0.78049988 <a title="43-lda-15" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>16 0.78043556 <a title="43-lda-16" href="../hunch_net-2008/hunch_net-2008-09-12-How_do_we_get_weak_action_dependence_for_learning_with_partial_observations%3F.html">317 hunch net-2008-09-12-How do we get weak action dependence for learning with partial observations?</a></p>
<p>17 0.78017491 <a title="43-lda-17" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<p>18 0.77959108 <a title="43-lda-18" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>19 0.77923453 <a title="43-lda-19" href="../hunch_net-2008/hunch_net-2008-06-30-ICML_has_a_comment_system.html">305 hunch net-2008-06-30-ICML has a comment system</a></p>
<p>20 0.77898985 <a title="43-lda-20" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
