<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>29 hunch net-2005-02-25-Solution: Reinforcement Learning with Classification</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-29" href="#">hunch_net-2005-29</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>29 hunch net-2005-02-25-Solution: Reinforcement Learning with Classification</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-29-html" href="http://hunch.net/?p=31">html</a></p><p>Introduction: I realized that the tools needed to solve the  problem just posted  were just created.  I tried to sketch out the solution  here  (also in  .lyx  and  .tex ).  It is still quite sketchy (and probably only the few people who understand reductions well can follow).
 
One of the reasons why I started this weblog was to experiment with “research in the open”, and this is an opportunity to do so.  Over the next few days, I’ll be filling in details and trying to get things to make sense.  If you have additions or ideas, please propose them.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 I realized that the tools needed to solve the  problem just posted  were just created. [sent-1, score-0.993]
</p><p>2 I tried to sketch out the solution  here  (also in  . [sent-2, score-0.575]
</p><p>3 It is still quite sketchy (and probably only the few people who understand reductions well can follow). [sent-5, score-0.938]
</p><p>4 One of the reasons why I started this weblog was to experiment with “research in the open”, and this is an opportunity to do so. [sent-6, score-0.909]
</p><p>5 Over the next few days, I’ll be filling in details and trying to get things to make sense. [sent-7, score-0.896]
</p><p>6 If you have additions or ideas, please propose them. [sent-8, score-0.402]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('weblog', 0.299), ('filling', 0.277), ('propose', 0.25), ('sketchy', 0.25), ('realized', 0.24), ('sketch', 0.24), ('posted', 0.231), ('tried', 0.224), ('days', 0.194), ('follow', 0.186), ('needed', 0.183), ('tools', 0.18), ('opportunity', 0.177), ('experiment', 0.169), ('ideas', 0.154), ('please', 0.152), ('started', 0.145), ('reductions', 0.139), ('probably', 0.139), ('details', 0.131), ('trying', 0.128), ('open', 0.127), ('next', 0.127), ('still', 0.123), ('reasons', 0.119), ('ll', 0.118), ('solution', 0.111), ('solve', 0.105), ('understand', 0.1), ('get', 0.089), ('quite', 0.083), ('things', 0.078), ('make', 0.066), ('research', 0.066), ('well', 0.058), ('also', 0.055), ('problem', 0.054), ('people', 0.046), ('one', 0.034)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="29-tfidf-1" href="../hunch_net-2005/hunch_net-2005-02-25-Solution%3A_Reinforcement_Learning_with_Classification.html">29 hunch net-2005-02-25-Solution: Reinforcement Learning with Classification</a></p>
<p>Introduction: I realized that the tools needed to solve the  problem just posted  were just created.  I tried to sketch out the solution  here  (also in  .lyx  and  .tex ).  It is still quite sketchy (and probably only the few people who understand reductions well can follow).
 
One of the reasons why I started this weblog was to experiment with “research in the open”, and this is an opportunity to do so.  Over the next few days, I’ll be filling in details and trying to get things to make sense.  If you have additions or ideas, please propose them.</p><p>2 0.11159392 <a title="29-tfidf-2" href="../hunch_net-2005/hunch_net-2005-01-19-Why_I_decided_to_run_a_weblog..html">1 hunch net-2005-01-19-Why I decided to run a weblog.</a></p>
<p>Introduction: I have decided to run a weblog on machine learning and learning theory research.  Here are some reasons:
 
1) Weblogs enable new functionality:
  
 Public comment on papers.  No mechanism for this exists at conferences and most journals.  I have encountered it once for a  science  paper.   Some communities have mailing lists supporting this, but not machine learning or learning theory.  I have often read papers and found myself wishing there was some method to consider other’s questions and read the replies. 
 Conference shortlists.  One of the most common conversations at a conference is “what did you find interesting?”  There is no explicit mechanism for sharing this information at conferences, and it’s easy to imagine that it would be handy to do so. 
 Evaluation and comment on research directions.  Papers are almost exclusively about new research, rather than evaluation (and consideration) of research directions.  This last role is satisfied by funding agencies to some extent, but</p><p>3 0.10431749 <a title="29-tfidf-3" href="../hunch_net-2007/hunch_net-2007-11-05-CMU_wins_DARPA_Urban_Challenge.html">271 hunch net-2007-11-05-CMU wins DARPA Urban Challenge</a></p>
<p>Introduction: The  results have been posted , with  CMU first ,  Stanford second , and  Virginia Tech Third .
 
Considering that this was an open event (at least for people in the US), this was a very strong showing for research at universities (instead of defense contractors, for example).  Some details should become public at the  NIPS workshops .
 
 Slashdot  has a  post  with many comments.</p><p>4 0.10387145 <a title="29-tfidf-4" href="../hunch_net-2007/hunch_net-2007-10-15-NIPS_workshops_extended_to_3_days.html">266 hunch net-2007-10-15-NIPS workshops extended to 3 days</a></p>
<p>Introduction: (Unofficially, at least.)  The  Deep Learning Workshop  is being held the afternoon before the rest of the workshops in Vancouver, BC.  Separate registration is needed, and open.
 
Whatâ&euro;&trade;s happening fundamentally here is that there are too many interesting workshops to fit into 2 days.  Perhaps we can get it officially expanded to 3 days next year.</p><p>5 0.095383242 <a title="29-tfidf-5" href="../hunch_net-2006/hunch_net-2006-01-25-1_year.html">151 hunch net-2006-01-25-1 year</a></p>
<p>Introduction: At the one year (+5 days) anniversary, the natural question is: “Was it helpful for research?”
 
Answer: Yes, and so it shall continue.
 
Some evidence is provided by noticing that I am about a factor of 2 more overloaded with paper ideas than I’ve ever previously been.  It is always hard to estimate counterfactual worlds, but I expect that this is also a factor of 2 more than “What if I had not started the blog?”
 
As for “Why?”, there seem to be two primary effects.
  
 A blog is a mechanism for connecting with people who either think like you or are interested in the same problems.  This allows for concentration of thinking which is very helpful in solving problems. 
 The process of stating things you don’t understand publicly is very helpful in understanding them.  Sometimes you are simply forced to express them in a way which aids understanding.  Sometimes someone else says something which helps.  And sometimes you discover that someone else has already solved the problem. 
  
The</p><p>6 0.078858368 <a title="29-tfidf-6" href="../hunch_net-2008/hunch_net-2008-06-30-ICML_has_a_comment_system.html">305 hunch net-2008-06-30-ICML has a comment system</a></p>
<p>7 0.076225378 <a title="29-tfidf-7" href="../hunch_net-2005/hunch_net-2005-06-17-Reopening_RL-%3EClassification.html">82 hunch net-2005-06-17-Reopening RL->Classification</a></p>
<p>8 0.074138641 <a title="29-tfidf-8" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>9 0.073490918 <a title="29-tfidf-9" href="../hunch_net-2011/hunch_net-2011-02-17-What_does_Watson_mean%3F.html">424 hunch net-2011-02-17-What does Watson mean?</a></p>
<p>10 0.072247572 <a title="29-tfidf-10" href="../hunch_net-2005/hunch_net-2005-07-21-Six_Months.html">96 hunch net-2005-07-21-Six Months</a></p>
<p>11 0.069651753 <a title="29-tfidf-11" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>12 0.06887994 <a title="29-tfidf-12" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>13 0.068383135 <a title="29-tfidf-13" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<p>14 0.068094157 <a title="29-tfidf-14" href="../hunch_net-2012/hunch_net-2012-03-06-COLT-ICML_Open_Questions_and_ICML_Instructions.html">458 hunch net-2012-03-06-COLT-ICML Open Questions and ICML Instructions</a></p>
<p>15 0.067094646 <a title="29-tfidf-15" href="../hunch_net-2005/hunch_net-2005-05-29-Bad_ideas.html">76 hunch net-2005-05-29-Bad ideas</a></p>
<p>16 0.065670475 <a title="29-tfidf-16" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>17 0.064623088 <a title="29-tfidf-17" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>18 0.061593529 <a title="29-tfidf-18" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>19 0.061355442 <a title="29-tfidf-19" href="../hunch_net-2013/hunch_net-2013-01-31-Remote_large_scale_learning_class_participation.html">479 hunch net-2013-01-31-Remote large scale learning class participation</a></p>
<p>20 0.060799595 <a title="29-tfidf-20" href="../hunch_net-2005/hunch_net-2005-12-22-Yes_%2C_I_am_applying.html">142 hunch net-2005-12-22-Yes , I am applying</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.127), (1, -0.037), (2, -0.065), (3, 0.055), (4, -0.079), (5, -0.005), (6, 0.077), (7, -0.052), (8, -0.027), (9, 0.025), (10, -0.066), (11, -0.023), (12, -0.024), (13, 0.055), (14, 0.021), (15, -0.014), (16, -0.02), (17, -0.021), (18, -0.063), (19, 0.014), (20, -0.03), (21, 0.069), (22, -0.014), (23, -0.078), (24, 0.056), (25, -0.034), (26, -0.033), (27, -0.015), (28, 0.007), (29, 0.027), (30, 0.169), (31, -0.024), (32, 0.025), (33, 0.005), (34, -0.096), (35, -0.027), (36, 0.09), (37, 0.026), (38, 0.015), (39, 0.046), (40, 0.018), (41, 0.071), (42, 0.007), (43, 0.036), (44, 0.05), (45, 0.023), (46, 0.025), (47, 0.066), (48, 0.014), (49, -0.01)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97405541 <a title="29-lsi-1" href="../hunch_net-2005/hunch_net-2005-02-25-Solution%3A_Reinforcement_Learning_with_Classification.html">29 hunch net-2005-02-25-Solution: Reinforcement Learning with Classification</a></p>
<p>Introduction: I realized that the tools needed to solve the  problem just posted  were just created.  I tried to sketch out the solution  here  (also in  .lyx  and  .tex ).  It is still quite sketchy (and probably only the few people who understand reductions well can follow).
 
One of the reasons why I started this weblog was to experiment with “research in the open”, and this is an opportunity to do so.  Over the next few days, I’ll be filling in details and trying to get things to make sense.  If you have additions or ideas, please propose them.</p><p>2 0.56410468 <a title="29-lsi-2" href="../hunch_net-2010/hunch_net-2010-09-28-Machined_Learnings.html">412 hunch net-2010-09-28-Machined Learnings</a></p>
<p>Introduction: Paul Mineiro  has started  Machined Learnings  where heâ&euro;&trade;s seriously attempting to do ML research in public.  I personally need to read through in greater detail, as much of it is learning reduction related, trying to deal with the sorts of complex source problems that come up in practice.</p><p>3 0.54892659 <a title="29-lsi-3" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<p>Introduction: Martin Pool  and I recently discussed the similarities and differences between academia and open source programming.   
 
Similarities:
  
  Cost profile   Research and programming share approximately the same cost profile: A large upfront effort is required to produce something useful, and then “anyone” can use it.  (The “anyone” is not quite right for either group because only sufficiently technical people could use it.) 
  Wealth profile  A “wealthy” academic or open source programmer is someone who has contributed a lot to other people in research or programs.  Much of academia is a “gift culture”: whoever gives the most is most respected. 
  Problems   Both academia and open source programming suffer from similar problems.
 
 Whether or not (and which) open source program is used are perhaps too-often personality driven rather than driven by capability or usefulness.  Similar phenomena can happen in academia with respect to directions of research. 
 Funding is often a problem for</p><p>4 0.54745013 <a title="29-lsi-4" href="../hunch_net-2007/hunch_net-2007-02-10-Best_Practices_for_Collaboration.html">231 hunch net-2007-02-10-Best Practices for Collaboration</a></p>
<p>Introduction: Many people, especially students, haven’t had an opportunity to collaborate with other researchers.  Collaboration, especially with remote people can be tricky.  Here are some observations of what has worked for me on collaborations involving a few people.
  
  Travel and Discuss  Almost all collaborations start with in-person discussion.  This implies that travel is often necessary.  We can hope that in the future we’ll have better systems for starting collaborations remotely (such as blogs), but we aren’t quite there yet. 
  Enable your collaborator .  A collaboration can fall apart because one collaborator disables another.  This sounds stupid (and it is), but it’s far easier than you might think.
 
  Avoid Duplication .  Discovering that you and a collaborator have been editing the same thing and now need to waste time reconciling changes is annoying.  The best way to avoid this to be explicit about who has write permission to what.  Most of the time, a write lock is held for the e</p><p>5 0.54704762 <a title="29-lsi-5" href="../hunch_net-2005/hunch_net-2005-06-17-Reopening_RL-%3EClassification.html">82 hunch net-2005-06-17-Reopening RL->Classification</a></p>
<p>Introduction: In research, it’s often the case that solving a problem helps you realize that it wasn’t the right problem to solve.  This is the case for the “ reduce RL to classification ” problem with the solution hinted at  here  and turned into a paper  here .  
 
The essential difficulty is that the method of stating and analyzing reductions ends up being nonalgorithmic (unlike previous reductions) unless you work with learning from teleoperated robots as  Greg Grudic  does.  The difficulty here is due to the reduction being dependent on the optimal policy (which a human teleoperator might simulate, but which is otherwise unavailable).
 
So, this  problem  is “open” again with the caveat that this time we want a more algorithmic solution.  
 
Whether or not this is feasible at all is still unclear and evidence in either direction would greatly interest me.  A positive answer might have many practical implications in the long run.</p><p>6 0.53436023 <a title="29-lsi-6" href="../hunch_net-2007/hunch_net-2007-11-05-CMU_wins_DARPA_Urban_Challenge.html">271 hunch net-2007-11-05-CMU wins DARPA Urban Challenge</a></p>
<p>7 0.52327001 <a title="29-lsi-7" href="../hunch_net-2008/hunch_net-2008-03-15-COLT_Open_Problems.html">292 hunch net-2008-03-15-COLT Open Problems</a></p>
<p>8 0.50517416 <a title="29-lsi-8" href="../hunch_net-2008/hunch_net-2008-04-12-Blog_compromised.html">294 hunch net-2008-04-12-Blog compromised</a></p>
<p>9 0.50052953 <a title="29-lsi-9" href="../hunch_net-2009/hunch_net-2009-06-01-Multitask_Poisoning.html">358 hunch net-2009-06-01-Multitask Poisoning</a></p>
<p>10 0.48530564 <a title="29-lsi-10" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>11 0.47645509 <a title="29-lsi-11" href="../hunch_net-2011/hunch_net-2011-02-17-What_does_Watson_mean%3F.html">424 hunch net-2011-02-17-What does Watson mean?</a></p>
<p>12 0.47455826 <a title="29-lsi-12" href="../hunch_net-2005/hunch_net-2005-05-29-Bad_ideas.html">76 hunch net-2005-05-29-Bad ideas</a></p>
<p>13 0.46660697 <a title="29-lsi-13" href="../hunch_net-2013/hunch_net-2013-12-01-NIPS_tutorials_and_Vowpal_Wabbit_7.4.html">492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</a></p>
<p>14 0.44934976 <a title="29-lsi-14" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>15 0.44109333 <a title="29-lsi-15" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>16 0.44024533 <a title="29-lsi-16" href="../hunch_net-2011/hunch_net-2011-11-26-Giving_Thanks.html">449 hunch net-2011-11-26-Giving Thanks</a></p>
<p>17 0.43853441 <a title="29-lsi-17" href="../hunch_net-2011/hunch_net-2011-04-06-COLT_open_questions.html">429 hunch net-2011-04-06-COLT open questions</a></p>
<p>18 0.43352544 <a title="29-lsi-18" href="../hunch_net-2005/hunch_net-2005-07-21-Six_Months.html">96 hunch net-2005-07-21-Six Months</a></p>
<p>19 0.4303557 <a title="29-lsi-19" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>20 0.42915753 <a title="29-lsi-20" href="../hunch_net-2005/hunch_net-2005-05-17-A_Short_Guide_to_PhD_Graduate_Study.html">73 hunch net-2005-05-17-A Short Guide to PhD Graduate Study</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(4, 0.431), (27, 0.148), (53, 0.107), (55, 0.17)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.91846609 <a title="29-lda-1" href="../hunch_net-2005/hunch_net-2005-06-28-The_cross_validation_problem%3A_cash_reward.html">86 hunch net-2005-06-28-The cross validation problem: cash reward</a></p>
<p>Introduction: I just  presented  the  cross validation  problem at  COLT .  
 
The problem now has a cash prize (up to $500) associated with itâ&euro;&rdquo;see the  presentation  for details.
 
The  write-up for colt .</p><p>2 0.87139809 <a title="29-lda-2" href="../hunch_net-2005/hunch_net-2005-07-01-The_Role_of_Impromptu_Talks.html">88 hunch net-2005-07-01-The Role of Impromptu Talks</a></p>
<p>Introduction: COLT  had an impromptu session which seemed as interesting or more interesting than any other single technical session (despite being only an hour long).  There are several roles that an impromptu session can play including:
  
 Announcing new work since the paper deadline. Letting this happen now rather than later helps aid the process of research. 
 Discussing a paper that was rejected.  Reviewers err sometimes and an impromptu session provides a means to remedy that. 
 Entertainment.  We all like to have a bit of fun. 
  
For design, the following seem important:
  
 Impromptu speakers should not have much time.  At COLT, it was 8 minutes, but I have seen even 5 work well. 
 The entire impromptu session should not last too long because the format is dense and promotes restlessness.  A half hour or hour can work well. 
  
Impromptu talks are a mechanism to let a little bit of chaos into the schedule.  They will be chaotic in content, presentation, and usefulness.  The fundamental adv</p><p>3 0.87046468 <a title="29-lda-3" href="../hunch_net-2005/hunch_net-2005-09-06-A_link.html">108 hunch net-2005-09-06-A link</a></p>
<p>Introduction: I read through some of the essays of  Michael Nielsen  today, and recommend them.   Principles of Effective Research  and  Extreme Thinking  are both relevant to several discussions here.</p><p>same-blog 4 0.8452208 <a title="29-lda-4" href="../hunch_net-2005/hunch_net-2005-02-25-Solution%3A_Reinforcement_Learning_with_Classification.html">29 hunch net-2005-02-25-Solution: Reinforcement Learning with Classification</a></p>
<p>Introduction: I realized that the tools needed to solve the  problem just posted  were just created.  I tried to sketch out the solution  here  (also in  .lyx  and  .tex ).  It is still quite sketchy (and probably only the few people who understand reductions well can follow).
 
One of the reasons why I started this weblog was to experiment with “research in the open”, and this is an opportunity to do so.  Over the next few days, I’ll be filling in details and trying to get things to make sense.  If you have additions or ideas, please propose them.</p><p>5 0.76519024 <a title="29-lda-5" href="../hunch_net-2005/hunch_net-2005-02-02-Kolmogorov_Complexity_and_Googling.html">10 hunch net-2005-02-02-Kolmogorov Complexity and Googling</a></p>
<p>Introduction: Machine learning makes the   New Scientist  . From the article: 
  

COMPUTERS can learn the meaning of words simply by plugging into Google. The finding could bring forward the day that true artificial intelligence is developedâ&euro;Ś. 
But Paul Vitanyi and Rudi Cilibrasi of the National Institute for Mathematics and Computer Science in Amsterdam, the Netherlands, realised that a Google search can be used to measure how closely two words relate to each other. For instance, imagine a computer needs to understand what a hat is.

  
You can read the paper at  KC Google .
 
Hat tip:   Kolmogorov Mailing List 
 
Any thoughts on the paper?</p><p>6 0.65985221 <a title="29-lda-6" href="../hunch_net-2005/hunch_net-2005-11-07-Prediction_Competitions.html">129 hunch net-2005-11-07-Prediction Competitions</a></p>
<p>7 0.63638622 <a title="29-lda-7" href="../hunch_net-2005/hunch_net-2005-05-28-Running_A_Machine_Learning_Summer_School.html">75 hunch net-2005-05-28-Running A Machine Learning Summer School</a></p>
<p>8 0.48704135 <a title="29-lda-8" href="../hunch_net-2012/hunch_net-2012-03-06-COLT-ICML_Open_Questions_and_ICML_Instructions.html">458 hunch net-2012-03-06-COLT-ICML Open Questions and ICML Instructions</a></p>
<p>9 0.47079867 <a title="29-lda-9" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>10 0.46921521 <a title="29-lda-10" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>11 0.46683204 <a title="29-lda-11" href="../hunch_net-2010/hunch_net-2010-04-26-Compassionate_Reviewing.html">395 hunch net-2010-04-26-Compassionate Reviewing</a></p>
<p>12 0.46596503 <a title="29-lda-12" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>13 0.46422535 <a title="29-lda-13" href="../hunch_net-2007/hunch_net-2007-11-02-The_Machine_Learning_Award_goes_to_%26%238230%3B.html">270 hunch net-2007-11-02-The Machine Learning Award goes to &#8230;</a></p>
<p>14 0.45820421 <a title="29-lda-14" href="../hunch_net-2005/hunch_net-2005-07-07-The_Limits_of_Learning_Theory.html">90 hunch net-2005-07-07-The Limits of Learning Theory</a></p>
<p>15 0.45320994 <a title="29-lda-15" href="../hunch_net-2008/hunch_net-2008-12-12-Summer_Conferences.html">331 hunch net-2008-12-12-Summer Conferences</a></p>
<p>16 0.45147279 <a title="29-lda-16" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>17 0.44940615 <a title="29-lda-17" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>18 0.44579995 <a title="29-lda-18" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>19 0.44514191 <a title="29-lda-19" href="../hunch_net-2005/hunch_net-2005-03-13-Avoiding_Bad_Reviewing.html">40 hunch net-2005-03-13-Avoiding Bad Reviewing</a></p>
<p>20 0.44258928 <a title="29-lda-20" href="../hunch_net-2006/hunch_net-2006-01-25-1_year.html">151 hunch net-2006-01-25-1 year</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
