<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>70 hunch net-2005-05-12-Math on the Web</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-70" href="#">hunch_net-2005-70</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>70 hunch net-2005-05-12-Math on the Web</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-70-html" href="http://hunch.net/?p=75">html</a></p><p>Introduction: Andrej Bauer has setup aMathematics and ComputationBlog. As a first step he
has tried to address the persistent and annoying problem of math on the web.
As a basic tool for precisely stating and transfering understanding of
technical subjects, mathematics is very necessary. Despite this necessity,
every mechanism for expressing mathematics on the web seems unnaturally
clumsy. Here are some of the methods and their drawbacks:MathMLThis was
supposed to be the answer, but it has two severe drawbacks: "Internet
Explorer" doesn't read it and the language is an example of push-XML-to-the-
limit which no one would ever consider writing in. (In contrast, html is easy
to write in.) It's also very annoying that math fonts must be installed
independent of the browser, even for mozilla based browsers.Create inline
images. This has several big drawbacks: font size is fixed for all viewers,
you can't cut & paste inside the images, and you can't hyperlink from (say)
symbol to definition.Math Worldis</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('latex', 0.295), ('math', 0.278), ('destination', 0.221), ('drawbacks', 0.209), ('mathematics', 0.199), ('andrej', 0.197), ('hyperlink', 0.197), ('html', 0.182), ('browser', 0.172), ('translate', 0.164), ('annoying', 0.164), ('formula', 0.157), ('language', 0.143), ('variable', 0.127), ('writing', 0.108), ('bauer', 0.098), ('editable', 0.098), ('explorer', 0.098), ('fractions', 0.098), ('installed', 0.098)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="70-tfidf-1" href="../hunch_net-2005/hunch_net-2005-05-12-Math_on_the_Web.html">70 hunch net-2005-05-12-Math on the Web</a></p>
<p>Introduction: Andrej Bauer has setup aMathematics and ComputationBlog. As a first step he
has tried to address the persistent and annoying problem of math on the web.
As a basic tool for precisely stating and transfering understanding of
technical subjects, mathematics is very necessary. Despite this necessity,
every mechanism for expressing mathematics on the web seems unnaturally
clumsy. Here are some of the methods and their drawbacks:MathMLThis was
supposed to be the answer, but it has two severe drawbacks: "Internet
Explorer" doesn't read it and the language is an example of push-XML-to-the-
limit which no one would ever consider writing in. (In contrast, html is easy
to write in.) It's also very annoying that math fonts must be installed
independent of the browser, even for mozilla based browsers.Create inline
images. This has several big drawbacks: font size is fixed for all viewers,
you can't cut & paste inside the images, and you can't hyperlink from (say)
symbol to definition.Math Worldis</p><p>2 0.14730133 <a title="70-tfidf-2" href="../hunch_net-2008/hunch_net-2008-06-30-ICML_has_a_comment_system.html">305 hunch net-2008-06-30-ICML has a comment system</a></p>
<p>Introduction: Mark Reidhas stepped up and created acomment system for ICML paperswhichGreger
Lindenhas tightly integrated.My understanding is that Mark spent quite a bit
of time on the details, and there are some cool features like working latex
math mode. This is an excellent chance for the ICML community to experiment
with making ICML year-round, so I hope it works out. Please do consider
experimenting with it.</p><p>3 0.12612262 <a title="70-tfidf-3" href="../hunch_net-2006/hunch_net-2006-08-10-Precision_is_not_accuracy.html">202 hunch net-2006-08-10-Precision is not accuracy</a></p>
<p>Introduction: In my experience, there are two different groups of people who believe the
same thing: the mathematics encountered in typical machine learning conference
papers is often of questionable value.The two groups who agree on this are
applied machine learning people who have given up on math, and mature
theoreticians who understand the limits of theory.Partly, this is just a
statement about where we are with respect to machine learning. In particular,
we have no mechanism capable of generating a prescription for how to solve all
learning problems. In the absence of such certainty, people try to come up
with formalisms that partially describe and motivate how and why they do
things. This is natural and healthy--we might hope that it will eventually
lead to just such a mechanism.But, part of this is simply an emphasis on
complexity over clarity. A very natural and simple theoretical statement is
often obscured by complexifications. Common sources of complexification
include:GeneralizationBy tr</p><p>4 0.1114585 <a title="70-tfidf-4" href="../hunch_net-2008/hunch_net-2008-05-25-Inappropriate_Mathematics_for_Machine_Learning.html">302 hunch net-2008-05-25-Inappropriate Mathematics for Machine Learning</a></p>
<p>Introduction: Reviewers and students are sometimes greatly concerned by the distinction
between:Anopen setand aclosed set.ASupremumand aMaximum.An event which happens
with probability 1 and an event that always happens.I don't appreciate this
distinction in machine learning & learning theory. All machine learning takes
place (by definition) on a machine where every parameter has finite precision.
Consequently, every set is closed, a maximal element always exists, and
probability 1 events always happen.The fundamental issue here is that
substantial parts of mathematics don't appear well-matched to computation in
the physical world, because the mathematics has concerns which are unphysical.
This mismatched mathematics makes irrelevant distinctions. We can ask "what
mathematics is appropriate to computation?"Andrejhas convinced me that a
pretty good answer to this question isconstructive mathematics.So, here's a
basic challenge: Can anyone name a situation where any of the distinctions
above (or simila</p><p>5 0.090586141 <a title="70-tfidf-5" href="../hunch_net-2005/hunch_net-2005-04-10-Is_the_Goal_Understanding_or_Prediction%3F.html">55 hunch net-2005-04-10-Is the Goal Understanding or Prediction?</a></p>
<p>Introduction: Steve Smaleand I have a debate about goals of learning theory.Steve likes
theorems with a dependence on unobservable quantities. For example, ifDis a
distribution over a spaceX x [0,1], you can state a theorem about the error
rate dependent on the variance,E(x,y)~D(y-Ey'~D|x[y'])2.I dislike this,
because I want to use the theorems to produce code solving learning problems.
Since I don't know (and can't measure) the variance, a theorem depending on
the variance does not help me--I would not know what variance to plug into the
learning algorithm.Recast more broadly, this is a debate between "declarative"
and "operative" mathematics. A strong example of "declarative" mathematics
is"a new kind of science". Roughly speaking, the goal of this kind of approach
seems to be finding a way to explain the observations we make. Examples
include "some things are unpredictable", "a phase transition exists",
etcâ&euro;Ś"Operative" mathematics helps you make predictions about the world. A
strong example of op</p><p>6 0.089448191 <a title="70-tfidf-6" href="../hunch_net-2006/hunch_net-2006-09-28-Programming_Languages_for_Machine_Learning_Implementations.html">210 hunch net-2006-09-28-Programming Languages for Machine Learning Implementations</a></p>
<p>7 0.088730328 <a title="70-tfidf-7" href="../hunch_net-2005/hunch_net-2005-10-13-Site_tweak.html">122 hunch net-2005-10-13-Site tweak</a></p>
<p>8 0.083207048 <a title="70-tfidf-8" href="../hunch_net-2006/hunch_net-2006-01-25-1_year.html">151 hunch net-2006-01-25-1 year</a></p>
<p>9 0.08209879 <a title="70-tfidf-9" href="../hunch_net-2006/hunch_net-2006-04-06-Bounds_greater_than_1.html">170 hunch net-2006-04-06-Bounds greater than 1</a></p>
<p>10 0.081583351 <a title="70-tfidf-10" href="../hunch_net-2005/hunch_net-2005-02-15-ESPgame_and_image_labeling.html">20 hunch net-2005-02-15-ESPgame and image labeling</a></p>
<p>11 0.078782357 <a title="70-tfidf-11" href="../hunch_net-2007/hunch_net-2007-01-02-Retrospective.html">225 hunch net-2007-01-02-Retrospective</a></p>
<p>12 0.070214354 <a title="70-tfidf-12" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>13 0.069046639 <a title="70-tfidf-13" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>14 0.067807734 <a title="70-tfidf-14" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>15 0.064875901 <a title="70-tfidf-15" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>16 0.064771801 <a title="70-tfidf-16" href="../hunch_net-2005/hunch_net-2005-07-07-The_Limits_of_Learning_Theory.html">90 hunch net-2005-07-07-The Limits of Learning Theory</a></p>
<p>17 0.062453054 <a title="70-tfidf-17" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>18 0.06093853 <a title="70-tfidf-18" href="../hunch_net-2006/hunch_net-2006-09-18-What_is_missing_for_online_collaborative_research%3F.html">208 hunch net-2006-09-18-What is missing for online collaborative research?</a></p>
<p>19 0.059843682 <a title="70-tfidf-19" href="../hunch_net-2005/hunch_net-2005-06-22-Languages__of_Learning.html">84 hunch net-2005-06-22-Languages  of Learning</a></p>
<p>20 0.057337314 <a title="70-tfidf-20" href="../hunch_net-2005/hunch_net-2005-02-25-Why_Papers%3F.html">30 hunch net-2005-02-25-Why Papers?</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.137), (1, 0.001), (2, 0.035), (3, -0.069), (4, 0.025), (5, -0.011), (6, 0.004), (7, -0.065), (8, 0.046), (9, 0.083), (10, -0.005), (11, 0.006), (12, 0.0), (13, -0.015), (14, -0.047), (15, -0.019), (16, 0.04), (17, 0.025), (18, -0.04), (19, 0.034), (20, 0.024), (21, 0.025), (22, -0.069), (23, -0.024), (24, -0.0), (25, 0.017), (26, -0.126), (27, -0.073), (28, -0.044), (29, -0.013), (30, 0.065), (31, 0.036), (32, -0.046), (33, -0.044), (34, -0.012), (35, 0.008), (36, -0.096), (37, -0.012), (38, 0.062), (39, 0.03), (40, 0.04), (41, -0.06), (42, -0.001), (43, -0.054), (44, -0.036), (45, 0.027), (46, 0.04), (47, 0.008), (48, 0.053), (49, -0.01)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96041024 <a title="70-lsi-1" href="../hunch_net-2005/hunch_net-2005-05-12-Math_on_the_Web.html">70 hunch net-2005-05-12-Math on the Web</a></p>
<p>Introduction: Andrej Bauer has setup aMathematics and ComputationBlog. As a first step he
has tried to address the persistent and annoying problem of math on the web.
As a basic tool for precisely stating and transfering understanding of
technical subjects, mathematics is very necessary. Despite this necessity,
every mechanism for expressing mathematics on the web seems unnaturally
clumsy. Here are some of the methods and their drawbacks:MathMLThis was
supposed to be the answer, but it has two severe drawbacks: "Internet
Explorer" doesn't read it and the language is an example of push-XML-to-the-
limit which no one would ever consider writing in. (In contrast, html is easy
to write in.) It's also very annoying that math fonts must be installed
independent of the browser, even for mozilla based browsers.Create inline
images. This has several big drawbacks: font size is fixed for all viewers,
you can't cut & paste inside the images, and you can't hyperlink from (say)
symbol to definition.Math Worldis</p><p>2 0.61342061 <a title="70-lsi-2" href="../hunch_net-2006/hunch_net-2006-09-28-Programming_Languages_for_Machine_Learning_Implementations.html">210 hunch net-2006-09-28-Programming Languages for Machine Learning Implementations</a></p>
<p>Introduction: Machine learning algorithms have a much better chance of being widely adopted
if they are implemented in some easy-to-use code. There are several important
concerns associated with machine learning which stress programming languages
on the ease-of-use vs. speed frontier.SpeedThe rate at which data sources are
growing seems to be outstripping the rate at which computational power is
growing, so it is important that we be able to eak out every bit of
computational power. Garbage collected languages (java,ocaml,perlandpython)
often have several issues here.Garbage collection often implies that floating
point numbers are "boxed": every float is represented by a pointer to a float.
Boxing can cause an order of magnitude slowdown because an extra nonlocalized
memory reference is made, and accesses to main memory can are many CPU cycles
long.Garbage collection often implies that considerably more memory is used
than is necessary. This has a variable effect. In some circumstances it
results in</p><p>3 0.60507923 <a title="70-lsi-3" href="../hunch_net-2006/hunch_net-2006-03-09-Use_of_Notation.html">162 hunch net-2006-03-09-Use of Notation</a></p>
<p>Introduction: For most people, a mathematical notation is like a language: you learn it and
stick with it. For people doing mathematical research, however, this is not
enough: they must design new notations for new problems. The design of good
notation is both hard and worthwhile since a bad initial notation can retard a
line of research greatly.Before we had mathematical notation, equations were
all written out in language. Since words have multiple meanings and variable
precedences, long equations written out in language can be extraordinarily
difficult and sometimes fundamentally ambiguous. A good representative example
of this is the legalese in the tax code. Since we want greater precision and
clarity, we adopt mathematical notation.One fundamental thing to understand
about mathematical notation, is that humans as logic verifiers, are barely
capable. This is the fundamental reason why one notation can be much better
than another. This observation is easier to miss than you might expect
because,</p><p>4 0.59254253 <a title="70-lsi-4" href="../hunch_net-2005/hunch_net-2005-10-13-Site_tweak.html">122 hunch net-2005-10-13-Site tweak</a></p>
<p>Introduction: Several people have had difficulty with comments which seem to have an allowed
language significantly poorer than posts. The set of allowed html tags has
been increased and themarkdown filterhas been put in place to try to make
commenting easier. I'll put some examples into the comments of this post.</p><p>5 0.54108125 <a title="70-lsi-5" href="../hunch_net-2005/hunch_net-2005-06-22-Languages__of_Learning.html">84 hunch net-2005-06-22-Languages  of Learning</a></p>
<p>Introduction: A language is a set of primitives which can be combined to succesfully create
complex objects. Languages arise in all sorts of situations: mechanical
construction, martial arts, communication, etcâ&euro;Ś Languages appear to be the key
to succesfully creating complex objects--it is difficult to come up with any
convincing example of a complex object which is not built using some language.
Since languages are so crucial to success, it is interesting to organize
various machine learning research programs by language.The most common
language in machine learning are languages for representing the solution to
machine learning. This includes:Bayes Nets and Graphical ModelsA language for
representing probability distributions. The key concept supporting modularity
is conditional independence.Michael Kearnshas been working on extending this
to game theory.Kernelized Linear ClassifiersA language for representing linear
separators, possibly in a large space. The key form of modularity here is
kerneliza</p><p>6 0.52280951 <a title="70-lsi-6" href="../hunch_net-2006/hunch_net-2006-08-10-Precision_is_not_accuracy.html">202 hunch net-2006-08-10-Precision is not accuracy</a></p>
<p>7 0.51643592 <a title="70-lsi-7" href="../hunch_net-2005/hunch_net-2005-06-13-Wikis_for_Summer_Schools_and_Workshops.html">81 hunch net-2005-06-13-Wikis for Summer Schools and Workshops</a></p>
<p>8 0.50698048 <a title="70-lsi-8" href="../hunch_net-2007/hunch_net-2007-06-21-Presentation_Preparation.html">249 hunch net-2007-06-21-Presentation Preparation</a></p>
<p>9 0.48262322 <a title="70-lsi-9" href="../hunch_net-2007/hunch_net-2007-07-28-Asking_questions.html">257 hunch net-2007-07-28-Asking questions</a></p>
<p>10 0.45724845 <a title="70-lsi-10" href="../hunch_net-2007/hunch_net-2007-01-02-Retrospective.html">225 hunch net-2007-01-02-Retrospective</a></p>
<p>11 0.45308971 <a title="70-lsi-11" href="../hunch_net-2007/hunch_net-2007-09-16-Optimizing_Machine_Learning_Programs.html">262 hunch net-2007-09-16-Optimizing Machine Learning Programs</a></p>
<p>12 0.44695979 <a title="70-lsi-12" href="../hunch_net-2006/hunch_net-2006-02-02-Introspectionism_as_a_Disease.html">153 hunch net-2006-02-02-Introspectionism as a Disease</a></p>
<p>13 0.43526158 <a title="70-lsi-13" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>14 0.43338472 <a title="70-lsi-14" href="../hunch_net-2005/hunch_net-2005-07-10-Thinking_the_Unthought.html">91 hunch net-2005-07-10-Thinking the Unthought</a></p>
<p>15 0.43337926 <a title="70-lsi-15" href="../hunch_net-2009/hunch_net-2009-06-01-Multitask_Poisoning.html">358 hunch net-2009-06-01-Multitask Poisoning</a></p>
<p>16 0.43290713 <a title="70-lsi-16" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>17 0.43246123 <a title="70-lsi-17" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>18 0.42872232 <a title="70-lsi-18" href="../hunch_net-2007/hunch_net-2007-02-10-Best_Practices_for_Collaboration.html">231 hunch net-2007-02-10-Best Practices for Collaboration</a></p>
<p>19 0.42631531 <a title="70-lsi-19" href="../hunch_net-2008/hunch_net-2008-11-16-Observations_on_Linearity_for_Reductions_to_Regression.html">327 hunch net-2008-11-16-Observations on Linearity for Reductions to Regression</a></p>
<p>20 0.42592478 <a title="70-lsi-20" href="../hunch_net-2008/hunch_net-2008-06-09-The_Minimum_Sample_Complexity_of_Importance_Weighting.html">303 hunch net-2008-06-09-The Minimum Sample Complexity of Importance Weighting</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(6, 0.051), (35, 0.055), (39, 0.034), (42, 0.182), (45, 0.012), (69, 0.027), (74, 0.085), (92, 0.445)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.90845174 <a title="70-lda-1" href="../hunch_net-2005/hunch_net-2005-12-14-More_NIPS_Papers_II.html">140 hunch net-2005-12-14-More NIPS Papers II</a></p>
<p>Introduction: I thought this was a very good NIPS with many excellent papers. The following
are a few NIPS papers which I liked and I hope to study more carefully when I
get the chance. The list is not exhaustive and in no particular
order…Preconditioner Approximations for Probabilistic Graphical
Models.Pradeeep Ravikumar and John Lafferty.I thought the use of
preconditioner methods from solving linear systems in the context of
approximate inference was novel and interesting. The results look good and I'd
like to understand the limitations.Rodeo: Sparse nonparametric regression in
high dimensions.John Lafferty and Larry Wasserman.A very interesting approach
to feature selection in nonparametric regression from a frequentist framework.
The use of lengthscale variables in each dimension reminds me a lot of
'Automatic Relevance Determination' in Gaussian process regression -- it would
be interesting to compare Rodeo to ARD in GPs.Interpolating between types and
tokens by estimating power law generators</p><p>same-blog 2 0.89120293 <a title="70-lda-2" href="../hunch_net-2005/hunch_net-2005-05-12-Math_on_the_Web.html">70 hunch net-2005-05-12-Math on the Web</a></p>
<p>Introduction: Andrej Bauer has setup aMathematics and ComputationBlog. As a first step he
has tried to address the persistent and annoying problem of math on the web.
As a basic tool for precisely stating and transfering understanding of
technical subjects, mathematics is very necessary. Despite this necessity,
every mechanism for expressing mathematics on the web seems unnaturally
clumsy. Here are some of the methods and their drawbacks:MathMLThis was
supposed to be the answer, but it has two severe drawbacks: "Internet
Explorer" doesn't read it and the language is an example of push-XML-to-the-
limit which no one would ever consider writing in. (In contrast, html is easy
to write in.) It's also very annoying that math fonts must be installed
independent of the browser, even for mozilla based browsers.Create inline
images. This has several big drawbacks: font size is fixed for all viewers,
you can't cut & paste inside the images, and you can't hyperlink from (say)
symbol to definition.Math Worldis</p><p>3 0.74582207 <a title="70-lda-3" href="../hunch_net-2007/hunch_net-2007-06-19-How_is_Compressed_Sensing_going_to_change_Machine_Learning_%3F.html">248 hunch net-2007-06-19-How is Compressed Sensing going to change Machine Learning ?</a></p>
<p>Introduction: Compressed Sensing(CS) is a new framework developed byEmmanuel Candes,Terry
TaoandDavid Donoho. To summarize, if you acquire a signal in some basis that
is incoherent with the basis in which you know the signal to be sparse in, it
is very likely you will be able to reconstruct the signal from these
incoherent projections.Terry Tao, the recentFields medalist, does a very nice
job at explaining the frameworkhere. He goes further in the theory description
in thispostwhere he mentions the central issue of the Uniform Uncertainty
Principle. It so happens that random projections are on average incoherent,
within the UUP meaning, with most known basis (sines, polynomials, splines,
wavelets, curvelets …) and are therefore an ideal basis for Compressed
Sensing. [ For more in-depth information on the subject, the Rice group has
done a very good job at providing a central library of papers relevant to the
growing subject:http://www.dsp.ece.rice.edu/cs/]The Machine Learning community
has looked at</p><p>4 0.59467065 <a title="70-lda-4" href="../hunch_net-2005/hunch_net-2005-03-22-Active_learning.html">45 hunch net-2005-03-22-Active learning</a></p>
<p>Introduction: Often, unlabeled data is easy to come by but labels are expensive. For
instance, if you're building a speech recognizer, it's easy enough to get raw
speech samples -- just walk around with a microphone -- but labeling even one
of these samples is a tedious process in which a human must examine the speech
signal and carefully segment it into phonemes. In the field of active
learning, the goal is as usual to construct an accurate classifier, but the
labels of the data points are initially hidden and there is a charge for each
label you want revealed. The hope is that by intelligent adaptive querying,
you can get away with significantly fewer labels than you would need in a
regular supervised learning framework.Here's an example. Suppose the data lie
on the real line, and the classifiers are simple thresholding functions, H =
{hw}:hw(x) = 1 if x > w, and 0 otherwise.VC theory tells us that if the
underlying distribution P can be classified perfectly by some hypothesis in H
(called thereal</p><p>5 0.55919415 <a title="70-lda-5" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>Introduction: "Search" is the other branch of AI research which has been succesful. Concrete
examples includeDeep Bluewhich beat the world chess champion andChinookthe
champion checkers program. A set of core search techniques exist including A*,
alpha-beta pruning, and others that can be applied to any of many different
search problems.Given this, it may be surprising to learn that there has been
relatively little succesful work on combining prediction and search. Given
also that humans typically solve search problems using a number of predictive
heuristics to narrow in on a solution, we might be surprised again. However,
the big successful search-based systems have typically not used "smart" search
algorithms. Insteady they have optimized for very fast search. This is not for
lack of tryingâ&euro;Ś many people have tried to synthesize search and prediction to
various degrees of success. For example,Knightcapachieves good-but-not-stellar
chess playing performance, andTD-gammonhas achieved near-optimal Bac</p><p>6 0.48311856 <a title="70-lda-6" href="../hunch_net-2007/hunch_net-2007-09-18-It%26%238217%3Bs_MDL_Jim%2C_but_not_as_we_know_it%26%238230%3B%28on_Bayes%2C_MDL_and_consistency%29.html">263 hunch net-2007-09-18-It&#8217;s MDL Jim, but not as we know it&#8230;(on Bayes, MDL and consistency)</a></p>
<p>7 0.47239941 <a title="70-lda-7" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>8 0.43585896 <a title="70-lda-8" href="../hunch_net-2012/hunch_net-2012-06-05-ICML_acceptance_statistics.html">466 hunch net-2012-06-05-ICML acceptance statistics</a></p>
<p>9 0.42188072 <a title="70-lda-9" href="../hunch_net-2009/hunch_net-2009-06-24-Interesting_papers_at_UAICMOLT_2009.html">361 hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</a></p>
<p>10 0.41870257 <a title="70-lda-10" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>11 0.41564703 <a title="70-lda-11" href="../hunch_net-2012/hunch_net-2012-02-24-ICML%2B50%25.html">456 hunch net-2012-02-24-ICML+50%</a></p>
<p>12 0.41531667 <a title="70-lda-12" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>13 0.41376835 <a title="70-lda-13" href="../hunch_net-2005/hunch_net-2005-02-15-ESPgame_and_image_labeling.html">20 hunch net-2005-02-15-ESPgame and image labeling</a></p>
<p>14 0.41372472 <a title="70-lda-14" href="../hunch_net-2006/hunch_net-2006-09-12-Incentive_Compatible_Reviewing.html">207 hunch net-2006-09-12-Incentive Compatible Reviewing</a></p>
<p>15 0.41249615 <a title="70-lda-15" href="../hunch_net-2005/hunch_net-2005-02-17-Learning_Research_Programs.html">21 hunch net-2005-02-17-Learning Research Programs</a></p>
<p>16 0.41242978 <a title="70-lda-16" href="../hunch_net-2006/hunch_net-2006-01-25-1_year.html">151 hunch net-2006-01-25-1 year</a></p>
<p>17 0.41195327 <a title="70-lda-17" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>18 0.41179079 <a title="70-lda-18" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>19 0.41163737 <a title="70-lda-19" href="../hunch_net-2007/hunch_net-2007-02-02-Thoughts_regarding_%26%238220%3BIs_machine_learning_different_from_statistics%3F%26%238221%3B.html">230 hunch net-2007-02-02-Thoughts regarding &#8220;Is machine learning different from statistics?&#8221;</a></p>
<p>20 0.41159859 <a title="70-lda-20" href="../hunch_net-2005/hunch_net-2005-02-25-Problem%3A_Online_Learning.html">28 hunch net-2005-02-25-Problem: Online Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
