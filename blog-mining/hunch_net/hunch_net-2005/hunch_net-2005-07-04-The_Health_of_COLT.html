<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>89 hunch net-2005-07-04-The Health of COLT</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-89" href="#">hunch_net-2005-89</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>89 hunch net-2005-07-04-The Health of COLT</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-89-html" href="http://hunch.net/?p=95">html</a></p><p>Introduction: The health ofCOLT(Conference on Learning Theory or Computational Learning
Theory depending on who you ask) has been questioned over the last few years.
Low points for the conference occurred whenEuroCOLTmerged with COLT in 2001,
and the attendance at the 2002 Sydney COLT fell to a new low. This occurred in
the general context of machine learning conferences rising in both number and
size over the last decade.Any discussion ofwhyCOLT has had difficulties is
inherently controversial as is any story about well-intentioned people making
the wrong decisions. Nevertheless, this may be worth discussing in the hope of
avoiding problems in the future and general understanding. In any such
discussion there is a strong tendency to identify with a conference/community
in a patriotic manner that is detrimental to thinking. Keep in mind that
conferences exist to further research.My understanding (I wasn't around) is
that COLT started as a subcommunity of the computer science theory community.
This i</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The health ofCOLT(Conference on Learning Theory or Computational Learning Theory depending on who you ask) has been questioned over the last few years. [sent-1, score-0.35]
</p><p>2 Low points for the conference occurred whenEuroCOLTmerged with COLT in 2001, and the attendance at the 2002 Sydney COLT fell to a new low. [sent-2, score-0.555]
</p><p>3 This occurred in the general context of machine learning conferences rising in both number and size over the last decade. [sent-3, score-0.54]
</p><p>4 Any discussion ofwhyCOLT has had difficulties is inherently controversial as is any story about well-intentioned people making the wrong decisions. [sent-4, score-0.33]
</p><p>5 Nevertheless, this may be worth discussing in the hope of avoiding problems in the future and general understanding. [sent-5, score-0.142]
</p><p>6 In any such discussion there is a strong tendency to identify with a conference/community in a patriotic manner that is detrimental to thinking. [sent-6, score-0.332]
</p><p>7 Keep in mind that conferences exist to further research. [sent-7, score-0.116]
</p><p>8 My understanding (I wasn't around) is that COLT started as a subcommunity of the computer science theory community. [sent-8, score-0.354]
</p><p>9 This implies several things:There was a basic tension facing authors: Do you submit to COLT or toFOCSorSTOCwhich are the "big" theory conferences? [sent-9, score-0.435]
</p><p>10 The research programs in COLT were motivated by theoretical concerns (rather than, say, practical experience). [sent-10, score-0.395]
</p><p>11 This includes motivations like understanding the combinatorics of some models of learning and the relationship with crypto. [sent-11, score-0.685]
</p><p>12 This worked well in the beginning when new research programs were being defined and new learning models were under investigation. [sent-12, score-0.661]
</p><p>13 Perhaps the community shifted focus from thinking about new learning models to simply trying to find solutions in older models, and this went stale. [sent-14, score-0.663]
</p><p>14 Many of the learning models under investigation at COLT strike empirically motivated people as implausibly useful. [sent-16, score-0.692]
</p><p>15 Perhaps the conference/community was not inviting enough to new forms of learning theory. [sent-17, score-0.255]
</p><p>16 Many pieces of learning theory have not appeared at COLT over the last 20 years. [sent-18, score-0.584]
</p><p>17 The good news is that this year's COLT appeared healthy. [sent-20, score-0.224]
</p><p>18 The topics covered by the program were diverse and often interesting. [sent-21, score-0.066]
</p><p>19 Perhaps an even better measure is that there were many younger people in attendance. [sent-23, score-0.1]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('colt', 0.474), ('motivations', 0.25), ('appeared', 0.224), ('models', 0.215), ('theory', 0.183), ('occurred', 0.16), ('motivated', 0.15), ('health', 0.145), ('went', 0.138), ('attendance', 0.129), ('concerns', 0.129), ('programs', 0.116), ('conferences', 0.116), ('last', 0.105), ('fell', 0.1), ('detrimental', 0.1), ('facing', 0.1), ('questioned', 0.1), ('subcommunity', 0.1), ('sydney', 0.1), ('younger', 0.1), ('new', 0.096), ('low', 0.096), ('implausibly', 0.093), ('venue', 0.093), ('wrong', 0.089), ('inviting', 0.087), ('rising', 0.087), ('strike', 0.087), ('tension', 0.087), ('discussion', 0.084), ('questionable', 0.083), ('controversial', 0.08), ('accepting', 0.08), ('relationship', 0.077), ('identify', 0.077), ('story', 0.077), ('investigation', 0.075), ('shifted', 0.075), ('discussing', 0.073), ('learning', 0.072), ('understanding', 0.071), ('tendency', 0.071), ('conference', 0.07), ('avoiding', 0.069), ('older', 0.067), ('addressed', 0.067), ('beginning', 0.066), ('covered', 0.066), ('submit', 0.065)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999982 <a title="89-tfidf-1" href="../hunch_net-2005/hunch_net-2005-07-04-The_Health_of_COLT.html">89 hunch net-2005-07-04-The Health of COLT</a></p>
<p>Introduction: The health ofCOLT(Conference on Learning Theory or Computational Learning
Theory depending on who you ask) has been questioned over the last few years.
Low points for the conference occurred whenEuroCOLTmerged with COLT in 2001,
and the attendance at the 2002 Sydney COLT fell to a new low. This occurred in
the general context of machine learning conferences rising in both number and
size over the last decade.Any discussion ofwhyCOLT has had difficulties is
inherently controversial as is any story about well-intentioned people making
the wrong decisions. Nevertheless, this may be worth discussing in the hope of
avoiding problems in the future and general understanding. In any such
discussion there is a strong tendency to identify with a conference/community
in a patriotic manner that is detrimental to thinking. Keep in mind that
conferences exist to further research.My understanding (I wasn't around) is
that COLT started as a subcommunity of the computer science theory community.
This i</p><p>2 0.30070972 <a title="89-tfidf-2" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<p>Introduction: ByShieandNatiFollowing John's advertisement for submitting to ICML, we thought
it appropriate to highlight the advantages of COLT, and the reasons it is
often the best place for theory papers. We would like to emphasize that we
both respect ICML, and are active in ICML, both as authors and as area chairs,
and certainly are not arguing that ICML is a bad place for your papers. For
many papers, ICML is the best venue. But for many theory papers, COLT is a
better and more appropriate place.Why should you submit to COLT?By-and-large,
theory papers go to COLT. This is the tradition of the field and most theory
papers are sent to COLT. This is the place to present your ground-breaking
theorems and new models that will shape the theory of machine learning. COLT
is more focused then ICML with a single track session. Unlike ICML, the norm
in COLT is for people to sit through most sessions, and hear most of the talks
presented. There is also often a lively discussion following paper
presentation</p><p>3 0.19521053 <a title="89-tfidf-3" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>Introduction: Here's a quick reference for summer ML-related conferences sorted by due
date:ConferenceDue dateLocationReviewingKDDFeb 10August 12-16, Beijing,
ChinaSingle BlindCOLTFeb 14June 25-June 27, Edinburgh, ScotlandSingle Blind?
(historically)ICMLFeb 24June 26-July 1, Edinburgh, ScotlandDouble Blind,
author response, zeroSPOFUAIMarch 30August 15-17, Catalina Islands,
CaliforniaDouble Blind, author responseGeographically, this is greatly
dispersed and the UAI/KDD conflict is unfortunate.Machine Learning conferences
are triannual now, betweenNIPS,AIStat, andICML. This has not always been the
case: the academic default is annual summer conferences, then NIPS started
with a December conference, and now AIStat has grown into an April
conference.However, the first claim is not quite correct. NIPS and AIStat have
few competing venues while ICML implicitly competes with many other
conferences accepting machine learning related papers. SinceJoelleand I are
taking a turn as program chairs this year, I</p><p>4 0.19128267 <a title="89-tfidf-4" href="../hunch_net-2008/hunch_net-2008-11-09-A_Healthy__COLT.html">324 hunch net-2008-11-09-A Healthy  COLT</a></p>
<p>Introduction: Awhile ago, we discussed the health ofCOLT.COLT 2008substantially addressed my
concerns. The papers were diverse and several were interesting. Attendance was
up, which is particularly notable in Europe. In my opinion, the colocation
with UAI and ICML was the best colocation since 1998.And, perhaps best of all,
registration ended up being free for all students due to various grants from
theAcademy of Finland,Google,IBM, andYahoo.A basic question is: what went
right? There seem to be several answers.Cost-wise, COLT had sufficient grants
to alleviate the high cost of the Euro and location at a university
substantially reduces the cost compared to a hotel.Organization-wise, the
Finns were great with hordes of volunteers helping set everything up. Having
too many volunteers is a good failure mode.Organization-wise, it was clear
that all 3 program chairs were cooperating in designing the program
.Facilities-wise, proximity in time and space made the colocation much more
real than many others</p><p>5 0.17832188 <a title="89-tfidf-5" href="../hunch_net-2005/hunch_net-2005-06-28-The_cross_validation_problem%3A_cash_reward.html">86 hunch net-2005-06-28-The cross validation problem: cash reward</a></p>
<p>Introduction: I justpresentedthecross validationproblem atCOLT.The problem now has a cash
prize (up to $500) associated with it--see thepresentationfor details
.Thewrite-up for colt.</p><p>6 0.17263666 <a title="89-tfidf-6" href="../hunch_net-2010/hunch_net-2010-04-24-COLT_Treasurer_is_now_Phil_Long.html">394 hunch net-2010-04-24-COLT Treasurer is now Phil Long</a></p>
<p>7 0.14631762 <a title="89-tfidf-7" href="../hunch_net-2009/hunch_net-2009-10-10-ALT_2009.html">374 hunch net-2009-10-10-ALT 2009</a></p>
<p>8 0.1384429 <a title="89-tfidf-8" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>9 0.13519356 <a title="89-tfidf-9" href="../hunch_net-2005/hunch_net-2005-02-17-Learning_Research_Programs.html">21 hunch net-2005-02-17-Learning Research Programs</a></p>
<p>10 0.12960668 <a title="89-tfidf-10" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>11 0.12307376 <a title="89-tfidf-11" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>12 0.12141765 <a title="89-tfidf-12" href="../hunch_net-2009/hunch_net-2009-08-26-Another_10-year_paper_in_Machine_Learning.html">368 hunch net-2009-08-26-Another 10-year paper in Machine Learning</a></p>
<p>13 0.11867026 <a title="89-tfidf-13" href="../hunch_net-2007/hunch_net-2007-04-30-COLT_2007.html">242 hunch net-2007-04-30-COLT 2007</a></p>
<p>14 0.11320063 <a title="89-tfidf-14" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>15 0.11079375 <a title="89-tfidf-15" href="../hunch_net-2005/hunch_net-2005-03-21-Research_Styles_in_Machine_Learning.html">44 hunch net-2005-03-21-Research Styles in Machine Learning</a></p>
<p>16 0.10935081 <a title="89-tfidf-16" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>17 0.1060027 <a title="89-tfidf-17" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>18 0.10146303 <a title="89-tfidf-18" href="../hunch_net-2008/hunch_net-2008-12-12-Summer_Conferences.html">331 hunch net-2008-12-12-Summer Conferences</a></p>
<p>19 0.100911 <a title="89-tfidf-19" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>20 0.099389546 <a title="89-tfidf-20" href="../hunch_net-2012/hunch_net-2012-03-06-COLT-ICML_Open_Questions_and_ICML_Instructions.html">458 hunch net-2012-03-06-COLT-ICML Open Questions and ICML Instructions</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.237), (1, 0.132), (2, 0.012), (3, 0.027), (4, 0.021), (5, -0.005), (6, -0.052), (7, -0.017), (8, 0.005), (9, -0.045), (10, -0.099), (11, -0.13), (12, 0.09), (13, 0.068), (14, 0.013), (15, 0.233), (16, 0.142), (17, 0.109), (18, 0.088), (19, 0.125), (20, -0.09), (21, -0.124), (22, 0.084), (23, -0.009), (24, 0.101), (25, 0.103), (26, 0.051), (27, -0.06), (28, -0.039), (29, 0.02), (30, -0.119), (31, -0.002), (32, 0.132), (33, -0.081), (34, 0.109), (35, -0.013), (36, 0.037), (37, -0.012), (38, -0.041), (39, 0.058), (40, 0.026), (41, 0.118), (42, 0.001), (43, -0.018), (44, -0.009), (45, -0.085), (46, 0.046), (47, -0.009), (48, -0.105), (49, 0.007)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94458741 <a title="89-lsi-1" href="../hunch_net-2005/hunch_net-2005-07-04-The_Health_of_COLT.html">89 hunch net-2005-07-04-The Health of COLT</a></p>
<p>Introduction: The health ofCOLT(Conference on Learning Theory or Computational Learning
Theory depending on who you ask) has been questioned over the last few years.
Low points for the conference occurred whenEuroCOLTmerged with COLT in 2001,
and the attendance at the 2002 Sydney COLT fell to a new low. This occurred in
the general context of machine learning conferences rising in both number and
size over the last decade.Any discussion ofwhyCOLT has had difficulties is
inherently controversial as is any story about well-intentioned people making
the wrong decisions. Nevertheless, this may be worth discussing in the hope of
avoiding problems in the future and general understanding. In any such
discussion there is a strong tendency to identify with a conference/community
in a patriotic manner that is detrimental to thinking. Keep in mind that
conferences exist to further research.My understanding (I wasn't around) is
that COLT started as a subcommunity of the computer science theory community.
This i</p><p>2 0.78050148 <a title="89-lsi-2" href="../hunch_net-2010/hunch_net-2010-04-24-COLT_Treasurer_is_now_Phil_Long.html">394 hunch net-2010-04-24-COLT Treasurer is now Phil Long</a></p>
<p>Introduction: For about 5 years, I've been the treasurer of the Association for
Computational Learning, otherwise known as COLT, taking over fromJohn
Casebefore me. A transfer of duties toPhil Longis now about complete. This
probably matters to almost no one, but I wanted to describe things a bit for
those interested.The immediate impetus for this decision was unhappiness over
reviewing decisions atCOLT 2009, one as an author and several as a member of
the program committee. I seem to have disagreements fairly often about what is
important work, partly because I'm focused on learning theory with practical
implications, partly because I define learning theory more broadly than is
typical amongst COLT members, and partly because COLT suffers a bit from
insider-clique issues. The degree to which these issues come up varies
substantially each year so last year is not predictive of this one. And, it's
important to understand that COLT remains healthy with these issues not nearly
so bad asthey were. Never</p><p>3 0.76626521 <a title="89-lsi-3" href="../hunch_net-2008/hunch_net-2008-11-09-A_Healthy__COLT.html">324 hunch net-2008-11-09-A Healthy  COLT</a></p>
<p>Introduction: Awhile ago, we discussed the health ofCOLT.COLT 2008substantially addressed my
concerns. The papers were diverse and several were interesting. Attendance was
up, which is particularly notable in Europe. In my opinion, the colocation
with UAI and ICML was the best colocation since 1998.And, perhaps best of all,
registration ended up being free for all students due to various grants from
theAcademy of Finland,Google,IBM, andYahoo.A basic question is: what went
right? There seem to be several answers.Cost-wise, COLT had sufficient grants
to alleviate the high cost of the Euro and location at a university
substantially reduces the cost compared to a hotel.Organization-wise, the
Finns were great with hordes of volunteers helping set everything up. Having
too many volunteers is a good failure mode.Organization-wise, it was clear
that all 3 program chairs were cooperating in designing the program
.Facilities-wise, proximity in time and space made the colocation much more
real than many others</p><p>4 0.74143338 <a title="89-lsi-4" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<p>Introduction: ByShieandNatiFollowing John's advertisement for submitting to ICML, we thought
it appropriate to highlight the advantages of COLT, and the reasons it is
often the best place for theory papers. We would like to emphasize that we
both respect ICML, and are active in ICML, both as authors and as area chairs,
and certainly are not arguing that ICML is a bad place for your papers. For
many papers, ICML is the best venue. But for many theory papers, COLT is a
better and more appropriate place.Why should you submit to COLT?By-and-large,
theory papers go to COLT. This is the tradition of the field and most theory
papers are sent to COLT. This is the place to present your ground-breaking
theorems and new models that will shape the theory of machine learning. COLT
is more focused then ICML with a single track session. Unlike ICML, the norm
in COLT is for people to sit through most sessions, and hear most of the talks
presented. There is also often a lively discussion following paper
presentation</p><p>5 0.70107633 <a title="89-lsi-5" href="../hunch_net-2005/hunch_net-2005-06-28-The_cross_validation_problem%3A_cash_reward.html">86 hunch net-2005-06-28-The cross validation problem: cash reward</a></p>
<p>Introduction: I justpresentedthecross validationproblem atCOLT.The problem now has a cash
prize (up to $500) associated with it--see thepresentationfor details
.Thewrite-up for colt.</p><p>6 0.6132316 <a title="89-lsi-6" href="../hunch_net-2007/hunch_net-2007-04-30-COLT_2007.html">242 hunch net-2007-04-30-COLT 2007</a></p>
<p>7 0.56896669 <a title="89-lsi-7" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>8 0.54063487 <a title="89-lsi-8" href="../hunch_net-2005/hunch_net-2005-07-01-The_Role_of_Impromptu_Talks.html">88 hunch net-2005-07-01-The Role of Impromptu Talks</a></p>
<p>9 0.49202749 <a title="89-lsi-9" href="../hunch_net-2011/hunch_net-2011-10-10-ML_Symposium_and_ICML_details.html">447 hunch net-2011-10-10-ML Symposium and ICML details</a></p>
<p>10 0.48799777 <a title="89-lsi-10" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>11 0.48356128 <a title="89-lsi-11" href="../hunch_net-2012/hunch_net-2012-03-06-COLT-ICML_Open_Questions_and_ICML_Instructions.html">458 hunch net-2012-03-06-COLT-ICML Open Questions and ICML Instructions</a></p>
<p>12 0.47657889 <a title="89-lsi-12" href="../hunch_net-2005/hunch_net-2005-02-17-Learning_Research_Programs.html">21 hunch net-2005-02-17-Learning Research Programs</a></p>
<p>13 0.46544003 <a title="89-lsi-13" href="../hunch_net-2005/hunch_net-2005-03-21-Research_Styles_in_Machine_Learning.html">44 hunch net-2005-03-21-Research Styles in Machine Learning</a></p>
<p>14 0.46444306 <a title="89-lsi-14" href="../hunch_net-2006/hunch_net-2006-01-08-Debugging_Your_Brain.html">147 hunch net-2006-01-08-Debugging Your Brain</a></p>
<p>15 0.46137592 <a title="89-lsi-15" href="../hunch_net-2007/hunch_net-2007-11-02-The_Machine_Learning_Award_goes_to_%26%238230%3B.html">270 hunch net-2007-11-02-The Machine Learning Award goes to &#8230;</a></p>
<p>16 0.45996889 <a title="89-lsi-16" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>17 0.44999894 <a title="89-lsi-17" href="../hunch_net-2009/hunch_net-2009-10-10-ALT_2009.html">374 hunch net-2009-10-10-ALT 2009</a></p>
<p>18 0.44546852 <a title="89-lsi-18" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>19 0.43849146 <a title="89-lsi-19" href="../hunch_net-2012/hunch_net-2012-05-12-ICML_accepted_papers_and_early_registration.html">465 hunch net-2012-05-12-ICML accepted papers and early registration</a></p>
<p>20 0.43135038 <a title="89-lsi-20" href="../hunch_net-2009/hunch_net-2009-08-26-Another_10-year_paper_in_Machine_Learning.html">368 hunch net-2009-08-26-Another 10-year paper in Machine Learning</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(35, 0.02), (42, 0.171), (45, 0.02), (69, 0.016), (74, 0.641), (82, 0.036)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.98825186 <a title="89-lda-1" href="../hunch_net-2007/hunch_net-2007-12-17-New_Machine_Learning_mailing_list.html">278 hunch net-2007-12-17-New Machine Learning mailing list</a></p>
<p>Introduction: IMLS(which is the nonprofit running ICML) has setup a new mailing list
forMachine Learning News. The list address is ML-news@googlegroups.com, and
signup requires a google account (which you can create). Only members can send
messages.</p><p>2 0.97905833 <a title="89-lda-2" href="../hunch_net-2008/hunch_net-2008-10-14-Who_is_Responsible_for_a_Bad_Review%3F.html">320 hunch net-2008-10-14-Who is Responsible for a Bad Review?</a></p>
<p>Introduction: Although I'm greatly interested in machine learning, I think it must be
admitted that there is a large amount of low quality logic being used in
reviews. The problem is bad enough that sometimes I wonder if theByzantine
generalslimit has been exceeded. For example, I've seen recent reviews where
the given reasons for rejecting are:[NIPS] Theorem A is uninteresting because
Theorem B is uninteresting.[UAI] When you learn by memorization, the problem
addressed is trivial.[NIPS] The proof is in the appendix.[NIPS] This has been
done before. (â&euro;Ś but not giving any relevant citations)Just for the record I
want to point out what's wrong with these reviews. A future world in which
such reasons never come up again would be great, but I'm sure these errors
will be committed many times more in the future.This is nonsense. A theorem
should be evaluated based on it's merits, rather than the merits of another
theorem.Learning by memorization requires an exponentially larger sample
complexity than man</p><p>same-blog 3 0.9774462 <a title="89-lda-3" href="../hunch_net-2005/hunch_net-2005-07-04-The_Health_of_COLT.html">89 hunch net-2005-07-04-The Health of COLT</a></p>
<p>Introduction: The health ofCOLT(Conference on Learning Theory or Computational Learning
Theory depending on who you ask) has been questioned over the last few years.
Low points for the conference occurred whenEuroCOLTmerged with COLT in 2001,
and the attendance at the 2002 Sydney COLT fell to a new low. This occurred in
the general context of machine learning conferences rising in both number and
size over the last decade.Any discussion ofwhyCOLT has had difficulties is
inherently controversial as is any story about well-intentioned people making
the wrong decisions. Nevertheless, this may be worth discussing in the hope of
avoiding problems in the future and general understanding. In any such
discussion there is a strong tendency to identify with a conference/community
in a patriotic manner that is detrimental to thinking. Keep in mind that
conferences exist to further research.My understanding (I wasn't around) is
that COLT started as a subcommunity of the computer science theory community.
This i</p><p>4 0.96979916 <a title="89-lda-4" href="../hunch_net-2006/hunch_net-2006-07-11-New_Models.html">194 hunch net-2006-07-11-New Models</a></p>
<p>Introduction: How should we, as researchers in machine learning, organize ourselves?The most
immediate measurable objective of computer science research is publishing a
paper. The most difficult aspect of publishing a paper is having reviewers
accept and recommend it for publication. The simplest mechanism for doing this
is to show theoretical progress on some standard, well-known easily understood
problem.In doing this, we often fall into a local minima of the research
process. The basic problem in machine learning is that it is very unclear that
the mathematical model is the right one for the (or some) real problem. A good
mathematical model in machine learning should have one fundamental trait: it
should aid the design of effective learning algorithms. To date, our ability
to solve interesting learning problems (speech recognition, machine
translation, object recognition, etcâ&euro;Ś) remains limited (although improving),
so the "rightness" of our models is in doubt.If our mathematical models are
bad, t</p><p>5 0.96947306 <a title="89-lda-5" href="../hunch_net-2005/hunch_net-2005-05-28-Running_A_Machine_Learning_Summer_School.html">75 hunch net-2005-05-28-Running A Machine Learning Summer School</a></p>
<p>Introduction: We just finished theChicago 2005 Machine Learning Summer School. The school
was 2 weeks long with about 130 (or 140 counting the speakers) participants.
For perspective, this is perhaps the largest graduate level machine learning
class I am aware of anywhere and anytime (previousMLSSs have been close).
Overall, it seemed to go well, although the students are the real authority on
this. For those who missed it, DVDs will be available from our Slovenian
friends. EmailMrs Spela Sitarof the Jozsef Stefan Institute for details.The
following are some notes for future planning and those interested.Good
DecisionsAcquiring the larger-than-necessary "Assembly Hall" atInternational
House. Our attendance came in well above our expectations, so this was a
critical early decision that made a huge difference.The invited speakers were
key. They made a huge difference in the quality of the content.Delegating
early and often was important. One key difficulty here is gauging how much a
volunteer can (or</p><p>6 0.95560026 <a title="89-lda-6" href="../hunch_net-2006/hunch_net-2006-09-18-What_is_missing_for_online_collaborative_research%3F.html">208 hunch net-2006-09-18-What is missing for online collaborative research?</a></p>
<p>7 0.93425477 <a title="89-lda-7" href="../hunch_net-2007/hunch_net-2007-01-26-Parallel_Machine_Learning_Problems.html">229 hunch net-2007-01-26-Parallel Machine Learning Problems</a></p>
<p>8 0.92254519 <a title="89-lda-8" href="../hunch_net-2006/hunch_net-2006-04-30-John_Langford_%26%238211%3B%3E_Yahoo_Research%2C_NY.html">175 hunch net-2006-04-30-John Langford &#8211;> Yahoo Research, NY</a></p>
<p>9 0.8698169 <a title="89-lda-9" href="../hunch_net-2012/hunch_net-2012-05-02-ICML%3A_Behind_the_Scenes.html">463 hunch net-2012-05-02-ICML: Behind the Scenes</a></p>
<p>10 0.86633635 <a title="89-lda-10" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>11 0.85247165 <a title="89-lda-11" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>12 0.84975272 <a title="89-lda-12" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>13 0.8430993 <a title="89-lda-13" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<p>14 0.84099936 <a title="89-lda-14" href="../hunch_net-2008/hunch_net-2008-12-12-Summer_Conferences.html">331 hunch net-2008-12-12-Summer Conferences</a></p>
<p>15 0.83485812 <a title="89-lda-15" href="../hunch_net-2005/hunch_net-2005-03-09-Bad_Reviewing.html">38 hunch net-2005-03-09-Bad Reviewing</a></p>
<p>16 0.83025301 <a title="89-lda-16" href="../hunch_net-2009/hunch_net-2009-05-24-2009_ICML_discussion_site.html">356 hunch net-2009-05-24-2009 ICML discussion site</a></p>
<p>17 0.82662493 <a title="89-lda-17" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>18 0.81747115 <a title="89-lda-18" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>19 0.81141794 <a title="89-lda-19" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>20 0.81102103 <a title="89-lda-20" href="../hunch_net-2012/hunch_net-2012-08-24-Patterns_for_research_in_machine_learning.html">471 hunch net-2012-08-24-Patterns for research in machine learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
