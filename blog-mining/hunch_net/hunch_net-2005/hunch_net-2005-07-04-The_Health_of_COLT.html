<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>89 hunch net-2005-07-04-The Health of COLT</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-89" href="#">hunch_net-2005-89</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>89 hunch net-2005-07-04-The Health of COLT</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-89-html" href="http://hunch.net/?p=95">html</a></p><p>Introduction: The health of  COLT  (Conference on Learning Theory or Computational Learning Theory depending on who you ask) has been questioned over the last few years.  Low points for the conference occurred when  EuroCOLT  merged with COLT in 2001, and the attendance at the 2002 Sydney COLT fell to a new low.  This occurred in the general context of machine learning conferences rising in both number and size over the last decade.
 
Any discussion of  why  COLT has had difficulties is inherently controversial as is any story about well-intentioned people making the wrong decisions.   Nevertheless, this may be worth discussing in the hope of avoiding problems in the future and general understanding.  In any such discussion there is a strong tendency to identify with a conference/community in a patriotic manner that is detrimental to thinking.  Keep in mind that conferences exist to further research.
 
My understanding (I wasn’t around) is that COLT started as a subcommunity of the computer science</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The health of  COLT  (Conference on Learning Theory or Computational Learning Theory depending on who you ask) has been questioned over the last few years. [sent-1, score-0.339]
</p><p>2 Low points for the conference occurred when  EuroCOLT  merged with COLT in 2001, and the attendance at the 2002 Sydney COLT fell to a new low. [sent-2, score-0.673]
</p><p>3 This occurred in the general context of machine learning conferences rising in both number and size over the last decade. [sent-3, score-0.46]
</p><p>4 Any discussion of  why  COLT has had difficulties is inherently controversial as is any story about well-intentioned people making the wrong decisions. [sent-4, score-0.318]
</p><p>5 Nevertheless, this may be worth discussing in the hope of avoiding problems in the future and general understanding. [sent-5, score-0.135]
</p><p>6 In any such discussion there is a strong tendency to identify with a conference/community in a patriotic manner that is detrimental to thinking. [sent-6, score-0.325]
</p><p>7 Keep in mind that conferences exist to further research. [sent-7, score-0.114]
</p><p>8 My understanding (I wasn’t around) is that COLT started as a subcommunity of the computer science theory community. [sent-8, score-0.378]
</p><p>9 This implies several things:     There was a basic tension facing authors: Do you submit to COLT or to  FOCS  or  STOC  which are the “big” theory conferences? [sent-9, score-0.395]
</p><p>10 The research programs in COLT were motivated by theoretical concerns (rather than, say, practical experience). [sent-10, score-0.391]
</p><p>11 This includes motivations like understanding the combinatorics of some models of learning and the relationship with crypto. [sent-11, score-0.598]
</p><p>12 This worked well in the beginning when new research programs were being defined and new learning models were under investigation. [sent-12, score-0.559]
</p><p>13 Perhaps the community shifted focus from thinking about new learning models to simply trying to find solutions in older models, and this went stale. [sent-14, score-0.566]
</p><p>14 Perhaps some critical motivations were left out. [sent-15, score-0.248]
</p><p>15 Many of the learning models under investigation at COLT strike empirically motivated people as implausibly useful. [sent-16, score-0.602]
</p><p>16 Perhaps the conference/community was not inviting enough to new forms of learning theory. [sent-17, score-0.175]
</p><p>17 Many pieces of learning theory have not appeared  at COLT over the last 20 years. [sent-18, score-0.532]
</p><p>18 The good news is that this year’s COLT appeared healthy. [sent-20, score-0.223]
</p><p>19 Perhaps an even better measure is that there were many younger people in attendance. [sent-23, score-0.099]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('colt', 0.449), ('motivations', 0.248), ('appeared', 0.223), ('theory', 0.209), ('models', 0.203), ('occurred', 0.159), ('motivated', 0.148), ('health', 0.14), ('went', 0.134), ('attendance', 0.128), ('concerns', 0.128), ('perhaps', 0.123), ('programs', 0.115), ('conferences', 0.114), ('conference', 0.1), ('last', 0.1), ('fell', 0.099), ('detrimental', 0.099), ('facing', 0.099), ('merged', 0.099), ('questioned', 0.099), ('subcommunity', 0.099), ('sydney', 0.099), ('younger', 0.099), ('low', 0.093), ('implausibly', 0.092), ('venue', 0.092), ('new', 0.088), ('wrong', 0.088), ('inviting', 0.087), ('rising', 0.087), ('strike', 0.087), ('tension', 0.087), ('questionable', 0.083), ('stoc', 0.083), ('controversial', 0.079), ('accepting', 0.079), ('discussion', 0.079), ('relationship', 0.077), ('identify', 0.077), ('shifted', 0.074), ('investigation', 0.072), ('story', 0.072), ('tendency', 0.07), ('understanding', 0.07), ('avoiding', 0.068), ('discussing', 0.067), ('older', 0.067), ('addressed', 0.067), ('beginning', 0.065)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999976 <a title="89-tfidf-1" href="../hunch_net-2005/hunch_net-2005-07-04-The_Health_of_COLT.html">89 hunch net-2005-07-04-The Health of COLT</a></p>
<p>Introduction: The health of  COLT  (Conference on Learning Theory or Computational Learning Theory depending on who you ask) has been questioned over the last few years.  Low points for the conference occurred when  EuroCOLT  merged with COLT in 2001, and the attendance at the 2002 Sydney COLT fell to a new low.  This occurred in the general context of machine learning conferences rising in both number and size over the last decade.
 
Any discussion of  why  COLT has had difficulties is inherently controversial as is any story about well-intentioned people making the wrong decisions.   Nevertheless, this may be worth discussing in the hope of avoiding problems in the future and general understanding.  In any such discussion there is a strong tendency to identify with a conference/community in a patriotic manner that is detrimental to thinking.  Keep in mind that conferences exist to further research.
 
My understanding (I wasn’t around) is that COLT started as a subcommunity of the computer science</p><p>2 0.29299176 <a title="89-tfidf-2" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<p>Introduction: By  Shie  and  Nati 
 
Following John’s advertisement for submitting to ICML, we thought it appropriate to highlight the advantages of COLT, and the reasons it is often the best place for theory papers.  We would like to emphasize that we both respect ICML, and are active in ICML, both as authors and as area chairs, and certainly are not arguing that ICML is a bad place for your papers.  For many papers, ICML is the best venue.  But for many theory papers, COLT is a better and more appropriate place.
 
Why should you submit to COLT?
 
By-and-large, theory papers go to COLT. This is the tradition of the field and most theory papers are sent to COLT. This is the place to present your ground-breaking theorems and new models that will shape the theory of machine learning. COLT is more focused then ICML with a single track session.  Unlike ICML, the norm in COLT is for people to sit through most sessions, and hear most of the talks presented.  There is also often a lively discussion followi</p><p>3 0.2175453 <a title="89-tfidf-3" href="../hunch_net-2008/hunch_net-2008-11-09-A_Healthy__COLT.html">324 hunch net-2008-11-09-A Healthy  COLT</a></p>
<p>Introduction: A  while ago , we discussed the health of  COLT .   COLT 2008  substantially addressed my concerns.  The papers were diverse and several were interesting.  Attendance was up, which is particularly notable in Europe.  In my opinion, the colocation with UAI and ICML was the best colocation since 1998.
 
And, perhaps best of all, registration ended up being free for all students due to various grants from the  Academy of Finland ,  Google ,  IBM , and  Yahoo .
 
A basic question is: what went right?  There seem to be several answers.
  
 Cost-wise, COLT had sufficient grants to alleviate the high cost of the Euro and location at a university substantially reduces the cost compared to a hotel. 
 Organization-wise, the Finns were great with hordes of volunteers helping set everything up.  Having too many volunteers is a good failure mode. 
 Organization-wise, it was clear that all 3 program chairs were cooperating in designing the program. 
 Facilities-wise, proximity in time and space made</p><p>4 0.2028162 <a title="89-tfidf-4" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>Introduction: Here’s a quick reference for summer ML-related conferences sorted by due date:
  
 
 Conference 
 Due date 
 Location 
 Reviewing 
 
 
  KDD  
 Feb 10 
 August 12-16, Beijing, China 
 Single Blind 
 
 
  COLT  
 Feb 14 
 June 25-June 27, Edinburgh, Scotland 
 Single Blind? (historically) 
 
 
  ICML  
 Feb 24 
 June 26-July 1, Edinburgh, Scotland 
 Double Blind, author response, zero  SPOF  
 
 
  UAI  
 March 30 
 August 15-17, Catalina Islands, California 
 Double Blind, author response 
 
  
Geographically, this is greatly dispersed and the UAI/KDD conflict is unfortunate.
 
Machine Learning conferences are triannual now, between  NIPS ,  AIStat , and  ICML .  This has not always been the case: the academic default is annual summer conferences, then NIPS started with a December conference, and now AIStat has grown into an April conference.  
 
However, the first claim is not quite correct.  NIPS and AIStat have few competing venues while ICML implicitly competes with many other conf</p><p>5 0.18893574 <a title="89-tfidf-5" href="../hunch_net-2005/hunch_net-2005-06-28-The_cross_validation_problem%3A_cash_reward.html">86 hunch net-2005-06-28-The cross validation problem: cash reward</a></p>
<p>Introduction: I just  presented  the  cross validation  problem at  COLT .  
 
The problem now has a cash prize (up to $500) associated with itâ&euro;&rdquo;see the  presentation  for details.
 
The  write-up for colt .</p><p>6 0.16000263 <a title="89-tfidf-6" href="../hunch_net-2010/hunch_net-2010-04-24-COLT_Treasurer_is_now_Phil_Long.html">394 hunch net-2010-04-24-COLT Treasurer is now Phil Long</a></p>
<p>7 0.14317256 <a title="89-tfidf-7" href="../hunch_net-2009/hunch_net-2009-08-26-Another_10-year_paper_in_Machine_Learning.html">368 hunch net-2009-08-26-Another 10-year paper in Machine Learning</a></p>
<p>8 0.14214717 <a title="89-tfidf-8" href="../hunch_net-2005/hunch_net-2005-02-17-Learning_Research_Programs.html">21 hunch net-2005-02-17-Learning Research Programs</a></p>
<p>9 0.14098813 <a title="89-tfidf-9" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>10 0.13931175 <a title="89-tfidf-10" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>11 0.13604133 <a title="89-tfidf-11" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>12 0.12217969 <a title="89-tfidf-12" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>13 0.12172125 <a title="89-tfidf-13" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>14 0.1128419 <a title="89-tfidf-14" href="../hunch_net-2009/hunch_net-2009-10-10-ALT_2009.html">374 hunch net-2009-10-10-ALT 2009</a></p>
<p>15 0.11124216 <a title="89-tfidf-15" href="../hunch_net-2007/hunch_net-2007-04-30-COLT_2007.html">242 hunch net-2007-04-30-COLT 2007</a></p>
<p>16 0.11030403 <a title="89-tfidf-16" href="../hunch_net-2012/hunch_net-2012-03-06-COLT-ICML_Open_Questions_and_ICML_Instructions.html">458 hunch net-2012-03-06-COLT-ICML Open Questions and ICML Instructions</a></p>
<p>17 0.10869491 <a title="89-tfidf-17" href="../hunch_net-2006/hunch_net-2006-07-11-New_Models.html">194 hunch net-2006-07-11-New Models</a></p>
<p>18 0.10824733 <a title="89-tfidf-18" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>19 0.10391119 <a title="89-tfidf-19" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>20 0.10339325 <a title="89-tfidf-20" href="../hunch_net-2009/hunch_net-2009-05-24-2009_ICML_discussion_site.html">356 hunch net-2009-05-24-2009 ICML discussion site</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.245), (1, -0.15), (2, 0.018), (3, -0.023), (4, -0.016), (5, -0.097), (6, -0.033), (7, 0.001), (8, 0.011), (9, -0.038), (10, 0.066), (11, 0.042), (12, -0.127), (13, 0.129), (14, 0.18), (15, -0.061), (16, 0.238), (17, -0.008), (18, -0.052), (19, -0.029), (20, 0.028), (21, 0.048), (22, 0.135), (23, 0.126), (24, -0.052), (25, -0.017), (26, 0.007), (27, 0.111), (28, -0.033), (29, -0.024), (30, 0.005), (31, 0.002), (32, -0.045), (33, 0.05), (34, 0.009), (35, 0.082), (36, -0.004), (37, -0.054), (38, -0.004), (39, -0.045), (40, -0.101), (41, 0.082), (42, -0.089), (43, -0.045), (44, -0.04), (45, 0.015), (46, 0.025), (47, -0.089), (48, 0.039), (49, 0.007)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96940595 <a title="89-lsi-1" href="../hunch_net-2005/hunch_net-2005-07-04-The_Health_of_COLT.html">89 hunch net-2005-07-04-The Health of COLT</a></p>
<p>Introduction: The health of  COLT  (Conference on Learning Theory or Computational Learning Theory depending on who you ask) has been questioned over the last few years.  Low points for the conference occurred when  EuroCOLT  merged with COLT in 2001, and the attendance at the 2002 Sydney COLT fell to a new low.  This occurred in the general context of machine learning conferences rising in both number and size over the last decade.
 
Any discussion of  why  COLT has had difficulties is inherently controversial as is any story about well-intentioned people making the wrong decisions.   Nevertheless, this may be worth discussing in the hope of avoiding problems in the future and general understanding.  In any such discussion there is a strong tendency to identify with a conference/community in a patriotic manner that is detrimental to thinking.  Keep in mind that conferences exist to further research.
 
My understanding (I wasn’t around) is that COLT started as a subcommunity of the computer science</p><p>2 0.8203842 <a title="89-lsi-2" href="../hunch_net-2010/hunch_net-2010-04-24-COLT_Treasurer_is_now_Phil_Long.html">394 hunch net-2010-04-24-COLT Treasurer is now Phil Long</a></p>
<p>Introduction: For about 5 years, I’ve been the treasurer of the Association for Computational Learning, otherwise known as COLT, taking over from  John Case  before me.  A transfer of duties to  Phil Long  is now about complete.  This probably matters to almost no one, but I wanted to describe things a bit for those interested.
 
The immediate impetus for this decision was unhappiness over reviewing decisions at  COLT 2009 , one as an author and several as a member of the program committee.  I seem to have disagreements fairly often about what is important work, partly because I’m focused on learning theory with practical implications, partly because I define learning theory more broadly than is typical amongst COLT members, and partly because COLT suffers a bit from insider-clique issues.  The degree to which these issues come up varies substantially each year so last year is not predictive of this one.  And, it’s important to understand that COLT remains healthy with these issues not nearly so bad</p><p>3 0.78458989 <a title="89-lsi-3" href="../hunch_net-2008/hunch_net-2008-11-09-A_Healthy__COLT.html">324 hunch net-2008-11-09-A Healthy  COLT</a></p>
<p>Introduction: A  while ago , we discussed the health of  COLT .   COLT 2008  substantially addressed my concerns.  The papers were diverse and several were interesting.  Attendance was up, which is particularly notable in Europe.  In my opinion, the colocation with UAI and ICML was the best colocation since 1998.
 
And, perhaps best of all, registration ended up being free for all students due to various grants from the  Academy of Finland ,  Google ,  IBM , and  Yahoo .
 
A basic question is: what went right?  There seem to be several answers.
  
 Cost-wise, COLT had sufficient grants to alleviate the high cost of the Euro and location at a university substantially reduces the cost compared to a hotel. 
 Organization-wise, the Finns were great with hordes of volunteers helping set everything up.  Having too many volunteers is a good failure mode. 
 Organization-wise, it was clear that all 3 program chairs were cooperating in designing the program. 
 Facilities-wise, proximity in time and space made</p><p>4 0.76309961 <a title="89-lsi-4" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<p>Introduction: By  Shie  and  Nati 
 
Following John’s advertisement for submitting to ICML, we thought it appropriate to highlight the advantages of COLT, and the reasons it is often the best place for theory papers.  We would like to emphasize that we both respect ICML, and are active in ICML, both as authors and as area chairs, and certainly are not arguing that ICML is a bad place for your papers.  For many papers, ICML is the best venue.  But for many theory papers, COLT is a better and more appropriate place.
 
Why should you submit to COLT?
 
By-and-large, theory papers go to COLT. This is the tradition of the field and most theory papers are sent to COLT. This is the place to present your ground-breaking theorems and new models that will shape the theory of machine learning. COLT is more focused then ICML with a single track session.  Unlike ICML, the norm in COLT is for people to sit through most sessions, and hear most of the talks presented.  There is also often a lively discussion followi</p><p>5 0.6718334 <a title="89-lsi-5" href="../hunch_net-2005/hunch_net-2005-06-28-The_cross_validation_problem%3A_cash_reward.html">86 hunch net-2005-06-28-The cross validation problem: cash reward</a></p>
<p>Introduction: I just  presented  the  cross validation  problem at  COLT .  
 
The problem now has a cash prize (up to $500) associated with itâ&euro;&rdquo;see the  presentation  for details.
 
The  write-up for colt .</p><p>6 0.65260237 <a title="89-lsi-6" href="../hunch_net-2005/hunch_net-2005-07-01-The_Role_of_Impromptu_Talks.html">88 hunch net-2005-07-01-The Role of Impromptu Talks</a></p>
<p>7 0.62857348 <a title="89-lsi-7" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>8 0.59330755 <a title="89-lsi-8" href="../hunch_net-2005/hunch_net-2005-03-28-Open_Problems_for_Colt.html">47 hunch net-2005-03-28-Open Problems for Colt</a></p>
<p>9 0.59277052 <a title="89-lsi-9" href="../hunch_net-2007/hunch_net-2007-04-30-COLT_2007.html">242 hunch net-2007-04-30-COLT 2007</a></p>
<p>10 0.57271302 <a title="89-lsi-10" href="../hunch_net-2009/hunch_net-2009-08-26-Another_10-year_paper_in_Machine_Learning.html">368 hunch net-2009-08-26-Another 10-year paper in Machine Learning</a></p>
<p>11 0.54124409 <a title="89-lsi-11" href="../hunch_net-2012/hunch_net-2012-03-06-COLT-ICML_Open_Questions_and_ICML_Instructions.html">458 hunch net-2012-03-06-COLT-ICML Open Questions and ICML Instructions</a></p>
<p>12 0.51652378 <a title="89-lsi-12" href="../hunch_net-2011/hunch_net-2011-10-10-ML_Symposium_and_ICML_details.html">447 hunch net-2011-10-10-ML Symposium and ICML details</a></p>
<p>13 0.51254094 <a title="89-lsi-13" href="../hunch_net-2005/hunch_net-2005-03-21-Research_Styles_in_Machine_Learning.html">44 hunch net-2005-03-21-Research Styles in Machine Learning</a></p>
<p>14 0.50034994 <a title="89-lsi-14" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>15 0.49187809 <a title="89-lsi-15" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>16 0.48327568 <a title="89-lsi-16" href="../hunch_net-2007/hunch_net-2007-11-02-The_Machine_Learning_Award_goes_to_%26%238230%3B.html">270 hunch net-2007-11-02-The Machine Learning Award goes to &#8230;</a></p>
<p>17 0.48027629 <a title="89-lsi-17" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>18 0.47350881 <a title="89-lsi-18" href="../hunch_net-2009/hunch_net-2009-01-08-Predictive_Analytics_World.html">335 hunch net-2009-01-08-Predictive Analytics World</a></p>
<p>19 0.47335887 <a title="89-lsi-19" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>20 0.46452564 <a title="89-lsi-20" href="../hunch_net-2005/hunch_net-2005-02-17-Learning_Research_Programs.html">21 hunch net-2005-02-17-Learning Research Programs</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(3, 0.018), (10, 0.026), (27, 0.211), (38, 0.046), (42, 0.261), (53, 0.049), (55, 0.173), (94, 0.057), (95, 0.071)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.95211822 <a title="89-lda-1" href="../hunch_net-2013/hunch_net-2013-06-29-The_Benefits_of_Double-Blind_Review.html">485 hunch net-2013-06-29-The Benefits of Double-Blind Review</a></p>
<p>Introduction: This post is a (near) transcript of a talk that I gave at the  ICML 2013 Workshop on Peer Review and Publishing Models . Although there’s a  PDF available on my website , I’ve chosen to post a slightly modified version here as well in order to better facilitate discussion.
 
 Disclaimers and Context 
 
I want to start with a couple of disclaimers and some context.
 
First, I want to point out that although I’ve read a lot about double-blind review, this isn’t my research area and the research discussed in this post is not my own. As a result, I probably can’t answer super detailed questions about these studies.
 
I also want to note that I’m not opposed to open peer review — I was a free and open source software developer for over ten years and I care a great deal about openness and transparency. Rather, my motivation in writing this post is simply to create awareness of and to initiate discussion about the benefits of double-blind review.
 
Lastly, and most importantly, I think it’s e</p><p>2 0.94963235 <a title="89-lda-2" href="../hunch_net-2007/hunch_net-2007-07-13-The_View_From_China.html">255 hunch net-2007-07-13-The View From China</a></p>
<p>Introduction: I’m visiting Beijing for the  Pao-Lu Hsu Statistics Conference  on Machine Learning.
 
I had several discussions about the state of Chinese research.  Given the large population and economy, you might expect substantial research—more than has been observed at international conferences.  The fundamental problem seems to be the  Cultural Revolution  which  lobotimized higher education, and the research associated with it.  There has been a process of slow recovery since then, which has begun to be felt in the research world via increased participation in international conferences and (now) conferences in China.
 
The amount of effort going into construction in Beijing is very impressive—people are literally building a skyscraper at night outside the window of the hotel I’m staying at (and this is not unusual).  If a small fraction of this effort is later focused onto supporting research, the effect could be very substantial.  General growth in China’s research portfolio should be expecte</p><p>3 0.94698256 <a title="89-lda-3" href="../hunch_net-2007/hunch_net-2007-04-30-COLT_2007.html">242 hunch net-2007-04-30-COLT 2007</a></p>
<p>Introduction: Registration for COLT 2007 is now open.
 
The conference will take place on 13-15 June, 2007, in San Diego, California, as part of the 2007 Federated Computing Research Conference (FCRC), which includes STOC, Complexity, and EC.
 
The website for COLT: http://www.learningtheory.org/colt2007/index.html
 
The early registration deadline is May 11, and the cutoff date for discounted hotel rates is May 9.
 
Before registering, take note that the fees are substantially lower for members of ACM and/or SIGACT than for nonmembers. If youâ&euro;&trade;ve been contemplating joining either of these two societies (annual dues: $99 for ACM, $18 for SIGACT), now would be a good time!</p><p>same-blog 4 0.88086122 <a title="89-lda-4" href="../hunch_net-2005/hunch_net-2005-07-04-The_Health_of_COLT.html">89 hunch net-2005-07-04-The Health of COLT</a></p>
<p>Introduction: The health of  COLT  (Conference on Learning Theory or Computational Learning Theory depending on who you ask) has been questioned over the last few years.  Low points for the conference occurred when  EuroCOLT  merged with COLT in 2001, and the attendance at the 2002 Sydney COLT fell to a new low.  This occurred in the general context of machine learning conferences rising in both number and size over the last decade.
 
Any discussion of  why  COLT has had difficulties is inherently controversial as is any story about well-intentioned people making the wrong decisions.   Nevertheless, this may be worth discussing in the hope of avoiding problems in the future and general understanding.  In any such discussion there is a strong tendency to identify with a conference/community in a patriotic manner that is detrimental to thinking.  Keep in mind that conferences exist to further research.
 
My understanding (I wasn’t around) is that COLT started as a subcommunity of the computer science</p><p>5 0.86524546 <a title="89-lda-5" href="../hunch_net-2008/hunch_net-2008-01-06-Research_Political_Issues.html">282 hunch net-2008-01-06-Research Political Issues</a></p>
<p>Introduction: I’ve avoided discussing politics here, although not for lack of interest.  The problem with discussing politics is that it’s customary for people to say much based upon little information.  Nevertheless, politics can have a substantial impact on science (and we might hope for the vice-versa).  It’s primary election time in the United States, so the topic is timely, although the issues are not.
 
There are several policy decisions which substantially effect development of science and technology in the US.
  
  Education  The US has great contrasts in education.  The top universities are very good places, yet the grade school education system produces mediocre results.  For me, the contrast between a  public education  and  Caltech  was bracing.  For many others attending Caltech, it clearly was not.  Upgrading the k-12 education system in the US is a long-standing chronic problem which I know relatively little about.  My own experience is that a basic attitude of “no child unrealized” i</p><p>6 0.85268819 <a title="89-lda-6" href="../hunch_net-2006/hunch_net-2006-11-02-2006_NIPS_workshops.html">216 hunch net-2006-11-02-2006 NIPS workshops</a></p>
<p>7 0.84241802 <a title="89-lda-7" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>8 0.76304251 <a title="89-lda-8" href="../hunch_net-2005/hunch_net-2005-03-13-Avoiding_Bad_Reviewing.html">40 hunch net-2005-03-13-Avoiding Bad Reviewing</a></p>
<p>9 0.75779372 <a title="89-lda-9" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>10 0.73184127 <a title="89-lda-10" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>11 0.73133808 <a title="89-lda-11" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>12 0.72818506 <a title="89-lda-12" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>13 0.72675574 <a title="89-lda-13" href="../hunch_net-2012/hunch_net-2012-06-05-ICML_acceptance_statistics.html">466 hunch net-2012-06-05-ICML acceptance statistics</a></p>
<p>14 0.72619343 <a title="89-lda-14" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>15 0.72425985 <a title="89-lda-15" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>16 0.72260994 <a title="89-lda-16" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>17 0.7209332 <a title="89-lda-17" href="../hunch_net-2005/hunch_net-2005-03-05-Funding_Research.html">36 hunch net-2005-03-05-Funding Research</a></p>
<p>18 0.72078955 <a title="89-lda-18" href="../hunch_net-2007/hunch_net-2007-01-02-Retrospective.html">225 hunch net-2007-01-02-Retrospective</a></p>
<p>19 0.72064906 <a title="89-lda-19" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>20 0.71613294 <a title="89-lda-20" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
