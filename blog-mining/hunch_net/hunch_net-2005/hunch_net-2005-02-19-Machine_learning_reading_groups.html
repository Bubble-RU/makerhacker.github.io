<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>24 hunch net-2005-02-19-Machine learning reading groups</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-24" href="#">hunch_net-2005-24</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>24 hunch net-2005-02-19-Machine learning reading groups</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-24-html" href="http://hunch.net/?p=27">html</a></p><p>Introduction: Yaroslav collected an extensive list of  machine learning reading groups .</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Yaroslav collected an extensive list of  machine learning reading groups . [sent-1, score-1.987]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('yaroslav', 0.502), ('collected', 0.479), ('extensive', 0.479), ('groups', 0.351), ('reading', 0.311), ('list', 0.25), ('machine', 0.083), ('learning', 0.034)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="24-tfidf-1" href="../hunch_net-2005/hunch_net-2005-02-19-Machine_learning_reading_groups.html">24 hunch net-2005-02-19-Machine learning reading groups</a></p>
<p>Introduction: Yaroslav collected an extensive list of  machine learning reading groups .</p><p>2 0.21243118 <a title="24-tfidf-2" href="../hunch_net-2005/hunch_net-2005-02-08-Some_Links.html">15 hunch net-2005-02-08-Some Links</a></p>
<p>Introduction: Yaroslav Bulatov collects some  links   to other technical blogs.</p><p>3 0.083916813 <a title="24-tfidf-3" href="../hunch_net-2007/hunch_net-2007-12-17-New_Machine_Learning_mailing_list.html">278 hunch net-2007-12-17-New Machine Learning mailing list</a></p>
<p>Introduction: IMLS  (which is the nonprofit running ICML) has setup a new mailing list for  Machine Learning News .  The list address is ML-news@googlegroups.com, and signup requires a google account (which you can create).  Only members can send messages.</p><p>4 0.066110864 <a title="24-tfidf-4" href="../hunch_net-2010/hunch_net-2010-10-28-NY_ML_Symposium_2010.html">415 hunch net-2010-10-28-NY ML Symposium 2010</a></p>
<p>Introduction: About 200 people attended the  2010 NYAS ML Symposium  this year.  (It was  about 170 last year .)  I particularly enjoyed several talks.
  
  Yann  has a new live demo of (limited) real-time object recognition learning.  
  Sanjoy  gave a fairly convincing and comprehensible explanation of why a  modified form of single-linkage clustering  is consistent in higher dimensions, and why consistency is a critical feature for clustering algorithms.  I’m curious how well this algorithm works in practice. 
  Matt Hoffman ‘s poster covering online LDA seemed pretty convincing to me as an algorithmic improvement. 
  
This year, we allocated more time towards posters & poster spotlights.  
 
For next year, we are considering some further changes.  The format has traditionally been 4 invited Professor speakers, with posters and poster spotlight for students.  Demand from other parties to participate is growing, for example from postdocs and startups in the area.  Another growing concern is the fa</p><p>5 0.060634878 <a title="24-tfidf-5" href="../hunch_net-2005/hunch_net-2005-04-01-Basic_computer_science_research_takes_a_hit.html">50 hunch net-2005-04-01-Basic computer science research takes a hit</a></p>
<p>Introduction: The New York Times has an interesting  article  about how DARPA has dropped funding for computer science to universities by about a factor of 2 over the last 5 years and become less directed towards basic research.  Partially in response, the number of grant submissions to NSF has grown by a factor of 3 (with the NSF budget staying approximately constant in the interim).
 
This is the sort of policy decision which may make sense for the defense department, but which means a large hit for basic research on information technology development in the US.  For example “darpa funded the invention of the internet” is reasonably correct.  This policy decision is particularly painful in the context of NSF budget cuts and the end of extensive phone monopoly funded research at Bell labs. 
 
The good news from a learning perspective is that (based on anecdotal evidence) much of the remaining funding is aimed at learning and learning-related fields.  Methods of making good automated predictions obv</p><p>6 0.058099672 <a title="24-tfidf-6" href="../hunch_net-2012/hunch_net-2012-05-02-ICML%3A_Behind_the_Scenes.html">463 hunch net-2012-05-02-ICML: Behind the Scenes</a></p>
<p>7 0.050550826 <a title="24-tfidf-7" href="../hunch_net-2009/hunch_net-2009-02-16-KDNuggets.html">342 hunch net-2009-02-16-KDNuggets</a></p>
<p>8 0.049642306 <a title="24-tfidf-8" href="../hunch_net-2006/hunch_net-2006-09-12-Incentive_Compatible_Reviewing.html">207 hunch net-2006-09-12-Incentive Compatible Reviewing</a></p>
<p>9 0.048937511 <a title="24-tfidf-9" href="../hunch_net-2005/hunch_net-2005-02-28-Regularization.html">33 hunch net-2005-02-28-Regularization</a></p>
<p>10 0.044292618 <a title="24-tfidf-10" href="../hunch_net-2009/hunch_net-2009-01-27-Key_Scientific_Challenges.html">339 hunch net-2009-01-27-Key Scientific Challenges</a></p>
<p>11 0.042879198 <a title="24-tfidf-11" href="../hunch_net-2007/hunch_net-2007-08-25-The_Privacy_Problem.html">260 hunch net-2007-08-25-The Privacy Problem</a></p>
<p>12 0.041281443 <a title="24-tfidf-12" href="../hunch_net-2006/hunch_net-2006-08-10-Precision_is_not_accuracy.html">202 hunch net-2006-08-10-Precision is not accuracy</a></p>
<p>13 0.040643379 <a title="24-tfidf-13" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>14 0.039517168 <a title="24-tfidf-14" href="../hunch_net-2006/hunch_net-2006-07-08-Some_recent_papers.html">192 hunch net-2006-07-08-Some recent papers</a></p>
<p>15 0.039350417 <a title="24-tfidf-15" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<p>16 0.036760911 <a title="24-tfidf-16" href="../hunch_net-2006/hunch_net-2006-09-28-Programming_Languages_for_Machine_Learning_Implementations.html">210 hunch net-2006-09-28-Programming Languages for Machine Learning Implementations</a></p>
<p>17 0.036423817 <a title="24-tfidf-17" href="../hunch_net-2005/hunch_net-2005-02-03-Learning_Theory%2C_by_assumption.html">12 hunch net-2005-02-03-Learning Theory, by assumption</a></p>
<p>18 0.034759112 <a title="24-tfidf-18" href="../hunch_net-2005/hunch_net-2005-09-19-NIPS_Workshops.html">113 hunch net-2005-09-19-NIPS Workshops</a></p>
<p>19 0.03186284 <a title="24-tfidf-19" href="../hunch_net-2005/hunch_net-2005-04-28-Science_Fiction_and_Research.html">64 hunch net-2005-04-28-Science Fiction and Research</a></p>
<p>20 0.028878309 <a title="24-tfidf-20" href="../hunch_net-2007/hunch_net-2007-09-16-Optimizing_Machine_Learning_Programs.html">262 hunch net-2007-09-16-Optimizing Machine Learning Programs</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.033), (1, -0.011), (2, -0.011), (3, 0.016), (4, -0.009), (5, 0.02), (6, -0.004), (7, 0.014), (8, -0.016), (9, -0.019), (10, 0.011), (11, 0.005), (12, 0.011), (13, -0.01), (14, -0.005), (15, -0.013), (16, -0.033), (17, -0.003), (18, 0.018), (19, -0.004), (20, 0.032), (21, -0.028), (22, -0.025), (23, -0.034), (24, -0.061), (25, -0.05), (26, 0.004), (27, 0.039), (28, -0.019), (29, 0.056), (30, 0.039), (31, 0.061), (32, 0.056), (33, 0.054), (34, -0.039), (35, -0.006), (36, 0.047), (37, 0.024), (38, -0.017), (39, -0.015), (40, -0.102), (41, -0.065), (42, -0.035), (43, -0.098), (44, -0.029), (45, 0.001), (46, -0.124), (47, -0.002), (48, -0.024), (49, -0.023)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.90323293 <a title="24-lsi-1" href="../hunch_net-2005/hunch_net-2005-02-19-Machine_learning_reading_groups.html">24 hunch net-2005-02-19-Machine learning reading groups</a></p>
<p>Introduction: Yaroslav collected an extensive list of  machine learning reading groups .</p><p>2 0.65335023 <a title="24-lsi-2" href="../hunch_net-2005/hunch_net-2005-02-08-Some_Links.html">15 hunch net-2005-02-08-Some Links</a></p>
<p>Introduction: Yaroslav Bulatov collects some  links   to other technical blogs.</p><p>3 0.64411795 <a title="24-lsi-3" href="../hunch_net-2007/hunch_net-2007-12-17-New_Machine_Learning_mailing_list.html">278 hunch net-2007-12-17-New Machine Learning mailing list</a></p>
<p>Introduction: IMLS  (which is the nonprofit running ICML) has setup a new mailing list for  Machine Learning News .  The list address is ML-news@googlegroups.com, and signup requires a google account (which you can create).  Only members can send messages.</p><p>4 0.57876384 <a title="24-lsi-4" href="../hunch_net-2009/hunch_net-2009-02-16-KDNuggets.html">342 hunch net-2009-02-16-KDNuggets</a></p>
<p>Introduction: Eric Zaetsch points out  KDNuggets  which is a well-developed mailing list/news site with a  KDD  flavor.  This might particularly interest people looking for industrial jobs in machine learning, as the mailing list has many such.</p><p>5 0.42288595 <a title="24-lsi-5" href="../hunch_net-2011/hunch_net-2011-03-27-Vowpal_Wabbit%2C_v5.1.html">428 hunch net-2011-03-27-Vowpal Wabbit, v5.1</a></p>
<p>Introduction: I just created  version 5.1  of  vowpal wabbit .  This almost entirely a bugfix release, so it’s an easy upgrade from v5.0.
 
In addition:
  
 There is now a  mailing list , which I and several other developers are subscribed to. 
 The main website has shifted to the wiki on github.  This means that anyone with a github account can now edit it. 
 I’m planning to give a tutorial tomorrow on it at  eHarmony / the LA machine learning meetup  at 10am.  Drop by if you’re interested. 
  
The status of VW amongst other open source projects has changed.  When VW first came out, it was relatively unique amongst existing projects in terms of features.  At this point, many other projects have started to appreciate the value of the design choices here.  This includes:
  
  Mahout , which now has an SGD implementation. 
  Shogun , where  Soeren  is keen on  incorporating features . 
  LibLinear , where they won the KDD best paper award for  out-of-core learning . 
  
This is expected—any open sourc</p><p>6 0.38242009 <a title="24-lsi-6" href="../hunch_net-2005/hunch_net-2005-02-28-Regularization.html">33 hunch net-2005-02-28-Regularization</a></p>
<p>7 0.36143374 <a title="24-lsi-7" href="../hunch_net-2006/hunch_net-2006-04-17-Rexa_is_live.html">173 hunch net-2006-04-17-Rexa is live</a></p>
<p>8 0.36105385 <a title="24-lsi-8" href="../hunch_net-2005/hunch_net-2005-02-02-Kolmogorov_Complexity_and_Googling.html">10 hunch net-2005-02-02-Kolmogorov Complexity and Googling</a></p>
<p>9 0.33890784 <a title="24-lsi-9" href="../hunch_net-2006/hunch_net-2006-07-09-The_Stock_Prediction_Machine_Learning_Problem.html">193 hunch net-2006-07-09-The Stock Prediction Machine Learning Problem</a></p>
<p>10 0.32381785 <a title="24-lsi-10" href="../hunch_net-2012/hunch_net-2012-05-02-ICML%3A_Behind_the_Scenes.html">463 hunch net-2012-05-02-ICML: Behind the Scenes</a></p>
<p>11 0.29786339 <a title="24-lsi-11" href="../hunch_net-2006/hunch_net-2006-09-12-Incentive_Compatible_Reviewing.html">207 hunch net-2006-09-12-Incentive Compatible Reviewing</a></p>
<p>12 0.29743281 <a title="24-lsi-12" href="../hunch_net-2005/hunch_net-2005-01-19-Why_I_decided_to_run_a_weblog..html">1 hunch net-2005-01-19-Why I decided to run a weblog.</a></p>
<p>13 0.29289564 <a title="24-lsi-13" href="../hunch_net-2006/hunch_net-2006-09-28-Programming_Languages_for_Machine_Learning_Implementations.html">210 hunch net-2006-09-28-Programming Languages for Machine Learning Implementations</a></p>
<p>14 0.28902832 <a title="24-lsi-14" href="../hunch_net-2005/hunch_net-2005-06-22-Languages__of_Learning.html">84 hunch net-2005-06-22-Languages  of Learning</a></p>
<p>15 0.2836692 <a title="24-lsi-15" href="../hunch_net-2009/hunch_net-2009-01-23-An_Active_Learning_Survey.html">338 hunch net-2009-01-23-An Active Learning Survey</a></p>
<p>16 0.28364757 <a title="24-lsi-16" href="../hunch_net-2008/hunch_net-2008-05-25-Inappropriate_Mathematics_for_Machine_Learning.html">302 hunch net-2008-05-25-Inappropriate Mathematics for Machine Learning</a></p>
<p>17 0.27787468 <a title="24-lsi-17" href="../hunch_net-2005/hunch_net-2005-04-01-Basic_computer_science_research_takes_a_hit.html">50 hunch net-2005-04-01-Basic computer science research takes a hit</a></p>
<p>18 0.27739295 <a title="24-lsi-18" href="../hunch_net-2006/hunch_net-2006-07-06-Branch_Prediction_Competition.html">190 hunch net-2006-07-06-Branch Prediction Competition</a></p>
<p>19 0.27632862 <a title="24-lsi-19" href="../hunch_net-2005/hunch_net-2005-09-19-NIPS_Workshops.html">113 hunch net-2005-09-19-NIPS Workshops</a></p>
<p>20 0.27295265 <a title="24-lsi-20" href="../hunch_net-2005/hunch_net-2005-04-10-Is_the_Goal_Understanding_or_Prediction%3F.html">55 hunch net-2005-04-10-Is the Goal Understanding or Prediction?</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(12, 0.682), (27, 0.037)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.89977211 <a title="24-lda-1" href="../hunch_net-2005/hunch_net-2005-02-19-Machine_learning_reading_groups.html">24 hunch net-2005-02-19-Machine learning reading groups</a></p>
<p>Introduction: Yaroslav collected an extensive list of  machine learning reading groups .</p><p>2 0.81445467 <a title="24-lda-2" href="../hunch_net-2011/hunch_net-2011-01-03-Herman_Goldstine_2011.html">421 hunch net-2011-01-03-Herman Goldstine 2011</a></p>
<p>Introduction: Vikas  points out the  Herman Goldstine Fellowship  at  IBM .  I was a Herman Goldstine Fellow, and benefited from the experience a great deal—that’s where work on learning reductions started.  If you can do research independently, it’s recommended.  Applications are due January 6.</p><p>3 0.63799077 <a title="24-lda-3" href="../hunch_net-2013/hunch_net-2013-05-04-COLT_and_ICML_registration.html">482 hunch net-2013-05-04-COLT and ICML registration</a></p>
<p>Introduction: Sebastien Bubeck  points out  COLT   registration  with a May 13 early registration deadline.   The local organizers have done an admirable job of containing costs with a $300 registration fee.
 
 ICML   registration  is also available, at about an x3 higher cost.  My understanding is that this is partly due to the costs of a larger conference being harder to contain, partly due to ICML lasting twice as long with tutorials and workshops, and partly because the conference organizers were a bit over-conservative in various ways.</p><p>4 0.45194098 <a title="24-lda-4" href="../hunch_net-2011/hunch_net-2011-07-11-Interesting_Neural_Network_Papers_at_ICML_2011.html">438 hunch net-2011-07-11-Interesting Neural Network Papers at ICML 2011</a></p>
<p>Introduction: Maybe it’s too early to call, but with four separate Neural Network sessions at this year’s  ICML ,  it looks like Neural Networks are making a comeback. Here are my  highlights of these sessions. In general, my feeling is that these  papers both demystify deep learning and show its broader applicability.
 
The first observation I made is that the once disreputable “Neural” nomenclature is being used again  in lieu of  “deep learning”. Maybe it’s because Adam Coates et al. showed that single layer networks can work surprisingly well.
  
  An Analysis of Single-Layer Networks in Unsupervised Feature       Learning ,  Adam Coates ,  Honglak Lee ,  Andrew Y. Ng  (AISTATS 2011) 
  The Importance of Encoding Versus Training with Sparse Coding and Vector Quantization ,  Adam Coates ,  Andrew Y. Ng  (ICML 2011) 
  
Another surprising result out of Andrew Ng’s group comes from Andrew  Saxe et al. who show that certain convolutional pooling architectures  can obtain close to state-of-the-art pe</p><p>5 0.21049249 <a title="24-lda-5" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<p>Introduction: There were  two   papers  at ICML presenting learning algorithms for a  contextual bandit -style setting, where the loss for all labels is not known, but the loss for one label is known.  (The first might require a  exploration scavenging  viewpoint to understand if the experimental assignment was nonrandom.)  I strongly approve of these papers and further work in this setting and its variants, because I expect it to become more important than supervised learning.  As a quick review, we are thinking about situations where repeatedly:
  
 The world reveals feature values (aka context information). 
 A policy chooses an action. 
 The world provides a reward. 
  
Sometimes this is done in an online fashion where the policy can change based on immediate feedback and sometimes it’s done in a batch setting where many samples are collected before the policy can change.  If you haven’t spent time thinking about the setting, you might want to because there are many natural applications.
 
I’m g</p><p>6 0.16491111 <a title="24-lda-6" href="../hunch_net-2007/hunch_net-2007-08-19-Choice_of_Metrics.html">259 hunch net-2007-08-19-Choice of Metrics</a></p>
<p>7 0.075059421 <a title="24-lda-7" href="../hunch_net-2005/hunch_net-2005-02-12-ROC_vs._Accuracy_vs._AROC.html">18 hunch net-2005-02-12-ROC vs. Accuracy vs. AROC</a></p>
<p>8 0.070559144 <a title="24-lda-8" href="../hunch_net-2005/hunch_net-2005-05-21-What_is_the_right_form_of_modularity_in_structured_prediction%3F.html">74 hunch net-2005-05-21-What is the right form of modularity in structured prediction?</a></p>
<p>9 0.070453569 <a title="24-lda-9" href="../hunch_net-2011/hunch_net-2011-11-26-Giving_Thanks.html">449 hunch net-2011-11-26-Giving Thanks</a></p>
<p>10 0.06957908 <a title="24-lda-10" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>11 0.066711351 <a title="24-lda-11" href="../hunch_net-2010/hunch_net-2010-09-21-Regretting_the_dead.html">411 hunch net-2010-09-21-Regretting the dead</a></p>
<p>12 0.053609975 <a title="24-lda-12" href="../hunch_net-2006/hunch_net-2006-03-24-NLPers.html">166 hunch net-2006-03-24-NLPers</a></p>
<p>13 0.053609975 <a title="24-lda-13" href="../hunch_net-2007/hunch_net-2007-06-13-Not_Posting.html">246 hunch net-2007-06-13-Not Posting</a></p>
<p>14 0.053609975 <a title="24-lda-14" href="../hunch_net-2010/hunch_net-2010-12-02-Traffic_Prediction_Problem.html">418 hunch net-2010-12-02-Traffic Prediction Problem</a></p>
<p>15 0.053554732 <a title="24-lda-15" href="../hunch_net-2007/hunch_net-2007-11-28-Computational_Consequences_of_Classification.html">274 hunch net-2007-11-28-Computational Consequences of Classification</a></p>
<p>16 0.05346638 <a title="24-lda-16" href="../hunch_net-2007/hunch_net-2007-06-14-Interesting_Papers_at_COLT_2007.html">247 hunch net-2007-06-14-Interesting Papers at COLT 2007</a></p>
<p>17 0.053346187 <a title="24-lda-17" href="../hunch_net-2008/hunch_net-2008-07-06-To_Dual_or_Not.html">308 hunch net-2008-07-06-To Dual or Not</a></p>
<p>18 0.053240072 <a title="24-lda-18" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>19 0.053217154 <a title="24-lda-19" href="../hunch_net-2007/hunch_net-2007-05-12-Loss_Function_Semantics.html">245 hunch net-2007-05-12-Loss Function Semantics</a></p>
<p>20 0.053207465 <a title="24-lda-20" href="../hunch_net-2006/hunch_net-2006-04-14-JMLR_is_a_success.html">172 hunch net-2006-04-14-JMLR is a success</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
