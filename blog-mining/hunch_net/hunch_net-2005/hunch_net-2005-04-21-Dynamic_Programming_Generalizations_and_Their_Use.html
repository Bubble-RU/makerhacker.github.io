<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>58 hunch net-2005-04-21-Dynamic Programming Generalizations and Their Use</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-58" href="#">hunch_net-2005-58</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>58 hunch net-2005-04-21-Dynamic Programming Generalizations and Their Use</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-58-html" href="http://hunch.net/?p=63">html</a></p><p>Introduction: David Mcallestergave a talk about thispaper(withPedro Felzenszwalb). I'll try
to give a high level summary of why it's interesting.Dynamic programming is
most familiar as instantiated by Viterbi decoding in a hidden markov model. It
is a general paradigm for problem solving where subproblems are solved and
used to solve larger problems. In the Viterbi decoding example, the subproblem
is "What is the most probable path ending at each state at timestept?", and
the larger problem is the same except at timestept+1. There are a few
optimizations you can do here:Dynamic Programming -> queued Dynamic
Programming. Keep track of the "cost so far" (or "most probable path") and
(carefully) only look at extensions to paths likely to yield the shortest
path. "Carefully" here is defined byDijkstra's shortest path algorithm.queued
Dynamic programming -> A*Add a lower bound on the cost to complete a path (or
an upper bound on the probability of a completion) for the priority queue of
Dijkstra's shorte</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 I'll try to give a high level summary of why it's interesting. [sent-2, score-0.323]
</p><p>2 Dynamic programming is most familiar as instantiated by Viterbi decoding in a hidden markov model. [sent-3, score-0.435]
</p><p>3 It is a general paradigm for problem solving where subproblems are solved and used to solve larger problems. [sent-4, score-0.371]
</p><p>4 In the Viterbi decoding example, the subproblem is "What is the most probable path ending at each state at timestept? [sent-5, score-0.858]
</p><p>5 ", and the larger problem is the same except at timestept+1. [sent-6, score-0.214]
</p><p>6 There are a few optimizations you can do here:Dynamic Programming -> queued Dynamic Programming. [sent-7, score-0.082]
</p><p>7 Keep track of the "cost so far" (or "most probable path") and (carefully) only look at extensions to paths likely to yield the shortest path. [sent-8, score-0.759]
</p><p>8 queued Dynamic programming -> A*Add a lower bound on the cost to complete a path (or an upper bound on the probability of a completion) for the priority queue of Dijkstra's shortest path. [sent-10, score-1.586]
</p><p>9 This can yield computational speedups varying between negligible and outstanding. [sent-11, score-0.256]
</p><p>10 A*-> Hierarchical A*The efficiency of A*search is dependent on the tightness of it's lower bound, which brings up the question: "Where do you get the lower bound? [sent-12, score-0.479]
</p><p>11 " One appealing answer is from A*applied to a simplified problem equivalent to the original problem, but with states aliased (many states in original problem = 1 state in new problem). [sent-13, score-0.912]
</p><p>12 This technique can be applied recursively until the problem is trivial. [sent-14, score-0.312]
</p><p>13 Each of these steps has been noted previously (although perhaps not in the generality of this paper). [sent-15, score-0.162]
</p><p>14 What seems new and interesting is that the entire hierarchy of A*searches can be done simultaneously on one priority queue. [sent-16, score-0.287]
</p><p>15 The resulting algorithm can use low level information to optimize high level search as well as high level information to optimize low level search in a holistic process. [sent-17, score-1.963]
</p><p>16 It's not clear yet how far this approach can be pushed, but this quality is quite appealing. [sent-18, score-0.078]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('path', 0.314), ('shortest', 0.294), ('timestept', 0.221), ('dynamic', 0.214), ('level', 0.206), ('priority', 0.196), ('bound', 0.185), ('decoding', 0.182), ('viterbi', 0.182), ('probable', 0.172), ('programming', 0.162), ('search', 0.159), ('lower', 0.157), ('problem', 0.129), ('original', 0.12), ('states', 0.118), ('high', 0.117), ('optimize', 0.116), ('yield', 0.109), ('carefully', 0.099), ('ending', 0.098), ('holistic', 0.098), ('paths', 0.098), ('recursively', 0.098), ('thispaper', 0.098), ('low', 0.094), ('cost', 0.093), ('state', 0.092), ('instantiated', 0.091), ('completion', 0.091), ('hierarchical', 0.091), ('hierarchy', 0.091), ('appealing', 0.086), ('extensions', 0.086), ('noted', 0.086), ('paradigm', 0.086), ('pushed', 0.086), ('searches', 0.086), ('tightness', 0.086), ('applied', 0.085), ('larger', 0.085), ('optimizations', 0.082), ('brings', 0.079), ('far', 0.078), ('generality', 0.076), ('speedups', 0.076), ('david', 0.073), ('subproblems', 0.071), ('varying', 0.071), ('resulting', 0.069)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="58-tfidf-1" href="../hunch_net-2005/hunch_net-2005-04-21-Dynamic_Programming_Generalizations_and_Their_Use.html">58 hunch net-2005-04-21-Dynamic Programming Generalizations and Their Use</a></p>
<p>Introduction: David Mcallestergave a talk about thispaper(withPedro Felzenszwalb). I'll try
to give a high level summary of why it's interesting.Dynamic programming is
most familiar as instantiated by Viterbi decoding in a hidden markov model. It
is a general paradigm for problem solving where subproblems are solved and
used to solve larger problems. In the Viterbi decoding example, the subproblem
is "What is the most probable path ending at each state at timestept?", and
the larger problem is the same except at timestept+1. There are a few
optimizations you can do here:Dynamic Programming -> queued Dynamic
Programming. Keep track of the "cost so far" (or "most probable path") and
(carefully) only look at extensions to paths likely to yield the shortest
path. "Carefully" here is defined byDijkstra's shortest path algorithm.queued
Dynamic programming -> A*Add a lower bound on the cost to complete a path (or
an upper bound on the probability of a completion) for the priority queue of
Dijkstra's shorte</p><p>2 0.15729724 <a title="58-tfidf-2" href="../hunch_net-2005/hunch_net-2005-06-18-Lower_Bounds_for_Learning_Reductions.html">83 hunch net-2005-06-18-Lower Bounds for Learning Reductions</a></p>
<p>Introduction: Learning reductionstransform a solver of one type of learning problem into a
solver of another type of learning problem. When we analyze these for
robustness we can make statement of the form "ReductionRhas the property that
regretr(or loss) on subproblems of typeAimplies regret at mostf ( r )on the
original problem of typeB".A lower bound for a learning reduction would have
the form "for all reductionsR, there exists a learning problem of typeBand
learning algorithm for problems of typeAwhere regretron induced problems
impliesat leastregretf ( r )forB".The pursuit of lower bounds is often
questionable because, unlike upper bounds, they do not yield practical
algorithms. Nevertheless, they may be helpful as a tool for thinking about
what is learnable and how learnable it is. This has already come
uphereandhere.At the moment, there is no coherent theory of lower bounds for
learning reductions, and we have little understanding of how feasible they are
or which techniques may be useful in</p><p>3 0.14756373 <a title="58-tfidf-3" href="../hunch_net-2005/hunch_net-2005-08-11-Why_Manifold-Based_Dimension_Reduction_Techniques%3F.html">102 hunch net-2005-08-11-Why Manifold-Based Dimension Reduction Techniques?</a></p>
<p>Introduction: Manifold based dimension-reduction algorithms share the following general
outline.Given: a metricd()and a set of pointsSConstruct a graph with a point
in every node and every edge connecting to the node of one of thek-nearest
neighbors. Associate with the edge a weight which is the distance between the
points in the connected nodes.Digest the graph. This might include computing
the shortest path between all points or figuring out how to linearly
interpolate the point from it's neighbors.Find a set of points in a low
dimensional space which preserve the digested properties.Examples include LLE,
Isomap (which I worked on), Hessian-LLE, SDE, and many others. The hope with
these algorithms is that they can recover the low dimensional structure of
point sets in high dimensional spaces. Many of them can be shown to work in
interesting ways producing various compelling pictures.Despite doing some
early work in this direction, I suffer from a motivational problem: Why do we
want to recover the</p><p>4 0.13499837 <a title="58-tfidf-4" href="../hunch_net-2006/hunch_net-2006-10-22-Exemplar_programming.html">215 hunch net-2006-10-22-Exemplar programming</a></p>
<p>Introduction: There are many different abstractions for problem definition and solution.
Here are a few examples:Functional programming: a set of functions are
defined. The composed execution of these functions yields the solution.Linear
programming: a set of constraints and a linear objective function are defined.
An LP solver finds the constrained optimum.Quadratic programming: Like linear
programming, but the language is a little more flexible (and the solution
slower).Convex programming: like quadratic programming, but the language is
more flexible (and the solutions even slower).Dynamic programming: a recursive
definition of the problem is defined and then solved efficiently via caching
tricks.SAT programming: A problem is specified as a satisfiability involving a
conjunction of a disjunction of boolean variables. A general engine attempts
to find a good satisfying assignment. For exampleKautz'sblackboxplanner.These
abstractions have different tradeoffs between ease of use, generality, and the</p><p>5 0.1078784 <a title="58-tfidf-5" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>Introduction: "Search" is the other branch of AI research which has been succesful. Concrete
examples includeDeep Bluewhich beat the world chess champion andChinookthe
champion checkers program. A set of core search techniques exist including A*,
alpha-beta pruning, and others that can be applied to any of many different
search problems.Given this, it may be surprising to learn that there has been
relatively little succesful work on combining prediction and search. Given
also that humans typically solve search problems using a number of predictive
heuristics to narrow in on a solution, we might be surprised again. However,
the big successful search-based systems have typically not used "smart" search
algorithms. Insteady they have optimized for very fast search. This is not for
lack of tryingâ&euro;Ś many people have tried to synthesize search and prediction to
various degrees of success. For example,Knightcapachieves good-but-not-stellar
chess playing performance, andTD-gammonhas achieved near-optimal Bac</p><p>6 0.10442737 <a title="58-tfidf-6" href="../hunch_net-2006/hunch_net-2006-09-28-Programming_Languages_for_Machine_Learning_Implementations.html">210 hunch net-2006-09-28-Programming Languages for Machine Learning Implementations</a></p>
<p>7 0.099094391 <a title="58-tfidf-7" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>8 0.098917983 <a title="58-tfidf-8" href="../hunch_net-2009/hunch_net-2009-10-03-Static_vs._Dynamic_multiclass_prediction.html">373 hunch net-2009-10-03-Static vs. Dynamic multiclass prediction</a></p>
<p>9 0.098873384 <a title="58-tfidf-9" href="../hunch_net-2007/hunch_net-2007-06-14-Interesting_Papers_at_COLT_2007.html">247 hunch net-2007-06-14-Interesting Papers at COLT 2007</a></p>
<p>10 0.096160725 <a title="58-tfidf-10" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<p>11 0.09464103 <a title="58-tfidf-11" href="../hunch_net-2006/hunch_net-2006-07-05-more_icml_papers.html">189 hunch net-2006-07-05-more icml papers</a></p>
<p>12 0.094286427 <a title="58-tfidf-12" href="../hunch_net-2005/hunch_net-2005-05-29-Maximum_Margin_Mismatch%3F.html">77 hunch net-2005-05-29-Maximum Margin Mismatch?</a></p>
<p>13 0.091350138 <a title="58-tfidf-13" href="../hunch_net-2007/hunch_net-2007-05-09-The_Missing_Bound.html">244 hunch net-2007-05-09-The Missing Bound</a></p>
<p>14 0.091042429 <a title="58-tfidf-14" href="../hunch_net-2006/hunch_net-2006-04-05-What_is_state%3F.html">169 hunch net-2006-04-05-What is state?</a></p>
<p>15 0.087285593 <a title="58-tfidf-15" href="../hunch_net-2005/hunch_net-2005-06-28-A_COLT_paper.html">85 hunch net-2005-06-28-A COLT paper</a></p>
<p>16 0.085603669 <a title="58-tfidf-16" href="../hunch_net-2006/hunch_net-2006-02-02-Introspectionism_as_a_Disease.html">153 hunch net-2006-02-02-Introspectionism as a Disease</a></p>
<p>17 0.082270399 <a title="58-tfidf-17" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>18 0.080696538 <a title="58-tfidf-18" href="../hunch_net-2008/hunch_net-2008-12-07-A_NIPS_paper.html">330 hunch net-2008-12-07-A NIPS paper</a></p>
<p>19 0.079941064 <a title="58-tfidf-19" href="../hunch_net-2008/hunch_net-2008-05-23-Three_levels_of_addressing_the_Netflix_Prize.html">301 hunch net-2008-05-23-Three levels of addressing the Netflix Prize</a></p>
<p>20 0.079537533 <a title="58-tfidf-20" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.164), (1, -0.061), (2, -0.014), (3, -0.026), (4, -0.024), (5, -0.087), (6, 0.01), (7, -0.025), (8, 0.061), (9, 0.005), (10, -0.087), (11, 0.021), (12, 0.037), (13, 0.029), (14, 0.051), (15, 0.081), (16, -0.023), (17, 0.015), (18, 0.069), (19, -0.071), (20, -0.135), (21, -0.012), (22, -0.119), (23, -0.069), (24, -0.151), (25, -0.002), (26, -0.15), (27, -0.034), (28, 0.033), (29, 0.032), (30, 0.089), (31, -0.043), (32, 0.025), (33, -0.014), (34, 0.103), (35, 0.122), (36, -0.013), (37, -0.059), (38, -0.077), (39, 0.064), (40, -0.036), (41, -0.055), (42, -0.046), (43, 0.088), (44, 0.01), (45, 0.011), (46, -0.071), (47, 0.091), (48, 0.033), (49, -0.052)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9733445 <a title="58-lsi-1" href="../hunch_net-2005/hunch_net-2005-04-21-Dynamic_Programming_Generalizations_and_Their_Use.html">58 hunch net-2005-04-21-Dynamic Programming Generalizations and Their Use</a></p>
<p>Introduction: David Mcallestergave a talk about thispaper(withPedro Felzenszwalb). I'll try
to give a high level summary of why it's interesting.Dynamic programming is
most familiar as instantiated by Viterbi decoding in a hidden markov model. It
is a general paradigm for problem solving where subproblems are solved and
used to solve larger problems. In the Viterbi decoding example, the subproblem
is "What is the most probable path ending at each state at timestept?", and
the larger problem is the same except at timestept+1. There are a few
optimizations you can do here:Dynamic Programming -> queued Dynamic
Programming. Keep track of the "cost so far" (or "most probable path") and
(carefully) only look at extensions to paths likely to yield the shortest
path. "Carefully" here is defined byDijkstra's shortest path algorithm.queued
Dynamic programming -> A*Add a lower bound on the cost to complete a path (or
an upper bound on the probability of a completion) for the priority queue of
Dijkstra's shorte</p><p>2 0.55059159 <a title="58-lsi-2" href="../hunch_net-2005/hunch_net-2005-06-18-Lower_Bounds_for_Learning_Reductions.html">83 hunch net-2005-06-18-Lower Bounds for Learning Reductions</a></p>
<p>Introduction: Learning reductionstransform a solver of one type of learning problem into a
solver of another type of learning problem. When we analyze these for
robustness we can make statement of the form "ReductionRhas the property that
regretr(or loss) on subproblems of typeAimplies regret at mostf ( r )on the
original problem of typeB".A lower bound for a learning reduction would have
the form "for all reductionsR, there exists a learning problem of typeBand
learning algorithm for problems of typeAwhere regretron induced problems
impliesat leastregretf ( r )forB".The pursuit of lower bounds is often
questionable because, unlike upper bounds, they do not yield practical
algorithms. Nevertheless, they may be helpful as a tool for thinking about
what is learnable and how learnable it is. This has already come
uphereandhere.At the moment, there is no coherent theory of lower bounds for
learning reductions, and we have little understanding of how feasible they are
or which techniques may be useful in</p><p>3 0.54386318 <a title="58-lsi-3" href="../hunch_net-2007/hunch_net-2007-05-09-The_Missing_Bound.html">244 hunch net-2007-05-09-The Missing Bound</a></p>
<p>Introduction: Sham Kakadepoints out that we are missing a bound.Suppose we
havemsamplesxdrawn IID from some distributionD. Through the magic of
exponential moment method we know that:If the range ofxis bounded by an
interval of sizeI, aChernoff/Hoeffding style boundgives us a bound on the
deviations likeO(I/m0.5)(at least in crude form). A proof is on page 9here.If
the range ofxis bounded, and the variance (or a bound on the variance) is
known, thenBennett's boundcan give tighter results (*). This can be a huge
improvment when the true variance small.What's missing here is a bound that
depends on the observed variance rather than a bound on the variance. This
means that many people attempt to use Bennett's bound (incorrectly) by
plugging the observed variance in as the true variance, invalidating the bound
application. Most of the time, they get away with it, but this is a dangerous
move when doing machine learning. In machine learning, we are typically trying
to find a predictor with 0 expected los</p><p>4 0.49076417 <a title="58-lsi-4" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>Introduction: "Search" is the other branch of AI research which has been succesful. Concrete
examples includeDeep Bluewhich beat the world chess champion andChinookthe
champion checkers program. A set of core search techniques exist including A*,
alpha-beta pruning, and others that can be applied to any of many different
search problems.Given this, it may be surprising to learn that there has been
relatively little succesful work on combining prediction and search. Given
also that humans typically solve search problems using a number of predictive
heuristics to narrow in on a solution, we might be surprised again. However,
the big successful search-based systems have typically not used "smart" search
algorithms. Insteady they have optimized for very fast search. This is not for
lack of tryingâ&euro;Ś many people have tried to synthesize search and prediction to
various degrees of success. For example,Knightcapachieves good-but-not-stellar
chess playing performance, andTD-gammonhas achieved near-optimal Bac</p><p>5 0.47869605 <a title="58-lsi-5" href="../hunch_net-2006/hunch_net-2006-04-06-Bounds_greater_than_1.html">170 hunch net-2006-04-06-Bounds greater than 1</a></p>
<p>Introduction: Nati SrebroandShai Ben-Davidhave apaperatCOLTwhich, in the appendix, proves
something very striking: several previous error bounds arealwaysgreater than
1.BackgroundOne branch of learning theory focuses on theorems whichAssume
samples are drawn IID from an unknown distributionD.Fix a set of
classifiersFind a high probability bound on the maximum true error rate (with
respect toD) as a function of the empirical error rate on the training
set.Many of these bounds become extremely complex and hairy.CurrentEveryone
working on this subject wants "tighter bounds", however there are different
definitions of "tighter". Some groups focus on "functional tightness" (getting
the right functional dependency between the size of the training set and a
parameterization of the hypothesis space) whileothersfocus on "practical
tightness" (finding bounds which work well on practical problems). (I am
definitely in the second camp.)One of the dangers of striving for "functional
tightness" is that the bound</p><p>6 0.46764931 <a title="58-lsi-6" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>7 0.46593851 <a title="58-lsi-7" href="../hunch_net-2005/hunch_net-2005-08-11-Why_Manifold-Based_Dimension_Reduction_Techniques%3F.html">102 hunch net-2005-08-11-Why Manifold-Based Dimension Reduction Techniques?</a></p>
<p>8 0.46388698 <a title="58-lsi-8" href="../hunch_net-2009/hunch_net-2009-10-03-Static_vs._Dynamic_multiclass_prediction.html">373 hunch net-2009-10-03-Static vs. Dynamic multiclass prediction</a></p>
<p>9 0.45554218 <a title="58-lsi-9" href="../hunch_net-2009/hunch_net-2009-08-26-Another_10-year_paper_in_Machine_Learning.html">368 hunch net-2009-08-26-Another 10-year paper in Machine Learning</a></p>
<p>10 0.44857448 <a title="58-lsi-10" href="../hunch_net-2007/hunch_net-2007-06-14-Interesting_Papers_at_COLT_2007.html">247 hunch net-2007-06-14-Interesting Papers at COLT 2007</a></p>
<p>11 0.44844925 <a title="58-lsi-11" href="../hunch_net-2010/hunch_net-2010-03-26-A_Variance_only_Deviation_Bound.html">392 hunch net-2010-03-26-A Variance only Deviation Bound</a></p>
<p>12 0.44604164 <a title="58-lsi-12" href="../hunch_net-2005/hunch_net-2005-06-28-A_COLT_paper.html">85 hunch net-2005-06-28-A COLT paper</a></p>
<p>13 0.4441573 <a title="58-lsi-13" href="../hunch_net-2006/hunch_net-2006-10-22-Exemplar_programming.html">215 hunch net-2006-10-22-Exemplar programming</a></p>
<p>14 0.42748472 <a title="58-lsi-14" href="../hunch_net-2005/hunch_net-2005-05-10-Learning_Reductions_are_Reductionist.html">68 hunch net-2005-05-10-Learning Reductions are Reductionist</a></p>
<p>15 0.41302115 <a title="58-lsi-15" href="../hunch_net-2005/hunch_net-2005-12-09-Some_NIPS_papers.html">138 hunch net-2005-12-09-Some NIPS papers</a></p>
<p>16 0.41138357 <a title="58-lsi-16" href="../hunch_net-2007/hunch_net-2007-10-24-Contextual_Bandits.html">269 hunch net-2007-10-24-Contextual Bandits</a></p>
<p>17 0.40028509 <a title="58-lsi-17" href="../hunch_net-2006/hunch_net-2006-04-05-What_is_state%3F.html">169 hunch net-2006-04-05-What is state?</a></p>
<p>18 0.40007091 <a title="58-lsi-18" href="../hunch_net-2005/hunch_net-2005-03-15-The_State_of_Tight_Bounds.html">41 hunch net-2005-03-15-The State of Tight Bounds</a></p>
<p>19 0.39887285 <a title="58-lsi-19" href="../hunch_net-2005/hunch_net-2005-09-26-Prediction_Bounds_as_the_Mathematics_of_Science.html">115 hunch net-2005-09-26-Prediction Bounds as the Mathematics of Science</a></p>
<p>20 0.39626288 <a title="58-lsi-20" href="../hunch_net-2005/hunch_net-2005-07-13-Text_Entailment_at_AAAI.html">94 hunch net-2005-07-13-Text Entailment at AAAI</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(42, 0.221), (45, 0.554), (74, 0.116)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.99230129 <a title="58-lda-1" href="../hunch_net-2010/hunch_net-2010-05-20-Google_Predict.html">399 hunch net-2010-05-20-Google Predict</a></p>
<p>Introduction: Slashdotpoints outGoogle Predict. I'm not privy to the details, but this has
the potential to be extremely useful, as in many applications simply having an
easy mechanism to apply existing learning algorithms can be extremely helpful.
This differs goalwise fromMLcomp--instead of public comparisons for research
purposes, it's about private utilization of good existing algorithms. It also
differs infrastructurally, since a system designed to do this is much less
awkward than using Amazon's cloud computing. The latter implies that datasets
several order of magnitude larger can be handled up to limits imposed by
network and storage.</p><p>2 0.99041402 <a title="58-lda-2" href="../hunch_net-2010/hunch_net-2010-03-12-Netflix_Challenge_2_Canceled.html">390 hunch net-2010-03-12-Netflix Challenge 2 Canceled</a></p>
<p>Introduction: Thesecond Netflix prize is canceleddue toprivacy problems. I continue to
believe my original assessment of this paper, that the privacy break was
somewhat overstated. I still haven't seen any serious privacy failures on the
scale of theAOL search log release.I expect privacy concerns to continue to be
a big issue when dealing with data releases by companies or governments. The
theory of maintaining privacy while using data is improving, but it is not yet
in a state where the limits of what's possible are clear let alone how to
achieve these limits in a manner friendly to a prediction competition.</p><p>3 0.98630154 <a title="58-lda-3" href="../hunch_net-2005/hunch_net-2005-09-05-Site_Update.html">107 hunch net-2005-09-05-Site Update</a></p>
<p>Introduction: I tweaked the site in a number of ways today, including:Updating
toWordPress1.5.Installing and heavily tweaking theGeeknichetheme. Update: I
switched back to a tweaked version of the old theme.Adding theCustomizable
Post Listingsplugin.Installing theStatTraqplugin.Updating some of the links. I
particularly recommend looking at thecomputer research
policyblog.Addingthreaded comments. This doesn't thread old comments
obviously, but the extra structure may be helpful for new ones.Overall, I
think this is an improvement, and it addresses a few of myearlier problems. If
you have any difficulties or anything seems "not quite right", please speak
up. A few other tweaks to the site may happen in the near future.</p><p>4 0.96703857 <a title="58-lda-4" href="../hunch_net-2010/hunch_net-2010-01-13-Sam_Roweis_died.html">386 hunch net-2010-01-13-Sam Roweis died</a></p>
<p>Introduction: and I can't help but remember him.I first metSamas an undergraduate
atCaltechwhere he was TA forHopfield's class, and again when I visitedGatsby,
when he invited me to visitToronto, and at too many conferences to recount.
His personality was a combination of enthusiastic and thoughtful, with a great
ability to phrase a problem so it's solution must be understood. With respect
to my own work, Sam was the one who advised me to makemy first tutorial,
leading to others, and to other things, all of which I'm grateful to him for.
In fact, my every interaction with Sam was positive, and that was his way.His
death isbeing called a suicidewhich is so incompatible with my understanding
of Sam that it strains my credibility. But we know that his many
responsibilities were great, and it is well understood that basically all sane
researchers have legions of inner doubts. Having been depressed now and then
myself, it's helpful to understand at least intellectually that the true
darkness of the now i</p><p>5 0.95236355 <a title="58-lda-5" href="../hunch_net-2005/hunch_net-2005-03-08-Fast_Physics_for_Learning.html">37 hunch net-2005-03-08-Fast Physics for Learning</a></p>
<p>Introduction: While everyone is silently working on ICML submissions, I found this
discussion about afast physics simulatorchip interesting from a learning
viewpoint. In many cases, learning attempts to predict the outcome of physical
processes. Access to a fast simulator for these processes might be quite
helpful in predicting the outcome. Bayesian learning in particular may
directly benefit while many other algorithms (like support vector machines)
might have their speed greatly increased.The biggest drawback is that writing
software for these odd architectures is always difficult and time consuming,
but a several-orders-of-magnitude speedup might make that worthwhile.</p><p>same-blog 6 0.9329505 <a title="58-lda-6" href="../hunch_net-2005/hunch_net-2005-04-21-Dynamic_Programming_Generalizations_and_Their_Use.html">58 hunch net-2005-04-21-Dynamic Programming Generalizations and Their Use</a></p>
<p>7 0.91593856 <a title="58-lda-7" href="../hunch_net-2009/hunch_net-2009-03-18-Parallel_ML_primitives.html">346 hunch net-2009-03-18-Parallel ML primitives</a></p>
<p>8 0.88498551 <a title="58-lda-8" href="../hunch_net-2007/hunch_net-2007-07-06-Idempotent-capable_Predictors.html">253 hunch net-2007-07-06-Idempotent-capable Predictors</a></p>
<p>9 0.84708571 <a title="58-lda-9" href="../hunch_net-2009/hunch_net-2009-01-08-Predictive_Analytics_World.html">335 hunch net-2009-01-08-Predictive Analytics World</a></p>
<p>10 0.63170487 <a title="58-lda-10" href="../hunch_net-2007/hunch_net-2007-11-05-CMU_wins_DARPA_Urban_Challenge.html">271 hunch net-2007-11-05-CMU wins DARPA Urban Challenge</a></p>
<p>11 0.59201157 <a title="58-lda-11" href="../hunch_net-2009/hunch_net-2009-05-17-Server_Update.html">354 hunch net-2009-05-17-Server Update</a></p>
<p>12 0.58408898 <a title="58-lda-12" href="../hunch_net-2007/hunch_net-2007-08-25-The_Privacy_Problem.html">260 hunch net-2007-08-25-The Privacy Problem</a></p>
<p>13 0.58223093 <a title="58-lda-13" href="../hunch_net-2011/hunch_net-2011-06-22-Ultra_LDA.html">436 hunch net-2011-06-22-Ultra LDA</a></p>
<p>14 0.57130045 <a title="58-lda-14" href="../hunch_net-2005/hunch_net-2005-06-18-Lower_Bounds_for_Learning_Reductions.html">83 hunch net-2005-06-18-Lower Bounds for Learning Reductions</a></p>
<p>15 0.57095975 <a title="58-lda-15" href="../hunch_net-2005/hunch_net-2005-12-07-Is_the_Google_way_the_way_for_machine_learning%3F.html">136 hunch net-2005-12-07-Is the Google way the way for machine learning?</a></p>
<p>16 0.56164843 <a title="58-lda-16" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<p>17 0.5615896 <a title="58-lda-17" href="../hunch_net-2007/hunch_net-2007-04-28-The_Coming_Patent_Apocalypse.html">241 hunch net-2007-04-28-The Coming Patent Apocalypse</a></p>
<p>18 0.5606094 <a title="58-lda-18" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>19 0.5599125 <a title="58-lda-19" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>20 0.55824399 <a title="58-lda-20" href="../hunch_net-2005/hunch_net-2005-09-12-Fast_Gradient_Descent.html">111 hunch net-2005-09-12-Fast Gradient Descent</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
