<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-27" href="#">hunch_net-2005-27</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-27-html" href="http://hunch.net/?p=30">html</a></p><p>Introduction: At an intuitive level, the question here is "Can reinforcement learning be
solved with classification?"ProblemConstruct a reinforcement learning
algorithm with near-optimal expected sum of rewards in thedirect experience
modelgiven access to a classifier learning algorithm which has a small error
rate or regret on all posed classification problems. The definition of "posed"
here is slightly murky. I consider a problem "posed" if there is an algorithm
for constructing labeled classification examples.Past WorkThere exists
areduction of reinforcement learning to classification given a generative
model.A generative model is an inherently stronger assumption than the direct
experience model.Otherwork on learning reductionsmay be important.Several
algorithms for solving reinforcement learning in the direct experience model
exist. Most, such asE3,Factored-E3, andmetric-E3andRmaxrequire that the
observation be the state. Recent workextends this approach to POMDPs.This
problem is related topred</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 At an intuitive level, the question here is "Can reinforcement learning be solved with classification? [sent-1, score-0.76]
</p><p>2 "ProblemConstruct a reinforcement learning algorithm with near-optimal expected sum of rewards in thedirect experience modelgiven access to a classifier learning algorithm which has a small error rate or regret on all posed classification problems. [sent-2, score-2.181]
</p><p>3 I consider a problem "posed" if there is an algorithm for constructing labeled classification examples. [sent-4, score-0.639]
</p><p>4 Past WorkThere exists areduction of reinforcement learning to classification given a generative model. [sent-5, score-1.112]
</p><p>5 A generative model is an inherently stronger assumption than the direct experience model. [sent-6, score-0.919]
</p><p>6 Several algorithms for solving reinforcement learning in the direct experience model exist. [sent-8, score-1.039]
</p><p>7 Most, such asE3,Factored-E3, andmetric-E3andRmaxrequire that the observation be the state. [sent-9, score-0.066]
</p><p>8 This problem is related topredictive state representations, because we are trying to solve reinforcement learning with prediction ability. [sent-11, score-0.886]
</p><p>9 DifficultyIt is not clear whether this problem is solvable or not. [sent-12, score-0.343]
</p><p>10 A proof that it is not solvable would be extremely interesting, and even partial success one way or another could be important. [sent-13, score-0.659]
</p><p>11 ImpactAt the theoretical level, it would be very nice to know if the ability to generalize implies the ability to solve reinforcement learning because (in a general sense) all problems can be cast as reinforcement learning. [sent-14, score-1.754]
</p><p>12 Even if the solution is exponential in the horizon time it can only motivate relaxations of the core algorithm likeRLgen. [sent-15, score-0.632]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('reinforcement', 0.509), ('posed', 0.388), ('classification', 0.241), ('generative', 0.224), ('solvable', 0.203), ('direct', 0.181), ('experience', 0.166), ('algorithm', 0.13), ('generalize', 0.122), ('relaxations', 0.122), ('level', 0.117), ('horizon', 0.116), ('ability', 0.115), ('intuitive', 0.112), ('motivate', 0.112), ('model', 0.11), ('constructing', 0.105), ('rewards', 0.102), ('solve', 0.098), ('sum', 0.094), ('slightly', 0.094), ('partial', 0.092), ('representations', 0.09), ('exponential', 0.085), ('recent', 0.084), ('stronger', 0.084), ('labeled', 0.084), ('nice', 0.08), ('problem', 0.079), ('extremely', 0.079), ('proof', 0.078), ('assumption', 0.077), ('inherently', 0.077), ('access', 0.074), ('learning', 0.073), ('definition', 0.072), ('even', 0.071), ('regret', 0.069), ('would', 0.069), ('success', 0.067), ('core', 0.067), ('observation', 0.066), ('solved', 0.066), ('classifier', 0.066), ('state', 0.066), ('expected', 0.066), ('exists', 0.065), ('theoretical', 0.064), ('trying', 0.061), ('whether', 0.061)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="27-tfidf-1" href="../hunch_net-2005/hunch_net-2005-02-23-Problem%3A_Reinforcement_Learning_with_Classification.html">27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</a></p>
<p>Introduction: At an intuitive level, the question here is "Can reinforcement learning be
solved with classification?"ProblemConstruct a reinforcement learning
algorithm with near-optimal expected sum of rewards in thedirect experience
modelgiven access to a classifier learning algorithm which has a small error
rate or regret on all posed classification problems. The definition of "posed"
here is slightly murky. I consider a problem "posed" if there is an algorithm
for constructing labeled classification examples.Past WorkThere exists
areduction of reinforcement learning to classification given a generative
model.A generative model is an inherently stronger assumption than the direct
experience model.Otherwork on learning reductionsmay be important.Several
algorithms for solving reinforcement learning in the direct experience model
exist. Most, such asE3,Factored-E3, andmetric-E3andRmaxrequire that the
observation be the state. Recent workextends this approach to POMDPs.This
problem is related topred</p><p>2 0.238757 <a title="27-tfidf-2" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>Introduction: What?Reductions are machines which turn solvers for one problem into solvers
for another problem.Why?Reductions are useful for several reasons.Laziness.
Reducing a problem to classification make at least 10 learning algorithms
available to solve a problem. Inventing 10 learning algorithms is quite a bit
of work. Similarly, programming a reduction is often trivial, while
programming a learning algorithm is a great deal of work.Crystallization. The
problems we often want to solve in learning are worst-case-impossible, but
average case feasible. By reducing all problems onto one or a few primitives,
we can fine tune these primitives to perform well on real-world problems with
greater precision due to the greater number of problems to validate
on.Theoretical Organization. By studying what reductions are easy vs. hard vs.
impossible, we can learn which problems are roughly equivalent in difficulty
and which are much harder.What we know now.Typesafe reductions. In the
beginning, there was th</p><p>3 0.22141683 <a title="27-tfidf-3" href="../hunch_net-2005/hunch_net-2005-08-18-SVM_Adaptability.html">103 hunch net-2005-08-18-SVM Adaptability</a></p>
<p>Introduction: Several recent papers have shown that SVM-like optimizations can be used to
handle several large family loss functions.This is a good thing because it is
implausible that thelossfunction imposed by the world can not be taken into
account in the process of solving a prediction problem. Even people used to
the hard-coreBayesianapproach to learning often note that some approximations
are almost inevitable in specifying apriorand/or integrating to achieve a
posterior. Taking into account how the system will be evaluated can allow both
computational effort and design effort to be focused so as to improve
performance.A current laundry list of capabilities includes:2002multiclass SVM
including arbitrary cost matricesICML 2003Hidden Markov ModelsNIPS 2003Markov
Networks(see somediscussion)EMNLP 2004Context free grammarsICML 2004Any loss
(with much computation)ICML 2005Anyconstrained linear prediction model(that's
my own name).ICML 2005Any loss dependent on a contingency tableI am personally
in</p><p>4 0.20703514 <a title="27-tfidf-4" href="../hunch_net-2005/hunch_net-2005-08-04-Why_Reinforcement_Learning_is_Important.html">100 hunch net-2005-08-04-Why Reinforcement Learning is Important</a></p>
<p>Introduction: One prescription for solving a problem well is:State the problem, in the
simplest way possible. In particular, this statement should involve no
contamination with or anticipation of the solution.Think about solutions to
the stated problem.Stating a problem in a succinct and crisp manner tends to
invite a simple elegant solution. When a problem can not be stated succinctly,
we wonder if the problem is even understood. (And when a problem is not
understood, we wonder if a solution can be meaningful.)Reinforcement learning
does step (1) well. It provides a clean simple language to state general AI
problems. In reinforcement learning there is a set of actionsA, a set of
observationsO, and a rewardr. The reinforcement learning problem, in general,
is defined by a conditional measureD( o, r | (o,r,a)*)which produces an
observationoand a rewardrgiven a history(o,r,a)*. The goal in reinforcement
learning is to find a policypi:(o,r,a)*-> amapping histories to actions so as
to maximize (or appro</p><p>5 0.20342104 <a title="27-tfidf-5" href="../hunch_net-2006/hunch_net-2006-01-13-Benchmarks_for_RL.html">148 hunch net-2006-01-13-Benchmarks for RL</a></p>
<p>Introduction: A couple years ago, Drew Bagnell and I started theRLBench projectto setup a
suite of reinforcement learning benchmark problems. We haven't been able to
touch it (due to lack of time) for a year so the project is on hold. Luckily,
there are several other projects such asCLSquareandRL-Gluewith a similar goal,
and we strongly endorse their continued development.I would like to explain
why, especially in the context of criticism of other learning benchmarks. For
example, sometimes theUCI Machine Learning Repositoryis criticized. There are
two criticisms I know of:Learning algorithms have overfit to the problems in
the repository. It is easy to imagine a mechanism for this happening
unintentionally. Strong evidence of this would be provided by learning
algorithms which perform great on the UCI machine learning repository but very
badly (relative to other learning algorithms) on non-UCI learning problems. I
have seen little evidence of this but it remains a point of concern. There is
a natur</p><p>6 0.18320437 <a title="27-tfidf-6" href="../hunch_net-2005/hunch_net-2005-01-24-Holy_grails_of_machine_learning%3F.html">2 hunch net-2005-01-24-Holy grails of machine learning?</a></p>
<p>7 0.13859747 <a title="27-tfidf-7" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>8 0.13319781 <a title="27-tfidf-8" href="../hunch_net-2005/hunch_net-2005-08-08-Apprenticeship_Reinforcement_Learning_for_Control.html">101 hunch net-2005-08-08-Apprenticeship Reinforcement Learning for Control</a></p>
<p>9 0.12237751 <a title="27-tfidf-9" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>10 0.12144391 <a title="27-tfidf-10" href="../hunch_net-2006/hunch_net-2006-07-13-Regression_vs._Classification_as_a_Primitive.html">196 hunch net-2006-07-13-Regression vs. Classification as a Primitive</a></p>
<p>11 0.1067825 <a title="27-tfidf-11" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>12 0.10493692 <a title="27-tfidf-12" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>13 0.1028473 <a title="27-tfidf-13" href="../hunch_net-2006/hunch_net-2006-04-05-What_is_state%3F.html">169 hunch net-2006-04-05-What is state?</a></p>
<p>14 0.10245641 <a title="27-tfidf-14" href="../hunch_net-2005/hunch_net-2005-02-03-Learning_Theory%2C_by_assumption.html">12 hunch net-2005-02-03-Learning Theory, by assumption</a></p>
<p>15 0.10186961 <a title="27-tfidf-15" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>16 0.10011312 <a title="27-tfidf-16" href="../hunch_net-2005/hunch_net-2005-05-16-Regret_minimizing_vs_error_limiting_reductions.html">72 hunch net-2005-05-16-Regret minimizing vs error limiting reductions</a></p>
<p>17 0.099127978 <a title="27-tfidf-17" href="../hunch_net-2006/hunch_net-2006-07-08-Some_recent_papers.html">192 hunch net-2006-07-08-Some recent papers</a></p>
<p>18 0.09709195 <a title="27-tfidf-18" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>19 0.094498746 <a title="27-tfidf-19" href="../hunch_net-2005/hunch_net-2005-01-24-The_Humanloop_Spectrum_of_Machine_Learning.html">3 hunch net-2005-01-24-The Humanloop Spectrum of Machine Learning</a></p>
<p>20 0.093325034 <a title="27-tfidf-20" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.203), (1, -0.149), (2, -0.047), (3, -0.01), (4, -0.043), (5, -0.166), (6, -0.034), (7, 0.052), (8, 0.028), (9, 0.068), (10, 0.058), (11, -0.025), (12, 0.117), (13, 0.103), (14, 0.066), (15, 0.124), (16, -0.001), (17, -0.09), (18, -0.048), (19, 0.029), (20, -0.002), (21, -0.054), (22, -0.019), (23, 0.071), (24, -0.117), (25, -0.062), (26, 0.045), (27, 0.114), (28, -0.045), (29, 0.126), (30, -0.129), (31, -0.012), (32, -0.031), (33, 0.049), (34, -0.097), (35, 0.025), (36, 0.033), (37, -0.061), (38, 0.019), (39, -0.046), (40, -0.017), (41, -0.135), (42, 0.001), (43, 0.051), (44, -0.026), (45, 0.035), (46, -0.043), (47, -0.088), (48, -0.022), (49, -0.06)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95644212 <a title="27-lsi-1" href="../hunch_net-2005/hunch_net-2005-02-23-Problem%3A_Reinforcement_Learning_with_Classification.html">27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</a></p>
<p>Introduction: At an intuitive level, the question here is "Can reinforcement learning be
solved with classification?"ProblemConstruct a reinforcement learning
algorithm with near-optimal expected sum of rewards in thedirect experience
modelgiven access to a classifier learning algorithm which has a small error
rate or regret on all posed classification problems. The definition of "posed"
here is slightly murky. I consider a problem "posed" if there is an algorithm
for constructing labeled classification examples.Past WorkThere exists
areduction of reinforcement learning to classification given a generative
model.A generative model is an inherently stronger assumption than the direct
experience model.Otherwork on learning reductionsmay be important.Several
algorithms for solving reinforcement learning in the direct experience model
exist. Most, such asE3,Factored-E3, andmetric-E3andRmaxrequire that the
observation be the state. Recent workextends this approach to POMDPs.This
problem is related topred</p><p>2 0.76589662 <a title="27-lsi-2" href="../hunch_net-2005/hunch_net-2005-08-04-Why_Reinforcement_Learning_is_Important.html">100 hunch net-2005-08-04-Why Reinforcement Learning is Important</a></p>
<p>Introduction: One prescription for solving a problem well is:State the problem, in the
simplest way possible. In particular, this statement should involve no
contamination with or anticipation of the solution.Think about solutions to
the stated problem.Stating a problem in a succinct and crisp manner tends to
invite a simple elegant solution. When a problem can not be stated succinctly,
we wonder if the problem is even understood. (And when a problem is not
understood, we wonder if a solution can be meaningful.)Reinforcement learning
does step (1) well. It provides a clean simple language to state general AI
problems. In reinforcement learning there is a set of actionsA, a set of
observationsO, and a rewardr. The reinforcement learning problem, in general,
is defined by a conditional measureD( o, r | (o,r,a)*)which produces an
observationoand a rewardrgiven a history(o,r,a)*. The goal in reinforcement
learning is to find a policypi:(o,r,a)*-> amapping histories to actions so as
to maximize (or appro</p><p>3 0.75151706 <a title="27-lsi-3" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>Introduction: What?Reductions are machines which turn solvers for one problem into solvers
for another problem.Why?Reductions are useful for several reasons.Laziness.
Reducing a problem to classification make at least 10 learning algorithms
available to solve a problem. Inventing 10 learning algorithms is quite a bit
of work. Similarly, programming a reduction is often trivial, while
programming a learning algorithm is a great deal of work.Crystallization. The
problems we often want to solve in learning are worst-case-impossible, but
average case feasible. By reducing all problems onto one or a few primitives,
we can fine tune these primitives to perform well on real-world problems with
greater precision due to the greater number of problems to validate
on.Theoretical Organization. By studying what reductions are easy vs. hard vs.
impossible, we can learn which problems are roughly equivalent in difficulty
and which are much harder.What we know now.Typesafe reductions. In the
beginning, there was th</p><p>4 0.68283081 <a title="27-lsi-4" href="../hunch_net-2005/hunch_net-2005-08-08-Apprenticeship_Reinforcement_Learning_for_Control.html">101 hunch net-2005-08-08-Apprenticeship Reinforcement Learning for Control</a></p>
<p>Introduction: Pieter Abbeelpresented a paper withAndrew NgatICMLonExploration and
Apprenticeship Learning in Reinforcement Learning. The basic idea of this
algorithm is:Collect data from a human controlling a machine.Build a
transition model based upon the experience.Build a policy which optimizes the
transition model.Evaluate the policy. If it works well, halt, otherwise add
the experience into the pool and go to (2).The paper proves that this
technique will converge to some policy with expected performance near human
expected performance assuming the world fits certain assumptions (MDP or
linear dynamics).This general idea of apprenticeship learning (i.e.
incorporating data from an expert) seems very compelling because (a) humans
often learn this way and (b) much harder problems can be solved. For (a), the
notion of teaching is about transferring knowledge from an expert to novices,
often via demonstration. To see (b), note that we can create intricate
reinforcement learning problems where a parti</p><p>5 0.67826003 <a title="27-lsi-5" href="../hunch_net-2005/hunch_net-2005-08-18-SVM_Adaptability.html">103 hunch net-2005-08-18-SVM Adaptability</a></p>
<p>Introduction: Several recent papers have shown that SVM-like optimizations can be used to
handle several large family loss functions.This is a good thing because it is
implausible that thelossfunction imposed by the world can not be taken into
account in the process of solving a prediction problem. Even people used to
the hard-coreBayesianapproach to learning often note that some approximations
are almost inevitable in specifying apriorand/or integrating to achieve a
posterior. Taking into account how the system will be evaluated can allow both
computational effort and design effort to be focused so as to improve
performance.A current laundry list of capabilities includes:2002multiclass SVM
including arbitrary cost matricesICML 2003Hidden Markov ModelsNIPS 2003Markov
Networks(see somediscussion)EMNLP 2004Context free grammarsICML 2004Any loss
(with much computation)ICML 2005Anyconstrained linear prediction model(that's
my own name).ICML 2005Any loss dependent on a contingency tableI am personally
in</p><p>6 0.6495837 <a title="27-lsi-6" href="../hunch_net-2005/hunch_net-2005-01-24-Holy_grails_of_machine_learning%3F.html">2 hunch net-2005-01-24-Holy grails of machine learning?</a></p>
<p>7 0.61735839 <a title="27-lsi-7" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>8 0.61472368 <a title="27-lsi-8" href="../hunch_net-2006/hunch_net-2006-01-13-Benchmarks_for_RL.html">148 hunch net-2006-01-13-Benchmarks for RL</a></p>
<p>9 0.59832293 <a title="27-lsi-9" href="../hunch_net-2006/hunch_net-2006-05-01-A_conversation_between_Theo_and_Pat.html">176 hunch net-2006-05-01-A conversation between Theo and Pat</a></p>
<p>10 0.59473944 <a title="27-lsi-10" href="../hunch_net-2005/hunch_net-2005-06-17-Reopening_RL-%3EClassification.html">82 hunch net-2005-06-17-Reopening RL->Classification</a></p>
<p>11 0.58599919 <a title="27-lsi-11" href="../hunch_net-2006/hunch_net-2006-04-05-What_is_state%3F.html">169 hunch net-2006-04-05-What is state?</a></p>
<p>12 0.53949404 <a title="27-lsi-12" href="../hunch_net-2005/hunch_net-2005-02-12-ROC_vs._Accuracy_vs._AROC.html">18 hunch net-2005-02-12-ROC vs. Accuracy vs. AROC</a></p>
<p>13 0.53630733 <a title="27-lsi-13" href="../hunch_net-2005/hunch_net-2005-01-24-The_Humanloop_Spectrum_of_Machine_Learning.html">3 hunch net-2005-01-24-The Humanloop Spectrum of Machine Learning</a></p>
<p>14 0.51926243 <a title="27-lsi-14" href="../hunch_net-2005/hunch_net-2005-02-26-Problem%3A_Reductions_and_Relative_Ranking_Metrics.html">31 hunch net-2005-02-26-Problem: Reductions and Relative Ranking Metrics</a></p>
<p>15 0.5158478 <a title="27-lsi-15" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>16 0.50832129 <a title="27-lsi-16" href="../hunch_net-2007/hunch_net-2007-10-19-Second_Annual_Reinforcement_Learning_Competition.html">268 hunch net-2007-10-19-Second Annual Reinforcement Learning Competition</a></p>
<p>17 0.49906579 <a title="27-lsi-17" href="../hunch_net-2005/hunch_net-2005-05-06-Don%26%238217%3Bt_mix_the_solution_into_the_problem.html">67 hunch net-2005-05-06-Don&#8217;t mix the solution into the problem</a></p>
<p>18 0.49143386 <a title="27-lsi-18" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>19 0.489629 <a title="27-lsi-19" href="../hunch_net-2010/hunch_net-2010-03-15-The_Efficient_Robust_Conditional_Probability_Estimation_Problem.html">391 hunch net-2010-03-15-The Efficient Robust Conditional Probability Estimation Problem</a></p>
<p>20 0.4891949 <a title="27-lsi-20" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(35, 0.012), (42, 0.436), (50, 0.011), (68, 0.047), (74, 0.076), (91, 0.273), (95, 0.024)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.97602892 <a title="27-lda-1" href="../hunch_net-2007/hunch_net-2007-05-09-The_Missing_Bound.html">244 hunch net-2007-05-09-The Missing Bound</a></p>
<p>Introduction: Sham Kakadepoints out that we are missing a bound.Suppose we
havemsamplesxdrawn IID from some distributionD. Through the magic of
exponential moment method we know that:If the range ofxis bounded by an
interval of sizeI, aChernoff/Hoeffding style boundgives us a bound on the
deviations likeO(I/m0.5)(at least in crude form). A proof is on page 9here.If
the range ofxis bounded, and the variance (or a bound on the variance) is
known, thenBennett's boundcan give tighter results (*). This can be a huge
improvment when the true variance small.What's missing here is a bound that
depends on the observed variance rather than a bound on the variance. This
means that many people attempt to use Bennett's bound (incorrectly) by
plugging the observed variance in as the true variance, invalidating the bound
application. Most of the time, they get away with it, but this is a dangerous
move when doing machine learning. In machine learning, we are typically trying
to find a predictor with 0 expected los</p><p>2 0.97055465 <a title="27-lda-2" href="../hunch_net-2009/hunch_net-2009-01-07-Interesting_Papers_at_SODA_2009.html">334 hunch net-2009-01-07-Interesting Papers at SODA 2009</a></p>
<p>Introduction: Several talks seem potentially interesting to ML folks at this year's SODA
.Maria-Florina Balcan,Avrim Blum, andAnupam Gupta,Approximate Clustering
without the Approximation. This paper gives reasonable algorithms with
provable approximation guarantees for k-median and other notions of
clustering. It's conceptually interesting, because it's the second example
I've seen where NP hardness is subverted by changing the problem definition
subtle but reasonable way. Essentially, they show that if any near-
approximation to an optimal solution is good, then it's computationally easy
to find a near-optimal solution. This subtle shift bears serious thought. A
similar one occurred inour ranking paperwith respect to minimum feedback
arcset. With two known examples, it suggests that many more NP-complete
problems might be finessed into irrelevance in this style.Yury
LifshitsandShengyu Zhang,Combinatorial Algorithms for Nearest Neighbors, Near-
Duplicates, and Small-World Design. The basic idea of</p><p>3 0.9681167 <a title="27-lda-3" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>Introduction: This post is about a general technique for problem solving which I've never
seen taught (in full generality), but which I've found very useful.Many
problems in computer science turn out to be discretely difficult. The best
known version of such problems are NP-hard problems, but I mean 'discretely
difficult' in a much more general way, which I only know how to capture by
examples.ERMIn empirical risk minimization, you choose a minimum error rate
classifier from a set of classifiers. This is NP hard for common sets, but it
can be much harder, depending on the set.ExpertsIn the online learning with
experts setting, you try to predict well so as to compete with a set of
(adversarial) experts. Here the alternating quantifiers of you and an
adversary playing out a game can yield a dynamic programming problem that
grows exponentially.Policy IterationThe problem with policy iteration is that
you learn a new policy with respect to an old policy, which implies that
simply adopting the new polic</p><p>4 0.94316918 <a title="27-lda-4" href="../hunch_net-2006/hunch_net-2006-01-18-Is_Multitask_Learning_Black-Boxable%3F.html">149 hunch net-2006-01-18-Is Multitask Learning Black-Boxable?</a></p>
<p>Introduction: Multitask learning is the learning to predict multiple outputs given the same
input. Mathematically, we might think of this as trying to learn a functionf:X
-> {0,1}n. Structured learning is similar at this level of abstraction. Many
people have worked on solving multitask learning (for exampleRich Caruana)
using methods which share an internal representation. On other words, the the
computation and learning of theith prediction is shared with the computation
and learning of thejth prediction. Another way to ask this question is: can we
avoid sharing the internal representation?For example, itmightbe feasible to
solve multitask learning by some process feeding theith predictionf(x)iinto
thejth predictorf(x,f(x)i)j,If the answer is "no", then it implies we can not
take binary classification as a basic primitive in the process of solving
prediction problems. If the answer is "yes", then we can reuse binary
classification algorithms to solve multitask learning problems.Finding a
satisfyin</p><p>same-blog 5 0.92435712 <a title="27-lda-5" href="../hunch_net-2005/hunch_net-2005-02-23-Problem%3A_Reinforcement_Learning_with_Classification.html">27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</a></p>
<p>Introduction: At an intuitive level, the question here is "Can reinforcement learning be
solved with classification?"ProblemConstruct a reinforcement learning
algorithm with near-optimal expected sum of rewards in thedirect experience
modelgiven access to a classifier learning algorithm which has a small error
rate or regret on all posed classification problems. The definition of "posed"
here is slightly murky. I consider a problem "posed" if there is an algorithm
for constructing labeled classification examples.Past WorkThere exists
areduction of reinforcement learning to classification given a generative
model.A generative model is an inherently stronger assumption than the direct
experience model.Otherwork on learning reductionsmay be important.Several
algorithms for solving reinforcement learning in the direct experience model
exist. Most, such asE3,Factored-E3, andmetric-E3andRmaxrequire that the
observation be the state. Recent workextends this approach to POMDPs.This
problem is related topred</p><p>6 0.9173277 <a title="27-lda-6" href="../hunch_net-2007/hunch_net-2007-12-21-Vowpal_Wabbit_Code_Release.html">281 hunch net-2007-12-21-Vowpal Wabbit Code Release</a></p>
<p>7 0.87692273 <a title="27-lda-7" href="../hunch_net-2007/hunch_net-2007-11-05-CMU_wins_DARPA_Urban_Challenge.html">271 hunch net-2007-11-05-CMU wins DARPA Urban Challenge</a></p>
<p>8 0.864815 <a title="27-lda-8" href="../hunch_net-2011/hunch_net-2011-10-24-2011_ML_symposium_and_the_bears.html">448 hunch net-2011-10-24-2011 ML symposium and the bears</a></p>
<p>9 0.85405153 <a title="27-lda-9" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>10 0.84749162 <a title="27-lda-10" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<p>11 0.84692389 <a title="27-lda-11" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>12 0.84661061 <a title="27-lda-12" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>13 0.84461921 <a title="27-lda-13" href="../hunch_net-2005/hunch_net-2005-07-07-The_Limits_of_Learning_Theory.html">90 hunch net-2005-07-07-The Limits of Learning Theory</a></p>
<p>14 0.84415191 <a title="27-lda-14" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>15 0.84396005 <a title="27-lda-15" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>16 0.84363151 <a title="27-lda-16" href="../hunch_net-2005/hunch_net-2005-03-29-Academic_Mechanism_Design.html">48 hunch net-2005-03-29-Academic Mechanism Design</a></p>
<p>17 0.84218359 <a title="27-lda-17" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>18 0.84156173 <a title="27-lda-18" href="../hunch_net-2006/hunch_net-2006-06-15-IJCAI_is_out_of_season.html">184 hunch net-2006-06-15-IJCAI is out of season</a></p>
<p>19 0.83917594 <a title="27-lda-19" href="../hunch_net-2005/hunch_net-2005-05-06-Don%26%238217%3Bt_mix_the_solution_into_the_problem.html">67 hunch net-2005-05-06-Don&#8217;t mix the solution into the problem</a></p>
<p>20 0.83896708 <a title="27-lda-20" href="../hunch_net-2010/hunch_net-2010-08-21-Rob_Schapire_at_NYC_ML_Meetup.html">405 hunch net-2010-08-21-Rob Schapire at NYC ML Meetup</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
