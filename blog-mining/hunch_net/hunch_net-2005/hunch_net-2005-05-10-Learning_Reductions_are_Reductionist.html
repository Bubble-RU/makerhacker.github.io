<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>68 hunch net-2005-05-10-Learning Reductions are Reductionist</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-68" href="#">hunch_net-2005-68</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>68 hunch net-2005-05-10-Learning Reductions are Reductionist</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-68-html" href="http://hunch.net/?p=73">html</a></p><p>Introduction: This is about a fundamental motivation for the investigation of reductions in
learning. It applies to many pieces of work other than my own.The reductionist
approach to problem solving is characterized by taking a problem, decomposing
it into as-small-as-possible subproblems, discovering how to solve the
subproblems, and then discovering how to use the solutions to the subproblems
to solve larger problems. The reductionist approach to solving problems has
often payed offverywell. Computer science related examples of the reductionist
approach include:Reducing computation to the transistor. All of our CPUs are
built from transistors.Reducing rendering of images to rendering a triangle
(or other simple polygons). Computers can now render near-realistic scenes in
real time. The big breakthrough came from learning how to render many
triangles quickly.This approach to problem solving extends well beyond
computer science. Many fields of science focus on theories making predictions
about very</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 This is about a fundamental motivation for the investigation of reductions in learning. [sent-1, score-0.273]
</p><p>2 It applies to many pieces of work other than my own. [sent-2, score-0.141]
</p><p>3 The reductionist approach to problem solving is characterized by taking a problem, decomposing it into as-small-as-possible subproblems, discovering how to solve the subproblems, and then discovering how to use the solutions to the subproblems to solve larger problems. [sent-3, score-1.845]
</p><p>4 The reductionist approach to solving problems has often payed offverywell. [sent-4, score-0.948]
</p><p>5 Computer science related examples of the reductionist approach include:Reducing computation to the transistor. [sent-5, score-0.688]
</p><p>6 Reducing rendering of images to rendering a triangle (or other simple polygons). [sent-7, score-0.79]
</p><p>7 Computers can now render near-realistic scenes in real time. [sent-8, score-0.354]
</p><p>8 The big breakthrough came from learning how to render many triangles quickly. [sent-9, score-0.43]
</p><p>9 This approach to problem solving extends well beyond computer science. [sent-10, score-0.707]
</p><p>10 Many fields of science focus on theories making predictions about very simple systems. [sent-11, score-0.529]
</p><p>11 These predictions are then composed to make predictions about where space craft go, how large a cannonball needs to be, etcâ&euro;Ś Obviously this approach has been quite successful. [sent-12, score-0.658]
</p><p>12 It is an open question whether or not this approach can really succeed at learning. [sent-13, score-0.356]
</p><p>13 Against: We know that succesful learning requires the incorporation of prior knowledge in fairly arbitrary forms. [sent-14, score-0.79]
</p><p>14 This suggests that we can not easily decompose the process of learning. [sent-15, score-0.099]
</p><p>15 For: We know that humans can succeed at general purpose learning. [sent-16, score-0.375]
</p><p>16 It may be that arbitrary prior knowledge is required to solve arbitrary learning problems, but perhaps there are specific learning algorithms incorporating specific prior knowledge capable of solving the specific problems we encounter. [sent-17, score-2.057]
</p><p>17 We don't yet have a good comparison of how well they work with other approaches. [sent-19, score-0.07]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('reductionist', 0.382), ('render', 0.255), ('rendering', 0.255), ('subproblems', 0.247), ('approach', 0.207), ('arbitrary', 0.198), ('specific', 0.184), ('predictions', 0.173), ('solving', 0.169), ('knowledge', 0.169), ('discovering', 0.16), ('prior', 0.156), ('succeed', 0.149), ('solve', 0.119), ('reductions', 0.115), ('cpus', 0.113), ('decomposing', 0.113), ('payed', 0.113), ('triangle', 0.113), ('breakthrough', 0.105), ('characterized', 0.105), ('composed', 0.105), ('incorporation', 0.105), ('theories', 0.105), ('computer', 0.101), ('science', 0.099), ('decompose', 0.099), ('extends', 0.099), ('scenes', 0.099), ('incorporating', 0.094), ('know', 0.09), ('images', 0.085), ('investigation', 0.085), ('simple', 0.082), ('problems', 0.077), ('built', 0.076), ('applies', 0.073), ('motivation', 0.073), ('computers', 0.072), ('succesful', 0.072), ('comparison', 0.07), ('came', 0.07), ('fields', 0.07), ('reducing', 0.069), ('humans', 0.069), ('pieces', 0.068), ('purpose', 0.067), ('beyond', 0.067), ('obviously', 0.065), ('problem', 0.064)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="68-tfidf-1" href="../hunch_net-2005/hunch_net-2005-05-10-Learning_Reductions_are_Reductionist.html">68 hunch net-2005-05-10-Learning Reductions are Reductionist</a></p>
<p>Introduction: This is about a fundamental motivation for the investigation of reductions in
learning. It applies to many pieces of work other than my own.The reductionist
approach to problem solving is characterized by taking a problem, decomposing
it into as-small-as-possible subproblems, discovering how to solve the
subproblems, and then discovering how to use the solutions to the subproblems
to solve larger problems. The reductionist approach to solving problems has
often payed offverywell. Computer science related examples of the reductionist
approach include:Reducing computation to the transistor. All of our CPUs are
built from transistors.Reducing rendering of images to rendering a triangle
(or other simple polygons). Computers can now render near-realistic scenes in
real time. The big breakthrough came from learning how to render many
triangles quickly.This approach to problem solving extends well beyond
computer science. Many fields of science focus on theories making predictions
about very</p><p>2 0.1456579 <a title="68-tfidf-2" href="../hunch_net-2006/hunch_net-2006-03-05-%26%238220%3BStructural%26%238221%3B_Learning.html">161 hunch net-2006-03-05-&#8220;Structural&#8221; Learning</a></p>
<p>Introduction: Fernando Pereirapointed outAndo andZhang'spaperon "structural" learning.
Structural learning is multitask learning on subproblems created from
unlabeled data.The basic idea is to take a look at the unlabeled data and
create many supervised problems. On text data, which they test on, these
subproblems might be of the form "Given surrounding words predict the middle
word". The hope here is that successfully predicting on these subproblems is
relevant to the prediction of your core problem.In the long run, the precise
mechanism used (essentially, linear predictors with parameters tied by a
common matrix) and the precise problems formed may not be critical. What seems
critical is that the hope is realized: the technique provides a significant
edge in practice.Some basic questions about this approach are:Are there
effective automated mechanisms for creating the subproblems?Is it necessary to
use a shared representation?</p><p>3 0.1268063 <a title="68-tfidf-3" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>Introduction: One of the remarkable things about machine learning is how diverse it is. The
viewpoints of Bayesian learning, reinforcement learning, graphical models,
supervised learning, unsupervised learning, genetic programming, etc… share
little enough overlap that many people can and do make their careers within
one without touching, or even necessarily understanding the others.There are
two fundamental reasons why this is possible.For many problems, many
approaches work in the sense that they do something useful. This is true
empirically, where for many problems we can observe that many different
approaches yield better performance than any constant predictor. It's also
true in theory, where we know that for any set of predictors representable in
a finite amount of RAM, minimizing training error over the set of predictors
does something nontrivial when there are a sufficient number of examples.There
is nothing like a unifying problem defining the field. In many other areas
there are unifying p</p><p>4 0.12671174 <a title="68-tfidf-4" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>Introduction: What?Reductions are machines which turn solvers for one problem into solvers
for another problem.Why?Reductions are useful for several reasons.Laziness.
Reducing a problem to classification make at least 10 learning algorithms
available to solve a problem. Inventing 10 learning algorithms is quite a bit
of work. Similarly, programming a reduction is often trivial, while
programming a learning algorithm is a great deal of work.Crystallization. The
problems we often want to solve in learning are worst-case-impossible, but
average case feasible. By reducing all problems onto one or a few primitives,
we can fine tune these primitives to perform well on real-world problems with
greater precision due to the greater number of problems to validate
on.Theoretical Organization. By studying what reductions are easy vs. hard vs.
impossible, we can learn which problems are roughly equivalent in difficulty
and which are much harder.What we know now.Typesafe reductions. In the
beginning, there was th</p><p>5 0.12580399 <a title="68-tfidf-5" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>Introduction: Attempts to abstract and study machine learning are within some given
framework or mathematical model. It turns out that all of these models are
significantly flawed for the purpose of studying machine learning. I've
created a table (below) outlining the major flaws in some common models of
machine learning.The point here is not simply "woe unto us". There are several
implications which seem important.The multitude of models is a point of
continuing confusion. It is common for people to learn about machine learning
within one framework which often becomes there "home framework" through which
they attempt to filter all machine learning. (Have you met people who can only
think in terms of kernels? Only via Bayes Law? Only via PAC Learning?)
Explicitly understanding the existence of these other frameworks can help
resolve the confusion. This is particularly important when reviewing and
particularly important for students.Algorithms which conform to multiple
approaches can have substantial</p><p>6 0.11577217 <a title="68-tfidf-6" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>7 0.11523017 <a title="68-tfidf-7" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>8 0.10856166 <a title="68-tfidf-8" href="../hunch_net-2007/hunch_net-2007-01-15-The_Machine_Learning_Department.html">228 hunch net-2007-01-15-The Machine Learning Department</a></p>
<p>9 0.10654031 <a title="68-tfidf-9" href="../hunch_net-2005/hunch_net-2005-02-25-Problem%3A_Online_Learning.html">28 hunch net-2005-02-25-Problem: Online Learning</a></p>
<p>10 0.10380827 <a title="68-tfidf-10" href="../hunch_net-2005/hunch_net-2005-09-14-The_Predictionist_Viewpoint.html">112 hunch net-2005-09-14-The Predictionist Viewpoint</a></p>
<p>11 0.10220451 <a title="68-tfidf-11" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>12 0.094926156 <a title="68-tfidf-12" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>13 0.094529822 <a title="68-tfidf-13" href="../hunch_net-2006/hunch_net-2006-04-02-Mad_%28Neuro%29science.html">168 hunch net-2006-04-02-Mad (Neuro)science</a></p>
<p>14 0.088532351 <a title="68-tfidf-14" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<p>15 0.088277362 <a title="68-tfidf-15" href="../hunch_net-2005/hunch_net-2005-04-23-Advantages_and_Disadvantages_of_Bayesian_Learning.html">60 hunch net-2005-04-23-Advantages and Disadvantages of Bayesian Learning</a></p>
<p>16 0.087225452 <a title="68-tfidf-16" href="../hunch_net-2009/hunch_net-2009-06-03-Functionally_defined_Nonlinear_Dynamic_Models.html">359 hunch net-2009-06-03-Functionally defined Nonlinear Dynamic Models</a></p>
<p>17 0.086750619 <a title="68-tfidf-17" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>18 0.086225845 <a title="68-tfidf-18" href="../hunch_net-2005/hunch_net-2005-09-08-Online_Learning_as_the_Mathematics_of_Accountability.html">109 hunch net-2005-09-08-Online Learning as the Mathematics of Accountability</a></p>
<p>19 0.08584775 <a title="68-tfidf-19" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>20 0.084213249 <a title="68-tfidf-20" href="../hunch_net-2005/hunch_net-2005-08-04-Why_Reinforcement_Learning_is_Important.html">100 hunch net-2005-08-04-Why Reinforcement Learning is Important</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.182), (1, -0.096), (2, 0.058), (3, -0.087), (4, -0.004), (5, -0.062), (6, -0.105), (7, 0.047), (8, -0.006), (9, 0.049), (10, -0.027), (11, -0.048), (12, 0.109), (13, -0.071), (14, 0.058), (15, 0.025), (16, 0.071), (17, -0.039), (18, 0.021), (19, 0.009), (20, -0.068), (21, 0.027), (22, -0.014), (23, 0.024), (24, -0.013), (25, -0.065), (26, -0.047), (27, -0.089), (28, -0.092), (29, -0.024), (30, 0.013), (31, -0.042), (32, -0.011), (33, 0.036), (34, 0.077), (35, -0.042), (36, 0.112), (37, -0.021), (38, -0.026), (39, 0.011), (40, -0.02), (41, -0.005), (42, -0.013), (43, 0.061), (44, -0.048), (45, -0.015), (46, -0.05), (47, 0.121), (48, -0.008), (49, -0.123)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96181011 <a title="68-lsi-1" href="../hunch_net-2005/hunch_net-2005-05-10-Learning_Reductions_are_Reductionist.html">68 hunch net-2005-05-10-Learning Reductions are Reductionist</a></p>
<p>Introduction: This is about a fundamental motivation for the investigation of reductions in
learning. It applies to many pieces of work other than my own.The reductionist
approach to problem solving is characterized by taking a problem, decomposing
it into as-small-as-possible subproblems, discovering how to solve the
subproblems, and then discovering how to use the solutions to the subproblems
to solve larger problems. The reductionist approach to solving problems has
often payed offverywell. Computer science related examples of the reductionist
approach include:Reducing computation to the transistor. All of our CPUs are
built from transistors.Reducing rendering of images to rendering a triangle
(or other simple polygons). Computers can now render near-realistic scenes in
real time. The big breakthrough came from learning how to render many
triangles quickly.This approach to problem solving extends well beyond
computer science. Many fields of science focus on theories making predictions
about very</p><p>2 0.6249783 <a title="68-lsi-2" href="../hunch_net-2006/hunch_net-2006-10-22-Exemplar_programming.html">215 hunch net-2006-10-22-Exemplar programming</a></p>
<p>Introduction: There are many different abstractions for problem definition and solution.
Here are a few examples:Functional programming: a set of functions are
defined. The composed execution of these functions yields the solution.Linear
programming: a set of constraints and a linear objective function are defined.
An LP solver finds the constrained optimum.Quadratic programming: Like linear
programming, but the language is a little more flexible (and the solution
slower).Convex programming: like quadratic programming, but the language is
more flexible (and the solutions even slower).Dynamic programming: a recursive
definition of the problem is defined and then solved efficiently via caching
tricks.SAT programming: A problem is specified as a satisfiability involving a
conjunction of a disjunction of boolean variables. A general engine attempts
to find a good satisfying assignment. For exampleKautz'sblackboxplanner.These
abstractions have different tradeoffs between ease of use, generality, and the</p><p>3 0.62340277 <a title="68-lsi-3" href="../hunch_net-2005/hunch_net-2005-06-17-Reopening_RL-%3EClassification.html">82 hunch net-2005-06-17-Reopening RL->Classification</a></p>
<p>Introduction: In research, it's often the case that solving a problem helps you realize that
it wasn't the right problem to solve. This is the case for the "reduce RL to
classification" problem with the solution hinted athereand turned into a
paperhere.The essential difficulty is that the method of stating and analyzing
reductions ends up being nonalgorithmic (unlike previous reductions) unless
you work with learning from teleoperated robots asGreg Grudicdoes. The
difficulty here is due to the reduction being dependent on the optimal policy
(which a human teleoperator might simulate, but which is otherwise
unavailable).So, thisproblemis "open" again with the caveat that this time we
want a more algorithmic solution.Whether or not this is feasible at all is
still unclear and evidence in either direction would greatly interest me. A
positive answer might have many practical implications in the long run.</p><p>4 0.60715598 <a title="68-lsi-4" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>Introduction: Researchers are typically confronted with big problems that they have no idea
how to solve. In trying to come up with a solution, a natural approach is to
decompose the big problem into a set of subproblems whose solution yields a
solution to the larger problem. This approach can go wrong in several
ways.Decomposition failure. The solution to the decomposition does not in fact
yield a solution to the overall problem.Artificial hardness. The subproblems
created are sufficient if solved to solve the overall problem, but they are
harder than necessary.As you can see, computational complexity forms a
relatively new (in research-history) razor by which to judge an approach
sufficient but not necessary.In my experience, the artificial hardness problem
is very common. Many researchers abdicate the responsibility of choosing a
problem to work on to other people. This process starts very naturally as a
graduate student, when an incoming student might have relatively little idea
about how to do</p><p>5 0.60627055 <a title="68-lsi-5" href="../hunch_net-2005/hunch_net-2005-08-04-Why_Reinforcement_Learning_is_Important.html">100 hunch net-2005-08-04-Why Reinforcement Learning is Important</a></p>
<p>Introduction: One prescription for solving a problem well is:State the problem, in the
simplest way possible. In particular, this statement should involve no
contamination with or anticipation of the solution.Think about solutions to
the stated problem.Stating a problem in a succinct and crisp manner tends to
invite a simple elegant solution. When a problem can not be stated succinctly,
we wonder if the problem is even understood. (And when a problem is not
understood, we wonder if a solution can be meaningful.)Reinforcement learning
does step (1) well. It provides a clean simple language to state general AI
problems. In reinforcement learning there is a set of actionsA, a set of
observationsO, and a rewardr. The reinforcement learning problem, in general,
is defined by a conditional measureD( o, r | (o,r,a)*)which produces an
observationoand a rewardrgiven a history(o,r,a)*. The goal in reinforcement
learning is to find a policypi:(o,r,a)*-> amapping histories to actions so as
to maximize (or appro</p><p>6 0.59410679 <a title="68-lsi-6" href="../hunch_net-2009/hunch_net-2009-06-01-Multitask_Poisoning.html">358 hunch net-2009-06-01-Multitask Poisoning</a></p>
<p>7 0.56601924 <a title="68-lsi-7" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>8 0.56119621 <a title="68-lsi-8" href="../hunch_net-2009/hunch_net-2009-06-03-Functionally_defined_Nonlinear_Dynamic_Models.html">359 hunch net-2009-06-03-Functionally defined Nonlinear Dynamic Models</a></p>
<p>9 0.55417591 <a title="68-lsi-9" href="../hunch_net-2006/hunch_net-2006-04-02-Mad_%28Neuro%29science.html">168 hunch net-2006-04-02-Mad (Neuro)science</a></p>
<p>10 0.55114895 <a title="68-lsi-10" href="../hunch_net-2005/hunch_net-2005-05-06-Don%26%238217%3Bt_mix_the_solution_into_the_problem.html">67 hunch net-2005-05-06-Don&#8217;t mix the solution into the problem</a></p>
<p>11 0.55002856 <a title="68-lsi-11" href="../hunch_net-2005/hunch_net-2005-04-21-Dynamic_Programming_Generalizations_and_Their_Use.html">58 hunch net-2005-04-21-Dynamic Programming Generalizations and Their Use</a></p>
<p>12 0.53407145 <a title="68-lsi-12" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>13 0.53081596 <a title="68-lsi-13" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>14 0.52685291 <a title="68-lsi-14" href="../hunch_net-2006/hunch_net-2006-05-01-A_conversation_between_Theo_and_Pat.html">176 hunch net-2006-05-01-A conversation between Theo and Pat</a></p>
<p>15 0.52457505 <a title="68-lsi-15" href="../hunch_net-2005/hunch_net-2005-07-07-The_Limits_of_Learning_Theory.html">90 hunch net-2005-07-07-The Limits of Learning Theory</a></p>
<p>16 0.52417976 <a title="68-lsi-16" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>17 0.51754731 <a title="68-lsi-17" href="../hunch_net-2012/hunch_net-2012-03-06-COLT-ICML_Open_Questions_and_ICML_Instructions.html">458 hunch net-2012-03-06-COLT-ICML Open Questions and ICML Instructions</a></p>
<p>18 0.51635087 <a title="68-lsi-18" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>19 0.51444805 <a title="68-lsi-19" href="../hunch_net-2005/hunch_net-2005-03-21-Research_Styles_in_Machine_Learning.html">44 hunch net-2005-03-21-Research Styles in Machine Learning</a></p>
<p>20 0.51251626 <a title="68-lsi-20" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(42, 0.274), (45, 0.014), (68, 0.036), (69, 0.02), (74, 0.054), (95, 0.496)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.99695802 <a title="68-lda-1" href="../hunch_net-2010/hunch_net-2010-11-18-ICML_2011_%26%238211%3B_Call_for_Tutorials.html">417 hunch net-2010-11-18-ICML 2011 &#8211; Call for Tutorials</a></p>
<p>Introduction: I would like to encourage people to consider giving a tutorial at next years
ICML. The ideal tutorial attracts a wide audience, provides a gentle and
easily taught introduction to the chosen research area, and also covers the
most important contributions in depth.Submissions are due January 14 Â (about
two weeks before paper
deadline).http://www.icml-2011.org/tutorials.phpRegards,Ulf</p><p>2 0.97991359 <a title="68-lda-2" href="../hunch_net-2007/hunch_net-2007-02-11-24.html">232 hunch net-2007-02-11-24</a></p>
<p>Introduction: To commemorate theTwenty Fourth Annual International Conference on Machine
Learning(ICML-07), the FOX Network has decided to launch a new spin-off series
in prime time. Through unofficial sources, I have obtained thestory arcfor the
first season, which appears frighteningly realistic.</p><p>3 0.96668124 <a title="68-lda-3" href="../hunch_net-2008/hunch_net-2008-07-04-More_Presentation_Preparation.html">307 hunch net-2008-07-04-More Presentation Preparation</a></p>
<p>Introduction: We've discussedpresentation preparation before, but I have one more thing to
add:transitioning. For a research presentation, it is substantially helpful
for the audience if transitions are clear. A common outline for a research
presentation in machine leanring is:The problem. Presentations which don't
describe the problem almost immediately lose people, because the context is
missing to understand the detail.Prior relevant work. In many cases, a paper
builds on some previous bit of work which must be understood in order to
understand what the paper does. A common failure mode seems to be spending too
much time on prior work. Discuss just the relevant aspects of prior work in
the language of your work. Sometimes this is missing when unneeded.What we
did. For theory papers in particular, it is often not possible to really cover
the details. Prioritizing what you present can be very important.How it
worked. Many papers in Machine Learning have some sort of experimental test of
the algorit</p><p>4 0.96363926 <a title="68-lda-4" href="../hunch_net-2009/hunch_net-2009-11-23-ICML_2009_Workshops_%28and_Tutorials%29.html">379 hunch net-2009-11-23-ICML 2009 Workshops (and Tutorials)</a></p>
<p>Introduction: I'm theworkshops chairforICMLthis year. As such, I would like to personally
encourage people to consider running a workshop.My general view of workshops
is that they are excellent as opportunities to discuss and develop research
directions--some of my best work has come from collaborations at workshops and
several workshops have substantially altered my thinking about various
problems. My experience running workshops is that setting them up and making
them fly often appears much harder than it actually is, and the workshops
often come off much better than expected in the end. Submissions are due
January 18, two weeks before papers.Similarly,Ben Taskaris looking for
goodtutorials, which is complementary. Workshops are about exploring a
subject, while a tutorial is about distilling it down into an easily taught
essence, a vital part of the research process. Tutorials are due February 13,
two weeks after papers.</p><p>5 0.94292855 <a title="68-lda-5" href="../hunch_net-2006/hunch_net-2006-01-08-Debugging_Your_Brain.html">147 hunch net-2006-01-08-Debugging Your Brain</a></p>
<p>Introduction: One part of doing research is debugging your understanding of reality. This is
hard work: How do you even discover where you misunderstand? If you discover a
misunderstanding, how do you go about removing it?The process of debugging
computer programs is quite analogous to debugging reality misunderstandings.
This is natural--a bug in a computer program is a misunderstanding between you
and the computer about what you said. Many of the familiar techniques from
debugging have exact parallels.DetailsWhen programming, there are often signs
that some bug exists like: "the graph my program output is shifted a little
bit" = maybe you have an indexing error. In debugging yourself, we often have
some impression that something is "not right". These impressions should be
addressed directly and immediately. (Some people have the habit of suppressing
worries in favor of excess certainty. That's not healthy for research.)Corner
CasesA "corner case" is an input to a program which is extreme in some w</p><p>6 0.92488194 <a title="68-lda-6" href="../hunch_net-2010/hunch_net-2010-12-02-Traffic_Prediction_Problem.html">418 hunch net-2010-12-02-Traffic Prediction Problem</a></p>
<p>same-blog 7 0.9204911 <a title="68-lda-7" href="../hunch_net-2005/hunch_net-2005-05-10-Learning_Reductions_are_Reductionist.html">68 hunch net-2005-05-10-Learning Reductions are Reductionist</a></p>
<p>8 0.89901817 <a title="68-lda-8" href="../hunch_net-2006/hunch_net-2006-10-22-Exemplar_programming.html">215 hunch net-2006-10-22-Exemplar programming</a></p>
<p>9 0.87840474 <a title="68-lda-9" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>10 0.81797987 <a title="68-lda-10" href="../hunch_net-2012/hunch_net-2012-04-09-ICML_author_feedback_is_open.html">461 hunch net-2012-04-09-ICML author feedback is open</a></p>
<p>11 0.76584423 <a title="68-lda-11" href="../hunch_net-2005/hunch_net-2005-03-24-The_Role_of_Workshops.html">46 hunch net-2005-03-24-The Role of Workshops</a></p>
<p>12 0.70802778 <a title="68-lda-12" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>13 0.67153347 <a title="68-lda-13" href="../hunch_net-2011/hunch_net-2011-08-20-The_Large_Scale_Learning_Survey_Tutorial.html">442 hunch net-2011-08-20-The Large Scale Learning Survey Tutorial</a></p>
<p>14 0.66179454 <a title="68-lda-14" href="../hunch_net-2007/hunch_net-2007-06-21-Presentation_Preparation.html">249 hunch net-2007-06-21-Presentation Preparation</a></p>
<p>15 0.65763217 <a title="68-lda-15" href="../hunch_net-2009/hunch_net-2009-10-26-NIPS_workshops.html">375 hunch net-2009-10-26-NIPS workshops</a></p>
<p>16 0.643906 <a title="68-lda-16" href="../hunch_net-2006/hunch_net-2006-09-28-Programming_Languages_for_Machine_Learning_Implementations.html">210 hunch net-2006-09-28-Programming Languages for Machine Learning Implementations</a></p>
<p>17 0.64266157 <a title="68-lda-17" href="../hunch_net-2009/hunch_net-2009-12-24-Top_graduates_this_season.html">384 hunch net-2009-12-24-Top graduates this season</a></p>
<p>18 0.62720996 <a title="68-lda-18" href="../hunch_net-2013/hunch_net-2013-12-01-NIPS_tutorials_and_Vowpal_Wabbit_7.4.html">492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</a></p>
<p>19 0.61913705 <a title="68-lda-19" href="../hunch_net-2005/hunch_net-2005-03-17-Going_all_the_Way%2C_Sometimes.html">42 hunch net-2005-03-17-Going all the Way, Sometimes</a></p>
<p>20 0.61595625 <a title="68-lda-20" href="../hunch_net-2005/hunch_net-2005-02-21-Problem%3A_Cross_Validation.html">26 hunch net-2005-02-21-Problem: Cross Validation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
