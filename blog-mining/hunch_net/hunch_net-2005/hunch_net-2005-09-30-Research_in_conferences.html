<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>116 hunch net-2005-09-30-Research in conferences</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-116" href="#">hunch_net-2005-116</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>116 hunch net-2005-09-30-Research in conferences</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-116-html" href="http://hunch.net/?p=126">html</a></p><p>Introduction: Conferences exist as part of the process of doing research.  They provide many roles including “announcing research”, “meeting people”, and  “point of reference”.  Not all conferences are alike so a basic question is: “to what extent do individual conferences attempt to aid research?”  This question is very difficult to answer in any satisfying way.  What we can do is compare details of the process across multiple conferences.
  
  Comments   The average quality of comments across conferences can vary dramatically.  At one extreme, the tradition in CS theory conferences is to provide essentially zero feedback.  At the other extreme, some conferences have a strong tradition of providing detailed constructive feedback.  Detailed feedback can give authors significant guidance about how to improve research.  This is the most subjective entry. 
  Blind  Virtually all conferences offer single blind review where authors do not know reviewers.  Some also provide  double blind  review where rev</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Not all conferences are alike so a basic question is: “to what extent do individual conferences attempt to aid research? [sent-3, score-0.624]
</p><p>2 Comments   The average quality of comments across conferences can vary dramatically. [sent-6, score-0.604]
</p><p>3 At one extreme, the tradition in CS theory conferences is to provide essentially zero feedback. [sent-7, score-0.448]
</p><p>4 At the other extreme, some conferences have a strong tradition of providing detailed constructive feedback. [sent-8, score-0.499]
</p><p>5 Detailed feedback can give authors significant guidance about how to improve research. [sent-9, score-0.293]
</p><p>6 Blind  Virtually all conferences offer single blind review where authors do not know reviewers. [sent-11, score-0.808]
</p><p>7 Some also provide  double blind  review where reviewers do not know authors. [sent-12, score-0.715]
</p><p>8 The intention with double blind reviewing is to make the conference more approachable to first-time authors. [sent-13, score-0.71]
</p><p>9 Author Feedback  Author feedback is a mechanism where authors can provide feedback to reviewers (and, to some extent, complain). [sent-14, score-0.737]
</p><p>10 Providing an author feedback mechanism provides an opportunity for the worst reviewing errors to be corrected. [sent-15, score-0.507]
</p><p>11 Conditional Accepts  A conditional accept is some form of “we will accept this paper if conditions X,Y, and Z are met”. [sent-16, score-0.382]
</p><p>12 A conditional accept allows reviewers to demand different experiments or other details they need in order to make a decision. [sent-17, score-0.489]
</p><p>13 Papers/PC member  How many papers can one person actually review well? [sent-19, score-0.345]
</p><p>14 When there is an incredible load of papers to review, it becomes very tempting to make snap decisions without a thorough attempt at understanding. [sent-20, score-0.371]
</p><p>15 working on new research and the speed of the review process itself. [sent-25, score-0.311]
</p><p>16 Also keep in mind that measurements of “impact” are inherently “trailing indicators” which are not necessarily relevant to the way the conference is currently run. [sent-30, score-0.304]
</p><p>17 average citations  Citeseer has been used to estimate the average impact of a conference’s papers  here  using the average number of citations per paper. [sent-31, score-1.447]
</p><p>18 max citations   A number of people believe that the maximum number of citations given to any one paper is a strong indicator of the success of the conference. [sent-32, score-0.841]
</p><p>19 Conference   Comments   blindness   author feedback   conditional accepts   Reviews/PC  member   log(average citations per paper+1)   max citations        ICML    Sometimes Helpful   Double   Yes   Yes   8   2. [sent-36, score-1.518]
</p><p>20 Keep in mind that the above is a very incomplete list (it only includes the conferences that I interacted with) and feel free to add details in the comments. [sent-47, score-0.386]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('citations', 0.294), ('conferences', 0.212), ('reviewing', 0.193), ('feedback', 0.19), ('single', 0.182), ('average', 0.174), ('helpful', 0.173), ('blind', 0.159), ('conditional', 0.156), ('snap', 0.156), ('review', 0.152), ('double', 0.15), ('yes', 0.148), ('comments', 0.144), ('conference', 0.139), ('reviewers', 0.129), ('provide', 0.125), ('extent', 0.125), ('author', 0.124), ('accepts', 0.121), ('max', 0.121), ('member', 0.121), ('accept', 0.113), ('tradition', 0.111), ('sometimes', 0.108), ('options', 0.107), ('authors', 0.103), ('impact', 0.102), ('detailed', 0.098), ('per', 0.097), ('details', 0.091), ('reference', 0.088), ('spent', 0.088), ('mind', 0.083), ('keep', 0.082), ('process', 0.08), ('speed', 0.079), ('providing', 0.078), ('extreme', 0.075), ('attempt', 0.075), ('across', 0.074), ('papers', 0.072), ('intention', 0.069), ('decisions', 0.068), ('number', 0.066), ('announcing', 0.064), ('roles', 0.064), ('ccc', 0.064), ('citeseer', 0.064), ('complain', 0.064)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="116-tfidf-1" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>Introduction: Conferences exist as part of the process of doing research.  They provide many roles including “announcing research”, “meeting people”, and  “point of reference”.  Not all conferences are alike so a basic question is: “to what extent do individual conferences attempt to aid research?”  This question is very difficult to answer in any satisfying way.  What we can do is compare details of the process across multiple conferences.
  
  Comments   The average quality of comments across conferences can vary dramatically.  At one extreme, the tradition in CS theory conferences is to provide essentially zero feedback.  At the other extreme, some conferences have a strong tradition of providing detailed constructive feedback.  Detailed feedback can give authors significant guidance about how to improve research.  This is the most subjective entry. 
  Blind  Virtually all conferences offer single blind review where authors do not know reviewers.  Some also provide  double blind  review where rev</p><p>2 0.31485173 <a title="116-tfidf-2" href="../hunch_net-2005/hunch_net-2005-03-13-Avoiding_Bad_Reviewing.html">40 hunch net-2005-03-13-Avoiding Bad Reviewing</a></p>
<p>Introduction: If we accept that bad reviewing often occurs and want to fix it, the question is “how”?
 
Reviewing is done by paper writers just like yourself, so a good proxy for this question is asking “How can I be a better reviewer?”  Here are a few things I’ve learned by trial (and error), as a paper writer, and as a reviewer.
  
 The secret ingredient is careful thought.  There is no good substitution for a deep and careful understanding. 
 Avoid reviewing papers that you feel competitive about.  You almost certainly will be asked to review papers that feel competitive if you work on subjects of common interest.  But, the feeling of competition can easily lead to bad judgement. 
 If you feel biased for some other reason, then you should avoid reviewing.  For example… 
 Feeling angry or threatened by a paper is a form of bias.  See above. 
 Double blind yourself (avoid looking at the name even in a single-blind situation).  The significant effect of a name you recognize is making you pay close a</p><p>3 0.30060193 <a title="116-tfidf-3" href="../hunch_net-2005/hunch_net-2005-05-02-Reviewing_techniques_for_conferences.html">65 hunch net-2005-05-02-Reviewing techniques for conferences</a></p>
<p>Introduction: The many reviews following the many paper deadlines are just about over.  AAAI and ICML in particular were experimenting with several reviewing techniques.  
  
 Double Blind: AAAI and ICML were both double blind this year.  It seemed (overall) beneficial, but two problems arose.
 
 For theoretical papers, with a lot to say, authors often leave out the proofs.  This is very hard to cope with under a double blind review because (1) you can not trust the authors got the proof right but (2) a blanket “reject” hits many probably-good papers.  Perhaps authors should more strongly favor proof-complete papers sent to double blind conferences. 
 On the author side, double blind reviewing is actually somewhat disruptive to research.  In particular, it discourages the author from talking about the subject, which is one of the mechanisms of research.  This is not a great drawback, but it is one not previously appreciated. 
 
 
 Author feedback: AAAI and ICML did author feedback this year. It seem</p><p>4 0.27962169 <a title="116-tfidf-4" href="../hunch_net-2010/hunch_net-2010-04-26-Compassionate_Reviewing.html">395 hunch net-2010-04-26-Compassionate Reviewing</a></p>
<p>Introduction: Most long conversations between academics seem to converge on the topic of reviewing where almost no one is happy.  A basic question is: Should most people be happy?
 
The case against is straightforward.  Anyone who watches the flow of papers realizes that most papers amount to little in the longer term.  By it’s nature research is brutal, where the second-best method is worthless, and the second person to discover things typically gets no credit.  If you think about this for a moment, it’s very different from most other human endeavors.  The second best migrant laborer, construction worker, manager, conductor, quarterback, etc… all can manage quite well. If a reviewer has even a vaguely predictive sense of what’s important in the longer term, then most people submitting papers will be unhappy.
 
But this argument unravels, in my experience.  Perhaps half of reviews are thoughtless or simply wrong with a small part being simply malicious.  And yet, I’m sure that most reviewers genuine</p><p>5 0.27610016 <a title="116-tfidf-5" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>Introduction: When thinking about how best to review papers, it seems helpful to have some conception of what good reviewing is.  As far as I can tell, this is almost always only discussed in the specific context of a paper (i.e. your rejected paper), or at most an area (i.e. what a “good paper” looks like for that area) rather than general principles.  Neither individual papers or areas are sufficiently general for a large conference—every paper differs in the details, and what if you want to build a new area and/or cross areas?
 
An unavoidable reason for reviewing is that the community of research is too large.  In particular, it is not possible for a researcher to read every paper which someone thinks might be of interest.  This reason for reviewing exists independent of constraints on rooms or scheduling formats of individual conferences.  Indeed, history suggests that physical constraints are relatively meaningless over the long term — growing conferences simply use more rooms and/or change fo</p><p>6 0.25733292 <a title="116-tfidf-6" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>7 0.24431685 <a title="116-tfidf-7" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>8 0.24148722 <a title="116-tfidf-8" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>9 0.24014942 <a title="116-tfidf-9" href="../hunch_net-2008/hunch_net-2008-12-12-Summer_Conferences.html">331 hunch net-2008-12-12-Summer Conferences</a></p>
<p>10 0.21328501 <a title="116-tfidf-10" href="../hunch_net-2012/hunch_net-2012-06-29-ICML_survey_and_comments.html">468 hunch net-2012-06-29-ICML survey and comments</a></p>
<p>11 0.20634365 <a title="116-tfidf-11" href="../hunch_net-2008/hunch_net-2008-10-14-Who_is_Responsible_for_a_Bad_Review%3F.html">320 hunch net-2008-10-14-Who is Responsible for a Bad Review?</a></p>
<p>12 0.20238566 <a title="116-tfidf-12" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>13 0.18582772 <a title="116-tfidf-13" href="../hunch_net-2006/hunch_net-2006-12-04-Structural_Problems_in_NIPS_Decision_Making.html">221 hunch net-2006-12-04-Structural Problems in NIPS Decision Making</a></p>
<p>14 0.1851207 <a title="116-tfidf-14" href="../hunch_net-2006/hunch_net-2006-09-12-Incentive_Compatible_Reviewing.html">207 hunch net-2006-09-12-Incentive Compatible Reviewing</a></p>
<p>15 0.18415692 <a title="116-tfidf-15" href="../hunch_net-2012/hunch_net-2012-06-05-ICML_acceptance_statistics.html">466 hunch net-2012-06-05-ICML acceptance statistics</a></p>
<p>16 0.18292062 <a title="116-tfidf-16" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<p>17 0.17766879 <a title="116-tfidf-17" href="../hunch_net-2012/hunch_net-2012-04-09-ICML_author_feedback_is_open.html">461 hunch net-2012-04-09-ICML author feedback is open</a></p>
<p>18 0.17131402 <a title="116-tfidf-18" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>19 0.16911963 <a title="116-tfidf-19" href="../hunch_net-2008/hunch_net-2008-09-03-Bidding_Problems.html">315 hunch net-2008-09-03-Bidding Problems</a></p>
<p>20 0.16894238 <a title="116-tfidf-20" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.294), (1, -0.273), (2, 0.298), (3, 0.089), (4, -0.012), (5, 0.004), (6, -0.0), (7, -0.018), (8, 0.046), (9, 0.079), (10, -0.084), (11, 0.003), (12, 0.082), (13, -0.023), (14, -0.081), (15, -0.017), (16, -0.038), (17, -0.048), (18, -0.02), (19, -0.012), (20, 0.008), (21, -0.072), (22, 0.05), (23, 0.023), (24, 0.085), (25, -0.037), (26, -0.041), (27, 0.101), (28, -0.064), (29, 0.021), (30, -0.024), (31, 0.023), (32, -0.034), (33, -0.084), (34, 0.101), (35, 0.057), (36, -0.033), (37, -0.042), (38, -0.069), (39, 0.02), (40, -0.047), (41, -0.018), (42, 0.083), (43, 0.034), (44, -0.012), (45, 0.033), (46, 0.07), (47, 0.028), (48, -0.02), (49, -0.036)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98901165 <a title="116-lsi-1" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>Introduction: Conferences exist as part of the process of doing research.  They provide many roles including “announcing research”, “meeting people”, and  “point of reference”.  Not all conferences are alike so a basic question is: “to what extent do individual conferences attempt to aid research?”  This question is very difficult to answer in any satisfying way.  What we can do is compare details of the process across multiple conferences.
  
  Comments   The average quality of comments across conferences can vary dramatically.  At one extreme, the tradition in CS theory conferences is to provide essentially zero feedback.  At the other extreme, some conferences have a strong tradition of providing detailed constructive feedback.  Detailed feedback can give authors significant guidance about how to improve research.  This is the most subjective entry. 
  Blind  Virtually all conferences offer single blind review where authors do not know reviewers.  Some also provide  double blind  review where rev</p><p>2 0.87238508 <a title="116-lsi-2" href="../hunch_net-2005/hunch_net-2005-03-13-Avoiding_Bad_Reviewing.html">40 hunch net-2005-03-13-Avoiding Bad Reviewing</a></p>
<p>Introduction: If we accept that bad reviewing often occurs and want to fix it, the question is “how”?
 
Reviewing is done by paper writers just like yourself, so a good proxy for this question is asking “How can I be a better reviewer?”  Here are a few things I’ve learned by trial (and error), as a paper writer, and as a reviewer.
  
 The secret ingredient is careful thought.  There is no good substitution for a deep and careful understanding. 
 Avoid reviewing papers that you feel competitive about.  You almost certainly will be asked to review papers that feel competitive if you work on subjects of common interest.  But, the feeling of competition can easily lead to bad judgement. 
 If you feel biased for some other reason, then you should avoid reviewing.  For example… 
 Feeling angry or threatened by a paper is a form of bias.  See above. 
 Double blind yourself (avoid looking at the name even in a single-blind situation).  The significant effect of a name you recognize is making you pay close a</p><p>3 0.87178993 <a title="116-lsi-3" href="../hunch_net-2010/hunch_net-2010-04-26-Compassionate_Reviewing.html">395 hunch net-2010-04-26-Compassionate Reviewing</a></p>
<p>Introduction: Most long conversations between academics seem to converge on the topic of reviewing where almost no one is happy.  A basic question is: Should most people be happy?
 
The case against is straightforward.  Anyone who watches the flow of papers realizes that most papers amount to little in the longer term.  By it’s nature research is brutal, where the second-best method is worthless, and the second person to discover things typically gets no credit.  If you think about this for a moment, it’s very different from most other human endeavors.  The second best migrant laborer, construction worker, manager, conductor, quarterback, etc… all can manage quite well. If a reviewer has even a vaguely predictive sense of what’s important in the longer term, then most people submitting papers will be unhappy.
 
But this argument unravels, in my experience.  Perhaps half of reviews are thoughtless or simply wrong with a small part being simply malicious.  And yet, I’m sure that most reviewers genuine</p><p>4 0.84235144 <a title="116-lsi-4" href="../hunch_net-2005/hunch_net-2005-05-02-Reviewing_techniques_for_conferences.html">65 hunch net-2005-05-02-Reviewing techniques for conferences</a></p>
<p>Introduction: The many reviews following the many paper deadlines are just about over.  AAAI and ICML in particular were experimenting with several reviewing techniques.  
  
 Double Blind: AAAI and ICML were both double blind this year.  It seemed (overall) beneficial, but two problems arose.
 
 For theoretical papers, with a lot to say, authors often leave out the proofs.  This is very hard to cope with under a double blind review because (1) you can not trust the authors got the proof right but (2) a blanket “reject” hits many probably-good papers.  Perhaps authors should more strongly favor proof-complete papers sent to double blind conferences. 
 On the author side, double blind reviewing is actually somewhat disruptive to research.  In particular, it discourages the author from talking about the subject, which is one of the mechanisms of research.  This is not a great drawback, but it is one not previously appreciated. 
 
 
 Author feedback: AAAI and ICML did author feedback this year. It seem</p><p>5 0.76328176 <a title="116-lsi-5" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>Introduction: When thinking about how best to review papers, it seems helpful to have some conception of what good reviewing is.  As far as I can tell, this is almost always only discussed in the specific context of a paper (i.e. your rejected paper), or at most an area (i.e. what a “good paper” looks like for that area) rather than general principles.  Neither individual papers or areas are sufficiently general for a large conference—every paper differs in the details, and what if you want to build a new area and/or cross areas?
 
An unavoidable reason for reviewing is that the community of research is too large.  In particular, it is not possible for a researcher to read every paper which someone thinks might be of interest.  This reason for reviewing exists independent of constraints on rooms or scheduling formats of individual conferences.  Indeed, history suggests that physical constraints are relatively meaningless over the long term — growing conferences simply use more rooms and/or change fo</p><p>6 0.75654876 <a title="116-lsi-6" href="../hunch_net-2006/hunch_net-2006-09-12-Incentive_Compatible_Reviewing.html">207 hunch net-2006-09-12-Incentive Compatible Reviewing</a></p>
<p>7 0.73184931 <a title="116-lsi-7" href="../hunch_net-2008/hunch_net-2008-09-03-Bidding_Problems.html">315 hunch net-2008-09-03-Bidding Problems</a></p>
<p>8 0.72717994 <a title="116-lsi-8" href="../hunch_net-2013/hunch_net-2013-06-29-The_Benefits_of_Double-Blind_Review.html">485 hunch net-2013-06-29-The Benefits of Double-Blind Review</a></p>
<p>9 0.69581455 <a title="116-lsi-9" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>10 0.69317812 <a title="116-lsi-10" href="../hunch_net-2012/hunch_net-2012-04-09-ICML_author_feedback_is_open.html">461 hunch net-2012-04-09-ICML author feedback is open</a></p>
<p>11 0.68442029 <a title="116-lsi-11" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>12 0.67838925 <a title="116-lsi-12" href="../hunch_net-2009/hunch_net-2009-07-09-The_Machine_Learning_Forum.html">363 hunch net-2009-07-09-The Machine Learning Forum</a></p>
<p>13 0.67309082 <a title="116-lsi-13" href="../hunch_net-2012/hunch_net-2012-06-29-ICML_survey_and_comments.html">468 hunch net-2012-06-29-ICML survey and comments</a></p>
<p>14 0.6603688 <a title="116-lsi-14" href="../hunch_net-2006/hunch_net-2006-12-04-Structural_Problems_in_NIPS_Decision_Making.html">221 hunch net-2006-12-04-Structural Problems in NIPS Decision Making</a></p>
<p>15 0.65726793 <a title="116-lsi-15" href="../hunch_net-2008/hunch_net-2008-12-12-Summer_Conferences.html">331 hunch net-2008-12-12-Summer Conferences</a></p>
<p>16 0.62357396 <a title="116-lsi-16" href="../hunch_net-2008/hunch_net-2008-10-14-Who_is_Responsible_for_a_Bad_Review%3F.html">320 hunch net-2008-10-14-Who is Responsible for a Bad Review?</a></p>
<p>17 0.62073874 <a title="116-lsi-17" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>18 0.61948383 <a title="116-lsi-18" href="../hunch_net-2009/hunch_net-2009-12-09-Future_Publication_Models_%40_NIPS.html">382 hunch net-2009-12-09-Future Publication Models @ NIPS</a></p>
<p>19 0.59682643 <a title="116-lsi-19" href="../hunch_net-2012/hunch_net-2012-05-02-ICML%3A_Behind_the_Scenes.html">463 hunch net-2012-05-02-ICML: Behind the Scenes</a></p>
<p>20 0.58575171 <a title="116-lsi-20" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(3, 0.03), (20, 0.165), (27, 0.202), (38, 0.017), (48, 0.059), (53, 0.112), (55, 0.216), (80, 0.012), (94, 0.081), (95, 0.024)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94136924 <a title="116-lda-1" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>Introduction: Conferences exist as part of the process of doing research.  They provide many roles including “announcing research”, “meeting people”, and  “point of reference”.  Not all conferences are alike so a basic question is: “to what extent do individual conferences attempt to aid research?”  This question is very difficult to answer in any satisfying way.  What we can do is compare details of the process across multiple conferences.
  
  Comments   The average quality of comments across conferences can vary dramatically.  At one extreme, the tradition in CS theory conferences is to provide essentially zero feedback.  At the other extreme, some conferences have a strong tradition of providing detailed constructive feedback.  Detailed feedback can give authors significant guidance about how to improve research.  This is the most subjective entry. 
  Blind  Virtually all conferences offer single blind review where authors do not know reviewers.  Some also provide  double blind  review where rev</p><p>2 0.92362159 <a title="116-lda-2" href="../hunch_net-2006/hunch_net-2006-09-18-What_is_missing_for_online_collaborative_research%3F.html">208 hunch net-2006-09-18-What is missing for online collaborative research?</a></p>
<p>Introduction: The internet has recently made the research process much smoother: papers are easy to obtain, citations are easy to follow, and unpublished “tutorials” are often available. Yet, new research fields can look very complicated to outsiders or newcomers. Every paper is like a small piece of an unfinished jigsaw puzzle: to understand just one publication, a researcher without experience in the field will typically have to follow several layers of citations, and many of the papers he encounters have a great deal of repeated information. Furthermore, from one publication to the next, notation and terminology may not be consistent which can further confuse the reader.
 
But the internet is now proving to be an extremely useful medium for collaboration and knowledge aggregation. Online forums allow users to ask and answer questions and to share ideas. The recent phenomenon of Wikipedia provides a proof-of-concept for the “anyone can edit” system. Can such models be used to facilitate research a</p><p>3 0.88448918 <a title="116-lda-3" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>Introduction: Yahoo! laid off people .  Unlike every previous time there have been layoffs, this is serious for  Yahoo! Research .  
 
We had advanced warning from  Prabhakar  through the  simple act of leaving .  Yahoo! Research was a world class organization that Prabhakar recruited much of personally, so it is deeply implausible that he would spontaneously decide to leave.  My first thought when I saw the news was “Uhoh,  Rob  said that he knew it was serious when the head of ATnT Research left.”  In this case it was even more significant, because Prabhakar recruited me on the premise that Y!R was an experiment in how research should be done: via a combination of high quality people and high engagement with the company.  Prabhakar’s departure is a clear end to that experiment.
 
The result is ambiguous from a business perspective.  Y!R clearly was not capable of saving the company from its illnesses.  I’m not privy to the internal accounting of impact and this is the kind of subject where there c</p><p>4 0.87123239 <a title="116-lda-4" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>Introduction: Here’s a quick reference for summer ML-related conferences sorted by due date:
  
 
 Conference 
 Due date 
 Location 
 Reviewing 
 
 
  KDD  
 Feb 10 
 August 12-16, Beijing, China 
 Single Blind 
 
 
  COLT  
 Feb 14 
 June 25-June 27, Edinburgh, Scotland 
 Single Blind? (historically) 
 
 
  ICML  
 Feb 24 
 June 26-July 1, Edinburgh, Scotland 
 Double Blind, author response, zero  SPOF  
 
 
  UAI  
 March 30 
 August 15-17, Catalina Islands, California 
 Double Blind, author response 
 
  
Geographically, this is greatly dispersed and the UAI/KDD conflict is unfortunate.
 
Machine Learning conferences are triannual now, between  NIPS ,  AIStat , and  ICML .  This has not always been the case: the academic default is annual summer conferences, then NIPS started with a December conference, and now AIStat has grown into an April conference.  
 
However, the first claim is not quite correct.  NIPS and AIStat have few competing venues while ICML implicitly competes with many other conf</p><p>5 0.85815579 <a title="116-lda-5" href="../hunch_net-2005/hunch_net-2005-03-13-Avoiding_Bad_Reviewing.html">40 hunch net-2005-03-13-Avoiding Bad Reviewing</a></p>
<p>Introduction: If we accept that bad reviewing often occurs and want to fix it, the question is “how”?
 
Reviewing is done by paper writers just like yourself, so a good proxy for this question is asking “How can I be a better reviewer?”  Here are a few things I’ve learned by trial (and error), as a paper writer, and as a reviewer.
  
 The secret ingredient is careful thought.  There is no good substitution for a deep and careful understanding. 
 Avoid reviewing papers that you feel competitive about.  You almost certainly will be asked to review papers that feel competitive if you work on subjects of common interest.  But, the feeling of competition can easily lead to bad judgement. 
 If you feel biased for some other reason, then you should avoid reviewing.  For example… 
 Feeling angry or threatened by a paper is a form of bias.  See above. 
 Double blind yourself (avoid looking at the name even in a single-blind situation).  The significant effect of a name you recognize is making you pay close a</p><p>6 0.85326427 <a title="116-lda-6" href="../hunch_net-2010/hunch_net-2010-04-26-Compassionate_Reviewing.html">395 hunch net-2010-04-26-Compassionate Reviewing</a></p>
<p>7 0.84841239 <a title="116-lda-7" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>8 0.84112024 <a title="116-lda-8" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>9 0.83546615 <a title="116-lda-9" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>10 0.83280486 <a title="116-lda-10" href="../hunch_net-2007/hunch_net-2007-11-02-The_Machine_Learning_Award_goes_to_%26%238230%3B.html">270 hunch net-2007-11-02-The Machine Learning Award goes to &#8230;</a></p>
<p>11 0.83265382 <a title="116-lda-11" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<p>12 0.82684129 <a title="116-lda-12" href="../hunch_net-2012/hunch_net-2012-06-05-ICML_acceptance_statistics.html">466 hunch net-2012-06-05-ICML acceptance statistics</a></p>
<p>13 0.82627279 <a title="116-lda-13" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>14 0.82506502 <a title="116-lda-14" href="../hunch_net-2012/hunch_net-2012-04-09-ICML_author_feedback_is_open.html">461 hunch net-2012-04-09-ICML author feedback is open</a></p>
<p>15 0.82339752 <a title="116-lda-15" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>16 0.8172096 <a title="116-lda-16" href="../hunch_net-2005/hunch_net-2005-07-21-Six_Months.html">96 hunch net-2005-07-21-Six Months</a></p>
<p>17 0.81519669 <a title="116-lda-17" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<p>18 0.81489748 <a title="116-lda-18" href="../hunch_net-2012/hunch_net-2012-06-29-ICML_survey_and_comments.html">468 hunch net-2012-06-29-ICML survey and comments</a></p>
<p>19 0.81489491 <a title="116-lda-19" href="../hunch_net-2005/hunch_net-2005-07-04-The_Health_of_COLT.html">89 hunch net-2005-07-04-The Health of COLT</a></p>
<p>20 0.81148648 <a title="116-lda-20" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
