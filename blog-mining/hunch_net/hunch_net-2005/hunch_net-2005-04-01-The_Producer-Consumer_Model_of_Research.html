<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>51 hunch net-2005-04-01-The Producer-Consumer Model of Research</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-51" href="#">hunch_net-2005-51</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>51 hunch net-2005-04-01-The Producer-Consumer Model of Research</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-51-html" href="http://hunch.net/?p=55">html</a></p><p>Introduction: In the quest to understand what good reviewing is, perhaps it's worthwhile to
think about what good research is. One way to think about good research is in
terms of a producer/consumer model.In the producer/consumer model of research,
for any element of research there are producers (authors and coauthors of
papers, for example) and consumers (people who use the papers to make new
papers or code solving problems). An produced bit of research is judged as
"good" if it is used by many consumers. There are two basic questions which
immediately arise:Is this a good model of research?Are there alternatives?The
producer/consumer model has some difficulties which can be (partially)
addressed.Disconnect.A group of people doing research on some subject may
become disconnected from the rest of the world. Each person uses the research
of other people in the group so it appears good research is being done, but
the group has no impact on the rest of the world. One way to detect this is by
looking at</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('consumers', 0.502), ('research', 0.363), ('model', 0.211), ('presented', 0.177), ('piece', 0.17), ('judging', 0.156), ('difficulties', 0.151), ('group', 0.147), ('good', 0.144), ('effect', 0.13), ('citations', 0.126), ('extent', 0.107), ('order', 0.106), ('higher', 0.102), ('rest', 0.099), ('may', 0.092), ('reviewer', 0.092), ('impact', 0.091), ('blow', 0.089), ('consumer', 0.089)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999982 <a title="51-tfidf-1" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>Introduction: In the quest to understand what good reviewing is, perhaps it's worthwhile to
think about what good research is. One way to think about good research is in
terms of a producer/consumer model.In the producer/consumer model of research,
for any element of research there are producers (authors and coauthors of
papers, for example) and consumers (people who use the papers to make new
papers or code solving problems). An produced bit of research is judged as
"good" if it is used by many consumers. There are two basic questions which
immediately arise:Is this a good model of research?Are there alternatives?The
producer/consumer model has some difficulties which can be (partially)
addressed.Disconnect.A group of people doing research on some subject may
become disconnected from the rest of the world. Each person uses the research
of other people in the group so it appears good research is being done, but
the group has no impact on the rest of the world. One way to detect this is by
looking at</p><p>2 0.2043829 <a title="51-tfidf-2" href="../hunch_net-2005/hunch_net-2005-03-05-Funding_Research.html">36 hunch net-2005-03-05-Funding Research</a></p>
<p>Introduction: The funding of research (and machine learning research) is an issue which
seems to have become more significant in the United States over the last
decade. The word "research" is applied broadly here to science, mathematics,
and engineering.There are two essential difficulties with funding
research:LongshotPaying a researcher is often a big gamble. Most research
projects don't pan out, but a few big payoffs can make it all
worthwhile.Information OnlyMuch of research is about finding the right way to
think about or do something.The Longshot difficulty means that there is high
variance in payoffs. This can be compensated for by funding many different
research projects, reducing variance.The Information-Only difficulty means
that it's hard to extract a profit directly from many types of research, so
companies have difficulty justifying basic research. (Patents are a mechanism
for doing this. They are often extraordinarily clumsy or simply not
applicable.)These two difficulties together imp</p><p>3 0.19518942 <a title="51-tfidf-3" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>Introduction: How do you create an optimal environment for research? Here are some essential
ingredients that I see.Stability. University-based research is relatively good
at this. On any particular day, researchers face choices in what they will
work on. A very common tradeoff is between:easy smalldifficult bigFor
researchers without stability, the 'easy small' option wins. This is often
"ok"--a series of incremental improvements on the state of the art can add up
to something very beneficial. However, it misses one of the big potentials of
research: finding entirely new and better ways of doing things.Stability comes
in many forms. The prototypical example is tenure at a university--a tenured
professor is almost imposssible to fire which means that the professor has the
freedom to consider far horizon activities. An iron-clad guarantee of a
paycheck is not necessary--industrial research labs have succeeded well with
research positions of indefinite duration. Atnt research was a great example
of th</p><p>4 0.16213536 <a title="51-tfidf-4" href="../hunch_net-2006/hunch_net-2006-07-11-New_Models.html">194 hunch net-2006-07-11-New Models</a></p>
<p>Introduction: How should we, as researchers in machine learning, organize ourselves?The most
immediate measurable objective of computer science research is publishing a
paper. The most difficult aspect of publishing a paper is having reviewers
accept and recommend it for publication. The simplest mechanism for doing this
is to show theoretical progress on some standard, well-known easily understood
problem.In doing this, we often fall into a local minima of the research
process. The basic problem in machine learning is that it is very unclear that
the mathematical model is the right one for the (or some) real problem. A good
mathematical model in machine learning should have one fundamental trait: it
should aid the design of effective learning algorithms. To date, our ability
to solve interesting learning problems (speech recognition, machine
translation, object recognition, etcâ&euro;Ś) remains limited (although improving),
so the "rightness" of our models is in doubt.If our mathematical models are
bad, t</p><p>5 0.15768556 <a title="51-tfidf-5" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>Introduction: Conferences exist as part of the process of doing research. They provide many
roles including "announcing research", "meeting people", and "point of
reference". Not all conferences are alike so a basic question is: "to what
extent do individual conferences attempt to aid research?" This question is
very difficult to answer in any satisfying way. What we can do is compare
details of the process across multiple conferences.CommentsThe average quality
of comments across conferences can vary dramatically. At one extreme, the
tradition in CS theory conferences is to provide essentially zero feedback. At
the other extreme, some conferences have a strong tradition of providing
detailed constructive feedback. Detailed feedback can give authors significant
guidance about how to improve research. This is the most subjective
entry.BlindVirtually all conferences offer single blind review where authors
do not know reviewers. Some also providedouble blindreview where reviewers do
not know authors. T</p><p>6 0.15531079 <a title="51-tfidf-6" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>7 0.15495601 <a title="51-tfidf-7" href="../hunch_net-2006/hunch_net-2006-09-12-Incentive_Compatible_Reviewing.html">207 hunch net-2006-09-12-Incentive Compatible Reviewing</a></p>
<p>8 0.147562 <a title="51-tfidf-8" href="../hunch_net-2009/hunch_net-2009-02-22-Effective_Research_Funding.html">344 hunch net-2009-02-22-Effective Research Funding</a></p>
<p>9 0.14489938 <a title="51-tfidf-9" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>10 0.14314468 <a title="51-tfidf-10" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>11 0.14146128 <a title="51-tfidf-11" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>12 0.13437191 <a title="51-tfidf-12" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>13 0.12674382 <a title="51-tfidf-13" href="../hunch_net-2005/hunch_net-2005-12-04-Watchword%3A_model.html">135 hunch net-2005-12-04-Watchword: model</a></p>
<p>14 0.12502339 <a title="51-tfidf-14" href="../hunch_net-2010/hunch_net-2010-04-26-Compassionate_Reviewing.html">395 hunch net-2010-04-26-Compassionate Reviewing</a></p>
<p>15 0.12454817 <a title="51-tfidf-15" href="../hunch_net-2005/hunch_net-2005-10-12-The_unrealized_potential_of_the_research_lab.html">121 hunch net-2005-10-12-The unrealized potential of the research lab</a></p>
<p>16 0.12287483 <a title="51-tfidf-16" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>17 0.12269381 <a title="51-tfidf-17" href="../hunch_net-2005/hunch_net-2005-09-10-%26%238220%3BFailure%26%238221%3B_is_an_option.html">110 hunch net-2005-09-10-&#8220;Failure&#8221; is an option</a></p>
<p>18 0.12209502 <a title="51-tfidf-18" href="../hunch_net-2007/hunch_net-2007-02-16-The_Forgetting.html">233 hunch net-2007-02-16-The Forgetting</a></p>
<p>19 0.12159256 <a title="51-tfidf-19" href="../hunch_net-2008/hunch_net-2008-09-03-Bidding_Problems.html">315 hunch net-2008-09-03-Bidding Problems</a></p>
<p>20 0.12125604 <a title="51-tfidf-20" href="../hunch_net-2006/hunch_net-2006-09-18-What_is_missing_for_online_collaborative_research%3F.html">208 hunch net-2006-09-18-What is missing for online collaborative research?</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.258), (1, 0.13), (2, 0.015), (3, -0.19), (4, 0.11), (5, -0.039), (6, -0.003), (7, 0.016), (8, -0.117), (9, -0.001), (10, 0.075), (11, 0.047), (12, -0.112), (13, 0.07), (14, 0.014), (15, -0.005), (16, 0.024), (17, -0.129), (18, -0.025), (19, -0.075), (20, -0.032), (21, -0.088), (22, 0.107), (23, 0.003), (24, -0.014), (25, 0.035), (26, 0.066), (27, 0.017), (28, -0.053), (29, 0.033), (30, 0.018), (31, 0.064), (32, 0.002), (33, 0.016), (34, -0.037), (35, 0.016), (36, -0.036), (37, -0.053), (38, -0.022), (39, 0.047), (40, 0.001), (41, 0.024), (42, 0.1), (43, -0.05), (44, 0.012), (45, -0.017), (46, 0.028), (47, 0.014), (48, -0.013), (49, 0.002)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97541624 <a title="51-lsi-1" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>Introduction: In the quest to understand what good reviewing is, perhaps it's worthwhile to
think about what good research is. One way to think about good research is in
terms of a producer/consumer model.In the producer/consumer model of research,
for any element of research there are producers (authors and coauthors of
papers, for example) and consumers (people who use the papers to make new
papers or code solving problems). An produced bit of research is judged as
"good" if it is used by many consumers. There are two basic questions which
immediately arise:Is this a good model of research?Are there alternatives?The
producer/consumer model has some difficulties which can be (partially)
addressed.Disconnect.A group of people doing research on some subject may
become disconnected from the rest of the world. Each person uses the research
of other people in the group so it appears good research is being done, but
the group has no impact on the rest of the world. One way to detect this is by
looking at</p><p>2 0.77439958 <a title="51-lsi-2" href="../hunch_net-2005/hunch_net-2005-10-12-The_unrealized_potential_of_the_research_lab.html">121 hunch net-2005-10-12-The unrealized potential of the research lab</a></p>
<p>Introduction: I attended theIBM research 60th anniversary. IBM research is, by any
reasonable account, the industrial research lab which has managed to bring the
most value to it's parent company over the long term. This can be seen by
simply counting the survivors: IBM research is the only older research lab
which has not gone through a period of massive firing. (Note that there are
alsonew research labs.)Despite this impressive record, IBM research has
failed, by far, to achieve it's potential. Examples which came up in this
meeting include:It took about a decade to produce DRAM after it was invented
in the lab. (In fact, Intel produced it first.)Relational databases and SQL
were invented and then languished. It was only under external competition that
IBM released it's own relational database. Why didn't IBM grow anOracle
division?An early lead in IP networking hardware did not result in IBM growing
aCisco division. Why not?And remember â&euro;Ś IBM research is a stark success story
compared to it's com</p><p>3 0.76921672 <a title="51-lsi-3" href="../hunch_net-2005/hunch_net-2005-05-29-Bad_ideas.html">76 hunch net-2005-05-29-Bad ideas</a></p>
<p>Introduction: I found these two essays on bad ideas interesting. Neither of these is written
from the viewpoint of research, but they are both highly relevant.Why smart
people have bad ideasby Paul GrahamWhy smart people defend bad ideasby Scott
Berkun (which appeared onslashdot)In my experience, bad ideas are
commonandover confidence in ideas is common. This overconfidence can take
either the form of excessive condemnation or excessive praise. Some of this is
necessary to the process of research. For example, some overconfidence in the
value of your own research is expected and probably necessary to motivate your
own investigation. Since research is a rather risky business, much of it does
not pan out. Learning to accept when something does not pan out is a critical
skill which is sometimes never acquired.Excessive condemnation can be a real
ill when it's encountered. This has two effects:When the penalty for being
wrong is too large, it means people have a great investment in defending
"their" ide</p><p>4 0.75634247 <a title="51-lsi-4" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>Introduction: How do you create an optimal environment for research? Here are some essential
ingredients that I see.Stability. University-based research is relatively good
at this. On any particular day, researchers face choices in what they will
work on. A very common tradeoff is between:easy smalldifficult bigFor
researchers without stability, the 'easy small' option wins. This is often
"ok"--a series of incremental improvements on the state of the art can add up
to something very beneficial. However, it misses one of the big potentials of
research: finding entirely new and better ways of doing things.Stability comes
in many forms. The prototypical example is tenure at a university--a tenured
professor is almost imposssible to fire which means that the professor has the
freedom to consider far horizon activities. An iron-clad guarantee of a
paycheck is not necessary--industrial research labs have succeeded well with
research positions of indefinite duration. Atnt research was a great example
of th</p><p>5 0.7398814 <a title="51-lsi-5" href="../hunch_net-2005/hunch_net-2005-03-05-Funding_Research.html">36 hunch net-2005-03-05-Funding Research</a></p>
<p>Introduction: The funding of research (and machine learning research) is an issue which
seems to have become more significant in the United States over the last
decade. The word "research" is applied broadly here to science, mathematics,
and engineering.There are two essential difficulties with funding
research:LongshotPaying a researcher is often a big gamble. Most research
projects don't pan out, but a few big payoffs can make it all
worthwhile.Information OnlyMuch of research is about finding the right way to
think about or do something.The Longshot difficulty means that there is high
variance in payoffs. This can be compensated for by funding many different
research projects, reducing variance.The Information-Only difficulty means
that it's hard to extract a profit directly from many types of research, so
companies have difficulty justifying basic research. (Patents are a mechanism
for doing this. They are often extraordinarily clumsy or simply not
applicable.)These two difficulties together imp</p><p>6 0.733661 <a title="51-lsi-6" href="../hunch_net-2009/hunch_net-2009-02-22-Effective_Research_Funding.html">344 hunch net-2009-02-22-Effective Research Funding</a></p>
<p>7 0.71744907 <a title="51-lsi-7" href="../hunch_net-2006/hunch_net-2006-07-11-New_Models.html">194 hunch net-2006-07-11-New Models</a></p>
<p>8 0.71694535 <a title="51-lsi-8" href="../hunch_net-2006/hunch_net-2006-09-18-What_is_missing_for_online_collaborative_research%3F.html">208 hunch net-2006-09-18-What is missing for online collaborative research?</a></p>
<p>9 0.6878981 <a title="51-lsi-9" href="../hunch_net-2006/hunch_net-2006-04-30-John_Langford_%26%238211%3B%3E_Yahoo_Research%2C_NY.html">175 hunch net-2006-04-30-John Langford &#8211;> Yahoo Research, NY</a></p>
<p>10 0.68465513 <a title="51-lsi-10" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>11 0.68433124 <a title="51-lsi-11" href="../hunch_net-2005/hunch_net-2005-03-29-Academic_Mechanism_Design.html">48 hunch net-2005-03-29-Academic Mechanism Design</a></p>
<p>12 0.67987496 <a title="51-lsi-12" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>13 0.67273593 <a title="51-lsi-13" href="../hunch_net-2008/hunch_net-2008-07-02-Proprietary_Data_in_Academic_Research%3F.html">306 hunch net-2008-07-02-Proprietary Data in Academic Research?</a></p>
<p>14 0.66383028 <a title="51-lsi-14" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>15 0.64809346 <a title="51-lsi-15" href="../hunch_net-2005/hunch_net-2005-01-19-Why_I_decided_to_run_a_weblog..html">1 hunch net-2005-01-19-Why I decided to run a weblog.</a></p>
<p>16 0.64271754 <a title="51-lsi-16" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>17 0.64201349 <a title="51-lsi-17" href="../hunch_net-2013/hunch_net-2013-06-29-The_Benefits_of_Double-Blind_Review.html">485 hunch net-2013-06-29-The Benefits of Double-Blind Review</a></p>
<p>18 0.63364589 <a title="51-lsi-18" href="../hunch_net-2007/hunch_net-2007-07-13-The_View_From_China.html">255 hunch net-2007-07-13-The View From China</a></p>
<p>19 0.6314137 <a title="51-lsi-19" href="../hunch_net-2005/hunch_net-2005-09-10-%26%238220%3BFailure%26%238221%3B_is_an_option.html">110 hunch net-2005-09-10-&#8220;Failure&#8221; is an option</a></p>
<p>20 0.63091481 <a title="51-lsi-20" href="../hunch_net-2006/hunch_net-2006-02-04-Research_Budget_Changes.html">154 hunch net-2006-02-04-Research Budget Changes</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(35, 0.04), (38, 0.028), (39, 0.062), (42, 0.242), (61, 0.051), (68, 0.064), (71, 0.102), (74, 0.218), (76, 0.014), (82, 0.023), (95, 0.026)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9499107 <a title="51-lda-1" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>Introduction: In the quest to understand what good reviewing is, perhaps it's worthwhile to
think about what good research is. One way to think about good research is in
terms of a producer/consumer model.In the producer/consumer model of research,
for any element of research there are producers (authors and coauthors of
papers, for example) and consumers (people who use the papers to make new
papers or code solving problems). An produced bit of research is judged as
"good" if it is used by many consumers. There are two basic questions which
immediately arise:Is this a good model of research?Are there alternatives?The
producer/consumer model has some difficulties which can be (partially)
addressed.Disconnect.A group of people doing research on some subject may
become disconnected from the rest of the world. Each person uses the research
of other people in the group so it appears good research is being done, but
the group has no impact on the rest of the world. One way to detect this is by
looking at</p><p>2 0.91639602 <a title="51-lda-2" href="../hunch_net-2006/hunch_net-2006-09-12-Incentive_Compatible_Reviewing.html">207 hunch net-2006-09-12-Incentive Compatible Reviewing</a></p>
<p>Introduction: Reviewing is a fairly formal process which is integral to the way academia is
run. Given this integral nature, the quality of reviewing is often
frustrating. I've seen plenty of examples of false statements, misbeliefs,
reading what isn't written, etcâ&euro;Ś, and I'm sure many other people have as
well.Recently, mechanisms like double blind review and author feedback have
been introduced to try to make the process more fair and accurate in many
machine learning (and related) conferences. My personal experience is that
these mechanisms help, especially the author feedback. Nevertheless, some
problems remain.The game theory take on reviewing is that the incentive for
truthful reviewing isn't there. Since reviewers are also authors, there are
sometimes perverse incentives created and acted upon. (Incidentially, these
incentives can be both positive and negative.)Setting up a truthful reviewing
system is tricky because their is no final reference truth available in any
acceptable (say: subyear)</p><p>3 0.91421789 <a title="51-lda-3" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>Introduction: Unfortunately, I ended up sick for much of this ICML. I did manage to catch
one interesting paper:Richard Socher,Cliff Lin,Andrew Y. Ng, andChristopher D.
ManningParsing Natural Scenes and Natural Language with Recursive Neural
Networks.I invited Richard to share his list of interesting papers, so
hopefully we'll hear from him soon. In the meantime,PaulandHalhave posted some
lists.the futureJoelleand I are program chairs for ICML 2012 inEdinburgh,
which I previously enjoyed visiting in2005. This is a huge responsibility,
that we hope to accomplish well. A part of this (perhaps the most fun part),
is imagining how we can make ICML better. A key and critical constraint is
choosing things that can be accomplished. So far we have:Colocation. The first
thing we looked into was potential colocations. We quickly discovered that
many other conferences precomitted their location. For the future, getting a
colocation withACLorSIGIR, seems to require more advanced planning. If that
can be done, I</p><p>4 0.91225743 <a title="51-lda-4" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>Introduction: One of the common trends in machine learning has been an emphasis on the use
of unlabeled data. The argument goes something like "there aren't many labeled
web pages out there, but there are ahugenumber of web pages, so we must find a
way to take advantage of them." There are several standard approaches for
doing this:Unsupervised Learning. You use only unlabeled data. In a typical
application, you cluster the data and hope that the clusters somehow
correspond to what you care about.Semisupervised Learning. You use both
unlabeled and labeled data to build a predictor. The unlabeled data influences
the learned predictor in some way.Active Learning. You have unlabeled data and
access to a labeling oracle. You interactively choose which examples to label
so as to optimize prediction accuracy.It seems there is a fourth approach
worth serious investigation--automated labeling. The approach goes as
follows:Identify some subset of observed values to predict from the
others.Build a predictor.U</p><p>5 0.90881139 <a title="51-lda-5" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>Introduction: One of the confusing things about research is that progress is very hard to
measure. One of the consequences of being in a hard-to-measure environment is
that the wrong things are often measured.Lines of CodeThe classical example of
this phenomenon is the old lines-of-code-produced metric for programming. It
is easy to imagine systems for producing many lines of code with very little
work that accomplish very little.Paper countIn academia, a "paper count" is an
analog of "lines of code", and it suffers from the same failure modes. The
obvious failure mode here is that we end up with a large number of
uninteresting papers since people end up spending a lot of time optimizing
this metric.ComplexityAnother metric, is "complexity" (in the eye of a
reviewer) of a paper. There is a common temptation to make a method appear
more complex than it is in order for reviewers to judge it worthy of
publication. The failure mode here is unclean thinking. Simple effective
methods are often overlooked</p><p>6 0.90234762 <a title="51-lda-6" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>7 0.89646834 <a title="51-lda-7" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>8 0.89337289 <a title="51-lda-8" href="../hunch_net-2010/hunch_net-2010-04-24-COLT_Treasurer_is_now_Phil_Long.html">394 hunch net-2010-04-24-COLT Treasurer is now Phil Long</a></p>
<p>9 0.89081347 <a title="51-lda-9" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>10 0.8903802 <a title="51-lda-10" href="../hunch_net-2006/hunch_net-2006-12-04-Structural_Problems_in_NIPS_Decision_Making.html">221 hunch net-2006-12-04-Structural Problems in NIPS Decision Making</a></p>
<p>11 0.89032543 <a title="51-lda-11" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>12 0.88986301 <a title="51-lda-12" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>13 0.88970202 <a title="51-lda-13" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>14 0.88919157 <a title="51-lda-14" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>15 0.88704056 <a title="51-lda-15" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>16 0.88680112 <a title="51-lda-16" href="../hunch_net-2008/hunch_net-2008-09-03-Bidding_Problems.html">315 hunch net-2008-09-03-Bidding Problems</a></p>
<p>17 0.88628805 <a title="51-lda-17" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>18 0.88452196 <a title="51-lda-18" href="../hunch_net-2011/hunch_net-2011-10-10-ML_Symposium_and_ICML_details.html">447 hunch net-2011-10-10-ML Symposium and ICML details</a></p>
<p>19 0.88214427 <a title="51-lda-19" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>20 0.88208652 <a title="51-lda-20" href="../hunch_net-2009/hunch_net-2009-02-22-Effective_Research_Funding.html">344 hunch net-2009-02-22-Effective Research Funding</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
