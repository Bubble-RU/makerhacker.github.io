<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>82 hunch net-2005-06-17-Reopening RL->Classification</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-82" href="#">hunch_net-2005-82</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>82 hunch net-2005-06-17-Reopening RL->Classification</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-82-html" href="http://hunch.net/?p=88">html</a></p><p>Introduction: In research, it’s often the case that solving a problem helps you realize that it wasn’t the right problem to solve.  This is the case for the “ reduce RL to classification ” problem with the solution hinted at  here  and turned into a paper  here .  
 
The essential difficulty is that the method of stating and analyzing reductions ends up being nonalgorithmic (unlike previous reductions) unless you work with learning from teleoperated robots as  Greg Grudic  does.  The difficulty here is due to the reduction being dependent on the optimal policy (which a human teleoperator might simulate, but which is otherwise unavailable).
 
So, this  problem  is “open” again with the caveat that this time we want a more algorithmic solution.  
 
Whether or not this is feasible at all is still unclear and evidence in either direction would greatly interest me.  A positive answer might have many practical implications in the long run.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In research, it’s often the case that solving a problem helps you realize that it wasn’t the right problem to solve. [sent-1, score-0.861]
</p><p>2 This is the case for the “ reduce RL to classification ” problem with the solution hinted at  here  and turned into a paper  here . [sent-2, score-0.83]
</p><p>3 The essential difficulty is that the method of stating and analyzing reductions ends up being nonalgorithmic (unlike previous reductions) unless you work with learning from teleoperated robots as  Greg Grudic  does. [sent-3, score-1.63]
</p><p>4 The difficulty here is due to the reduction being dependent on the optimal policy (which a human teleoperator might simulate, but which is otherwise unavailable). [sent-4, score-1.096]
</p><p>5 So, this  problem  is “open” again with the caveat that this time we want a more algorithmic solution. [sent-5, score-0.485]
</p><p>6 Whether or not this is feasible at all is still unclear and evidence in either direction would greatly interest me. [sent-6, score-0.92]
</p><p>7 A positive answer might have many practical implications in the long run. [sent-7, score-0.665]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('robots', 0.231), ('reductions', 0.215), ('greg', 0.214), ('simulate', 0.214), ('difficulty', 0.211), ('turned', 0.193), ('ends', 0.185), ('stating', 0.179), ('caveat', 0.173), ('feasible', 0.168), ('problem', 0.168), ('case', 0.164), ('analyzing', 0.16), ('unless', 0.156), ('implications', 0.153), ('realize', 0.15), ('wasn', 0.15), ('rl', 0.15), ('unlike', 0.144), ('algorithmic', 0.144), ('dependent', 0.142), ('positive', 0.133), ('unclear', 0.133), ('direction', 0.127), ('helps', 0.125), ('reduce', 0.124), ('policy', 0.121), ('otherwise', 0.112), ('practical', 0.111), ('human', 0.111), ('optimal', 0.111), ('reduction', 0.111), ('greatly', 0.111), ('essential', 0.105), ('evidence', 0.104), ('might', 0.101), ('previous', 0.098), ('open', 0.098), ('whether', 0.098), ('run', 0.097), ('still', 0.095), ('either', 0.095), ('classification', 0.095), ('method', 0.09), ('interest', 0.087), ('solving', 0.086), ('solution', 0.086), ('answer', 0.085), ('long', 0.082), ('due', 0.076)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="82-tfidf-1" href="../hunch_net-2005/hunch_net-2005-06-17-Reopening_RL-%3EClassification.html">82 hunch net-2005-06-17-Reopening RL->Classification</a></p>
<p>Introduction: In research, it’s often the case that solving a problem helps you realize that it wasn’t the right problem to solve.  This is the case for the “ reduce RL to classification ” problem with the solution hinted at  here  and turned into a paper  here .  
 
The essential difficulty is that the method of stating and analyzing reductions ends up being nonalgorithmic (unlike previous reductions) unless you work with learning from teleoperated robots as  Greg Grudic  does.  The difficulty here is due to the reduction being dependent on the optimal policy (which a human teleoperator might simulate, but which is otherwise unavailable).
 
So, this  problem  is “open” again with the caveat that this time we want a more algorithmic solution.  
 
Whether or not this is feasible at all is still unclear and evidence in either direction would greatly interest me.  A positive answer might have many practical implications in the long run.</p><p>2 0.15867627 <a title="82-tfidf-2" href="../hunch_net-2005/hunch_net-2005-06-18-Lower_Bounds_for_Learning_Reductions.html">83 hunch net-2005-06-18-Lower Bounds for Learning Reductions</a></p>
<p>Introduction: Learning reductions  transform a solver of one type of learning problem into a solver of another type of learning problem.  When we analyze these for robustness we can make statement of the form “Reduction  R  has the property that regret  r  (or loss) on subproblems of type  A  implies regret at most   f ( r )  on the original problem of type  B “.
 
A lower bound for a learning reduction would have the form “for all reductions  R , there exists a learning problem of type  B  and learning algorithm for problems of type  A  where regret  r  on induced problems implies  at least  regret  f ( r )  for  B “.
 
The pursuit of lower bounds is often questionable because, unlike upper bounds, they do not yield practical algorithms.  Nevertheless, they may be helpful as a tool for thinking about what is learnable and how learnable it is.  This has already come up  here  and  here .
 
At the moment, there is no coherent theory of lower bounds for learning reductions, and we have little understa</p><p>3 0.158136 <a title="82-tfidf-3" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>Introduction: What?  Reductions are machines which turn solvers for one problem into solvers for another problem. 
 Why?  Reductions are useful for several reasons.
  
  Laziness .  Reducing a problem to classification make at least 10 learning algorithms available to solve a problem.  Inventing 10 learning algorithms is quite a bit of work.  Similarly, programming a reduction is often trivial, while programming a learning algorithm is a great deal of work. 
  Crystallization .  The problems we often want to solve in learning are worst-case-impossible, but average case feasible.  By reducing all problems onto one or a few primitives, we can fine tune these primitives to perform well on real-world problems with greater precision due to the greater number of problems to validate on. 
  Theoretical Organization .  By studying what reductions are easy vs. hard vs. impossible, we can learn which problems are roughly equivalent in difficulty and which are much harder. 
  
 What we know now .
 
 Typesafe r</p><p>4 0.13153504 <a title="82-tfidf-4" href="../hunch_net-2007/hunch_net-2007-03-15-Alternative_Machine_Learning_Reductions_Definitions.html">236 hunch net-2007-03-15-Alternative Machine Learning Reductions Definitions</a></p>
<p>Introduction: A type of prediction problem is specified by the type of samples produced by a data source (Example:  X x {0,1} ,  X x [0,1] ,  X x {1,2,3,4,5} , etc…) and a loss function (0/1 loss, squared error loss, cost sensitive losses, etc…).  For simplicity, we’ll assume that all losses have a minimum of zero.
 
For this post, we can think of a learning reduction as
  
 A mapping  R  from samples of one type  T  (like multiclass classification) to another type  T’  (like binary classification). 
 A mapping  Q  from predictors for type  T’  to predictors for type  T . 
  
The simplest sort of learning reduction is a “loss reduction”.  The idea in a loss reduction is to prove a statement of the form: 
 Theorem  For all base predictors  b , for all distributions  D  over examples of type  T : 
  E (x,y) ~ D  L T (y,Q(b,x)) <= f(E (x’,y’)~R(D)  L T’ (y’,b(x’)))   
Here  L T   is the loss for the type  T  problem and  L T’   is the loss for the type  T’  problem.  Also,  R(D)  is the distribution ov</p><p>5 0.11600762 <a title="82-tfidf-5" href="../hunch_net-2005/hunch_net-2005-08-04-Why_Reinforcement_Learning_is_Important.html">100 hunch net-2005-08-04-Why Reinforcement Learning is Important</a></p>
<p>Introduction: One prescription for solving a problem well is:
  
 State the problem, in the simplest way possible. In particular, this statement should involve no contamination with or anticipation of the solution. 
 Think about solutions to the stated problem. 
  
Stating a problem in a succinct and crisp manner tends to invite a simple elegant solution.  When a problem can not be stated succinctly, we  wonder if the problem is even understood. (And when a problem is not understood, we wonder if a solution can be meaningful.)
 
Reinforcement learning does step (1) well.  It provides a clean simple language to state general AI problems.  In reinforcement learning there is a set of actions  A , a set of observations  O , and a reward  r .  The reinforcement learning problem, in general, is defined by a conditional measure  D( o, r | (o,r,a) * )  which produces an observation  o  and a reward  r  given a history  (o,r,a) *  .  The goal in reinforcement learning is to find a policy  pi:(o,r,a) *  -> a</p><p>6 0.11277928 <a title="82-tfidf-6" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>7 0.10696791 <a title="82-tfidf-7" href="../hunch_net-2005/hunch_net-2005-08-18-SVM_Adaptability.html">103 hunch net-2005-08-18-SVM Adaptability</a></p>
<p>8 0.099687263 <a title="82-tfidf-8" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>9 0.096859984 <a title="82-tfidf-9" href="../hunch_net-2006/hunch_net-2006-05-23-What_is_the_best_regret_transform_reduction_from_multiclass_to_binary%3F.html">181 hunch net-2006-05-23-What is the best regret transform reduction from multiclass to binary?</a></p>
<p>10 0.094653949 <a title="82-tfidf-10" href="../hunch_net-2009/hunch_net-2009-06-01-Multitask_Poisoning.html">358 hunch net-2009-06-01-Multitask Poisoning</a></p>
<p>11 0.093783505 <a title="82-tfidf-11" href="../hunch_net-2005/hunch_net-2005-02-23-Problem%3A_Reinforcement_Learning_with_Classification.html">27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</a></p>
<p>12 0.090478964 <a title="82-tfidf-12" href="../hunch_net-2005/hunch_net-2005-06-06-Exact_Online_Learning_for_Classification.html">78 hunch net-2005-06-06-Exact Online Learning for Classification</a></p>
<p>13 0.089552902 <a title="82-tfidf-13" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>14 0.085101701 <a title="82-tfidf-14" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>15 0.083845749 <a title="82-tfidf-15" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>16 0.083768837 <a title="82-tfidf-16" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>17 0.083322287 <a title="82-tfidf-17" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>18 0.081779703 <a title="82-tfidf-18" href="../hunch_net-2007/hunch_net-2007-10-24-Contextual_Bandits.html">269 hunch net-2007-10-24-Contextual Bandits</a></p>
<p>19 0.081015088 <a title="82-tfidf-19" href="../hunch_net-2005/hunch_net-2005-02-26-Problem%3A_Reductions_and_Relative_Ranking_Metrics.html">31 hunch net-2005-02-26-Problem: Reductions and Relative Ranking Metrics</a></p>
<p>20 0.079853214 <a title="82-tfidf-20" href="../hunch_net-2005/hunch_net-2005-07-10-Thinking_the_Unthought.html">91 hunch net-2005-07-10-Thinking the Unthought</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.186), (1, 0.05), (2, -0.0), (3, 0.016), (4, -0.061), (5, -0.024), (6, 0.17), (7, 0.006), (8, -0.06), (9, -0.007), (10, -0.076), (11, -0.067), (12, -0.056), (13, 0.117), (14, 0.027), (15, -0.009), (16, 0.091), (17, -0.063), (18, -0.019), (19, -0.02), (20, -0.062), (21, 0.006), (22, -0.057), (23, -0.073), (24, 0.027), (25, 0.094), (26, 0.013), (27, 0.093), (28, 0.03), (29, 0.071), (30, 0.093), (31, -0.007), (32, -0.03), (33, -0.031), (34, -0.013), (35, -0.005), (36, -0.046), (37, 0.001), (38, 0.038), (39, 0.005), (40, 0.04), (41, 0.073), (42, 0.069), (43, 0.064), (44, 0.003), (45, -0.017), (46, -0.019), (47, -0.008), (48, -0.02), (49, -0.015)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97505915 <a title="82-lsi-1" href="../hunch_net-2005/hunch_net-2005-06-17-Reopening_RL-%3EClassification.html">82 hunch net-2005-06-17-Reopening RL->Classification</a></p>
<p>Introduction: In research, it’s often the case that solving a problem helps you realize that it wasn’t the right problem to solve.  This is the case for the “ reduce RL to classification ” problem with the solution hinted at  here  and turned into a paper  here .  
 
The essential difficulty is that the method of stating and analyzing reductions ends up being nonalgorithmic (unlike previous reductions) unless you work with learning from teleoperated robots as  Greg Grudic  does.  The difficulty here is due to the reduction being dependent on the optimal policy (which a human teleoperator might simulate, but which is otherwise unavailable).
 
So, this  problem  is “open” again with the caveat that this time we want a more algorithmic solution.  
 
Whether or not this is feasible at all is still unclear and evidence in either direction would greatly interest me.  A positive answer might have many practical implications in the long run.</p><p>2 0.69254494 <a title="82-lsi-2" href="../hunch_net-2005/hunch_net-2005-08-04-Why_Reinforcement_Learning_is_Important.html">100 hunch net-2005-08-04-Why Reinforcement Learning is Important</a></p>
<p>Introduction: One prescription for solving a problem well is:
  
 State the problem, in the simplest way possible. In particular, this statement should involve no contamination with or anticipation of the solution. 
 Think about solutions to the stated problem. 
  
Stating a problem in a succinct and crisp manner tends to invite a simple elegant solution.  When a problem can not be stated succinctly, we  wonder if the problem is even understood. (And when a problem is not understood, we wonder if a solution can be meaningful.)
 
Reinforcement learning does step (1) well.  It provides a clean simple language to state general AI problems.  In reinforcement learning there is a set of actions  A , a set of observations  O , and a reward  r .  The reinforcement learning problem, in general, is defined by a conditional measure  D( o, r | (o,r,a) * )  which produces an observation  o  and a reward  r  given a history  (o,r,a) *  .  The goal in reinforcement learning is to find a policy  pi:(o,r,a) *  -> a</p><p>3 0.66811687 <a title="82-lsi-3" href="../hunch_net-2005/hunch_net-2005-02-23-Problem%3A_Reinforcement_Learning_with_Classification.html">27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</a></p>
<p>Introduction: At an intuitive level, the question here is “Can reinforcement learning be solved with classification?”  
 
 Problem  Construct a reinforcement learning algorithm with near-optimal expected sum of rewards in the  direct experience model  given access to a classifier learning algorithm which has a small error rate or regret on all posed classification problems.  The definition of “posed” here is slightly murky.  I consider a problem “posed” if there is an algorithm for constructing labeled classification examples.
 
 Past Work 
  
 There exists a  reduction of reinforcement learning to classification given a generative model.   A generative model is an inherently stronger assumption than the direct experience model. 
 Other  work on learning reductions  may be important. 
 Several algorithms for solving reinforcement learning in the direct experience model exist.  Most, such as  E 3  ,  Factored-E 3  , and  metric-E 3   and  Rmax  require that the observation be the state.  Recent work</p><p>4 0.66247576 <a title="82-lsi-4" href="../hunch_net-2005/hunch_net-2005-05-10-Learning_Reductions_are_Reductionist.html">68 hunch net-2005-05-10-Learning Reductions are Reductionist</a></p>
<p>Introduction: This is about a fundamental motivation for the investigation of reductions in learning.  It applies to many pieces of work other than my own.
 
The reductionist approach to problem solving is characterized by taking a problem, decomposing it into as-small-as-possible subproblems, discovering how to solve the subproblems, and then discovering how to use the solutions to the subproblems to solve larger problems.  The reductionist approach to solving problems has often payed off  very  well.  Computer science related examples of the reductionist approach include:
  
 Reducing computation to the transistor. All of our CPUs are built from transistors. 
 Reducing rendering of images to rendering a triangle (or other simple polygons).  Computers can now render near-realistic scenes in real time. The big breakthrough came from learning how to render many triangles quickly.
  
  
This approach to problem solving extends well beyond computer science.  Many fields of science focus on theories mak</p><p>5 0.66119534 <a title="82-lsi-5" href="../hunch_net-2005/hunch_net-2005-06-18-Lower_Bounds_for_Learning_Reductions.html">83 hunch net-2005-06-18-Lower Bounds for Learning Reductions</a></p>
<p>Introduction: Learning reductions  transform a solver of one type of learning problem into a solver of another type of learning problem.  When we analyze these for robustness we can make statement of the form “Reduction  R  has the property that regret  r  (or loss) on subproblems of type  A  implies regret at most   f ( r )  on the original problem of type  B “.
 
A lower bound for a learning reduction would have the form “for all reductions  R , there exists a learning problem of type  B  and learning algorithm for problems of type  A  where regret  r  on induced problems implies  at least  regret  f ( r )  for  B “.
 
The pursuit of lower bounds is often questionable because, unlike upper bounds, they do not yield practical algorithms.  Nevertheless, they may be helpful as a tool for thinking about what is learnable and how learnable it is.  This has already come up  here  and  here .
 
At the moment, there is no coherent theory of lower bounds for learning reductions, and we have little understa</p><p>6 0.66062683 <a title="82-lsi-6" href="../hunch_net-2008/hunch_net-2008-03-15-COLT_Open_Problems.html">292 hunch net-2008-03-15-COLT Open Problems</a></p>
<p>7 0.64273351 <a title="82-lsi-7" href="../hunch_net-2007/hunch_net-2007-03-15-Alternative_Machine_Learning_Reductions_Definitions.html">236 hunch net-2007-03-15-Alternative Machine Learning Reductions Definitions</a></p>
<p>8 0.63606924 <a title="82-lsi-8" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>9 0.62441123 <a title="82-lsi-9" href="../hunch_net-2012/hunch_net-2012-03-06-COLT-ICML_Open_Questions_and_ICML_Instructions.html">458 hunch net-2012-03-06-COLT-ICML Open Questions and ICML Instructions</a></p>
<p>10 0.59911478 <a title="82-lsi-10" href="../hunch_net-2005/hunch_net-2005-08-18-SVM_Adaptability.html">103 hunch net-2005-08-18-SVM Adaptability</a></p>
<p>11 0.58669519 <a title="82-lsi-11" href="../hunch_net-2005/hunch_net-2005-02-25-Solution%3A_Reinforcement_Learning_with_Classification.html">29 hunch net-2005-02-25-Solution: Reinforcement Learning with Classification</a></p>
<p>12 0.55219692 <a title="82-lsi-12" href="../hunch_net-2010/hunch_net-2010-03-15-The_Efficient_Robust_Conditional_Probability_Estimation_Problem.html">391 hunch net-2010-03-15-The Efficient Robust Conditional Probability Estimation Problem</a></p>
<p>13 0.54720384 <a title="82-lsi-13" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>14 0.54192859 <a title="82-lsi-14" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>15 0.53932232 <a title="82-lsi-15" href="../hunch_net-2009/hunch_net-2009-06-01-Multitask_Poisoning.html">358 hunch net-2009-06-01-Multitask Poisoning</a></p>
<p>16 0.53065073 <a title="82-lsi-16" href="../hunch_net-2005/hunch_net-2005-04-21-Dynamic_Programming_Generalizations_and_Their_Use.html">58 hunch net-2005-04-21-Dynamic Programming Generalizations and Their Use</a></p>
<p>17 0.52833599 <a title="82-lsi-17" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>18 0.52289081 <a title="82-lsi-18" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>19 0.519023 <a title="82-lsi-19" href="../hunch_net-2011/hunch_net-2011-04-06-COLT_open_questions.html">429 hunch net-2011-04-06-COLT open questions</a></p>
<p>20 0.51525289 <a title="82-lsi-20" href="../hunch_net-2006/hunch_net-2006-05-23-What_is_the_best_regret_transform_reduction_from_multiclass_to_binary%3F.html">181 hunch net-2006-05-23-What is the best regret transform reduction from multiclass to binary?</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(27, 0.243), (34, 0.262), (38, 0.147), (53, 0.099), (55, 0.115), (94, 0.019)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94435954 <a title="82-lda-1" href="../hunch_net-2005/hunch_net-2005-06-17-Reopening_RL-%3EClassification.html">82 hunch net-2005-06-17-Reopening RL->Classification</a></p>
<p>Introduction: In research, it’s often the case that solving a problem helps you realize that it wasn’t the right problem to solve.  This is the case for the “ reduce RL to classification ” problem with the solution hinted at  here  and turned into a paper  here .  
 
The essential difficulty is that the method of stating and analyzing reductions ends up being nonalgorithmic (unlike previous reductions) unless you work with learning from teleoperated robots as  Greg Grudic  does.  The difficulty here is due to the reduction being dependent on the optimal policy (which a human teleoperator might simulate, but which is otherwise unavailable).
 
So, this  problem  is “open” again with the caveat that this time we want a more algorithmic solution.  
 
Whether or not this is feasible at all is still unclear and evidence in either direction would greatly interest me.  A positive answer might have many practical implications in the long run.</p><p>2 0.92245096 <a title="82-lda-2" href="../hunch_net-2006/hunch_net-2006-02-02-Introspectionism_as_a_Disease.html">153 hunch net-2006-02-02-Introspectionism as a Disease</a></p>
<p>Introduction: In the AI-related parts of machine learning, it is often tempting to examine how  you  do things in order to imagine how a machine should do things.  This is introspection, and it can easily go awry.  I will call introspection gone awry introspectionism.
 
Introspectionism is almost unique to AI (and the AI-related parts of machine learning) and it can lead to huge wasted effort in research.  It’s easiest to show how introspectionism arises by an example.
 
Suppose we want to solve the problem of navigating a robot from point A to point B given a camera.  Then, the following research action plan might seem natural when you examine your own capabilities:
  
 Build an edge detector for still images. 
 Build an object recognition system given the edge detector. 
 Build a system to predict distance and orientation to objects given the object recognition system. 
 Build a system to plan a path through the scene you construct from {object identification, distance, orientation} predictions.</p><p>3 0.90510434 <a title="82-lda-3" href="../hunch_net-2010/hunch_net-2010-10-28-NY_ML_Symposium_2010.html">415 hunch net-2010-10-28-NY ML Symposium 2010</a></p>
<p>Introduction: About 200 people attended the  2010 NYAS ML Symposium  this year.  (It was  about 170 last year .)  I particularly enjoyed several talks.
  
  Yann  has a new live demo of (limited) real-time object recognition learning.  
  Sanjoy  gave a fairly convincing and comprehensible explanation of why a  modified form of single-linkage clustering  is consistent in higher dimensions, and why consistency is a critical feature for clustering algorithms.  I’m curious how well this algorithm works in practice. 
  Matt Hoffman ‘s poster covering online LDA seemed pretty convincing to me as an algorithmic improvement. 
  
This year, we allocated more time towards posters & poster spotlights.  
 
For next year, we are considering some further changes.  The format has traditionally been 4 invited Professor speakers, with posters and poster spotlight for students.  Demand from other parties to participate is growing, for example from postdocs and startups in the area.  Another growing concern is the fa</p><p>4 0.88296264 <a title="82-lda-4" href="../hunch_net-2006/hunch_net-2006-06-30-ICML_papers.html">188 hunch net-2006-06-30-ICML papers</a></p>
<p>Introduction: Here are some ICML papers which interested me.
  
  Arindam Banerjee  had a  paper  which notes that PAC-Bayes bounds, a core theorem in online learning, and the optimality of Bayesian learning statements share a core inequality in their proof. 
  Pieter Abbeel ,  Morgan Quigley  and  Andrew Y. Ng  have a  paper  discussing RL techniques for learning given a bad (but not too bad) model of the world. 
  Nina Balcan  and  Avrim Blum  have a  paper  which discusses how to learn given a similarity function rather than a kernel.  A similarity function requires less structure than a kernel, implying that a learning algorithm using a similarity function might be applied in situations where no effective kernel is evident. 
  Nathan Ratliff ,  Drew Bagnell , and  Marty Zinkevich  have a  paper  describing an algorithm which attempts to fuse A *  path planning with learning of transition costs based on human demonstration. 
  
Papers (2), (3), and (4), all seem like an initial pass at solving in</p><p>5 0.75567597 <a title="82-lda-5" href="../hunch_net-2012/hunch_net-2012-02-29-Key_Scientific_Challenges_and_the_Franklin_Symposium.html">457 hunch net-2012-02-29-Key Scientific Challenges and the Franklin Symposium</a></p>
<p>Introduction: For graduate students, the  Yahoo!   Key Scientific Challenges program  including in  machine learning  is on again,  due March 9 .  The application is easy and the $5K award is high quality “no strings attached” funding.   Consider submitting.
 
Those in Washington DC, Philadelphia, and New York, may consider attending the  Franklin Institute Symposium   April 25  which has several speakers and an award for  V .  Attendance is free with an RSVP.</p><p>6 0.74115628 <a title="82-lda-6" href="../hunch_net-2009/hunch_net-2009-05-08-Computability_in_Artificial_Intelligence.html">353 hunch net-2009-05-08-Computability in Artificial Intelligence</a></p>
<p>7 0.73643589 <a title="82-lda-7" href="../hunch_net-2007/hunch_net-2007-03-15-Alternative_Machine_Learning_Reductions_Definitions.html">236 hunch net-2007-03-15-Alternative Machine Learning Reductions Definitions</a></p>
<p>8 0.7354539 <a title="82-lda-8" href="../hunch_net-2005/hunch_net-2005-11-16-The_Everything_Ensemble_Edge.html">131 hunch net-2005-11-16-The Everything Ensemble Edge</a></p>
<p>9 0.729693 <a title="82-lda-9" href="../hunch_net-2007/hunch_net-2007-02-16-The_Forgetting.html">233 hunch net-2007-02-16-The Forgetting</a></p>
<p>10 0.72250336 <a title="82-lda-10" href="../hunch_net-2007/hunch_net-2007-01-10-A_Deep_Belief_Net_Learning_Problem.html">227 hunch net-2007-01-10-A Deep Belief Net Learning Problem</a></p>
<p>11 0.72036099 <a title="82-lda-11" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>12 0.71508193 <a title="82-lda-12" href="../hunch_net-2007/hunch_net-2007-06-24-Interesting_Papers_at_ICML_2007.html">251 hunch net-2007-06-24-Interesting Papers at ICML 2007</a></p>
<p>13 0.71423835 <a title="82-lda-13" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>14 0.71262383 <a title="82-lda-14" href="../hunch_net-2005/hunch_net-2005-05-16-Regret_minimizing_vs_error_limiting_reductions.html">72 hunch net-2005-05-16-Regret minimizing vs error limiting reductions</a></p>
<p>15 0.70771867 <a title="82-lda-15" href="../hunch_net-2007/hunch_net-2007-02-02-Thoughts_regarding_%26%238220%3BIs_machine_learning_different_from_statistics%3F%26%238221%3B.html">230 hunch net-2007-02-02-Thoughts regarding &#8220;Is machine learning different from statistics?&#8221;</a></p>
<p>16 0.70716226 <a title="82-lda-16" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>17 0.70430446 <a title="82-lda-17" href="../hunch_net-2006/hunch_net-2006-07-11-New_Models.html">194 hunch net-2006-07-11-New Models</a></p>
<p>18 0.70263213 <a title="82-lda-18" href="../hunch_net-2005/hunch_net-2005-02-03-Learning_Theory%2C_by_assumption.html">12 hunch net-2005-02-03-Learning Theory, by assumption</a></p>
<p>19 0.70148301 <a title="82-lda-19" href="../hunch_net-2005/hunch_net-2005-03-21-Research_Styles_in_Machine_Learning.html">44 hunch net-2005-03-21-Research Styles in Machine Learning</a></p>
<p>20 0.70049101 <a title="82-lda-20" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
