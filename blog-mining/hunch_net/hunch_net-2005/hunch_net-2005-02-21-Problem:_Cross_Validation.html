<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>26 hunch net-2005-02-21-Problem: Cross Validation</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-26" href="#">hunch_net-2005-26</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>26 hunch net-2005-02-21-Problem: Cross Validation</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-26-html" href="http://hunch.net/?p=29">html</a></p><p>Introduction: The essential problem here is the large gap between experimental observation
and theoretical understanding.MethodK-fold cross validation is a commonly used
technique which takes a set ofmexamples and partitions them intoKsets
("folds") of sizem/K. For each fold, a classifier is trained on the other
folds and then test on the fold.ProblemAssume only independent samples. Derive
a classifier from the K classifiers with a small bound on the true error
rate.Past Work(I'll add more as I remember/learn.)Devroye, Rogers, and Wagner
analyzed cross validation and found algorithm specific bounds. Not all of this
is online, but here is onepaper.Michael KearnsandDana Ronanalyzed cross
validationand found that under additional stability assumptions the bound for
the classifier which learns on all the data is not much worse than for a test
set of sizem/K.Avrim Blum,Adam Kalai, andmyselfanalyzed cross validationand
found that you can do at least as well as a test set of sizem/Kwith no
additional assum</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The essential problem here is the large gap between experimental observation and theoretical understanding. [sent-1, score-0.128]
</p><p>2 MethodK-fold cross validation is a commonly used technique which takes a set ofmexamples and partitions them intoKsets ("folds") of sizem/K. [sent-2, score-1.007]
</p><p>3 For each fold, a classifier is trained on the other folds and then test on the fold. [sent-3, score-0.714]
</p><p>4 Derive a classifier from the K classifiers with a small bound on the true error rate. [sent-5, score-0.462]
</p><p>5 )Devroye, Rogers, and Wagner analyzed cross validation and found algorithm specific bounds. [sent-7, score-1.008]
</p><p>6 Michael KearnsandDana Ronanalyzed cross validationand found that under additional stability assumptions the bound for the classifier which learns on all the data is not much worse than for a test set of sizem/K. [sent-9, score-1.99]
</p><p>7 Avrim Blum,Adam Kalai, andmyselfanalyzed cross validationand found that you can do at least as well as a test set of sizem/Kwith no additional assumptions using the randomized classifier which draws uniformly from the set of sizeK. [sent-10, score-2.147]
</p><p>8 Yoshua Bengio andYves Grandvaletanalyzed cross validationand concluded that there was no unbiased estimator of variance. [sent-11, score-0.973]
</p><p>9 Matti KÃ¤Ã¤riÃ¤inennoted that you can safely derandomize a stochastic classifier (such as one that randomizes over theKfolds)using unlabeled datawithout additional assumptions. [sent-12, score-0.654]
</p><p>10 Some Extreme Cases to Sharpen IntuitionSuppose on every fold the learned classifier is the same. [sent-13, score-0.446]
</p><p>11 Then, the cross-validation error should behave something like a test set of sizem. [sent-14, score-0.497]
</p><p>12 This is radically superior to a test set of sizem/K. [sent-15, score-0.472]
</p><p>13 Suppose we have a "learning" algorithm that uses the classification rule "always predict the parity of the labels on the training set". [sent-17, score-0.053]
</p><p>14 Suppose the learning problem is defined by a distribution which picksy=1with probability0. [sent-18, score-0.066]
</p><p>15 5, all leave-one-out errors will be0and otherwise1(like a single coin flip). [sent-21, score-0.142]
</p><p>16 I've worked on it without success and it's an obvious problem (due to the pervasive use of cross validation) that I suspect other people have considered. [sent-23, score-0.724]
</p><p>17 Analyzing the dependency structure of cross validation is quite difficult. [sent-24, score-0.929]
</p><p>18 ImpactOn any individual problem, solving this might have only have a small impact due to slightly improved judgement of success. [sent-25, score-0.341]
</p><p>19 But, because cross validation is used extensively, the overall impact of a good solution might be very significant. [sent-26, score-0.93]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cross', 0.522), ('validation', 0.319), ('validationand', 0.297), ('classifier', 0.248), ('fold', 0.198), ('folds', 0.198), ('test', 0.191), ('set', 0.166), ('additional', 0.139), ('suppose', 0.111), ('found', 0.099), ('assumptions', 0.095), ('impact', 0.089), ('dependency', 0.088), ('draws', 0.088), ('kalai', 0.088), ('randomizes', 0.088), ('rogers', 0.088), ('bound', 0.083), ('coin', 0.081), ('estimator', 0.081), ('ishere', 0.081), ('ri', 0.081), ('bengio', 0.077), ('learns', 0.077), ('trained', 0.077), ('stability', 0.073), ('unbiased', 0.073), ('behave', 0.07), ('judgement', 0.07), ('pervasive', 0.07), ('error', 0.07), ('randomized', 0.068), ('analyzed', 0.068), ('safely', 0.068), ('uniformly', 0.068), ('problem', 0.066), ('suspect', 0.066), ('derive', 0.064), ('gap', 0.062), ('analyzing', 0.062), ('due', 0.062), ('errors', 0.061), ('small', 0.061), ('slightly', 0.059), ('superior', 0.058), ('radically', 0.057), ('stochastic', 0.057), ('unlabeled', 0.054), ('rule', 0.053)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="26-tfidf-1" href="../hunch_net-2005/hunch_net-2005-02-21-Problem%3A_Cross_Validation.html">26 hunch net-2005-02-21-Problem: Cross Validation</a></p>
<p>Introduction: The essential problem here is the large gap between experimental observation
and theoretical understanding.MethodK-fold cross validation is a commonly used
technique which takes a set ofmexamples and partitions them intoKsets
("folds") of sizem/K. For each fold, a classifier is trained on the other
folds and then test on the fold.ProblemAssume only independent samples. Derive
a classifier from the K classifiers with a small bound on the true error
rate.Past Work(I'll add more as I remember/learn.)Devroye, Rogers, and Wagner
analyzed cross validation and found algorithm specific bounds. Not all of this
is online, but here is onepaper.Michael KearnsandDana Ronanalyzed cross
validationand found that under additional stability assumptions the bound for
the classifier which learns on all the data is not much worse than for a test
set of sizem/K.Avrim Blum,Adam Kalai, andmyselfanalyzed cross validationand
found that you can do at least as well as a test set of sizem/Kwith no
additional assum</p><p>2 0.22125924 <a title="26-tfidf-2" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>Introduction: "Overfitting" is traditionally defined as training some flexible
representation so that it memorizes the data but fails to predict well in the
future. For this post, I will define overfitting more generally as over-
representing the performance of systems. There are two styles of general
overfitting: overrepresenting performance on particular datasets and
(implicitly) overrepresenting performance of a method on future datasets.We
should all be aware of these methods, avoid them where possible, and take them
into account otherwise. I have used "reproblem" and "old datasets", and may
have participated in "overfitting by review"--some of these are very difficult
to avoid.NameMethodExplanationRemedyTraditional overfittingTrain a complex
predictor on too-few examples.Hold out pristine examples for testing.Use a
simpler predictor.Get more training examples.Integrate over many
predictors.Reject papers which do this.Parameter tweak overfittingUse a
learning algorithm with many parameters. Choo</p><p>3 0.16443786 <a title="26-tfidf-3" href="../hunch_net-2007/hunch_net-2007-06-14-Interesting_Papers_at_COLT_2007.html">247 hunch net-2007-06-14-Interesting Papers at COLT 2007</a></p>
<p>Introduction: Here are two papers that seem particularly interesting at this year's
COLT.Gilles BlanchardandFranÃƒÂ§ois Fleuret,Occam's Hammer. When we are
interested in very tight bounds on the true error rate of a classifier, it is
tempting to use a PAC-Bayes bound which can (empirically) bequite tight. A
disadvantage of the PAC-Bayes bound is that it applies to a classifier which
is randomized over a set of base classifiers rather than a single classifier.
This paper shows that a similar bound can be proved which holds for a single
classifier drawn from the set. The ability to safely use a single classifier
is very nice. This technique applies generically to any base bound, so it has
other applications covered in the paper.Adam Tauman Kalai.Learning Nested
Halfspaces and Uphill Decision Trees. Classification PAC-learning, where you
prove that any problem amongst some set is polytime learnable with respect to
any distribution over the inputXis extraordinarily challenging as judged by
lack of progr</p><p>4 0.15700147 <a title="26-tfidf-4" href="../hunch_net-2005/hunch_net-2005-03-15-The_State_of_Tight_Bounds.html">41 hunch net-2005-03-15-The State of Tight Bounds</a></p>
<p>Introduction: What?Bounds are mathematical formulas relating observations to future error
rates assuming that data is drawn independently. In classical statistics, they
are calld confidence intervals.Why?Good Judgement. In many applications of
learning, it is desirable to know how well the learned predictor works in the
future. This helps you decide if the problem is solved or not.Learning
Essence. The form of some of these bounds helps you understand what the
essence of learning is.Algorithm Design. Some of these bounds suggest,
motivate, or even directly imply learning algorithms.What We Know NowThere are
several families of bounds, based on how information is used.Testing Bounds.
These are methods which use labeled data not used in training to estimate the
future error rate. Examples include thetest set bound,progressive
validationalsohereandhere,train and test bounds, and cross-validation (but see
thebig open problem). These techniques are the best available for goal (1)
above, but provide littl</p><p>5 0.15015994 <a title="26-tfidf-5" href="../hunch_net-2005/hunch_net-2005-11-16-The_Everything_Ensemble_Edge.html">131 hunch net-2005-11-16-The Everything Ensemble Edge</a></p>
<p>Introduction: Rich Caruana,Alexandru Niculescu, Geoff Crew, and Alex Ksikes have donea lot
of empirical testingwhich shows thatusing all methods to make a predictionis
more powerful than using any single method. This is in rough agreement with
the Bayesian way of solving problems, but based upon a different (essentially
empirical) motivation. A rough summary is:Take all of {decision trees, boosted
decision trees, bagged decision trees, boosted decision stumps, K nearest
neighbors, neural networks, SVM} with all reasonable parameter settings.Run
the methods on each problem of 8 problems with a large test set, calibrating
margins using eithersigmoid fittingorisotonic regression.For each loss of
{accuracy, area under the ROC curve, cross entropy, squared error, etcâ&euro;Ś}
evaluate the average performance of the method.A series of conclusions can be
drawn from the observations.(Calibrated) boosted decision trees appear to
perform best, in general although support vector machines and neural networks
give cred</p><p>6 0.14142747 <a title="26-tfidf-6" href="../hunch_net-2005/hunch_net-2005-03-21-Research_Styles_in_Machine_Learning.html">44 hunch net-2005-03-21-Research Styles in Machine Learning</a></p>
<p>7 0.1304573 <a title="26-tfidf-7" href="../hunch_net-2005/hunch_net-2005-03-18-Binomial_Weighting.html">43 hunch net-2005-03-18-Binomial Weighting</a></p>
<p>8 0.12108176 <a title="26-tfidf-8" href="../hunch_net-2005/hunch_net-2005-01-24-Holy_grails_of_machine_learning%3F.html">2 hunch net-2005-01-24-Holy grails of machine learning?</a></p>
<p>9 0.11727242 <a title="26-tfidf-9" href="../hunch_net-2005/hunch_net-2005-05-16-Regret_minimizing_vs_error_limiting_reductions.html">72 hunch net-2005-05-16-Regret minimizing vs error limiting reductions</a></p>
<p>10 0.11657635 <a title="26-tfidf-10" href="../hunch_net-2005/hunch_net-2005-06-28-A_COLT_paper.html">85 hunch net-2005-06-28-A COLT paper</a></p>
<p>11 0.095465824 <a title="26-tfidf-11" href="../hunch_net-2005/hunch_net-2005-03-22-Active_learning.html">45 hunch net-2005-03-22-Active learning</a></p>
<p>12 0.090977609 <a title="26-tfidf-12" href="../hunch_net-2007/hunch_net-2007-06-24-Interesting_Papers_at_ICML_2007.html">251 hunch net-2007-06-24-Interesting Papers at ICML 2007</a></p>
<p>13 0.087475047 <a title="26-tfidf-13" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>14 0.087281995 <a title="26-tfidf-14" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>15 0.085592888 <a title="26-tfidf-15" href="../hunch_net-2007/hunch_net-2007-11-28-Computational_Consequences_of_Classification.html">274 hunch net-2007-11-28-Computational Consequences of Classification</a></p>
<p>16 0.085174248 <a title="26-tfidf-16" href="../hunch_net-2007/hunch_net-2007-08-19-Choice_of_Metrics.html">259 hunch net-2007-08-19-Choice of Metrics</a></p>
<p>17 0.084498748 <a title="26-tfidf-17" href="../hunch_net-2006/hunch_net-2006-02-18-Multiplication_of_Learned_Probabilities_is_Dangerous.html">157 hunch net-2006-02-18-Multiplication of Learned Probabilities is Dangerous</a></p>
<p>18 0.083541818 <a title="26-tfidf-18" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>19 0.082990438 <a title="26-tfidf-19" href="../hunch_net-2005/hunch_net-2005-04-27-DARPA_project%3A_LAGR.html">63 hunch net-2005-04-27-DARPA project: LAGR</a></p>
<p>20 0.080250122 <a title="26-tfidf-20" href="../hunch_net-2005/hunch_net-2005-09-26-Prediction_Bounds_as_the_Mathematics_of_Science.html">115 hunch net-2005-09-26-Prediction Bounds as the Mathematics of Science</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.161), (1, -0.115), (2, -0.064), (3, 0.023), (4, -0.052), (5, -0.138), (6, 0.045), (7, 0.021), (8, 0.087), (9, 0.001), (10, -0.095), (11, 0.002), (12, -0.161), (13, -0.031), (14, 0.0), (15, -0.008), (16, -0.041), (17, 0.023), (18, 0.105), (19, 0.021), (20, 0.086), (21, 0.048), (22, 0.075), (23, 0.009), (24, -0.076), (25, -0.013), (26, 0.059), (27, -0.018), (28, -0.035), (29, 0.038), (30, -0.039), (31, 0.06), (32, -0.013), (33, 0.08), (34, -0.067), (35, 0.133), (36, 0.005), (37, -0.011), (38, 0.048), (39, -0.025), (40, -0.018), (41, -0.046), (42, 0.096), (43, -0.075), (44, -0.005), (45, 0.031), (46, 0.031), (47, 0.088), (48, -0.15), (49, -0.087)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98123443 <a title="26-lsi-1" href="../hunch_net-2005/hunch_net-2005-02-21-Problem%3A_Cross_Validation.html">26 hunch net-2005-02-21-Problem: Cross Validation</a></p>
<p>Introduction: The essential problem here is the large gap between experimental observation
and theoretical understanding.MethodK-fold cross validation is a commonly used
technique which takes a set ofmexamples and partitions them intoKsets
("folds") of sizem/K. For each fold, a classifier is trained on the other
folds and then test on the fold.ProblemAssume only independent samples. Derive
a classifier from the K classifiers with a small bound on the true error
rate.Past Work(I'll add more as I remember/learn.)Devroye, Rogers, and Wagner
analyzed cross validation and found algorithm specific bounds. Not all of this
is online, but here is onepaper.Michael KearnsandDana Ronanalyzed cross
validationand found that under additional stability assumptions the bound for
the classifier which learns on all the data is not much worse than for a test
set of sizem/K.Avrim Blum,Adam Kalai, andmyselfanalyzed cross validationand
found that you can do at least as well as a test set of sizem/Kwith no
additional assum</p><p>2 0.73296863 <a title="26-lsi-2" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>Introduction: "Overfitting" is traditionally defined as training some flexible
representation so that it memorizes the data but fails to predict well in the
future. For this post, I will define overfitting more generally as over-
representing the performance of systems. There are two styles of general
overfitting: overrepresenting performance on particular datasets and
(implicitly) overrepresenting performance of a method on future datasets.We
should all be aware of these methods, avoid them where possible, and take them
into account otherwise. I have used "reproblem" and "old datasets", and may
have participated in "overfitting by review"--some of these are very difficult
to avoid.NameMethodExplanationRemedyTraditional overfittingTrain a complex
predictor on too-few examples.Hold out pristine examples for testing.Use a
simpler predictor.Get more training examples.Integrate over many
predictors.Reject papers which do this.Parameter tweak overfittingUse a
learning algorithm with many parameters. Choo</p><p>3 0.62765473 <a title="26-lsi-3" href="../hunch_net-2005/hunch_net-2005-11-16-The_Everything_Ensemble_Edge.html">131 hunch net-2005-11-16-The Everything Ensemble Edge</a></p>
<p>Introduction: Rich Caruana,Alexandru Niculescu, Geoff Crew, and Alex Ksikes have donea lot
of empirical testingwhich shows thatusing all methods to make a predictionis
more powerful than using any single method. This is in rough agreement with
the Bayesian way of solving problems, but based upon a different (essentially
empirical) motivation. A rough summary is:Take all of {decision trees, boosted
decision trees, bagged decision trees, boosted decision stumps, K nearest
neighbors, neural networks, SVM} with all reasonable parameter settings.Run
the methods on each problem of 8 problems with a large test set, calibrating
margins using eithersigmoid fittingorisotonic regression.For each loss of
{accuracy, area under the ROC curve, cross entropy, squared error, etcâ&euro;Ś}
evaluate the average performance of the method.A series of conclusions can be
drawn from the observations.(Calibrated) boosted decision trees appear to
perform best, in general although support vector machines and neural networks
give cred</p><p>4 0.62303871 <a title="26-lsi-4" href="../hunch_net-2007/hunch_net-2007-06-14-Interesting_Papers_at_COLT_2007.html">247 hunch net-2007-06-14-Interesting Papers at COLT 2007</a></p>
<p>Introduction: Here are two papers that seem particularly interesting at this year's
COLT.Gilles BlanchardandFranÃƒÂ§ois Fleuret,Occam's Hammer. When we are
interested in very tight bounds on the true error rate of a classifier, it is
tempting to use a PAC-Bayes bound which can (empirically) bequite tight. A
disadvantage of the PAC-Bayes bound is that it applies to a classifier which
is randomized over a set of base classifiers rather than a single classifier.
This paper shows that a similar bound can be proved which holds for a single
classifier drawn from the set. The ability to safely use a single classifier
is very nice. This technique applies generically to any base bound, so it has
other applications covered in the paper.Adam Tauman Kalai.Learning Nested
Halfspaces and Uphill Decision Trees. Classification PAC-learning, where you
prove that any problem amongst some set is polytime learnable with respect to
any distribution over the inputXis extraordinarily challenging as judged by
lack of progr</p><p>5 0.61257744 <a title="26-lsi-5" href="../hunch_net-2005/hunch_net-2005-03-18-Binomial_Weighting.html">43 hunch net-2005-03-18-Binomial Weighting</a></p>
<p>Introduction: Suppose we have a set of classifierscmaking binary predictions from an
inputxand we see examples in an online fashion. In particular, we repeatedly
see an unlabeled examplex, make a predictiony'(possibly based on the
classifiersc), and then see the correct labely.When one of these classifiers
is perfect, there is a great algorithm available: predict according to the
majority vote over every classifier consistent with every previous example.
This is called the Halving algorithm. It makes at mostlog2|c|mistakes since on
any mistake, at least half of the classifiers are eliminated.Obviously, we
can't generally hope that the there exists a classifier which never errs.
TheBinomial Weighting algorithmis an elegant technique allowing a variant
Halving algorithm to cope with errors by creating a set of virtual classifiers
for every classifier which occasionally disagree with the original classifier.
The Halving algorithm on this set of virtual classifiers satisfies a theorem
of the form:errors</p><p>6 0.59772247 <a title="26-lsi-6" href="../hunch_net-2005/hunch_net-2005-03-15-The_State_of_Tight_Bounds.html">41 hunch net-2005-03-15-The State of Tight Bounds</a></p>
<p>7 0.58523947 <a title="26-lsi-7" href="../hunch_net-2005/hunch_net-2005-02-12-ROC_vs._Accuracy_vs._AROC.html">18 hunch net-2005-02-12-ROC vs. Accuracy vs. AROC</a></p>
<p>8 0.56043851 <a title="26-lsi-8" href="../hunch_net-2005/hunch_net-2005-05-06-Don%26%238217%3Bt_mix_the_solution_into_the_problem.html">67 hunch net-2005-05-06-Don&#8217;t mix the solution into the problem</a></p>
<p>9 0.47513214 <a title="26-lsi-9" href="../hunch_net-2009/hunch_net-2009-06-26-Netflix_nearly_done.html">362 hunch net-2009-06-26-Netflix nearly done</a></p>
<p>10 0.4729493 <a title="26-lsi-10" href="../hunch_net-2007/hunch_net-2007-04-18-%2450K_Spock_Challenge.html">239 hunch net-2007-04-18-$50K Spock Challenge</a></p>
<p>11 0.47016311 <a title="26-lsi-11" href="../hunch_net-2006/hunch_net-2006-09-09-How_to_solve_an_NP_hard_problem_in_quadratic_time.html">206 hunch net-2006-09-09-How to solve an NP hard problem in quadratic time</a></p>
<p>12 0.46578693 <a title="26-lsi-12" href="../hunch_net-2005/hunch_net-2005-12-09-Some_NIPS_papers.html">138 hunch net-2005-12-09-Some NIPS papers</a></p>
<p>13 0.44513595 <a title="26-lsi-13" href="../hunch_net-2006/hunch_net-2006-03-12-Online_learning_or_online_preservation_of_learning%3F.html">163 hunch net-2006-03-12-Online learning or online preservation of learning?</a></p>
<p>14 0.44312239 <a title="26-lsi-14" href="../hunch_net-2005/hunch_net-2005-05-16-Regret_minimizing_vs_error_limiting_reductions.html">72 hunch net-2005-05-16-Regret minimizing vs error limiting reductions</a></p>
<p>15 0.44294709 <a title="26-lsi-15" href="../hunch_net-2005/hunch_net-2005-06-28-A_COLT_paper.html">85 hunch net-2005-06-28-A COLT paper</a></p>
<p>16 0.43247491 <a title="26-lsi-16" href="../hunch_net-2006/hunch_net-2006-04-06-Bounds_greater_than_1.html">170 hunch net-2006-04-06-Bounds greater than 1</a></p>
<p>17 0.42716479 <a title="26-lsi-17" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>18 0.42634621 <a title="26-lsi-18" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>19 0.42626786 <a title="26-lsi-19" href="../hunch_net-2005/hunch_net-2005-07-13-Text_Entailment_at_AAAI.html">94 hunch net-2005-07-13-Text Entailment at AAAI</a></p>
<p>20 0.4210642 <a title="26-lsi-20" href="../hunch_net-2006/hunch_net-2006-05-05-An_ICML_reject.html">177 hunch net-2006-05-05-An ICML reject</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(6, 0.023), (9, 0.338), (16, 0.014), (35, 0.051), (42, 0.278), (69, 0.023), (74, 0.051), (81, 0.019), (95, 0.081)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.93414557 <a title="26-lda-1" href="../hunch_net-2009/hunch_net-2009-01-28-Nielsen%26%238217%3Bs_talk.html">340 hunch net-2009-01-28-Nielsen&#8217;s talk</a></p>
<p>Introduction: I wanted to point toMichael Nielsen's talkabout blogging science, which I
found interesting.</p><p>same-blog 2 0.86083484 <a title="26-lda-2" href="../hunch_net-2005/hunch_net-2005-02-21-Problem%3A_Cross_Validation.html">26 hunch net-2005-02-21-Problem: Cross Validation</a></p>
<p>Introduction: The essential problem here is the large gap between experimental observation
and theoretical understanding.MethodK-fold cross validation is a commonly used
technique which takes a set ofmexamples and partitions them intoKsets
("folds") of sizem/K. For each fold, a classifier is trained on the other
folds and then test on the fold.ProblemAssume only independent samples. Derive
a classifier from the K classifiers with a small bound on the true error
rate.Past Work(I'll add more as I remember/learn.)Devroye, Rogers, and Wagner
analyzed cross validation and found algorithm specific bounds. Not all of this
is online, but here is onepaper.Michael KearnsandDana Ronanalyzed cross
validationand found that under additional stability assumptions the bound for
the classifier which learns on all the data is not much worse than for a test
set of sizem/K.Avrim Blum,Adam Kalai, andmyselfanalyzed cross validationand
found that you can do at least as well as a test set of sizem/Kwith no
additional assum</p><p>3 0.84728187 <a title="26-lda-3" href="../hunch_net-2006/hunch_net-2006-02-02-Introspectionism_as_a_Disease.html">153 hunch net-2006-02-02-Introspectionism as a Disease</a></p>
<p>Introduction: In the AI-related parts of machine learning, it is often tempting to examine
howyoudo things in order to imagine how a machine should do things. This is
introspection, and it can easily go awry. I will call introspection gone awry
introspectionism.Introspectionism is almost unique to AI (and the AI-related
parts of machine learning) and it can lead to huge wasted effort in research.
It's easiest to show how introspectionism arises by an example.Suppose we want
to solve the problem of navigating a robot from point A to point B given a
camera. Then, the following research action plan might seem natural when you
examine your own capabilities:Build an edge detector for still images.Build an
object recognition system given the edge detector.Build a system to predict
distance and orientation to objects given the object recognition system.Build
a system to plan a path through the scene you construct from {object
identification, distance, orientation} predictions.As you execute the above,
cons</p><p>4 0.64524364 <a title="26-lda-4" href="../hunch_net-2005/hunch_net-2005-02-25-Problem%3A_Online_Learning.html">28 hunch net-2005-02-25-Problem: Online Learning</a></p>
<p>Introduction: Despite my best intentions, this is not a fully specified problem, but rather
a research direction.Competitive online learning is one of the more compelling
pieces of learning theory because typical statements of the form "this
algorithm will perform almost as well as a large set of other algorithms" rely
only on fully-observable quantities, and are therefore applicable in many
situations. Examples includeWinnow,Weighted Majority, andBinomial Weighting.
Algorithms with this property haven't taken over the world yet. Here might be
some reasons:Lack of caring. Many people working on learning theory don't care
about particular applications much. This means constants in the algorithm are
not optimized, usable code is often not produced, and empirical studies aren't
done.Inefficiency. Viewed from the perspective of other learning algorithms,
online learning is terribly inefficient. It requires that every hypothesis
(called an expert in the online learning setting) be enumerated and tested o</p><p>5 0.64328665 <a title="26-lda-5" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>Introduction: A little over 4 years ago,Sanjoymade a postsaying roughly "we should study
active learning theoretically, because not much is understood".At the time, we
did not understand basic things such as whether or not it was possible to PAC-
learn with an active algorithm without making strong assumptions about the
noise rate. In other words, the fundamental question was "can we do it?"The
nature of the question has fundamentally changed in my mind. The answer is to
the previous question is "yes", both information theoretically and
computationally, most places where supervised learning could be applied.In
many situation, the question has now changed to: "is it worth it?" Is the
programming and computational overhead low enough to make the label cost
savings of active learning worthwhile? Currently, there are situations where
this question could go either way. Much of the challenge for the future is in
figuring out how to make active learning easier or more worthwhile.At
theactive learning tutor</p><p>6 0.64203143 <a title="26-lda-6" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>7 0.64053106 <a title="26-lda-7" href="../hunch_net-2009/hunch_net-2009-12-24-Top_graduates_this_season.html">384 hunch net-2009-12-24-Top graduates this season</a></p>
<p>8 0.64050949 <a title="26-lda-8" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>9 0.63980246 <a title="26-lda-9" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>10 0.63929832 <a title="26-lda-10" href="../hunch_net-2006/hunch_net-2006-01-13-Benchmarks_for_RL.html">148 hunch net-2006-01-13-Benchmarks for RL</a></p>
<p>11 0.63742 <a title="26-lda-11" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>12 0.63693094 <a title="26-lda-12" href="../hunch_net-2013/hunch_net-2013-12-01-NIPS_tutorials_and_Vowpal_Wabbit_7.4.html">492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</a></p>
<p>13 0.63539231 <a title="26-lda-13" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>14 0.634839 <a title="26-lda-14" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<p>15 0.63349199 <a title="26-lda-15" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>16 0.63244933 <a title="26-lda-16" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>17 0.63223004 <a title="26-lda-17" href="../hunch_net-2005/hunch_net-2005-03-15-The_State_of_Tight_Bounds.html">41 hunch net-2005-03-15-The State of Tight Bounds</a></p>
<p>18 0.63206971 <a title="26-lda-18" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>19 0.63073772 <a title="26-lda-19" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>20 0.6303336 <a title="26-lda-20" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
