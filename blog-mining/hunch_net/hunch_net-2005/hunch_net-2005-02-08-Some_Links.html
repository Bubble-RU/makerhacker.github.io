<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>15 hunch net-2005-02-08-Some Links</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-15" href="#">hunch_net-2005-15</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>15 hunch net-2005-02-08-Some Links</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-15-html" href="http://hunch.net/?p=18">html</a></p><p>Introduction: Yaroslav Bulatov collects somelinksto other technical blogs.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('collects', 0.598), ('yaroslav', 0.554), ('blogs', 0.448), ('technical', 0.366)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="15-tfidf-1" href="../hunch_net-2005/hunch_net-2005-02-08-Some_Links.html">15 hunch net-2005-02-08-Some Links</a></p>
<p>Introduction: Yaroslav Bulatov collects somelinksto other technical blogs.</p><p>2 0.28939393 <a title="15-tfidf-2" href="../hunch_net-2005/hunch_net-2005-02-19-Machine_learning_reading_groups.html">24 hunch net-2005-02-19-Machine learning reading groups</a></p>
<p>Introduction: Yaroslav collected an extensive list ofmachine learning reading groups.</p><p>3 0.16780274 <a title="15-tfidf-3" href="../hunch_net-2007/hunch_net-2007-06-13-Not_Posting.html">246 hunch net-2007-06-13-Not Posting</a></p>
<p>Introduction: If you have been disappointed by the lack of a post for the last month,
considercontributing your own(I've been busy+uninspired). Also, keep in mind
that there is a community of machine learning blogs (see the sidebar).</p><p>4 0.074258514 <a title="15-tfidf-4" href="../hunch_net-2009/hunch_net-2009-05-17-Server_Update.html">354 hunch net-2009-05-17-Server Update</a></p>
<p>Introduction: The hunch.net server has been updated. I've taken the opportunity to upgrade
the version of wordpress which caused cascading changes.Old threaded comments
are now flattened. The system we used to use (Brian's threaded comments)
appears incompatible with the new threading system built into wordpress. I
haven't yet figured out a workaround.I setup afeedburner account.I added an
RSS aggregator for both Machine Learning and other research blogs that I like
to follow. This is something that I've wanted to do for awhile.Many other
minor changes in font and format, with some help fromAlina.If you have any
suggestions for site tweaks, please speak up.</p><p>5 0.059139561 <a title="15-tfidf-5" href="../hunch_net-2005/hunch_net-2005-02-28-Regularization.html">33 hunch net-2005-02-28-Regularization</a></p>
<p>Introduction: Yaroslav Bulatovsays that we should think about regularization a bit. It's a
complex topic which I only partially understand, so I'll try to explain from a
couple viewpoints.Functionally. Regularization is optimizing some
representation to fit the dataandminimize some notion of predictor complexity.
This notion of complexity is often the l1or l2norm on a set of parameters, but
the term can be used much more generally. Empirically, this often works much
better than simply fitting the data.Statistical Learning
ViewpointRegularization is about the failiure of statistical learning to
adequately predict generalization error. Lete(c,D)be the expected error rate
with respect toDof classifiercande(c,S)the observed error rate on a sampleS.
There are numerous bounds of the form: assuming i.i.d. samples, with high
probability over the drawn samplesS,e(c,D) less than e(c,S) +
f(complexity)wherecomplexityis some measure of the size of a set of functions.
Unfortunately, we have never convincingly na</p><p>6 0.057799343 <a title="15-tfidf-6" href="../hunch_net-2008/hunch_net-2008-04-21-The_Science_2.0_article.html">296 hunch net-2008-04-21-The Science 2.0 article</a></p>
<p>7 0.049323615 <a title="15-tfidf-7" href="../hunch_net-2007/hunch_net-2007-01-02-Retrospective.html">225 hunch net-2007-01-02-Retrospective</a></p>
<p>8 0.031069726 <a title="15-tfidf-8" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>9 0.027316602 <a title="15-tfidf-9" href="../hunch_net-2005/hunch_net-2005-07-01-The_Role_of_Impromptu_Talks.html">88 hunch net-2005-07-01-The Role of Impromptu Talks</a></p>
<p>10 0.025244756 <a title="15-tfidf-10" href="../hunch_net-2007/hunch_net-2007-02-10-Best_Practices_for_Collaboration.html">231 hunch net-2007-02-10-Best Practices for Collaboration</a></p>
<p>11 0.024680341 <a title="15-tfidf-11" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<p>12 0.021998934 <a title="15-tfidf-12" href="../hunch_net-2005/hunch_net-2005-05-12-Math_on_the_Web.html">70 hunch net-2005-05-12-Math on the Web</a></p>
<p>13 0.021686543 <a title="15-tfidf-13" href="../hunch_net-2005/hunch_net-2005-06-13-Wikis_for_Summer_Schools_and_Workshops.html">81 hunch net-2005-06-13-Wikis for Summer Schools and Workshops</a></p>
<p>14 0.021266788 <a title="15-tfidf-14" href="../hunch_net-2009/hunch_net-2009-08-03-Carbon_in_Computer_Science_Research.html">366 hunch net-2009-08-03-Carbon in Computer Science Research</a></p>
<p>15 0.020821718 <a title="15-tfidf-15" href="../hunch_net-2012/hunch_net-2012-05-02-ICML%3A_Behind_the_Scenes.html">463 hunch net-2012-05-02-ICML: Behind the Scenes</a></p>
<p>16 0.018001631 <a title="15-tfidf-16" href="../hunch_net-2012/hunch_net-2012-04-09-ICML_author_feedback_is_open.html">461 hunch net-2012-04-09-ICML author feedback is open</a></p>
<p>17 0.017419023 <a title="15-tfidf-17" href="../hunch_net-2011/hunch_net-2011-08-01-Interesting_papers_at_COLT_2011.html">439 hunch net-2011-08-01-Interesting papers at COLT 2011</a></p>
<p>18 0.016995186 <a title="15-tfidf-18" href="../hunch_net-2005/hunch_net-2005-10-07-On-line_learning_of_regular_decision_rules.html">118 hunch net-2005-10-07-On-line learning of regular decision rules</a></p>
<p>19 0.015520437 <a title="15-tfidf-19" href="../hunch_net-2010/hunch_net-2010-04-26-Compassionate_Reviewing.html">395 hunch net-2010-04-26-Compassionate Reviewing</a></p>
<p>20 0.015379141 <a title="15-tfidf-20" href="../hunch_net-2008/hunch_net-2008-10-14-Who_is_Responsible_for_a_Bad_Review%3F.html">320 hunch net-2008-10-14-Who is Responsible for a Bad Review?</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.009), (1, 0.012), (2, -0.005), (3, -0.011), (4, 0.001), (5, 0.003), (6, 0.01), (7, -0.039), (8, 0.015), (9, 0.018), (10, 0.005), (11, -0.009), (12, -0.006), (13, -0.022), (14, -0.024), (15, -0.029), (16, 0.023), (17, 0.03), (18, -0.001), (19, 0.011), (20, 0.037), (21, -0.01), (22, 0.024), (23, 0.022), (24, -0.016), (25, 0.007), (26, -0.026), (27, 0.027), (28, -0.068), (29, 0.044), (30, 0.005), (31, 0.024), (32, 0.021), (33, 0.015), (34, 0.07), (35, -0.035), (36, -0.067), (37, -0.108), (38, 0.047), (39, -0.027), (40, 0.028), (41, 0.057), (42, -0.041), (43, -0.118), (44, 0.003), (45, -0.004), (46, 0.005), (47, 0.048), (48, -0.013), (49, 0.032)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99918604 <a title="15-lsi-1" href="../hunch_net-2005/hunch_net-2005-02-08-Some_Links.html">15 hunch net-2005-02-08-Some Links</a></p>
<p>Introduction: Yaroslav Bulatov collects somelinksto other technical blogs.</p><p>2 0.65596718 <a title="15-lsi-2" href="../hunch_net-2005/hunch_net-2005-02-19-Machine_learning_reading_groups.html">24 hunch net-2005-02-19-Machine learning reading groups</a></p>
<p>Introduction: Yaroslav collected an extensive list ofmachine learning reading groups.</p><p>3 0.55135477 <a title="15-lsi-3" href="../hunch_net-2007/hunch_net-2007-06-13-Not_Posting.html">246 hunch net-2007-06-13-Not Posting</a></p>
<p>Introduction: If you have been disappointed by the lack of a post for the last month,
considercontributing your own(I've been busy+uninspired). Also, keep in mind
that there is a community of machine learning blogs (see the sidebar).</p><p>4 0.44981551 <a title="15-lsi-4" href="../hunch_net-2009/hunch_net-2009-05-17-Server_Update.html">354 hunch net-2009-05-17-Server Update</a></p>
<p>Introduction: The hunch.net server has been updated. I've taken the opportunity to upgrade
the version of wordpress which caused cascading changes.Old threaded comments
are now flattened. The system we used to use (Brian's threaded comments)
appears incompatible with the new threading system built into wordpress. I
haven't yet figured out a workaround.I setup afeedburner account.I added an
RSS aggregator for both Machine Learning and other research blogs that I like
to follow. This is something that I've wanted to do for awhile.Many other
minor changes in font and format, with some help fromAlina.If you have any
suggestions for site tweaks, please speak up.</p><p>5 0.33074808 <a title="15-lsi-5" href="../hunch_net-2008/hunch_net-2008-04-21-The_Science_2.0_article.html">296 hunch net-2008-04-21-The Science 2.0 article</a></p>
<p>Introduction: I found the article aboutscience using modern tools interesting, especially
the part about 'blogophobia', which in my experience is often a substantial
issue: many potential guest posters aren't quite ready, because of the fear of
a permanent public mistake, because it is particularly hard to write about the
unknown (the essence of research), and because the system for public credit
doesn't yet really handle blog posts.So far, science has been relatively
resistant to discussing research on blogs. Some things need to change to get
there. Public tolerance of the occasional mistake is essential, as is a
willingness to cite (and credit) blogs as freely as papers.I've often run into
another reason for holding back myself: I don't want to overtalk my own
research. Nevertheless, I'm slowly changing to the opinion that I'm holding
back too much: the real power of a blog in research is that it can be used to
confer with many people, and that just makes research work better.</p><p>6 0.29647112 <a title="15-lsi-6" href="../hunch_net-2007/hunch_net-2007-07-12-ICML_Trends.html">254 hunch net-2007-07-12-ICML Trends</a></p>
<p>7 0.29570055 <a title="15-lsi-7" href="../hunch_net-2005/hunch_net-2005-10-13-Site_tweak.html">122 hunch net-2005-10-13-Site tweak</a></p>
<p>8 0.28281397 <a title="15-lsi-8" href="../hunch_net-2006/hunch_net-2006-06-05-Server_Shift%2C_Site_Tweaks%2C_Suggestions%3F.html">182 hunch net-2006-06-05-Server Shift, Site Tweaks, Suggestions?</a></p>
<p>9 0.28215909 <a title="15-lsi-9" href="../hunch_net-2008/hunch_net-2008-04-12-Blog_compromised.html">294 hunch net-2008-04-12-Blog compromised</a></p>
<p>10 0.27842572 <a title="15-lsi-10" href="../hunch_net-2005/hunch_net-2005-06-13-Wikis_for_Summer_Schools_and_Workshops.html">81 hunch net-2005-06-13-Wikis for Summer Schools and Workshops</a></p>
<p>11 0.2673471 <a title="15-lsi-11" href="../hunch_net-2005/hunch_net-2005-09-05-Site_Update.html">107 hunch net-2005-09-05-Site Update</a></p>
<p>12 0.26402101 <a title="15-lsi-12" href="../hunch_net-2006/hunch_net-2006-03-24-NLPers.html">166 hunch net-2006-03-24-NLPers</a></p>
<p>13 0.25397506 <a title="15-lsi-13" href="../hunch_net-2005/hunch_net-2005-04-14-Families_of_Learning_Theory_Statements.html">56 hunch net-2005-04-14-Families of Learning Theory Statements</a></p>
<p>14 0.24998327 <a title="15-lsi-14" href="../hunch_net-2005/hunch_net-2005-02-25-Solution%3A_Reinforcement_Learning_with_Classification.html">29 hunch net-2005-02-25-Solution: Reinforcement Learning with Classification</a></p>
<p>15 0.24302787 <a title="15-lsi-15" href="../hunch_net-2007/hunch_net-2007-12-17-New_Machine_Learning_mailing_list.html">278 hunch net-2007-12-17-New Machine Learning mailing list</a></p>
<p>16 0.24279727 <a title="15-lsi-16" href="../hunch_net-2008/hunch_net-2008-06-30-ICML_has_a_comment_system.html">305 hunch net-2008-06-30-ICML has a comment system</a></p>
<p>17 0.23480061 <a title="15-lsi-17" href="../hunch_net-2012/hunch_net-2012-09-29-Vowpal_Wabbit%2C_version_7.0.html">473 hunch net-2012-09-29-Vowpal Wabbit, version 7.0</a></p>
<p>18 0.23338911 <a title="15-lsi-18" href="../hunch_net-2010/hunch_net-2010-01-13-Sam_Roweis_died.html">386 hunch net-2010-01-13-Sam Roweis died</a></p>
<p>19 0.22899875 <a title="15-lsi-19" href="../hunch_net-2012/hunch_net-2012-04-09-ICML_author_feedback_is_open.html">461 hunch net-2012-04-09-ICML author feedback is open</a></p>
<p>20 0.22349402 <a title="15-lsi-20" href="../hunch_net-2010/hunch_net-2010-10-28-NY_ML_Symposium_2010.html">415 hunch net-2010-10-28-NY ML Symposium 2010</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(31, 0.666)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="15-lda-1" href="../hunch_net-2005/hunch_net-2005-02-08-Some_Links.html">15 hunch net-2005-02-08-Some Links</a></p>
<p>Introduction: Yaroslav Bulatov collects somelinksto other technical blogs.</p><p>2 0.83953464 <a title="15-lda-2" href="../hunch_net-2005/hunch_net-2005-11-16-MLSS_2006.html">130 hunch net-2005-11-16-MLSS 2006</a></p>
<p>Introduction: There will be twomachine learning summer schoolsin 2006.One is inCanberra,
Australiafrom February 6 to February 17 (Aussie summer). The webpage is fully
'live' so you should actively consider it now.The other is inTaipei,
Taiwanfrom July 24 to August 4. This one is still in the planning phase, but
that should be settled soon.Attending an MLSS is probably the quickest and
easiest way to bootstrap yourself into a reasonable initial understanding of
the field of machine learning.</p><p>3 0.32804433 <a title="15-lda-3" href="../hunch_net-2010/hunch_net-2010-04-14-MLcomp%3A_a_website_for_objectively_comparing_ML_algorithms.html">393 hunch net-2010-04-14-MLcomp: a website for objectively comparing ML algorithms</a></p>
<p>Introduction: Much of the success and popularity of machine learning has been driven by its
practical impact. Of course, the evaluation of empirical work is an integral
part of the field. But are the existing mechanisms for evaluating algorithms
and comparing results good enough? We (PercyandJake) believe there are
currently a number of shortcomings:Incomplete Disclosure:You read a paper that
proposes Algorithm A which is shown to outperform SVMs on two datasets.
Great.  But what about on other datasets?  How sensitive is this result?
What about compute time - does the algorithm take two seconds on a laptop or
two weeks on a 100-node cluster?Lack of Standardization:Algorithm A beats
Algorithm B on one version of a dataset.  Algorithm B beats Algorithm A on
another version yet uses slightly different preprocessing.  Though doing a
head-on comparison would be ideal, it would be tedious since the programs
probably use different dataset formats and have a large array of options.  And
what if we wanted t</p><p>4 0.0 <a title="15-lda-4" href="../hunch_net-2005/hunch_net-2005-01-19-Why_I_decided_to_run_a_weblog..html">1 hunch net-2005-01-19-Why I decided to run a weblog.</a></p>
<p>Introduction: I have decided to run a weblog on machine learning and learning theory
research. Here are some reasons:1) Weblogs enable new functionality:Public
comment on papers. No mechanism for this exists at conferences and most
journals. I have encountered it once for asciencepaper. Some communities have
mailing lists supporting this, but not machine learning or learning theory. I
have often read papers and found myself wishing there was some method to
consider other's questions and read the replies.Conference shortlists. One of
the most common conversations at a conference is "what did you find
interesting?" There is no explicit mechanism for sharing this information at
conferences, and it's easy to imagine that it would be handy to do
so.Evaluation and comment on research directions. Papers are almost
exclusively about new research, rather than evaluation (and consideration) of
research directions. This last role is satisfied by funding agencies to some
extent, but that is a private debate of</p><p>5 0.0 <a title="15-lda-5" href="../hunch_net-2005/hunch_net-2005-01-24-Holy_grails_of_machine_learning%3F.html">2 hunch net-2005-01-24-Holy grails of machine learning?</a></p>
<p>Introduction: Let me kick things off by posing this question to ML researchers:What do you
think are some important holy grails of machine learning?For example:- "A
classifier with SVM-level performance but much more scalable"- "Practical
confidence bounds (or learning bounds) for classification"- "A reinforcement
learning algorithm that can handle the ___ problem"- "Understanding
theoretically why ___ works so well in practice"etc.I pose this question
because I believe that when goals are stated explicitly and well (thus
providing clarity as well as opening up the problems to more people), rather
than left implicit, they are likely to be achieved much more quickly. I would
also like to know more about the internal goals of the various machine
learning sub-areas (theory, kernel methods, graphical models, reinforcement
learning, etc) as stated by people in these respective areas. This could help
people cross sub-areas.</p><p>6 0.0 <a title="15-lda-6" href="../hunch_net-2005/hunch_net-2005-01-24-The_Humanloop_Spectrum_of_Machine_Learning.html">3 hunch net-2005-01-24-The Humanloop Spectrum of Machine Learning</a></p>
<p>7 0.0 <a title="15-lda-7" href="../hunch_net-2005/hunch_net-2005-01-26-Summer_Schools.html">4 hunch net-2005-01-26-Summer Schools</a></p>
<p>8 0.0 <a title="15-lda-8" href="../hunch_net-2005/hunch_net-2005-01-26-Watchword%3A_Probability.html">5 hunch net-2005-01-26-Watchword: Probability</a></p>
<p>9 0.0 <a title="15-lda-9" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>10 0.0 <a title="15-lda-10" href="../hunch_net-2005/hunch_net-2005-01-31-Watchword%3A_Assumption.html">7 hunch net-2005-01-31-Watchword: Assumption</a></p>
<p>11 0.0 <a title="15-lda-11" href="../hunch_net-2005/hunch_net-2005-02-01-NIPS%3A_Online_Bayes.html">8 hunch net-2005-02-01-NIPS: Online Bayes</a></p>
<p>12 0.0 <a title="15-lda-12" href="../hunch_net-2005/hunch_net-2005-02-01-Watchword%3A_Loss.html">9 hunch net-2005-02-01-Watchword: Loss</a></p>
<p>13 0.0 <a title="15-lda-13" href="../hunch_net-2005/hunch_net-2005-02-02-Kolmogorov_Complexity_and_Googling.html">10 hunch net-2005-02-02-Kolmogorov Complexity and Googling</a></p>
<p>14 0.0 <a title="15-lda-14" href="../hunch_net-2005/hunch_net-2005-02-02-Paper_Deadlines.html">11 hunch net-2005-02-02-Paper Deadlines</a></p>
<p>15 0.0 <a title="15-lda-15" href="../hunch_net-2005/hunch_net-2005-02-03-Learning_Theory%2C_by_assumption.html">12 hunch net-2005-02-03-Learning Theory, by assumption</a></p>
<p>16 0.0 <a title="15-lda-16" href="../hunch_net-2005/hunch_net-2005-02-04-JMLG.html">13 hunch net-2005-02-04-JMLG</a></p>
<p>17 0.0 <a title="15-lda-17" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>18 0.0 <a title="15-lda-18" href="../hunch_net-2005/hunch_net-2005-02-09-Intuitions_from_applied_learning.html">16 hunch net-2005-02-09-Intuitions from applied learning</a></p>
<p>19 0.0 <a title="15-lda-19" href="../hunch_net-2005/hunch_net-2005-02-10-Conferences%2C_Dates%2C_Locations.html">17 hunch net-2005-02-10-Conferences, Dates, Locations</a></p>
<p>20 0.0 <a title="15-lda-20" href="../hunch_net-2005/hunch_net-2005-02-12-ROC_vs._Accuracy_vs._AROC.html">18 hunch net-2005-02-12-ROC vs. Accuracy vs. AROC</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
