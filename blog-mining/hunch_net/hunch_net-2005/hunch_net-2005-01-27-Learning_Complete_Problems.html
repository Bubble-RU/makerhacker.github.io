<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>6 hunch net-2005-01-27-Learning Complete Problems</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="../home/hunch_net-2005_home.html">hunch_net-2005</a> <a title="hunch_net-2005-6" href="#">hunch_net-2005-6</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>6 hunch net-2005-01-27-Learning Complete Problems</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2005-6-html" href="http://hunch.net/?p=9">html</a></p><p>Introduction: Let's define a learning problem as making predictions given past data. There
are several ways to attack the learning problem which seem to be equivalent to
solving the learning problem.Find the InvariantThis viewpoint says that
learning is all about learning (or incorporating) transformations of objects
that do not change the correct prediction. The best possible invariant is the
one which says "all things of the same class are the same". Finding this is
equivalent to learning. This viewpoint is particularly common when working
with image features.Feature SelectionThis viewpoint says that the way to learn
is by finding the right features to input to a learning algorithm. The best
feature is the one which is the class to predict. Finding this is equivalent
to learning for all reasonable learning algorithms. This viewpoint is common
in several applications of machine learning. SeeGilad's and Bianca's
comments.Find the RepresentationThis is almost the same as feature selection,
except int</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 There are several ways to attack the learning problem which seem to be equivalent to solving the learning problem. [sent-2, score-0.524]
</p><p>2 Find the InvariantThis viewpoint says that learning is all about learning (or incorporating) transformations of objects that do not change the correct prediction. [sent-3, score-1.115]
</p><p>3 The best possible invariant is the one which says "all things of the same class are the same". [sent-4, score-0.834]
</p><p>4 This viewpoint is particularly common when working with image features. [sent-6, score-0.584]
</p><p>5 Feature SelectionThis viewpoint says that the way to learn is by finding the right features to input to a learning algorithm. [sent-7, score-1.494]
</p><p>6 The best feature is the one which is the class to predict. [sent-8, score-0.63]
</p><p>7 Finding this is equivalent to learning for all reasonable learning algorithms. [sent-9, score-0.443]
</p><p>8 This viewpoint is common in several applications of machine learning. [sent-10, score-0.507]
</p><p>9 Find the RepresentationThis is almost the same as feature selection, except internal to the learning algorithm rather than external. [sent-12, score-0.425]
</p><p>10 The key to learning is viewed as finding the best way to process the features in order to make predictions. [sent-13, score-1.313]
</p><p>11 The best representation is the one which processes the features to produce the correct prediction. [sent-14, score-0.811]
</p><p>12 Find the RightKernelThe key to learning is finding the "right" kernel. [sent-16, score-0.614]
</p><p>13 The optimal kernel is the one for whichK(x, z)=1whenxandzhave the same class and0otherwise. [sent-17, score-0.468]
</p><p>14 With the right kernel, an SVM(or SVM-like optimization process) can solve any learning problem. [sent-18, score-0.396]
</p><p>15 This viewpoint is common for people who work with SVMs. [sent-19, score-0.507]
</p><p>16 Find the Right MetricThe key to learning is finding the right metric. [sent-20, score-0.81]
</p><p>17 The best metric is one which states that features with the same class label have distance 0 while features with different class labels have distance 1. [sent-21, score-1.955]
</p><p>18 With the best metric, the nearest neighbor algorithm can solve any problem. [sent-22, score-0.536]
</p><p>19 Each of these viewpoints seems to be "right", and each seems to have some utility in it's context. [sent-23, score-0.328]
</p><p>20 One consequence of this observation is that "wrapper methods" which try to automatically find a subset of features to feed into a learning algorithm in order to improve learning performance are simply trying to repair weaknesses in the learning algorithm. [sent-25, score-1.407]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('viewpoint', 0.376), ('finding', 0.313), ('features', 0.289), ('class', 0.261), ('equivalent', 0.197), ('says', 0.197), ('right', 0.196), ('best', 0.189), ('key', 0.178), ('distance', 0.153), ('metric', 0.146), ('common', 0.131), ('algorithm', 0.129), ('kernel', 0.123), ('learning', 0.123), ('correct', 0.118), ('feed', 0.111), ('invariant', 0.103), ('repair', 0.103), ('utility', 0.103), ('weaknesses', 0.097), ('transformations', 0.097), ('feature', 0.096), ('incorporating', 0.092), ('versions', 0.092), ('viewpoints', 0.089), ('order', 0.088), ('one', 0.084), ('consequence', 0.083), ('attack', 0.081), ('objects', 0.081), ('svm', 0.078), ('selection', 0.078), ('solve', 0.077), ('internal', 0.077), ('image', 0.077), ('neighbor', 0.072), ('realize', 0.072), ('subset', 0.07), ('nearest', 0.069), ('processes', 0.069), ('automatically', 0.068), ('define', 0.068), ('viewed', 0.068), ('seems', 0.068), ('states', 0.067), ('process', 0.065), ('labels', 0.063), ('produce', 0.062), ('past', 0.061)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="6-tfidf-1" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>Introduction: Let's define a learning problem as making predictions given past data. There
are several ways to attack the learning problem which seem to be equivalent to
solving the learning problem.Find the InvariantThis viewpoint says that
learning is all about learning (or incorporating) transformations of objects
that do not change the correct prediction. The best possible invariant is the
one which says "all things of the same class are the same". Finding this is
equivalent to learning. This viewpoint is particularly common when working
with image features.Feature SelectionThis viewpoint says that the way to learn
is by finding the right features to input to a learning algorithm. The best
feature is the one which is the class to predict. Finding this is equivalent
to learning for all reasonable learning algorithms. This viewpoint is common
in several applications of machine learning. SeeGilad's and Bianca's
comments.Find the RepresentationThis is almost the same as feature selection,
except int</p><p>2 0.17039631 <a title="6-tfidf-2" href="../hunch_net-2013/hunch_net-2013-01-07-NYU_Large_Scale_Machine_Learning_Class.html">478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</a></p>
<p>Introduction: Yann LeCunand I are coteaching a class onLarge Scale Machine Learningstarting
late Januaryat NYU. This class will cover many tricks to get machine learning
working well on datasets with many features, examples, and classes, along with
several elements of deep learning and support systems enabling the
previous.This is not a beginning class--you really need to have taken a basic
machine learning class previously to follow along. Students will be able to
run and experiment with large scale learning algorithms sinceYahoo!has donated
servers which are being configured into a small scaleHadoopcluster. We are
planning to cover the frontier of research in scalable learning algorithms, so
good class projects could easily lead to papers.For me, this is a chance to
teach on many topics of past research. In general, it seems like researchers
should engage in at least occasional teaching of research, both as a proof of
teachability and to see their own research through that lens. More generally,
I</p><p>3 0.15408701 <a title="6-tfidf-3" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>Introduction: One viewpoint on academia is that it is inherently adversarial: there are
finite research dollars, positions, and students to work with, implying a
zero-sum game between different participants. This is not a viewpoint that I
want to promote, as I consider it flawed. However, I know several people
believe strongly in this viewpoint, and I have found it to have substantial
explanatory power.For example:It explains why your paper was rejected based on
poor logic. The reviewer wasn't concerned with research quality, but rather
with rejecting a competitor.It explains why professors rarely work together.
The goal of a non-tenured professor (at least) is to get tenure, and a case
for tenure comes from a portfolio of work that is undisputably yours.It
explains why new research programs are not quickly adopted. Adopting a
competitor's program is impossible, if your career is based on the competitor
being wrong.Different academic groups subscribe to the adversarial viewpoint
in different degrees</p><p>4 0.13270952 <a title="6-tfidf-4" href="../hunch_net-2007/hunch_net-2007-08-19-Choice_of_Metrics.html">259 hunch net-2007-08-19-Choice of Metrics</a></p>
<p>Introduction: How do we judge success in Machine Learning? AsAaronnotes, the best way is to
use the loss imposed on you by the world. This turns out to be infeasible
sometimes for various reasons. The ones I've seen are:The learned prediction
is used in some complicated process that does not give the feedback necessary
to understand the prediction's impact on the loss.The prediction is used by
some other system which expects some semantics to the predicted value. This is
similar to the previous example, except that the issue is design modularity
rather than engineering modularity.The correct loss function is simply unknown
(and perhaps unknowable, except by experimentation).In these situations, it's
unclear what metric for evaluation should be chosen. This post has some design
advice for this murkier case. I'm using the word "metric" here to distinguish
the fact that we are considering methods forevaluatingpredictive systems
rather than a loss imposed by the real world or a loss which is optimized b</p><p>5 0.12747473 <a title="6-tfidf-5" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>Introduction: There are several different flavors of Machine Learning classes. Many classes
are of the 'zoo' sort: many different learning algorithms are presented.
Others avoid the zoo by not covering the full scope of machine learning.This
is my view of what makes a good machine learning class, along with why. I'd
like to specifically invite comment on whether things are missing,
misemphasized, or misplaced.PhaseSubjectWhy?IntroductionWhat is a machine
learning problem?A good understanding of the characteristics of machine
learning problems seems essential. Characteristics include: a data source,
some hope the data is predictive, and a need for generalization. This is
probably best taught in a case study manner: lay out the specifics of some
problem and then ask "Is this a machine learning problem?"IntroductionMachine
Learning Problem IdentificationIdentification and recognition of the type of
learning problems is (obviously) a very important step in solving such
problems. People need to be famili</p><p>6 0.12141383 <a title="6-tfidf-6" href="../hunch_net-2005/hunch_net-2005-02-26-Problem%3A_Reductions_and_Relative_Ranking_Metrics.html">31 hunch net-2005-02-26-Problem: Reductions and Relative Ranking Metrics</a></p>
<p>7 0.119264 <a title="6-tfidf-7" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>8 0.11895208 <a title="6-tfidf-8" href="../hunch_net-2007/hunch_net-2007-08-12-Exponentiated_Gradient.html">258 hunch net-2007-08-12-Exponentiated Gradient</a></p>
<p>9 0.11758822 <a title="6-tfidf-9" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>10 0.1130419 <a title="6-tfidf-10" href="../hunch_net-2005/hunch_net-2005-02-03-Learning_Theory%2C_by_assumption.html">12 hunch net-2005-02-03-Learning Theory, by assumption</a></p>
<p>11 0.10991241 <a title="6-tfidf-11" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>12 0.10972086 <a title="6-tfidf-12" href="../hunch_net-2005/hunch_net-2005-03-10-Breaking_Abstractions.html">39 hunch net-2005-03-10-Breaking Abstractions</a></p>
<p>13 0.10906509 <a title="6-tfidf-13" href="../hunch_net-2005/hunch_net-2005-09-26-Prediction_Bounds_as_the_Mathematics_of_Science.html">115 hunch net-2005-09-26-Prediction Bounds as the Mathematics of Science</a></p>
<p>14 0.10414984 <a title="6-tfidf-14" href="../hunch_net-2007/hunch_net-2007-07-06-Idempotent-capable_Predictors.html">253 hunch net-2007-07-06-Idempotent-capable Predictors</a></p>
<p>15 0.10299233 <a title="6-tfidf-15" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>16 0.10216966 <a title="6-tfidf-16" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>17 0.10101394 <a title="6-tfidf-17" href="../hunch_net-2006/hunch_net-2006-01-30-Should_the_Input_Representation_be_a_Vector%3F.html">152 hunch net-2006-01-30-Should the Input Representation be a Vector?</a></p>
<p>18 0.10072614 <a title="6-tfidf-18" href="../hunch_net-2007/hunch_net-2007-01-10-A_Deep_Belief_Net_Learning_Problem.html">227 hunch net-2007-01-10-A Deep Belief Net Learning Problem</a></p>
<p>19 0.10036013 <a title="6-tfidf-19" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>20 0.09931571 <a title="6-tfidf-20" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.226), (1, -0.108), (2, 0.001), (3, -0.034), (4, -0.043), (5, 0.026), (6, -0.053), (7, 0.011), (8, 0.014), (9, 0.026), (10, 0.024), (11, -0.081), (12, 0.013), (13, -0.045), (14, -0.052), (15, -0.005), (16, -0.057), (17, -0.033), (18, 0.054), (19, 0.008), (20, 0.106), (21, -0.006), (22, 0.062), (23, -0.03), (24, 0.0), (25, -0.133), (26, -0.073), (27, -0.061), (28, -0.021), (29, -0.012), (30, -0.009), (31, 0.004), (32, -0.005), (33, -0.013), (34, -0.034), (35, -0.049), (36, 0.022), (37, 0.07), (38, 0.032), (39, 0.05), (40, 0.032), (41, 0.012), (42, -0.061), (43, 0.014), (44, -0.086), (45, -0.148), (46, 0.009), (47, 0.067), (48, 0.143), (49, 0.077)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.92324424 <a title="6-lsi-1" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>Introduction: Let's define a learning problem as making predictions given past data. There
are several ways to attack the learning problem which seem to be equivalent to
solving the learning problem.Find the InvariantThis viewpoint says that
learning is all about learning (or incorporating) transformations of objects
that do not change the correct prediction. The best possible invariant is the
one which says "all things of the same class are the same". Finding this is
equivalent to learning. This viewpoint is particularly common when working
with image features.Feature SelectionThis viewpoint says that the way to learn
is by finding the right features to input to a learning algorithm. The best
feature is the one which is the class to predict. Finding this is equivalent
to learning for all reasonable learning algorithms. This viewpoint is common
in several applications of machine learning. SeeGilad's and Bianca's
comments.Find the RepresentationThis is almost the same as feature selection,
except int</p><p>2 0.72294253 <a title="6-lsi-2" href="../hunch_net-2007/hunch_net-2007-07-06-Idempotent-capable_Predictors.html">253 hunch net-2007-07-06-Idempotent-capable Predictors</a></p>
<p>Introduction: One way to distinguish different learning algorithms is by their ability or
inability to easily use an input variable as the predicted output. This is
desirable for at least two reasons:ModularityIf we want to build complex
learning systems via reuse of a subsystem, it's important to have compatible
I/O."Prior" knowledgeMachine learning is often applied in situations where we
do have some knowledge of what the right solution is, often in the form of an
existing system. In such situations, it's good to start with a learning
algorithm that can be at least as good as any existing system.When doing
classification, most learning algorithms can do this. For example, a decision
tree can split on a feature, and then classify. The real differences come up
when we attempt regression. Many of the algorithms we know and commonly use
are not idempotent predictors.Logistic regressors can not be idempotent,
because all input features are mapped through a nonlinearity.Linear regressors
can be idempote</p><p>3 0.65142196 <a title="6-lsi-3" href="../hunch_net-2005/hunch_net-2005-08-22-Do_you_believe_in_induction%3F.html">104 hunch net-2005-08-22-Do you believe in induction?</a></p>
<p>Introduction: Foster Provostgave a talk at the ICMLmetalearning workshopon "metalearning"
and the "no free lunch theorem" which seems worth summarizing.As a review: the
no free lunch theorem is the most complicated way we know of to say that
abiasis required in order to learn. The simplest way to see this is in a
nonprobabilistic setting. If you are given examples of the form(x,y)and you
wish to predictyfromxthen any prediction mechanism errs half the time in
expectation over all sequences of examples. The proof of this is very simple:
on every example a predictor must make some prediction and by symmetry over
the set of sequences it will be wrong half the time and right half the time.
The basic idea of this proof has been applied to many other settings.The
simplistic interpretation of this theorem which many people jump to is
"machine learning is dead" since there can be no single learning algorithm
which can solve all learning problems. This is the wrong way to think about
it. In the real world, w</p><p>4 0.65023673 <a title="6-lsi-4" href="../hunch_net-2009/hunch_net-2009-04-02-Asymmophobia.html">348 hunch net-2009-04-02-Asymmophobia</a></p>
<p>Introduction: One striking feature of many machine learning algorithms is the gymnastics
that designers go through to avoid symmetry breaking. In the most basic form
of machine learning, there are labeled examples composed of features. Each of
these can be treated symmetrically or asymmetrically by algorithms.feature
symmetryEvery feature is treated the same. In gradient update rules, the same
update is applied whether the feature is first or last. In metric-based
predictions, every feature is just as important in computing the
distance.example symmetryEvery example is treated the same. Batch learning
algorithms are great exemplars of this approach.label symmetryEvery label is
treated the same. This is particularly noticeable in multiclass classification
systems which predict according toarg maxlwlxbut it occurs in many other
places as well.Empirically, breaking symmetry well seems to yield great
algorithms.feature asymmetryFor those who like the "boosting is stepwise
additive regression on exponent</p><p>5 0.63743579 <a title="6-lsi-5" href="../hunch_net-2006/hunch_net-2006-01-30-Should_the_Input_Representation_be_a_Vector%3F.html">152 hunch net-2006-01-30-Should the Input Representation be a Vector?</a></p>
<p>Introduction: Let's suppose that we are trying to create a general purpose machine learning
box. The box is fed many examples of the function it is supposed to learn and
(hopefully) succeeds.To date, most such attempts to produce a box of this form
take a vector as input. The elements of the vector might be bits, real
numbers, or 'categorical' data (a discrete set of values).On the other hand,
there are a number of succesful applications of machine learning which do not
seem to use a vector representation as input. For example, in
vision,convolutional neural networkshave been used to solve several vision
problems. The input to the convolutional neural network is essentially the raw
camera image as amatrix. In learning for natural languages, several people
have had success on problems like parts-of-speech tagging using predictors
restricted to a window surrounding the word to be predicted.A vector window
and a matrix both imply a notion of locality which is being actively and
effectively used by thes</p><p>6 0.63564146 <a title="6-lsi-6" href="../hunch_net-2005/hunch_net-2005-02-26-Problem%3A_Reductions_and_Relative_Ranking_Metrics.html">31 hunch net-2005-02-26-Problem: Reductions and Relative Ranking Metrics</a></p>
<p>7 0.63404125 <a title="6-lsi-7" href="../hunch_net-2005/hunch_net-2005-11-28-A_question_of_quantification.html">133 hunch net-2005-11-28-A question of quantification</a></p>
<p>8 0.61733192 <a title="6-lsi-8" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>9 0.61669397 <a title="6-lsi-9" href="../hunch_net-2013/hunch_net-2013-01-07-NYU_Large_Scale_Machine_Learning_Class.html">478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</a></p>
<p>10 0.61658454 <a title="6-lsi-10" href="../hunch_net-2006/hunch_net-2006-08-10-Precision_is_not_accuracy.html">202 hunch net-2006-08-10-Precision is not accuracy</a></p>
<p>11 0.59210539 <a title="6-lsi-11" href="../hunch_net-2005/hunch_net-2005-02-25-Problem%3A_Online_Learning.html">28 hunch net-2005-02-25-Problem: Online Learning</a></p>
<p>12 0.58522725 <a title="6-lsi-12" href="../hunch_net-2006/hunch_net-2006-03-12-Online_learning_or_online_preservation_of_learning%3F.html">163 hunch net-2006-03-12-Online learning or online preservation of learning?</a></p>
<p>13 0.57253534 <a title="6-lsi-13" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>14 0.57111937 <a title="6-lsi-14" href="../hunch_net-2006/hunch_net-2006-03-02-Why_do_people_count_for_learning%3F.html">160 hunch net-2006-03-02-Why do people count for learning?</a></p>
<p>15 0.56959671 <a title="6-lsi-15" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>16 0.56921709 <a title="6-lsi-16" href="../hunch_net-2005/hunch_net-2005-02-03-Learning_Theory%2C_by_assumption.html">12 hunch net-2005-02-03-Learning Theory, by assumption</a></p>
<p>17 0.55962378 <a title="6-lsi-17" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>18 0.55491978 <a title="6-lsi-18" href="../hunch_net-2005/hunch_net-2005-04-16-Which_Assumptions_are_Reasonable%3F.html">57 hunch net-2005-04-16-Which Assumptions are Reasonable?</a></p>
<p>19 0.55390823 <a title="6-lsi-19" href="../hunch_net-2006/hunch_net-2006-06-16-Regularization_%3D_Robustness.html">185 hunch net-2006-06-16-Regularization = Robustness</a></p>
<p>20 0.55160558 <a title="6-lsi-20" href="../hunch_net-2005/hunch_net-2005-03-17-Going_all_the_Way%2C_Sometimes.html">42 hunch net-2005-03-17-Going all the Way, Sometimes</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(35, 0.085), (42, 0.371), (45, 0.055), (67, 0.226), (68, 0.047), (74, 0.107)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.96736312 <a title="6-lda-1" href="../hunch_net-2005/hunch_net-2005-04-16-Which_Assumptions_are_Reasonable%3F.html">57 hunch net-2005-04-16-Which Assumptions are Reasonable?</a></p>
<p>Introduction: One of the most confusing things about understanding learning theory is the
vast array of differing assumptions. Some critical thought about which of
these assumptions are reasonable for real-world problems may be useful.Before
we even start thinking about assumptions, it's important to realize that the
word hasmultiple meanings. The meaning used here is "assumption = axiom" (i.e.
something you can not verify).AssumptionReasonable?Which
analysis?Example/notesIndependent and Identically Distributed
DataSometimesPAC,ERM,Prediction bounds,statisticsTheKDD cup 2004 physics
datasetis plausibly IID data. There are a number of situations which are
"almost IID" in the sense that IID analysis results in correct intuitions.
Unreasonable in adversarial situations (stock market, war, etcâ&euro;Ś)Independently
Distributed DataMore than IID, but still only sometimesonline->batch
conversionLosing "identical" can be helpful in situations where you have a
cyclic process generating data.Finite exchangeability</p><p>2 0.94751585 <a title="6-lda-2" href="../hunch_net-2011/hunch_net-2011-06-22-Ultra_LDA.html">436 hunch net-2011-06-22-Ultra LDA</a></p>
<p>Introduction: ShravanandAlex's LDA code isreleased. On a single machine, I'm not sure how it
currently compares to the online LDA inVW, but the ability to effectively
scale across very many machines is surely interesting.</p><p>3 0.91642201 <a title="6-lda-3" href="../hunch_net-2005/hunch_net-2005-03-21-Research_Styles_in_Machine_Learning.html">44 hunch net-2005-03-21-Research Styles in Machine Learning</a></p>
<p>Introduction: Machine Learning is a field with an impressively diverse set of reseearch
styles. Understanding this may be important in appreciating what you see at a
conference.Engineering. How can I solve this problem? People in the
engineering research style try to solve hard problems directly by any means
available and then describe how they did it. This is typical of problem-
specific conferences and communities.Scientific. What are the principles for
solving learning problems? People in this research style test techniques on
many different problems. This is fairly common at ICML and NIPS.Mathematical.
How can the learning problem be mathematically understood? People in this
research style prove theorems with implications for learning but often do not
implement (or test algorithms). COLT is a typical conference for this
style.Many people manage to cross these styles, and that is often
beneficial.Whenver we list a set of alternative, it becomes natural to think
"which is best?" In this case of le</p><p>same-blog 4 0.9132846 <a title="6-lda-4" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>Introduction: Let's define a learning problem as making predictions given past data. There
are several ways to attack the learning problem which seem to be equivalent to
solving the learning problem.Find the InvariantThis viewpoint says that
learning is all about learning (or incorporating) transformations of objects
that do not change the correct prediction. The best possible invariant is the
one which says "all things of the same class are the same". Finding this is
equivalent to learning. This viewpoint is particularly common when working
with image features.Feature SelectionThis viewpoint says that the way to learn
is by finding the right features to input to a learning algorithm. The best
feature is the one which is the class to predict. Finding this is equivalent
to learning for all reasonable learning algorithms. This viewpoint is common
in several applications of machine learning. SeeGilad's and Bianca's
comments.Find the RepresentationThis is almost the same as feature selection,
except int</p><p>5 0.87419224 <a title="6-lda-5" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>Introduction: Founding a successful new conference is extraordinarily difficult. As a
conference founder, you must manage to attract a significant number of good
papers--enough to entice the participants into participating next year and to
(generally) to grow the conference. For someone choosing to participate in a
new conference, there is a very significant decision to make: do you send a
paper to some new conference with no guarantee that the conference will work
out? Or do you send it to another (possibly less related) conference that you
are sure will work?The conference founding problem is a joint agreement
problem with a very significant barrier. Workshops are a way around this
problem, and workshops attached to conferences are a particularly effective
means for this. A workshop at a conference is sure to have people available to
speak and attend and is sure to have a large audience available. Presenting
work at a workshop is not generally exclusive: it can also be presented at a
conference. F</p><p>6 0.86970401 <a title="6-lda-6" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>7 0.86055422 <a title="6-lda-7" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>8 0.85809076 <a title="6-lda-8" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>9 0.85724187 <a title="6-lda-9" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>10 0.85623103 <a title="6-lda-10" href="../hunch_net-2006/hunch_net-2006-01-18-Is_Multitask_Learning_Black-Boxable%3F.html">149 hunch net-2006-01-18-Is Multitask Learning Black-Boxable?</a></p>
<p>11 0.85383928 <a title="6-lda-11" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>12 0.85345924 <a title="6-lda-12" href="../hunch_net-2011/hunch_net-2011-10-24-2011_ML_symposium_and_the_bears.html">448 hunch net-2011-10-24-2011 ML symposium and the bears</a></p>
<p>13 0.8521263 <a title="6-lda-13" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>14 0.85199583 <a title="6-lda-14" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>15 0.85172552 <a title="6-lda-15" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>16 0.85139811 <a title="6-lda-16" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>17 0.8509177 <a title="6-lda-17" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>18 0.85028321 <a title="6-lda-18" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>19 0.85014701 <a title="6-lda-19" href="../hunch_net-2006/hunch_net-2006-03-02-Why_do_people_count_for_learning%3F.html">160 hunch net-2006-03-02-Why do people count for learning?</a></p>
<p>20 0.84949821 <a title="6-lda-20" href="../hunch_net-2007/hunch_net-2007-01-10-A_Deep_Belief_Net_Learning_Problem.html">227 hunch net-2007-01-10-A Deep Belief Net Learning Problem</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
