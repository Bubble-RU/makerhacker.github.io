<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>349 hunch net-2009-04-21-Interesting Presentations at Snowbird</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2009" href="../home/hunch_net-2009_home.html">hunch_net-2009</a> <a title="hunch_net-2009-349" href="#">hunch_net-2009-349</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>349 hunch net-2009-04-21-Interesting Presentations at Snowbird</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2009-349-html" href="http://hunch.net/?p=657">html</a></p><p>Introduction: Here are a few of presentations interesting me at thesnowbird learningworkshop
(which, amusingly, was in Florida withAIStat).Thomas Breueldescribed machine
learning problems within OCR and an open sourceOCR software/researchplatform
with modular learning components as well has a 60Million size dataset derived
fromGoogle's scanned books.Kristen GraumanandFei-Fei Lidiscussed using active
learning with different cost labels and large datasets forimage ontology. Both
of them usedMechanical Turkas alabeling system, which looks to become routine,
at least for vision problems.Russ Tedrakediscussed using machine learning for
control, with a basic claim that it was the way to go for problems involving a
mediumReynold's numbersuch as in bird flight, where simulation is extremely
intense.Yann LeCunpresented a poster on anFPGA for convolutional neural
networksyielding a factor of 100 speedup in processing. In addition to the
graphics processor approachRajathas worked on, this seems like an effecti</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('florida', 0.219), ('products', 0.219), ('withaistat', 0.219), ('speedup', 0.203), ('thesnowbird', 0.203), ('flight', 0.203), ('graphics', 0.203), ('modular', 0.203), ('learningworkshop', 0.191), ('dot', 0.191), ('processor', 0.183), ('convolutional', 0.175), ('routine', 0.175), ('derived', 0.169), ('components', 0.164), ('involving', 0.164), ('poster', 0.145), ('presentations', 0.142), ('control', 0.136), ('vision', 0.134)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="349-tfidf-1" href="../hunch_net-2009/hunch_net-2009-04-21-Interesting_Presentations_at_Snowbird.html">349 hunch net-2009-04-21-Interesting Presentations at Snowbird</a></p>
<p>Introduction: Here are a few of presentations interesting me at thesnowbird learningworkshop
(which, amusingly, was in Florida withAIStat).Thomas Breueldescribed machine
learning problems within OCR and an open sourceOCR software/researchplatform
with modular learning components as well has a 60Million size dataset derived
fromGoogle's scanned books.Kristen GraumanandFei-Fei Lidiscussed using active
learning with different cost labels and large datasets forimage ontology. Both
of them usedMechanical Turkas alabeling system, which looks to become routine,
at least for vision problems.Russ Tedrakediscussed using machine learning for
control, with a basic claim that it was the way to go for problems involving a
mediumReynold's numbersuch as in bird flight, where simulation is extremely
intense.Yann LeCunpresented a poster on anFPGA for convolutional neural
networksyielding a factor of 100 speedup in processing. In addition to the
graphics processor approachRajathas worked on, this seems like an effecti</p><p>2 0.09143766 <a title="349-tfidf-2" href="../hunch_net-2008/hunch_net-2008-04-30-Concerns_about_the_Large_Scale_Learning_Challenge.html">300 hunch net-2008-04-30-Concerns about the Large Scale Learning Challenge</a></p>
<p>Introduction: Thelarge scale learning challengefor ICML interests me a great deal, although
I have concerns about the way it is structured.From theinstructions page,
several issues come up:Large DefinitionMy personal definition of dataset size
is:smallA dataset is small if a human could look at the dataset and plausibly
find a good solution.mediumA dataset is mediumsize if it fits in the RAM of a
reasonably priced computer.largeA large dataset does not fit in the RAM of a
reasonably priced computer.By this definition, all of the datasets are medium
sized. This might sound like a pissing match over dataset size, but I believe
it is more than that.The fundamental reason for these definitions is that they
correspond to transitions in the sorts of approaches which are feasible. From
small to medium, the ability to use a human as the learning algorithm
degrades. From medium to large, it becomes essential to have learning
algorithms that don't require random access to examples.No Loading TimeThe
medium sc</p><p>3 0.090255618 <a title="349-tfidf-3" href="../hunch_net-2007/hunch_net-2007-01-26-Parallel_Machine_Learning_Problems.html">229 hunch net-2007-01-26-Parallel Machine Learning Problems</a></p>
<p>Introduction: Parallel machine learning is a subject rarely addressed at machine learning
conferences. Nevertheless, it seems likely to increase in importance
because:Data set sizes appear to be growing substantially faster than
computation. Essentially, this happens because more and more sensors of
various sorts are being hooked up to the internet.Serial speedups of
processors seem are relatively stalled. The new trend is to make processors
more powerful by making themmulticore.BothAMDandIntelare making dual core
designs standard, with plans for more parallelism in the future.IBM'sCell
processorhas (essentially) 9 cores.Modern graphics chips can have an order of
magnitude more separate execution units.The meaning of 'core' varies a bit
from processor to processor, but the overall trend seems quite clear.So, how
do we parallelize machine learning algorithms?The simplest and most common
technique is to simply run the same learning algorithm with different
parameters on different processors. Cluster m</p><p>4 0.088701084 <a title="349-tfidf-4" href="../hunch_net-2011/hunch_net-2011-07-11-Interesting_Neural_Network_Papers_at_ICML_2011.html">438 hunch net-2011-07-11-Interesting Neural Network Papers at ICML 2011</a></p>
<p>Introduction: Maybe it's too early to call, but with four separate Neural Network sessions
at this year'sICML, it looks like Neural Networks are making a comeback. Here
are my highlights of these sessions. In general, my feeling is that these
papers both demystify deep learning and show its broader applicability.The
first observation I made is that the once disreputable "Neural" nomenclature
is being used againin lieu of"deep learning". Maybe it's because Adam Coates
et al. showed that single layer networks can work surprisingly well.An
Analysis of Single-Layer Networks in Unsupervised Feature Learning,Adam
Coates,Honglak Lee,Andrew Y. Ng(AISTATS 2011)The Importance of Encoding Versus
Training with Sparse Coding and Vector Quantization,Adam Coates,Andrew Y.
Ng(ICML 2011)Another surprising result out of Andrew Ng's group comes from
Andrew Saxe et al. who show that certain convolutional pooling architectures
can obtain close to state-of-the-art performance with random weights (that is,
without actuall</p><p>5 0.086004294 <a title="349-tfidf-5" href="../hunch_net-2009/hunch_net-2009-07-31-Vowpal_Wabbit_Open_Source_Project.html">365 hunch net-2009-07-31-Vowpal Wabbit Open Source Project</a></p>
<p>Introduction: Today brings a new release of theVowpal Wabbitfast online learning software.
This time, unlike the previous release, the project itself is going open
source, developing viagithub. For example, the lastest and greatest can be
downloaded via:git clone git://github.com/JohnLangford/vowpal_wabbit.gitIf you
aren't familiar withgit, it's a distributed version control system which
supports quick and easy branching, as well as reconciliation.This version of
the code is confirmed to compile without complaint on at least some flavors of
OSX as well as Linux boxes.As much of the point of this project is pushing the
limits of fast and effective machine learning, let me mention a few datapoints
from my experience.The program can effectively scale up to batch-style
training on sparse terafeature (i.e. 1012sparse feature) size datasets. The
limiting factor is typically i/o.I started using the the real datasets from
thelarge-scale learningworkshop as a convenient benchmark. The largest dataset
takes a</p><p>6 0.085801698 <a title="349-tfidf-6" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>7 0.08144103 <a title="349-tfidf-7" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<p>8 0.079590224 <a title="349-tfidf-8" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<p>9 0.078675836 <a title="349-tfidf-9" href="../hunch_net-2005/hunch_net-2005-04-06-Structured_Regret_Minimization.html">53 hunch net-2005-04-06-Structured Regret Minimization</a></p>
<p>10 0.07756719 <a title="349-tfidf-10" href="../hunch_net-2007/hunch_net-2007-06-24-Interesting_Papers_at_ICML_2007.html">251 hunch net-2007-06-24-Interesting Papers at ICML 2007</a></p>
<p>11 0.076851629 <a title="349-tfidf-11" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>12 0.075803526 <a title="349-tfidf-12" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>13 0.075033151 <a title="349-tfidf-13" href="../hunch_net-2005/hunch_net-2005-11-02-Progress_in_Active_Learning.html">127 hunch net-2005-11-02-Progress in Active Learning</a></p>
<p>14 0.074846737 <a title="349-tfidf-14" href="../hunch_net-2005/hunch_net-2005-07-11-AAAI_blog.html">92 hunch net-2005-07-11-AAAI blog</a></p>
<p>15 0.074072585 <a title="349-tfidf-15" href="../hunch_net-2010/hunch_net-2010-12-04-Vowpal_Wabbit%2C_version_5.0%2C_and_the_second_heresy.html">419 hunch net-2010-12-04-Vowpal Wabbit, version 5.0, and the second heresy</a></p>
<p>16 0.07310386 <a title="349-tfidf-16" href="../hunch_net-2005/hunch_net-2005-07-13-%26%238220%3BSister_Conference%26%238221%3B_presentations.html">93 hunch net-2005-07-13-&#8220;Sister Conference&#8221; presentations</a></p>
<p>17 0.071845457 <a title="349-tfidf-17" href="../hunch_net-2005/hunch_net-2005-03-22-Active_learning.html">45 hunch net-2005-03-22-Active learning</a></p>
<p>18 0.0718062 <a title="349-tfidf-18" href="../hunch_net-2010/hunch_net-2010-04-14-MLcomp%3A_a_website_for_objectively_comparing_ML_algorithms.html">393 hunch net-2010-04-14-MLcomp: a website for objectively comparing ML algorithms</a></p>
<p>19 0.071359232 <a title="349-tfidf-19" href="../hunch_net-2005/hunch_net-2005-11-16-The_Everything_Ensemble_Edge.html">131 hunch net-2005-11-16-The Everything Ensemble Edge</a></p>
<p>20 0.071261302 <a title="349-tfidf-20" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.145), (1, -0.037), (2, 0.071), (3, 0.032), (4, -0.07), (5, 0.086), (6, 0.016), (7, 0.02), (8, 0.013), (9, 0.014), (10, -0.019), (11, -0.002), (12, 0.006), (13, 0.057), (14, 0.039), (15, -0.047), (16, -0.048), (17, 0.019), (18, 0.033), (19, -0.025), (20, 0.043), (21, 0.017), (22, 0.004), (23, 0.038), (24, 0.007), (25, 0.125), (26, -0.034), (27, -0.038), (28, -0.126), (29, -0.069), (30, 0.089), (31, -0.002), (32, 0.028), (33, 0.113), (34, -0.03), (35, 0.049), (36, -0.003), (37, 0.09), (38, -0.012), (39, -0.093), (40, -0.064), (41, -0.044), (42, -0.125), (43, 0.067), (44, 0.035), (45, 0.007), (46, -0.014), (47, 0.041), (48, -0.058), (49, -0.029)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.91415781 <a title="349-lsi-1" href="../hunch_net-2009/hunch_net-2009-04-21-Interesting_Presentations_at_Snowbird.html">349 hunch net-2009-04-21-Interesting Presentations at Snowbird</a></p>
<p>Introduction: Here are a few of presentations interesting me at thesnowbird learningworkshop
(which, amusingly, was in Florida withAIStat).Thomas Breueldescribed machine
learning problems within OCR and an open sourceOCR software/researchplatform
with modular learning components as well has a 60Million size dataset derived
fromGoogle's scanned books.Kristen GraumanandFei-Fei Lidiscussed using active
learning with different cost labels and large datasets forimage ontology. Both
of them usedMechanical Turkas alabeling system, which looks to become routine,
at least for vision problems.Russ Tedrakediscussed using machine learning for
control, with a basic claim that it was the way to go for problems involving a
mediumReynold's numbersuch as in bird flight, where simulation is extremely
intense.Yann LeCunpresented a poster on anFPGA for convolutional neural
networksyielding a factor of 100 speedup in processing. In addition to the
graphics processor approachRajathas worked on, this seems like an effecti</p><p>2 0.55943757 <a title="349-lsi-2" href="../hunch_net-2005/hunch_net-2005-02-15-ESPgame_and_image_labeling.html">20 hunch net-2005-02-15-ESPgame and image labeling</a></p>
<p>Introduction: Luis von Ahnhas been running theespgamefor awhile now. The espgame provides a
picture to two randomly paired people across the web, and asks them to agree
on a label. It hasn't managed to label the web yet, but it has produced alarge
datasetof (image, label) pairs. I organized the dataset so you couldexplore
the implied bipartite graph(requires much bandwidth).Relative to other image
datasets, this one is quite large--67000 images, 358,000 labels (average of
5/image with variation from 1 to 19), and 22,000 unique labels (one every 3
images). The dataset is also very 'natural', consisting of images spidered
from the internet. The multiple label characteristic is intriguing because
'learning to learn' and metalearning techniques may be applicable. The
'natural' quality means that this dataset varies greatly in difficulty from
easy (predicting "red") to hard (predicting "funny") and potentially more
rewarding to tackle.The open problem here is, of course, to make an internet
image labelin</p><p>3 0.50499296 <a title="349-lsi-3" href="../hunch_net-2006/hunch_net-2006-02-27-The_Peekaboom_Dataset.html">159 hunch net-2006-02-27-The Peekaboom Dataset</a></p>
<p>Introduction: Luis von Ahn'sPeekaboom projecthas yieldeddata(830MB).Peekaboom is the second
attempt (afterEspgame) to produce a dataset which is useful for learning to
solve vision problems based on voluntary game play. As a second attempt, it is
meant to address all of the shortcomings of the first attempt. In
particular:The locations of specific objects are provided by the data.The data
collection is far more complete and extensive.The data consists of:The source
images. (1 file per image, just short of 60K images.)The in-game events. (1
file per image, in a lispy syntax.)A description of the event language.There
is a great deal of very specific and relevant data here so the hope that this
will help solve vision problems seems quite reasonable.</p><p>4 0.48605329 <a title="349-lsi-4" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<p>Introduction: This post is by Daniel Hsu and John Langford.In selective sampling style
active learning, a learning algorithm chooses which examples to label. We now
have an active learning algorithm that is:Efficientin label complexity,
unlabeled complexity, and computational complexity.Competitivewith supervised
learning anywhere that supervised learning works.Compatiblewith online
learning, with any optimization-based learning algorithm, with any loss
function, with offline testing, and even with changing learning
algorithms.Empiricallyeffective.The basic idea is to combinedisagreement
region-based samplingwithimportance weighting: an example is selected to be
labeled with probability proportional to how useful it is for distinguishing
among near-optimal classifiers, and labeled examples are importance-weighted
by the inverse of these probabilities. The combination of these simple ideas
removes thesampling biasproblem that has plagued many previous heuristics for
active learning, and yet leads to</p><p>5 0.48497477 <a title="349-lsi-5" href="../hunch_net-2010/hunch_net-2010-12-02-Traffic_Prediction_Problem.html">418 hunch net-2010-12-02-Traffic Prediction Problem</a></p>
<p>Introduction: Slashdotpoints out theTraffic Prediction Challengewhich looks pretty fun. The
temporal aspect seems to be very common in many real-world problems and
somewhat understudied.</p><p>6 0.47335976 <a title="349-lsi-6" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>7 0.47253367 <a title="349-lsi-7" href="../hunch_net-2005/hunch_net-2005-04-06-Structured_Regret_Minimization.html">53 hunch net-2005-04-06-Structured Regret Minimization</a></p>
<p>8 0.47166717 <a title="349-lsi-8" href="../hunch_net-2008/hunch_net-2008-04-30-Concerns_about_the_Large_Scale_Learning_Challenge.html">300 hunch net-2008-04-30-Concerns about the Large Scale Learning Challenge</a></p>
<p>9 0.45907232 <a title="349-lsi-9" href="../hunch_net-2007/hunch_net-2007-01-26-Parallel_Machine_Learning_Problems.html">229 hunch net-2007-01-26-Parallel Machine Learning Problems</a></p>
<p>10 0.44800857 <a title="349-lsi-10" href="../hunch_net-2006/hunch_net-2006-01-30-Should_the_Input_Representation_be_a_Vector%3F.html">152 hunch net-2006-01-30-Should the Input Representation be a Vector?</a></p>
<p>11 0.44615412 <a title="349-lsi-11" href="../hunch_net-2005/hunch_net-2005-11-05-The_design_of_a_computing_cluster.html">128 hunch net-2005-11-05-The design of a computing cluster</a></p>
<p>12 0.44328162 <a title="349-lsi-12" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>13 0.4430548 <a title="349-lsi-13" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>14 0.4320569 <a title="349-lsi-14" href="../hunch_net-2010/hunch_net-2010-05-20-Google_Predict.html">399 hunch net-2010-05-20-Google Predict</a></p>
<p>15 0.42878577 <a title="349-lsi-15" href="../hunch_net-2006/hunch_net-2006-12-06-The_Spam_Problem.html">223 hunch net-2006-12-06-The Spam Problem</a></p>
<p>16 0.42825863 <a title="349-lsi-16" href="../hunch_net-2005/hunch_net-2005-11-16-The_Everything_Ensemble_Edge.html">131 hunch net-2005-11-16-The Everything Ensemble Edge</a></p>
<p>17 0.4271467 <a title="349-lsi-17" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>18 0.42581132 <a title="349-lsi-18" href="../hunch_net-2007/hunch_net-2007-12-19-Cool_and_interesting_things_seen_at_NIPS.html">279 hunch net-2007-12-19-Cool and interesting things seen at NIPS</a></p>
<p>19 0.4250842 <a title="349-lsi-19" href="../hunch_net-2007/hunch_net-2007-12-21-Vowpal_Wabbit_Code_Release.html">281 hunch net-2007-12-21-Vowpal Wabbit Code Release</a></p>
<p>20 0.42208406 <a title="349-lsi-20" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(29, 0.603), (42, 0.197), (45, 0.076)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.91752231 <a title="349-lda-1" href="../hunch_net-2006/hunch_net-2006-10-02-%241M_Netflix_prediction_contest.html">211 hunch net-2006-10-02-$1M Netflix prediction contest</a></p>
<p>Introduction: Netflix isrunning a contestto improve recommender prediction systems. A 10%
improvement over their current system yields a $1M prize. Failing that, the
best smaller improvement yields a smaller $50K prize. This contest looks quite
real, and the $50K prize money is almost certainly achievable with a bit of
thought. The contest also comes with a dataset which is apparently 2 orders of
magnitude larger than any other public recommendation system datasets.</p><p>2 0.90299708 <a title="349-lda-2" href="../hunch_net-2006/hunch_net-2006-07-17-A_Winner.html">197 hunch net-2006-07-17-A Winner</a></p>
<p>Introduction: Ed Snelsonwon thePredictive Uncertainty in Environmental Modelling
Competitionin the temp(erature) category usingthis algorithm. Some
characteristics of the algorithm are:Gradient descent… on about 600
parameters… with local minima… to solve regression.This bears a strong
resemblance to a neural network. The two main differences seem to be:The
system has a probabilistic interpretation (which may aid design).There are
(perhaps) fewer parameters than a typical neural network might have for the
same problem (aiding speed).</p><p>same-blog 3 0.84993166 <a title="349-lda-3" href="../hunch_net-2009/hunch_net-2009-04-21-Interesting_Presentations_at_Snowbird.html">349 hunch net-2009-04-21-Interesting Presentations at Snowbird</a></p>
<p>Introduction: Here are a few of presentations interesting me at thesnowbird learningworkshop
(which, amusingly, was in Florida withAIStat).Thomas Breueldescribed machine
learning problems within OCR and an open sourceOCR software/researchplatform
with modular learning components as well has a 60Million size dataset derived
fromGoogle's scanned books.Kristen GraumanandFei-Fei Lidiscussed using active
learning with different cost labels and large datasets forimage ontology. Both
of them usedMechanical Turkas alabeling system, which looks to become routine,
at least for vision problems.Russ Tedrakediscussed using machine learning for
control, with a basic claim that it was the way to go for problems involving a
mediumReynold's numbersuch as in bird flight, where simulation is extremely
intense.Yann LeCunpresented a poster on anFPGA for convolutional neural
networksyielding a factor of 100 speedup in processing. In addition to the
graphics processor approachRajathas worked on, this seems like an effecti</p><p>4 0.65789396 <a title="349-lda-4" href="../hunch_net-2007/hunch_net-2007-04-13-What_to_do_with_an_unreasonable_conditional_accept.html">238 hunch net-2007-04-13-What to do with an unreasonable conditional accept</a></p>
<p>Introduction: Last year about this time, we received a conditional accept for thesearn
paper, which asked us to reference a paper that was not reasonable to cite
because there was strictly more relevant work by the same authors that we
already cited. We wrote a response explaining this, and didn't cite it in the
final draft, giving the SPC an excuse toreject the paper, leading to
unhappiness for all.Later,Sanjoy Dasguptasuggested that an alternative was to
talk to the PC chair instead, as soon as you see that a conditional accept is
unreasonable.William Cohenand I spoke about this by email, the relevant bit of
which is:If an SPC asks for a revision that is inappropriate, the
correctaction is to contact the chairs as soon as the decision is made,clearly
explaining what the problem is, so we can decide whether ornot to over-rule
the SPC. As you say, this is extra work for uschairs, but that's part of the
job, and we're willing to do that sortof work to improve the overall quality
of the reviewing proc</p><p>5 0.60670847 <a title="349-lda-5" href="../hunch_net-2008/hunch_net-2008-12-07-A_NIPS_paper.html">330 hunch net-2008-12-07-A NIPS paper</a></p>
<p>Introduction: I'm skipping NIPS this year in favor ofAda, but I wanted to point outthis
paperbyAndriy MnihandGeoff Hinton. The basic claim of the paper is that by
carefully but automatically constructing a binary tree over words, it's
possible to predict words well with huge computational resource savings over
unstructured approaches.I'm interested in this beyond the application to word
prediction because it is relevant to the general normalization problem: If you
want to predict the probability of one of a large number of events, often you
must compute a predicted score for all the events and then normalize, a
computationally inefficient operation. The problem comes up in many places
using probabilistic models, but I've run into it with high-dimensional
regression.There are a couple workarounds for this computational
bug:Approximate. There are many ways. Often the approximations are
uncontrolled (i.e. can be arbitrarily bad), and hence finicky in
application.Avoid. You don't really want a probabili</p><p>6 0.35789129 <a title="349-lda-6" href="../hunch_net-2008/hunch_net-2008-04-30-Concerns_about_the_Large_Scale_Learning_Challenge.html">300 hunch net-2008-04-30-Concerns about the Large Scale Learning Challenge</a></p>
<p>7 0.33623326 <a title="349-lda-7" href="../hunch_net-2005/hunch_net-2005-02-15-ESPgame_and_image_labeling.html">20 hunch net-2005-02-15-ESPgame and image labeling</a></p>
<p>8 0.32386339 <a title="349-lda-8" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>9 0.31869575 <a title="349-lda-9" href="../hunch_net-2008/hunch_net-2008-05-23-Three_levels_of_addressing_the_Netflix_Prize.html">301 hunch net-2008-05-23-Three levels of addressing the Netflix Prize</a></p>
<p>10 0.31664291 <a title="349-lda-10" href="../hunch_net-2009/hunch_net-2009-09-21-Netflix_finishes_%28and_starts%29.html">371 hunch net-2009-09-21-Netflix finishes (and starts)</a></p>
<p>11 0.31334239 <a title="349-lda-11" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>12 0.31265342 <a title="349-lda-12" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>13 0.31158814 <a title="349-lda-13" href="../hunch_net-2012/hunch_net-2012-10-26-ML_Symposium_and_Strata-Hadoop_World.html">475 hunch net-2012-10-26-ML Symposium and Strata-Hadoop World</a></p>
<p>14 0.31143236 <a title="349-lda-14" href="../hunch_net-2009/hunch_net-2009-07-31-Vowpal_Wabbit_Open_Source_Project.html">365 hunch net-2009-07-31-Vowpal Wabbit Open Source Project</a></p>
<p>15 0.31076467 <a title="349-lda-15" href="../hunch_net-2006/hunch_net-2006-01-23-On_Coding_via_Mutual_Information_%26%23038%3B_Bayes_Nets.html">150 hunch net-2006-01-23-On Coding via Mutual Information &#038; Bayes Nets</a></p>
<p>16 0.31041062 <a title="349-lda-16" href="../hunch_net-2005/hunch_net-2005-11-02-Progress_in_Active_Learning.html">127 hunch net-2005-11-02-Progress in Active Learning</a></p>
<p>17 0.31021333 <a title="349-lda-17" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<p>18 0.30987969 <a title="349-lda-18" href="../hunch_net-2005/hunch_net-2005-05-06-Don%26%238217%3Bt_mix_the_solution_into_the_problem.html">67 hunch net-2005-05-06-Don&#8217;t mix the solution into the problem</a></p>
<p>19 0.30986953 <a title="349-lda-19" href="../hunch_net-2005/hunch_net-2005-06-18-Lower_Bounds_for_Learning_Reductions.html">83 hunch net-2005-06-18-Lower Bounds for Learning Reductions</a></p>
<p>20 0.30934373 <a title="349-lda-20" href="../hunch_net-2010/hunch_net-2010-04-14-MLcomp%3A_a_website_for_objectively_comparing_ML_algorithms.html">393 hunch net-2010-04-14-MLcomp: a website for objectively comparing ML algorithms</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
