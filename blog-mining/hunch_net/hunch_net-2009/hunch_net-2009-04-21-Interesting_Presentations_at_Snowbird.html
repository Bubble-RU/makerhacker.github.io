<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>349 hunch net-2009-04-21-Interesting Presentations at Snowbird</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2009" href="../home/hunch_net-2009_home.html">hunch_net-2009</a> <a title="hunch_net-2009-349" href="#">hunch_net-2009-349</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>349 hunch net-2009-04-21-Interesting Presentations at Snowbird</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2009-349-html" href="http://hunch.net/?p=657">html</a></p><p>Introduction: Here are a few of presentations interesting me at the  snowbird learning  workshop (which, amusingly, was in Florida with  AIStat ).  
  
  Thomas Breuel  described machine learning problems within OCR and an open source  OCR software/research  platform with modular learning components as well has a 60Million size dataset derived from  Google ‘s scanned books. 
  Kristen Grauman  and  Fei-Fei Li  discussed using active learning with different cost labels and large datasets for  image ontology .  Both of them used  Mechanical Turk  as a  labeling system , which looks to become routine, at least for vision problems. 
  Russ Tedrake  discussed using machine learning for control, with a basic claim that it was the way to go for problems involving a medium  Reynold’s number  such as in bird flight, where simulation is extremely intense. 
  Yann LeCun  presented a poster on an  FPGA for convolutional neural networks  yielding a factor of 100 speedup in processing.   In addition to the graphi</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Here are a few of presentations interesting me at the  snowbird learning  workshop (which, amusingly, was in Florida with  AIStat ). [sent-1, score-0.245]
</p><p>2 Thomas Breuel  described machine learning problems within OCR and an open source  OCR software/research  platform with modular learning components as well has a 60Million size dataset derived from  Google ‘s scanned books. [sent-2, score-0.692]
</p><p>3 Kristen Grauman  and  Fei-Fei Li  discussed using active learning with different cost labels and large datasets for  image ontology . [sent-3, score-0.6]
</p><p>4 Both of them used  Mechanical Turk  as a  labeling system , which looks to become routine, at least for vision problems. [sent-4, score-0.314]
</p><p>5 Russ Tedrake  discussed using machine learning for control, with a basic claim that it was the way to go for problems involving a medium  Reynold’s number  such as in bird flight, where simulation is extremely intense. [sent-5, score-0.744]
</p><p>6 Yann LeCun  presented a poster on an  FPGA for convolutional neural networks  yielding a factor of 100 speedup in processing. [sent-6, score-0.775]
</p><p>7 In addition to the graphics processor approach  Rajat  has worked on, this seems like an effective approach to deal with the need to compute many dot products. [sent-7, score-0.827]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ocr', 0.373), ('florida', 0.166), ('breuel', 0.166), ('products', 0.166), ('discussed', 0.161), ('speedup', 0.153), ('ontology', 0.153), ('flight', 0.153), ('graphics', 0.153), ('modular', 0.153), ('simulation', 0.153), ('rajat', 0.145), ('processor', 0.145), ('dot', 0.145), ('turk', 0.145), ('snowbird', 0.138), ('mechanical', 0.138), ('aistat', 0.133), ('routine', 0.133), ('thomas', 0.133), ('medium', 0.128), ('derived', 0.128), ('convolutional', 0.124), ('components', 0.124), ('involving', 0.124), ('described', 0.12), ('li', 0.12), ('lecun', 0.114), ('labeling', 0.114), ('yielding', 0.114), ('image', 0.109), ('poster', 0.109), ('presentations', 0.107), ('yann', 0.105), ('control', 0.103), ('compute', 0.1), ('looks', 0.1), ('vision', 0.1), ('approach', 0.099), ('presented', 0.098), ('google', 0.094), ('extremely', 0.094), ('labels', 0.092), ('factor', 0.091), ('addition', 0.086), ('neural', 0.086), ('datasets', 0.085), ('dataset', 0.084), ('claim', 0.084), ('within', 0.083)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="349-tfidf-1" href="../hunch_net-2009/hunch_net-2009-04-21-Interesting_Presentations_at_Snowbird.html">349 hunch net-2009-04-21-Interesting Presentations at Snowbird</a></p>
<p>Introduction: Here are a few of presentations interesting me at the  snowbird learning  workshop (which, amusingly, was in Florida with  AIStat ).  
  
  Thomas Breuel  described machine learning problems within OCR and an open source  OCR software/research  platform with modular learning components as well has a 60Million size dataset derived from  Google ‘s scanned books. 
  Kristen Grauman  and  Fei-Fei Li  discussed using active learning with different cost labels and large datasets for  image ontology .  Both of them used  Mechanical Turk  as a  labeling system , which looks to become routine, at least for vision problems. 
  Russ Tedrake  discussed using machine learning for control, with a basic claim that it was the way to go for problems involving a medium  Reynold’s number  such as in bird flight, where simulation is extremely intense. 
  Yann LeCun  presented a poster on an  FPGA for convolutional neural networks  yielding a factor of 100 speedup in processing.   In addition to the graphi</p><p>2 0.18611445 <a title="349-tfidf-2" href="../hunch_net-2011/hunch_net-2011-04-18-A_paper_not_at_Snowbird.html">431 hunch net-2011-04-18-A paper not at Snowbird</a></p>
<p>Introduction: Unfortunately, a scheduling failure meant I missed all of  AIStat  and most of the  learning workshop , otherwise known as Snowbird, when it’s at  Snowbird .  
 
At snowbird, the talk on  Sum-Product  networks by  Hoifung Poon  stood out to me ( Pedro Domingos  is a coauthor.).  The basic point was that by appropriately constructing networks based on sums and products, the normalization problem in probabilistic models is eliminated, yielding a highly tractable yet flexible representation+learning algorithm.  As an algorithm, this is noticeably cleaner than deep belief networks with a claim to being an order of magnitude faster and working better on an image completion task.
 
Snowbird doesn’t have real papers—just the abstract above.  I look forward to seeing the paper.  (added: Rodrigo points out the deep learning workshop  draft .)</p><p>3 0.11796457 <a title="349-tfidf-3" href="../hunch_net-2007/hunch_net-2007-12-12-Workshop_Summary%26%238212%3BPrinciples_of_Learning_Problem_Design.html">277 hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</a></p>
<p>Introduction: This is a summary of the  workshop on Learning Problem Design  which  Alina  and I ran at  NIPS  this year.
 
The first question many people have is “What is learning problem design?”  This workshop is about admitting that solving learning problems does not start with labeled data, but rather somewhere before.  When humans are hired to produce labels, this is usually not a serious problem because you can tell them precisely what semantics you want the labels to have, and we can fix some set of features in advance.  However, when other methods are used this becomes more problematic.  This focus is important for Machine Learning because there are very large quantities of data which are not labeled by a hired human.
 
The title of the workshop was a bit ambitious, because a workshop is not long enough to synthesize a diversity of approaches into a coherent set of principles.  For me, the posters at the end of the workshop were quite helpful in getting approaches to gel.
 
Here are some an</p><p>4 0.11258009 <a title="349-tfidf-4" href="../hunch_net-2008/hunch_net-2008-04-30-Concerns_about_the_Large_Scale_Learning_Challenge.html">300 hunch net-2008-04-30-Concerns about the Large Scale Learning Challenge</a></p>
<p>Introduction: The  large scale learning challenge  for ICML interests me a great deal, although I have concerns about the way it is structured.
 
From the  instructions page , several issues come up:
  
  Large Definition   My personal definition of dataset size is:
 
  small   A dataset is small if a human could look at the dataset and plausibly find a good solution. 
  medium   A dataset is mediumsize if it fits in the RAM of a reasonably priced computer. 
  large  A large dataset does not fit in the RAM of a reasonably priced computer. 
 

By this definition, all of the datasets are medium sized.  This might sound like a pissing match over dataset size, but I believe it is more than that.


The fundamental reason for these definitions is that they correspond to transitions in the sorts of approaches which are feasible.  From small to medium, the ability to use a human as the learning algorithm degrades.  From medium to large, it becomes essential to have learning algorithms that don’t require ran</p><p>5 0.10373066 <a title="349-tfidf-5" href="../hunch_net-2005/hunch_net-2005-02-15-ESPgame_and_image_labeling.html">20 hunch net-2005-02-15-ESPgame and image labeling</a></p>
<p>Introduction: Luis von Ahn  has been running the  espgame  for awhile now.  The espgame provides a picture to two randomly paired people across the web, and asks them to agree on a label.  It hasn’t managed to label the web yet, but it has produced a  large dataset  of (image, label) pairs.  I organized the dataset so you could  explore the implied bipartite graph  (requires much bandwidth).
 
Relative to other image datasets, this one is quite large—67000 images, 358,000 labels (average of 5/image with variation from 1 to 19), and 22,000 unique labels (one every 3 images).  The dataset is also very ‘natural’, consisting of images spidered from the internet.  The multiple label characteristic is intriguing because ‘learning to learn’ and metalearning techniques may be applicable.  The ‘natural’ quality means that this dataset varies greatly in difficulty from easy (predicting “red”) to hard (predicting “funny”) and potentially more rewarding to tackle.
 
The open problem here is, of course, to make</p><p>6 0.1004534 <a title="349-tfidf-6" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>7 0.096523419 <a title="349-tfidf-7" href="../hunch_net-2007/hunch_net-2007-01-26-Parallel_Machine_Learning_Problems.html">229 hunch net-2007-01-26-Parallel Machine Learning Problems</a></p>
<p>8 0.085845955 <a title="349-tfidf-8" href="../hunch_net-2005/hunch_net-2005-02-09-Intuitions_from_applied_learning.html">16 hunch net-2005-02-09-Intuitions from applied learning</a></p>
<p>9 0.085518755 <a title="349-tfidf-9" href="../hunch_net-2011/hunch_net-2011-07-11-Interesting_Neural_Network_Papers_at_ICML_2011.html">438 hunch net-2011-07-11-Interesting Neural Network Papers at ICML 2011</a></p>
<p>10 0.084958151 <a title="349-tfidf-10" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>11 0.08465457 <a title="349-tfidf-11" href="../hunch_net-2007/hunch_net-2007-12-21-Vowpal_Wabbit_Code_Release.html">281 hunch net-2007-12-21-Vowpal Wabbit Code Release</a></p>
<p>12 0.084471256 <a title="349-tfidf-12" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<p>13 0.075616181 <a title="349-tfidf-13" href="../hunch_net-2006/hunch_net-2006-02-27-The_Peekaboom_Dataset.html">159 hunch net-2006-02-27-The Peekaboom Dataset</a></p>
<p>14 0.074403673 <a title="349-tfidf-14" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>15 0.069747992 <a title="349-tfidf-15" href="../hunch_net-2006/hunch_net-2006-01-30-Should_the_Input_Representation_be_a_Vector%3F.html">152 hunch net-2006-01-30-Should the Input Representation be a Vector?</a></p>
<p>16 0.068164662 <a title="349-tfidf-16" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<p>17 0.067520186 <a title="349-tfidf-17" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>18 0.067244872 <a title="349-tfidf-18" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<p>19 0.066943564 <a title="349-tfidf-19" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>20 0.065909527 <a title="349-tfidf-20" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.142), (1, 0.026), (2, -0.081), (3, -0.016), (4, 0.087), (5, 0.038), (6, -0.089), (7, -0.008), (8, 0.0), (9, -0.006), (10, -0.08), (11, -0.028), (12, -0.051), (13, -0.01), (14, -0.07), (15, 0.053), (16, 0.001), (17, 0.055), (18, -0.132), (19, 0.067), (20, -0.011), (21, -0.006), (22, -0.003), (23, 0.047), (24, 0.062), (25, 0.051), (26, -0.051), (27, 0.038), (28, -0.052), (29, -0.008), (30, 0.06), (31, 0.083), (32, 0.014), (33, 0.054), (34, 0.068), (35, -0.057), (36, -0.061), (37, 0.006), (38, -0.019), (39, 0.021), (40, -0.0), (41, -0.061), (42, -0.02), (43, -0.016), (44, 0.13), (45, -0.123), (46, -0.033), (47, -0.008), (48, 0.015), (49, 0.096)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.91958702 <a title="349-lsi-1" href="../hunch_net-2009/hunch_net-2009-04-21-Interesting_Presentations_at_Snowbird.html">349 hunch net-2009-04-21-Interesting Presentations at Snowbird</a></p>
<p>Introduction: Here are a few of presentations interesting me at the  snowbird learning  workshop (which, amusingly, was in Florida with  AIStat ).  
  
  Thomas Breuel  described machine learning problems within OCR and an open source  OCR software/research  platform with modular learning components as well has a 60Million size dataset derived from  Google ‘s scanned books. 
  Kristen Grauman  and  Fei-Fei Li  discussed using active learning with different cost labels and large datasets for  image ontology .  Both of them used  Mechanical Turk  as a  labeling system , which looks to become routine, at least for vision problems. 
  Russ Tedrake  discussed using machine learning for control, with a basic claim that it was the way to go for problems involving a medium  Reynold’s number  such as in bird flight, where simulation is extremely intense. 
  Yann LeCun  presented a poster on an  FPGA for convolutional neural networks  yielding a factor of 100 speedup in processing.   In addition to the graphi</p><p>2 0.6471135 <a title="349-lsi-2" href="../hunch_net-2011/hunch_net-2011-04-18-A_paper_not_at_Snowbird.html">431 hunch net-2011-04-18-A paper not at Snowbird</a></p>
<p>Introduction: Unfortunately, a scheduling failure meant I missed all of  AIStat  and most of the  learning workshop , otherwise known as Snowbird, when it’s at  Snowbird .  
 
At snowbird, the talk on  Sum-Product  networks by  Hoifung Poon  stood out to me ( Pedro Domingos  is a coauthor.).  The basic point was that by appropriately constructing networks based on sums and products, the normalization problem in probabilistic models is eliminated, yielding a highly tractable yet flexible representation+learning algorithm.  As an algorithm, this is noticeably cleaner than deep belief networks with a claim to being an order of magnitude faster and working better on an image completion task.
 
Snowbird doesn’t have real papers—just the abstract above.  I look forward to seeing the paper.  (added: Rodrigo points out the deep learning workshop  draft .)</p><p>3 0.54681516 <a title="349-lsi-3" href="../hunch_net-2011/hunch_net-2011-07-11-Interesting_Neural_Network_Papers_at_ICML_2011.html">438 hunch net-2011-07-11-Interesting Neural Network Papers at ICML 2011</a></p>
<p>Introduction: Maybe it’s too early to call, but with four separate Neural Network sessions at this year’s  ICML ,  it looks like Neural Networks are making a comeback. Here are my  highlights of these sessions. In general, my feeling is that these  papers both demystify deep learning and show its broader applicability.
 
The first observation I made is that the once disreputable “Neural” nomenclature is being used again  in lieu of  “deep learning”. Maybe it’s because Adam Coates et al. showed that single layer networks can work surprisingly well.
  
  An Analysis of Single-Layer Networks in Unsupervised Feature       Learning ,  Adam Coates ,  Honglak Lee ,  Andrew Y. Ng  (AISTATS 2011) 
  The Importance of Encoding Versus Training with Sparse Coding and Vector Quantization ,  Adam Coates ,  Andrew Y. Ng  (ICML 2011) 
  
Another surprising result out of Andrew Ng’s group comes from Andrew  Saxe et al. who show that certain convolutional pooling architectures  can obtain close to state-of-the-art pe</p><p>4 0.51995587 <a title="349-lsi-4" href="../hunch_net-2005/hunch_net-2005-09-20-Workshop_Proposal%3A_Atomic_Learning.html">114 hunch net-2005-09-20-Workshop Proposal: Atomic Learning</a></p>
<p>Introduction: This is a proposal for a workshop.  It may or may not happen depending on the level of interest.  If you are interested, feel free to indicate so (by email or comments).
 
Description: 
Assume(*) that any system for solving large difficult learning problems must decompose into repeated use of basic elements (i.e. atoms).  There are many basic questions which remain:
  
  What are the viable basic elements? 
  What makes a basic element viable? 
  What are the viable principles for the composition of these basic elements? 
  What are the viable principles for learning in such systems? 
  What problems can this approach handle? 
  
Hal Daume adds:
  
 Can composition of atoms be (semi-) automatically constructed[?] 
 When atoms are constructed through reductions, is there some notion of the “naturalness” of the created leaning problems? 
 Other than Markov fields/graphical models/Bayes nets, is there a good language for representing atoms and their compositions? 
  
The answer to these a</p><p>5 0.51317114 <a title="349-lsi-5" href="../hunch_net-2005/hunch_net-2005-02-09-Intuitions_from_applied_learning.html">16 hunch net-2005-02-09-Intuitions from applied learning</a></p>
<p>Introduction: Since learning is far from an exact science, it’s good to pay attention to basic intuitions of applied learning.  Here are a few I’ve collected.
  
  Integration   In Bayesian learning, the posterior is computed by an integral, and the optimal thing to do is to predict according to this integral.  This phenomena seems to be far more general.  Bagging, Boosting, SVMs, and Neural Networks all take advantage of this idea to some extent.  The phenomena is more general: you can average over many different  classification predictors  to improve performance.  Sources:  Zoubin ,  Caruana  
  Differentiation  Different pieces of an average should differentiate to achieve good performance by different methods.  This is know as the ‘symmetry breaking’ problem for neural networks, and it’s why weights are initialized randomly.   Boosting explicitly attempts to achieve good differentiation by creating new, different, learning problems.  Sources:  Yann LeCun ,  Phil Long  
  Deep Representation   Ha</p><p>6 0.50397146 <a title="349-lsi-6" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>7 0.5012548 <a title="349-lsi-7" href="../hunch_net-2005/hunch_net-2005-02-15-ESPgame_and_image_labeling.html">20 hunch net-2005-02-15-ESPgame and image labeling</a></p>
<p>8 0.48785543 <a title="349-lsi-8" href="../hunch_net-2008/hunch_net-2008-04-30-Concerns_about_the_Large_Scale_Learning_Challenge.html">300 hunch net-2008-04-30-Concerns about the Large Scale Learning Challenge</a></p>
<p>9 0.47721463 <a title="349-lsi-9" href="../hunch_net-2010/hunch_net-2010-05-20-Google_Predict.html">399 hunch net-2010-05-20-Google Predict</a></p>
<p>10 0.46976531 <a title="349-lsi-10" href="../hunch_net-2005/hunch_net-2005-04-08-Fast_SVMs.html">54 hunch net-2005-04-08-Fast SVMs</a></p>
<p>11 0.46617684 <a title="349-lsi-11" href="../hunch_net-2007/hunch_net-2007-12-12-Workshop_Summary%26%238212%3BPrinciples_of_Learning_Problem_Design.html">277 hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</a></p>
<p>12 0.45892447 <a title="349-lsi-12" href="../hunch_net-2010/hunch_net-2010-12-02-Traffic_Prediction_Problem.html">418 hunch net-2010-12-02-Traffic Prediction Problem</a></p>
<p>13 0.45303148 <a title="349-lsi-13" href="../hunch_net-2006/hunch_net-2006-01-30-Should_the_Input_Representation_be_a_Vector%3F.html">152 hunch net-2006-01-30-Should the Input Representation be a Vector?</a></p>
<p>14 0.42856735 <a title="349-lsi-14" href="../hunch_net-2007/hunch_net-2007-12-21-Vowpal_Wabbit_Code_Release.html">281 hunch net-2007-12-21-Vowpal Wabbit Code Release</a></p>
<p>15 0.4261665 <a title="349-lsi-15" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>16 0.4051199 <a title="349-lsi-16" href="../hunch_net-2007/hunch_net-2007-01-26-Parallel_Machine_Learning_Problems.html">229 hunch net-2007-01-26-Parallel Machine Learning Problems</a></p>
<p>17 0.40244907 <a title="349-lsi-17" href="../hunch_net-2010/hunch_net-2010-12-26-NIPS_2010.html">420 hunch net-2010-12-26-NIPS 2010</a></p>
<p>18 0.39599317 <a title="349-lsi-18" href="../hunch_net-2006/hunch_net-2006-12-12-Interesting_Papers_at_NIPS_2006.html">224 hunch net-2006-12-12-Interesting Papers at NIPS 2006</a></p>
<p>19 0.39386722 <a title="349-lsi-19" href="../hunch_net-2005/hunch_net-2005-11-05-The_design_of_a_computing_cluster.html">128 hunch net-2005-11-05-The design of a computing cluster</a></p>
<p>20 0.39071909 <a title="349-lsi-20" href="../hunch_net-2009/hunch_net-2009-12-09-Future_Publication_Models_%40_NIPS.html">382 hunch net-2009-12-09-Future Publication Models @ NIPS</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(27, 0.152), (38, 0.044), (53, 0.062), (55, 0.098), (58, 0.441), (94, 0.069), (95, 0.032)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.92673463 <a title="349-lda-1" href="../hunch_net-2012/hunch_net-2012-07-17-MUCMD_and_BayLearn.html">470 hunch net-2012-07-17-MUCMD and BayLearn</a></p>
<p>Introduction: The workshop on the  Meaningful Use of Complex Medical Data  is happening again, August 9-12 in LA, near  UAI  on Catalina Island August 15-17.  I enjoyed my visit last year, and expect this year to be interesting also.
 
The first  Bay Area Machine Learning Symposium  is August 30 at  Google .  Abstracts are due July 30.</p><p>same-blog 2 0.84462488 <a title="349-lda-2" href="../hunch_net-2009/hunch_net-2009-04-21-Interesting_Presentations_at_Snowbird.html">349 hunch net-2009-04-21-Interesting Presentations at Snowbird</a></p>
<p>Introduction: Here are a few of presentations interesting me at the  snowbird learning  workshop (which, amusingly, was in Florida with  AIStat ).  
  
  Thomas Breuel  described machine learning problems within OCR and an open source  OCR software/research  platform with modular learning components as well has a 60Million size dataset derived from  Google ‘s scanned books. 
  Kristen Grauman  and  Fei-Fei Li  discussed using active learning with different cost labels and large datasets for  image ontology .  Both of them used  Mechanical Turk  as a  labeling system , which looks to become routine, at least for vision problems. 
  Russ Tedrake  discussed using machine learning for control, with a basic claim that it was the way to go for problems involving a medium  Reynold’s number  such as in bird flight, where simulation is extremely intense. 
  Yann LeCun  presented a poster on an  FPGA for convolutional neural networks  yielding a factor of 100 speedup in processing.   In addition to the graphi</p><p>3 0.71717811 <a title="349-lda-3" href="../hunch_net-2009/hunch_net-2009-02-16-KDNuggets.html">342 hunch net-2009-02-16-KDNuggets</a></p>
<p>Introduction: Eric Zaetsch points out  KDNuggets  which is a well-developed mailing list/news site with a  KDD  flavor.  This might particularly interest people looking for industrial jobs in machine learning, as the mailing list has many such.</p><p>4 0.64291835 <a title="349-lda-4" href="../hunch_net-2006/hunch_net-2006-01-18-Is_Multitask_Learning_Black-Boxable%3F.html">149 hunch net-2006-01-18-Is Multitask Learning Black-Boxable?</a></p>
<p>Introduction: Multitask learning is the learning to predict multiple outputs given the same input.  Mathematically, we might think of this as trying to learn a function  f:X -> {0,1} n  .  Structured learning is similar at this level of abstraction.  Many people have worked on solving multitask learning (for example  Rich Caruana ) using methods which share an internal representation.  On other words, the the computation and learning  of the  i th prediction is shared with the computation and learning of the  j th prediction.  Another way to ask this question is: can we avoid sharing the internal representation?  
 
For example, it  might  be feasible to solve multitask learning by some process feeding the  i th prediction  f(x) i   into the  j th predictor  f(x,f(x) i ) j  , 
 
If the answer is “no”, then it implies we can not take binary classification as a basic primitive in the process of solving prediction problems.  If the answer is “yes”, then we can reuse binary classification algorithms to</p><p>5 0.42010745 <a title="349-lda-5" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>Introduction: Unfortunately, I ended up sick for much of this ICML.  I did manage to catch one interesting paper:
 
 Richard Socher ,  Cliff Lin ,  Andrew Y. Ng , and  Christopher D. Manning   Parsing Natural Scenes and Natural Language with Recursive Neural Networks .
 
I invited Richard to share his list of interesting papers, so hopefully we’ll hear from him soon.  In the meantime,  Paul  and  Hal  have posted some lists.
 
 the future 
 
 Joelle  and I are program chairs for ICML 2012 in  Edinburgh , which I previously enjoyed visiting in  2005 .  This is a huge responsibility, that we hope to accomplish well.  A part of this (perhaps the most fun part), is imagining how we can make ICML better.  A key and critical constraint is choosing things that can be accomplished.  So far we have:
  
  Colocation . The first thing we looked into was potential colocations.  We quickly discovered that many other conferences precomitted their location.  For the future, getting a colocation with  ACL  or  SIGI</p><p>6 0.41470999 <a title="349-lda-6" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>7 0.4144814 <a title="349-lda-7" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>8 0.41263312 <a title="349-lda-8" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>9 0.41254675 <a title="349-lda-9" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>10 0.41173714 <a title="349-lda-10" href="../hunch_net-2005/hunch_net-2005-03-21-Research_Styles_in_Machine_Learning.html">44 hunch net-2005-03-21-Research Styles in Machine Learning</a></p>
<p>11 0.41151428 <a title="349-lda-11" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>12 0.41145825 <a title="349-lda-12" href="../hunch_net-2007/hunch_net-2007-07-20-Motivation_should_be_the_Responsibility_of_the_Reviewer.html">256 hunch net-2007-07-20-Motivation should be the Responsibility of the Reviewer</a></p>
<p>13 0.41138086 <a title="349-lda-13" href="../hunch_net-2007/hunch_net-2007-01-02-Retrospective.html">225 hunch net-2007-01-02-Retrospective</a></p>
<p>14 0.41092759 <a title="349-lda-14" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<p>15 0.4105978 <a title="349-lda-15" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>16 0.41019905 <a title="349-lda-16" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>17 0.40907809 <a title="349-lda-17" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>18 0.40903291 <a title="349-lda-18" href="../hunch_net-2005/hunch_net-2005-01-19-Why_I_decided_to_run_a_weblog..html">1 hunch net-2005-01-19-Why I decided to run a weblog.</a></p>
<p>19 0.40859565 <a title="349-lda-19" href="../hunch_net-2006/hunch_net-2006-09-12-Incentive_Compatible_Reviewing.html">207 hunch net-2006-09-12-Incentive Compatible Reviewing</a></p>
<p>20 0.40837947 <a title="349-lda-20" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
