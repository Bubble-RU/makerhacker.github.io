<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>339 hunch net-2009-01-27-Key Scientific Challenges</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2009" href="../home/hunch_net-2009_home.html">hunch_net-2009</a> <a title="hunch_net-2009-339" href="#">hunch_net-2009-339</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>339 hunch net-2009-01-27-Key Scientific Challenges</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2009-339-html" href="http://hunch.net/?p=542">html</a></p><p>Introduction: Yahoo released theKey Scientific Challengesprogram. There is aMachine
Learninglist I worked on and aStatisticslist whichDeepakworked on.I'm hoping
this is taken quite seriously by graduate students. The primary value, is that
it gave us a chance to sit down and publicly specify directions of research
which would be valuable to make progress on. A good strategy for a beginning
graduate student is to pick one of these directions, pursue it, and make
substantial advances for a PhD. The directions are sufficiently general that
I'm sure any serious advance has applications well beyond Yahoo.A secondary
point, (which I'm sure is primary for many) is that there is money for
graduate students here. It's unrestricted, so you can use it for any
reasonable travel, supplies, etcâ&euro;Ś</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('directions', 0.383), ('graduate', 0.383), ('primary', 0.22), ('unrestricted', 0.197), ('amachine', 0.183), ('sure', 0.173), ('pursue', 0.173), ('advances', 0.173), ('sit', 0.165), ('hoping', 0.165), ('released', 0.158), ('scientific', 0.152), ('secondary', 0.152), ('publicly', 0.143), ('seriously', 0.143), ('gave', 0.143), ('travel', 0.14), ('strategy', 0.136), ('valuable', 0.133), ('sufficiently', 0.133)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="339-tfidf-1" href="../hunch_net-2009/hunch_net-2009-01-27-Key_Scientific_Challenges.html">339 hunch net-2009-01-27-Key Scientific Challenges</a></p>
<p>Introduction: Yahoo released theKey Scientific Challengesprogram. There is aMachine
Learninglist I worked on and aStatisticslist whichDeepakworked on.I'm hoping
this is taken quite seriously by graduate students. The primary value, is that
it gave us a chance to sit down and publicly specify directions of research
which would be valuable to make progress on. A good strategy for a beginning
graduate student is to pick one of these directions, pursue it, and make
substantial advances for a PhD. The directions are sufficiently general that
I'm sure any serious advance has applications well beyond Yahoo.A secondary
point, (which I'm sure is primary for many) is that there is money for
graduate students here. It's unrestricted, so you can use it for any
reasonable travel, supplies, etcâ&euro;Ś</p><p>2 0.24418738 <a title="339-tfidf-2" href="../hunch_net-2011/hunch_net-2011-02-25-Yahoo%21_Machine_Learning_grant_due_March_11.html">425 hunch net-2011-02-25-Yahoo! Machine Learning grant due March 11</a></p>
<p>Introduction: Yahoo!'sKey Scientific ChallengesforMachine Learninggrant applications are due
March 11. If you are a student working on relevant research, please consider
applying. It's for $5K of unrestricted funding.</p><p>3 0.1476898 <a title="339-tfidf-3" href="../hunch_net-2005/hunch_net-2005-09-10-%26%238220%3BFailure%26%238221%3B_is_an_option.html">110 hunch net-2005-09-10-&#8220;Failure&#8221; is an option</a></p>
<p>Introduction: This is about the hard choices that graduate students must make.The cultural
definition of success in academic research is to:Produce good research which
many other people appreciate.Produce many students who go on to do the
same.There are fundamental reasons why this is success in the local culture.
Good research appreciated by others means access to jobs. Many students
succesful in the same way implies that there are a number of people who think
in a similar way and appreciate your work.In order to graduate, a phd student
must live in an academic culture for a period of several years. It is common
to adopt the culture's definition of success during this time. It's also
common for many phd students discover they are not suited to an academic
research lifestyle. This collision of values and abilities naturally results
in depression.The most fundamental advice when this happens is: change
something. Pick a new advisor. Pick a new research topic. Or leave the program
(and do something el</p><p>4 0.11621143 <a title="339-tfidf-4" href="../hunch_net-2012/hunch_net-2012-02-29-Key_Scientific_Challenges_and_the_Franklin_Symposium.html">457 hunch net-2012-02-29-Key Scientific Challenges and the Franklin Symposium</a></p>
<p>Introduction: For graduate students, theYahoo!Key Scientific Challenges programincluding
inmachine learningis on again,due March 9. The application is easy and the $5K
award is high quality "no strings attached" funding. Consider submitting.Those
in Washington DC, Philadelphia, and New York, may consider attending
theFranklin Institute SymposiumApril 25which has several speakers and an award
forV. Attendance is free with an RSVP.</p><p>5 0.099421307 <a title="339-tfidf-5" href="../hunch_net-2010/hunch_net-2010-02-26-Yahoo%21_ML_events.html">389 hunch net-2010-02-26-Yahoo! ML events</a></p>
<p>Introduction: Yahoo! is sponsoring two machine learning events that might interest
people.TheKey Scientific Challengesprogram (due March 5) forMachine
LearningandStatisticsoffers $5K (plus bonuses) for graduate students working
on a core problem of interest to Y! If you are already working on one of these
problems, there is no reason not to submit, and if you aren't you might want
to think about it for next year, as I am confident they all press the boundary
of the possible in Machine Learning. There are 7 days left.TheLearning to Rank
challenge(due May 31) offers an $8K first prize for the best ranking algorithm
on a real (and really used) dataset for search ranking, with presentations at
an ICML workshop. Unlike the Netflix competition, there are prizes for 2nd,
3rd, and 4th place, perhaps avoiding the heartbreakthe ensembleencountered. If
you think you know how to rank, you should give it a try, and we might all
learn something. There are 3 months left.</p><p>6 0.097862929 <a title="339-tfidf-6" href="../hunch_net-2006/hunch_net-2006-04-30-John_Langford_%26%238211%3B%3E_Yahoo_Research%2C_NY.html">175 hunch net-2006-04-30-John Langford &#8211;> Yahoo Research, NY</a></p>
<p>7 0.079919294 <a title="339-tfidf-7" href="../hunch_net-2006/hunch_net-2006-02-11-Yahoo%26%238217%3Bs_Learning_Problems..html">156 hunch net-2006-02-11-Yahoo&#8217;s Learning Problems.</a></p>
<p>8 0.077487916 <a title="339-tfidf-8" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>9 0.077189736 <a title="339-tfidf-9" href="../hunch_net-2011/hunch_net-2011-09-28-Somebody%26%238217%3Bs_Eating_Your_Lunch.html">445 hunch net-2011-09-28-Somebody&#8217;s Eating Your Lunch</a></p>
<p>10 0.07702592 <a title="339-tfidf-10" href="../hunch_net-2008/hunch_net-2008-04-12-It_Doesn%26%238217%3Bt_Stop.html">295 hunch net-2008-04-12-It Doesn&#8217;t Stop</a></p>
<p>11 0.076111853 <a title="339-tfidf-11" href="../hunch_net-2008/hunch_net-2008-11-28-A_Bumper_Crop_of_Machine_Learning_Graduates.html">329 hunch net-2008-11-28-A Bumper Crop of Machine Learning Graduates</a></p>
<p>12 0.07364469 <a title="339-tfidf-12" href="../hunch_net-2005/hunch_net-2005-09-26-Prediction_Bounds_as_the_Mathematics_of_Science.html">115 hunch net-2005-09-26-Prediction Bounds as the Mathematics of Science</a></p>
<p>13 0.072278924 <a title="339-tfidf-13" href="../hunch_net-2011/hunch_net-2011-11-26-Giving_Thanks.html">449 hunch net-2011-11-26-Giving Thanks</a></p>
<p>14 0.069757648 <a title="339-tfidf-14" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>15 0.069433995 <a title="339-tfidf-15" href="../hunch_net-2005/hunch_net-2005-05-17-A_Short_Guide_to_PhD_Graduate_Study.html">73 hunch net-2005-05-17-A Short Guide to PhD Graduate Study</a></p>
<p>16 0.069115885 <a title="339-tfidf-16" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>17 0.067359224 <a title="339-tfidf-17" href="../hunch_net-2010/hunch_net-2010-05-02-What%26%238217%3Bs_the_difference_between_gambling_and_rewarding_good_prediction%3F.html">397 hunch net-2010-05-02-What&#8217;s the difference between gambling and rewarding good prediction?</a></p>
<p>18 0.065969713 <a title="339-tfidf-18" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>19 0.065847553 <a title="339-tfidf-19" href="../hunch_net-2005/hunch_net-2005-10-12-The_unrealized_potential_of_the_research_lab.html">121 hunch net-2005-10-12-The unrealized potential of the research lab</a></p>
<p>20 0.06312871 <a title="339-tfidf-20" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.114), (1, 0.04), (2, 0.113), (3, -0.043), (4, 0.071), (5, -0.008), (6, 0.049), (7, 0.045), (8, 0.034), (9, -0.107), (10, 0.025), (11, 0.012), (12, -0.013), (13, 0.101), (14, -0.058), (15, -0.049), (16, -0.01), (17, -0.023), (18, 0.029), (19, 0.054), (20, -0.016), (21, 0.051), (22, -0.003), (23, 0.056), (24, 0.092), (25, -0.069), (26, 0.058), (27, -0.001), (28, 0.133), (29, -0.037), (30, 0.024), (31, -0.076), (32, -0.079), (33, 0.055), (34, -0.008), (35, 0.077), (36, 0.02), (37, -0.044), (38, 0.016), (39, 0.052), (40, 0.127), (41, -0.017), (42, -0.012), (43, 0.06), (44, -0.121), (45, -0.012), (46, -0.116), (47, 0.009), (48, -0.017), (49, 0.042)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98155254 <a title="339-lsi-1" href="../hunch_net-2009/hunch_net-2009-01-27-Key_Scientific_Challenges.html">339 hunch net-2009-01-27-Key Scientific Challenges</a></p>
<p>Introduction: Yahoo released theKey Scientific Challengesprogram. There is aMachine
Learninglist I worked on and aStatisticslist whichDeepakworked on.I'm hoping
this is taken quite seriously by graduate students. The primary value, is that
it gave us a chance to sit down and publicly specify directions of research
which would be valuable to make progress on. A good strategy for a beginning
graduate student is to pick one of these directions, pursue it, and make
substantial advances for a PhD. The directions are sufficiently general that
I'm sure any serious advance has applications well beyond Yahoo.A secondary
point, (which I'm sure is primary for many) is that there is money for
graduate students here. It's unrestricted, so you can use it for any
reasonable travel, supplies, etcâ&euro;Ś</p><p>2 0.68177003 <a title="339-lsi-2" href="../hunch_net-2011/hunch_net-2011-02-25-Yahoo%21_Machine_Learning_grant_due_March_11.html">425 hunch net-2011-02-25-Yahoo! Machine Learning grant due March 11</a></p>
<p>Introduction: Yahoo!'sKey Scientific ChallengesforMachine Learninggrant applications are due
March 11. If you are a student working on relevant research, please consider
applying. It's for $5K of unrestricted funding.</p><p>3 0.63827044 <a title="339-lsi-3" href="../hunch_net-2005/hunch_net-2005-09-10-%26%238220%3BFailure%26%238221%3B_is_an_option.html">110 hunch net-2005-09-10-&#8220;Failure&#8221; is an option</a></p>
<p>Introduction: This is about the hard choices that graduate students must make.The cultural
definition of success in academic research is to:Produce good research which
many other people appreciate.Produce many students who go on to do the
same.There are fundamental reasons why this is success in the local culture.
Good research appreciated by others means access to jobs. Many students
succesful in the same way implies that there are a number of people who think
in a similar way and appreciate your work.In order to graduate, a phd student
must live in an academic culture for a period of several years. It is common
to adopt the culture's definition of success during this time. It's also
common for many phd students discover they are not suited to an academic
research lifestyle. This collision of values and abilities naturally results
in depression.The most fundamental advice when this happens is: change
something. Pick a new advisor. Pick a new research topic. Or leave the program
(and do something el</p><p>4 0.5922702 <a title="339-lsi-4" href="../hunch_net-2005/hunch_net-2005-05-17-A_Short_Guide_to_PhD_Graduate_Study.html">73 hunch net-2005-05-17-A Short Guide to PhD Graduate Study</a></p>
<p>Introduction: Graduate study is a mysterious and uncertain process. This easiest way to see
this is by noting that a very old advisor/student mechanism is preferred.
There is no known succesful mechanism for "mass producing" PhDs as is done (in
some sense) for undergraduate and masters study. Here are a few hints that
might be useful to prospective or current students based on my own
experience.Masters or PhD(a) You want a PhD if you want to do research. (b)
You want a masters if you want to make money. People wanting (b) will be
manifestly unhappy with (a) because it typically means years of low pay.
People wanting (a) should try to avoid (b) because it prolongs an already long
process.Attitude.Manystudents struggle for awhile with the wrong attitude
towards research. Most students come into graduate school with 16-19 years of
schooling where the principle means of success is proving that you know
something via assignments, tests, etcâ&euro;Ś Research doesnotwork this way. Research
is what a PhD is about.</p><p>5 0.57708776 <a title="339-lsi-5" href="../hunch_net-2008/hunch_net-2008-02-27-The_Stats_Handicap.html">290 hunch net-2008-02-27-The Stats Handicap</a></p>
<p>Introduction: Graduating students in Statistics appear to be at a substantial handicap
compared to graduating students in Machine Learning, despite being in
substantially overlapping subjects.The problem seems to be cultural.
Statistics comes from a mathematics background which emphasizes large
publications slowly published under review at journals. Machine Learning comes
from a Computer Science background which emphasizes quick publishing at
reviewed conferences. This has a number of implications:Graduating statistics
PhDs often have 0-2 publications while graduating machine learning PhDs might
have 5-15.Graduating ML students have had a chance for others to build on
their work. Stats students have had no such chance.Graduating ML students have
attended a number of conferences and presented their work, giving them a
chance to meet people. Stats students have had fewer chances of this sort.In
short, Stats students have had relatively few chances to distinguish
themselves and are heavily reliant on t</p><p>6 0.55915469 <a title="339-lsi-6" href="../hunch_net-2010/hunch_net-2010-02-26-Yahoo%21_ML_events.html">389 hunch net-2010-02-26-Yahoo! ML events</a></p>
<p>7 0.54968709 <a title="339-lsi-7" href="../hunch_net-2012/hunch_net-2012-02-29-Key_Scientific_Challenges_and_the_Franklin_Symposium.html">457 hunch net-2012-02-29-Key Scientific Challenges and the Franklin Symposium</a></p>
<p>8 0.4895632 <a title="339-lsi-8" href="../hunch_net-2011/hunch_net-2011-09-28-Somebody%26%238217%3Bs_Eating_Your_Lunch.html">445 hunch net-2011-09-28-Somebody&#8217;s Eating Your Lunch</a></p>
<p>9 0.44614911 <a title="339-lsi-9" href="../hunch_net-2011/hunch_net-2011-11-26-Giving_Thanks.html">449 hunch net-2011-11-26-Giving Thanks</a></p>
<p>10 0.44559902 <a title="339-lsi-10" href="../hunch_net-2006/hunch_net-2006-04-30-John_Langford_%26%238211%3B%3E_Yahoo_Research%2C_NY.html">175 hunch net-2006-04-30-John Langford &#8211;> Yahoo Research, NY</a></p>
<p>11 0.42979532 <a title="339-lsi-11" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>12 0.41727418 <a title="339-lsi-12" href="../hunch_net-2006/hunch_net-2006-02-11-Yahoo%26%238217%3Bs_Learning_Problems..html">156 hunch net-2006-02-11-Yahoo&#8217;s Learning Problems.</a></p>
<p>13 0.37475926 <a title="339-lsi-13" href="../hunch_net-2006/hunch_net-2006-07-12-Who_is_having_visa_problems_reaching_US_conferences%3F.html">195 hunch net-2006-07-12-Who is having visa problems reaching US conferences?</a></p>
<p>14 0.36720091 <a title="339-lsi-14" href="../hunch_net-2010/hunch_net-2010-08-21-Rob_Schapire_at_NYC_ML_Meetup.html">405 hunch net-2010-08-21-Rob Schapire at NYC ML Meetup</a></p>
<p>15 0.35550141 <a title="339-lsi-15" href="../hunch_net-2005/hunch_net-2005-12-22-Yes_%2C_I_am_applying.html">142 hunch net-2005-12-22-Yes , I am applying</a></p>
<p>16 0.35367861 <a title="339-lsi-16" href="../hunch_net-2007/hunch_net-2007-07-28-Asking_questions.html">257 hunch net-2007-07-28-Asking questions</a></p>
<p>17 0.35066551 <a title="339-lsi-17" href="../hunch_net-2014/hunch_net-2014-02-16-Metacademy%3A_a_package_manager_for_knowledge.html">493 hunch net-2014-02-16-Metacademy: a package manager for knowledge</a></p>
<p>18 0.34939846 <a title="339-lsi-18" href="../hunch_net-2005/hunch_net-2005-05-03-Conference_attendance_is_mandatory.html">66 hunch net-2005-05-03-Conference attendance is mandatory</a></p>
<p>19 0.34754997 <a title="339-lsi-19" href="../hunch_net-2013/hunch_net-2013-01-07-NYU_Large_Scale_Machine_Learning_Class.html">478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</a></p>
<p>20 0.34217304 <a title="339-lsi-20" href="../hunch_net-2010/hunch_net-2010-10-17-Partha_Niyogi_has_died.html">414 hunch net-2010-10-17-Partha Niyogi has died</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.08), (42, 0.254), (68, 0.073), (72, 0.335), (74, 0.126)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.92800605 <a title="339-lda-1" href="../hunch_net-2009/hunch_net-2009-01-27-Key_Scientific_Challenges.html">339 hunch net-2009-01-27-Key Scientific Challenges</a></p>
<p>Introduction: Yahoo released theKey Scientific Challengesprogram. There is aMachine
Learninglist I worked on and aStatisticslist whichDeepakworked on.I'm hoping
this is taken quite seriously by graduate students. The primary value, is that
it gave us a chance to sit down and publicly specify directions of research
which would be valuable to make progress on. A good strategy for a beginning
graduate student is to pick one of these directions, pursue it, and make
substantial advances for a PhD. The directions are sufficiently general that
I'm sure any serious advance has applications well beyond Yahoo.A secondary
point, (which I'm sure is primary for many) is that there is money for
graduate students here. It's unrestricted, so you can use it for any
reasonable travel, supplies, etcâ&euro;Ś</p><p>2 0.76539057 <a title="339-lda-2" href="../hunch_net-2007/hunch_net-2007-12-12-Workshop_Summary%26%238212%3BPrinciples_of_Learning_Problem_Design.html">277 hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</a></p>
<p>Introduction: This is a summary of theworkshop on Learning Problem DesignwhichAlinaand I ran
atNIPSthis year.The first question many people have is "What is learning
problem design?" This workshop is about admitting that solving learning
problems does not start with labeled data, but rather somewhere before. When
humans are hired to produce labels, this is usually not a serious problem
because you can tell them precisely what semantics you want the labels to
have, and we can fix some set of features in advance. However, when other
methods are used this becomes more problematic. This focus is important for
Machine Learning because there are very large quantities of data which are not
labeled by a hired human.The title of the workshop was a bit ambitious,
because a workshop is not long enough to synthesize a diversity of approaches
into a coherent set of principles. For me, the posters at the end of the
workshop were quite helpful in getting approaches to gel.Here are some answers
to "where do the lab</p><p>3 0.75124294 <a title="339-lda-3" href="../hunch_net-2005/hunch_net-2005-06-06-Exact_Online_Learning_for_Classification.html">78 hunch net-2005-06-06-Exact Online Learning for Classification</a></p>
<p>Introduction: Jacob Abernethy and I have found a computationally tractable method for
computing an optimal (or near optimal depending on setting) master algorithm
combining expert predictions addressingthis open problem. A draft ishere.The
effect of this improvement seems to be about a factor of2decrease in the
regret (= error rate minus best possible error rate) for the low error rate
situation. (At large error rates, there may be no significant
difference.)There are some unfinished details still to consider:When we remove
all of the approximation slack from online learning, is the result a
satisfying learning algorithm, in practice? I consider online learning is one
of the more compelling methods of analyzing and deriving algorithms, but that
expectation must be either met or not by this algorithmSome extra details: The
algorithm is optimal given a small amount of side information (kin the draft).
What is the best way to remove this side information? The removal is necessary
for a practical algori</p><p>4 0.64040232 <a title="339-lda-4" href="../hunch_net-2006/hunch_net-2006-07-26-Two_more_UAI_papers_of_interest.html">199 hunch net-2006-07-26-Two more UAI papers of interest</a></p>
<p>Introduction: In addition to Ed Snelson's paper, there were (at least) two other papers that
caught my eye at UAI.One wasthis paperby Sanjoy Dasgupta, Daniel Hsu and Nakul
Verma at UCSD which shows in a surprisingly general and strong way that almost
all linear projections of any jointly distributed vector random variable with
finite first and second moments look sphereical and unimodal (in fact look
like a scale mixture of Gaussians). Great result, as you'd expect from
Sanjoy.The other paper which I found intriguing but which I just haven't
groked yet isthis beastby Manfred and Dima Kuzmin.You can check out the
(beautiful)slidesif that helps. I feel like there is something deep here, but
my brain is too small to understand it. The COLT and last NIPS papers/slides
are also on Manfred's page. Hopefully someone here can illuminate.</p><p>5 0.63444561 <a title="339-lda-5" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>Introduction: I wanted to expand on thispostand some of the previousproblems/research
directionsabout where learning theory might make large strides.Why theory?The
essential reason for theory is "intuition extension". A very good applied
learning person can master some particular application domain yielding the
best computer algorithms for solving that problem. A very good theory can take
the intuitions discovered by this and other applied learning people and extend
them to new domains in a relatively automatic fashion. To do this, we take
these basic intuitions and try to find a mathematical model that:Explains the
basic intuitions.Makes new testable predictions about how to learn.Succeeds in
so learning.This is "intuition extension": taking what we have learned
somewhere else and applying it in new domains. It is fundamentally useful to
everyone because it increases the level of automation in solving
problems.Where next for learning theory?I like the analogy with physics. Back
before we-the-humans</p><p>6 0.63389611 <a title="339-lda-6" href="../hunch_net-2008/hunch_net-2008-05-23-Three_levels_of_addressing_the_Netflix_Prize.html">301 hunch net-2008-05-23-Three levels of addressing the Netflix Prize</a></p>
<p>7 0.63295925 <a title="339-lda-7" href="../hunch_net-2005/hunch_net-2005-09-10-%26%238220%3BFailure%26%238221%3B_is_an_option.html">110 hunch net-2005-09-10-&#8220;Failure&#8221; is an option</a></p>
<p>8 0.63131571 <a title="339-lda-8" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>9 0.63007021 <a title="339-lda-9" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>10 0.62904638 <a title="339-lda-10" href="../hunch_net-2005/hunch_net-2005-06-08-Question%3A_%26%238220%3BWhen_is_the_right_time_to_insert_the_loss_function%3F%26%238221%3B.html">79 hunch net-2005-06-08-Question: &#8220;When is the right time to insert the loss function?&#8221;</a></p>
<p>11 0.62890643 <a title="339-lda-11" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>12 0.62597048 <a title="339-lda-12" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>13 0.62363714 <a title="339-lda-13" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>14 0.62304342 <a title="339-lda-14" href="../hunch_net-2005/hunch_net-2005-01-24-The_Humanloop_Spectrum_of_Machine_Learning.html">3 hunch net-2005-01-24-The Humanloop Spectrum of Machine Learning</a></p>
<p>15 0.62125582 <a title="339-lda-15" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>16 0.620884 <a title="339-lda-16" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>17 0.61859524 <a title="339-lda-17" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>18 0.6179781 <a title="339-lda-18" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>19 0.61784905 <a title="339-lda-19" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>20 0.61744428 <a title="339-lda-20" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
