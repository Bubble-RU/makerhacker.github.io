<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>339 hunch net-2009-01-27-Key Scientific Challenges</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2009" href="../home/hunch_net-2009_home.html">hunch_net-2009</a> <a title="hunch_net-2009-339" href="#">hunch_net-2009-339</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>339 hunch net-2009-01-27-Key Scientific Challenges</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2009-339-html" href="http://hunch.net/?p=542">html</a></p><p>Introduction: Yahoo released the  Key Scientific Challenges  program.  There is a  Machine Learning  list I worked on and a  Statistics  list which  Deepak  worked on.
 
I’m hoping this is taken quite seriously by graduate students.  The primary value, is that it gave us a chance to sit down and publicly specify directions of research which would be valuable to make progress on.  A good strategy for a beginning graduate student is to pick one of these directions, pursue it, and make substantial advances for a PhD.  The directions are sufficiently general that I’m sure any serious advance has applications well beyond Yahoo.
 
A secondary point, (which I’m sure is primary for many    ) is that there is money for graduate students here.  It’s unrestricted, so you can use it for any reasonable travel, supplies, etc…</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Yahoo released the  Key Scientific Challenges  program. [sent-1, score-0.129]
</p><p>2 There is a  Machine Learning  list I worked on and a  Statistics  list which  Deepak  worked on. [sent-2, score-0.706]
</p><p>3 I’m hoping this is taken quite seriously by graduate students. [sent-3, score-0.826]
</p><p>4 The primary value, is that it gave us a chance to sit down and publicly specify directions of research which would be valuable to make progress on. [sent-4, score-1.649]
</p><p>5 A good strategy for a beginning graduate student is to pick one of these directions, pursue it, and make substantial advances for a PhD. [sent-5, score-1.336]
</p><p>6 The directions are sufficiently general that I’m sure any serious advance has applications well beyond Yahoo. [sent-6, score-1.105]
</p><p>7 A secondary point, (which I’m sure is primary for many    ) is that there is money for graduate students here. [sent-7, score-1.117]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('graduate', 0.372), ('directions', 0.351), ('primary', 0.213), ('unrestricted', 0.192), ('worked', 0.186), ('deepak', 0.177), ('sure', 0.168), ('pursue', 0.167), ('advances', 0.167), ('list', 0.167), ('sit', 0.16), ('hoping', 0.16), ('secondary', 0.148), ('publicly', 0.139), ('scientific', 0.139), ('seriously', 0.139), ('travel', 0.136), ('challenges', 0.132), ('released', 0.129), ('valuable', 0.129), ('sufficiently', 0.129), ('strategy', 0.129), ('beginning', 0.126), ('advance', 0.124), ('gave', 0.122), ('money', 0.117), ('beyond', 0.113), ('specify', 0.113), ('pick', 0.113), ('yahoo', 0.111), ('student', 0.11), ('taken', 0.102), ('statistics', 0.102), ('key', 0.099), ('students', 0.099), ('progress', 0.096), ('serious', 0.094), ('chance', 0.089), ('make', 0.085), ('etc', 0.08), ('applications', 0.075), ('value', 0.068), ('substantial', 0.067), ('reasonable', 0.063), ('us', 0.062), ('point', 0.057), ('quite', 0.053), ('general', 0.051), ('would', 0.048), ('research', 0.042)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="339-tfidf-1" href="../hunch_net-2009/hunch_net-2009-01-27-Key_Scientific_Challenges.html">339 hunch net-2009-01-27-Key Scientific Challenges</a></p>
<p>Introduction: Yahoo released the  Key Scientific Challenges  program.  There is a  Machine Learning  list I worked on and a  Statistics  list which  Deepak  worked on.
 
I’m hoping this is taken quite seriously by graduate students.  The primary value, is that it gave us a chance to sit down and publicly specify directions of research which would be valuable to make progress on.  A good strategy for a beginning graduate student is to pick one of these directions, pursue it, and make substantial advances for a PhD.  The directions are sufficiently general that I’m sure any serious advance has applications well beyond Yahoo.
 
A secondary point, (which I’m sure is primary for many    ) is that there is money for graduate students here.  It’s unrestricted, so you can use it for any reasonable travel, supplies, etc…</p><p>2 0.26499856 <a title="339-tfidf-2" href="../hunch_net-2011/hunch_net-2011-02-25-Yahoo%21_Machine_Learning_grant_due_March_11.html">425 hunch net-2011-02-25-Yahoo! Machine Learning grant due March 11</a></p>
<p>Introduction: Yahoo!’s  Key Scientific Challenges  for  Machine Learning  grant applications are due March 11.  If you are a student working on relevant research, please consider applying.  It’s for $5K of unrestricted funding.</p><p>3 0.16170736 <a title="339-tfidf-3" href="../hunch_net-2012/hunch_net-2012-02-29-Key_Scientific_Challenges_and_the_Franklin_Symposium.html">457 hunch net-2012-02-29-Key Scientific Challenges and the Franklin Symposium</a></p>
<p>Introduction: For graduate students, the  Yahoo!   Key Scientific Challenges program  including in  machine learning  is on again,  due March 9 .  The application is easy and the $5K award is high quality “no strings attached” funding.   Consider submitting.
 
Those in Washington DC, Philadelphia, and New York, may consider attending the  Franklin Institute Symposium   April 25  which has several speakers and an award for  V .  Attendance is free with an RSVP.</p><p>4 0.14931008 <a title="339-tfidf-4" href="../hunch_net-2005/hunch_net-2005-09-10-%26%238220%3BFailure%26%238221%3B_is_an_option.html">110 hunch net-2005-09-10-&#8220;Failure&#8221; is an option</a></p>
<p>Introduction: This is about the hard choices that graduate students must make.
 
The cultural definition of success in academic research is to:
  
 Produce good research which many other people appreciate. 
 Produce many students who go on to do the same. 
  
There are fundamental reasons why this is success in the local culture.   Good research appreciated by others means access to jobs.  Many students succesful in the same way implies that there are a number of people who think in a similar way and appreciate your work.
 
In order to graduate, a phd student must live in an academic culture for a period of several years. It is common to adopt the culture’s definition of success during this time.  It’s also common for many phd students discover they are not suited to an academic research lifestyle.  This collision of values and abilities naturally results in depression.
 
The most fundamental advice when this happens is: change something.  Pick a new advisor.  Pick a new research topic.  Or leave th</p><p>5 0.12902462 <a title="339-tfidf-5" href="../hunch_net-2010/hunch_net-2010-02-26-Yahoo%21_ML_events.html">389 hunch net-2010-02-26-Yahoo! ML events</a></p>
<p>Introduction: Yahoo! is sponsoring two machine learning events that might interest people.  
  
 The  Key Scientific Challenges  program (due March 5) for  Machine Learning  and  Statistics  offers $5K (plus bonuses) for graduate students working on a core problem of interest to Y!  If you are already working on one of these problems, there is no reason not to submit, and if you arenâ&euro;&trade;t you might want to think about it for next year, as I am confident they all press the boundary of the possible in Machine Learning.   There are 7 days left. 
 The  Learning to Rank challenge  (due May 31) offers an $8K first prize for the best ranking algorithm on a real (and really used) dataset for search ranking, with presentations at an ICML workshop.  Unlike the Netflix competition, there are prizes for 2nd, 3rd, and 4th place, perhaps avoiding the heartbreak  the ensemble  encountered.  If you think you know how to rank, you should give it a try, and we might all learn something.  There are 3 months left.</p><p>6 0.10688657 <a title="339-tfidf-6" href="../hunch_net-2006/hunch_net-2006-04-30-John_Langford_%26%238211%3B%3E_Yahoo_Research%2C_NY.html">175 hunch net-2006-04-30-John Langford &#8211;> Yahoo Research, NY</a></p>
<p>7 0.089244731 <a title="339-tfidf-7" href="../hunch_net-2011/hunch_net-2011-11-26-Giving_Thanks.html">449 hunch net-2011-11-26-Giving Thanks</a></p>
<p>8 0.080932632 <a title="339-tfidf-8" href="../hunch_net-2006/hunch_net-2006-02-11-Yahoo%26%238217%3Bs_Learning_Problems..html">156 hunch net-2006-02-11-Yahoo&#8217;s Learning Problems.</a></p>
<p>9 0.080469191 <a title="339-tfidf-9" href="../hunch_net-2005/hunch_net-2005-09-26-Prediction_Bounds_as_the_Mathematics_of_Science.html">115 hunch net-2005-09-26-Prediction Bounds as the Mathematics of Science</a></p>
<p>10 0.078570232 <a title="339-tfidf-10" href="../hunch_net-2008/hunch_net-2008-11-28-A_Bumper_Crop_of_Machine_Learning_Graduates.html">329 hunch net-2008-11-28-A Bumper Crop of Machine Learning Graduates</a></p>
<p>11 0.07653743 <a title="339-tfidf-11" href="../hunch_net-2011/hunch_net-2011-09-28-Somebody%26%238217%3Bs_Eating_Your_Lunch.html">445 hunch net-2011-09-28-Somebody&#8217;s Eating Your Lunch</a></p>
<p>12 0.074802019 <a title="339-tfidf-12" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>13 0.072607286 <a title="339-tfidf-13" href="../hunch_net-2008/hunch_net-2008-04-12-It_Doesn%26%238217%3Bt_Stop.html">295 hunch net-2008-04-12-It Doesn&#8217;t Stop</a></p>
<p>14 0.072573528 <a title="339-tfidf-14" href="../hunch_net-2010/hunch_net-2010-10-17-Partha_Niyogi_has_died.html">414 hunch net-2010-10-17-Partha Niyogi has died</a></p>
<p>15 0.070193738 <a title="339-tfidf-15" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>16 0.0669083 <a title="339-tfidf-16" href="../hunch_net-2005/hunch_net-2005-05-17-A_Short_Guide_to_PhD_Graduate_Study.html">73 hunch net-2005-05-17-A Short Guide to PhD Graduate Study</a></p>
<p>17 0.064604327 <a title="339-tfidf-17" href="../hunch_net-2008/hunch_net-2008-02-27-The_Stats_Handicap.html">290 hunch net-2008-02-27-The Stats Handicap</a></p>
<p>18 0.063372433 <a title="339-tfidf-18" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>19 0.060369819 <a title="339-tfidf-19" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>20 0.060349677 <a title="339-tfidf-20" href="../hunch_net-2010/hunch_net-2010-05-02-What%26%238217%3Bs_the_difference_between_gambling_and_rewarding_good_prediction%3F.html">397 hunch net-2010-05-02-What&#8217;s the difference between gambling and rewarding good prediction?</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.125), (1, -0.035), (2, -0.11), (3, 0.071), (4, -0.087), (5, -0.039), (6, -0.006), (7, 0.047), (8, -0.1), (9, -0.029), (10, 0.083), (11, 0.047), (12, 0.007), (13, -0.01), (14, 0.02), (15, 0.087), (16, -0.07), (17, -0.002), (18, -0.035), (19, -0.109), (20, 0.096), (21, 0.062), (22, 0.078), (23, -0.098), (24, 0.104), (25, 0.014), (26, 0.062), (27, -0.124), (28, 0.046), (29, 0.075), (30, -0.045), (31, 0.043), (32, -0.033), (33, 0.005), (34, 0.046), (35, 0.081), (36, -0.099), (37, 0.012), (38, -0.027), (39, 0.056), (40, -0.101), (41, 0.018), (42, 0.136), (43, -0.02), (44, -0.015), (45, 0.081), (46, -0.027), (47, -0.016), (48, -0.047), (49, -0.019)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97863936 <a title="339-lsi-1" href="../hunch_net-2009/hunch_net-2009-01-27-Key_Scientific_Challenges.html">339 hunch net-2009-01-27-Key Scientific Challenges</a></p>
<p>Introduction: Yahoo released the  Key Scientific Challenges  program.  There is a  Machine Learning  list I worked on and a  Statistics  list which  Deepak  worked on.
 
I’m hoping this is taken quite seriously by graduate students.  The primary value, is that it gave us a chance to sit down and publicly specify directions of research which would be valuable to make progress on.  A good strategy for a beginning graduate student is to pick one of these directions, pursue it, and make substantial advances for a PhD.  The directions are sufficiently general that I’m sure any serious advance has applications well beyond Yahoo.
 
A secondary point, (which I’m sure is primary for many    ) is that there is money for graduate students here.  It’s unrestricted, so you can use it for any reasonable travel, supplies, etc…</p><p>2 0.79307812 <a title="339-lsi-2" href="../hunch_net-2011/hunch_net-2011-02-25-Yahoo%21_Machine_Learning_grant_due_March_11.html">425 hunch net-2011-02-25-Yahoo! Machine Learning grant due March 11</a></p>
<p>Introduction: Yahoo!’s  Key Scientific Challenges  for  Machine Learning  grant applications are due March 11.  If you are a student working on relevant research, please consider applying.  It’s for $5K of unrestricted funding.</p><p>3 0.75318897 <a title="339-lsi-3" href="../hunch_net-2012/hunch_net-2012-02-29-Key_Scientific_Challenges_and_the_Franklin_Symposium.html">457 hunch net-2012-02-29-Key Scientific Challenges and the Franklin Symposium</a></p>
<p>Introduction: For graduate students, the  Yahoo!   Key Scientific Challenges program  including in  machine learning  is on again,  due March 9 .  The application is easy and the $5K award is high quality “no strings attached” funding.   Consider submitting.
 
Those in Washington DC, Philadelphia, and New York, may consider attending the  Franklin Institute Symposium   April 25  which has several speakers and an award for  V .  Attendance is free with an RSVP.</p><p>4 0.63138568 <a title="339-lsi-4" href="../hunch_net-2010/hunch_net-2010-02-26-Yahoo%21_ML_events.html">389 hunch net-2010-02-26-Yahoo! ML events</a></p>
<p>Introduction: Yahoo! is sponsoring two machine learning events that might interest people.  
  
 The  Key Scientific Challenges  program (due March 5) for  Machine Learning  and  Statistics  offers $5K (plus bonuses) for graduate students working on a core problem of interest to Y!  If you are already working on one of these problems, there is no reason not to submit, and if you arenâ&euro;&trade;t you might want to think about it for next year, as I am confident they all press the boundary of the possible in Machine Learning.   There are 7 days left. 
 The  Learning to Rank challenge  (due May 31) offers an $8K first prize for the best ranking algorithm on a real (and really used) dataset for search ranking, with presentations at an ICML workshop.  Unlike the Netflix competition, there are prizes for 2nd, 3rd, and 4th place, perhaps avoiding the heartbreak  the ensemble  encountered.  If you think you know how to rank, you should give it a try, and we might all learn something.  There are 3 months left.</p><p>5 0.62224293 <a title="339-lsi-5" href="../hunch_net-2006/hunch_net-2006-04-30-John_Langford_%26%238211%3B%3E_Yahoo_Research%2C_NY.html">175 hunch net-2006-04-30-John Langford &#8211;> Yahoo Research, NY</a></p>
<p>Introduction: I will join  Yahoo Research  (in New York) after my contract ends at  TTI-Chicago .
 
The deciding reasons are:
  
 Yahoo is running into many hard learning problems.  This is precisely the situation where basic research might hope to have the greatest impact. 
 Yahoo Research understands research including publishing, conferences, etc… 
 Yahoo Research is growing, so there is a chance I can help it grow well. 
 Yahoo understands the internet, including (but not at all limited to) experimenting with research blogs. 
  
In the end, Yahoo Research seems like the place where I might have a chance to make the greatest difference.  
 
Yahoo (as a company) has made a strong bet on Yahoo Research.  We-the-researchers all hope that bet will pay off, and this seems plausible.  I’ll certainly have fun trying.</p><p>6 0.58632332 <a title="339-lsi-6" href="../hunch_net-2005/hunch_net-2005-09-10-%26%238220%3BFailure%26%238221%3B_is_an_option.html">110 hunch net-2005-09-10-&#8220;Failure&#8221; is an option</a></p>
<p>7 0.57537305 <a title="339-lsi-7" href="../hunch_net-2008/hunch_net-2008-02-27-The_Stats_Handicap.html">290 hunch net-2008-02-27-The Stats Handicap</a></p>
<p>8 0.53013951 <a title="339-lsi-8" href="../hunch_net-2005/hunch_net-2005-05-17-A_Short_Guide_to_PhD_Graduate_Study.html">73 hunch net-2005-05-17-A Short Guide to PhD Graduate Study</a></p>
<p>9 0.49320447 <a title="339-lsi-9" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>10 0.49091992 <a title="339-lsi-10" href="../hunch_net-2006/hunch_net-2006-02-11-Yahoo%26%238217%3Bs_Learning_Problems..html">156 hunch net-2006-02-11-Yahoo&#8217;s Learning Problems.</a></p>
<p>11 0.48606455 <a title="339-lsi-11" href="../hunch_net-2009/hunch_net-2009-01-08-Predictive_Analytics_World.html">335 hunch net-2009-01-08-Predictive Analytics World</a></p>
<p>12 0.44595554 <a title="339-lsi-12" href="../hunch_net-2011/hunch_net-2011-11-26-Giving_Thanks.html">449 hunch net-2011-11-26-Giving Thanks</a></p>
<p>13 0.44145665 <a title="339-lsi-13" href="../hunch_net-2007/hunch_net-2007-11-02-The_Machine_Learning_Award_goes_to_%26%238230%3B.html">270 hunch net-2007-11-02-The Machine Learning Award goes to &#8230;</a></p>
<p>14 0.42565092 <a title="339-lsi-14" href="../hunch_net-2014/hunch_net-2014-02-16-Metacademy%3A_a_package_manager_for_knowledge.html">493 hunch net-2014-02-16-Metacademy: a package manager for knowledge</a></p>
<p>15 0.41498235 <a title="339-lsi-15" href="../hunch_net-2005/hunch_net-2005-05-11-Visa_Casualties.html">69 hunch net-2005-05-11-Visa Casualties</a></p>
<p>16 0.41166496 <a title="339-lsi-16" href="../hunch_net-2005/hunch_net-2005-03-29-Academic_Mechanism_Design.html">48 hunch net-2005-03-29-Academic Mechanism Design</a></p>
<p>17 0.41045341 <a title="339-lsi-17" href="../hunch_net-2010/hunch_net-2010-10-17-Partha_Niyogi_has_died.html">414 hunch net-2010-10-17-Partha Niyogi has died</a></p>
<p>18 0.40184018 <a title="339-lsi-18" href="../hunch_net-2005/hunch_net-2005-12-22-Yes_%2C_I_am_applying.html">142 hunch net-2005-12-22-Yes , I am applying</a></p>
<p>19 0.39398283 <a title="339-lsi-19" href="../hunch_net-2006/hunch_net-2006-05-08-Big_machine_learning.html">178 hunch net-2006-05-08-Big machine learning</a></p>
<p>20 0.36251417 <a title="339-lsi-20" href="../hunch_net-2006/hunch_net-2006-07-12-Who_is_having_visa_problems_reaching_US_conferences%3F.html">195 hunch net-2006-07-12-Who is having visa problems reaching US conferences?</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(27, 0.146), (36, 0.106), (38, 0.619)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.96815658 <a title="339-lda-1" href="../hunch_net-2005/hunch_net-2005-10-20-Machine_Learning_in_the_News.html">125 hunch net-2005-10-20-Machine Learning in the News</a></p>
<p>Introduction: The New York Times had a  short interview  about machine learning in datamining being used pervasively by the IRS and large corporations to predict who to audit and who to target for various marketing campaigns.  This is a big application area of machine learning.  It can be harmful (learning + databases = another way to invade privacy) or beneficial (as google demonstrates, better targeting of marketing campaigns is far less annoying).  This is yet more evidence that we can not rely upon “I’m just another fish in the school” logic for our expectations about treatment by government and large corporations.</p><p>same-blog 2 0.94746244 <a title="339-lda-2" href="../hunch_net-2009/hunch_net-2009-01-27-Key_Scientific_Challenges.html">339 hunch net-2009-01-27-Key Scientific Challenges</a></p>
<p>Introduction: Yahoo released the  Key Scientific Challenges  program.  There is a  Machine Learning  list I worked on and a  Statistics  list which  Deepak  worked on.
 
I’m hoping this is taken quite seriously by graduate students.  The primary value, is that it gave us a chance to sit down and publicly specify directions of research which would be valuable to make progress on.  A good strategy for a beginning graduate student is to pick one of these directions, pursue it, and make substantial advances for a PhD.  The directions are sufficiently general that I’m sure any serious advance has applications well beyond Yahoo.
 
A secondary point, (which I’m sure is primary for many    ) is that there is money for graduate students here.  It’s unrestricted, so you can use it for any reasonable travel, supplies, etc…</p><p>3 0.91576719 <a title="339-lda-3" href="../hunch_net-2013/hunch_net-2013-08-31-Extreme_Classification_workshop_at_NIPS.html">488 hunch net-2013-08-31-Extreme Classification workshop at NIPS</a></p>
<p>Introduction: Manik  and I are organizing the  extreme classification  workshop at NIPS this year.  We have a number of good speakers lined up, but I would further encourage anyone working in the area to submit an abstract by October 9.  I believe this is an idea whose time has now come.
 
The NIPS website doesnâ&euro;&trade;t have other workshops listed yet, but I expect several others to be of significant interest.</p><p>4 0.90415132 <a title="339-lda-4" href="../hunch_net-2006/hunch_net-2006-05-23-What_is_the_best_regret_transform_reduction_from_multiclass_to_binary%3F.html">181 hunch net-2006-05-23-What is the best regret transform reduction from multiclass to binary?</a></p>
<p>Introduction: This post is about an open problem in learning reductions.
 
 Background  A reduction might transform a a multiclass prediction problem where there are  k  possible labels into a binary learning problem where there are only 2 possible labels.   On this induced binary problem we might learn a binary classifier with some error rate  e .  After subtracting the minimum possible (Bayes) error rate  b , we get a regret  r = e – b .  The  PECOC (Probabilistic Error Correcting Output Code) reduction has the property that binary regret  r  implies multiclass regret at most  4r 0.5  .
 
 The problem  This is not the “rightest” answer.  Consider the  k=2  case, where we reduce binary to binary.  There exists a reduction (the identity) with the property that regret  r  implies regret  r .  This is substantially superior to the transform given by the PECOC reduction, which suggests that a better reduction may exist for general  k .  For example, we can not rule out the possibility that a reduction</p><p>5 0.89750272 <a title="339-lda-5" href="../hunch_net-2005/hunch_net-2005-06-18-Lower_Bounds_for_Learning_Reductions.html">83 hunch net-2005-06-18-Lower Bounds for Learning Reductions</a></p>
<p>Introduction: Learning reductions  transform a solver of one type of learning problem into a solver of another type of learning problem.  When we analyze these for robustness we can make statement of the form “Reduction  R  has the property that regret  r  (or loss) on subproblems of type  A  implies regret at most   f ( r )  on the original problem of type  B “.
 
A lower bound for a learning reduction would have the form “for all reductions  R , there exists a learning problem of type  B  and learning algorithm for problems of type  A  where regret  r  on induced problems implies  at least  regret  f ( r )  for  B “.
 
The pursuit of lower bounds is often questionable because, unlike upper bounds, they do not yield practical algorithms.  Nevertheless, they may be helpful as a tool for thinking about what is learnable and how learnable it is.  This has already come up  here  and  here .
 
At the moment, there is no coherent theory of lower bounds for learning reductions, and we have little understa</p><p>6 0.8560282 <a title="339-lda-6" href="../hunch_net-2006/hunch_net-2006-04-06-Bounds_greater_than_1.html">170 hunch net-2006-04-06-Bounds greater than 1</a></p>
<p>7 0.75146192 <a title="339-lda-7" href="../hunch_net-2009/hunch_net-2009-05-08-Computability_in_Artificial_Intelligence.html">353 hunch net-2009-05-08-Computability in Artificial Intelligence</a></p>
<p>8 0.74544704 <a title="339-lda-8" href="../hunch_net-2007/hunch_net-2007-02-16-The_Forgetting.html">233 hunch net-2007-02-16-The Forgetting</a></p>
<p>9 0.71101767 <a title="339-lda-9" href="../hunch_net-2007/hunch_net-2007-03-15-Alternative_Machine_Learning_Reductions_Definitions.html">236 hunch net-2007-03-15-Alternative Machine Learning Reductions Definitions</a></p>
<p>10 0.70863849 <a title="339-lda-10" href="../hunch_net-2007/hunch_net-2007-06-24-Interesting_Papers_at_ICML_2007.html">251 hunch net-2007-06-24-Interesting Papers at ICML 2007</a></p>
<p>11 0.6505664 <a title="339-lda-11" href="../hunch_net-2007/hunch_net-2007-04-18-%2450K_Spock_Challenge.html">239 hunch net-2007-04-18-$50K Spock Challenge</a></p>
<p>12 0.61293429 <a title="339-lda-12" href="../hunch_net-2005/hunch_net-2005-05-16-Regret_minimizing_vs_error_limiting_reductions.html">72 hunch net-2005-05-16-Regret minimizing vs error limiting reductions</a></p>
<p>13 0.59475058 <a title="339-lda-13" href="../hunch_net-2008/hunch_net-2008-01-18-Datasets.html">284 hunch net-2008-01-18-Datasets</a></p>
<p>14 0.53759414 <a title="339-lda-14" href="../hunch_net-2005/hunch_net-2005-02-21-Problem%3A_Cross_Validation.html">26 hunch net-2005-02-21-Problem: Cross Validation</a></p>
<p>15 0.49515998 <a title="339-lda-15" href="../hunch_net-2005/hunch_net-2005-06-17-Reopening_RL-%3EClassification.html">82 hunch net-2005-06-17-Reopening RL->Classification</a></p>
<p>16 0.47421619 <a title="339-lda-16" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>17 0.46606433 <a title="339-lda-17" href="../hunch_net-2005/hunch_net-2005-11-16-The_Everything_Ensemble_Edge.html">131 hunch net-2005-11-16-The Everything Ensemble Edge</a></p>
<p>18 0.46229464 <a title="339-lda-18" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>19 0.4611918 <a title="339-lda-19" href="../hunch_net-2008/hunch_net-2008-11-16-Observations_on_Linearity_for_Reductions_to_Regression.html">327 hunch net-2008-11-16-Observations on Linearity for Reductions to Regression</a></p>
<p>20 0.45426404 <a title="339-lda-20" href="../hunch_net-2010/hunch_net-2010-03-15-The_Efficient_Robust_Conditional_Probability_Estimation_Problem.html">391 hunch net-2010-03-15-The Efficient Robust Conditional Probability Estimation Problem</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
