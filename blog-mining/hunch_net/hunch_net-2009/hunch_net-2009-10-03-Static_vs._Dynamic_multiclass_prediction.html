<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>373 hunch net-2009-10-03-Static vs. Dynamic multiclass prediction</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2009" href="../home/hunch_net-2009_home.html">hunch_net-2009</a> <a title="hunch_net-2009-373" href="#">hunch_net-2009-373</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>373 hunch net-2009-10-03-Static vs. Dynamic multiclass prediction</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2009-373-html" href="http://hunch.net/?p=975">html</a></p><p>Introduction: I have had interesting discussions about distinction between static vs.
dynamic classes withKishoreandHal.The distinction arises in multiclass
prediction settings. A static set of classes is given by a set of
labels{1,â&euro;Ś,k}and the goal is generally to choose the most likely label given
features. The static approach is the one that we typically analyze and think
about in machine learning.The dynamic setting is one that is often used in
practice. The basic idea is that the number of classes is not fixed, varying
on a per example basis. These different classes are generally defined by a
choice of features.The distinction between these two settings as far as theory
goes, appears to be very substantial. For example, in the static setting,
inlearning reductions land, we have techniques now for robustO(log(k))time
prediction in many multiclass setting variants. In the dynamic setting, the
best techniques known areO(k), and furthermore this exponential gap may be
essential, at least without fur</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('static', 0.445), ('classes', 0.432), ('sublinear', 0.334), ('dynamic', 0.324), ('dynamically', 0.275), ('distinction', 0.186), ('multiclass', 0.17), ('set', 0.16), ('defined', 0.128), ('valid', 0.124), ('choose', 0.122), ('setting', 0.114), ('techniques', 0.095), ('time', 0.076), ('amongst', 0.075), ('embed', 0.074), ('areo', 0.074), ('prediction', 0.073), ('modified', 0.069), ('ranging', 0.069)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="373-tfidf-1" href="../hunch_net-2009/hunch_net-2009-10-03-Static_vs._Dynamic_multiclass_prediction.html">373 hunch net-2009-10-03-Static vs. Dynamic multiclass prediction</a></p>
<p>Introduction: I have had interesting discussions about distinction between static vs.
dynamic classes withKishoreandHal.The distinction arises in multiclass
prediction settings. A static set of classes is given by a set of
labels{1,â&euro;Ś,k}and the goal is generally to choose the most likely label given
features. The static approach is the one that we typically analyze and think
about in machine learning.The dynamic setting is one that is often used in
practice. The basic idea is that the number of classes is not fixed, varying
on a per example basis. These different classes are generally defined by a
choice of features.The distinction between these two settings as far as theory
goes, appears to be very substantial. For example, in the static setting,
inlearning reductions land, we have techniques now for robustO(log(k))time
prediction in many multiclass setting variants. In the dynamic setting, the
best techniques known areO(k), and furthermore this exponential gap may be
essential, at least without fur</p><p>2 0.1525737 <a title="373-tfidf-2" href="../hunch_net-2011/hunch_net-2011-09-28-Somebody%26%238217%3Bs_Eating_Your_Lunch.html">445 hunch net-2011-09-28-Somebody&#8217;s Eating Your Lunch</a></p>
<p>Introduction: Since we last discussedthe other online learning,Stanfordhas very visibly
started pushing mass teaching inAI,Machine Learning, andDatabases. In
retrospect, it's not too surprising that the next step up in serious online
teaching experiments are occurring at the computer science department of a
university embedded in the land of startups. Numbers on the order of100000are
quite significant--similar in scale to the number ofcomputer science
undergraduate students/yearin the US. Although these populations surely
differ, the fact that theycouldoverlap is worth considering for the
future.It's too soon to say how successful these classes will be and there are
many easy criticisms to make:Registration != Learning… but if only 1/10th
complete these classes, the scale of teaching still surpasses the scale of any
traditional process.1st year excitement != nth year routine… but if only
1/10th take future classes, the scale of teaching still surpasses the scale of
any traditional process.Hello, che</p><p>3 0.1252284 <a title="373-tfidf-3" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>Introduction: One thing which is clear on a little reflection is that there exists a single
master learning problem capable of encoding essentially all learning problems.
This problem is of course a very general sort of reinforcement learning where
the world interacts with an agent as:The world announces an observationx.The
agent makes a choicea.The world announces a rewardr.The goal here is to
maximize the sum of the rewards over the time of the agent. No particular
structure relatingxtoaoratoris implied by this setting so we do not know
effective general algorithms for the agent. It's very easy to prove lower
bounds showing that an agent cannot hope to succeed here--just consider the
case where actions are unrelated to rewards. Nevertheless, there is a real
sense in which essentially all forms of life are agents operating in this
setting, somehow succeeding. The gap between these observations drives
research--How can we find tractable specializations of the master problem
general enough to provide</p><p>4 0.10697381 <a title="373-tfidf-4" href="../hunch_net-2007/hunch_net-2007-10-17-Online_as_the_new_adjective.html">267 hunch net-2007-10-17-Online as the new adjective</a></p>
<p>Introduction: Online learning is in vogue, which means we should expect to see in the near
future:Online boosting.Online decision trees.Online SVMs. (actually, we've
already seen)Online deep learning.Online parallel learning.etc…There are three
fundamental drivers of this trend.Increasing size of datasets makes online
algorithms attractive.Online learning can simply be more efficient than batch
learning. Here is a picture from a class on online learning:The point of this
picture is that even in 3 dimensions and even with linear constraints, finding
the minima of a set in an online fashion can be typically faster than finding
the minima in a batch fashion. To see this, note that there is a minimal
number of gradient updates (i.e. 2) required in order to reach the minima in
the typical case. Given this, it's best to do these updates as quickly as
possible, which implies doing the first update online (i.e. before seeing all
the examples) is preferred. Note that this is the simplest possible setting--
m</p><p>5 0.10173221 <a title="373-tfidf-5" href="../hunch_net-2006/hunch_net-2006-05-01-A_conversation_between_Theo_and_Pat.html">176 hunch net-2006-05-01-A conversation between Theo and Pat</a></p>
<p>Introduction: Pat (the practitioner)I need to do multiclass classification and I only have a
decision tree.Theo (the thoeretician)Use anerror correcting output code.PatOh,
that's cool. But the created binary problems seem unintuitive. I'm not sure
the decision tree can solve them.TheoOh? Is your problem a decision
list?PatNo, I don't think so.TheoHmm. Are the classes well separated by axis
aligned splits?PatErr, maybe. I'm not sure.TheoWell, if they are, under the
IID assumption I can tell you how many samples you need.PatIID? The data is
definitely not IID.TheoOh dear.PatCan we get back to the choice of ECOC? I
suspect we need to build it dynamically in response to which subsets of the
labels are empirically separable from each other.TheoOk. What do you know
about your problem?PatNot much. My friend just gave me the dataset.TheoThen,
no one can help you.Pat(What a fuzzy thinker. Theo keeps jumping to
assumptions that just aren't true.)Theo(What a fuzzy thinker. Pat's problem is
unsolvable without m</p><p>6 0.10006185 <a title="373-tfidf-6" href="../hunch_net-2007/hunch_net-2007-01-15-The_Machine_Learning_Department.html">228 hunch net-2007-01-15-The Machine Learning Department</a></p>
<p>7 0.098917983 <a title="373-tfidf-7" href="../hunch_net-2005/hunch_net-2005-04-21-Dynamic_Programming_Generalizations_and_Their_Use.html">58 hunch net-2005-04-21-Dynamic Programming Generalizations and Their Use</a></p>
<p>8 0.094028778 <a title="373-tfidf-8" href="../hunch_net-2005/hunch_net-2005-10-16-Complexity%3A_It%26%238217%3Bs_all_in_your_head.html">123 hunch net-2005-10-16-Complexity: It&#8217;s all in your head</a></p>
<p>9 0.088429213 <a title="373-tfidf-9" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>10 0.082407266 <a title="373-tfidf-10" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>11 0.075687788 <a title="373-tfidf-11" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>12 0.075527437 <a title="373-tfidf-12" href="../hunch_net-2009/hunch_net-2009-12-27-Interesting_things_at_NIPS_2009.html">385 hunch net-2009-12-27-Interesting things at NIPS 2009</a></p>
<p>13 0.075370356 <a title="373-tfidf-13" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>14 0.074208245 <a title="373-tfidf-14" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>15 0.072835766 <a title="373-tfidf-15" href="../hunch_net-2013/hunch_net-2013-01-07-NYU_Large_Scale_Machine_Learning_Class.html">478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</a></p>
<p>16 0.072410807 <a title="373-tfidf-16" href="../hunch_net-2008/hunch_net-2008-05-25-Inappropriate_Mathematics_for_Machine_Learning.html">302 hunch net-2008-05-25-Inappropriate Mathematics for Machine Learning</a></p>
<p>17 0.070436426 <a title="373-tfidf-17" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>18 0.070268624 <a title="373-tfidf-18" href="../hunch_net-2005/hunch_net-2005-05-16-Regret_minimizing_vs_error_limiting_reductions.html">72 hunch net-2005-05-16-Regret minimizing vs error limiting reductions</a></p>
<p>19 0.068554409 <a title="373-tfidf-19" href="../hunch_net-2009/hunch_net-2009-06-03-Functionally_defined_Nonlinear_Dynamic_Models.html">359 hunch net-2009-06-03-Functionally defined Nonlinear Dynamic Models</a></p>
<p>20 0.06826447 <a title="373-tfidf-20" href="../hunch_net-2006/hunch_net-2006-03-02-Why_do_people_count_for_learning%3F.html">160 hunch net-2006-03-02-Why do people count for learning?</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.154), (1, -0.08), (2, -0.001), (3, -0.02), (4, -0.025), (5, -0.042), (6, 0.067), (7, 0.029), (8, -0.0), (9, -0.02), (10, -0.052), (11, -0.005), (12, 0.01), (13, -0.054), (14, 0.031), (15, 0.028), (16, -0.027), (17, 0.054), (18, -0.074), (19, -0.015), (20, 0.004), (21, 0.057), (22, 0.009), (23, -0.024), (24, -0.053), (25, -0.095), (26, -0.087), (27, -0.014), (28, -0.002), (29, -0.015), (30, 0.002), (31, -0.03), (32, 0.062), (33, -0.046), (34, 0.01), (35, 0.035), (36, 0.011), (37, -0.01), (38, 0.022), (39, 0.021), (40, 0.098), (41, 0.069), (42, 0.017), (43, 0.124), (44, -0.008), (45, 0.066), (46, -0.019), (47, 0.03), (48, 0.011), (49, -0.044)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94615805 <a title="373-lsi-1" href="../hunch_net-2009/hunch_net-2009-10-03-Static_vs._Dynamic_multiclass_prediction.html">373 hunch net-2009-10-03-Static vs. Dynamic multiclass prediction</a></p>
<p>Introduction: I have had interesting discussions about distinction between static vs.
dynamic classes withKishoreandHal.The distinction arises in multiclass
prediction settings. A static set of classes is given by a set of
labels{1,â&euro;Ś,k}and the goal is generally to choose the most likely label given
features. The static approach is the one that we typically analyze and think
about in machine learning.The dynamic setting is one that is often used in
practice. The basic idea is that the number of classes is not fixed, varying
on a per example basis. These different classes are generally defined by a
choice of features.The distinction between these two settings as far as theory
goes, appears to be very substantial. For example, in the static setting,
inlearning reductions land, we have techniques now for robustO(log(k))time
prediction in many multiclass setting variants. In the dynamic setting, the
best techniques known areO(k), and furthermore this exponential gap may be
essential, at least without fur</p><p>2 0.63570952 <a title="373-lsi-2" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>Introduction: This post is about a general technique for problem solving which I've never
seen taught (in full generality), but which I've found very useful.Many
problems in computer science turn out to be discretely difficult. The best
known version of such problems are NP-hard problems, but I mean 'discretely
difficult' in a much more general way, which I only know how to capture by
examples.ERMIn empirical risk minimization, you choose a minimum error rate
classifier from a set of classifiers. This is NP hard for common sets, but it
can be much harder, depending on the set.ExpertsIn the online learning with
experts setting, you try to predict well so as to compete with a set of
(adversarial) experts. Here the alternating quantifiers of you and an
adversary playing out a game can yield a dynamic programming problem that
grows exponentially.Policy IterationThe problem with policy iteration is that
you learn a new policy with respect to an old policy, which implies that
simply adopting the new polic</p><p>3 0.6264112 <a title="373-lsi-3" href="../hunch_net-2011/hunch_net-2011-09-28-Somebody%26%238217%3Bs_Eating_Your_Lunch.html">445 hunch net-2011-09-28-Somebody&#8217;s Eating Your Lunch</a></p>
<p>Introduction: Since we last discussedthe other online learning,Stanfordhas very visibly
started pushing mass teaching inAI,Machine Learning, andDatabases. In
retrospect, it's not too surprising that the next step up in serious online
teaching experiments are occurring at the computer science department of a
university embedded in the land of startups. Numbers on the order of100000are
quite significant--similar in scale to the number ofcomputer science
undergraduate students/yearin the US. Although these populations surely
differ, the fact that theycouldoverlap is worth considering for the
future.It's too soon to say how successful these classes will be and there are
many easy criticisms to make:Registration != Learning… but if only 1/10th
complete these classes, the scale of teaching still surpasses the scale of any
traditional process.1st year excitement != nth year routine… but if only
1/10th take future classes, the scale of teaching still surpasses the scale of
any traditional process.Hello, che</p><p>4 0.58861589 <a title="373-lsi-4" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>Introduction: If you search for "online learning" with anymajorsearchengine, it's
interesting to note that zero of the results are for online machine learning.
This may not be a mistake if you are committed to a global ordering. In other
words, the number of people specifically interested in the least interesting
top-10 online human learning result might exceed the number of people
interested in online machine learning, even given the presence of the other 9
results. The essential observation here is that the process of human learning
is a big business (around 5% of GDP) effecting virtually everyone.The internet
is changing this dramatically, by altering the economics of teaching. Consider
two possibilities:The classroom-style teaching environment continues as is,
with many teachers for the same subject.All the teachers for one subject get
together, along with perhaps a factor of 2 more people who are experts in
online delivery. They spend a factor of 4 more time designing the perfect
lecture & lear</p><p>5 0.57637882 <a title="373-lsi-5" href="../hunch_net-2006/hunch_net-2006-07-06-Branch_Prediction_Competition.html">190 hunch net-2006-07-06-Branch Prediction Competition</a></p>
<p>Introduction: Alan Fernpoints out thesecond branch prediction challenge(due September 29)
which is a follow up to thefirst branch prediction competition. Branch
prediction is one of the fundamental learning problems of the computer age:
without it our computers might run an order of magnitude slower. This is a
tough problem since there are sharp constraints on time and space complexity
in an online environment. For machine learning, the "idealistic track" may fit
well. Essentially, they remove these constraints to gain a weak upper bound on
what might be done.</p><p>6 0.56339532 <a title="373-lsi-6" href="../hunch_net-2007/hunch_net-2007-10-17-Online_as_the_new_adjective.html">267 hunch net-2007-10-17-Online as the new adjective</a></p>
<p>7 0.54006082 <a title="373-lsi-7" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>8 0.53832984 <a title="373-lsi-8" href="../hunch_net-2007/hunch_net-2007-10-24-Contextual_Bandits.html">269 hunch net-2007-10-24-Contextual Bandits</a></p>
<p>9 0.52771091 <a title="373-lsi-9" href="../hunch_net-2005/hunch_net-2005-07-13-Text_Entailment_at_AAAI.html">94 hunch net-2005-07-13-Text Entailment at AAAI</a></p>
<p>10 0.5274002 <a title="373-lsi-10" href="../hunch_net-2007/hunch_net-2007-07-01-Watchword%3A_Online_Learning.html">252 hunch net-2007-07-01-Watchword: Online Learning</a></p>
<p>11 0.52464205 <a title="373-lsi-11" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>12 0.5227682 <a title="373-lsi-12" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<p>13 0.50249815 <a title="373-lsi-13" href="../hunch_net-2009/hunch_net-2009-12-27-Interesting_things_at_NIPS_2009.html">385 hunch net-2009-12-27-Interesting things at NIPS 2009</a></p>
<p>14 0.49586973 <a title="373-lsi-14" href="../hunch_net-2010/hunch_net-2010-03-15-The_Efficient_Robust_Conditional_Probability_Estimation_Problem.html">391 hunch net-2010-03-15-The Efficient Robust Conditional Probability Estimation Problem</a></p>
<p>15 0.48489225 <a title="373-lsi-15" href="../hunch_net-2005/hunch_net-2005-04-21-Dynamic_Programming_Generalizations_and_Their_Use.html">58 hunch net-2005-04-21-Dynamic Programming Generalizations and Their Use</a></p>
<p>16 0.48198387 <a title="373-lsi-16" href="../hunch_net-2005/hunch_net-2005-09-08-Online_Learning_as_the_Mathematics_of_Accountability.html">109 hunch net-2005-09-08-Online Learning as the Mathematics of Accountability</a></p>
<p>17 0.47557968 <a title="373-lsi-17" href="../hunch_net-2008/hunch_net-2008-04-26-Eliminating_the_Birthday_Paradox_for_Universal_Features.html">298 hunch net-2008-04-26-Eliminating the Birthday Paradox for Universal Features</a></p>
<p>18 0.45423117 <a title="373-lsi-18" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>19 0.44636723 <a title="373-lsi-19" href="../hunch_net-2008/hunch_net-2008-02-27-The_Stats_Handicap.html">290 hunch net-2008-02-27-The Stats Handicap</a></p>
<p>20 0.4435029 <a title="373-lsi-20" href="../hunch_net-2005/hunch_net-2005-05-10-Learning_Reductions_are_Reductionist.html">68 hunch net-2005-05-10-Learning Reductions are Reductionist</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(42, 0.321), (45, 0.039), (68, 0.035), (69, 0.025), (74, 0.079), (95, 0.019), (99, 0.358)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.91718769 <a title="373-lda-1" href="../hunch_net-2005/hunch_net-2005-10-16-Complexity%3A_It%26%238217%3Bs_all_in_your_head.html">123 hunch net-2005-10-16-Complexity: It&#8217;s all in your head</a></p>
<p>Introduction: One of the central concerns of learning is to understand and toprevent
overfitting. Various notion of "function complexity" oftenarise: VC dimension,
Rademacher complexity, comparison classes ofexperts, and program length are
just a few.The term "complexity" to me seems somehow misleading; the terms
nevercapture something that meets my intuitive notion of complexity.
TheBayesian notion clearly captures what's going on. Functions
aren't"complex"- they're just "surprising": we assign to them lowprobability.
Most (all?) complexity notions I know boil downto some (generally loose) bound
on the prior probability of the function.In a sense, "complexity"
fundementally arises because probabilitydistributions must sum to one. You
can't believe in all possibilitiesat the same time, or at least not equally.
Rather you have tocarefully spread the probability mass over the options you'd
like toconsider. Large complexity classes means that beliefs are spreadthinly.
In it's simplest form, this phenom</p><p>same-blog 2 0.87229604 <a title="373-lda-2" href="../hunch_net-2009/hunch_net-2009-10-03-Static_vs._Dynamic_multiclass_prediction.html">373 hunch net-2009-10-03-Static vs. Dynamic multiclass prediction</a></p>
<p>Introduction: I have had interesting discussions about distinction between static vs.
dynamic classes withKishoreandHal.The distinction arises in multiclass
prediction settings. A static set of classes is given by a set of
labels{1,â&euro;Ś,k}and the goal is generally to choose the most likely label given
features. The static approach is the one that we typically analyze and think
about in machine learning.The dynamic setting is one that is often used in
practice. The basic idea is that the number of classes is not fixed, varying
on a per example basis. These different classes are generally defined by a
choice of features.The distinction between these two settings as far as theory
goes, appears to be very substantial. For example, in the static setting,
inlearning reductions land, we have techniques now for robustO(log(k))time
prediction in many multiclass setting variants. In the dynamic setting, the
best techniques known areO(k), and furthermore this exponential gap may be
essential, at least without fur</p><p>3 0.85471588 <a title="373-lda-3" href="../hunch_net-2009/hunch_net-2009-04-02-Asymmophobia.html">348 hunch net-2009-04-02-Asymmophobia</a></p>
<p>Introduction: One striking feature of many machine learning algorithms is the gymnastics
that designers go through to avoid symmetry breaking. In the most basic form
of machine learning, there are labeled examples composed of features. Each of
these can be treated symmetrically or asymmetrically by algorithms.feature
symmetryEvery feature is treated the same. In gradient update rules, the same
update is applied whether the feature is first or last. In metric-based
predictions, every feature is just as important in computing the
distance.example symmetryEvery example is treated the same. Batch learning
algorithms are great exemplars of this approach.label symmetryEvery label is
treated the same. This is particularly noticeable in multiclass classification
systems which predict according toarg maxlwlxbut it occurs in many other
places as well.Empirically, breaking symmetry well seems to yield great
algorithms.feature asymmetryFor those who like the "boosting is stepwise
additive regression on exponent</p><p>4 0.67644179 <a title="373-lda-4" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>Introduction: I've had serious conversations with several people who believe that the theory
in machine learning is "only useful for getting papers published". That's a
compelling statement, as I've seen many papers where the algorithm clearly
came first, and the theoretical justification for it came second, purely as a
perceived means to improve the chance of publication.Naturally, I disagree and
believe that learning theory has much more substantial applications.Even in
core learning algorithm design, I've found learning theory to be useful,
although it's application is more subtle than many realize. The most
straightforward applications can fail, because (as expectation suggests) worst
case bounds tend to be loose in practice (*). In my experience, considering
learning theory when designing an algorithm has two important effects in
practice:It can help make your algorithm behave right at a crude level of
analysis, leaving finer details to tuning or common sense. The best example I
have of this is</p><p>5 0.675879 <a title="373-lda-5" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>Introduction: One of the remarkable things about machine learning is how diverse it is. The
viewpoints of Bayesian learning, reinforcement learning, graphical models,
supervised learning, unsupervised learning, genetic programming, etc… share
little enough overlap that many people can and do make their careers within
one without touching, or even necessarily understanding the others.There are
two fundamental reasons why this is possible.For many problems, many
approaches work in the sense that they do something useful. This is true
empirically, where for many problems we can observe that many different
approaches yield better performance than any constant predictor. It's also
true in theory, where we know that for any set of predictors representable in
a finite amount of RAM, minimizing training error over the set of predictors
does something nontrivial when there are a sufficient number of examples.There
is nothing like a unifying problem defining the field. In many other areas
there are unifying p</p><p>6 0.67547417 <a title="373-lda-6" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>7 0.67505038 <a title="373-lda-7" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>8 0.67428964 <a title="373-lda-8" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>9 0.67377585 <a title="373-lda-9" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>10 0.67368835 <a title="373-lda-10" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<p>11 0.6735605 <a title="373-lda-11" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>12 0.67226356 <a title="373-lda-12" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>13 0.67201138 <a title="373-lda-13" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>14 0.67114097 <a title="373-lda-14" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>15 0.67020929 <a title="373-lda-15" href="../hunch_net-2005/hunch_net-2005-03-21-Research_Styles_in_Machine_Learning.html">44 hunch net-2005-03-21-Research Styles in Machine Learning</a></p>
<p>16 0.67018008 <a title="373-lda-16" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>17 0.66978276 <a title="373-lda-17" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>18 0.66783655 <a title="373-lda-18" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>19 0.66763771 <a title="373-lda-19" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>20 0.66622823 <a title="373-lda-20" href="../hunch_net-2011/hunch_net-2011-10-24-2011_ML_symposium_and_the_bears.html">448 hunch net-2011-10-24-2011 ML symposium and the bears</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
