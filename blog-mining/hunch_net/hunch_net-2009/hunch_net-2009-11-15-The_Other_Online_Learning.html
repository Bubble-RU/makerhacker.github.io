<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>378 hunch net-2009-11-15-The Other Online Learning</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2009" href="../home/hunch_net-2009_home.html">hunch_net-2009</a> <a title="hunch_net-2009-378" href="#">hunch_net-2009-378</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>378 hunch net-2009-11-15-The Other Online Learning</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2009-378-html" href="http://hunch.net/?p=1022">html</a></p><p>Introduction: If you search for "online learning" with anymajorsearchengine, it's
interesting to note that zero of the results are for online machine learning.
This may not be a mistake if you are committed to a global ordering. In other
words, the number of people specifically interested in the least interesting
top-10 online human learning result might exceed the number of people
interested in online machine learning, even given the presence of the other 9
results. The essential observation here is that the process of human learning
is a big business (around 5% of GDP) effecting virtually everyone.The internet
is changing this dramatically, by altering the economics of teaching. Consider
two possibilities:The classroom-style teaching environment continues as is,
with many teachers for the same subject.All the teachers for one subject get
together, along with perhaps a factor of 2 more people who are experts in
online delivery. They spend a factor of 4 more time designing the perfect
lecture & lear</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('online', 0.297), ('teacher', 0.255), ('student', 0.23), ('teaching', 0.183), ('teachers', 0.17), ('human', 0.165), ('students', 0.145), ('apprentice', 0.127), ('lecture', 0.127), ('approach', 0.121), ('perfect', 0.12), ('education', 0.12), ('centralized', 0.113), ('testing', 0.11), ('expect', 0.109), ('room', 0.1), ('economics', 0.099), ('similarly', 0.093), ('day', 0.092), ('difficult', 0.09)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999958 <a title="378-tfidf-1" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>Introduction: If you search for "online learning" with anymajorsearchengine, it's
interesting to note that zero of the results are for online machine learning.
This may not be a mistake if you are committed to a global ordering. In other
words, the number of people specifically interested in the least interesting
top-10 online human learning result might exceed the number of people
interested in online machine learning, even given the presence of the other 9
results. The essential observation here is that the process of human learning
is a big business (around 5% of GDP) effecting virtually everyone.The internet
is changing this dramatically, by altering the economics of teaching. Consider
two possibilities:The classroom-style teaching environment continues as is,
with many teachers for the same subject.All the teachers for one subject get
together, along with perhaps a factor of 2 more people who are experts in
online delivery. They spend a factor of 4 more time designing the perfect
lecture & lear</p><p>2 0.27301183 <a title="378-tfidf-2" href="../hunch_net-2011/hunch_net-2011-09-28-Somebody%26%238217%3Bs_Eating_Your_Lunch.html">445 hunch net-2011-09-28-Somebody&#8217;s Eating Your Lunch</a></p>
<p>Introduction: Since we last discussedthe other online learning,Stanfordhas very visibly
started pushing mass teaching inAI,Machine Learning, andDatabases. In
retrospect, it's not too surprising that the next step up in serious online
teaching experiments are occurring at the computer science department of a
university embedded in the land of startups. Numbers on the order of100000are
quite significant--similar in scale to the number ofcomputer science
undergraduate students/yearin the US. Although these populations surely
differ, the fact that theycouldoverlap is worth considering for the
future.It's too soon to say how successful these classes will be and there are
many easy criticisms to make:Registration != Learning… but if only 1/10th
complete these classes, the scale of teaching still surpasses the scale of any
traditional process.1st year excitement != nth year routine… but if only
1/10th take future classes, the scale of teaching still surpasses the scale of
any traditional process.Hello, che</p><p>3 0.168567 <a title="378-tfidf-3" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>Introduction: How do you create an optimal environment for research? Here are some essential
ingredients that I see.Stability. University-based research is relatively good
at this. On any particular day, researchers face choices in what they will
work on. A very common tradeoff is between:easy smalldifficult bigFor
researchers without stability, the 'easy small' option wins. This is often
"ok"--a series of incremental improvements on the state of the art can add up
to something very beneficial. However, it misses one of the big potentials of
research: finding entirely new and better ways of doing things.Stability comes
in many forms. The prototypical example is tenure at a university--a tenured
professor is almost imposssible to fire which means that the professor has the
freedom to consider far horizon activities. An iron-clad guarantee of a
paycheck is not necessary--industrial research labs have succeeded well with
research positions of indefinite duration. Atnt research was a great example
of th</p><p>4 0.16271268 <a title="378-tfidf-4" href="../hunch_net-2007/hunch_net-2007-10-17-Online_as_the_new_adjective.html">267 hunch net-2007-10-17-Online as the new adjective</a></p>
<p>Introduction: Online learning is in vogue, which means we should expect to see in the near
future:Online boosting.Online decision trees.Online SVMs. (actually, we've
already seen)Online deep learning.Online parallel learning.etc…There are three
fundamental drivers of this trend.Increasing size of datasets makes online
algorithms attractive.Online learning can simply be more efficient than batch
learning. Here is a picture from a class on online learning:The point of this
picture is that even in 3 dimensions and even with linear constraints, finding
the minima of a set in an online fashion can be typically faster than finding
the minima in a batch fashion. To see this, note that there is a minimal
number of gradient updates (i.e. 2) required in order to reach the minima in
the typical case. Given this, it's best to do these updates as quickly as
possible, which implies doing the first update online (i.e. before seeing all
the examples) is preferred. Note that this is the simplest possible setting--
m</p><p>5 0.1458029 <a title="378-tfidf-5" href="../hunch_net-2011/hunch_net-2011-11-26-Giving_Thanks.html">449 hunch net-2011-11-26-Giving Thanks</a></p>
<p>Introduction: Thanksgiving is perhaps my favorite holiday, because pausing your life and
giving thanks provides a needed moment of perspective.As a researcher, I am
most thankful for my education, without which I could not function. I want to
share this, because it provides some sense of how a researcher starts.My long
term memory seems to function particularly well, which makes any education I
get is particularly useful.I am naturally obsessive, which makes me chase down
details until I fully understand things. Natural obsessiveness can go wrong,
of course, but it's a great ally when you absolutely must get things right.My
childhood was all in one hometown, which was a conscious sacrifice on the part
of my father, implying disruptions from moving around were eliminated. I'm not
sure how important this was since travel has it's own benefits, but it bears
thought.I had several great teachers in grade school, and naturally gravitated
towards teachers over classmates, as they seemed more interesting. I</p><p>6 0.14448516 <a title="378-tfidf-6" href="../hunch_net-2013/hunch_net-2013-01-07-NYU_Large_Scale_Machine_Learning_Class.html">478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</a></p>
<p>7 0.1407683 <a title="378-tfidf-7" href="../hunch_net-2005/hunch_net-2005-09-08-Online_Learning_as_the_Mathematics_of_Accountability.html">109 hunch net-2005-09-08-Online Learning as the Mathematics of Accountability</a></p>
<p>8 0.13801226 <a title="378-tfidf-8" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>9 0.13126646 <a title="378-tfidf-9" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>10 0.12805352 <a title="378-tfidf-10" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>11 0.12524521 <a title="378-tfidf-11" href="../hunch_net-2009/hunch_net-2009-02-22-Effective_Research_Funding.html">344 hunch net-2009-02-22-Effective Research Funding</a></p>
<p>12 0.1247071 <a title="378-tfidf-12" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>13 0.12365747 <a title="378-tfidf-13" href="../hunch_net-2007/hunch_net-2007-12-19-Cool_and_interesting_things_seen_at_NIPS.html">279 hunch net-2007-12-19-Cool and interesting things seen at NIPS</a></p>
<p>14 0.12293974 <a title="378-tfidf-14" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>15 0.11829069 <a title="378-tfidf-15" href="../hunch_net-2005/hunch_net-2005-02-25-Problem%3A_Online_Learning.html">28 hunch net-2005-02-25-Problem: Online Learning</a></p>
<p>16 0.11678682 <a title="378-tfidf-16" href="../hunch_net-2011/hunch_net-2011-02-17-What_does_Watson_mean%3F.html">424 hunch net-2011-02-17-What does Watson mean?</a></p>
<p>17 0.11483817 <a title="378-tfidf-17" href="../hunch_net-2005/hunch_net-2005-09-10-%26%238220%3BFailure%26%238221%3B_is_an_option.html">110 hunch net-2005-09-10-&#8220;Failure&#8221; is an option</a></p>
<p>18 0.11208482 <a title="378-tfidf-18" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>19 0.10824769 <a title="378-tfidf-19" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<p>20 0.10612641 <a title="378-tfidf-20" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.3), (1, 0.002), (2, 0.144), (3, -0.058), (4, 0.024), (5, 0.044), (6, 0.122), (7, 0.035), (8, -0.079), (9, -0.106), (10, 0.04), (11, -0.004), (12, -0.018), (13, -0.07), (14, -0.045), (15, 0.069), (16, -0.003), (17, 0.046), (18, -0.073), (19, -0.019), (20, 0.043), (21, 0.011), (22, -0.03), (23, -0.021), (24, -0.024), (25, -0.207), (26, 0.035), (27, -0.017), (28, -0.005), (29, -0.047), (30, -0.074), (31, -0.073), (32, 0.059), (33, 0.017), (34, -0.035), (35, 0.056), (36, -0.037), (37, -0.002), (38, -0.041), (39, -0.08), (40, 0.05), (41, 0.001), (42, 0.046), (43, 0.149), (44, -0.006), (45, 0.104), (46, 0.029), (47, -0.0), (48, 0.011), (49, -0.013)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95142448 <a title="378-lsi-1" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>Introduction: If you search for "online learning" with anymajorsearchengine, it's
interesting to note that zero of the results are for online machine learning.
This may not be a mistake if you are committed to a global ordering. In other
words, the number of people specifically interested in the least interesting
top-10 online human learning result might exceed the number of people
interested in online machine learning, even given the presence of the other 9
results. The essential observation here is that the process of human learning
is a big business (around 5% of GDP) effecting virtually everyone.The internet
is changing this dramatically, by altering the economics of teaching. Consider
two possibilities:The classroom-style teaching environment continues as is,
with many teachers for the same subject.All the teachers for one subject get
together, along with perhaps a factor of 2 more people who are experts in
online delivery. They spend a factor of 4 more time designing the perfect
lecture & lear</p><p>2 0.86533111 <a title="378-lsi-2" href="../hunch_net-2011/hunch_net-2011-09-28-Somebody%26%238217%3Bs_Eating_Your_Lunch.html">445 hunch net-2011-09-28-Somebody&#8217;s Eating Your Lunch</a></p>
<p>Introduction: Since we last discussedthe other online learning,Stanfordhas very visibly
started pushing mass teaching inAI,Machine Learning, andDatabases. In
retrospect, it's not too surprising that the next step up in serious online
teaching experiments are occurring at the computer science department of a
university embedded in the land of startups. Numbers on the order of100000are
quite significant--similar in scale to the number ofcomputer science
undergraduate students/yearin the US. Although these populations surely
differ, the fact that theycouldoverlap is worth considering for the
future.It's too soon to say how successful these classes will be and there are
many easy criticisms to make:Registration != Learning… but if only 1/10th
complete these classes, the scale of teaching still surpasses the scale of any
traditional process.1st year excitement != nth year routine… but if only
1/10th take future classes, the scale of teaching still surpasses the scale of
any traditional process.Hello, che</p><p>3 0.69311178 <a title="378-lsi-3" href="../hunch_net-2013/hunch_net-2013-01-07-NYU_Large_Scale_Machine_Learning_Class.html">478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</a></p>
<p>Introduction: Yann LeCunand I are coteaching a class onLarge Scale Machine Learningstarting
late Januaryat NYU. This class will cover many tricks to get machine learning
working well on datasets with many features, examples, and classes, along with
several elements of deep learning and support systems enabling the
previous.This is not a beginning class--you really need to have taken a basic
machine learning class previously to follow along. Students will be able to
run and experiment with large scale learning algorithms sinceYahoo!has donated
servers which are being configured into a small scaleHadoopcluster. We are
planning to cover the frontier of research in scalable learning algorithms, so
good class projects could easily lead to papers.For me, this is a chance to
teach on many topics of past research. In general, it seems like researchers
should engage in at least occasional teaching of research, both as a proof of
teachability and to see their own research through that lens. More generally,
I</p><p>4 0.66359198 <a title="378-lsi-4" href="../hunch_net-2007/hunch_net-2007-10-17-Online_as_the_new_adjective.html">267 hunch net-2007-10-17-Online as the new adjective</a></p>
<p>Introduction: Online learning is in vogue, which means we should expect to see in the near
future:Online boosting.Online decision trees.Online SVMs. (actually, we've
already seen)Online deep learning.Online parallel learning.etc…There are three
fundamental drivers of this trend.Increasing size of datasets makes online
algorithms attractive.Online learning can simply be more efficient than batch
learning. Here is a picture from a class on online learning:The point of this
picture is that even in 3 dimensions and even with linear constraints, finding
the minima of a set in an online fashion can be typically faster than finding
the minima in a batch fashion. To see this, note that there is a minimal
number of gradient updates (i.e. 2) required in order to reach the minima in
the typical case. Given this, it's best to do these updates as quickly as
possible, which implies doing the first update online (i.e. before seeing all
the examples) is preferred. Note that this is the simplest possible setting--
m</p><p>5 0.65782362 <a title="378-lsi-5" href="../hunch_net-2009/hunch_net-2009-10-03-Static_vs._Dynamic_multiclass_prediction.html">373 hunch net-2009-10-03-Static vs. Dynamic multiclass prediction</a></p>
<p>Introduction: I have had interesting discussions about distinction between static vs.
dynamic classes withKishoreandHal.The distinction arises in multiclass
prediction settings. A static set of classes is given by a set of
labels{1,â&euro;Ś,k}and the goal is generally to choose the most likely label given
features. The static approach is the one that we typically analyze and think
about in machine learning.The dynamic setting is one that is often used in
practice. The basic idea is that the number of classes is not fixed, varying
on a per example basis. These different classes are generally defined by a
choice of features.The distinction between these two settings as far as theory
goes, appears to be very substantial. For example, in the static setting,
inlearning reductions land, we have techniques now for robustO(log(k))time
prediction in many multiclass setting variants. In the dynamic setting, the
best techniques known areO(k), and furthermore this exponential gap may be
essential, at least without fur</p><p>6 0.63990569 <a title="378-lsi-6" href="../hunch_net-2005/hunch_net-2005-09-08-Online_Learning_as_the_Mathematics_of_Accountability.html">109 hunch net-2005-09-08-Online Learning as the Mathematics of Accountability</a></p>
<p>7 0.63138223 <a title="378-lsi-7" href="../hunch_net-2014/hunch_net-2014-02-16-Metacademy%3A_a_package_manager_for_knowledge.html">493 hunch net-2014-02-16-Metacademy: a package manager for knowledge</a></p>
<p>8 0.62580866 <a title="378-lsi-8" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>9 0.60050976 <a title="378-lsi-9" href="../hunch_net-2009/hunch_net-2009-12-27-Interesting_things_at_NIPS_2009.html">385 hunch net-2009-12-27-Interesting things at NIPS 2009</a></p>
<p>10 0.55708849 <a title="378-lsi-10" href="../hunch_net-2010/hunch_net-2010-05-02-What%26%238217%3Bs_the_difference_between_gambling_and_rewarding_good_prediction%3F.html">397 hunch net-2010-05-02-What&#8217;s the difference between gambling and rewarding good prediction?</a></p>
<p>11 0.55641407 <a title="378-lsi-11" href="../hunch_net-2006/hunch_net-2006-09-18-What_is_missing_for_online_collaborative_research%3F.html">208 hunch net-2006-09-18-What is missing for online collaborative research?</a></p>
<p>12 0.55419034 <a title="378-lsi-12" href="../hunch_net-2011/hunch_net-2011-06-22-Ultra_LDA.html">436 hunch net-2011-06-22-Ultra LDA</a></p>
<p>13 0.55393201 <a title="378-lsi-13" href="../hunch_net-2005/hunch_net-2005-02-04-JMLG.html">13 hunch net-2005-02-04-JMLG</a></p>
<p>14 0.54997915 <a title="378-lsi-14" href="../hunch_net-2008/hunch_net-2008-02-27-The_Stats_Handicap.html">290 hunch net-2008-02-27-The Stats Handicap</a></p>
<p>15 0.5460577 <a title="378-lsi-15" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>16 0.54436862 <a title="378-lsi-16" href="../hunch_net-2011/hunch_net-2011-11-26-Giving_Thanks.html">449 hunch net-2011-11-26-Giving Thanks</a></p>
<p>17 0.54369795 <a title="378-lsi-17" href="../hunch_net-2007/hunch_net-2007-07-01-Watchword%3A_Online_Learning.html">252 hunch net-2007-07-01-Watchword: Online Learning</a></p>
<p>18 0.5414632 <a title="378-lsi-18" href="../hunch_net-2011/hunch_net-2011-10-24-2011_ML_symposium_and_the_bears.html">448 hunch net-2011-10-24-2011 ML symposium and the bears</a></p>
<p>19 0.54131615 <a title="378-lsi-19" href="../hunch_net-2005/hunch_net-2005-02-25-Problem%3A_Online_Learning.html">28 hunch net-2005-02-25-Problem: Online Learning</a></p>
<p>20 0.53963572 <a title="378-lsi-20" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.025), (7, 0.015), (10, 0.013), (35, 0.02), (42, 0.595), (45, 0.045), (68, 0.027), (69, 0.036), (73, 0.012), (74, 0.07), (82, 0.019), (95, 0.036)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99909484 <a title="378-lda-1" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>Introduction: If you search for "online learning" with anymajorsearchengine, it's
interesting to note that zero of the results are for online machine learning.
This may not be a mistake if you are committed to a global ordering. In other
words, the number of people specifically interested in the least interesting
top-10 online human learning result might exceed the number of people
interested in online machine learning, even given the presence of the other 9
results. The essential observation here is that the process of human learning
is a big business (around 5% of GDP) effecting virtually everyone.The internet
is changing this dramatically, by altering the economics of teaching. Consider
two possibilities:The classroom-style teaching environment continues as is,
with many teachers for the same subject.All the teachers for one subject get
together, along with perhaps a factor of 2 more people who are experts in
online delivery. They spend a factor of 4 more time designing the perfect
lecture & lear</p><p>2 0.99645555 <a title="378-lda-2" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<p>Introduction: This post is by Daniel Hsu and John Langford.In selective sampling style
active learning, a learning algorithm chooses which examples to label. We now
have an active learning algorithm that is:Efficientin label complexity,
unlabeled complexity, and computational complexity.Competitivewith supervised
learning anywhere that supervised learning works.Compatiblewith online
learning, with any optimization-based learning algorithm, with any loss
function, with offline testing, and even with changing learning
algorithms.Empiricallyeffective.The basic idea is to combinedisagreement
region-based samplingwithimportance weighting: an example is selected to be
labeled with probability proportional to how useful it is for distinguishing
among near-optimal classifiers, and labeled examples are importance-weighted
by the inverse of these probabilities. The combination of these simple ideas
removes thesampling biasproblem that has plagued many previous heuristics for
active learning, and yet leads to</p><p>3 0.9910351 <a title="378-lda-3" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>Introduction: This post is some combination of belaboring the obvious and speculating wildly
about the future. The basic issue to be addressed is how to think about
machine learning in terms given to us from Programming Language theory.Types
and ReductionsJohn's research programme (I feel this should be in British
spelling to reflect the grandiousness of the ideaâ&euro;Ś) of machine learning
reductionsStateOfReductionis at some essential level type-theoretic in nature.
The fundamental elements are the classifier, a function f: alpha -> beta, and
the corresponding classifier trainer g: List of (alpha,beta) -> (alpha ->
beta). The research goal is to create *combinators* that produce new f's and
g's given existing ones. John (probably quite rightly) seems unwilling at the
moment to commit to any notion stronger than these combinators are correctly
typed. One way to see the result of a reduction is something typed like: (For
those denied the joy of the Hindly-Milner type system, "simple" is probably
wildly wr</p><p>4 0.98975277 <a title="378-lda-4" href="../hunch_net-2005/hunch_net-2005-07-07-The_Limits_of_Learning_Theory.html">90 hunch net-2005-07-07-The Limits of Learning Theory</a></p>
<p>Introduction: Suppose we had an infinitely powerful mathematician sitting in a room and
proving theorems about learning. Could he solve machine learning?The answer is
"no". This answer is both obvious and sometimes underappreciated.There are
several ways to conclude that somebiasis necessary in order to succesfully
learn. For example, suppose we are trying to solve classification. At
prediction time, we observe some featuresXand want to make a prediction of
either0or1. Bias is what makes us prefer one answer over the other based on
past experience. In order to learn we must:Have a bias. Always predicting0is
as likely as1is useless.Have the "right" bias. Predicting1when the answer
is0is also not helpful.The implication of "have a bias" is that we can not
design effective learning algorithms with "a uniform prior over all
possibilities". The implication of "have the 'right' bias" is that our
mathematician fails since "right" is defined with respect to the solutions to
problems encountered in the real</p><p>5 0.98964721 <a title="378-lda-5" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>Introduction: Here are some papers fromICML 2008that I found interesting.Risi
KondorandKarsten Borgwardt,The Skew Spectrum of Graphs. This paper is about a
new family of functions on graphs which is invariant under node label
permutation. They show that these quantities appear to yield good features for
learning.Sanjoy DasguptaandDaniel Hsu.Hierarchical sampling for active
learning.This is the first published practical consistent active learning
algorithm. The abstract is also pretty impressive.Lihong Li,Michael Littman,
andThomas WalshKnows What It Knows: A Framework For Self-Aware Learning.This
is an attempt to create learning algorithms that know when they err, (other
work includesVovk). It's not yet clear to me what the right model forfeature-
dependent confidence intervalsis.Novi Quadrianto,Alex Smola,TIberio Caetano,
andQuoc Viet LeEstimating Labels from Label Proportions. This is an example of
learning in a specialization of the offline contextual bandit setting.Filip
Radlinski,Robert Kleinbe</p><p>6 0.9894715 <a title="378-lda-6" href="../hunch_net-2005/hunch_net-2005-03-29-Academic_Mechanism_Design.html">48 hunch net-2005-03-29-Academic Mechanism Design</a></p>
<p>7 0.98858267 <a title="378-lda-7" href="../hunch_net-2012/hunch_net-2012-10-26-ML_Symposium_and_Strata-Hadoop_World.html">475 hunch net-2012-10-26-ML Symposium and Strata-Hadoop World</a></p>
<p>8 0.98849565 <a title="378-lda-8" href="../hunch_net-2006/hunch_net-2006-06-15-IJCAI_is_out_of_season.html">184 hunch net-2006-06-15-IJCAI is out of season</a></p>
<p>9 0.98774838 <a title="378-lda-9" href="../hunch_net-2005/hunch_net-2005-05-06-Don%26%238217%3Bt_mix_the_solution_into_the_problem.html">67 hunch net-2005-05-06-Don&#8217;t mix the solution into the problem</a></p>
<p>10 0.98705924 <a title="378-lda-10" href="../hunch_net-2005/hunch_net-2005-12-09-Some_NIPS_papers.html">138 hunch net-2005-12-09-Some NIPS papers</a></p>
<p>11 0.98628145 <a title="378-lda-11" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>12 0.9857493 <a title="378-lda-12" href="../hunch_net-2006/hunch_net-2006-01-23-On_Coding_via_Mutual_Information_%26%23038%3B_Bayes_Nets.html">150 hunch net-2006-01-23-On Coding via Mutual Information &#038; Bayes Nets</a></p>
<p>13 0.98456794 <a title="378-lda-13" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>14 0.9837321 <a title="378-lda-14" href="../hunch_net-2008/hunch_net-2008-04-27-Watchword%3A_Supervised_Learning.html">299 hunch net-2008-04-27-Watchword: Supervised Learning</a></p>
<p>15 0.98244572 <a title="378-lda-15" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>16 0.9821974 <a title="378-lda-16" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>17 0.9807902 <a title="378-lda-17" href="../hunch_net-2005/hunch_net-2005-02-23-Problem%3A_Reinforcement_Learning_with_Classification.html">27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</a></p>
<p>18 0.97703874 <a title="378-lda-18" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>19 0.97508234 <a title="378-lda-19" href="../hunch_net-2006/hunch_net-2006-01-13-Benchmarks_for_RL.html">148 hunch net-2006-01-13-Benchmarks for RL</a></p>
<p>20 0.97471708 <a title="378-lda-20" href="../hunch_net-2005/hunch_net-2005-03-21-Research_Styles_in_Machine_Learning.html">44 hunch net-2005-03-21-Research Styles in Machine Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
