<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>342 hunch net-2009-02-16-KDNuggets</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2009" href="../home/hunch_net-2009_home.html">hunch_net-2009</a> <a title="hunch_net-2009-342" href="#">hunch_net-2009-342</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>342 hunch net-2009-02-16-KDNuggets</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2009-342-html" href="http://hunch.net/?p=569">html</a></p><p>Introduction: Eric Zaetsch points out  KDNuggets  which is a well-developed mailing list/news site with a  KDD  flavor.  This might particularly interest people looking for industrial jobs in machine learning, as the mailing list has many such.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Eric Zaetsch points out  KDNuggets  which is a well-developed mailing list/news site with a  KDD  flavor. [sent-1, score-1.048]
</p><p>2 This might particularly interest people looking for industrial jobs in machine learning, as the mailing list has many such. [sent-2, score-2.205]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mailing', 0.637), ('eric', 0.36), ('jobs', 0.318), ('industrial', 0.308), ('site', 0.244), ('looking', 0.212), ('kdd', 0.21), ('list', 0.179), ('points', 0.167), ('interest', 0.154), ('particularly', 0.133), ('might', 0.09), ('people', 0.064), ('machine', 0.06), ('many', 0.05), ('learning', 0.025)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="342-tfidf-1" href="../hunch_net-2009/hunch_net-2009-02-16-KDNuggets.html">342 hunch net-2009-02-16-KDNuggets</a></p>
<p>Introduction: Eric Zaetsch points out  KDNuggets  which is a well-developed mailing list/news site with a  KDD  flavor.  This might particularly interest people looking for industrial jobs in machine learning, as the mailing list has many such.</p><p>2 0.23878744 <a title="342-tfidf-2" href="../hunch_net-2007/hunch_net-2007-12-17-New_Machine_Learning_mailing_list.html">278 hunch net-2007-12-17-New Machine Learning mailing list</a></p>
<p>Introduction: IMLS  (which is the nonprofit running ICML) has setup a new mailing list for  Machine Learning News .  The list address is ML-news@googlegroups.com, and signup requires a google account (which you can create).  Only members can send messages.</p><p>3 0.12523831 <a title="342-tfidf-3" href="../hunch_net-2005/hunch_net-2005-02-02-Kolmogorov_Complexity_and_Googling.html">10 hunch net-2005-02-02-Kolmogorov Complexity and Googling</a></p>
<p>Introduction: Machine learning makes the   New Scientist  . From the article: 
  

COMPUTERS can learn the meaning of words simply by plugging into Google. The finding could bring forward the day that true artificial intelligence is developedâ&euro;Ś. 
But Paul Vitanyi and Rudi Cilibrasi of the National Institute for Mathematics and Computer Science in Amsterdam, the Netherlands, realised that a Google search can be used to measure how closely two words relate to each other. For instance, imagine a computer needs to understand what a hat is.

  
You can read the paper at  KC Google .
 
Hat tip:   Kolmogorov Mailing List 
 
Any thoughts on the paper?</p><p>4 0.10869544 <a title="342-tfidf-4" href="../hunch_net-2011/hunch_net-2011-03-27-Vowpal_Wabbit%2C_v5.1.html">428 hunch net-2011-03-27-Vowpal Wabbit, v5.1</a></p>
<p>Introduction: I just created  version 5.1  of  vowpal wabbit .  This almost entirely a bugfix release, so it’s an easy upgrade from v5.0.
 
In addition:
  
 There is now a  mailing list , which I and several other developers are subscribed to. 
 The main website has shifted to the wiki on github.  This means that anyone with a github account can now edit it. 
 I’m planning to give a tutorial tomorrow on it at  eHarmony / the LA machine learning meetup  at 10am.  Drop by if you’re interested. 
  
The status of VW amongst other open source projects has changed.  When VW first came out, it was relatively unique amongst existing projects in terms of features.  At this point, many other projects have started to appreciate the value of the design choices here.  This includes:
  
  Mahout , which now has an SGD implementation. 
  Shogun , where  Soeren  is keen on  incorporating features . 
  LibLinear , where they won the KDD best paper award for  out-of-core learning . 
  
This is expected—any open sourc</p><p>5 0.090447612 <a title="342-tfidf-5" href="../hunch_net-2009/hunch_net-2009-01-08-Predictive_Analytics_World.html">335 hunch net-2009-01-08-Predictive Analytics World</a></p>
<p>Introduction: Carla Vicens and  Eric Siegel  contacted me about  Predictive Analytics World  in San Francisco February 18&19, which I wasn’t familiar with.  A quick look at the  agenda  reveals several people I know working on applications of machine learning in businesses, covering deployed applications topics.  It’s interesting to see a business-focused machine learning conference, as it says that we are succeeding as a field.  If you are interested in deployed applications, you might attend.
 
Eric and I did a quick interview by email.
 
John > 
I’ve mostly published and participated in academic machine learning conferences like ICML, COLT, and NIPS.   When I look at the  set of speakers and subjects  for your conference  I think “machine learning for business”.  Is that your understanding of things? What I’m trying to ask is: what do you view as the primary goal for this conference?
 
Eric > 
 You got it.  This is the business event focused on the commercial deployment of technology developed at</p><p>6 0.089577205 <a title="342-tfidf-6" href="../hunch_net-2009/hunch_net-2009-07-09-The_Machine_Learning_Forum.html">363 hunch net-2009-07-09-The Machine Learning Forum</a></p>
<p>7 0.085070267 <a title="342-tfidf-7" href="../hunch_net-2005/hunch_net-2005-01-19-Why_I_decided_to_run_a_weblog..html">1 hunch net-2005-01-19-Why I decided to run a weblog.</a></p>
<p>8 0.084662125 <a title="342-tfidf-8" href="../hunch_net-2007/hunch_net-2007-06-23-Machine_Learning_Jobs_are_Growing_on_Trees.html">250 hunch net-2007-06-23-Machine Learning Jobs are Growing on Trees</a></p>
<p>9 0.081050448 <a title="342-tfidf-9" href="../hunch_net-2005/hunch_net-2005-09-05-Site_Update.html">107 hunch net-2005-09-05-Site Update</a></p>
<p>10 0.06873329 <a title="342-tfidf-10" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>11 0.063836835 <a title="342-tfidf-11" href="../hunch_net-2011/hunch_net-2011-08-15-Vowpal_Wabbit_6.0.html">441 hunch net-2011-08-15-Vowpal Wabbit 6.0</a></p>
<p>12 0.063353173 <a title="342-tfidf-12" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>13 0.062369037 <a title="342-tfidf-13" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>14 0.056583982 <a title="342-tfidf-14" href="../hunch_net-2008/hunch_net-2008-11-11-COLT_CFP.html">326 hunch net-2008-11-11-COLT CFP</a></p>
<p>15 0.05285297 <a title="342-tfidf-15" href="../hunch_net-2007/hunch_net-2007-04-21-Videolectures.net.html">240 hunch net-2007-04-21-Videolectures.net</a></p>
<p>16 0.052057549 <a title="342-tfidf-16" href="../hunch_net-2012/hunch_net-2012-10-26-ML_Symposium_and_Strata-Hadoop_World.html">475 hunch net-2012-10-26-ML Symposium and Strata-Hadoop World</a></p>
<p>17 0.050550826 <a title="342-tfidf-17" href="../hunch_net-2005/hunch_net-2005-02-19-Machine_learning_reading_groups.html">24 hunch net-2005-02-19-Machine learning reading groups</a></p>
<p>18 0.050169736 <a title="342-tfidf-18" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>19 0.047875986 <a title="342-tfidf-19" href="../hunch_net-2005/hunch_net-2005-03-24-The_Role_of_Workshops.html">46 hunch net-2005-03-24-The Role of Workshops</a></p>
<p>20 0.047342181 <a title="342-tfidf-20" href="../hunch_net-2010/hunch_net-2010-10-29-To_Vidoelecture_or_not.html">416 hunch net-2010-10-29-To Vidoelecture or not</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.073), (1, -0.059), (2, -0.064), (3, -0.025), (4, -0.018), (5, 0.008), (6, -0.029), (7, -0.018), (8, -0.026), (9, -0.022), (10, -0.02), (11, 0.017), (12, -0.048), (13, 0.018), (14, 0.005), (15, -0.027), (16, -0.039), (17, -0.045), (18, 0.01), (19, 0.02), (20, 0.077), (21, 0.003), (22, -0.071), (23, -0.008), (24, -0.12), (25, -0.043), (26, 0.057), (27, 0.066), (28, -0.141), (29, 0.017), (30, 0.013), (31, 0.08), (32, 0.015), (33, 0.039), (34, 0.094), (35, -0.017), (36, -0.101), (37, 0.057), (38, 0.083), (39, -0.049), (40, -0.188), (41, -0.03), (42, 0.001), (43, -0.098), (44, -0.032), (45, -0.036), (46, -0.111), (47, -0.01), (48, -0.028), (49, -0.065)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95724201 <a title="342-lsi-1" href="../hunch_net-2009/hunch_net-2009-02-16-KDNuggets.html">342 hunch net-2009-02-16-KDNuggets</a></p>
<p>Introduction: Eric Zaetsch points out  KDNuggets  which is a well-developed mailing list/news site with a  KDD  flavor.  This might particularly interest people looking for industrial jobs in machine learning, as the mailing list has many such.</p><p>2 0.80562955 <a title="342-lsi-2" href="../hunch_net-2007/hunch_net-2007-12-17-New_Machine_Learning_mailing_list.html">278 hunch net-2007-12-17-New Machine Learning mailing list</a></p>
<p>Introduction: IMLS  (which is the nonprofit running ICML) has setup a new mailing list for  Machine Learning News .  The list address is ML-news@googlegroups.com, and signup requires a google account (which you can create).  Only members can send messages.</p><p>3 0.66659957 <a title="342-lsi-3" href="../hunch_net-2005/hunch_net-2005-02-19-Machine_learning_reading_groups.html">24 hunch net-2005-02-19-Machine learning reading groups</a></p>
<p>Introduction: Yaroslav collected an extensive list of  machine learning reading groups .</p><p>4 0.58611858 <a title="342-lsi-4" href="../hunch_net-2005/hunch_net-2005-02-02-Kolmogorov_Complexity_and_Googling.html">10 hunch net-2005-02-02-Kolmogorov Complexity and Googling</a></p>
<p>Introduction: Machine learning makes the   New Scientist  . From the article: 
  

COMPUTERS can learn the meaning of words simply by plugging into Google. The finding could bring forward the day that true artificial intelligence is developedâ&euro;Ś. 
But Paul Vitanyi and Rudi Cilibrasi of the National Institute for Mathematics and Computer Science in Amsterdam, the Netherlands, realised that a Google search can be used to measure how closely two words relate to each other. For instance, imagine a computer needs to understand what a hat is.

  
You can read the paper at  KC Google .
 
Hat tip:   Kolmogorov Mailing List 
 
Any thoughts on the paper?</p><p>5 0.48902345 <a title="342-lsi-5" href="../hunch_net-2011/hunch_net-2011-03-27-Vowpal_Wabbit%2C_v5.1.html">428 hunch net-2011-03-27-Vowpal Wabbit, v5.1</a></p>
<p>Introduction: I just created  version 5.1  of  vowpal wabbit .  This almost entirely a bugfix release, so it’s an easy upgrade from v5.0.
 
In addition:
  
 There is now a  mailing list , which I and several other developers are subscribed to. 
 The main website has shifted to the wiki on github.  This means that anyone with a github account can now edit it. 
 I’m planning to give a tutorial tomorrow on it at  eHarmony / the LA machine learning meetup  at 10am.  Drop by if you’re interested. 
  
The status of VW amongst other open source projects has changed.  When VW first came out, it was relatively unique amongst existing projects in terms of features.  At this point, many other projects have started to appreciate the value of the design choices here.  This includes:
  
  Mahout , which now has an SGD implementation. 
  Shogun , where  Soeren  is keen on  incorporating features . 
  LibLinear , where they won the KDD best paper award for  out-of-core learning . 
  
This is expected—any open sourc</p><p>6 0.46551165 <a title="342-lsi-6" href="../hunch_net-2007/hunch_net-2007-04-21-Videolectures.net.html">240 hunch net-2007-04-21-Videolectures.net</a></p>
<p>7 0.44132853 <a title="342-lsi-7" href="../hunch_net-2009/hunch_net-2009-05-17-Server_Update.html">354 hunch net-2009-05-17-Server Update</a></p>
<p>8 0.42861384 <a title="342-lsi-8" href="../hunch_net-2006/hunch_net-2006-04-17-Rexa_is_live.html">173 hunch net-2006-04-17-Rexa is live</a></p>
<p>9 0.41811362 <a title="342-lsi-9" href="../hunch_net-2006/hunch_net-2006-10-04-Health_of_Conferences_Wiki.html">212 hunch net-2006-10-04-Health of Conferences Wiki</a></p>
<p>10 0.41675192 <a title="342-lsi-10" href="../hunch_net-2009/hunch_net-2009-01-08-Predictive_Analytics_World.html">335 hunch net-2009-01-08-Predictive Analytics World</a></p>
<p>11 0.41129583 <a title="342-lsi-11" href="../hunch_net-2011/hunch_net-2011-10-03-Monday_announcements.html">446 hunch net-2011-10-03-Monday announcements</a></p>
<p>12 0.4068175 <a title="342-lsi-12" href="../hunch_net-2013/hunch_net-2013-04-15-NEML_II.html">481 hunch net-2013-04-15-NEML II</a></p>
<p>13 0.40472403 <a title="342-lsi-13" href="../hunch_net-2005/hunch_net-2005-12-09-Machine_Learning_Thoughts.html">137 hunch net-2005-12-09-Machine Learning Thoughts</a></p>
<p>14 0.40213183 <a title="342-lsi-14" href="../hunch_net-2005/hunch_net-2005-01-19-Why_I_decided_to_run_a_weblog..html">1 hunch net-2005-01-19-Why I decided to run a weblog.</a></p>
<p>15 0.37729076 <a title="342-lsi-15" href="../hunch_net-2010/hunch_net-2010-12-02-Traffic_Prediction_Problem.html">418 hunch net-2010-12-02-Traffic Prediction Problem</a></p>
<p>16 0.36731476 <a title="342-lsi-16" href="../hunch_net-2007/hunch_net-2007-06-23-Machine_Learning_Jobs_are_Growing_on_Trees.html">250 hunch net-2007-06-23-Machine Learning Jobs are Growing on Trees</a></p>
<p>17 0.36232615 <a title="342-lsi-17" href="../hunch_net-2011/hunch_net-2011-04-23-ICML_workshops_due.html">433 hunch net-2011-04-23-ICML workshops due</a></p>
<p>18 0.35395807 <a title="342-lsi-18" href="../hunch_net-2005/hunch_net-2005-02-08-Some_Links.html">15 hunch net-2005-02-08-Some Links</a></p>
<p>19 0.3508631 <a title="342-lsi-19" href="../hunch_net-2005/hunch_net-2005-09-19-NIPS_Workshops.html">113 hunch net-2005-09-19-NIPS Workshops</a></p>
<p>20 0.34782398 <a title="342-lsi-20" href="../hunch_net-2008/hunch_net-2008-11-26-Efficient_Reinforcement_Learning_in_MDPs.html">328 hunch net-2008-11-26-Efficient Reinforcement Learning in MDPs</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(27, 0.221), (53, 0.16), (58, 0.388)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.86702657 <a title="342-lda-1" href="../hunch_net-2009/hunch_net-2009-02-16-KDNuggets.html">342 hunch net-2009-02-16-KDNuggets</a></p>
<p>Introduction: Eric Zaetsch points out  KDNuggets  which is a well-developed mailing list/news site with a  KDD  flavor.  This might particularly interest people looking for industrial jobs in machine learning, as the mailing list has many such.</p><p>2 0.8385796 <a title="342-lda-2" href="../hunch_net-2009/hunch_net-2009-04-21-Interesting_Presentations_at_Snowbird.html">349 hunch net-2009-04-21-Interesting Presentations at Snowbird</a></p>
<p>Introduction: Here are a few of presentations interesting me at the  snowbird learning  workshop (which, amusingly, was in Florida with  AIStat ).  
  
  Thomas Breuel  described machine learning problems within OCR and an open source  OCR software/research  platform with modular learning components as well has a 60Million size dataset derived from  Google ‘s scanned books. 
  Kristen Grauman  and  Fei-Fei Li  discussed using active learning with different cost labels and large datasets for  image ontology .  Both of them used  Mechanical Turk  as a  labeling system , which looks to become routine, at least for vision problems. 
  Russ Tedrake  discussed using machine learning for control, with a basic claim that it was the way to go for problems involving a medium  Reynold’s number  such as in bird flight, where simulation is extremely intense. 
  Yann LeCun  presented a poster on an  FPGA for convolutional neural networks  yielding a factor of 100 speedup in processing.   In addition to the graphi</p><p>3 0.80821526 <a title="342-lda-3" href="../hunch_net-2012/hunch_net-2012-07-17-MUCMD_and_BayLearn.html">470 hunch net-2012-07-17-MUCMD and BayLearn</a></p>
<p>Introduction: The workshop on the  Meaningful Use of Complex Medical Data  is happening again, August 9-12 in LA, near  UAI  on Catalina Island August 15-17.  I enjoyed my visit last year, and expect this year to be interesting also.
 
The first  Bay Area Machine Learning Symposium  is August 30 at  Google .  Abstracts are due July 30.</p><p>4 0.68118417 <a title="342-lda-4" href="../hunch_net-2006/hunch_net-2006-01-18-Is_Multitask_Learning_Black-Boxable%3F.html">149 hunch net-2006-01-18-Is Multitask Learning Black-Boxable?</a></p>
<p>Introduction: Multitask learning is the learning to predict multiple outputs given the same input.  Mathematically, we might think of this as trying to learn a function  f:X -> {0,1} n  .  Structured learning is similar at this level of abstraction.  Many people have worked on solving multitask learning (for example  Rich Caruana ) using methods which share an internal representation.  On other words, the the computation and learning  of the  i th prediction is shared with the computation and learning of the  j th prediction.  Another way to ask this question is: can we avoid sharing the internal representation?  
 
For example, it  might  be feasible to solve multitask learning by some process feeding the  i th prediction  f(x) i   into the  j th predictor  f(x,f(x) i ) j  , 
 
If the answer is “no”, then it implies we can not take binary classification as a basic primitive in the process of solving prediction problems.  If the answer is “yes”, then we can reuse binary classification algorithms to</p><p>5 0.5511148 <a title="342-lda-5" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>Introduction: Let’s define a learning problem as making predictions given past data. There are several ways to attack the learning problem which seem to be equivalent to solving the learning problem.
  
  Find the Invariant  This viewpoint says that learning is all about learning (or incorporating) transformations of objects that do not change the correct prediction. The best possible invariant is the one which says “all things of the same class are the same”.  Finding this is equivalent to learning.  This viewpoint is particularly common when working with image features.  
  Feature Selection  This viewpoint says that the way to learn is by finding the right features to input to a learning algorithm.  The best feature is the one which is the class to predict.  Finding this is equivalent to learning for all reasonable learning algorithms.  This viewpoint is common in several  applications of machine learning.  See  Gilad’s and Bianca’s comments . 
  Find the Representation  This is almost the same a</p><p>6 0.53520131 <a title="342-lda-6" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>7 0.52342534 <a title="342-lda-7" href="../hunch_net-2013/hunch_net-2013-06-10-The_Large_Scale_Learning_class_notes.html">483 hunch net-2013-06-10-The Large Scale Learning class notes</a></p>
<p>8 0.52012831 <a title="342-lda-8" href="../hunch_net-2009/hunch_net-2009-08-16-Centmail_comments.html">367 hunch net-2009-08-16-Centmail comments</a></p>
<p>9 0.51872838 <a title="342-lda-9" href="../hunch_net-2005/hunch_net-2005-04-23-Advantages_and_Disadvantages_of_Bayesian_Learning.html">60 hunch net-2005-04-23-Advantages and Disadvantages of Bayesian Learning</a></p>
<p>10 0.51776701 <a title="342-lda-10" href="../hunch_net-2006/hunch_net-2006-01-30-Should_the_Input_Representation_be_a_Vector%3F.html">152 hunch net-2006-01-30-Should the Input Representation be a Vector?</a></p>
<p>11 0.51711434 <a title="342-lda-11" href="../hunch_net-2007/hunch_net-2007-01-10-A_Deep_Belief_Net_Learning_Problem.html">227 hunch net-2007-01-10-A Deep Belief Net Learning Problem</a></p>
<p>12 0.51202869 <a title="342-lda-12" href="../hunch_net-2005/hunch_net-2005-01-24-Holy_grails_of_machine_learning%3F.html">2 hunch net-2005-01-24-Holy grails of machine learning?</a></p>
<p>13 0.50938863 <a title="342-lda-13" href="../hunch_net-2013/hunch_net-2013-01-07-NYU_Large_Scale_Machine_Learning_Class.html">478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</a></p>
<p>14 0.50855255 <a title="342-lda-14" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>15 0.50479466 <a title="342-lda-15" href="../hunch_net-2005/hunch_net-2005-02-01-Watchword%3A_Loss.html">9 hunch net-2005-02-01-Watchword: Loss</a></p>
<p>16 0.50375229 <a title="342-lda-16" href="../hunch_net-2005/hunch_net-2005-11-16-The_Everything_Ensemble_Edge.html">131 hunch net-2005-11-16-The Everything Ensemble Edge</a></p>
<p>17 0.50344348 <a title="342-lda-17" href="../hunch_net-2005/hunch_net-2005-07-10-Thinking_the_Unthought.html">91 hunch net-2005-07-10-Thinking the Unthought</a></p>
<p>18 0.50332135 <a title="342-lda-18" href="../hunch_net-2005/hunch_net-2005-02-23-Problem%3A_Reinforcement_Learning_with_Classification.html">27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</a></p>
<p>19 0.50307399 <a title="342-lda-19" href="../hunch_net-2005/hunch_net-2005-05-06-Don%26%238217%3Bt_mix_the_solution_into_the_problem.html">67 hunch net-2005-05-06-Don&#8217;t mix the solution into the problem</a></p>
<p>20 0.5023241 <a title="342-lda-20" href="../hunch_net-2005/hunch_net-2005-02-03-Learning_Theory%2C_by_assumption.html">12 hunch net-2005-02-03-Learning Theory, by assumption</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
