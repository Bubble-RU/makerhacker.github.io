<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>371 hunch net-2009-09-21-Netflix finishes (and starts)</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2009" href="../home/hunch_net-2009_home.html">hunch_net-2009</a> <a title="hunch_net-2009-371" href="#">hunch_net-2009-371</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>371 hunch net-2009-09-21-Netflix finishes (and starts)</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2009-371-html" href="http://hunch.net/?p=949">html</a></p><p>Introduction: I attended theNetflix prizeceremony this morning. The press conference part
iscovered fine elsewhere, with the basic outcome being thatBellKor's Pragmatic
Chaoswon overThe Ensembleby 15-20minutes, because they were tied in
performance on the ultimate holdout set. I'm sure the individual participants
will have many chances to speak about the solution. One of these is Bell at
theNYAS ML symposium on Nov. 6.Several additional details may interest ML
people.The degree of overfitting exhibited by the difference in performance on
theleaderboard test setand the ultimate hold out set was small, but
determining at .02 to .03%.A tie was possible, because the rules cut off
measurements below the fourth digit based on significance concerns. In
actuality, of course, the scores do differ before rounding, but everyone I
spoke to claimed not to know how. The complete dataset has beenreleased on
UCI, so each team could compute their own score to whatever accuracy desired.I
was impressed by the slick sy</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The press conference part iscovered fine elsewhere, with the basic outcome being thatBellKor's Pragmatic Chaoswon overThe Ensembleby 15-20minutes, because they were tied in performance on the ultimate holdout set. [sent-2, score-0.382]
</p><p>2 The degree of overfitting exhibited by the difference in performance on theleaderboard test setand the ultimate hold out set was small, but determining at . [sent-7, score-0.486]
</p><p>3 I was impressed by the slick systematic uses of SVD mentioned in the technical presentation, as implied by thefirst comment here. [sent-13, score-0.256]
</p><p>4 The amount of programming and time which went into this contest was pretty shocking. [sent-14, score-0.641]
</p><p>5 I was particularly impressed with the amount of effort that went into various techniques for blending results from different systems. [sent-15, score-0.308]
</p><p>6 In this respect, the lack of release of the source code is a little bit disappointing. [sent-16, score-0.326]
</p><p>7 Because squared loss is convex, any two different solutions of similar performance can be linearly blended to yield a mixed solution of superior performance. [sent-20, score-0.499]
</p><p>8 This brings up a basic issue: How should a contest be designed? [sent-24, score-0.447]
</p><p>9 In the main, the finished Netflix contest seems to have been well designed. [sent-25, score-0.447]
</p><p>10 One improvement they are already implementing is asymptopia removal--the contest will award $0. [sent-27, score-0.447]
</p><p>11 MetricOne criticism is that squared loss does not very directly reflect the actual value to Netflix of a particular set of recommendations. [sent-31, score-0.521]
</p><p>12 The degree to which suboptimal squared loss prediction controls suboptimality of a recommendation loss is weak, but it should kick in when squared loss is deeply optimized as happened in this contest. [sent-33, score-0.975]
</p><p>13 So my basic take is that the squared loss metric seems "ok", with the proviso that it could be done better if you start the data collection with some amount of random exploration. [sent-37, score-0.619]
</p><p>14 A good case can be made that this isn't optimal design for a contest where we are trying to learn new things. [sent-39, score-0.507]
</p><p>15 This essentially corresponds to having a "softmax" prize distribution where the distribution to a participantpis according toe-C(winner - p)whereCis a problem dependent constant. [sent-42, score-0.347]
</p><p>16 Source CodeAfter the Netflix prize was going for some time, the programming-time complexity of entering the contest became very formidable. [sent-44, score-0.572]
</p><p>17 The use of convex loss function and requiring participants to publish helped some with this, but it remained quite difficult. [sent-45, score-0.407]
</p><p>18 If the contest required the release of source code as well, I could imagine both lowering the barrier to late entry, and helping advance the field a bit more. [sent-46, score-0.773]
</p><p>19 Of course, it's hard to go halfway with this--if you really want to guarantee that the source code works, you need to make the information exchange interface be the source code itself (which is then compiled and run in a sandbox), rather than labels. [sent-47, score-0.53]
</p><p>20 For the Netflix contest in particular, the contest has educated me a bit about ensemble and SVD-style techniques, and I'm sure it's generally helped crystallize out a set of applicable ML technologies for many people, which I expect to see widely used elsewhere in the future. [sent-50, score-1.263]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('contest', 0.447), ('squared', 0.188), ('loss', 0.175), ('netflix', 0.174), ('ensemble', 0.131), ('code', 0.125), ('prize', 0.125), ('contests', 0.123), ('ultimate', 0.123), ('holdout', 0.123), ('impressed', 0.114), ('source', 0.109), ('amount', 0.109), ('criticism', 0.092), ('release', 0.092), ('elsewhere', 0.092), ('overfitting', 0.087), ('ml', 0.085), ('users', 0.085), ('went', 0.085), ('metric', 0.081), ('distribution', 0.081), ('convex', 0.08), ('mentioned', 0.08), ('helped', 0.08), ('degree', 0.074), ('performance', 0.074), ('experts', 0.073), ('participants', 0.072), ('set', 0.066), ('data', 0.066), ('thefirst', 0.062), ('spoke', 0.062), ('claimed', 0.062), ('chances', 0.062), ('svd', 0.062), ('setand', 0.062), ('halfway', 0.062), ('qualitative', 0.062), ('anonymization', 0.062), ('blended', 0.062), ('functioning', 0.062), ('joins', 0.062), ('nicely', 0.062), ('press', 0.062), ('race', 0.062), ('thenyas', 0.062), ('wherecis', 0.062), ('according', 0.06), ('optimal', 0.06)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="371-tfidf-1" href="../hunch_net-2009/hunch_net-2009-09-21-Netflix_finishes_%28and_starts%29.html">371 hunch net-2009-09-21-Netflix finishes (and starts)</a></p>
<p>Introduction: I attended theNetflix prizeceremony this morning. The press conference part
iscovered fine elsewhere, with the basic outcome being thatBellKor's Pragmatic
Chaoswon overThe Ensembleby 15-20minutes, because they were tied in
performance on the ultimate holdout set. I'm sure the individual participants
will have many chances to speak about the solution. One of these is Bell at
theNYAS ML symposium on Nov. 6.Several additional details may interest ML
people.The degree of overfitting exhibited by the difference in performance on
theleaderboard test setand the ultimate hold out set was small, but
determining at .02 to .03%.A tie was possible, because the rules cut off
measurements below the fourth digit based on significance concerns. In
actuality, of course, the scores do differ before rounding, but everyone I
spoke to claimed not to know how. The complete dataset has beenreleased on
UCI, so each team could compute their own score to whatever accuracy desired.I
was impressed by the slick sy</p><p>2 0.26861179 <a title="371-tfidf-2" href="../hunch_net-2009/hunch_net-2009-06-26-Netflix_nearly_done.html">362 hunch net-2009-06-26-Netflix nearly done</a></p>
<p>Introduction: A $1M qualifying result was achieved on thepublic Netflix test setby a3-way
ensemble team. This is just in time forYehuda's presentation atKDD, which I'm
sure will be one of the best attended ever.This isn't quite over--there are a
few days for another super-conglomerate team to come together and there is
some small chance that the performance is nonrepresentative of the final test
set, but I expect not.Regardless of the final outcome, the biggest lesson for
ML from the Netflix contest has been the formidable performance edge of
ensemble methods.</p><p>3 0.25315103 <a title="371-tfidf-3" href="../hunch_net-2011/hunch_net-2011-04-11-The_Heritage_Health_Prize.html">430 hunch net-2011-04-11-The Heritage Health Prize</a></p>
<p>Introduction: TheHeritage Health Prizeis potentially the largest prediction prize yet at
$3M, which is sure to get many people interested. Several elements of the
competition may be worth discussing.The most straightforward way for HPN to
deploy this predictor is in determining who to cover with insurance. This
might easily cover the costs of running the contest itself, but the value to
the health system of a whole is minimal, as people not covered still exist.
While HPN itself is a provider network, they have active relationships with a
number of insurance companies, and the right to resell any entrant. It's worth
keeping in mind that the research and development may nevertheless end up
being useful in the longer term, especially as entrants also keep the right to
their code.Thejudging metricis something I haven't seen previously. If a
patient has probability 0.5 of being in the hospital 0 days and probability
0.5 of being in the hospital ~53.6 days, the optimal prediction in expectation
is ~6.4 da</p><p>4 0.25191587 <a title="371-tfidf-4" href="../hunch_net-2006/hunch_net-2006-10-02-%241M_Netflix_prediction_contest.html">211 hunch net-2006-10-02-$1M Netflix prediction contest</a></p>
<p>Introduction: Netflix isrunning a contestto improve recommender prediction systems. A 10%
improvement over their current system yields a $1M prize. Failing that, the
best smaller improvement yields a smaller $50K prize. This contest looks quite
real, and the $50K prize money is almost certainly achievable with a bit of
thought. The contest also comes with a dataset which is apparently 2 orders of
magnitude larger than any other public recommendation system datasets.</p><p>5 0.20009868 <a title="371-tfidf-5" href="../hunch_net-2007/hunch_net-2007-08-19-Choice_of_Metrics.html">259 hunch net-2007-08-19-Choice of Metrics</a></p>
<p>Introduction: How do we judge success in Machine Learning? AsAaronnotes, the best way is to
use the loss imposed on you by the world. This turns out to be infeasible
sometimes for various reasons. The ones I've seen are:The learned prediction
is used in some complicated process that does not give the feedback necessary
to understand the prediction's impact on the loss.The prediction is used by
some other system which expects some semantics to the predicted value. This is
similar to the previous example, except that the issue is design modularity
rather than engineering modularity.The correct loss function is simply unknown
(and perhaps unknowable, except by experimentation).In these situations, it's
unclear what metric for evaluation should be chosen. This post has some design
advice for this murkier case. I'm using the word "metric" here to distinguish
the fact that we are considering methods forevaluatingpredictive systems
rather than a loss imposed by the real world or a loss which is optimized b</p><p>6 0.1987461 <a title="371-tfidf-6" href="../hunch_net-2005/hunch_net-2005-11-07-Prediction_Competitions.html">129 hunch net-2005-11-07-Prediction Competitions</a></p>
<p>7 0.18604924 <a title="371-tfidf-7" href="../hunch_net-2009/hunch_net-2009-02-04-Optimal_Proxy_Loss_for_Classification.html">341 hunch net-2009-02-04-Optimal Proxy Loss for Classification</a></p>
<p>8 0.18161148 <a title="371-tfidf-8" href="../hunch_net-2011/hunch_net-2011-03-20-KDD_Cup_2011.html">427 hunch net-2011-03-20-KDD Cup 2011</a></p>
<p>9 0.18142523 <a title="371-tfidf-9" href="../hunch_net-2007/hunch_net-2007-11-28-Computational_Consequences_of_Classification.html">274 hunch net-2007-11-28-Computational Consequences of Classification</a></p>
<p>10 0.16040972 <a title="371-tfidf-10" href="../hunch_net-2008/hunch_net-2008-05-23-Three_levels_of_addressing_the_Netflix_Prize.html">301 hunch net-2008-05-23-Three levels of addressing the Netflix Prize</a></p>
<p>11 0.15911266 <a title="371-tfidf-11" href="../hunch_net-2007/hunch_net-2007-05-12-Loss_Function_Semantics.html">245 hunch net-2007-05-12-Loss Function Semantics</a></p>
<p>12 0.15644918 <a title="371-tfidf-12" href="../hunch_net-2005/hunch_net-2005-02-01-Watchword%3A_Loss.html">9 hunch net-2005-02-01-Watchword: Loss</a></p>
<p>13 0.13794678 <a title="371-tfidf-13" href="../hunch_net-2005/hunch_net-2005-06-08-Question%3A_%26%238220%3BWhen_is_the_right_time_to_insert_the_loss_function%3F%26%238221%3B.html">79 hunch net-2005-06-08-Question: &#8220;When is the right time to insert the loss function?&#8221;</a></p>
<p>14 0.13369729 <a title="371-tfidf-14" href="../hunch_net-2006/hunch_net-2006-07-13-Regression_vs._Classification_as_a_Primitive.html">196 hunch net-2006-07-13-Regression vs. Classification as a Primitive</a></p>
<p>15 0.12628824 <a title="371-tfidf-15" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>16 0.12467615 <a title="371-tfidf-16" href="../hunch_net-2007/hunch_net-2007-08-12-Exponentiated_Gradient.html">258 hunch net-2007-08-12-Exponentiated Gradient</a></p>
<p>17 0.1175951 <a title="371-tfidf-17" href="../hunch_net-2009/hunch_net-2009-07-31-Vowpal_Wabbit_Open_Source_Project.html">365 hunch net-2009-07-31-Vowpal Wabbit Open Source Project</a></p>
<p>18 0.11654381 <a title="371-tfidf-18" href="../hunch_net-2007/hunch_net-2007-11-29-The_Netflix_Crack.html">275 hunch net-2007-11-29-The Netflix Crack</a></p>
<p>19 0.11614064 <a title="371-tfidf-19" href="../hunch_net-2009/hunch_net-2009-10-10-ALT_2009.html">374 hunch net-2009-10-10-ALT 2009</a></p>
<p>20 0.11534981 <a title="371-tfidf-20" href="../hunch_net-2005/hunch_net-2005-09-08-Online_Learning_as_the_Mathematics_of_Accountability.html">109 hunch net-2005-09-08-Online Learning as the Mathematics of Accountability</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.295), (1, -0.093), (2, -0.034), (3, 0.117), (4, 0.206), (5, 0.207), (6, 0.035), (7, 0.028), (8, 0.157), (9, -0.049), (10, -0.266), (11, 0.038), (12, -0.106), (13, 0.09), (14, 0.023), (15, 0.088), (16, -0.089), (17, -0.009), (18, -0.051), (19, 0.019), (20, 0.026), (21, 0.109), (22, 0.084), (23, 0.062), (24, 0.019), (25, -0.007), (26, 0.023), (27, 0.101), (28, -0.041), (29, 0.045), (30, 0.085), (31, -0.04), (32, -0.004), (33, -0.054), (34, 0.006), (35, -0.04), (36, 0.018), (37, 0.04), (38, 0.032), (39, 0.009), (40, -0.02), (41, 0.011), (42, 0.048), (43, 0.02), (44, -0.017), (45, -0.001), (46, -0.008), (47, -0.018), (48, 0.0), (49, 0.004)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96628201 <a title="371-lsi-1" href="../hunch_net-2009/hunch_net-2009-09-21-Netflix_finishes_%28and_starts%29.html">371 hunch net-2009-09-21-Netflix finishes (and starts)</a></p>
<p>Introduction: I attended theNetflix prizeceremony this morning. The press conference part
iscovered fine elsewhere, with the basic outcome being thatBellKor's Pragmatic
Chaoswon overThe Ensembleby 15-20minutes, because they were tied in
performance on the ultimate holdout set. I'm sure the individual participants
will have many chances to speak about the solution. One of these is Bell at
theNYAS ML symposium on Nov. 6.Several additional details may interest ML
people.The degree of overfitting exhibited by the difference in performance on
theleaderboard test setand the ultimate hold out set was small, but
determining at .02 to .03%.A tie was possible, because the rules cut off
measurements below the fourth digit based on significance concerns. In
actuality, of course, the scores do differ before rounding, but everyone I
spoke to claimed not to know how. The complete dataset has beenreleased on
UCI, so each team could compute their own score to whatever accuracy desired.I
was impressed by the slick sy</p><p>2 0.77759683 <a title="371-lsi-2" href="../hunch_net-2011/hunch_net-2011-04-11-The_Heritage_Health_Prize.html">430 hunch net-2011-04-11-The Heritage Health Prize</a></p>
<p>Introduction: TheHeritage Health Prizeis potentially the largest prediction prize yet at
$3M, which is sure to get many people interested. Several elements of the
competition may be worth discussing.The most straightforward way for HPN to
deploy this predictor is in determining who to cover with insurance. This
might easily cover the costs of running the contest itself, but the value to
the health system of a whole is minimal, as people not covered still exist.
While HPN itself is a provider network, they have active relationships with a
number of insurance companies, and the right to resell any entrant. It's worth
keeping in mind that the research and development may nevertheless end up
being useful in the longer term, especially as entrants also keep the right to
their code.Thejudging metricis something I haven't seen previously. If a
patient has probability 0.5 of being in the hospital 0 days and probability
0.5 of being in the hospital ~53.6 days, the optimal prediction in expectation
is ~6.4 da</p><p>3 0.75031739 <a title="371-lsi-3" href="../hunch_net-2009/hunch_net-2009-06-26-Netflix_nearly_done.html">362 hunch net-2009-06-26-Netflix nearly done</a></p>
<p>Introduction: A $1M qualifying result was achieved on thepublic Netflix test setby a3-way
ensemble team. This is just in time forYehuda's presentation atKDD, which I'm
sure will be one of the best attended ever.This isn't quite over--there are a
few days for another super-conglomerate team to come together and there is
some small chance that the performance is nonrepresentative of the final test
set, but I expect not.Regardless of the final outcome, the biggest lesson for
ML from the Netflix contest has been the formidable performance edge of
ensemble methods.</p><p>4 0.73628241 <a title="371-lsi-4" href="../hunch_net-2006/hunch_net-2006-10-02-%241M_Netflix_prediction_contest.html">211 hunch net-2006-10-02-$1M Netflix prediction contest</a></p>
<p>Introduction: Netflix isrunning a contestto improve recommender prediction systems. A 10%
improvement over their current system yields a $1M prize. Failing that, the
best smaller improvement yields a smaller $50K prize. This contest looks quite
real, and the $50K prize money is almost certainly achievable with a bit of
thought. The contest also comes with a dataset which is apparently 2 orders of
magnitude larger than any other public recommendation system datasets.</p><p>5 0.67131412 <a title="371-lsi-5" href="../hunch_net-2005/hunch_net-2005-11-07-Prediction_Competitions.html">129 hunch net-2005-11-07-Prediction Competitions</a></p>
<p>Introduction: There are two prediction competitions currently in the air.ThePerformance
Prediction ChallengebyIsabelle Guyon. Good entries minimize a weighted 0/1
loss + the difference between a prediction of this loss and the observed truth
on 5 datasets. Isabelle tells me all of the problems are "real world" and the
test datasets are large enough (17K minimum) that the winner should be well
determined by ability rather than luck. This is due March 1.ThePredictive
Uncertainty ChallengebyGavin Cawley. Good entries minimize log loss on real
valued output variables for one synthetic and 3 "real" datasets related to
atmospheric prediction. The use of log loss (which can be infinite and hence
is never convergent) and smaller test sets of size 1K to 7K examples makes the
winner of this contest more luck dependent. Nevertheless, the contest may be
of some interest particularly to the branch of learning (typically Bayes
learning) which prefers to optimize log loss.May the best predictor win.</p><p>6 0.61560184 <a title="371-lsi-6" href="../hunch_net-2007/hunch_net-2007-08-19-Choice_of_Metrics.html">259 hunch net-2007-08-19-Choice of Metrics</a></p>
<p>7 0.6101777 <a title="371-lsi-7" href="../hunch_net-2007/hunch_net-2007-11-29-The_Netflix_Crack.html">275 hunch net-2007-11-29-The Netflix Crack</a></p>
<p>8 0.59928423 <a title="371-lsi-8" href="../hunch_net-2007/hunch_net-2007-04-18-%2450K_Spock_Challenge.html">239 hunch net-2007-04-18-$50K Spock Challenge</a></p>
<p>9 0.55730623 <a title="371-lsi-9" href="../hunch_net-2011/hunch_net-2011-03-20-KDD_Cup_2011.html">427 hunch net-2011-03-20-KDD Cup 2011</a></p>
<p>10 0.50511092 <a title="371-lsi-10" href="../hunch_net-2005/hunch_net-2005-10-08-We_have_a_winner.html">119 hunch net-2005-10-08-We have a winner</a></p>
<p>11 0.48377717 <a title="371-lsi-11" href="../hunch_net-2005/hunch_net-2005-06-08-Question%3A_%26%238220%3BWhen_is_the_right_time_to_insert_the_loss_function%3F%26%238221%3B.html">79 hunch net-2005-06-08-Question: &#8220;When is the right time to insert the loss function?&#8221;</a></p>
<p>12 0.48362821 <a title="371-lsi-12" href="../hunch_net-2008/hunch_net-2008-05-23-Three_levels_of_addressing_the_Netflix_Prize.html">301 hunch net-2008-05-23-Three levels of addressing the Netflix Prize</a></p>
<p>13 0.48308823 <a title="371-lsi-13" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>14 0.47737628 <a title="371-lsi-14" href="../hunch_net-2007/hunch_net-2007-11-28-Computational_Consequences_of_Classification.html">274 hunch net-2007-11-28-Computational Consequences of Classification</a></p>
<p>15 0.46751308 <a title="371-lsi-15" href="../hunch_net-2009/hunch_net-2009-02-04-Optimal_Proxy_Loss_for_Classification.html">341 hunch net-2009-02-04-Optimal Proxy Loss for Classification</a></p>
<p>16 0.46056607 <a title="371-lsi-16" href="../hunch_net-2005/hunch_net-2005-02-01-Watchword%3A_Loss.html">9 hunch net-2005-02-01-Watchword: Loss</a></p>
<p>17 0.4584941 <a title="371-lsi-17" href="../hunch_net-2007/hunch_net-2007-05-12-Loss_Function_Semantics.html">245 hunch net-2007-05-12-Loss Function Semantics</a></p>
<p>18 0.45408326 <a title="371-lsi-18" href="../hunch_net-2009/hunch_net-2009-10-10-ALT_2009.html">374 hunch net-2009-10-10-ALT 2009</a></p>
<p>19 0.44484976 <a title="371-lsi-19" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>20 0.44219622 <a title="371-lsi-20" href="../hunch_net-2009/hunch_net-2009-01-19-Netflix_prize_within_epsilon.html">336 hunch net-2009-01-19-Netflix prize within epsilon</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.031), (29, 0.014), (35, 0.096), (42, 0.222), (45, 0.041), (47, 0.191), (50, 0.024), (55, 0.016), (61, 0.013), (64, 0.011), (68, 0.063), (69, 0.027), (74, 0.084), (76, 0.026), (88, 0.021), (95, 0.049)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.93233258 <a title="371-lda-1" href="../hunch_net-2013/hunch_net-2013-11-09-Graduates_and_Postdocs.html">490 hunch net-2013-11-09-Graduates and Postdocs</a></p>
<p>Introduction: Several strong graduates are on the job market this year.Alekh Agarwalmade
themost scalable public learning algorithmas an intern two years ago. He has a
deep and broad understanding of optimization and learning as well as the
ability and will to make things happen programming-wise. I've been privileged
to have Alekh visiting me in NY where he will be sorely missed.John
DuchicreatedAdagradwhich is a commonly helpful improvement over online
gradient descent that is seeing wide adoption, including inVowpal Wabbit. He
has a similarly deep and broad understanding of optimization and learning with
significant industry experience atGoogle. Alekh and John have often coauthored
together.Stephane Rossvisited me a year ago over the summer, implementing many
new algorithms and working out the firstscale free online update rulewhich is
now the default in Vowpal Wabbit. Stephane isnoton the market--Google robbed
the cradle successfullyI'm sure that he will do great things.Anna
Choromanskavisited me</p><p>same-blog 2 0.90762055 <a title="371-lda-2" href="../hunch_net-2009/hunch_net-2009-09-21-Netflix_finishes_%28and_starts%29.html">371 hunch net-2009-09-21-Netflix finishes (and starts)</a></p>
<p>Introduction: I attended theNetflix prizeceremony this morning. The press conference part
iscovered fine elsewhere, with the basic outcome being thatBellKor's Pragmatic
Chaoswon overThe Ensembleby 15-20minutes, because they were tied in
performance on the ultimate holdout set. I'm sure the individual participants
will have many chances to speak about the solution. One of these is Bell at
theNYAS ML symposium on Nov. 6.Several additional details may interest ML
people.The degree of overfitting exhibited by the difference in performance on
theleaderboard test setand the ultimate hold out set was small, but
determining at .02 to .03%.A tie was possible, because the rules cut off
measurements below the fourth digit based on significance concerns. In
actuality, of course, the scores do differ before rounding, but everyone I
spoke to claimed not to know how. The complete dataset has beenreleased on
UCI, so each team could compute their own score to whatever accuracy desired.I
was impressed by the slick sy</p><p>3 0.9074356 <a title="371-lda-3" href="../hunch_net-2007/hunch_net-2007-06-13-Not_Posting.html">246 hunch net-2007-06-13-Not Posting</a></p>
<p>Introduction: If you have been disappointed by the lack of a post for the last month,
considercontributing your own(I've been busy+uninspired). Also, keep in mind
that there is a community of machine learning blogs (see the sidebar).</p><p>4 0.87191582 <a title="371-lda-4" href="../hunch_net-2012/hunch_net-2012-10-18-7th_Annual_Machine_Learning_Symposium.html">474 hunch net-2012-10-18-7th Annual Machine Learning Symposium</a></p>
<p>Introduction: A reminder that theNew York Academy of Scienceswill be hosting the7th Annual
Machine Learning Symposiumtomorrow from 9:30am.The main program will feature
invited talks fromPeter Bartlett,William Freeman, andVladimir Vapnik, along
with numerous spotlight talks and a poster session. Following the main
program,hackNYandMicrosoft Researchare sponsoring a networking hour with talks
from machine learning practitioners at NYC startups
(specificallybit.ly,Buzzfeed,Chartbeat, andSense Networks,Visual Revenue).
This should be of great interest to everyone considering working in machine
learning.</p><p>5 0.86052269 <a title="371-lda-5" href="../hunch_net-2005/hunch_net-2005-02-25-Why_Papers%3F.html">30 hunch net-2005-02-25-Why Papers?</a></p>
<p>Introduction: Makc asked a goodquestionin comments--"Why bother to make a paper, at all?"
There are several reasons for writing papers which may not be immediately
obvious to people not in academia.The basic idea is that papers have
considerably more utility than the obvious "present an idea".Papers are a
formalized units of work. Academics (especially young ones) are often judged
on the number of papers they produce.Papers have a formalized method of citing
and crediting other--the bibliography. Academics (especially older ones) are
often judged on the number of citations they receive.Papers enable a "more
fair" anonymous review. Conferences receivemanypapers, from which a subset are
selected. Discussion forums are inherently not anonymous for anyone who wants
to build a reputation for good work.Papers are an excuse to meet your friends.
Papers are the content of conferences, but much of what you do is talk to
friends about interesting problems while there. Sometimes you even solve
them.Papers are</p><p>6 0.79285765 <a title="371-lda-6" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">23 hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<p>7 0.78996974 <a title="371-lda-7" href="../hunch_net-2012/hunch_net-2012-05-02-ICML%3A_Behind_the_Scenes.html">463 hunch net-2012-05-02-ICML: Behind the Scenes</a></p>
<p>8 0.7893647 <a title="371-lda-8" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<p>9 0.78403044 <a title="371-lda-9" href="../hunch_net-2005/hunch_net-2005-09-12-Fast_Gradient_Descent.html">111 hunch net-2005-09-12-Fast Gradient Descent</a></p>
<p>10 0.7829954 <a title="371-lda-10" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>11 0.7821824 <a title="371-lda-11" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>12 0.78195632 <a title="371-lda-12" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>13 0.77721465 <a title="371-lda-13" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>14 0.77490246 <a title="371-lda-14" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>15 0.77489942 <a title="371-lda-15" href="../hunch_net-2008/hunch_net-2008-05-23-Three_levels_of_addressing_the_Netflix_Prize.html">301 hunch net-2008-05-23-Three levels of addressing the Netflix Prize</a></p>
<p>16 0.77253622 <a title="371-lda-16" href="../hunch_net-2010/hunch_net-2010-12-26-NIPS_2010.html">420 hunch net-2010-12-26-NIPS 2010</a></p>
<p>17 0.77218854 <a title="371-lda-17" href="../hunch_net-2007/hunch_net-2007-01-02-Retrospective.html">225 hunch net-2007-01-02-Retrospective</a></p>
<p>18 0.77201152 <a title="371-lda-18" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>19 0.7719537 <a title="371-lda-19" href="../hunch_net-2005/hunch_net-2005-03-17-Going_all_the_Way%2C_Sometimes.html">42 hunch net-2005-03-17-Going all the Way, Sometimes</a></p>
<p>20 0.76877534 <a title="371-lda-20" href="../hunch_net-2007/hunch_net-2007-08-19-Choice_of_Metrics.html">259 hunch net-2007-08-19-Choice of Metrics</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
