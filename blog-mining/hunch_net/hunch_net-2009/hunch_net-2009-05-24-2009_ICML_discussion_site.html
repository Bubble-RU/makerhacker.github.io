<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>356 hunch net-2009-05-24-2009 ICML discussion site</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2009" href="../home/hunch_net-2009_home.html">hunch_net-2009</a> <a title="hunch_net-2009-356" href="#">hunch_net-2009-356</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>356 hunch net-2009-05-24-2009 ICML discussion site</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2009-356-html" href="http://hunch.net/?p=752">html</a></p><p>Introduction: Mark Reid  has setup a  discussion site for ICML papers  again this year and  Monica Dinculescu  has linked it in from the ICML site.  Last year’s attempt appears to have been an acceptable but not wild success as a little bit of fruitful discussion occurred.  I’m hoping this year will be a bit more of a success—please don’t be shy   
 
I’d like to also point out that  ICML ‘s early  registration  deadline has a few hours left, while  UAI ‘s and  COLT ‘s are in a week.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Mark Reid  has setup a  discussion site for ICML papers  again this year and  Monica Dinculescu  has linked it in from the ICML site. [sent-1, score-1.069]
</p><p>2 Last year’s attempt appears to have been an acceptable but not wild success as a little bit of fruitful discussion occurred. [sent-2, score-1.7]
</p><p>3 I’m hoping this year will be a bit more of a success—please don’t be shy      I’d like to also point out that  ICML ‘s early  registration  deadline has a few hours left, while  UAI ‘s and  COLT ‘s are in a week. [sent-3, score-1.687]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('fruitful', 0.265), ('success', 0.255), ('icml', 0.247), ('shy', 0.246), ('linked', 0.246), ('wild', 0.246), ('year', 0.239), ('hoping', 0.221), ('discussion', 0.211), ('acceptable', 0.205), ('reid', 0.205), ('hours', 0.198), ('week', 0.188), ('mark', 0.183), ('registration', 0.165), ('bit', 0.16), ('site', 0.157), ('left', 0.154), ('setup', 0.148), ('attempt', 0.144), ('uai', 0.144), ('early', 0.144), ('please', 0.135), ('deadline', 0.135), ('appears', 0.121), ('colt', 0.109), ('little', 0.093), ('last', 0.089), ('point', 0.079), ('papers', 0.068), ('like', 0.052), ('also', 0.048)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="356-tfidf-1" href="../hunch_net-2009/hunch_net-2009-05-24-2009_ICML_discussion_site.html">356 hunch net-2009-05-24-2009 ICML discussion site</a></p>
<p>Introduction: Mark Reid  has setup a  discussion site for ICML papers  again this year and  Monica Dinculescu  has linked it in from the ICML site.  Last year’s attempt appears to have been an acceptable but not wild success as a little bit of fruitful discussion occurred.  I’m hoping this year will be a bit more of a success—please don’t be shy   
 
I’d like to also point out that  ICML ‘s early  registration  deadline has a few hours left, while  UAI ‘s and  COLT ‘s are in a week.</p><p>2 0.24471039 <a title="356-tfidf-2" href="../hunch_net-2008/hunch_net-2008-06-30-ICML_has_a_comment_system.html">305 hunch net-2008-06-30-ICML has a comment system</a></p>
<p>Introduction: Mark Reid  has stepped up and created a  comment system for ICML papers  which  Greger Linden  has tightly integrated.  
 
My understanding is that Mark spent quite  a bit of time on the details, and there are some cool features like working latex math mode.  This is an excellent chance for the ICML community to experiment with making ICML year-round, so I hope it works out.  Please do consider experimenting with it.</p><p>3 0.24241388 <a title="356-tfidf-3" href="../hunch_net-2007/hunch_net-2007-07-12-ICML_Trends.html">254 hunch net-2007-07-12-ICML Trends</a></p>
<p>Introduction: Mark Reid  did a post on  ICML trends  that I found interesting.</p><p>4 0.18833491 <a title="356-tfidf-4" href="../hunch_net-2010/hunch_net-2010-06-20-2010_ICML_discussion_site.html">401 hunch net-2010-06-20-2010 ICML discussion site</a></p>
<p>Introduction: A substantial difficulty with the 2009 and 2008  ICML discussion system  was a communication vacuum, where authors were not informed of comments, and commenters were not informed of responses to their comments without explicit monitoring.   Mark Reid  has setup a  new discussion system for 2010  with the goal of addressing this.
 
Mark didn’t want to make it to intrusive, so you must opt-in.  As an author,  find your paper  and “Subscribe by email” to the comments.  As a commenter, you have the option of providing an email for follow-up notification.</p><p>5 0.17990722 <a title="356-tfidf-5" href="../hunch_net-2012/hunch_net-2012-06-29-ICML_survey_and_comments.html">468 hunch net-2012-06-29-ICML survey and comments</a></p>
<p>Introduction: Just about nothing could keep me from attending  ICML , except for  Dora  who arrived on Monday.  Consequently, I have only secondhand reports that the conference is going well.
 
For those who are remote (like me) or after the conference (like everyone),  Mark Reid  has setup the  ICML discussion  site where you can comment on any paper or subscribe to papers.  Authors are automatically subscribed to their own papers, so it should be possible to have a discussion significantly after the fact, as people desire.
 
We also conducted a survey before the conference and have the  survey results  now.  This can be compared with the  ICML 2010 survey results .  Looking at the comparable questions, we can sometimes order the answers to have scores ranging from 0 to 3 or 0 to 4 with 3 or 4 being best and 0 worst, then compute the average difference between 2012 and 2010.
 
Glancing through them, I see:
  
 Most people found the papers they reviewed a good fit for their expertise (-.037 w.r.t 20</p><p>6 0.15669054 <a title="356-tfidf-6" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>7 0.15305531 <a title="356-tfidf-7" href="../hunch_net-2013/hunch_net-2013-05-04-COLT_and_ICML_registration.html">482 hunch net-2013-05-04-COLT and ICML registration</a></p>
<p>8 0.1512603 <a title="356-tfidf-8" href="../hunch_net-2012/hunch_net-2012-05-12-ICML_accepted_papers_and_early_registration.html">465 hunch net-2012-05-12-ICML accepted papers and early registration</a></p>
<p>9 0.14610928 <a title="356-tfidf-9" href="../hunch_net-2012/hunch_net-2012-08-27-NYAS_ML_2012_and_ICML_2013.html">472 hunch net-2012-08-27-NYAS ML 2012 and ICML 2013</a></p>
<p>10 0.13282849 <a title="356-tfidf-10" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<p>11 0.11748701 <a title="356-tfidf-11" href="../hunch_net-2011/hunch_net-2011-10-10-ML_Symposium_and_ICML_details.html">447 hunch net-2011-10-10-ML Symposium and ICML details</a></p>
<p>12 0.10474788 <a title="356-tfidf-12" href="../hunch_net-2010/hunch_net-2010-01-19-Deadline_Season%2C_2010.html">387 hunch net-2010-01-19-Deadline Season, 2010</a></p>
<p>13 0.10339325 <a title="356-tfidf-13" href="../hunch_net-2005/hunch_net-2005-07-04-The_Health_of_COLT.html">89 hunch net-2005-07-04-The Health of COLT</a></p>
<p>14 0.096803524 <a title="356-tfidf-14" href="../hunch_net-2010/hunch_net-2010-04-24-COLT_Treasurer_is_now_Phil_Long.html">394 hunch net-2010-04-24-COLT Treasurer is now Phil Long</a></p>
<p>15 0.094039753 <a title="356-tfidf-15" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>16 0.090268455 <a title="356-tfidf-16" href="../hunch_net-2005/hunch_net-2005-12-29-Deadline_Season.html">145 hunch net-2005-12-29-Deadline Season</a></p>
<p>17 0.089218929 <a title="356-tfidf-17" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>18 0.087293446 <a title="356-tfidf-18" href="../hunch_net-2007/hunch_net-2007-04-30-COLT_2007.html">242 hunch net-2007-04-30-COLT 2007</a></p>
<p>19 0.082798734 <a title="356-tfidf-19" href="../hunch_net-2009/hunch_net-2009-10-26-NIPS_workshops.html">375 hunch net-2009-10-26-NIPS workshops</a></p>
<p>20 0.082592979 <a title="356-tfidf-20" href="../hunch_net-2005/hunch_net-2005-10-03-Not_ICML.html">117 hunch net-2005-10-03-Not ICML</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.125), (1, -0.196), (2, 0.016), (3, -0.106), (4, 0.009), (5, -0.093), (6, -0.064), (7, -0.059), (8, -0.068), (9, -0.05), (10, 0.0), (11, -0.019), (12, -0.221), (13, 0.092), (14, 0.216), (15, -0.028), (16, -0.159), (17, -0.198), (18, -0.053), (19, 0.071), (20, 0.035), (21, -0.098), (22, 0.012), (23, 0.064), (24, -0.093), (25, -0.078), (26, 0.067), (27, -0.137), (28, 0.021), (29, -0.065), (30, 0.039), (31, -0.072), (32, 0.001), (33, -0.002), (34, -0.053), (35, -0.032), (36, -0.016), (37, -0.003), (38, -0.031), (39, 0.001), (40, 0.037), (41, -0.063), (42, -0.079), (43, 0.038), (44, -0.052), (45, 0.006), (46, -0.037), (47, 0.005), (48, 0.129), (49, -0.042)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98820668 <a title="356-lsi-1" href="../hunch_net-2009/hunch_net-2009-05-24-2009_ICML_discussion_site.html">356 hunch net-2009-05-24-2009 ICML discussion site</a></p>
<p>Introduction: Mark Reid  has setup a  discussion site for ICML papers  again this year and  Monica Dinculescu  has linked it in from the ICML site.  Last year’s attempt appears to have been an acceptable but not wild success as a little bit of fruitful discussion occurred.  I’m hoping this year will be a bit more of a success—please don’t be shy   
 
I’d like to also point out that  ICML ‘s early  registration  deadline has a few hours left, while  UAI ‘s and  COLT ‘s are in a week.</p><p>2 0.8382687 <a title="356-lsi-2" href="../hunch_net-2008/hunch_net-2008-06-30-ICML_has_a_comment_system.html">305 hunch net-2008-06-30-ICML has a comment system</a></p>
<p>Introduction: Mark Reid  has stepped up and created a  comment system for ICML papers  which  Greger Linden  has tightly integrated.  
 
My understanding is that Mark spent quite  a bit of time on the details, and there are some cool features like working latex math mode.  This is an excellent chance for the ICML community to experiment with making ICML year-round, so I hope it works out.  Please do consider experimenting with it.</p><p>3 0.76457584 <a title="356-lsi-3" href="../hunch_net-2007/hunch_net-2007-07-12-ICML_Trends.html">254 hunch net-2007-07-12-ICML Trends</a></p>
<p>Introduction: Mark Reid  did a post on  ICML trends  that I found interesting.</p><p>4 0.75413239 <a title="356-lsi-4" href="../hunch_net-2010/hunch_net-2010-06-20-2010_ICML_discussion_site.html">401 hunch net-2010-06-20-2010 ICML discussion site</a></p>
<p>Introduction: A substantial difficulty with the 2009 and 2008  ICML discussion system  was a communication vacuum, where authors were not informed of comments, and commenters were not informed of responses to their comments without explicit monitoring.   Mark Reid  has setup a  new discussion system for 2010  with the goal of addressing this.
 
Mark didn’t want to make it to intrusive, so you must opt-in.  As an author,  find your paper  and “Subscribe by email” to the comments.  As a commenter, you have the option of providing an email for follow-up notification.</p><p>5 0.66792721 <a title="356-lsi-5" href="../hunch_net-2005/hunch_net-2005-10-03-Not_ICML.html">117 hunch net-2005-10-03-Not ICML</a></p>
<p>Introduction: Alex Smola  showed me this  ICML 2006  webpage.  This is  NOT  the ICML we know, but rather some people at “Enformatika”.  Investigation shows that they registered with an anonymous yahoo email account from  dotregistrar.com  the “Home of the $6.79 wholesale domain!” and their nameservers are by  Turkticaret , a Turkish internet company.
 
It appears the website has since been altered to “ ICNL ” (the above link uses the google cache).
 
They say that imitation is the sincerest form of flattery, so the organizers of the real  ICML 2006  must feel quite flattered.</p><p>6 0.59935975 <a title="356-lsi-6" href="../hunch_net-2012/hunch_net-2012-06-29-ICML_survey_and_comments.html">468 hunch net-2012-06-29-ICML survey and comments</a></p>
<p>7 0.57725883 <a title="356-lsi-7" href="../hunch_net-2012/hunch_net-2012-08-27-NYAS_ML_2012_and_ICML_2013.html">472 hunch net-2012-08-27-NYAS ML 2012 and ICML 2013</a></p>
<p>8 0.55621493 <a title="356-lsi-8" href="../hunch_net-2011/hunch_net-2011-10-10-ML_Symposium_and_ICML_details.html">447 hunch net-2011-10-10-ML Symposium and ICML details</a></p>
<p>9 0.54521507 <a title="356-lsi-9" href="../hunch_net-2013/hunch_net-2013-05-04-COLT_and_ICML_registration.html">482 hunch net-2013-05-04-COLT and ICML registration</a></p>
<p>10 0.48835671 <a title="356-lsi-10" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>11 0.46150208 <a title="356-lsi-11" href="../hunch_net-2012/hunch_net-2012-05-12-ICML_accepted_papers_and_early_registration.html">465 hunch net-2012-05-12-ICML accepted papers and early registration</a></p>
<p>12 0.43521938 <a title="356-lsi-12" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<p>13 0.42028287 <a title="356-lsi-13" href="../hunch_net-2006/hunch_net-2006-06-15-IJCAI_is_out_of_season.html">184 hunch net-2006-06-15-IJCAI is out of season</a></p>
<p>14 0.40608954 <a title="356-lsi-14" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>15 0.40110776 <a title="356-lsi-15" href="../hunch_net-2010/hunch_net-2010-10-29-To_Vidoelecture_or_not.html">416 hunch net-2010-10-29-To Vidoelecture or not</a></p>
<p>16 0.39599109 <a title="356-lsi-16" href="../hunch_net-2005/hunch_net-2005-03-08-Fast_Physics_for_Learning.html">37 hunch net-2005-03-08-Fast Physics for Learning</a></p>
<p>17 0.39334384 <a title="356-lsi-17" href="../hunch_net-2010/hunch_net-2010-04-24-COLT_Treasurer_is_now_Phil_Long.html">394 hunch net-2010-04-24-COLT Treasurer is now Phil Long</a></p>
<p>18 0.38089299 <a title="356-lsi-18" href="../hunch_net-2008/hunch_net-2008-11-09-A_Healthy__COLT.html">324 hunch net-2008-11-09-A Healthy  COLT</a></p>
<p>19 0.37234145 <a title="356-lsi-19" href="../hunch_net-2007/hunch_net-2007-04-30-COLT_2007.html">242 hunch net-2007-04-30-COLT 2007</a></p>
<p>20 0.36518267 <a title="356-lsi-20" href="../hunch_net-2009/hunch_net-2009-07-11-Interesting_papers_at_KDD.html">364 hunch net-2009-07-11-Interesting papers at KDD</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(27, 0.082), (53, 0.118), (55, 0.254), (56, 0.393)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.86771142 <a title="356-lda-1" href="../hunch_net-2009/hunch_net-2009-05-24-2009_ICML_discussion_site.html">356 hunch net-2009-05-24-2009 ICML discussion site</a></p>
<p>Introduction: Mark Reid  has setup a  discussion site for ICML papers  again this year and  Monica Dinculescu  has linked it in from the ICML site.  Last year’s attempt appears to have been an acceptable but not wild success as a little bit of fruitful discussion occurred.  I’m hoping this year will be a bit more of a success—please don’t be shy   
 
I’d like to also point out that  ICML ‘s early  registration  deadline has a few hours left, while  UAI ‘s and  COLT ‘s are in a week.</p><p>2 0.84034252 <a title="356-lda-2" href="../hunch_net-2006/hunch_net-2006-06-25-Presentation_of_Proofs_is_Hard..html">187 hunch net-2006-06-25-Presentation of Proofs is Hard.</a></p>
<p>Introduction: When presenting part of the  Reinforcement Learning theory tutorial  at  ICML 2006 , I was forcibly reminded of this.
 
There are several difficulties.
  
  When creating the presentation, the correct level of detail is tricky.  With too much detail, the proof takes too much time and people may be lost to boredom.  With too little detail, the steps of the proof involve too-great a jump. This is very difficult to judge.
 
 What may be an easy step in the careful thought of a quiet room is not so easy when you are occupied by the process of presentation. 
 What may be easy after having gone over this (and other) proofs is not so easy to follow in the first pass by a viewer. 
 

  These problems seem only correctable by process of repeated test-and-revise.
 
 When presenting the proof, simply speaking with sufficient precision is substantially harder than in normal conversation (where precision is not so critical).  Practice can help here. 
 When presenting the proof, going at the right p</p><p>3 0.67165679 <a title="356-lda-3" href="../hunch_net-2007/hunch_net-2007-06-23-Machine_Learning_Jobs_are_Growing_on_Trees.html">250 hunch net-2007-06-23-Machine Learning Jobs are Growing on Trees</a></p>
<p>Introduction: The consensus of several discussions at ICML is that the number of jobs for people knowing machine learning well substantially exceeds supply.  This is my experience as well.  Demand comes from many places, but I’ve seen particularly strong demand from trading companies and internet startups.
 
Like all interest bursts, this one will probably pass because of economic recession or other distractions.  Nevertheless, the general outlook for machine learning in business seems to be good.  Machine learning is all about optimization when there is uncertainty and lots of data.  The quantity of data available is growing quickly as computer-run processes and sensors become more common, and the quality of the data is dropping since there is little editorial control in it’s collection.  Machine Learning is a difficult subject to master (*), so those who do should remain in demand over the long term.
 
(*) In fact, it would be reasonable to claim that no one has mastered it—there are just some peo</p><p>4 0.64810467 <a title="356-lda-4" href="../hunch_net-2012/hunch_net-2012-03-24-David_Waltz.html">460 hunch net-2012-03-24-David Waltz</a></p>
<p>Introduction: has  died .   He lived a full life.  I know him personally as a founder of the  Center for Computational Learning Systems  and the  New York Machine Learning Symposium , both of which have sheltered and promoted the advancement of machine learning.  I expect much of the New York area machine learning community will miss him, as well as many others around the world.</p><p>5 0.62616014 <a title="356-lda-5" href="../hunch_net-2008/hunch_net-2008-07-04-More_Presentation_Preparation.html">307 hunch net-2008-07-04-More Presentation Preparation</a></p>
<p>Introduction: We’ve discussed  presentation preparation before , but I have one more thing to add:  transitioning .  For a research presentation, it is substantially helpful for the audience if transitions are clear.  A common outline for a research presentation in machine leanring is:
  
  The problem .  Presentations which don’t describe the problem almost immediately lose people, because the context is missing to understand the detail. 
  Prior relevant work .  In many cases, a paper builds on some previous bit of work which must be understood in order to understand what the paper does.  A common failure mode seems to be spending too much time on prior work.  Discuss just the relevant aspects of prior work in the language of your work.  Sometimes this is missing when unneeded. 
  What we did . For theory papers in particular, it is often not possible to really cover the details.  Prioritizing what you present can be very important. 
  How it worked .  Many papers in Machine Learning have some sor</p><p>6 0.59948158 <a title="356-lda-6" href="../hunch_net-2006/hunch_net-2006-08-10-Precision_is_not_accuracy.html">202 hunch net-2006-08-10-Precision is not accuracy</a></p>
<p>7 0.58277142 <a title="356-lda-7" href="../hunch_net-2008/hunch_net-2008-12-12-Summer_Conferences.html">331 hunch net-2008-12-12-Summer Conferences</a></p>
<p>8 0.56606257 <a title="356-lda-8" href="../hunch_net-2005/hunch_net-2005-07-07-The_Limits_of_Learning_Theory.html">90 hunch net-2005-07-07-The Limits of Learning Theory</a></p>
<p>9 0.55671549 <a title="356-lda-9" href="../hunch_net-2005/hunch_net-2005-02-15-ESPgame_and_image_labeling.html">20 hunch net-2005-02-15-ESPgame and image labeling</a></p>
<p>10 0.55292243 <a title="356-lda-10" href="../hunch_net-2007/hunch_net-2007-11-05-CMU_wins_DARPA_Urban_Challenge.html">271 hunch net-2007-11-05-CMU wins DARPA Urban Challenge</a></p>
<p>11 0.54891318 <a title="356-lda-11" href="../hunch_net-2010/hunch_net-2010-01-19-Deadline_Season%2C_2010.html">387 hunch net-2010-01-19-Deadline Season, 2010</a></p>
<p>12 0.5485568 <a title="356-lda-12" href="../hunch_net-2011/hunch_net-2011-10-24-2011_ML_symposium_and_the_bears.html">448 hunch net-2011-10-24-2011 ML symposium and the bears</a></p>
<p>13 0.54459262 <a title="356-lda-13" href="../hunch_net-2011/hunch_net-2011-10-03-Monday_announcements.html">446 hunch net-2011-10-03-Monday announcements</a></p>
<p>14 0.54419976 <a title="356-lda-14" href="../hunch_net-2008/hunch_net-2008-05-25-Inappropriate_Mathematics_for_Machine_Learning.html">302 hunch net-2008-05-25-Inappropriate Mathematics for Machine Learning</a></p>
<p>15 0.54278284 <a title="356-lda-15" href="../hunch_net-2007/hunch_net-2007-11-02-The_Machine_Learning_Award_goes_to_%26%238230%3B.html">270 hunch net-2007-11-02-The Machine Learning Award goes to &#8230;</a></p>
<p>16 0.54061615 <a title="356-lda-16" href="../hunch_net-2010/hunch_net-2010-04-26-Compassionate_Reviewing.html">395 hunch net-2010-04-26-Compassionate Reviewing</a></p>
<p>17 0.53674293 <a title="356-lda-17" href="../hunch_net-2012/hunch_net-2012-08-27-NYAS_ML_2012_and_ICML_2013.html">472 hunch net-2012-08-27-NYAS ML 2012 and ICML 2013</a></p>
<p>18 0.53287971 <a title="356-lda-18" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>19 0.53148282 <a title="356-lda-19" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<p>20 0.51827538 <a title="356-lda-20" href="../hunch_net-2008/hunch_net-2008-11-11-COLT_CFP.html">326 hunch net-2008-11-11-COLT CFP</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
