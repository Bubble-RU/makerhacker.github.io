<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>361 hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2009" href="../home/hunch_net-2009_home.html">hunch_net-2009</a> <a title="hunch_net-2009-361" href="#">hunch_net-2009-361</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>361 hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2009-361-html" href="http://hunch.net/?p=813">html</a></p><p>Introduction: Here's a list of papers that I found interesting atICML/COLT/UAIin 2009.Elad
HazanandComandur SeshadhriEfficient learning algorithms for changing
environmentsat ICML. This paper shows how to adapt learning algorithms that
compete with fixed predictors to compete with changing policies. The
definition of regret they deal with seems particularly useful in many
situation.Hal Daume,Unsupervised Search-based Structured Predictionat ICML.
This paper shows a technique for reducing unsupervised learning to supervised
learning which (a) make a fast unsupervised learning algorithm and (b) makes
semisupervised learning both easy and highly effective.There were two papers
with similar results on active learning in the KWIK framework for linear
regression, both reducing the sample complexity to . One wasNicolo Cesa-
Bianchi,Claudio Gentile, andFrancesco OrabonaRobust Bounds for Classification
via Selective Samplingat ICML and the other wasThomas Walsh,Istvan
Szita,Carlos Diuk,Michael LittmanExplori</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('paper', 0.302), ('proper', 0.286), ('shows', 0.276), ('unsupervised', 0.181), ('compete', 0.176), ('changing', 0.165), ('reducing', 0.156), ('regression', 0.153), ('moments', 0.128), ('characterizes', 0.128), ('covers', 0.128), ('gentile', 0.128), ('littlestone', 0.128), ('regret', 0.127), ('semisupervised', 0.118), ('bounds', 0.115), ('generalize', 0.111), ('anyways', 0.111), ('selective', 0.111), ('characterization', 0.111)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="361-tfidf-1" href="../hunch_net-2009/hunch_net-2009-06-24-Interesting_papers_at_UAICMOLT_2009.html">361 hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</a></p>
<p>Introduction: Here's a list of papers that I found interesting atICML/COLT/UAIin 2009.Elad
HazanandComandur SeshadhriEfficient learning algorithms for changing
environmentsat ICML. This paper shows how to adapt learning algorithms that
compete with fixed predictors to compete with changing policies. The
definition of regret they deal with seems particularly useful in many
situation.Hal Daume,Unsupervised Search-based Structured Predictionat ICML.
This paper shows a technique for reducing unsupervised learning to supervised
learning which (a) make a fast unsupervised learning algorithm and (b) makes
semisupervised learning both easy and highly effective.There were two papers
with similar results on active learning in the KWIK framework for linear
regression, both reducing the sample complexity to . One wasNicolo Cesa-
Bianchi,Claudio Gentile, andFrancesco OrabonaRobust Bounds for Classification
via Selective Samplingat ICML and the other wasThomas Walsh,Istvan
Szita,Carlos Diuk,Michael LittmanExplori</p><p>2 0.17422825 <a title="361-tfidf-2" href="../hunch_net-2011/hunch_net-2011-08-01-Interesting_papers_at_COLT_2011.html">439 hunch net-2011-08-01-Interesting papers at COLT 2011</a></p>
<p>Introduction: SinceJohndid not attendCOLTthis year, I have been volunteered to report back
on the hot stuff at this year's meeting. The conference seemed to have pretty
high quality stuff this year, and I found plenty of interesting papers on all
the three days. I'm gonna pick some of my favorites going through the program
in a chronological order.The first session on matrices seemed interesting for
two reasons. First, the papers were quite nice. But more interestingly, this
is a topic that has had a lot of presence in Statistics and Compressed sensing
literature recently. So it was good to see high-dimensional matrices finally
make their entry at COLT. The paper ofOhadandShaionCollaborative Filtering
with the Trace Norm: Learning, Bounding, and Transducingprovides non-trivial
guarantees on trace norm regularization in an agnostic setup, while Rina
andNatishow how Rademacher averages can be used to get sharper results for
matrix completion problems in their paperConcentration-Based Guarantees for
Lo</p><p>3 0.16642995 <a title="361-tfidf-3" href="../hunch_net-2007/hunch_net-2007-12-19-Cool_and_interesting_things_seen_at_NIPS.html">279 hunch net-2007-12-19-Cool and interesting things seen at NIPS</a></p>
<p>Introduction: I learned a number of things atNIPS.The financial people were there in greater
force than previously.Two Sigmasponsored NIPS whileDRW Tradinghad a
booth.Theadversarial machine learning workshophad a number of talks about
interesting applications where an adversary really is out to try and mess up
your learning algorithm. This is very different from the situation we often
think of where the world is oblivious to our learning. This may present new
and convincing applications for the learning-against-an-adversary work common
atCOLT.There were several interesing papers.Sanjoy Dasgupta,Daniel Hsu,
andClaire Monteleonihad a paper onGeneral Agnostic Active Learning. The basic
idea is that active learning can be done via reduction to a form of supervised
learning problem. This is great, because we have many supervised learning
algorithms from which the benefits of active learning may be derived.Joseph
BradleyandRobert Schapirehad aPaper on Filterboost. Filterboost is an online
boosting algorit</p><p>4 0.16340426 <a title="361-tfidf-4" href="../hunch_net-2008/hunch_net-2008-07-15-Interesting_papers_at_COLT_%28and_a_bit_of_UAI_%26%23038%3B_workshops%29.html">310 hunch net-2008-07-15-Interesting papers at COLT (and a bit of UAI &#038; workshops)</a></p>
<p>Introduction: Here are a few papers fromCOLT 2008that I found interesting.Maria-Florina
Balcan,Steve Hanneke, andJenn Wortman,The True Sample Complexity of Active
Learning. This paper shows that in an asymptotic setting, active learning
isalwaysbetter than supervised learning (although the gap may be small). This
is evidence that the only thing in the way of universal active learning is us
knowing how to do it properly.Nir AilonandMehryar Mohri,An Efficient Reduction
of Ranking to Classification. This paper shows how to robustly ranknobjects
withn log(n)classifications using a quicksort based algorithm. The result is
applicable to many ranking loss functions and has implications for
others.Michael KearnsandJennifer Wortman.Learning from Collective Behavior.
This is about learning in a new model, where the goal is to predict how a
collection of interacting agents behave. One claim is that learning in this
setting can be reduced to IID learning.Due to the relation withMetric-E3, I
was particularly int</p><p>5 0.1440645 <a title="361-tfidf-5" href="../hunch_net-2006/hunch_net-2006-07-13-Regression_vs._Classification_as_a_Primitive.html">196 hunch net-2006-07-13-Regression vs. Classification as a Primitive</a></p>
<p>Introduction: Forlearning reductionswe have been concentrating on reducing various complex
learning problems to binary classification. This choice needs to be actively
questioned, because it was not carefully considered.Binary clasification is
learning a classifierc:X -> {0,1}so as to minimize the probability of being
wrong,Prx,y~D(c(x)y).The primary alternative candidate seems to be squared
error regression. In squared error regression, you learn a regressors:X ->
[0,1]so as to minimize squared error,Ex,y~D(s(x)-y)2.It is difficult to judge
one primitive against another. The judgement must at least partially be made
on nontheoretical grounds because (essentially) we are evaluating a choice
between two axioms/assumptions.These two primitives are significantly related.
Classification can be reduced to regression in the obvious way: you use the
regressor to predictD(y=1|x), then threshold at0.5. For this simple reduction
a squared error regret ofrimplies a classification regret of at mostr0.5.
Regress</p><p>6 0.13589522 <a title="361-tfidf-6" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>7 0.13228896 <a title="361-tfidf-7" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<p>8 0.13055374 <a title="361-tfidf-8" href="../hunch_net-2007/hunch_net-2007-06-14-Interesting_Papers_at_COLT_2007.html">247 hunch net-2007-06-14-Interesting Papers at COLT 2007</a></p>
<p>9 0.13054636 <a title="361-tfidf-9" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>10 0.13001543 <a title="361-tfidf-10" href="../hunch_net-2007/hunch_net-2007-06-24-Interesting_Papers_at_ICML_2007.html">251 hunch net-2007-06-24-Interesting Papers at ICML 2007</a></p>
<p>11 0.12810041 <a title="361-tfidf-11" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">23 hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<p>12 0.12635757 <a title="361-tfidf-12" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>13 0.11738728 <a title="361-tfidf-13" href="../hunch_net-2006/hunch_net-2006-07-26-Two_more_UAI_papers_of_interest.html">199 hunch net-2006-07-26-Two more UAI papers of interest</a></p>
<p>14 0.11706056 <a title="361-tfidf-14" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>15 0.11660216 <a title="361-tfidf-15" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>16 0.1134271 <a title="361-tfidf-16" href="../hunch_net-2005/hunch_net-2005-08-11-Why_Manifold-Based_Dimension_Reduction_Techniques%3F.html">102 hunch net-2005-08-11-Why Manifold-Based Dimension Reduction Techniques?</a></p>
<p>17 0.11336936 <a title="361-tfidf-17" href="../hunch_net-2005/hunch_net-2005-03-22-Active_learning.html">45 hunch net-2005-03-22-Active learning</a></p>
<p>18 0.11298479 <a title="361-tfidf-18" href="../hunch_net-2009/hunch_net-2009-01-07-Interesting_Papers_at_SODA_2009.html">334 hunch net-2009-01-07-Interesting Papers at SODA 2009</a></p>
<p>19 0.11163674 <a title="361-tfidf-19" href="../hunch_net-2010/hunch_net-2010-12-26-NIPS_2010.html">420 hunch net-2010-12-26-NIPS 2010</a></p>
<p>20 0.10967985 <a title="361-tfidf-20" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.246), (1, -0.051), (2, -0.148), (3, 0.101), (4, -0.156), (5, -0.016), (6, 0.15), (7, -0.069), (8, -0.045), (9, -0.085), (10, 0.085), (11, 0.099), (12, 0.029), (13, 0.1), (14, 0.015), (15, 0.041), (16, 0.121), (17, 0.033), (18, 0.027), (19, -0.075), (20, 0.04), (21, -0.057), (22, -0.053), (23, -0.055), (24, 0.064), (25, -0.042), (26, 0.007), (27, -0.044), (28, 0.01), (29, -0.001), (30, -0.007), (31, 0.006), (32, -0.018), (33, -0.002), (34, 0.023), (35, 0.012), (36, 0.046), (37, 0.064), (38, 0.055), (39, 0.02), (40, -0.012), (41, -0.046), (42, 0.007), (43, 0.005), (44, 0.042), (45, 0.003), (46, -0.002), (47, -0.01), (48, 0.035), (49, -0.056)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96517825 <a title="361-lsi-1" href="../hunch_net-2009/hunch_net-2009-06-24-Interesting_papers_at_UAICMOLT_2009.html">361 hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</a></p>
<p>Introduction: Here's a list of papers that I found interesting atICML/COLT/UAIin 2009.Elad
HazanandComandur SeshadhriEfficient learning algorithms for changing
environmentsat ICML. This paper shows how to adapt learning algorithms that
compete with fixed predictors to compete with changing policies. The
definition of regret they deal with seems particularly useful in many
situation.Hal Daume,Unsupervised Search-based Structured Predictionat ICML.
This paper shows a technique for reducing unsupervised learning to supervised
learning which (a) make a fast unsupervised learning algorithm and (b) makes
semisupervised learning both easy and highly effective.There were two papers
with similar results on active learning in the KWIK framework for linear
regression, both reducing the sample complexity to . One wasNicolo Cesa-
Bianchi,Claudio Gentile, andFrancesco OrabonaRobust Bounds for Classification
via Selective Samplingat ICML and the other wasThomas Walsh,Istvan
Szita,Carlos Diuk,Michael LittmanExplori</p><p>2 0.7456615 <a title="361-lsi-2" href="../hunch_net-2011/hunch_net-2011-08-01-Interesting_papers_at_COLT_2011.html">439 hunch net-2011-08-01-Interesting papers at COLT 2011</a></p>
<p>Introduction: SinceJohndid not attendCOLTthis year, I have been volunteered to report back
on the hot stuff at this year's meeting. The conference seemed to have pretty
high quality stuff this year, and I found plenty of interesting papers on all
the three days. I'm gonna pick some of my favorites going through the program
in a chronological order.The first session on matrices seemed interesting for
two reasons. First, the papers were quite nice. But more interestingly, this
is a topic that has had a lot of presence in Statistics and Compressed sensing
literature recently. So it was good to see high-dimensional matrices finally
make their entry at COLT. The paper ofOhadandShaionCollaborative Filtering
with the Trace Norm: Learning, Bounding, and Transducingprovides non-trivial
guarantees on trace norm regularization in an agnostic setup, while Rina
andNatishow how Rademacher averages can be used to get sharper results for
matrix completion problems in their paperConcentration-Based Guarantees for
Lo</p><p>3 0.72978699 <a title="361-lsi-3" href="../hunch_net-2008/hunch_net-2008-07-15-Interesting_papers_at_COLT_%28and_a_bit_of_UAI_%26%23038%3B_workshops%29.html">310 hunch net-2008-07-15-Interesting papers at COLT (and a bit of UAI &#038; workshops)</a></p>
<p>Introduction: Here are a few papers fromCOLT 2008that I found interesting.Maria-Florina
Balcan,Steve Hanneke, andJenn Wortman,The True Sample Complexity of Active
Learning. This paper shows that in an asymptotic setting, active learning
isalwaysbetter than supervised learning (although the gap may be small). This
is evidence that the only thing in the way of universal active learning is us
knowing how to do it properly.Nir AilonandMehryar Mohri,An Efficient Reduction
of Ranking to Classification. This paper shows how to robustly ranknobjects
withn log(n)classifications using a quicksort based algorithm. The result is
applicable to many ranking loss functions and has implications for
others.Michael KearnsandJennifer Wortman.Learning from Collective Behavior.
This is about learning in a new model, where the goal is to predict how a
collection of interacting agents behave. One claim is that learning in this
setting can be reduced to IID learning.Due to the relation withMetric-E3, I
was particularly int</p><p>4 0.67937893 <a title="361-lsi-4" href="../hunch_net-2006/hunch_net-2006-07-05-more_icml_papers.html">189 hunch net-2006-07-05-more icml papers</a></p>
<p>Introduction: Here are a few other papers I enjoyed from ICML06.Topic Models:Dynamic Topic
ModelsDavid Blei, John LaffertyA nice model for how topics in LDA type models
can evolve over time,using a linear dynamical system on the natural parameters
and a veryclever structured variational approximation (in which the mean
fieldparameters are pseudo-observations of a virtual LDS). Like all
Bleipapers, he makes it look easy, but it is extremely impressive.Pachinko
AllocationWei Li, Andrew McCallumA very elegant (but computationally
challenging) model which inducescorrelation amongst topics using a multi-level
DAG whose interior nodesare "super-topics" and "sub-topics" and whose leaves
are thevocabulary words. Makes the slumbering monster of structure learning
stir.Sequence Analysis (I missed these talks since I was chairing another
session)Online Decoding of Markov Models with Latency ConstraintsMukund
Narasimhan, Paul Viola, Michael ShilmanAn "ah-ha!" paper showing how to trade
off latency and decodinga</p><p>5 0.67051941 <a title="361-lsi-5" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>Introduction: There were several papers that seemed fairly interesting atKDD this year. The
ones that caught my attention are:Xin Jin, Mingyang Zhang,Nan Zhang, andGautam
Das,Versatile Publishing For Privacy Preservation. This paper provides a
conservative method for safely determining which data is publishable from any
complete source of information (for example, a hospital) such that it does not
violate privacy rules in a natural language. It is not differentially private,
so no external sources of join information can exist. However, it is a
mechanism forpublishingdata rather than (say) the output of a learning
algorithm.Arik FriedmanAssaf Schuster,Data Mining with Differential Privacy.
This paper shows how to create effective differentially private decision
trees. Progress in differentially private datamining is pretty impressive, as
it wasdefined in 2006.David Chan, Rong Ge, Ori Gershony,Tim Hesterberg,Diane
Lambert,Evaluating Online Ad Campaigns in a Pipeline: Causal Models At
ScaleThis paper</p><p>6 0.66917932 <a title="361-lsi-6" href="../hunch_net-2007/hunch_net-2007-06-24-Interesting_Papers_at_ICML_2007.html">251 hunch net-2007-06-24-Interesting Papers at ICML 2007</a></p>
<p>7 0.6578812 <a title="361-lsi-7" href="../hunch_net-2009/hunch_net-2009-12-27-Interesting_things_at_NIPS_2009.html">385 hunch net-2009-12-27-Interesting things at NIPS 2009</a></p>
<p>8 0.63232523 <a title="361-lsi-8" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>9 0.62600935 <a title="361-lsi-9" href="../hunch_net-2006/hunch_net-2006-07-26-Two_more_UAI_papers_of_interest.html">199 hunch net-2006-07-26-Two more UAI papers of interest</a></p>
<p>10 0.62386107 <a title="361-lsi-10" href="../hunch_net-2005/hunch_net-2005-12-14-More_NIPS_Papers_II.html">140 hunch net-2005-12-14-More NIPS Papers II</a></p>
<p>11 0.61207831 <a title="361-lsi-11" href="../hunch_net-2009/hunch_net-2009-01-07-Interesting_Papers_at_SODA_2009.html">334 hunch net-2009-01-07-Interesting Papers at SODA 2009</a></p>
<p>12 0.60081625 <a title="361-lsi-12" href="../hunch_net-2007/hunch_net-2007-12-19-Cool_and_interesting_things_seen_at_NIPS.html">279 hunch net-2007-12-19-Cool and interesting things seen at NIPS</a></p>
<p>13 0.59286928 <a title="361-lsi-13" href="../hunch_net-2005/hunch_net-2005-12-09-Some_NIPS_papers.html">138 hunch net-2005-12-09-Some NIPS papers</a></p>
<p>14 0.58733875 <a title="361-lsi-14" href="../hunch_net-2005/hunch_net-2005-12-11-More_NIPS_Papers.html">139 hunch net-2005-12-11-More NIPS Papers</a></p>
<p>15 0.58708066 <a title="361-lsi-15" href="../hunch_net-2007/hunch_net-2007-06-14-Interesting_Papers_at_COLT_2007.html">247 hunch net-2007-06-14-Interesting Papers at COLT 2007</a></p>
<p>16 0.57970721 <a title="361-lsi-16" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>17 0.57612967 <a title="361-lsi-17" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">23 hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<p>18 0.56914264 <a title="361-lsi-18" href="../hunch_net-2007/hunch_net-2007-12-20-Cool_and_Interesting_things_at_NIPS%2C_take_three.html">280 hunch net-2007-12-20-Cool and Interesting things at NIPS, take three</a></p>
<p>19 0.56655204 <a title="361-lsi-19" href="../hunch_net-2009/hunch_net-2009-08-26-Another_10-year_paper_in_Machine_Learning.html">368 hunch net-2009-08-26-Another 10-year paper in Machine Learning</a></p>
<p>20 0.56344014 <a title="361-lsi-20" href="../hunch_net-2008/hunch_net-2008-11-10-ICML_Reviewing_Criteria.html">325 hunch net-2008-11-10-ICML Reviewing Criteria</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(7, 0.365), (35, 0.05), (42, 0.255), (45, 0.032), (68, 0.066), (74, 0.126), (88, 0.015)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.96836692 <a title="361-lda-1" href="../hunch_net-2006/hunch_net-2006-02-07-Pittsburgh_Mind_Reading_Competition.html">155 hunch net-2006-02-07-Pittsburgh Mind Reading Competition</a></p>
<p>Introduction: Francisco Pereirapoints out a funPrediction Competition. Francisco says:DARPA
is sponsoring a competition to analyze data from an unusual functional
Magnetic Resonance Imaging experiment. Subjects watch videos inside the
scanner while fMRI data are acquired. Unbeknownst to these subjects, the
videos have been seen by a panel of other subjects that labeled each instant
with labels in categories such as representation (are there tools, body parts,
motion, sound), location, presence of actors, emotional content, etc.The
challenge is to predict all of these different labels on an instant-by-instant
basis from the fMRI data. A few reasons why this is particularly
interesting:This is beyond the current state of the art, but not inconceivably
hard.This is a new type of experiment design current analysis methods cannot
deal with.This is an opportunity to work with a heavily examined and
preprocessed neuroimaging dataset.DARPA is offering prizes!</p><p>2 0.94805348 <a title="361-lda-2" href="../hunch_net-2013/hunch_net-2013-05-04-COLT_and_ICML_registration.html">482 hunch net-2013-05-04-COLT and ICML registration</a></p>
<p>Introduction: Sebastien Bubeckpoints outCOLTregistrationwith a May 13 early registration
deadline. The local organizers have done an admirable job of containing costs
with a $300 registration fee.ICMLregistrationis also available, at about an x3
higher cost. My understanding is that this is partly due to the costs of a
larger conference being harder to contain, partly due to ICML lasting twice as
long with tutorials and workshops, and partly because the conference
organizers were a bit over-conservative in various ways.</p><p>3 0.86139536 <a title="361-lda-3" href="../hunch_net-2009/hunch_net-2009-07-09-The_Machine_Learning_Forum.html">363 hunch net-2009-07-09-The Machine Learning Forum</a></p>
<p>Introduction: Dear Fellow Machine Learners,For the past year or so I have become
increasingly frustrated with the peer review system in our field. I constantly
get asked to review papers in which I have no interest. At the same time, as
an action editor in JMLR, I constantly have to harass people to review papers.
When I send papers to conferences and to journals I often get rejected with
reviews that, at least in my mind, make no sense. Finally, I have a very hard
time keeping up with the best new work, because I don't know where to look for
itâ&euro;ŚI decided to try an do something to improve the situation. I started a new
web site, which I decided to call "The machine learning forum" the URL
ishttp://themachinelearningforum.orgThe main idea behind this web site is to
remove anonymity from the review process. In this site, all opinions are
attributed to the actual person that expressed them. I expect that this will
improve the quality of the reviews. An obvious other effect is that there will
be fewer n</p><p>4 0.85324866 <a title="361-lda-4" href="../hunch_net-2005/hunch_net-2005-02-19-Machine_learning_reading_groups.html">24 hunch net-2005-02-19-Machine learning reading groups</a></p>
<p>Introduction: Yaroslav collected an extensive list ofmachine learning reading groups.</p><p>5 0.84328961 <a title="361-lda-5" href="../hunch_net-2005/hunch_net-2005-09-08-Online_Learning_as_the_Mathematics_of_Accountability.html">109 hunch net-2005-09-08-Online Learning as the Mathematics of Accountability</a></p>
<p>Introduction: Accountability is a social problem. When someone screws up, do you fire them?
Or do you accept the error and let them continue? This is a very difficult
problem and we all know of stories where the wrong decision was made.Online
learning(as meant here), is a subfield of learning theory which analyzes the
online learning model.In the online learning model, there are a set of
hypotheses or "experts". On any instantancex, each expert makes a predictiony.
A master algorithmAuses these predictions to form it's own predictionyAand
then learns the correct predictiony*. This process repeats.The goal of online
learning is to find a master algorithmAwhich uses the advice of the experts to
make good predictions. In particular, we typically want to guarantee that the
master algorithm performs almost as well as the best expert. IfL(e)is the loss
of experteandL(A)is the loss of the master algorithm, it is often possible to
prove:L(A) less than mineL(e) + log(number of experts)over all sequences.In
p</p><p>same-blog 6 0.83773619 <a title="361-lda-6" href="../hunch_net-2009/hunch_net-2009-06-24-Interesting_papers_at_UAICMOLT_2009.html">361 hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</a></p>
<p>7 0.70858383 <a title="361-lda-7" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>8 0.64486617 <a title="361-lda-8" href="../hunch_net-2006/hunch_net-2006-03-12-Online_learning_or_online_preservation_of_learning%3F.html">163 hunch net-2006-03-12-Online learning or online preservation of learning?</a></p>
<p>9 0.63726932 <a title="361-lda-9" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>10 0.62084699 <a title="361-lda-10" href="../hunch_net-2007/hunch_net-2007-08-12-Exponentiated_Gradient.html">258 hunch net-2007-08-12-Exponentiated Gradient</a></p>
<p>11 0.61919695 <a title="361-lda-11" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>12 0.61900258 <a title="361-lda-12" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>13 0.61877984 <a title="361-lda-13" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>14 0.61653507 <a title="361-lda-14" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>15 0.61627948 <a title="361-lda-15" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>16 0.61607999 <a title="361-lda-16" href="../hunch_net-2005/hunch_net-2005-12-07-Is_the_Google_way_the_way_for_machine_learning%3F.html">136 hunch net-2005-12-07-Is the Google way the way for machine learning?</a></p>
<p>17 0.61495799 <a title="361-lda-17" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<p>18 0.61473739 <a title="361-lda-18" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>19 0.61310387 <a title="361-lda-19" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>20 0.61296785 <a title="361-lda-20" href="../hunch_net-2006/hunch_net-2006-05-05-An_ICML_reject.html">177 hunch net-2006-05-05-An ICML reject</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
