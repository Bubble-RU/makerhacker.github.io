<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>361 hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2009" href="../home/hunch_net-2009_home.html">hunch_net-2009</a> <a title="hunch_net-2009-361" href="#">hunch_net-2009-361</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>361 hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2009-361-html" href="http://hunch.net/?p=813">html</a></p><p>Introduction: Here’s a list of papers that I found interesting at  ICML / COLT / UAI  in 2009.
  
  Elad Hazan  and  Comandur Seshadhri   Efficient learning algorithms for changing environments  at ICML.  This paper shows how to adapt learning algorithms that compete with fixed predictors to compete with changing policies.  The definition of regret they deal with seems particularly useful in many situation. 
  Hal Daume ,  Unsupervised Search-based Structured Prediction  at ICML.  This paper shows a technique for reducing unsupervised learning to supervised learning which (a) make a fast unsupervised learning algorithm and (b)  makes semisupervised learning both easy and highly effective.  
 There were two papers with similar results on active learning in the KWIK framework for linear regression, both reducing the sample complexity to .  One was  Nicolo Cesa-Bianchi ,  Claudio Gentile , and  Francesco Orabona   Robust Bounds for Classification via Selective Sampling  at ICML and the other was  Thoma</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Elad Hazan  and  Comandur Seshadhri   Efficient learning algorithms for changing environments  at ICML. [sent-2, score-0.221]
</p><p>2 This paper shows how to adapt learning algorithms that compete with fixed predictors to compete with changing policies. [sent-3, score-0.939]
</p><p>3 The definition of regret they deal with seems particularly useful in many situation. [sent-4, score-0.101]
</p><p>4 This paper shows a technique for reducing unsupervised learning to supervised learning which (a) make a fast unsupervised learning algorithm and (b)  makes semisupervised learning both easy and highly effective. [sent-6, score-1.072]
</p><p>5 There were two papers with similar results on active learning in the KWIK framework for linear regression, both reducing the sample complexity to . [sent-7, score-0.214]
</p><p>6 This paper talks about how to keep track of the moments in a datastream with very little space and computation. [sent-11, score-0.324]
</p><p>7 This paper points out that via the integral characterization of proper losses, proper scoring rules can be reduced to binary classification. [sent-14, score-0.941]
</p><p>8 The results unify and generalize the  Probing  and  Quanting  reductions we worked on previously. [sent-15, score-0.082]
</p><p>9 This paper is also related to  Nicolas Lambert ‘s  work , which is quite thought provoking in terms of specifying what is learnable. [sent-16, score-0.311]
</p><p>10 This paper shows that a subset of HMMs can be learned using an SVD-based algorithm. [sent-20, score-0.438]
</p><p>11 Samory Kpotufe ,  Escaping the curse of dimensionality with a tree-based regressor  at COLT. [sent-21, score-0.082]
</p><p>12 This paper shows how to directly applying regression in high dimensional vector spaces and have it succeed anyways because the data is naturally low-dimensional. [sent-22, score-0.695]
</p><p>13 This paper characterizes the ability to learn when an adversary is choosing features in the online setting as the “Littlestone dimension”. [sent-25, score-0.324]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('proper', 0.231), ('paper', 0.221), ('shows', 0.217), ('unsupervised', 0.213), ('regression', 0.167), ('shai', 0.154), ('compete', 0.142), ('losses', 0.139), ('changing', 0.131), ('reducing', 0.126), ('uai', 0.111), ('moments', 0.103), ('kwik', 0.103), ('characterizes', 0.103), ('covers', 0.103), ('diuk', 0.103), ('francesco', 0.103), ('littlestone', 0.103), ('orabona', 0.103), ('ping', 0.103), ('szita', 0.103), ('regret', 0.101), ('nicolas', 0.095), ('surrogate', 0.095), ('walsh', 0.095), ('gentile', 0.095), ('lambert', 0.095), ('bounds', 0.092), ('probing', 0.09), ('anyways', 0.09), ('characterization', 0.09), ('carlos', 0.09), ('environments', 0.09), ('nicolo', 0.09), ('williamson', 0.09), ('exploring', 0.09), ('littman', 0.09), ('provoking', 0.09), ('linear', 0.088), ('selective', 0.086), ('compact', 0.086), ('adapt', 0.086), ('claudio', 0.086), ('scoring', 0.086), ('integral', 0.082), ('generalize', 0.082), ('semisupervised', 0.082), ('daume', 0.082), ('regressor', 0.082), ('compressed', 0.082)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="361-tfidf-1" href="../hunch_net-2009/hunch_net-2009-06-24-Interesting_papers_at_UAICMOLT_2009.html">361 hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</a></p>
<p>Introduction: Here’s a list of papers that I found interesting at  ICML / COLT / UAI  in 2009.
  
  Elad Hazan  and  Comandur Seshadhri   Efficient learning algorithms for changing environments  at ICML.  This paper shows how to adapt learning algorithms that compete with fixed predictors to compete with changing policies.  The definition of regret they deal with seems particularly useful in many situation. 
  Hal Daume ,  Unsupervised Search-based Structured Prediction  at ICML.  This paper shows a technique for reducing unsupervised learning to supervised learning which (a) make a fast unsupervised learning algorithm and (b)  makes semisupervised learning both easy and highly effective.  
 There were two papers with similar results on active learning in the KWIK framework for linear regression, both reducing the sample complexity to .  One was  Nicolo Cesa-Bianchi ,  Claudio Gentile , and  Francesco Orabona   Robust Bounds for Classification via Selective Sampling  at ICML and the other was  Thoma</p><p>2 0.18006451 <a title="361-tfidf-2" href="../hunch_net-2011/hunch_net-2011-08-01-Interesting_papers_at_COLT_2011.html">439 hunch net-2011-08-01-Interesting papers at COLT 2011</a></p>
<p>Introduction: Since  John  did not attend  COLT  this year, I have been volunteered to report back on the hot stuff at this year’s meeting. The conference seemed to have pretty high quality stuff this year, and I found plenty of interesting papers on all the three days. I’m gonna pick some of my favorites going through the program in a chronological order.
 
The first session on matrices seemed interesting for two reasons. First, the papers were quite nice. But more interestingly, this is a topic that has had a lot of presence in Statistics and Compressed sensing literature recently. So it was good to see high-dimensional matrices finally make their entry at COLT. The paper of  Ohad  and  Shai  on  Collaborative Filtering with the Trace Norm: Learning, Bounding, and Transducing  provides non-trivial guarantees on trace norm regularization in an agnostic setup, while Rina and  Nati  show how Rademacher averages can be used to get sharper results for matrix completion problems in their paper  Concentr</p><p>3 0.17310824 <a title="361-tfidf-3" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>Introduction: Here are some papers from  ICML 2008  that I found interesting.  
  
  Risi Kondor  and  Karsten Borgwardt ,  The Skew Spectrum of Graphs . This paper is about a new family of functions on graphs which is invariant under node label permutation.  They show that these quantities appear to yield good features for learning. 
  Sanjoy Dasgupta  and  Daniel Hsu .   Hierarchical sampling for active learning.   This is the first published practical consistent active learning algorithm.  The abstract is also pretty impressive. 
  Lihong Li ,  Michael Littman , and  Thomas Walsh   Knows What It Knows: A Framework For Self-Aware Learning.   This is an attempt to create learning algorithms that know when they err, (other work includes  Vovk ).  It’s not yet clear to me what the right model for  feature-dependent confidence intervals  is. 
  Novi Quadrianto ,  Alex Smola ,  TIberio Caetano , and  Quoc Viet Le   Estimating Labels from Label Proportions .  This is an example of learning in a speciali</p><p>4 0.17130174 <a title="361-tfidf-4" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>Introduction: The papers which interested me most at  ICML  and  COLT  2010 were:
  
  Thomas Walsh ,  Kaushik Subramanian ,  Michael Littman  and  Carlos Diuk   Generalizing Apprenticeship Learning across Hypothesis Classes .  This paper formalizes and provides algorithms with guarantees for mixed-mode apprenticeship and traditional reinforcement learning algorithms, allowing RL algorithms that perform better than for either setting alone. 
  István Szita  and  Csaba Szepesvári   Model-based reinforcement learning with nearly tight exploration complexity bounds .  This paper and  another represent the frontier of best-known algorithm for Reinforcement Learning in a Markov Decision Process. 
  James Martens   Deep learning via Hessian-free optimization .  About a new not-quite-online second order gradient algorithm for learning deep functional structures.  Potentially this is very powerful because while people have often talked about end-to-end learning, it has rarely worked in practice. 
  Chrisoph</p><p>5 0.16355248 <a title="361-tfidf-5" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<p>Introduction: This post is by Daniel Hsu and John Langford.
 
In selective sampling style active learning, a learning algorithm chooses which examples to label.  We now have an active learning algorithm that is:
  
  Efficient  in label complexity, unlabeled complexity, and computational complexity. 
  Competitive  with supervised learning anywhere that supervised learning works. 
  Compatible  with online learning, with any optimization-based learning algorithm, with any loss function, with offline testing, and even with changing learning algorithms. 
  Empirically  effective. 
  
The basic idea is to combine  disagreement region-based sampling  with  importance weighting : an example is selected to be labeled with probability proportional to how useful it is for distinguishing among near-optimal classifiers, and labeled examples are importance-weighted by the inverse of these probabilities.  The combination of these simple ideas removes the  sampling bias  problem that has plagued many previous he</p><p>6 0.15966251 <a title="361-tfidf-6" href="../hunch_net-2006/hunch_net-2006-07-13-Regression_vs._Classification_as_a_Primitive.html">196 hunch net-2006-07-13-Regression vs. Classification as a Primitive</a></p>
<p>7 0.14902316 <a title="361-tfidf-7" href="../hunch_net-2007/hunch_net-2007-12-19-Cool_and_interesting_things_seen_at_NIPS.html">279 hunch net-2007-12-19-Cool and interesting things seen at NIPS</a></p>
<p>8 0.12496643 <a title="361-tfidf-8" href="../hunch_net-2006/hunch_net-2006-07-26-Two_more_UAI_papers_of_interest.html">199 hunch net-2006-07-26-Two more UAI papers of interest</a></p>
<p>9 0.12432893 <a title="361-tfidf-9" href="../hunch_net-2007/hunch_net-2007-06-24-Interesting_Papers_at_ICML_2007.html">251 hunch net-2007-06-24-Interesting Papers at ICML 2007</a></p>
<p>10 0.12327796 <a title="361-tfidf-10" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>11 0.12327261 <a title="361-tfidf-11" href="../hunch_net-2009/hunch_net-2009-12-27-Interesting_things_at_NIPS_2009.html">385 hunch net-2009-12-27-Interesting things at NIPS 2009</a></p>
<p>12 0.11801328 <a title="361-tfidf-12" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>13 0.11473976 <a title="361-tfidf-13" href="../hunch_net-2009/hunch_net-2009-12-24-Top_graduates_this_season.html">384 hunch net-2009-12-24-Top graduates this season</a></p>
<p>14 0.11245746 <a title="361-tfidf-14" href="../hunch_net-2008/hunch_net-2008-07-15-Interesting_papers_at_COLT_%28and_a_bit_of_UAI_%26%23038%3B_workshops%29.html">310 hunch net-2008-07-15-Interesting papers at COLT (and a bit of UAI &#038; workshops)</a></p>
<p>15 0.11150099 <a title="361-tfidf-15" href="../hunch_net-2005/hunch_net-2005-02-01-NIPS%3A_Online_Bayes.html">8 hunch net-2005-02-01-NIPS: Online Bayes</a></p>
<p>16 0.11149186 <a title="361-tfidf-16" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>17 0.10550538 <a title="361-tfidf-17" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">23 hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<p>18 0.10273871 <a title="361-tfidf-18" href="../hunch_net-2007/hunch_net-2007-07-12-ICML_Trends.html">254 hunch net-2007-07-12-ICML Trends</a></p>
<p>19 0.10148847 <a title="361-tfidf-19" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>20 0.1001787 <a title="361-tfidf-20" href="../hunch_net-2009/hunch_net-2009-01-07-Interesting_Papers_at_SODA_2009.html">334 hunch net-2009-01-07-Interesting Papers at SODA 2009</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.234), (1, 0.052), (2, 0.092), (3, -0.133), (4, 0.158), (5, -0.021), (6, 0.004), (7, -0.143), (8, -0.094), (9, 0.009), (10, 0.14), (11, -0.012), (12, -0.232), (13, 0.006), (14, 0.009), (15, 0.04), (16, -0.064), (17, 0.077), (18, 0.069), (19, -0.051), (20, 0.079), (21, 0.01), (22, 0.005), (23, 0.042), (24, 0.045), (25, -0.053), (26, 0.001), (27, -0.025), (28, -0.013), (29, 0.033), (30, -0.009), (31, -0.001), (32, 0.022), (33, 0.044), (34, -0.008), (35, -0.015), (36, -0.042), (37, 0.01), (38, 0.028), (39, 0.045), (40, 0.028), (41, 0.008), (42, 0.033), (43, 0.054), (44, -0.039), (45, 0.003), (46, -0.083), (47, -0.014), (48, 0.011), (49, -0.149)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96376377 <a title="361-lsi-1" href="../hunch_net-2009/hunch_net-2009-06-24-Interesting_papers_at_UAICMOLT_2009.html">361 hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</a></p>
<p>Introduction: Here’s a list of papers that I found interesting at  ICML / COLT / UAI  in 2009.
  
  Elad Hazan  and  Comandur Seshadhri   Efficient learning algorithms for changing environments  at ICML.  This paper shows how to adapt learning algorithms that compete with fixed predictors to compete with changing policies.  The definition of regret they deal with seems particularly useful in many situation. 
  Hal Daume ,  Unsupervised Search-based Structured Prediction  at ICML.  This paper shows a technique for reducing unsupervised learning to supervised learning which (a) make a fast unsupervised learning algorithm and (b)  makes semisupervised learning both easy and highly effective.  
 There were two papers with similar results on active learning in the KWIK framework for linear regression, both reducing the sample complexity to .  One was  Nicolo Cesa-Bianchi ,  Claudio Gentile , and  Francesco Orabona   Robust Bounds for Classification via Selective Sampling  at ICML and the other was  Thoma</p><p>2 0.75838232 <a title="361-lsi-2" href="../hunch_net-2011/hunch_net-2011-08-01-Interesting_papers_at_COLT_2011.html">439 hunch net-2011-08-01-Interesting papers at COLT 2011</a></p>
<p>Introduction: Since  John  did not attend  COLT  this year, I have been volunteered to report back on the hot stuff at this year’s meeting. The conference seemed to have pretty high quality stuff this year, and I found plenty of interesting papers on all the three days. I’m gonna pick some of my favorites going through the program in a chronological order.
 
The first session on matrices seemed interesting for two reasons. First, the papers were quite nice. But more interestingly, this is a topic that has had a lot of presence in Statistics and Compressed sensing literature recently. So it was good to see high-dimensional matrices finally make their entry at COLT. The paper of  Ohad  and  Shai  on  Collaborative Filtering with the Trace Norm: Learning, Bounding, and Transducing  provides non-trivial guarantees on trace norm regularization in an agnostic setup, while Rina and  Nati  show how Rademacher averages can be used to get sharper results for matrix completion problems in their paper  Concentr</p><p>3 0.67095399 <a title="361-lsi-3" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>Introduction: Here are some papers from  ICML 2008  that I found interesting.  
  
  Risi Kondor  and  Karsten Borgwardt ,  The Skew Spectrum of Graphs . This paper is about a new family of functions on graphs which is invariant under node label permutation.  They show that these quantities appear to yield good features for learning. 
  Sanjoy Dasgupta  and  Daniel Hsu .   Hierarchical sampling for active learning.   This is the first published practical consistent active learning algorithm.  The abstract is also pretty impressive. 
  Lihong Li ,  Michael Littman , and  Thomas Walsh   Knows What It Knows: A Framework For Self-Aware Learning.   This is an attempt to create learning algorithms that know when they err, (other work includes  Vovk ).  It’s not yet clear to me what the right model for  feature-dependent confidence intervals  is. 
  Novi Quadrianto ,  Alex Smola ,  TIberio Caetano , and  Quoc Viet Le   Estimating Labels from Label Proportions .  This is an example of learning in a speciali</p><p>4 0.6684348 <a title="361-lsi-4" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>Introduction: The papers which interested me most at  ICML  and  COLT  2010 were:
  
  Thomas Walsh ,  Kaushik Subramanian ,  Michael Littman  and  Carlos Diuk   Generalizing Apprenticeship Learning across Hypothesis Classes .  This paper formalizes and provides algorithms with guarantees for mixed-mode apprenticeship and traditional reinforcement learning algorithms, allowing RL algorithms that perform better than for either setting alone. 
  István Szita  and  Csaba Szepesvári   Model-based reinforcement learning with nearly tight exploration complexity bounds .  This paper and  another represent the frontier of best-known algorithm for Reinforcement Learning in a Markov Decision Process. 
  James Martens   Deep learning via Hessian-free optimization .  About a new not-quite-online second order gradient algorithm for learning deep functional structures.  Potentially this is very powerful because while people have often talked about end-to-end learning, it has rarely worked in practice. 
  Chrisoph</p><p>5 0.65081137 <a title="361-lsi-5" href="../hunch_net-2008/hunch_net-2008-07-15-Interesting_papers_at_COLT_%28and_a_bit_of_UAI_%26%23038%3B_workshops%29.html">310 hunch net-2008-07-15-Interesting papers at COLT (and a bit of UAI &#038; workshops)</a></p>
<p>Introduction: Here are a few papers from  COLT 2008  that I found interesting.
  
  Maria-Florina Balcan ,  Steve Hanneke , and  Jenn Wortman ,  The True Sample Complexity of Active Learning .  This paper shows that in an asymptotic setting, active learning is  always  better than supervised learning (although the gap may be small).  This is evidence that the only thing in the way of universal active learning is us knowing how to do it properly. 
  Nir Ailon  and  Mehryar Mohri ,  An Efficient Reduction of Ranking to Classification .  This paper shows how to robustly rank  n  objects with  n log(n)  classifications using a quicksort based algorithm.  The result is applicable to many ranking loss functions and has implications for others. 
  Michael Kearns  and  Jennifer Wortman .  Learning from Collective Behavior .  This is about learning in a new model, where the goal is to predict how a collection of interacting agents behave.  One claim is that learning in this setting can be reduced to IID lear</p><p>6 0.64752215 <a title="361-lsi-6" href="../hunch_net-2009/hunch_net-2009-12-27-Interesting_things_at_NIPS_2009.html">385 hunch net-2009-12-27-Interesting things at NIPS 2009</a></p>
<p>7 0.64424455 <a title="361-lsi-7" href="../hunch_net-2006/hunch_net-2006-07-26-Two_more_UAI_papers_of_interest.html">199 hunch net-2006-07-26-Two more UAI papers of interest</a></p>
<p>8 0.63053268 <a title="361-lsi-8" href="../hunch_net-2006/hunch_net-2006-06-30-ICML_papers.html">188 hunch net-2006-06-30-ICML papers</a></p>
<p>9 0.60336417 <a title="361-lsi-9" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>10 0.60240602 <a title="361-lsi-10" href="../hunch_net-2007/hunch_net-2007-06-24-Interesting_Papers_at_ICML_2007.html">251 hunch net-2007-06-24-Interesting Papers at ICML 2007</a></p>
<p>11 0.57142884 <a title="361-lsi-11" href="../hunch_net-2005/hunch_net-2005-12-11-More_NIPS_Papers.html">139 hunch net-2005-12-11-More NIPS Papers</a></p>
<p>12 0.5541063 <a title="361-lsi-12" href="../hunch_net-2007/hunch_net-2007-12-20-Cool_and_Interesting_things_at_NIPS%2C_take_three.html">280 hunch net-2007-12-20-Cool and Interesting things at NIPS, take three</a></p>
<p>13 0.55017424 <a title="361-lsi-13" href="../hunch_net-2007/hunch_net-2007-12-19-Cool_and_interesting_things_seen_at_NIPS.html">279 hunch net-2007-12-19-Cool and interesting things seen at NIPS</a></p>
<p>14 0.54227406 <a title="361-lsi-14" href="../hunch_net-2005/hunch_net-2005-12-14-More_NIPS_Papers_II.html">140 hunch net-2005-12-14-More NIPS Papers II</a></p>
<p>15 0.54080415 <a title="361-lsi-15" href="../hunch_net-2006/hunch_net-2006-07-05-more_icml_papers.html">189 hunch net-2006-07-05-more icml papers</a></p>
<p>16 0.53391331 <a title="361-lsi-16" href="../hunch_net-2007/hunch_net-2007-06-19-How_is_Compressed_Sensing_going_to_change_Machine_Learning_%3F.html">248 hunch net-2007-06-19-How is Compressed Sensing going to change Machine Learning ?</a></p>
<p>17 0.51544374 <a title="361-lsi-17" href="../hunch_net-2009/hunch_net-2009-12-24-Top_graduates_this_season.html">384 hunch net-2009-12-24-Top graduates this season</a></p>
<p>18 0.51092839 <a title="361-lsi-18" href="../hunch_net-2009/hunch_net-2009-01-07-Interesting_Papers_at_SODA_2009.html">334 hunch net-2009-01-07-Interesting Papers at SODA 2009</a></p>
<p>19 0.50882429 <a title="361-lsi-19" href="../hunch_net-2007/hunch_net-2007-06-14-Interesting_Papers_at_COLT_2007.html">247 hunch net-2007-06-14-Interesting Papers at COLT 2007</a></p>
<p>20 0.50502872 <a title="361-lsi-20" href="../hunch_net-2006/hunch_net-2006-07-08-Some_recent_papers.html">192 hunch net-2006-07-08-Some recent papers</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(3, 0.013), (9, 0.388), (27, 0.207), (38, 0.079), (53, 0.041), (55, 0.094), (94, 0.041), (95, 0.05)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.96399325 <a title="361-lda-1" href="../hunch_net-2010/hunch_net-2010-05-20-Google_Predict.html">399 hunch net-2010-05-20-Google Predict</a></p>
<p>Introduction: Slashdot  points out  Google Predict .  I’m not privy to the details, but this has the potential to be extremely useful, as in many applications simply having an easy mechanism to apply existing learning algorithms can be extremely helpful.  This differs goalwise from  MLcomp —instead of public comparisons for research purposes, it’s about private utilization of good existing algorithms.  It also differs infrastructurally, since a system designed to do this is much less awkward than using Amazon’s cloud computing.  The latter implies that datasets several order of magnitude larger can be handled up to limits imposed by network and storage.</p><p>same-blog 2 0.9080615 <a title="361-lda-2" href="../hunch_net-2009/hunch_net-2009-06-24-Interesting_papers_at_UAICMOLT_2009.html">361 hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</a></p>
<p>Introduction: Here’s a list of papers that I found interesting at  ICML / COLT / UAI  in 2009.
  
  Elad Hazan  and  Comandur Seshadhri   Efficient learning algorithms for changing environments  at ICML.  This paper shows how to adapt learning algorithms that compete with fixed predictors to compete with changing policies.  The definition of regret they deal with seems particularly useful in many situation. 
  Hal Daume ,  Unsupervised Search-based Structured Prediction  at ICML.  This paper shows a technique for reducing unsupervised learning to supervised learning which (a) make a fast unsupervised learning algorithm and (b)  makes semisupervised learning both easy and highly effective.  
 There were two papers with similar results on active learning in the KWIK framework for linear regression, both reducing the sample complexity to .  One was  Nicolo Cesa-Bianchi ,  Claudio Gentile , and  Francesco Orabona   Robust Bounds for Classification via Selective Sampling  at ICML and the other was  Thoma</p><p>3 0.85632449 <a title="361-lda-3" href="../hunch_net-2013/hunch_net-2013-11-09-Graduates_and_Postdocs.html">490 hunch net-2013-11-09-Graduates and Postdocs</a></p>
<p>Introduction: Several strong graduates are on the job market this year.
  
  Alekh Agarwal  made the  most scalable public learning algorithm  as an intern two years ago.  He has a deep and broad understanding of optimization and learning as well as the ability and will to make things happen programming-wise.  I’ve been privileged to have Alekh visiting me in NY where he will be sorely missed. 
  John Duchi  created  Adagrad  which is a commonly helpful improvement over online gradient descent that is seeing wide adoption, including in  Vowpal Wabbit .  He has a similarly deep and broad understanding of optimization and learning with significant industry experience at  Google .  Alekh and John have often coauthored together. 
  Stephane Ross  visited me a year ago over the summer, implementing many new algorithms and working out the first  scale free online update rule  which is now the default in Vowpal Wabbit.  Stephane is  not  on the market—Google robbed the cradle successfully    I’m sure that</p><p>4 0.83725804 <a title="361-lda-4" href="../hunch_net-2008/hunch_net-2008-12-07-A_NIPS_paper.html">330 hunch net-2008-12-07-A NIPS paper</a></p>
<p>Introduction: I’m skipping NIPS this year in favor of  Ada , but I wanted to point out  this paper  by  Andriy Mnih  and  Geoff Hinton .  The basic claim of the paper is that by carefully but automatically constructing a binary tree over words, it’s possible to predict words well with huge computational resource savings over unstructured approaches.
 
I’m interested in this beyond the application to word prediction because it is relevant to the general normalization problem: If you want to predict the probability of one of a large number of events, often you must compute a predicted score for all the events and then normalize, a computationally inefficient operation.  The problem comes up in many places using probabilistic models, but I’ve run into it with high-dimensional regression.
 
There are a couple workarounds for this computational bug:
  
 Approximate. There are many ways.  Often the approximations are uncontrolled (i.e. can be arbitrarily bad), and hence finicky in application. 
 Avoid.  Y</p><p>5 0.83586711 <a title="361-lda-5" href="../hunch_net-2006/hunch_net-2006-04-27-Conferences%2C_Workshops%2C_and_Tutorials.html">174 hunch net-2006-04-27-Conferences, Workshops, and Tutorials</a></p>
<p>Introduction: This is a reminder that many deadlines for summer conference registration are coming up, and attendance is a very good idea.  
  
 It’s entirely reasonable for anyone to visit a conference once, even when they don’t have a paper.  For students, visiting a conference is almost a ‘must’—there is no where else that a broad cross-section of research is on display. 
 Workshops are also a very good idea.   ICML has 11 ,  KDD has 9 , and  AAAI has 19 .  Workshops provide an opportunity to get a good understanding of some current area of research.  They are probably the forum most conducive to starting new lines of research because they are so interactive. 
 Tutorials are a good way to gain some understanding of a long-standing direction of research.  They are generally more coherent than workshops.   ICML has 7  and  AAAI has 15 .</p><p>6 0.83437067 <a title="361-lda-6" href="../hunch_net-2006/hunch_net-2006-02-18-Multiplication_of_Learned_Probabilities_is_Dangerous.html">157 hunch net-2006-02-18-Multiplication of Learned Probabilities is Dangerous</a></p>
<p>7 0.63643223 <a title="361-lda-7" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>8 0.58608693 <a title="361-lda-8" href="../hunch_net-2006/hunch_net-2006-03-02-Why_do_people_count_for_learning%3F.html">160 hunch net-2006-03-02-Why do people count for learning?</a></p>
<p>9 0.58556849 <a title="361-lda-9" href="../hunch_net-2005/hunch_net-2005-02-01-NIPS%3A_Online_Bayes.html">8 hunch net-2005-02-01-NIPS: Online Bayes</a></p>
<p>10 0.582183 <a title="361-lda-10" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>11 0.5787012 <a title="361-lda-11" href="../hunch_net-2008/hunch_net-2008-11-10-ICML_Reviewing_Criteria.html">325 hunch net-2008-11-10-ICML Reviewing Criteria</a></p>
<p>12 0.5733282 <a title="361-lda-12" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>13 0.56706882 <a title="361-lda-13" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>14 0.5598917 <a title="361-lda-14" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<p>15 0.55859011 <a title="361-lda-15" href="../hunch_net-2007/hunch_net-2007-08-19-Choice_of_Metrics.html">259 hunch net-2007-08-19-Choice of Metrics</a></p>
<p>16 0.54918677 <a title="361-lda-16" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>17 0.54799139 <a title="361-lda-17" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>18 0.54366422 <a title="361-lda-18" href="../hunch_net-2007/hunch_net-2007-08-12-Exponentiated_Gradient.html">258 hunch net-2007-08-12-Exponentiated Gradient</a></p>
<p>19 0.54321617 <a title="361-lda-19" href="../hunch_net-2005/hunch_net-2005-03-05-Funding_Research.html">36 hunch net-2005-03-05-Funding Research</a></p>
<p>20 0.54105538 <a title="361-lda-20" href="../hunch_net-2009/hunch_net-2009-02-04-Optimal_Proxy_Loss_for_Classification.html">341 hunch net-2009-02-04-Optimal Proxy Loss for Classification</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
