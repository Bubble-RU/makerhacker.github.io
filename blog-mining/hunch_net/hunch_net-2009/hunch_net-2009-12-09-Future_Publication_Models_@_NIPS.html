<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>382 hunch net-2009-12-09-Future Publication Models @ NIPS</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2009" href="../home/hunch_net-2009_home.html">hunch_net-2009</a> <a title="hunch_net-2009-382" href="#">hunch_net-2009-382</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>382 hunch net-2009-12-09-Future Publication Models @ NIPS</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2009-382-html" href="http://hunch.net/?p=1086">html</a></p><p>Introduction: Yesterday, there was a discussion about  future publication models at NIPS .   Yann  and  Zoubin  have specific detailed proposals which I’ll add links to when I get them ( Yann’s proposal  and  Zoubin’s proposal ).
 
What struck me about the discussion is that there are many simultaneous concerns as well as many simultaneous proposals, which makes it difficult to keep all the distinctions straight in a verbal conversation.  It also seemed like people were serious enough about this that we may see some real movement.  Certainly, my personal experience motivates that as I’ve  posted many times  about the substantial flaws in our review process, including some very poor personal experiences.
 
Concerns include the following:
  
 (Several) Reviewers are overloaded, boosting the noise in decision making. 
 ( Yann ) A new system should run with as little built-in delay and friction to the process of research as possible. 
 ( Hanna Wallach (updated)) Double-blind review is particularly impor</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Yann  and  Zoubin  have specific detailed proposals which I’ll add links to when I get them ( Yann’s proposal  and  Zoubin’s proposal ). [sent-2, score-0.714]
</p><p>2 What struck me about the discussion is that there are many simultaneous concerns as well as many simultaneous proposals, which makes it difficult to keep all the distinctions straight in a verbal conversation. [sent-3, score-0.678]
</p><p>3 Certainly, my personal experience motivates that as I’ve  posted many times  about the substantial flaws in our review process, including some very poor personal experiences. [sent-5, score-0.497]
</p><p>4 ( Yann ) A new system should run with as little built-in delay and friction to the process of research as possible. [sent-7, score-0.192]
</p><p>5 ( Hanna Wallach (updated)) Double-blind review is particularly important for people who are unknown or from an unknown institution. [sent-8, score-0.384]
</p><p>6 (Several) But, it’s bad to take double blind so seriously as to disallow publishing on  arxiv  or personal webpages. [sent-9, score-0.724]
</p><p>7 ( Yann ) And double-blind is bad when it prevents publishing for substantial periods of time. [sent-10, score-0.356]
</p><p>8 ( Zoubin ) Any new system should appear to outsiders as if it’s the old system, or a journal, because it’s already hard enough to justify CS tenure cases to other disciplines. [sent-12, score-0.259]
</p><p>9 There were other concerns as well, but these are the ones that I remember. [sent-14, score-0.203]
</p><p>10 Elements of proposals include:     ( Yann ) Everything should go to Arxiv or an arxiv-like system first, as per physics or mathematics. [sent-15, score-0.355]
</p><p>11 This addresses (1), because it delinks dissemination from review, relieving some of the burden of reviewing. [sent-16, score-0.432]
</p><p>12 It also addresses (2) since with good authors they can immediately begin building on each other’s work. [sent-17, score-0.422]
</p><p>13 ( Fernando ) Create a conference coincident journal in which people can publish at any time. [sent-20, score-0.295]
</p><p>14 It can be done smoothly by allowing submission in either conference deadline mode or journal mode. [sent-22, score-0.291]
</p><p>15 This proposal addresses (1) by reducing peak demand on reviewing. [sent-23, score-0.669]
</p><p>16 This addresses (1), by eliminating some concerns for the reviewer. [sent-26, score-0.556]
</p><p>17 In biology,  such a journal exists  (pointer updated), because delays were becoming absurd and intolerable. [sent-28, score-0.295]
</p><p>18 ( Yann ) There should be multiple publishing entities (people or groups of people) that can bless a paper as interesting. [sent-29, score-0.223]
</p><p>19 There are many other proposal elements (too many for my memory), which hopefully we’ll see in particular proposals. [sent-31, score-0.333]
</p><p>20 If other people have concrete proposals, now is probably the right time to formalize them. [sent-32, score-0.142]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('addresses', 0.353), ('yann', 0.299), ('proposal', 0.243), ('proposals', 0.228), ('journal', 0.222), ('zoubin', 0.206), ('arxiv', 0.206), ('concerns', 0.203), ('publishing', 0.144), ('fernando', 0.131), ('personal', 0.129), ('system', 0.127), ('simultaneous', 0.126), ('updated', 0.118), ('apparently', 0.114), ('unknown', 0.104), ('review', 0.103), ('elements', 0.09), ('blind', 0.09), ('double', 0.085), ('struck', 0.079), ('orthogonal', 0.079), ('straight', 0.079), ('daphne', 0.079), ('entities', 0.079), ('burden', 0.079), ('yesterday', 0.079), ('include', 0.075), ('people', 0.073), ('periods', 0.073), ('conflicts', 0.073), ('biology', 0.073), ('absurd', 0.073), ('motivates', 0.073), ('peak', 0.073), ('cvpr', 0.073), ('bad', 0.07), ('shouldn', 0.069), ('begin', 0.069), ('justify', 0.069), ('prevents', 0.069), ('formalize', 0.069), ('smoothly', 0.069), ('pointer', 0.065), ('correctness', 0.065), ('delay', 0.065), ('distinctions', 0.065), ('overloaded', 0.065), ('tenure', 0.063), ('flaws', 0.063)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="382-tfidf-1" href="../hunch_net-2009/hunch_net-2009-12-09-Future_Publication_Models_%40_NIPS.html">382 hunch net-2009-12-09-Future Publication Models @ NIPS</a></p>
<p>Introduction: Yesterday, there was a discussion about  future publication models at NIPS .   Yann  and  Zoubin  have specific detailed proposals which I’ll add links to when I get them ( Yann’s proposal  and  Zoubin’s proposal ).
 
What struck me about the discussion is that there are many simultaneous concerns as well as many simultaneous proposals, which makes it difficult to keep all the distinctions straight in a verbal conversation.  It also seemed like people were serious enough about this that we may see some real movement.  Certainly, my personal experience motivates that as I’ve  posted many times  about the substantial flaws in our review process, including some very poor personal experiences.
 
Concerns include the following:
  
 (Several) Reviewers are overloaded, boosting the noise in decision making. 
 ( Yann ) A new system should run with as little built-in delay and friction to the process of research as possible. 
 ( Hanna Wallach (updated)) Double-blind review is particularly impor</p><p>2 0.15224043 <a title="382-tfidf-2" href="../hunch_net-2005/hunch_net-2005-03-13-Avoiding_Bad_Reviewing.html">40 hunch net-2005-03-13-Avoiding Bad Reviewing</a></p>
<p>Introduction: If we accept that bad reviewing often occurs and want to fix it, the question is “how”?
 
Reviewing is done by paper writers just like yourself, so a good proxy for this question is asking “How can I be a better reviewer?”  Here are a few things I’ve learned by trial (and error), as a paper writer, and as a reviewer.
  
 The secret ingredient is careful thought.  There is no good substitution for a deep and careful understanding. 
 Avoid reviewing papers that you feel competitive about.  You almost certainly will be asked to review papers that feel competitive if you work on subjects of common interest.  But, the feeling of competition can easily lead to bad judgement. 
 If you feel biased for some other reason, then you should avoid reviewing.  For example… 
 Feeling angry or threatened by a paper is a form of bias.  See above. 
 Double blind yourself (avoid looking at the name even in a single-blind situation).  The significant effect of a name you recognize is making you pay close a</p><p>3 0.14146233 <a title="382-tfidf-3" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>Introduction: One viewpoint on academia is that it is inherently adversarial: there are finite research dollars, positions, and students to work with, implying a zero-sum game between different participants.  This is not a viewpoint that I want to promote, as I consider it flawed.  However, I know several people believe strongly in this viewpoint, and I have found it to have  substantial explanatory power.
 
For example:
  
 It explains why your paper was rejected based on poor logic.  The reviewer wasn’t concerned with research quality, but rather with rejecting a competitor. 
 It explains why professors rarely work together.  The goal of a non-tenured professor (at least) is to get tenure, and a case for tenure comes from a portfolio of work that is undisputably yours. 
 It explains why new research programs are not quickly adopted.  Adopting a competitor’s program is impossible, if your career is based on the competitor being wrong. 
  
Different academic groups subscribe to the adversarial viewp</p><p>4 0.1364153 <a title="382-tfidf-4" href="../hunch_net-2005/hunch_net-2005-05-02-Reviewing_techniques_for_conferences.html">65 hunch net-2005-05-02-Reviewing techniques for conferences</a></p>
<p>Introduction: The many reviews following the many paper deadlines are just about over.  AAAI and ICML in particular were experimenting with several reviewing techniques.  
  
 Double Blind: AAAI and ICML were both double blind this year.  It seemed (overall) beneficial, but two problems arose.
 
 For theoretical papers, with a lot to say, authors often leave out the proofs.  This is very hard to cope with under a double blind review because (1) you can not trust the authors got the proof right but (2) a blanket “reject” hits many probably-good papers.  Perhaps authors should more strongly favor proof-complete papers sent to double blind conferences. 
 On the author side, double blind reviewing is actually somewhat disruptive to research.  In particular, it discourages the author from talking about the subject, which is one of the mechanisms of research.  This is not a great drawback, but it is one not previously appreciated. 
 
 
 Author feedback: AAAI and ICML did author feedback this year. It seem</p><p>5 0.12676543 <a title="382-tfidf-5" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>Introduction: Unfortunately, I ended up sick for much of this ICML.  I did manage to catch one interesting paper:
 
 Richard Socher ,  Cliff Lin ,  Andrew Y. Ng , and  Christopher D. Manning   Parsing Natural Scenes and Natural Language with Recursive Neural Networks .
 
I invited Richard to share his list of interesting papers, so hopefully we’ll hear from him soon.  In the meantime,  Paul  and  Hal  have posted some lists.
 
 the future 
 
 Joelle  and I are program chairs for ICML 2012 in  Edinburgh , which I previously enjoyed visiting in  2005 .  This is a huge responsibility, that we hope to accomplish well.  A part of this (perhaps the most fun part), is imagining how we can make ICML better.  A key and critical constraint is choosing things that can be accomplished.  So far we have:
  
  Colocation . The first thing we looked into was potential colocations.  We quickly discovered that many other conferences precomitted their location.  For the future, getting a colocation with  ACL  or  SIGI</p><p>6 0.125531 <a title="382-tfidf-6" href="../hunch_net-2005/hunch_net-2005-03-29-Academic_Mechanism_Design.html">48 hunch net-2005-03-29-Academic Mechanism Design</a></p>
<p>7 0.12143148 <a title="382-tfidf-7" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>8 0.1177882 <a title="382-tfidf-8" href="../hunch_net-2005/hunch_net-2005-02-09-Intuitions_from_applied_learning.html">16 hunch net-2005-02-09-Intuitions from applied learning</a></p>
<p>9 0.11334945 <a title="382-tfidf-9" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>10 0.11303617 <a title="382-tfidf-10" href="../hunch_net-2010/hunch_net-2010-04-26-Compassionate_Reviewing.html">395 hunch net-2010-04-26-Compassionate Reviewing</a></p>
<p>11 0.10755381 <a title="382-tfidf-11" href="../hunch_net-2013/hunch_net-2013-06-29-The_Benefits_of_Double-Blind_Review.html">485 hunch net-2013-06-29-The Benefits of Double-Blind Review</a></p>
<p>12 0.10442385 <a title="382-tfidf-12" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>13 0.098745205 <a title="382-tfidf-13" href="../hunch_net-2012/hunch_net-2012-08-27-NYAS_ML_2012_and_ICML_2013.html">472 hunch net-2012-08-27-NYAS ML 2012 and ICML 2013</a></p>
<p>14 0.091679454 <a title="382-tfidf-14" href="../hunch_net-2008/hunch_net-2008-10-14-Who_is_Responsible_for_a_Bad_Review%3F.html">320 hunch net-2008-10-14-Who is Responsible for a Bad Review?</a></p>
<p>15 0.091382451 <a title="382-tfidf-15" href="../hunch_net-2012/hunch_net-2012-04-09-ICML_author_feedback_is_open.html">461 hunch net-2012-04-09-ICML author feedback is open</a></p>
<p>16 0.088353172 <a title="382-tfidf-16" href="../hunch_net-2009/hunch_net-2009-06-01-Multitask_Poisoning.html">358 hunch net-2009-06-01-Multitask Poisoning</a></p>
<p>17 0.087507479 <a title="382-tfidf-17" href="../hunch_net-2008/hunch_net-2008-02-10-Complexity_Illness.html">288 hunch net-2008-02-10-Complexity Illness</a></p>
<p>18 0.084646888 <a title="382-tfidf-18" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>19 0.084570505 <a title="382-tfidf-19" href="../hunch_net-2011/hunch_net-2011-12-13-Vowpal_Wabbit_version_6.1_%26%23038%3B_the_NIPS_tutorial.html">451 hunch net-2011-12-13-Vowpal Wabbit version 6.1 &#038; the NIPS tutorial</a></p>
<p>20 0.081643514 <a title="382-tfidf-20" href="../hunch_net-2006/hunch_net-2006-09-12-Incentive_Compatible_Reviewing.html">207 hunch net-2006-09-12-Incentive Compatible Reviewing</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.177), (1, -0.119), (2, 0.061), (3, 0.075), (4, -0.017), (5, 0.025), (6, -0.031), (7, 0.01), (8, -0.001), (9, 0.02), (10, -0.084), (11, -0.037), (12, 0.033), (13, -0.003), (14, -0.01), (15, 0.033), (16, -0.027), (17, -0.012), (18, 0.017), (19, 0.013), (20, -0.021), (21, -0.069), (22, -0.022), (23, 0.005), (24, 0.057), (25, -0.015), (26, -0.032), (27, 0.027), (28, -0.053), (29, 0.047), (30, 0.051), (31, -0.015), (32, 0.049), (33, -0.059), (34, -0.033), (35, -0.026), (36, -0.052), (37, -0.045), (38, -0.042), (39, 0.037), (40, 0.023), (41, -0.055), (42, -0.07), (43, -0.044), (44, 0.097), (45, -0.068), (46, 0.086), (47, -0.146), (48, -0.021), (49, 0.075)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96255052 <a title="382-lsi-1" href="../hunch_net-2009/hunch_net-2009-12-09-Future_Publication_Models_%40_NIPS.html">382 hunch net-2009-12-09-Future Publication Models @ NIPS</a></p>
<p>Introduction: Yesterday, there was a discussion about  future publication models at NIPS .   Yann  and  Zoubin  have specific detailed proposals which I’ll add links to when I get them ( Yann’s proposal  and  Zoubin’s proposal ).
 
What struck me about the discussion is that there are many simultaneous concerns as well as many simultaneous proposals, which makes it difficult to keep all the distinctions straight in a verbal conversation.  It also seemed like people were serious enough about this that we may see some real movement.  Certainly, my personal experience motivates that as I’ve  posted many times  about the substantial flaws in our review process, including some very poor personal experiences.
 
Concerns include the following:
  
 (Several) Reviewers are overloaded, boosting the noise in decision making. 
 ( Yann ) A new system should run with as little built-in delay and friction to the process of research as possible. 
 ( Hanna Wallach (updated)) Double-blind review is particularly impor</p><p>2 0.66575855 <a title="382-lsi-2" href="../hunch_net-2010/hunch_net-2010-04-26-Compassionate_Reviewing.html">395 hunch net-2010-04-26-Compassionate Reviewing</a></p>
<p>Introduction: Most long conversations between academics seem to converge on the topic of reviewing where almost no one is happy.  A basic question is: Should most people be happy?
 
The case against is straightforward.  Anyone who watches the flow of papers realizes that most papers amount to little in the longer term.  By it’s nature research is brutal, where the second-best method is worthless, and the second person to discover things typically gets no credit.  If you think about this for a moment, it’s very different from most other human endeavors.  The second best migrant laborer, construction worker, manager, conductor, quarterback, etc… all can manage quite well. If a reviewer has even a vaguely predictive sense of what’s important in the longer term, then most people submitting papers will be unhappy.
 
But this argument unravels, in my experience.  Perhaps half of reviews are thoughtless or simply wrong with a small part being simply malicious.  And yet, I’m sure that most reviewers genuine</p><p>3 0.61264205 <a title="382-lsi-3" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>Introduction: One viewpoint on academia is that it is inherently adversarial: there are finite research dollars, positions, and students to work with, implying a zero-sum game between different participants.  This is not a viewpoint that I want to promote, as I consider it flawed.  However, I know several people believe strongly in this viewpoint, and I have found it to have  substantial explanatory power.
 
For example:
  
 It explains why your paper was rejected based on poor logic.  The reviewer wasn’t concerned with research quality, but rather with rejecting a competitor. 
 It explains why professors rarely work together.  The goal of a non-tenured professor (at least) is to get tenure, and a case for tenure comes from a portfolio of work that is undisputably yours. 
 It explains why new research programs are not quickly adopted.  Adopting a competitor’s program is impossible, if your career is based on the competitor being wrong. 
  
Different academic groups subscribe to the adversarial viewp</p><p>4 0.60300392 <a title="382-lsi-4" href="../hunch_net-2013/hunch_net-2013-06-29-The_Benefits_of_Double-Blind_Review.html">485 hunch net-2013-06-29-The Benefits of Double-Blind Review</a></p>
<p>Introduction: This post is a (near) transcript of a talk that I gave at the  ICML 2013 Workshop on Peer Review and Publishing Models . Although there’s a  PDF available on my website , I’ve chosen to post a slightly modified version here as well in order to better facilitate discussion.
 
 Disclaimers and Context 
 
I want to start with a couple of disclaimers and some context.
 
First, I want to point out that although I’ve read a lot about double-blind review, this isn’t my research area and the research discussed in this post is not my own. As a result, I probably can’t answer super detailed questions about these studies.
 
I also want to note that I’m not opposed to open peer review — I was a free and open source software developer for over ten years and I care a great deal about openness and transparency. Rather, my motivation in writing this post is simply to create awareness of and to initiate discussion about the benefits of double-blind review.
 
Lastly, and most importantly, I think it’s e</p><p>5 0.59239131 <a title="382-lsi-5" href="../hunch_net-2005/hunch_net-2005-03-13-Avoiding_Bad_Reviewing.html">40 hunch net-2005-03-13-Avoiding Bad Reviewing</a></p>
<p>Introduction: If we accept that bad reviewing often occurs and want to fix it, the question is “how”?
 
Reviewing is done by paper writers just like yourself, so a good proxy for this question is asking “How can I be a better reviewer?”  Here are a few things I’ve learned by trial (and error), as a paper writer, and as a reviewer.
  
 The secret ingredient is careful thought.  There is no good substitution for a deep and careful understanding. 
 Avoid reviewing papers that you feel competitive about.  You almost certainly will be asked to review papers that feel competitive if you work on subjects of common interest.  But, the feeling of competition can easily lead to bad judgement. 
 If you feel biased for some other reason, then you should avoid reviewing.  For example… 
 Feeling angry or threatened by a paper is a form of bias.  See above. 
 Double blind yourself (avoid looking at the name even in a single-blind situation).  The significant effect of a name you recognize is making you pay close a</p><p>6 0.55787224 <a title="382-lsi-6" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>7 0.54170942 <a title="382-lsi-7" href="../hunch_net-2008/hunch_net-2008-09-03-Bidding_Problems.html">315 hunch net-2008-09-03-Bidding Problems</a></p>
<p>8 0.53723782 <a title="382-lsi-8" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>9 0.53321153 <a title="382-lsi-9" href="../hunch_net-2005/hunch_net-2005-05-02-Reviewing_techniques_for_conferences.html">65 hunch net-2005-05-02-Reviewing techniques for conferences</a></p>
<p>10 0.52884173 <a title="382-lsi-10" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>11 0.52865082 <a title="382-lsi-11" href="../hunch_net-2012/hunch_net-2012-04-09-ICML_author_feedback_is_open.html">461 hunch net-2012-04-09-ICML author feedback is open</a></p>
<p>12 0.52235121 <a title="382-lsi-12" href="../hunch_net-2006/hunch_net-2006-09-12-Incentive_Compatible_Reviewing.html">207 hunch net-2006-09-12-Incentive Compatible Reviewing</a></p>
<p>13 0.52128243 <a title="382-lsi-13" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>14 0.50641507 <a title="382-lsi-14" href="../hunch_net-2013/hunch_net-2013-01-31-Remote_large_scale_learning_class_participation.html">479 hunch net-2013-01-31-Remote large scale learning class participation</a></p>
<p>15 0.49322671 <a title="382-lsi-15" href="../hunch_net-2006/hunch_net-2006-04-14-JMLR_is_a_success.html">172 hunch net-2006-04-14-JMLR is a success</a></p>
<p>16 0.48304054 <a title="382-lsi-16" href="../hunch_net-2007/hunch_net-2007-07-28-Asking_questions.html">257 hunch net-2007-07-28-Asking questions</a></p>
<p>17 0.4815281 <a title="382-lsi-17" href="../hunch_net-2013/hunch_net-2013-06-10-The_Large_Scale_Learning_class_notes.html">483 hunch net-2013-06-10-The Large Scale Learning class notes</a></p>
<p>18 0.47711062 <a title="382-lsi-18" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>19 0.46564022 <a title="382-lsi-19" href="../hunch_net-2005/hunch_net-2005-02-09-Intuitions_from_applied_learning.html">16 hunch net-2005-02-09-Intuitions from applied learning</a></p>
<p>20 0.45602986 <a title="382-lsi-20" href="../hunch_net-2012/hunch_net-2012-06-29-ICML_survey_and_comments.html">468 hunch net-2012-06-29-ICML survey and comments</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(3, 0.016), (10, 0.021), (27, 0.18), (38, 0.056), (53, 0.113), (55, 0.125), (68, 0.012), (85, 0.248), (94, 0.083), (95, 0.056)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.94511676 <a title="382-lda-1" href="../hunch_net-2005/hunch_net-2005-05-12-Math_on_the_Web.html">70 hunch net-2005-05-12-Math on the Web</a></p>
<p>Introduction: Andrej Bauer has setup a  Mathematics and Computation  Blog.  As a first step he has tried to address the persistent and annoying problem of math on the web.  As a basic tool for precisely stating and transfering understanding of technical subjects, mathematics is very necessary.  Despite this necessity, every mechanism for expressing mathematics on the web seems unnaturally clumsy.  Here are some of the methods and their drawbacks:
  
  MathML   This was supposed to be the answer, but it has two severe drawbacks: “Internet Explorer” doesn’t read it and the language is an example of push-XML-to-the-limit which no one would ever consider writing in.  (In contrast, html is easy to write in.)  It’s also very annoying that math fonts must be installed independent of the browser, even for mozilla based browsers. 
 Create inline images.  This has several big drawbacks: font size is fixed for all viewers, you can’t cut & paste inside the images, and you can’t hyperlink from (say) symbol to de</p><p>same-blog 2 0.8948974 <a title="382-lda-2" href="../hunch_net-2009/hunch_net-2009-12-09-Future_Publication_Models_%40_NIPS.html">382 hunch net-2009-12-09-Future Publication Models @ NIPS</a></p>
<p>Introduction: Yesterday, there was a discussion about  future publication models at NIPS .   Yann  and  Zoubin  have specific detailed proposals which I’ll add links to when I get them ( Yann’s proposal  and  Zoubin’s proposal ).
 
What struck me about the discussion is that there are many simultaneous concerns as well as many simultaneous proposals, which makes it difficult to keep all the distinctions straight in a verbal conversation.  It also seemed like people were serious enough about this that we may see some real movement.  Certainly, my personal experience motivates that as I’ve  posted many times  about the substantial flaws in our review process, including some very poor personal experiences.
 
Concerns include the following:
  
 (Several) Reviewers are overloaded, boosting the noise in decision making. 
 ( Yann ) A new system should run with as little built-in delay and friction to the process of research as possible. 
 ( Hanna Wallach (updated)) Double-blind review is particularly impor</p><p>3 0.82813269 <a title="382-lda-3" href="../hunch_net-2013/hunch_net-2013-04-15-NEML_II.html">481 hunch net-2013-04-15-NEML II</a></p>
<p>Introduction: Adam Kalai  points out the  New England Machine Learning Day  May 1 at MSR New England.  There is a poster session with abstracts due April 19.  I understand last year’s  NEML  went well and it’s great to meet your neighbors at regional workshops like this.</p><p>4 0.80469429 <a title="382-lda-4" href="../hunch_net-2012/hunch_net-2012-06-05-ICML_acceptance_statistics.html">466 hunch net-2012-06-05-ICML acceptance statistics</a></p>
<p>Introduction: People are naturally interested in slicing the ICML acceptance statistics in various ways.  Here’s a rundown for the top categories.
  
 
 18/66 = 0.27 
 in (0.18,0.36) 
  Reinforcement Learning 
 
 
 10/52 = 0.19 
  in (0.17,0.37) 
  Supervised Learning 
 
 
  9/51 = 0.18 
   not in (0.18, 0.37)  
  Clustering 
 
 
  12/46 = 0.26 
  in (0.17, 0.37) 
  Kernel Methods 
 
 
  11/40 = 0.28 
  in (0.15, 0.4) 
  Optimization Algorithms 
 
 
  8/33 = 0.24 
  in (0.15, 0.39) 
  Learning Theory 
 
 
  14/33 = 0.42 
   not in (0.15, 0.39)  
  Graphical Models 
 
 
  10/32 = 0.31 
  in (0.15, 0.41) 
  Applications (+5 invited) 
 
 
  8/29 = 0.28 
  in (0.14, 0.41]) 
 Probabilistic Models 
 
 
  13/29 = 0.45 
  not in (0.14, 0.41)  
  NN & Deep Learning 
 
 
   8/26 = 0.31 
  in (0.12, 0.42) 
  Transfer and Multi-Task Learning 
 
 
  13/25 = 0.52 
  not in (0.12, 0.44)  
  Online Learning 
 
 
  5/25 = 0.20 
  in (0.12, 0.44) 
  Active Learning 
 
 
  6/22 = 0.27 
  in (0.14, 0.41) 
  Semi-Superv</p><p>5 0.72797495 <a title="382-lda-5" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>Introduction: At NIPS,  Andrew Ng  asked me what should be in a large scale learning class.  After some discussion with him and  Nando  and mulling it over a bit, these are the topics that I think should be covered.
 
There are many different kinds of scaling.
  
  Scaling in examples   This is the most basic kind of scaling.
 
  Online Gradient Descent  This is an old algorithm—I’m not sure if anyone can be credited with it in particular.  Perhaps the  Perceptron  is a good precursor, but substantial improvements come from the notion of a loss function of which  squared loss ,  logistic loss , Hinge Loss, and  Quantile Loss  are all worth covering.  It’s important to cover the  semantics  of these loss functions as well.   Vowpal Wabbit  is a reasonably fast codebase implementing these. 
  Second Order Gradient Descent methods  For some problems, methods taking into account second derivative information can be more effective.  I’ve seen preconditioned conjugate gradient work well, for which  Jonath</p><p>6 0.71640176 <a title="382-lda-6" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>7 0.70635653 <a title="382-lda-7" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>8 0.70474344 <a title="382-lda-8" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>9 0.70268768 <a title="382-lda-9" href="../hunch_net-2006/hunch_net-2006-09-12-Incentive_Compatible_Reviewing.html">207 hunch net-2006-09-12-Incentive Compatible Reviewing</a></p>
<p>10 0.69948274 <a title="382-lda-10" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>11 0.69879663 <a title="382-lda-11" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>12 0.69810003 <a title="382-lda-12" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>13 0.69692713 <a title="382-lda-13" href="../hunch_net-2007/hunch_net-2007-07-20-Motivation_should_be_the_Responsibility_of_the_Reviewer.html">256 hunch net-2007-07-20-Motivation should be the Responsibility of the Reviewer</a></p>
<p>14 0.69646251 <a title="382-lda-14" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<p>15 0.69484419 <a title="382-lda-15" href="../hunch_net-2007/hunch_net-2007-01-02-Retrospective.html">225 hunch net-2007-01-02-Retrospective</a></p>
<p>16 0.69484282 <a title="382-lda-16" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>17 0.69397235 <a title="382-lda-17" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>18 0.69209224 <a title="382-lda-18" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>19 0.69186467 <a title="382-lda-19" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>20 0.69181275 <a title="382-lda-20" href="../hunch_net-2006/hunch_net-2006-01-25-1_year.html">151 hunch net-2006-01-25-1 year</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
