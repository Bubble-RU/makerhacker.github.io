<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>384 hunch net-2009-12-24-Top graduates this season</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2009" href="../home/hunch_net-2009_home.html">hunch_net-2009</a> <a title="hunch_net-2009-384" href="#">hunch_net-2009-384</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>384 hunch net-2009-12-24-Top graduates this season</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2009-384-html" href="http://hunch.net/?p=1131">html</a></p><p>Introduction: I would like to point out 3 graduates this season as having my confidence they
are capable of doing great things.Daniel Hsuhas diverse papers with diverse
coauthors on {active learning, mulitlabeling, temporal learning, â&euro;Ś} each
covering new algorithms and methods of analysis. He is also a capable
programmer, having helped me with some nitty-gritty details of cluster
parallelVowpal Wabbitthis summer. He has an excellent tendency to just get
things done.Nicolas Lambertdoesn't nominally work in machine learning, but
I've found his work inelicitationrelevant nevertheless. In essence, elicitable
properties are closely related to learnable properties, and the elicitation
complexity is related to a notion of learning complexity. See theSurrogate
regret bounds paperfor some related discussion. Few people successfully work
at such a general level that it crosses fields, but he's one of them.Yisong
Yueis deeply focused on interactive learning, which he has attacked at all
levels: theory, algorit</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('properties', 0.247), ('excellent', 0.232), ('diverse', 0.219), ('related', 0.206), ('capable', 0.199), ('nominally', 0.179), ('multidimensional', 0.179), ('temporal', 0.166), ('work', 0.158), ('sure', 0.157), ('successfully', 0.156), ('elicitation', 0.156), ('graduates', 0.156), ('season', 0.149), ('programmer', 0.149), ('appreciated', 0.149), ('coauthors', 0.143), ('learnable', 0.143), ('closely', 0.138), ('levels', 0.138)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="384-tfidf-1" href="../hunch_net-2009/hunch_net-2009-12-24-Top_graduates_this_season.html">384 hunch net-2009-12-24-Top graduates this season</a></p>
<p>Introduction: I would like to point out 3 graduates this season as having my confidence they
are capable of doing great things.Daniel Hsuhas diverse papers with diverse
coauthors on {active learning, mulitlabeling, temporal learning, â&euro;Ś} each
covering new algorithms and methods of analysis. He is also a capable
programmer, having helped me with some nitty-gritty details of cluster
parallelVowpal Wabbitthis summer. He has an excellent tendency to just get
things done.Nicolas Lambertdoesn't nominally work in machine learning, but
I've found his work inelicitationrelevant nevertheless. In essence, elicitable
properties are closely related to learnable properties, and the elicitation
complexity is related to a notion of learning complexity. See theSurrogate
regret bounds paperfor some related discussion. Few people successfully work
at such a general level that it crosses fields, but he's one of them.Yisong
Yueis deeply focused on interactive learning, which he has attacked at all
levels: theory, algorit</p><p>2 0.11735849 <a title="384-tfidf-2" href="../hunch_net-2008/hunch_net-2008-11-28-A_Bumper_Crop_of_Machine_Learning_Graduates.html">329 hunch net-2008-11-28-A Bumper Crop of Machine Learning Graduates</a></p>
<p>Introduction: My impression is that this is a particularly strong year for machine learning
graduates. Here's my short list of the strong graduates I know. Analpha (for
perversity's sake) by last name:Jenn Wortmann. When Jenn visited us for the
summer, she hadone,two,three,fourpapers. That is typical--she's smart,
capable, and follows up many directions of research. I believe approximately
all of her many papers are on different subjects.Ruslan Salakhutdinov.
AScience paper on bijective dimensionality reduction, mastered and improved on
deep belief nets which seems like an important flavor of nonlinear learning,
and in my experience he's very fast, capable and creative at problem
solving.Marc'Aurelio Ranzato. I haven't spoken with Marc very much, but he had
a great visit at Yahoo! this summer, and has an impressive portfolio of
applications and improvements on convolutional neural networks and other deep
learning algorithms.Lihong Li. Lihong developed theKWIK ("Knows what it
Knows") learning framewo</p><p>3 0.11000274 <a title="384-tfidf-3" href="../hunch_net-2008/hunch_net-2008-03-23-Interactive_Machine_Learning.html">293 hunch net-2008-03-23-Interactive Machine Learning</a></p>
<p>Introduction: A new direction of research seems to be arising in machine learning:
Interactive Machine Learning. This isn't a familiar term, although it does
include some familiar subjects.What is Interactive Machine Learning?The
fundamental requirement is (a) learning algorithms which interact with the
world and (b) learn.For our purposes, let's define learning as efficiently
competing with a large set of possible predictors. Examples include:Online
learning against an adversary (Avrim's Notes). The interaction is almost
trivial: the learning algorithm makes a prediction and then receives feedback.
The learning is choosing based upon the advice of many experts.Active
Learning. In active learning, the interaction is choosing which examples to
label, and the learning is choosing from amongst a large set of
hypotheses.Contextual Bandits. The interaction is choosing one of several
actions and learning only the value of the chosen action (weaker than active
learning feedback).More forms of interaction w</p><p>4 0.10360044 <a title="384-tfidf-4" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>Introduction: One of the remarkable things about machine learning is how diverse it is. The
viewpoints of Bayesian learning, reinforcement learning, graphical models,
supervised learning, unsupervised learning, genetic programming, etc… share
little enough overlap that many people can and do make their careers within
one without touching, or even necessarily understanding the others.There are
two fundamental reasons why this is possible.For many problems, many
approaches work in the sense that they do something useful. This is true
empirically, where for many problems we can observe that many different
approaches yield better performance than any constant predictor. It's also
true in theory, where we know that for any set of predictors representable in
a finite amount of RAM, minimizing training error over the set of predictors
does something nontrivial when there are a sufficient number of examples.There
is nothing like a unifying problem defining the field. In many other areas
there are unifying p</p><p>5 0.10063119 <a title="384-tfidf-5" href="../hunch_net-2007/hunch_net-2007-09-30-NIPS_workshops_are_out..html">264 hunch net-2007-09-30-NIPS workshops are out.</a></p>
<p>Introduction: Here. I'm particularly interested in theWeb Search,Efficient ML, and (of
course)Learning Problem Designworkshops but there are many others to check out
as well. Workshops are a great chance to make progress on or learn about a
topic. Relevance and interaction amongst diverse people can sometimes be
magical.</p><p>6 0.098098569 <a title="384-tfidf-6" href="../hunch_net-2005/hunch_net-2005-03-15-The_State_of_Tight_Bounds.html">41 hunch net-2005-03-15-The State of Tight Bounds</a></p>
<p>7 0.097932331 <a title="384-tfidf-7" href="../hunch_net-2006/hunch_net-2006-10-22-Exemplar_programming.html">215 hunch net-2006-10-22-Exemplar programming</a></p>
<p>8 0.095529318 <a title="384-tfidf-8" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>9 0.094929092 <a title="384-tfidf-9" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>10 0.093999214 <a title="384-tfidf-10" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>11 0.093747228 <a title="384-tfidf-11" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>12 0.090515956 <a title="384-tfidf-12" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<p>13 0.085773878 <a title="384-tfidf-13" href="../hunch_net-2007/hunch_net-2007-12-19-Cool_and_interesting_things_seen_at_NIPS.html">279 hunch net-2007-12-19-Cool and interesting things seen at NIPS</a></p>
<p>14 0.084976308 <a title="384-tfidf-14" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>15 0.084923573 <a title="384-tfidf-15" href="../hunch_net-2006/hunch_net-2006-10-08-Incompatibilities_between_classical_confidence_intervals_and_learning..html">213 hunch net-2006-10-08-Incompatibilities between classical confidence intervals and learning.</a></p>
<p>16 0.083755098 <a title="384-tfidf-16" href="../hunch_net-2010/hunch_net-2010-12-02-Traffic_Prediction_Problem.html">418 hunch net-2010-12-02-Traffic Prediction Problem</a></p>
<p>17 0.08235085 <a title="384-tfidf-17" href="../hunch_net-2009/hunch_net-2009-10-26-NIPS_workshops.html">375 hunch net-2009-10-26-NIPS workshops</a></p>
<p>18 0.079291508 <a title="384-tfidf-18" href="../hunch_net-2009/hunch_net-2009-07-11-Interesting_papers_at_KDD.html">364 hunch net-2009-07-11-Interesting papers at KDD</a></p>
<p>19 0.076641649 <a title="384-tfidf-19" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>20 0.076359041 <a title="384-tfidf-20" href="../hunch_net-2005/hunch_net-2005-06-18-Lower_Bounds_for_Learning_Reductions.html">83 hunch net-2005-06-18-Lower Bounds for Learning Reductions</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.195), (1, -0.001), (2, 0.052), (3, 0.043), (4, -0.056), (5, -0.062), (6, 0.021), (7, -0.025), (8, -0.047), (9, -0.048), (10, -0.038), (11, 0.038), (12, 0.057), (13, 0.018), (14, 0.007), (15, 0.04), (16, -0.005), (17, -0.008), (18, 0.092), (19, -0.042), (20, 0.049), (21, -0.03), (22, -0.111), (23, 0.078), (24, 0.014), (25, 0.081), (26, -0.003), (27, 0.045), (28, 0.006), (29, -0.011), (30, 0.07), (31, -0.07), (32, -0.117), (33, 0.035), (34, 0.032), (35, -0.038), (36, -0.01), (37, 0.011), (38, 0.069), (39, -0.079), (40, -0.04), (41, 0.115), (42, 0.008), (43, 0.003), (44, 0.021), (45, -0.054), (46, 0.03), (47, 0.064), (48, -0.042), (49, -0.072)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94987196 <a title="384-lsi-1" href="../hunch_net-2009/hunch_net-2009-12-24-Top_graduates_this_season.html">384 hunch net-2009-12-24-Top graduates this season</a></p>
<p>Introduction: I would like to point out 3 graduates this season as having my confidence they
are capable of doing great things.Daniel Hsuhas diverse papers with diverse
coauthors on {active learning, mulitlabeling, temporal learning, â&euro;Ś} each
covering new algorithms and methods of analysis. He is also a capable
programmer, having helped me with some nitty-gritty details of cluster
parallelVowpal Wabbitthis summer. He has an excellent tendency to just get
things done.Nicolas Lambertdoesn't nominally work in machine learning, but
I've found his work inelicitationrelevant nevertheless. In essence, elicitable
properties are closely related to learnable properties, and the elicitation
complexity is related to a notion of learning complexity. See theSurrogate
regret bounds paperfor some related discussion. Few people successfully work
at such a general level that it crosses fields, but he's one of them.Yisong
Yueis deeply focused on interactive learning, which he has attacked at all
levels: theory, algorit</p><p>2 0.67029703 <a title="384-lsi-2" href="../hunch_net-2008/hunch_net-2008-11-28-A_Bumper_Crop_of_Machine_Learning_Graduates.html">329 hunch net-2008-11-28-A Bumper Crop of Machine Learning Graduates</a></p>
<p>Introduction: My impression is that this is a particularly strong year for machine learning
graduates. Here's my short list of the strong graduates I know. Analpha (for
perversity's sake) by last name:Jenn Wortmann. When Jenn visited us for the
summer, she hadone,two,three,fourpapers. That is typical--she's smart,
capable, and follows up many directions of research. I believe approximately
all of her many papers are on different subjects.Ruslan Salakhutdinov.
AScience paper on bijective dimensionality reduction, mastered and improved on
deep belief nets which seems like an important flavor of nonlinear learning,
and in my experience he's very fast, capable and creative at problem
solving.Marc'Aurelio Ranzato. I haven't spoken with Marc very much, but he had
a great visit at Yahoo! this summer, and has an impressive portfolio of
applications and improvements on convolutional neural networks and other deep
learning algorithms.Lihong Li. Lihong developed theKWIK ("Knows what it
Knows") learning framewo</p><p>3 0.61968082 <a title="384-lsi-3" href="../hunch_net-2007/hunch_net-2007-09-30-NIPS_workshops_are_out..html">264 hunch net-2007-09-30-NIPS workshops are out.</a></p>
<p>Introduction: Here. I'm particularly interested in theWeb Search,Efficient ML, and (of
course)Learning Problem Designworkshops but there are many others to check out
as well. Workshops are a great chance to make progress on or learn about a
topic. Relevance and interaction amongst diverse people can sometimes be
magical.</p><p>4 0.59950191 <a title="384-lsi-4" href="../hunch_net-2010/hunch_net-2010-09-28-Machined_Learnings.html">412 hunch net-2010-09-28-Machined Learnings</a></p>
<p>Introduction: Paul Mineirohas startedMachined Learningswhere he's seriously attempting to do
ML research in public. I personally need to read through in greater detail, as
much of it is learning reduction related, trying to deal with the sorts of
complex source problems that come up in practice.</p><p>5 0.57904357 <a title="384-lsi-5" href="../hunch_net-2008/hunch_net-2008-03-23-Interactive_Machine_Learning.html">293 hunch net-2008-03-23-Interactive Machine Learning</a></p>
<p>Introduction: A new direction of research seems to be arising in machine learning:
Interactive Machine Learning. This isn't a familiar term, although it does
include some familiar subjects.What is Interactive Machine Learning?The
fundamental requirement is (a) learning algorithms which interact with the
world and (b) learn.For our purposes, let's define learning as efficiently
competing with a large set of possible predictors. Examples include:Online
learning against an adversary (Avrim's Notes). The interaction is almost
trivial: the learning algorithm makes a prediction and then receives feedback.
The learning is choosing based upon the advice of many experts.Active
Learning. In active learning, the interaction is choosing which examples to
label, and the learning is choosing from amongst a large set of
hypotheses.Contextual Bandits. The interaction is choosing one of several
actions and learning only the value of the chosen action (weaker than active
learning feedback).More forms of interaction w</p><p>6 0.56993186 <a title="384-lsi-6" href="../hunch_net-2010/hunch_net-2010-01-13-Sam_Roweis_died.html">386 hunch net-2010-01-13-Sam Roweis died</a></p>
<p>7 0.56192487 <a title="384-lsi-7" href="../hunch_net-2007/hunch_net-2007-12-19-Cool_and_interesting_things_seen_at_NIPS.html">279 hunch net-2007-12-19-Cool and interesting things seen at NIPS</a></p>
<p>8 0.56179976 <a title="384-lsi-8" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>9 0.53753853 <a title="384-lsi-9" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>10 0.53130031 <a title="384-lsi-10" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>11 0.52658802 <a title="384-lsi-11" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>12 0.52549386 <a title="384-lsi-12" href="../hunch_net-2006/hunch_net-2006-10-22-Exemplar_programming.html">215 hunch net-2006-10-22-Exemplar programming</a></p>
<p>13 0.52470034 <a title="384-lsi-13" href="../hunch_net-2010/hunch_net-2010-12-26-NIPS_2010.html">420 hunch net-2010-12-26-NIPS 2010</a></p>
<p>14 0.52248377 <a title="384-lsi-14" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>15 0.51863956 <a title="384-lsi-15" href="../hunch_net-2007/hunch_net-2007-07-20-Motivation_should_be_the_Responsibility_of_the_Reviewer.html">256 hunch net-2007-07-20-Motivation should be the Responsibility of the Reviewer</a></p>
<p>16 0.51546359 <a title="384-lsi-16" href="../hunch_net-2008/hunch_net-2008-08-24-Mass_Customized_Medicine_in_the_Future%3F.html">314 hunch net-2008-08-24-Mass Customized Medicine in the Future?</a></p>
<p>17 0.51476955 <a title="384-lsi-17" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>18 0.51376718 <a title="384-lsi-18" href="../hunch_net-2009/hunch_net-2009-11-06-Yisong_Yue_on_Self-improving_Systems.html">376 hunch net-2009-11-06-Yisong Yue on Self-improving Systems</a></p>
<p>19 0.51085883 <a title="384-lsi-19" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>20 0.50795841 <a title="384-lsi-20" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(8, 0.323), (42, 0.34), (74, 0.09), (95, 0.114), (98, 0.024)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.90403229 <a title="384-lda-1" href="../hunch_net-2011/hunch_net-2011-08-01-Interesting_papers_at_COLT_2011.html">439 hunch net-2011-08-01-Interesting papers at COLT 2011</a></p>
<p>Introduction: SinceJohndid not attendCOLTthis year, I have been volunteered to report back
on the hot stuff at this year's meeting. The conference seemed to have pretty
high quality stuff this year, and I found plenty of interesting papers on all
the three days. I'm gonna pick some of my favorites going through the program
in a chronological order.The first session on matrices seemed interesting for
two reasons. First, the papers were quite nice. But more interestingly, this
is a topic that has had a lot of presence in Statistics and Compressed sensing
literature recently. So it was good to see high-dimensional matrices finally
make their entry at COLT. The paper ofOhadandShaionCollaborative Filtering
with the Trace Norm: Learning, Bounding, and Transducingprovides non-trivial
guarantees on trace norm regularization in an agnostic setup, while Rina
andNatishow how Rademacher averages can be used to get sharper results for
matrix completion problems in their paperConcentration-Based Guarantees for
Lo</p><p>2 0.86817163 <a title="384-lda-2" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>Introduction: TheInternational Planning Competition(IPC) is a biennial event organized in
the context of theInternational Conference on Automated Planning and
Scheduling(ICAPS). This year, for the first time, there will a learning track
of the competition. For more information you can go to the competitionweb-
site.The competitions are typically organized around a number of planning
domains that can vary from year to year, where a planning domain is simply a
class of problems that share a common action schema--e.g. Blocksworld is a
well-known planning domain that contains a problem instance each possible
initial tower configuration and goal configuration. Some other domains have
included Logistics, Airport, Freecell, PipesWorld, and manyothers. For each
domain the competition includes a number of problems (say 40-50) and the
planners are run on each problem with a time limit for each problem (around 30
minutes). The problems are hard enough that many problems are not solved
within the time limit.Giv</p><p>3 0.85985321 <a title="384-lda-3" href="../hunch_net-2006/hunch_net-2006-07-08-MaxEnt_contradicts_Bayes_Rule%3F.html">191 hunch net-2006-07-08-MaxEnt contradicts Bayes Rule?</a></p>
<p>Introduction: A few weeks ago I readthis. David Blei and I spent some time thinking hard
about this a few years back (thanks to Kary Myers for pointing us to it):In
short I was thinking that Ã¢â‚¬Å“bayesian belief updatingÃ¢â‚¬Â and
Ã¢â‚¬Å“maximum entropyÃ¢â‚¬Â were two othogonal principles. But it appear
that they are not, and that they can even be in conflict !Example (from Kass
1996); consider a Die (6 sides), consider prior knowledge E[X]=3.5.Maximum
entropy leads to P(X)= (1/6, 1/6, 1/6, 1/6, 1/6, 1/6).Now consider a new piece
of evidence A=Ã¢â‚¬ÂX is an odd numberÃ¢â‚¬ÂBayesian posterior P(X|A)=
P(A|X) P(X) = (1/3, 0, 1/3, 0, 1/3, 0).But MaxEnt with the constraints
E[X]=3.5 and E[Indicator function of A]=1 leads to (.22, 0, .32, 0, .47, 0) !!
(note that E[Indicator function of A]=P(A))Indeed, for MaxEnt, because there
is no more Ã¢â‚¬Ëœ6Ã¢â‚¬Â², big numbers must be more probable to ensure an
average of 3.5. For bayesian updating, P(X|A) doesnÃ¢â‚¬â„¢t have to have a
3.5 expectation. P(X) a</p><p>same-blog 4 0.85515589 <a title="384-lda-4" href="../hunch_net-2009/hunch_net-2009-12-24-Top_graduates_this_season.html">384 hunch net-2009-12-24-Top graduates this season</a></p>
<p>Introduction: I would like to point out 3 graduates this season as having my confidence they
are capable of doing great things.Daniel Hsuhas diverse papers with diverse
coauthors on {active learning, mulitlabeling, temporal learning, â&euro;Ś} each
covering new algorithms and methods of analysis. He is also a capable
programmer, having helped me with some nitty-gritty details of cluster
parallelVowpal Wabbitthis summer. He has an excellent tendency to just get
things done.Nicolas Lambertdoesn't nominally work in machine learning, but
I've found his work inelicitationrelevant nevertheless. In essence, elicitable
properties are closely related to learnable properties, and the elicitation
complexity is related to a notion of learning complexity. See theSurrogate
regret bounds paperfor some related discussion. Few people successfully work
at such a general level that it crosses fields, but he's one of them.Yisong
Yueis deeply focused on interactive learning, which he has attacked at all
levels: theory, algorit</p><p>5 0.82420719 <a title="384-lda-5" href="../hunch_net-2006/hunch_net-2006-11-06-Data_Linkage_Problems.html">217 hunch net-2006-11-06-Data Linkage Problems</a></p>
<p>Introduction: Data linkage is a problem which seems to come up in various applied machine
learning problems. I have heard it mentioned in various data mining contexts,
but it seems relatively less studied for systemic reasons.A very simple
version of the data linkage problem is a cross hospital patient record merge.
Suppose a patient (John Doe) is admitted to a hospital (General Health),
treated, and released. Later, John Doe is admitted to a second hospital
(Health General), treated, and released. Given a large number of records of
this sort, it becomes very tempting to try and predict the outcomes of
treatments. This is reasonably straightforward as a machine learning problem
if there is a shared unique identifier for John Doe used by General Health and
Health General along with time stamps. We can merge the records and create
examples of the form "Given symptoms and treatment, did the patient come back
to a hospital within the next year?" These examples could be fed into a
learning algorithm, and</p><p>6 0.72360843 <a title="384-lda-6" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>7 0.72192866 <a title="384-lda-7" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>8 0.71994603 <a title="384-lda-8" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>9 0.71937126 <a title="384-lda-9" href="../hunch_net-2008/hunch_net-2008-03-23-Interactive_Machine_Learning.html">293 hunch net-2008-03-23-Interactive Machine Learning</a></p>
<p>10 0.718656 <a title="384-lda-10" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>11 0.71825147 <a title="384-lda-11" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>12 0.71816748 <a title="384-lda-12" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>13 0.71666026 <a title="384-lda-13" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>14 0.71660841 <a title="384-lda-14" href="../hunch_net-2013/hunch_net-2013-12-01-NIPS_tutorials_and_Vowpal_Wabbit_7.4.html">492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</a></p>
<p>15 0.71577352 <a title="384-lda-15" href="../hunch_net-2005/hunch_net-2005-02-25-Problem%3A_Online_Learning.html">28 hunch net-2005-02-25-Problem: Online Learning</a></p>
<p>16 0.71574354 <a title="384-lda-16" href="../hunch_net-2006/hunch_net-2006-01-13-Benchmarks_for_RL.html">148 hunch net-2006-01-13-Benchmarks for RL</a></p>
<p>17 0.7153371 <a title="384-lda-17" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>18 0.71492183 <a title="384-lda-18" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>19 0.714311 <a title="384-lda-19" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>20 0.71416736 <a title="384-lda-20" href="../hunch_net-2006/hunch_net-2006-10-22-Exemplar_programming.html">215 hunch net-2006-10-22-Exemplar programming</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
