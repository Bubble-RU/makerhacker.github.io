<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>350 hunch net-2009-04-23-Jonathan Chang at Slycoder</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2009" href="../home/hunch_net-2009_home.html">hunch_net-2009</a> <a title="hunch_net-2009-350" href="#">hunch_net-2009-350</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>350 hunch net-2009-04-23-Jonathan Chang at Slycoder</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2009-350-html" href="http://hunch.net/?p=661">html</a></p><p>Introduction: Jonathan Changhas aresearch blogon aspects of machine learning.</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('aspects', 0.965), ('machine', 0.241), ('learning', 0.101)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="350-tfidf-1" href="../hunch_net-2009/hunch_net-2009-04-23-Jonathan_Chang_at_Slycoder.html">350 hunch net-2009-04-23-Jonathan Chang at Slycoder</a></p>
<p>Introduction: Jonathan Changhas aresearch blogon aspects of machine learning.</p><p>2 0.15549456 <a title="350-tfidf-2" href="../hunch_net-2012/hunch_net-2012-02-20-Berkeley_Streaming_Data_Workshop.html">455 hunch net-2012-02-20-Berkeley Streaming Data Workshop</a></p>
<p>Introduction: TheFrom Data to Knowledgeworkshop May 7-11 atBerkeleyshould be of interest to
the many people encountering streaming data in different disciplines. It's run
by a group of astronomers who encounter streaming data all the time. I metJosh
Bloomrecently and he is broadly interested in a workshop covering all aspects
of Machine Learning on streaming data. The hope here is that techniques
developed in one area turn out useful in another which seems quite plausible.
Particularly if you are in the bay area, consider checking it out.</p><p>3 0.14453715 <a title="350-tfidf-3" href="../hunch_net-2009/hunch_net-2009-11-09-NYAS_ML_Symposium_this_year..html">377 hunch net-2009-11-09-NYAS ML Symposium this year.</a></p>
<p>Introduction: TheNYAS ML symposiumgrew again this year to 170 participants, despite the need
to outsmart or otherwise tunnel througha crowd.Perhaps the most distinct talk
was by Bob Bell on various aspects of theNetflix prizecompetition. I also
enjoyed several student posters includingMatt Hoffman's cool examples of blind
source separation for music.I'm somewhat surprised how much the workshop has
grown, as it is now comparable in size to a small conference, although in
style more similar to a workshop. At some point as an event grows, it becomes
owned by the community rather than the organizers, so if anyone has
suggestions on improving it, speak up and be heard.</p><p>4 0.11200996 <a title="350-tfidf-4" href="../hunch_net-2005/hunch_net-2005-06-22-Languages__of_Learning.html">84 hunch net-2005-06-22-Languages  of Learning</a></p>
<p>Introduction: A language is a set of primitives which can be combined to succesfully create
complex objects. Languages arise in all sorts of situations: mechanical
construction, martial arts, communication, etcâ&euro;Ś Languages appear to be the key
to succesfully creating complex objects--it is difficult to come up with any
convincing example of a complex object which is not built using some language.
Since languages are so crucial to success, it is interesting to organize
various machine learning research programs by language.The most common
language in machine learning are languages for representing the solution to
machine learning. This includes:Bayes Nets and Graphical ModelsA language for
representing probability distributions. The key concept supporting modularity
is conditional independence.Michael Kearnshas been working on extending this
to game theory.Kernelized Linear ClassifiersA language for representing linear
separators, possibly in a large space. The key form of modularity here is
kerneliza</p><p>5 0.093429029 <a title="350-tfidf-5" href="../hunch_net-2008/hunch_net-2008-07-04-More_Presentation_Preparation.html">307 hunch net-2008-07-04-More Presentation Preparation</a></p>
<p>Introduction: We've discussedpresentation preparation before, but I have one more thing to
add:transitioning. For a research presentation, it is substantially helpful
for the audience if transitions are clear. A common outline for a research
presentation in machine leanring is:The problem. Presentations which don't
describe the problem almost immediately lose people, because the context is
missing to understand the detail.Prior relevant work. In many cases, a paper
builds on some previous bit of work which must be understood in order to
understand what the paper does. A common failure mode seems to be spending too
much time on prior work. Discuss just the relevant aspects of prior work in
the language of your work. Sometimes this is missing when unneeded.What we
did. For theory papers in particular, it is often not possible to really cover
the details. Prioritizing what you present can be very important.How it
worked. Many papers in Machine Learning have some sort of experimental test of
the algorit</p><p>6 0.084774509 <a title="350-tfidf-6" href="../hunch_net-2009/hunch_net-2009-01-19-Netflix_prize_within_epsilon.html">336 hunch net-2009-01-19-Netflix prize within epsilon</a></p>
<p>7 0.080437519 <a title="350-tfidf-7" href="../hunch_net-2009/hunch_net-2009-05-08-Computability_in_Artificial_Intelligence.html">353 hunch net-2009-05-08-Computability in Artificial Intelligence</a></p>
<p>8 0.079782397 <a title="350-tfidf-8" href="../hunch_net-2005/hunch_net-2005-05-11-Visa_Casualties.html">69 hunch net-2005-05-11-Visa Casualties</a></p>
<p>9 0.069383822 <a title="350-tfidf-9" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>10 0.061936438 <a title="350-tfidf-10" href="../hunch_net-2007/hunch_net-2007-09-16-Optimizing_Machine_Learning_Programs.html">262 hunch net-2007-09-16-Optimizing Machine Learning Programs</a></p>
<p>11 0.057444394 <a title="350-tfidf-11" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>12 0.056337744 <a title="350-tfidf-12" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>13 0.054070756 <a title="350-tfidf-13" href="../hunch_net-2007/hunch_net-2007-01-15-The_Machine_Learning_Department.html">228 hunch net-2007-01-15-The Machine Learning Department</a></p>
<p>14 0.049975254 <a title="350-tfidf-14" href="../hunch_net-2008/hunch_net-2008-09-03-Bidding_Problems.html">315 hunch net-2008-09-03-Bidding Problems</a></p>
<p>15 0.046363607 <a title="350-tfidf-15" href="../hunch_net-2012/hunch_net-2012-03-24-David_Waltz.html">460 hunch net-2012-03-24-David Waltz</a></p>
<p>16 0.046296656 <a title="350-tfidf-16" href="../hunch_net-2008/hunch_net-2008-04-12-It_Doesn%26%238217%3Bt_Stop.html">295 hunch net-2008-04-12-It Doesn&#8217;t Stop</a></p>
<p>17 0.039311167 <a title="350-tfidf-17" href="../hunch_net-2005/hunch_net-2005-06-13-Wikis_for_Summer_Schools_and_Workshops.html">81 hunch net-2005-06-13-Wikis for Summer Schools and Workshops</a></p>
<p>18 0.03876454 <a title="350-tfidf-18" href="../hunch_net-2009/hunch_net-2009-10-26-NIPS_workshops.html">375 hunch net-2009-10-26-NIPS workshops</a></p>
<p>19 0.037759423 <a title="350-tfidf-19" href="../hunch_net-2007/hunch_net-2007-11-16-MLSS_2008.html">273 hunch net-2007-11-16-MLSS 2008</a></p>
<p>20 0.036848292 <a title="350-tfidf-20" href="../hunch_net-2006/hunch_net-2006-01-08-Debugging_Your_Brain.html">147 hunch net-2006-01-08-Debugging Your Brain</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.05), (1, 0.005), (2, 0.063), (3, 0.021), (4, -0.021), (5, 0.004), (6, -0.044), (7, 0.017), (8, 0.033), (9, -0.017), (10, -0.003), (11, 0.035), (12, 0.081), (13, -0.059), (14, -0.053), (15, 0.014), (16, 0.011), (17, 0.012), (18, 0.026), (19, -0.032), (20, 0.038), (21, -0.025), (22, 0.008), (23, -0.035), (24, -0.087), (25, 0.005), (26, 0.063), (27, 0.031), (28, -0.06), (29, -0.027), (30, 0.016), (31, -0.058), (32, -0.037), (33, 0.004), (34, -0.042), (35, -0.074), (36, -0.132), (37, 0.043), (38, 0.067), (39, 0.094), (40, -0.016), (41, -0.034), (42, -0.03), (43, -0.073), (44, 0.113), (45, -0.01), (46, 0.094), (47, -0.036), (48, 0.023), (49, -0.136)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.90253031 <a title="350-lsi-1" href="../hunch_net-2009/hunch_net-2009-04-23-Jonathan_Chang_at_Slycoder.html">350 hunch net-2009-04-23-Jonathan Chang at Slycoder</a></p>
<p>Introduction: Jonathan Changhas aresearch blogon aspects of machine learning.</p><p>2 0.58616287 <a title="350-lsi-2" href="../hunch_net-2005/hunch_net-2005-06-22-Languages__of_Learning.html">84 hunch net-2005-06-22-Languages  of Learning</a></p>
<p>Introduction: A language is a set of primitives which can be combined to succesfully create
complex objects. Languages arise in all sorts of situations: mechanical
construction, martial arts, communication, etcâ&euro;Ś Languages appear to be the key
to succesfully creating complex objects--it is difficult to come up with any
convincing example of a complex object which is not built using some language.
Since languages are so crucial to success, it is interesting to organize
various machine learning research programs by language.The most common
language in machine learning are languages for representing the solution to
machine learning. This includes:Bayes Nets and Graphical ModelsA language for
representing probability distributions. The key concept supporting modularity
is conditional independence.Michael Kearnshas been working on extending this
to game theory.Kernelized Linear ClassifiersA language for representing linear
separators, possibly in a large space. The key form of modularity here is
kerneliza</p><p>3 0.57450825 <a title="350-lsi-3" href="../hunch_net-2006/hunch_net-2006-04-09-Progress_in_Machine_Translation.html">171 hunch net-2006-04-09-Progress in Machine Translation</a></p>
<p>Introduction: I just visitedISIwhereDaniel Marcuand others are working on machine
translation. Apparently, machine translation is rapidly improving. A
particularly dramatic year was 2002->2003 when systems switched from word-
based translation to phrase-based translation. From a (now famous) slide by
Charles Wayne atDARPA(which funds much of the work on machine translation)
here is some anecdotal evidence:20022003insistent Wednesday may recurred her
trips to Libya tomorrow for flying.Cairo 6-4 ( AFP ) - An official announced
today in the Egyptian lines company for flying Tuesday is a company "insistent
for flying" may resumed a consideration of a day Wednesday tomorrow her trips
to Libya of Security Council decision trace international the imposed ban
comment.And said the official "the institution sent a speech to Ministry of
Foreign Affairs of lifting on Libya air, a situation her recieving replying
are so a trip will pull to Libya a morning Wednesday."Egyptair has tomorrow to
Resume Its flight to</p><p>4 0.56969279 <a title="350-lsi-4" href="../hunch_net-2012/hunch_net-2012-10-18-7th_Annual_Machine_Learning_Symposium.html">474 hunch net-2012-10-18-7th Annual Machine Learning Symposium</a></p>
<p>Introduction: A reminder that theNew York Academy of Scienceswill be hosting the7th Annual
Machine Learning Symposiumtomorrow from 9:30am.The main program will feature
invited talks fromPeter Bartlett,William Freeman, andVladimir Vapnik, along
with numerous spotlight talks and a poster session. Following the main
program,hackNYandMicrosoft Researchare sponsoring a networking hour with talks
from machine learning practitioners at NYC startups
(specificallybit.ly,Buzzfeed,Chartbeat, andSense Networks,Visual Revenue).
This should be of great interest to everyone considering working in machine
learning.</p><p>5 0.56394935 <a title="350-lsi-5" href="../hunch_net-2012/hunch_net-2012-06-15-Normal_Deviate_and_the_UCSC_Machine_Learning_Summer_School.html">467 hunch net-2012-06-15-Normal Deviate and the UCSC Machine Learning Summer School</a></p>
<p>Introduction: Larry Wassermanhas started theNormal Deviateblog which I added to the blogroll
on the right.Manfred Warmuthpoints out theUCSC machine learning summer
schoolrunning July 9-20 which may be of particular interest to those in
silicon valley.</p><p>6 0.5598157 <a title="350-lsi-6" href="../hunch_net-2012/hunch_net-2012-07-09-Videolectures.html">469 hunch net-2012-07-09-Videolectures</a></p>
<p>7 0.55149788 <a title="350-lsi-7" href="../hunch_net-2012/hunch_net-2012-03-24-David_Waltz.html">460 hunch net-2012-03-24-David Waltz</a></p>
<p>8 0.5365569 <a title="350-lsi-8" href="../hunch_net-2005/hunch_net-2005-10-20-Machine_Learning_in_the_News.html">125 hunch net-2005-10-20-Machine Learning in the News</a></p>
<p>9 0.5334515 <a title="350-lsi-9" href="../hunch_net-2012/hunch_net-2012-02-20-Berkeley_Streaming_Data_Workshop.html">455 hunch net-2012-02-20-Berkeley Streaming Data Workshop</a></p>
<p>10 0.52916592 <a title="350-lsi-10" href="../hunch_net-2012/hunch_net-2012-04-20-Both_new%3A_STOC_workshops_and_NEML.html">462 hunch net-2012-04-20-Both new: STOC workshops and NEML</a></p>
<p>11 0.52401859 <a title="350-lsi-11" href="../hunch_net-2008/hunch_net-2008-05-25-Inappropriate_Mathematics_for_Machine_Learning.html">302 hunch net-2008-05-25-Inappropriate Mathematics for Machine Learning</a></p>
<p>12 0.50836754 <a title="350-lsi-12" href="../hunch_net-2007/hunch_net-2007-06-23-Machine_Learning_Jobs_are_Growing_on_Trees.html">250 hunch net-2007-06-23-Machine Learning Jobs are Growing on Trees</a></p>
<p>13 0.49679118 <a title="350-lsi-13" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>14 0.4857879 <a title="350-lsi-14" href="../hunch_net-2005/hunch_net-2005-02-04-JMLG.html">13 hunch net-2005-02-04-JMLG</a></p>
<p>15 0.46610618 <a title="350-lsi-15" href="../hunch_net-2007/hunch_net-2007-01-15-The_Machine_Learning_Department.html">228 hunch net-2007-01-15-The Machine Learning Department</a></p>
<p>16 0.46204132 <a title="350-lsi-16" href="../hunch_net-2007/hunch_net-2007-02-11-24.html">232 hunch net-2007-02-11-24</a></p>
<p>17 0.44619346 <a title="350-lsi-17" href="../hunch_net-2008/hunch_net-2008-10-20-New_York%26%238217%3Bs_ML_Day.html">322 hunch net-2008-10-20-New York&#8217;s ML Day</a></p>
<p>18 0.43004292 <a title="350-lsi-18" href="../hunch_net-2006/hunch_net-2006-08-10-Precision_is_not_accuracy.html">202 hunch net-2006-08-10-Precision is not accuracy</a></p>
<p>19 0.42794767 <a title="350-lsi-19" href="../hunch_net-2006/hunch_net-2006-04-17-Rexa_is_live.html">173 hunch net-2006-04-17-Rexa is live</a></p>
<p>20 0.42672658 <a title="350-lsi-20" href="../hunch_net-2006/hunch_net-2006-05-08-Big_machine_learning.html">178 hunch net-2006-05-08-Big machine learning</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(42, 0.088), (64, 0.487)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.99391156 <a title="350-lda-1" href="../hunch_net-2008/hunch_net-2008-11-04-Rise_of_the_Machines.html">323 hunch net-2008-11-04-Rise of the Machines</a></p>
<p>Introduction: On theenduring topic of how people deal with intelligent machines, we have
this importantelection bulletin.</p><p>2 0.64221263 <a title="350-lda-2" href="../hunch_net-2011/hunch_net-2011-08-15-Vowpal_Wabbit_6.0.html">441 hunch net-2011-08-15-Vowpal Wabbit 6.0</a></p>
<p>Introduction: I just releasedVowpal Wabbit 6.0. Since the last version:VW is now 2-3 orders
of magnitude faster at linear learning, primarily thanks toAlekh. Given the
baseline, this is loads of fun, allowing us to easily deal with terafeature
datasets, and dwarfing the scale of any other open source projects. The core
improvement here comes from effective parallelization over kilonode clusters
(eitherHadoopor not). This code is highly scalable, so it even helps with
clusters of size 2 (and doesn't hurt for clusters of size 1). The core
allreduce technique appears widely and easily reused--we've already used it to
parallelize Conjugate Gradient, LBFGS, and two variants of online learning.
We'll be documenting how to do this more thoroughly, but for now
"README_cluster" and associated scripts should provide a good starting
point.The newLBFGScode fromMiroseems to commonly dominate the existing
conjugate gradient code in time/quality tradeoffs.The new matrix factorization
code fromJakeadds a core algor</p><p>same-blog 3 0.64076412 <a title="350-lda-3" href="../hunch_net-2009/hunch_net-2009-04-23-Jonathan_Chang_at_Slycoder.html">350 hunch net-2009-04-23-Jonathan Chang at Slycoder</a></p>
<p>Introduction: Jonathan Changhas aresearch blogon aspects of machine learning.</p><p>4 0.61854643 <a title="350-lda-4" href="../hunch_net-2008/hunch_net-2008-05-25-Inappropriate_Mathematics_for_Machine_Learning.html">302 hunch net-2008-05-25-Inappropriate Mathematics for Machine Learning</a></p>
<p>Introduction: Reviewers and students are sometimes greatly concerned by the distinction
between:Anopen setand aclosed set.ASupremumand aMaximum.An event which happens
with probability 1 and an event that always happens.I don't appreciate this
distinction in machine learning & learning theory. All machine learning takes
place (by definition) on a machine where every parameter has finite precision.
Consequently, every set is closed, a maximal element always exists, and
probability 1 events always happen.The fundamental issue here is that
substantial parts of mathematics don't appear well-matched to computation in
the physical world, because the mathematics has concerns which are unphysical.
This mismatched mathematics makes irrelevant distinctions. We can ask "what
mathematics is appropriate to computation?"Andrejhas convinced me that a
pretty good answer to this question isconstructive mathematics.So, here's a
basic challenge: Can anyone name a situation where any of the distinctions
above (or simila</p><p>5 0.53988302 <a title="350-lda-5" href="../hunch_net-2010/hunch_net-2010-02-26-Yahoo%21_ML_events.html">389 hunch net-2010-02-26-Yahoo! ML events</a></p>
<p>Introduction: Yahoo! is sponsoring two machine learning events that might interest
people.TheKey Scientific Challengesprogram (due March 5) forMachine
LearningandStatisticsoffers $5K (plus bonuses) for graduate students working
on a core problem of interest to Y! If you are already working on one of these
problems, there is no reason not to submit, and if you aren't you might want
to think about it for next year, as I am confident they all press the boundary
of the possible in Machine Learning. There are 7 days left.TheLearning to Rank
challenge(due May 31) offers an $8K first prize for the best ranking algorithm
on a real (and really used) dataset for search ranking, with presentations at
an ICML workshop. Unlike the Netflix competition, there are prizes for 2nd,
3rd, and 4th place, perhaps avoiding the heartbreakthe ensembleencountered. If
you think you know how to rank, you should give it a try, and we might all
learn something. There are 3 months left.</p><p>6 0.40925097 <a title="350-lda-6" href="../hunch_net-2010/hunch_net-2010-12-26-NIPS_2010.html">420 hunch net-2010-12-26-NIPS 2010</a></p>
<p>7 0.39315385 <a title="350-lda-7" href="../hunch_net-2007/hunch_net-2007-06-24-Interesting_Papers_at_ICML_2007.html">251 hunch net-2007-06-24-Interesting Papers at ICML 2007</a></p>
<p>8 0.25237715 <a title="350-lda-8" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>9 0.24956217 <a title="350-lda-9" href="../hunch_net-2009/hunch_net-2009-07-31-Vowpal_Wabbit_Open_Source_Project.html">365 hunch net-2009-07-31-Vowpal Wabbit Open Source Project</a></p>
<p>10 0.23420131 <a title="350-lda-10" href="../hunch_net-2007/hunch_net-2007-12-21-Vowpal_Wabbit_Code_Release.html">281 hunch net-2007-12-21-Vowpal Wabbit Code Release</a></p>
<p>11 0.22005266 <a title="350-lda-11" href="../hunch_net-2008/hunch_net-2008-04-27-Watchword%3A_Supervised_Learning.html">299 hunch net-2008-04-27-Watchword: Supervised Learning</a></p>
<p>12 0.21989378 <a title="350-lda-12" href="../hunch_net-2006/hunch_net-2006-01-23-On_Coding_via_Mutual_Information_%26%23038%3B_Bayes_Nets.html">150 hunch net-2006-01-23-On Coding via Mutual Information &#038; Bayes Nets</a></p>
<p>13 0.2198797 <a title="350-lda-13" href="../hunch_net-2012/hunch_net-2012-10-26-ML_Symposium_and_Strata-Hadoop_World.html">475 hunch net-2012-10-26-ML Symposium and Strata-Hadoop World</a></p>
<p>14 0.21981782 <a title="350-lda-14" href="../hunch_net-2005/hunch_net-2005-05-06-Don%26%238217%3Bt_mix_the_solution_into_the_problem.html">67 hunch net-2005-05-06-Don&#8217;t mix the solution into the problem</a></p>
<p>15 0.21966209 <a title="350-lda-15" href="../hunch_net-2005/hunch_net-2005-12-09-Some_NIPS_papers.html">138 hunch net-2005-12-09-Some NIPS papers</a></p>
<p>16 0.21963997 <a title="350-lda-16" href="../hunch_net-2006/hunch_net-2006-06-15-IJCAI_is_out_of_season.html">184 hunch net-2006-06-15-IJCAI is out of season</a></p>
<p>17 0.21951346 <a title="350-lda-17" href="../hunch_net-2005/hunch_net-2005-03-29-Academic_Mechanism_Design.html">48 hunch net-2005-03-29-Academic Mechanism Design</a></p>
<p>18 0.21912687 <a title="350-lda-18" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>19 0.2190806 <a title="350-lda-19" href="../hunch_net-2005/hunch_net-2005-07-07-The_Limits_of_Learning_Theory.html">90 hunch net-2005-07-07-The Limits of Learning Theory</a></p>
<p>20 0.21729848 <a title="350-lda-20" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
