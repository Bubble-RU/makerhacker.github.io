<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>381 hunch net-2009-12-07-Vowpal Wabbit version 4.0, and a NIPS heresy</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2009" href="../home/hunch_net-2009_home.html">hunch_net-2009</a> <a title="hunch_net-2009-381" href="#">hunch_net-2009-381</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>381 hunch net-2009-12-07-Vowpal Wabbit version 4.0, and a NIPS heresy</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2009-381-html" href="http://hunch.net/?p=1068">html</a></p><p>Introduction: I’m releasing  version 4.0 ( tarball ) of  Vowpal Wabbit .  The biggest change (by far) in this release is experimental support for cluster parallelism, with notable help from  Daniel Hsu .  
 
I also took advantage of the major version number to introduce some incompatible changes, including switching to  murmurhash 2 , and other alterations to cachefiles.  You’ll need to delete and regenerate them.  In addition, the precise specification for a “tag” (i.e. string that can be used to identify an example) changed—you can’t have a space between the tag and the ‘|’ at the beginning of the feature namespace.  
 
And, of course, we made it faster.
 
For the future, I put up my  todo list  outlining the major future improvements I want to see in the code.  I’m planning to discuss the current mechanism and results of the cluster parallel implementation at the  large scale machine learning workshop  at  NIPS  later this week.  Several people have asked me to do a tutorial/walkthrough of VW, wh</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The biggest change (by far) in this release is experimental support for cluster parallelism, with notable help from  Daniel Hsu . [sent-3, score-0.753]
</p><p>2 I also took advantage of the major version number to introduce some incompatible changes, including switching to  murmurhash 2 , and other alterations to cachefiles. [sent-4, score-1.167]
</p><p>3 In addition, the precise specification for a “tag” (i. [sent-6, score-0.229]
</p><p>4 string that can be used to identify an example) changed—you can’t have a space between the tag and the ‘|’ at the beginning of the feature namespace. [sent-8, score-0.77]
</p><p>5 For the future, I put up my  todo list  outlining the major future improvements I want to see in the code. [sent-10, score-0.717]
</p><p>6 I’m planning to discuss the current mechanism and results of the cluster parallel implementation at the  large scale machine learning workshop  at  NIPS  later this week. [sent-11, score-0.926]
</p><p>7 Several people have asked me to do a tutorial/walkthrough of VW, which is arranged for friday 2pm in the workshop room—no skiing for me Friday. [sent-12, score-0.664]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tag', 0.376), ('major', 0.237), ('cluster', 0.216), ('outlining', 0.167), ('delete', 0.167), ('string', 0.155), ('alterations', 0.155), ('incompatible', 0.155), ('arranged', 0.155), ('skiing', 0.146), ('version', 0.144), ('switching', 0.139), ('introduce', 0.134), ('releasing', 0.134), ('friday', 0.134), ('parallelism', 0.134), ('workshop', 0.13), ('specification', 0.129), ('biggest', 0.129), ('identify', 0.129), ('join', 0.129), ('future', 0.126), ('notable', 0.121), ('took', 0.118), ('interests', 0.118), ('release', 0.115), ('hsu', 0.115), ('vw', 0.113), ('beginning', 0.11), ('changed', 0.11), ('implementation', 0.108), ('daniel', 0.104), ('vowpal', 0.102), ('wabbit', 0.102), ('precise', 0.1), ('parallel', 0.1), ('later', 0.099), ('asked', 0.099), ('planning', 0.099), ('discuss', 0.096), ('room', 0.096), ('put', 0.094), ('improvements', 0.093), ('experimental', 0.092), ('changes', 0.089), ('addition', 0.087), ('advantage', 0.085), ('change', 0.08), ('current', 0.078), ('course', 0.077)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="381-tfidf-1" href="../hunch_net-2009/hunch_net-2009-12-07-Vowpal_Wabbit_version_4.0%2C_and_a_NIPS_heresy.html">381 hunch net-2009-12-07-Vowpal Wabbit version 4.0, and a NIPS heresy</a></p>
<p>Introduction: I’m releasing  version 4.0 ( tarball ) of  Vowpal Wabbit .  The biggest change (by far) in this release is experimental support for cluster parallelism, with notable help from  Daniel Hsu .  
 
I also took advantage of the major version number to introduce some incompatible changes, including switching to  murmurhash 2 , and other alterations to cachefiles.  You’ll need to delete and regenerate them.  In addition, the precise specification for a “tag” (i.e. string that can be used to identify an example) changed—you can’t have a space between the tag and the ‘|’ at the beginning of the feature namespace.  
 
And, of course, we made it faster.
 
For the future, I put up my  todo list  outlining the major future improvements I want to see in the code.  I’m planning to discuss the current mechanism and results of the cluster parallel implementation at the  large scale machine learning workshop  at  NIPS  later this week.  Several people have asked me to do a tutorial/walkthrough of VW, wh</p><p>2 0.20057952 <a title="381-tfidf-2" href="../hunch_net-2011/hunch_net-2011-12-13-Vowpal_Wabbit_version_6.1_%26%23038%3B_the_NIPS_tutorial.html">451 hunch net-2011-12-13-Vowpal Wabbit version 6.1 &#038; the NIPS tutorial</a></p>
<p>Introduction: I just made  version 6.1  of  Vowpal Wabbit .  Relative to  6.0 , there are few new features, but many refinements. 
  
 The cluster parallel learning code better supports multiple simultaneous runs, and other forms of parallelism have been mostly removed.  This incidentally significantly simplifies the learning core. 
 The online learning algorithms are more general, with support for l 1  (via a truncated gradient variant) and l 2  regularization, and a generalized form of variable metric learning. 
 There is a solid persistent server mode which can train online, as well as serve answers to many simultaneous queries, either in text or binary. 
  
This should be a very good release if you are just getting started, as we’ve made it compile more automatically out of the box, have several new  examples  and updated documentation.
 
As  per   tradition , we’re planning to do a tutorial at NIPS during the break at the  parallel learning workshop  at 2pm Spanish time Friday.  I’ll cover the</p><p>3 0.15341425 <a title="381-tfidf-3" href="../hunch_net-2010/hunch_net-2010-12-04-Vowpal_Wabbit%2C_version_5.0%2C_and_the_second_heresy.html">419 hunch net-2010-12-04-Vowpal Wabbit, version 5.0, and the second heresy</a></p>
<p>Introduction: I’ve released  version 5.0  of the  Vowpal Wabbit  online learning software.  The major number has changed since the  last release  because I regard all earlier versions as obsolete—there are several new algorithms & features including substantial changes and upgrades to the default learning algorithm.  
 
The biggest changes are new algorithms:
  
  Nikos  and I improved the default algorithm.  The basic update rule still uses gradient descent, but the size of the update is carefully controlled so that it’s impossible to overrun the label.  In addition, the normalization has changed.  Computationally, these changes are virtually free and yield better results, sometimes much better.  Less careful updates can be reenabled with –loss_function classic, although results are still not identical to previous due to normalization changes. 
 Nikos also implemented the per-feature learning rates as per these  two   papers .  Often, this works better than the default algorithm.  It isn’t the defa</p><p>4 0.12453533 <a title="381-tfidf-4" href="../hunch_net-2010/hunch_net-2010-08-20-The_Workshop_on_Cores%2C_Clusters%2C_and_Clouds.html">404 hunch net-2010-08-20-The Workshop on Cores, Clusters, and Clouds</a></p>
<p>Introduction: Alekh ,  John ,  Ofer , and I are organizing a  workshop  at  NIPS  this year on learning in parallel and distributed environments.  The general interest level in parallel learning seems to be growing rapidly, so I expect quite a bit of attendance.  Please join us if you are parallel-interested.
 
And, if you are working in the area of parallel learning, please consider  submitting an abstract  due Oct. 17 for presentation at the workshop.</p><p>5 0.11912999 <a title="381-tfidf-5" href="../hunch_net-2013/hunch_net-2013-12-01-NIPS_tutorials_and_Vowpal_Wabbit_7.4.html">492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</a></p>
<p>Introduction: At NIPS I’m giving a  tutorial on Learning to Interact .  In essence this is about dealing with causality in a contextual bandit framework.  Relative to  previous tutorials , I’ll be covering several new results that changed my understanding of the nature of the problem.  Note that  Judea Pearl  and  Elias Bareinboim  have a  tutorial on causality .  This might appear similar, but is quite different in practice.  Pearl and Bareinboim’s tutorial will be about the general concepts while mine will be about total mastery of the simplest nontrivial case, including code.  Luckily, they have the right order.  I recommend going to both   
 
I also just released version 7.4 of  Vowpal Wabbit .  When I was a frustrated learning theorist, I did not understand why people were not using learning reductions to solve problems.  I’ve been slowly discovering why with VW, and addressing the issues.  One of the issues is that machine learning itself was not automatic enough, while another is that creatin</p><p>6 0.10803989 <a title="381-tfidf-6" href="../hunch_net-2012/hunch_net-2012-09-29-Vowpal_Wabbit%2C_version_7.0.html">473 hunch net-2012-09-29-Vowpal Wabbit, version 7.0</a></p>
<p>7 0.10768677 <a title="381-tfidf-7" href="../hunch_net-2009/hunch_net-2009-03-18-Parallel_ML_primitives.html">346 hunch net-2009-03-18-Parallel ML primitives</a></p>
<p>8 0.10421567 <a title="381-tfidf-8" href="../hunch_net-2011/hunch_net-2011-03-27-Vowpal_Wabbit%2C_v5.1.html">428 hunch net-2011-03-27-Vowpal Wabbit, v5.1</a></p>
<p>9 0.10007671 <a title="381-tfidf-9" href="../hunch_net-2007/hunch_net-2007-12-21-Vowpal_Wabbit_Code_Release.html">281 hunch net-2007-12-21-Vowpal Wabbit Code Release</a></p>
<p>10 0.079844125 <a title="381-tfidf-10" href="../hunch_net-2008/hunch_net-2008-04-30-Concerns_about_the_Large_Scale_Learning_Challenge.html">300 hunch net-2008-04-30-Concerns about the Large Scale Learning Challenge</a></p>
<p>11 0.078177914 <a title="381-tfidf-11" href="../hunch_net-2009/hunch_net-2009-12-24-Top_graduates_this_season.html">384 hunch net-2009-12-24-Top graduates this season</a></p>
<p>12 0.072966278 <a title="381-tfidf-12" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>13 0.072828233 <a title="381-tfidf-13" href="../hunch_net-2009/hunch_net-2009-07-31-Vowpal_Wabbit_Open_Source_Project.html">365 hunch net-2009-07-31-Vowpal Wabbit Open Source Project</a></p>
<p>14 0.072441131 <a title="381-tfidf-14" href="../hunch_net-2007/hunch_net-2007-02-22-Create_Your_Own_ICML_Workshop.html">234 hunch net-2007-02-22-Create Your Own ICML Workshop</a></p>
<p>15 0.070485368 <a title="381-tfidf-15" href="../hunch_net-2007/hunch_net-2007-12-19-Cool_and_interesting_things_seen_at_NIPS.html">279 hunch net-2007-12-19-Cool and interesting things seen at NIPS</a></p>
<p>16 0.069105268 <a title="381-tfidf-16" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>17 0.068124302 <a title="381-tfidf-17" href="../hunch_net-2009/hunch_net-2009-10-26-NIPS_workshops.html">375 hunch net-2009-10-26-NIPS workshops</a></p>
<p>18 0.06706354 <a title="381-tfidf-18" href="../hunch_net-2008/hunch_net-2008-01-23-Why_Workshop%3F.html">285 hunch net-2008-01-23-Why Workshop?</a></p>
<p>19 0.066337809 <a title="381-tfidf-19" href="../hunch_net-2012/hunch_net-2012-05-12-ICML_accepted_papers_and_early_registration.html">465 hunch net-2012-05-12-ICML accepted papers and early registration</a></p>
<p>20 0.06478247 <a title="381-tfidf-20" href="../hunch_net-2009/hunch_net-2009-05-17-Server_Update.html">354 hunch net-2009-05-17-Server Update</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.133), (1, -0.022), (2, -0.111), (3, -0.032), (4, 0.065), (5, 0.112), (6, -0.079), (7, -0.047), (8, -0.109), (9, 0.075), (10, -0.084), (11, -0.068), (12, 0.058), (13, 0.029), (14, -0.012), (15, -0.118), (16, -0.044), (17, 0.102), (18, -0.028), (19, -0.105), (20, -0.005), (21, 0.078), (22, -0.071), (23, 0.012), (24, -0.04), (25, 0.038), (26, -0.002), (27, -0.007), (28, -0.003), (29, 0.019), (30, 0.065), (31, -0.008), (32, -0.001), (33, -0.084), (34, 0.008), (35, 0.016), (36, 0.024), (37, 0.053), (38, -0.051), (39, 0.033), (40, -0.048), (41, 0.014), (42, -0.096), (43, -0.011), (44, 0.019), (45, 0.024), (46, -0.011), (47, -0.006), (48, 0.053), (49, 0.011)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97881114 <a title="381-lsi-1" href="../hunch_net-2009/hunch_net-2009-12-07-Vowpal_Wabbit_version_4.0%2C_and_a_NIPS_heresy.html">381 hunch net-2009-12-07-Vowpal Wabbit version 4.0, and a NIPS heresy</a></p>
<p>Introduction: I’m releasing  version 4.0 ( tarball ) of  Vowpal Wabbit .  The biggest change (by far) in this release is experimental support for cluster parallelism, with notable help from  Daniel Hsu .  
 
I also took advantage of the major version number to introduce some incompatible changes, including switching to  murmurhash 2 , and other alterations to cachefiles.  You’ll need to delete and regenerate them.  In addition, the precise specification for a “tag” (i.e. string that can be used to identify an example) changed—you can’t have a space between the tag and the ‘|’ at the beginning of the feature namespace.  
 
And, of course, we made it faster.
 
For the future, I put up my  todo list  outlining the major future improvements I want to see in the code.  I’m planning to discuss the current mechanism and results of the cluster parallel implementation at the  large scale machine learning workshop  at  NIPS  later this week.  Several people have asked me to do a tutorial/walkthrough of VW, wh</p><p>2 0.77553052 <a title="381-lsi-2" href="../hunch_net-2011/hunch_net-2011-12-13-Vowpal_Wabbit_version_6.1_%26%23038%3B_the_NIPS_tutorial.html">451 hunch net-2011-12-13-Vowpal Wabbit version 6.1 &#038; the NIPS tutorial</a></p>
<p>Introduction: I just made  version 6.1  of  Vowpal Wabbit .  Relative to  6.0 , there are few new features, but many refinements. 
  
 The cluster parallel learning code better supports multiple simultaneous runs, and other forms of parallelism have been mostly removed.  This incidentally significantly simplifies the learning core. 
 The online learning algorithms are more general, with support for l 1  (via a truncated gradient variant) and l 2  regularization, and a generalized form of variable metric learning. 
 There is a solid persistent server mode which can train online, as well as serve answers to many simultaneous queries, either in text or binary. 
  
This should be a very good release if you are just getting started, as we’ve made it compile more automatically out of the box, have several new  examples  and updated documentation.
 
As  per   tradition , we’re planning to do a tutorial at NIPS during the break at the  parallel learning workshop  at 2pm Spanish time Friday.  I’ll cover the</p><p>3 0.66776097 <a title="381-lsi-3" href="../hunch_net-2013/hunch_net-2013-12-01-NIPS_tutorials_and_Vowpal_Wabbit_7.4.html">492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</a></p>
<p>Introduction: At NIPS I’m giving a  tutorial on Learning to Interact .  In essence this is about dealing with causality in a contextual bandit framework.  Relative to  previous tutorials , I’ll be covering several new results that changed my understanding of the nature of the problem.  Note that  Judea Pearl  and  Elias Bareinboim  have a  tutorial on causality .  This might appear similar, but is quite different in practice.  Pearl and Bareinboim’s tutorial will be about the general concepts while mine will be about total mastery of the simplest nontrivial case, including code.  Luckily, they have the right order.  I recommend going to both   
 
I also just released version 7.4 of  Vowpal Wabbit .  When I was a frustrated learning theorist, I did not understand why people were not using learning reductions to solve problems.  I’ve been slowly discovering why with VW, and addressing the issues.  One of the issues is that machine learning itself was not automatic enough, while another is that creatin</p><p>4 0.66680747 <a title="381-lsi-4" href="../hunch_net-2010/hunch_net-2010-12-04-Vowpal_Wabbit%2C_version_5.0%2C_and_the_second_heresy.html">419 hunch net-2010-12-04-Vowpal Wabbit, version 5.0, and the second heresy</a></p>
<p>Introduction: I’ve released  version 5.0  of the  Vowpal Wabbit  online learning software.  The major number has changed since the  last release  because I regard all earlier versions as obsolete—there are several new algorithms & features including substantial changes and upgrades to the default learning algorithm.  
 
The biggest changes are new algorithms:
  
  Nikos  and I improved the default algorithm.  The basic update rule still uses gradient descent, but the size of the update is carefully controlled so that it’s impossible to overrun the label.  In addition, the normalization has changed.  Computationally, these changes are virtually free and yield better results, sometimes much better.  Less careful updates can be reenabled with –loss_function classic, although results are still not identical to previous due to normalization changes. 
 Nikos also implemented the per-feature learning rates as per these  two   papers .  Often, this works better than the default algorithm.  It isn’t the defa</p><p>5 0.65084469 <a title="381-lsi-5" href="../hunch_net-2011/hunch_net-2011-08-15-Vowpal_Wabbit_6.0.html">441 hunch net-2011-08-15-Vowpal Wabbit 6.0</a></p>
<p>Introduction: I just released  Vowpal Wabbit 6.0 .  Since the last version:
  
 VW is now 2-3 orders of magnitude faster at linear learning, primarily thanks to  Alekh .  Given the baseline, this is loads of fun, allowing us to easily deal with terafeature datasets, and dwarfing the scale of any other open source projects.  The core improvement here comes from effective parallelization over kilonode clusters (either  Hadoop  or not).   This code is highly scalable, so it even helps with clusters of size 2 (and doesn’t hurt for clusters of size 1).  The core allreduce technique appears widely and easily reused—we’ve already used it to parallelize Conjugate Gradient, LBFGS, and two variants of online learning.  We’ll be documenting how to do this more thoroughly, but for now “README_cluster” and associated scripts should provide a good starting point.
 
 The new  LBFGS  code from  Miro  seems to commonly dominate the existing conjugate gradient code in time/quality tradeoffs. 
 The new matrix factoriz</p><p>6 0.63927126 <a title="381-lsi-6" href="../hunch_net-2012/hunch_net-2012-09-29-Vowpal_Wabbit%2C_version_7.0.html">473 hunch net-2012-09-29-Vowpal Wabbit, version 7.0</a></p>
<p>7 0.56947404 <a title="381-lsi-7" href="../hunch_net-2010/hunch_net-2010-08-20-The_Workshop_on_Cores%2C_Clusters%2C_and_Clouds.html">404 hunch net-2010-08-20-The Workshop on Cores, Clusters, and Clouds</a></p>
<p>8 0.56788564 <a title="381-lsi-8" href="../hunch_net-2011/hunch_net-2011-03-27-Vowpal_Wabbit%2C_v5.1.html">428 hunch net-2011-03-27-Vowpal Wabbit, v5.1</a></p>
<p>9 0.52078283 <a title="381-lsi-9" href="../hunch_net-2009/hunch_net-2009-07-31-Vowpal_Wabbit_Open_Source_Project.html">365 hunch net-2009-07-31-Vowpal Wabbit Open Source Project</a></p>
<p>10 0.51974028 <a title="381-lsi-10" href="../hunch_net-2007/hunch_net-2007-12-21-Vowpal_Wabbit_Code_Release.html">281 hunch net-2007-12-21-Vowpal Wabbit Code Release</a></p>
<p>11 0.51844358 <a title="381-lsi-11" href="../hunch_net-2011/hunch_net-2011-08-20-The_Large_Scale_Learning_Survey_Tutorial.html">442 hunch net-2011-08-20-The Large Scale Learning Survey Tutorial</a></p>
<p>12 0.51145583 <a title="381-lsi-12" href="../hunch_net-2011/hunch_net-2011-06-22-Ultra_LDA.html">436 hunch net-2011-06-22-Ultra LDA</a></p>
<p>13 0.50445187 <a title="381-lsi-13" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>14 0.48280227 <a title="381-lsi-14" href="../hunch_net-2009/hunch_net-2009-03-18-Parallel_ML_primitives.html">346 hunch net-2009-03-18-Parallel ML primitives</a></p>
<p>15 0.44458842 <a title="381-lsi-15" href="../hunch_net-2007/hunch_net-2007-01-26-Parallel_Machine_Learning_Problems.html">229 hunch net-2007-01-26-Parallel Machine Learning Problems</a></p>
<p>16 0.44296595 <a title="381-lsi-16" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>17 0.44143733 <a title="381-lsi-17" href="../hunch_net-2008/hunch_net-2008-04-30-Concerns_about_the_Large_Scale_Learning_Challenge.html">300 hunch net-2008-04-30-Concerns about the Large Scale Learning Challenge</a></p>
<p>18 0.42462629 <a title="381-lsi-18" href="../hunch_net-2013/hunch_net-2013-11-09-Graduates_and_Postdocs.html">490 hunch net-2013-11-09-Graduates and Postdocs</a></p>
<p>19 0.41091421 <a title="381-lsi-19" href="../hunch_net-2005/hunch_net-2005-09-20-Workshop_Proposal%3A_Atomic_Learning.html">114 hunch net-2005-09-20-Workshop Proposal: Atomic Learning</a></p>
<p>20 0.40078726 <a title="381-lsi-20" href="../hunch_net-2005/hunch_net-2005-11-05-The_design_of_a_computing_cluster.html">128 hunch net-2005-11-05-The design of a computing cluster</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(27, 0.224), (53, 0.031), (55, 0.05), (93, 0.399), (94, 0.164), (95, 0.026)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.89318836 <a title="381-lda-1" href="../hunch_net-2009/hunch_net-2009-12-07-Vowpal_Wabbit_version_4.0%2C_and_a_NIPS_heresy.html">381 hunch net-2009-12-07-Vowpal Wabbit version 4.0, and a NIPS heresy</a></p>
<p>Introduction: I’m releasing  version 4.0 ( tarball ) of  Vowpal Wabbit .  The biggest change (by far) in this release is experimental support for cluster parallelism, with notable help from  Daniel Hsu .  
 
I also took advantage of the major version number to introduce some incompatible changes, including switching to  murmurhash 2 , and other alterations to cachefiles.  You’ll need to delete and regenerate them.  In addition, the precise specification for a “tag” (i.e. string that can be used to identify an example) changed—you can’t have a space between the tag and the ‘|’ at the beginning of the feature namespace.  
 
And, of course, we made it faster.
 
For the future, I put up my  todo list  outlining the major future improvements I want to see in the code.  I’m planning to discuss the current mechanism and results of the cluster parallel implementation at the  large scale machine learning workshop  at  NIPS  later this week.  Several people have asked me to do a tutorial/walkthrough of VW, wh</p><p>2 0.85229337 <a title="381-lda-2" href="../hunch_net-2005/hunch_net-2005-09-14-The_Predictionist_Viewpoint.html">112 hunch net-2005-09-14-The Predictionist Viewpoint</a></p>
<p>Introduction: Virtually every discipline of significant human endeavor has a way explaining itself as fundamental and important.  In all the cases I know of, they are both right (they are vital) and wrong (they are not solely vital).
  
 Politics.  This is the one that everyone is familiar with at the moment.  “What could be more important than the process of making decisions?” 
 Science and Technology.  This is the one that we-the-academics are familiar with.  “The loss of modern science and technology would be catastrophic.” 
 Military.  “Without the military, a nation will be invaded and destroyed.” 
 (insert your favorite here) 
  
Within science and technology, the same thing happens again.
  
 Mathematics. “What could be more important than a precise language for establishing truths?” 
 Physics.  “Nothing is more fundamental than the laws which govern the universe.  Understanding them is the key to understanding everything else.” 
 Biology.  “Without life, we wouldn’t be here, so clearly the s</p><p>3 0.7871868 <a title="381-lda-3" href="../hunch_net-2009/hunch_net-2009-07-09-The_Machine_Learning_Forum.html">363 hunch net-2009-07-09-The Machine Learning Forum</a></p>
<p>Introduction: Dear Fellow Machine Learners,
 
For the past year or so I have become increasingly frustrated with the peer review system in our field. I constantly get asked to review papers in which I have no interest. At the same time, as an action editor in JMLR, I constantly have to harass people to review papers. When I send papers to conferences and to journals I often get rejected with reviews that, at least in my mind, make no sense. Finally, I have a very hard time keeping up with the best new work, because I don’t know where to look for it…
 
I decided to try an do something to improve the situation. I started a new web site, which I decided to call “The machine learning forum” the URL is  http://themachinelearningforum.org 
 
The main idea behind this web site is to remove anonymity from the review process. In this site, all opinions are attributed to the actual person that expressed them. I expect that this will improve the quality of the reviews. An obvious other effect is that there wil</p><p>4 0.56780422 <a title="381-lda-4" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>Introduction: The  International Planning Competition  (IPC) is a biennial event organized in the context of the  International Conference on Automated Planning and Scheduling  (ICAPS). This year, for the first time, there will a learning track of the competition. For more information you can go to the competition  web-site . 
 
The competitions are typically organized around a number of planning domains that can vary from year to year, where a planning domain is simply a class of problems that share a common action schema—e.g. Blocksworld is a well-known planning domain that contains a problem instance each possible initial tower configuration and goal configuration. Some other domains have included Logistics, Airport, Freecell, PipesWorld, and many  others . For each domain the competition includes a number of problems (say 40-50) and the planners are run on each problem with a time limit for each problem (around 30 minutes). The problems are hard enough that many problems are not solved within th</p><p>5 0.56366926 <a title="381-lda-5" href="../hunch_net-2007/hunch_net-2007-01-26-Parallel_Machine_Learning_Problems.html">229 hunch net-2007-01-26-Parallel Machine Learning Problems</a></p>
<p>Introduction: Parallel machine learning is a subject rarely addressed at machine learning conferences.  Nevertheless, it seems likely to increase in importance because:
  
 Data set sizes appear to be growing substantially faster than computation.  Essentially, this happens because more and more sensors of various sorts are being hooked up to the internet. 
 Serial speedups of processors seem are relatively stalled.  The new trend is to make processors more powerful by making them  multicore .
 
 Both  AMD  and  Intel  are making dual core designs standard, with plans for more parallelism in the future. 
 IBM’s  Cell processor  has (essentially) 9 cores. 
 Modern graphics chips can have an order of magnitude more separate execution units. 
 

The meaning of ‘core’ varies a bit from processor to processor, but the overall trend seems quite clear.

 
  
So, how do we parallelize machine learning algorithms?
  
 The simplest and most common technique is to simply run the same learning algorithm with di</p><p>6 0.5621748 <a title="381-lda-6" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>7 0.55795783 <a title="381-lda-7" href="../hunch_net-2006/hunch_net-2006-12-04-Structural_Problems_in_NIPS_Decision_Making.html">221 hunch net-2006-12-04-Structural Problems in NIPS Decision Making</a></p>
<p>8 0.55755198 <a title="381-lda-8" href="../hunch_net-2010/hunch_net-2010-12-04-Vowpal_Wabbit%2C_version_5.0%2C_and_the_second_heresy.html">419 hunch net-2010-12-04-Vowpal Wabbit, version 5.0, and the second heresy</a></p>
<p>9 0.55730432 <a title="381-lda-9" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>10 0.55589551 <a title="381-lda-10" href="../hunch_net-2005/hunch_net-2005-12-07-Is_the_Google_way_the_way_for_machine_learning%3F.html">136 hunch net-2005-12-07-Is the Google way the way for machine learning?</a></p>
<p>11 0.55196851 <a title="381-lda-11" href="../hunch_net-2005/hunch_net-2005-03-18-Binomial_Weighting.html">43 hunch net-2005-03-18-Binomial Weighting</a></p>
<p>12 0.54907882 <a title="381-lda-12" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>13 0.54677659 <a title="381-lda-13" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>14 0.54668576 <a title="381-lda-14" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>15 0.54666156 <a title="381-lda-15" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>16 0.54622442 <a title="381-lda-16" href="../hunch_net-2009/hunch_net-2009-06-03-Functionally_defined_Nonlinear_Dynamic_Models.html">359 hunch net-2009-06-03-Functionally defined Nonlinear Dynamic Models</a></p>
<p>17 0.54505861 <a title="381-lda-17" href="../hunch_net-2007/hunch_net-2007-07-01-Watchword%3A_Online_Learning.html">252 hunch net-2007-07-01-Watchword: Online Learning</a></p>
<p>18 0.54476321 <a title="381-lda-18" href="../hunch_net-2009/hunch_net-2009-09-21-Netflix_finishes_%28and_starts%29.html">371 hunch net-2009-09-21-Netflix finishes (and starts)</a></p>
<p>19 0.54421556 <a title="381-lda-19" href="../hunch_net-2007/hunch_net-2007-07-06-Idempotent-capable_Predictors.html">253 hunch net-2007-07-06-Idempotent-capable Predictors</a></p>
<p>20 0.54397076 <a title="381-lda-20" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
