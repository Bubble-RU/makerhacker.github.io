<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>381 hunch net-2009-12-07-Vowpal Wabbit version 4.0, and a NIPS heresy</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2009" href="../home/hunch_net-2009_home.html">hunch_net-2009</a> <a title="hunch_net-2009-381" href="#">hunch_net-2009-381</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>381 hunch net-2009-12-07-Vowpal Wabbit version 4.0, and a NIPS heresy</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2009-381-html" href="http://hunch.net/?p=1068">html</a></p><p>Introduction: I'm releasingversion 4.0(tarball) ofVowpal Wabbit. The biggest change (by far)
in this release is experimental support for cluster parallelism, with notable
help fromDaniel Hsu.I also took advantage of the major version number to
introduce some incompatible changes, including switching tomurmurhash 2, and
other alterations to cachefiles. You'll need to delete and regenerate them. In
addition, the precise specification for a "tag" (i.e. string that can be used
to identify an example) changed--you can't have a space between the tag and
the '|' at the beginning of the feature namespace.And, of course, we made it
faster.For the future, I put up mytodo listoutlining the major future
improvements I want to see in the code. I'm planning to discuss the current
mechanism and results of the cluster parallel implementation at thelarge scale
machine learning workshopatNIPSlater this week. Several people have asked me
to do a tutorial/walkthrough of VW, which is arranged for friday 2pm in the
works</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tag', 0.394), ('major', 0.254), ('cluster', 0.226), ('delete', 0.175), ('ofvowpal', 0.175), ('string', 0.162), ('alterations', 0.162), ('skiing', 0.162), ('incompatible', 0.162), ('arranged', 0.162), ('introduce', 0.146), ('switching', 0.146), ('parallelism', 0.146), ('thelarge', 0.146), ('friday', 0.14), ('join', 0.14), ('specification', 0.135), ('biggest', 0.135), ('identify', 0.135), ('future', 0.135)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="381-tfidf-1" href="../hunch_net-2009/hunch_net-2009-12-07-Vowpal_Wabbit_version_4.0%2C_and_a_NIPS_heresy.html">381 hunch net-2009-12-07-Vowpal Wabbit version 4.0, and a NIPS heresy</a></p>
<p>Introduction: I'm releasingversion 4.0(tarball) ofVowpal Wabbit. The biggest change (by far)
in this release is experimental support for cluster parallelism, with notable
help fromDaniel Hsu.I also took advantage of the major version number to
introduce some incompatible changes, including switching tomurmurhash 2, and
other alterations to cachefiles. You'll need to delete and regenerate them. In
addition, the precise specification for a "tag" (i.e. string that can be used
to identify an example) changed--you can't have a space between the tag and
the '|' at the beginning of the feature namespace.And, of course, we made it
faster.For the future, I put up mytodo listoutlining the major future
improvements I want to see in the code. I'm planning to discuss the current
mechanism and results of the cluster parallel implementation at thelarge scale
machine learning workshopatNIPSlater this week. Several people have asked me
to do a tutorial/walkthrough of VW, which is arranged for friday 2pm in the
works</p><p>2 0.13994578 <a title="381-tfidf-2" href="../hunch_net-2011/hunch_net-2011-12-13-Vowpal_Wabbit_version_6.1_%26%23038%3B_the_NIPS_tutorial.html">451 hunch net-2011-12-13-Vowpal Wabbit version 6.1 &#038; the NIPS tutorial</a></p>
<p>Introduction: I just madeversion 6.1ofVowpal Wabbit. Relative to6.0, there are few new
features, but many refinements.The cluster parallel learning code better
supports multiple simultaneous runs, and other forms of parallelism have been
mostly removed. This incidentally significantly simplifies the learning
core.The online learning algorithms are more general, with support for l1(via
a truncated gradient variant) and l2regularization, and a generalized form of
variable metric learning.There is a solid persistent server mode which can
train online, as well as serve answers to many simultaneous queries, either in
text or binary.This should be a very good release if you are just getting
started, as we've made it compile more automatically out of the box, have
several newexamplesand updated documentation.Aspertradition, we're planning to
do a tutorial at NIPS during the break at theparallel learning workshopat 2pm
Spanish time Friday. I'll cover the basics, leaving the fun stuff for
others.Mirowill cov</p><p>3 0.12662092 <a title="381-tfidf-3" href="../hunch_net-2010/hunch_net-2010-12-04-Vowpal_Wabbit%2C_version_5.0%2C_and_the_second_heresy.html">419 hunch net-2010-12-04-Vowpal Wabbit, version 5.0, and the second heresy</a></p>
<p>Introduction: I've releasedversion 5.0of theVowpal Wabbitonline learning software. The major
number has changed since thelast releasebecause I regard all earlier versions
as obsolete--there are several new algorithms & features including substantial
changes and upgrades to the default learning algorithm.The biggest changes are
new algorithms:Nikosand I improved the default algorithm. The basic update
rule still uses gradient descent, but the size of the update is carefully
controlled so that it's impossible to overrun the label. In addition, the
normalization has changed. Computationally, these changes are virtually free
and yield better results, sometimes much better. Less careful updates can be
reenabled with -loss_function classic, although results are still not
identical to previous due to normalization changes.Nikos also implemented the
per-feature learning rates as per thesetwopapers. Often, this works better
than the default algorithm. It isn't the default because it isn't (yet) as
adaptable</p><p>4 0.12458058 <a title="381-tfidf-4" href="../hunch_net-2010/hunch_net-2010-08-20-The_Workshop_on_Cores%2C_Clusters%2C_and_Clouds.html">404 hunch net-2010-08-20-The Workshop on Cores, Clusters, and Clouds</a></p>
<p>Introduction: Alekh,John,Ofer, and I are organizing aworkshopatNIPSthis year on learning in
parallel and distributed environments. The general interest level in parallel
learning seems to be growing rapidly, so I expect quite a bit of attendance.
Please join us if you are parallel-interested.And, if you are working in the
area of parallel learning, please considersubmitting an abstractdue Oct. 17
for presentation at the workshop.</p><p>5 0.12006091 <a title="381-tfidf-5" href="../hunch_net-2012/hunch_net-2012-09-29-Vowpal_Wabbit%2C_version_7.0.html">473 hunch net-2012-09-29-Vowpal Wabbit, version 7.0</a></p>
<p>Introduction: A new version ofVWisout. The primary changes are:Learning Reductions: I've
wanted to getlearning reductionsworking and we've finally done it. Not
everything is implemented yet, but VW now supports direct:Multiclass
Classification-oaaor-ect.Cost Sensitive Multiclass Classification-csoaaor-
wap.Contextual Bandit Classification-cb.Sequential Structured Prediction-
searnor-daggerIn addition, it is now easy to build your own custom learning
reductions for various plausible uses: feature diddling, custom structured
prediction problems, or alternate learning reductions. This effort is far from
done, but it is now in a generally useful state. Note that all learning
reductions inherit the ability to do cluster parallel learning.Library
interface: VW now has a basic library interface. The library provides most of
the functionality of VW, with the limitation that it is monolithic and
nonreentrant. These will be improved over time.Windows port: The priority of a
windows port jumped way up once we</p><p>6 0.11472537 <a title="381-tfidf-6" href="../hunch_net-2009/hunch_net-2009-03-18-Parallel_ML_primitives.html">346 hunch net-2009-03-18-Parallel ML primitives</a></p>
<p>7 0.10548501 <a title="381-tfidf-7" href="../hunch_net-2013/hunch_net-2013-12-01-NIPS_tutorials_and_Vowpal_Wabbit_7.4.html">492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</a></p>
<p>8 0.096360698 <a title="381-tfidf-8" href="../hunch_net-2013/hunch_net-2013-06-10-The_Large_Scale_Learning_class_notes.html">483 hunch net-2013-06-10-The Large Scale Learning class notes</a></p>
<p>9 0.076367155 <a title="381-tfidf-9" href="../hunch_net-2011/hunch_net-2011-03-27-Vowpal_Wabbit%2C_v5.1.html">428 hunch net-2011-03-27-Vowpal Wabbit, v5.1</a></p>
<p>10 0.073545679 <a title="381-tfidf-10" href="../hunch_net-2007/hunch_net-2007-12-21-Vowpal_Wabbit_Code_Release.html">281 hunch net-2007-12-21-Vowpal Wabbit Code Release</a></p>
<p>11 0.073421426 <a title="381-tfidf-11" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>12 0.07195691 <a title="381-tfidf-12" href="../hunch_net-2008/hunch_net-2008-04-30-Concerns_about_the_Large_Scale_Learning_Challenge.html">300 hunch net-2008-04-30-Concerns about the Large Scale Learning Challenge</a></p>
<p>13 0.067204341 <a title="381-tfidf-13" href="../hunch_net-2013/hunch_net-2013-01-31-Remote_large_scale_learning_class_participation.html">479 hunch net-2013-01-31-Remote large scale learning class participation</a></p>
<p>14 0.064717725 <a title="381-tfidf-14" href="../hunch_net-2007/hunch_net-2007-01-26-Parallel_Machine_Learning_Problems.html">229 hunch net-2007-01-26-Parallel Machine Learning Problems</a></p>
<p>15 0.063900694 <a title="381-tfidf-15" href="../hunch_net-2009/hunch_net-2009-10-26-NIPS_workshops.html">375 hunch net-2009-10-26-NIPS workshops</a></p>
<p>16 0.063188374 <a title="381-tfidf-16" href="../hunch_net-2005/hunch_net-2005-11-05-The_design_of_a_computing_cluster.html">128 hunch net-2005-11-05-The design of a computing cluster</a></p>
<p>17 0.062680393 <a title="381-tfidf-17" href="../hunch_net-2009/hunch_net-2009-05-17-Server_Update.html">354 hunch net-2009-05-17-Server Update</a></p>
<p>18 0.061590813 <a title="381-tfidf-18" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>19 0.061116077 <a title="381-tfidf-19" href="../hunch_net-2011/hunch_net-2011-08-20-The_Large_Scale_Learning_Survey_Tutorial.html">442 hunch net-2011-08-20-The Large Scale Learning Survey Tutorial</a></p>
<p>20 0.060937416 <a title="381-tfidf-20" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.119), (1, 0.009), (2, 0.087), (3, 0.063), (4, -0.029), (5, 0.103), (6, 0.082), (7, 0.031), (8, 0.021), (9, 0.103), (10, -0.034), (11, 0.072), (12, 0.012), (13, -0.04), (14, -0.055), (15, -0.019), (16, 0.026), (17, -0.045), (18, 0.031), (19, -0.065), (20, -0.032), (21, 0.004), (22, -0.026), (23, 0.093), (24, 0.021), (25, -0.037), (26, -0.056), (27, 0.095), (28, -0.03), (29, 0.012), (30, -0.09), (31, 0.028), (32, 0.06), (33, 0.056), (34, 0.029), (35, 0.029), (36, 0.064), (37, 0.027), (38, -0.049), (39, 0.021), (40, 0.035), (41, 0.055), (42, 0.095), (43, -0.084), (44, 0.018), (45, -0.064), (46, -0.084), (47, -0.02), (48, 0.015), (49, 0.117)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97230655 <a title="381-lsi-1" href="../hunch_net-2009/hunch_net-2009-12-07-Vowpal_Wabbit_version_4.0%2C_and_a_NIPS_heresy.html">381 hunch net-2009-12-07-Vowpal Wabbit version 4.0, and a NIPS heresy</a></p>
<p>Introduction: I'm releasingversion 4.0(tarball) ofVowpal Wabbit. The biggest change (by far)
in this release is experimental support for cluster parallelism, with notable
help fromDaniel Hsu.I also took advantage of the major version number to
introduce some incompatible changes, including switching tomurmurhash 2, and
other alterations to cachefiles. You'll need to delete and regenerate them. In
addition, the precise specification for a "tag" (i.e. string that can be used
to identify an example) changed--you can't have a space between the tag and
the '|' at the beginning of the feature namespace.And, of course, we made it
faster.For the future, I put up mytodo listoutlining the major future
improvements I want to see in the code. I'm planning to discuss the current
mechanism and results of the cluster parallel implementation at thelarge scale
machine learning workshopatNIPSlater this week. Several people have asked me
to do a tutorial/walkthrough of VW, which is arranged for friday 2pm in the
works</p><p>2 0.70937526 <a title="381-lsi-2" href="../hunch_net-2011/hunch_net-2011-12-13-Vowpal_Wabbit_version_6.1_%26%23038%3B_the_NIPS_tutorial.html">451 hunch net-2011-12-13-Vowpal Wabbit version 6.1 &#038; the NIPS tutorial</a></p>
<p>Introduction: I just madeversion 6.1ofVowpal Wabbit. Relative to6.0, there are few new
features, but many refinements.The cluster parallel learning code better
supports multiple simultaneous runs, and other forms of parallelism have been
mostly removed. This incidentally significantly simplifies the learning
core.The online learning algorithms are more general, with support for l1(via
a truncated gradient variant) and l2regularization, and a generalized form of
variable metric learning.There is a solid persistent server mode which can
train online, as well as serve answers to many simultaneous queries, either in
text or binary.This should be a very good release if you are just getting
started, as we've made it compile more automatically out of the box, have
several newexamplesand updated documentation.Aspertradition, we're planning to
do a tutorial at NIPS during the break at theparallel learning workshopat 2pm
Spanish time Friday. I'll cover the basics, leaving the fun stuff for
others.Mirowill cov</p><p>3 0.69130564 <a title="381-lsi-3" href="../hunch_net-2012/hunch_net-2012-09-29-Vowpal_Wabbit%2C_version_7.0.html">473 hunch net-2012-09-29-Vowpal Wabbit, version 7.0</a></p>
<p>Introduction: A new version ofVWisout. The primary changes are:Learning Reductions: I've
wanted to getlearning reductionsworking and we've finally done it. Not
everything is implemented yet, but VW now supports direct:Multiclass
Classification-oaaor-ect.Cost Sensitive Multiclass Classification-csoaaor-
wap.Contextual Bandit Classification-cb.Sequential Structured Prediction-
searnor-daggerIn addition, it is now easy to build your own custom learning
reductions for various plausible uses: feature diddling, custom structured
prediction problems, or alternate learning reductions. This effort is far from
done, but it is now in a generally useful state. Note that all learning
reductions inherit the ability to do cluster parallel learning.Library
interface: VW now has a basic library interface. The library provides most of
the functionality of VW, with the limitation that it is monolithic and
nonreentrant. These will be improved over time.Windows port: The priority of a
windows port jumped way up once we</p><p>4 0.62027824 <a title="381-lsi-4" href="../hunch_net-2011/hunch_net-2011-08-20-The_Large_Scale_Learning_Survey_Tutorial.html">442 hunch net-2011-08-20-The Large Scale Learning Survey Tutorial</a></p>
<p>Introduction: Ron Bekkermaninitiated an effort to create anedited book on parallel machine
learningthatMishaand I have been helping with. The breadth of efforts to
parallelize machine learning surprised me: I was only aware of a small
fraction initially.This put us in a unique position, with knowledge of a wide
array of different efforts, so it is natural to put together asurvey tutorial
on the subject of parallel learningforKDD, tomorrow. This tutorial
isnotlimited to the book itself however, as several interesting new algorithms
have come out since we started inviting chapters.This tutorial should interest
anyone trying to use machine learning on significant quantities of data,
anyone interested in developing algorithms for such, and of course who has
bragging rights to the fastest learning algorithm on planet earth(Also note
the Modeling with Hadoop tutorial just before ours which deals with one way of
trying to speed up learning algorithms. We have almost no overlap.)</p><p>5 0.61269265 <a title="381-lsi-5" href="../hunch_net-2013/hunch_net-2013-12-01-NIPS_tutorials_and_Vowpal_Wabbit_7.4.html">492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</a></p>
<p>Introduction: At NIPS I'm giving atutorial on Learning to Interact. In essence this is about
dealing with causality in a contextual bandit framework. Relative toprevious
tutorials, I'll be covering several new results that changed my understanding
of the nature of the problem. Note thatJudea PearlandElias Bareinboimhave
atutorial on causality. This might appear similar, but is quite different in
practice. Pearl and Bareinboim's tutorial will be about the general concepts
while mine will be about total mastery of the simplest nontrivial case,
including code. Luckily, they have the right order. I recommend going to bothI
also just released version 7.4 ofVowpal Wabbit. When I was a frustrated
learning theorist, I did not understand why people were not using learning
reductions to solve problems. I've been slowly discovering why with VW, and
addressing the issues. One of the issues is that machine learning itself was
not automatic enough, while another is that creating a very low overhead
process for do</p><p>6 0.60917163 <a title="381-lsi-6" href="../hunch_net-2011/hunch_net-2011-03-27-Vowpal_Wabbit%2C_v5.1.html">428 hunch net-2011-03-27-Vowpal Wabbit, v5.1</a></p>
<p>7 0.5729326 <a title="381-lsi-7" href="../hunch_net-2010/hunch_net-2010-08-20-The_Workshop_on_Cores%2C_Clusters%2C_and_Clouds.html">404 hunch net-2010-08-20-The Workshop on Cores, Clusters, and Clouds</a></p>
<p>8 0.56634563 <a title="381-lsi-8" href="../hunch_net-2010/hunch_net-2010-12-04-Vowpal_Wabbit%2C_version_5.0%2C_and_the_second_heresy.html">419 hunch net-2010-12-04-Vowpal Wabbit, version 5.0, and the second heresy</a></p>
<p>9 0.50882411 <a title="381-lsi-9" href="../hunch_net-2009/hunch_net-2009-07-31-Vowpal_Wabbit_Open_Source_Project.html">365 hunch net-2009-07-31-Vowpal Wabbit Open Source Project</a></p>
<p>10 0.49137044 <a title="381-lsi-10" href="../hunch_net-2008/hunch_net-2008-04-30-Concerns_about_the_Large_Scale_Learning_Challenge.html">300 hunch net-2008-04-30-Concerns about the Large Scale Learning Challenge</a></p>
<p>11 0.47601148 <a title="381-lsi-11" href="../hunch_net-2011/hunch_net-2011-08-15-Vowpal_Wabbit_6.0.html">441 hunch net-2011-08-15-Vowpal Wabbit 6.0</a></p>
<p>12 0.47432938 <a title="381-lsi-12" href="../hunch_net-2009/hunch_net-2009-03-18-Parallel_ML_primitives.html">346 hunch net-2009-03-18-Parallel ML primitives</a></p>
<p>13 0.44045681 <a title="381-lsi-13" href="../hunch_net-2013/hunch_net-2013-01-07-NYU_Large_Scale_Machine_Learning_Class.html">478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</a></p>
<p>14 0.42637193 <a title="381-lsi-14" href="../hunch_net-2013/hunch_net-2013-06-10-The_Large_Scale_Learning_class_notes.html">483 hunch net-2013-06-10-The Large Scale Learning class notes</a></p>
<p>15 0.42185026 <a title="381-lsi-15" href="../hunch_net-2005/hunch_net-2005-02-25-Solution%3A_Reinforcement_Learning_with_Classification.html">29 hunch net-2005-02-25-Solution: Reinforcement Learning with Classification</a></p>
<p>16 0.41628957 <a title="381-lsi-16" href="../hunch_net-2005/hunch_net-2005-04-08-Fast_SVMs.html">54 hunch net-2005-04-08-Fast SVMs</a></p>
<p>17 0.400298 <a title="381-lsi-17" href="../hunch_net-2005/hunch_net-2005-11-05-The_design_of_a_computing_cluster.html">128 hunch net-2005-11-05-The design of a computing cluster</a></p>
<p>18 0.39517045 <a title="381-lsi-18" href="../hunch_net-2007/hunch_net-2007-12-21-Vowpal_Wabbit_Code_Release.html">281 hunch net-2007-12-21-Vowpal Wabbit Code Release</a></p>
<p>19 0.37351868 <a title="381-lsi-19" href="../hunch_net-2007/hunch_net-2007-01-26-Parallel_Machine_Learning_Problems.html">229 hunch net-2007-01-26-Parallel Machine Learning Problems</a></p>
<p>20 0.36553961 <a title="381-lsi-20" href="../hunch_net-2006/hunch_net-2006-06-05-Server_Shift%2C_Site_Tweaks%2C_Suggestions%3F.html">182 hunch net-2006-06-05-Server Shift, Site Tweaks, Suggestions?</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(35, 0.089), (42, 0.069), (68, 0.037), (74, 0.163), (84, 0.469), (88, 0.058)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.88219655 <a title="381-lda-1" href="../hunch_net-2009/hunch_net-2009-12-07-Vowpal_Wabbit_version_4.0%2C_and_a_NIPS_heresy.html">381 hunch net-2009-12-07-Vowpal Wabbit version 4.0, and a NIPS heresy</a></p>
<p>Introduction: I'm releasingversion 4.0(tarball) ofVowpal Wabbit. The biggest change (by far)
in this release is experimental support for cluster parallelism, with notable
help fromDaniel Hsu.I also took advantage of the major version number to
introduce some incompatible changes, including switching tomurmurhash 2, and
other alterations to cachefiles. You'll need to delete and regenerate them. In
addition, the precise specification for a "tag" (i.e. string that can be used
to identify an example) changed--you can't have a space between the tag and
the '|' at the beginning of the feature namespace.And, of course, we made it
faster.For the future, I put up mytodo listoutlining the major future
improvements I want to see in the code. I'm planning to discuss the current
mechanism and results of the cluster parallel implementation at thelarge scale
machine learning workshopatNIPSlater this week. Several people have asked me
to do a tutorial/walkthrough of VW, which is arranged for friday 2pm in the
works</p><p>2 0.36784226 <a title="381-lda-2" href="../hunch_net-2005/hunch_net-2005-02-25-Problem%3A_Online_Learning.html">28 hunch net-2005-02-25-Problem: Online Learning</a></p>
<p>Introduction: Despite my best intentions, this is not a fully specified problem, but rather
a research direction.Competitive online learning is one of the more compelling
pieces of learning theory because typical statements of the form "this
algorithm will perform almost as well as a large set of other algorithms" rely
only on fully-observable quantities, and are therefore applicable in many
situations. Examples includeWinnow,Weighted Majority, andBinomial Weighting.
Algorithms with this property haven't taken over the world yet. Here might be
some reasons:Lack of caring. Many people working on learning theory don't care
about particular applications much. This means constants in the algorithm are
not optimized, usable code is often not produced, and empirical studies aren't
done.Inefficiency. Viewed from the perspective of other learning algorithms,
online learning is terribly inefficient. It requires that every hypothesis
(called an expert in the online learning setting) be enumerated and tested o</p><p>3 0.35878667 <a title="381-lda-3" href="../hunch_net-2006/hunch_net-2006-04-30-John_Langford_%26%238211%3B%3E_Yahoo_Research%2C_NY.html">175 hunch net-2006-04-30-John Langford &#8211;> Yahoo Research, NY</a></p>
<p>Introduction: I will joinYahoo Research(in New York) after my contract ends atTTI-
Chicago.The deciding reasons are:Yahoo is running into many hard learning
problems. This is precisely the situation where basic research might hope to
have the greatest impact.Yahoo Research understands research including
publishing, conferences, etc…Yahoo Research is growing, so there is a chance I
can help it grow well.Yahoo understands the internet, including (but not at
all limited to) experimenting with research blogs.In the end, Yahoo Research
seems like the place where I might have a chance to make the greatest
difference.Yahoo (as a company) has made a strong bet on Yahoo Research. We-
the-researchers all hope that bet will pay off, and this seems plausible. I'll
certainly have fun trying.</p><p>4 0.35276183 <a title="381-lda-4" href="../hunch_net-2008/hunch_net-2008-10-14-Who_is_Responsible_for_a_Bad_Review%3F.html">320 hunch net-2008-10-14-Who is Responsible for a Bad Review?</a></p>
<p>Introduction: Although I'm greatly interested in machine learning, I think it must be
admitted that there is a large amount of low quality logic being used in
reviews. The problem is bad enough that sometimes I wonder if theByzantine
generalslimit has been exceeded. For example, I've seen recent reviews where
the given reasons for rejecting are:[NIPS] Theorem A is uninteresting because
Theorem B is uninteresting.[UAI] When you learn by memorization, the problem
addressed is trivial.[NIPS] The proof is in the appendix.[NIPS] This has been
done before. (â&euro;Ś but not giving any relevant citations)Just for the record I
want to point out what's wrong with these reviews. A future world in which
such reasons never come up again would be great, but I'm sure these errors
will be committed many times more in the future.This is nonsense. A theorem
should be evaluated based on it's merits, rather than the merits of another
theorem.Learning by memorization requires an exponentially larger sample
complexity than man</p><p>5 0.35162824 <a title="381-lda-5" href="../hunch_net-2005/hunch_net-2005-05-28-Running_A_Machine_Learning_Summer_School.html">75 hunch net-2005-05-28-Running A Machine Learning Summer School</a></p>
<p>Introduction: We just finished theChicago 2005 Machine Learning Summer School. The school
was 2 weeks long with about 130 (or 140 counting the speakers) participants.
For perspective, this is perhaps the largest graduate level machine learning
class I am aware of anywhere and anytime (previousMLSSs have been close).
Overall, it seemed to go well, although the students are the real authority on
this. For those who missed it, DVDs will be available from our Slovenian
friends. EmailMrs Spela Sitarof the Jozsef Stefan Institute for details.The
following are some notes for future planning and those interested.Good
DecisionsAcquiring the larger-than-necessary "Assembly Hall" atInternational
House. Our attendance came in well above our expectations, so this was a
critical early decision that made a huge difference.The invited speakers were
key. They made a huge difference in the quality of the content.Delegating
early and often was important. One key difficulty here is gauging how much a
volunteer can (or</p><p>6 0.35116887 <a title="381-lda-6" href="../hunch_net-2005/hunch_net-2005-07-04-The_Health_of_COLT.html">89 hunch net-2005-07-04-The Health of COLT</a></p>
<p>7 0.35079473 <a title="381-lda-7" href="../hunch_net-2006/hunch_net-2006-09-18-What_is_missing_for_online_collaborative_research%3F.html">208 hunch net-2006-09-18-What is missing for online collaborative research?</a></p>
<p>8 0.3490836 <a title="381-lda-8" href="../hunch_net-2006/hunch_net-2006-07-11-New_Models.html">194 hunch net-2006-07-11-New Models</a></p>
<p>9 0.34358916 <a title="381-lda-9" href="../hunch_net-2007/hunch_net-2007-01-26-Parallel_Machine_Learning_Problems.html">229 hunch net-2007-01-26-Parallel Machine Learning Problems</a></p>
<p>10 0.34137511 <a title="381-lda-10" href="../hunch_net-2012/hunch_net-2012-05-02-ICML%3A_Behind_the_Scenes.html">463 hunch net-2012-05-02-ICML: Behind the Scenes</a></p>
<p>11 0.3388069 <a title="381-lda-11" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>12 0.33693773 <a title="381-lda-12" href="../hunch_net-2006/hunch_net-2006-03-27-Gradients_everywhere.html">167 hunch net-2006-03-27-Gradients everywhere</a></p>
<p>13 0.33683369 <a title="381-lda-13" href="../hunch_net-2005/hunch_net-2005-03-09-Bad_Reviewing.html">38 hunch net-2005-03-09-Bad Reviewing</a></p>
<p>14 0.33523792 <a title="381-lda-14" href="../hunch_net-2012/hunch_net-2012-06-05-ICML_acceptance_statistics.html">466 hunch net-2012-06-05-ICML acceptance statistics</a></p>
<p>15 0.3343322 <a title="381-lda-15" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>16 0.33264473 <a title="381-lda-16" href="../hunch_net-2007/hunch_net-2007-12-17-New_Machine_Learning_mailing_list.html">278 hunch net-2007-12-17-New Machine Learning mailing list</a></p>
<p>17 0.33138633 <a title="381-lda-17" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>18 0.3313553 <a title="381-lda-18" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>19 0.33028051 <a title="381-lda-19" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>20 0.32700735 <a title="381-lda-20" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
