<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>368 hunch net-2009-08-26-Another 10-year paper in Machine Learning</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2009" href="../home/hunch_net-2009_home.html">hunch_net-2009</a> <a title="hunch_net-2009-368" href="#">hunch_net-2009-368</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>368 hunch net-2009-08-26-Another 10-year paper in Machine Learning</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2009-368-html" href="http://hunch.net/?p=915">html</a></p><p>Introduction: When I was thinking about the best "10 year paper" forICML, I also took a look
at a few other conferences. Here is one from 10 years ago that interested
me:David McAllesterPAC-Bayesian Model Averaging,COLT1999.2001 Journal
Draft.Prior to this paper, the only mechanism known for controlling or
estimating the necessary sample complexity for learning over continuously
parameterized predictors was VC theory and variants, all of which suffered
from a basic problem: they were incredibly pessimistic in practice. This meant
that only very gross guidance could be provided for learning algorithm design.
The PAC-Bayes bound provided an alternative approach to sample complexity
bounds which was radically tighter, quantitatively. It also imported and
explained many of the motivations for Bayesian learning in a way that learning
theory and perhaps optimization people might appreciate. Since this paper came
out, there have been a number of moderately successful attempts to drive
algorithms directly b</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 When I was thinking about the best "10 year paper" forICML, I also took a look at a few other conferences. [sent-1, score-0.255]
</p><p>2 Here is one from 10 years ago that interested me:David McAllesterPAC-Bayesian Model Averaging,COLT1999. [sent-2, score-0.396]
</p><p>3 Prior to this paper, the only mechanism known for controlling or estimating the necessary sample complexity for learning over continuously parameterized predictors was VC theory and variants, all of which suffered from a basic problem: they were incredibly pessimistic in practice. [sent-4, score-1.699]
</p><p>4 This meant that only very gross guidance could be provided for learning algorithm design. [sent-5, score-0.683]
</p><p>5 The PAC-Bayes bound provided an alternative approach to sample complexity bounds which was radically tighter, quantitatively. [sent-6, score-1.0]
</p><p>6 It also imported and explained many of the motivations for Bayesian learning in a way that learning theory and perhaps optimization people might appreciate. [sent-7, score-0.485]
</p><p>7 Since this paper came out, there have been a number of moderately successful attempts to drive algorithms directly by the PAC-Bayes bound. [sent-8, score-0.719]
</p><p>8 We've gone from thinking that a bound driven algorithm is completely useless to merely a bit more pessimistic and computationally intense than might be necessary. [sent-9, score-1.531]
</p><p>9 The PAC-Bayes bound is related to the"bits-back" argumentthatGeoff HintonandDrew van Campmade at COLT 6 years earlier. [sent-10, score-0.594]
</p><p>10 What other machine learning or learning theory papers from 10 years ago have had a substantial impact? [sent-11, score-0.542]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pessimistic', 0.296), ('provided', 0.226), ('bound', 0.225), ('years', 0.221), ('ago', 0.175), ('sample', 0.152), ('incredibly', 0.148), ('vc', 0.148), ('van', 0.148), ('foricml', 0.148), ('theory', 0.146), ('thinking', 0.142), ('intense', 0.14), ('drive', 0.14), ('gross', 0.14), ('moderately', 0.14), ('guidance', 0.14), ('continuously', 0.14), ('suffered', 0.14), ('motivations', 0.133), ('estimating', 0.133), ('parameterized', 0.128), ('controlling', 0.128), ('explained', 0.128), ('paper', 0.126), ('complexity', 0.124), ('merely', 0.123), ('driven', 0.123), ('journal', 0.123), ('david', 0.12), ('tighter', 0.116), ('useless', 0.113), ('took', 0.113), ('successful', 0.113), ('gone', 0.11), ('variants', 0.108), ('completely', 0.105), ('radically', 0.103), ('meant', 0.103), ('attempts', 0.101), ('came', 0.099), ('alternative', 0.098), ('predictors', 0.09), ('colt', 0.084), ('impact', 0.081), ('computationally', 0.08), ('optimization', 0.078), ('algorithm', 0.074), ('necessary', 0.074), ('bounds', 0.072)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999976 <a title="368-tfidf-1" href="../hunch_net-2009/hunch_net-2009-08-26-Another_10-year_paper_in_Machine_Learning.html">368 hunch net-2009-08-26-Another 10-year paper in Machine Learning</a></p>
<p>Introduction: When I was thinking about the best "10 year paper" forICML, I also took a look
at a few other conferences. Here is one from 10 years ago that interested
me:David McAllesterPAC-Bayesian Model Averaging,COLT1999.2001 Journal
Draft.Prior to this paper, the only mechanism known for controlling or
estimating the necessary sample complexity for learning over continuously
parameterized predictors was VC theory and variants, all of which suffered
from a basic problem: they were incredibly pessimistic in practice. This meant
that only very gross guidance could be provided for learning algorithm design.
The PAC-Bayes bound provided an alternative approach to sample complexity
bounds which was radically tighter, quantitatively. It also imported and
explained many of the motivations for Bayesian learning in a way that learning
theory and perhaps optimization people might appreciate. Since this paper came
out, there have been a number of moderately successful attempts to drive
algorithms directly b</p><p>2 0.15627055 <a title="368-tfidf-2" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>Introduction: I've had serious conversations with several people who believe that the theory
in machine learning is "only useful for getting papers published". That's a
compelling statement, as I've seen many papers where the algorithm clearly
came first, and the theoretical justification for it came second, purely as a
perceived means to improve the chance of publication.Naturally, I disagree and
believe that learning theory has much more substantial applications.Even in
core learning algorithm design, I've found learning theory to be useful,
although it's application is more subtle than many realize. The most
straightforward applications can fail, because (as expectation suggests) worst
case bounds tend to be loose in practice (*). In my experience, considering
learning theory when designing an algorithm has two important effects in
practice:It can help make your algorithm behave right at a crude level of
analysis, leaving finer details to tuning or common sense. The best example I
have of this is</p><p>3 0.13735236 <a title="368-tfidf-3" href="../hunch_net-2005/hunch_net-2005-03-15-The_State_of_Tight_Bounds.html">41 hunch net-2005-03-15-The State of Tight Bounds</a></p>
<p>Introduction: What?Bounds are mathematical formulas relating observations to future error
rates assuming that data is drawn independently. In classical statistics, they
are calld confidence intervals.Why?Good Judgement. In many applications of
learning, it is desirable to know how well the learned predictor works in the
future. This helps you decide if the problem is solved or not.Learning
Essence. The form of some of these bounds helps you understand what the
essence of learning is.Algorithm Design. Some of these bounds suggest,
motivate, or even directly imply learning algorithms.What We Know NowThere are
several families of bounds, based on how information is used.Testing Bounds.
These are methods which use labeled data not used in training to estimate the
future error rate. Examples include thetest set bound,progressive
validationalsohereandhere,train and test bounds, and cross-validation (but see
thebig open problem). These techniques are the best available for goal (1)
above, but provide littl</p><p>4 0.12141765 <a title="368-tfidf-4" href="../hunch_net-2005/hunch_net-2005-07-04-The_Health_of_COLT.html">89 hunch net-2005-07-04-The Health of COLT</a></p>
<p>Introduction: The health ofCOLT(Conference on Learning Theory or Computational Learning
Theory depending on who you ask) has been questioned over the last few years.
Low points for the conference occurred whenEuroCOLTmerged with COLT in 2001,
and the attendance at the 2002 Sydney COLT fell to a new low. This occurred in
the general context of machine learning conferences rising in both number and
size over the last decade.Any discussion ofwhyCOLT has had difficulties is
inherently controversial as is any story about well-intentioned people making
the wrong decisions. Nevertheless, this may be worth discussing in the hope of
avoiding problems in the future and general understanding. In any such
discussion there is a strong tendency to identify with a conference/community
in a patriotic manner that is detrimental to thinking. Keep in mind that
conferences exist to further research.My understanding (I wasn't around) is
that COLT started as a subcommunity of the computer science theory community.
This i</p><p>5 0.11953912 <a title="368-tfidf-5" href="../hunch_net-2008/hunch_net-2008-01-18-Datasets.html">284 hunch net-2008-01-18-Datasets</a></p>
<p>Introduction: David Pennocknotes the impressiveset of datasetsatdatawrangling.</p><p>6 0.1170532 <a title="368-tfidf-6" href="../hunch_net-2007/hunch_net-2007-05-09-The_Missing_Bound.html">244 hunch net-2007-05-09-The Missing Bound</a></p>
<p>7 0.10923685 <a title="368-tfidf-7" href="../hunch_net-2006/hunch_net-2006-03-12-Online_learning_or_online_preservation_of_learning%3F.html">163 hunch net-2006-03-12-Online learning or online preservation of learning?</a></p>
<p>8 0.10743575 <a title="368-tfidf-8" href="../hunch_net-2005/hunch_net-2005-06-18-Lower_Bounds_for_Learning_Reductions.html">83 hunch net-2005-06-18-Lower Bounds for Learning Reductions</a></p>
<p>9 0.10284502 <a title="368-tfidf-9" href="../hunch_net-2007/hunch_net-2007-06-24-Interesting_Papers_at_ICML_2007.html">251 hunch net-2007-06-24-Interesting Papers at ICML 2007</a></p>
<p>10 0.1021737 <a title="368-tfidf-10" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>11 0.10013987 <a title="368-tfidf-11" href="../hunch_net-2007/hunch_net-2007-06-14-Interesting_Papers_at_COLT_2007.html">247 hunch net-2007-06-14-Interesting Papers at COLT 2007</a></p>
<p>12 0.097223468 <a title="368-tfidf-12" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>13 0.094629109 <a title="368-tfidf-13" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>14 0.092745438 <a title="368-tfidf-14" href="../hunch_net-2009/hunch_net-2009-06-24-Interesting_papers_at_UAICMOLT_2009.html">361 hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</a></p>
<p>15 0.092563473 <a title="368-tfidf-15" href="../hunch_net-2013/hunch_net-2013-07-10-Thoughts_on_Artificial_Intelligence.html">486 hunch net-2013-07-10-Thoughts on Artificial Intelligence</a></p>
<p>16 0.091994375 <a title="368-tfidf-16" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>17 0.09151341 <a title="368-tfidf-17" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>18 0.09136796 <a title="368-tfidf-18" href="../hunch_net-2006/hunch_net-2006-04-06-Bounds_greater_than_1.html">170 hunch net-2006-04-06-Bounds greater than 1</a></p>
<p>19 0.087991156 <a title="368-tfidf-19" href="../hunch_net-2005/hunch_net-2005-06-28-A_COLT_paper.html">85 hunch net-2005-06-28-A COLT paper</a></p>
<p>20 0.087670907 <a title="368-tfidf-20" href="../hunch_net-2005/hunch_net-2005-09-26-Prediction_Bounds_as_the_Mathematics_of_Science.html">115 hunch net-2005-09-26-Prediction Bounds as the Mathematics of Science</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.198), (1, -0.009), (2, -0.051), (3, 0.004), (4, -0.072), (5, -0.071), (6, 0.027), (7, -0.138), (8, -0.035), (9, -0.116), (10, -0.044), (11, -0.023), (12, -0.054), (13, 0.015), (14, 0.027), (15, 0.118), (16, 0.04), (17, -0.03), (18, 0.204), (19, -0.041), (20, -0.068), (21, -0.066), (22, -0.074), (23, -0.011), (24, -0.009), (25, -0.013), (26, -0.082), (27, -0.105), (28, 0.007), (29, 0.091), (30, -0.0), (31, 0.031), (32, 0.08), (33, -0.014), (34, 0.025), (35, 0.067), (36, -0.079), (37, 0.06), (38, -0.091), (39, -0.054), (40, -0.047), (41, 0.063), (42, 0.05), (43, 0.048), (44, 0.019), (45, -0.076), (46, -0.057), (47, -0.029), (48, 0.052), (49, 0.05)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.92995632 <a title="368-lsi-1" href="../hunch_net-2009/hunch_net-2009-08-26-Another_10-year_paper_in_Machine_Learning.html">368 hunch net-2009-08-26-Another 10-year paper in Machine Learning</a></p>
<p>Introduction: When I was thinking about the best "10 year paper" forICML, I also took a look
at a few other conferences. Here is one from 10 years ago that interested
me:David McAllesterPAC-Bayesian Model Averaging,COLT1999.2001 Journal
Draft.Prior to this paper, the only mechanism known for controlling or
estimating the necessary sample complexity for learning over continuously
parameterized predictors was VC theory and variants, all of which suffered
from a basic problem: they were incredibly pessimistic in practice. This meant
that only very gross guidance could be provided for learning algorithm design.
The PAC-Bayes bound provided an alternative approach to sample complexity
bounds which was radically tighter, quantitatively. It also imported and
explained many of the motivations for Bayesian learning in a way that learning
theory and perhaps optimization people might appreciate. Since this paper came
out, there have been a number of moderately successful attempts to drive
algorithms directly b</p><p>2 0.63926846 <a title="368-lsi-2" href="../hunch_net-2007/hunch_net-2007-05-09-The_Missing_Bound.html">244 hunch net-2007-05-09-The Missing Bound</a></p>
<p>Introduction: Sham Kakadepoints out that we are missing a bound.Suppose we
havemsamplesxdrawn IID from some distributionD. Through the magic of
exponential moment method we know that:If the range ofxis bounded by an
interval of sizeI, aChernoff/Hoeffding style boundgives us a bound on the
deviations likeO(I/m0.5)(at least in crude form). A proof is on page 9here.If
the range ofxis bounded, and the variance (or a bound on the variance) is
known, thenBennett's boundcan give tighter results (*). This can be a huge
improvment when the true variance small.What's missing here is a bound that
depends on the observed variance rather than a bound on the variance. This
means that many people attempt to use Bennett's bound (incorrectly) by
plugging the observed variance in as the true variance, invalidating the bound
application. Most of the time, they get away with it, but this is a dangerous
move when doing machine learning. In machine learning, we are typically trying
to find a predictor with 0 expected los</p><p>3 0.6186285 <a title="368-lsi-3" href="../hunch_net-2005/hunch_net-2005-06-28-A_COLT_paper.html">85 hunch net-2005-06-28-A COLT paper</a></p>
<p>Introduction: I found Tong Zhang's paper onData Dependent Concentration Bounds for
Sequential Prediction Algorithmsinteresting. Roughly speaking, it states a
tight bound on the future error rate for online learning algorithms assuming
that samples are drawn independently. This bound is easily computed and will
make the progressive validation approaches usedheresignificantly more
practical.</p><p>4 0.58793885 <a title="368-lsi-4" href="../hunch_net-2006/hunch_net-2006-04-06-Bounds_greater_than_1.html">170 hunch net-2006-04-06-Bounds greater than 1</a></p>
<p>Introduction: Nati SrebroandShai Ben-Davidhave apaperatCOLTwhich, in the appendix, proves
something very striking: several previous error bounds arealwaysgreater than
1.BackgroundOne branch of learning theory focuses on theorems whichAssume
samples are drawn IID from an unknown distributionD.Fix a set of
classifiersFind a high probability bound on the maximum true error rate (with
respect toD) as a function of the empirical error rate on the training
set.Many of these bounds become extremely complex and hairy.CurrentEveryone
working on this subject wants "tighter bounds", however there are different
definitions of "tighter". Some groups focus on "functional tightness" (getting
the right functional dependency between the size of the training set and a
parameterization of the hypothesis space) whileothersfocus on "practical
tightness" (finding bounds which work well on practical problems). (I am
definitely in the second camp.)One of the dangers of striving for "functional
tightness" is that the bound</p><p>5 0.53264654 <a title="368-lsi-5" href="../hunch_net-2007/hunch_net-2007-06-14-Interesting_Papers_at_COLT_2007.html">247 hunch net-2007-06-14-Interesting Papers at COLT 2007</a></p>
<p>Introduction: Here are two papers that seem particularly interesting at this year's
COLT.Gilles BlanchardandFranÃƒÂ§ois Fleuret,Occam's Hammer. When we are
interested in very tight bounds on the true error rate of a classifier, it is
tempting to use a PAC-Bayes bound which can (empirically) bequite tight. A
disadvantage of the PAC-Bayes bound is that it applies to a classifier which
is randomized over a set of base classifiers rather than a single classifier.
This paper shows that a similar bound can be proved which holds for a single
classifier drawn from the set. The ability to safely use a single classifier
is very nice. This technique applies generically to any base bound, so it has
other applications covered in the paper.Adam Tauman Kalai.Learning Nested
Halfspaces and Uphill Decision Trees. Classification PAC-learning, where you
prove that any problem amongst some set is polytime learnable with respect to
any distribution over the inputXis extraordinarily challenging as judged by
lack of progr</p><p>6 0.52430242 <a title="368-lsi-6" href="../hunch_net-2010/hunch_net-2010-03-26-A_Variance_only_Deviation_Bound.html">392 hunch net-2010-03-26-A Variance only Deviation Bound</a></p>
<p>7 0.51602238 <a title="368-lsi-7" href="../hunch_net-2008/hunch_net-2008-11-10-ICML_Reviewing_Criteria.html">325 hunch net-2008-11-10-ICML Reviewing Criteria</a></p>
<p>8 0.51572907 <a title="368-lsi-8" href="../hunch_net-2005/hunch_net-2005-03-15-The_State_of_Tight_Bounds.html">41 hunch net-2005-03-15-The State of Tight Bounds</a></p>
<p>9 0.5071888 <a title="368-lsi-9" href="../hunch_net-2006/hunch_net-2006-03-12-Online_learning_or_online_preservation_of_learning%3F.html">163 hunch net-2006-03-12-Online learning or online preservation of learning?</a></p>
<p>10 0.50373286 <a title="368-lsi-10" href="../hunch_net-2005/hunch_net-2005-04-21-Dynamic_Programming_Generalizations_and_Their_Use.html">58 hunch net-2005-04-21-Dynamic Programming Generalizations and Their Use</a></p>
<p>11 0.49536324 <a title="368-lsi-11" href="../hunch_net-2006/hunch_net-2006-07-05-more_icml_papers.html">189 hunch net-2006-07-05-more icml papers</a></p>
<p>12 0.48456198 <a title="368-lsi-12" href="../hunch_net-2005/hunch_net-2005-09-26-Prediction_Bounds_as_the_Mathematics_of_Science.html">115 hunch net-2005-09-26-Prediction Bounds as the Mathematics of Science</a></p>
<p>13 0.48176634 <a title="368-lsi-13" href="../hunch_net-2007/hunch_net-2007-12-20-Cool_and_Interesting_things_at_NIPS%2C_take_three.html">280 hunch net-2007-12-20-Cool and Interesting things at NIPS, take three</a></p>
<p>14 0.47844681 <a title="368-lsi-14" href="../hunch_net-2009/hunch_net-2009-06-24-Interesting_papers_at_UAICMOLT_2009.html">361 hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</a></p>
<p>15 0.47586641 <a title="368-lsi-15" href="../hunch_net-2006/hunch_net-2006-06-30-ICML_papers.html">188 hunch net-2006-06-30-ICML papers</a></p>
<p>16 0.45652276 <a title="368-lsi-16" href="../hunch_net-2009/hunch_net-2009-01-07-Interesting_Papers_at_SODA_2009.html">334 hunch net-2009-01-07-Interesting Papers at SODA 2009</a></p>
<p>17 0.4524833 <a title="368-lsi-17" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>18 0.44726628 <a title="368-lsi-18" href="../hunch_net-2007/hunch_net-2007-06-24-Interesting_Papers_at_ICML_2007.html">251 hunch net-2007-06-24-Interesting Papers at ICML 2007</a></p>
<p>19 0.44235647 <a title="368-lsi-19" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>20 0.43326277 <a title="368-lsi-20" href="../hunch_net-2011/hunch_net-2011-08-01-Interesting_papers_at_COLT_2011.html">439 hunch net-2011-08-01-Interesting papers at COLT 2011</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(35, 0.097), (42, 0.279), (45, 0.027), (63, 0.424), (74, 0.065)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.91433722 <a title="368-lda-1" href="../hunch_net-2008/hunch_net-2008-04-21-The_Science_2.0_article.html">296 hunch net-2008-04-21-The Science 2.0 article</a></p>
<p>Introduction: I found the article aboutscience using modern tools interesting, especially
the part about 'blogophobia', which in my experience is often a substantial
issue: many potential guest posters aren't quite ready, because of the fear of
a permanent public mistake, because it is particularly hard to write about the
unknown (the essence of research), and because the system for public credit
doesn't yet really handle blog posts.So far, science has been relatively
resistant to discussing research on blogs. Some things need to change to get
there. Public tolerance of the occasional mistake is essential, as is a
willingness to cite (and credit) blogs as freely as papers.I've often run into
another reason for holding back myself: I don't want to overtalk my own
research. Nevertheless, I'm slowly changing to the opinion that I'm holding
back too much: the real power of a blog in research is that it can be used to
confer with many people, and that just makes research work better.</p><p>2 0.90814859 <a title="368-lda-2" href="../hunch_net-2010/hunch_net-2010-08-20-The_Workshop_on_Cores%2C_Clusters%2C_and_Clouds.html">404 hunch net-2010-08-20-The Workshop on Cores, Clusters, and Clouds</a></p>
<p>Introduction: Alekh,John,Ofer, and I are organizing aworkshopatNIPSthis year on learning in
parallel and distributed environments. The general interest level in parallel
learning seems to be growing rapidly, so I expect quite a bit of attendance.
Please join us if you are parallel-interested.And, if you are working in the
area of parallel learning, please considersubmitting an abstractdue Oct. 17
for presentation at the workshop.</p><p>same-blog 3 0.83668339 <a title="368-lda-3" href="../hunch_net-2009/hunch_net-2009-08-26-Another_10-year_paper_in_Machine_Learning.html">368 hunch net-2009-08-26-Another 10-year paper in Machine Learning</a></p>
<p>Introduction: When I was thinking about the best "10 year paper" forICML, I also took a look
at a few other conferences. Here is one from 10 years ago that interested
me:David McAllesterPAC-Bayesian Model Averaging,COLT1999.2001 Journal
Draft.Prior to this paper, the only mechanism known for controlling or
estimating the necessary sample complexity for learning over continuously
parameterized predictors was VC theory and variants, all of which suffered
from a basic problem: they were incredibly pessimistic in practice. This meant
that only very gross guidance could be provided for learning algorithm design.
The PAC-Bayes bound provided an alternative approach to sample complexity
bounds which was radically tighter, quantitatively. It also imported and
explained many of the motivations for Bayesian learning in a way that learning
theory and perhaps optimization people might appreciate. Since this paper came
out, there have been a number of moderately successful attempts to drive
algorithms directly b</p><p>4 0.81267643 <a title="368-lda-4" href="../hunch_net-2013/hunch_net-2013-07-10-Thoughts_on_Artificial_Intelligence.html">486 hunch net-2013-07-10-Thoughts on Artificial Intelligence</a></p>
<p>Introduction: David McAllesterstarts a blog.</p><p>5 0.74709028 <a title="368-lda-5" href="../hunch_net-2008/hunch_net-2008-01-06-Research_Political_Issues.html">282 hunch net-2008-01-06-Research Political Issues</a></p>
<p>Introduction: I've avoided discussing politics here, although not for lack of interest. The
problem with discussing politics is that it's customary for people to say much
based upon little information. Nevertheless, politics can have a substantial
impact on science (and we might hope for the vice-versa). It's primary
election time in the United States, so the topic is timely, although the
issues are not.There are several policy decisions which substantially effect
development of science and technology in the US.EducationThe US has great
contrasts in education. The top universities are very good places, yet the
grade school education system produces mediocre results. For me, the contrast
between apublic educationandCaltechwas bracing. For many others attending
Caltech, it clearly was not. Upgrading the k-12 education system in the US is
a long-standing chronic problem which I know relatively little about. My own
experience is that a basic attitude of "no child unrealized" is better than
"no child lef</p><p>6 0.73679644 <a title="368-lda-6" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>7 0.60945237 <a title="368-lda-7" href="../hunch_net-2007/hunch_net-2007-01-02-Retrospective.html">225 hunch net-2007-01-02-Retrospective</a></p>
<p>8 0.57369792 <a title="368-lda-8" href="../hunch_net-2005/hunch_net-2005-02-15-ESPgame_and_image_labeling.html">20 hunch net-2005-02-15-ESPgame and image labeling</a></p>
<p>9 0.56444675 <a title="368-lda-9" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>10 0.56185615 <a title="368-lda-10" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>11 0.56024218 <a title="368-lda-11" href="../hunch_net-2009/hunch_net-2009-04-02-Asymmophobia.html">348 hunch net-2009-04-02-Asymmophobia</a></p>
<p>12 0.55936217 <a title="368-lda-12" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>13 0.5590784 <a title="368-lda-13" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>14 0.55880654 <a title="368-lda-14" href="../hunch_net-2006/hunch_net-2006-02-11-Yahoo%26%238217%3Bs_Learning_Problems..html">156 hunch net-2006-02-11-Yahoo&#8217;s Learning Problems.</a></p>
<p>15 0.55860651 <a title="368-lda-15" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>16 0.55822706 <a title="368-lda-16" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>17 0.55739659 <a title="368-lda-17" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>18 0.55729562 <a title="368-lda-18" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<p>19 0.55669039 <a title="368-lda-19" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>20 0.55632919 <a title="368-lda-20" href="../hunch_net-2011/hunch_net-2011-10-24-2011_ML_symposium_and_the_bears.html">448 hunch net-2011-10-24-2011 ML symposium and the bears</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
