<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>385 hunch net-2009-12-27-Interesting things at NIPS 2009</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2009" href="../home/hunch_net-2009_home.html">hunch_net-2009</a> <a title="hunch_net-2009-385" href="#">hunch_net-2009-385</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>385 hunch net-2009-12-27-Interesting things at NIPS 2009</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2009-385-html" href="http://hunch.net/?p=1152">html</a></p><p>Introduction: Several papers at NIPS caught my attention.Elad HazanandSatyen Kale,Online
Submodular OptimizationThey define an algorithm for online optimization of
submodular functions with regret guarantees. This places submodular
optimization roughly on par with online convex optimization as tractable
settings for online learning.Elad HazanandSatyen KaleOn Stochastic and Worst-
Case Models of Investing. At it's core, this is yet another example of
modifying worst-case online learning to deal with variance, but the
application to financial models is particularly cool and it seems plausibly
superior other common approaches for financial modeling.Mark Palatucci,Dean
Pomerlau,Tom Mitchell, andGeoff HintonZero Shot Learning with Semantic Output
CodesThe goal here is predicting a label in a multiclass supervised setting
where the label never occurs in the training data. They have some basic
analysis and also a nice application to FMRI brain reading.Shobha
Venkataraman,Avrim Blum,Dawn Song,Subhabrata Sen</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('submodular', 0.344), ('semantic', 0.267), ('hazanandsatyen', 0.204), ('online', 0.201), ('financial', 0.17), ('optimization', 0.15), ('talk', 0.138), ('involves', 0.137), ('representations', 0.132), ('parameter', 0.132), ('exponential', 0.125), ('label', 0.105), ('workshops', 0.105), ('based', 0.102), ('carlos', 0.102), ('justified', 0.102), ('sexy', 0.102), ('fmri', 0.102), ('modifying', 0.102), ('asa', 0.102)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="385-tfidf-1" href="../hunch_net-2009/hunch_net-2009-12-27-Interesting_things_at_NIPS_2009.html">385 hunch net-2009-12-27-Interesting things at NIPS 2009</a></p>
<p>Introduction: Several papers at NIPS caught my attention.Elad HazanandSatyen Kale,Online
Submodular OptimizationThey define an algorithm for online optimization of
submodular functions with regret guarantees. This places submodular
optimization roughly on par with online convex optimization as tractable
settings for online learning.Elad HazanandSatyen KaleOn Stochastic and Worst-
Case Models of Investing. At it's core, this is yet another example of
modifying worst-case online learning to deal with variance, but the
application to financial models is particularly cool and it seems plausibly
superior other common approaches for financial modeling.Mark Palatucci,Dean
Pomerlau,Tom Mitchell, andGeoff HintonZero Shot Learning with Semantic Output
CodesThe goal here is predicting a label in a multiclass supervised setting
where the label never occurs in the training data. They have some basic
analysis and also a nice application to FMRI brain reading.Shobha
Venkataraman,Avrim Blum,Dawn Song,Subhabrata Sen</p><p>2 0.16091268 <a title="385-tfidf-2" href="../hunch_net-2007/hunch_net-2007-12-19-Cool_and_interesting_things_seen_at_NIPS.html">279 hunch net-2007-12-19-Cool and interesting things seen at NIPS</a></p>
<p>Introduction: I learned a number of things atNIPS.The financial people were there in greater
force than previously.Two Sigmasponsored NIPS whileDRW Tradinghad a
booth.Theadversarial machine learning workshophad a number of talks about
interesting applications where an adversary really is out to try and mess up
your learning algorithm. This is very different from the situation we often
think of where the world is oblivious to our learning. This may present new
and convincing applications for the learning-against-an-adversary work common
atCOLT.There were several interesing papers.Sanjoy Dasgupta,Daniel Hsu,
andClaire Monteleonihad a paper onGeneral Agnostic Active Learning. The basic
idea is that active learning can be done via reduction to a form of supervised
learning problem. This is great, because we have many supervised learning
algorithms from which the benefits of active learning may be derived.Joseph
BradleyandRobert Schapirehad aPaper on Filterboost. Filterboost is an online
boosting algorit</p><p>3 0.13935748 <a title="385-tfidf-3" href="../hunch_net-2006/hunch_net-2006-06-24-Online_convex_optimization_at_COLT.html">186 hunch net-2006-06-24-Online convex optimization at COLT</a></p>
<p>Introduction: AtICML 2003,Marty Zinkevichproposedthe online convex optimization setting and
showed that a particular gradient descent algorithm has regret O(T0.5) with
respect to the best predictor where T is the number of rounds. This seems to
be a nice model for online learning, and there has been some significant
follow-up work.AtCOLT 2006Elad Hazan,Adam Kalai,Satyen Kale, andAmit
Agarwalpresenteda modification which takes a Newton stepguaranteeing O(log T)
regret when the first and second derivatives are bounded.Then they applied
these algorithms to portfolio managementatICML 2006(withRobert Schapire)
yielding some very fun graphs.</p><p>4 0.13762751 <a title="385-tfidf-4" href="../hunch_net-2007/hunch_net-2007-06-24-Interesting_Papers_at_ICML_2007.html">251 hunch net-2007-06-24-Interesting Papers at ICML 2007</a></p>
<p>Introduction: Here are a few of the papers I enjoyed at ICML.Steffen Bickel, Michael
BrÃƒÂ¼eckner,Tobias Scheffer,Discriminative Learning for Differing Training
and Test DistributionsThere is a nice trick in this paper: they predict the
probability that an unlabeled sample is in the training set vs. the test set,
and then use this prediction to importance weight labeled samples in the
training set. This paper uses a specific parametric model, but the approach is
easily generalized.Steve HannekeA Bound on the Label Complexity of Agnostic
Active LearningThis paper bounds the number of labels required by the
A2algorithm for active learning in the agnostic case. Last year we figured out
agnostic active learning was possible. This year, it's quantified. Hopefull
soon, it will be practical.Sylvian Gelly,David SilverCombining Online and
Offline Knowledge in UCT. This paper is about techniques for improvingMoGowith
various sorts of learning. MoGo has a fair claim at being the world's best Go
algorithm.There</p><p>5 0.11759498 <a title="385-tfidf-5" href="../hunch_net-2008/hunch_net-2008-07-15-Interesting_papers_at_COLT_%28and_a_bit_of_UAI_%26%23038%3B_workshops%29.html">310 hunch net-2008-07-15-Interesting papers at COLT (and a bit of UAI &#038; workshops)</a></p>
<p>Introduction: Here are a few papers fromCOLT 2008that I found interesting.Maria-Florina
Balcan,Steve Hanneke, andJenn Wortman,The True Sample Complexity of Active
Learning. This paper shows that in an asymptotic setting, active learning
isalwaysbetter than supervised learning (although the gap may be small). This
is evidence that the only thing in the way of universal active learning is us
knowing how to do it properly.Nir AilonandMehryar Mohri,An Efficient Reduction
of Ranking to Classification. This paper shows how to robustly ranknobjects
withn log(n)classifications using a quicksort based algorithm. The result is
applicable to many ranking loss functions and has implications for
others.Michael KearnsandJennifer Wortman.Learning from Collective Behavior.
This is about learning in a new model, where the goal is to predict how a
collection of interacting agents behave. One claim is that learning in this
setting can be reduced to IID learning.Due to the relation withMetric-E3, I
was particularly int</p><p>6 0.11713499 <a title="385-tfidf-6" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>7 0.11617381 <a title="385-tfidf-7" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>8 0.10956182 <a title="385-tfidf-8" href="../hunch_net-2009/hunch_net-2009-06-24-Interesting_papers_at_UAICMOLT_2009.html">361 hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</a></p>
<p>9 0.10948682 <a title="385-tfidf-9" href="../hunch_net-2009/hunch_net-2009-01-21-Nearly_all_natural_problems_require_nonlinearity.html">337 hunch net-2009-01-21-Nearly all natural problems require nonlinearity</a></p>
<p>10 0.10941854 <a title="385-tfidf-10" href="../hunch_net-2011/hunch_net-2011-07-11-Interesting_Neural_Network_Papers_at_ICML_2011.html">438 hunch net-2011-07-11-Interesting Neural Network Papers at ICML 2011</a></p>
<p>11 0.10696384 <a title="385-tfidf-11" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>12 0.10582024 <a title="385-tfidf-12" href="../hunch_net-2009/hunch_net-2009-01-07-Interesting_Papers_at_SODA_2009.html">334 hunch net-2009-01-07-Interesting Papers at SODA 2009</a></p>
<p>13 0.10506035 <a title="385-tfidf-13" href="../hunch_net-2008/hunch_net-2008-07-06-To_Dual_or_Not.html">308 hunch net-2008-07-06-To Dual or Not</a></p>
<p>14 0.10370684 <a title="385-tfidf-14" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>15 0.10335127 <a title="385-tfidf-15" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>16 0.10205953 <a title="385-tfidf-16" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>17 0.10127012 <a title="385-tfidf-17" href="../hunch_net-2007/hunch_net-2007-10-17-Online_as_the_new_adjective.html">267 hunch net-2007-10-17-Online as the new adjective</a></p>
<p>18 0.10115067 <a title="385-tfidf-18" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>19 0.10091624 <a title="385-tfidf-19" href="../hunch_net-2011/hunch_net-2011-10-24-2011_ML_symposium_and_the_bears.html">448 hunch net-2011-10-24-2011 ML symposium and the bears</a></p>
<p>20 0.099406667 <a title="385-tfidf-20" href="../hunch_net-2010/hunch_net-2010-05-02-What%26%238217%3Bs_the_difference_between_gambling_and_rewarding_good_prediction%3F.html">397 hunch net-2010-05-02-What&#8217;s the difference between gambling and rewarding good prediction?</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.241), (1, -0.073), (2, 0.004), (3, 0.103), (4, -0.128), (5, 0.036), (6, 0.106), (7, -0.114), (8, -0.053), (9, 0.004), (10, 0.066), (11, -0.025), (12, -0.065), (13, 0.02), (14, 0.003), (15, 0.035), (16, 0.016), (17, 0.03), (18, -0.03), (19, -0.011), (20, -0.008), (21, 0.021), (22, -0.014), (23, -0.063), (24, 0.011), (25, -0.099), (26, 0.025), (27, -0.055), (28, 0.023), (29, -0.018), (30, 0.026), (31, -0.042), (32, -0.046), (33, -0.06), (34, 0.026), (35, -0.042), (36, 0.002), (37, -0.016), (38, 0.021), (39, -0.007), (40, 0.042), (41, -0.007), (42, 0.028), (43, 0.075), (44, 0.107), (45, 0.102), (46, 0.027), (47, 0.053), (48, -0.04), (49, -0.13)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94125444 <a title="385-lsi-1" href="../hunch_net-2009/hunch_net-2009-12-27-Interesting_things_at_NIPS_2009.html">385 hunch net-2009-12-27-Interesting things at NIPS 2009</a></p>
<p>Introduction: Several papers at NIPS caught my attention.Elad HazanandSatyen Kale,Online
Submodular OptimizationThey define an algorithm for online optimization of
submodular functions with regret guarantees. This places submodular
optimization roughly on par with online convex optimization as tractable
settings for online learning.Elad HazanandSatyen KaleOn Stochastic and Worst-
Case Models of Investing. At it's core, this is yet another example of
modifying worst-case online learning to deal with variance, but the
application to financial models is particularly cool and it seems plausibly
superior other common approaches for financial modeling.Mark Palatucci,Dean
Pomerlau,Tom Mitchell, andGeoff HintonZero Shot Learning with Semantic Output
CodesThe goal here is predicting a label in a multiclass supervised setting
where the label never occurs in the training data. They have some basic
analysis and also a nice application to FMRI brain reading.Shobha
Venkataraman,Avrim Blum,Dawn Song,Subhabrata Sen</p><p>2 0.6876303 <a title="385-lsi-2" href="../hunch_net-2008/hunch_net-2008-07-06-To_Dual_or_Not.html">308 hunch net-2008-07-06-To Dual or Not</a></p>
<p>Introduction: YoramandShai'sonline learning tutorialatICMLbrings up a question for me, "Why
use thedual?"The basic setting is learning a weight vectorwiso that the
functionf(x)= sumiwixioptimizes some convex loss function.The functional view
of the dual is that instead of (or in addition to) keeping track ofwiover the
feature space, you keep track of a vectorajover the examples and definewi=
sumjajxji.The above view of duality makes operating in the dual appear
unnecessary, because in the end a weight vector is always used. The tutorial
suggests that thinking about the dual gives a unified algorithmic font for
deriving online learning algorithms. I haven't worked with the dual
representation much myself, but I have seen a few examples where it appears
helpful.NoiseWhen doing online optimization (i.e. online learning where you
are allowed to look at individual examples multiple times), the dual
representation may be helpful in dealing with noisy labels.RatesOne annoyance
of working in the primal spac</p><p>3 0.67159796 <a title="385-lsi-3" href="../hunch_net-2009/hunch_net-2009-06-24-Interesting_papers_at_UAICMOLT_2009.html">361 hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</a></p>
<p>Introduction: Here's a list of papers that I found interesting atICML/COLT/UAIin 2009.Elad
HazanandComandur SeshadhriEfficient learning algorithms for changing
environmentsat ICML. This paper shows how to adapt learning algorithms that
compete with fixed predictors to compete with changing policies. The
definition of regret they deal with seems particularly useful in many
situation.Hal Daume,Unsupervised Search-based Structured Predictionat ICML.
This paper shows a technique for reducing unsupervised learning to supervised
learning which (a) make a fast unsupervised learning algorithm and (b) makes
semisupervised learning both easy and highly effective.There were two papers
with similar results on active learning in the KWIK framework for linear
regression, both reducing the sample complexity to . One wasNicolo Cesa-
Bianchi,Claudio Gentile, andFrancesco OrabonaRobust Bounds for Classification
via Selective Samplingat ICML and the other wasThomas Walsh,Istvan
Szita,Carlos Diuk,Michael LittmanExplori</p><p>4 0.66787684 <a title="385-lsi-4" href="../hunch_net-2007/hunch_net-2007-06-24-Interesting_Papers_at_ICML_2007.html">251 hunch net-2007-06-24-Interesting Papers at ICML 2007</a></p>
<p>Introduction: Here are a few of the papers I enjoyed at ICML.Steffen Bickel, Michael
BrÃƒÂ¼eckner,Tobias Scheffer,Discriminative Learning for Differing Training
and Test DistributionsThere is a nice trick in this paper: they predict the
probability that an unlabeled sample is in the training set vs. the test set,
and then use this prediction to importance weight labeled samples in the
training set. This paper uses a specific parametric model, but the approach is
easily generalized.Steve HannekeA Bound on the Label Complexity of Agnostic
Active LearningThis paper bounds the number of labels required by the
A2algorithm for active learning in the agnostic case. Last year we figured out
agnostic active learning was possible. This year, it's quantified. Hopefull
soon, it will be practical.Sylvian Gelly,David SilverCombining Online and
Offline Knowledge in UCT. This paper is about techniques for improvingMoGowith
various sorts of learning. MoGo has a fair claim at being the world's best Go
algorithm.There</p><p>5 0.63897538 <a title="385-lsi-5" href="../hunch_net-2007/hunch_net-2007-10-17-Online_as_the_new_adjective.html">267 hunch net-2007-10-17-Online as the new adjective</a></p>
<p>Introduction: Online learning is in vogue, which means we should expect to see in the near
future:Online boosting.Online decision trees.Online SVMs. (actually, we've
already seen)Online deep learning.Online parallel learning.etc…There are three
fundamental drivers of this trend.Increasing size of datasets makes online
algorithms attractive.Online learning can simply be more efficient than batch
learning. Here is a picture from a class on online learning:The point of this
picture is that even in 3 dimensions and even with linear constraints, finding
the minima of a set in an online fashion can be typically faster than finding
the minima in a batch fashion. To see this, note that there is a minimal
number of gradient updates (i.e. 2) required in order to reach the minima in
the typical case. Given this, it's best to do these updates as quickly as
possible, which implies doing the first update online (i.e. before seeing all
the examples) is preferred. Note that this is the simplest possible setting--
m</p><p>6 0.62316298 <a title="385-lsi-6" href="../hunch_net-2006/hunch_net-2006-06-24-Online_convex_optimization_at_COLT.html">186 hunch net-2006-06-24-Online convex optimization at COLT</a></p>
<p>7 0.60985702 <a title="385-lsi-7" href="../hunch_net-2007/hunch_net-2007-12-19-Cool_and_interesting_things_seen_at_NIPS.html">279 hunch net-2007-12-19-Cool and interesting things seen at NIPS</a></p>
<p>8 0.60739917 <a title="385-lsi-8" href="../hunch_net-2007/hunch_net-2007-07-01-Watchword%3A_Online_Learning.html">252 hunch net-2007-07-01-Watchword: Online Learning</a></p>
<p>9 0.60160947 <a title="385-lsi-9" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>10 0.58555257 <a title="385-lsi-10" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>11 0.58143461 <a title="385-lsi-11" href="../hunch_net-2009/hunch_net-2009-01-07-Interesting_Papers_at_SODA_2009.html">334 hunch net-2009-01-07-Interesting Papers at SODA 2009</a></p>
<p>12 0.58120459 <a title="385-lsi-12" href="../hunch_net-2005/hunch_net-2005-12-09-Some_NIPS_papers.html">138 hunch net-2005-12-09-Some NIPS papers</a></p>
<p>13 0.57162756 <a title="385-lsi-13" href="../hunch_net-2009/hunch_net-2009-01-21-Nearly_all_natural_problems_require_nonlinearity.html">337 hunch net-2009-01-21-Nearly all natural problems require nonlinearity</a></p>
<p>14 0.56311625 <a title="385-lsi-14" href="../hunch_net-2005/hunch_net-2005-09-08-Online_Learning_as_the_Mathematics_of_Accountability.html">109 hunch net-2005-09-08-Online Learning as the Mathematics of Accountability</a></p>
<p>15 0.56158823 <a title="385-lsi-15" href="../hunch_net-2006/hunch_net-2006-07-05-more_icml_papers.html">189 hunch net-2006-07-05-more icml papers</a></p>
<p>16 0.56083316 <a title="385-lsi-16" href="../hunch_net-2006/hunch_net-2006-06-30-ICML_papers.html">188 hunch net-2006-06-30-ICML papers</a></p>
<p>17 0.5565111 <a title="385-lsi-17" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>18 0.55211896 <a title="385-lsi-18" href="../hunch_net-2008/hunch_net-2008-07-15-Interesting_papers_at_COLT_%28and_a_bit_of_UAI_%26%23038%3B_workshops%29.html">310 hunch net-2008-07-15-Interesting papers at COLT (and a bit of UAI &#038; workshops)</a></p>
<p>19 0.54981661 <a title="385-lsi-19" href="../hunch_net-2008/hunch_net-2008-04-26-Eliminating_the_Birthday_Paradox_for_Universal_Features.html">298 hunch net-2008-04-26-Eliminating the Birthday Paradox for Universal Features</a></p>
<p>20 0.54224271 <a title="385-lsi-20" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(26, 0.273), (35, 0.06), (38, 0.013), (39, 0.043), (42, 0.274), (45, 0.027), (68, 0.066), (74, 0.095), (91, 0.033), (95, 0.039)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.93856871 <a title="385-lda-1" href="../hunch_net-2007/hunch_net-2007-12-20-Cool_and_Interesting_things_at_NIPS%2C_take_three.html">280 hunch net-2007-12-20-Cool and Interesting things at NIPS, take three</a></p>
<p>Introduction: Following up on Hal Daume's post and John's post on cool and interesting
things seen at NIPS I'll post my own little list of neat papers here as well.
Of course it's going to be biased towards what I think is interesting. Also, I
have to say that I hadn't been able to see many papers this year at nips due
to myself being too busy, so please feel free to contribute the papers that
you liked1. P. Mudigonda, V. Kolmogorov, P. Torr. An Analysis of Convex
Relaxations for MAP Estimation. A surprising paper which shows that many of
the more sophisticated convex relaxations that had been proposed recently
turns out to be subsumed by the simplest LP relaxation. Be careful next time
you try a cool new convex relaxation!2. D. Sontag, T. Jaakkola. New Outer
Bounds on the Marginal Polytope. The title says it all. The marginal polytope
is the set of local marginal distributions over subsets of variables that are
globally consistent in the sense that there is at least one distribution over
all the va</p><p>same-blog 2 0.8828522 <a title="385-lda-2" href="../hunch_net-2009/hunch_net-2009-12-27-Interesting_things_at_NIPS_2009.html">385 hunch net-2009-12-27-Interesting things at NIPS 2009</a></p>
<p>Introduction: Several papers at NIPS caught my attention.Elad HazanandSatyen Kale,Online
Submodular OptimizationThey define an algorithm for online optimization of
submodular functions with regret guarantees. This places submodular
optimization roughly on par with online convex optimization as tractable
settings for online learning.Elad HazanandSatyen KaleOn Stochastic and Worst-
Case Models of Investing. At it's core, this is yet another example of
modifying worst-case online learning to deal with variance, but the
application to financial models is particularly cool and it seems plausibly
superior other common approaches for financial modeling.Mark Palatucci,Dean
Pomerlau,Tom Mitchell, andGeoff HintonZero Shot Learning with Semantic Output
CodesThe goal here is predicting a label in a multiclass supervised setting
where the label never occurs in the training data. They have some basic
analysis and also a nice application to FMRI brain reading.Shobha
Venkataraman,Avrim Blum,Dawn Song,Subhabrata Sen</p><p>3 0.84175968 <a title="385-lda-3" href="../hunch_net-2005/hunch_net-2005-09-26-Prediction_Bounds_as_the_Mathematics_of_Science.html">115 hunch net-2005-09-26-Prediction Bounds as the Mathematics of Science</a></p>
<p>Introduction: "Science" has many meanings, but one common meaning is "thescientific method"
which is a principled method for investigating the world using the following
steps:Form a hypothesis about the world.Use the hypothesis to make
predictions.Run experiments to confirm or disprove the predictions.The
ordering of these steps is very important to the scientific method. In
particular, predictionsmustbe made before experiments are run.Given that we
all believe in the scientific method of investigation, it may be surprising to
learn that cheating is very common. This happens for many reasons, some
innocent and some not.Drug studies. Pharmaceutical companies make predictions
about the effects of their drugs and then conduct blind clinical studies to
determine their effect. Unfortunately, they have also been caught using some
of the more advanced techniques for cheatinghere: including "reprobleming",
"data set selection", and probably "overfitting by review". It isn't too
surprising to observe this: w</p><p>4 0.8084439 <a title="385-lda-4" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<p>Introduction: Martin Pooland I recently discussed the similarities and differences between
academia and open source programming.Similarities:Cost profileResearch and
programming share approximately the same cost profile: A large upfront effort
is required to produce something useful, and then "anyone" can use it. (The
"anyone" is not quite right for either group because only sufficiently
technical people could use it.)Wealth profileA "wealthy" academic or open
source programmer is someone who has contributed a lot to other people in
research or programs. Much of academia is a "gift culture": whoever gives the
most is most respected.ProblemsBoth academia and open source programming
suffer from similar problems.Whether or not (and which) open source program is
used are perhaps too-often personality driven rather than driven by capability
or usefulness. Similar phenomena can happen in academia with respect to
directions of research.Funding is often a problem for both groups. Academics
often invest many</p><p>5 0.73037189 <a title="385-lda-5" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>Introduction: At NIPS,Andrew Ngasked me what should be in a large scale learning class.
After some discussion with him andNandoand mulling it over a bit, these are
the topics that I think should be covered.There are many different kinds of
scaling.Scaling in examplesThis is the most basic kind of scaling.Online
Gradient DescentThis is an old algorithm--I'm not sure if anyone can be
credited with it in particular. Perhaps thePerceptronis a good precursor, but
substantial improvements come from the notion of a loss function of
whichsquared loss,logistic loss, Hinge Loss, andQuantile Lossare all worth
covering. It's important to cover thesemanticsof these loss functions as
well.Vowpal Wabbitis a reasonably fast codebase implementing these.Second
Order Gradient Descent methodsFor some problems, methods taking into account
second derivative information can be more effective. I've seen preconditioned
conjugate gradient work well, for whichJonathan Shewchuck'swriteupis
reasonable. Nando likesL-BFGSwhich I</p><p>6 0.73024678 <a title="385-lda-6" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>7 0.72889775 <a title="385-lda-7" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>8 0.7285887 <a title="385-lda-8" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>9 0.72839123 <a title="385-lda-9" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>10 0.72786975 <a title="385-lda-10" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>11 0.72752035 <a title="385-lda-11" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>12 0.72746497 <a title="385-lda-12" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>13 0.72729796 <a title="385-lda-13" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>14 0.72662139 <a title="385-lda-14" href="../hunch_net-2005/hunch_net-2005-03-17-Going_all_the_Way%2C_Sometimes.html">42 hunch net-2005-03-17-Going all the Way, Sometimes</a></p>
<p>15 0.72646356 <a title="385-lda-15" href="../hunch_net-2007/hunch_net-2007-12-21-Vowpal_Wabbit_Code_Release.html">281 hunch net-2007-12-21-Vowpal Wabbit Code Release</a></p>
<p>16 0.72643787 <a title="385-lda-16" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>17 0.72555971 <a title="385-lda-17" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>18 0.72449398 <a title="385-lda-18" href="../hunch_net-2007/hunch_net-2007-08-19-Choice_of_Metrics.html">259 hunch net-2007-08-19-Choice of Metrics</a></p>
<p>19 0.72441381 <a title="385-lda-19" href="../hunch_net-2006/hunch_net-2006-01-18-Is_Multitask_Learning_Black-Boxable%3F.html">149 hunch net-2006-01-18-Is Multitask Learning Black-Boxable?</a></p>
<p>20 0.72374314 <a title="385-lda-20" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
