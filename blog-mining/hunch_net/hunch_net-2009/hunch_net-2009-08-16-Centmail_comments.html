<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>367 hunch net-2009-08-16-Centmail comments</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2009" href="../home/hunch_net-2009_home.html">hunch_net-2009</a> <a title="hunch_net-2009-367" href="#">hunch_net-2009-367</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>367 hunch net-2009-08-16-Centmail comments</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2009-367-html" href="http://hunch.net/?p=887">html</a></p><p>Introduction: Centmail  is a scheme which makes charity donations have a secondary value, as a stamp for email.  When discussed on  newscientist ,  slashdot , and others, some of the comments make the academic review process appear thoughtful   .  Some prominent fallacies are:
  
 Costing money fallacy.  Some commenters appear to believe the system charges money per email.  Instead, the basic idea is that users get an extra benefit from donations to a charity and participation is strictly voluntary.  The solution to this fallacy is simply reading  the details . 
 Single solution fallacy.  Some commenters seem to think this is proposed as a complete solution to spam, and since not everyone will opt to participate, it won’t work.  But a complete solution is not at all necessary or even possible given the  flag-day problem .  Deployed machine learning systems for fighting spam are great at taking advantage of a partial solution.  The solution to this fallacy is learning about machine learning.  In the</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Centmail  is a scheme which makes charity donations have a secondary value, as a stamp for email. [sent-1, score-0.577]
</p><p>2 When discussed on  newscientist ,  slashdot , and others, some of the comments make the academic review process appear thoughtful   . [sent-2, score-0.381]
</p><p>3 Some prominent fallacies are:     Costing money fallacy. [sent-3, score-0.129]
</p><p>4 Some commenters appear to believe the system charges money per email. [sent-4, score-0.521]
</p><p>5 Instead, the basic idea is that users get an extra benefit from donations to a charity and participation is strictly voluntary. [sent-5, score-0.724]
</p><p>6 The solution to this fallacy is simply reading  the details . [sent-6, score-0.645]
</p><p>7 Some commenters seem to think this is proposed as a complete solution to spam, and since not everyone will opt to participate, it won’t work. [sent-8, score-0.839]
</p><p>8 But a complete solution is not at all necessary or even possible given the  flag-day problem . [sent-9, score-0.354]
</p><p>9 Deployed machine learning systems for fighting spam are great at taking advantage of a partial solution. [sent-10, score-0.597]
</p><p>10 The solution to this fallacy is learning about machine learning. [sent-11, score-0.552]
</p><p>11 In the current state of affairs, informed comment about spam fighting without knowing machine learning is difficult to imagine. [sent-12, score-0.68]
</p><p>12 Some commenters seem to think that stamps can be reused arbitrarily on emails. [sent-14, score-0.464]
</p><p>13 The solution to this fallacy is simply checking the details and possibly learning about cryptographics hashes. [sent-16, score-0.793]
</p><p>14 Dan Reeves made a very detailed  FAQ  trying to address all the failure modes seen in comments, and there is a bit more discussion at  messy matters . [sent-17, score-0.432]
</p><p>15 My personal opinion is that Centmail is an interesting idea that might work, avoids the failure modes of many other ideas, hasn’t failed yet, and hence is worth trying. [sent-18, score-0.591]
</p><p>16 It’s a better approach than  my earlier thoughts on economic solutions to spam . [sent-19, score-0.465]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('fallacy', 0.317), ('spam', 0.316), ('commenters', 0.294), ('centmail', 0.238), ('solution', 0.235), ('fighting', 0.211), ('donations', 0.211), ('charity', 0.196), ('modes', 0.169), ('money', 0.129), ('complete', 0.119), ('comments', 0.11), ('failure', 0.109), ('costing', 0.106), ('affairs', 0.106), ('opt', 0.106), ('deployed', 0.106), ('reeves', 0.106), ('appear', 0.098), ('ignores', 0.098), ('details', 0.093), ('strictly', 0.092), ('thoughtful', 0.088), ('scheme', 0.088), ('proposed', 0.085), ('crypto', 0.085), ('existence', 0.085), ('slashdot', 0.085), ('failed', 0.085), ('reused', 0.085), ('arbitrarily', 0.085), ('dan', 0.085), ('participation', 0.082), ('informed', 0.082), ('secondary', 0.082), ('hasn', 0.079), ('opinion', 0.079), ('matters', 0.079), ('economic', 0.079), ('avoids', 0.077), ('checking', 0.077), ('detailed', 0.075), ('participate', 0.075), ('broken', 0.073), ('idea', 0.072), ('possibly', 0.071), ('knowing', 0.071), ('users', 0.071), ('thoughts', 0.07), ('partial', 0.07)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999982 <a title="367-tfidf-1" href="../hunch_net-2009/hunch_net-2009-08-16-Centmail_comments.html">367 hunch net-2009-08-16-Centmail comments</a></p>
<p>Introduction: Centmail  is a scheme which makes charity donations have a secondary value, as a stamp for email.  When discussed on  newscientist ,  slashdot , and others, some of the comments make the academic review process appear thoughtful   .  Some prominent fallacies are:
  
 Costing money fallacy.  Some commenters appear to believe the system charges money per email.  Instead, the basic idea is that users get an extra benefit from donations to a charity and participation is strictly voluntary.  The solution to this fallacy is simply reading  the details . 
 Single solution fallacy.  Some commenters seem to think this is proposed as a complete solution to spam, and since not everyone will opt to participate, it won’t work.  But a complete solution is not at all necessary or even possible given the  flag-day problem .  Deployed machine learning systems for fighting spam are great at taking advantage of a partial solution.  The solution to this fallacy is learning about machine learning.  In the</p><p>2 0.27777895 <a title="367-tfidf-2" href="../hunch_net-2006/hunch_net-2006-12-06-The_Spam_Problem.html">223 hunch net-2006-12-06-The Spam Problem</a></p>
<p>Introduction: The  New York Times  has an article on the  growth of spam .  Interesting facts include: 9/10 of all email is spam, spam source identification is nearly useless due to botnet spam senders, and image based spam (emails which consist of an image only) are on the growth.
 
Estimates of the cost of spam are almost certainly far to low, because they do not account for the cost in time lost by people.
 
The image based spam which is currently penetrating many filters should be catchable with a more sophisticated application of machine learning technology.  For the spam I see, the rendered images come in only a few formats, which would be easy to recognize via a support vector machine (with RBF kernel), neural network, or even nearest-neighbor architecture.  The mechanics of setting this up to run efficiently is the only real challenge.  This is the next step in the spam war.
 
The response to this system is to make the image based spam even more random.  We should (essentially) expect to see</p><p>3 0.15044774 <a title="367-tfidf-3" href="../hunch_net-2010/hunch_net-2010-06-20-2010_ICML_discussion_site.html">401 hunch net-2010-06-20-2010 ICML discussion site</a></p>
<p>Introduction: A substantial difficulty with the 2009 and 2008  ICML discussion system  was a communication vacuum, where authors were not informed of comments, and commenters were not informed of responses to their comments without explicit monitoring.   Mark Reid  has setup a  new discussion system for 2010  with the goal of addressing this.
 
Mark didn’t want to make it to intrusive, so you must opt-in.  As an author,  find your paper  and “Subscribe by email” to the comments.  As a commenter, you have the option of providing an email for follow-up notification.</p><p>4 0.13099357 <a title="367-tfidf-4" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>Introduction: At the  last ICML ,  Tom Dietterich  asked me to look into systems for commenting on papers.  I’ve been slow getting to this, but it’s relevant now.
 
The essential observation is that we now have many tools for online collaboration, but they are not yet much used in academic research.  If we can find the right way to use them, then perhaps great things might happen, with extra kudos to the first conference that manages to really create an online community.  Various conferences have been poking at this.  For example,  UAI has setup a wiki , COLT has  started using   Joomla , with some dynamic content, and AAAI has been setting up a “ student blog “.  Similarly,  Dinoj Surendran  setup a twiki for the  Chicago Machine Learning Summer School , which was quite useful for coordinating events and other things.
 
I believe the most important thing is a willingness to experiment.  A good place to start seems to be enhancing existing conference websites.  For example, the  ICML 2007 papers pag</p><p>5 0.12849604 <a title="367-tfidf-5" href="../hunch_net-2011/hunch_net-2011-04-06-COLT_open_questions.html">429 hunch net-2011-04-06-COLT open questions</a></p>
<p>Introduction: Alina  and  Jake  point out the COLT  Call for Open Questions  due May 11.  In general, this is cool, and worth doing if you can come up with a crisp question.  In my case, I particularly enjoyed  crafting an open question  with precisely a form such that a  critic targeting my papers  would be forced to confront their fallacy or make a case for the reward.  But less esoterically, this is a way to get the attention of some very smart people focused on a problem that really matters, which is the real value.</p><p>6 0.11230032 <a title="367-tfidf-6" href="../hunch_net-2005/hunch_net-2005-02-20-At_One_Month.html">25 hunch net-2005-02-20-At One Month</a></p>
<p>7 0.098745972 <a title="367-tfidf-7" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>8 0.079770789 <a title="367-tfidf-8" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>9 0.072594248 <a title="367-tfidf-9" href="../hunch_net-2007/hunch_net-2007-07-20-Motivation_should_be_the_Responsibility_of_the_Reviewer.html">256 hunch net-2007-07-20-Motivation should be the Responsibility of the Reviewer</a></p>
<p>10 0.070876457 <a title="367-tfidf-10" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>11 0.070191406 <a title="367-tfidf-11" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>12 0.066933125 <a title="367-tfidf-12" href="../hunch_net-2007/hunch_net-2007-11-05-CMU_wins_DARPA_Urban_Challenge.html">271 hunch net-2007-11-05-CMU wins DARPA Urban Challenge</a></p>
<p>13 0.064919755 <a title="367-tfidf-13" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>14 0.063788667 <a title="367-tfidf-14" href="../hunch_net-2009/hunch_net-2009-11-29-AI_Safety.html">380 hunch net-2009-11-29-AI Safety</a></p>
<p>15 0.063651145 <a title="367-tfidf-15" href="../hunch_net-2009/hunch_net-2009-02-22-Effective_Research_Funding.html">344 hunch net-2009-02-22-Effective Research Funding</a></p>
<p>16 0.06213278 <a title="367-tfidf-16" href="../hunch_net-2008/hunch_net-2008-02-10-Complexity_Illness.html">288 hunch net-2008-02-10-Complexity Illness</a></p>
<p>17 0.061435204 <a title="367-tfidf-17" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>18 0.060912095 <a title="367-tfidf-18" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>19 0.057841346 <a title="367-tfidf-19" href="../hunch_net-2005/hunch_net-2005-02-25-Solution%3A_Reinforcement_Learning_with_Classification.html">29 hunch net-2005-02-25-Solution: Reinforcement Learning with Classification</a></p>
<p>20 0.05746635 <a title="367-tfidf-20" href="../hunch_net-2009/hunch_net-2009-06-01-Multitask_Poisoning.html">358 hunch net-2009-06-01-Multitask Poisoning</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.139), (1, -0.024), (2, -0.018), (3, 0.089), (4, -0.037), (5, 0.01), (6, 0.015), (7, -0.021), (8, -0.006), (9, -0.0), (10, -0.063), (11, -0.009), (12, -0.056), (13, 0.075), (14, 0.02), (15, 0.018), (16, -0.102), (17, -0.072), (18, 0.013), (19, 0.144), (20, -0.113), (21, 0.033), (22, -0.065), (23, -0.028), (24, 0.01), (25, -0.007), (26, 0.07), (27, 0.038), (28, 0.053), (29, -0.038), (30, 0.028), (31, -0.007), (32, -0.062), (33, 0.025), (34, 0.046), (35, 0.04), (36, 0.073), (37, -0.095), (38, 0.011), (39, -0.046), (40, 0.039), (41, -0.029), (42, 0.037), (43, -0.0), (44, -0.007), (45, -0.161), (46, -0.057), (47, 0.025), (48, -0.095), (49, -0.041)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9557727 <a title="367-lsi-1" href="../hunch_net-2009/hunch_net-2009-08-16-Centmail_comments.html">367 hunch net-2009-08-16-Centmail comments</a></p>
<p>Introduction: Centmail  is a scheme which makes charity donations have a secondary value, as a stamp for email.  When discussed on  newscientist ,  slashdot , and others, some of the comments make the academic review process appear thoughtful   .  Some prominent fallacies are:
  
 Costing money fallacy.  Some commenters appear to believe the system charges money per email.  Instead, the basic idea is that users get an extra benefit from donations to a charity and participation is strictly voluntary.  The solution to this fallacy is simply reading  the details . 
 Single solution fallacy.  Some commenters seem to think this is proposed as a complete solution to spam, and since not everyone will opt to participate, it won’t work.  But a complete solution is not at all necessary or even possible given the  flag-day problem .  Deployed machine learning systems for fighting spam are great at taking advantage of a partial solution.  The solution to this fallacy is learning about machine learning.  In the</p><p>2 0.85925871 <a title="367-lsi-2" href="../hunch_net-2006/hunch_net-2006-12-06-The_Spam_Problem.html">223 hunch net-2006-12-06-The Spam Problem</a></p>
<p>Introduction: The  New York Times  has an article on the  growth of spam .  Interesting facts include: 9/10 of all email is spam, spam source identification is nearly useless due to botnet spam senders, and image based spam (emails which consist of an image only) are on the growth.
 
Estimates of the cost of spam are almost certainly far to low, because they do not account for the cost in time lost by people.
 
The image based spam which is currently penetrating many filters should be catchable with a more sophisticated application of machine learning technology.  For the spam I see, the rendered images come in only a few formats, which would be easy to recognize via a support vector machine (with RBF kernel), neural network, or even nearest-neighbor architecture.  The mechanics of setting this up to run efficiently is the only real challenge.  This is the next step in the spam war.
 
The response to this system is to make the image based spam even more random.  We should (essentially) expect to see</p><p>3 0.6857667 <a title="367-lsi-3" href="../hunch_net-2005/hunch_net-2005-02-20-At_One_Month.html">25 hunch net-2005-02-20-At One Month</a></p>
<p>Introduction: This is near the one month point, so it seems appropriate to consider meta-issues for the moment.
 
The number of posts is a bit over 20. 
The number of people speaking up in discussions is about 10. 
The number of people viewing the site is somewhat more than 100.
 
I am (naturally) dissatisfied with many things.
  
 Many of the  potential uses  haven’t been realized.  This is partly a matter of opportunity (no conferences in the last month), partly a matter of will (no open problems because it’s hard to give them up), and partly a matter of tradition.  In academia, there is a strong tradition of trying to get everything perfectly right before presentation.  This is somewhat contradictory to the nature of making many posts, and it’s definitely contradictory to the idea of doing “public research”.  If that sort of idea is to pay off, it must be significantly more succesful than previous methods. In an effort to continue experimenting, I’m going to use the next week as “open problems we</p><p>4 0.582461 <a title="367-lsi-4" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>Introduction: At the  last ICML ,  Tom Dietterich  asked me to look into systems for commenting on papers.  I’ve been slow getting to this, but it’s relevant now.
 
The essential observation is that we now have many tools for online collaboration, but they are not yet much used in academic research.  If we can find the right way to use them, then perhaps great things might happen, with extra kudos to the first conference that manages to really create an online community.  Various conferences have been poking at this.  For example,  UAI has setup a wiki , COLT has  started using   Joomla , with some dynamic content, and AAAI has been setting up a “ student blog “.  Similarly,  Dinoj Surendran  setup a twiki for the  Chicago Machine Learning Summer School , which was quite useful for coordinating events and other things.
 
I believe the most important thing is a willingness to experiment.  A good place to start seems to be enhancing existing conference websites.  For example, the  ICML 2007 papers pag</p><p>5 0.51519281 <a title="367-lsi-5" href="../hunch_net-2006/hunch_net-2006-07-12-Who_is_having_visa_problems_reaching_US_conferences%3F.html">195 hunch net-2006-07-12-Who is having visa problems reaching US conferences?</a></p>
<p>Introduction: Many of the large machine learning conferences were in the US this summer.  A common problem which students from abroad encounter is visa issues. 
Just getting a visa to visit can be pretty rough: you stand around in lines, sometimes for days.  Even worse is the timing with respect to ticket buying.  Airplane tickets typically need to be bought well in advance on nonrefundable terms to secure a reasonable rate for air travel.  When a visa is denied, as happens reasonably often, a very expensive ticket is burnt.
 
A serious effort is under way to raise this as in issue in need of fixing.  Over the long term, effectively driving research conferences to locate outside of the US seems an unwise policy.   Robert Schapire  is planning to talk to a congressman.   Sally Goldman  suggested putting together a list of problem cases, and  Phil Long  setup an email address  immigration.and.confs@gmail.com  to collect them.
 
If you (or someone you know) has had insurmountable difficulties reaching</p><p>6 0.51249993 <a title="367-lsi-6" href="../hunch_net-2010/hunch_net-2010-06-20-2010_ICML_discussion_site.html">401 hunch net-2010-06-20-2010 ICML discussion site</a></p>
<p>7 0.50573999 <a title="367-lsi-7" href="../hunch_net-2007/hunch_net-2007-11-05-CMU_wins_DARPA_Urban_Challenge.html">271 hunch net-2007-11-05-CMU wins DARPA Urban Challenge</a></p>
<p>8 0.47494218 <a title="367-lsi-8" href="../hunch_net-2009/hunch_net-2009-06-01-Multitask_Poisoning.html">358 hunch net-2009-06-01-Multitask Poisoning</a></p>
<p>9 0.46693298 <a title="367-lsi-9" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>10 0.46316373 <a title="367-lsi-10" href="../hunch_net-2007/hunch_net-2007-07-20-Motivation_should_be_the_Responsibility_of_the_Reviewer.html">256 hunch net-2007-07-20-Motivation should be the Responsibility of the Reviewer</a></p>
<p>11 0.46174762 <a title="367-lsi-11" href="../hunch_net-2006/hunch_net-2006-01-25-1_year.html">151 hunch net-2006-01-25-1 year</a></p>
<p>12 0.44746065 <a title="367-lsi-12" href="../hunch_net-2005/hunch_net-2005-05-11-Visa_Casualties.html">69 hunch net-2005-05-11-Visa Casualties</a></p>
<p>13 0.42630351 <a title="367-lsi-13" href="../hunch_net-2010/hunch_net-2010-05-20-Google_Predict.html">399 hunch net-2010-05-20-Google Predict</a></p>
<p>14 0.42427826 <a title="367-lsi-14" href="../hunch_net-2009/hunch_net-2009-05-17-Server_Update.html">354 hunch net-2009-05-17-Server Update</a></p>
<p>15 0.41897911 <a title="367-lsi-15" href="../hunch_net-2005/hunch_net-2005-12-22-Yes_%2C_I_am_applying.html">142 hunch net-2005-12-22-Yes , I am applying</a></p>
<p>16 0.4144201 <a title="367-lsi-16" href="../hunch_net-2012/hunch_net-2012-03-06-COLT-ICML_Open_Questions_and_ICML_Instructions.html">458 hunch net-2012-03-06-COLT-ICML Open Questions and ICML Instructions</a></p>
<p>17 0.40908986 <a title="367-lsi-17" href="../hunch_net-2005/hunch_net-2005-06-17-Reopening_RL-%3EClassification.html">82 hunch net-2005-06-17-Reopening RL->Classification</a></p>
<p>18 0.4061437 <a title="367-lsi-18" href="../hunch_net-2005/hunch_net-2005-10-13-Site_tweak.html">122 hunch net-2005-10-13-Site tweak</a></p>
<p>19 0.40494299 <a title="367-lsi-19" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<p>20 0.4026956 <a title="367-lsi-20" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(3, 0.025), (27, 0.158), (38, 0.02), (53, 0.602), (55, 0.053), (94, 0.04)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.99724162 <a title="367-lda-1" href="../hunch_net-2005/hunch_net-2005-09-05-Site_Update.html">107 hunch net-2005-09-05-Site Update</a></p>
<p>Introduction: I tweaked the site in a number of ways today, including:
  
 Updating to  WordPress  1.5. 
 Installing and heavily tweaking the  Geekniche  theme.  Update: I switched back to a tweaked version of the old theme. 
 Adding the  Customizable Post Listings  plugin. 
 Installing the  StatTraq  plugin. 
 Updating some of the links.  I particularly recommend looking at the  computer research policy  blog. 
 Adding  threaded comments .  This doesn’t thread old comments obviously, but the extra structure may be helpful for new ones. 
  
Overall, I think this is an improvement, and it addresses a few of my  earlier problems .  If you have any difficulties or anything seems “not quite right”, please speak up.  A few other tweaks to the site may happen in the near future.</p><p>2 0.99290788 <a title="367-lda-2" href="../hunch_net-2005/hunch_net-2005-02-09-Intuitions_from_applied_learning.html">16 hunch net-2005-02-09-Intuitions from applied learning</a></p>
<p>Introduction: Since learning is far from an exact science, it’s good to pay attention to basic intuitions of applied learning.  Here are a few I’ve collected.
  
  Integration   In Bayesian learning, the posterior is computed by an integral, and the optimal thing to do is to predict according to this integral.  This phenomena seems to be far more general.  Bagging, Boosting, SVMs, and Neural Networks all take advantage of this idea to some extent.  The phenomena is more general: you can average over many different  classification predictors  to improve performance.  Sources:  Zoubin ,  Caruana  
  Differentiation  Different pieces of an average should differentiate to achieve good performance by different methods.  This is know as the ‘symmetry breaking’ problem for neural networks, and it’s why weights are initialized randomly.   Boosting explicitly attempts to achieve good differentiation by creating new, different, learning problems.  Sources:  Yann LeCun ,  Phil Long  
  Deep Representation   Ha</p><p>3 0.99069476 <a title="367-lda-3" href="../hunch_net-2005/hunch_net-2005-04-14-Families_of_Learning_Theory_Statements.html">56 hunch net-2005-04-14-Families of Learning Theory Statements</a></p>
<p>Introduction: The diagram above shows a very broad viewpoint of learning theory. 
  
 
 arrow 
 Typical statement 
 Examples 
 
 
 Past->Past 
  Some prediction algorithm  A  does almost as well as any of a set of algorithms. 
 Weighted Majority 
 
 
 Past->Future 
 Assuming independent samples, past performance predicts future performance. 
 PAC analysis, ERM analysis 
 
 
 Future->Future 
 Future prediction performance on subproblems implies future prediction performance using algorithm  A . 
 ECOC, Probing 
 
  
A basic question is: Are there other varieties of statements of this type?   Avrim  noted that there are also “arrows between arrows”: generic methods for transforming between Past->Past statements and Past->Future statements.  Are there others?</p><p>4 0.97621006 <a title="367-lda-4" href="../hunch_net-2005/hunch_net-2005-07-10-Thinking_the_Unthought.html">91 hunch net-2005-07-10-Thinking the Unthought</a></p>
<p>Introduction: One thing common to much research is that the researcher must be the first person  ever  to have some thought.  How do you think of something that has never been thought of?  There seems to be no methodical manner of doing this, but there are some tricks.
  
 The easiest method is to just have some connection come to you.  There is a trick here however: you should write it down and fill out the idea immediately because  it can just as easily go away. 
 A harder method is to set aside a block of time and simply think about an idea.  Distraction elimination is essential here because thinking about the unthought is hard work which your mind will avoid. 
 Another common method is in conversation.  Sometimes the process of verbalizing implies new ideas come up and sometimes whoever you are talking to replies just the right way.  This method is dangerous though—you must speak to someone who helps you think rather than someone who occupies your thoughts. 
 Try to rephrase the problem so the a</p><p>5 0.97228611 <a title="367-lda-5" href="../hunch_net-2005/hunch_net-2005-01-24-Holy_grails_of_machine_learning%3F.html">2 hunch net-2005-01-24-Holy grails of machine learning?</a></p>
<p>Introduction: Let me kick things off by posing this question to ML researchers:
 
   What do you think are some important holy grails of machine learning?
 
For example: 
 – “A classifier with SVM-level performance but much more scalable” 
 – “Practical confidence bounds (or learning bounds) for classification” 
 – “A reinforcement learning algorithm that can handle the ___ problem” 
 – “Understanding theoretically why ___ works so well in practice” 
etc.
 
I pose this question because I believe that when goals are stated explicitly and well (thus providing clarity as well as opening up the problems to more people), rather than left implicit, they are likely to be achieved much more quickly.  I would also like to know more about the internal goals of the various machine learning sub-areas (theory, kernel methods, graphical models, reinforcement learning, etc) as stated by people in these respective areas.  This could help people cross sub-areas.</p><p>same-blog 6 0.96186495 <a title="367-lda-6" href="../hunch_net-2009/hunch_net-2009-08-16-Centmail_comments.html">367 hunch net-2009-08-16-Centmail comments</a></p>
<p>7 0.93735874 <a title="367-lda-7" href="../hunch_net-2005/hunch_net-2005-12-29-Deadline_Season.html">145 hunch net-2005-12-29-Deadline Season</a></p>
<p>8 0.91651988 <a title="367-lda-8" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>9 0.78932065 <a title="367-lda-9" href="../hunch_net-2005/hunch_net-2005-02-17-Learning_Research_Programs.html">21 hunch net-2005-02-17-Learning Research Programs</a></p>
<p>10 0.68324393 <a title="367-lda-10" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>11 0.67540526 <a title="367-lda-11" href="../hunch_net-2006/hunch_net-2006-01-25-1_year.html">151 hunch net-2006-01-25-1 year</a></p>
<p>12 0.67215329 <a title="367-lda-12" href="../hunch_net-2006/hunch_net-2006-07-08-MaxEnt_contradicts_Bayes_Rule%3F.html">191 hunch net-2006-07-08-MaxEnt contradicts Bayes Rule?</a></p>
<p>13 0.66848975 <a title="367-lda-13" href="../hunch_net-2005/hunch_net-2005-04-23-Advantages_and_Disadvantages_of_Bayesian_Learning.html">60 hunch net-2005-04-23-Advantages and Disadvantages of Bayesian Learning</a></p>
<p>14 0.66413641 <a title="367-lda-14" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>15 0.6328941 <a title="367-lda-15" href="../hunch_net-2006/hunch_net-2006-01-30-Should_the_Input_Representation_be_a_Vector%3F.html">152 hunch net-2006-01-30-Should the Input Representation be a Vector?</a></p>
<p>16 0.63027978 <a title="367-lda-16" href="../hunch_net-2007/hunch_net-2007-10-14-NIPS_workshp%3A_Learning_Problem_Design.html">265 hunch net-2007-10-14-NIPS workshp: Learning Problem Design</a></p>
<p>17 0.61398602 <a title="367-lda-17" href="../hunch_net-2010/hunch_net-2010-08-23-Boosted_Decision_Trees_for_Deep_Learning.html">407 hunch net-2010-08-23-Boosted Decision Trees for Deep Learning</a></p>
<p>18 0.60754025 <a title="367-lda-18" href="../hunch_net-2010/hunch_net-2010-10-29-To_Vidoelecture_or_not.html">416 hunch net-2010-10-29-To Vidoelecture or not</a></p>
<p>19 0.60481536 <a title="367-lda-19" href="../hunch_net-2005/hunch_net-2005-02-20-At_One_Month.html">25 hunch net-2005-02-20-At One Month</a></p>
<p>20 0.59759063 <a title="367-lda-20" href="../hunch_net-2011/hunch_net-2011-07-11-Interesting_Neural_Network_Papers_at_ICML_2011.html">438 hunch net-2011-07-11-Interesting Neural Network Papers at ICML 2011</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
