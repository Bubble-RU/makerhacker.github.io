<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>345 hunch net-2009-03-08-Prediction Science</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2009" href="../home/hunch_net-2009_home.html">hunch_net-2009</a> <a title="hunch_net-2009-345" href="#">hunch_net-2009-345</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>345 hunch net-2009-03-08-Prediction Science</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2009-345-html" href="http://hunch.net/?p=612">html</a></p><p>Introduction: One view of machine learning is that it’s about how to program computers to predict well.  This suggests a broader research program centered around the more pervasive goal of simply predicting well. 
There are many distinct strands of this broader research program which are only partially unified.  Here are the ones that I know of:
  
  Learning Theory .  Learning theory focuses on several topics related to the dynamics and process of prediction.  Convergence bounds like the  VC bound   give an intellectual foundation to many learning algorithms.  Online learning algorithms like  Weighted Majority  provide an alternate purely game theoretic foundation for learning.   Boosting algorithms  yield algorithms for purifying prediction abiliity.   Reduction algorithms  provide means for changing esoteric problems into well known ones. 
  Machine Learning .  A great deal of experience has accumulated in practical algorithm design from a mixture of paradigms, including bayesian, biological, opt</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Online learning algorithms like  Weighted Majority  provide an alternate purely game theoretic foundation for learning. [sent-7, score-0.401]
</p><p>2 The core focus in game theory is on equilibria, mostly typically  Nash equilibria , but also many other kinds of equilibria. [sent-13, score-0.397]
</p><p>3 When this is employed well, principally in mechanism design for auctions, it can be a very powerful concept. [sent-15, score-0.436]
</p><p>4 The basic idea in a prediction market is that commodities can be designed so that their buy/sell price reflects a form of wealth-weighted consensus estimate of the probability of some event. [sent-17, score-0.382]
</p><p>5 This is not simply mechanism design, because (a) the thin market problem must be dealt with and (b) the structure of plausible guarantees is limited. [sent-18, score-0.326]
</p><p>6 I have yet to find an example of robust search which isn’t useful—and there are several varieties. [sent-24, score-0.559]
</p><p>7 This includes active learning,  robust min finding ,  and (more generally)  compressed sensing  and  error correcting codes . [sent-25, score-0.39]
</p><p>8 The concept of mechanism design is mostly missing from learning theory, but it is sure to be essential when interactive agents are learning. [sent-28, score-0.723]
</p><p>9 We’ve found several applications for robust search as well as new settings for robust search such as  active learning , and  error correcting tournaments , but there are surely others. [sent-29, score-1.346]
</p><p>10 There is a strong relationship between incentive compatibility and choice of loss functions, both for choosing proxy losses and approximating the real loss function imposed by the world. [sent-32, score-0.341]
</p><p>11 It’s easy to imagine designer loss functions from the study of incentive compatibility mechanisms giving learning algorithm an edge. [sent-33, score-0.54]
</p><p>12 Since machine learning and information markets share a design goal, are there hybrid approaches which can outperform either? [sent-35, score-0.893]
</p><p>13 For example there are papers about  learning on permutations  and  pricing in combinatorial markets . [sent-38, score-0.449]
</p><p>14 In general, the idea of using mechanism design with context information (as is done in machine learning), could also be extremely powerful. [sent-40, score-0.771]
</p><p>15 Prediction markets are partly an empirical field and partly a mechanism design field. [sent-42, score-0.894]
</p><p>16 There seems to be relatively little understanding about how well and how exactly information from multiple agents is supposed to interact to derive a good probability estimate. [sent-43, score-0.347]
</p><p>17 Can an information market be designed with the guarantee that an imperfect but best player decides the vote after not-too-many rounds? [sent-46, score-0.376]
</p><p>18 Investigations into robust search are extremely diverse, essentially only unified in a mathematically based analysis. [sent-49, score-0.626]
</p><p>19 For people interested in robust search, machine learning and information markets provide a fertile ground for empirical application and new settings. [sent-50, score-1.117]
</p><p>20 Can all mechanisms for robust search be done with context information, as is common in learning? [sent-51, score-0.688]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('markets', 0.32), ('robust', 0.306), ('search', 0.253), ('design', 0.236), ('mechanism', 0.2), ('equilibria', 0.165), ('agents', 0.148), ('fertile', 0.133), ('market', 0.126), ('information', 0.122), ('prediction', 0.11), ('foundation', 0.11), ('compatibility', 0.11), ('branches', 0.099), ('focuses', 0.099), ('statistics', 0.095), ('wealth', 0.095), ('broader', 0.092), ('theory', 0.089), ('incentive', 0.089), ('machine', 0.086), ('correcting', 0.084), ('provide', 0.08), ('probability', 0.077), ('majority', 0.075), ('game', 0.074), ('surely', 0.074), ('study', 0.072), ('loss', 0.071), ('learning', 0.07), ('topics', 0.07), ('mechanisms', 0.069), ('designed', 0.069), ('weighted', 0.069), ('partly', 0.069), ('mostly', 0.069), ('program', 0.069), ('algorithms', 0.067), ('extremely', 0.067), ('predictive', 0.067), ('research', 0.065), ('suggests', 0.062), ('context', 0.06), ('varieties', 0.059), ('imperfect', 0.059), ('territory', 0.059), ('hybrid', 0.059), ('pricing', 0.059), ('investigations', 0.059), ('designer', 0.059)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="345-tfidf-1" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>Introduction: One view of machine learning is that it’s about how to program computers to predict well.  This suggests a broader research program centered around the more pervasive goal of simply predicting well. 
There are many distinct strands of this broader research program which are only partially unified.  Here are the ones that I know of:
  
  Learning Theory .  Learning theory focuses on several topics related to the dynamics and process of prediction.  Convergence bounds like the  VC bound   give an intellectual foundation to many learning algorithms.  Online learning algorithms like  Weighted Majority  provide an alternate purely game theoretic foundation for learning.   Boosting algorithms  yield algorithms for purifying prediction abiliity.   Reduction algorithms  provide means for changing esoteric problems into well known ones. 
  Machine Learning .  A great deal of experience has accumulated in practical algorithm design from a mixture of paradigms, including bayesian, biological, opt</p><p>2 0.28912106 <a title="345-tfidf-2" href="../hunch_net-2006/hunch_net-2006-10-13-David_Pennock_starts_Oddhead.html">214 hunch net-2006-10-13-David Pennock starts Oddhead</a></p>
<p>Introduction: his blog on information markets and other research topics .</p><p>3 0.17137507 <a title="345-tfidf-3" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>Introduction: “Search” is the other branch of AI research which has been succesful.   Concrete examples include  Deep Blue  which beat the world chess champion and  Chinook  the champion checkers program.  A set of core search techniques exist including A * , alpha-beta pruning, and others that can be applied to any of many different search problems.
 
Given this, it may be surprising to learn that there has been relatively little succesful work on combining prediction and search.  Given also that  humans typically solve search problems using a number of predictive heuristics to narrow in on a solution, we might be surprised again.  However, the big successful search-based systems have typically not used “smart” search algorithms.  Insteady they have optimized for very fast search.  This is not for lack of trying… many people have tried to synthesize search and prediction to various degrees of success.   For example,  Knightcap  achieves good-but-not-stellar chess playing performance, and  TD-gammon</p><p>4 0.15072063 <a title="345-tfidf-4" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>Introduction: I’ve had serious conversations with several people who believe that the theory in machine learning is “only useful for getting papers published”.  That’s a compelling statement, as I’ve seen many papers where the algorithm clearly came first, and the theoretical justification for it came second, purely as a perceived means to improve the chance of publication. 
 
Naturally, I disagree and believe that learning theory has much more substantial applications.  
 
Even in core learning algorithm design, I’ve found learning theory to be useful, although it’s application is more subtle than many realize.  The most straightforward applications can fail, because (as expectation suggests) worst case bounds tend to be loose in practice (*).  In my experience, considering learning theory when designing an algorithm has two important effects in practice:
  
 It can help make your algorithm behave right at a crude level of analysis, leaving finer details to tuning or common sense.  The best example</p><p>5 0.14629894 <a title="345-tfidf-5" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>Introduction: Normally, I don’t indulge in posters for  ICML , but this year is naturally an exception for me.   If you want one, there are a small number  left here , if you sign up before February.
 
It also seems worthwhile to give some sense of the scope and reviewing criteria for ICML for authors considering submitting papers.  At ICML, the (very large) program committee does the reviewing which informs final decisions by area chairs on most papers.  Program chairs setup the process, deal with exceptions or disagreements, and provide advice for the reviewing process.  Providing advice is tricky (and easily misleading) because a conference is a community, and in the end the aggregate interests of the community determine the conference.  Nevertheless, as a program chair this year it seems worthwhile to state the overall philosophy I have and what I plan to encourage (and occasionally discourage).
 
At the highest level, I believe ICML exists to further research into machine learning, which I gene</p><p>6 0.13841213 <a title="345-tfidf-6" href="../hunch_net-2010/hunch_net-2010-05-02-What%26%238217%3Bs_the_difference_between_gambling_and_rewarding_good_prediction%3F.html">397 hunch net-2010-05-02-What&#8217;s the difference between gambling and rewarding good prediction?</a></p>
<p>7 0.13648254 <a title="345-tfidf-7" href="../hunch_net-2005/hunch_net-2005-03-29-Academic_Mechanism_Design.html">48 hunch net-2005-03-29-Academic Mechanism Design</a></p>
<p>8 0.13264336 <a title="345-tfidf-8" href="../hunch_net-2009/hunch_net-2009-10-26-NIPS_workshops.html">375 hunch net-2009-10-26-NIPS workshops</a></p>
<p>9 0.13103493 <a title="345-tfidf-9" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>10 0.13039866 <a title="345-tfidf-10" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>11 0.12764443 <a title="345-tfidf-11" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<p>12 0.12363747 <a title="345-tfidf-12" href="../hunch_net-2010/hunch_net-2010-12-26-NIPS_2010.html">420 hunch net-2010-12-26-NIPS 2010</a></p>
<p>13 0.12353871 <a title="345-tfidf-13" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>14 0.11905227 <a title="345-tfidf-14" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>15 0.11597265 <a title="345-tfidf-15" href="../hunch_net-2006/hunch_net-2006-02-11-Yahoo%26%238217%3Bs_Learning_Problems..html">156 hunch net-2006-02-11-Yahoo&#8217;s Learning Problems.</a></p>
<p>16 0.11257093 <a title="345-tfidf-16" href="../hunch_net-2005/hunch_net-2005-09-14-The_Predictionist_Viewpoint.html">112 hunch net-2005-09-14-The Predictionist Viewpoint</a></p>
<p>17 0.11182039 <a title="345-tfidf-17" href="../hunch_net-2005/hunch_net-2005-02-01-Watchword%3A_Loss.html">9 hunch net-2005-02-01-Watchword: Loss</a></p>
<p>18 0.11171436 <a title="345-tfidf-18" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>19 0.11046513 <a title="345-tfidf-19" href="../hunch_net-2007/hunch_net-2007-09-30-NIPS_workshops_are_out..html">264 hunch net-2007-09-30-NIPS workshops are out.</a></p>
<p>20 0.10702597 <a title="345-tfidf-20" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.299), (1, 0.074), (2, -0.06), (3, 0.012), (4, -0.062), (5, 0.002), (6, 0.012), (7, -0.015), (8, 0.039), (9, 0.022), (10, 0.067), (11, 0.051), (12, -0.014), (13, 0.052), (14, -0.077), (15, 0.067), (16, 0.033), (17, 0.001), (18, 0.054), (19, -0.055), (20, 0.125), (21, -0.075), (22, 0.038), (23, 0.08), (24, -0.059), (25, 0.019), (26, 0.143), (27, 0.042), (28, 0.054), (29, -0.048), (30, -0.082), (31, 0.062), (32, 0.034), (33, -0.144), (34, -0.083), (35, -0.069), (36, -0.147), (37, -0.07), (38, 0.073), (39, 0.022), (40, -0.002), (41, 0.064), (42, -0.071), (43, -0.051), (44, -0.021), (45, -0.026), (46, -0.054), (47, 0.023), (48, 0.051), (49, 0.107)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.93952394 <a title="345-lsi-1" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>Introduction: One view of machine learning is that it’s about how to program computers to predict well.  This suggests a broader research program centered around the more pervasive goal of simply predicting well. 
There are many distinct strands of this broader research program which are only partially unified.  Here are the ones that I know of:
  
  Learning Theory .  Learning theory focuses on several topics related to the dynamics and process of prediction.  Convergence bounds like the  VC bound   give an intellectual foundation to many learning algorithms.  Online learning algorithms like  Weighted Majority  provide an alternate purely game theoretic foundation for learning.   Boosting algorithms  yield algorithms for purifying prediction abiliity.   Reduction algorithms  provide means for changing esoteric problems into well known ones. 
  Machine Learning .  A great deal of experience has accumulated in practical algorithm design from a mixture of paradigms, including bayesian, biological, opt</p><p>2 0.67901498 <a title="345-lsi-2" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>Introduction: “Search” is the other branch of AI research which has been succesful.   Concrete examples include  Deep Blue  which beat the world chess champion and  Chinook  the champion checkers program.  A set of core search techniques exist including A * , alpha-beta pruning, and others that can be applied to any of many different search problems.
 
Given this, it may be surprising to learn that there has been relatively little succesful work on combining prediction and search.  Given also that  humans typically solve search problems using a number of predictive heuristics to narrow in on a solution, we might be surprised again.  However, the big successful search-based systems have typically not used “smart” search algorithms.  Insteady they have optimized for very fast search.  This is not for lack of trying… many people have tried to synthesize search and prediction to various degrees of success.   For example,  Knightcap  achieves good-but-not-stellar chess playing performance, and  TD-gammon</p><p>3 0.63415349 <a title="345-lsi-3" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<p>Introduction: I want to comment on the “Bing copies Google” discussion  here ,  here , and  here , because there are data-related issues which the general public may not understand, and some of the framing seems substantially misleading to me.
 
As a not-distant-outsider, let me mention the sources of bias I may have.  I work at  Yahoo! , which has started using  Bing .  This might predispose me towards Bing, but on the other hand I’m still at Yahoo!, and have been using  Linux  exclusively as an OS for many years, including even a couple minor kernel patches.  And,  on the gripping hand , I’ve spent quite a bit of time thinking about the basic  principles of incorporating user feedback in machine learning .  Also note, this post is not  related to official Yahoo! policy, it’s just my personal view.
 
 The issue  Google engineers inserted synthetic responses to synthetic queries on google.com, then executed the synthetic searches on google.com using Internet Explorer with the Bing toolbar and later</p><p>4 0.61634386 <a title="345-lsi-4" href="../hunch_net-2010/hunch_net-2010-09-21-Regretting_the_dead.html">411 hunch net-2010-09-21-Regretting the dead</a></p>
<p>Introduction: Nikos  pointed out this  new york times  article about  poor clinical design killing people .  For those of us who study learning from exploration information this is a reminder that low regret algorithms are particularly important, as regret in clinical trials is measured by patient deaths.
 
Two obvious improvements on the experimental design are:
  
 With reasonable record keeping of existing outcomes for the standard treatments, there is no need to explicitly assign people to a control group with the standard treatment, as that approach is effectively explored with great certainty.  Asserting otherwise would imply that the nature of effective treatments for cancer has changed between now and a year ago, which denies the value of any clinical trial. 
 An optimal experimental design will smoothly phase between exploration and exploitation as evidence for a new treatment shows that it can be effective.  This is old tech, for example in the  EXP3.P algorithm (page 12 aka 59)  although</p><p>5 0.61036265 <a title="345-lsi-5" href="../hunch_net-2006/hunch_net-2006-05-16-The_value_of_the_orthodox_view_of_Boosting.html">179 hunch net-2006-05-16-The value of the orthodox view of Boosting</a></p>
<p>Introduction: The term “boosting” comes from the idea of using a meta-algorithm which takes “weak” learners (that may be able to only barely predict slightly better than random) and turn them into strongly capable learners (which predict very well).    Adaboost  in 1995 was the first widely used (and useful) boosting algorithm, although there were theoretical boosting algorithms floating around since 1990 (see the bottom of  this page ).
 
Since then, many different interpretations of why boosting works have arisen.  There is significant discussion about these different views in the  annals of statistics , including a  response  by  Yoav Freund  and  Robert Schapire .
 
I believe there is a great deal of value to be found in the original view of boosting (meta-algorithm for creating a strong learner from a weak learner).  This is not a claim that one particular viewpoint obviates the value of all others, but rather that no other viewpoint seems to really capture important properties.
 
Comparing wit</p><p>6 0.59945756 <a title="345-lsi-6" href="../hunch_net-2005/hunch_net-2005-09-14-The_Predictionist_Viewpoint.html">112 hunch net-2005-09-14-The Predictionist Viewpoint</a></p>
<p>7 0.59361541 <a title="345-lsi-7" href="../hunch_net-2008/hunch_net-2008-08-04-Electoralmarkets.com.html">312 hunch net-2008-08-04-Electoralmarkets.com</a></p>
<p>8 0.59305519 <a title="345-lsi-8" href="../hunch_net-2010/hunch_net-2010-05-02-What%26%238217%3Bs_the_difference_between_gambling_and_rewarding_good_prediction%3F.html">397 hunch net-2010-05-02-What&#8217;s the difference between gambling and rewarding good prediction?</a></p>
<p>9 0.58328682 <a title="345-lsi-9" href="../hunch_net-2010/hunch_net-2010-12-26-NIPS_2010.html">420 hunch net-2010-12-26-NIPS 2010</a></p>
<p>10 0.5818069 <a title="345-lsi-10" href="../hunch_net-2006/hunch_net-2006-09-07-Objective_and_subjective_interpretations_of_probability.html">205 hunch net-2006-09-07-Objective and subjective interpretations of probability</a></p>
<p>11 0.56019354 <a title="345-lsi-11" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>12 0.55562979 <a title="345-lsi-12" href="../hunch_net-2006/hunch_net-2006-07-09-The_Stock_Prediction_Machine_Learning_Problem.html">193 hunch net-2006-07-09-The Stock Prediction Machine Learning Problem</a></p>
<p>13 0.54879206 <a title="345-lsi-13" href="../hunch_net-2005/hunch_net-2005-09-04-Science_in_the_Government.html">106 hunch net-2005-09-04-Science in the Government</a></p>
<p>14 0.52786613 <a title="345-lsi-14" href="../hunch_net-2008/hunch_net-2008-03-23-Interactive_Machine_Learning.html">293 hunch net-2008-03-23-Interactive Machine Learning</a></p>
<p>15 0.52464551 <a title="345-lsi-15" href="../hunch_net-2007/hunch_net-2007-04-28-The_Coming_Patent_Apocalypse.html">241 hunch net-2007-04-28-The Coming Patent Apocalypse</a></p>
<p>16 0.52375954 <a title="345-lsi-16" href="../hunch_net-2009/hunch_net-2009-04-02-Asymmophobia.html">348 hunch net-2009-04-02-Asymmophobia</a></p>
<p>17 0.52272242 <a title="345-lsi-17" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>18 0.5170204 <a title="345-lsi-18" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>19 0.51578093 <a title="345-lsi-19" href="../hunch_net-2005/hunch_net-2005-01-19-Why_I_decided_to_run_a_weblog..html">1 hunch net-2005-01-19-Why I decided to run a weblog.</a></p>
<p>20 0.51290393 <a title="345-lsi-20" href="../hunch_net-2006/hunch_net-2006-08-03-AOL%26%238217%3Bs_data_drop.html">200 hunch net-2006-08-03-AOL&#8217;s data drop</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(3, 0.021), (9, 0.024), (27, 0.219), (37, 0.01), (38, 0.058), (51, 0.025), (53, 0.042), (55, 0.076), (64, 0.016), (68, 0.225), (77, 0.019), (83, 0.03), (94, 0.118), (95, 0.043)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.96173674 <a title="345-lda-1" href="../hunch_net-2005/hunch_net-2005-03-29-Academic_Mechanism_Design.html">48 hunch net-2005-03-29-Academic Mechanism Design</a></p>
<p>Introduction: From game theory, there is a notion of “mechanism design”: setting up the structure of the world so that participants have some incentive to do sane things (rather than obviously counterproductive things).  Application of this principle to academic research may be fruitful.
 
What is misdesigned about academic research?
  
 The  JMLG  guides give many hints. 
 The common nature of  bad reviewing  also suggests the system isn’t working optimally. 
 There are many ways to experimentally  “cheat” in machine learning . 
  Funding Prisoner’s Delimma.  Good researchers often write grant proposals for funding rather than doing research.  Since the pool of grant money is finite, this means that grant proposals are often rejected, implying that more must be written.  This is essentially a “prisoner’s delimma”: anyone not writing grant proposals loses, but the entire process of doing research is slowed by distraction.  If everyone wrote 1/2 as many grant proposals, roughly the same distribution</p><p>2 0.93808401 <a title="345-lda-2" href="../hunch_net-2005/hunch_net-2005-09-10-%26%238220%3BFailure%26%238221%3B_is_an_option.html">110 hunch net-2005-09-10-&#8220;Failure&#8221; is an option</a></p>
<p>Introduction: This is about the hard choices that graduate students must make.
 
The cultural definition of success in academic research is to:
  
 Produce good research which many other people appreciate. 
 Produce many students who go on to do the same. 
  
There are fundamental reasons why this is success in the local culture.   Good research appreciated by others means access to jobs.  Many students succesful in the same way implies that there are a number of people who think in a similar way and appreciate your work.
 
In order to graduate, a phd student must live in an academic culture for a period of several years. It is common to adopt the culture’s definition of success during this time.  It’s also common for many phd students discover they are not suited to an academic research lifestyle.  This collision of values and abilities naturally results in depression.
 
The most fundamental advice when this happens is: change something.  Pick a new advisor.  Pick a new research topic.  Or leave th</p><p>3 0.93057102 <a title="345-lda-3" href="../hunch_net-2011/hunch_net-2011-03-27-Vowpal_Wabbit%2C_v5.1.html">428 hunch net-2011-03-27-Vowpal Wabbit, v5.1</a></p>
<p>Introduction: I just created  version 5.1  of  vowpal wabbit .  This almost entirely a bugfix release, so it’s an easy upgrade from v5.0.
 
In addition:
  
 There is now a  mailing list , which I and several other developers are subscribed to. 
 The main website has shifted to the wiki on github.  This means that anyone with a github account can now edit it. 
 I’m planning to give a tutorial tomorrow on it at  eHarmony / the LA machine learning meetup  at 10am.  Drop by if you’re interested. 
  
The status of VW amongst other open source projects has changed.  When VW first came out, it was relatively unique amongst existing projects in terms of features.  At this point, many other projects have started to appreciate the value of the design choices here.  This includes:
  
  Mahout , which now has an SGD implementation. 
  Shogun , where  Soeren  is keen on  incorporating features . 
  LibLinear , where they won the KDD best paper award for  out-of-core learning . 
  
This is expected—any open sourc</p><p>4 0.92941356 <a title="345-lda-4" href="../hunch_net-2006/hunch_net-2006-02-11-Yahoo%26%238217%3Bs_Learning_Problems..html">156 hunch net-2006-02-11-Yahoo&#8217;s Learning Problems.</a></p>
<p>Introduction: I just visited  Yahoo Research  which has several fundamental learning problems near to (or beyond) the set of problems we know how to solve well.  Here are 3 of them.
  
  Ranking   This is the canonical problem of all search engines.  It is made extra difficult for several reasons.
 
 There is relatively little “good” supervised learning data and a great deal of data with some signal (such as click through rates). 
 The learning must occur in a partially adversarial environment. Many people very actively attempt to place themselves at the top of 
rankings. 
 It is not even quite clear whether the problem should be posed as ‘ranking’ or as ‘regression’ which is then used to produce a 
ranking. 
 
 
  Collaborative filtering  Yahoo has a large number of recommendation systems for music, movies, etc…  In these sorts of systems, users specify how they liked a set of things, and then the system can (hopefully) find some more examples of things they might like 
by reasoning across multiple</p><p>same-blog 5 0.87783134 <a title="345-lda-5" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>Introduction: One view of machine learning is that it’s about how to program computers to predict well.  This suggests a broader research program centered around the more pervasive goal of simply predicting well. 
There are many distinct strands of this broader research program which are only partially unified.  Here are the ones that I know of:
  
  Learning Theory .  Learning theory focuses on several topics related to the dynamics and process of prediction.  Convergence bounds like the  VC bound   give an intellectual foundation to many learning algorithms.  Online learning algorithms like  Weighted Majority  provide an alternate purely game theoretic foundation for learning.   Boosting algorithms  yield algorithms for purifying prediction abiliity.   Reduction algorithms  provide means for changing esoteric problems into well known ones. 
  Machine Learning .  A great deal of experience has accumulated in practical algorithm design from a mixture of paradigms, including bayesian, biological, opt</p><p>6 0.76206553 <a title="345-lda-6" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>7 0.75909972 <a title="345-lda-7" href="../hunch_net-2005/hunch_net-2005-05-02-Reviewing_techniques_for_conferences.html">65 hunch net-2005-05-02-Reviewing techniques for conferences</a></p>
<p>8 0.75643229 <a title="345-lda-8" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>9 0.75559127 <a title="345-lda-9" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>10 0.753474 <a title="345-lda-10" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>11 0.75086802 <a title="345-lda-11" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>12 0.7504546 <a title="345-lda-12" href="../hunch_net-2007/hunch_net-2007-01-02-Retrospective.html">225 hunch net-2007-01-02-Retrospective</a></p>
<p>13 0.74954748 <a title="345-lda-13" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>14 0.74852073 <a title="345-lda-14" href="../hunch_net-2009/hunch_net-2009-06-01-Multitask_Poisoning.html">358 hunch net-2009-06-01-Multitask Poisoning</a></p>
<p>15 0.74752587 <a title="345-lda-15" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>16 0.74660254 <a title="345-lda-16" href="../hunch_net-2009/hunch_net-2009-06-03-Functionally_defined_Nonlinear_Dynamic_Models.html">359 hunch net-2009-06-03-Functionally defined Nonlinear Dynamic Models</a></p>
<p>17 0.74477226 <a title="345-lda-17" href="../hunch_net-2011/hunch_net-2011-11-26-Giving_Thanks.html">449 hunch net-2011-11-26-Giving Thanks</a></p>
<p>18 0.74407643 <a title="345-lda-18" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>19 0.74341446 <a title="345-lda-19" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>20 0.74319696 <a title="345-lda-20" href="../hunch_net-2009/hunch_net-2009-09-21-Netflix_finishes_%28and_starts%29.html">371 hunch net-2009-09-21-Netflix finishes (and starts)</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
