<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>352 hunch net-2009-05-06-Machine Learning to AI</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2009" href="../home/hunch_net-2009_home.html">hunch_net-2009</a> <a title="hunch_net-2009-352" href="#">hunch_net-2009-352</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>352 hunch net-2009-05-06-Machine Learning to AI</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2009-352-html" href="http://hunch.net/?p=703">html</a></p><p>Introduction: I recently had fun discussions with both  Vikash Mansinghka  and  Thomas Breuel  about approaching AI with machine learning.  The general interest in taking a crack at AI with machine learning seems to be rising on many fronts including  DARPA .
 
As a matter of history, there was a great deal of interest in AI which died down before I began research.  There remain many projects and conferences spawned in this earlier AI wave, as well as a good bit of experience about what did not work, or at least did not work yet.  Here are a few examples of failure modes that people seem to run into:
  
  Supply/Product confusion .  Sometimes we think “Intelligences use X, so I’ll create X and have an Intelligence.”  An example of this is the  Cyc Project  which inspires some people as “intelligences use ontologies, so I’ll create an ontology and a system using it to have an Intelligence.”  The flaw here is that Intelligences  create  ontologies, which they use, and without the ability to create ont</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 As a matter of history, there was a great deal of interest in AI which died down before I began research. [sent-3, score-0.128]
</p><p>2 Sometimes we think “Intelligences use X, so I’ll create X and have an Intelligence. [sent-6, score-0.135]
</p><p>3 ”  An example of this is the  Cyc Project  which inspires some people as “intelligences use ontologies, so I’ll create an ontology and a system using it to have an Intelligence. [sent-7, score-0.198]
</p><p>4 ”  The flaw here is that Intelligences  create  ontologies, which they use, and without the ability to create ontologies you don’t have an Intelligence. [sent-8, score-0.605]
</p><p>5 If we are unlucky, it fails to even be partially useful, because the format is unnatural for the internal representations of an Intelligence. [sent-10, score-0.176]
</p><p>6 If you asked the people working on them, they might agree that uncertainty was an important but secondary concern to be solved after the main problem. [sent-13, score-0.557]
</p><p>7 Unfortunately, it seems that uncertainty is a primary concern in practice. [sent-14, score-0.418]
</p><p>8 One example of this is  blocks world  where a system for planning how to rearrange blocks on a table might easily fail in practice because the robot fails to grab a block properly. [sent-15, score-0.612]
</p><p>9 Many people think of uncertainty as a second order concern, because they don’t experience uncertainty in their daily lives. [sent-16, score-0.847]
</p><p>10 I believe this is incorrect—a mental illusion due to the effect that focusing attention on a specific subject implies reducing uncertainty on that subject. [sent-17, score-0.312]
</p><p>11 More generally, because any Intelligence is a small part of the world, the ability of any intelligence to perceive, understand, and manipulate the world is inherently limited, requiring the ability to deal with uncertainty. [sent-18, score-0.632]
</p><p>12 Some people try to create an intelligence without reference to efficient computation. [sent-22, score-0.518]
</p><p>13 The algorithm is very difficult to deploy in practice because there were no computational constraints other than computability designed into it’s creation. [sent-24, score-0.336]
</p><p>14 There was a time when some people thought, “If we could just get a program that mastered chess so well it could beat the best humans, we will learn enough about AI to create an AI. [sent-27, score-0.198]
</p><p>15 Here  A  might be nearest neighbors, decision trees, two-layer neural networks, support vector machines, nonparametric statistics, nonparametric Bayes, or something else. [sent-34, score-0.25]
</p><p>16 Solving AI is undeniably hard, as evidenced by the amount of time spent on it, and the set of approaches which haven’t succeeded. [sent-37, score-0.175]
</p><p>17 The first is that there is, or soon will be  sufficient computation  available, unlike the last time. [sent-39, score-0.157]
</p><p>18 The second is that the machine learning approach fails well, because there are industrial uses for machine learning. [sent-40, score-0.201]
</p><p>19 The machine learning approach to AI has this goodness property, unlike many other approaches, which partially explains why the ML approach is successful despite “failing” so far to achieve AI. [sent-45, score-0.138]
</p><p>20 Given this, a fair strategy seems to be first mastering one strategy, and then incorporating others, always checking that that incorporation properly addresses real world problems. [sent-47, score-0.244]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ai', 0.348), ('uncertainty', 0.312), ('intelligence', 0.252), ('intelligences', 0.195), ('ontologies', 0.195), ('create', 0.135), ('constraints', 0.131), ('blocks', 0.13), ('cyc', 0.13), ('imitate', 0.13), ('fails', 0.11), ('world', 0.108), ('undeniably', 0.107), ('concern', 0.106), ('humans', 0.104), ('second', 0.091), ('computational', 0.089), ('nonparametric', 0.087), ('computation', 0.085), ('ml', 0.078), ('strategy', 0.078), ('might', 0.076), ('unlike', 0.072), ('ability', 0.072), ('deal', 0.07), ('property', 0.07), ('experience', 0.069), ('approaches', 0.068), ('without', 0.068), ('programs', 0.067), ('partially', 0.066), ('people', 0.063), ('statistics', 0.062), ('everything', 0.062), ('substantial', 0.061), ('chunking', 0.058), ('upside', 0.058), ('unlucky', 0.058), ('mastering', 0.058), ('breuel', 0.058), ('aixi', 0.058), ('asymptopia', 0.058), ('breath', 0.058), ('computability', 0.058), ('decomposition', 0.058), ('deploy', 0.058), ('died', 0.058), ('fool', 0.058), ('grab', 0.058), ('manipulate', 0.058)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000002 <a title="352-tfidf-1" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<p>Introduction: I recently had fun discussions with both  Vikash Mansinghka  and  Thomas Breuel  about approaching AI with machine learning.  The general interest in taking a crack at AI with machine learning seems to be rising on many fronts including  DARPA .
 
As a matter of history, there was a great deal of interest in AI which died down before I began research.  There remain many projects and conferences spawned in this earlier AI wave, as well as a good bit of experience about what did not work, or at least did not work yet.  Here are a few examples of failure modes that people seem to run into:
  
  Supply/Product confusion .  Sometimes we think “Intelligences use X, so I’ll create X and have an Intelligence.”  An example of this is the  Cyc Project  which inspires some people as “intelligences use ontologies, so I’ll create an ontology and a system using it to have an Intelligence.”  The flaw here is that Intelligences  create  ontologies, which they use, and without the ability to create ont</p><p>2 0.28121743 <a title="352-tfidf-2" href="../hunch_net-2009/hunch_net-2009-11-29-AI_Safety.html">380 hunch net-2009-11-29-AI Safety</a></p>
<p>Introduction: Dan Reeves  introduced me to  Michael Vassar  who ran the  Singularity Summit  and educated me a bit on the subject of AI safety which the  Singularity Institute  has  small grants for .  
 
I still believe that  interstellar space travel is necessary for long term civilization survival, and the AI is necessary for interstellar space travel .  On these grounds alone, we could judge that developing AI is much more safe than not.  Nevertheless, there is a basic reasonable fear, as expressed by some commenters, that AI could go bad.
 
A basic scenario starts with someone inventing an AI and telling it to make as much money as possible.  The AI promptly starts trading in various markets to make money.  To improve, it crafts a virus that takes over most of the world’s computers using it as a surveillance network so that it can always make the right decision.  The AI also branches out into any form of distance work, taking over the entire outsourcing process for all jobs that are entirely di</p><p>3 0.27320382 <a title="352-tfidf-3" href="../hunch_net-2009/hunch_net-2009-05-08-Computability_in_Artificial_Intelligence.html">353 hunch net-2009-05-08-Computability in Artificial Intelligence</a></p>
<p>Introduction: Normally I do not blog, but John kindly invited me to do so.  Since computability issues play a major role in Artificial Intelligence and Machine Learning, I would like to take the opportunity to comment on that and raise some questions.
 
The general attitude is that AI is about finding efficient smart algorithms. For large parts of machine learning, the same attitude is not too dangerous. If you want to concentrate on conceptual problems, simply become a statistician. There is no analogous escape for modern research on AI (as opposed to  GOFAI  rooted in logic).

 
Let me show by analogy why  limiting research to computational questions is bad for any field. 
 
Except in computer science, computational aspects play little role in the development of  fundamental  theories: Consider e.g. set theory with axiom of choice, foundations of logic, exact/full minimax for zero-sum games, quantum (field) theory, string theory, … Indeed, at least in physics, every new fundamental theory seems to</p><p>4 0.15218934 <a title="352-tfidf-4" href="../hunch_net-2008/hunch_net-2008-04-12-It_Doesn%26%238217%3Bt_Stop.html">295 hunch net-2008-04-12-It Doesn&#8217;t Stop</a></p>
<p>Introduction: I’ve enjoyed the  Terminator  movies and show.  Neglecting the whacky aspects (time travel and associated paradoxes), there is an enduring topic of discussion: how do people deal with intelligent machines (and vice versa)?
 
In Terminator-land, the primary method for dealing with intelligent machines is to prevent them from being made.  This approach works pretty badly, because a new angle on building an intelligent machine keeps coming up.  This is partly a ploy for writer’s to avoid writing themselves out of a job, but there is a fundamental truth to it as well: preventing progress in research is hard.
 
The United States, has been experimenting with trying to stop research on  stem cells .  It hasn’t worked very well—the net effect has been retarding research programs a bit, and exporting some research to other countries.  Another less recent example was encryption technology, for which the United States generally did not encourage early public research and even  discouraged as a mu</p><p>5 0.14900093 <a title="352-tfidf-5" href="../hunch_net-2011/hunch_net-2011-02-17-What_does_Watson_mean%3F.html">424 hunch net-2011-02-17-What does Watson mean?</a></p>
<p>Introduction: Watson  convincingly beat the best champion  Jeopardy!  players.  The apparent significance of this varies hugely, depending on your background knowledge about the related machine learning, NLP, and search technology.  For a random person, this might seem evidence of serious machine intelligence, while for people working on the system itself, it probably seems like a reasonably good assemblage of existing technologies with several twists to make the entire system work.
 
Above all, I think we should congratulate the people who managed to put together and execute this project—many years of effort by a diverse set of highly skilled people were needed to make this happen.  In academia, it’s pretty difficult for one professor to assemble that quantity of talent, and in industry it’s rarely the case that such a capable group has both a worthwhile project and the support needed to pursue something like this for several years before success.
 
 Alina  invited me to the Jeopardy watching party</p><p>6 0.1375078 <a title="352-tfidf-6" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>7 0.13315187 <a title="352-tfidf-7" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>8 0.11801822 <a title="352-tfidf-8" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>9 0.11550077 <a title="352-tfidf-9" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>10 0.11363708 <a title="352-tfidf-10" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>11 0.11272525 <a title="352-tfidf-11" href="../hunch_net-2011/hunch_net-2011-08-06-Interesting_thing_at_UAI_2011.html">440 hunch net-2011-08-06-Interesting thing at UAI 2011</a></p>
<p>12 0.11185073 <a title="352-tfidf-12" href="../hunch_net-2007/hunch_net-2007-06-23-Machine_Learning_Jobs_are_Growing_on_Trees.html">250 hunch net-2007-06-23-Machine Learning Jobs are Growing on Trees</a></p>
<p>13 0.11178423 <a title="352-tfidf-13" href="../hunch_net-2008/hunch_net-2008-01-28-Sufficient_Computation.html">287 hunch net-2008-01-28-Sufficient Computation</a></p>
<p>14 0.10850773 <a title="352-tfidf-14" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>15 0.10722635 <a title="352-tfidf-15" href="../hunch_net-2006/hunch_net-2006-01-25-1_year.html">151 hunch net-2006-01-25-1 year</a></p>
<p>16 0.1044182 <a title="352-tfidf-16" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>17 0.10355983 <a title="352-tfidf-17" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>18 0.10298779 <a title="352-tfidf-18" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>19 0.098981395 <a title="352-tfidf-19" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>20 0.09853746 <a title="352-tfidf-20" href="../hunch_net-2009/hunch_net-2009-12-09-Inherent_Uncertainty.html">383 hunch net-2009-12-09-Inherent Uncertainty</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.279), (1, 0.033), (2, -0.08), (3, 0.09), (4, -0.013), (5, -0.006), (6, -0.024), (7, 0.028), (8, 0.035), (9, -0.072), (10, -0.088), (11, -0.057), (12, 0.036), (13, 0.053), (14, -0.035), (15, 0.041), (16, 0.135), (17, -0.09), (18, 0.091), (19, 0.084), (20, -0.009), (21, 0.107), (22, -0.129), (23, 0.164), (24, 0.096), (25, -0.066), (26, 0.026), (27, -0.135), (28, 0.077), (29, -0.001), (30, 0.083), (31, -0.038), (32, -0.041), (33, -0.069), (34, 0.122), (35, -0.006), (36, -0.098), (37, 0.019), (38, -0.013), (39, -0.022), (40, -0.071), (41, -0.054), (42, 0.056), (43, -0.132), (44, -0.059), (45, -0.011), (46, -0.006), (47, -0.005), (48, -0.007), (49, -0.052)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94519961 <a title="352-lsi-1" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<p>Introduction: I recently had fun discussions with both  Vikash Mansinghka  and  Thomas Breuel  about approaching AI with machine learning.  The general interest in taking a crack at AI with machine learning seems to be rising on many fronts including  DARPA .
 
As a matter of history, there was a great deal of interest in AI which died down before I began research.  There remain many projects and conferences spawned in this earlier AI wave, as well as a good bit of experience about what did not work, or at least did not work yet.  Here are a few examples of failure modes that people seem to run into:
  
  Supply/Product confusion .  Sometimes we think “Intelligences use X, so I’ll create X and have an Intelligence.”  An example of this is the  Cyc Project  which inspires some people as “intelligences use ontologies, so I’ll create an ontology and a system using it to have an Intelligence.”  The flaw here is that Intelligences  create  ontologies, which they use, and without the ability to create ont</p><p>2 0.93597454 <a title="352-lsi-2" href="../hunch_net-2009/hunch_net-2009-11-29-AI_Safety.html">380 hunch net-2009-11-29-AI Safety</a></p>
<p>Introduction: Dan Reeves  introduced me to  Michael Vassar  who ran the  Singularity Summit  and educated me a bit on the subject of AI safety which the  Singularity Institute  has  small grants for .  
 
I still believe that  interstellar space travel is necessary for long term civilization survival, and the AI is necessary for interstellar space travel .  On these grounds alone, we could judge that developing AI is much more safe than not.  Nevertheless, there is a basic reasonable fear, as expressed by some commenters, that AI could go bad.
 
A basic scenario starts with someone inventing an AI and telling it to make as much money as possible.  The AI promptly starts trading in various markets to make money.  To improve, it crafts a virus that takes over most of the world’s computers using it as a surveillance network so that it can always make the right decision.  The AI also branches out into any form of distance work, taking over the entire outsourcing process for all jobs that are entirely di</p><p>3 0.91415983 <a title="352-lsi-3" href="../hunch_net-2009/hunch_net-2009-05-08-Computability_in_Artificial_Intelligence.html">353 hunch net-2009-05-08-Computability in Artificial Intelligence</a></p>
<p>Introduction: Normally I do not blog, but John kindly invited me to do so.  Since computability issues play a major role in Artificial Intelligence and Machine Learning, I would like to take the opportunity to comment on that and raise some questions.
 
The general attitude is that AI is about finding efficient smart algorithms. For large parts of machine learning, the same attitude is not too dangerous. If you want to concentrate on conceptual problems, simply become a statistician. There is no analogous escape for modern research on AI (as opposed to  GOFAI  rooted in logic).

 
Let me show by analogy why  limiting research to computational questions is bad for any field. 
 
Except in computer science, computational aspects play little role in the development of  fundamental  theories: Consider e.g. set theory with axiom of choice, foundations of logic, exact/full minimax for zero-sum games, quantum (field) theory, string theory, … Indeed, at least in physics, every new fundamental theory seems to</p><p>4 0.79432631 <a title="352-lsi-4" href="../hunch_net-2008/hunch_net-2008-01-28-Sufficient_Computation.html">287 hunch net-2008-01-28-Sufficient Computation</a></p>
<p>Introduction: Do we have computer hardware sufficient for AI?  This question is difficult to answer, but here’s a try:
 
One way to achieve AI is by simulating a human brain.  A human brain has about 10 15  synapses which operate at about 10 2  per second implying about 10 17  bit ops per second.
 
A modern computer runs at 10 9  cycles/second and operates on 10 2  bits per cycle implying 10 11  bits processed per second.  
 
The gap here is only 6 orders of magnitude, which can be plausibly surpassed via cluster machines.  For example, the  BlueGene/L  operates 10 5  nodes (one order of magnitude short).  It’s peak recorded performance is about 0.5*10 15  FLOPS which translates to about 10 16  bit ops per second, which is nearly 10 17 .
 
There are many criticisms (both positive and negative) for this argument.
  
 Simulation of a human brain might require substantially more detail.  Perhaps an additional 10 2  is required per neuron. 
 We may not need to simulate a human brain to achieve AI.  Ther</p><p>5 0.75591731 <a title="352-lsi-5" href="../hunch_net-2008/hunch_net-2008-04-12-It_Doesn%26%238217%3Bt_Stop.html">295 hunch net-2008-04-12-It Doesn&#8217;t Stop</a></p>
<p>Introduction: I’ve enjoyed the  Terminator  movies and show.  Neglecting the whacky aspects (time travel and associated paradoxes), there is an enduring topic of discussion: how do people deal with intelligent machines (and vice versa)?
 
In Terminator-land, the primary method for dealing with intelligent machines is to prevent them from being made.  This approach works pretty badly, because a new angle on building an intelligent machine keeps coming up.  This is partly a ploy for writer’s to avoid writing themselves out of a job, but there is a fundamental truth to it as well: preventing progress in research is hard.
 
The United States, has been experimenting with trying to stop research on  stem cells .  It hasn’t worked very well—the net effect has been retarding research programs a bit, and exporting some research to other countries.  Another less recent example was encryption technology, for which the United States generally did not encourage early public research and even  discouraged as a mu</p><p>6 0.72260338 <a title="352-lsi-6" href="../hunch_net-2006/hunch_net-2006-02-02-Introspectionism_as_a_Disease.html">153 hunch net-2006-02-02-Introspectionism as a Disease</a></p>
<p>7 0.65814614 <a title="352-lsi-7" href="../hunch_net-2011/hunch_net-2011-02-17-What_does_Watson_mean%3F.html">424 hunch net-2011-02-17-What does Watson mean?</a></p>
<p>8 0.55360752 <a title="352-lsi-8" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>9 0.54210436 <a title="352-lsi-9" href="../hunch_net-2014/hunch_net-2014-02-16-Metacademy%3A_a_package_manager_for_knowledge.html">493 hunch net-2014-02-16-Metacademy: a package manager for knowledge</a></p>
<p>10 0.54029948 <a title="352-lsi-10" href="../hunch_net-2006/hunch_net-2006-04-02-Mad_%28Neuro%29science.html">168 hunch net-2006-04-02-Mad (Neuro)science</a></p>
<p>11 0.52590322 <a title="352-lsi-11" href="../hunch_net-2005/hunch_net-2005-01-24-The_Humanloop_Spectrum_of_Machine_Learning.html">3 hunch net-2005-01-24-The Humanloop Spectrum of Machine Learning</a></p>
<p>12 0.49468765 <a title="352-lsi-12" href="../hunch_net-2008/hunch_net-2008-11-26-Efficient_Reinforcement_Learning_in_MDPs.html">328 hunch net-2008-11-26-Efficient Reinforcement Learning in MDPs</a></p>
<p>13 0.49035326 <a title="352-lsi-13" href="../hunch_net-2009/hunch_net-2009-08-03-Carbon_in_Computer_Science_Research.html">366 hunch net-2009-08-03-Carbon in Computer Science Research</a></p>
<p>14 0.48928723 <a title="352-lsi-14" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>15 0.47646251 <a title="352-lsi-15" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>16 0.46656439 <a title="352-lsi-16" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>17 0.4644264 <a title="352-lsi-17" href="../hunch_net-2007/hunch_net-2007-11-02-The_Machine_Learning_Award_goes_to_%26%238230%3B.html">270 hunch net-2007-11-02-The Machine Learning Award goes to &#8230;</a></p>
<p>18 0.46014398 <a title="352-lsi-18" href="../hunch_net-2007/hunch_net-2007-02-02-Thoughts_regarding_%26%238220%3BIs_machine_learning_different_from_statistics%3F%26%238221%3B.html">230 hunch net-2007-02-02-Thoughts regarding &#8220;Is machine learning different from statistics?&#8221;</a></p>
<p>19 0.45670372 <a title="352-lsi-19" href="../hunch_net-2007/hunch_net-2007-06-23-Machine_Learning_Jobs_are_Growing_on_Trees.html">250 hunch net-2007-06-23-Machine Learning Jobs are Growing on Trees</a></p>
<p>20 0.45426187 <a title="352-lsi-20" href="../hunch_net-2005/hunch_net-2005-11-05-The_design_of_a_computing_cluster.html">128 hunch net-2005-11-05-The design of a computing cluster</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(10, 0.029), (27, 0.558), (38, 0.063), (53, 0.056), (55, 0.065), (56, 0.016), (58, 0.018), (64, 0.014), (66, 0.014), (94, 0.08), (95, 0.02)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99780732 <a title="352-lda-1" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<p>Introduction: I recently had fun discussions with both  Vikash Mansinghka  and  Thomas Breuel  about approaching AI with machine learning.  The general interest in taking a crack at AI with machine learning seems to be rising on many fronts including  DARPA .
 
As a matter of history, there was a great deal of interest in AI which died down before I began research.  There remain many projects and conferences spawned in this earlier AI wave, as well as a good bit of experience about what did not work, or at least did not work yet.  Here are a few examples of failure modes that people seem to run into:
  
  Supply/Product confusion .  Sometimes we think “Intelligences use X, so I’ll create X and have an Intelligence.”  An example of this is the  Cyc Project  which inspires some people as “intelligences use ontologies, so I’ll create an ontology and a system using it to have an Intelligence.”  The flaw here is that Intelligences  create  ontologies, which they use, and without the ability to create ont</p><p>2 0.99080288 <a title="352-lda-2" href="../hunch_net-2005/hunch_net-2005-03-22-Active_learning.html">45 hunch net-2005-03-22-Active learning</a></p>
<p>Introduction: Often, unlabeled data is easy to come by but labels are expensive. For instance, if you’re building a speech recognizer, it’s easy enough to get raw speech samples — just walk around with a microphone — but labeling even one of these samples is a tedious process in which a human must examine the speech signal and carefully segment it into phonemes. In the field of active learning, the goal is as usual to construct an accurate classifier, but the labels of the data points are initially hidden and there is a charge for each label you want revealed. The hope is that by intelligent adaptive querying, you can get away with significantly fewer labels than you would need in a regular supervised learning framework.
 
Here’s an example. Suppose the data lie on the real line, and the classifiers are simple thresholding functions, H = {h w }: 
  h w (x) = 1 if x > w, and 0 otherwise.  
 
VC theory tells us that if the underlying distribution P can be classified perfectly by some hypothesis in H (</p><p>3 0.98917025 <a title="352-lda-3" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>Introduction: Essentially everyone who writes research papers suffers rejections.  They always sting immediately, but upon further reflection many of these rejections come to seem reasonable.  Maybe the equations had too many typos or maybe the topic just isn’t as important as was originally thought.  A few rejections do not come to seem acceptable, and these form the basis of reviewing horror stories, a great material for conversations.  I’ve decided to share three of mine, now all safely a bit distant in the past.
  
  Prediction Theory for Classification Tutorial .  This is a tutorial about tight sample complexity bounds for classification that I submitted to  JMLR .  The first decision I heard was a reject which appeared quite unjust to me—for example one of the reviewers appeared to claim that all the content was in standard statistics books.  Upon further inquiry, several citations were given, none of which actually covered the content.  Later, I was shocked to hear the paper was accepted. App</p><p>4 0.98839861 <a title="352-lda-4" href="../hunch_net-2006/hunch_net-2006-07-13-Regression_vs._Classification_as_a_Primitive.html">196 hunch net-2006-07-13-Regression vs. Classification as a Primitive</a></p>
<p>Introduction: For  learning reductions  we have been concentrating on reducing various complex learning problems to binary classification.  This choice needs to be actively questioned, because it was not carefully considered.
 
Binary clasification is learning a classifier  c:X -> {0,1}  so as to minimize the probability of being wrong,  Pr x,y~D (c(x)   y)  .
 
The primary alternative candidate seems to be squared error regression.  In squared error regression, you learn a regressor  s:X -> [0,1]  so as to minimize squared error,  E x,y~D  (s(x)-y) 2  .
 
It is difficult to judge one primitive against another.  The judgement must at least partially be made on nontheoretical grounds because (essentially) we are evaluating a choice between two axioms/assumptions.
 
These two primitives are significantly related.  Classification can be reduced to regression in the obvious way: you use the regressor to predict  D(y=1|x) , then threshold at  0.5 .   For this simple reduction a squared error regret of  r</p><p>5 0.98675793 <a title="352-lda-5" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>Introduction: Consider the contextual bandit setting where, repeatedly:
  
 A context  x  is observed. 
 An action  a  is taken given the context  x .  
 A reward  r  is observed, dependent on  x  and  a . 
  
Where the goal of a learning agent is to find a policy for step 2 achieving a large expected reward.  
 
This setting is of obvious importance, because in the real world we typically make decisions based on some set of information and then get feedback only about the single action taken.  It also fundamentally differs from supervised learning settings because knowing the value of one action is not equivalent to knowing the value of all actions.
 
A decade ago the best machine learning techniques for this setting where implausibly inefficient.   Dean Foster  once told me he thought the area was a research sinkhole with little progress to be expected.  Now we are on the verge of being able to routinely attack these problems, in almost exactly the same sense that we routinely attack bread and but</p><p>6 0.98282582 <a title="352-lda-6" href="../hunch_net-2005/hunch_net-2005-02-01-Watchword%3A_Loss.html">9 hunch net-2005-02-01-Watchword: Loss</a></p>
<p>7 0.98258293 <a title="352-lda-7" href="../hunch_net-2008/hunch_net-2008-07-06-To_Dual_or_Not.html">308 hunch net-2008-07-06-To Dual or Not</a></p>
<p>8 0.98160034 <a title="352-lda-8" href="../hunch_net-2006/hunch_net-2006-04-14-JMLR_is_a_success.html">172 hunch net-2006-04-14-JMLR is a success</a></p>
<p>9 0.98149848 <a title="352-lda-9" href="../hunch_net-2008/hunch_net-2008-02-10-Complexity_Illness.html">288 hunch net-2008-02-10-Complexity Illness</a></p>
<p>10 0.97981179 <a title="352-lda-10" href="../hunch_net-2009/hunch_net-2009-02-04-Optimal_Proxy_Loss_for_Classification.html">341 hunch net-2009-02-04-Optimal Proxy Loss for Classification</a></p>
<p>11 0.97885835 <a title="352-lda-11" href="../hunch_net-2007/hunch_net-2007-05-12-Loss_Function_Semantics.html">245 hunch net-2007-05-12-Loss Function Semantics</a></p>
<p>12 0.97727156 <a title="352-lda-12" href="../hunch_net-2007/hunch_net-2007-06-14-Interesting_Papers_at_COLT_2007.html">247 hunch net-2007-06-14-Interesting Papers at COLT 2007</a></p>
<p>13 0.97532064 <a title="352-lda-13" href="../hunch_net-2007/hunch_net-2007-11-28-Computational_Consequences_of_Classification.html">274 hunch net-2007-11-28-Computational Consequences of Classification</a></p>
<p>14 0.97185445 <a title="352-lda-14" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>15 0.97082543 <a title="352-lda-15" href="../hunch_net-2008/hunch_net-2008-03-23-Interactive_Machine_Learning.html">293 hunch net-2008-03-23-Interactive Machine Learning</a></p>
<p>16 0.96953952 <a title="352-lda-16" href="../hunch_net-2006/hunch_net-2006-03-24-NLPers.html">166 hunch net-2006-03-24-NLPers</a></p>
<p>17 0.96953952 <a title="352-lda-17" href="../hunch_net-2007/hunch_net-2007-06-13-Not_Posting.html">246 hunch net-2007-06-13-Not Posting</a></p>
<p>18 0.96953952 <a title="352-lda-18" href="../hunch_net-2010/hunch_net-2010-12-02-Traffic_Prediction_Problem.html">418 hunch net-2010-12-02-Traffic Prediction Problem</a></p>
<p>19 0.96869963 <a title="352-lda-19" href="../hunch_net-2007/hunch_net-2007-05-09-The_Missing_Bound.html">244 hunch net-2007-05-09-The Missing Bound</a></p>
<p>20 0.96862555 <a title="352-lda-20" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
