<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>hunch_net 2009 knowledge graph</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2009" href="#">hunch_net-2009</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>hunch_net 2009 knowledge graph</h1>
<br/><h3>similar blogs computed by tfidf model</h3><br/><h3>similar blogs computed by <a title="lsi-model" href="./hunch_net_lsi.html">lsi model</a></h3><br/><h3>similar blogs computed by <a title="lda-model" href="./hunch_net_lda.html">lda model</a></h3><br/><h2>blogs list:</h2><p>1 <a title="hunch_net-2009-385" href="../hunch_net-2009/hunch_net-2009-12-27-Interesting_things_at_NIPS_2009.html">hunch net-2009-12-27-Interesting things at NIPS 2009</a></p>
<p>Introduction: Several papers at NIPS caught my attention.
  
  Elad Hazan  and  Satyen Kale ,  Online Submodular Optimization  They define an algorithm for online optimization of submodular functions with regret guarantees.  This places submodular optimization roughly on par with online convex optimization as tractable settings for online learning.   
  Elad Hazan  and  Satyen Kale   On Stochastic and Worst-Case Models of Investing .  At it’s core, this is yet another example of modifying worst-case online learning to deal with variance, but the application to financial models is particularly cool and it seems plausibly superior other common approaches for financial modeling. 
  Mark Palatucci ,  Dean Pomerlau ,  Tom Mitchell , and  Geoff Hinton   Zero Shot Learning with Semantic Output Codes  The goal here is predicting a label in a multiclass supervised setting where the label never occurs in the training data.  They have some basic analysis and also a nice application to FMRI brain reading. 
  Sh</p><p>2 <a title="hunch_net-2009-384" href="../hunch_net-2009/hunch_net-2009-12-24-Top_graduates_this_season.html">hunch net-2009-12-24-Top graduates this season</a></p>
<p>Introduction: I would like to point out 3 graduates this season as having my confidence they are capable of doing great things. 
  
  Daniel Hsu  has diverse papers with diverse coauthors on {active learning, mulitlabeling, temporal learning, …} each covering new algorithms and methods of analysis.  He is also a capable programmer, having helped me with some nitty-gritty details of cluster parallel  Vowpal Wabbit  this summer.  He has an excellent tendency to just get things done. 
  Nicolas Lambert  doesn’t nominally work in machine learning, but I’ve found his work in  elicitation  relevant nevertheless.  In essence, elicitable properties are closely related to learnable properties, and the elicitation complexity is related to a notion of learning complexity.  See the  Surrogate regret bounds paper  for some related discussion.  Few people successfully work at such a general level that it crosses fields, but he’s one of them. 
  Yisong Yue  is deeply focused on interactive learning, which he has a</p><p>3 <a title="hunch_net-2009-383" href="../hunch_net-2009/hunch_net-2009-12-09-Inherent_Uncertainty.html">hunch net-2009-12-09-Inherent Uncertainty</a></p>
<p>Introduction: I’d like to point out  Inherent Uncertainty , which I’ve added to the ML blog post scanner on the right.   My understanding from  Jake  is that the intention is to have a multiauthor blog which is more specialized towards learning theory/game theory than this one.  Nevertheless, several of the posts seem to be of wider interest.</p><p>4 <a title="hunch_net-2009-382" href="../hunch_net-2009/hunch_net-2009-12-09-Future_Publication_Models_%40_NIPS.html">hunch net-2009-12-09-Future Publication Models @ NIPS</a></p>
<p>Introduction: Yesterday, there was a discussion about  future publication models at NIPS .   Yann  and  Zoubin  have specific detailed proposals which I’ll add links to when I get them ( Yann’s proposal  and  Zoubin’s proposal ).
 
What struck me about the discussion is that there are many simultaneous concerns as well as many simultaneous proposals, which makes it difficult to keep all the distinctions straight in a verbal conversation.  It also seemed like people were serious enough about this that we may see some real movement.  Certainly, my personal experience motivates that as I’ve  posted many times  about the substantial flaws in our review process, including some very poor personal experiences.
 
Concerns include the following:
  
 (Several) Reviewers are overloaded, boosting the noise in decision making. 
 ( Yann ) A new system should run with as little built-in delay and friction to the process of research as possible. 
 ( Hanna Wallach (updated)) Double-blind review is particularly impor</p><p>5 <a title="hunch_net-2009-381" href="../hunch_net-2009/hunch_net-2009-12-07-Vowpal_Wabbit_version_4.0%2C_and_a_NIPS_heresy.html">hunch net-2009-12-07-Vowpal Wabbit version 4.0, and a NIPS heresy</a></p>
<p>Introduction: I’m releasing  version 4.0 ( tarball ) of  Vowpal Wabbit .  The biggest change (by far) in this release is experimental support for cluster parallelism, with notable help from  Daniel Hsu .  
 
I also took advantage of the major version number to introduce some incompatible changes, including switching to  murmurhash 2 , and other alterations to cachefiles.  You’ll need to delete and regenerate them.  In addition, the precise specification for a “tag” (i.e. string that can be used to identify an example) changed—you can’t have a space between the tag and the ‘|’ at the beginning of the feature namespace.  
 
And, of course, we made it faster.
 
For the future, I put up my  todo list  outlining the major future improvements I want to see in the code.  I’m planning to discuss the current mechanism and results of the cluster parallel implementation at the  large scale machine learning workshop  at  NIPS  later this week.  Several people have asked me to do a tutorial/walkthrough of VW, wh</p><p>6 <a title="hunch_net-2009-380" href="../hunch_net-2009/hunch_net-2009-11-29-AI_Safety.html">hunch net-2009-11-29-AI Safety</a></p>
<p>Introduction: Dan Reeves  introduced me to  Michael Vassar  who ran the  Singularity Summit  and educated me a bit on the subject of AI safety which the  Singularity Institute  has  small grants for .  
 
I still believe that  interstellar space travel is necessary for long term civilization survival, and the AI is necessary for interstellar space travel .  On these grounds alone, we could judge that developing AI is much more safe than not.  Nevertheless, there is a basic reasonable fear, as expressed by some commenters, that AI could go bad.
 
A basic scenario starts with someone inventing an AI and telling it to make as much money as possible.  The AI promptly starts trading in various markets to make money.  To improve, it crafts a virus that takes over most of the world’s computers using it as a surveillance network so that it can always make the right decision.  The AI also branches out into any form of distance work, taking over the entire outsourcing process for all jobs that are entirely di</p><p>7 <a title="hunch_net-2009-379" href="../hunch_net-2009/hunch_net-2009-11-23-ICML_2009_Workshops_%28and_Tutorials%29.html">hunch net-2009-11-23-ICML 2009 Workshops (and Tutorials)</a></p>
<p>Introduction: I’m the  workshops chair  for  ICML  this year.  As such, I would like to personally encourage people to consider running a workshop.
 
My general view of workshops is that they are excellent as opportunities to discuss and develop research directions—some of my best work has come from collaborations at workshops and several workshops have substantially altered my thinking about various problems.  My experience running workshops is that setting them up and making them fly often appears much harder than it actually is, and the workshops often come off much better than expected in the end.  Submissions are due January 18, two weeks before papers.
 
Similarly,  Ben Taskar  is looking for good  tutorials , which is complementary.  Workshops are about exploring a subject, while a tutorial is about distilling it down into an easily taught essence, a vital part of the research process.  Tutorials are due February 13, two weeks after papers.</p><p>8 <a title="hunch_net-2009-378" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">hunch net-2009-11-15-The Other Online Learning</a></p>
<p>Introduction: If you search for “online learning” with any  major   search   engine , it’s interesting to note that zero of the results are for online machine learning.  This may not be a mistake if you are committed to a global ordering.  In other words, the number of people specifically interested in the least interesting top-10 online human learning result might exceed the number of people interested in online machine learning, even given the presence of the other 9 results.  The essential observation here is that the process of human learning is a big business (around 5% of GDP) effecting virtually everyone.  
 
The internet is changing this dramatically, by altering the economics of teaching. Consider two possibilities:
  
 The classroom-style teaching environment continues as is, with many teachers for the same subject. 
 All the teachers for one subject get together, along with perhaps a factor of 2 more people who are experts in online delivery.  They spend a factor of 4 more time designing</p><p>9 <a title="hunch_net-2009-377" href="../hunch_net-2009/hunch_net-2009-11-09-NYAS_ML_Symposium_this_year..html">hunch net-2009-11-09-NYAS ML Symposium this year.</a></p>
<p>Introduction: The  NYAS ML symposium  grew again this year to 170 participants, despite the need to outsmart or otherwise tunnel through  a crowd .  
 
Perhaps the most distinct talk was by Bob Bell on various aspects of the  Netflix prize  competition.  I also enjoyed several student posters including  Matt Hoffman ‘s cool examples of blind source separation for music.
 
I’m somewhat surprised how much the workshop has grown, as it is now comparable in size to a small conference, although in style more similar to a workshop.  At some point as an event grows, it becomes owned by the community rather than the organizers, so if anyone has suggestions on improving it, speak up and be heard.</p><p>10 <a title="hunch_net-2009-376" href="../hunch_net-2009/hunch_net-2009-11-06-Yisong_Yue_on_Self-improving_Systems.html">hunch net-2009-11-06-Yisong Yue on Self-improving Systems</a></p>
<p>Introduction: I’d like to point out  Yisong Yue ‘s  post on Self-improving systems , which is a nicely readable description of the necessity and potential of interactive learning to deal with the information overload problem that is endemic to the modern internet.</p><p>11 <a title="hunch_net-2009-375" href="../hunch_net-2009/hunch_net-2009-10-26-NIPS_workshops.html">hunch net-2009-10-26-NIPS workshops</a></p>
<p>Introduction: Many of the  NIPS workshops  have a deadline about now, and the NIPS  early registration deadline is Nov. 6 .  Several interest me:
  
  Adaptive Sensing, Active Learning, and Experimental Design  due 10/27. 
  Discrete Optimization in Machine Learning: Submodularity, Sparsity & Polyhedra , due Nov. 6. 
  Large-Scale Machine Learning: Parallelism and Massive Datasets , due 10/23 (i.e. past) 
  Analysis and Design of Algorithms for Interactive Machine Learning , due 10/30. 
  
And Iâ&euro;&trade;m sure many of the others interest others.   Workshops are great as a mechanism for research, so take a look if there is any chance you might be interested.</p><p>12 <a title="hunch_net-2009-374" href="../hunch_net-2009/hunch_net-2009-10-10-ALT_2009.html">hunch net-2009-10-10-ALT 2009</a></p>
<p>Introduction: I attended  ALT  (“Algorithmic Learning Theory”) for the first time this year.  My impression is ALT = 0.5 COLT, by attendance and also by some more intangible “what do I get from it?” measure.  There are many differences which can’t quite be described this way though.  The program for ALT seems to be substantially more diverse than COLT, which is both a weakness and a strength.  
 
One paper that might interest people generally is:
 
 Alexey Chernov  and  Vladimir Vovk ,  Prediction with Expert Evaluators’ Advice .  The basic observation here is that in the online learning with experts setting you can simultaneously compete with several compatible loss functions simultaneously.  Restated, debating between competing with log loss and squared loss is a waste of breath, because it’s almost free to compete with them both simultaneously.  This might interest anyone who has run into “which loss function?” debates that come up periodically.</p><p>13 <a title="hunch_net-2009-373" href="../hunch_net-2009/hunch_net-2009-10-03-Static_vs._Dynamic_multiclass_prediction.html">hunch net-2009-10-03-Static vs. Dynamic multiclass prediction</a></p>
<p>Introduction: I have had interesting discussions about distinction between static vs. dynamic classes with  Kishore  and  Hal .  
 
The distinction arises in multiclass prediction settings.  A static set of classes is given by a set of labels  {1,…,k}  and the goal is generally to choose the most likely label given features.  The static approach is the one that we typically analyze and think about in machine learning.  
 
The dynamic setting is one that is often used in practice.  The basic idea is that the number of classes is not fixed, varying on a per example basis.  These different classes are generally defined by a choice of features.  
 
The distinction between these two settings as far as theory goes, appears to be very substantial.  For example, in the static setting, in  learning reductions land , we have techniques now for robust  O(log(k))  time prediction in many multiclass setting variants.  In the dynamic setting, the best techniques known are  O(k) , and furthermore this exponential</p><p>14 <a title="hunch_net-2009-372" href="../hunch_net-2009/hunch_net-2009-09-29-Machine_Learning_Protests_at_the_G20.html">hunch net-2009-09-29-Machine Learning Protests at the G20</a></p>
<p>Introduction: The  machine learning department at CMU  turned out en masse to protest the G20 summit in Pittsburgh.    Arthur Gretton  uploaded some  great photos  covering the event</p><p>15 <a title="hunch_net-2009-371" href="../hunch_net-2009/hunch_net-2009-09-21-Netflix_finishes_%28and_starts%29.html">hunch net-2009-09-21-Netflix finishes (and starts)</a></p>
<p>Introduction: I attended the  Netflix prize  ceremony this morning.  The press conference part is  covered fine elsewhere , with the basic outcome being that  BellKor’s Pragmatic Chaos  won over  The Ensemble  by 15-20  minutes , because they were tied in performance on the ultimate holdout set.  I’m sure the individual participants will have many chances to speak about the solution.  One of these is Bell at the  NYAS ML symposium on Nov. 6 .
 
Several additional details may interest ML people.
  
 The degree of overfitting exhibited by the difference in performance on the  leaderboard test set  and the ultimate hold out set was small, but determining at .02 to .03%. 
 A tie was possible, because the rules cut off measurements below the fourth digit based on significance concerns.  In actuality, of course, the scores do differ before rounding, but everyone I spoke to claimed not to know how.  The complete dataset has been  released on UCI , so each team could compute their own score to whatever accu</p><p>16 <a title="hunch_net-2009-370" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>Introduction: Researchers are typically confronted with big problems that they have no idea how to solve.  In trying to come up with a solution, a natural approach is to decompose the big problem into a set of subproblems whose solution yields a solution to the larger problem.  This approach can go wrong in several ways. 
  
  Decomposition failure .  The solution to the decomposition does not in fact yield a solution to the overall problem. 
  Artificial hardness .  The subproblems created are sufficient if solved to solve the overall problem, but they are harder than necessary. 
  
As you can see, computational complexity forms a relatively new (in research-history) razor by which to judge an approach sufficient but not necessary.
 
In my experience, the artificial hardness problem is very common.  Many researchers abdicate the responsibility of choosing a problem to work on to other people.  This process starts very naturally as a graduate student, when an incoming student might have relatively l</p><p>17 <a title="hunch_net-2009-369" href="../hunch_net-2009/hunch_net-2009-08-27-New_York_Area_Machine_Learning_Events.html">hunch net-2009-08-27-New York Area Machine Learning Events</a></p>
<p>Introduction: Several events are happening in the NY area.
  
  Barriers in Computational Learning Theory Workshop, Aug 28.   That’s tomorrow near Princeton.  I’m looking forward to speaking at this one on “Getting around Barriers in Learning Theory”, but several other talks are of interest, particularly to the CS theory inclined. 
  Claudia Perlich  is running the  INFORMS Data Mining Contest  with a deadline of Sept. 25. This is a contest using real health record data (they partnered with  HealthCare Intelligence ) to predict transfers and mortality. In the current US health care reform debate, the case studies of high costs we hear strongly suggest machine learning & statistics can save many billions. 
  The Singularity Summit October 3&4 .  This is for the AIists out there.  Several of the talks look interesting, although unfortunately I’ll miss it for  ALT . 
  Predictive Analytics World, Oct 20-21 .  This is stretching the definition of “New York Area” a bit, but the train to DC is reasonable.</p><p>18 <a title="hunch_net-2009-368" href="../hunch_net-2009/hunch_net-2009-08-26-Another_10-year_paper_in_Machine_Learning.html">hunch net-2009-08-26-Another 10-year paper in Machine Learning</a></p>
<p>Introduction: When I was thinking about the best “10 year paper” for  ICML , I also took a look at a few other conferences.  Here is one from 10 years ago that interested me:
 
 David McAllester   PAC-Bayesian Model Averaging ,  COLT  1999.    2001 Journal Draft . 
 
Prior to this paper, the only mechanism known for controlling or estimating the necessary sample complexity for learning over continuously parameterized predictors was VC theory and variants, all of which suffered from a basic problem: they were incredibly pessimistic in practice.  This meant that only very gross guidance could be provided for learning algorithm design.  The PAC-Bayes bound provided an alternative approach to sample complexity bounds which was radically tighter, quantitatively.  It also imported and explained many of the motivations for Bayesian learning in a way that learning theory and perhaps optimization people might appreciate.  Since this paper came out, there have been a number of moderately successful attempts t</p><p>19 <a title="hunch_net-2009-367" href="../hunch_net-2009/hunch_net-2009-08-16-Centmail_comments.html">hunch net-2009-08-16-Centmail comments</a></p>
<p>Introduction: Centmail  is a scheme which makes charity donations have a secondary value, as a stamp for email.  When discussed on  newscientist ,  slashdot , and others, some of the comments make the academic review process appear thoughtful   .  Some prominent fallacies are:
  
 Costing money fallacy.  Some commenters appear to believe the system charges money per email.  Instead, the basic idea is that users get an extra benefit from donations to a charity and participation is strictly voluntary.  The solution to this fallacy is simply reading  the details . 
 Single solution fallacy.  Some commenters seem to think this is proposed as a complete solution to spam, and since not everyone will opt to participate, it won’t work.  But a complete solution is not at all necessary or even possible given the  flag-day problem .  Deployed machine learning systems for fighting spam are great at taking advantage of a partial solution.  The solution to this fallacy is learning about machine learning.  In the</p><p>20 <a title="hunch_net-2009-366" href="../hunch_net-2009/hunch_net-2009-08-03-Carbon_in_Computer_Science_Research.html">hunch net-2009-08-03-Carbon in Computer Science Research</a></p>
<p>Introduction: Al Gore ‘s  film  and gradually more assertive and thorough science has managed to mostly shift the debate on climate change from “Is it happening?” to “What should be done?”  In that context, it’s worthwhile to think a bit about what can be done within computer science research.
 
There are two things we can think about:
  
  Doing Research  At a cartoon level, computer science research consists of some combination of commuting to&from; work, writing programs, running them on computers, writing papers, and presenting them at conferences.  A typical computer has a power usage on the order of 100 Watts, which works out to 2.4 kiloWatt-hours/day.  Looking up  David MacKay ‘s  reference on power usage per person , it becomes clear that this is a relatively minor part of the lifestyle, although it could become substantial if many more computers are required.  Much larger costs are associated with commuting (which is in common with many people) and attending conferences.  Since local commuti</p><p>21 <a title="hunch_net-2009-365" href="../hunch_net-2009/hunch_net-2009-07-31-Vowpal_Wabbit_Open_Source_Project.html">hunch net-2009-07-31-Vowpal Wabbit Open Source Project</a></p>
<p>22 <a title="hunch_net-2009-364" href="../hunch_net-2009/hunch_net-2009-07-11-Interesting_papers_at_KDD.html">hunch net-2009-07-11-Interesting papers at KDD</a></p>
<p>23 <a title="hunch_net-2009-363" href="../hunch_net-2009/hunch_net-2009-07-09-The_Machine_Learning_Forum.html">hunch net-2009-07-09-The Machine Learning Forum</a></p>
<p>24 <a title="hunch_net-2009-362" href="../hunch_net-2009/hunch_net-2009-06-26-Netflix_nearly_done.html">hunch net-2009-06-26-Netflix nearly done</a></p>
<p>25 <a title="hunch_net-2009-361" href="../hunch_net-2009/hunch_net-2009-06-24-Interesting_papers_at_UAICMOLT_2009.html">hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</a></p>
<p>26 <a title="hunch_net-2009-360" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>27 <a title="hunch_net-2009-359" href="../hunch_net-2009/hunch_net-2009-06-03-Functionally_defined_Nonlinear_Dynamic_Models.html">hunch net-2009-06-03-Functionally defined Nonlinear Dynamic Models</a></p>
<p>28 <a title="hunch_net-2009-358" href="../hunch_net-2009/hunch_net-2009-06-01-Multitask_Poisoning.html">hunch net-2009-06-01-Multitask Poisoning</a></p>
<p>29 <a title="hunch_net-2009-357" href="../hunch_net-2009/hunch_net-2009-05-30-Many_ways_to_Learn_this_summer.html">hunch net-2009-05-30-Many ways to Learn this summer</a></p>
<p>30 <a title="hunch_net-2009-356" href="../hunch_net-2009/hunch_net-2009-05-24-2009_ICML_discussion_site.html">hunch net-2009-05-24-2009 ICML discussion site</a></p>
<p>31 <a title="hunch_net-2009-355" href="../hunch_net-2009/hunch_net-2009-05-19-CI_Fellows.html">hunch net-2009-05-19-CI Fellows</a></p>
<p>32 <a title="hunch_net-2009-354" href="../hunch_net-2009/hunch_net-2009-05-17-Server_Update.html">hunch net-2009-05-17-Server Update</a></p>
<p>33 <a title="hunch_net-2009-353" href="../hunch_net-2009/hunch_net-2009-05-08-Computability_in_Artificial_Intelligence.html">hunch net-2009-05-08-Computability in Artificial Intelligence</a></p>
<p>34 <a title="hunch_net-2009-352" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">hunch net-2009-05-06-Machine Learning to AI</a></p>
<p>35 <a title="hunch_net-2009-351" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>36 <a title="hunch_net-2009-350" href="../hunch_net-2009/hunch_net-2009-04-23-Jonathan_Chang_at_Slycoder.html">hunch net-2009-04-23-Jonathan Chang at Slycoder</a></p>
<p>37 <a title="hunch_net-2009-349" href="../hunch_net-2009/hunch_net-2009-04-21-Interesting_Presentations_at_Snowbird.html">hunch net-2009-04-21-Interesting Presentations at Snowbird</a></p>
<p>38 <a title="hunch_net-2009-348" href="../hunch_net-2009/hunch_net-2009-04-02-Asymmophobia.html">hunch net-2009-04-02-Asymmophobia</a></p>
<p>39 <a title="hunch_net-2009-347" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>40 <a title="hunch_net-2009-346" href="../hunch_net-2009/hunch_net-2009-03-18-Parallel_ML_primitives.html">hunch net-2009-03-18-Parallel ML primitives</a></p>
<p>41 <a title="hunch_net-2009-345" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">hunch net-2009-03-08-Prediction Science</a></p>
<p>42 <a title="hunch_net-2009-344" href="../hunch_net-2009/hunch_net-2009-02-22-Effective_Research_Funding.html">hunch net-2009-02-22-Effective Research Funding</a></p>
<p>43 <a title="hunch_net-2009-343" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>44 <a title="hunch_net-2009-342" href="../hunch_net-2009/hunch_net-2009-02-16-KDNuggets.html">hunch net-2009-02-16-KDNuggets</a></p>
<p>45 <a title="hunch_net-2009-341" href="../hunch_net-2009/hunch_net-2009-02-04-Optimal_Proxy_Loss_for_Classification.html">hunch net-2009-02-04-Optimal Proxy Loss for Classification</a></p>
<p>46 <a title="hunch_net-2009-340" href="../hunch_net-2009/hunch_net-2009-01-28-Nielsen%26%238217%3Bs_talk.html">hunch net-2009-01-28-Nielsen&#8217;s talk</a></p>
<p>47 <a title="hunch_net-2009-339" href="../hunch_net-2009/hunch_net-2009-01-27-Key_Scientific_Challenges.html">hunch net-2009-01-27-Key Scientific Challenges</a></p>
<p>48 <a title="hunch_net-2009-338" href="../hunch_net-2009/hunch_net-2009-01-23-An_Active_Learning_Survey.html">hunch net-2009-01-23-An Active Learning Survey</a></p>
<p>49 <a title="hunch_net-2009-337" href="../hunch_net-2009/hunch_net-2009-01-21-Nearly_all_natural_problems_require_nonlinearity.html">hunch net-2009-01-21-Nearly all natural problems require nonlinearity</a></p>
<p>50 <a title="hunch_net-2009-336" href="../hunch_net-2009/hunch_net-2009-01-19-Netflix_prize_within_epsilon.html">hunch net-2009-01-19-Netflix prize within epsilon</a></p>
<p>51 <a title="hunch_net-2009-335" href="../hunch_net-2009/hunch_net-2009-01-08-Predictive_Analytics_World.html">hunch net-2009-01-08-Predictive Analytics World</a></p>
<p>52 <a title="hunch_net-2009-334" href="../hunch_net-2009/hunch_net-2009-01-07-Interesting_Papers_at_SODA_2009.html">hunch net-2009-01-07-Interesting Papers at SODA 2009</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
