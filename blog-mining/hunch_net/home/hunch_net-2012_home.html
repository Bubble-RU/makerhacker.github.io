<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>hunch_net 2012 knowledge graph</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2012" href="#">hunch_net-2012</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>hunch_net 2012 knowledge graph</h1>
<br/><h3>similar blogs computed by tfidf model</h3><br/><h3>similar blogs computed by <a title="lsi-model" href="./hunch_net_lsi.html">lsi model</a></h3><br/><h3>similar blogs computed by <a title="lda-model" href="./hunch_net_lda.html">lda model</a></h3><br/><h2>blogs list:</h2><p>1 <a title="hunch_net-2012-476" href="../hunch_net-2012/hunch_net-2012-12-29-Simons_Institute_Big_Data_Program.html">hunch net-2012-12-29-Simons Institute Big Data Program</a></p>
<p>Introduction: Michael Jordansends the below:The newSimons Institute for the Theory of
Computingwill begin organizing semester-long programs starting in 2013.One of
our first programs, set for Fall 2013, will be on the "Theoretical
Foundationsof Big Data Analysis". The organizers of this program are Michael
Jordan (chair),Stephen Boyd, Peter Buehlmann, Ravi Kannan, Michael Mahoney,
and Muthu
Muthukrishnan.Seehttp://simons.berkeley.edu/program_bigdata2013.htmlfor more
information onthe program.The Simons Institute has created a number of
"Research Fellowships" for youngresearchers (within at most six years of the
award of their PhD) who wish toparticipate in Institute programs, including
the Big Data program. Individualswho already hold postdoctoral positions or
who are junior faculty are welcometo apply, as are finishing PhDs.Please note
that the application deadline is January 15, 2013. Further detailsare
available athttp://simons.berkeley.edu/fellows.html.Mike Jordan</p><p>2 <a title="hunch_net-2012-475" href="../hunch_net-2012/hunch_net-2012-10-26-ML_Symposium_and_Strata-Hadoop_World.html">hunch net-2012-10-26-ML Symposium and Strata-Hadoop World</a></p>
<p>Introduction: TheNew York ML symposiumwas last Friday. There were 303 registrations, up a
bit fromlast year. I particularly enjoyed talks byBill Freemanon vision and
ML,Jon Lenchneron strategy in Jeopardy, andTara N. Sainathand Brian Kingsbury
ondeep learning for speech recognition. If anyone has suggestions or thoughts
for next year, please speak up.I also attendedStrata + Hadoop Worldfor the
first time. This is primarily a trade conference rather than an academic
conference, but I found it pretty interesting as a first time attendee. This
is ground zero for theBig databuzzword, and I see now why. It's about data,
and the word "big" is so ambiguous that everyone can lay claim to it. There
were essentially zero academic talks. Instead, the focus was on war stories,
product announcements, and education. The general level of education is much
lower--explaining Machine Learning to the SQL educated is the primary
operating point. Nevertheless that's happening, and the fact that machine
learning is consi</p><p>3 <a title="hunch_net-2012-474" href="../hunch_net-2012/hunch_net-2012-10-18-7th_Annual_Machine_Learning_Symposium.html">hunch net-2012-10-18-7th Annual Machine Learning Symposium</a></p>
<p>Introduction: A reminder that theNew York Academy of Scienceswill be hosting the7th Annual
Machine Learning Symposiumtomorrow from 9:30am.The main program will feature
invited talks fromPeter Bartlett,William Freeman, andVladimir Vapnik, along
with numerous spotlight talks and a poster session. Following the main
program,hackNYandMicrosoft Researchare sponsoring a networking hour with talks
from machine learning practitioners at NYC startups
(specificallybit.ly,Buzzfeed,Chartbeat, andSense Networks,Visual Revenue).
This should be of great interest to everyone considering working in machine
learning.</p><p>4 <a title="hunch_net-2012-473" href="../hunch_net-2012/hunch_net-2012-09-29-Vowpal_Wabbit%2C_version_7.0.html">hunch net-2012-09-29-Vowpal Wabbit, version 7.0</a></p>
<p>Introduction: A new version ofVWisout. The primary changes are:Learning Reductions: I've
wanted to getlearning reductionsworking and we've finally done it. Not
everything is implemented yet, but VW now supports direct:Multiclass
Classification-oaaor-ect.Cost Sensitive Multiclass Classification-csoaaor-
wap.Contextual Bandit Classification-cb.Sequential Structured Prediction-
searnor-daggerIn addition, it is now easy to build your own custom learning
reductions for various plausible uses: feature diddling, custom structured
prediction problems, or alternate learning reductions. This effort is far from
done, but it is now in a generally useful state. Note that all learning
reductions inherit the ability to do cluster parallel learning.Library
interface: VW now has a basic library interface. The library provides most of
the functionality of VW, with the limitation that it is monolithic and
nonreentrant. These will be improved over time.Windows port: The priority of a
windows port jumped way up once we</p><p>5 <a title="hunch_net-2012-472" href="../hunch_net-2012/hunch_net-2012-08-27-NYAS_ML_2012_and_ICML_2013.html">hunch net-2012-08-27-NYAS ML 2012 and ICML 2013</a></p>
<p>Introduction: TheNew York Machine Learning Symposiumis October 19 with a 2 page abstract
deadline due September 13 via email with subject "Machine Learning Poster
Submission" sent to physicalscience@nyas.org. Everyone is welcome to submit.
Last year's attendance was 246 and I expect more this year.The primary
experiment forICML 2013is multiple paper submission deadlines with rolling
review cycles. The key dates are October 1, December 15, and February 15. This
is an attempt to shift ICML further towards a journal style review process and
reduce peak load. The "not for proceedings" experiment from this year's ICML
is not continuing.Edit: Fixed second ICML deadline.</p><p>6 <a title="hunch_net-2012-471" href="../hunch_net-2012/hunch_net-2012-08-24-Patterns_for_research_in_machine_learning.html">hunch net-2012-08-24-Patterns for research in machine learning</a></p>
<p>Introduction: There area handful of basic code patternsthat I wish I was more aware of when
I started research in machine learning. Each on its own may seem pointless,
but collectively they go a long way towards making the typical research
workflow more efficient. Here they are:Separate code from data.Separate input
data, working data and output data.Save everything to disk frequently.Separate
options from parameters.Do not use global variables.Record the options used to
generate each run of the algorithm.Make it easy to sweep options.Make it easy
to execute only portions of the code.Use checkpointing.Write demos and
tests.Clickherefor discussion and examples for each item. Also seeCharles
Sutton'sandHackerNews'thoughts on the same topic.My guess is that these
patterns will not only be useful for machine learning, but also any other
computational work that involves either a) processing large amounts of data,
or b) algorithms that take a significant amount of time to execute. Share this
list with you</p><p>7 <a title="hunch_net-2012-470" href="../hunch_net-2012/hunch_net-2012-07-17-MUCMD_and_BayLearn.html">hunch net-2012-07-17-MUCMD and BayLearn</a></p>
<p>Introduction: The workshop on theMeaningful Use of Complex Medical Datais happening again,
August 9-12 in LA, nearUAIon Catalina Island August 15-17. I enjoyed my visit
last year, and expect this year to be interesting also.The firstBay Area
Machine Learning Symposiumis August 30 atGoogle. Abstracts are due July 30.</p><p>8 <a title="hunch_net-2012-469" href="../hunch_net-2012/hunch_net-2012-07-09-Videolectures.html">hunch net-2012-07-09-Videolectures</a></p>
<p>Introduction: Yaserpoints out some nicelyvideotaped machine learning lecturesatCaltech.
Yaser taught me machine learning, and I always found the lectures clear and
interesting, so I expect many people can benefit from watching. Relative
toAndrew Ng'sML classthere are somewhat different areas of emphasis but the
topic is the same, so picking and choosing the union may be helpful.</p><p>9 <a title="hunch_net-2012-468" href="../hunch_net-2012/hunch_net-2012-06-29-ICML_survey_and_comments.html">hunch net-2012-06-29-ICML survey and comments</a></p>
<p>Introduction: Just about nothing could keep me from attendingICML, except forDorawho arrived
on Monday. Consequently, I have only secondhand reports that the conference is
going well.For those who are remote (like me) or after the conference (like
everyone),Mark Reidhas setup theICML discussionsite where you can comment on
any paper or subscribe to papers. Authors are automatically subscribed to
their own papers, so it should be possible to have a discussion significantly
after the fact, as people desire.We also conducted a survey before the
conference and have thesurvey resultsnow. This can be compared with theICML
2010 survey results. Looking at the comparable questions, we can sometimes
order the answers to have scores ranging from 0 to 3 or 0 to 4 with 3 or 4
being best and 0 worst, then compute the average difference between 2012 and
2010.Glancing through them, I see:Most people found the papers they reviewed a
good fit for their expertise (-.037 w.r.t 2010). Achieving this was one of our
subgo</p><p>10 <a title="hunch_net-2012-467" href="../hunch_net-2012/hunch_net-2012-06-15-Normal_Deviate_and_the_UCSC_Machine_Learning_Summer_School.html">hunch net-2012-06-15-Normal Deviate and the UCSC Machine Learning Summer School</a></p>
<p>Introduction: Larry Wassermanhas started theNormal Deviateblog which I added to the blogroll
on the right.Manfred Warmuthpoints out theUCSC machine learning summer
schoolrunning July 9-20 which may be of particular interest to those in
silicon valley.</p><p>11 <a title="hunch_net-2012-466" href="../hunch_net-2012/hunch_net-2012-06-05-ICML_acceptance_statistics.html">hunch net-2012-06-05-ICML acceptance statistics</a></p>
<p>Introduction: People are naturally interested in slicing the ICML acceptance statistics in
various ways. Here's a rundown for the top categories.18/66 = 0.27in
(0.18,0.36)Reinforcement Learning10/52 = 0.19in (0.17,0.37)Supervised
Learning9/51 = 0.18not in (0.18, 0.37)Clustering12/46 = 0.26in (0.17,
0.37)Kernel Methods11/40 = 0.28in (0.15, 0.4)Optimization Algorithms8/33 =
0.24in (0.15, 0.39)Learning Theory14/33 = 0.42not in (0.15, 0.39)Graphical
Models10/32 = 0.31in (0.15, 0.41)Applications (+5 invited)8/29 = 0.28in (0.14,
0.41])Probabilistic Models13/29 = 0.45not in (0.14, 0.41)NN & Deep
Learning8/26 = 0.31in (0.12, 0.42)Transfer and Multi-Task Learning13/25 =
0.52not in (0.12, 0.44)Online Learning5/25 = 0.20in (0.12, 0.44)Active
Learning6/22 = 0.27in (0.14, 0.41)Semi-Supervised Learning7/20 = 0.35in (0.1,
0.45)Statistical Methods4/20 = 0.20in (0.1, 0.45)Sparsity and Compressed
Sensing1/19 = 0.05not in (0.11, 0.42)Ensemble Methods5/18 = 0.28in (0.11,
0.44)Structured Output Prediction4/18 = 0.22in (</p><p>12 <a title="hunch_net-2012-465" href="../hunch_net-2012/hunch_net-2012-05-12-ICML_accepted_papers_and_early_registration.html">hunch net-2012-05-12-ICML accepted papers and early registration</a></p>
<p>Introduction: Theaccepted papersare up in full detail. We are still struggling with the
precise program itself, but that's coming along. Also note theMay 13deadline
forearly registrationand room booking.</p><p>13 <a title="hunch_net-2012-464" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>Introduction: Yahoo! laid off people. Unlike every previous time there have been layoffs,
this is serious forYahoo! Research.We had advanced warning
fromPrabhakarthrough thesimple act of leaving. Yahoo! Research was a world
class organization that Prabhakar recruited much of personally, so it is
deeply implausible that he would spontaneously decide to leave. My first
thought when I saw the news was "Uhoh,Robsaid that he knew it was serious when
the head of ATnT Research left." In this case it was even more significant,
because Prabhakar recruited me on the premise that Y!R was an experiment in
how research should be done: via a combination of high quality people and high
engagement with the company. Prabhakar's departure is a clear end to that
experiment.The result is ambiguous from a business perspective. Y!R clearly
was not capable of saving the company from its illnesses. I'm not privy to the
internal accounting of impact and this is the kind of subject where there can
easily be great disagreemen</p><p>14 <a title="hunch_net-2012-463" href="../hunch_net-2012/hunch_net-2012-05-02-ICML%3A_Behind_the_Scenes.html">hunch net-2012-05-02-ICML: Behind the Scenes</a></p>
<p>Introduction: This is a rather long post, detailing the ICML 2012 review process. The goal
is to make the process more transparent, help authors understand how we came
to a decision, and discuss the strengths and weaknesses of this process for
future conference organizers.Microsoft’s Conference Management Toolkit (CMT)We
chose to useCMTover other conference management software mainly because of its
rich toolkit. The interface is sub-optimal (to say the least!) but it has
extensive capabilities (to handle bids, author response, resubmissions, etc.),
good import/export mechanisms (to process the data elsewhere), excellent
technical support (to answer late night emails, add new functionalities).
Overall, it was the right choice, although we hope a designer will look at
that interface sometime soon!Toronto Matching System (TMS)TMSis now being used
by many major conferences in our field (including NIPS and UAI). It is an
automated system (developed byLaurent CharlinandRich Zemelat U. Toronto) to
match re</p><p>15 <a title="hunch_net-2012-462" href="../hunch_net-2012/hunch_net-2012-04-20-Both_new%3A_STOC_workshops_and_NEML.html">hunch net-2012-04-20-Both new: STOC workshops and NEML</a></p>
<p>Introduction: May 16 in Cambridge, is theNew England Machine Learning Day, a first regional
workshop/symposium on machine learning. To present a poster, submit an
abstract byMay 5.May 19 in New York,STOCis coming to town and rather
surprisingly havingworkshopswhich should be quite a bit of fun. I'll be
speaking atAlgorithms for Distributed and Streaming Data.</p><p>16 <a title="hunch_net-2012-461" href="../hunch_net-2012/hunch_net-2012-04-09-ICML_author_feedback_is_open.html">hunch net-2012-04-09-ICML author feedback is open</a></p>
<p>Introduction: as of last night, late.When the reviewing deadline passed Wednesday night 15%
of reviews were still missing, much higher than I expected. Between late
reviews coming in, ACs working overtime through the weekend, and people
willing to help in the pinch another ~390 reviews came in, reducing the
missing mass to 0.2%. Nailing that last bit and a similar quantity of papers
with uniformly low confidence reviews is what remains to be done in terms of
basic reviews. We are trying to make all of those happen this week so authors
have some chance to respond.I was surprised by the quantity of late reviews,
and I think that's an area where ICML needs to improve in future years. Good
reviews are not done in a rush--they are done by setting aside time (like an
afternoon), and carefully reading the paper while thinking about implications.
Many reviewers do this well but a significant minority aren't good at
scheduling their personal time. In this situation there are several ways to
fail:Give early w</p><p>17 <a title="hunch_net-2012-460" href="../hunch_net-2012/hunch_net-2012-03-24-David_Waltz.html">hunch net-2012-03-24-David Waltz</a></p>
<p>Introduction: hasdied. He lived a full life. I know him personally as a founder of theCenter
for Computational Learning Systemsand theNew York Machine Learning Symposium,
both of which have sheltered and promoted the advancement of machine learning.
I expect much of the New York area machine learning community will miss him,
as well as many others around the world.</p><p>18 <a title="hunch_net-2012-459" href="../hunch_net-2012/hunch_net-2012-03-13-The_Submodularity_workshop_and_Lucca_Professorship.html">hunch net-2012-03-13-The Submodularity workshop and Lucca Professorship</a></p>
<p>Introduction: Ninapoints out theSubmodularity WorkshopMarch 19-20next week atGeorgia Tech.
Many people want to make Submodularity the new Convexity in machine learning,
and it certainly seems worth exploring.Sara Olsonalso points out atenured
faculty positionatIMT Luccawith a deadline ofMay 15th. Lucca happens to be the
ancestral home of 1/4 of my heritage</p><p>19 <a title="hunch_net-2012-458" href="../hunch_net-2012/hunch_net-2012-03-06-COLT-ICML_Open_Questions_and_ICML_Instructions.html">hunch net-2012-03-06-COLT-ICML Open Questions and ICML Instructions</a></p>
<p>Introduction: Sashais theopen problemschair for bothCOLTandICML. Open problems will be
presented in a joint session in the evening of the COLT/ICML overlap day. COLT
has a history of open sessions, but this is new for ICML. If you have a
difficult theoretically definable problem in machine learning, consider
submitting it for review,due March 16. You'll benefit three ways:The effort of
writing down a precise formulation of what you want often helps you understand
the nature of the problem.Your problem will be officially published and
citable.You might have it solved by some very intelligent bored people.The
general idea could easily be applied to any problem which can be crisply
stated with an easily verifiable solution, and we may consider expanding this
in later years, but for this year all problems need to be of a theoretical
variety.Joelleand I (andMahdi, andLaurent) finished an initial assignment
ofProgram CommitteeandArea Chairsto papers. We'll be updatinginstructions for
the PCand ACsas we fi</p><p>20 <a title="hunch_net-2012-457" href="../hunch_net-2012/hunch_net-2012-02-29-Key_Scientific_Challenges_and_the_Franklin_Symposium.html">hunch net-2012-02-29-Key Scientific Challenges and the Franklin Symposium</a></p>
<p>Introduction: For graduate students, theYahoo!Key Scientific Challenges programincluding
inmachine learningis on again,due March 9. The application is easy and the $5K
award is high quality "no strings attached" funding. Consider submitting.Those
in Washington DC, Philadelphia, and New York, may consider attending
theFranklin Institute SymposiumApril 25which has several speakers and an award
forV. Attendance is free with an RSVP.</p><p>21 <a title="hunch_net-2012-456" href="../hunch_net-2012/hunch_net-2012-02-24-ICML%2B50%25.html">hunch net-2012-02-24-ICML+50%</a></p>
<p>22 <a title="hunch_net-2012-455" href="../hunch_net-2012/hunch_net-2012-02-20-Berkeley_Streaming_Data_Workshop.html">hunch net-2012-02-20-Berkeley Streaming Data Workshop</a></p>
<p>23 <a title="hunch_net-2012-454" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>24 <a title="hunch_net-2012-453" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">hunch net-2012-01-28-Why COLT?</a></p>
<p>25 <a title="hunch_net-2012-452" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
