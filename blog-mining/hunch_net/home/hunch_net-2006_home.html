<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>hunch_net 2006 knowledge graph</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2006" href="#">hunch_net-2006</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>hunch_net 2006 knowledge graph</h1>
<br/><h3>similar blogs computed by tfidf model</h3><br/><h3>similar blogs computed by <a title="lsi-model" href="./hunch_net_lsi.html">lsi model</a></h3><br/><h3>similar blogs computed by <a title="lda-model" href="./hunch_net_lda.html">lda model</a></h3><br/><h2>blogs list:</h2><p>1 <a title="hunch_net-2006-224" href="../hunch_net-2006/hunch_net-2006-12-12-Interesting_Papers_at_NIPS_2006.html">hunch net-2006-12-12-Interesting Papers at NIPS 2006</a></p>
<p>Introduction: Here are some papers that I found surprisingly interesting.
  
  Yoshua Bengio , Pascal Lamblin, Dan Popovici, Hugo Larochelle,  Greedy Layer-wise Training of Deep Networks . Empirically investigates some of the design choices behind deep belief networks.
 
  Long Zhu , Yuanhao Chen,  Alan Yuille  Unsupervised Learning of a Probabilistic Grammar for Object Detection and Parsing.  An unsupervised method for detecting objects using simple feature filters that works remarkably well on the (supervised)  caltech-101 dataset . 
  Shai Ben-David ,  John Blitzer ,  Koby Crammer , and  Fernando Pereira ,  Analysis of Representations for Domain Adaptation .  This is the first analysis I’ve seen of learning with respect to samples drawn differently from the evaluation distribution which depends on reasonable measurable quantities. 
  
All of these papers turn out to have a common theme—the power of unlabeled data to do generically useful things.</p><p>2 <a title="hunch_net-2006-223" href="../hunch_net-2006/hunch_net-2006-12-06-The_Spam_Problem.html">hunch net-2006-12-06-The Spam Problem</a></p>
<p>Introduction: The  New York Times  has an article on the  growth of spam .  Interesting facts include: 9/10 of all email is spam, spam source identification is nearly useless due to botnet spam senders, and image based spam (emails which consist of an image only) are on the growth.
 
Estimates of the cost of spam are almost certainly far to low, because they do not account for the cost in time lost by people.
 
The image based spam which is currently penetrating many filters should be catchable with a more sophisticated application of machine learning technology.  For the spam I see, the rendered images come in only a few formats, which would be easy to recognize via a support vector machine (with RBF kernel), neural network, or even nearest-neighbor architecture.  The mechanics of setting this up to run efficiently is the only real challenge.  This is the next step in the spam war.
 
The response to this system is to make the image based spam even more random.  We should (essentially) expect to see</p><p>3 <a title="hunch_net-2006-222" href="../hunch_net-2006/hunch_net-2006-12-05-Recruitment_Conferences.html">hunch net-2006-12-05-Recruitment Conferences</a></p>
<p>Introduction: One of the subsidiary roles of conferences is recruitment.   NIPS  is optimally placed in time for this because it falls right before the major recruitment season.
 
I personally found job hunting embarrassing, and was relatively inept at it.  I expect this is true of many people, because it is not something done often. 
 
The basic rule is: make the plausible hirers aware of your interest.  Any  corporate sponsor  is  a “plausible”, regardless of whether or not there is a booth.   CRA  and the  acm job center  are other reasonable sources.
 
There are substantial differences between the different possibilities.  Putting some effort into understanding the distinctions is a good idea, although you should always remember where the other person is coming from.</p><p>4 <a title="hunch_net-2006-221" href="../hunch_net-2006/hunch_net-2006-12-04-Structural_Problems_in_NIPS_Decision_Making.html">hunch net-2006-12-04-Structural Problems in NIPS Decision Making</a></p>
<p>Introduction: This is a very difficult post to write, because it is about a perenially touchy subject.  Nevertheless, it is an important one which needs to be thought about carefully.
 
There are a few things which should be understood:
  
 The system is changing and responsive.  We-the-authors are we-the-reviewers, we-the-PC, and even we-the-NIPS-board.  NIPS has implemented ‘secondary program chairs’, ‘author response’, and ‘double blind reviewing’ in the last few years to help with the decision process, and more changes may happen in the future. 
 Agreement creates a perception of correctness.  When any PC meets and makes a group decision about a paper, there is a strong tendency for the reinforcement inherent in a group decision to create the perception of correctness.  For the many people who have been on the NIPS PC it’s reasonable to entertain a healthy skepticism in the face of this reinforcing certainty. 
 This post is about structural problems.  What problems arise because of the structure</p><p>5 <a title="hunch_net-2006-220" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>Introduction: This post is about a general technique for problem solving which I’ve never seen taught (in full generality), but which I’ve found very useful.
 
Many problems in computer science turn out to be discretely difficult.  The best known version of such problems are NP-hard problems, but I mean ‘discretely difficult’ in a much more general way, which I only know how to capture by examples.
  
  ERM  In empirical risk minimization, you choose a minimum error rate classifier from a set of classifiers.  This is NP hard for common sets, but it can be much harder, depending on the set. 
  Experts  In the online learning with experts setting, you try to predict well so as to compete with a set of (adversarial) experts.  Here the alternating quantifiers of you and an adversary playing out a game can yield a dynamic programming problem that grows exponentially. 
  Policy Iteration  The problem with policy iteration is that you learn a new policy with respect to an old policy, which implies that sim</p><p>6 <a title="hunch_net-2006-219" href="../hunch_net-2006/hunch_net-2006-11-22-Explicit_Randomization_in_Learning_algorithms.html">hunch net-2006-11-22-Explicit Randomization in Learning algorithms</a></p>
<p>Introduction: There are a number of learning algorithms which explicitly incorporate randomness into their execution.  This includes at amongst others:
  
 Neural Networks.  Neural networks use randomization to assign initial weights. 
 Boltzmann Machines/ Deep Belief Networks . Boltzmann machines are something like a stochastic version of multinode logistic regression.  The use of randomness is more essential in Boltzmann machines, because the predicted value at test time also uses randomness. 
 Bagging.  Bagging is a process where a learning algorithm is run several different times on several different datasets, creating a final predictor which makes a majority vote. 
 Policy descent.  Several algorithms in reinforcement learning such as  Conservative Policy Iteration  use random bits to create stochastic policies. 
 Experts algorithms.  Randomized weighted majority use random bits as a part of the prediction process to achieve better theoretical guarantees. 
  
A basic question is: “Should there</p><p>7 <a title="hunch_net-2006-218" href="../hunch_net-2006/hunch_net-2006-11-20-Context_and_the_calculation_misperception.html">hunch net-2006-11-20-Context and the calculation misperception</a></p>
<p>Introduction: This post is really for people  not  in machine learning (or related fields).  It is about a common misperception which affects people who have not thought about the process of trying to predict somethinng.  Hopefully, by precisely stating it, we can remove it.
 
Suppose we have a set of events, each described by a vector of features.
  
 
 0 
 1 
 0 
 1 
 1 
 
 
 1 
 0 
 1 
 0 
 1 
 
 
 1 
 1 
 0 
 1 
 0 
 
 
 0 
 0 
 1 
 1 
 1 
 
 
 1 
 1 
 0 
 0 
 1 
 
 
 1 
 0 
 0 
 0 
 1 
 
 
 0 
 1 
 1 
 1 
 0 
 
  
Suppose we want to predict the value of the first feature given the others.  One approach is to bin the data by  one  feature.  For the above example, we might partition the data according to feature 2, then observe that when feature 2 is 0 the label (feature 1) is mostly 1. On the other hand, when feature 2 is 1, the label (feature 1) is mostly 0.  Using this simple rule we get an observed error rate of 3/7.  
 
There are two issues here.   The first is that this is really a training</p><p>8 <a title="hunch_net-2006-217" href="../hunch_net-2006/hunch_net-2006-11-06-Data_Linkage_Problems.html">hunch net-2006-11-06-Data Linkage Problems</a></p>
<p>Introduction: Data linkage is a problem which seems to come up in various applied machine learning problems.  I have heard it mentioned in various data mining contexts, but it seems relatively less studied for systemic reasons.
 
A very simple version of the data linkage problem is a cross hospital patient record merge.  Suppose a patient (John Doe) is admitted to a hospital (General Health), treated, and released.  Later, John Doe is admitted to a second hospital (Health General), treated, and released.  Given a large number of records of this sort, it becomes very tempting to try and predict the outcomes of treatments.  This is reasonably straightforward as a machine learning problem if there is a shared unique identifier for John Doe used by General Health and Health General along with time stamps.  We can merge the records and create examples of the form “Given symptoms and treatment, did the patient come back to a hospital within the next year?”  These examples could be fed into a learning algo</p><p>9 <a title="hunch_net-2006-216" href="../hunch_net-2006/hunch_net-2006-11-02-2006_NIPS_workshops.html">hunch net-2006-11-02-2006 NIPS workshops</a></p>
<p>Introduction: I expect the  NIPS 2006 workshops  to be quite interesting, and recommend going for anyone interested in machine learning research.  (Most or all of the workshops webpages can be found two links deep.)</p><p>10 <a title="hunch_net-2006-215" href="../hunch_net-2006/hunch_net-2006-10-22-Exemplar_programming.html">hunch net-2006-10-22-Exemplar programming</a></p>
<p>Introduction: There are many different abstractions for problem definition and solution.  Here are a few examples:
  
 Functional programming: a set of functions are defined.  The composed execution of these functions yields the solution. 
 Linear programming: a set of constraints and a linear objective function are defined.  An LP solver finds the constrained optimum. 
 Quadratic programming: Like linear programming, but the language is a little more flexible (and the solution slower). 
 Convex programming: like quadratic programming, but the language is more flexible (and the solutions even slower). 
 Dynamic programming: a recursive definition of the problem is defined and then solved efficiently via caching tricks. 
 SAT programming: A problem is specified as a satisfiability involving a conjunction of a disjunction of boolean variables.  A general engine attempts to find a good satisfying assignment.  For example  Kautz’s   blackbox  planner. 
  
These abstractions have different tradeoffs betw</p><p>11 <a title="hunch_net-2006-214" href="../hunch_net-2006/hunch_net-2006-10-13-David_Pennock_starts_Oddhead.html">hunch net-2006-10-13-David Pennock starts Oddhead</a></p>
<p>Introduction: his blog on information markets and other research topics .</p><p>12 <a title="hunch_net-2006-213" href="../hunch_net-2006/hunch_net-2006-10-08-Incompatibilities_between_classical_confidence_intervals_and_learning..html">hunch net-2006-10-08-Incompatibilities between classical confidence intervals and learning.</a></p>
<p>Introduction: Classical confidence intervals satisfy a theorem of the form: For some data sources  D , 
  Pr S ~ D (f(D) > g(S)) > 1-d   
where  f  is some function of the distribution (such as the mean) and  g  is some function of the observed sample  S .   The constraints on  D  can vary between “Independent and identically distributed (IID) samples from a gaussian with an unknown mean” to “IID samples from an arbitrary distribution  D “.  There are even some confidence intervals which do not require IID samples.
 
Classical confidence intervals often confuse people.  They do  not  say “with high probability, for my observed sample, the bounds holds”.   Instead, they tell you that if you reason according to the confidence interval in the future (and the constraints on  D  are satisfied), then you are not often wrong.  Restated, they tell you something about what a safe procedure is in a stochastic world where  d  is the safety parameter.
 
There are a number of results in theoretical machine learn</p><p>13 <a title="hunch_net-2006-212" href="../hunch_net-2006/hunch_net-2006-10-04-Health_of_Conferences_Wiki.html">hunch net-2006-10-04-Health of Conferences Wiki</a></p>
<p>Introduction: Aaron Hertzmann  points out the  health of conferences wiki , which has a great deal of information about how many different conferences function.</p><p>14 <a title="hunch_net-2006-211" href="../hunch_net-2006/hunch_net-2006-10-02-%241M_Netflix_prediction_contest.html">hunch net-2006-10-02-$1M Netflix prediction contest</a></p>
<p>Introduction: Netflix is  running a contest  to improve recommender prediction systems.   A 10% improvement over their current system yields a $1M prize.  Failing that, the best smaller improvement yields a smaller $50K prize.  This contest looks quite real, and the $50K prize money is almost certainly achievable with a bit of thought.  The contest also comes with a dataset which is apparently 2 orders of magnitude larger than any other public recommendation system datasets.</p><p>15 <a title="hunch_net-2006-210" href="../hunch_net-2006/hunch_net-2006-09-28-Programming_Languages_for_Machine_Learning_Implementations.html">hunch net-2006-09-28-Programming Languages for Machine Learning Implementations</a></p>
<p>Introduction: Machine learning algorithms have a much better chance of being widely adopted if they are implemented in some easy-to-use code.  There are several important concerns associated with machine learning which stress programming languages on the ease-of-use vs. speed frontier.
  
  Speed   The rate at which data sources are growing seems to be outstripping the rate at which computational power is growing, so it is important that we be able to eak out every bit of computational power.  Garbage collected languages ( java ,  ocaml ,  perl  and  python ) often have several issues here.
 
 Garbage collection often implies that floating point numbers are “boxed”: every float is represented by a pointer to a float.  Boxing can cause an order of magnitude slowdown because an extra nonlocalized memory reference is made, and accesses to main memory can are many CPU cycles long. 
 Garbage collection often implies that considerably more memory is used than is necessary.   This has a variable effect.  I</p><p>16 <a title="hunch_net-2006-209" href="../hunch_net-2006/hunch_net-2006-09-19-Luis_von_Ahn_is_awarded_a_MacArthur_fellowship..html">hunch net-2006-09-19-Luis von Ahn is awarded a MacArthur fellowship.</a></p>
<p>Introduction: For  his  work on the subject of human computation including  ESPGame ,  Peekaboom , and  Phetch .  The  new MacArthur fellows .</p><p>17 <a title="hunch_net-2006-208" href="../hunch_net-2006/hunch_net-2006-09-18-What_is_missing_for_online_collaborative_research%3F.html">hunch net-2006-09-18-What is missing for online collaborative research?</a></p>
<p>Introduction: The internet has recently made the research process much smoother: papers are easy to obtain, citations are easy to follow, and unpublished “tutorials” are often available. Yet, new research fields can look very complicated to outsiders or newcomers. Every paper is like a small piece of an unfinished jigsaw puzzle: to understand just one publication, a researcher without experience in the field will typically have to follow several layers of citations, and many of the papers he encounters have a great deal of repeated information. Furthermore, from one publication to the next, notation and terminology may not be consistent which can further confuse the reader.
 
But the internet is now proving to be an extremely useful medium for collaboration and knowledge aggregation. Online forums allow users to ask and answer questions and to share ideas. The recent phenomenon of Wikipedia provides a proof-of-concept for the “anyone can edit” system. Can such models be used to facilitate research a</p><p>18 <a title="hunch_net-2006-207" href="../hunch_net-2006/hunch_net-2006-09-12-Incentive_Compatible_Reviewing.html">hunch net-2006-09-12-Incentive Compatible Reviewing</a></p>
<p>Introduction: Reviewing is a fairly formal process which is integral to the way academia is run.  Given this integral nature, the quality of reviewing is often frustrating.  I’ve seen plenty of examples of false statements, misbeliefs, reading what isn’t written, etc…, and I’m sure many other people have as well.  
 
Recently, mechanisms like double blind review and author feedback have been introduced to try to make the process more fair and accurate in many machine learning (and related) conferences.  My personal experience is that these mechanisms help, especially the author feedback.   Nevertheless, some problems remain.  
 
The game theory take on reviewing is that the incentive for truthful reviewing isn’t there.  Since reviewers are also authors, there are sometimes perverse incentives created and acted upon.  (Incidentially, these incentives can be both positive and negative.)  
 
Setting up a truthful reviewing system is tricky because their is no final reference truth available in any acce</p><p>19 <a title="hunch_net-2006-206" href="../hunch_net-2006/hunch_net-2006-09-09-How_to_solve_an_NP_hard_problem_in_quadratic_time.html">hunch net-2006-09-09-How to solve an NP hard problem in quadratic time</a></p>
<p>Introduction: This title is a lie, but it is a special lie which has a bit of truth.
 
If  n  players each play each other, you have a tournament.   How do you order the players from weakest to strongest?
 
The standard first attempt is “find the ordering which agrees with the tournament on as many player pairs as possible”.  This is called the “minimum feedback arcset” problem in the CS theory literature and it is a well known NP-hard problem.  A basic guarantee holds for the solution to this problem: if there is some “true” intrinsic ordering, and the outcome of the tournament disagrees  k  times (due to noise for instance), then the output ordering will disagree with the original ordering on at most  2k  edges (and no solution can be better).
 
One standard approach to tractably solving an NP-hard problem is to find another algorithm with an approximation guarantee.  For example,  Don Coppersmith ,  Lisa Fleischer  and  Atri Rudra  proved that  ordering players according to the number of wins is</p><p>20 <a title="hunch_net-2006-205" href="../hunch_net-2006/hunch_net-2006-09-07-Objective_and_subjective_interpretations_of_probability.html">hunch net-2006-09-07-Objective and subjective interpretations of probability</a></p>
<p>Introduction: An amusing tidbit (reproduced without permission) from Herman Chernoff’s delightful monograph, “Sequential analysis and optimal design”:
 
The use of randomization raises a philosophical question which is articulated by the following probably apocryphal anecdote.
 
The metallurgist told his friend the statistician how he planned to test the effect of heat on the strength of a metal bar by sawing the bar into six pieces. The first two would go into the hot oven, the next two into the medium oven, and the last two into the cool oven. The statistician, horrified, explained how he should randomize to avoid the effect of a possible gradient of strength in the metal bar. The method of randomization was applied, and it turned out that the randomized experiment called for putting the first two pieces into the hot oven, the next two into the medium oven, and the last two into the cool oven. “Obviously, we can’t do that,” said the metallurgist. “On the contrary, you have to do that,” said the st</p><p>21 <a title="hunch_net-2006-204" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>22 <a title="hunch_net-2006-203" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>23 <a title="hunch_net-2006-202" href="../hunch_net-2006/hunch_net-2006-08-10-Precision_is_not_accuracy.html">hunch net-2006-08-10-Precision is not accuracy</a></p>
<p>24 <a title="hunch_net-2006-201" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">hunch net-2006-08-07-The Call of the Deep</a></p>
<p>25 <a title="hunch_net-2006-200" href="../hunch_net-2006/hunch_net-2006-08-03-AOL%26%238217%3Bs_data_drop.html">hunch net-2006-08-03-AOL&#8217;s data drop</a></p>
<p>26 <a title="hunch_net-2006-199" href="../hunch_net-2006/hunch_net-2006-07-26-Two_more_UAI_papers_of_interest.html">hunch net-2006-07-26-Two more UAI papers of interest</a></p>
<p>27 <a title="hunch_net-2006-198" href="../hunch_net-2006/hunch_net-2006-07-25-Upcoming_conference.html">hunch net-2006-07-25-Upcoming conference</a></p>
<p>28 <a title="hunch_net-2006-197" href="../hunch_net-2006/hunch_net-2006-07-17-A_Winner.html">hunch net-2006-07-17-A Winner</a></p>
<p>29 <a title="hunch_net-2006-196" href="../hunch_net-2006/hunch_net-2006-07-13-Regression_vs._Classification_as_a_Primitive.html">hunch net-2006-07-13-Regression vs. Classification as a Primitive</a></p>
<p>30 <a title="hunch_net-2006-195" href="../hunch_net-2006/hunch_net-2006-07-12-Who_is_having_visa_problems_reaching_US_conferences%3F.html">hunch net-2006-07-12-Who is having visa problems reaching US conferences?</a></p>
<p>31 <a title="hunch_net-2006-194" href="../hunch_net-2006/hunch_net-2006-07-11-New_Models.html">hunch net-2006-07-11-New Models</a></p>
<p>32 <a title="hunch_net-2006-193" href="../hunch_net-2006/hunch_net-2006-07-09-The_Stock_Prediction_Machine_Learning_Problem.html">hunch net-2006-07-09-The Stock Prediction Machine Learning Problem</a></p>
<p>33 <a title="hunch_net-2006-192" href="../hunch_net-2006/hunch_net-2006-07-08-Some_recent_papers.html">hunch net-2006-07-08-Some recent papers</a></p>
<p>34 <a title="hunch_net-2006-191" href="../hunch_net-2006/hunch_net-2006-07-08-MaxEnt_contradicts_Bayes_Rule%3F.html">hunch net-2006-07-08-MaxEnt contradicts Bayes Rule?</a></p>
<p>35 <a title="hunch_net-2006-190" href="../hunch_net-2006/hunch_net-2006-07-06-Branch_Prediction_Competition.html">hunch net-2006-07-06-Branch Prediction Competition</a></p>
<p>36 <a title="hunch_net-2006-189" href="../hunch_net-2006/hunch_net-2006-07-05-more_icml_papers.html">hunch net-2006-07-05-more icml papers</a></p>
<p>37 <a title="hunch_net-2006-188" href="../hunch_net-2006/hunch_net-2006-06-30-ICML_papers.html">hunch net-2006-06-30-ICML papers</a></p>
<p>38 <a title="hunch_net-2006-187" href="../hunch_net-2006/hunch_net-2006-06-25-Presentation_of_Proofs_is_Hard..html">hunch net-2006-06-25-Presentation of Proofs is Hard.</a></p>
<p>39 <a title="hunch_net-2006-186" href="../hunch_net-2006/hunch_net-2006-06-24-Online_convex_optimization_at_COLT.html">hunch net-2006-06-24-Online convex optimization at COLT</a></p>
<p>40 <a title="hunch_net-2006-185" href="../hunch_net-2006/hunch_net-2006-06-16-Regularization_%3D_Robustness.html">hunch net-2006-06-16-Regularization = Robustness</a></p>
<p>41 <a title="hunch_net-2006-184" href="../hunch_net-2006/hunch_net-2006-06-15-IJCAI_is_out_of_season.html">hunch net-2006-06-15-IJCAI is out of season</a></p>
<p>42 <a title="hunch_net-2006-183" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>43 <a title="hunch_net-2006-182" href="../hunch_net-2006/hunch_net-2006-06-05-Server_Shift%2C_Site_Tweaks%2C_Suggestions%3F.html">hunch net-2006-06-05-Server Shift, Site Tweaks, Suggestions?</a></p>
<p>44 <a title="hunch_net-2006-181" href="../hunch_net-2006/hunch_net-2006-05-23-What_is_the_best_regret_transform_reduction_from_multiclass_to_binary%3F.html">hunch net-2006-05-23-What is the best regret transform reduction from multiclass to binary?</a></p>
<p>45 <a title="hunch_net-2006-180" href="../hunch_net-2006/hunch_net-2006-05-21-NIPS_paper_evaluation_criteria.html">hunch net-2006-05-21-NIPS paper evaluation criteria</a></p>
<p>46 <a title="hunch_net-2006-179" href="../hunch_net-2006/hunch_net-2006-05-16-The_value_of_the_orthodox_view_of_Boosting.html">hunch net-2006-05-16-The value of the orthodox view of Boosting</a></p>
<p>47 <a title="hunch_net-2006-178" href="../hunch_net-2006/hunch_net-2006-05-08-Big_machine_learning.html">hunch net-2006-05-08-Big machine learning</a></p>
<p>48 <a title="hunch_net-2006-177" href="../hunch_net-2006/hunch_net-2006-05-05-An_ICML_reject.html">hunch net-2006-05-05-An ICML reject</a></p>
<p>49 <a title="hunch_net-2006-176" href="../hunch_net-2006/hunch_net-2006-05-01-A_conversation_between_Theo_and_Pat.html">hunch net-2006-05-01-A conversation between Theo and Pat</a></p>
<p>50 <a title="hunch_net-2006-175" href="../hunch_net-2006/hunch_net-2006-04-30-John_Langford_%26%238211%3B%3E_Yahoo_Research%2C_NY.html">hunch net-2006-04-30-John Langford &#8211;> Yahoo Research, NY</a></p>
<p>51 <a title="hunch_net-2006-174" href="../hunch_net-2006/hunch_net-2006-04-27-Conferences%2C_Workshops%2C_and_Tutorials.html">hunch net-2006-04-27-Conferences, Workshops, and Tutorials</a></p>
<p>52 <a title="hunch_net-2006-173" href="../hunch_net-2006/hunch_net-2006-04-17-Rexa_is_live.html">hunch net-2006-04-17-Rexa is live</a></p>
<p>53 <a title="hunch_net-2006-172" href="../hunch_net-2006/hunch_net-2006-04-14-JMLR_is_a_success.html">hunch net-2006-04-14-JMLR is a success</a></p>
<p>54 <a title="hunch_net-2006-171" href="../hunch_net-2006/hunch_net-2006-04-09-Progress_in_Machine_Translation.html">hunch net-2006-04-09-Progress in Machine Translation</a></p>
<p>55 <a title="hunch_net-2006-170" href="../hunch_net-2006/hunch_net-2006-04-06-Bounds_greater_than_1.html">hunch net-2006-04-06-Bounds greater than 1</a></p>
<p>56 <a title="hunch_net-2006-169" href="../hunch_net-2006/hunch_net-2006-04-05-What_is_state%3F.html">hunch net-2006-04-05-What is state?</a></p>
<p>57 <a title="hunch_net-2006-168" href="../hunch_net-2006/hunch_net-2006-04-02-Mad_%28Neuro%29science.html">hunch net-2006-04-02-Mad (Neuro)science</a></p>
<p>58 <a title="hunch_net-2006-167" href="../hunch_net-2006/hunch_net-2006-03-27-Gradients_everywhere.html">hunch net-2006-03-27-Gradients everywhere</a></p>
<p>59 <a title="hunch_net-2006-166" href="../hunch_net-2006/hunch_net-2006-03-24-NLPers.html">hunch net-2006-03-24-NLPers</a></p>
<p>60 <a title="hunch_net-2006-165" href="../hunch_net-2006/hunch_net-2006-03-23-The_Approximation_Argument.html">hunch net-2006-03-23-The Approximation Argument</a></p>
<p>61 <a title="hunch_net-2006-164" href="../hunch_net-2006/hunch_net-2006-03-17-Multitask_learning_is_Black-Boxable.html">hunch net-2006-03-17-Multitask learning is Black-Boxable</a></p>
<p>62 <a title="hunch_net-2006-163" href="../hunch_net-2006/hunch_net-2006-03-12-Online_learning_or_online_preservation_of_learning%3F.html">hunch net-2006-03-12-Online learning or online preservation of learning?</a></p>
<p>63 <a title="hunch_net-2006-162" href="../hunch_net-2006/hunch_net-2006-03-09-Use_of_Notation.html">hunch net-2006-03-09-Use of Notation</a></p>
<p>64 <a title="hunch_net-2006-161" href="../hunch_net-2006/hunch_net-2006-03-05-%26%238220%3BStructural%26%238221%3B_Learning.html">hunch net-2006-03-05-&#8220;Structural&#8221; Learning</a></p>
<p>65 <a title="hunch_net-2006-160" href="../hunch_net-2006/hunch_net-2006-03-02-Why_do_people_count_for_learning%3F.html">hunch net-2006-03-02-Why do people count for learning?</a></p>
<p>66 <a title="hunch_net-2006-159" href="../hunch_net-2006/hunch_net-2006-02-27-The_Peekaboom_Dataset.html">hunch net-2006-02-27-The Peekaboom Dataset</a></p>
<p>67 <a title="hunch_net-2006-158" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>68 <a title="hunch_net-2006-157" href="../hunch_net-2006/hunch_net-2006-02-18-Multiplication_of_Learned_Probabilities_is_Dangerous.html">hunch net-2006-02-18-Multiplication of Learned Probabilities is Dangerous</a></p>
<p>69 <a title="hunch_net-2006-156" href="../hunch_net-2006/hunch_net-2006-02-11-Yahoo%26%238217%3Bs_Learning_Problems..html">hunch net-2006-02-11-Yahoo&#8217;s Learning Problems.</a></p>
<p>70 <a title="hunch_net-2006-155" href="../hunch_net-2006/hunch_net-2006-02-07-Pittsburgh_Mind_Reading_Competition.html">hunch net-2006-02-07-Pittsburgh Mind Reading Competition</a></p>
<p>71 <a title="hunch_net-2006-154" href="../hunch_net-2006/hunch_net-2006-02-04-Research_Budget_Changes.html">hunch net-2006-02-04-Research Budget Changes</a></p>
<p>72 <a title="hunch_net-2006-153" href="../hunch_net-2006/hunch_net-2006-02-02-Introspectionism_as_a_Disease.html">hunch net-2006-02-02-Introspectionism as a Disease</a></p>
<p>73 <a title="hunch_net-2006-152" href="../hunch_net-2006/hunch_net-2006-01-30-Should_the_Input_Representation_be_a_Vector%3F.html">hunch net-2006-01-30-Should the Input Representation be a Vector?</a></p>
<p>74 <a title="hunch_net-2006-151" href="../hunch_net-2006/hunch_net-2006-01-25-1_year.html">hunch net-2006-01-25-1 year</a></p>
<p>75 <a title="hunch_net-2006-150" href="../hunch_net-2006/hunch_net-2006-01-23-On_Coding_via_Mutual_Information_%26%23038%3B_Bayes_Nets.html">hunch net-2006-01-23-On Coding via Mutual Information &#038; Bayes Nets</a></p>
<p>76 <a title="hunch_net-2006-149" href="../hunch_net-2006/hunch_net-2006-01-18-Is_Multitask_Learning_Black-Boxable%3F.html">hunch net-2006-01-18-Is Multitask Learning Black-Boxable?</a></p>
<p>77 <a title="hunch_net-2006-148" href="../hunch_net-2006/hunch_net-2006-01-13-Benchmarks_for_RL.html">hunch net-2006-01-13-Benchmarks for RL</a></p>
<p>78 <a title="hunch_net-2006-147" href="../hunch_net-2006/hunch_net-2006-01-08-Debugging_Your_Brain.html">hunch net-2006-01-08-Debugging Your Brain</a></p>
<p>79 <a title="hunch_net-2006-146" href="../hunch_net-2006/hunch_net-2006-01-06-MLTV.html">hunch net-2006-01-06-MLTV</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
