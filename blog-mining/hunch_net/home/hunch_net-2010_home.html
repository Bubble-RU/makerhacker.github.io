<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>hunch_net 2010 knowledge graph</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2010" href="#">hunch_net-2010</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>hunch_net 2010 knowledge graph</h1>
<br/><h3>similar blogs computed by tfidf model</h3><br/><h3>similar blogs computed by <a title="lsi-model" href="./hunch_net_lsi.html">lsi model</a></h3><br/><h3>similar blogs computed by <a title="lda-model" href="./hunch_net_lda.html">lda model</a></h3><br/><h2>blogs list:</h2><p>1 <a title="hunch_net-2010-420" href="../hunch_net-2010/hunch_net-2010-12-26-NIPS_2010.html">hunch net-2010-12-26-NIPS 2010</a></p>
<p>Introduction: I enjoyed attending  NIPS  this year, with several things interesting me.  For the conference itself:
  
  Peter Welinder ,  Steve  Branson ,  Serge Belongie , and  Pietro Perona ,  The Multidimensional Wisdom of Crowds .  This paper is about using  mechanical turk  to get label information, with results superior to a majority vote approach. 
  David McAllester ,  Tamir Hazan , and  Joseph Keshet   Direct Loss Minimization for Structured Prediction .  This is about another technique for directly optimizing the loss in structured prediction, with an application to speech recognition.  
  Mohammad Saberian  and  Nuno Vasconcelos   Boosting Classifier Cascades .  This is about an algorithm for simultaneously optimizing loss and computation in a classifier cascade construction.  There were several other papers on cascades which are worth looking at if interested. 
  Alan Fern  and  Prasad Tadepalli ,  A Computational Decision Theory for Interactive Assistants .  This paper carves out some</p><p>2 <a title="hunch_net-2010-419" href="../hunch_net-2010/hunch_net-2010-12-04-Vowpal_Wabbit%2C_version_5.0%2C_and_the_second_heresy.html">hunch net-2010-12-04-Vowpal Wabbit, version 5.0, and the second heresy</a></p>
<p>Introduction: I’ve released  version 5.0  of the  Vowpal Wabbit  online learning software.  The major number has changed since the  last release  because I regard all earlier versions as obsolete—there are several new algorithms & features including substantial changes and upgrades to the default learning algorithm.  
 
The biggest changes are new algorithms:
  
  Nikos  and I improved the default algorithm.  The basic update rule still uses gradient descent, but the size of the update is carefully controlled so that it’s impossible to overrun the label.  In addition, the normalization has changed.  Computationally, these changes are virtually free and yield better results, sometimes much better.  Less careful updates can be reenabled with –loss_function classic, although results are still not identical to previous due to normalization changes. 
 Nikos also implemented the per-feature learning rates as per these  two   papers .  Often, this works better than the default algorithm.  It isn’t the defa</p><p>3 <a title="hunch_net-2010-418" href="../hunch_net-2010/hunch_net-2010-12-02-Traffic_Prediction_Problem.html">hunch net-2010-12-02-Traffic Prediction Problem</a></p>
<p>Introduction: Slashdot  points out the  Traffic Prediction Challenge  which looks pretty fun.  The temporal aspect seems to be very common in many real-world problems and somewhat understudied.</p><p>4 <a title="hunch_net-2010-417" href="../hunch_net-2010/hunch_net-2010-11-18-ICML_2011_%26%238211%3B_Call_for_Tutorials.html">hunch net-2010-11-18-ICML 2011 &#8211; Call for Tutorials</a></p>
<p>Introduction: I would like to encourage people to consider giving a tutorial at next years ICML. The ideal tutorial attracts a wide audience, provides a gentle and easily taught introduction to the chosen research area, and also covers the most important contributions in depth.
 
Submissions are due January 14 Â (about two weeks before paper deadline). 
 http://www.icml-2011.org/tutorials.php 
 
Regards, 
Ulf</p><p>5 <a title="hunch_net-2010-416" href="../hunch_net-2010/hunch_net-2010-10-29-To_Vidoelecture_or_not.html">hunch net-2010-10-29-To Vidoelecture or not</a></p>
<p>Introduction: (update:  cross-posted  on  CACM )
 
For the first time in several years,  ICML 2010  did not have  videolectures  attending.  Luckily, the  tutorial on exploration and learning  which  Alina  and I put together can  be viewed , since we also presented at  KDD 2010 , which included videolecture support. 
 
ICML didn’t cover the cost of a videolecture, because  PASCAL  didn’t provide a grant for it this year.  On the other hand, KDD covered it out of registration costs.  The cost of videolectures isn’t cheap.  For  a workshop  the baseline quote we have is 270 euro per hour, plus a similar cost for the cameraman’s travel and accomodation.  This can be reduced substantially by having a volunteer with a camera handle the cameraman duties, uploading the video and slides to be processed for a quoted 216 euro per hour.
 
 Youtube  is the most predominant free video site with a cost of $0, but it turns out to be a poor alternative.   15 minute upload limits  do not match typical talk lengths.</p><p>6 <a title="hunch_net-2010-415" href="../hunch_net-2010/hunch_net-2010-10-28-NY_ML_Symposium_2010.html">hunch net-2010-10-28-NY ML Symposium 2010</a></p>
<p>Introduction: About 200 people attended the  2010 NYAS ML Symposium  this year.  (It was  about 170 last year .)  I particularly enjoyed several talks.
  
  Yann  has a new live demo of (limited) real-time object recognition learning.  
  Sanjoy  gave a fairly convincing and comprehensible explanation of why a  modified form of single-linkage clustering  is consistent in higher dimensions, and why consistency is a critical feature for clustering algorithms.  I’m curious how well this algorithm works in practice. 
  Matt Hoffman ‘s poster covering online LDA seemed pretty convincing to me as an algorithmic improvement. 
  
This year, we allocated more time towards posters & poster spotlights.  
 
For next year, we are considering some further changes.  The format has traditionally been 4 invited Professor speakers, with posters and poster spotlight for students.  Demand from other parties to participate is growing, for example from postdocs and startups in the area.  Another growing concern is the fa</p><p>7 <a title="hunch_net-2010-414" href="../hunch_net-2010/hunch_net-2010-10-17-Partha_Niyogi_has_died.html">hunch net-2010-10-17-Partha Niyogi has died</a></p>
<p>Introduction: from brain cancer.  I asked  Misha  who worked with him to write about it. 
  
Partha Niyogi, Louis Block Professor in Computer Science and Statistics at the University of Chicago passed away on October 1, 2010, aged 43. 
 
I first met Partha Niyogi almost exactly ten years ago when I was a graduate student in math and he had just started as a faculty in Computer Science and Statistics at the University of Chicago. Strangely, we first talked at length due to a somewhat convoluted mathematical argument in a paper on pattern recognition. I asked him some questions about the paper, and, even though the topic was new to him, he had put serious thought into it and we started regular meetings. We made significant progress and developed a line of research stemming initially just from trying to understand that one paper and to simplify one derivation. I think this was typical of Partha, showing both his intellectual curiosity and his intuition for the serendipitous; having a sense and focus fo</p><p>8 <a title="hunch_net-2010-413" href="../hunch_net-2010/hunch_net-2010-10-08-An_easy_proof_of_the_Chernoff-Hoeffding_bound.html">hunch net-2010-10-08-An easy proof of the Chernoff-Hoeffding bound</a></p>
<p>Introduction: Textbooks invariably seem to carry the proof that uses Markov’s inequality, moment-generating functions, and Taylor approximations. Here’s an easier way.
 
For  , let   be the KL divergence between a coin of bias   and one of bias  :  
 
 Theorem:  Suppose you do   independent tosses of a coin of bias  . The probability of seeing   heads or more, for  , is at most  . So is the probability of seeing   heads or less, for  .
 
 Remark:  By Pinsker’s inequality,  .
 
 Proof  Let’s do the   case; the other is identical.
 
Let   be the distribution over   induced by a coin of bias  , and likewise   for a coin of bias  . Let   be the set of all sequences of   tosses which contain   heads or more. We’d like to show that   is unlikely under  .
 
Pick any  , with say   heads. Then: 
 
 
Since   for every  , we have   and we’re done.</p><p>9 <a title="hunch_net-2010-412" href="../hunch_net-2010/hunch_net-2010-09-28-Machined_Learnings.html">hunch net-2010-09-28-Machined Learnings</a></p>
<p>Introduction: Paul Mineiro  has started  Machined Learnings  where heâ&euro;&trade;s seriously attempting to do ML research in public.  I personally need to read through in greater detail, as much of it is learning reduction related, trying to deal with the sorts of complex source problems that come up in practice.</p><p>10 <a title="hunch_net-2010-411" href="../hunch_net-2010/hunch_net-2010-09-21-Regretting_the_dead.html">hunch net-2010-09-21-Regretting the dead</a></p>
<p>Introduction: Nikos  pointed out this  new york times  article about  poor clinical design killing people .  For those of us who study learning from exploration information this is a reminder that low regret algorithms are particularly important, as regret in clinical trials is measured by patient deaths.
 
Two obvious improvements on the experimental design are:
  
 With reasonable record keeping of existing outcomes for the standard treatments, there is no need to explicitly assign people to a control group with the standard treatment, as that approach is effectively explored with great certainty.  Asserting otherwise would imply that the nature of effective treatments for cancer has changed between now and a year ago, which denies the value of any clinical trial. 
 An optimal experimental design will smoothly phase between exploration and exploitation as evidence for a new treatment shows that it can be effective.  This is old tech, for example in the  EXP3.P algorithm (page 12 aka 59)  although</p><p>11 <a title="hunch_net-2010-410" href="../hunch_net-2010/hunch_net-2010-09-17-New_York_Area_Machine_Learning_Events.html">hunch net-2010-09-17-New York Area Machine Learning Events</a></p>
<p>Introduction: On Sept 21, there is another  machine learning meetup  where I’ll be speaking.  Although the topic is contextual bandits, I think of it as “the future of machine learning”.  In particular, it’s all about how to learn in an interactive environment, such as for ad display, trading, news recommendation, etc…
 
On Sept 24, abstracts for the  New York Machine Learning Symposium  are due.  This is the largest Machine Learning event in the area, so it’s a great way to have a conversation with other people.
 
On Oct 22, the NY ML Symposium actually happens.  This year, we are expanding the spotlights, and trying to have more time for posters.  In addition, we have a strong set of invited speakers:  David Blei ,  Sanjoy Dasgupta ,  Tommi Jaakkola , and  Yann LeCun .  After the meeting, a late  hackNY  related event is planned where students and startups can meet.
 
I’d also like to point out the related  CS/Econ symposium  as I have interests there as well.</p><p>12 <a title="hunch_net-2010-409" href="../hunch_net-2010/hunch_net-2010-09-13-AIStats.html">hunch net-2010-09-13-AIStats</a></p>
<p>Introduction: Geoff Gordon  points out  AIStats 2011  in Ft. Lauderdale, Florida.  The  call for papers  is now out, due Nov. 1.  The plan is to  experiment with the review process  to encourage quality in several ways.  I expect to submit a paper and would encourage others with good research to do likewise.</p><p>13 <a title="hunch_net-2010-408" href="../hunch_net-2010/hunch_net-2010-08-24-Alex_Smola_starts_a_blog.html">hunch net-2010-08-24-Alex Smola starts a blog</a></p>
<p>Introduction: Adventures in Data Land .</p><p>14 <a title="hunch_net-2010-407" href="../hunch_net-2010/hunch_net-2010-08-23-Boosted_Decision_Trees_for_Deep_Learning.html">hunch net-2010-08-23-Boosted Decision Trees for Deep Learning</a></p>
<p>Introduction: About 4 years ago, I speculated that  decision trees qualify as a deep learning algorithm  because they can make decisions which are substantially nonlinear in the input representation.   Ping Li  has  proved this correct, empirically  at  UAI  by showing that boosted decision trees can beat deep belief networks on versions of  Mnist  which are artificially hardened so as to make them solvable only by deep learning algorithms.  
 
This is an important point, because the ability to solve these sorts of problems is probably the best objective definition of a deep learning algorithm we have.   Iâ&euro;&trade;m not that surprised.  In my experience, if you can accept the computational drawbacks of a boosted decision tree, they can achieve pretty good performance.
 
 Geoff Hinton  once told me that the great thing about deep belief networks is that they work.  I understand that Ping had very substantial difficulty in getting this published, so I hope some reviewers step up to the standard of valuing wha</p><p>15 <a title="hunch_net-2010-406" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">hunch net-2010-08-22-KDD 2010</a></p>
<p>Introduction: There were several papers that seemed fairly interesting at  KDD this year .  The ones that caught my attention are:
  
  Xin Jin , Mingyang Zhang,  Nan Zhang , and  Gautam Das ,  Versatile Publishing For Privacy Preservation .  This paper provides a conservative method for safely determining which data is publishable from any complete source of information (for example, a hospital) such that it does not violate privacy rules in a natural language.  It is not differentially private, so no external sources of join information can exist.  However, it is a mechanism for  publishing  data rather than (say) the output of a learning algorithm. 
  Arik Friedman   Assaf Schuster ,  Data Mining with Differential Privacy .  This paper shows how to create effective differentially private decision trees.  Progress in differentially private datamining is pretty impressive, as it was  defined in 2006 . 
 David Chan, Rong Ge, Ori Gershony,  Tim Hesterberg ,  Diane Lambert ,  Evaluating Online Ad Camp</p><p>16 <a title="hunch_net-2010-405" href="../hunch_net-2010/hunch_net-2010-08-21-Rob_Schapire_at_NYC_ML_Meetup.html">hunch net-2010-08-21-Rob Schapire at NYC ML Meetup</a></p>
<p>Introduction: I’ve been wanting to attend the  NYC ML Meetup  for some time and hope to make it  next week on the 25th .   Rob Schapire  is talking about “Playing Repeated Games”, which in my experience is far more relevant to machine learning than the title might indicate.</p><p>17 <a title="hunch_net-2010-404" href="../hunch_net-2010/hunch_net-2010-08-20-The_Workshop_on_Cores%2C_Clusters%2C_and_Clouds.html">hunch net-2010-08-20-The Workshop on Cores, Clusters, and Clouds</a></p>
<p>Introduction: Alekh ,  John ,  Ofer , and I are organizing a  workshop  at  NIPS  this year on learning in parallel and distributed environments.  The general interest level in parallel learning seems to be growing rapidly, so I expect quite a bit of attendance.  Please join us if you are parallel-interested.
 
And, if you are working in the area of parallel learning, please consider  submitting an abstract  due Oct. 17 for presentation at the workshop.</p><p>18 <a title="hunch_net-2010-403" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>Introduction: The papers which interested me most at  ICML  and  COLT  2010 were:
  
  Thomas Walsh ,  Kaushik Subramanian ,  Michael Littman  and  Carlos Diuk   Generalizing Apprenticeship Learning across Hypothesis Classes .  This paper formalizes and provides algorithms with guarantees for mixed-mode apprenticeship and traditional reinforcement learning algorithms, allowing RL algorithms that perform better than for either setting alone. 
  István Szita  and  Csaba Szepesvári   Model-based reinforcement learning with nearly tight exploration complexity bounds .  This paper and  another represent the frontier of best-known algorithm for Reinforcement Learning in a Markov Decision Process. 
  James Martens   Deep learning via Hessian-free optimization .  About a new not-quite-online second order gradient algorithm for learning deep functional structures.  Potentially this is very powerful because while people have often talked about end-to-end learning, it has rarely worked in practice. 
  Chrisoph</p><p>19 <a title="hunch_net-2010-402" href="../hunch_net-2010/hunch_net-2010-07-02-MetaOptimize.html">hunch net-2010-07-02-MetaOptimize</a></p>
<p>Introduction: Joseph Turian  creates  MetaOptimize  for discussion of NLP and ML on big datasets.  This includes a  blog , but perhaps more importantly a  question and answer section .  Iâ&euro;&trade;m hopeful it will take off.</p><p>20 <a title="hunch_net-2010-401" href="../hunch_net-2010/hunch_net-2010-06-20-2010_ICML_discussion_site.html">hunch net-2010-06-20-2010 ICML discussion site</a></p>
<p>Introduction: A substantial difficulty with the 2009 and 2008  ICML discussion system  was a communication vacuum, where authors were not informed of comments, and commenters were not informed of responses to their comments without explicit monitoring.   Mark Reid  has setup a  new discussion system for 2010  with the goal of addressing this.
 
Mark didn’t want to make it to intrusive, so you must opt-in.  As an author,  find your paper  and “Subscribe by email” to the comments.  As a commenter, you have the option of providing an email for follow-up notification.</p><p>21 <a title="hunch_net-2010-400" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>22 <a title="hunch_net-2010-399" href="../hunch_net-2010/hunch_net-2010-05-20-Google_Predict.html">hunch net-2010-05-20-Google Predict</a></p>
<p>23 <a title="hunch_net-2010-398" href="../hunch_net-2010/hunch_net-2010-05-10-Aggregation_of_estimators%2C_sparsity_in_high_dimension_and_computational_feasibility.html">hunch net-2010-05-10-Aggregation of estimators, sparsity in high dimension and computational feasibility</a></p>
<p>24 <a title="hunch_net-2010-397" href="../hunch_net-2010/hunch_net-2010-05-02-What%26%238217%3Bs_the_difference_between_gambling_and_rewarding_good_prediction%3F.html">hunch net-2010-05-02-What&#8217;s the difference between gambling and rewarding good prediction?</a></p>
<p>25 <a title="hunch_net-2010-396" href="../hunch_net-2010/hunch_net-2010-04-28-CI_Fellows_program_renewed.html">hunch net-2010-04-28-CI Fellows program renewed</a></p>
<p>26 <a title="hunch_net-2010-395" href="../hunch_net-2010/hunch_net-2010-04-26-Compassionate_Reviewing.html">hunch net-2010-04-26-Compassionate Reviewing</a></p>
<p>27 <a title="hunch_net-2010-394" href="../hunch_net-2010/hunch_net-2010-04-24-COLT_Treasurer_is_now_Phil_Long.html">hunch net-2010-04-24-COLT Treasurer is now Phil Long</a></p>
<p>28 <a title="hunch_net-2010-393" href="../hunch_net-2010/hunch_net-2010-04-14-MLcomp%3A_a_website_for_objectively_comparing_ML_algorithms.html">hunch net-2010-04-14-MLcomp: a website for objectively comparing ML algorithms</a></p>
<p>29 <a title="hunch_net-2010-392" href="../hunch_net-2010/hunch_net-2010-03-26-A_Variance_only_Deviation_Bound.html">hunch net-2010-03-26-A Variance only Deviation Bound</a></p>
<p>30 <a title="hunch_net-2010-391" href="../hunch_net-2010/hunch_net-2010-03-15-The_Efficient_Robust_Conditional_Probability_Estimation_Problem.html">hunch net-2010-03-15-The Efficient Robust Conditional Probability Estimation Problem</a></p>
<p>31 <a title="hunch_net-2010-390" href="../hunch_net-2010/hunch_net-2010-03-12-Netflix_Challenge_2_Canceled.html">hunch net-2010-03-12-Netflix Challenge 2 Canceled</a></p>
<p>32 <a title="hunch_net-2010-389" href="../hunch_net-2010/hunch_net-2010-02-26-Yahoo%21_ML_events.html">hunch net-2010-02-26-Yahoo! ML events</a></p>
<p>33 <a title="hunch_net-2010-388" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>34 <a title="hunch_net-2010-387" href="../hunch_net-2010/hunch_net-2010-01-19-Deadline_Season%2C_2010.html">hunch net-2010-01-19-Deadline Season, 2010</a></p>
<p>35 <a title="hunch_net-2010-386" href="../hunch_net-2010/hunch_net-2010-01-13-Sam_Roweis_died.html">hunch net-2010-01-13-Sam Roweis died</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
