<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>hunch_net 2005 knowledge graph</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2005" href="#">hunch_net-2005</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>hunch_net 2005 knowledge graph</h1>
<br/><h3>similar blogs computed by tfidf model</h3><br/><h3>similar blogs computed by <a title="lsi-model" href="./hunch_net_lsi.html">lsi model</a></h3><br/><h3>similar blogs computed by <a title="lda-model" href="./hunch_net_lda.html">lda model</a></h3><br/><h2>blogs list:</h2><p>1 <a title="hunch_net-2005-145" href="../hunch_net-2005/hunch_net-2005-12-29-Deadline_Season.html">hunch net-2005-12-29-Deadline Season</a></p>
<p>Introduction: Many different paper deadlines are coming up soon so I made a little reference
table. Out of curiosity, I also computed the interval between submission
deadline and
conference.ConferenceLocationDateDeadlineintervalCOLTPittsburghJune
22-25January 21152ICMLPittsburghJune 26-28January 30/February 6140UAIMITJuly
13-16March 9/March 16119AAAIBostonJuly 16-20February
16/21145KDDPhiladelphiaAugust 23-26March 3/March 10166It looks like the
northeastern US is the big winner as far as location this year.</p><p>2 <a title="hunch_net-2005-144" href="../hunch_net-2005/hunch_net-2005-12-28-Yet_more_nips_thoughts.html">hunch net-2005-12-28-Yet more nips thoughts</a></p>
<p>Introduction: I only managed to make it out to the NIPS workshops this year soI'll give my
comments on what I saw there.The Learing and Robotics workshops lives again. I
hope itcontinues and gets more high quality papers in the future. Themost
interesting talk for me was Larry Jackel's on the LAGRprogram (see John's
previous post on said program). I got someideas as to what progress has been
made. Larry really explainedthe types of benchmarks and the tradeoffs that had
to be made tomake the goals achievable but challenging.Hal Daume gave a very
interesting talk about structuredprediction using RL techniques, something
near and dear to my ownheart. He achieved rather impressive results using only
a verygreedy search.The non-parametric Bayes workshop was great. I enjoyed the
entiremorning session I spent there, and particularly (the usuallydesultory)
discussion periods. One interesting topic was theGibbs/Variational inference
divide. I won't try to summarizeespecially as no conclusion was reached. It</p><p>3 <a title="hunch_net-2005-143" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">hunch net-2005-12-27-Automated Labeling</a></p>
<p>Introduction: One of the common trends in machine learning has been an emphasis on the use
of unlabeled data. The argument goes something like "there aren't many labeled
web pages out there, but there are ahugenumber of web pages, so we must find a
way to take advantage of them." There are several standard approaches for
doing this:Unsupervised Learning. You use only unlabeled data. In a typical
application, you cluster the data and hope that the clusters somehow
correspond to what you care about.Semisupervised Learning. You use both
unlabeled and labeled data to build a predictor. The unlabeled data influences
the learned predictor in some way.Active Learning. You have unlabeled data and
access to a labeling oracle. You interactively choose which examples to label
so as to optimize prediction accuracy.It seems there is a fourth approach
worth serious investigation--automated labeling. The approach goes as
follows:Identify some subset of observed values to predict from the
others.Build a predictor.U</p><p>4 <a title="hunch_net-2005-142" href="../hunch_net-2005/hunch_net-2005-12-22-Yes_%2C_I_am_applying.html">hunch net-2005-12-22-Yes , I am applying</a></p>
<p>Introduction: Every year about now hundreds of applicants apply for a research/teaching job
with the timing governed by the university recruitment schedule. This time,
it's my turn--the hat's in the ring, I am a contender, etcâ&euro;Ś What I have heard
is that this year is good in both directions--both an increased supply and an
increased demand for machine learning expertise.I consider this post a bit of
an abuse as it is neither about general research nor machine learning. Please
forgive me this once.My hope is that I will learn about new places interested
in funding basic research--it's easy to imagine that I have overlooked
possibilities.I am not dogmatic about where I end up in any particular way.
Several earlier posts detail what I think of as a good research environment,
so I will avoid a repeat. A few more details seem important:Application. There
is often a tension between basic research and immediate application. This
tension is not as strong as might be expected in my case. As evidence, many of</p><p>5 <a title="hunch_net-2005-141" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>Introduction: Founding a successful new conference is extraordinarily difficult. As a
conference founder, you must manage to attract a significant number of good
papers--enough to entice the participants into participating next year and to
(generally) to grow the conference. For someone choosing to participate in a
new conference, there is a very significant decision to make: do you send a
paper to some new conference with no guarantee that the conference will work
out? Or do you send it to another (possibly less related) conference that you
are sure will work?The conference founding problem is a joint agreement
problem with a very significant barrier. Workshops are a way around this
problem, and workshops attached to conferences are a particularly effective
means for this. A workshop at a conference is sure to have people available to
speak and attend and is sure to have a large audience available. Presenting
work at a workshop is not generally exclusive: it can also be presented at a
conference. F</p><p>6 <a title="hunch_net-2005-140" href="../hunch_net-2005/hunch_net-2005-12-14-More_NIPS_Papers_II.html">hunch net-2005-12-14-More NIPS Papers II</a></p>
<p>Introduction: I thought this was a very good NIPS with many excellent papers. The following
are a few NIPS papers which I liked and I hope to study more carefully when I
get the chance. The list is not exhaustive and in no particular
order…Preconditioner Approximations for Probabilistic Graphical
Models.Pradeeep Ravikumar and John Lafferty.I thought the use of
preconditioner methods from solving linear systems in the context of
approximate inference was novel and interesting. The results look good and I'd
like to understand the limitations.Rodeo: Sparse nonparametric regression in
high dimensions.John Lafferty and Larry Wasserman.A very interesting approach
to feature selection in nonparametric regression from a frequentist framework.
The use of lengthscale variables in each dimension reminds me a lot of
'Automatic Relevance Determination' in Gaussian process regression -- it would
be interesting to compare Rodeo to ARD in GPs.Interpolating between types and
tokens by estimating power law generators</p><p>7 <a title="hunch_net-2005-139" href="../hunch_net-2005/hunch_net-2005-12-11-More_NIPS_Papers.html">hunch net-2005-12-11-More NIPS Papers</a></p>
<p>Introduction: Let me add to John's post with a few of my own favouritesfrom this year's
conference. First, let me say thatSanjoy's talk,Coarse Sample Complexity
Bounds for ActiveLearningwas also one of my favourites, as was theForgettron
paper.I also really enjoyed the last third ofChristos'talkon the complexity of
finding Nash equilibria.And, speaking of tagging, I thinkthe U.Mass Citeseer
replacement systemRexafrom the demo track is very cool.Finally, let me add my
recommendations for specific papers:Z. Ghahramani, K. Heller:Bayesian Sets[no
preprint](A very elegant probabilistic information retrieval style modelof
which objects are "most like" a given subset of objects.)T. Griffiths, Z.
Ghahramani:Infinite Latent Feature Models andthe Indian Buffet
Process[preprint](A Dirichlet style prior over infinite binary matrices
withbeautiful exchangeability properties.)K. Weinberger, J. Blitzer, L.
Saul:Distance Metric Learning forLarge Margin Nearest Neighbor
Classification[preprint](A nice idea about ho</p><p>8 <a title="hunch_net-2005-138" href="../hunch_net-2005/hunch_net-2005-12-09-Some_NIPS_papers.html">hunch net-2005-12-09-Some NIPS papers</a></p>
<p>Introduction: Here is a set of papers that I found interesting (and why).A PAC-Bayes
approach to the Set Covering Machineimproves the set covering machine. The set
covering machine approach is a new way to do classification characterized by a
very close connection between theory and algorithm. At this point, the
approach seems to be competing well with SVMs in about all dimensions: similar
computational speed, similar accuracy, stronger learning theory guarantees,
more general information source (a kernel has strictly more structure than a
metric), and more sparsity. Developing a classification algorithm is not very
easy, but the results so far are encouraging.Off-Road Obstacle Avoidance
through End-to-End LearningandLearning Depth from Single Monocular Imagesboth
effectively showed that depth information can be predicted from camera images
(using notably different techniques). This ability is strongly enabling
because cameras are cheap, tiny, light, and potentially provider longer range
distance in</p><p>9 <a title="hunch_net-2005-137" href="../hunch_net-2005/hunch_net-2005-12-09-Machine_Learning_Thoughts.html">hunch net-2005-12-09-Machine Learning Thoughts</a></p>
<p>Introduction: I added a link to Olivier Bousquet'smachine learning thoughtsblog. Several of
the posts may be of interest.</p><p>10 <a title="hunch_net-2005-136" href="../hunch_net-2005/hunch_net-2005-12-07-Is_the_Google_way_the_way_for_machine_learning%3F.html">hunch net-2005-12-07-Is the Google way the way for machine learning?</a></p>
<p>Introduction: Urs HoelzlefromGooglegave an invited presentation atNIPS. In the presentation,
he strongly advocates interacting with data in a particular scalable manner
which is something like the following:Make a cluster of machines.Build a
unified filesystem. (Google uses GFS, but NFS or other approaches work
reasonably well for smaller clusters.)Interact with data viaMapReduce.Creating
a cluster of machines is, by this point, relatively straightforward.Unified
filesystems are a little bit tricky--GFS is capable by design of essentially
unlimited speed throughput to disk. NFS can bottleneck because all of the data
has to move through one machine. Nevertheless, this may not be a limiting
factor for smaller clusters.MapReduce is a programming paradigm. Essentially,
it is a combination of a data element transform (map) and an
agreggator/selector (reduce). These operations are highly parallelizable and
the claim is that they support the forms of data interaction which are
necessary.Apparently, theNutc</p><p>11 <a title="hunch_net-2005-135" href="../hunch_net-2005/hunch_net-2005-12-04-Watchword%3A_model.html">hunch net-2005-12-04-Watchword: model</a></p>
<p>Introduction: In everyday use a model is a system which explains the behavior of some
system, hopefully at the level where some alteration of the model predicts
some alteration of the real-world system. In machine learning "model" has
several variant definitions.Everyday. The common definition is sometimes
used.Parameterized. Sometimes model is a short-hand for "parameterized model".
Here, it refers to a model with unspecified free parameters. In the Bayesian
learning approach, you typically have a prior over (everyday)
models.Predictive. Even further from everyday use is the predictive model.
Examples of this are "my model is a decision tree" or "my model is a support
vector machine". Here, there is no real sense in which an SVM explains the
underlying process. For example, an SVM tells us nothing in particular about
how alterations to the real-world system would create a change.Which
definition is being used at any particular time is important information. For
example, if it's a parameterized or p</p><p>12 <a title="hunch_net-2005-134" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">hunch net-2005-12-01-The Webscience Future</a></p>
<p>Introduction: The internet has significantly effected the way we do research but it's
capabilities have not yet been fully realized.First, let's acknowledge some
known effects.Self-publishingBy default, all researchers in machine learning
(and more generally computer science and physics) place their papers online
for anyone to download. The exact mechanism differs--physicists tend to use a
central repository (Arxiv) while computer scientists tend to place the papers
on their webpage. Arxiv has been slowly growing in subject breadth so it now
sometimes used by computer scientists.CollaborationEmail has enabled working
remotely with coauthors. This has allowed collaborationis which would not
otherwise have been possible and generally speeds research.Now, let's look at
attempts to go further.Blogs(like this one) allow public discussion about
topics which are not easily categorized as "a new idea in machine learning"
(like this topic).Organizationof some subfield of research. This
includesSatinder Singh</p><p>13 <a title="hunch_net-2005-133" href="../hunch_net-2005/hunch_net-2005-11-28-A_question_of_quantification.html">hunch net-2005-11-28-A question of quantification</a></p>
<p>Introduction: This is about methods for phrasing and think about the scope of some theorems
in learning theory. The basic claim is that there are several different ways
of quantifying the scope which sound different yet are essentially the
same.For all sequences of examples. This is the standard quantification in
online learning analysis. Standard theorems would say something like "for all
sequences of predictions by experts, the algorithm A will perform almost as
well as the best expert."For all training sets. This is the standard
quantification for boosting analysis such asadaboostormulticlass
boosting.Standard theorems have the form "for all training sets the error rate
inequalities … hold".For all distributions over examples. This is the one that
we have been using for reductions analysis. Standard theorem statements have
the form "For all distributions over examples, the error rate inequalities …
hold".It is not quite true that each of these is equivalent. For example, in
the online learning se</p><p>14 <a title="hunch_net-2005-132" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>Introduction: How do you create an optimal environment for research? Here are some essential
ingredients that I see.Stability. University-based research is relatively good
at this. On any particular day, researchers face choices in what they will
work on. A very common tradeoff is between:easy smalldifficult bigFor
researchers without stability, the 'easy small' option wins. This is often
"ok"--a series of incremental improvements on the state of the art can add up
to something very beneficial. However, it misses one of the big potentials of
research: finding entirely new and better ways of doing things.Stability comes
in many forms. The prototypical example is tenure at a university--a tenured
professor is almost imposssible to fire which means that the professor has the
freedom to consider far horizon activities. An iron-clad guarantee of a
paycheck is not necessary--industrial research labs have succeeded well with
research positions of indefinite duration. Atnt research was a great example
of th</p><p>15 <a title="hunch_net-2005-131" href="../hunch_net-2005/hunch_net-2005-11-16-The_Everything_Ensemble_Edge.html">hunch net-2005-11-16-The Everything Ensemble Edge</a></p>
<p>Introduction: Rich Caruana,Alexandru Niculescu, Geoff Crew, and Alex Ksikes have donea lot
of empirical testingwhich shows thatusing all methods to make a predictionis
more powerful than using any single method. This is in rough agreement with
the Bayesian way of solving problems, but based upon a different (essentially
empirical) motivation. A rough summary is:Take all of {decision trees, boosted
decision trees, bagged decision trees, boosted decision stumps, K nearest
neighbors, neural networks, SVM} with all reasonable parameter settings.Run
the methods on each problem of 8 problems with a large test set, calibrating
margins using eithersigmoid fittingorisotonic regression.For each loss of
{accuracy, area under the ROC curve, cross entropy, squared error, etcâ&euro;Ś}
evaluate the average performance of the method.A series of conclusions can be
drawn from the observations.(Calibrated) boosted decision trees appear to
perform best, in general although support vector machines and neural networks
give cred</p><p>16 <a title="hunch_net-2005-130" href="../hunch_net-2005/hunch_net-2005-11-16-MLSS_2006.html">hunch net-2005-11-16-MLSS 2006</a></p>
<p>Introduction: There will be twomachine learning summer schoolsin 2006.One is inCanberra,
Australiafrom February 6 to February 17 (Aussie summer). The webpage is fully
'live' so you should actively consider it now.The other is inTaipei,
Taiwanfrom July 24 to August 4. This one is still in the planning phase, but
that should be settled soon.Attending an MLSS is probably the quickest and
easiest way to bootstrap yourself into a reasonable initial understanding of
the field of machine learning.</p><p>17 <a title="hunch_net-2005-129" href="../hunch_net-2005/hunch_net-2005-11-07-Prediction_Competitions.html">hunch net-2005-11-07-Prediction Competitions</a></p>
<p>Introduction: There are two prediction competitions currently in the air.ThePerformance
Prediction ChallengebyIsabelle Guyon. Good entries minimize a weighted 0/1
loss + the difference between a prediction of this loss and the observed truth
on 5 datasets. Isabelle tells me all of the problems are "real world" and the
test datasets are large enough (17K minimum) that the winner should be well
determined by ability rather than luck. This is due March 1.ThePredictive
Uncertainty ChallengebyGavin Cawley. Good entries minimize log loss on real
valued output variables for one synthetic and 3 "real" datasets related to
atmospheric prediction. The use of log loss (which can be infinite and hence
is never convergent) and smaller test sets of size 1K to 7K examples makes the
winner of this contest more luck dependent. Nevertheless, the contest may be
of some interest particularly to the branch of learning (typically Bayes
learning) which prefers to optimize log loss.May the best predictor win.</p><p>18 <a title="hunch_net-2005-128" href="../hunch_net-2005/hunch_net-2005-11-05-The_design_of_a_computing_cluster.html">hunch net-2005-11-05-The design of a computing cluster</a></p>
<p>Introduction: This is about the design of a computing cluster from the viewpoint of applied
machine learning using current technology. We just built a small one at TTI so
this is some evidence of what is feasible and thoughts about the design
choices.ArchitectureThere are several architectural choices.AMD Athlon64 based
system. This seems to have the cheapest bang/buck. Maximum RAM is typically
2-3GB.AMD Opteron based system. Opterons provide the additional capability to
buy an SMP motherboard with two chips, and the motherboards often support 16GB
of RAM. The RAM is also the more expensive error correcting type.Intel PIV or
Xeon based system. The PIV and Xeon based systems are the intel analog of the
above 2. Due to architectural design reasons, these chips tend to run a bit
hotter and be a bit more expensive.Dual core chips. Both Intel and AMD have
chips that actually have 2 processors embedded in them.In the end, we decided
to go with option (2). Roughly speaking, the AMD system seemed like a bet</p><p>19 <a title="hunch_net-2005-127" href="../hunch_net-2005/hunch_net-2005-11-02-Progress_in_Active_Learning.html">hunch net-2005-11-02-Progress in Active Learning</a></p>
<p>Introduction: Several bits of progress have been made sinceSanjoypointed out the
significantlack of theoretical understanding of active learning. This is an
update on the progress I know of. As a refresher, active learning as meant
here is:There is a source of unlabeled data.There is an oracle from which
labels can be requested for unlabeled data produced by the source.The goal is
to perform well with minimal use of the oracle.Here is what I've
learned:Sanjoy has developed sufficient and semi-necessary conditions for
active learning given the assumptions of IID data and "realizability" (that
one of the classifiers is a correct classifier).Nina,Alina, and I developed an
algorithm for active learning relying on only the assumption of IID data. A
draft ishere.Nicolo,Claudio, andLucashowed that it is possible to do active
learning in an entirely adversarial setting for linear threshold
classifiershere. This was published a year or two ago and I recently learned
about it.All of these results are relative</p><p>20 <a title="hunch_net-2005-126" href="../hunch_net-2005/hunch_net-2005-10-26-Fallback_Analysis_is_a_Secret_to_Useful_Algorithms.html">hunch net-2005-10-26-Fallback Analysis is a Secret to Useful Algorithms</a></p>
<p>Introduction: The ideal of theoretical algorithm analysis is to construct an algorithm with
accompanying optimality theorems proving that it is a useful algorithm. This
ideal often fails, particularly for learning algorithms and theory. The
general form of a theorem is:IfpreconditionsThenpostconditionsWhen we design
learning algorithms it is very common to come up with precondition assumptions
such as "the data is IID", "the learning problem is drawn from a known
distribution over learning problems", or "there is a perfect classifier". All
of these example preconditions can be false for real-world problems in ways
that are not easily detectable. This means that algorithms derived and
justified by these very common forms of analysis may be prone to catastrophic
failure in routine (mis)application.Wecanhope for better. Several different
kinds of learning algorithm analysis have been developed some of which have
fewer preconditions. Simply demanding that these forms of analysis be used may
be too stron</p><p>21 <a title="hunch_net-2005-125" href="../hunch_net-2005/hunch_net-2005-10-20-Machine_Learning_in_the_News.html">hunch net-2005-10-20-Machine Learning in the News</a></p>
<p>22 <a title="hunch_net-2005-124" href="../hunch_net-2005/hunch_net-2005-10-19-Workshop%3A_Atomic_Learning.html">hunch net-2005-10-19-Workshop: Atomic Learning</a></p>
<p>23 <a title="hunch_net-2005-123" href="../hunch_net-2005/hunch_net-2005-10-16-Complexity%3A_It%26%238217%3Bs_all_in_your_head.html">hunch net-2005-10-16-Complexity: It&#8217;s all in your head</a></p>
<p>24 <a title="hunch_net-2005-122" href="../hunch_net-2005/hunch_net-2005-10-13-Site_tweak.html">hunch net-2005-10-13-Site tweak</a></p>
<p>25 <a title="hunch_net-2005-121" href="../hunch_net-2005/hunch_net-2005-10-12-The_unrealized_potential_of_the_research_lab.html">hunch net-2005-10-12-The unrealized potential of the research lab</a></p>
<p>26 <a title="hunch_net-2005-120" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>27 <a title="hunch_net-2005-119" href="../hunch_net-2005/hunch_net-2005-10-08-We_have_a_winner.html">hunch net-2005-10-08-We have a winner</a></p>
<p>28 <a title="hunch_net-2005-118" href="../hunch_net-2005/hunch_net-2005-10-07-On-line_learning_of_regular_decision_rules.html">hunch net-2005-10-07-On-line learning of regular decision rules</a></p>
<p>29 <a title="hunch_net-2005-117" href="../hunch_net-2005/hunch_net-2005-10-03-Not_ICML.html">hunch net-2005-10-03-Not ICML</a></p>
<p>30 <a title="hunch_net-2005-116" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">hunch net-2005-09-30-Research in conferences</a></p>
<p>31 <a title="hunch_net-2005-115" href="../hunch_net-2005/hunch_net-2005-09-26-Prediction_Bounds_as_the_Mathematics_of_Science.html">hunch net-2005-09-26-Prediction Bounds as the Mathematics of Science</a></p>
<p>32 <a title="hunch_net-2005-114" href="../hunch_net-2005/hunch_net-2005-09-20-Workshop_Proposal%3A_Atomic_Learning.html">hunch net-2005-09-20-Workshop Proposal: Atomic Learning</a></p>
<p>33 <a title="hunch_net-2005-113" href="../hunch_net-2005/hunch_net-2005-09-19-NIPS_Workshops.html">hunch net-2005-09-19-NIPS Workshops</a></p>
<p>34 <a title="hunch_net-2005-112" href="../hunch_net-2005/hunch_net-2005-09-14-The_Predictionist_Viewpoint.html">hunch net-2005-09-14-The Predictionist Viewpoint</a></p>
<p>35 <a title="hunch_net-2005-111" href="../hunch_net-2005/hunch_net-2005-09-12-Fast_Gradient_Descent.html">hunch net-2005-09-12-Fast Gradient Descent</a></p>
<p>36 <a title="hunch_net-2005-110" href="../hunch_net-2005/hunch_net-2005-09-10-%26%238220%3BFailure%26%238221%3B_is_an_option.html">hunch net-2005-09-10-&#8220;Failure&#8221; is an option</a></p>
<p>37 <a title="hunch_net-2005-109" href="../hunch_net-2005/hunch_net-2005-09-08-Online_Learning_as_the_Mathematics_of_Accountability.html">hunch net-2005-09-08-Online Learning as the Mathematics of Accountability</a></p>
<p>38 <a title="hunch_net-2005-108" href="../hunch_net-2005/hunch_net-2005-09-06-A_link.html">hunch net-2005-09-06-A link</a></p>
<p>39 <a title="hunch_net-2005-107" href="../hunch_net-2005/hunch_net-2005-09-05-Site_Update.html">hunch net-2005-09-05-Site Update</a></p>
<p>40 <a title="hunch_net-2005-106" href="../hunch_net-2005/hunch_net-2005-09-04-Science_in_the_Government.html">hunch net-2005-09-04-Science in the Government</a></p>
<p>41 <a title="hunch_net-2005-105" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<p>42 <a title="hunch_net-2005-104" href="../hunch_net-2005/hunch_net-2005-08-22-Do_you_believe_in_induction%3F.html">hunch net-2005-08-22-Do you believe in induction?</a></p>
<p>43 <a title="hunch_net-2005-103" href="../hunch_net-2005/hunch_net-2005-08-18-SVM_Adaptability.html">hunch net-2005-08-18-SVM Adaptability</a></p>
<p>44 <a title="hunch_net-2005-102" href="../hunch_net-2005/hunch_net-2005-08-11-Why_Manifold-Based_Dimension_Reduction_Techniques%3F.html">hunch net-2005-08-11-Why Manifold-Based Dimension Reduction Techniques?</a></p>
<p>45 <a title="hunch_net-2005-101" href="../hunch_net-2005/hunch_net-2005-08-08-Apprenticeship_Reinforcement_Learning_for_Control.html">hunch net-2005-08-08-Apprenticeship Reinforcement Learning for Control</a></p>
<p>46 <a title="hunch_net-2005-100" href="../hunch_net-2005/hunch_net-2005-08-04-Why_Reinforcement_Learning_is_Important.html">hunch net-2005-08-04-Why Reinforcement Learning is Important</a></p>
<p>47 <a title="hunch_net-2005-99" href="../hunch_net-2005/hunch_net-2005-08-01-Peekaboom.html">hunch net-2005-08-01-Peekaboom</a></p>
<p>48 <a title="hunch_net-2005-98" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">hunch net-2005-07-27-Not goal metrics</a></p>
<p>49 <a title="hunch_net-2005-97" href="../hunch_net-2005/hunch_net-2005-07-23-Interesting_papers_at_ACL.html">hunch net-2005-07-23-Interesting papers at ACL</a></p>
<p>50 <a title="hunch_net-2005-96" href="../hunch_net-2005/hunch_net-2005-07-21-Six_Months.html">hunch net-2005-07-21-Six Months</a></p>
<p>51 <a title="hunch_net-2005-95" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>52 <a title="hunch_net-2005-94" href="../hunch_net-2005/hunch_net-2005-07-13-Text_Entailment_at_AAAI.html">hunch net-2005-07-13-Text Entailment at AAAI</a></p>
<p>53 <a title="hunch_net-2005-93" href="../hunch_net-2005/hunch_net-2005-07-13-%26%238220%3BSister_Conference%26%238221%3B_presentations.html">hunch net-2005-07-13-&#8220;Sister Conference&#8221; presentations</a></p>
<p>54 <a title="hunch_net-2005-92" href="../hunch_net-2005/hunch_net-2005-07-11-AAAI_blog.html">hunch net-2005-07-11-AAAI blog</a></p>
<p>55 <a title="hunch_net-2005-91" href="../hunch_net-2005/hunch_net-2005-07-10-Thinking_the_Unthought.html">hunch net-2005-07-10-Thinking the Unthought</a></p>
<p>56 <a title="hunch_net-2005-90" href="../hunch_net-2005/hunch_net-2005-07-07-The_Limits_of_Learning_Theory.html">hunch net-2005-07-07-The Limits of Learning Theory</a></p>
<p>57 <a title="hunch_net-2005-89" href="../hunch_net-2005/hunch_net-2005-07-04-The_Health_of_COLT.html">hunch net-2005-07-04-The Health of COLT</a></p>
<p>58 <a title="hunch_net-2005-88" href="../hunch_net-2005/hunch_net-2005-07-01-The_Role_of_Impromptu_Talks.html">hunch net-2005-07-01-The Role of Impromptu Talks</a></p>
<p>59 <a title="hunch_net-2005-87" href="../hunch_net-2005/hunch_net-2005-06-29-Not_EM_for_clustering_at_COLT.html">hunch net-2005-06-29-Not EM for clustering at COLT</a></p>
<p>60 <a title="hunch_net-2005-86" href="../hunch_net-2005/hunch_net-2005-06-28-The_cross_validation_problem%3A_cash_reward.html">hunch net-2005-06-28-The cross validation problem: cash reward</a></p>
<p>61 <a title="hunch_net-2005-85" href="../hunch_net-2005/hunch_net-2005-06-28-A_COLT_paper.html">hunch net-2005-06-28-A COLT paper</a></p>
<p>62 <a title="hunch_net-2005-84" href="../hunch_net-2005/hunch_net-2005-06-22-Languages__of_Learning.html">hunch net-2005-06-22-Languages  of Learning</a></p>
<p>63 <a title="hunch_net-2005-83" href="../hunch_net-2005/hunch_net-2005-06-18-Lower_Bounds_for_Learning_Reductions.html">hunch net-2005-06-18-Lower Bounds for Learning Reductions</a></p>
<p>64 <a title="hunch_net-2005-82" href="../hunch_net-2005/hunch_net-2005-06-17-Reopening_RL-%3EClassification.html">hunch net-2005-06-17-Reopening RL->Classification</a></p>
<p>65 <a title="hunch_net-2005-81" href="../hunch_net-2005/hunch_net-2005-06-13-Wikis_for_Summer_Schools_and_Workshops.html">hunch net-2005-06-13-Wikis for Summer Schools and Workshops</a></p>
<p>66 <a title="hunch_net-2005-80" href="../hunch_net-2005/hunch_net-2005-06-10-Workshops_are_not_Conferences.html">hunch net-2005-06-10-Workshops are not Conferences</a></p>
<p>67 <a title="hunch_net-2005-79" href="../hunch_net-2005/hunch_net-2005-06-08-Question%3A_%26%238220%3BWhen_is_the_right_time_to_insert_the_loss_function%3F%26%238221%3B.html">hunch net-2005-06-08-Question: &#8220;When is the right time to insert the loss function?&#8221;</a></p>
<p>68 <a title="hunch_net-2005-78" href="../hunch_net-2005/hunch_net-2005-06-06-Exact_Online_Learning_for_Classification.html">hunch net-2005-06-06-Exact Online Learning for Classification</a></p>
<p>69 <a title="hunch_net-2005-77" href="../hunch_net-2005/hunch_net-2005-05-29-Maximum_Margin_Mismatch%3F.html">hunch net-2005-05-29-Maximum Margin Mismatch?</a></p>
<p>70 <a title="hunch_net-2005-76" href="../hunch_net-2005/hunch_net-2005-05-29-Bad_ideas.html">hunch net-2005-05-29-Bad ideas</a></p>
<p>71 <a title="hunch_net-2005-75" href="../hunch_net-2005/hunch_net-2005-05-28-Running_A_Machine_Learning_Summer_School.html">hunch net-2005-05-28-Running A Machine Learning Summer School</a></p>
<p>72 <a title="hunch_net-2005-74" href="../hunch_net-2005/hunch_net-2005-05-21-What_is_the_right_form_of_modularity_in_structured_prediction%3F.html">hunch net-2005-05-21-What is the right form of modularity in structured prediction?</a></p>
<p>73 <a title="hunch_net-2005-73" href="../hunch_net-2005/hunch_net-2005-05-17-A_Short_Guide_to_PhD_Graduate_Study.html">hunch net-2005-05-17-A Short Guide to PhD Graduate Study</a></p>
<p>74 <a title="hunch_net-2005-72" href="../hunch_net-2005/hunch_net-2005-05-16-Regret_minimizing_vs_error_limiting_reductions.html">hunch net-2005-05-16-Regret minimizing vs error limiting reductions</a></p>
<p>75 <a title="hunch_net-2005-71" href="../hunch_net-2005/hunch_net-2005-05-14-NIPS.html">hunch net-2005-05-14-NIPS</a></p>
<p>76 <a title="hunch_net-2005-70" href="../hunch_net-2005/hunch_net-2005-05-12-Math_on_the_Web.html">hunch net-2005-05-12-Math on the Web</a></p>
<p>77 <a title="hunch_net-2005-69" href="../hunch_net-2005/hunch_net-2005-05-11-Visa_Casualties.html">hunch net-2005-05-11-Visa Casualties</a></p>
<p>78 <a title="hunch_net-2005-68" href="../hunch_net-2005/hunch_net-2005-05-10-Learning_Reductions_are_Reductionist.html">hunch net-2005-05-10-Learning Reductions are Reductionist</a></p>
<p>79 <a title="hunch_net-2005-67" href="../hunch_net-2005/hunch_net-2005-05-06-Don%26%238217%3Bt_mix_the_solution_into_the_problem.html">hunch net-2005-05-06-Don&#8217;t mix the solution into the problem</a></p>
<p>80 <a title="hunch_net-2005-66" href="../hunch_net-2005/hunch_net-2005-05-03-Conference_attendance_is_mandatory.html">hunch net-2005-05-03-Conference attendance is mandatory</a></p>
<p>81 <a title="hunch_net-2005-65" href="../hunch_net-2005/hunch_net-2005-05-02-Reviewing_techniques_for_conferences.html">hunch net-2005-05-02-Reviewing techniques for conferences</a></p>
<p>82 <a title="hunch_net-2005-64" href="../hunch_net-2005/hunch_net-2005-04-28-Science_Fiction_and_Research.html">hunch net-2005-04-28-Science Fiction and Research</a></p>
<p>83 <a title="hunch_net-2005-63" href="../hunch_net-2005/hunch_net-2005-04-27-DARPA_project%3A_LAGR.html">hunch net-2005-04-27-DARPA project: LAGR</a></p>
<p>84 <a title="hunch_net-2005-62" href="../hunch_net-2005/hunch_net-2005-04-26-To_calibrate_or_not%3F.html">hunch net-2005-04-26-To calibrate or not?</a></p>
<p>85 <a title="hunch_net-2005-61" href="../hunch_net-2005/hunch_net-2005-04-25-Embeddings%3A_what_are_they_good_for%3F.html">hunch net-2005-04-25-Embeddings: what are they good for?</a></p>
<p>86 <a title="hunch_net-2005-60" href="../hunch_net-2005/hunch_net-2005-04-23-Advantages_and_Disadvantages_of_Bayesian_Learning.html">hunch net-2005-04-23-Advantages and Disadvantages of Bayesian Learning</a></p>
<p>87 <a title="hunch_net-2005-59" href="../hunch_net-2005/hunch_net-2005-04-22-New_Blog%3A_%5BLowerbounds%2CUpperbounds%5D.html">hunch net-2005-04-22-New Blog: [Lowerbounds,Upperbounds]</a></p>
<p>88 <a title="hunch_net-2005-58" href="../hunch_net-2005/hunch_net-2005-04-21-Dynamic_Programming_Generalizations_and_Their_Use.html">hunch net-2005-04-21-Dynamic Programming Generalizations and Their Use</a></p>
<p>89 <a title="hunch_net-2005-57" href="../hunch_net-2005/hunch_net-2005-04-16-Which_Assumptions_are_Reasonable%3F.html">hunch net-2005-04-16-Which Assumptions are Reasonable?</a></p>
<p>90 <a title="hunch_net-2005-56" href="../hunch_net-2005/hunch_net-2005-04-14-Families_of_Learning_Theory_Statements.html">hunch net-2005-04-14-Families of Learning Theory Statements</a></p>
<p>91 <a title="hunch_net-2005-55" href="../hunch_net-2005/hunch_net-2005-04-10-Is_the_Goal_Understanding_or_Prediction%3F.html">hunch net-2005-04-10-Is the Goal Understanding or Prediction?</a></p>
<p>92 <a title="hunch_net-2005-54" href="../hunch_net-2005/hunch_net-2005-04-08-Fast_SVMs.html">hunch net-2005-04-08-Fast SVMs</a></p>
<p>93 <a title="hunch_net-2005-53" href="../hunch_net-2005/hunch_net-2005-04-06-Structured_Regret_Minimization.html">hunch net-2005-04-06-Structured Regret Minimization</a></p>
<p>94 <a title="hunch_net-2005-52" href="../hunch_net-2005/hunch_net-2005-04-04-Grounds_for_Rejection.html">hunch net-2005-04-04-Grounds for Rejection</a></p>
<p>95 <a title="hunch_net-2005-51" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>96 <a title="hunch_net-2005-50" href="../hunch_net-2005/hunch_net-2005-04-01-Basic_computer_science_research_takes_a_hit.html">hunch net-2005-04-01-Basic computer science research takes a hit</a></p>
<p>97 <a title="hunch_net-2005-49" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>98 <a title="hunch_net-2005-48" href="../hunch_net-2005/hunch_net-2005-03-29-Academic_Mechanism_Design.html">hunch net-2005-03-29-Academic Mechanism Design</a></p>
<p>99 <a title="hunch_net-2005-47" href="../hunch_net-2005/hunch_net-2005-03-28-Open_Problems_for_Colt.html">hunch net-2005-03-28-Open Problems for Colt</a></p>
<p>100 <a title="hunch_net-2005-46" href="../hunch_net-2005/hunch_net-2005-03-24-The_Role_of_Workshops.html">hunch net-2005-03-24-The Role of Workshops</a></p>
<p>101 <a title="hunch_net-2005-45" href="../hunch_net-2005/hunch_net-2005-03-22-Active_learning.html">hunch net-2005-03-22-Active learning</a></p>
<p>102 <a title="hunch_net-2005-44" href="../hunch_net-2005/hunch_net-2005-03-21-Research_Styles_in_Machine_Learning.html">hunch net-2005-03-21-Research Styles in Machine Learning</a></p>
<p>103 <a title="hunch_net-2005-43" href="../hunch_net-2005/hunch_net-2005-03-18-Binomial_Weighting.html">hunch net-2005-03-18-Binomial Weighting</a></p>
<p>104 <a title="hunch_net-2005-42" href="../hunch_net-2005/hunch_net-2005-03-17-Going_all_the_Way%2C_Sometimes.html">hunch net-2005-03-17-Going all the Way, Sometimes</a></p>
<p>105 <a title="hunch_net-2005-41" href="../hunch_net-2005/hunch_net-2005-03-15-The_State_of_Tight_Bounds.html">hunch net-2005-03-15-The State of Tight Bounds</a></p>
<p>106 <a title="hunch_net-2005-40" href="../hunch_net-2005/hunch_net-2005-03-13-Avoiding_Bad_Reviewing.html">hunch net-2005-03-13-Avoiding Bad Reviewing</a></p>
<p>107 <a title="hunch_net-2005-39" href="../hunch_net-2005/hunch_net-2005-03-10-Breaking_Abstractions.html">hunch net-2005-03-10-Breaking Abstractions</a></p>
<p>108 <a title="hunch_net-2005-38" href="../hunch_net-2005/hunch_net-2005-03-09-Bad_Reviewing.html">hunch net-2005-03-09-Bad Reviewing</a></p>
<p>109 <a title="hunch_net-2005-37" href="../hunch_net-2005/hunch_net-2005-03-08-Fast_Physics_for_Learning.html">hunch net-2005-03-08-Fast Physics for Learning</a></p>
<p>110 <a title="hunch_net-2005-36" href="../hunch_net-2005/hunch_net-2005-03-05-Funding_Research.html">hunch net-2005-03-05-Funding Research</a></p>
<p>111 <a title="hunch_net-2005-35" href="../hunch_net-2005/hunch_net-2005-03-04-The_Big_O_and_Constants_in_Learning.html">hunch net-2005-03-04-The Big O and Constants in Learning</a></p>
<p>112 <a title="hunch_net-2005-34" href="../hunch_net-2005/hunch_net-2005-03-02-Prior%2C_%26%238220%3BPrior%26%238221%3B_and_Bias.html">hunch net-2005-03-02-Prior, &#8220;Prior&#8221; and Bias</a></p>
<p>113 <a title="hunch_net-2005-33" href="../hunch_net-2005/hunch_net-2005-02-28-Regularization.html">hunch net-2005-02-28-Regularization</a></p>
<p>114 <a title="hunch_net-2005-32" href="../hunch_net-2005/hunch_net-2005-02-27-Antilearning%3A_When_proximity_goes_bad.html">hunch net-2005-02-27-Antilearning: When proximity goes bad</a></p>
<p>115 <a title="hunch_net-2005-31" href="../hunch_net-2005/hunch_net-2005-02-26-Problem%3A_Reductions_and_Relative_Ranking_Metrics.html">hunch net-2005-02-26-Problem: Reductions and Relative Ranking Metrics</a></p>
<p>116 <a title="hunch_net-2005-30" href="../hunch_net-2005/hunch_net-2005-02-25-Why_Papers%3F.html">hunch net-2005-02-25-Why Papers?</a></p>
<p>117 <a title="hunch_net-2005-29" href="../hunch_net-2005/hunch_net-2005-02-25-Solution%3A_Reinforcement_Learning_with_Classification.html">hunch net-2005-02-25-Solution: Reinforcement Learning with Classification</a></p>
<p>118 <a title="hunch_net-2005-28" href="../hunch_net-2005/hunch_net-2005-02-25-Problem%3A_Online_Learning.html">hunch net-2005-02-25-Problem: Online Learning</a></p>
<p>119 <a title="hunch_net-2005-27" href="../hunch_net-2005/hunch_net-2005-02-23-Problem%3A_Reinforcement_Learning_with_Classification.html">hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</a></p>
<p>120 <a title="hunch_net-2005-26" href="../hunch_net-2005/hunch_net-2005-02-21-Problem%3A_Cross_Validation.html">hunch net-2005-02-21-Problem: Cross Validation</a></p>
<p>121 <a title="hunch_net-2005-25" href="../hunch_net-2005/hunch_net-2005-02-20-At_One_Month.html">hunch net-2005-02-20-At One Month</a></p>
<p>122 <a title="hunch_net-2005-24" href="../hunch_net-2005/hunch_net-2005-02-19-Machine_learning_reading_groups.html">hunch net-2005-02-19-Machine learning reading groups</a></p>
<p>123 <a title="hunch_net-2005-23" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<p>124 <a title="hunch_net-2005-22" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">hunch net-2005-02-18-What it means to do research.</a></p>
<p>125 <a title="hunch_net-2005-21" href="../hunch_net-2005/hunch_net-2005-02-17-Learning_Research_Programs.html">hunch net-2005-02-17-Learning Research Programs</a></p>
<p>126 <a title="hunch_net-2005-20" href="../hunch_net-2005/hunch_net-2005-02-15-ESPgame_and_image_labeling.html">hunch net-2005-02-15-ESPgame and image labeling</a></p>
<p>127 <a title="hunch_net-2005-19" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>128 <a title="hunch_net-2005-18" href="../hunch_net-2005/hunch_net-2005-02-12-ROC_vs._Accuracy_vs._AROC.html">hunch net-2005-02-12-ROC vs. Accuracy vs. AROC</a></p>
<p>129 <a title="hunch_net-2005-17" href="../hunch_net-2005/hunch_net-2005-02-10-Conferences%2C_Dates%2C_Locations.html">hunch net-2005-02-10-Conferences, Dates, Locations</a></p>
<p>130 <a title="hunch_net-2005-16" href="../hunch_net-2005/hunch_net-2005-02-09-Intuitions_from_applied_learning.html">hunch net-2005-02-09-Intuitions from applied learning</a></p>
<p>131 <a title="hunch_net-2005-15" href="../hunch_net-2005/hunch_net-2005-02-08-Some_Links.html">hunch net-2005-02-08-Some Links</a></p>
<p>132 <a title="hunch_net-2005-14" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">hunch net-2005-02-07-The State of the Reduction</a></p>
<p>133 <a title="hunch_net-2005-13" href="../hunch_net-2005/hunch_net-2005-02-04-JMLG.html">hunch net-2005-02-04-JMLG</a></p>
<p>134 <a title="hunch_net-2005-12" href="../hunch_net-2005/hunch_net-2005-02-03-Learning_Theory%2C_by_assumption.html">hunch net-2005-02-03-Learning Theory, by assumption</a></p>
<p>135 <a title="hunch_net-2005-11" href="../hunch_net-2005/hunch_net-2005-02-02-Paper_Deadlines.html">hunch net-2005-02-02-Paper Deadlines</a></p>
<p>136 <a title="hunch_net-2005-10" href="../hunch_net-2005/hunch_net-2005-02-02-Kolmogorov_Complexity_and_Googling.html">hunch net-2005-02-02-Kolmogorov Complexity and Googling</a></p>
<p>137 <a title="hunch_net-2005-9" href="../hunch_net-2005/hunch_net-2005-02-01-Watchword%3A_Loss.html">hunch net-2005-02-01-Watchword: Loss</a></p>
<p>138 <a title="hunch_net-2005-8" href="../hunch_net-2005/hunch_net-2005-02-01-NIPS%3A_Online_Bayes.html">hunch net-2005-02-01-NIPS: Online Bayes</a></p>
<p>139 <a title="hunch_net-2005-7" href="../hunch_net-2005/hunch_net-2005-01-31-Watchword%3A_Assumption.html">hunch net-2005-01-31-Watchword: Assumption</a></p>
<p>140 <a title="hunch_net-2005-6" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>141 <a title="hunch_net-2005-5" href="../hunch_net-2005/hunch_net-2005-01-26-Watchword%3A_Probability.html">hunch net-2005-01-26-Watchword: Probability</a></p>
<p>142 <a title="hunch_net-2005-4" href="../hunch_net-2005/hunch_net-2005-01-26-Summer_Schools.html">hunch net-2005-01-26-Summer Schools</a></p>
<p>143 <a title="hunch_net-2005-3" href="../hunch_net-2005/hunch_net-2005-01-24-The_Humanloop_Spectrum_of_Machine_Learning.html">hunch net-2005-01-24-The Humanloop Spectrum of Machine Learning</a></p>
<p>144 <a title="hunch_net-2005-2" href="../hunch_net-2005/hunch_net-2005-01-24-Holy_grails_of_machine_learning%3F.html">hunch net-2005-01-24-Holy grails of machine learning?</a></p>
<p>145 <a title="hunch_net-2005-1" href="../hunch_net-2005/hunch_net-2005-01-19-Why_I_decided_to_run_a_weblog..html">hunch net-2005-01-19-Why I decided to run a weblog.</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
