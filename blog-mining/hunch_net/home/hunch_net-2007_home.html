<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>hunch_net 2007 knowledge graph</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2007" href="#">hunch_net-2007</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>hunch_net 2007 knowledge graph</h1>
<br/><h3>similar blogs computed by tfidf model</h3><br/><h3>similar blogs computed by <a title="lsi-model" href="./hunch_net_lsi.html">lsi model</a></h3><br/><h3>similar blogs computed by <a title="lda-model" href="./hunch_net_lda.html">lda model</a></h3><br/><h2>blogs list:</h2><p>1 <a title="hunch_net-2007-281" href="../hunch_net-2007/hunch_net-2007-12-21-Vowpal_Wabbit_Code_Release.html">hunch net-2007-12-21-Vowpal Wabbit Code Release</a></p>
<p>Introduction: We are releasing theVowpal Wabbit (Fast Online Learning) codeas open source
under a BSD (revised) license. This is aproject at Yahoo! Researchto build a
useful large scale learning algorithm whichLihong Li,Alex Strehl, and I have
been working on.To appreciate the meaning of "large", it's useful to define
"small" and "medium". A "small" supervised learning problem is one where a
human could use a labeled dataset and come up with a reasonable predictor. A
"medium" supervised learning problem dataset fits into the RAM of a modern
desktop computer. A "large" supervised learning problem is one which does not
fit into the RAM of a normal machine. VW tackles large scale learning problems
by this definition of large. I'm not aware of any other open source Machine
Learning tools which can handle this scale (although they may exist). A few
close ones are:IBM's Parallel Machine Learning Toolboxisn't quite open source.
The approach used by this toolbox is essentially map-reduce style computation,</p><p>2 <a title="hunch_net-2007-280" href="../hunch_net-2007/hunch_net-2007-12-20-Cool_and_Interesting_things_at_NIPS%2C_take_three.html">hunch net-2007-12-20-Cool and Interesting things at NIPS, take three</a></p>
<p>Introduction: Following up on Hal Daume's post and John's post on cool and interesting
things seen at NIPS I'll post my own little list of neat papers here as well.
Of course it's going to be biased towards what I think is interesting. Also, I
have to say that I hadn't been able to see many papers this year at nips due
to myself being too busy, so please feel free to contribute the papers that
you liked1. P. Mudigonda, V. Kolmogorov, P. Torr. An Analysis of Convex
Relaxations for MAP Estimation. A surprising paper which shows that many of
the more sophisticated convex relaxations that had been proposed recently
turns out to be subsumed by the simplest LP relaxation. Be careful next time
you try a cool new convex relaxation!2. D. Sontag, T. Jaakkola. New Outer
Bounds on the Marginal Polytope. The title says it all. The marginal polytope
is the set of local marginal distributions over subsets of variables that are
globally consistent in the sense that there is at least one distribution over
all the va</p><p>3 <a title="hunch_net-2007-279" href="../hunch_net-2007/hunch_net-2007-12-19-Cool_and_interesting_things_seen_at_NIPS.html">hunch net-2007-12-19-Cool and interesting things seen at NIPS</a></p>
<p>Introduction: I learned a number of things atNIPS.The financial people were there in greater
force than previously.Two Sigmasponsored NIPS whileDRW Tradinghad a
booth.Theadversarial machine learning workshophad a number of talks about
interesting applications where an adversary really is out to try and mess up
your learning algorithm. This is very different from the situation we often
think of where the world is oblivious to our learning. This may present new
and convincing applications for the learning-against-an-adversary work common
atCOLT.There were several interesing papers.Sanjoy Dasgupta,Daniel Hsu,
andClaire Monteleonihad a paper onGeneral Agnostic Active Learning. The basic
idea is that active learning can be done via reduction to a form of supervised
learning problem. This is great, because we have many supervised learning
algorithms from which the benefits of active learning may be derived.Joseph
BradleyandRobert Schapirehad aPaper on Filterboost. Filterboost is an online
boosting algorit</p><p>4 <a title="hunch_net-2007-278" href="../hunch_net-2007/hunch_net-2007-12-17-New_Machine_Learning_mailing_list.html">hunch net-2007-12-17-New Machine Learning mailing list</a></p>
<p>Introduction: IMLS(which is the nonprofit running ICML) has setup a new mailing list
forMachine Learning News. The list address is ML-news@googlegroups.com, and
signup requires a google account (which you can create). Only members can send
messages.</p><p>5 <a title="hunch_net-2007-277" href="../hunch_net-2007/hunch_net-2007-12-12-Workshop_Summary%26%238212%3BPrinciples_of_Learning_Problem_Design.html">hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</a></p>
<p>Introduction: This is a summary of theworkshop on Learning Problem DesignwhichAlinaand I ran
atNIPSthis year.The first question many people have is "What is learning
problem design?" This workshop is about admitting that solving learning
problems does not start with labeled data, but rather somewhere before. When
humans are hired to produce labels, this is usually not a serious problem
because you can tell them precisely what semantics you want the labels to
have, and we can fix some set of features in advance. However, when other
methods are used this becomes more problematic. This focus is important for
Machine Learning because there are very large quantities of data which are not
labeled by a hired human.The title of the workshop was a bit ambitious,
because a workshop is not long enough to synthesize a diversity of approaches
into a coherent set of principles. For me, the posters at the end of the
workshop were quite helpful in getting approaches to gel.Here are some answers
to "where do the lab</p><p>6 <a title="hunch_net-2007-276" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>Introduction: TheInternational Planning Competition(IPC) is a biennial event organized in
the context of theInternational Conference on Automated Planning and
Scheduling(ICAPS). This year, for the first time, there will a learning track
of the competition. For more information you can go to the competitionweb-
site.The competitions are typically organized around a number of planning
domains that can vary from year to year, where a planning domain is simply a
class of problems that share a common action schema--e.g. Blocksworld is a
well-known planning domain that contains a problem instance each possible
initial tower configuration and goal configuration. Some other domains have
included Logistics, Airport, Freecell, PipesWorld, and manyothers. For each
domain the competition includes a number of problems (say 40-50) and the
planners are run on each problem with a time limit for each problem (around 30
minutes). The problems are hard enough that many problems are not solved
within the time limit.Giv</p><p>7 <a title="hunch_net-2007-275" href="../hunch_net-2007/hunch_net-2007-11-29-The_Netflix_Crack.html">hunch net-2007-11-29-The Netflix Crack</a></p>
<p>Introduction: A couple security researchersclaim to have cracked the netflix dataset. The
claims of success appear somewhat overstated to me, but the method of attack
is valid and could plausibly be substantially improved so as to reveal the
movie preferences of a small fraction of Netflix users.The basic idea is to
use a heuristic similarity function between ratings in a public database (from
IMDB) and an anonymized database (Netflix) to link ratings in the private
database to public identities (in IMDB). They claim to have linked two of a
few dozen IMDB users to anonymized netflix users.The claims seem a bit
inflated to me, because (a) knowing the IMDB identity isn't equivalent to
knowing the person and (b) the claims of statistical significance are with
respect to a model of the world they created (rather than one they
created).Overall, this is another example showing that completeprivacy is
hard. It may be worth remembering that there are some substantial benefits
from the Netflix challenge as w</p><p>8 <a title="hunch_net-2007-274" href="../hunch_net-2007/hunch_net-2007-11-28-Computational_Consequences_of_Classification.html">hunch net-2007-11-28-Computational Consequences of Classification</a></p>
<p>Introduction: In theregression vs classification debate, I'm adding a new "pro" to
classification. It seems there are computational shortcuts available for
classification which simply aren't available for regression. This arises in
several situations.Inactive learningit is sometimes possible to find aneerror
classifier with justlog(e)labeled samples. Only much more modest improvements
appear to be achievable for squared loss regression. The essential reason is
that the loss function on many examples is flat with respect to large
variations in the parameter spaces of a learned classifier, which implies that
many of these classifiers do not need to be considered. In contrast, for
squared loss regression, most substantial variations in the parameter space
influence the loss at most points.In budgeted learning, where there is either
a computational time constraint or a feature cost constraint, a classifier can
sometimes be learned to very high accuracy under the constraints while a
squared loss regresso</p><p>9 <a title="hunch_net-2007-273" href="../hunch_net-2007/hunch_net-2007-11-16-MLSS_2008.html">hunch net-2007-11-16-MLSS 2008</a></p>
<p>Introduction: â&euro;Ś is in Kioloa, Australia from March 3 to March 14. It's a great chance to
learn something about Machine Learning and I've enjoyed severalprevious
Machine Learning Summer Schools.Thewebsite has many more details, but
registration is open now for the first 80 to sign up.</p><p>10 <a title="hunch_net-2007-272" href="../hunch_net-2007/hunch_net-2007-11-14-BellKor_wins_Netflix.html">hunch net-2007-11-14-BellKor wins Netflix</a></p>
<p>Introduction: â&euro;Ś but only the little prize. TheBellKor teamfocused on integrating predictions
from many different methods. The base methods consist of:Nearest Neighbor
MethodsMatrix Factorization Methods (asymmetric and symmetric)Linear
Regression on various feature spacesRestricted Boltzman MachinesThe final
predictor was an ensemble (as was reasonable to expect), although it's a
little bit more complicated than just a weighted average--it's essentially a
customized learning algorithm. Base approaches (1)-(3) seem like relatively
well-known approaches (although I haven't seen the asymmetric factorization
variant before). RBMs are the new approach.Thewriteupis pretty clear for more
details.The contestants are close to reaching the big prize, but the last 1.5%
is probably at least as hard as what's been done. A few new structurally
different methods for making predictions may need to be discovered and added
into the mixture. In other words, research may be required.</p><p>11 <a title="hunch_net-2007-271" href="../hunch_net-2007/hunch_net-2007-11-05-CMU_wins_DARPA_Urban_Challenge.html">hunch net-2007-11-05-CMU wins DARPA Urban Challenge</a></p>
<p>Introduction: Theresults have been posted, withCMU first,Stanford second, andVirginia Tech
Third.Considering that this was an open event (at least for people in the US),
this was a very strong showing for research at universities (instead of
defense contractors, for example). Some details should become public at
theNIPS workshops.Slashdothas apostwith many comments.</p><p>12 <a title="hunch_net-2007-270" href="../hunch_net-2007/hunch_net-2007-11-02-The_Machine_Learning_Award_goes_to_%26%238230%3B.html">hunch net-2007-11-02-The Machine Learning Award goes to &#8230;</a></p>
<p>Introduction: Perhaps the biggest CS prize for research is theTuring Award, which has a
$0.25M cash prize associated with it. It appears none of the prizes so far
have been for anything like machine learning (the closest are perhaps database
awards).In CS theory, there is theGÃƒÂ¶del Prizewhich is smaller and newer,
offering a $5K prize along and perhaps (more importantly) recognition. One
such award has been given for Machine Learning, toRobert SchapireandYoav
Freundfor Adaboost.In Machine Learning, there seems to be no equivalent of
these sorts of prizes. There are several plausible reasons for this:There is
no coherent community.People drift in and out of the central conferences all
the time. Most of the author names from 10 years ago do not occur in the
conferences of today. In addition, the entire subject area is fairly new.There
are at least a core group of people who have stayed around.Machine Learning
work doesn't lastAlmost every paper is forgotten, because {the goals change,
there isn't an</p><p>13 <a title="hunch_net-2007-269" href="../hunch_net-2007/hunch_net-2007-10-24-Contextual_Bandits.html">hunch net-2007-10-24-Contextual Bandits</a></p>
<p>Introduction: One of the fundamental underpinnings of the internet is advertising based
content. This has become much more effective due to targeted advertising where
ads are specifically matched to interests. Everyone is familiar with this,
because everyone uses search engines and all search engines try to make money
this way.The problem of matching ads to interests is a natural machine
learning problem in some ways since there is much information in who clicks on
what. A fundamental problem with this information is that it is not supervised
--in particular a click-or-not on one ad doesn't generally tell you if a
different ad would have been clicked on. This implies we have a fundamental
exploration problem.A standard mathematical setting for this situation is
"k-Armed Bandits", often with various relevant embellishments. Thek-Armed
Bandit setting works on a round-by-round basis. On each round:A policy chooses
armafrom1ofkarms (i.e. 1 of k ads).The world reveals the rewardraof the chosen
arm (i.e.</p><p>14 <a title="hunch_net-2007-268" href="../hunch_net-2007/hunch_net-2007-10-19-Second_Annual_Reinforcement_Learning_Competition.html">hunch net-2007-10-19-Second Annual Reinforcement Learning Competition</a></p>
<p>Introduction: The Second Annual Reinforcement Learning Competition is about to get started.
The aim of the competition is to facilitate direct comparisons between various
learning methods on important and realistic domains. This year's event will
feature well-known benchmark domains as well as more challenging problems of
real-world complexity, such as helicopter control and robot soccer
keepaway.The competition begins on November 1st, 2007 when training software
is released. Results must be submitted by July 1st, 2008. The competition will
culminate in an event at ICML-08 in Helsinki, Finland, at which the winners
will be announced.For more information, visitthe competition website.</p><p>15 <a title="hunch_net-2007-267" href="../hunch_net-2007/hunch_net-2007-10-17-Online_as_the_new_adjective.html">hunch net-2007-10-17-Online as the new adjective</a></p>
<p>Introduction: Online learning is in vogue, which means we should expect to see in the near
future:Online boosting.Online decision trees.Online SVMs. (actually, we've
already seen)Online deep learning.Online parallel learning.etc…There are three
fundamental drivers of this trend.Increasing size of datasets makes online
algorithms attractive.Online learning can simply be more efficient than batch
learning. Here is a picture from a class on online learning:The point of this
picture is that even in 3 dimensions and even with linear constraints, finding
the minima of a set in an online fashion can be typically faster than finding
the minima in a batch fashion. To see this, note that there is a minimal
number of gradient updates (i.e. 2) required in order to reach the minima in
the typical case. Given this, it's best to do these updates as quickly as
possible, which implies doing the first update online (i.e. before seeing all
the examples) is preferred. Note that this is the simplest possible setting--
m</p><p>16 <a title="hunch_net-2007-266" href="../hunch_net-2007/hunch_net-2007-10-15-NIPS_workshops_extended_to_3_days.html">hunch net-2007-10-15-NIPS workshops extended to 3 days</a></p>
<p>Introduction: (Unofficially, at least.) TheDeep Learning Workshopis being held the afternoon
before the rest of the workshops in Vancouver, BC. Separate registration is
needed, and open.What's happening fundamentally here is that there are too
many interesting workshops to fit into 2 days. Perhaps we can get it
officially expanded to 3 days next year.</p><p>17 <a title="hunch_net-2007-265" href="../hunch_net-2007/hunch_net-2007-10-14-NIPS_workshp%3A_Learning_Problem_Design.html">hunch net-2007-10-14-NIPS workshp: Learning Problem Design</a></p>
<p>Introduction: Alinaand I are organizing a workshop onLearning Problem DesignatNIPS.What is
learning problem design?It's about being clever in creating learning problems
from otherwise unlabeled data. Read the webpage above for examples.I want to
participate!Email us before Nov. 1 with a description of what you want to talk
about.</p><p>18 <a title="hunch_net-2007-264" href="../hunch_net-2007/hunch_net-2007-09-30-NIPS_workshops_are_out..html">hunch net-2007-09-30-NIPS workshops are out.</a></p>
<p>Introduction: Here. I'm particularly interested in theWeb Search,Efficient ML, and (of
course)Learning Problem Designworkshops but there are many others to check out
as well. Workshops are a great chance to make progress on or learn about a
topic. Relevance and interaction amongst diverse people can sometimes be
magical.</p><p>19 <a title="hunch_net-2007-263" href="../hunch_net-2007/hunch_net-2007-09-18-It%26%238217%3Bs_MDL_Jim%2C_but_not_as_we_know_it%26%238230%3B%28on_Bayes%2C_MDL_and_consistency%29.html">hunch net-2007-09-18-It&#8217;s MDL Jim, but not as we know it&#8230;(on Bayes, MDL and consistency)</a></p>
<p>Introduction: I have recently completeda 500+ page-book on MDL, the first comprehensive
overview of the field (yes, this is a sneak advertisement).Chapter 17compares
MDL to a menagerie of other methods and paradigms for learning and statistics.
By far the most time (20 pages) is spent on the relation between MDL and
Bayes. My two main points here are:In sharp contrast to Bayes, MDL is by
definition based on designing universal codes for the data relative to some
given (parametric or nonparametric) probabilistic model M. By some theorems
due toAndrew Barron, MDL inferencemusttherefore be statistically consistent,
and it is immune to Bayesian inconsistency results such as those by Diaconis,
Freedman and Barron (I explain what I mean by "inconsistency" further below).
Hence, MDL must be different from Bayes!In contrast to what has sometimes been
claimed, practical MDL algorithms do have a subjective component (which in
many, but not all cases, may be implemented by something similar to a Bayesian
prior</p><p>20 <a title="hunch_net-2007-262" href="../hunch_net-2007/hunch_net-2007-09-16-Optimizing_Machine_Learning_Programs.html">hunch net-2007-09-16-Optimizing Machine Learning Programs</a></p>
<p>Introduction: Machine learning is often computationally bounded which implies that the
ability to write fast code becomes important if you ever want to implement a
machine learning algorithm. Basic tactical optimizations are covered
wellelsewhere, but I haven't seen a reasonable guide to higher level
optimizations, which are the most important in my experience. Here are some of
the higher level optimizations I've often found useful.Algorithmic Improvement
First. This is Hard, but it is the most important consideration, and typically
yields the most benefits. Good optimizations here are publishable. In the
context of machine learning, you should be familiar with the arguments for
online vs. batch learning.Choice of Language. There are many arguments about
thechoice of language. Sometimes you don't have a choice when interfacing with
other people. Personally, I favor C/C++ when I want to write fast code. This
(admittedly) makes me a slower programmer than when using higher level
languages. (Sometimes</p><p>21 <a title="hunch_net-2007-261" href="../hunch_net-2007/hunch_net-2007-08-28-Live_ML_Class.html">hunch net-2007-08-28-Live ML Class</a></p>
<p>22 <a title="hunch_net-2007-260" href="../hunch_net-2007/hunch_net-2007-08-25-The_Privacy_Problem.html">hunch net-2007-08-25-The Privacy Problem</a></p>
<p>23 <a title="hunch_net-2007-259" href="../hunch_net-2007/hunch_net-2007-08-19-Choice_of_Metrics.html">hunch net-2007-08-19-Choice of Metrics</a></p>
<p>24 <a title="hunch_net-2007-258" href="../hunch_net-2007/hunch_net-2007-08-12-Exponentiated_Gradient.html">hunch net-2007-08-12-Exponentiated Gradient</a></p>
<p>25 <a title="hunch_net-2007-257" href="../hunch_net-2007/hunch_net-2007-07-28-Asking_questions.html">hunch net-2007-07-28-Asking questions</a></p>
<p>26 <a title="hunch_net-2007-256" href="../hunch_net-2007/hunch_net-2007-07-20-Motivation_should_be_the_Responsibility_of_the_Reviewer.html">hunch net-2007-07-20-Motivation should be the Responsibility of the Reviewer</a></p>
<p>27 <a title="hunch_net-2007-255" href="../hunch_net-2007/hunch_net-2007-07-13-The_View_From_China.html">hunch net-2007-07-13-The View From China</a></p>
<p>28 <a title="hunch_net-2007-254" href="../hunch_net-2007/hunch_net-2007-07-12-ICML_Trends.html">hunch net-2007-07-12-ICML Trends</a></p>
<p>29 <a title="hunch_net-2007-253" href="../hunch_net-2007/hunch_net-2007-07-06-Idempotent-capable_Predictors.html">hunch net-2007-07-06-Idempotent-capable Predictors</a></p>
<p>30 <a title="hunch_net-2007-252" href="../hunch_net-2007/hunch_net-2007-07-01-Watchword%3A_Online_Learning.html">hunch net-2007-07-01-Watchword: Online Learning</a></p>
<p>31 <a title="hunch_net-2007-251" href="../hunch_net-2007/hunch_net-2007-06-24-Interesting_Papers_at_ICML_2007.html">hunch net-2007-06-24-Interesting Papers at ICML 2007</a></p>
<p>32 <a title="hunch_net-2007-250" href="../hunch_net-2007/hunch_net-2007-06-23-Machine_Learning_Jobs_are_Growing_on_Trees.html">hunch net-2007-06-23-Machine Learning Jobs are Growing on Trees</a></p>
<p>33 <a title="hunch_net-2007-249" href="../hunch_net-2007/hunch_net-2007-06-21-Presentation_Preparation.html">hunch net-2007-06-21-Presentation Preparation</a></p>
<p>34 <a title="hunch_net-2007-248" href="../hunch_net-2007/hunch_net-2007-06-19-How_is_Compressed_Sensing_going_to_change_Machine_Learning_%3F.html">hunch net-2007-06-19-How is Compressed Sensing going to change Machine Learning ?</a></p>
<p>35 <a title="hunch_net-2007-247" href="../hunch_net-2007/hunch_net-2007-06-14-Interesting_Papers_at_COLT_2007.html">hunch net-2007-06-14-Interesting Papers at COLT 2007</a></p>
<p>36 <a title="hunch_net-2007-246" href="../hunch_net-2007/hunch_net-2007-06-13-Not_Posting.html">hunch net-2007-06-13-Not Posting</a></p>
<p>37 <a title="hunch_net-2007-245" href="../hunch_net-2007/hunch_net-2007-05-12-Loss_Function_Semantics.html">hunch net-2007-05-12-Loss Function Semantics</a></p>
<p>38 <a title="hunch_net-2007-244" href="../hunch_net-2007/hunch_net-2007-05-09-The_Missing_Bound.html">hunch net-2007-05-09-The Missing Bound</a></p>
<p>39 <a title="hunch_net-2007-243" href="../hunch_net-2007/hunch_net-2007-05-08-Conditional_Tournaments_for_Multiclass_to_Binary.html">hunch net-2007-05-08-Conditional Tournaments for Multiclass to Binary</a></p>
<p>40 <a title="hunch_net-2007-242" href="../hunch_net-2007/hunch_net-2007-04-30-COLT_2007.html">hunch net-2007-04-30-COLT 2007</a></p>
<p>41 <a title="hunch_net-2007-241" href="../hunch_net-2007/hunch_net-2007-04-28-The_Coming_Patent_Apocalypse.html">hunch net-2007-04-28-The Coming Patent Apocalypse</a></p>
<p>42 <a title="hunch_net-2007-240" href="../hunch_net-2007/hunch_net-2007-04-21-Videolectures.net.html">hunch net-2007-04-21-Videolectures.net</a></p>
<p>43 <a title="hunch_net-2007-239" href="../hunch_net-2007/hunch_net-2007-04-18-%2450K_Spock_Challenge.html">hunch net-2007-04-18-$50K Spock Challenge</a></p>
<p>44 <a title="hunch_net-2007-238" href="../hunch_net-2007/hunch_net-2007-04-13-What_to_do_with_an_unreasonable_conditional_accept.html">hunch net-2007-04-13-What to do with an unreasonable conditional accept</a></p>
<p>45 <a title="hunch_net-2007-237" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">hunch net-2007-04-02-Contextual Scaling</a></p>
<p>46 <a title="hunch_net-2007-236" href="../hunch_net-2007/hunch_net-2007-03-15-Alternative_Machine_Learning_Reductions_Definitions.html">hunch net-2007-03-15-Alternative Machine Learning Reductions Definitions</a></p>
<p>47 <a title="hunch_net-2007-235" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>48 <a title="hunch_net-2007-234" href="../hunch_net-2007/hunch_net-2007-02-22-Create_Your_Own_ICML_Workshop.html">hunch net-2007-02-22-Create Your Own ICML Workshop</a></p>
<p>49 <a title="hunch_net-2007-233" href="../hunch_net-2007/hunch_net-2007-02-16-The_Forgetting.html">hunch net-2007-02-16-The Forgetting</a></p>
<p>50 <a title="hunch_net-2007-232" href="../hunch_net-2007/hunch_net-2007-02-11-24.html">hunch net-2007-02-11-24</a></p>
<p>51 <a title="hunch_net-2007-231" href="../hunch_net-2007/hunch_net-2007-02-10-Best_Practices_for_Collaboration.html">hunch net-2007-02-10-Best Practices for Collaboration</a></p>
<p>52 <a title="hunch_net-2007-230" href="../hunch_net-2007/hunch_net-2007-02-02-Thoughts_regarding_%26%238220%3BIs_machine_learning_different_from_statistics%3F%26%238221%3B.html">hunch net-2007-02-02-Thoughts regarding &#8220;Is machine learning different from statistics?&#8221;</a></p>
<p>53 <a title="hunch_net-2007-229" href="../hunch_net-2007/hunch_net-2007-01-26-Parallel_Machine_Learning_Problems.html">hunch net-2007-01-26-Parallel Machine Learning Problems</a></p>
<p>54 <a title="hunch_net-2007-228" href="../hunch_net-2007/hunch_net-2007-01-15-The_Machine_Learning_Department.html">hunch net-2007-01-15-The Machine Learning Department</a></p>
<p>55 <a title="hunch_net-2007-227" href="../hunch_net-2007/hunch_net-2007-01-10-A_Deep_Belief_Net_Learning_Problem.html">hunch net-2007-01-10-A Deep Belief Net Learning Problem</a></p>
<p>56 <a title="hunch_net-2007-226" href="../hunch_net-2007/hunch_net-2007-01-04-2007_Summer_Machine_Learning_Conferences.html">hunch net-2007-01-04-2007 Summer Machine Learning Conferences</a></p>
<p>57 <a title="hunch_net-2007-225" href="../hunch_net-2007/hunch_net-2007-01-02-Retrospective.html">hunch net-2007-01-02-Retrospective</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
