<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>228 hunch net-2007-01-15-The Machine Learning Department</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2007" href="../home/hunch_net-2007_home.html">hunch_net-2007</a> <a title="hunch_net-2007-228" href="#">hunch_net-2007-228</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>228 hunch net-2007-01-15-The Machine Learning Department</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2007-228-html" href="http://hunch.net/?p=248">html</a></p><p>Introduction: Carnegie MellonSchool of Computer Sciencehas the first academicMachine
Learning department. This department already existed as theCenter for
Automated Learning and Discovery, but recently changed it's name.The reason
for changing the name is obvious: very few people think of themselves as
"Automated Learner and Discoverers", but there are number of people who think
of themselves as "Machine Learners". Machine learning is both more succinct
and recognizable--good properties for a name.A more interesting question is
"Should there be a Machine Learning Department?".Tom Mitchellhas a
relevantwhitepaperclaiming that machine learning is answering a different
question than other fields or departments. The fundamental debate here is "Is
machine learning different from statistics?"At a cultural level, there is no
real debate: they are different. Machine learning is characterized by several
very active large peer reviewed conferences, operating in a computer science
mode. Statistics tends to fun</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 This department already existed as theCenter for Automated Learning and Discovery, but recently changed it's name. [sent-2, score-0.356]
</p><p>2 The reason for changing the name is obvious: very few people think of themselves as "Automated Learner and Discoverers", but there are number of people who think of themselves as "Machine Learners". [sent-3, score-0.117]
</p><p>3 Machine learning is both more succinct and recognizable--good properties for a name. [sent-4, score-0.166]
</p><p>4 A more interesting question is "Should there be a Machine Learning Department? [sent-5, score-0.085]
</p><p>5 Tom Mitchellhas a relevantwhitepaperclaiming that machine learning is answering a different question than other fields or departments. [sent-7, score-0.498]
</p><p>6 The fundamental debate here is "Is machine learning different from statistics? [sent-8, score-0.467]
</p><p>7 "At a cultural level, there is no real debate: they are different. [sent-9, score-0.072]
</p><p>8 Machine learning is characterized by several very active large peer reviewed conferences, operating in a computer science mode. [sent-10, score-0.699]
</p><p>9 Statistics tends to function with a greater emphasis on journals and a lesser emphasis on conferences which often implies a much longer publishing cycle. [sent-11, score-0.327]
</p><p>10 It is true that the core problems of statistics in the past have typically differed from the core problems of machine learning today. [sent-13, score-0.858]
</p><p>11 Yet, there has been some substantial overlap, and there are a number of statisticians nowadays that are actively doing machine learning. [sent-14, score-0.189]
</p><p>12 It's reasonably plausible that in the long term statistics departments will adopt the core problems of machine learning, removing the reasons for a separate machine learning department. [sent-15, score-1.066]
</p><p>13 The parallel question for computer science comes up less often perhaps because computer science is a notoriously broad field. [sent-16, score-1.121]
</p><p>14 The practical implication of a new department is the ability to create a more specific curricula, admit more specific students, and hire faculty based upon more specific interests. [sent-17, score-1.026]
</p><p>15 An alternative solution like "learn everything from computer science and statistics" is personally appealing to me, and I have benefitted from and recommend a broad education. [sent-19, score-0.736]
</p><p>16 In my experience, a machine learning skill set is an effective specialization with which people can do important things in the world. [sent-21, score-0.524]
</p><p>17 Given this, having a department with a machine learning centered curricula seems like a good idea. [sent-22, score-0.811]
</p><p>18 In the future and elsewhere it may have a different name, but the value of the machine learning skill set should grow with research, improving computers, and improving data sources. [sent-24, score-0.725]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('statistics', 0.334), ('computer', 0.272), ('department', 0.269), ('carnegie', 0.196), ('curricula', 0.196), ('science', 0.191), ('machine', 0.189), ('skill', 0.174), ('classes', 0.169), ('dropped', 0.152), ('specific', 0.141), ('debate', 0.126), ('core', 0.125), ('emphasis', 0.123), ('favor', 0.12), ('name', 0.117), ('broad', 0.11), ('automated', 0.105), ('improving', 0.105), ('programming', 0.095), ('compared', 0.095), ('admit', 0.087), ('benefitted', 0.087), ('existed', 0.087), ('discovery', 0.087), ('hire', 0.087), ('mellon', 0.087), ('thecenter', 0.087), ('question', 0.085), ('learning', 0.085), ('practical', 0.084), ('characterized', 0.081), ('succinct', 0.081), ('graphics', 0.081), ('lesser', 0.081), ('architecture', 0.076), ('appealing', 0.076), ('specialization', 0.076), ('faculty', 0.076), ('removing', 0.072), ('centered', 0.072), ('cultural', 0.072), ('adopt', 0.072), ('learners', 0.072), ('answering', 0.072), ('program', 0.07), ('learner', 0.07), ('peer', 0.07), ('different', 0.067), ('languages', 0.067)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="228-tfidf-1" href="../hunch_net-2007/hunch_net-2007-01-15-The_Machine_Learning_Department.html">228 hunch net-2007-01-15-The Machine Learning Department</a></p>
<p>Introduction: Carnegie MellonSchool of Computer Sciencehas the first academicMachine
Learning department. This department already existed as theCenter for
Automated Learning and Discovery, but recently changed it's name.The reason
for changing the name is obvious: very few people think of themselves as
"Automated Learner and Discoverers", but there are number of people who think
of themselves as "Machine Learners". Machine learning is both more succinct
and recognizable--good properties for a name.A more interesting question is
"Should there be a Machine Learning Department?".Tom Mitchellhas a
relevantwhitepaperclaiming that machine learning is answering a different
question than other fields or departments. The fundamental debate here is "Is
machine learning different from statistics?"At a cultural level, there is no
real debate: they are different. Machine learning is characterized by several
very active large peer reviewed conferences, operating in a computer science
mode. Statistics tends to fun</p><p>2 0.15298074 <a title="228-tfidf-2" href="../hunch_net-2009/hunch_net-2009-09-29-Machine_Learning_Protests_at_the_G20.html">372 hunch net-2009-09-29-Machine Learning Protests at the G20</a></p>
<p>Introduction: Themachine learning department at CMUturned out en masse to protest the G20
summit in Pittsburgh.Arthur Grettonuploaded somegreat photoscovering the event</p><p>3 0.15239617 <a title="228-tfidf-3" href="../hunch_net-2008/hunch_net-2008-02-27-The_Stats_Handicap.html">290 hunch net-2008-02-27-The Stats Handicap</a></p>
<p>Introduction: Graduating students in Statistics appear to be at a substantial handicap
compared to graduating students in Machine Learning, despite being in
substantially overlapping subjects.The problem seems to be cultural.
Statistics comes from a mathematics background which emphasizes large
publications slowly published under review at journals. Machine Learning comes
from a Computer Science background which emphasizes quick publishing at
reviewed conferences. This has a number of implications:Graduating statistics
PhDs often have 0-2 publications while graduating machine learning PhDs might
have 5-15.Graduating ML students have had a chance for others to build on
their work. Stats students have had no such chance.Graduating ML students have
attended a number of conferences and presented their work, giving them a
chance to meet people. Stats students have had fewer chances of this sort.In
short, Stats students have had relatively few chances to distinguish
themselves and are heavily reliant on t</p><p>4 0.14702544 <a title="228-tfidf-4" href="../hunch_net-2008/hunch_net-2008-08-18-Radford_Neal_starts_a_blog.html">313 hunch net-2008-08-18-Radford Neal starts a blog</a></p>
<p>Introduction: hereon statistics, ML, CS, and other things he knows well.</p><p>5 0.13517322 <a title="228-tfidf-5" href="../hunch_net-2010/hunch_net-2010-10-17-Partha_Niyogi_has_died.html">414 hunch net-2010-10-17-Partha Niyogi has died</a></p>
<p>Introduction: from brain cancer. I askedMishawho worked with him to write about it.Partha
Niyogi, Louis Block Professor in Computer Science and Statistics at the
University of Chicago passed away on October 1, 2010, aged 43.I first met
Partha Niyogi almost exactly ten years ago when I was a graduate student in
math and he had just started as a faculty in Computer Science and Statistics
at the University of Chicago. Strangely, we first talked at length due to a
somewhat convoluted mathematical argument in a paper on pattern recognition. I
asked him some questions about the paper, and, even though the topic was new
to him, he had put serious thought into it and we started regular meetings. We
made significant progress and developed a line of research stemming initially
just from trying to understand that one paper and to simplify one derivation.
I think this was typical of Partha, showing both his intellectual curiosity
and his intuition for the serendipitous; having a sense and focus for
inquiries wo</p><p>6 0.12539425 <a title="228-tfidf-6" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>7 0.12055659 <a title="228-tfidf-7" href="../hunch_net-2005/hunch_net-2005-04-28-Science_Fiction_and_Research.html">64 hunch net-2005-04-28-Science Fiction and Research</a></p>
<p>8 0.11926886 <a title="228-tfidf-8" href="../hunch_net-2011/hunch_net-2011-09-28-Somebody%26%238217%3Bs_Eating_Your_Lunch.html">445 hunch net-2011-09-28-Somebody&#8217;s Eating Your Lunch</a></p>
<p>9 0.11364479 <a title="228-tfidf-9" href="../hunch_net-2006/hunch_net-2006-10-22-Exemplar_programming.html">215 hunch net-2006-10-22-Exemplar programming</a></p>
<p>10 0.11212301 <a title="228-tfidf-10" href="../hunch_net-2009/hunch_net-2009-08-03-Carbon_in_Computer_Science_Research.html">366 hunch net-2009-08-03-Carbon in Computer Science Research</a></p>
<p>11 0.11018354 <a title="228-tfidf-11" href="../hunch_net-2005/hunch_net-2005-09-14-The_Predictionist_Viewpoint.html">112 hunch net-2005-09-14-The Predictionist Viewpoint</a></p>
<p>12 0.10856166 <a title="228-tfidf-12" href="../hunch_net-2005/hunch_net-2005-05-10-Learning_Reductions_are_Reductionist.html">68 hunch net-2005-05-10-Learning Reductions are Reductionist</a></p>
<p>13 0.1071797 <a title="228-tfidf-13" href="../hunch_net-2005/hunch_net-2005-04-01-Basic_computer_science_research_takes_a_hit.html">50 hunch net-2005-04-01-Basic computer science research takes a hit</a></p>
<p>14 0.10526969 <a title="228-tfidf-14" href="../hunch_net-2005/hunch_net-2005-02-02-Kolmogorov_Complexity_and_Googling.html">10 hunch net-2005-02-02-Kolmogorov Complexity and Googling</a></p>
<p>15 0.10498878 <a title="228-tfidf-15" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>16 0.10404391 <a title="228-tfidf-16" href="../hunch_net-2006/hunch_net-2006-07-11-New_Models.html">194 hunch net-2006-07-11-New Models</a></p>
<p>17 0.10076579 <a title="228-tfidf-17" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>18 0.10006185 <a title="228-tfidf-18" href="../hunch_net-2009/hunch_net-2009-10-03-Static_vs._Dynamic_multiclass_prediction.html">373 hunch net-2009-10-03-Static vs. Dynamic multiclass prediction</a></p>
<p>19 0.094917782 <a title="228-tfidf-19" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>20 0.09201739 <a title="228-tfidf-20" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.222), (1, 0.003), (2, 0.135), (3, -0.062), (4, 0.006), (5, -0.018), (6, -0.07), (7, 0.058), (8, 0.011), (9, -0.048), (10, 0.005), (11, 0.054), (12, 0.096), (13, -0.143), (14, 0.064), (15, 0.109), (16, 0.054), (17, 0.12), (18, 0.098), (19, -0.043), (20, 0.016), (21, -0.035), (22, 0.034), (23, 0.004), (24, -0.009), (25, 0.013), (26, 0.067), (27, 0.089), (28, -0.127), (29, -0.142), (30, 0.06), (31, 0.035), (32, 0.003), (33, -0.091), (34, 0.041), (35, 0.029), (36, -0.036), (37, 0.048), (38, 0.114), (39, 0.038), (40, 0.023), (41, -0.055), (42, -0.101), (43, 0.134), (44, -0.03), (45, 0.055), (46, -0.151), (47, 0.06), (48, -0.079), (49, -0.031)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.91782504 <a title="228-lsi-1" href="../hunch_net-2007/hunch_net-2007-01-15-The_Machine_Learning_Department.html">228 hunch net-2007-01-15-The Machine Learning Department</a></p>
<p>Introduction: Carnegie MellonSchool of Computer Sciencehas the first academicMachine
Learning department. This department already existed as theCenter for
Automated Learning and Discovery, but recently changed it's name.The reason
for changing the name is obvious: very few people think of themselves as
"Automated Learner and Discoverers", but there are number of people who think
of themselves as "Machine Learners". Machine learning is both more succinct
and recognizable--good properties for a name.A more interesting question is
"Should there be a Machine Learning Department?".Tom Mitchellhas a
relevantwhitepaperclaiming that machine learning is answering a different
question than other fields or departments. The fundamental debate here is "Is
machine learning different from statistics?"At a cultural level, there is no
real debate: they are different. Machine learning is characterized by several
very active large peer reviewed conferences, operating in a computer science
mode. Statistics tends to fun</p><p>2 0.70950669 <a title="228-lsi-2" href="../hunch_net-2005/hunch_net-2005-04-28-Science_Fiction_and_Research.html">64 hunch net-2005-04-28-Science Fiction and Research</a></p>
<p>Introduction: A big part of doing research is imagining how things could be different, and
then trying to figure out how to get there.A big part of science fiction is
imagining how things could be different, and then working through the
implications.Because of the similarity here, reading science fiction can
sometimes be helpful in understanding and doing research. (And, hey, it's
fun.) Here's some list of science fiction books I enjoyed which seem
particularly relevant to computer science and (sometimes) learning
systems:Vernor Vinge, "True Names", "A Fire Upon the Deep"Marc Stiegler,
"David's Sling", "Earthweb"Charles Stross, "Singularity Sky"Greg Egan,
"Diaspora"Joe Haldeman, "Forever Peace"(There are surely many
others.)Incidentally, the nature of science fiction itself has changed.
Decades ago, science fiction projected great increases in the power humans
control (example: E.E. Smith Lensman series). That didn't really happen in the
last 50 years. Instead, we gradually refined the degree to whi</p><p>3 0.70163268 <a title="228-lsi-3" href="../hunch_net-2008/hunch_net-2008-02-27-The_Stats_Handicap.html">290 hunch net-2008-02-27-The Stats Handicap</a></p>
<p>Introduction: Graduating students in Statistics appear to be at a substantial handicap
compared to graduating students in Machine Learning, despite being in
substantially overlapping subjects.The problem seems to be cultural.
Statistics comes from a mathematics background which emphasizes large
publications slowly published under review at journals. Machine Learning comes
from a Computer Science background which emphasizes quick publishing at
reviewed conferences. This has a number of implications:Graduating statistics
PhDs often have 0-2 publications while graduating machine learning PhDs might
have 5-15.Graduating ML students have had a chance for others to build on
their work. Stats students have had no such chance.Graduating ML students have
attended a number of conferences and presented their work, giving them a
chance to meet people. Stats students have had fewer chances of this sort.In
short, Stats students have had relatively few chances to distinguish
themselves and are heavily reliant on t</p><p>4 0.6415844 <a title="228-lsi-4" href="../hunch_net-2005/hunch_net-2005-09-14-The_Predictionist_Viewpoint.html">112 hunch net-2005-09-14-The Predictionist Viewpoint</a></p>
<p>Introduction: Virtually every discipline of significant human endeavor has a way explaining
itself as fundamental and important. In all the cases I know of, they are both
right (they are vital) and wrong (they are not solely vital).Politics. This is
the one that everyone is familiar with at the moment. "What could be more
important than the process of making decisions?"Science and Technology. This
is the one that we-the-academics are familiar with. "The loss of modern
science and technology would be catastrophic."Military. "Without the military,
a nation will be invaded and destroyed."(insert your favorite here)Within
science and technology, the same thing happens again.Mathematics. "What could
be more important than a precise language for establishing truths?"Physics.
"Nothing is more fundamental than the laws which govern the universe.
Understanding them is the key to understanding everything else."Biology.
"Without life, we wouldn't be here, so clearly the study of life is
fundamental."Computer S</p><p>5 0.63145989 <a title="228-lsi-5" href="../hunch_net-2010/hunch_net-2010-10-17-Partha_Niyogi_has_died.html">414 hunch net-2010-10-17-Partha Niyogi has died</a></p>
<p>Introduction: from brain cancer. I askedMishawho worked with him to write about it.Partha
Niyogi, Louis Block Professor in Computer Science and Statistics at the
University of Chicago passed away on October 1, 2010, aged 43.I first met
Partha Niyogi almost exactly ten years ago when I was a graduate student in
math and he had just started as a faculty in Computer Science and Statistics
at the University of Chicago. Strangely, we first talked at length due to a
somewhat convoluted mathematical argument in a paper on pattern recognition. I
asked him some questions about the paper, and, even though the topic was new
to him, he had put serious thought into it and we started regular meetings. We
made significant progress and developed a line of research stemming initially
just from trying to understand that one paper and to simplify one derivation.
I think this was typical of Partha, showing both his intellectual curiosity
and his intuition for the serendipitous; having a sense and focus for
inquiries wo</p><p>6 0.59917992 <a title="228-lsi-6" href="../hunch_net-2009/hunch_net-2009-08-03-Carbon_in_Computer_Science_Research.html">366 hunch net-2009-08-03-Carbon in Computer Science Research</a></p>
<p>7 0.5876044 <a title="228-lsi-7" href="../hunch_net-2006/hunch_net-2006-10-22-Exemplar_programming.html">215 hunch net-2006-10-22-Exemplar programming</a></p>
<p>8 0.54818219 <a title="228-lsi-8" href="../hunch_net-2009/hunch_net-2009-09-29-Machine_Learning_Protests_at_the_G20.html">372 hunch net-2009-09-29-Machine Learning Protests at the G20</a></p>
<p>9 0.53640133 <a title="228-lsi-9" href="../hunch_net-2009/hunch_net-2009-01-28-Nielsen%26%238217%3Bs_talk.html">340 hunch net-2009-01-28-Nielsen&#8217;s talk</a></p>
<p>10 0.52430516 <a title="228-lsi-10" href="../hunch_net-2009/hunch_net-2009-01-08-Predictive_Analytics_World.html">335 hunch net-2009-01-08-Predictive Analytics World</a></p>
<p>11 0.52325815 <a title="228-lsi-11" href="../hunch_net-2008/hunch_net-2008-05-25-Inappropriate_Mathematics_for_Machine_Learning.html">302 hunch net-2008-05-25-Inappropriate Mathematics for Machine Learning</a></p>
<p>12 0.51943374 <a title="228-lsi-12" href="../hunch_net-2006/hunch_net-2006-04-02-Mad_%28Neuro%29science.html">168 hunch net-2006-04-02-Mad (Neuro)science</a></p>
<p>13 0.50927782 <a title="228-lsi-13" href="../hunch_net-2007/hunch_net-2007-09-16-Optimizing_Machine_Learning_Programs.html">262 hunch net-2007-09-16-Optimizing Machine Learning Programs</a></p>
<p>14 0.50868869 <a title="228-lsi-14" href="../hunch_net-2007/hunch_net-2007-11-02-The_Machine_Learning_Award_goes_to_%26%238230%3B.html">270 hunch net-2007-11-02-The Machine Learning Award goes to &#8230;</a></p>
<p>15 0.50589174 <a title="228-lsi-15" href="../hunch_net-2005/hunch_net-2005-09-04-Science_in_the_Government.html">106 hunch net-2005-09-04-Science in the Government</a></p>
<p>16 0.5024035 <a title="228-lsi-16" href="../hunch_net-2012/hunch_net-2012-07-09-Videolectures.html">469 hunch net-2012-07-09-Videolectures</a></p>
<p>17 0.50055003 <a title="228-lsi-17" href="../hunch_net-2008/hunch_net-2008-01-06-Research_Political_Issues.html">282 hunch net-2008-01-06-Research Political Issues</a></p>
<p>18 0.50008732 <a title="228-lsi-18" href="../hunch_net-2005/hunch_net-2005-04-10-Is_the_Goal_Understanding_or_Prediction%3F.html">55 hunch net-2005-04-10-Is the Goal Understanding or Prediction?</a></p>
<p>19 0.49935961 <a title="228-lsi-19" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>20 0.49570248 <a title="228-lsi-20" href="../hunch_net-2009/hunch_net-2009-04-21-Interesting_Presentations_at_Snowbird.html">349 hunch net-2009-04-21-Interesting Presentations at Snowbird</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(35, 0.421), (42, 0.252), (45, 0.018), (68, 0.056), (74, 0.139), (95, 0.021)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.98921967 <a title="228-lda-1" href="../hunch_net-2010/hunch_net-2010-12-04-Vowpal_Wabbit%2C_version_5.0%2C_and_the_second_heresy.html">419 hunch net-2010-12-04-Vowpal Wabbit, version 5.0, and the second heresy</a></p>
<p>Introduction: I've releasedversion 5.0of theVowpal Wabbitonline learning software. The major
number has changed since thelast releasebecause I regard all earlier versions
as obsolete--there are several new algorithms & features including substantial
changes and upgrades to the default learning algorithm.The biggest changes are
new algorithms:Nikosand I improved the default algorithm. The basic update
rule still uses gradient descent, but the size of the update is carefully
controlled so that it's impossible to overrun the label. In addition, the
normalization has changed. Computationally, these changes are virtually free
and yield better results, sometimes much better. Less careful updates can be
reenabled with -loss_function classic, although results are still not
identical to previous due to normalization changes.Nikos also implemented the
per-feature learning rates as per thesetwopapers. Often, this works better
than the default algorithm. It isn't the default because it isn't (yet) as
adaptable</p><p>2 0.97610551 <a title="228-lda-2" href="../hunch_net-2005/hunch_net-2005-05-29-Maximum_Margin_Mismatch%3F.html">77 hunch net-2005-05-29-Maximum Margin Mismatch?</a></p>
<p>Introduction: John makes a fascinating point about structured classification (and slightly
scooped my post!). Maximum Margin Markov Networks (M3N) are an interesting
example of the second class of structured classifiers (where the
classification of one label depends on the others), and one of my favorite
papers. I'm not alone: the paper won the best student paper award at NIPS in
2003.There are some things I find odd about the paper. For instance, it says
of probabilistic models"cannot handle high dimensional feature spaces and lack
strong theoretical guarrantees."I'm aware of no such limitations.
Also:"Unfortunately, even probabilistic graphical models that are trained
discriminatively do not achieve the same level of performance as SVMs,
especially when kernel features are used."This is quite interesting and
contradicts my own experience as well as that of a number of
peopleIgreatlyrespect. I wonder what the root cause is: perhaps there is
something different about the data Ben+Carlos were working</p><p>3 0.97513598 <a title="228-lda-3" href="../hunch_net-2006/hunch_net-2006-05-16-The_value_of_the_orthodox_view_of_Boosting.html">179 hunch net-2006-05-16-The value of the orthodox view of Boosting</a></p>
<p>Introduction: The term "boosting" comes from the idea of using a meta-algorithm which takes
"weak" learners (that may be able to only barely predict slightly better than
random) and turn them into strongly capable learners (which predict very
well).Adaboostin 1995 was the first widely used (and useful) boosting
algorithm, although there were theoretical boosting algorithms floating around
since 1990 (see the bottom ofthis page).Since then, many different
interpretations of why boosting works have arisen. There is significant
discussion about these different views in theannals of statistics, including
aresponsebyYoav FreundandRobert Schapire.I believe there is a great deal of
value to be found in the original view of boosting (meta-algorithm for
creating a strong learner from a weak learner). This is not a claim that one
particular viewpoint obviates the value of all others, but rather that no
other viewpoint seems to really capture important properties.Comparing with
all other views of boosting is t</p><p>4 0.96186155 <a title="228-lda-4" href="../hunch_net-2005/hunch_net-2005-06-13-Wikis_for_Summer_Schools_and_Workshops.html">81 hunch net-2005-06-13-Wikis for Summer Schools and Workshops</a></p>
<p>Introduction: Chicago '05ended a couple of weeks ago. This was the sixthMachine Learning
Summer School, and the second one that used awiki. (The first was Berder '04,
thanks to Gunnar Raetsch.) Wikis are relatively easy to set up, greatly aid
social interaction, and should be used a lot more at summer schools and
workshops. They can even be used as the meeting's webpage, as a permanent
record of its participants' collaborations -- see for example the wiki/website
for last year'sNVO Summer School.A basic wiki is a collection of editable
webpages, maintained by software called awiki engine. The engine used at both
Berder and Chicago wasTikiWiki-- it is well documented and gets you something
running fast. It uses PHP and MySQL, but doesn't require you to know either.
Tikiwiki has far more features than most wikis, as it is really a fullContent
Management System. (My thanks to Sebastian Stark for pointing this out.) Here
are the features we found most useful:Bulletin boards, or forums. The most-
used on</p><p>5 0.95934767 <a title="228-lda-5" href="../hunch_net-2006/hunch_net-2006-06-24-Online_convex_optimization_at_COLT.html">186 hunch net-2006-06-24-Online convex optimization at COLT</a></p>
<p>Introduction: AtICML 2003,Marty Zinkevichproposedthe online convex optimization setting and
showed that a particular gradient descent algorithm has regret O(T0.5) with
respect to the best predictor where T is the number of rounds. This seems to
be a nice model for online learning, and there has been some significant
follow-up work.AtCOLT 2006Elad Hazan,Adam Kalai,Satyen Kale, andAmit
Agarwalpresenteda modification which takes a Newton stepguaranteeing O(log T)
regret when the first and second derivatives are bounded.Then they applied
these algorithms to portfolio managementatICML 2006(withRobert Schapire)
yielding some very fun graphs.</p><p>6 0.95464742 <a title="228-lda-6" href="../hunch_net-2005/hunch_net-2005-05-21-What_is_the_right_form_of_modularity_in_structured_prediction%3F.html">74 hunch net-2005-05-21-What is the right form of modularity in structured prediction?</a></p>
<p>same-blog 7 0.93171632 <a title="228-lda-7" href="../hunch_net-2007/hunch_net-2007-01-15-The_Machine_Learning_Department.html">228 hunch net-2007-01-15-The Machine Learning Department</a></p>
<p>8 0.9038313 <a title="228-lda-8" href="../hunch_net-2013/hunch_net-2013-09-20-No_NY_ML_Symposium_in_2013%2C_and_some_good_news.html">489 hunch net-2013-09-20-No NY ML Symposium in 2013, and some good news</a></p>
<p>9 0.89822602 <a title="228-lda-9" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>10 0.77651399 <a title="228-lda-10" href="../hunch_net-2006/hunch_net-2006-03-27-Gradients_everywhere.html">167 hunch net-2006-03-27-Gradients everywhere</a></p>
<p>11 0.77581 <a title="228-lda-11" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">23 hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<p>12 0.76717609 <a title="228-lda-12" href="../hunch_net-2012/hunch_net-2012-06-05-ICML_acceptance_statistics.html">466 hunch net-2012-06-05-ICML acceptance statistics</a></p>
<p>13 0.75070041 <a title="228-lda-13" href="../hunch_net-2005/hunch_net-2005-03-10-Breaking_Abstractions.html">39 hunch net-2005-03-10-Breaking Abstractions</a></p>
<p>14 0.74971801 <a title="228-lda-14" href="../hunch_net-2005/hunch_net-2005-04-28-Science_Fiction_and_Research.html">64 hunch net-2005-04-28-Science Fiction and Research</a></p>
<p>15 0.74953651 <a title="228-lda-15" href="../hunch_net-2011/hunch_net-2011-08-15-Vowpal_Wabbit_6.0.html">441 hunch net-2011-08-15-Vowpal Wabbit 6.0</a></p>
<p>16 0.74646437 <a title="228-lda-16" href="../hunch_net-2008/hunch_net-2008-08-04-Electoralmarkets.com.html">312 hunch net-2008-08-04-Electoralmarkets.com</a></p>
<p>17 0.73478574 <a title="228-lda-17" href="../hunch_net-2008/hunch_net-2008-12-07-A_NIPS_paper.html">330 hunch net-2008-12-07-A NIPS paper</a></p>
<p>18 0.73353928 <a title="228-lda-18" href="../hunch_net-2005/hunch_net-2005-09-12-Fast_Gradient_Descent.html">111 hunch net-2005-09-12-Fast Gradient Descent</a></p>
<p>19 0.73118931 <a title="228-lda-19" href="../hunch_net-2013/hunch_net-2013-11-09-Graduates_and_Postdocs.html">490 hunch net-2013-11-09-Graduates and Postdocs</a></p>
<p>20 0.71925294 <a title="228-lda-20" href="../hunch_net-2009/hunch_net-2009-09-21-Netflix_finishes_%28and_starts%29.html">371 hunch net-2009-09-21-Netflix finishes (and starts)</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
