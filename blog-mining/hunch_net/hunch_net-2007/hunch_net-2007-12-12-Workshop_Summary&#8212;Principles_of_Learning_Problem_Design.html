<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>277 hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2007" href="../home/hunch_net-2007_home.html">hunch_net-2007</a> <a title="hunch_net-2007-277" href="#">hunch_net-2007-277</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>277 hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2007-277-html" href="http://hunch.net/?p=305">html</a></p><p>Introduction: This is a summary of theworkshop on Learning Problem DesignwhichAlinaand I ran
atNIPSthis year.The first question many people have is "What is learning
problem design?" This workshop is about admitting that solving learning
problems does not start with labeled data, but rather somewhere before. When
humans are hired to produce labels, this is usually not a serious problem
because you can tell them precisely what semantics you want the labels to
have, and we can fix some set of features in advance. However, when other
methods are used this becomes more problematic. This focus is important for
Machine Learning because there are very large quantities of data which are not
labeled by a hired human.The title of the workshop was a bit ambitious,
because a workshop is not long enough to synthesize a diversity of approaches
into a coherent set of principles. For me, the posters at the end of the
workshop were quite helpful in getting approaches to gel.Here are some answers
to "where do the lab</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 " This workshop is about admitting that solving learning problems does not start with labeled data, but rather somewhere before. [sent-3, score-0.485]
</p><p>2 When humans are hired to produce labels, this is usually not a serious problem because you can tell them precisely what semantics you want the labels to have, and we can fix some set of features in advance. [sent-4, score-0.606]
</p><p>3 This focus is important for Machine Learning because there are very large quantities of data which are not labeled by a hired human. [sent-6, score-0.443]
</p><p>4 The title of the workshop was a bit ambitious, because a workshop is not long enough to synthesize a diversity of approaches into a coherent set of principles. [sent-7, score-0.846]
</p><p>5 For me, the posters at the end of the workshop were quite helpful in getting approaches to gel. [sent-8, score-0.44]
</p><p>6 Here are some answers to "where do the labels come from? [sent-9, score-0.423]
</p><p>7 ":SimulationUse a simulator (which need not be that good) to predict the cost of various choices and turn that into label information. [sent-10, score-0.275]
</p><p>8 Ashutoshhad some cool demos showing the power of this approach. [sent-11, score-0.181]
</p><p>9 Luisoften used an agreement mechanism to induce labels with games. [sent-14, score-0.507]
</p><p>10 Huzefa's work on bioprediction can be thought of as partly using agreement with previous structures to simulate the label of a new structure. [sent-16, score-0.563]
</p><p>11 Some answers to "where do the data come from" are:EverywhereThe essential idea is to integrate as many data sources as possible. [sent-20, score-0.572]
</p><p>12 Rakeshhad several algorithms which (in combination) allowed him to use a large number of diverse data sources in a text domain. [sent-21, score-0.233]
</p><p>13 SparsityA representation is formed by finding a sparse set of basis functions on otherwise totally unlabeled data. [sent-22, score-0.441]
</p><p>14 Self-predictionA representation is formed by learning to self-predict a set of raw features. [sent-24, score-0.433]
</p><p>15 A workshop like this is successful if it informs the questions we ask (and answer) in the future. [sent-26, score-0.43]
</p><p>16 Some natural questions (some of which were discussed) are:What is a natural, sufficient langauge for adding prior information into a learning system? [sent-27, score-0.251]
</p><p>17 Shaidescribed a sense in which kernels are insufficient as a language for prior information. [sent-29, score-0.477]
</p><p>18 Bayesian analysis emphasizes reasoning about the parameters of the model, but the language of examples or maybe label expectations may be more natural. [sent-30, score-0.512]
</p><p>19 That's to be expected for a direction of research which is just starting, but it's important to modularize these techniques so they can be repeatedly and easily applied. [sent-35, score-0.208]
</p><p>20 The other approaches and questions are essentially unexplored territory where some serious thinking may be helpful. [sent-39, score-0.538]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('workshop', 0.215), ('labels', 0.208), ('hired', 0.208), ('modularize', 0.208), ('agreement', 0.207), ('label', 0.19), ('insufficient', 0.185), ('formed', 0.161), ('approaches', 0.147), ('lists', 0.143), ('prior', 0.128), ('data', 0.124), ('questions', 0.123), ('answers', 0.115), ('labeled', 0.111), ('sources', 0.109), ('power', 0.104), ('come', 0.1), ('set', 0.099), ('representation', 0.096), ('induce', 0.092), ('atnipsthis', 0.092), ('compiling', 0.092), ('signals', 0.092), ('ambitious', 0.092), ('informs', 0.092), ('territory', 0.092), ('serious', 0.091), ('language', 0.09), ('simulator', 0.085), ('synthesize', 0.085), ('unexplored', 0.085), ('simulate', 0.085), ('somewhere', 0.085), ('diversity', 0.085), ('totally', 0.085), ('formalize', 0.085), ('talk', 0.083), ('emphasizes', 0.081), ('structures', 0.081), ('helpful', 0.078), ('talked', 0.077), ('demos', 0.077), ('raw', 0.077), ('expectations', 0.077), ('listed', 0.077), ('items', 0.077), ('kernels', 0.074), ('reasoning', 0.074), ('admitting', 0.074)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000005 <a title="277-tfidf-1" href="../hunch_net-2007/hunch_net-2007-12-12-Workshop_Summary%26%238212%3BPrinciples_of_Learning_Problem_Design.html">277 hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</a></p>
<p>Introduction: This is a summary of theworkshop on Learning Problem DesignwhichAlinaand I ran
atNIPSthis year.The first question many people have is "What is learning
problem design?" This workshop is about admitting that solving learning
problems does not start with labeled data, but rather somewhere before. When
humans are hired to produce labels, this is usually not a serious problem
because you can tell them precisely what semantics you want the labels to
have, and we can fix some set of features in advance. However, when other
methods are used this becomes more problematic. This focus is important for
Machine Learning because there are very large quantities of data which are not
labeled by a hired human.The title of the workshop was a bit ambitious,
because a workshop is not long enough to synthesize a diversity of approaches
into a coherent set of principles. For me, the posters at the end of the
workshop were quite helpful in getting approaches to gel.Here are some answers
to "where do the lab</p><p>2 0.1538672 <a title="277-tfidf-2" href="../hunch_net-2005/hunch_net-2005-03-22-Active_learning.html">45 hunch net-2005-03-22-Active learning</a></p>
<p>Introduction: Often, unlabeled data is easy to come by but labels are expensive. For
instance, if you're building a speech recognizer, it's easy enough to get raw
speech samples -- just walk around with a microphone -- but labeling even one
of these samples is a tedious process in which a human must examine the speech
signal and carefully segment it into phonemes. In the field of active
learning, the goal is as usual to construct an accurate classifier, but the
labels of the data points are initially hidden and there is a charge for each
label you want revealed. The hope is that by intelligent adaptive querying,
you can get away with significantly fewer labels than you would need in a
regular supervised learning framework.Here's an example. Suppose the data lie
on the real line, and the classifiers are simple thresholding functions, H =
{hw}:hw(x) = 1 if x > w, and 0 otherwise.VC theory tells us that if the
underlying distribution P can be classified perfectly by some hypothesis in H
(called thereal</p><p>3 0.13949285 <a title="277-tfidf-3" href="../hunch_net-2010/hunch_net-2010-12-26-NIPS_2010.html">420 hunch net-2010-12-26-NIPS 2010</a></p>
<p>Introduction: I enjoyed attendingNIPSthis year, with several things interesting me. For the
conference itself:Peter Welinder,Steve Branson,Serge Belongie, andPietro
Perona,The Multidimensional Wisdom of Crowds. This paper is about
usingmechanical turkto get label information, with results superior to a
majority vote approach.David McAllester,Tamir Hazan, andJoseph KeshetDirect
Loss Minimization for Structured Prediction. This is about another technique
for directly optimizing the loss in structured prediction, with an application
to speech recognition.Mohammad SaberianandNuno VasconcelosBoosting Classifier
Cascades. This is about an algorithm for simultaneously optimizing loss and
computation in a classifier cascade construction. There were several other
papers on cascades which are worth looking at if interested.Alan FernandPrasad
Tadepalli,A Computational Decision Theory for Interactive Assistants. This
paper carves out some forms of natural not-MDP problems and shows their RL-
style solution is t</p><p>4 0.13548082 <a title="277-tfidf-4" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>Introduction: AtKDDI enjoyedStephen Boyd's invited talk about optimization quite a bit.
However, the most interesting talk for me wasDavid Haussler's. His talk
started out with a formidable load of biological complexity. About half-way
through you start wondering, "can this be used to help with cancer?" And at
the end he connects it directly to use with a call to arms for the audience:
cure cancer. The core thesis here is that cancer is a complex set of diseases
which can be distentangled via genetic assays, allowing attacking the specific
signature of individual cancers. However, the data quantity and complex
dependencies within the data require systematic and relatively automatic
prediction and analysis algorithms of the kind that we are best familiar
with.Some of the papers which interested me are:Kai-Wei ChangandDan
Roth,Selective Block Minimization for Faster Convergence of Limited Memory
Large-Scale Linear Models, which is about effectively using a hard-example
cache to speedup learning.Leland</p><p>5 0.13267009 <a title="277-tfidf-5" href="../hunch_net-2007/hunch_net-2007-02-22-Create_Your_Own_ICML_Workshop.html">234 hunch net-2007-02-22-Create Your Own ICML Workshop</a></p>
<p>Introduction: As usualICML 2007will be hosting aworkshop programto be held this year on June
24th. The success of the program depends on having researchers like you
propose interesting workshop topics and then organize the workshops. I'd like
to encourage all of you to consider sending a workshop proposal. The proposal
deadline has been extended to March 5. See the workshop web-site for
details.Organizing a workshop is a unique way to gather an international group
of researchers together to focus for an entire day on a topic of your
choosing. I've always found that the cost of organizing a workshop is not so
large, and very low compared to the benefits. The topic and format of a
workshop are limited only by your imagination (and the attractiveness to
potential participants) and need not follow the usual model of a mini-
conference on a particular ML sub-area. Hope to see some interesting proposals
rolling in.</p><p>6 0.13009882 <a title="277-tfidf-6" href="../hunch_net-2005/hunch_net-2005-06-22-Languages__of_Learning.html">84 hunch net-2005-06-22-Languages  of Learning</a></p>
<p>7 0.12980792 <a title="277-tfidf-7" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>8 0.12385117 <a title="277-tfidf-8" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>9 0.1155597 <a title="277-tfidf-9" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>10 0.11478251 <a title="277-tfidf-10" href="../hunch_net-2006/hunch_net-2006-03-23-The_Approximation_Argument.html">165 hunch net-2006-03-23-The Approximation Argument</a></p>
<p>11 0.11325727 <a title="277-tfidf-11" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>12 0.11035816 <a title="277-tfidf-12" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>13 0.11030148 <a title="277-tfidf-13" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>14 0.10881724 <a title="277-tfidf-14" href="../hunch_net-2006/hunch_net-2006-09-28-Programming_Languages_for_Machine_Learning_Implementations.html">210 hunch net-2006-09-28-Programming Languages for Machine Learning Implementations</a></p>
<p>15 0.10827331 <a title="277-tfidf-15" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>16 0.10798195 <a title="277-tfidf-16" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>17 0.10651006 <a title="277-tfidf-17" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>18 0.10536183 <a title="277-tfidf-18" href="../hunch_net-2007/hunch_net-2007-10-14-NIPS_workshp%3A_Learning_Problem_Design.html">265 hunch net-2007-10-14-NIPS workshp: Learning Problem Design</a></p>
<p>19 0.10409235 <a title="277-tfidf-19" href="../hunch_net-2005/hunch_net-2005-09-20-Workshop_Proposal%3A_Atomic_Learning.html">114 hunch net-2005-09-20-Workshop Proposal: Atomic Learning</a></p>
<p>20 0.10404182 <a title="277-tfidf-20" href="../hunch_net-2005/hunch_net-2005-03-24-The_Role_of_Workshops.html">46 hunch net-2005-03-24-The Role of Workshops</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.255), (1, -0.075), (2, 0.081), (3, 0.055), (4, -0.078), (5, -0.006), (6, -0.14), (7, -0.075), (8, -0.062), (9, 0.134), (10, -0.086), (11, 0.112), (12, 0.016), (13, 0.037), (14, -0.015), (15, -0.12), (16, 0.019), (17, 0.042), (18, 0.051), (19, -0.034), (20, -0.12), (21, 0.02), (22, 0.141), (23, 0.027), (24, -0.049), (25, -0.054), (26, -0.043), (27, -0.103), (28, -0.008), (29, 0.032), (30, 0.078), (31, -0.051), (32, -0.028), (33, 0.011), (34, -0.028), (35, 0.005), (36, -0.018), (37, 0.004), (38, -0.038), (39, 0.042), (40, 0.041), (41, 0.065), (42, -0.047), (43, -0.035), (44, -0.051), (45, 0.044), (46, 0.056), (47, 0.058), (48, 0.067), (49, -0.009)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94386536 <a title="277-lsi-1" href="../hunch_net-2007/hunch_net-2007-12-12-Workshop_Summary%26%238212%3BPrinciples_of_Learning_Problem_Design.html">277 hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</a></p>
<p>Introduction: This is a summary of theworkshop on Learning Problem DesignwhichAlinaand I ran
atNIPSthis year.The first question many people have is "What is learning
problem design?" This workshop is about admitting that solving learning
problems does not start with labeled data, but rather somewhere before. When
humans are hired to produce labels, this is usually not a serious problem
because you can tell them precisely what semantics you want the labels to
have, and we can fix some set of features in advance. However, when other
methods are used this becomes more problematic. This focus is important for
Machine Learning because there are very large quantities of data which are not
labeled by a hired human.The title of the workshop was a bit ambitious,
because a workshop is not long enough to synthesize a diversity of approaches
into a coherent set of principles. For me, the posters at the end of the
workshop were quite helpful in getting approaches to gel.Here are some answers
to "where do the lab</p><p>2 0.67453027 <a title="277-lsi-2" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>Introduction: AtKDDI enjoyedStephen Boyd's invited talk about optimization quite a bit.
However, the most interesting talk for me wasDavid Haussler's. His talk
started out with a formidable load of biological complexity. About half-way
through you start wondering, "can this be used to help with cancer?" And at
the end he connects it directly to use with a call to arms for the audience:
cure cancer. The core thesis here is that cancer is a complex set of diseases
which can be distentangled via genetic assays, allowing attacking the specific
signature of individual cancers. However, the data quantity and complex
dependencies within the data require systematic and relatively automatic
prediction and analysis algorithms of the kind that we are best familiar
with.Some of the papers which interested me are:Kai-Wei ChangandDan
Roth,Selective Block Minimization for Faster Convergence of Limited Memory
Large-Scale Linear Models, which is about effectively using a hard-example
cache to speedup learning.Leland</p><p>3 0.63902843 <a title="277-lsi-3" href="../hunch_net-2005/hunch_net-2005-09-20-Workshop_Proposal%3A_Atomic_Learning.html">114 hunch net-2005-09-20-Workshop Proposal: Atomic Learning</a></p>
<p>Introduction: This is a proposal for a workshop. It may or may not happen depending on the
level of interest. If you are interested, feel free to indicate so (by email
or comments).Description:Assume(*) that any system for solving large difficult
learning problems must decompose into repeated use of basic elements (i.e.
atoms). There are many basic questions which remain:What are the viable basic
elements?What makes a basic element viable?What are the viable principles for
the composition of these basic elements?What are the viable principles for
learning in such systems?What problems can this approach handle?Hal Daume
adds:Can composition of atoms be (semi-) automatically constructed[?]When
atoms are constructed through reductions, is there some notion of the
"naturalness" of the created leaning problems?Other than Markov
fields/graphical models/Bayes nets, is there a good language for representing
atoms and their compositions?The answer to these and related questions remain
unclear to me. A worksh</p><p>4 0.59871966 <a title="277-lsi-4" href="../hunch_net-2008/hunch_net-2008-10-01-NIPS_2008_workshop_on_%26%238216%3BLearning_over_Empirical_Hypothesis_Spaces%26%238217%3B.html">319 hunch net-2008-10-01-NIPS 2008 workshop on &#8216;Learning over Empirical Hypothesis Spaces&#8217;</a></p>
<p>Introduction: This workshop asks for insights how far we may/can push the theoretical
boundary of using data in the design of learning machines. Can we express our
classification rule in terms of the sample, or do we have to stick to a core
assumption of classical statistical learning theory, namely that the
hypothesis space is to be defined independent from the sample? This workshop
is particularly interested in - but not restricted to - the 'luckiness
framework' and the recently introduced notion of 'compatibility functions' in
a semi-supervised learning context (more information can be found
athttp://www.kuleuven.be/wehys).</p><p>5 0.5810051 <a title="277-lsi-5" href="../hunch_net-2010/hunch_net-2010-12-26-NIPS_2010.html">420 hunch net-2010-12-26-NIPS 2010</a></p>
<p>Introduction: I enjoyed attendingNIPSthis year, with several things interesting me. For the
conference itself:Peter Welinder,Steve Branson,Serge Belongie, andPietro
Perona,The Multidimensional Wisdom of Crowds. This paper is about
usingmechanical turkto get label information, with results superior to a
majority vote approach.David McAllester,Tamir Hazan, andJoseph KeshetDirect
Loss Minimization for Structured Prediction. This is about another technique
for directly optimizing the loss in structured prediction, with an application
to speech recognition.Mohammad SaberianandNuno VasconcelosBoosting Classifier
Cascades. This is about an algorithm for simultaneously optimizing loss and
computation in a classifier cascade construction. There were several other
papers on cascades which are worth looking at if interested.Alan FernandPrasad
Tadepalli,A Computational Decision Theory for Interactive Assistants. This
paper carves out some forms of natural not-MDP problems and shows their RL-
style solution is t</p><p>6 0.57889158 <a title="277-lsi-6" href="../hunch_net-2007/hunch_net-2007-10-14-NIPS_workshp%3A_Learning_Problem_Design.html">265 hunch net-2007-10-14-NIPS workshp: Learning Problem Design</a></p>
<p>7 0.55546308 <a title="277-lsi-7" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>8 0.55409944 <a title="277-lsi-8" href="../hunch_net-2006/hunch_net-2006-03-05-%26%238220%3BStructural%26%238221%3B_Learning.html">161 hunch net-2006-03-05-&#8220;Structural&#8221; Learning</a></p>
<p>9 0.55038708 <a title="277-lsi-9" href="../hunch_net-2008/hunch_net-2008-10-19-NIPS_2008_workshop_on_Kernel_Learning.html">321 hunch net-2008-10-19-NIPS 2008 workshop on Kernel Learning</a></p>
<p>10 0.53672987 <a title="277-lsi-10" href="../hunch_net-2005/hunch_net-2005-03-22-Active_learning.html">45 hunch net-2005-03-22-Active learning</a></p>
<p>11 0.53552312 <a title="277-lsi-11" href="../hunch_net-2006/hunch_net-2006-02-18-Multiplication_of_Learned_Probabilities_is_Dangerous.html">157 hunch net-2006-02-18-Multiplication of Learned Probabilities is Dangerous</a></p>
<p>12 0.52620322 <a title="277-lsi-12" href="../hunch_net-2006/hunch_net-2006-09-28-Programming_Languages_for_Machine_Learning_Implementations.html">210 hunch net-2006-09-28-Programming Languages for Machine Learning Implementations</a></p>
<p>13 0.51589292 <a title="277-lsi-13" href="../hunch_net-2006/hunch_net-2006-03-02-Why_do_people_count_for_learning%3F.html">160 hunch net-2006-03-02-Why do people count for learning?</a></p>
<p>14 0.51560527 <a title="277-lsi-14" href="../hunch_net-2005/hunch_net-2005-06-22-Languages__of_Learning.html">84 hunch net-2005-06-22-Languages  of Learning</a></p>
<p>15 0.51210499 <a title="277-lsi-15" href="../hunch_net-2007/hunch_net-2007-02-22-Create_Your_Own_ICML_Workshop.html">234 hunch net-2007-02-22-Create Your Own ICML Workshop</a></p>
<p>16 0.5079881 <a title="277-lsi-16" href="../hunch_net-2009/hunch_net-2009-01-21-Nearly_all_natural_problems_require_nonlinearity.html">337 hunch net-2009-01-21-Nearly all natural problems require nonlinearity</a></p>
<p>17 0.50165355 <a title="277-lsi-17" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>18 0.49984708 <a title="277-lsi-18" href="../hunch_net-2005/hunch_net-2005-08-01-Peekaboom.html">99 hunch net-2005-08-01-Peekaboom</a></p>
<p>19 0.49937057 <a title="277-lsi-19" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>20 0.49704024 <a title="277-lsi-20" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(6, 0.049), (35, 0.021), (42, 0.257), (45, 0.042), (68, 0.063), (69, 0.022), (72, 0.235), (74, 0.085), (76, 0.028), (82, 0.026), (88, 0.031), (95, 0.054)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.93444246 <a title="277-lda-1" href="../hunch_net-2009/hunch_net-2009-01-27-Key_Scientific_Challenges.html">339 hunch net-2009-01-27-Key Scientific Challenges</a></p>
<p>Introduction: Yahoo released theKey Scientific Challengesprogram. There is aMachine
Learninglist I worked on and aStatisticslist whichDeepakworked on.I'm hoping
this is taken quite seriously by graduate students. The primary value, is that
it gave us a chance to sit down and publicly specify directions of research
which would be valuable to make progress on. A good strategy for a beginning
graduate student is to pick one of these directions, pursue it, and make
substantial advances for a PhD. The directions are sufficiently general that
I'm sure any serious advance has applications well beyond Yahoo.A secondary
point, (which I'm sure is primary for many) is that there is money for
graduate students here. It's unrestricted, so you can use it for any
reasonable travel, supplies, etcâ&euro;Ś</p><p>same-blog 2 0.89012754 <a title="277-lda-2" href="../hunch_net-2007/hunch_net-2007-12-12-Workshop_Summary%26%238212%3BPrinciples_of_Learning_Problem_Design.html">277 hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</a></p>
<p>Introduction: This is a summary of theworkshop on Learning Problem DesignwhichAlinaand I ran
atNIPSthis year.The first question many people have is "What is learning
problem design?" This workshop is about admitting that solving learning
problems does not start with labeled data, but rather somewhere before. When
humans are hired to produce labels, this is usually not a serious problem
because you can tell them precisely what semantics you want the labels to
have, and we can fix some set of features in advance. However, when other
methods are used this becomes more problematic. This focus is important for
Machine Learning because there are very large quantities of data which are not
labeled by a hired human.The title of the workshop was a bit ambitious,
because a workshop is not long enough to synthesize a diversity of approaches
into a coherent set of principles. For me, the posters at the end of the
workshop were quite helpful in getting approaches to gel.Here are some answers
to "where do the lab</p><p>3 0.87617004 <a title="277-lda-3" href="../hunch_net-2005/hunch_net-2005-06-06-Exact_Online_Learning_for_Classification.html">78 hunch net-2005-06-06-Exact Online Learning for Classification</a></p>
<p>Introduction: Jacob Abernethy and I have found a computationally tractable method for
computing an optimal (or near optimal depending on setting) master algorithm
combining expert predictions addressingthis open problem. A draft ishere.The
effect of this improvement seems to be about a factor of2decrease in the
regret (= error rate minus best possible error rate) for the low error rate
situation. (At large error rates, there may be no significant
difference.)There are some unfinished details still to consider:When we remove
all of the approximation slack from online learning, is the result a
satisfying learning algorithm, in practice? I consider online learning is one
of the more compelling methods of analyzing and deriving algorithms, but that
expectation must be either met or not by this algorithmSome extra details: The
algorithm is optimal given a small amount of side information (kin the draft).
What is the best way to remove this side information? The removal is necessary
for a practical algori</p><p>4 0.75928503 <a title="277-lda-4" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>Introduction: Attempts to abstract and study machine learning are within some given
framework or mathematical model. It turns out that all of these models are
significantly flawed for the purpose of studying machine learning. I've
created a table (below) outlining the major flaws in some common models of
machine learning.The point here is not simply "woe unto us". There are several
implications which seem important.The multitude of models is a point of
continuing confusion. It is common for people to learn about machine learning
within one framework which often becomes there "home framework" through which
they attempt to filter all machine learning. (Have you met people who can only
think in terms of kernels? Only via Bayes Law? Only via PAC Learning?)
Explicitly understanding the existence of these other frameworks can help
resolve the confusion. This is particularly important when reviewing and
particularly important for students.Algorithms which conform to multiple
approaches can have substantial</p><p>5 0.75466692 <a title="277-lda-5" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<p>Introduction: I recently had fun discussions with bothVikash MansinghkaandThomas Breuelabout
approaching AI with machine learning. The general interest in taking a crack
at AI with machine learning seems to be rising on many fronts
includingDARPA.As a matter of history, there was a great deal of interest in
AI which died down before I began research. There remain many projects and
conferences spawned in this earlier AI wave, as well as a good bit of
experience about what did not work, or at least did not work yet. Here are a
few examples of failure modes that people seem to run into:Supply/Product
confusion. Sometimes we think "Intelligences use X, so I'll create X and have
an Intelligence." An example of this is theCyc Projectwhich inspires some
people as "intelligences use ontologies, so I'll create an ontology and a
system using it to have an Intelligence." The flaw here is that
Intelligencescreateontologies, which they use, and without the ability to
create ontologies you don't have an Intellige</p><p>6 0.75454211 <a title="277-lda-6" href="../hunch_net-2007/hunch_net-2007-08-19-Choice_of_Metrics.html">259 hunch net-2007-08-19-Choice of Metrics</a></p>
<p>7 0.75379324 <a title="277-lda-7" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>8 0.75356829 <a title="277-lda-8" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>9 0.75248945 <a title="277-lda-9" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>10 0.75229752 <a title="277-lda-10" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>11 0.7522316 <a title="277-lda-11" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>12 0.75197709 <a title="277-lda-12" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>13 0.75143778 <a title="277-lda-13" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>14 0.7514087 <a title="277-lda-14" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>15 0.75130093 <a title="277-lda-15" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>16 0.75025582 <a title="277-lda-16" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>17 0.74870169 <a title="277-lda-17" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>18 0.74840033 <a title="277-lda-18" href="../hunch_net-2005/hunch_net-2005-03-17-Going_all_the_Way%2C_Sometimes.html">42 hunch net-2005-03-17-Going all the Way, Sometimes</a></p>
<p>19 0.74802226 <a title="277-lda-19" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>20 0.74695259 <a title="277-lda-20" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
