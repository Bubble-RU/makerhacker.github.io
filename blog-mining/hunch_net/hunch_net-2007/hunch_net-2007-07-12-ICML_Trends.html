<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>254 hunch net-2007-07-12-ICML Trends</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2007" href="../home/hunch_net-2007_home.html">hunch_net-2007</a> <a title="hunch_net-2007-254" href="#">hunch_net-2007-254</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>254 hunch net-2007-07-12-ICML Trends</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2007-254-html" href="http://hunch.net/?p=281">html</a></p><p>Introduction: Mark Reid  did a post on  ICML trends  that I found interesting.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Mark Reid  did a post on  ICML trends  that I found interesting. [sent-1, score-1.055]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('trends', 0.559), ('reid', 0.518), ('mark', 0.463), ('post', 0.249), ('found', 0.247), ('icml', 0.209), ('interesting', 0.197)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="254-tfidf-1" href="../hunch_net-2007/hunch_net-2007-07-12-ICML_Trends.html">254 hunch net-2007-07-12-ICML Trends</a></p>
<p>Introduction: Mark Reid  did a post on  ICML trends  that I found interesting.</p><p>2 0.3840287 <a title="254-tfidf-2" href="../hunch_net-2008/hunch_net-2008-06-30-ICML_has_a_comment_system.html">305 hunch net-2008-06-30-ICML has a comment system</a></p>
<p>Introduction: Mark Reid  has stepped up and created a  comment system for ICML papers  which  Greger Linden  has tightly integrated.  
 
My understanding is that Mark spent quite  a bit of time on the details, and there are some cool features like working latex math mode.  This is an excellent chance for the ICML community to experiment with making ICML year-round, so I hope it works out.  Please do consider experimenting with it.</p><p>3 0.28168201 <a title="254-tfidf-3" href="../hunch_net-2010/hunch_net-2010-06-20-2010_ICML_discussion_site.html">401 hunch net-2010-06-20-2010 ICML discussion site</a></p>
<p>Introduction: A substantial difficulty with the 2009 and 2008  ICML discussion system  was a communication vacuum, where authors were not informed of comments, and commenters were not informed of responses to their comments without explicit monitoring.   Mark Reid  has setup a  new discussion system for 2010  with the goal of addressing this.
 
Mark didn’t want to make it to intrusive, so you must opt-in.  As an author,  find your paper  and “Subscribe by email” to the comments.  As a commenter, you have the option of providing an email for follow-up notification.</p><p>4 0.24241388 <a title="254-tfidf-4" href="../hunch_net-2009/hunch_net-2009-05-24-2009_ICML_discussion_site.html">356 hunch net-2009-05-24-2009 ICML discussion site</a></p>
<p>Introduction: Mark Reid  has setup a  discussion site for ICML papers  again this year and  Monica Dinculescu  has linked it in from the ICML site.  Last year’s attempt appears to have been an acceptable but not wild success as a little bit of fruitful discussion occurred.  I’m hoping this year will be a bit more of a success—please don’t be shy   
 
I’d like to also point out that  ICML ‘s early  registration  deadline has a few hours left, while  UAI ‘s and  COLT ‘s are in a week.</p><p>5 0.12554201 <a title="254-tfidf-5" href="../hunch_net-2012/hunch_net-2012-06-29-ICML_survey_and_comments.html">468 hunch net-2012-06-29-ICML survey and comments</a></p>
<p>Introduction: Just about nothing could keep me from attending  ICML , except for  Dora  who arrived on Monday.  Consequently, I have only secondhand reports that the conference is going well.
 
For those who are remote (like me) or after the conference (like everyone),  Mark Reid  has setup the  ICML discussion  site where you can comment on any paper or subscribe to papers.  Authors are automatically subscribed to their own papers, so it should be possible to have a discussion significantly after the fact, as people desire.
 
We also conducted a survey before the conference and have the  survey results  now.  This can be compared with the  ICML 2010 survey results .  Looking at the comparable questions, we can sometimes order the answers to have scores ranging from 0 to 3 or 0 to 4 with 3 or 4 being best and 0 worst, then compute the average difference between 2012 and 2010.
 
Glancing through them, I see:
  
 Most people found the papers they reviewed a good fit for their expertise (-.037 w.r.t 20</p><p>6 0.10273871 <a title="254-tfidf-6" href="../hunch_net-2009/hunch_net-2009-06-24-Interesting_papers_at_UAICMOLT_2009.html">361 hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</a></p>
<p>7 0.098660052 <a title="254-tfidf-7" href="../hunch_net-2009/hunch_net-2009-07-11-Interesting_papers_at_KDD.html">364 hunch net-2009-07-11-Interesting papers at KDD</a></p>
<p>8 0.083765417 <a title="254-tfidf-8" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>9 0.081264623 <a title="254-tfidf-9" href="../hunch_net-2009/hunch_net-2009-01-28-Nielsen%26%238217%3Bs_talk.html">340 hunch net-2009-01-28-Nielsen&#8217;s talk</a></p>
<p>10 0.073573776 <a title="254-tfidf-10" href="../hunch_net-2006/hunch_net-2006-11-02-2006_NIPS_workshops.html">216 hunch net-2006-11-02-2006 NIPS workshops</a></p>
<p>11 0.06539163 <a title="254-tfidf-11" href="../hunch_net-2007/hunch_net-2007-01-02-Retrospective.html">225 hunch net-2007-01-02-Retrospective</a></p>
<p>12 0.062907666 <a title="254-tfidf-12" href="../hunch_net-2008/hunch_net-2008-07-02-Proprietary_Data_in_Academic_Research%3F.html">306 hunch net-2008-07-02-Proprietary Data in Academic Research?</a></p>
<p>13 0.059097134 <a title="254-tfidf-13" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>14 0.056199871 <a title="254-tfidf-14" href="../hunch_net-2005/hunch_net-2005-10-03-Not_ICML.html">117 hunch net-2005-10-03-Not ICML</a></p>
<p>15 0.055416159 <a title="254-tfidf-15" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>16 0.054430436 <a title="254-tfidf-16" href="../hunch_net-2012/hunch_net-2012-08-27-NYAS_ML_2012_and_ICML_2013.html">472 hunch net-2012-08-27-NYAS ML 2012 and ICML 2013</a></p>
<p>17 0.050190005 <a title="254-tfidf-17" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<p>18 0.048821613 <a title="254-tfidf-18" href="../hunch_net-2005/hunch_net-2005-03-08-Fast_Physics_for_Learning.html">37 hunch net-2005-03-08-Fast Physics for Learning</a></p>
<p>19 0.048014488 <a title="254-tfidf-19" href="../hunch_net-2007/hunch_net-2007-02-22-Create_Your_Own_ICML_Workshop.html">234 hunch net-2007-02-22-Create Your Own ICML Workshop</a></p>
<p>20 0.047316644 <a title="254-tfidf-20" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.056), (1, -0.08), (2, 0.02), (3, -0.053), (4, 0.04), (5, -0.008), (6, 0.001), (7, -0.102), (8, -0.013), (9, -0.013), (10, -0.02), (11, -0.002), (12, -0.217), (13, 0.084), (14, 0.208), (15, 0.018), (16, -0.268), (17, -0.196), (18, -0.024), (19, 0.062), (20, 0.052), (21, -0.096), (22, -0.094), (23, 0.11), (24, -0.021), (25, -0.151), (26, 0.029), (27, -0.136), (28, 0.068), (29, 0.056), (30, -0.114), (31, -0.06), (32, 0.007), (33, 0.105), (34, -0.066), (35, 0.006), (36, -0.069), (37, -0.047), (38, 0.033), (39, 0.018), (40, 0.111), (41, -0.015), (42, -0.03), (43, 0.084), (44, 0.08), (45, 0.076), (46, -0.031), (47, 0.018), (48, 0.103), (49, -0.018)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99276471 <a title="254-lsi-1" href="../hunch_net-2007/hunch_net-2007-07-12-ICML_Trends.html">254 hunch net-2007-07-12-ICML Trends</a></p>
<p>Introduction: Mark Reid  did a post on  ICML trends  that I found interesting.</p><p>2 0.89352822 <a title="254-lsi-2" href="../hunch_net-2008/hunch_net-2008-06-30-ICML_has_a_comment_system.html">305 hunch net-2008-06-30-ICML has a comment system</a></p>
<p>Introduction: Mark Reid  has stepped up and created a  comment system for ICML papers  which  Greger Linden  has tightly integrated.  
 
My understanding is that Mark spent quite  a bit of time on the details, and there are some cool features like working latex math mode.  This is an excellent chance for the ICML community to experiment with making ICML year-round, so I hope it works out.  Please do consider experimenting with it.</p><p>3 0.85408622 <a title="254-lsi-3" href="../hunch_net-2010/hunch_net-2010-06-20-2010_ICML_discussion_site.html">401 hunch net-2010-06-20-2010 ICML discussion site</a></p>
<p>Introduction: A substantial difficulty with the 2009 and 2008  ICML discussion system  was a communication vacuum, where authors were not informed of comments, and commenters were not informed of responses to their comments without explicit monitoring.   Mark Reid  has setup a  new discussion system for 2010  with the goal of addressing this.
 
Mark didn’t want to make it to intrusive, so you must opt-in.  As an author,  find your paper  and “Subscribe by email” to the comments.  As a commenter, you have the option of providing an email for follow-up notification.</p><p>4 0.73156708 <a title="254-lsi-4" href="../hunch_net-2009/hunch_net-2009-05-24-2009_ICML_discussion_site.html">356 hunch net-2009-05-24-2009 ICML discussion site</a></p>
<p>Introduction: Mark Reid  has setup a  discussion site for ICML papers  again this year and  Monica Dinculescu  has linked it in from the ICML site.  Last year’s attempt appears to have been an acceptable but not wild success as a little bit of fruitful discussion occurred.  I’m hoping this year will be a bit more of a success—please don’t be shy   
 
I’d like to also point out that  ICML ‘s early  registration  deadline has a few hours left, while  UAI ‘s and  COLT ‘s are in a week.</p><p>5 0.60456485 <a title="254-lsi-5" href="../hunch_net-2005/hunch_net-2005-10-03-Not_ICML.html">117 hunch net-2005-10-03-Not ICML</a></p>
<p>Introduction: Alex Smola  showed me this  ICML 2006  webpage.  This is  NOT  the ICML we know, but rather some people at “Enformatika”.  Investigation shows that they registered with an anonymous yahoo email account from  dotregistrar.com  the “Home of the $6.79 wholesale domain!” and their nameservers are by  Turkticaret , a Turkish internet company.
 
It appears the website has since been altered to “ ICNL ” (the above link uses the google cache).
 
They say that imitation is the sincerest form of flattery, so the organizers of the real  ICML 2006  must feel quite flattered.</p><p>6 0.4898856 <a title="254-lsi-6" href="../hunch_net-2012/hunch_net-2012-06-29-ICML_survey_and_comments.html">468 hunch net-2012-06-29-ICML survey and comments</a></p>
<p>7 0.45045999 <a title="254-lsi-7" href="../hunch_net-2005/hunch_net-2005-03-08-Fast_Physics_for_Learning.html">37 hunch net-2005-03-08-Fast Physics for Learning</a></p>
<p>8 0.3707847 <a title="254-lsi-8" href="../hunch_net-2012/hunch_net-2012-08-27-NYAS_ML_2012_and_ICML_2013.html">472 hunch net-2012-08-27-NYAS ML 2012 and ICML 2013</a></p>
<p>9 0.3561092 <a title="254-lsi-9" href="../hunch_net-2007/hunch_net-2007-06-13-Not_Posting.html">246 hunch net-2007-06-13-Not Posting</a></p>
<p>10 0.34885928 <a title="254-lsi-10" href="../hunch_net-2013/hunch_net-2013-07-24-ICML_2012_videos_lost.html">487 hunch net-2013-07-24-ICML 2012 videos lost</a></p>
<p>11 0.34719926 <a title="254-lsi-11" href="../hunch_net-2009/hunch_net-2009-07-11-Interesting_papers_at_KDD.html">364 hunch net-2009-07-11-Interesting papers at KDD</a></p>
<p>12 0.33103952 <a title="254-lsi-12" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>13 0.33078468 <a title="254-lsi-13" href="../hunch_net-2009/hunch_net-2009-01-28-Nielsen%26%238217%3Bs_talk.html">340 hunch net-2009-01-28-Nielsen&#8217;s talk</a></p>
<p>14 0.31019315 <a title="254-lsi-14" href="../hunch_net-2009/hunch_net-2009-06-24-Interesting_papers_at_UAICMOLT_2009.html">361 hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</a></p>
<p>15 0.3094452 <a title="254-lsi-15" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>16 0.288445 <a title="254-lsi-16" href="../hunch_net-2011/hunch_net-2011-10-10-ML_Symposium_and_ICML_details.html">447 hunch net-2011-10-10-ML Symposium and ICML details</a></p>
<p>17 0.27245674 <a title="254-lsi-17" href="../hunch_net-2011/hunch_net-2011-04-23-ICML_workshops_due.html">433 hunch net-2011-04-23-ICML workshops due</a></p>
<p>18 0.27038616 <a title="254-lsi-18" href="../hunch_net-2005/hunch_net-2005-05-02-Reviewing_techniques_for_conferences.html">65 hunch net-2005-05-02-Reviewing techniques for conferences</a></p>
<p>19 0.22619717 <a title="254-lsi-19" href="../hunch_net-2006/hunch_net-2006-06-30-ICML_papers.html">188 hunch net-2006-06-30-ICML papers</a></p>
<p>20 0.22458236 <a title="254-lsi-20" href="../hunch_net-2009/hunch_net-2009-05-17-Server_Update.html">354 hunch net-2009-05-17-Server Update</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(79, 0.712)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.93574798 <a title="254-lda-1" href="../hunch_net-2007/hunch_net-2007-07-12-ICML_Trends.html">254 hunch net-2007-07-12-ICML Trends</a></p>
<p>Introduction: Mark Reid  did a post on  ICML trends  that I found interesting.</p><p>2 0.47548622 <a title="254-lda-2" href="../hunch_net-2009/hunch_net-2009-05-17-Server_Update.html">354 hunch net-2009-05-17-Server Update</a></p>
<p>Introduction: The hunch.net server has been updated.  I’ve taken the opportunity to upgrade the version of wordpress which caused cascading changes.
  
 Old threaded comments are now flattened.  The system we used to use ( Brian’s threaded comments ) appears incompatible with the new threading system built into wordpress.  I haven’t yet figured out a workaround. 
 I setup a  feedburner account . 
 I added an RSS aggregator for both Machine Learning and other research blogs that I like to follow.  This is something that I’ve wanted to do for awhile. 
 Many other minor changes in font and format, with some help from  Alina . 
  
If you have any suggestions for site tweaks, please speak up.</p><p>3 0.39725241 <a title="254-lda-3" href="../hunch_net-2007/hunch_net-2007-06-19-How_is_Compressed_Sensing_going_to_change_Machine_Learning_%3F.html">248 hunch net-2007-06-19-How is Compressed Sensing going to change Machine Learning ?</a></p>
<p>Introduction: Compressed Sensing  (CS) is a new framework developed by  Emmanuel Candes ,  Terry Tao  and  David Donoho . To summarize, if you acquire a signal in some basis that is incoherent with the basis in which you know the signal to be sparse in, it is very likely you will be able to reconstruct the signal from these incoherent projections. 
 
Terry Tao, the recent  Fields medalist , does a very nice job at explaining the framework  here . He goes further in the theory description in this  post  where he mentions the central issue of the Uniform Uncertainty Principle. It so happens that random projections are on average incoherent, within the UUP meaning, with most known basis (sines, polynomials, splines, wavelets, curvelets …) and are therefore an ideal basis for Compressed Sensing. [ For more in-depth information on the subject, the Rice group has done a very good job at providing a central library of papers relevant to the growing subject:  http://www.dsp.ece.rice.edu/cs/  ]
 
The Machine</p><p>4 0.38713396 <a title="254-lda-4" href="../hunch_net-2006/hunch_net-2006-03-09-Use_of_Notation.html">162 hunch net-2006-03-09-Use of Notation</a></p>
<p>Introduction: For most people, a mathematical notation is like a language: you learn it and stick with it.  For people doing mathematical research, however, this is not enough: they must design new notations for new problems.  The design of good notation is both hard and worthwhile since a bad initial notation can retard a line of research greatly.  
 
Before we had mathematical notation, equations were all written out in language.  Since words have multiple meanings and variable precedences, long equations written out in language can be extraordinarily difficult and sometimes fundamentally ambiguous.  A good representative example of this is the legalese in the tax code.  Since we want greater precision and clarity, we adopt mathematical notation.
 
One fundamental thing to understand about mathematical notation, is that humans as logic verifiers, are barely capable.  This is the fundamental reason why one notation can be much better than another.  This observation is easier to miss than you might</p><p>5 0.28149578 <a title="254-lda-5" href="../hunch_net-2005/hunch_net-2005-02-23-Problem%3A_Reinforcement_Learning_with_Classification.html">27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</a></p>
<p>Introduction: At an intuitive level, the question here is “Can reinforcement learning be solved with classification?”  
 
 Problem  Construct a reinforcement learning algorithm with near-optimal expected sum of rewards in the  direct experience model  given access to a classifier learning algorithm which has a small error rate or regret on all posed classification problems.  The definition of “posed” here is slightly murky.  I consider a problem “posed” if there is an algorithm for constructing labeled classification examples.
 
 Past Work 
  
 There exists a  reduction of reinforcement learning to classification given a generative model.   A generative model is an inherently stronger assumption than the direct experience model. 
 Other  work on learning reductions  may be important. 
 Several algorithms for solving reinforcement learning in the direct experience model exist.  Most, such as  E 3  ,  Factored-E 3  , and  metric-E 3   and  Rmax  require that the observation be the state.  Recent work</p><p>6 0.21898967 <a title="254-lda-6" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<p>7 0.21614593 <a title="254-lda-7" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>8 0.089761332 <a title="254-lda-8" href="../hunch_net-2009/hunch_net-2009-05-24-2009_ICML_discussion_site.html">356 hunch net-2009-05-24-2009 ICML discussion site</a></p>
<p>9 0.043983769 <a title="254-lda-9" href="../hunch_net-2006/hunch_net-2006-04-05-What_is_state%3F.html">169 hunch net-2006-04-05-What is state?</a></p>
<p>10 0.030421989 <a title="254-lda-10" href="../hunch_net-2006/hunch_net-2006-02-11-Yahoo%26%238217%3Bs_Learning_Problems..html">156 hunch net-2006-02-11-Yahoo&#8217;s Learning Problems.</a></p>
<p>11 0.024393193 <a title="254-lda-11" href="../hunch_net-2009/hunch_net-2009-06-24-Interesting_papers_at_UAICMOLT_2009.html">361 hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</a></p>
<p>12 0.0 <a title="254-lda-12" href="../hunch_net-2005/hunch_net-2005-01-19-Why_I_decided_to_run_a_weblog..html">1 hunch net-2005-01-19-Why I decided to run a weblog.</a></p>
<p>13 0.0 <a title="254-lda-13" href="../hunch_net-2005/hunch_net-2005-01-24-Holy_grails_of_machine_learning%3F.html">2 hunch net-2005-01-24-Holy grails of machine learning?</a></p>
<p>14 0.0 <a title="254-lda-14" href="../hunch_net-2005/hunch_net-2005-01-24-The_Humanloop_Spectrum_of_Machine_Learning.html">3 hunch net-2005-01-24-The Humanloop Spectrum of Machine Learning</a></p>
<p>15 0.0 <a title="254-lda-15" href="../hunch_net-2005/hunch_net-2005-01-26-Summer_Schools.html">4 hunch net-2005-01-26-Summer Schools</a></p>
<p>16 0.0 <a title="254-lda-16" href="../hunch_net-2005/hunch_net-2005-01-26-Watchword%3A_Probability.html">5 hunch net-2005-01-26-Watchword: Probability</a></p>
<p>17 0.0 <a title="254-lda-17" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>18 0.0 <a title="254-lda-18" href="../hunch_net-2005/hunch_net-2005-01-31-Watchword%3A_Assumption.html">7 hunch net-2005-01-31-Watchword: Assumption</a></p>
<p>19 0.0 <a title="254-lda-19" href="../hunch_net-2005/hunch_net-2005-02-01-NIPS%3A_Online_Bayes.html">8 hunch net-2005-02-01-NIPS: Online Bayes</a></p>
<p>20 0.0 <a title="254-lda-20" href="../hunch_net-2005/hunch_net-2005-02-01-Watchword%3A_Loss.html">9 hunch net-2005-02-01-Watchword: Loss</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
