<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>233 hunch net-2007-02-16-The Forgetting</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2007" href="../home/hunch_net-2007_home.html">hunch_net-2007</a> <a title="hunch_net-2007-233" href="#">hunch_net-2007-233</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>233 hunch net-2007-02-16-The Forgetting</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2007-233-html" href="http://hunch.net/?p=205">html</a></p><p>Introduction: How many papers do you remember from 2006? 2005? 2002? 1997? 1987? 1967?  One way to judge this would be to look at the citations of the papers you write—how many came from which year?  For myself, the answers on recent papers are:
  
 
 year 
 2006 
 2005 
 2002 
 1997 
 1987 
 1967 
 
 
 count 
 4 
 10 
 5 
 1 
 0 
 0 
 
  
This spectrum is fairly typical of papers in general.  There are many reasons that citations are focused on recent papers.
  
 The number of papers being published continues to grow.  This is not a very significant effect, because the rate of publication has not grown nearly as fast. 
 Dead men don’t reject your papers for not citing them.  This reason seems lame, because it’s a distortion from the ideal of science.  Nevertheless, it must be stated because the effect can be significant. 
 In 1997, I started as a PhD student.  Naturally, papers after 1997 are better remembered because they were absorbed in real time.  A large fraction of people writing papers and a</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 One way to judge this would be to look at the citations of the papers you write—how many came from which year? [sent-7, score-0.514]
</p><p>2 For myself, the answers on recent papers are:       year   2006   2005   2002   1997   1987   1967       count   4   10   5   1   0   0       This spectrum is fairly typical of papers in general. [sent-8, score-0.882]
</p><p>3 There are many reasons that citations are focused on recent papers. [sent-9, score-0.349]
</p><p>4 The number of papers being published continues to grow. [sent-10, score-0.324]
</p><p>5 Dead men don’t reject your papers for not citing them. [sent-12, score-0.504]
</p><p>6 Nevertheless, it must be stated because the effect can be significant. [sent-14, score-0.174]
</p><p>7 Naturally, papers after 1997 are better remembered because they were absorbed in real time. [sent-16, score-0.324]
</p><p>8 A large fraction of people writing papers and attending conferences haven’t been doing it for 10 years. [sent-17, score-0.407]
</p><p>9 This is huge effect for any papers prior to 1995 (or so). [sent-19, score-0.498]
</p><p>10 The ease of examining a paper greatly influences the ability of an author to read and understand it. [sent-20, score-0.349]
</p><p>11 For example, when people forget, they reinvent, and sometimes they reinvent better. [sent-27, score-0.285]
</p><p>12 Nevertheless, it seems like the effect of forgetting is bad overall, because it causes wasted effort. [sent-28, score-0.763]
</p><p>13 There are two implications:     For paper writers, it is very common to overestimate the value of a paper, even though we know that the impact of most papers is bounded in time. [sent-29, score-0.609]
</p><p>14 Perhaps by looking at those older papers, we can get an idea of what is important in the long term. [sent-30, score-0.213]
</p><p>15 For example, looking at my own older citations, simplicity is it. [sent-31, score-0.282]
</p><p>16 Are the review criteria promoting the papers which a hope of survival? [sent-37, score-0.414]
</p><p>17 Then, you merely had to stand on the shoulders of giants to succeed. [sent-40, score-0.56]
</p><p>18 Now, it seems that even the ability to peer over the shoulders of people standing on the shoulders of giants might be helpful. [sent-41, score-1.174]
</p><p>19 Nevertheless, it seems that much of this effort is getting wasted in forgetting, because we do not have the right mechanisms to remember the information. [sent-43, score-0.342]
</p><p>20 Which is going to be the first conference to switch away from an  ordered   list  of papers to something with structure? [sent-44, score-0.643]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('papers', 0.324), ('shoulders', 0.303), ('forgetting', 0.269), ('reinvent', 0.202), ('citations', 0.19), ('giants', 0.179), ('effect', 0.174), ('recent', 0.159), ('wasted', 0.157), ('older', 0.121), ('nevertheless', 0.114), ('remember', 0.112), ('paper', 0.107), ('looking', 0.092), ('conference', 0.09), ('citing', 0.09), ('examining', 0.09), ('overestimate', 0.09), ('standing', 0.09), ('causes', 0.09), ('distortion', 0.09), ('forgotten', 0.09), ('men', 0.09), ('privileged', 0.09), ('promoting', 0.09), ('impact', 0.088), ('people', 0.083), ('writers', 0.083), ('ordered', 0.083), ('giant', 0.083), ('promote', 0.083), ('teachable', 0.083), ('teach', 0.078), ('originally', 0.078), ('influences', 0.078), ('stand', 0.078), ('going', 0.077), ('spectrum', 0.075), ('survival', 0.075), ('ability', 0.074), ('seems', 0.073), ('dead', 0.072), ('phd', 0.072), ('stuck', 0.072), ('publication', 0.072), ('switch', 0.069), ('simplicity', 0.069), ('wouldn', 0.069), ('grown', 0.069), ('peer', 0.069)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999976 <a title="233-tfidf-1" href="../hunch_net-2007/hunch_net-2007-02-16-The_Forgetting.html">233 hunch net-2007-02-16-The Forgetting</a></p>
<p>Introduction: How many papers do you remember from 2006? 2005? 2002? 1997? 1987? 1967?  One way to judge this would be to look at the citations of the papers you write—how many came from which year?  For myself, the answers on recent papers are:
  
 
 year 
 2006 
 2005 
 2002 
 1997 
 1987 
 1967 
 
 
 count 
 4 
 10 
 5 
 1 
 0 
 0 
 
  
This spectrum is fairly typical of papers in general.  There are many reasons that citations are focused on recent papers.
  
 The number of papers being published continues to grow.  This is not a very significant effect, because the rate of publication has not grown nearly as fast. 
 Dead men don’t reject your papers for not citing them.  This reason seems lame, because it’s a distortion from the ideal of science.  Nevertheless, it must be stated because the effect can be significant. 
 In 1997, I started as a PhD student.  Naturally, papers after 1997 are better remembered because they were absorbed in real time.  A large fraction of people writing papers and a</p><p>2 0.25182289 <a title="233-tfidf-2" href="../hunch_net-2005/hunch_net-2005-02-25-Why_Papers%3F.html">30 hunch net-2005-02-25-Why Papers?</a></p>
<p>Introduction: Makc asked a good  question  in comments—”Why bother to make a paper, at all?”  There are several reasons for writing papers which may not be immediately obvious to people not in academia.
 
The basic idea is that papers have considerably more utility than the obvious “present an idea”.
  
 Papers are a formalized units of work. Academics (especially young ones) are often judged on the number of papers they produce. 
 Papers have a formalized method of citing and crediting other—the bibliography.  Academics (especially older ones) are often judged on the number of citations they receive. 
 Papers enable a “more fair” anonymous review.  Conferences receive  many  papers, from which a subset are selected.  Discussion forums are inherently not anonymous for anyone who wants to build a reputation for good work. 
 Papers are an excuse to meet your friends.  Papers are the content of conferences, but much of what you do is talk to friends about interesting problems while there.  Sometimes yo</p><p>3 0.14952493 <a title="233-tfidf-3" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>Introduction: Conferences exist as part of the process of doing research.  They provide many roles including “announcing research”, “meeting people”, and  “point of reference”.  Not all conferences are alike so a basic question is: “to what extent do individual conferences attempt to aid research?”  This question is very difficult to answer in any satisfying way.  What we can do is compare details of the process across multiple conferences.
  
  Comments   The average quality of comments across conferences can vary dramatically.  At one extreme, the tradition in CS theory conferences is to provide essentially zero feedback.  At the other extreme, some conferences have a strong tradition of providing detailed constructive feedback.  Detailed feedback can give authors significant guidance about how to improve research.  This is the most subjective entry. 
  Blind  Virtually all conferences offer single blind review where authors do not know reviewers.  Some also provide  double blind  review where rev</p><p>4 0.139098 <a title="233-tfidf-4" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>Introduction: Unfortunately, I ended up sick for much of this ICML.  I did manage to catch one interesting paper:
 
 Richard Socher ,  Cliff Lin ,  Andrew Y. Ng , and  Christopher D. Manning   Parsing Natural Scenes and Natural Language with Recursive Neural Networks .
 
I invited Richard to share his list of interesting papers, so hopefully we’ll hear from him soon.  In the meantime,  Paul  and  Hal  have posted some lists.
 
 the future 
 
 Joelle  and I are program chairs for ICML 2012 in  Edinburgh , which I previously enjoyed visiting in  2005 .  This is a huge responsibility, that we hope to accomplish well.  A part of this (perhaps the most fun part), is imagining how we can make ICML better.  A key and critical constraint is choosing things that can be accomplished.  So far we have:
  
  Colocation . The first thing we looked into was potential colocations.  We quickly discovered that many other conferences precomitted their location.  For the future, getting a colocation with  ACL  or  SIGI</p><p>5 0.13801695 <a title="233-tfidf-5" href="../hunch_net-2008/hunch_net-2008-09-03-Bidding_Problems.html">315 hunch net-2008-09-03-Bidding Problems</a></p>
<p>Introduction: One way that many conferences in machine learning assign reviewers to papers is via bidding, which has steps something like:
  
 Invite people to review 
 Accept papers 
 Reviewers look at title and abstract and state the papers they are interested in reviewing. 
 Some massaging happens, but reviewers often get approximately the papers they bid for. 
  
At the ICML business meeting,  Andrew McCallum  suggested getting rid of bidding for papers.  A couple reasons were given:
  
  Privacy  The title and abstract of the entire set of papers is visible to every participating reviewer.  Some authors might be uncomfortable about this for submitted papers.  I’m not sympathetic to this reason: the point of submitting a paper to review is to publish it, so the value (if any) of not publishing a part of it a little bit earlier seems limited. 
  Cliques   A bidding system is gameable.  If you have 3 buddies and you inform each other of your submissions, you can each bid for your friend’s papers a</p><p>6 0.13781886 <a title="233-tfidf-6" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>7 0.13488646 <a title="233-tfidf-7" href="../hunch_net-2008/hunch_net-2008-11-10-ICML_Reviewing_Criteria.html">325 hunch net-2008-11-10-ICML Reviewing Criteria</a></p>
<p>8 0.13421443 <a title="233-tfidf-8" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>9 0.12898287 <a title="233-tfidf-9" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>10 0.12892784 <a title="233-tfidf-10" href="../hunch_net-2005/hunch_net-2005-04-04-Grounds_for_Rejection.html">52 hunch net-2005-04-04-Grounds for Rejection</a></p>
<p>11 0.12400945 <a title="233-tfidf-11" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>12 0.12255856 <a title="233-tfidf-12" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>13 0.1220498 <a title="233-tfidf-13" href="../hunch_net-2005/hunch_net-2005-03-13-Avoiding_Bad_Reviewing.html">40 hunch net-2005-03-13-Avoiding Bad Reviewing</a></p>
<p>14 0.1194403 <a title="233-tfidf-14" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>15 0.11923906 <a title="233-tfidf-15" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>16 0.11779077 <a title="233-tfidf-16" href="../hunch_net-2006/hunch_net-2006-12-04-Structural_Problems_in_NIPS_Decision_Making.html">221 hunch net-2006-12-04-Structural Problems in NIPS Decision Making</a></p>
<p>17 0.11718131 <a title="233-tfidf-17" href="../hunch_net-2008/hunch_net-2008-10-14-Who_is_Responsible_for_a_Bad_Review%3F.html">320 hunch net-2008-10-14-Who is Responsible for a Bad Review?</a></p>
<p>18 0.11700009 <a title="233-tfidf-18" href="../hunch_net-2008/hunch_net-2008-02-10-Complexity_Illness.html">288 hunch net-2008-02-10-Complexity Illness</a></p>
<p>19 0.11176561 <a title="233-tfidf-19" href="../hunch_net-2007/hunch_net-2007-01-02-Retrospective.html">225 hunch net-2007-01-02-Retrospective</a></p>
<p>20 0.11089239 <a title="233-tfidf-20" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.244), (1, -0.13), (2, 0.11), (3, 0.099), (4, 0.011), (5, 0.018), (6, 0.017), (7, -0.006), (8, 0.041), (9, 0.021), (10, 0.027), (11, 0.038), (12, -0.043), (13, -0.029), (14, 0.051), (15, -0.028), (16, -0.005), (17, 0.084), (18, 0.049), (19, 0.002), (20, 0.049), (21, 0.002), (22, -0.028), (23, -0.031), (24, 0.029), (25, -0.043), (26, -0.039), (27, 0.024), (28, -0.084), (29, -0.133), (30, -0.007), (31, 0.086), (32, 0.041), (33, -0.051), (34, 0.103), (35, -0.07), (36, 0.024), (37, 0.059), (38, 0.074), (39, 0.105), (40, 0.009), (41, 0.065), (42, 0.037), (43, 0.033), (44, -0.035), (45, 0.058), (46, -0.046), (47, -0.043), (48, 0.073), (49, 0.006)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98663634 <a title="233-lsi-1" href="../hunch_net-2007/hunch_net-2007-02-16-The_Forgetting.html">233 hunch net-2007-02-16-The Forgetting</a></p>
<p>Introduction: How many papers do you remember from 2006? 2005? 2002? 1997? 1987? 1967?  One way to judge this would be to look at the citations of the papers you write—how many came from which year?  For myself, the answers on recent papers are:
  
 
 year 
 2006 
 2005 
 2002 
 1997 
 1987 
 1967 
 
 
 count 
 4 
 10 
 5 
 1 
 0 
 0 
 
  
This spectrum is fairly typical of papers in general.  There are many reasons that citations are focused on recent papers.
  
 The number of papers being published continues to grow.  This is not a very significant effect, because the rate of publication has not grown nearly as fast. 
 Dead men don’t reject your papers for not citing them.  This reason seems lame, because it’s a distortion from the ideal of science.  Nevertheless, it must be stated because the effect can be significant. 
 In 1997, I started as a PhD student.  Naturally, papers after 1997 are better remembered because they were absorbed in real time.  A large fraction of people writing papers and a</p><p>2 0.92534566 <a title="233-lsi-2" href="../hunch_net-2005/hunch_net-2005-02-25-Why_Papers%3F.html">30 hunch net-2005-02-25-Why Papers?</a></p>
<p>Introduction: Makc asked a good  question  in comments—”Why bother to make a paper, at all?”  There are several reasons for writing papers which may not be immediately obvious to people not in academia.
 
The basic idea is that papers have considerably more utility than the obvious “present an idea”.
  
 Papers are a formalized units of work. Academics (especially young ones) are often judged on the number of papers they produce. 
 Papers have a formalized method of citing and crediting other—the bibliography.  Academics (especially older ones) are often judged on the number of citations they receive. 
 Papers enable a “more fair” anonymous review.  Conferences receive  many  papers, from which a subset are selected.  Discussion forums are inherently not anonymous for anyone who wants to build a reputation for good work. 
 Papers are an excuse to meet your friends.  Papers are the content of conferences, but much of what you do is talk to friends about interesting problems while there.  Sometimes yo</p><p>3 0.77628547 <a title="233-lsi-3" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>Introduction: One of the confusing things about research is that progress is very hard to measure.  One of the consequences of being in a hard-to-measure environment is that the wrong things are often measured.  
  
  Lines of Code  The classical example of this phenomenon is the old lines-of-code-produced metric for programming.  It is easy to imagine systems for producing many lines of code with very little work that accomplish very little. 
  Paper count  In academia, a “paper count” is an analog of “lines of code”, and it suffers from the same failure modes.  The obvious failure mode here is that we end up with a large number of uninteresting papers since people end up spending a lot of time optimizing this metric. 
  Complexity  Another metric, is “complexity” (in the eye of a reviewer) of a paper.  There is a common temptation to make a method appear more complex than it is in order for reviewers to judge it worthy of publication.  The failure mode here is unclean thinking.  Simple effective m</p><p>4 0.76865423 <a title="233-lsi-4" href="../hunch_net-2008/hunch_net-2008-02-10-Complexity_Illness.html">288 hunch net-2008-02-10-Complexity Illness</a></p>
<p>Introduction: One of the enduring stereotypes of academia is that people spend a great deal of intelligence, time, and effort finding complexity rather than simplicity.  This is at least anecdotally true in my experience.
  
  Math++  Several people have found that adding useless math makes their paper more publishable as evidenced by a reject-add-accept sequence. 
  8 page minimum  Who submitted a paper to  ICML  violating the 8 page minimum?  Every author fears that the reviewers won’t take their work seriously unless the allowed length is fully used.  The best minimum violation I know is  Adam ‘s paper at SODA on  generating random factored numbers , but this is deeply exceptional.  It’s a fair bet that 90% of papers submitted are exactly at the page limit.  We could imagine that this is because papers naturally take more space, but few people seem to be clamoring for more space. 
  Journalong   Has anyone been asked to review a 100 page journal paper?  I have.  Journal papers can be nice, becaus</p><p>5 0.72551852 <a title="233-lsi-5" href="../hunch_net-2005/hunch_net-2005-04-04-Grounds_for_Rejection.html">52 hunch net-2005-04-04-Grounds for Rejection</a></p>
<p>Introduction: It’s reviewing season right now, so I thought I would list (at a high level) the sorts of problems which I see in papers.  Hopefully, this will help us all write better papers.
 
The following flaws are fatal to any paper:
  
  Incorrect theorem or lemma statements  A typo might be “ok”, if it can be understood.  Any theorem or lemma which indicates an incorrect understanding of reality must be rejected.  Not doing so would severely harm the integrity of the conference.  A paper rejected for this reason must be fixed. 
  Lack of Understanding  If a paper is understood by none of the (typically 3) reviewers then it must be rejected for the same reason.  This is more controversial than it sounds because there are some people who maximize paper complexity in the hope of impressing the reviewer.  The tactic sometimes succeeds with some reviewers (but not with me).

As a reviewer, I sometimes get lost for stupid reasons.  This is why an anonymized  communication channel  with the author can</p><p>6 0.69281816 <a title="233-lsi-6" href="../hunch_net-2005/hunch_net-2005-01-19-Why_I_decided_to_run_a_weblog..html">1 hunch net-2005-01-19-Why I decided to run a weblog.</a></p>
<p>7 0.68336713 <a title="233-lsi-7" href="../hunch_net-2008/hunch_net-2008-11-10-ICML_Reviewing_Criteria.html">325 hunch net-2008-11-10-ICML Reviewing Criteria</a></p>
<p>8 0.67885894 <a title="233-lsi-8" href="../hunch_net-2006/hunch_net-2006-12-04-Structural_Problems_in_NIPS_Decision_Making.html">221 hunch net-2006-12-04-Structural Problems in NIPS Decision Making</a></p>
<p>9 0.67500758 <a title="233-lsi-9" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>10 0.6594336 <a title="233-lsi-10" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>11 0.65003258 <a title="233-lsi-11" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>12 0.64831597 <a title="233-lsi-12" href="../hunch_net-2006/hunch_net-2006-01-06-MLTV.html">146 hunch net-2006-01-06-MLTV</a></p>
<p>13 0.62853581 <a title="233-lsi-13" href="../hunch_net-2007/hunch_net-2007-07-20-Motivation_should_be_the_Responsibility_of_the_Reviewer.html">256 hunch net-2007-07-20-Motivation should be the Responsibility of the Reviewer</a></p>
<p>14 0.6259703 <a title="233-lsi-14" href="../hunch_net-2008/hunch_net-2008-09-03-Bidding_Problems.html">315 hunch net-2008-09-03-Bidding Problems</a></p>
<p>15 0.61398315 <a title="233-lsi-15" href="../hunch_net-2009/hunch_net-2009-07-09-The_Machine_Learning_Forum.html">363 hunch net-2009-07-09-The Machine Learning Forum</a></p>
<p>16 0.61222929 <a title="233-lsi-16" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>17 0.60825741 <a title="233-lsi-17" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>18 0.59840983 <a title="233-lsi-18" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>19 0.59201825 <a title="233-lsi-19" href="../hunch_net-2006/hunch_net-2006-09-18-What_is_missing_for_online_collaborative_research%3F.html">208 hunch net-2006-09-18-What is missing for online collaborative research?</a></p>
<p>20 0.59166509 <a title="233-lsi-20" href="../hunch_net-2007/hunch_net-2007-12-20-Cool_and_Interesting_things_at_NIPS%2C_take_three.html">280 hunch net-2007-12-20-Cool and Interesting things at NIPS, take three</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(10, 0.011), (27, 0.166), (38, 0.427), (42, 0.017), (53, 0.072), (55, 0.079), (94, 0.095), (95, 0.055)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.97714496 <a title="233-lda-1" href="../hunch_net-2005/hunch_net-2005-10-20-Machine_Learning_in_the_News.html">125 hunch net-2005-10-20-Machine Learning in the News</a></p>
<p>Introduction: The New York Times had a  short interview  about machine learning in datamining being used pervasively by the IRS and large corporations to predict who to audit and who to target for various marketing campaigns.  This is a big application area of machine learning.  It can be harmful (learning + databases = another way to invade privacy) or beneficial (as google demonstrates, better targeting of marketing campaigns is far less annoying).  This is yet more evidence that we can not rely upon “I’m just another fish in the school” logic for our expectations about treatment by government and large corporations.</p><p>2 0.96384609 <a title="233-lda-2" href="../hunch_net-2013/hunch_net-2013-08-31-Extreme_Classification_workshop_at_NIPS.html">488 hunch net-2013-08-31-Extreme Classification workshop at NIPS</a></p>
<p>Introduction: Manik  and I are organizing the  extreme classification  workshop at NIPS this year.  We have a number of good speakers lined up, but I would further encourage anyone working in the area to submit an abstract by October 9.  I believe this is an idea whose time has now come.
 
The NIPS website doesnâ&euro;&trade;t have other workshops listed yet, but I expect several others to be of significant interest.</p><p>3 0.95757675 <a title="233-lda-3" href="../hunch_net-2009/hunch_net-2009-01-27-Key_Scientific_Challenges.html">339 hunch net-2009-01-27-Key Scientific Challenges</a></p>
<p>Introduction: Yahoo released the  Key Scientific Challenges  program.  There is a  Machine Learning  list I worked on and a  Statistics  list which  Deepak  worked on.
 
I’m hoping this is taken quite seriously by graduate students.  The primary value, is that it gave us a chance to sit down and publicly specify directions of research which would be valuable to make progress on.  A good strategy for a beginning graduate student is to pick one of these directions, pursue it, and make substantial advances for a PhD.  The directions are sufficiently general that I’m sure any serious advance has applications well beyond Yahoo.
 
A secondary point, (which I’m sure is primary for many    ) is that there is money for graduate students here.  It’s unrestricted, so you can use it for any reasonable travel, supplies, etc…</p><p>4 0.95742357 <a title="233-lda-4" href="../hunch_net-2006/hunch_net-2006-05-23-What_is_the_best_regret_transform_reduction_from_multiclass_to_binary%3F.html">181 hunch net-2006-05-23-What is the best regret transform reduction from multiclass to binary?</a></p>
<p>Introduction: This post is about an open problem in learning reductions.
 
 Background  A reduction might transform a a multiclass prediction problem where there are  k  possible labels into a binary learning problem where there are only 2 possible labels.   On this induced binary problem we might learn a binary classifier with some error rate  e .  After subtracting the minimum possible (Bayes) error rate  b , we get a regret  r = e – b .  The  PECOC (Probabilistic Error Correcting Output Code) reduction has the property that binary regret  r  implies multiclass regret at most  4r 0.5  .
 
 The problem  This is not the “rightest” answer.  Consider the  k=2  case, where we reduce binary to binary.  There exists a reduction (the identity) with the property that regret  r  implies regret  r .  This is substantially superior to the transform given by the PECOC reduction, which suggests that a better reduction may exist for general  k .  For example, we can not rule out the possibility that a reduction</p><p>5 0.95489156 <a title="233-lda-5" href="../hunch_net-2005/hunch_net-2005-06-18-Lower_Bounds_for_Learning_Reductions.html">83 hunch net-2005-06-18-Lower Bounds for Learning Reductions</a></p>
<p>Introduction: Learning reductions  transform a solver of one type of learning problem into a solver of another type of learning problem.  When we analyze these for robustness we can make statement of the form “Reduction  R  has the property that regret  r  (or loss) on subproblems of type  A  implies regret at most   f ( r )  on the original problem of type  B “.
 
A lower bound for a learning reduction would have the form “for all reductions  R , there exists a learning problem of type  B  and learning algorithm for problems of type  A  where regret  r  on induced problems implies  at least  regret  f ( r )  for  B “.
 
The pursuit of lower bounds is often questionable because, unlike upper bounds, they do not yield practical algorithms.  Nevertheless, they may be helpful as a tool for thinking about what is learnable and how learnable it is.  This has already come up  here  and  here .
 
At the moment, there is no coherent theory of lower bounds for learning reductions, and we have little understa</p><p>6 0.9294771 <a title="233-lda-6" href="../hunch_net-2006/hunch_net-2006-04-06-Bounds_greater_than_1.html">170 hunch net-2006-04-06-Bounds greater than 1</a></p>
<p>same-blog 7 0.91323537 <a title="233-lda-7" href="../hunch_net-2007/hunch_net-2007-02-16-The_Forgetting.html">233 hunch net-2007-02-16-The Forgetting</a></p>
<p>8 0.88883072 <a title="233-lda-8" href="../hunch_net-2009/hunch_net-2009-05-08-Computability_in_Artificial_Intelligence.html">353 hunch net-2009-05-08-Computability in Artificial Intelligence</a></p>
<p>9 0.84548718 <a title="233-lda-9" href="../hunch_net-2007/hunch_net-2007-03-15-Alternative_Machine_Learning_Reductions_Definitions.html">236 hunch net-2007-03-15-Alternative Machine Learning Reductions Definitions</a></p>
<p>10 0.80417651 <a title="233-lda-10" href="../hunch_net-2007/hunch_net-2007-06-24-Interesting_Papers_at_ICML_2007.html">251 hunch net-2007-06-24-Interesting Papers at ICML 2007</a></p>
<p>11 0.721605 <a title="233-lda-11" href="../hunch_net-2005/hunch_net-2005-05-16-Regret_minimizing_vs_error_limiting_reductions.html">72 hunch net-2005-05-16-Regret minimizing vs error limiting reductions</a></p>
<p>12 0.70783371 <a title="233-lda-12" href="../hunch_net-2005/hunch_net-2005-02-21-Problem%3A_Cross_Validation.html">26 hunch net-2005-02-21-Problem: Cross Validation</a></p>
<p>13 0.69216603 <a title="233-lda-13" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>14 0.68449485 <a title="233-lda-14" href="../hunch_net-2007/hunch_net-2007-04-18-%2450K_Spock_Challenge.html">239 hunch net-2007-04-18-$50K Spock Challenge</a></p>
<p>15 0.68435884 <a title="233-lda-15" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>16 0.67816275 <a title="233-lda-16" href="../hunch_net-2005/hunch_net-2005-06-17-Reopening_RL-%3EClassification.html">82 hunch net-2005-06-17-Reopening RL->Classification</a></p>
<p>17 0.67228973 <a title="233-lda-17" href="../hunch_net-2005/hunch_net-2005-11-16-The_Everything_Ensemble_Edge.html">131 hunch net-2005-11-16-The Everything Ensemble Edge</a></p>
<p>18 0.65703595 <a title="233-lda-18" href="../hunch_net-2006/hunch_net-2006-03-09-Use_of_Notation.html">162 hunch net-2006-03-09-Use of Notation</a></p>
<p>19 0.63617831 <a title="233-lda-19" href="../hunch_net-2008/hunch_net-2008-07-02-Proprietary_Data_in_Academic_Research%3F.html">306 hunch net-2008-07-02-Proprietary Data in Academic Research?</a></p>
<p>20 0.63512504 <a title="233-lda-20" href="../hunch_net-2011/hunch_net-2011-08-01-Interesting_papers_at_COLT_2011.html">439 hunch net-2011-08-01-Interesting papers at COLT 2011</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
