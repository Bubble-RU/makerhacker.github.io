<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>269 hunch net-2007-10-24-Contextual Bandits</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2007" href="../home/hunch_net-2007_home.html">hunch_net-2007</a> <a title="hunch_net-2007-269" href="#">hunch_net-2007-269</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>269 hunch net-2007-10-24-Contextual Bandits</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2007-269-html" href="http://hunch.net/?p=298">html</a></p><p>Introduction: One of the fundamental underpinnings of the internet is advertising based
content. This has become much more effective due to targeted advertising where
ads are specifically matched to interests. Everyone is familiar with this,
because everyone uses search engines and all search engines try to make money
this way.The problem of matching ads to interests is a natural machine
learning problem in some ways since there is much information in who clicks on
what. A fundamental problem with this information is that it is not supervised
--in particular a click-or-not on one ad doesn't generally tell you if a
different ad would have been clicked on. This implies we have a fundamental
exploration problem.A standard mathematical setting for this situation is
"k-Armed Bandits", often with various relevant embellishments. Thek-Armed
Bandit setting works on a round-by-round basis. On each round:A policy chooses
armafrom1ofkarms (i.e. 1 of k ads).The world reveals the rewardraof the chosen
arm (i.e.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 This has become much more effective due to targeted advertising where ads are specifically matched to interests. [sent-2, score-0.556]
</p><p>2 Everyone is familiar with this, because everyone uses search engines and all search engines try to make money this way. [sent-3, score-0.524]
</p><p>3 The problem of matching ads to interests is a natural machine learning problem in some ways since there is much information in who clicks on what. [sent-4, score-0.425]
</p><p>4 A fundamental problem with this information is that it is not supervised --in particular a click-or-not on one ad doesn't generally tell you if a different ad would have been clicked on. [sent-5, score-0.994]
</p><p>5 As information is accumulated over multiple rounds, a good policy might converge on a good choice of arm (i. [sent-15, score-0.43]
</p><p>6 This setting (and its variants) fails to capture a critical phenomenon: each of these displayed ads are done in the context of a search or other webpage. [sent-18, score-0.907]
</p><p>7 To model this, we might think of a different setting where on each round:The world announces some context informationx(think of this as a high dimensional bit vector if that helps). [sent-19, score-0.387]
</p><p>8 First, note that policies usingxcan encode much more rich decisions than a policy not usingx. [sent-27, score-0.408]
</p><p>9 Second, we can try to reduce this setting to thek- Armed Bandit setting, and note that it can not be done well. [sent-29, score-0.216]
</p><p>10 The amount of information required to do well scales linearly in the number of contexts. [sent-31, score-0.234]
</p><p>11 In contrast, good supervised learning algorithms often require information which is (essentially) independent of the number of contexts. [sent-32, score-0.454]
</p><p>12 This removes an explicit dependence on the number of contexts, but it creates a linear dependence on the number of policies. [sent-34, score-0.582]
</p><p>13 Via Occam's razor/VC dimension/Margin bounds, we already know that supervised learning requires experience much smaller than the number of policies. [sent-35, score-0.297]
</p><p>14 The first algorithm for solving this problem isEXP4 (page 19 = 66)which has a regret with respect to the best policy in a set ofO( T0. [sent-37, score-0.221]
</p><p>15 5)whereTis the number of rounds and|H|is the number of policies. [sent-39, score-0.394]
</p><p>16 ) This result is independent of the number of contextsxand only weakly dependent (similar to supervised learning) on the number of policies. [sent-41, score-0.51]
</p><p>17 EXP4 has a number of drawbacks--it has severe computational requirements and doesn't work for continuously parameterized policies (*). [sent-42, score-0.275]
</p><p>18 Tongand I worked out a reasonably simple meta-algorithmEpoch-Greedywhich addresses these drawbacks (**), at the cost of sometimes worsening the regret bound toO(T2/3S1/3)whereSis related to the representational complexity of supervised learning on the set of policies. [sent-43, score-0.27]
</p><p>19 ThisTdependence is of great concern to people who have worked on bandit problems in the past (where, basically, only the dependence onTcould be optimized). [sent-44, score-0.457]
</p><p>20 For myself, bandit algorithms are (at best) motivational because they can not be applied to real-world problems without altering them to take context into account. [sent-50, score-0.497]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ads', 0.336), ('bandit', 0.282), ('bandits', 0.261), ('ad', 0.249), ('arm', 0.179), ('clicked', 0.179), ('contextual', 0.165), ('policy', 0.162), ('setting', 0.16), ('context', 0.16), ('supervised', 0.152), ('number', 0.145), ('armed', 0.134), ('rewardraof', 0.134), ('policies', 0.13), ('search', 0.129), ('dependence', 0.116), ('engines', 0.104), ('rounds', 0.104), ('advertising', 0.1), ('reveals', 0.1), ('round', 0.089), ('information', 0.089), ('chooses', 0.085), ('chosen', 0.081), ('fundamental', 0.076), ('independent', 0.068), ('world', 0.067), ('contrast', 0.067), ('critical', 0.062), ('encode', 0.06), ('dividing', 0.06), ('displayed', 0.06), ('wheresis', 0.06), ('targeted', 0.06), ('composable', 0.06), ('ln', 0.06), ('matched', 0.06), ('removes', 0.06), ('wheretis', 0.06), ('regret', 0.059), ('worked', 0.059), ('everyone', 0.058), ('note', 0.056), ('ofo', 0.055), ('treat', 0.055), ('motivational', 0.055), ('composition', 0.055), ('occam', 0.055), ('modifications', 0.055)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="269-tfidf-1" href="../hunch_net-2007/hunch_net-2007-10-24-Contextual_Bandits.html">269 hunch net-2007-10-24-Contextual Bandits</a></p>
<p>Introduction: One of the fundamental underpinnings of the internet is advertising based
content. This has become much more effective due to targeted advertising where
ads are specifically matched to interests. Everyone is familiar with this,
because everyone uses search engines and all search engines try to make money
this way.The problem of matching ads to interests is a natural machine
learning problem in some ways since there is much information in who clicks on
what. A fundamental problem with this information is that it is not supervised
--in particular a click-or-not on one ad doesn't generally tell you if a
different ad would have been clicked on. This implies we have a fundamental
exploration problem.A standard mathematical setting for this situation is
"k-Armed Bandits", often with various relevant embellishments. Thek-Armed
Bandit setting works on a round-by-round basis. On each round:A policy chooses
armafrom1ofkarms (i.e. 1 of k ads).The world reveals the rewardraof the chosen
arm (i.e.</p><p>2 0.25411993 <a title="269-tfidf-2" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>Introduction: Consider the contextual bandit setting where, repeatedly:A contextxis
observed.An actionais taken given the contextx.A rewardris observed, dependent
onxanda.Where the goal of a learning agent is to find a policy for step 2
achieving a large expected reward.This setting is of obvious importance,
because in the real world we typically make decisions based on some set of
information and then get feedback only about the single action taken. It also
fundamentally differs from supervised learning settings because knowing the
value of one action is not equivalent to knowing the value of all actions.A
decade ago the best machine learning techniques for this setting where
implausibly inefficient.Dean Fosteronce told me he thought the area was a
research sinkhole with little progress to be expected. Now we are on the verge
of being able to routinely attack these problems, in almost exactly the same
sense that we routinely attack bread and butter supervised learning problems.
Just as for supervis</p><p>3 0.24843273 <a title="269-tfidf-3" href="../hunch_net-2008/hunch_net-2008-09-12-How_do_we_get_weak_action_dependence_for_learning_with_partial_observations%3F.html">317 hunch net-2008-09-12-How do we get weak action dependence for learning with partial observations?</a></p>
<p>Introduction: This post is about contextual bandit problems where, repeatedly:The world
chooses featuresxand rewards for each actionr1,â&euro;Ś,rkthen announces the
featuresx(but not the rewards).A policy chooses an actiona.The world announces
the rewardraThe goal in these situations is to learn a policy which
maximizesrain expectation efficiently. I'm thinking about all situations which
fit the above setting, whether they are drawn IID or adversarially from round
to round and whether they involve past logged data or rapidly learning via
interaction.One common drawback of all algorithms for solving this setting, is
that they have a poor dependence on the number of actions. For example ifkis
the number of actions,EXP4 (page 66)has a dependence onk0.5,epoch-greedy(and
the simpler epsilon greedy) have a dependence onk1/3, and theoffset treehas a
dependence onk-1. These results aren't directly comparable because different
things are being analyzed. The fact thatallanalyses have poor dependence onkis
troublesom</p><p>4 0.22515517 <a title="269-tfidf-4" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>Introduction: One thing which is clear on a little reflection is that there exists a single
master learning problem capable of encoding essentially all learning problems.
This problem is of course a very general sort of reinforcement learning where
the world interacts with an agent as:The world announces an observationx.The
agent makes a choicea.The world announces a rewardr.The goal here is to
maximize the sum of the rewards over the time of the agent. No particular
structure relatingxtoaoratoris implied by this setting so we do not know
effective general algorithms for the agent. It's very easy to prove lower
bounds showing that an agent cannot hope to succeed here--just consider the
case where actions are unrelated to rewards. Nevertheless, there is a real
sense in which essentially all forms of life are agents operating in this
setting, somehow succeeding. The gap between these observations drives
research--How can we find tractable specializations of the master problem
general enough to provide</p><p>5 0.20141843 <a title="269-tfidf-5" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>Introduction: Here are some papers fromICML 2008that I found interesting.Risi
KondorandKarsten Borgwardt,The Skew Spectrum of Graphs. This paper is about a
new family of functions on graphs which is invariant under node label
permutation. They show that these quantities appear to yield good features for
learning.Sanjoy DasguptaandDaniel Hsu.Hierarchical sampling for active
learning.This is the first published practical consistent active learning
algorithm. The abstract is also pretty impressive.Lihong Li,Michael Littman,
andThomas WalshKnows What It Knows: A Framework For Self-Aware Learning.This
is an attempt to create learning algorithms that know when they err, (other
work includesVovk). It's not yet clear to me what the right model forfeature-
dependent confidence intervalsis.Novi Quadrianto,Alex Smola,TIberio Caetano,
andQuoc Viet LeEstimating Labels from Label Proportions. This is an example of
learning in a specialization of the offline contextual bandit setting.Filip
Radlinski,Robert Kleinbe</p><p>6 0.18767452 <a title="269-tfidf-6" href="../hunch_net-2008/hunch_net-2008-03-23-Interactive_Machine_Learning.html">293 hunch net-2008-03-23-Interactive Machine Learning</a></p>
<p>7 0.1798597 <a title="269-tfidf-7" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<p>8 0.14778863 <a title="269-tfidf-8" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>9 0.13587168 <a title="269-tfidf-9" href="../hunch_net-2010/hunch_net-2010-09-17-New_York_Area_Machine_Learning_Events.html">410 hunch net-2010-09-17-New York Area Machine Learning Events</a></p>
<p>10 0.12364338 <a title="269-tfidf-10" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>11 0.12356722 <a title="269-tfidf-11" href="../hunch_net-2006/hunch_net-2006-02-11-Yahoo%26%238217%3Bs_Learning_Problems..html">156 hunch net-2006-02-11-Yahoo&#8217;s Learning Problems.</a></p>
<p>12 0.12041266 <a title="269-tfidf-12" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>13 0.11541329 <a title="269-tfidf-13" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>14 0.1150217 <a title="269-tfidf-14" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<p>15 0.10850815 <a title="269-tfidf-15" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>16 0.10828468 <a title="269-tfidf-16" href="../hunch_net-2010/hunch_net-2010-03-26-A_Variance_only_Deviation_Bound.html">392 hunch net-2010-03-26-A Variance only Deviation Bound</a></p>
<p>17 0.10520801 <a title="269-tfidf-17" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>18 0.10500942 <a title="269-tfidf-18" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>19 0.10115816 <a title="269-tfidf-19" href="../hunch_net-2007/hunch_net-2007-08-12-Exponentiated_Gradient.html">258 hunch net-2007-08-12-Exponentiated Gradient</a></p>
<p>20 0.099545032 <a title="269-tfidf-20" href="../hunch_net-2007/hunch_net-2007-12-19-Cool_and_interesting_things_seen_at_NIPS.html">279 hunch net-2007-12-19-Cool and interesting things seen at NIPS</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.253), (1, -0.104), (2, 0.024), (3, -0.025), (4, -0.092), (5, -0.077), (6, 0.096), (7, 0.003), (8, -0.086), (9, -0.115), (10, -0.051), (11, -0.004), (12, 0.152), (13, 0.029), (14, 0.093), (15, -0.022), (16, -0.097), (17, 0.009), (18, -0.231), (19, 0.064), (20, -0.054), (21, -0.044), (22, -0.073), (23, -0.089), (24, -0.087), (25, 0.051), (26, -0.13), (27, 0.039), (28, 0.063), (29, -0.038), (30, 0.076), (31, -0.035), (32, -0.01), (33, -0.066), (34, 0.002), (35, 0.081), (36, 0.012), (37, 0.06), (38, -0.009), (39, 0.01), (40, 0.035), (41, 0.067), (42, 0.02), (43, -0.009), (44, 0.01), (45, -0.12), (46, -0.03), (47, 0.05), (48, -0.023), (49, 0.076)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95998049 <a title="269-lsi-1" href="../hunch_net-2007/hunch_net-2007-10-24-Contextual_Bandits.html">269 hunch net-2007-10-24-Contextual Bandits</a></p>
<p>Introduction: One of the fundamental underpinnings of the internet is advertising based
content. This has become much more effective due to targeted advertising where
ads are specifically matched to interests. Everyone is familiar with this,
because everyone uses search engines and all search engines try to make money
this way.The problem of matching ads to interests is a natural machine
learning problem in some ways since there is much information in who clicks on
what. A fundamental problem with this information is that it is not supervised
--in particular a click-or-not on one ad doesn't generally tell you if a
different ad would have been clicked on. This implies we have a fundamental
exploration problem.A standard mathematical setting for this situation is
"k-Armed Bandits", often with various relevant embellishments. Thek-Armed
Bandit setting works on a round-by-round basis. On each round:A policy chooses
armafrom1ofkarms (i.e. 1 of k ads).The world reveals the rewardraof the chosen
arm (i.e.</p><p>2 0.83156139 <a title="269-lsi-2" href="../hunch_net-2008/hunch_net-2008-09-12-How_do_we_get_weak_action_dependence_for_learning_with_partial_observations%3F.html">317 hunch net-2008-09-12-How do we get weak action dependence for learning with partial observations?</a></p>
<p>Introduction: This post is about contextual bandit problems where, repeatedly:The world
chooses featuresxand rewards for each actionr1,â&euro;Ś,rkthen announces the
featuresx(but not the rewards).A policy chooses an actiona.The world announces
the rewardraThe goal in these situations is to learn a policy which
maximizesrain expectation efficiently. I'm thinking about all situations which
fit the above setting, whether they are drawn IID or adversarially from round
to round and whether they involve past logged data or rapidly learning via
interaction.One common drawback of all algorithms for solving this setting, is
that they have a poor dependence on the number of actions. For example ifkis
the number of actions,EXP4 (page 66)has a dependence onk0.5,epoch-greedy(and
the simpler epsilon greedy) have a dependence onk1/3, and theoffset treehas a
dependence onk-1. These results aren't directly comparable because different
things are being analyzed. The fact thatallanalyses have poor dependence onkis
troublesom</p><p>3 0.83122265 <a title="269-lsi-3" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>Introduction: Consider the contextual bandit setting where, repeatedly:A contextxis
observed.An actionais taken given the contextx.A rewardris observed, dependent
onxanda.Where the goal of a learning agent is to find a policy for step 2
achieving a large expected reward.This setting is of obvious importance,
because in the real world we typically make decisions based on some set of
information and then get feedback only about the single action taken. It also
fundamentally differs from supervised learning settings because knowing the
value of one action is not equivalent to knowing the value of all actions.A
decade ago the best machine learning techniques for this setting where
implausibly inefficient.Dean Fosteronce told me he thought the area was a
research sinkhole with little progress to be expected. Now we are on the verge
of being able to routinely attack these problems, in almost exactly the same
sense that we routinely attack bread and butter supervised learning problems.
Just as for supervis</p><p>4 0.77428705 <a title="269-lsi-4" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>Introduction: One thing which is clear on a little reflection is that there exists a single
master learning problem capable of encoding essentially all learning problems.
This problem is of course a very general sort of reinforcement learning where
the world interacts with an agent as:The world announces an observationx.The
agent makes a choicea.The world announces a rewardr.The goal here is to
maximize the sum of the rewards over the time of the agent. No particular
structure relatingxtoaoratoris implied by this setting so we do not know
effective general algorithms for the agent. It's very easy to prove lower
bounds showing that an agent cannot hope to succeed here--just consider the
case where actions are unrelated to rewards. Nevertheless, there is a real
sense in which essentially all forms of life are agents operating in this
setting, somehow succeeding. The gap between these observations drives
research--How can we find tractable specializations of the master problem
general enough to provide</p><p>5 0.72559839 <a title="269-lsi-5" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<p>Introduction: There weretwopapersat ICML presenting learning algorithms for acontextual
bandit-style setting, where the loss for all labels is not known, but the loss
for one label is known. (The first might require aexploration
scavengingviewpoint to understand if the experimental assignment was
nonrandom.) I strongly approve of these papers and further work in this
setting and its variants, because I expect it to become more important than
supervised learning. As a quick review, we are thinking about situations where
repeatedly:The world reveals feature values (aka context information).A policy
chooses an action.The world provides a reward.Sometimes this is done in an
online fashion where the policy can change based on immediate feedback and
sometimes it's done in a batch setting where many samples are collected before
the policy can change. If you haven't spent time thinking about the setting,
you might want to because there are many natural applications.I'm going to
pick on the Banditron paper (</p><p>6 0.66194451 <a title="269-lsi-6" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>7 0.60746998 <a title="269-lsi-7" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>8 0.5723561 <a title="269-lsi-8" href="../hunch_net-2009/hunch_net-2009-10-03-Static_vs._Dynamic_multiclass_prediction.html">373 hunch net-2009-10-03-Static vs. Dynamic multiclass prediction</a></p>
<p>9 0.56512779 <a title="269-lsi-9" href="../hunch_net-2008/hunch_net-2008-03-23-Interactive_Machine_Learning.html">293 hunch net-2008-03-23-Interactive Machine Learning</a></p>
<p>10 0.55164886 <a title="269-lsi-10" href="../hunch_net-2010/hunch_net-2010-03-15-The_Efficient_Robust_Conditional_Probability_Estimation_Problem.html">391 hunch net-2010-03-15-The Efficient Robust Conditional Probability Estimation Problem</a></p>
<p>11 0.55054569 <a title="269-lsi-11" href="../hunch_net-2010/hunch_net-2010-03-26-A_Variance_only_Deviation_Bound.html">392 hunch net-2010-03-26-A Variance only Deviation Bound</a></p>
<p>12 0.54715359 <a title="269-lsi-12" href="../hunch_net-2007/hunch_net-2007-07-01-Watchword%3A_Online_Learning.html">252 hunch net-2007-07-01-Watchword: Online Learning</a></p>
<p>13 0.50347507 <a title="269-lsi-13" href="../hunch_net-2006/hunch_net-2006-02-11-Yahoo%26%238217%3Bs_Learning_Problems..html">156 hunch net-2006-02-11-Yahoo&#8217;s Learning Problems.</a></p>
<p>14 0.497657 <a title="269-lsi-14" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>15 0.48340139 <a title="269-lsi-15" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>16 0.48296604 <a title="269-lsi-16" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>17 0.48254696 <a title="269-lsi-17" href="../hunch_net-2006/hunch_net-2006-04-05-What_is_state%3F.html">169 hunch net-2006-04-05-What is state?</a></p>
<p>18 0.47897646 <a title="269-lsi-18" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>19 0.47523254 <a title="269-lsi-19" href="../hunch_net-2008/hunch_net-2008-08-24-Mass_Customized_Medicine_in_the_Future%3F.html">314 hunch net-2008-08-24-Mass Customized Medicine in the Future?</a></p>
<p>20 0.47466004 <a title="269-lsi-20" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(6, 0.048), (35, 0.033), (38, 0.292), (42, 0.306), (45, 0.038), (50, 0.016), (68, 0.049), (69, 0.043), (74, 0.063), (95, 0.018)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.95409775 <a title="269-lda-1" href="../hunch_net-2008/hunch_net-2008-02-27-The_Stats_Handicap.html">290 hunch net-2008-02-27-The Stats Handicap</a></p>
<p>Introduction: Graduating students in Statistics appear to be at a substantial handicap
compared to graduating students in Machine Learning, despite being in
substantially overlapping subjects.The problem seems to be cultural.
Statistics comes from a mathematics background which emphasizes large
publications slowly published under review at journals. Machine Learning comes
from a Computer Science background which emphasizes quick publishing at
reviewed conferences. This has a number of implications:Graduating statistics
PhDs often have 0-2 publications while graduating machine learning PhDs might
have 5-15.Graduating ML students have had a chance for others to build on
their work. Stats students have had no such chance.Graduating ML students have
attended a number of conferences and presented their work, giving them a
chance to meet people. Stats students have had fewer chances of this sort.In
short, Stats students have had relatively few chances to distinguish
themselves and are heavily reliant on t</p><p>2 0.94637471 <a title="269-lda-2" href="../hunch_net-2008/hunch_net-2008-09-04-Fall_ML_Conferences.html">316 hunch net-2008-09-04-Fall ML Conferences</a></p>
<p>Introduction: If you are in the New York area and interested in machine learning, consider
submitting a 2 page abstract to theML symposiumby tomorrow (Sept 5th)
midnight. It's a fun one day affair on October 10 in an awesome location
overlooking the world trade center site.A bit further off (but a real
conference) is theAI and Statsdeadline on November 5, to be held in Florida
April 16-19.</p><p>3 0.93913525 <a title="269-lda-3" href="../hunch_net-2007/hunch_net-2007-06-14-Interesting_Papers_at_COLT_2007.html">247 hunch net-2007-06-14-Interesting Papers at COLT 2007</a></p>
<p>Introduction: Here are two papers that seem particularly interesting at this year's
COLT.Gilles BlanchardandFranÃƒÂ§ois Fleuret,Occam's Hammer. When we are
interested in very tight bounds on the true error rate of a classifier, it is
tempting to use a PAC-Bayes bound which can (empirically) bequite tight. A
disadvantage of the PAC-Bayes bound is that it applies to a classifier which
is randomized over a set of base classifiers rather than a single classifier.
This paper shows that a similar bound can be proved which holds for a single
classifier drawn from the set. The ability to safely use a single classifier
is very nice. This technique applies generically to any base bound, so it has
other applications covered in the paper.Adam Tauman Kalai.Learning Nested
Halfspaces and Uphill Decision Trees. Classification PAC-learning, where you
prove that any problem amongst some set is polytime learnable with respect to
any distribution over the inputXis extraordinarily challenging as judged by
lack of progr</p><p>4 0.93170983 <a title="269-lda-4" href="../hunch_net-2008/hunch_net-2008-06-30-ICML_has_a_comment_system.html">305 hunch net-2008-06-30-ICML has a comment system</a></p>
<p>Introduction: Mark Reidhas stepped up and created acomment system for ICML paperswhichGreger
Lindenhas tightly integrated.My understanding is that Mark spent quite a bit
of time on the details, and there are some cool features like working latex
math mode. This is an excellent chance for the ICML community to experiment
with making ICML year-round, so I hope it works out. Please do consider
experimenting with it.</p><p>5 0.92981744 <a title="269-lda-5" href="../hunch_net-2005/hunch_net-2005-02-27-Antilearning%3A_When_proximity_goes_bad.html">32 hunch net-2005-02-27-Antilearning: When proximity goes bad</a></p>
<p>Introduction: Joel Preddmentioned"Antilearning" byAdam Kowalczyk, which is interesting from
a foundational intuitions viewpoint.There is a pervasive intuition that
"nearby things tend to have the same label". This intuition is instantiated in
SVMs, nearest neighbor classifiers, decision trees, and neural networks. It
turns out there are natural problems where this intuition is opposite of the
truth.One natural situation where this occurs is in competition. For example,
whenIntelfails to meet its earnings estimate, is this evidence thatAMDis doing
badly also? Or evidence that AMD is doing well?This violation of the proximity
intuition means that when the number of examples is few,negatinga classifier
which attempts to exploit proximity can provide predictive power (thus, the
term "antilearning").</p><p>same-blog 6 0.8865993 <a title="269-lda-6" href="../hunch_net-2007/hunch_net-2007-10-24-Contextual_Bandits.html">269 hunch net-2007-10-24-Contextual Bandits</a></p>
<p>7 0.87295866 <a title="269-lda-7" href="../hunch_net-2006/hunch_net-2006-08-10-Precision_is_not_accuracy.html">202 hunch net-2006-08-10-Precision is not accuracy</a></p>
<p>8 0.83349866 <a title="269-lda-8" href="../hunch_net-2011/hunch_net-2011-10-10-ML_Symposium_and_ICML_details.html">447 hunch net-2011-10-10-ML Symposium and ICML details</a></p>
<p>9 0.80312419 <a title="269-lda-9" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>10 0.74311018 <a title="269-lda-10" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>11 0.74092084 <a title="269-lda-11" href="../hunch_net-2008/hunch_net-2008-03-23-Interactive_Machine_Learning.html">293 hunch net-2008-03-23-Interactive Machine Learning</a></p>
<p>12 0.73794478 <a title="269-lda-12" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>13 0.73685575 <a title="269-lda-13" href="../hunch_net-2005/hunch_net-2005-02-03-Learning_Theory%2C_by_assumption.html">12 hunch net-2005-02-03-Learning Theory, by assumption</a></p>
<p>14 0.7364291 <a title="269-lda-14" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>15 0.73027313 <a title="269-lda-15" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>16 0.72832644 <a title="269-lda-16" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>17 0.72727555 <a title="269-lda-17" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<p>18 0.72701621 <a title="269-lda-18" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>19 0.72577792 <a title="269-lda-19" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>20 0.72507191 <a title="269-lda-20" href="../hunch_net-2005/hunch_net-2005-02-25-Problem%3A_Online_Learning.html">28 hunch net-2005-02-25-Problem: Online Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
