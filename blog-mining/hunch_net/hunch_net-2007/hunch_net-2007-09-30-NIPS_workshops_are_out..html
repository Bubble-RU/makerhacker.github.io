<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>264 hunch net-2007-09-30-NIPS workshops are out.</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2007" href="../home/hunch_net-2007_home.html">hunch_net-2007</a> <a title="hunch_net-2007-264" href="#">hunch_net-2007-264</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>264 hunch net-2007-09-30-NIPS workshops are out.</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2007-264-html" href="http://hunch.net/?p=292">html</a></p><p>Introduction: Here. I'm particularly interested in theWeb Search,Efficient ML, and (of
course)Learning Problem Designworkshops but there are many others to check out
as well. Workshops are a great chance to make progress on or learn about a
topic. Relevance and interaction amongst diverse people can sometimes be
magical.</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('magical', 0.441), ('relevance', 0.368), ('interaction', 0.305), ('diverse', 0.27), ('check', 0.265), ('workshops', 0.227), ('progress', 0.222), ('amongst', 0.222), ('chance', 0.206), ('ml', 0.203), ('course', 0.203), ('others', 0.172), ('interested', 0.154), ('learn', 0.145), ('particularly', 0.142), ('sometimes', 0.141), ('great', 0.136), ('make', 0.099), ('problem', 0.083), ('people', 0.07)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="264-tfidf-1" href="../hunch_net-2007/hunch_net-2007-09-30-NIPS_workshops_are_out..html">264 hunch net-2007-09-30-NIPS workshops are out.</a></p>
<p>Introduction: Here. I'm particularly interested in theWeb Search,Efficient ML, and (of
course)Learning Problem Designworkshops but there are many others to check out
as well. Workshops are a great chance to make progress on or learn about a
topic. Relevance and interaction amongst diverse people can sometimes be
magical.</p><p>2 0.19676545 <a title="264-tfidf-2" href="../hunch_net-2008/hunch_net-2008-03-23-Interactive_Machine_Learning.html">293 hunch net-2008-03-23-Interactive Machine Learning</a></p>
<p>Introduction: A new direction of research seems to be arising in machine learning:
Interactive Machine Learning. This isn't a familiar term, although it does
include some familiar subjects.What is Interactive Machine Learning?The
fundamental requirement is (a) learning algorithms which interact with the
world and (b) learn.For our purposes, let's define learning as efficiently
competing with a large set of possible predictors. Examples include:Online
learning against an adversary (Avrim's Notes). The interaction is almost
trivial: the learning algorithm makes a prediction and then receives feedback.
The learning is choosing based upon the advice of many experts.Active
Learning. In active learning, the interaction is choosing which examples to
label, and the learning is choosing from amongst a large set of
hypotheses.Contextual Bandits. The interaction is choosing one of several
actions and learning only the value of the chosen action (weaker than active
learning feedback).More forms of interaction w</p><p>3 0.13591789 <a title="264-tfidf-3" href="../hunch_net-2009/hunch_net-2009-11-23-ICML_2009_Workshops_%28and_Tutorials%29.html">379 hunch net-2009-11-23-ICML 2009 Workshops (and Tutorials)</a></p>
<p>Introduction: I'm theworkshops chairforICMLthis year. As such, I would like to personally
encourage people to consider running a workshop.My general view of workshops
is that they are excellent as opportunities to discuss and develop research
directions--some of my best work has come from collaborations at workshops and
several workshops have substantially altered my thinking about various
problems. My experience running workshops is that setting them up and making
them fly often appears much harder than it actually is, and the workshops
often come off much better than expected in the end. Submissions are due
January 18, two weeks before papers.Similarly,Ben Taskaris looking for
goodtutorials, which is complementary. Workshops are about exploring a
subject, while a tutorial is about distilling it down into an easily taught
essence, a vital part of the research process. Tutorials are due February 13,
two weeks after papers.</p><p>4 0.12994112 <a title="264-tfidf-4" href="../hunch_net-2005/hunch_net-2005-03-24-The_Role_of_Workshops.html">46 hunch net-2005-03-24-The Role of Workshops</a></p>
<p>Introduction: A good workshop is often far more interesting than the papers at a conference.
This happens because a workshop has a much tighter focus than a conference.
Since you choose the workshops fitting your interest, the increased relevance
can greatly enhance the level of your interest and attention. Roughly
speaking, a workshop program consists of elements related to a subject of your
interest. The main conference program consists of elements related to
someone's interest (which is rarely your own). Workshops are more about doing
research while conferences are more about presenting research.Several
conferences have associated workshop programs, some with deadlines due
shortly.ICML workshopsDue April 1IJCAI workshopsDeadlines VaryKDD workshopsNot
yet finalizedAnyone going to these conferences should examine the workshops
and see if any are of interest. (If none are, then maybe you should organize
one next year.)</p><p>5 0.11119 <a title="264-tfidf-5" href="../hunch_net-2009/hunch_net-2009-10-26-NIPS_workshops.html">375 hunch net-2009-10-26-NIPS workshops</a></p>
<p>Introduction: Many of theNIPS workshopshave a deadline about now, and the NIPSearly
registration deadline is Nov. 6. Several interest me:Adaptive Sensing, Active
Learning, and Experimental Designdue 10/27.Discrete Optimization in Machine
Learning: Submodularity, Sparsity & Polyhedra, due Nov. 6.Large-Scale Machine
Learning: Parallelism and Massive Datasets, due 10/23 (i.e. past)Analysis and
Design of Algorithms for Interactive Machine Learning, due 10/30.And I'm sure
many of the others interest others. Workshops are great as a mechanism for
research, so take a look if there is any chance you might be interested.</p><p>6 0.10945695 <a title="264-tfidf-6" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>7 0.10117522 <a title="264-tfidf-7" href="../hunch_net-2013/hunch_net-2013-06-10-The_Large_Scale_Learning_class_notes.html">483 hunch net-2013-06-10-The Large Scale Learning class notes</a></p>
<p>8 0.10063119 <a title="264-tfidf-8" href="../hunch_net-2009/hunch_net-2009-12-24-Top_graduates_this_season.html">384 hunch net-2009-12-24-Top graduates this season</a></p>
<p>9 0.091204286 <a title="264-tfidf-9" href="../hunch_net-2010/hunch_net-2010-01-13-Sam_Roweis_died.html">386 hunch net-2010-01-13-Sam Roweis died</a></p>
<p>10 0.086574093 <a title="264-tfidf-10" href="../hunch_net-2008/hunch_net-2008-03-15-COLT_Open_Problems.html">292 hunch net-2008-03-15-COLT Open Problems</a></p>
<p>11 0.085784964 <a title="264-tfidf-11" href="../hunch_net-2006/hunch_net-2006-11-02-2006_NIPS_workshops.html">216 hunch net-2006-11-02-2006 NIPS workshops</a></p>
<p>12 0.085393049 <a title="264-tfidf-12" href="../hunch_net-2005/hunch_net-2005-06-10-Workshops_are_not_Conferences.html">80 hunch net-2005-06-10-Workshops are not Conferences</a></p>
<p>13 0.084169909 <a title="264-tfidf-13" href="../hunch_net-2007/hunch_net-2007-11-16-MLSS_2008.html">273 hunch net-2007-11-16-MLSS 2008</a></p>
<p>14 0.082343578 <a title="264-tfidf-14" href="../hunch_net-2005/hunch_net-2005-05-14-NIPS.html">71 hunch net-2005-05-14-NIPS</a></p>
<p>15 0.081403762 <a title="264-tfidf-15" href="../hunch_net-2008/hunch_net-2008-01-23-Why_Workshop%3F.html">285 hunch net-2008-01-23-Why Workshop?</a></p>
<p>16 0.078428216 <a title="264-tfidf-16" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>17 0.07582324 <a title="264-tfidf-17" href="../hunch_net-2007/hunch_net-2007-10-15-NIPS_workshops_extended_to_3_days.html">266 hunch net-2007-10-15-NIPS workshops extended to 3 days</a></p>
<p>18 0.074881874 <a title="264-tfidf-18" href="../hunch_net-2010/hunch_net-2010-01-19-Deadline_Season%2C_2010.html">387 hunch net-2010-01-19-Deadline Season, 2010</a></p>
<p>19 0.072787017 <a title="264-tfidf-19" href="../hunch_net-2008/hunch_net-2008-08-18-Radford_Neal_starts_a_blog.html">313 hunch net-2008-08-18-Radford Neal starts a blog</a></p>
<p>20 0.069439322 <a title="264-tfidf-20" href="../hunch_net-2011/hunch_net-2011-10-24-2011_ML_symposium_and_the_bears.html">448 hunch net-2011-10-24-2011 ML symposium and the bears</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.12), (1, 0.053), (2, 0.094), (3, 0.12), (4, -0.01), (5, -0.052), (6, -0.04), (7, -0.034), (8, -0.056), (9, 0.025), (10, -0.03), (11, 0.007), (12, 0.035), (13, 0.004), (14, 0.005), (15, 0.079), (16, -0.099), (17, -0.045), (18, 0.048), (19, 0.034), (20, 0.106), (21, 0.092), (22, -0.088), (23, -0.001), (24, 0.151), (25, -0.085), (26, 0.004), (27, 0.114), (28, 0.013), (29, -0.046), (30, 0.139), (31, -0.152), (32, -0.044), (33, -0.018), (34, 0.027), (35, 0.063), (36, 0.02), (37, -0.116), (38, 0.033), (39, -0.052), (40, -0.095), (41, 0.133), (42, 0.068), (43, 0.041), (44, 0.035), (45, -0.038), (46, -0.048), (47, 0.057), (48, 0.037), (49, -0.017)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97293186 <a title="264-lsi-1" href="../hunch_net-2007/hunch_net-2007-09-30-NIPS_workshops_are_out..html">264 hunch net-2007-09-30-NIPS workshops are out.</a></p>
<p>Introduction: Here. I'm particularly interested in theWeb Search,Efficient ML, and (of
course)Learning Problem Designworkshops but there are many others to check out
as well. Workshops are a great chance to make progress on or learn about a
topic. Relevance and interaction amongst diverse people can sometimes be
magical.</p><p>2 0.58098429 <a title="264-lsi-2" href="../hunch_net-2006/hunch_net-2006-11-02-2006_NIPS_workshops.html">216 hunch net-2006-11-02-2006 NIPS workshops</a></p>
<p>Introduction: I expect theNIPS 2006 workshopsto be quite interesting, and recommend going
for anyone interested in machine learning research. (Most or all of the
workshops webpages can be found two links deep.)</p><p>3 0.57014316 <a title="264-lsi-3" href="../hunch_net-2010/hunch_net-2010-08-21-Rob_Schapire_at_NYC_ML_Meetup.html">405 hunch net-2010-08-21-Rob Schapire at NYC ML Meetup</a></p>
<p>Introduction: I've been wanting to attend theNYC ML Meetupfor some time and hope to make
itnext week on the 25th.Rob Schapireis talking about "Playing Repeated Games",
which in my experience is far more relevant to machine learning than the title
might indicate.</p><p>4 0.55185103 <a title="264-lsi-4" href="../hunch_net-2007/hunch_net-2007-10-15-NIPS_workshops_extended_to_3_days.html">266 hunch net-2007-10-15-NIPS workshops extended to 3 days</a></p>
<p>Introduction: (Unofficially, at least.) TheDeep Learning Workshopis being held the afternoon
before the rest of the workshops in Vancouver, BC. Separate registration is
needed, and open.What's happening fundamentally here is that there are too
many interesting workshops to fit into 2 days. Perhaps we can get it
officially expanded to 3 days next year.</p><p>5 0.54089028 <a title="264-lsi-5" href="../hunch_net-2013/hunch_net-2013-06-10-The_Large_Scale_Learning_class_notes.html">483 hunch net-2013-06-10-The Large Scale Learning class notes</a></p>
<p>Introduction: Thelarge scale machine learning classI taught withYann LeCunhas finished. As I
expected, it took quite a bit of time. We had about 25 people attending in
person on average and 400 regularly watching therecorded lectureswhich is
substantially more sustained interest than I expected for an advanced ML
class. We also had some fun with class projects--I'm hopeful that several will
eventually turn into papers.I expect there are a number of professors
interested in lecturing on this and related topics. Everyone will have their
personal taste in subjects of course, but hopefully there will be some
convergence to common course materials as well. To help with this, I am making
thesources to my presentations available. Feel free to
use/improve/embelish/ridicule/etcâ&euro;Ś in the pursuit of the perfect course.</p><p>6 0.52323467 <a title="264-lsi-6" href="../hunch_net-2009/hunch_net-2009-12-24-Top_graduates_this_season.html">384 hunch net-2009-12-24-Top graduates this season</a></p>
<p>7 0.50696349 <a title="264-lsi-7" href="../hunch_net-2010/hunch_net-2010-01-13-Sam_Roweis_died.html">386 hunch net-2010-01-13-Sam Roweis died</a></p>
<p>8 0.49729124 <a title="264-lsi-8" href="../hunch_net-2009/hunch_net-2009-11-23-ICML_2009_Workshops_%28and_Tutorials%29.html">379 hunch net-2009-11-23-ICML 2009 Workshops (and Tutorials)</a></p>
<p>9 0.47397754 <a title="264-lsi-9" href="../hunch_net-2013/hunch_net-2013-03-22-I%26%238217%3Bm_a_bandit.html">480 hunch net-2013-03-22-I&#8217;m a bandit</a></p>
<p>10 0.46929666 <a title="264-lsi-10" href="../hunch_net-2005/hunch_net-2005-05-14-NIPS.html">71 hunch net-2005-05-14-NIPS</a></p>
<p>11 0.44750085 <a title="264-lsi-11" href="../hunch_net-2008/hunch_net-2008-11-28-A_Bumper_Crop_of_Machine_Learning_Graduates.html">329 hunch net-2008-11-28-A Bumper Crop of Machine Learning Graduates</a></p>
<p>12 0.4395639 <a title="264-lsi-12" href="../hunch_net-2005/hunch_net-2005-09-19-NIPS_Workshops.html">113 hunch net-2005-09-19-NIPS Workshops</a></p>
<p>13 0.42880464 <a title="264-lsi-13" href="../hunch_net-2008/hunch_net-2008-03-23-Interactive_Machine_Learning.html">293 hunch net-2008-03-23-Interactive Machine Learning</a></p>
<p>14 0.42710039 <a title="264-lsi-14" href="../hunch_net-2009/hunch_net-2009-10-26-NIPS_workshops.html">375 hunch net-2009-10-26-NIPS workshops</a></p>
<p>15 0.41506588 <a title="264-lsi-15" href="../hunch_net-2010/hunch_net-2010-09-28-Machined_Learnings.html">412 hunch net-2010-09-28-Machined Learnings</a></p>
<p>16 0.38602382 <a title="264-lsi-16" href="../hunch_net-2008/hunch_net-2008-08-18-Radford_Neal_starts_a_blog.html">313 hunch net-2008-08-18-Radford Neal starts a blog</a></p>
<p>17 0.38470709 <a title="264-lsi-17" href="../hunch_net-2008/hunch_net-2008-01-23-Why_Workshop%3F.html">285 hunch net-2008-01-23-Why Workshop?</a></p>
<p>18 0.38052258 <a title="264-lsi-18" href="../hunch_net-2010/hunch_net-2010-09-17-New_York_Area_Machine_Learning_Events.html">410 hunch net-2010-09-17-New York Area Machine Learning Events</a></p>
<p>19 0.37720579 <a title="264-lsi-19" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>20 0.37643588 <a title="264-lsi-20" href="../hunch_net-2009/hunch_net-2009-11-06-Yisong_Yue_on_Self-improving_Systems.html">376 hunch net-2009-11-06-Yisong Yue on Self-improving Systems</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(14, 0.316), (42, 0.263), (74, 0.184), (95, 0.051)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.89335811 <a title="264-lda-1" href="../hunch_net-2005/hunch_net-2005-09-20-Workshop_Proposal%3A_Atomic_Learning.html">114 hunch net-2005-09-20-Workshop Proposal: Atomic Learning</a></p>
<p>Introduction: This is a proposal for a workshop. It may or may not happen depending on the
level of interest. If you are interested, feel free to indicate so (by email
or comments).Description:Assume(*) that any system for solving large difficult
learning problems must decompose into repeated use of basic elements (i.e.
atoms). There are many basic questions which remain:What are the viable basic
elements?What makes a basic element viable?What are the viable principles for
the composition of these basic elements?What are the viable principles for
learning in such systems?What problems can this approach handle?Hal Daume
adds:Can composition of atoms be (semi-) automatically constructed[?]When
atoms are constructed through reductions, is there some notion of the
"naturalness" of the created leaning problems?Other than Markov
fields/graphical models/Bayes nets, is there a good language for representing
atoms and their compositions?The answer to these and related questions remain
unclear to me. A worksh</p><p>2 0.88500166 <a title="264-lda-2" href="../hunch_net-2007/hunch_net-2007-11-14-BellKor_wins_Netflix.html">272 hunch net-2007-11-14-BellKor wins Netflix</a></p>
<p>Introduction: â&euro;Ś but only the little prize. TheBellKor teamfocused on integrating predictions
from many different methods. The base methods consist of:Nearest Neighbor
MethodsMatrix Factorization Methods (asymmetric and symmetric)Linear
Regression on various feature spacesRestricted Boltzman MachinesThe final
predictor was an ensemble (as was reasonable to expect), although it's a
little bit more complicated than just a weighted average--it's essentially a
customized learning algorithm. Base approaches (1)-(3) seem like relatively
well-known approaches (although I haven't seen the asymmetric factorization
variant before). RBMs are the new approach.Thewriteupis pretty clear for more
details.The contestants are close to reaching the big prize, but the last 1.5%
is probably at least as hard as what's been done. A few new structurally
different methods for making predictions may need to be discovered and added
into the mixture. In other words, research may be required.</p><p>same-blog 3 0.85163993 <a title="264-lda-3" href="../hunch_net-2007/hunch_net-2007-09-30-NIPS_workshops_are_out..html">264 hunch net-2007-09-30-NIPS workshops are out.</a></p>
<p>Introduction: Here. I'm particularly interested in theWeb Search,Efficient ML, and (of
course)Learning Problem Designworkshops but there are many others to check out
as well. Workshops are a great chance to make progress on or learn about a
topic. Relevance and interaction amongst diverse people can sometimes be
magical.</p><p>4 0.82041264 <a title="264-lda-4" href="../hunch_net-2005/hunch_net-2005-11-07-Prediction_Competitions.html">129 hunch net-2005-11-07-Prediction Competitions</a></p>
<p>Introduction: There are two prediction competitions currently in the air.ThePerformance
Prediction ChallengebyIsabelle Guyon. Good entries minimize a weighted 0/1
loss + the difference between a prediction of this loss and the observed truth
on 5 datasets. Isabelle tells me all of the problems are "real world" and the
test datasets are large enough (17K minimum) that the winner should be well
determined by ability rather than luck. This is due March 1.ThePredictive
Uncertainty ChallengebyGavin Cawley. Good entries minimize log loss on real
valued output variables for one synthetic and 3 "real" datasets related to
atmospheric prediction. The use of log loss (which can be infinite and hence
is never convergent) and smaller test sets of size 1K to 7K examples makes the
winner of this contest more luck dependent. Nevertheless, the contest may be
of some interest particularly to the branch of learning (typically Bayes
learning) which prefers to optimize log loss.May the best predictor win.</p><p>5 0.79491264 <a title="264-lda-5" href="../hunch_net-2008/hunch_net-2008-12-12-Summer_Conferences.html">331 hunch net-2008-12-12-Summer Conferences</a></p>
<p>Introduction: Here's a handy table for the summer conferences.ConferenceDeadlineReviewer
TargetingDouble BlindAuthor FeedbackLocationDateICML(wrong ICML)January
26YesYesYesMontreal, CanadaJune 14-17COLTFebruary 13NoNoYesMontrealJune
19-21UAIMarch 13NoYesNoMontrealJune 19-21KDDFebruary 2/6NoNoNoParis,
FranceJune 28-July 1Reviewer targeting is new this year. The idea is that many
poor decisions happen because the papers go to reviewers who are unqualified,
and the hope is that allowing authors to point out who is qualified results in
better decisions. In my experience, this is a reasonable idea to test.Both UAI
and COLT are experimenting this year as well with double blind and author
feedback, respectively. Of the two, I believe author feedback is more
important, as I've seen it make a difference. However, I still consider double
blind reviewing a net win, as it's a substantial public commitment to
fairness.</p><p>6 0.70124823 <a title="264-lda-6" href="../hunch_net-2008/hunch_net-2008-11-10-ICML_Reviewing_Criteria.html">325 hunch net-2008-11-10-ICML Reviewing Criteria</a></p>
<p>7 0.69824696 <a title="264-lda-7" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>8 0.69705713 <a title="264-lda-8" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>9 0.69458187 <a title="264-lda-9" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>10 0.69276083 <a title="264-lda-10" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>11 0.69216138 <a title="264-lda-11" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>12 0.68999493 <a title="264-lda-12" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>13 0.68959606 <a title="264-lda-13" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>14 0.6885249 <a title="264-lda-14" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>15 0.68690121 <a title="264-lda-15" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>16 0.68505013 <a title="264-lda-16" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>17 0.68451166 <a title="264-lda-17" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>18 0.68386316 <a title="264-lda-18" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>19 0.68180662 <a title="264-lda-19" href="../hunch_net-2010/hunch_net-2010-04-24-COLT_Treasurer_is_now_Phil_Long.html">394 hunch net-2010-04-24-COLT Treasurer is now Phil Long</a></p>
<p>20 0.68140751 <a title="264-lda-20" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
