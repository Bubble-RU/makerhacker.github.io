<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>272 hunch net-2007-11-14-BellKor wins Netflix</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2007" href="../home/hunch_net-2007_home.html">hunch_net-2007</a> <a title="hunch_net-2007-272" href="#">hunch_net-2007-272</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>272 hunch net-2007-11-14-BellKor wins Netflix</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2007-272-html" href="http://hunch.net/?p=300">html</a></p><p>Introduction: â&euro;Ś but only the little prize. TheBellKor teamfocused on integrating predictions
from many different methods. The base methods consist of:Nearest Neighbor
MethodsMatrix Factorization Methods (asymmetric and symmetric)Linear
Regression on various feature spacesRestricted Boltzman MachinesThe final
predictor was an ensemble (as was reasonable to expect), although it's a
little bit more complicated than just a weighted average--it's essentially a
customized learning algorithm. Base approaches (1)-(3) seem like relatively
well-known approaches (although I haven't seen the asymmetric factorization
variant before). RBMs are the new approach.Thewriteupis pretty clear for more
details.The contestants are close to reaching the big prize, but the last 1.5%
is probably at least as hard as what's been done. A few new structurally
different methods for making predictions may need to be discovered and added
into the mixture. In other words, research may be required.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 TheBellKor teamfocused on integrating predictions from many different methods. [sent-2, score-0.433]
</p><p>2 Base approaches (1)-(3) seem like relatively well-known approaches (although I haven't seen the asymmetric factorization variant before). [sent-4, score-1.29]
</p><p>3 The contestants are close to reaching the big prize, but the last 1. [sent-7, score-0.575]
</p><p>4 5% is probably at least as hard as what's been done. [sent-8, score-0.216]
</p><p>5 A few new structurally different methods for making predictions may need to be discovered and added into the mixture. [sent-9, score-1.252]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('asymmetric', 0.351), ('factorization', 0.351), ('base', 0.228), ('methods', 0.221), ('predictions', 0.193), ('structurally', 0.189), ('consist', 0.189), ('rbms', 0.189), ('symmetric', 0.189), ('contestants', 0.175), ('reaching', 0.152), ('approaches', 0.151), ('discovered', 0.146), ('complicated', 0.146), ('integrating', 0.142), ('although', 0.136), ('ensemble', 0.134), ('little', 0.132), ('variant', 0.128), ('prize', 0.128), ('neighbor', 0.122), ('weighted', 0.12), ('nearest', 0.118), ('regression', 0.114), ('added', 0.114), ('close', 0.11), ('words', 0.109), ('final', 0.103), ('different', 0.098), ('predictor', 0.093), ('probably', 0.089), ('required', 0.087), ('pretty', 0.086), ('feature', 0.082), ('seen', 0.082), ('linear', 0.082), ('clear', 0.079), ('may', 0.078), ('relatively', 0.076), ('essentially', 0.074), ('expect', 0.073), ('new', 0.073), ('big', 0.072), ('various', 0.071), ('need', 0.07), ('making', 0.07), ('last', 0.066), ('hard', 0.064), ('least', 0.063), ('reasonable', 0.063)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="272-tfidf-1" href="../hunch_net-2007/hunch_net-2007-11-14-BellKor_wins_Netflix.html">272 hunch net-2007-11-14-BellKor wins Netflix</a></p>
<p>Introduction: â&euro;Ś but only the little prize. TheBellKor teamfocused on integrating predictions
from many different methods. The base methods consist of:Nearest Neighbor
MethodsMatrix Factorization Methods (asymmetric and symmetric)Linear
Regression on various feature spacesRestricted Boltzman MachinesThe final
predictor was an ensemble (as was reasonable to expect), although it's a
little bit more complicated than just a weighted average--it's essentially a
customized learning algorithm. Base approaches (1)-(3) seem like relatively
well-known approaches (although I haven't seen the asymmetric factorization
variant before). RBMs are the new approach.Thewriteupis pretty clear for more
details.The contestants are close to reaching the big prize, but the last 1.5%
is probably at least as hard as what's been done. A few new structurally
different methods for making predictions may need to be discovered and added
into the mixture. In other words, research may be required.</p><p>2 0.11198895 <a title="272-tfidf-2" href="../hunch_net-2012/hunch_net-2012-02-24-ICML%2B50%25.html">456 hunch net-2012-02-24-ICML+50%</a></p>
<p>Introduction: TheICMLpaper deadline has passed.Joelleand I were surprised to see the number
of submissions jump from last year by about 50% to around 900 submissions. A
tiny portion of these are immediate rejects(*), so this is a much larger set
of papers than expected. The number of workshop submissions also doubled
compared to last year, so ICML may grow significantly this year, if we can
manage to handle the load well. The prospect of making 900 good decisions is
fundamentally daunting, and success will rely heavily on theprogram
committeeandarea chairsat this point.For those who want to rubberneck a bit
more, here's a breakdown of submissions by primary topic of submitted
papers:66 Reinforcement Learning 52 Supervised Learning 51 Clustering 46
Kernel Methods 40 Optimization Algorithms 39 Feature Selection and
Dimensionality Reduction 33 Learning Theory 33 Graphical Models 33
Applications 29 Probabilistic Models 29 NN & Deep Learning 26 Transfer and
Multi-Task Learning 25 Online Learning 25 Activ</p><p>3 0.10341228 <a title="272-tfidf-3" href="../hunch_net-2009/hunch_net-2009-01-21-Nearly_all_natural_problems_require_nonlinearity.html">337 hunch net-2009-01-21-Nearly all natural problems require nonlinearity</a></p>
<p>Introduction: One conventional wisdom is that learning algorithms with linear
representations are sufficient to solve natural learning problems. This
conventional wisdom appears unsupported by empirical evidence as far as I can
tell. In nearly all vision, language, robotics, and speech applications I know
where machine learning is effectively applied, the approach involves either a
linear representation on hand crafted features capturing substantial
nonlinearities or learning directly on nonlinear representations.There are a
few exceptions to this--for example, if the problem of interest to you is
predicting the next word given previous words, n-gram methods have been shown
effective. Viewed the right way, n-gram methods are essentially linear
predictors on an enormous sparse feature space, learned from an enormous
number of examples. Hal's postheredescribes some of this in more detail.In
contrast, if you go to a machine learning conference, a large number of the
new algorithms are variations of lea</p><p>4 0.10134181 <a title="272-tfidf-4" href="../hunch_net-2009/hunch_net-2009-06-26-Netflix_nearly_done.html">362 hunch net-2009-06-26-Netflix nearly done</a></p>
<p>Introduction: A $1M qualifying result was achieved on thepublic Netflix test setby a3-way
ensemble team. This is just in time forYehuda's presentation atKDD, which I'm
sure will be one of the best attended ever.This isn't quite over--there are a
few days for another super-conglomerate team to come together and there is
some small chance that the performance is nonrepresentative of the final test
set, but I expect not.Regardless of the final outcome, the biggest lesson for
ML from the Netflix contest has been the formidable performance edge of
ensemble methods.</p><p>5 0.093498744 <a title="272-tfidf-5" href="../hunch_net-2009/hunch_net-2009-01-19-Netflix_prize_within_epsilon.html">336 hunch net-2009-01-19-Netflix prize within epsilon</a></p>
<p>Introduction: The competitors for theNetflix Prizeare tantalizingly close winning the
million dollar prize. This year,BellKorandCommendo Researchsent a combined
solution that won theprogress prize. Reading thewriteups2is instructive.
Several aspects of solutions are taken for granted including stochastic
gradient descent, ensemble prediction, and targeting residuals (a form of
boosting). Relatively to last year, it appears that many approaches have added
parameterizations, especially for the purpose of modeling through time.The big
question is: will they make the big prize? At this point, the level of
complexity in entering the competition is prohibitive, so perhaps only the
existing competitors will continue to try. (This equation might change
drastically if the teams open source their existing solutions, including
parameter settings.) One fear is that the progress is asymptoting on the wrong
side of the 10% threshold. In the first year, the teams progressed through
84.3% of the 10% gap, and in the</p><p>6 0.087604605 <a title="272-tfidf-6" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>7 0.077643916 <a title="272-tfidf-7" href="../hunch_net-2005/hunch_net-2005-11-16-The_Everything_Ensemble_Edge.html">131 hunch net-2005-11-16-The Everything Ensemble Edge</a></p>
<p>8 0.076208264 <a title="272-tfidf-8" href="../hunch_net-2005/hunch_net-2005-03-15-The_State_of_Tight_Bounds.html">41 hunch net-2005-03-15-The State of Tight Bounds</a></p>
<p>9 0.074098445 <a title="272-tfidf-9" href="../hunch_net-2007/hunch_net-2007-03-15-Alternative_Machine_Learning_Reductions_Definitions.html">236 hunch net-2007-03-15-Alternative Machine Learning Reductions Definitions</a></p>
<p>10 0.071689658 <a title="272-tfidf-10" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>11 0.070962027 <a title="272-tfidf-11" href="../hunch_net-2006/hunch_net-2006-07-13-Regression_vs._Classification_as_a_Primitive.html">196 hunch net-2006-07-13-Regression vs. Classification as a Primitive</a></p>
<p>12 0.070167869 <a title="272-tfidf-12" href="../hunch_net-2006/hunch_net-2006-07-09-The_Stock_Prediction_Machine_Learning_Problem.html">193 hunch net-2006-07-09-The Stock Prediction Machine Learning Problem</a></p>
<p>13 0.070058785 <a title="272-tfidf-13" href="../hunch_net-2009/hunch_net-2009-09-21-Netflix_finishes_%28and_starts%29.html">371 hunch net-2009-09-21-Netflix finishes (and starts)</a></p>
<p>14 0.06717512 <a title="272-tfidf-14" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>15 0.064410612 <a title="272-tfidf-15" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>16 0.063037887 <a title="272-tfidf-16" href="../hunch_net-2009/hunch_net-2009-04-02-Asymmophobia.html">348 hunch net-2009-04-02-Asymmophobia</a></p>
<p>17 0.062957011 <a title="272-tfidf-17" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>18 0.062171359 <a title="272-tfidf-18" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>19 0.061900873 <a title="272-tfidf-19" href="../hunch_net-2009/hunch_net-2009-03-18-Parallel_ML_primitives.html">346 hunch net-2009-03-18-Parallel ML primitives</a></p>
<p>20 0.061421599 <a title="272-tfidf-20" href="../hunch_net-2005/hunch_net-2005-06-28-The_cross_validation_problem%3A_cash_reward.html">86 hunch net-2005-06-28-The cross validation problem: cash reward</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.142), (1, -0.043), (2, 0.03), (3, 0.003), (4, 0.004), (5, 0.047), (6, -0.016), (7, 0.016), (8, 0.068), (9, -0.01), (10, -0.012), (11, -0.024), (12, -0.055), (13, -0.003), (14, -0.003), (15, 0.008), (16, -0.009), (17, 0.004), (18, -0.012), (19, 0.008), (20, 0.06), (21, -0.02), (22, 0.04), (23, 0.019), (24, 0.031), (25, 0.025), (26, 0.051), (27, 0.077), (28, -0.014), (29, 0.036), (30, -0.027), (31, -0.039), (32, -0.026), (33, 0.025), (34, 0.061), (35, -0.063), (36, -0.04), (37, 0.022), (38, -0.012), (39, 0.023), (40, 0.028), (41, -0.062), (42, 0.033), (43, 0.079), (44, 0.005), (45, -0.086), (46, 0.12), (47, 0.068), (48, 0.046), (49, -0.074)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96843719 <a title="272-lsi-1" href="../hunch_net-2007/hunch_net-2007-11-14-BellKor_wins_Netflix.html">272 hunch net-2007-11-14-BellKor wins Netflix</a></p>
<p>Introduction: â&euro;Ś but only the little prize. TheBellKor teamfocused on integrating predictions
from many different methods. The base methods consist of:Nearest Neighbor
MethodsMatrix Factorization Methods (asymmetric and symmetric)Linear
Regression on various feature spacesRestricted Boltzman MachinesThe final
predictor was an ensemble (as was reasonable to expect), although it's a
little bit more complicated than just a weighted average--it's essentially a
customized learning algorithm. Base approaches (1)-(3) seem like relatively
well-known approaches (although I haven't seen the asymmetric factorization
variant before). RBMs are the new approach.Thewriteupis pretty clear for more
details.The contestants are close to reaching the big prize, but the last 1.5%
is probably at least as hard as what's been done. A few new structurally
different methods for making predictions may need to be discovered and added
into the mixture. In other words, research may be required.</p><p>2 0.58795387 <a title="272-lsi-2" href="../hunch_net-2005/hunch_net-2005-10-08-We_have_a_winner.html">119 hunch net-2005-10-08-We have a winner</a></p>
<p>Introduction: The DARPA grandchallenge is a big contest for autonomous robot vehicle
driving. It was run once in 2004 for the first time and all teams did badly.
This year was notably different with theStanfordandCMUteams succesfully
completing the course. A number of details arehereandwikipedia has continuing
coverage.A formal winner hasn't been declared yet although Stanford completed
the course quickest.The Stanford and CMU teams deserve a large round of
applause as they have strongly demonstrated the feasibility of autonomous
vehicles.The good news for machine learning is that the Stanford team (at
least) is using some machine learning techniques.</p><p>3 0.56823224 <a title="272-lsi-3" href="../hunch_net-2009/hunch_net-2009-04-02-Asymmophobia.html">348 hunch net-2009-04-02-Asymmophobia</a></p>
<p>Introduction: One striking feature of many machine learning algorithms is the gymnastics
that designers go through to avoid symmetry breaking. In the most basic form
of machine learning, there are labeled examples composed of features. Each of
these can be treated symmetrically or asymmetrically by algorithms.feature
symmetryEvery feature is treated the same. In gradient update rules, the same
update is applied whether the feature is first or last. In metric-based
predictions, every feature is just as important in computing the
distance.example symmetryEvery example is treated the same. Batch learning
algorithms are great exemplars of this approach.label symmetryEvery label is
treated the same. This is particularly noticeable in multiclass classification
systems which predict according toarg maxlwlxbut it occurs in many other
places as well.Empirically, breaking symmetry well seems to yield great
algorithms.feature asymmetryFor those who like the "boosting is stepwise
additive regression on exponent</p><p>4 0.5586105 <a title="272-lsi-4" href="../hunch_net-2006/hunch_net-2006-07-09-The_Stock_Prediction_Machine_Learning_Problem.html">193 hunch net-2006-07-09-The Stock Prediction Machine Learning Problem</a></p>
<p>Introduction: …is discussed inthis nytimes article. I generally expect such approaches to
become more common since computers are getting faster, machine learning is
getting better, and data is becoming more plentiful. This is another example
where machine learning technology may have a huge economic impact. Some side
notes:We-in-research know almost nothing about how these things are done
(because it is typically a corporate secret).… but the limited discussion in
the article seem naive from a machine learning viewpoint.The learning process
used apparently often fails to take into account transaction costs.What little
of the approaches is discussed appears modeling based. It seems plausible that
more direct prediction methods can yield an edge.One difficulty with stock
picking as a research topic is that it is inherently a zero sum game (for
every winner, there is a loser). Much of the rest of research is positive sum
(basically, everyone wins).</p><p>5 0.54229528 <a title="272-lsi-5" href="../hunch_net-2007/hunch_net-2007-07-06-Idempotent-capable_Predictors.html">253 hunch net-2007-07-06-Idempotent-capable Predictors</a></p>
<p>Introduction: One way to distinguish different learning algorithms is by their ability or
inability to easily use an input variable as the predicted output. This is
desirable for at least two reasons:ModularityIf we want to build complex
learning systems via reuse of a subsystem, it's important to have compatible
I/O."Prior" knowledgeMachine learning is often applied in situations where we
do have some knowledge of what the right solution is, often in the form of an
existing system. In such situations, it's good to start with a learning
algorithm that can be at least as good as any existing system.When doing
classification, most learning algorithms can do this. For example, a decision
tree can split on a feature, and then classify. The real differences come up
when we attempt regression. Many of the algorithms we know and commonly use
are not idempotent predictors.Logistic regressors can not be idempotent,
because all input features are mapped through a nonlinearity.Linear regressors
can be idempote</p><p>6 0.52732331 <a title="272-lsi-6" href="../hunch_net-2011/hunch_net-2011-04-11-The_Heritage_Health_Prize.html">430 hunch net-2011-04-11-The Heritage Health Prize</a></p>
<p>7 0.52693135 <a title="272-lsi-7" href="../hunch_net-2006/hunch_net-2006-03-02-Why_do_people_count_for_learning%3F.html">160 hunch net-2006-03-02-Why do people count for learning?</a></p>
<p>8 0.5213114 <a title="272-lsi-8" href="../hunch_net-2009/hunch_net-2009-01-19-Netflix_prize_within_epsilon.html">336 hunch net-2009-01-19-Netflix prize within epsilon</a></p>
<p>9 0.51652837 <a title="272-lsi-9" href="../hunch_net-2012/hunch_net-2012-02-24-ICML%2B50%25.html">456 hunch net-2012-02-24-ICML+50%</a></p>
<p>10 0.5115003 <a title="272-lsi-10" href="../hunch_net-2006/hunch_net-2006-03-17-Multitask_learning_is_Black-Boxable.html">164 hunch net-2006-03-17-Multitask learning is Black-Boxable</a></p>
<p>11 0.48903108 <a title="272-lsi-11" href="../hunch_net-2007/hunch_net-2007-06-23-Machine_Learning_Jobs_are_Growing_on_Trees.html">250 hunch net-2007-06-23-Machine Learning Jobs are Growing on Trees</a></p>
<p>12 0.48873818 <a title="272-lsi-12" href="../hunch_net-2005/hunch_net-2005-12-22-Yes_%2C_I_am_applying.html">142 hunch net-2005-12-22-Yes , I am applying</a></p>
<p>13 0.48454729 <a title="272-lsi-13" href="../hunch_net-2007/hunch_net-2007-06-13-Not_Posting.html">246 hunch net-2007-06-13-Not Posting</a></p>
<p>14 0.47560713 <a title="272-lsi-14" href="../hunch_net-2006/hunch_net-2006-02-18-Multiplication_of_Learned_Probabilities_is_Dangerous.html">157 hunch net-2006-02-18-Multiplication of Learned Probabilities is Dangerous</a></p>
<p>15 0.4729439 <a title="272-lsi-15" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>16 0.47220463 <a title="272-lsi-16" href="../hunch_net-2009/hunch_net-2009-09-21-Netflix_finishes_%28and_starts%29.html">371 hunch net-2009-09-21-Netflix finishes (and starts)</a></p>
<p>17 0.47142935 <a title="272-lsi-17" href="../hunch_net-2006/hunch_net-2006-01-18-Is_Multitask_Learning_Black-Boxable%3F.html">149 hunch net-2006-01-18-Is Multitask Learning Black-Boxable?</a></p>
<p>18 0.46997461 <a title="272-lsi-18" href="../hunch_net-2006/hunch_net-2006-01-30-Should_the_Input_Representation_be_a_Vector%3F.html">152 hunch net-2006-01-30-Should the Input Representation be a Vector?</a></p>
<p>19 0.46345311 <a title="272-lsi-19" href="../hunch_net-2009/hunch_net-2009-06-26-Netflix_nearly_done.html">362 hunch net-2009-06-26-Netflix nearly done</a></p>
<p>20 0.45885769 <a title="272-lsi-20" href="../hunch_net-2005/hunch_net-2005-08-22-Do_you_believe_in_induction%3F.html">104 hunch net-2005-08-22-Do you believe in induction?</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.033), (14, 0.31), (42, 0.224), (45, 0.082), (68, 0.111), (74, 0.115)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.90073586 <a title="272-lda-1" href="../hunch_net-2005/hunch_net-2005-11-07-Prediction_Competitions.html">129 hunch net-2005-11-07-Prediction Competitions</a></p>
<p>Introduction: There are two prediction competitions currently in the air.ThePerformance
Prediction ChallengebyIsabelle Guyon. Good entries minimize a weighted 0/1
loss + the difference between a prediction of this loss and the observed truth
on 5 datasets. Isabelle tells me all of the problems are "real world" and the
test datasets are large enough (17K minimum) that the winner should be well
determined by ability rather than luck. This is due March 1.ThePredictive
Uncertainty ChallengebyGavin Cawley. Good entries minimize log loss on real
valued output variables for one synthetic and 3 "real" datasets related to
atmospheric prediction. The use of log loss (which can be infinite and hence
is never convergent) and smaller test sets of size 1K to 7K examples makes the
winner of this contest more luck dependent. Nevertheless, the contest may be
of some interest particularly to the branch of learning (typically Bayes
learning) which prefers to optimize log loss.May the best predictor win.</p><p>same-blog 2 0.89682013 <a title="272-lda-2" href="../hunch_net-2007/hunch_net-2007-11-14-BellKor_wins_Netflix.html">272 hunch net-2007-11-14-BellKor wins Netflix</a></p>
<p>Introduction: â&euro;Ś but only the little prize. TheBellKor teamfocused on integrating predictions
from many different methods. The base methods consist of:Nearest Neighbor
MethodsMatrix Factorization Methods (asymmetric and symmetric)Linear
Regression on various feature spacesRestricted Boltzman MachinesThe final
predictor was an ensemble (as was reasonable to expect), although it's a
little bit more complicated than just a weighted average--it's essentially a
customized learning algorithm. Base approaches (1)-(3) seem like relatively
well-known approaches (although I haven't seen the asymmetric factorization
variant before). RBMs are the new approach.Thewriteupis pretty clear for more
details.The contestants are close to reaching the big prize, but the last 1.5%
is probably at least as hard as what's been done. A few new structurally
different methods for making predictions may need to be discovered and added
into the mixture. In other words, research may be required.</p><p>3 0.8381961 <a title="272-lda-3" href="../hunch_net-2005/hunch_net-2005-09-20-Workshop_Proposal%3A_Atomic_Learning.html">114 hunch net-2005-09-20-Workshop Proposal: Atomic Learning</a></p>
<p>Introduction: This is a proposal for a workshop. It may or may not happen depending on the
level of interest. If you are interested, feel free to indicate so (by email
or comments).Description:Assume(*) that any system for solving large difficult
learning problems must decompose into repeated use of basic elements (i.e.
atoms). There are many basic questions which remain:What are the viable basic
elements?What makes a basic element viable?What are the viable principles for
the composition of these basic elements?What are the viable principles for
learning in such systems?What problems can this approach handle?Hal Daume
adds:Can composition of atoms be (semi-) automatically constructed[?]When
atoms are constructed through reductions, is there some notion of the
"naturalness" of the created leaning problems?Other than Markov
fields/graphical models/Bayes nets, is there a good language for representing
atoms and their compositions?The answer to these and related questions remain
unclear to me. A worksh</p><p>4 0.7601279 <a title="272-lda-4" href="../hunch_net-2007/hunch_net-2007-09-30-NIPS_workshops_are_out..html">264 hunch net-2007-09-30-NIPS workshops are out.</a></p>
<p>Introduction: Here. I'm particularly interested in theWeb Search,Efficient ML, and (of
course)Learning Problem Designworkshops but there are many others to check out
as well. Workshops are a great chance to make progress on or learn about a
topic. Relevance and interaction amongst diverse people can sometimes be
magical.</p><p>5 0.71851778 <a title="272-lda-5" href="../hunch_net-2008/hunch_net-2008-12-12-Summer_Conferences.html">331 hunch net-2008-12-12-Summer Conferences</a></p>
<p>Introduction: Here's a handy table for the summer conferences.ConferenceDeadlineReviewer
TargetingDouble BlindAuthor FeedbackLocationDateICML(wrong ICML)January
26YesYesYesMontreal, CanadaJune 14-17COLTFebruary 13NoNoYesMontrealJune
19-21UAIMarch 13NoYesNoMontrealJune 19-21KDDFebruary 2/6NoNoNoParis,
FranceJune 28-July 1Reviewer targeting is new this year. The idea is that many
poor decisions happen because the papers go to reviewers who are unqualified,
and the hope is that allowing authors to point out who is qualified results in
better decisions. In my experience, this is a reasonable idea to test.Both UAI
and COLT are experimenting this year as well with double blind and author
feedback, respectively. Of the two, I believe author feedback is more
important, as I've seen it make a difference. However, I still consider double
blind reviewing a net win, as it's a substantial public commitment to
fairness.</p><p>6 0.6522634 <a title="272-lda-6" href="../hunch_net-2005/hunch_net-2005-09-10-%26%238220%3BFailure%26%238221%3B_is_an_option.html">110 hunch net-2005-09-10-&#8220;Failure&#8221; is an option</a></p>
<p>7 0.64155412 <a title="272-lda-7" href="../hunch_net-2008/hunch_net-2008-04-12-It_Doesn%26%238217%3Bt_Stop.html">295 hunch net-2008-04-12-It Doesn&#8217;t Stop</a></p>
<p>8 0.63917398 <a title="272-lda-8" href="../hunch_net-2005/hunch_net-2005-08-11-Why_Manifold-Based_Dimension_Reduction_Techniques%3F.html">102 hunch net-2005-08-11-Why Manifold-Based Dimension Reduction Techniques?</a></p>
<p>9 0.63593853 <a title="272-lda-9" href="../hunch_net-2005/hunch_net-2005-06-08-Question%3A_%26%238220%3BWhen_is_the_right_time_to_insert_the_loss_function%3F%26%238221%3B.html">79 hunch net-2005-06-08-Question: &#8220;When is the right time to insert the loss function?&#8221;</a></p>
<p>10 0.63593698 <a title="272-lda-10" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>11 0.63558811 <a title="272-lda-11" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>12 0.63179535 <a title="272-lda-12" href="../hunch_net-2010/hunch_net-2010-12-26-NIPS_2010.html">420 hunch net-2010-12-26-NIPS 2010</a></p>
<p>13 0.63179326 <a title="272-lda-13" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>14 0.63147593 <a title="272-lda-14" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>15 0.63085747 <a title="272-lda-15" href="../hunch_net-2006/hunch_net-2006-01-30-Should_the_Input_Representation_be_a_Vector%3F.html">152 hunch net-2006-01-30-Should the Input Representation be a Vector?</a></p>
<p>16 0.63014811 <a title="272-lda-16" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>17 0.62974238 <a title="272-lda-17" href="../hunch_net-2011/hunch_net-2011-11-26-Giving_Thanks.html">449 hunch net-2011-11-26-Giving Thanks</a></p>
<p>18 0.62813717 <a title="272-lda-18" href="../hunch_net-2005/hunch_net-2005-12-07-Is_the_Google_way_the_way_for_machine_learning%3F.html">136 hunch net-2005-12-07-Is the Google way the way for machine learning?</a></p>
<p>19 0.62811345 <a title="272-lda-19" href="../hunch_net-2005/hunch_net-2005-08-18-SVM_Adaptability.html">103 hunch net-2005-08-18-SVM Adaptability</a></p>
<p>20 0.62806749 <a title="272-lda-20" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
