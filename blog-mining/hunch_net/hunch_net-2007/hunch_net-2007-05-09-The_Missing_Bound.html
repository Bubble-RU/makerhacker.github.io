<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>244 hunch net-2007-05-09-The Missing Bound</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2007" href="../home/hunch_net-2007_home.html">hunch_net-2007</a> <a title="hunch_net-2007-244" href="#">hunch_net-2007-244</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>244 hunch net-2007-05-09-The Missing Bound</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2007-244-html" href="http://hunch.net/?p=268">html</a></p><p>Introduction: Sham Kakadepoints out that we are missing a bound.Suppose we
havemsamplesxdrawn IID from some distributionD. Through the magic of
exponential moment method we know that:If the range ofxis bounded by an
interval of sizeI, aChernoff/Hoeffding style boundgives us a bound on the
deviations likeO(I/m0.5)(at least in crude form). A proof is on page 9here.If
the range ofxis bounded, and the variance (or a bound on the variance) is
known, thenBennett's boundcan give tighter results (*). This can be a huge
improvment when the true variance small.What's missing here is a bound that
depends on the observed variance rather than a bound on the variance. This
means that many people attempt to use Bennett's bound (incorrectly) by
plugging the observed variance in as the true variance, invalidating the bound
application. Most of the time, they get away with it, but this is a dangerous
move when doing machine learning. In machine learning, we are typically trying
to find a predictor with 0 expected los</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('variance', 0.528), ('bound', 0.46), ('bennett', 0.441), ('observed', 0.235), ('range', 0.176), ('ofxis', 0.147), ('bounded', 0.127), ('true', 0.11), ('plugging', 0.104), ('missing', 0.075), ('apply', 0.075), ('within', 0.066), ('incorrectly', 0.065), ('overconfident', 0.065), ('plug', 0.065), ('deviations', 0.065), ('invalidating', 0.065), ('magic', 0.065), ('dfor', 0.065), ('dm', 0.065)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="244-tfidf-1" href="../hunch_net-2007/hunch_net-2007-05-09-The_Missing_Bound.html">244 hunch net-2007-05-09-The Missing Bound</a></p>
<p>Introduction: Sham Kakadepoints out that we are missing a bound.Suppose we
havemsamplesxdrawn IID from some distributionD. Through the magic of
exponential moment method we know that:If the range ofxis bounded by an
interval of sizeI, aChernoff/Hoeffding style boundgives us a bound on the
deviations likeO(I/m0.5)(at least in crude form). A proof is on page 9here.If
the range ofxis bounded, and the variance (or a bound on the variance) is
known, thenBennett's boundcan give tighter results (*). This can be a huge
improvment when the true variance small.What's missing here is a bound that
depends on the observed variance rather than a bound on the variance. This
means that many people attempt to use Bennett's bound (incorrectly) by
plugging the observed variance in as the true variance, invalidating the bound
application. Most of the time, they get away with it, but this is a dangerous
move when doing machine learning. In machine learning, we are typically trying
to find a predictor with 0 expected los</p><p>2 0.19087954 <a title="244-tfidf-2" href="../hunch_net-2010/hunch_net-2010-03-26-A_Variance_only_Deviation_Bound.html">392 hunch net-2010-03-26-A Variance only Deviation Bound</a></p>
<p>Introduction: At thePAC-Bayesworkshop earlier this week,Olivier Catonidescribed a result
that I hadn't believed was possible:a deviation bound dependingonlyon the
variance of a random variable.For people not familiar with deviation bounds,
this may be hard to appreciate. Deviation bounds, are one of the core
components for the foundations of machine learning theory, so developments
here have a potential to alter our understanding of how to learn and what is
learnable. My understanding is that the basic proof techniques started
withBernsteinand have evolved into several variants specialized for various
applications. All of the variants I knew had a dependence on the range, with
some also having a dependence on the variance of an IID or martingale random
variable. This one is the first I know of with a dependence on only the
variance.The basic idea is to use a biased estimator of the mean which is not
influenced much by outliers. Then, a deviation bound can be proved by using
the exponential moment me</p><p>3 0.16053396 <a title="244-tfidf-3" href="../hunch_net-2007/hunch_net-2007-06-14-Interesting_Papers_at_COLT_2007.html">247 hunch net-2007-06-14-Interesting Papers at COLT 2007</a></p>
<p>Introduction: Here are two papers that seem particularly interesting at this year's
COLT.Gilles BlanchardandFranÃƒÂ§ois Fleuret,Occam's Hammer. When we are
interested in very tight bounds on the true error rate of a classifier, it is
tempting to use a PAC-Bayes bound which can (empirically) bequite tight. A
disadvantage of the PAC-Bayes bound is that it applies to a classifier which
is randomized over a set of base classifiers rather than a single classifier.
This paper shows that a similar bound can be proved which holds for a single
classifier drawn from the set. The ability to safely use a single classifier
is very nice. This technique applies generically to any base bound, so it has
other applications covered in the paper.Adam Tauman Kalai.Learning Nested
Halfspaces and Uphill Decision Trees. Classification PAC-learning, where you
prove that any problem amongst some set is polytime learnable with respect to
any distribution over the inputXis extraordinarily challenging as judged by
lack of progr</p><p>4 0.15229376 <a title="244-tfidf-4" href="../hunch_net-2005/hunch_net-2005-09-26-Prediction_Bounds_as_the_Mathematics_of_Science.html">115 hunch net-2005-09-26-Prediction Bounds as the Mathematics of Science</a></p>
<p>Introduction: "Science" has many meanings, but one common meaning is "thescientific method"
which is a principled method for investigating the world using the following
steps:Form a hypothesis about the world.Use the hypothesis to make
predictions.Run experiments to confirm or disprove the predictions.The
ordering of these steps is very important to the scientific method. In
particular, predictionsmustbe made before experiments are run.Given that we
all believe in the scientific method of investigation, it may be surprising to
learn that cheating is very common. This happens for many reasons, some
innocent and some not.Drug studies. Pharmaceutical companies make predictions
about the effects of their drugs and then conduct blind clinical studies to
determine their effect. Unfortunately, they have also been caught using some
of the more advanced techniques for cheatinghere: including "reprobleming",
"data set selection", and probably "overfitting by review". It isn't too
surprising to observe this: w</p><p>5 0.14462054 <a title="244-tfidf-5" href="../hunch_net-2005/hunch_net-2005-06-28-A_COLT_paper.html">85 hunch net-2005-06-28-A COLT paper</a></p>
<p>Introduction: I found Tong Zhang's paper onData Dependent Concentration Bounds for
Sequential Prediction Algorithmsinteresting. Roughly speaking, it states a
tight bound on the future error rate for online learning algorithms assuming
that samples are drawn independently. This bound is easily computed and will
make the progressive validation approaches usedheresignificantly more
practical.</p><p>6 0.13980648 <a title="244-tfidf-6" href="../hunch_net-2005/hunch_net-2005-04-10-Is_the_Goal_Understanding_or_Prediction%3F.html">55 hunch net-2005-04-10-Is the Goal Understanding or Prediction?</a></p>
<p>7 0.13980131 <a title="244-tfidf-7" href="../hunch_net-2005/hunch_net-2005-03-15-The_State_of_Tight_Bounds.html">41 hunch net-2005-03-15-The State of Tight Bounds</a></p>
<p>8 0.13656732 <a title="244-tfidf-8" href="../hunch_net-2006/hunch_net-2006-03-12-Online_learning_or_online_preservation_of_learning%3F.html">163 hunch net-2006-03-12-Online learning or online preservation of learning?</a></p>
<p>9 0.1170532 <a title="244-tfidf-9" href="../hunch_net-2009/hunch_net-2009-08-26-Another_10-year_paper_in_Machine_Learning.html">368 hunch net-2009-08-26-Another 10-year paper in Machine Learning</a></p>
<p>10 0.10800718 <a title="244-tfidf-10" href="../hunch_net-2005/hunch_net-2005-06-18-Lower_Bounds_for_Learning_Reductions.html">83 hunch net-2005-06-18-Lower Bounds for Learning Reductions</a></p>
<p>11 0.10657197 <a title="244-tfidf-11" href="../hunch_net-2006/hunch_net-2006-10-08-Incompatibilities_between_classical_confidence_intervals_and_learning..html">213 hunch net-2006-10-08-Incompatibilities between classical confidence intervals and learning.</a></p>
<p>12 0.091350138 <a title="244-tfidf-12" href="../hunch_net-2005/hunch_net-2005-04-21-Dynamic_Programming_Generalizations_and_Their_Use.html">58 hunch net-2005-04-21-Dynamic Programming Generalizations and Their Use</a></p>
<p>13 0.091172434 <a title="244-tfidf-13" href="../hunch_net-2006/hunch_net-2006-04-06-Bounds_greater_than_1.html">170 hunch net-2006-04-06-Bounds greater than 1</a></p>
<p>14 0.089661554 <a title="244-tfidf-14" href="../hunch_net-2009/hunch_net-2009-01-07-Interesting_Papers_at_SODA_2009.html">334 hunch net-2009-01-07-Interesting Papers at SODA 2009</a></p>
<p>15 0.0871085 <a title="244-tfidf-15" href="../hunch_net-2005/hunch_net-2005-05-16-Regret_minimizing_vs_error_limiting_reductions.html">72 hunch net-2005-05-16-Regret minimizing vs error limiting reductions</a></p>
<p>16 0.081457749 <a title="244-tfidf-16" href="../hunch_net-2008/hunch_net-2008-02-17-The_Meaning_of_Confidence.html">289 hunch net-2008-02-17-The Meaning of Confidence</a></p>
<p>17 0.065094255 <a title="244-tfidf-17" href="../hunch_net-2009/hunch_net-2009-12-27-Interesting_things_at_NIPS_2009.html">385 hunch net-2009-12-27-Interesting things at NIPS 2009</a></p>
<p>18 0.06451726 <a title="244-tfidf-18" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>19 0.064091519 <a title="244-tfidf-19" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>20 0.061941355 <a title="244-tfidf-20" href="../hunch_net-2007/hunch_net-2007-08-19-Choice_of_Metrics.html">259 hunch net-2007-08-19-Choice of Metrics</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.119), (1, -0.079), (2, -0.051), (3, 0.0), (4, -0.008), (5, -0.142), (6, 0.037), (7, -0.029), (8, -0.006), (9, -0.112), (10, -0.077), (11, 0.019), (12, -0.115), (13, -0.047), (14, -0.038), (15, 0.006), (16, -0.015), (17, 0.028), (18, 0.204), (19, 0.004), (20, 0.03), (21, -0.038), (22, -0.179), (23, -0.007), (24, -0.099), (25, 0.024), (26, -0.11), (27, -0.059), (28, 0.04), (29, 0.007), (30, 0.067), (31, -0.009), (32, 0.047), (33, 0.006), (34, 0.038), (35, -0.048), (36, -0.016), (37, 0.027), (38, -0.074), (39, 0.096), (40, -0.015), (41, -0.03), (42, 0.076), (43, 0.051), (44, 0.067), (45, -0.016), (46, 0.005), (47, 0.006), (48, 0.054), (49, 0.012)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98484176 <a title="244-lsi-1" href="../hunch_net-2007/hunch_net-2007-05-09-The_Missing_Bound.html">244 hunch net-2007-05-09-The Missing Bound</a></p>
<p>Introduction: Sham Kakadepoints out that we are missing a bound.Suppose we
havemsamplesxdrawn IID from some distributionD. Through the magic of
exponential moment method we know that:If the range ofxis bounded by an
interval of sizeI, aChernoff/Hoeffding style boundgives us a bound on the
deviations likeO(I/m0.5)(at least in crude form). A proof is on page 9here.If
the range ofxis bounded, and the variance (or a bound on the variance) is
known, thenBennett's boundcan give tighter results (*). This can be a huge
improvment when the true variance small.What's missing here is a bound that
depends on the observed variance rather than a bound on the variance. This
means that many people attempt to use Bennett's bound (incorrectly) by
plugging the observed variance in as the true variance, invalidating the bound
application. Most of the time, they get away with it, but this is a dangerous
move when doing machine learning. In machine learning, we are typically trying
to find a predictor with 0 expected los</p><p>2 0.77124512 <a title="244-lsi-2" href="../hunch_net-2005/hunch_net-2005-06-28-A_COLT_paper.html">85 hunch net-2005-06-28-A COLT paper</a></p>
<p>Introduction: I found Tong Zhang's paper onData Dependent Concentration Bounds for
Sequential Prediction Algorithmsinteresting. Roughly speaking, it states a
tight bound on the future error rate for online learning algorithms assuming
that samples are drawn independently. This bound is easily computed and will
make the progressive validation approaches usedheresignificantly more
practical.</p><p>3 0.73114127 <a title="244-lsi-3" href="../hunch_net-2005/hunch_net-2005-09-26-Prediction_Bounds_as_the_Mathematics_of_Science.html">115 hunch net-2005-09-26-Prediction Bounds as the Mathematics of Science</a></p>
<p>Introduction: "Science" has many meanings, but one common meaning is "thescientific method"
which is a principled method for investigating the world using the following
steps:Form a hypothesis about the world.Use the hypothesis to make
predictions.Run experiments to confirm or disprove the predictions.The
ordering of these steps is very important to the scientific method. In
particular, predictionsmustbe made before experiments are run.Given that we
all believe in the scientific method of investigation, it may be surprising to
learn that cheating is very common. This happens for many reasons, some
innocent and some not.Drug studies. Pharmaceutical companies make predictions
about the effects of their drugs and then conduct blind clinical studies to
determine their effect. Unfortunately, they have also been caught using some
of the more advanced techniques for cheatinghere: including "reprobleming",
"data set selection", and probably "overfitting by review". It isn't too
surprising to observe this: w</p><p>4 0.71937299 <a title="244-lsi-4" href="../hunch_net-2010/hunch_net-2010-03-26-A_Variance_only_Deviation_Bound.html">392 hunch net-2010-03-26-A Variance only Deviation Bound</a></p>
<p>Introduction: At thePAC-Bayesworkshop earlier this week,Olivier Catonidescribed a result
that I hadn't believed was possible:a deviation bound dependingonlyon the
variance of a random variable.For people not familiar with deviation bounds,
this may be hard to appreciate. Deviation bounds, are one of the core
components for the foundations of machine learning theory, so developments
here have a potential to alter our understanding of how to learn and what is
learnable. My understanding is that the basic proof techniques started
withBernsteinand have evolved into several variants specialized for various
applications. All of the variants I knew had a dependence on the range, with
some also having a dependence on the variance of an IID or martingale random
variable. This one is the first I know of with a dependence on only the
variance.The basic idea is to use a biased estimator of the mean which is not
influenced much by outliers. Then, a deviation bound can be proved by using
the exponential moment me</p><p>5 0.71797866 <a title="244-lsi-5" href="../hunch_net-2006/hunch_net-2006-04-06-Bounds_greater_than_1.html">170 hunch net-2006-04-06-Bounds greater than 1</a></p>
<p>Introduction: Nati SrebroandShai Ben-Davidhave apaperatCOLTwhich, in the appendix, proves
something very striking: several previous error bounds arealwaysgreater than
1.BackgroundOne branch of learning theory focuses on theorems whichAssume
samples are drawn IID from an unknown distributionD.Fix a set of
classifiersFind a high probability bound on the maximum true error rate (with
respect toD) as a function of the empirical error rate on the training
set.Many of these bounds become extremely complex and hairy.CurrentEveryone
working on this subject wants "tighter bounds", however there are different
definitions of "tighter". Some groups focus on "functional tightness" (getting
the right functional dependency between the size of the training set and a
parameterization of the hypothesis space) whileothersfocus on "practical
tightness" (finding bounds which work well on practical problems). (I am
definitely in the second camp.)One of the dangers of striving for "functional
tightness" is that the bound</p><p>6 0.65389854 <a title="244-lsi-6" href="../hunch_net-2005/hunch_net-2005-03-15-The_State_of_Tight_Bounds.html">41 hunch net-2005-03-15-The State of Tight Bounds</a></p>
<p>7 0.60967076 <a title="244-lsi-7" href="../hunch_net-2007/hunch_net-2007-06-14-Interesting_Papers_at_COLT_2007.html">247 hunch net-2007-06-14-Interesting Papers at COLT 2007</a></p>
<p>8 0.59676987 <a title="244-lsi-8" href="../hunch_net-2005/hunch_net-2005-06-18-Lower_Bounds_for_Learning_Reductions.html">83 hunch net-2005-06-18-Lower Bounds for Learning Reductions</a></p>
<p>9 0.56745869 <a title="244-lsi-9" href="../hunch_net-2009/hunch_net-2009-08-26-Another_10-year_paper_in_Machine_Learning.html">368 hunch net-2009-08-26-Another 10-year paper in Machine Learning</a></p>
<p>10 0.53091425 <a title="244-lsi-10" href="../hunch_net-2005/hunch_net-2005-04-21-Dynamic_Programming_Generalizations_and_Their_Use.html">58 hunch net-2005-04-21-Dynamic Programming Generalizations and Their Use</a></p>
<p>11 0.49225703 <a title="244-lsi-11" href="../hunch_net-2006/hunch_net-2006-03-12-Online_learning_or_online_preservation_of_learning%3F.html">163 hunch net-2006-03-12-Online learning or online preservation of learning?</a></p>
<p>12 0.4616439 <a title="244-lsi-12" href="../hunch_net-2005/hunch_net-2005-04-10-Is_the_Goal_Understanding_or_Prediction%3F.html">55 hunch net-2005-04-10-Is the Goal Understanding or Prediction?</a></p>
<p>13 0.41275817 <a title="244-lsi-13" href="../hunch_net-2006/hunch_net-2006-07-06-Branch_Prediction_Competition.html">190 hunch net-2006-07-06-Branch Prediction Competition</a></p>
<p>14 0.41201085 <a title="244-lsi-14" href="../hunch_net-2006/hunch_net-2006-10-08-Incompatibilities_between_classical_confidence_intervals_and_learning..html">213 hunch net-2006-10-08-Incompatibilities between classical confidence intervals and learning.</a></p>
<p>15 0.37311 <a title="244-lsi-15" href="../hunch_net-2007/hunch_net-2007-02-02-Thoughts_regarding_%26%238220%3BIs_machine_learning_different_from_statistics%3F%26%238221%3B.html">230 hunch net-2007-02-02-Thoughts regarding &#8220;Is machine learning different from statistics?&#8221;</a></p>
<p>16 0.34897512 <a title="244-lsi-16" href="../hunch_net-2008/hunch_net-2008-02-17-The_Meaning_of_Confidence.html">289 hunch net-2008-02-17-The Meaning of Confidence</a></p>
<p>17 0.34518674 <a title="244-lsi-17" href="../hunch_net-2008/hunch_net-2008-09-12-How_do_we_get_weak_action_dependence_for_learning_with_partial_observations%3F.html">317 hunch net-2008-09-12-How do we get weak action dependence for learning with partial observations?</a></p>
<p>18 0.3306683 <a title="244-lsi-18" href="../hunch_net-2005/hunch_net-2005-12-09-Some_NIPS_papers.html">138 hunch net-2005-12-09-Some NIPS papers</a></p>
<p>19 0.32830313 <a title="244-lsi-19" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>20 0.32803428 <a title="244-lsi-20" href="../hunch_net-2005/hunch_net-2005-02-28-Regularization.html">33 hunch net-2005-02-28-Regularization</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(16, 0.014), (35, 0.039), (42, 0.242), (45, 0.052), (69, 0.018), (74, 0.066), (90, 0.016), (91, 0.346), (95, 0.058), (98, 0.015)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.95479113 <a title="244-lda-1" href="../hunch_net-2010/hunch_net-2010-08-21-Rob_Schapire_at_NYC_ML_Meetup.html">405 hunch net-2010-08-21-Rob Schapire at NYC ML Meetup</a></p>
<p>Introduction: I've been wanting to attend theNYC ML Meetupfor some time and hope to make
itnext week on the 25th.Rob Schapireis talking about "Playing Repeated Games",
which in my experience is far more relevant to machine learning than the title
might indicate.</p><p>2 0.90474927 <a title="244-lda-2" href="../hunch_net-2007/hunch_net-2007-11-05-CMU_wins_DARPA_Urban_Challenge.html">271 hunch net-2007-11-05-CMU wins DARPA Urban Challenge</a></p>
<p>Introduction: Theresults have been posted, withCMU first,Stanford second, andVirginia Tech
Third.Considering that this was an open event (at least for people in the US),
this was a very strong showing for research at universities (instead of
defense contractors, for example). Some details should become public at
theNIPS workshops.Slashdothas apostwith many comments.</p><p>same-blog 3 0.89012027 <a title="244-lda-3" href="../hunch_net-2007/hunch_net-2007-05-09-The_Missing_Bound.html">244 hunch net-2007-05-09-The Missing Bound</a></p>
<p>Introduction: Sham Kakadepoints out that we are missing a bound.Suppose we
havemsamplesxdrawn IID from some distributionD. Through the magic of
exponential moment method we know that:If the range ofxis bounded by an
interval of sizeI, aChernoff/Hoeffding style boundgives us a bound on the
deviations likeO(I/m0.5)(at least in crude form). A proof is on page 9here.If
the range ofxis bounded, and the variance (or a bound on the variance) is
known, thenBennett's boundcan give tighter results (*). This can be a huge
improvment when the true variance small.What's missing here is a bound that
depends on the observed variance rather than a bound on the variance. This
means that many people attempt to use Bennett's bound (incorrectly) by
plugging the observed variance in as the true variance, invalidating the bound
application. Most of the time, they get away with it, but this is a dangerous
move when doing machine learning. In machine learning, we are typically trying
to find a predictor with 0 expected los</p><p>4 0.8482399 <a title="244-lda-4" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>Introduction: This post is about a general technique for problem solving which I've never
seen taught (in full generality), but which I've found very useful.Many
problems in computer science turn out to be discretely difficult. The best
known version of such problems are NP-hard problems, but I mean 'discretely
difficult' in a much more general way, which I only know how to capture by
examples.ERMIn empirical risk minimization, you choose a minimum error rate
classifier from a set of classifiers. This is NP hard for common sets, but it
can be much harder, depending on the set.ExpertsIn the online learning with
experts setting, you try to predict well so as to compete with a set of
(adversarial) experts. Here the alternating quantifiers of you and an
adversary playing out a game can yield a dynamic programming problem that
grows exponentially.Policy IterationThe problem with policy iteration is that
you learn a new policy with respect to an old policy, which implies that
simply adopting the new polic</p><p>5 0.82104784 <a title="244-lda-5" href="../hunch_net-2009/hunch_net-2009-01-07-Interesting_Papers_at_SODA_2009.html">334 hunch net-2009-01-07-Interesting Papers at SODA 2009</a></p>
<p>Introduction: Several talks seem potentially interesting to ML folks at this year's SODA
.Maria-Florina Balcan,Avrim Blum, andAnupam Gupta,Approximate Clustering
without the Approximation. This paper gives reasonable algorithms with
provable approximation guarantees for k-median and other notions of
clustering. It's conceptually interesting, because it's the second example
I've seen where NP hardness is subverted by changing the problem definition
subtle but reasonable way. Essentially, they show that if any near-
approximation to an optimal solution is good, then it's computationally easy
to find a near-optimal solution. This subtle shift bears serious thought. A
similar one occurred inour ranking paperwith respect to minimum feedback
arcset. With two known examples, it suggests that many more NP-complete
problems might be finessed into irrelevance in this style.Yury
LifshitsandShengyu Zhang,Combinatorial Algorithms for Nearest Neighbors, Near-
Duplicates, and Small-World Design. The basic idea of</p><p>6 0.79120499 <a title="244-lda-6" href="../hunch_net-2008/hunch_net-2008-03-07-Spock_Challenge_Winners.html">291 hunch net-2008-03-07-Spock Challenge Winners</a></p>
<p>7 0.7621693 <a title="244-lda-7" href="../hunch_net-2006/hunch_net-2006-01-18-Is_Multitask_Learning_Black-Boxable%3F.html">149 hunch net-2006-01-18-Is Multitask Learning Black-Boxable?</a></p>
<p>8 0.7511394 <a title="244-lda-8" href="../hunch_net-2007/hunch_net-2007-12-21-Vowpal_Wabbit_Code_Release.html">281 hunch net-2007-12-21-Vowpal Wabbit Code Release</a></p>
<p>9 0.70275307 <a title="244-lda-9" href="../hunch_net-2005/hunch_net-2005-02-23-Problem%3A_Reinforcement_Learning_with_Classification.html">27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</a></p>
<p>10 0.65163571 <a title="244-lda-10" href="../hunch_net-2011/hunch_net-2011-10-24-2011_ML_symposium_and_the_bears.html">448 hunch net-2011-10-24-2011 ML symposium and the bears</a></p>
<p>11 0.61245519 <a title="244-lda-11" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>12 0.61192745 <a title="244-lda-12" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>13 0.60391033 <a title="244-lda-13" href="../hunch_net-2005/hunch_net-2005-04-10-Is_the_Goal_Understanding_or_Prediction%3F.html">55 hunch net-2005-04-10-Is the Goal Understanding or Prediction?</a></p>
<p>14 0.60271806 <a title="244-lda-14" href="../hunch_net-2009/hunch_net-2009-12-27-Interesting_things_at_NIPS_2009.html">385 hunch net-2009-12-27-Interesting things at NIPS 2009</a></p>
<p>15 0.60057569 <a title="244-lda-15" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>16 0.59879571 <a title="244-lda-16" href="../hunch_net-2008/hunch_net-2008-04-30-Concerns_about_the_Large_Scale_Learning_Challenge.html">300 hunch net-2008-04-30-Concerns about the Large Scale Learning Challenge</a></p>
<p>17 0.59718645 <a title="244-lda-17" href="../hunch_net-2010/hunch_net-2010-03-26-A_Variance_only_Deviation_Bound.html">392 hunch net-2010-03-26-A Variance only Deviation Bound</a></p>
<p>18 0.59595311 <a title="244-lda-18" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>19 0.59483176 <a title="244-lda-19" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>20 0.59340715 <a title="244-lda-20" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
