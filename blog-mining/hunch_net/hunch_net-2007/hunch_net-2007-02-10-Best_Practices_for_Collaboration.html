<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>231 hunch net-2007-02-10-Best Practices for Collaboration</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2007" href="../home/hunch_net-2007_home.html">hunch_net-2007</a> <a title="hunch_net-2007-231" href="#">hunch_net-2007-231</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>231 hunch net-2007-02-10-Best Practices for Collaboration</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2007-231-html" href="http://hunch.net/?p=251">html</a></p><p>Introduction: Many people, especially students, haven't had an opportunity to collaborate
with other researchers. Collaboration, especially with remote people can be
tricky. Here are some observations of what has worked for me on collaborations
involving a few people.Travel and DiscussAlmost all collaborations start with
in-person discussion. This implies that travel is often necessary. We can hope
that in the future we'll have better systems for starting collaborations
remotely (such as blogs), but we aren't quite there yet.Enable your
collaborator. A collaboration can fall apart because one collaborator disables
another. This sounds stupid (and it is), but it's far easier than you might
think.Avoid Duplication. Discovering that you and a collaborator have been
editing the same thing and now need to waste time reconciling changes is
annoying. The best way to avoid this to be explicit about who has write
permission to what. Most of the time, a write lock is held for the entire
document, just to be s</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('collaborator', 0.377), ('lock', 0.339), ('collaborators', 0.254), ('write', 0.223), ('collaborations', 0.188), ('draft', 0.175), ('coauthor', 0.17), ('collaboration', 0.132), ('prevents', 0.132), ('senior', 0.132), ('bandwidth', 0.126), ('passed', 0.126), ('ordering', 0.11), ('version', 0.102), ('helped', 0.097), ('default', 0.096), ('control', 0.094), ('produce', 0.084), ('naturally', 0.077), ('especially', 0.076)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="231-tfidf-1" href="../hunch_net-2007/hunch_net-2007-02-10-Best_Practices_for_Collaboration.html">231 hunch net-2007-02-10-Best Practices for Collaboration</a></p>
<p>Introduction: Many people, especially students, haven't had an opportunity to collaborate
with other researchers. Collaboration, especially with remote people can be
tricky. Here are some observations of what has worked for me on collaborations
involving a few people.Travel and DiscussAlmost all collaborations start with
in-person discussion. This implies that travel is often necessary. We can hope
that in the future we'll have better systems for starting collaborations
remotely (such as blogs), but we aren't quite there yet.Enable your
collaborator. A collaboration can fall apart because one collaborator disables
another. This sounds stupid (and it is), but it's far easier than you might
think.Avoid Duplication. Discovering that you and a collaborator have been
editing the same thing and now need to waste time reconciling changes is
annoying. The best way to avoid this to be explicit about who has write
permission to what. Most of the time, a write lock is held for the entire
document, just to be s</p><p>2 0.087323785 <a title="231-tfidf-2" href="../hunch_net-2010/hunch_net-2010-10-17-Partha_Niyogi_has_died.html">414 hunch net-2010-10-17-Partha Niyogi has died</a></p>
<p>Introduction: from brain cancer. I askedMishawho worked with him to write about it.Partha
Niyogi, Louis Block Professor in Computer Science and Statistics at the
University of Chicago passed away on October 1, 2010, aged 43.I first met
Partha Niyogi almost exactly ten years ago when I was a graduate student in
math and he had just started as a faculty in Computer Science and Statistics
at the University of Chicago. Strangely, we first talked at length due to a
somewhat convoluted mathematical argument in a paper on pattern recognition. I
asked him some questions about the paper, and, even though the topic was new
to him, he had put serious thought into it and we started regular meetings. We
made significant progress and developed a line of research stemming initially
just from trying to understand that one paper and to simplify one derivation.
I think this was typical of Partha, showing both his intellectual curiosity
and his intuition for the serendipitous; having a sense and focus for
inquiries wo</p><p>3 0.083321378 <a title="231-tfidf-3" href="../hunch_net-2008/hunch_net-2008-04-21-The_Science_2.0_article.html">296 hunch net-2008-04-21-The Science 2.0 article</a></p>
<p>Introduction: I found the article aboutscience using modern tools interesting, especially
the part about 'blogophobia', which in my experience is often a substantial
issue: many potential guest posters aren't quite ready, because of the fear of
a permanent public mistake, because it is particularly hard to write about the
unknown (the essence of research), and because the system for public credit
doesn't yet really handle blog posts.So far, science has been relatively
resistant to discussing research on blogs. Some things need to change to get
there. Public tolerance of the occasional mistake is essential, as is a
willingness to cite (and credit) blogs as freely as papers.I've often run into
another reason for holding back myself: I don't want to overtalk my own
research. Nevertheless, I'm slowly changing to the opinion that I'm holding
back too much: the real power of a blog in research is that it can be used to
confer with many people, and that just makes research work better.</p><p>4 0.081285 <a title="231-tfidf-4" href="../hunch_net-2008/hunch_net-2008-03-15-COLT_Open_Problems.html">292 hunch net-2008-03-15-COLT Open Problems</a></p>
<p>Introduction: COLT has acall for open problemsdue March 21. I encourage anyone with a
specifiable open problem to write it down and send it in. Just the effort of
specifying an open problem precisely and concisely has been very helpful for
my own solutions, and there is a substantial chance others will solve it. To
increase the chance someone will take it up, you can even put a bounty on the
solution. (Perhaps I should raise the$500 bountyon theK-fold cross-validation
problemas it hasn't yet been solved).</p><p>5 0.080017194 <a title="231-tfidf-5" href="../hunch_net-2012/hunch_net-2012-04-09-ICML_author_feedback_is_open.html">461 hunch net-2012-04-09-ICML author feedback is open</a></p>
<p>Introduction: as of last night, late.When the reviewing deadline passed Wednesday night 15%
of reviews were still missing, much higher than I expected. Between late
reviews coming in, ACs working overtime through the weekend, and people
willing to help in the pinch another ~390 reviews came in, reducing the
missing mass to 0.2%. Nailing that last bit and a similar quantity of papers
with uniformly low confidence reviews is what remains to be done in terms of
basic reviews. We are trying to make all of those happen this week so authors
have some chance to respond.I was surprised by the quantity of late reviews,
and I think that's an area where ICML needs to improve in future years. Good
reviews are not done in a rush--they are done by setting aside time (like an
afternoon), and carefully reading the paper while thinking about implications.
Many reviewers do this well but a significant minority aren't good at
scheduling their personal time. In this situation there are several ways to
fail:Give early w</p><p>6 0.076322407 <a title="231-tfidf-6" href="../hunch_net-2006/hunch_net-2006-09-09-How_to_solve_an_NP_hard_problem_in_quadratic_time.html">206 hunch net-2006-09-09-How to solve an NP hard problem in quadratic time</a></p>
<p>7 0.075494699 <a title="231-tfidf-7" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>8 0.073961526 <a title="231-tfidf-8" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>9 0.073780455 <a title="231-tfidf-9" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>10 0.07152544 <a title="231-tfidf-10" href="../hunch_net-2006/hunch_net-2006-09-18-What_is_missing_for_online_collaborative_research%3F.html">208 hunch net-2006-09-18-What is missing for online collaborative research?</a></p>
<p>11 0.070973545 <a title="231-tfidf-11" href="../hunch_net-2005/hunch_net-2005-06-06-Exact_Online_Learning_for_Classification.html">78 hunch net-2005-06-06-Exact Online Learning for Classification</a></p>
<p>12 0.070969991 <a title="231-tfidf-12" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>13 0.069310255 <a title="231-tfidf-13" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>14 0.069081604 <a title="231-tfidf-14" href="../hunch_net-2007/hunch_net-2007-01-02-Retrospective.html">225 hunch net-2007-01-02-Retrospective</a></p>
<p>15 0.067960396 <a title="231-tfidf-15" href="../hunch_net-2005/hunch_net-2005-02-25-Why_Papers%3F.html">30 hunch net-2005-02-25-Why Papers?</a></p>
<p>16 0.06657093 <a title="231-tfidf-16" href="../hunch_net-2009/hunch_net-2009-11-23-ICML_2009_Workshops_%28and_Tutorials%29.html">379 hunch net-2009-11-23-ICML 2009 Workshops (and Tutorials)</a></p>
<p>17 0.065162078 <a title="231-tfidf-17" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>18 0.063790813 <a title="231-tfidf-18" href="../hunch_net-2008/hunch_net-2008-11-26-Efficient_Reinforcement_Learning_in_MDPs.html">328 hunch net-2008-11-26-Efficient Reinforcement Learning in MDPs</a></p>
<p>19 0.063529596 <a title="231-tfidf-19" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>20 0.063491069 <a title="231-tfidf-20" href="../hunch_net-2005/hunch_net-2005-07-10-Thinking_the_Unthought.html">91 hunch net-2005-07-10-Thinking the Unthought</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.163), (1, 0.055), (2, 0.043), (3, -0.022), (4, 0.036), (5, -0.012), (6, 0.02), (7, -0.016), (8, -0.003), (9, 0.039), (10, -0.031), (11, -0.001), (12, -0.003), (13, 0.02), (14, -0.051), (15, 0.03), (16, 0.0), (17, -0.01), (18, -0.017), (19, -0.034), (20, 0.021), (21, 0.086), (22, -0.024), (23, -0.035), (24, 0.059), (25, 0.006), (26, -0.04), (27, 0.009), (28, -0.027), (29, 0.022), (30, 0.03), (31, -0.015), (32, -0.005), (33, -0.005), (34, 0.027), (35, -0.054), (36, 0.066), (37, 0.043), (38, 0.0), (39, -0.059), (40, -0.063), (41, -0.029), (42, 0.006), (43, -0.071), (44, -0.109), (45, 0.042), (46, -0.035), (47, 0.032), (48, -0.01), (49, 0.037)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95312983 <a title="231-lsi-1" href="../hunch_net-2007/hunch_net-2007-02-10-Best_Practices_for_Collaboration.html">231 hunch net-2007-02-10-Best Practices for Collaboration</a></p>
<p>Introduction: Many people, especially students, haven't had an opportunity to collaborate
with other researchers. Collaboration, especially with remote people can be
tricky. Here are some observations of what has worked for me on collaborations
involving a few people.Travel and DiscussAlmost all collaborations start with
in-person discussion. This implies that travel is often necessary. We can hope
that in the future we'll have better systems for starting collaborations
remotely (such as blogs), but we aren't quite there yet.Enable your
collaborator. A collaboration can fall apart because one collaborator disables
another. This sounds stupid (and it is), but it's far easier than you might
think.Avoid Duplication. Discovering that you and a collaborator have been
editing the same thing and now need to waste time reconciling changes is
annoying. The best way to avoid this to be explicit about who has write
permission to what. Most of the time, a write lock is held for the entire
document, just to be s</p><p>2 0.67325062 <a title="231-lsi-2" href="../hunch_net-2005/hunch_net-2005-07-10-Thinking_the_Unthought.html">91 hunch net-2005-07-10-Thinking the Unthought</a></p>
<p>Introduction: One thing common to much research is that the researcher must be the first
personeverto have some thought. How do you think of something that has never
been thought of? There seems to be no methodical manner of doing this, but
there are some tricks.The easiest method is to just have some connection come
to you. There is a trick here however: you should write it down and fill out
the idea immediately because it can just as easily go away.A harder method is
to set aside a block of time and simply think about an idea. Distraction
elimination is essential here because thinking about the unthought is hard
work which your mind will avoid.Another common method is in conversation.
Sometimes the process of verbalizing implies new ideas come up and sometimes
whoever you are talking to replies just the right way. This method is
dangerous though--you must speak to someone who helps you think rather than
someone who occupies your thoughts.Try to rephrase the problem so the answer
is simple. This is</p><p>3 0.65124369 <a title="231-lsi-3" href="../hunch_net-2013/hunch_net-2013-11-21-Ben_Taskar_is_gone.html">491 hunch net-2013-11-21-Ben Taskar is gone</a></p>
<p>Introduction: I was not as personally close toBenasSam, but the level of tragedy is similar
and I can't help but be greatly saddened by the loss.Variousnewsstorieshave
coverage, but the synopsis is that he had a heart attack on Sunday and is
survived by his wife Anat and daughter Aviv. There is discussion of creating a
memorial fund for them, which I hope comes to fruition, and plan to contribute
to.I will remember Ben as someone who thought carefully and comprehensively
about new ways to do things, then fought hard and successfully for what he
believed in. It is an ideal we strive for, that Ben accomplished.Edit:
donationsgo here, and more information ishere.</p><p>4 0.61105484 <a title="231-lsi-4" href="../hunch_net-2007/hunch_net-2007-06-21-Presentation_Preparation.html">249 hunch net-2007-06-21-Presentation Preparation</a></p>
<p>Introduction: A big part of doing research is presenting it at a conference. Since many
people start out shy of public presentations, this can be a substantial
challenge. Here are a few notes which might be helpful when thinking about
preparing a presentation on research.Motivate. Talks which don't start by
describing the problem to solve cause many people to zone out.Prioritize. It
is typical that you have more things to say than time to say them, and many
presenters fall into the failure mode of trying to say too much. This is an
easy-to-understand failure mode as it's very natural to want to include
everything. A basic fact is: you can't. Example of this are:Your slides are so
densely full of equations and words that you can't cover them.Your talk runs
over and a moderator prioritizes for you by cutting you off.You motor-mouth
through the presentation, and the information absorption rate of the audience
prioritizes in some uncontrolled fashion.The rate of flow of concepts simply
exceeds the infor</p><p>5 0.59578383 <a title="231-lsi-5" href="../hunch_net-2009/hunch_net-2009-06-01-Multitask_Poisoning.html">358 hunch net-2009-06-01-Multitask Poisoning</a></p>
<p>Introduction: There are many ways that interesting research gets done. For example it's
common at a conference for someone to discuss a problem with a partial
solution, and for someone else to know how to solve a piece of it, resulting
in a paper. In some sense, these are the easiest results we can achieve, so we
should ask: Can all research be this easy?The answer is certainly no for
fields where research inherently requires experimentation to discover how the
real world works. However, mathematics, including parts of physics, computer
science, statistics, etcâ&euro;Ś which are effectively mathematics don't require
experimentation. In effect, a paper can be simply a pure expression of
thinking. Can all mathematical-style research be this easy?What's going on
here is research-by-communication. Someone knows something, someone knows
something else, and as soon as someone knows both things, a problem is solved.
The interesting thing about research-by-communication is that it is becoming
radically easier with</p><p>6 0.58099198 <a title="231-lsi-6" href="../hunch_net-2005/hunch_net-2005-02-25-Why_Papers%3F.html">30 hunch net-2005-02-25-Why Papers?</a></p>
<p>7 0.56346536 <a title="231-lsi-7" href="../hunch_net-2006/hunch_net-2006-12-05-Recruitment_Conferences.html">222 hunch net-2006-12-05-Recruitment Conferences</a></p>
<p>8 0.54533291 <a title="231-lsi-8" href="../hunch_net-2011/hunch_net-2011-03-27-Vowpal_Wabbit%2C_v5.1.html">428 hunch net-2011-03-27-Vowpal Wabbit, v5.1</a></p>
<p>9 0.54257619 <a title="231-lsi-9" href="../hunch_net-2007/hunch_net-2007-07-20-Motivation_should_be_the_Responsibility_of_the_Reviewer.html">256 hunch net-2007-07-20-Motivation should be the Responsibility of the Reviewer</a></p>
<p>10 0.54153031 <a title="231-lsi-10" href="../hunch_net-2008/hunch_net-2008-02-10-Complexity_Illness.html">288 hunch net-2008-02-10-Complexity Illness</a></p>
<p>11 0.54116309 <a title="231-lsi-11" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>12 0.53838408 <a title="231-lsi-12" href="../hunch_net-2007/hunch_net-2007-02-16-The_Forgetting.html">233 hunch net-2007-02-16-The Forgetting</a></p>
<p>13 0.53275025 <a title="231-lsi-13" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>14 0.52132416 <a title="231-lsi-14" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>15 0.51611775 <a title="231-lsi-15" href="../hunch_net-2006/hunch_net-2006-01-25-1_year.html">151 hunch net-2006-01-25-1 year</a></p>
<p>16 0.5105567 <a title="231-lsi-16" href="../hunch_net-2011/hunch_net-2011-04-06-COLT_open_questions.html">429 hunch net-2011-04-06-COLT open questions</a></p>
<p>17 0.50987637 <a title="231-lsi-17" href="../hunch_net-2006/hunch_net-2006-03-09-Use_of_Notation.html">162 hunch net-2006-03-09-Use of Notation</a></p>
<p>18 0.50727051 <a title="231-lsi-18" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<p>19 0.5070985 <a title="231-lsi-19" href="../hunch_net-2006/hunch_net-2006-01-06-MLTV.html">146 hunch net-2006-01-06-MLTV</a></p>
<p>20 0.50467986 <a title="231-lsi-20" href="../hunch_net-2008/hunch_net-2008-07-04-More_Presentation_Preparation.html">307 hunch net-2008-07-04-More Presentation Preparation</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(6, 0.014), (24, 0.372), (35, 0.044), (42, 0.176), (47, 0.022), (68, 0.022), (69, 0.026), (74, 0.122), (83, 0.024), (95, 0.09)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.88243824 <a title="231-lda-1" href="../hunch_net-2005/hunch_net-2005-09-06-A_link.html">108 hunch net-2005-09-06-A link</a></p>
<p>Introduction: I read through some of the essays ofMichael Nielsentoday, and recommend
them.Principles of Effective ResearchandExtreme Thinkingare both relevant to
several discussions here.</p><p>same-blog 2 0.82281232 <a title="231-lda-2" href="../hunch_net-2007/hunch_net-2007-02-10-Best_Practices_for_Collaboration.html">231 hunch net-2007-02-10-Best Practices for Collaboration</a></p>
<p>Introduction: Many people, especially students, haven't had an opportunity to collaborate
with other researchers. Collaboration, especially with remote people can be
tricky. Here are some observations of what has worked for me on collaborations
involving a few people.Travel and DiscussAlmost all collaborations start with
in-person discussion. This implies that travel is often necessary. We can hope
that in the future we'll have better systems for starting collaborations
remotely (such as blogs), but we aren't quite there yet.Enable your
collaborator. A collaboration can fall apart because one collaborator disables
another. This sounds stupid (and it is), but it's far easier than you might
think.Avoid Duplication. Discovering that you and a collaborator have been
editing the same thing and now need to waste time reconciling changes is
annoying. The best way to avoid this to be explicit about who has write
permission to what. Most of the time, a write lock is held for the entire
document, just to be s</p><p>3 0.55425406 <a title="231-lda-3" href="../hunch_net-2005/hunch_net-2005-06-08-Question%3A_%26%238220%3BWhen_is_the_right_time_to_insert_the_loss_function%3F%26%238221%3B.html">79 hunch net-2005-06-08-Question: &#8220;When is the right time to insert the loss function?&#8221;</a></p>
<p>Introduction: Halasksa very good question: "When is the right time to insert the loss
function?" In particular, should it be used at testing time or at training
time?When the world imposes a loss on us, the standard Bayesian recipe is to
predict the (conditional) probability of each possibility and then choose the
possibility which minimizes the expected loss. In contrast, as
theconfusionover "loss = money lost" or "loss = the thing you optimize" might
indicate, many people ignore the Bayesian approach and simply optimize their
loss (or a close proxy for their loss) over the representation on the training
set.The best answer I can give is "it's unclear, but I prefer optimizing the
loss at training time". My experience is that optimizing the loss in the most
direct manner possible typically yields best performance. This question is
related to a basic principle which bothYann LeCun(applied) andVladimir
Vapnik(theoretical) advocate: "solve the simplest prediction problem that
solves the problem". (One</p><p>4 0.52186632 <a title="231-lda-4" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>Introduction: Here's a quick reference for summer ML-related conferences sorted by due
date:ConferenceDue dateLocationReviewingKDDFeb 10August 12-16, Beijing,
ChinaSingle BlindCOLTFeb 14June 25-June 27, Edinburgh, ScotlandSingle Blind?
(historically)ICMLFeb 24June 26-July 1, Edinburgh, ScotlandDouble Blind,
author response, zeroSPOFUAIMarch 30August 15-17, Catalina Islands,
CaliforniaDouble Blind, author responseGeographically, this is greatly
dispersed and the UAI/KDD conflict is unfortunate.Machine Learning conferences
are triannual now, betweenNIPS,AIStat, andICML. This has not always been the
case: the academic default is annual summer conferences, then NIPS started
with a December conference, and now AIStat has grown into an April
conference.However, the first claim is not quite correct. NIPS and AIStat have
few competing venues while ICML implicitly competes with many other
conferences accepting machine learning related papers. SinceJoelleand I are
taking a turn as program chairs this year, I</p><p>5 0.51721585 <a title="231-lda-5" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>Introduction: AtKDDI enjoyedStephen Boyd's invited talk about optimization quite a bit.
However, the most interesting talk for me wasDavid Haussler's. His talk
started out with a formidable load of biological complexity. About half-way
through you start wondering, "can this be used to help with cancer?" And at
the end he connects it directly to use with a call to arms for the audience:
cure cancer. The core thesis here is that cancer is a complex set of diseases
which can be distentangled via genetic assays, allowing attacking the specific
signature of individual cancers. However, the data quantity and complex
dependencies within the data require systematic and relatively automatic
prediction and analysis algorithms of the kind that we are best familiar
with.Some of the papers which interested me are:Kai-Wei ChangandDan
Roth,Selective Block Minimization for Faster Convergence of Limited Memory
Large-Scale Linear Models, which is about effectively using a hard-example
cache to speedup learning.Leland</p><p>6 0.51224494 <a title="231-lda-6" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>7 0.5121156 <a title="231-lda-7" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>8 0.50913578 <a title="231-lda-8" href="../hunch_net-2006/hunch_net-2006-09-28-Programming_Languages_for_Machine_Learning_Implementations.html">210 hunch net-2006-09-28-Programming Languages for Machine Learning Implementations</a></p>
<p>9 0.50642329 <a title="231-lda-9" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>10 0.50561076 <a title="231-lda-10" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>11 0.50539374 <a title="231-lda-11" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>12 0.50526971 <a title="231-lda-12" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>13 0.50369656 <a title="231-lda-13" href="../hunch_net-2012/hunch_net-2012-04-09-ICML_author_feedback_is_open.html">461 hunch net-2012-04-09-ICML author feedback is open</a></p>
<p>14 0.50343025 <a title="231-lda-14" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>15 0.50228482 <a title="231-lda-15" href="../hunch_net-2010/hunch_net-2010-04-24-COLT_Treasurer_is_now_Phil_Long.html">394 hunch net-2010-04-24-COLT Treasurer is now Phil Long</a></p>
<p>16 0.501706 <a title="231-lda-16" href="../hunch_net-2005/hunch_net-2005-03-17-Going_all_the_Way%2C_Sometimes.html">42 hunch net-2005-03-17-Going all the Way, Sometimes</a></p>
<p>17 0.50163674 <a title="231-lda-17" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>18 0.50083727 <a title="231-lda-18" href="../hunch_net-2005/hunch_net-2005-03-24-The_Role_of_Workshops.html">46 hunch net-2005-03-24-The Role of Workshops</a></p>
<p>19 0.50076503 <a title="231-lda-19" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>20 0.49948394 <a title="231-lda-20" href="../hunch_net-2013/hunch_net-2013-12-01-NIPS_tutorials_and_Vowpal_Wabbit_7.4.html">492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
