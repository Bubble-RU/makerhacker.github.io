<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>270 hunch net-2007-11-02-The Machine Learning Award goes to &#8230;</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2007" href="../home/hunch_net-2007_home.html">hunch_net-2007</a> <a title="hunch_net-2007-270" href="#">hunch_net-2007-270</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>270 hunch net-2007-11-02-The Machine Learning Award goes to &#8230;</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2007-270-html" href="http://hunch.net/?p=293">html</a></p><p>Introduction: Perhaps the biggest CS prize for research is theTuring Award, which has a
$0.25M cash prize associated with it. It appears none of the prizes so far
have been for anything like machine learning (the closest are perhaps database
awards).In CS theory, there is theGÃƒÂ¶del Prizewhich is smaller and newer,
offering a $5K prize along and perhaps (more importantly) recognition. One
such award has been given for Machine Learning, toRobert SchapireandYoav
Freundfor Adaboost.In Machine Learning, there seems to be no equivalent of
these sorts of prizes. There are several plausible reasons for this:There is
no coherent community.People drift in and out of the central conferences all
the time. Most of the author names from 10 years ago do not occur in the
conferences of today. In addition, the entire subject area is fairly new.There
are at least a core group of people who have stayed around.Machine Learning
work doesn't lastAlmost every paper is forgotten, because {the goals change,
there isn't an</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Perhaps the biggest CS prize for research is theTuring Award, which has a $0. [sent-1, score-0.276]
</p><p>2 It appears none of the prizes so far have been for anything like machine learning (the closest are perhaps database awards). [sent-3, score-0.417]
</p><p>3 In CS theory, there is theGÃƒÂ¶del Prizewhich is smaller and newer, offering a $5K prize along and perhaps (more importantly) recognition. [sent-4, score-0.386]
</p><p>4 One such award has been given for Machine Learning, toRobert SchapireandYoav Freundfor Adaboost. [sent-5, score-0.502]
</p><p>5 There are several plausible reasons for this:There is no coherent community. [sent-7, score-0.155]
</p><p>6 People drift in and out of the central conferences all the time. [sent-8, score-0.287]
</p><p>7 Most of the author names from 10 years ago do not occur in the conferences of today. [sent-9, score-0.408]
</p><p>8 There are at least a core group of people who have stayed around. [sent-11, score-0.099]
</p><p>9 Machine Learning work doesn't lastAlmost every paper is forgotten, because {the goals change, there isn't any real progress, there are no teachable foundations}. [sent-12, score-0.091]
</p><p>10 The field is fractured between many very different viewpoints-- statistical, empirical, AI, and theoretical. [sent-15, score-0.175]
</p><p>11 The prioritization of results across these very different viewpoints is hard. [sent-16, score-0.361]
</p><p>12 AspirationPerhaps the most valuable aspect of an award is that it gives people an incentive to aim for something in the long term. [sent-20, score-0.866]
</p><p>13 The closest approximation that we have right now is "best papers" awards at individual conferences. [sent-21, score-0.417]
</p><p>14 Best paper awards have a role, but it's not the same. [sent-22, score-0.259]
</p><p>15 10 years from now, when we look back 10 years, which papers will seem most significant? [sent-23, score-0.136]
</p><p>16 RepresentationOne function of an award is that tells other people what we consider good work. [sent-25, score-0.502]
</p><p>17 In an academic reference frame, it gives information of the form "this person deserves tenure". [sent-26, score-0.218]
</p><p>18 CrystallizationResearch is a process of discovering information and placing it carefully in context. [sent-28, score-0.149]
</p><p>19 An award has some role in furthering that process. [sent-29, score-0.642]
</p><p>20 How do you avoid wasting time and playing favorites while keeping the higher level vision of what might be useful in the long term? [sent-31, score-0.342]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('award', 0.502), ('awards', 0.259), ('prize', 0.2), ('closest', 0.158), ('role', 0.14), ('years', 0.136), ('cs', 0.136), ('gives', 0.119), ('conferences', 0.114), ('across', 0.107), ('theg', 0.099), ('prioritization', 0.099), ('fractured', 0.099), ('deserves', 0.099), ('forgotten', 0.099), ('drift', 0.099), ('aim', 0.099), ('favorites', 0.099), ('stayed', 0.099), ('theturing', 0.099), ('wasting', 0.099), ('perhaps', 0.095), ('offering', 0.091), ('teachable', 0.091), ('clarity', 0.086), ('consensus', 0.086), ('foundations', 0.086), ('occur', 0.082), ('cash', 0.082), ('frame', 0.082), ('prizes', 0.082), ('database', 0.082), ('viewpoints', 0.079), ('tenure', 0.079), ('insight', 0.079), ('placing', 0.079), ('outsiders', 0.079), ('reasons', 0.079), ('different', 0.076), ('newer', 0.076), ('biggest', 0.076), ('names', 0.076), ('coherent', 0.076), ('turing', 0.074), ('incentive', 0.074), ('central', 0.074), ('importantly', 0.072), ('playing', 0.072), ('long', 0.072), ('discovering', 0.07)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9999997 <a title="270-tfidf-1" href="../hunch_net-2007/hunch_net-2007-11-02-The_Machine_Learning_Award_goes_to_%26%238230%3B.html">270 hunch net-2007-11-02-The Machine Learning Award goes to &#8230;</a></p>
<p>Introduction: Perhaps the biggest CS prize for research is theTuring Award, which has a
$0.25M cash prize associated with it. It appears none of the prizes so far
have been for anything like machine learning (the closest are perhaps database
awards).In CS theory, there is theGÃƒÂ¶del Prizewhich is smaller and newer,
offering a $5K prize along and perhaps (more importantly) recognition. One
such award has been given for Machine Learning, toRobert SchapireandYoav
Freundfor Adaboost.In Machine Learning, there seems to be no equivalent of
these sorts of prizes. There are several plausible reasons for this:There is
no coherent community.People drift in and out of the central conferences all
the time. Most of the author names from 10 years ago do not occur in the
conferences of today. In addition, the entire subject area is fairly new.There
are at least a core group of people who have stayed around.Machine Learning
work doesn't lastAlmost every paper is forgotten, because {the goals change,
there isn't an</p><p>2 0.2108746 <a title="270-tfidf-2" href="../hunch_net-2012/hunch_net-2012-02-29-Key_Scientific_Challenges_and_the_Franklin_Symposium.html">457 hunch net-2012-02-29-Key Scientific Challenges and the Franklin Symposium</a></p>
<p>Introduction: For graduate students, theYahoo!Key Scientific Challenges programincluding
inmachine learningis on again,due March 9. The application is easy and the $5K
award is high quality "no strings attached" funding. Consider submitting.Those
in Washington DC, Philadelphia, and New York, may consider attending
theFranklin Institute SymposiumApril 25which has several speakers and an award
forV. Attendance is free with an RSVP.</p><p>3 0.16599566 <a title="270-tfidf-3" href="../hunch_net-2005/hunch_net-2005-06-28-The_cross_validation_problem%3A_cash_reward.html">86 hunch net-2005-06-28-The cross validation problem: cash reward</a></p>
<p>Introduction: I justpresentedthecross validationproblem atCOLT.The problem now has a cash
prize (up to $500) associated with it--see thepresentationfor details
.Thewrite-up for colt.</p><p>4 0.10613132 <a title="270-tfidf-4" href="../hunch_net-2011/hunch_net-2011-03-27-Vowpal_Wabbit%2C_v5.1.html">428 hunch net-2011-03-27-Vowpal Wabbit, v5.1</a></p>
<p>Introduction: I just createdversion 5.1ofvowpal wabbit. This almost entirely a bugfix
release, so it's an easy upgrade from v5.0.In addition:There is now amailing
list, which I and several other developers are subscribed to.The main website
has shifted to the wiki on github. This means that anyone with a github
account can now edit it.I'm planning to give a tutorial tomorrow on it
ateHarmony/the LA machine learning meetupat 10am. Drop by if you're
interested.The status of VW amongst other open source projects has changed.
When VW first came out, it was relatively unique amongst existing projects in
terms of features. At this point, many other projects have started to
appreciate the value of the design choices here. This includes:Mahout, which
now has an SGD implementation.Shogun, whereSoerenis keen onincorporating
features.LibLinear, where they won the KDD best paper award forout-of-core
learning.This is expected--any open source approach which works well should be
widely adopted. None of these othe</p><p>5 0.10577277 <a title="270-tfidf-5" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>Introduction: Normally, I don't indulge in posters forICML, but this year is naturally an
exception for me. If you want one, there are a small numberleft here, if you
sign up before February.It also seems worthwhile to give some sense of the
scope and reviewing criteria for ICML for authors considering submitting
papers. At ICML, the (very large) program committee does the reviewing which
informs final decisions by area chairs on most papers. Program chairs setup
the process, deal with exceptions or disagreements, and provide advice for the
reviewing process. Providing advice is tricky (and easily misleading) because
a conference is a community, and in the end the aggregate interests of the
community determine the conference. Nevertheless, as a program chair this year
it seems worthwhile to state the overall philosophy I have and what I plan to
encourage (and occasionally discourage).At the highest level, I believe ICML
exists to further research into machine learning, which I generally think of
as</p><p>6 0.10345545 <a title="270-tfidf-6" href="../hunch_net-2005/hunch_net-2005-07-13-%26%238220%3BSister_Conference%26%238221%3B_presentations.html">93 hunch net-2005-07-13-&#8220;Sister Conference&#8221; presentations</a></p>
<p>7 0.10195238 <a title="270-tfidf-7" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>8 0.10023692 <a title="270-tfidf-8" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>9 0.094883472 <a title="270-tfidf-9" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>10 0.094219387 <a title="270-tfidf-10" href="../hunch_net-2005/hunch_net-2005-04-22-New_Blog%3A_%5BLowerbounds%2CUpperbounds%5D.html">59 hunch net-2005-04-22-New Blog: [Lowerbounds,Upperbounds]</a></p>
<p>11 0.090396702 <a title="270-tfidf-11" href="../hunch_net-2007/hunch_net-2007-02-16-The_Forgetting.html">233 hunch net-2007-02-16-The Forgetting</a></p>
<p>12 0.090204954 <a title="270-tfidf-12" href="../hunch_net-2009/hunch_net-2009-09-21-Netflix_finishes_%28and_starts%29.html">371 hunch net-2009-09-21-Netflix finishes (and starts)</a></p>
<p>13 0.090061426 <a title="270-tfidf-13" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>14 0.089339525 <a title="270-tfidf-14" href="../hunch_net-2006/hunch_net-2006-10-04-Health_of_Conferences_Wiki.html">212 hunch net-2006-10-04-Health of Conferences Wiki</a></p>
<p>15 0.088224038 <a title="270-tfidf-15" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<p>16 0.087784924 <a title="270-tfidf-16" href="../hunch_net-2006/hunch_net-2006-04-14-JMLR_is_a_success.html">172 hunch net-2006-04-14-JMLR is a success</a></p>
<p>17 0.085674748 <a title="270-tfidf-17" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>18 0.0844732 <a title="270-tfidf-18" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>19 0.083390974 <a title="270-tfidf-19" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>20 0.082802407 <a title="270-tfidf-20" href="../hunch_net-2009/hunch_net-2009-05-08-Computability_in_Artificial_Intelligence.html">353 hunch net-2009-05-08-Computability in Artificial Intelligence</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.209), (1, 0.077), (2, 0.032), (3, -0.025), (4, 0.022), (5, 0.029), (6, -0.017), (7, 0.021), (8, 0.054), (9, -0.038), (10, -0.08), (11, -0.02), (12, 0.002), (13, 0.026), (14, -0.014), (15, 0.123), (16, -0.035), (17, 0.022), (18, 0.05), (19, -0.061), (20, -0.038), (21, -0.03), (22, -0.014), (23, 0.055), (24, 0.078), (25, 0.016), (26, 0.079), (27, 0.01), (28, 0.006), (29, -0.116), (30, 0.122), (31, 0.033), (32, 0.07), (33, -0.079), (34, 0.012), (35, -0.03), (36, -0.009), (37, 0.016), (38, 0.023), (39, 0.088), (40, -0.066), (41, -0.051), (42, -0.009), (43, -0.023), (44, -0.144), (45, -0.207), (46, -0.064), (47, -0.021), (48, 0.067), (49, -0.029)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94080341 <a title="270-lsi-1" href="../hunch_net-2007/hunch_net-2007-11-02-The_Machine_Learning_Award_goes_to_%26%238230%3B.html">270 hunch net-2007-11-02-The Machine Learning Award goes to &#8230;</a></p>
<p>Introduction: Perhaps the biggest CS prize for research is theTuring Award, which has a
$0.25M cash prize associated with it. It appears none of the prizes so far
have been for anything like machine learning (the closest are perhaps database
awards).In CS theory, there is theGÃƒÂ¶del Prizewhich is smaller and newer,
offering a $5K prize along and perhaps (more importantly) recognition. One
such award has been given for Machine Learning, toRobert SchapireandYoav
Freundfor Adaboost.In Machine Learning, there seems to be no equivalent of
these sorts of prizes. There are several plausible reasons for this:There is
no coherent community.People drift in and out of the central conferences all
the time. Most of the author names from 10 years ago do not occur in the
conferences of today. In addition, the entire subject area is fairly new.There
are at least a core group of people who have stayed around.Machine Learning
work doesn't lastAlmost every paper is forgotten, because {the goals change,
there isn't an</p><p>2 0.5502919 <a title="270-lsi-2" href="../hunch_net-2006/hunch_net-2006-04-14-JMLR_is_a_success.html">172 hunch net-2006-04-14-JMLR is a success</a></p>
<p>Introduction: In 2001, the "Journal of Machine Learning Research" was created in reaction to
unadaptive publisher policies atMLJ. Essentially, with the creation of the
internet, the bottleneck in publishing research shifted from publishing to
research. Thedeclaration of independenceaccompanying this move expresses the
reasons why in greater detail.MLJ has strongly changed its policy in reaction
to this. In particular, there is no longer an assignment of copyright to the
publisher (*), and MLJ regularly sponsors many student "best paper awards"
across several conferences with cash prizes. This is an advantage of MLJ over
JMLR: MLJ can afford to sponsor cash prizes for the machine learning
community. The remaining disadvantage is that reading papers in MLJ sometimes
requires searching for the author's website where the free version is
available. In contrast, JMLR articles are freely available to everyone off the
JMLR website. Whether or not this disadvantage cancels the advantage is
debatable, but ess</p><p>3 0.53312707 <a title="270-lsi-3" href="../hunch_net-2005/hunch_net-2005-10-08-We_have_a_winner.html">119 hunch net-2005-10-08-We have a winner</a></p>
<p>Introduction: The DARPA grandchallenge is a big contest for autonomous robot vehicle
driving. It was run once in 2004 for the first time and all teams did badly.
This year was notably different with theStanfordandCMUteams succesfully
completing the course. A number of details arehereandwikipedia has continuing
coverage.A formal winner hasn't been declared yet although Stanford completed
the course quickest.The Stanford and CMU teams deserve a large round of
applause as they have strongly demonstrated the feasibility of autonomous
vehicles.The good news for machine learning is that the Stanford team (at
least) is using some machine learning techniques.</p><p>4 0.487959 <a title="270-lsi-4" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<p>Introduction: I recently had fun discussions with bothVikash MansinghkaandThomas Breuelabout
approaching AI with machine learning. The general interest in taking a crack
at AI with machine learning seems to be rising on many fronts
includingDARPA.As a matter of history, there was a great deal of interest in
AI which died down before I began research. There remain many projects and
conferences spawned in this earlier AI wave, as well as a good bit of
experience about what did not work, or at least did not work yet. Here are a
few examples of failure modes that people seem to run into:Supply/Product
confusion. Sometimes we think "Intelligences use X, so I'll create X and have
an Intelligence." An example of this is theCyc Projectwhich inspires some
people as "intelligences use ontologies, so I'll create an ontology and a
system using it to have an Intelligence." The flaw here is that
Intelligencescreateontologies, which they use, and without the ability to
create ontologies you don't have an Intellige</p><p>5 0.48115405 <a title="270-lsi-5" href="../hunch_net-2009/hunch_net-2009-12-09-Future_Publication_Models_%40_NIPS.html">382 hunch net-2009-12-09-Future Publication Models @ NIPS</a></p>
<p>Introduction: Yesterday, there was a discussion aboutfuture publication models at
NIPS.YannandZoubinhave specific detailed proposals which I'll add links to
when I get them (Yann's proposalandZoubin's proposal).What struck me about the
discussion is that there are many simultaneous concerns as well as many
simultaneous proposals, which makes it difficult to keep all the distinctions
straight in a verbal conversation. It also seemed like people were serious
enough about this that we may see some real movement. Certainly, my personal
experience motivates that as I'veposted many timesabout the substantial flaws
in our review process, including some very poor personal experiences.Concerns
include the following:(Several) Reviewers are overloaded, boosting the noise
in decision making.(Yann) A new system should run with as little built-in
delay and friction to the process of research as possible.(Hanna
Wallach(updated)) Double-blind review is particularly important for people who
are unknown or from an un</p><p>6 0.4712446 <a title="270-lsi-6" href="../hunch_net-2006/hunch_net-2006-10-04-Health_of_Conferences_Wiki.html">212 hunch net-2006-10-04-Health of Conferences Wiki</a></p>
<p>7 0.46689245 <a title="270-lsi-7" href="../hunch_net-2009/hunch_net-2009-05-08-Computability_in_Artificial_Intelligence.html">353 hunch net-2009-05-08-Computability in Artificial Intelligence</a></p>
<p>8 0.46342444 <a title="270-lsi-8" href="../hunch_net-2009/hunch_net-2009-01-19-Netflix_prize_within_epsilon.html">336 hunch net-2009-01-19-Netflix prize within epsilon</a></p>
<p>9 0.45585057 <a title="270-lsi-9" href="../hunch_net-2005/hunch_net-2005-04-22-New_Blog%3A_%5BLowerbounds%2CUpperbounds%5D.html">59 hunch net-2005-04-22-New Blog: [Lowerbounds,Upperbounds]</a></p>
<p>10 0.44242033 <a title="270-lsi-10" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>11 0.44090497 <a title="270-lsi-11" href="../hunch_net-2005/hunch_net-2005-07-04-The_Health_of_COLT.html">89 hunch net-2005-07-04-The Health of COLT</a></p>
<p>12 0.43853393 <a title="270-lsi-12" href="../hunch_net-2008/hunch_net-2008-04-12-It_Doesn%26%238217%3Bt_Stop.html">295 hunch net-2008-04-12-It Doesn&#8217;t Stop</a></p>
<p>13 0.43741885 <a title="270-lsi-13" href="../hunch_net-2012/hunch_net-2012-02-29-Key_Scientific_Challenges_and_the_Franklin_Symposium.html">457 hunch net-2012-02-29-Key Scientific Challenges and the Franklin Symposium</a></p>
<p>14 0.42217594 <a title="270-lsi-14" href="../hunch_net-2008/hunch_net-2008-07-02-Proprietary_Data_in_Academic_Research%3F.html">306 hunch net-2008-07-02-Proprietary Data in Academic Research?</a></p>
<p>15 0.41747931 <a title="270-lsi-15" href="../hunch_net-2005/hunch_net-2005-07-13-%26%238220%3BSister_Conference%26%238221%3B_presentations.html">93 hunch net-2005-07-13-&#8220;Sister Conference&#8221; presentations</a></p>
<p>16 0.41470745 <a title="270-lsi-16" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>17 0.41325733 <a title="270-lsi-17" href="../hunch_net-2007/hunch_net-2007-02-16-The_Forgetting.html">233 hunch net-2007-02-16-The Forgetting</a></p>
<p>18 0.41166195 <a title="270-lsi-18" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>19 0.4073202 <a title="270-lsi-19" href="../hunch_net-2009/hunch_net-2009-09-21-Netflix_finishes_%28and_starts%29.html">371 hunch net-2009-09-21-Netflix finishes (and starts)</a></p>
<p>20 0.40445012 <a title="270-lsi-20" href="../hunch_net-2010/hunch_net-2010-10-17-Partha_Niyogi_has_died.html">414 hunch net-2010-10-17-Partha Niyogi has died</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(35, 0.037), (42, 0.199), (45, 0.046), (68, 0.021), (69, 0.021), (74, 0.141), (83, 0.363), (88, 0.032), (95, 0.043)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.98145705 <a title="270-lda-1" href="../hunch_net-2013/hunch_net-2013-01-31-Remote_large_scale_learning_class_participation.html">479 hunch net-2013-01-31-Remote large scale learning class participation</a></p>
<p>Introduction: Yann and I have arranged so that people who are interested in ourlarge scale
machine learning classand not able to attend in person can follow along via
two methods.Videoswill be posted with about a 1 day delay ontechtalks. This is
a side-by-side capture of video+slides fromWeyond.We are experimenting
withPiazzaas a discussion forum. Anyone is welcome to subscribe to Piazza and
ask questions there, where I will be monitoring things.update2: Sign
uphere.The first lecture is up now, including therevised version of the
slideswhich fixes a few typos and rounds out references.</p><p>2 0.92667359 <a title="270-lda-2" href="../hunch_net-2012/hunch_net-2012-06-15-Normal_Deviate_and_the_UCSC_Machine_Learning_Summer_School.html">467 hunch net-2012-06-15-Normal Deviate and the UCSC Machine Learning Summer School</a></p>
<p>Introduction: Larry Wassermanhas started theNormal Deviateblog which I added to the blogroll
on the right.Manfred Warmuthpoints out theUCSC machine learning summer
schoolrunning July 9-20 which may be of particular interest to those in
silicon valley.</p><p>same-blog 3 0.87966663 <a title="270-lda-3" href="../hunch_net-2007/hunch_net-2007-11-02-The_Machine_Learning_Award_goes_to_%26%238230%3B.html">270 hunch net-2007-11-02-The Machine Learning Award goes to &#8230;</a></p>
<p>Introduction: Perhaps the biggest CS prize for research is theTuring Award, which has a
$0.25M cash prize associated with it. It appears none of the prizes so far
have been for anything like machine learning (the closest are perhaps database
awards).In CS theory, there is theGÃƒÂ¶del Prizewhich is smaller and newer,
offering a $5K prize along and perhaps (more importantly) recognition. One
such award has been given for Machine Learning, toRobert SchapireandYoav
Freundfor Adaboost.In Machine Learning, there seems to be no equivalent of
these sorts of prizes. There are several plausible reasons for this:There is
no coherent community.People drift in and out of the central conferences all
the time. Most of the author names from 10 years ago do not occur in the
conferences of today. In addition, the entire subject area is fairly new.There
are at least a core group of people who have stayed around.Machine Learning
work doesn't lastAlmost every paper is forgotten, because {the goals change,
there isn't an</p><p>4 0.84968227 <a title="270-lda-4" href="../hunch_net-2009/hunch_net-2009-05-30-Many_ways_to_Learn_this_summer.html">357 hunch net-2009-05-30-Many ways to Learn this summer</a></p>
<p>Introduction: There are at least3summer schools related to machine learning this summer.The
firstis atUniversity of ChicagoJune 1-11 organized byMisha Belkin,Partha
Niyogi, andSteve Smale. Registration is closed for this one, meaning they met
their capacity limit. The format is essentially an extended Tutorial/Workshop.
I was particularly interested to seeValiantamongst the speakers. I'm also
presenting Saturday June 6, on logarithmic time prediction.Praveen
Srinivasanpoints out the second atPeking Universityin Beijing, China, July
20-27.This onediffers substantially, as it is about vision, machine learning,
and their intersection. The deadline for applications is June 10 or 15. This
is also another example of the growth of research in China, with active
support fromNSF.The third one is atCambridge, England, August 29-September 10.
It's in theMLSS series. Compared to the Chicago one, this one is more about
the Bayesian side of ML, although effort has been made to create a good cross
section of topic</p><p>5 0.83439279 <a title="270-lda-5" href="../hunch_net-2005/hunch_net-2005-04-04-Grounds_for_Rejection.html">52 hunch net-2005-04-04-Grounds for Rejection</a></p>
<p>Introduction: It's reviewing season right now, so I thought I would list (at a high level)
the sorts of problems which I see in papers. Hopefully, this will help us all
write better papers.The following flaws are fatal to any paper:Incorrect
theorem or lemma statementsA typo might be "ok", if it can be understood. Any
theorem or lemma which indicates an incorrect understanding of reality must be
rejected. Not doing so would severely harm the integrity of the conference. A
paper rejected for this reason must be fixed.Lack of UnderstandingIf a paper
is understood by none of the (typically 3) reviewers then it must be rejected
for the same reason. This is more controversial than it sounds because there
are some people who maximize paper complexity in the hope of impressing the
reviewer. The tactic sometimes succeeds with some reviewers (but not with
me).As a reviewer, I sometimes get lost for stupid reasons. This is why an
anonymizedcommunication channelwith the author can be very helpful.Bad
ideaRarel</p><p>6 0.80644792 <a title="270-lda-6" href="../hunch_net-2009/hunch_net-2009-05-08-Computability_in_Artificial_Intelligence.html">353 hunch net-2009-05-08-Computability in Artificial Intelligence</a></p>
<p>7 0.69993049 <a title="270-lda-7" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>8 0.59068513 <a title="270-lda-8" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>9 0.58565897 <a title="270-lda-9" href="../hunch_net-2005/hunch_net-2005-03-17-Going_all_the_Way%2C_Sometimes.html">42 hunch net-2005-03-17-Going all the Way, Sometimes</a></p>
<p>10 0.58536834 <a title="270-lda-10" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>11 0.577887 <a title="270-lda-11" href="../hunch_net-2009/hunch_net-2009-06-01-Multitask_Poisoning.html">358 hunch net-2009-06-01-Multitask Poisoning</a></p>
<p>12 0.5688166 <a title="270-lda-12" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>13 0.56492442 <a title="270-lda-13" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>14 0.56074882 <a title="270-lda-14" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>15 0.56067628 <a title="270-lda-15" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>16 0.55907691 <a title="270-lda-16" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>17 0.55905819 <a title="270-lda-17" href="../hunch_net-2008/hunch_net-2008-01-06-Research_Political_Issues.html">282 hunch net-2008-01-06-Research Political Issues</a></p>
<p>18 0.55816305 <a title="270-lda-18" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>19 0.55745131 <a title="270-lda-19" href="../hunch_net-2009/hunch_net-2009-12-09-Future_Publication_Models_%40_NIPS.html">382 hunch net-2009-12-09-Future Publication Models @ NIPS</a></p>
<p>20 0.55738711 <a title="270-lda-20" href="../hunch_net-2005/hunch_net-2005-02-17-Learning_Research_Programs.html">21 hunch net-2005-02-17-Learning Research Programs</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
