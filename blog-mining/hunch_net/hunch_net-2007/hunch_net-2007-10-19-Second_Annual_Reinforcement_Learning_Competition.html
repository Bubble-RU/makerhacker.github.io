<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>268 hunch net-2007-10-19-Second Annual Reinforcement Learning Competition</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2007" href="../home/hunch_net-2007_home.html">hunch_net-2007</a> <a title="hunch_net-2007-268" href="#">hunch_net-2007-268</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>268 hunch net-2007-10-19-Second Annual Reinforcement Learning Competition</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2007-268-html" href="http://hunch.net/?p=297">html</a></p><p>Introduction: The Second Annual Reinforcement Learning Competition is about to get started.
The aim of the competition is to facilitate direct comparisons between various
learning methods on important and realistic domains. This year's event will
feature well-known benchmark domains as well as more challenging problems of
real-world complexity, such as helicopter control and robot soccer
keepaway.The competition begins on November 1st, 2007 when training software
is released. Results must be submitted by July 1st, 2008. The competition will
culminate in an event at ICML-08 in Helsinki, Finland, at which the winners
will be announced.For more information, visitthe competition website.</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('competition', 0.593), ('event', 0.218), ('winners', 0.187), ('helicopter', 0.187), ('soccer', 0.187), ('aim', 0.187), ('helsinki', 0.187), ('november', 0.187), ('benchmark', 0.173), ('begins', 0.173), ('facilitate', 0.173), ('realistic', 0.173), ('comparisons', 0.163), ('domains', 0.156), ('robot', 0.15), ('annual', 0.144), ('july', 0.14), ('challenging', 0.136), ('submitted', 0.123), ('direct', 0.121)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="268-tfidf-1" href="../hunch_net-2007/hunch_net-2007-10-19-Second_Annual_Reinforcement_Learning_Competition.html">268 hunch net-2007-10-19-Second Annual Reinforcement Learning Competition</a></p>
<p>Introduction: The Second Annual Reinforcement Learning Competition is about to get started.
The aim of the competition is to facilitate direct comparisons between various
learning methods on important and realistic domains. This year's event will
feature well-known benchmark domains as well as more challenging problems of
real-world complexity, such as helicopter control and robot soccer
keepaway.The competition begins on November 1st, 2007 when training software
is released. Results must be submitted by July 1st, 2008. The competition will
culminate in an event at ICML-08 in Helsinki, Finland, at which the winners
will be announced.For more information, visitthe competition website.</p><p>2 0.23317544 <a title="268-tfidf-2" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>Introduction: TheInternational Planning Competition(IPC) is a biennial event organized in
the context of theInternational Conference on Automated Planning and
Scheduling(ICAPS). This year, for the first time, there will a learning track
of the competition. For more information you can go to the competitionweb-
site.The competitions are typically organized around a number of planning
domains that can vary from year to year, where a planning domain is simply a
class of problems that share a common action schema--e.g. Blocksworld is a
well-known planning domain that contains a problem instance each possible
initial tower configuration and goal configuration. Some other domains have
included Logistics, Airport, Freecell, PipesWorld, and manyothers. For each
domain the competition includes a number of problems (say 40-50) and the
planners are run on each problem with a time limit for each problem (around 30
minutes). The problems are hard enough that many problems are not solved
within the time limit.Giv</p><p>3 0.12722947 <a title="268-tfidf-3" href="../hunch_net-2006/hunch_net-2006-01-13-Benchmarks_for_RL.html">148 hunch net-2006-01-13-Benchmarks for RL</a></p>
<p>Introduction: A couple years ago, Drew Bagnell and I started theRLBench projectto setup a
suite of reinforcement learning benchmark problems. We haven't been able to
touch it (due to lack of time) for a year so the project is on hold. Luckily,
there are several other projects such asCLSquareandRL-Gluewith a similar goal,
and we strongly endorse their continued development.I would like to explain
why, especially in the context of criticism of other learning benchmarks. For
example, sometimes theUCI Machine Learning Repositoryis criticized. There are
two criticisms I know of:Learning algorithms have overfit to the problems in
the repository. It is easy to imagine a mechanism for this happening
unintentionally. Strong evidence of this would be provided by learning
algorithms which perform great on the UCI machine learning repository but very
badly (relative to other learning algorithms) on non-UCI learning problems. I
have seen little evidence of this but it remains a point of concern. There is
a natur</p><p>4 0.092044815 <a title="268-tfidf-4" href="../hunch_net-2009/hunch_net-2009-09-29-Machine_Learning_Protests_at_the_G20.html">372 hunch net-2009-09-29-Machine Learning Protests at the G20</a></p>
<p>Introduction: Themachine learning department at CMUturned out en masse to protest the G20
summit in Pittsburgh.Arthur Grettonuploaded somegreat photoscovering the event</p><p>5 0.089517012 <a title="268-tfidf-5" href="../hunch_net-2007/hunch_net-2007-02-11-24.html">232 hunch net-2007-02-11-24</a></p>
<p>Introduction: To commemorate theTwenty Fourth Annual International Conference on Machine
Learning(ICML-07), the FOX Network has decided to launch a new spin-off series
in prime time. Through unofficial sources, I have obtained thestory arcfor the
first season, which appears frighteningly realistic.</p><p>6 0.079958841 <a title="268-tfidf-6" href="../hunch_net-2010/hunch_net-2010-02-26-Yahoo%21_ML_events.html">389 hunch net-2010-02-26-Yahoo! ML events</a></p>
<p>7 0.078600615 <a title="268-tfidf-7" href="../hunch_net-2010/hunch_net-2010-03-12-Netflix_Challenge_2_Canceled.html">390 hunch net-2010-03-12-Netflix Challenge 2 Canceled</a></p>
<p>8 0.078190714 <a title="268-tfidf-8" href="../hunch_net-2005/hunch_net-2005-02-10-Conferences%2C_Dates%2C_Locations.html">17 hunch net-2005-02-10-Conferences, Dates, Locations</a></p>
<p>9 0.075435527 <a title="268-tfidf-9" href="../hunch_net-2005/hunch_net-2005-04-27-DARPA_project%3A_LAGR.html">63 hunch net-2005-04-27-DARPA project: LAGR</a></p>
<p>10 0.07452552 <a title="268-tfidf-10" href="../hunch_net-2005/hunch_net-2005-02-23-Problem%3A_Reinforcement_Learning_with_Classification.html">27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</a></p>
<p>11 0.070071295 <a title="268-tfidf-11" href="../hunch_net-2009/hunch_net-2009-01-19-Netflix_prize_within_epsilon.html">336 hunch net-2009-01-19-Netflix prize within epsilon</a></p>
<p>12 0.066086546 <a title="268-tfidf-12" href="../hunch_net-2006/hunch_net-2006-02-07-Pittsburgh_Mind_Reading_Competition.html">155 hunch net-2006-02-07-Pittsburgh Mind Reading Competition</a></p>
<p>13 0.063680574 <a title="268-tfidf-13" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>14 0.061286155 <a title="268-tfidf-14" href="../hunch_net-2007/hunch_net-2007-11-05-CMU_wins_DARPA_Urban_Challenge.html">271 hunch net-2007-11-05-CMU wins DARPA Urban Challenge</a></p>
<p>15 0.056990337 <a title="268-tfidf-15" href="../hunch_net-2006/hunch_net-2006-11-20-Context_and_the_calculation_misperception.html">218 hunch net-2006-11-20-Context and the calculation misperception</a></p>
<p>16 0.056824572 <a title="268-tfidf-16" href="../hunch_net-2010/hunch_net-2010-09-17-New_York_Area_Machine_Learning_Events.html">410 hunch net-2010-09-17-New York Area Machine Learning Events</a></p>
<p>17 0.056171574 <a title="268-tfidf-17" href="../hunch_net-2008/hunch_net-2008-09-04-Fall_ML_Conferences.html">316 hunch net-2008-09-04-Fall ML Conferences</a></p>
<p>18 0.054955378 <a title="268-tfidf-18" href="../hunch_net-2008/hunch_net-2008-01-23-Why_Workshop%3F.html">285 hunch net-2008-01-23-Why Workshop?</a></p>
<p>19 0.049810261 <a title="268-tfidf-19" href="../hunch_net-2012/hunch_net-2012-06-15-Normal_Deviate_and_the_UCSC_Machine_Learning_Summer_School.html">467 hunch net-2012-06-15-Normal Deviate and the UCSC Machine Learning Summer School</a></p>
<p>20 0.049458664 <a title="268-tfidf-20" href="../hunch_net-2012/hunch_net-2012-02-24-ICML%2B50%25.html">456 hunch net-2012-02-24-ICML+50%</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.077), (1, -0.004), (2, 0.027), (3, 0.045), (4, -0.02), (5, 0.001), (6, -0.048), (7, 0.005), (8, 0.076), (9, -0.013), (10, -0.028), (11, -0.029), (12, 0.008), (13, 0.037), (14, 0.033), (15, -0.001), (16, -0.005), (17, 0.044), (18, -0.111), (19, -0.05), (20, 0.003), (21, -0.009), (22, -0.072), (23, -0.006), (24, -0.11), (25, 0.051), (26, -0.004), (27, 0.053), (28, -0.069), (29, 0.088), (30, -0.093), (31, 0.003), (32, -0.045), (33, 0.002), (34, -0.062), (35, 0.005), (36, 0.048), (37, -0.03), (38, 0.021), (39, -0.126), (40, 0.015), (41, -0.141), (42, -0.066), (43, -0.018), (44, 0.064), (45, -0.114), (46, -0.04), (47, -0.072), (48, 0.145), (49, 0.001)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97306156 <a title="268-lsi-1" href="../hunch_net-2007/hunch_net-2007-10-19-Second_Annual_Reinforcement_Learning_Competition.html">268 hunch net-2007-10-19-Second Annual Reinforcement Learning Competition</a></p>
<p>Introduction: The Second Annual Reinforcement Learning Competition is about to get started.
The aim of the competition is to facilitate direct comparisons between various
learning methods on important and realistic domains. This year's event will
feature well-known benchmark domains as well as more challenging problems of
real-world complexity, such as helicopter control and robot soccer
keepaway.The competition begins on November 1st, 2007 when training software
is released. Results must be submitted by July 1st, 2008. The competition will
culminate in an event at ICML-08 in Helsinki, Finland, at which the winners
will be announced.For more information, visitthe competition website.</p><p>2 0.50986838 <a title="268-lsi-2" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>Introduction: TheInternational Planning Competition(IPC) is a biennial event organized in
the context of theInternational Conference on Automated Planning and
Scheduling(ICAPS). This year, for the first time, there will a learning track
of the competition. For more information you can go to the competitionweb-
site.The competitions are typically organized around a number of planning
domains that can vary from year to year, where a planning domain is simply a
class of problems that share a common action schema--e.g. Blocksworld is a
well-known planning domain that contains a problem instance each possible
initial tower configuration and goal configuration. Some other domains have
included Logistics, Airport, Freecell, PipesWorld, and manyothers. For each
domain the competition includes a number of problems (say 40-50) and the
planners are run on each problem with a time limit for each problem (around 30
minutes). The problems are hard enough that many problems are not solved
within the time limit.Giv</p><p>3 0.47735605 <a title="268-lsi-3" href="../hunch_net-2009/hunch_net-2009-09-29-Machine_Learning_Protests_at_the_G20.html">372 hunch net-2009-09-29-Machine Learning Protests at the G20</a></p>
<p>Introduction: Themachine learning department at CMUturned out en masse to protest the G20
summit in Pittsburgh.Arthur Grettonuploaded somegreat photoscovering the event</p><p>4 0.39994395 <a title="268-lsi-4" href="../hunch_net-2006/hunch_net-2006-04-05-What_is_state%3F.html">169 hunch net-2006-04-05-What is state?</a></p>
<p>Introduction: In reinforcement learning (and sometimes other settings), there is a notion of
"state". Based upon the state various predictions are made such as "Which
action should be taken next?" or "How much cumulative reward do I expect if I
take some action from this state?" Given the importance of state, it is
important to examine the meaning. There are actually several distinct options
and it turns out the definition variation is very important in motivating
different pieces of work.Newtonian State. State is the physical pose of the
world. Under this definition, there areverymany states, often too many for
explicit representation. This is also the definition typically used in
games.Abstracted State. State is an abstracted physical state of the world.
"Is the door open or closed?" "Are you in room A or not?" The number of states
is much smaller here. A basic issue here is: "How do you compute the state
from observations?"Mathematical State. State is a sufficient statistic of
observations for ma</p><p>5 0.39262396 <a title="268-lsi-5" href="../hunch_net-2005/hunch_net-2005-10-20-Machine_Learning_in_the_News.html">125 hunch net-2005-10-20-Machine Learning in the News</a></p>
<p>Introduction: The New York Times had ashort interviewabout machine learning in datamining
being used pervasively by the IRS and large corporations to predict who to
audit and who to target for various marketing campaigns. This is a big
application area of machine learning. It can be harmful (learning + databases
= another way to invade privacy) or beneficial (as google demonstrates, better
targeting of marketing campaigns is far less annoying). This is yet more
evidence that we can not rely upon "I'm just another fish in the school" logic
for our expectations about treatment by government and large corporations.</p><p>6 0.38710058 <a title="268-lsi-6" href="../hunch_net-2005/hunch_net-2005-02-23-Problem%3A_Reinforcement_Learning_with_Classification.html">27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</a></p>
<p>7 0.38694692 <a title="268-lsi-7" href="../hunch_net-2005/hunch_net-2005-08-08-Apprenticeship_Reinforcement_Learning_for_Control.html">101 hunch net-2005-08-08-Apprenticeship Reinforcement Learning for Control</a></p>
<p>8 0.37508672 <a title="268-lsi-8" href="../hunch_net-2007/hunch_net-2007-02-11-24.html">232 hunch net-2007-02-11-24</a></p>
<p>9 0.37254715 <a title="268-lsi-9" href="../hunch_net-2009/hunch_net-2009-01-19-Netflix_prize_within_epsilon.html">336 hunch net-2009-01-19-Netflix prize within epsilon</a></p>
<p>10 0.37197524 <a title="268-lsi-10" href="../hunch_net-2005/hunch_net-2005-01-26-Watchword%3A_Probability.html">5 hunch net-2005-01-26-Watchword: Probability</a></p>
<p>11 0.36637855 <a title="268-lsi-11" href="../hunch_net-2006/hunch_net-2006-01-13-Benchmarks_for_RL.html">148 hunch net-2006-01-13-Benchmarks for RL</a></p>
<p>12 0.36364368 <a title="268-lsi-12" href="../hunch_net-2009/hunch_net-2009-11-09-NYAS_ML_Symposium_this_year..html">377 hunch net-2009-11-09-NYAS ML Symposium this year.</a></p>
<p>13 0.36232653 <a title="268-lsi-13" href="../hunch_net-2011/hunch_net-2011-08-06-Interesting_thing_at_UAI_2011.html">440 hunch net-2011-08-06-Interesting thing at UAI 2011</a></p>
<p>14 0.3571257 <a title="268-lsi-14" href="../hunch_net-2005/hunch_net-2005-10-16-Complexity%3A_It%26%238217%3Bs_all_in_your_head.html">123 hunch net-2005-10-16-Complexity: It&#8217;s all in your head</a></p>
<p>15 0.35313189 <a title="268-lsi-15" href="../hunch_net-2006/hunch_net-2006-02-07-Pittsburgh_Mind_Reading_Competition.html">155 hunch net-2006-02-07-Pittsburgh Mind Reading Competition</a></p>
<p>16 0.33160439 <a title="268-lsi-16" href="../hunch_net-2009/hunch_net-2009-07-11-Interesting_papers_at_KDD.html">364 hunch net-2009-07-11-Interesting papers at KDD</a></p>
<p>17 0.32587343 <a title="268-lsi-17" href="../hunch_net-2010/hunch_net-2010-03-12-Netflix_Challenge_2_Canceled.html">390 hunch net-2010-03-12-Netflix Challenge 2 Canceled</a></p>
<p>18 0.32226118 <a title="268-lsi-18" href="../hunch_net-2005/hunch_net-2005-10-08-We_have_a_winner.html">119 hunch net-2005-10-08-We have a winner</a></p>
<p>19 0.31988809 <a title="268-lsi-19" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>20 0.31854051 <a title="268-lsi-20" href="../hunch_net-2008/hunch_net-2008-04-12-Blog_compromised.html">294 hunch net-2008-04-12-Blog compromised</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(42, 0.107), (59, 0.7), (74, 0.034)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94164515 <a title="268-lda-1" href="../hunch_net-2007/hunch_net-2007-10-19-Second_Annual_Reinforcement_Learning_Competition.html">268 hunch net-2007-10-19-Second Annual Reinforcement Learning Competition</a></p>
<p>Introduction: The Second Annual Reinforcement Learning Competition is about to get started.
The aim of the competition is to facilitate direct comparisons between various
learning methods on important and realistic domains. This year's event will
feature well-known benchmark domains as well as more challenging problems of
real-world complexity, such as helicopter control and robot soccer
keepaway.The competition begins on November 1st, 2007 when training software
is released. Results must be submitted by July 1st, 2008. The competition will
culminate in an event at ICML-08 in Helsinki, Finland, at which the winners
will be announced.For more information, visitthe competition website.</p><p>2 0.59098864 <a title="268-lda-2" href="../hunch_net-2009/hunch_net-2009-07-11-Interesting_papers_at_KDD.html">364 hunch net-2009-07-11-Interesting papers at KDD</a></p>
<p>Introduction: I attendedKDDthis year. The conference has always had a strong grounding in
what works based on theKDDcup, but it has developed a halo of workshops on
various subjects. It seems that KDD has become a place where the economy meets
machine learning in a stronger sense than many other conferences.There were
several papers that other people might like to take a look at.Yehuda
KorenCollaborative Filtering with Temporal Dynamics. This paper describes how
to incorporate temporal dynamics into a couple of collaborative filtering
approaches. This was also a best paper award.D. Sculley, Robert Malkin,Sugato
Basu,Roberto J. Bayardo,Predicting Bounce Rates in Sponsored Search
Advertisements. The basic claim of this paper is that the probability people
immediately leave ("bounce") after clicking on an advertisement is
predictable.Frank McSherryandIlya MironovDifferentially Private Recommender
Systems: Building Privacy into the Netflix Prize Contenders. The basic claim
here is that it is possible to</p><p>3 0.41741562 <a title="268-lda-3" href="../hunch_net-2006/hunch_net-2006-03-27-Gradients_everywhere.html">167 hunch net-2006-03-27-Gradients everywhere</a></p>
<p>Introduction: One of the basic observations from theatomic learning workshopis that
gradient-based optimization is pervasive. For example, at least 7 (of 12)
speakers used the word 'gradient' in their talk and several others may be
approximating a gradient. The essential useful quality of a gradient is that
it decouples local updates from global optimization. Restated: Given a
gradient, we can determine how to change individual parameters of the system
so as to improve overall performance.It's easy to feel depressed about this
and think "nothing has happened", but that appears untrue. Many of the talks
were about clever techniques for computing gradients where your calculus
textbook breaks down.Sometimes there are clever approximations of the
gradient. (Simon Osindero)Sometimes we can compute constrained gradients via
iterated gradient/project steps. (Ben Taskar)Sometimes we can compute
gradients anyways over mildly nondifferentiable functions. (Drew Bagnell)Even
given a gradient, the choice of upda</p><p>4 0.39501449 <a title="268-lda-4" href="../hunch_net-2007/hunch_net-2007-08-25-The_Privacy_Problem.html">260 hunch net-2007-08-25-The Privacy Problem</a></p>
<p>Introduction: Machine Learning is rising in importance because data is being collected for
all sorts of tasks where it either wasn't previously collected, or for tasks
that did not previously exist. While this is great for Machine Learning, it
has a downside--the massive data collection which is so useful can also lead
to substantial privacy problems.It's important to understand that this is a
much harder problem than many people appreciate. TheAOLdatareleaseis a good
example. To those doing machine learning, the following strategies might be
obvious:Just delete any names or other obviously personally identifiable
information. The logic here seems to be "if I can't easily find the person
then no one can". That doesn't work as demonstrated by the people who were
found circumstantially from the AOL data.… then just hash all the search
terms! The logic here is "if I can't read it, then no one can". It's also
trivially broken by a dictionary attack--just hash all the strings that might
be in the data an</p><p>5 0.15550233 <a title="268-lda-5" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>Introduction: This post is some combination of belaboring the obvious and speculating wildly
about the future. The basic issue to be addressed is how to think about
machine learning in terms given to us from Programming Language theory.Types
and ReductionsJohn's research programme (I feel this should be in British
spelling to reflect the grandiousness of the ideaâ&euro;Ś) of machine learning
reductionsStateOfReductionis at some essential level type-theoretic in nature.
The fundamental elements are the classifier, a function f: alpha -> beta, and
the corresponding classifier trainer g: List of (alpha,beta) -> (alpha ->
beta). The research goal is to create *combinators* that produce new f's and
g's given existing ones. John (probably quite rightly) seems unwilling at the
moment to commit to any notion stronger than these combinators are correctly
typed. One way to see the result of a reduction is something typed like: (For
those denied the joy of the Hindly-Milner type system, "simple" is probably
wildly wr</p><p>6 0.1549779 <a title="268-lda-6" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>7 0.15468752 <a title="268-lda-7" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>8 0.15462731 <a title="268-lda-8" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>9 0.15436743 <a title="268-lda-9" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>10 0.15432763 <a title="268-lda-10" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>11 0.15426449 <a title="268-lda-11" href="../hunch_net-2005/hunch_net-2005-03-21-Research_Styles_in_Machine_Learning.html">44 hunch net-2005-03-21-Research Styles in Machine Learning</a></p>
<p>12 0.15420815 <a title="268-lda-12" href="../hunch_net-2005/hunch_net-2005-07-07-The_Limits_of_Learning_Theory.html">90 hunch net-2005-07-07-The Limits of Learning Theory</a></p>
<p>13 0.15397674 <a title="268-lda-13" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>14 0.15396604 <a title="268-lda-14" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<p>15 0.15346949 <a title="268-lda-15" href="../hunch_net-2005/hunch_net-2005-03-29-Academic_Mechanism_Design.html">48 hunch net-2005-03-29-Academic Mechanism Design</a></p>
<p>16 0.1532611 <a title="268-lda-16" href="../hunch_net-2006/hunch_net-2006-06-15-IJCAI_is_out_of_season.html">184 hunch net-2006-06-15-IJCAI is out of season</a></p>
<p>17 0.15313607 <a title="268-lda-17" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>18 0.15312853 <a title="268-lda-18" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>19 0.15305901 <a title="268-lda-19" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>20 0.15304145 <a title="268-lda-20" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
