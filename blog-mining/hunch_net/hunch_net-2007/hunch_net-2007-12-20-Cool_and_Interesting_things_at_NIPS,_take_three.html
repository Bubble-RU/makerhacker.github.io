<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>280 hunch net-2007-12-20-Cool and Interesting things at NIPS, take three</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2007" href="../home/hunch_net-2007_home.html">hunch_net-2007</a> <a title="hunch_net-2007-280" href="#">hunch_net-2007-280</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>280 hunch net-2007-12-20-Cool and Interesting things at NIPS, take three</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2007-280-html" href="http://hunch.net/?p=308">html</a></p><p>Introduction: Following up on Hal Daume's post and John's post on cool and interesting
things seen at NIPS I'll post my own little list of neat papers here as well.
Of course it's going to be biased towards what I think is interesting. Also, I
have to say that I hadn't been able to see many papers this year at nips due
to myself being too busy, so please feel free to contribute the papers that
you liked1. P. Mudigonda, V. Kolmogorov, P. Torr. An Analysis of Convex
Relaxations for MAP Estimation. A surprising paper which shows that many of
the more sophisticated convex relaxations that had been proposed recently
turns out to be subsumed by the simplest LP relaxation. Be careful next time
you try a cool new convex relaxation!2. D. Sontag, T. Jaakkola. New Outer
Bounds on the Marginal Polytope. The title says it all. The marginal polytope
is the set of local marginal distributions over subsets of variables that are
globally consistent in the sense that there is at least one distribution over
all the va</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Following up on Hal Daume's post and John's post on cool and interesting things seen at NIPS I'll post my own little list of neat papers here as well. [sent-1, score-0.702]
</p><p>2 Also, I have to say that I hadn't been able to see many papers this year at nips due to myself being too busy, so please feel free to contribute the papers that you liked1. [sent-3, score-0.408]
</p><p>3 A surprising paper which shows that many of the more sophisticated convex relaxations that had been proposed recently turns out to be subsumed by the simplest LP relaxation. [sent-9, score-0.501]
</p><p>4 Be careful next time you try a cool new convex relaxation! [sent-10, score-0.332]
</p><p>5 The marginal polytope is the set of local marginal distributions over subsets of variables that are globally consistent in the sense that there is at least one distribution over all the variables consistent with all the local marginal distributions. [sent-17, score-1.942]
</p><p>6 It is an interesting mathematical object to study, and this work builds on the work by Martin Wainwright's upper bounding the log partition function paper, proposing improved outer bounds on the marginal polytope. [sent-18, score-1.161]
</p><p>7 I think there is a little theme going on this year relating approximate inference to convex optimization. [sent-19, score-0.308]
</p><p>8 Besides the above two papers there were some other papers as well. [sent-20, score-0.24]
</p><p>9 A cute idea of how you can construct an experimental set-up such that people act as accept/reject modules in a Metropolis-Hastings framework, so that we can probe what is the prior distribution encoded in people's brains. [sent-26, score-0.36]
</p><p>10 Another surprising result, that in attractive networks, if loopy belief propagation converges, the Bethe free energy is actually a LOWER bound on the log partition function. [sent-33, score-0.773]
</p><p>11 An interesting idea to construct Bayesian networks with infinite number of states, using a pretty complex set-up involving hierarchical Dirichlet processes. [sent-40, score-0.516]
</p><p>12 I am not sure if the software is out, but I think building such general frameworks for nonparametric models is quite useful for many people who want to use such models but don't want to spend too much time coding up the sometimes involved MCMC samplers. [sent-41, score-0.081]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('marginal', 0.365), ('convex', 0.227), ('outer', 0.197), ('bethe', 0.175), ('wainwright', 0.162), ('attractive', 0.162), ('relaxations', 0.153), ('partition', 0.146), ('infinite', 0.14), ('variables', 0.124), ('surprising', 0.121), ('papers', 0.12), ('bounds', 0.119), ('construct', 0.118), ('consistent', 0.116), ('post', 0.106), ('cool', 0.105), ('local', 0.101), ('networks', 0.099), ('propagation', 0.088), ('modules', 0.088), ('sudderth', 0.088), ('globally', 0.088), ('loopy', 0.088), ('variational', 0.088), ('apologies', 0.088), ('bounding', 0.088), ('mcmc', 0.088), ('log', 0.087), ('nips', 0.087), ('kolmogorov', 0.081), ('luis', 0.081), ('relating', 0.081), ('loop', 0.081), ('hierarchical', 0.081), ('neat', 0.081), ('daume', 0.081), ('lp', 0.081), ('relaxation', 0.081), ('guest', 0.081), ('builds', 0.081), ('frameworks', 0.081), ('free', 0.081), ('interesting', 0.078), ('distribution', 0.077), ('chain', 0.077), ('martin', 0.077), ('encoded', 0.077), ('carlo', 0.077), ('monte', 0.077)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000002 <a title="280-tfidf-1" href="../hunch_net-2007/hunch_net-2007-12-20-Cool_and_Interesting_things_at_NIPS%2C_take_three.html">280 hunch net-2007-12-20-Cool and Interesting things at NIPS, take three</a></p>
<p>Introduction: Following up on Hal Daume's post and John's post on cool and interesting
things seen at NIPS I'll post my own little list of neat papers here as well.
Of course it's going to be biased towards what I think is interesting. Also, I
have to say that I hadn't been able to see many papers this year at nips due
to myself being too busy, so please feel free to contribute the papers that
you liked1. P. Mudigonda, V. Kolmogorov, P. Torr. An Analysis of Convex
Relaxations for MAP Estimation. A surprising paper which shows that many of
the more sophisticated convex relaxations that had been proposed recently
turns out to be subsumed by the simplest LP relaxation. Be careful next time
you try a cool new convex relaxation!2. D. Sontag, T. Jaakkola. New Outer
Bounds on the Marginal Polytope. The title says it all. The marginal polytope
is the set of local marginal distributions over subsets of variables that are
globally consistent in the sense that there is at least one distribution over
all the va</p><p>2 0.13461907 <a title="280-tfidf-2" href="../hunch_net-2006/hunch_net-2006-07-05-more_icml_papers.html">189 hunch net-2006-07-05-more icml papers</a></p>
<p>Introduction: Here are a few other papers I enjoyed from ICML06.Topic Models:Dynamic Topic
ModelsDavid Blei, John LaffertyA nice model for how topics in LDA type models
can evolve over time,using a linear dynamical system on the natural parameters
and a veryclever structured variational approximation (in which the mean
fieldparameters are pseudo-observations of a virtual LDS). Like all
Bleipapers, he makes it look easy, but it is extremely impressive.Pachinko
AllocationWei Li, Andrew McCallumA very elegant (but computationally
challenging) model which inducescorrelation amongst topics using a multi-level
DAG whose interior nodesare "super-topics" and "sub-topics" and whose leaves
are thevocabulary words. Makes the slumbering monster of structure learning
stir.Sequence Analysis (I missed these talks since I was chairing another
session)Online Decoding of Markov Models with Latency ConstraintsMukund
Narasimhan, Paul Viola, Michael ShilmanAn "ah-ha!" paper showing how to trade
off latency and decodinga</p><p>3 0.12647621 <a title="280-tfidf-3" href="../hunch_net-2005/hunch_net-2005-05-29-Maximum_Margin_Mismatch%3F.html">77 hunch net-2005-05-29-Maximum Margin Mismatch?</a></p>
<p>Introduction: John makes a fascinating point about structured classification (and slightly
scooped my post!). Maximum Margin Markov Networks (M3N) are an interesting
example of the second class of structured classifiers (where the
classification of one label depends on the others), and one of my favorite
papers. I'm not alone: the paper won the best student paper award at NIPS in
2003.There are some things I find odd about the paper. For instance, it says
of probabilistic models"cannot handle high dimensional feature spaces and lack
strong theoretical guarrantees."I'm aware of no such limitations.
Also:"Unfortunately, even probabilistic graphical models that are trained
discriminatively do not achieve the same level of performance as SVMs,
especially when kernel features are used."This is quite interesting and
contradicts my own experience as well as that of a number of
peopleIgreatlyrespect. I wonder what the root cause is: perhaps there is
something different about the data Ben+Carlos were working</p><p>4 0.11697935 <a title="280-tfidf-4" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>Introduction: Attempts to abstract and study machine learning are within some given
framework or mathematical model. It turns out that all of these models are
significantly flawed for the purpose of studying machine learning. I've
created a table (below) outlining the major flaws in some common models of
machine learning.The point here is not simply "woe unto us". There are several
implications which seem important.The multitude of models is a point of
continuing confusion. It is common for people to learn about machine learning
within one framework which often becomes there "home framework" through which
they attempt to filter all machine learning. (Have you met people who can only
think in terms of kernels? Only via Bayes Law? Only via PAC Learning?)
Explicitly understanding the existence of these other frameworks can help
resolve the confusion. This is particularly important when reviewing and
particularly important for students.Algorithms which conform to multiple
approaches can have substantial</p><p>5 0.108624 <a title="280-tfidf-5" href="../hunch_net-2007/hunch_net-2007-01-02-Retrospective.html">225 hunch net-2007-01-02-Retrospective</a></p>
<p>Introduction: It's been almost two years since this blog began. In that time, I've learned
enough to shift my expectations in several ways.Initially, the idea was for a
general purpose ML blog where different people could contribute posts. What
has actually happened is most posts come from me, with a few guest posts that
I greatly value. There are a few reasons I see for this.Overload. A couple
years ago, I had not fully appreciated just how busy life gets for a
researcher. Making a post is not simply a matter of getting to it, but rather
of prioritizing between {writing a grant, finishing an overdue review, writing
a paper, teaching a class, writing a program, etcâ&euro;Ś}. This is a substantial
transition away from what life as a graduate student is like. At some point
the question is not "when will I get to it?" but rather "will I get to it?"
and the answer starts to become "no" most of the time.Feedback failure. This
blog currently receives about 3K unique visitors per day from about 13K unique
sites p</p><p>6 0.1003397 <a title="280-tfidf-6" href="../hunch_net-2005/hunch_net-2005-03-15-The_State_of_Tight_Bounds.html">41 hunch net-2005-03-15-The State of Tight Bounds</a></p>
<p>7 0.096448153 <a title="280-tfidf-7" href="../hunch_net-2006/hunch_net-2006-05-05-An_ICML_reject.html">177 hunch net-2006-05-05-An ICML reject</a></p>
<p>8 0.095852122 <a title="280-tfidf-8" href="../hunch_net-2006/hunch_net-2006-06-16-Regularization_%3D_Robustness.html">185 hunch net-2006-06-16-Regularization = Robustness</a></p>
<p>9 0.094653502 <a title="280-tfidf-9" href="../hunch_net-2011/hunch_net-2011-07-11-Interesting_Neural_Network_Papers_at_ICML_2011.html">438 hunch net-2011-07-11-Interesting Neural Network Papers at ICML 2011</a></p>
<p>10 0.093526445 <a title="280-tfidf-10" href="../hunch_net-2005/hunch_net-2005-12-14-More_NIPS_Papers_II.html">140 hunch net-2005-12-14-More NIPS Papers II</a></p>
<p>11 0.090827577 <a title="280-tfidf-11" href="../hunch_net-2010/hunch_net-2010-12-26-NIPS_2010.html">420 hunch net-2010-12-26-NIPS 2010</a></p>
<p>12 0.090565428 <a title="280-tfidf-12" href="../hunch_net-2005/hunch_net-2005-12-11-More_NIPS_Papers.html">139 hunch net-2005-12-11-More NIPS Papers</a></p>
<p>13 0.086589016 <a title="280-tfidf-13" href="../hunch_net-2009/hunch_net-2009-06-24-Interesting_papers_at_UAICMOLT_2009.html">361 hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</a></p>
<p>14 0.083437614 <a title="280-tfidf-14" href="../hunch_net-2007/hunch_net-2007-09-18-It%26%238217%3Bs_MDL_Jim%2C_but_not_as_we_know_it%26%238230%3B%28on_Bayes%2C_MDL_and_consistency%29.html">263 hunch net-2007-09-18-It&#8217;s MDL Jim, but not as we know it&#8230;(on Bayes, MDL and consistency)</a></p>
<p>15 0.080945991 <a title="280-tfidf-15" href="../hunch_net-2005/hunch_net-2005-07-23-Interesting_papers_at_ACL.html">97 hunch net-2005-07-23-Interesting papers at ACL</a></p>
<p>16 0.080288947 <a title="280-tfidf-16" href="../hunch_net-2005/hunch_net-2005-02-01-NIPS%3A_Online_Bayes.html">8 hunch net-2005-02-01-NIPS: Online Bayes</a></p>
<p>17 0.078927085 <a title="280-tfidf-17" href="../hunch_net-2012/hunch_net-2012-02-24-ICML%2B50%25.html">456 hunch net-2012-02-24-ICML+50%</a></p>
<p>18 0.077186041 <a title="280-tfidf-18" href="../hunch_net-2005/hunch_net-2005-12-28-Yet_more_nips_thoughts.html">144 hunch net-2005-12-28-Yet more nips thoughts</a></p>
<p>19 0.076978691 <a title="280-tfidf-19" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>20 0.07548508 <a title="280-tfidf-20" href="../hunch_net-2005/hunch_net-2005-06-18-Lower_Bounds_for_Learning_Reductions.html">83 hunch net-2005-06-18-Lower Bounds for Learning Reductions</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.196), (1, -0.003), (2, -0.042), (3, 0.027), (4, -0.052), (5, -0.018), (6, -0.069), (7, -0.165), (8, 0.017), (9, -0.039), (10, 0.031), (11, 0.028), (12, -0.116), (13, 0.044), (14, -0.028), (15, 0.053), (16, 0.144), (17, -0.051), (18, -0.005), (19, -0.006), (20, 0.008), (21, 0.026), (22, -0.046), (23, -0.001), (24, -0.02), (25, -0.03), (26, -0.061), (27, 0.03), (28, 0.047), (29, 0.006), (30, 0.044), (31, -0.017), (32, -0.011), (33, 0.017), (34, 0.034), (35, 0.012), (36, 0.062), (37, -0.036), (38, 0.015), (39, -0.037), (40, 0.074), (41, -0.022), (42, -0.021), (43, 0.012), (44, 0.03), (45, -0.008), (46, 0.01), (47, -0.005), (48, 0.025), (49, -0.0)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96059602 <a title="280-lsi-1" href="../hunch_net-2007/hunch_net-2007-12-20-Cool_and_Interesting_things_at_NIPS%2C_take_three.html">280 hunch net-2007-12-20-Cool and Interesting things at NIPS, take three</a></p>
<p>Introduction: Following up on Hal Daume's post and John's post on cool and interesting
things seen at NIPS I'll post my own little list of neat papers here as well.
Of course it's going to be biased towards what I think is interesting. Also, I
have to say that I hadn't been able to see many papers this year at nips due
to myself being too busy, so please feel free to contribute the papers that
you liked1. P. Mudigonda, V. Kolmogorov, P. Torr. An Analysis of Convex
Relaxations for MAP Estimation. A surprising paper which shows that many of
the more sophisticated convex relaxations that had been proposed recently
turns out to be subsumed by the simplest LP relaxation. Be careful next time
you try a cool new convex relaxation!2. D. Sontag, T. Jaakkola. New Outer
Bounds on the Marginal Polytope. The title says it all. The marginal polytope
is the set of local marginal distributions over subsets of variables that are
globally consistent in the sense that there is at least one distribution over
all the va</p><p>2 0.79090083 <a title="280-lsi-2" href="../hunch_net-2005/hunch_net-2005-12-11-More_NIPS_Papers.html">139 hunch net-2005-12-11-More NIPS Papers</a></p>
<p>Introduction: Let me add to John's post with a few of my own favouritesfrom this year's
conference. First, let me say thatSanjoy's talk,Coarse Sample Complexity
Bounds for ActiveLearningwas also one of my favourites, as was theForgettron
paper.I also really enjoyed the last third ofChristos'talkon the complexity of
finding Nash equilibria.And, speaking of tagging, I thinkthe U.Mass Citeseer
replacement systemRexafrom the demo track is very cool.Finally, let me add my
recommendations for specific papers:Z. Ghahramani, K. Heller:Bayesian Sets[no
preprint](A very elegant probabilistic information retrieval style modelof
which objects are "most like" a given subset of objects.)T. Griffiths, Z.
Ghahramani:Infinite Latent Feature Models andthe Indian Buffet
Process[preprint](A Dirichlet style prior over infinite binary matrices
withbeautiful exchangeability properties.)K. Weinberger, J. Blitzer, L.
Saul:Distance Metric Learning forLarge Margin Nearest Neighbor
Classification[preprint](A nice idea about ho</p><p>3 0.76252472 <a title="280-lsi-3" href="../hunch_net-2005/hunch_net-2005-12-14-More_NIPS_Papers_II.html">140 hunch net-2005-12-14-More NIPS Papers II</a></p>
<p>Introduction: I thought this was a very good NIPS with many excellent papers. The following
are a few NIPS papers which I liked and I hope to study more carefully when I
get the chance. The list is not exhaustive and in no particular
order…Preconditioner Approximations for Probabilistic Graphical
Models.Pradeeep Ravikumar and John Lafferty.I thought the use of
preconditioner methods from solving linear systems in the context of
approximate inference was novel and interesting. The results look good and I'd
like to understand the limitations.Rodeo: Sparse nonparametric regression in
high dimensions.John Lafferty and Larry Wasserman.A very interesting approach
to feature selection in nonparametric regression from a frequentist framework.
The use of lengthscale variables in each dimension reminds me a lot of
'Automatic Relevance Determination' in Gaussian process regression -- it would
be interesting to compare Rodeo to ARD in GPs.Interpolating between types and
tokens by estimating power law generators</p><p>4 0.74160105 <a title="280-lsi-4" href="../hunch_net-2006/hunch_net-2006-07-05-more_icml_papers.html">189 hunch net-2006-07-05-more icml papers</a></p>
<p>Introduction: Here are a few other papers I enjoyed from ICML06.Topic Models:Dynamic Topic
ModelsDavid Blei, John LaffertyA nice model for how topics in LDA type models
can evolve over time,using a linear dynamical system on the natural parameters
and a veryclever structured variational approximation (in which the mean
fieldparameters are pseudo-observations of a virtual LDS). Like all
Bleipapers, he makes it look easy, but it is extremely impressive.Pachinko
AllocationWei Li, Andrew McCallumA very elegant (but computationally
challenging) model which inducescorrelation amongst topics using a multi-level
DAG whose interior nodesare "super-topics" and "sub-topics" and whose leaves
are thevocabulary words. Makes the slumbering monster of structure learning
stir.Sequence Analysis (I missed these talks since I was chairing another
session)Online Decoding of Markov Models with Latency ConstraintsMukund
Narasimhan, Paul Viola, Michael ShilmanAn "ah-ha!" paper showing how to trade
off latency and decodinga</p><p>5 0.72085959 <a title="280-lsi-5" href="../hunch_net-2005/hunch_net-2005-05-29-Maximum_Margin_Mismatch%3F.html">77 hunch net-2005-05-29-Maximum Margin Mismatch?</a></p>
<p>Introduction: John makes a fascinating point about structured classification (and slightly
scooped my post!). Maximum Margin Markov Networks (M3N) are an interesting
example of the second class of structured classifiers (where the
classification of one label depends on the others), and one of my favorite
papers. I'm not alone: the paper won the best student paper award at NIPS in
2003.There are some things I find odd about the paper. For instance, it says
of probabilistic models"cannot handle high dimensional feature spaces and lack
strong theoretical guarrantees."I'm aware of no such limitations.
Also:"Unfortunately, even probabilistic graphical models that are trained
discriminatively do not achieve the same level of performance as SVMs,
especially when kernel features are used."This is quite interesting and
contradicts my own experience as well as that of a number of
peopleIgreatlyrespect. I wonder what the root cause is: perhaps there is
something different about the data Ben+Carlos were working</p><p>6 0.70323884 <a title="280-lsi-6" href="../hunch_net-2005/hunch_net-2005-12-28-Yet_more_nips_thoughts.html">144 hunch net-2005-12-28-Yet more nips thoughts</a></p>
<p>7 0.68164188 <a title="280-lsi-7" href="../hunch_net-2006/hunch_net-2006-06-16-Regularization_%3D_Robustness.html">185 hunch net-2006-06-16-Regularization = Robustness</a></p>
<p>8 0.61496353 <a title="280-lsi-8" href="../hunch_net-2005/hunch_net-2005-07-23-Interesting_papers_at_ACL.html">97 hunch net-2005-07-23-Interesting papers at ACL</a></p>
<p>9 0.60216469 <a title="280-lsi-9" href="../hunch_net-2006/hunch_net-2006-01-23-On_Coding_via_Mutual_Information_%26%23038%3B_Bayes_Nets.html">150 hunch net-2006-01-23-On Coding via Mutual Information &#038; Bayes Nets</a></p>
<p>10 0.58526826 <a title="280-lsi-10" href="../hunch_net-2011/hunch_net-2011-08-06-Interesting_thing_at_UAI_2011.html">440 hunch net-2011-08-06-Interesting thing at UAI 2011</a></p>
<p>11 0.5471791 <a title="280-lsi-11" href="../hunch_net-2006/hunch_net-2006-07-08-Some_recent_papers.html">192 hunch net-2006-07-08-Some recent papers</a></p>
<p>12 0.53416562 <a title="280-lsi-12" href="../hunch_net-2011/hunch_net-2011-07-11-Interesting_Neural_Network_Papers_at_ICML_2011.html">438 hunch net-2011-07-11-Interesting Neural Network Papers at ICML 2011</a></p>
<p>13 0.52779198 <a title="280-lsi-13" href="../hunch_net-2007/hunch_net-2007-09-18-It%26%238217%3Bs_MDL_Jim%2C_but_not_as_we_know_it%26%238230%3B%28on_Bayes%2C_MDL_and_consistency%29.html">263 hunch net-2007-09-18-It&#8217;s MDL Jim, but not as we know it&#8230;(on Bayes, MDL and consistency)</a></p>
<p>14 0.51332396 <a title="280-lsi-14" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>15 0.51086336 <a title="280-lsi-15" href="../hunch_net-2005/hunch_net-2005-02-01-NIPS%3A_Online_Bayes.html">8 hunch net-2005-02-01-NIPS: Online Bayes</a></p>
<p>16 0.50378877 <a title="280-lsi-16" href="../hunch_net-2006/hunch_net-2006-07-26-Two_more_UAI_papers_of_interest.html">199 hunch net-2006-07-26-Two more UAI papers of interest</a></p>
<p>17 0.50335342 <a title="280-lsi-17" href="../hunch_net-2010/hunch_net-2010-05-10-Aggregation_of_estimators%2C_sparsity_in_high_dimension_and_computational_feasibility.html">398 hunch net-2010-05-10-Aggregation of estimators, sparsity in high dimension and computational feasibility</a></p>
<p>18 0.50197095 <a title="280-lsi-18" href="../hunch_net-2011/hunch_net-2011-08-01-Interesting_papers_at_COLT_2011.html">439 hunch net-2011-08-01-Interesting papers at COLT 2011</a></p>
<p>19 0.49315766 <a title="280-lsi-19" href="../hunch_net-2006/hunch_net-2006-06-30-ICML_papers.html">188 hunch net-2006-06-30-ICML papers</a></p>
<p>20 0.48910123 <a title="280-lsi-20" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(6, 0.018), (26, 0.329), (35, 0.113), (42, 0.196), (45, 0.034), (56, 0.021), (68, 0.029), (69, 0.016), (74, 0.081), (95, 0.049), (98, 0.033)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.88406372 <a title="280-lda-1" href="../hunch_net-2007/hunch_net-2007-12-20-Cool_and_Interesting_things_at_NIPS%2C_take_three.html">280 hunch net-2007-12-20-Cool and Interesting things at NIPS, take three</a></p>
<p>Introduction: Following up on Hal Daume's post and John's post on cool and interesting
things seen at NIPS I'll post my own little list of neat papers here as well.
Of course it's going to be biased towards what I think is interesting. Also, I
have to say that I hadn't been able to see many papers this year at nips due
to myself being too busy, so please feel free to contribute the papers that
you liked1. P. Mudigonda, V. Kolmogorov, P. Torr. An Analysis of Convex
Relaxations for MAP Estimation. A surprising paper which shows that many of
the more sophisticated convex relaxations that had been proposed recently
turns out to be subsumed by the simplest LP relaxation. Be careful next time
you try a cool new convex relaxation!2. D. Sontag, T. Jaakkola. New Outer
Bounds on the Marginal Polytope. The title says it all. The marginal polytope
is the set of local marginal distributions over subsets of variables that are
globally consistent in the sense that there is at least one distribution over
all the va</p><p>2 0.7370311 <a title="280-lda-2" href="../hunch_net-2009/hunch_net-2009-12-27-Interesting_things_at_NIPS_2009.html">385 hunch net-2009-12-27-Interesting things at NIPS 2009</a></p>
<p>Introduction: Several papers at NIPS caught my attention.Elad HazanandSatyen Kale,Online
Submodular OptimizationThey define an algorithm for online optimization of
submodular functions with regret guarantees. This places submodular
optimization roughly on par with online convex optimization as tractable
settings for online learning.Elad HazanandSatyen KaleOn Stochastic and Worst-
Case Models of Investing. At it's core, this is yet another example of
modifying worst-case online learning to deal with variance, but the
application to financial models is particularly cool and it seems plausibly
superior other common approaches for financial modeling.Mark Palatucci,Dean
Pomerlau,Tom Mitchell, andGeoff HintonZero Shot Learning with Semantic Output
CodesThe goal here is predicting a label in a multiclass supervised setting
where the label never occurs in the training data. They have some basic
analysis and also a nice application to FMRI brain reading.Shobha
Venkataraman,Avrim Blum,Dawn Song,Subhabrata Sen</p><p>3 0.70494419 <a title="280-lda-3" href="../hunch_net-2005/hunch_net-2005-09-26-Prediction_Bounds_as_the_Mathematics_of_Science.html">115 hunch net-2005-09-26-Prediction Bounds as the Mathematics of Science</a></p>
<p>Introduction: "Science" has many meanings, but one common meaning is "thescientific method"
which is a principled method for investigating the world using the following
steps:Form a hypothesis about the world.Use the hypothesis to make
predictions.Run experiments to confirm or disprove the predictions.The
ordering of these steps is very important to the scientific method. In
particular, predictionsmustbe made before experiments are run.Given that we
all believe in the scientific method of investigation, it may be surprising to
learn that cheating is very common. This happens for many reasons, some
innocent and some not.Drug studies. Pharmaceutical companies make predictions
about the effects of their drugs and then conduct blind clinical studies to
determine their effect. Unfortunately, they have also been caught using some
of the more advanced techniques for cheatinghere: including "reprobleming",
"data set selection", and probably "overfitting by review". It isn't too
surprising to observe this: w</p><p>4 0.67155021 <a title="280-lda-4" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<p>Introduction: Martin Pooland I recently discussed the similarities and differences between
academia and open source programming.Similarities:Cost profileResearch and
programming share approximately the same cost profile: A large upfront effort
is required to produce something useful, and then "anyone" can use it. (The
"anyone" is not quite right for either group because only sufficiently
technical people could use it.)Wealth profileA "wealthy" academic or open
source programmer is someone who has contributed a lot to other people in
research or programs. Much of academia is a "gift culture": whoever gives the
most is most respected.ProblemsBoth academia and open source programming
suffer from similar problems.Whether or not (and which) open source program is
used are perhaps too-often personality driven rather than driven by capability
or usefulness. Similar phenomena can happen in academia with respect to
directions of research.Funding is often a problem for both groups. Academics
often invest many</p><p>5 0.59547824 <a title="280-lda-5" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>Introduction: Yahoo! laid off people. Unlike every previous time there have been layoffs,
this is serious forYahoo! Research.We had advanced warning
fromPrabhakarthrough thesimple act of leaving. Yahoo! Research was a world
class organization that Prabhakar recruited much of personally, so it is
deeply implausible that he would spontaneously decide to leave. My first
thought when I saw the news was "Uhoh,Robsaid that he knew it was serious when
the head of ATnT Research left." In this case it was even more significant,
because Prabhakar recruited me on the premise that Y!R was an experiment in
how research should be done: via a combination of high quality people and high
engagement with the company. Prabhakar's departure is a clear end to that
experiment.The result is ambiguous from a business perspective. Y!R clearly
was not capable of saving the company from its illnesses. I'm not privy to the
internal accounting of impact and this is the kind of subject where there can
easily be great disagreemen</p><p>6 0.58489621 <a title="280-lda-6" href="../hunch_net-2007/hunch_net-2007-01-15-The_Machine_Learning_Department.html">228 hunch net-2007-01-15-The Machine Learning Department</a></p>
<p>7 0.57742041 <a title="280-lda-7" href="../hunch_net-2005/hunch_net-2005-05-21-What_is_the_right_form_of_modularity_in_structured_prediction%3F.html">74 hunch net-2005-05-21-What is the right form of modularity in structured prediction?</a></p>
<p>8 0.57324207 <a title="280-lda-8" href="../hunch_net-2005/hunch_net-2005-06-13-Wikis_for_Summer_Schools_and_Workshops.html">81 hunch net-2005-06-13-Wikis for Summer Schools and Workshops</a></p>
<p>9 0.57214302 <a title="280-lda-9" href="../hunch_net-2005/hunch_net-2005-09-12-Fast_Gradient_Descent.html">111 hunch net-2005-09-12-Fast Gradient Descent</a></p>
<p>10 0.57115424 <a title="280-lda-10" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">23 hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<p>11 0.56818807 <a title="280-lda-11" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>12 0.56653142 <a title="280-lda-12" href="../hunch_net-2006/hunch_net-2006-05-05-An_ICML_reject.html">177 hunch net-2006-05-05-An ICML reject</a></p>
<p>13 0.56361634 <a title="280-lda-13" href="../hunch_net-2009/hunch_net-2009-09-21-Netflix_finishes_%28and_starts%29.html">371 hunch net-2009-09-21-Netflix finishes (and starts)</a></p>
<p>14 0.5632118 <a title="280-lda-14" href="../hunch_net-2006/hunch_net-2006-09-28-Programming_Languages_for_Machine_Learning_Implementations.html">210 hunch net-2006-09-28-Programming Languages for Machine Learning Implementations</a></p>
<p>15 0.56278664 <a title="280-lda-15" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>16 0.5618884 <a title="280-lda-16" href="../hunch_net-2005/hunch_net-2005-12-28-Yet_more_nips_thoughts.html">144 hunch net-2005-12-28-Yet more nips thoughts</a></p>
<p>17 0.56153649 <a title="280-lda-17" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>18 0.56126487 <a title="280-lda-18" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>19 0.56020498 <a title="280-lda-19" href="../hunch_net-2005/hunch_net-2005-03-10-Breaking_Abstractions.html">39 hunch net-2005-03-10-Breaking Abstractions</a></p>
<p>20 0.55969411 <a title="280-lda-20" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
