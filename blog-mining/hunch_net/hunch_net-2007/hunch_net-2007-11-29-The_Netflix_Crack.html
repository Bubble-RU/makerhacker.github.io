<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>275 hunch net-2007-11-29-The Netflix Crack</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2007" href="../home/hunch_net-2007_home.html">hunch_net-2007</a> <a title="hunch_net-2007-275" href="#">hunch_net-2007-275</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>275 hunch net-2007-11-29-The Netflix Crack</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2007-275-html" href="http://hunch.net/?p=303">html</a></p><p>Introduction: A couple security researchersclaim to have cracked the netflix dataset. The
claims of success appear somewhat overstated to me, but the method of attack
is valid and could plausibly be substantially improved so as to reveal the
movie preferences of a small fraction of Netflix users.The basic idea is to
use a heuristic similarity function between ratings in a public database (from
IMDB) and an anonymized database (Netflix) to link ratings in the private
database to public identities (in IMDB). They claim to have linked two of a
few dozen IMDB users to anonymized netflix users.The claims seem a bit
inflated to me, because (a) knowing the IMDB identity isn't equivalent to
knowing the person and (b) the claims of statistical significance are with
respect to a model of the world they created (rather than one they
created).Overall, this is another example showing that completeprivacy is
hard. It may be worth remembering that there are some substantial benefits
from the Netflix challenge as w</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('imdb', 0.513), ('netflix', 0.403), ('database', 0.285), ('claims', 0.264), ('anonymized', 0.228), ('ratings', 0.199), ('knowing', 0.154), ('created', 0.127), ('public', 0.119), ('preferences', 0.114), ('recommending', 0.114), ('linked', 0.105), ('cracked', 0.105), ('identities', 0.105), ('movie', 0.105), ('reveal', 0.105), ('significance', 0.105), ('security', 0.1), ('collaborative', 0.1), ('valid', 0.095)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="275-tfidf-1" href="../hunch_net-2007/hunch_net-2007-11-29-The_Netflix_Crack.html">275 hunch net-2007-11-29-The Netflix Crack</a></p>
<p>Introduction: A couple security researchersclaim to have cracked the netflix dataset. The
claims of success appear somewhat overstated to me, but the method of attack
is valid and could plausibly be substantially improved so as to reveal the
movie preferences of a small fraction of Netflix users.The basic idea is to
use a heuristic similarity function between ratings in a public database (from
IMDB) and an anonymized database (Netflix) to link ratings in the private
database to public identities (in IMDB). They claim to have linked two of a
few dozen IMDB users to anonymized netflix users.The claims seem a bit
inflated to me, because (a) knowing the IMDB identity isn't equivalent to
knowing the person and (b) the claims of statistical significance are with
respect to a model of the world they created (rather than one they
created).Overall, this is another example showing that completeprivacy is
hard. It may be worth remembering that there are some substantial benefits
from the Netflix challenge as w</p><p>2 0.21012963 <a title="275-tfidf-2" href="../hunch_net-2007/hunch_net-2007-04-18-%2450K_Spock_Challenge.html">239 hunch net-2007-04-18-$50K Spock Challenge</a></p>
<p>Introduction: Apparently, the companySpockis setting up a$50k entity resolution challenge.
$50k is much less than the Netflix challenge, but it's effectively the same as
Netflix untilsomeone reaches 10%. It's also nice that the Spock challenge has
a short duration. The (visible) test set is of size 25k and the training set
has size 75k.</p><p>3 0.16112146 <a title="275-tfidf-3" href="../hunch_net-2008/hunch_net-2008-05-23-Three_levels_of_addressing_the_Netflix_Prize.html">301 hunch net-2008-05-23-Three levels of addressing the Netflix Prize</a></p>
<p>Introduction: In October 2006, the online movie renter, Netflix, announced theNetflix
Prizecontest. They published a comprehensive dataset including more than 100
million movie ratings, which were performed by about 480,000 real customers on
17,770 movies.Ã‚ÂCompetitors in the challenge are required to estimate a few
million ratings.Ã‚ÂTo win the "grand prize," they need to deliver a 10%
improvement in the prediction error compared with the results of Cinematch,
Netflix's proprietary recommender system. Best current results deliver
9.12%improvement, which is quite close to the 10% goal, yet painfully
distant.Ã‚ÂThe Netflix Prize breathed new life and excitement into recommender
systems research. The competition allowed the wide research community to
access a large scale, real life dataset. Beyond this, the competition changed
the rules of the game. Claiming that your nice idea could outperform some
mediocre algorithms on some toy dataset is no longer acceptable. Now
researchers should face a new gol</p><p>4 0.14589049 <a title="275-tfidf-4" href="../hunch_net-2009/hunch_net-2009-06-26-Netflix_nearly_done.html">362 hunch net-2009-06-26-Netflix nearly done</a></p>
<p>Introduction: A $1M qualifying result was achieved on thepublic Netflix test setby a3-way
ensemble team. This is just in time forYehuda's presentation atKDD, which I'm
sure will be one of the best attended ever.This isn't quite over--there are a
few days for another super-conglomerate team to come together and there is
some small chance that the performance is nonrepresentative of the final test
set, but I expect not.Regardless of the final outcome, the biggest lesson for
ML from the Netflix contest has been the formidable performance edge of
ensemble methods.</p><p>5 0.12476345 <a title="275-tfidf-5" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">23 hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<p>Introduction: This is apaperby Yann LeCun and Fu Jie Huang published atAISTAT 2005. I found
this paper very difficult to read, but it does have some point about a
computational shortcut.This paper takes for granted that the method of solving
a problem is gradient descent on parameters. Given this assumption, the
question arises: Do you want to do gradient descent on a probabilistic model
or something else?All (conditional) probabilistic models have the formp(y|x) =
f(x,y)/Z(x)whereZ(x) = sumyf(x,y)(the paper calls- log f(x,y)an "energy").
Iffis parameterized by somew, the gradient has a term forZ(x), and hence for
every value ofy. The paper claims, that such models can be optimized for
classification purposes using only the correctyand the othery' not ywhich
maximizesf(x,y). This can even be done on unnormalizable models. The paper
further claims that this can be done with an approximate maximum. These claims
are plausible based on experimental results and intuition.It wouldn't surprise
me to learn</p><p>6 0.11654381 <a title="275-tfidf-6" href="../hunch_net-2009/hunch_net-2009-09-21-Netflix_finishes_%28and_starts%29.html">371 hunch net-2009-09-21-Netflix finishes (and starts)</a></p>
<p>7 0.094596639 <a title="275-tfidf-7" href="../hunch_net-2009/hunch_net-2009-07-11-Interesting_papers_at_KDD.html">364 hunch net-2009-07-11-Interesting papers at KDD</a></p>
<p>8 0.091388069 <a title="275-tfidf-8" href="../hunch_net-2006/hunch_net-2006-10-02-%241M_Netflix_prediction_contest.html">211 hunch net-2006-10-02-$1M Netflix prediction contest</a></p>
<p>9 0.066516146 <a title="275-tfidf-9" href="../hunch_net-2010/hunch_net-2010-02-26-Yahoo%21_ML_events.html">389 hunch net-2010-02-26-Yahoo! ML events</a></p>
<p>10 0.065524377 <a title="275-tfidf-10" href="../hunch_net-2010/hunch_net-2010-03-12-Netflix_Challenge_2_Canceled.html">390 hunch net-2010-03-12-Netflix Challenge 2 Canceled</a></p>
<p>11 0.064629763 <a title="275-tfidf-11" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>12 0.059751406 <a title="275-tfidf-12" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>13 0.058084957 <a title="275-tfidf-13" href="../hunch_net-2006/hunch_net-2006-06-30-ICML_papers.html">188 hunch net-2006-06-30-ICML papers</a></p>
<p>14 0.053688325 <a title="275-tfidf-14" href="../hunch_net-2005/hunch_net-2005-03-28-Open_Problems_for_Colt.html">47 hunch net-2005-03-28-Open Problems for Colt</a></p>
<p>15 0.051081643 <a title="275-tfidf-15" href="../hunch_net-2011/hunch_net-2011-04-11-The_Heritage_Health_Prize.html">430 hunch net-2011-04-11-The Heritage Health Prize</a></p>
<p>16 0.048772827 <a title="275-tfidf-16" href="../hunch_net-2009/hunch_net-2009-05-24-2009_ICML_discussion_site.html">356 hunch net-2009-05-24-2009 ICML discussion site</a></p>
<p>17 0.046010807 <a title="275-tfidf-17" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>18 0.044845589 <a title="275-tfidf-18" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>19 0.044809081 <a title="275-tfidf-19" href="../hunch_net-2005/hunch_net-2005-12-09-Machine_Learning_Thoughts.html">137 hunch net-2005-12-09-Machine Learning Thoughts</a></p>
<p>20 0.044564404 <a title="275-tfidf-20" href="../hunch_net-2007/hunch_net-2007-01-02-Retrospective.html">225 hunch net-2007-01-02-Retrospective</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.085), (1, -0.0), (2, 0.013), (3, -0.009), (4, 0.022), (5, 0.08), (6, 0.017), (7, -0.019), (8, 0.076), (9, -0.026), (10, -0.131), (11, 0.04), (12, -0.095), (13, 0.094), (14, 0.084), (15, 0.054), (16, 0.006), (17, -0.024), (18, -0.117), (19, 0.021), (20, -0.008), (21, 0.072), (22, -0.021), (23, -0.025), (24, -0.076), (25, 0.023), (26, 0.072), (27, 0.096), (28, -0.008), (29, 0.128), (30, 0.035), (31, -0.015), (32, -0.052), (33, -0.006), (34, 0.004), (35, 0.003), (36, -0.122), (37, 0.114), (38, -0.026), (39, 0.077), (40, 0.018), (41, 0.02), (42, -0.014), (43, -0.019), (44, -0.011), (45, 0.013), (46, -0.02), (47, -0.029), (48, -0.029), (49, -0.076)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98793769 <a title="275-lsi-1" href="../hunch_net-2007/hunch_net-2007-11-29-The_Netflix_Crack.html">275 hunch net-2007-11-29-The Netflix Crack</a></p>
<p>Introduction: A couple security researchersclaim to have cracked the netflix dataset. The
claims of success appear somewhat overstated to me, but the method of attack
is valid and could plausibly be substantially improved so as to reveal the
movie preferences of a small fraction of Netflix users.The basic idea is to
use a heuristic similarity function between ratings in a public database (from
IMDB) and an anonymized database (Netflix) to link ratings in the private
database to public identities (in IMDB). They claim to have linked two of a
few dozen IMDB users to anonymized netflix users.The claims seem a bit
inflated to me, because (a) knowing the IMDB identity isn't equivalent to
knowing the person and (b) the claims of statistical significance are with
respect to a model of the world they created (rather than one they
created).Overall, this is another example showing that completeprivacy is
hard. It may be worth remembering that there are some substantial benefits
from the Netflix challenge as w</p><p>2 0.66507721 <a title="275-lsi-2" href="../hunch_net-2007/hunch_net-2007-04-18-%2450K_Spock_Challenge.html">239 hunch net-2007-04-18-$50K Spock Challenge</a></p>
<p>Introduction: Apparently, the companySpockis setting up a$50k entity resolution challenge.
$50k is much less than the Netflix challenge, but it's effectively the same as
Netflix untilsomeone reaches 10%. It's also nice that the Spock challenge has
a short duration. The (visible) test set is of size 25k and the training set
has size 75k.</p><p>3 0.62323165 <a title="275-lsi-3" href="../hunch_net-2008/hunch_net-2008-05-23-Three_levels_of_addressing_the_Netflix_Prize.html">301 hunch net-2008-05-23-Three levels of addressing the Netflix Prize</a></p>
<p>Introduction: In October 2006, the online movie renter, Netflix, announced theNetflix
Prizecontest. They published a comprehensive dataset including more than 100
million movie ratings, which were performed by about 480,000 real customers on
17,770 movies.Ã‚ÂCompetitors in the challenge are required to estimate a few
million ratings.Ã‚ÂTo win the "grand prize," they need to deliver a 10%
improvement in the prediction error compared with the results of Cinematch,
Netflix's proprietary recommender system. Best current results deliver
9.12%improvement, which is quite close to the 10% goal, yet painfully
distant.Ã‚ÂThe Netflix Prize breathed new life and excitement into recommender
systems research. The competition allowed the wide research community to
access a large scale, real life dataset. Beyond this, the competition changed
the rules of the game. Claiming that your nice idea could outperform some
mediocre algorithms on some toy dataset is no longer acceptable. Now
researchers should face a new gol</p><p>4 0.58525556 <a title="275-lsi-4" href="../hunch_net-2009/hunch_net-2009-06-26-Netflix_nearly_done.html">362 hunch net-2009-06-26-Netflix nearly done</a></p>
<p>Introduction: A $1M qualifying result was achieved on thepublic Netflix test setby a3-way
ensemble team. This is just in time forYehuda's presentation atKDD, which I'm
sure will be one of the best attended ever.This isn't quite over--there are a
few days for another super-conglomerate team to come together and there is
some small chance that the performance is nonrepresentative of the final test
set, but I expect not.Regardless of the final outcome, the biggest lesson for
ML from the Netflix contest has been the formidable performance edge of
ensemble methods.</p><p>5 0.54022795 <a title="275-lsi-5" href="../hunch_net-2006/hunch_net-2006-10-02-%241M_Netflix_prediction_contest.html">211 hunch net-2006-10-02-$1M Netflix prediction contest</a></p>
<p>Introduction: Netflix isrunning a contestto improve recommender prediction systems. A 10%
improvement over their current system yields a $1M prize. Failing that, the
best smaller improvement yields a smaller $50K prize. This contest looks quite
real, and the $50K prize money is almost certainly achievable with a bit of
thought. The contest also comes with a dataset which is apparently 2 orders of
magnitude larger than any other public recommendation system datasets.</p><p>6 0.50919628 <a title="275-lsi-6" href="../hunch_net-2010/hunch_net-2010-03-12-Netflix_Challenge_2_Canceled.html">390 hunch net-2010-03-12-Netflix Challenge 2 Canceled</a></p>
<p>7 0.49342075 <a title="275-lsi-7" href="../hunch_net-2009/hunch_net-2009-07-11-Interesting_papers_at_KDD.html">364 hunch net-2009-07-11-Interesting papers at KDD</a></p>
<p>8 0.49187997 <a title="275-lsi-8" href="../hunch_net-2009/hunch_net-2009-09-21-Netflix_finishes_%28and_starts%29.html">371 hunch net-2009-09-21-Netflix finishes (and starts)</a></p>
<p>9 0.47396085 <a title="275-lsi-9" href="../hunch_net-2011/hunch_net-2011-04-11-The_Heritage_Health_Prize.html">430 hunch net-2011-04-11-The Heritage Health Prize</a></p>
<p>10 0.45640776 <a title="275-lsi-10" href="../hunch_net-2005/hunch_net-2005-07-13-Text_Entailment_at_AAAI.html">94 hunch net-2005-07-13-Text Entailment at AAAI</a></p>
<p>11 0.40819722 <a title="275-lsi-11" href="../hunch_net-2009/hunch_net-2009-01-19-Netflix_prize_within_epsilon.html">336 hunch net-2009-01-19-Netflix prize within epsilon</a></p>
<p>12 0.37234741 <a title="275-lsi-12" href="../hunch_net-2005/hunch_net-2005-12-04-Watchword%3A_model.html">135 hunch net-2005-12-04-Watchword: model</a></p>
<p>13 0.34221292 <a title="275-lsi-13" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>14 0.32888511 <a title="275-lsi-14" href="../hunch_net-2010/hunch_net-2010-02-26-Yahoo%21_ML_events.html">389 hunch net-2010-02-26-Yahoo! ML events</a></p>
<p>15 0.3197203 <a title="275-lsi-15" href="../hunch_net-2005/hunch_net-2005-10-08-We_have_a_winner.html">119 hunch net-2005-10-08-We have a winner</a></p>
<p>16 0.31704298 <a title="275-lsi-16" href="../hunch_net-2007/hunch_net-2007-04-28-The_Coming_Patent_Apocalypse.html">241 hunch net-2007-04-28-The Coming Patent Apocalypse</a></p>
<p>17 0.31266633 <a title="275-lsi-17" href="../hunch_net-2011/hunch_net-2011-08-06-Interesting_thing_at_UAI_2011.html">440 hunch net-2011-08-06-Interesting thing at UAI 2011</a></p>
<p>18 0.31240627 <a title="275-lsi-18" href="../hunch_net-2005/hunch_net-2005-07-23-Interesting_papers_at_ACL.html">97 hunch net-2005-07-23-Interesting papers at ACL</a></p>
<p>19 0.31032029 <a title="275-lsi-19" href="../hunch_net-2007/hunch_net-2007-08-25-The_Privacy_Problem.html">260 hunch net-2007-08-25-The Privacy Problem</a></p>
<p>20 0.3041248 <a title="275-lsi-20" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">23 hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.021), (35, 0.019), (42, 0.093), (45, 0.04), (50, 0.581), (68, 0.026), (74, 0.064), (95, 0.025)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96403718 <a title="275-lda-1" href="../hunch_net-2007/hunch_net-2007-11-29-The_Netflix_Crack.html">275 hunch net-2007-11-29-The Netflix Crack</a></p>
<p>Introduction: A couple security researchersclaim to have cracked the netflix dataset. The
claims of success appear somewhat overstated to me, but the method of attack
is valid and could plausibly be substantially improved so as to reveal the
movie preferences of a small fraction of Netflix users.The basic idea is to
use a heuristic similarity function between ratings in a public database (from
IMDB) and an anonymized database (Netflix) to link ratings in the private
database to public identities (in IMDB). They claim to have linked two of a
few dozen IMDB users to anonymized netflix users.The claims seem a bit
inflated to me, because (a) knowing the IMDB identity isn't equivalent to
knowing the person and (b) the claims of statistical significance are with
respect to a model of the world they created (rather than one they
created).Overall, this is another example showing that completeprivacy is
hard. It may be worth remembering that there are some substantial benefits
from the Netflix challenge as w</p><p>2 0.73183477 <a title="275-lda-2" href="../hunch_net-2006/hunch_net-2006-06-25-Presentation_of_Proofs_is_Hard..html">187 hunch net-2006-06-25-Presentation of Proofs is Hard.</a></p>
<p>Introduction: When presenting part of theReinforcement Learning theory tutorialatICML 2006,
I was forcibly reminded of this.There are several difficulties.When creating
the presentation, the correct level of detail is tricky. With too much detail,
the proof takes too much time and people may be lost to boredom. With too
little detail, the steps of the proof involve too-great a jump. This is very
difficult to judge.What may be an easy step in the careful thought of a quiet
room is not so easy when you are occupied by the process of presentation.What
may be easy after having gone over this (and other) proofs is not so easy to
follow in the first pass by a viewer.These problems seem only correctable by
process of repeated test-and-revise.When presenting the proof, simply speaking
with sufficient precision is substantially harder than in normal conversation
(where precision is not so critical). Practice can help here.When presenting
the proof, going at the right pace for understanding is difficult. When</p><p>3 0.7296266 <a title="275-lda-3" href="../hunch_net-2005/hunch_net-2005-12-04-Watchword%3A_model.html">135 hunch net-2005-12-04-Watchword: model</a></p>
<p>Introduction: In everyday use a model is a system which explains the behavior of some
system, hopefully at the level where some alteration of the model predicts
some alteration of the real-world system. In machine learning "model" has
several variant definitions.Everyday. The common definition is sometimes
used.Parameterized. Sometimes model is a short-hand for "parameterized model".
Here, it refers to a model with unspecified free parameters. In the Bayesian
learning approach, you typically have a prior over (everyday)
models.Predictive. Even further from everyday use is the predictive model.
Examples of this are "my model is a decision tree" or "my model is a support
vector machine". Here, there is no real sense in which an SVM explains the
underlying process. For example, an SVM tells us nothing in particular about
how alterations to the real-world system would create a change.Which
definition is being used at any particular time is important information. For
example, if it's a parameterized or p</p><p>4 0.71745718 <a title="275-lda-4" href="../hunch_net-2005/hunch_net-2005-01-31-Watchword%3A_Assumption.html">7 hunch net-2005-01-31-Watchword: Assumption</a></p>
<p>Introduction: "Assumption" is another word to be careful with in machine learning because it
is used in several ways.Assumption = BiasThere are several ways to see that
some form of 'bias' (= preferring of one solution over another) is necessary.
This is obvious in an adversarial setting. A good bit of work has been
expended explaining this in other settings with "no free lunch" theorems. This
is a usage specialized to learning which is particularly common when talking
about priors for Bayesian Learning.Assumption = "if" of a theoremThe
assumptions are the 'if' part of the 'if-then' in a theorem. This is a fairly
common usage.Assumption = AxiomThe assumptions are the things that we assume
are true, but which we cannot verify. Examples are "the IID assumption" or "my
problem is a DNF on a small number of bits". This is the usage which I
prefer.One difficulty with any use of the word "assumption" is that you often
encounter "ifassumptionthenconclusionso ifnot assumptionthennot conclusion".
This is inc</p><p>5 0.67909724 <a title="275-lda-5" href="../hunch_net-2009/hunch_net-2009-08-27-New_York_Area_Machine_Learning_Events.html">369 hunch net-2009-08-27-New York Area Machine Learning Events</a></p>
<p>Introduction: Several events are happening in the NY area.Barriers in Computational Learning
Theory Workshop, Aug 28.That's tomorrow near Princeton. I'm looking forward to
speaking at this one on "Getting around Barriers in Learning Theory", but
several other talks are of interest, particularly to the CS theory
inclined.Claudia Perlichis running theINFORMS Data Mining Contestwith a
deadline of Sept. 25. This is a contest using real health record data (they
partnered withHealthCare Intelligence) to predict transfers and mortality. In
the current US health care reform debate, the case studies of high costs we
hear strongly suggest machine learning & statistics can save many billions.The
Singularity Summit October 3&4\. This is for the AIists out there. Several of
the talks look interesting, although unfortunately I'll miss it
forALT.Predictive Analytics World, Oct 20-21. This is stretching the
definition of "New York Area" a bit, but the train to DC is reasonable. This
is a conference of case studies</p><p>6 0.67006665 <a title="275-lda-6" href="../hunch_net-2005/hunch_net-2005-07-23-Interesting_papers_at_ACL.html">97 hunch net-2005-07-23-Interesting papers at ACL</a></p>
<p>7 0.58910185 <a title="275-lda-7" href="../hunch_net-2008/hunch_net-2008-11-28-A_Bumper_Crop_of_Machine_Learning_Graduates.html">329 hunch net-2008-11-28-A Bumper Crop of Machine Learning Graduates</a></p>
<p>8 0.40030584 <a title="275-lda-8" href="../hunch_net-2007/hunch_net-2007-02-02-Thoughts_regarding_%26%238220%3BIs_machine_learning_different_from_statistics%3F%26%238221%3B.html">230 hunch net-2007-02-02-Thoughts regarding &#8220;Is machine learning different from statistics?&#8221;</a></p>
<p>9 0.3108972 <a title="275-lda-9" href="../hunch_net-2006/hunch_net-2006-05-01-A_conversation_between_Theo_and_Pat.html">176 hunch net-2006-05-01-A conversation between Theo and Pat</a></p>
<p>10 0.29141688 <a title="275-lda-10" href="../hunch_net-2008/hunch_net-2008-05-23-Three_levels_of_addressing_the_Netflix_Prize.html">301 hunch net-2008-05-23-Three levels of addressing the Netflix Prize</a></p>
<p>11 0.28671509 <a title="275-lda-11" href="../hunch_net-2005/hunch_net-2005-06-08-Question%3A_%26%238220%3BWhen_is_the_right_time_to_insert_the_loss_function%3F%26%238221%3B.html">79 hunch net-2005-06-08-Question: &#8220;When is the right time to insert the loss function?&#8221;</a></p>
<p>12 0.28182164 <a title="275-lda-12" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>13 0.27611572 <a title="275-lda-13" href="../hunch_net-2011/hunch_net-2011-07-11-Interesting_Neural_Network_Papers_at_ICML_2011.html">438 hunch net-2011-07-11-Interesting Neural Network Papers at ICML 2011</a></p>
<p>14 0.27399781 <a title="275-lda-14" href="../hunch_net-2005/hunch_net-2005-04-16-Which_Assumptions_are_Reasonable%3F.html">57 hunch net-2005-04-16-Which Assumptions are Reasonable?</a></p>
<p>15 0.27212664 <a title="275-lda-15" href="../hunch_net-2005/hunch_net-2005-09-26-Prediction_Bounds_as_the_Mathematics_of_Science.html">115 hunch net-2005-09-26-Prediction Bounds as the Mathematics of Science</a></p>
<p>16 0.2699734 <a title="275-lda-16" href="../hunch_net-2005/hunch_net-2005-11-02-Progress_in_Active_Learning.html">127 hunch net-2005-11-02-Progress in Active Learning</a></p>
<p>17 0.26576343 <a title="275-lda-17" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>18 0.26003438 <a title="275-lda-18" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>19 0.25585186 <a title="275-lda-19" href="../hunch_net-2006/hunch_net-2006-07-13-Regression_vs._Classification_as_a_Primitive.html">196 hunch net-2006-07-13-Regression vs. Classification as a Primitive</a></p>
<p>20 0.25353047 <a title="275-lda-20" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">23 hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
