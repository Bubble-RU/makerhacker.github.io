<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>257 hunch net-2007-07-28-Asking questions</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2007" href="../home/hunch_net-2007_home.html">hunch_net-2007</a> <a title="hunch_net-2007-257" href="#">hunch_net-2007-257</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>257 hunch net-2007-07-28-Asking questions</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2007-257-html" href="http://hunch.net/?p=285">html</a></p><p>Introduction: There are very substantial differences in how question asking is viewed
culturally. For example, all of the following are common:If no one asks a
question, then no one is paying attention.To ask a question is disrespectful
of the speaker.Asking a question is admitting your own ignorance.The first
view seems to be the right one for research, for several reasons.Research is
quite hard--it's difficult to guess how people won't understand something in
advance while preparing a presentation. Consequently, it's very common to lose
people. No worthwhile presenter wants that.Real understanding is precious. By
asking a question, you are really declaring "I want to understand", and
everyone should respect that.Asking a question wakes you up. I don't mean from
"asleep" to "awake" but from "awake" to "really awake". It's easy to drift
through something sort-of-understanding. When you ask a question, especially
because you are on the spot, you will do much better.Some of these effects
might seem mi</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('awake', 0.45), ('question', 0.39), ('asking', 0.276), ('cultural', 0.222), ('ask', 0.203), ('accumulate', 0.133), ('conscious', 0.133), ('declaring', 0.133), ('drift', 0.133), ('presenter', 0.133), ('spot', 0.123), ('accepts', 0.123), ('preparing', 0.123), ('speaker', 0.123), ('paying', 0.116), ('asks', 0.111), ('admitting', 0.107), ('guess', 0.103), ('wants', 0.1), ('really', 0.098)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="257-tfidf-1" href="../hunch_net-2007/hunch_net-2007-07-28-Asking_questions.html">257 hunch net-2007-07-28-Asking questions</a></p>
<p>Introduction: There are very substantial differences in how question asking is viewed
culturally. For example, all of the following are common:If no one asks a
question, then no one is paying attention.To ask a question is disrespectful
of the speaker.Asking a question is admitting your own ignorance.The first
view seems to be the right one for research, for several reasons.Research is
quite hard--it's difficult to guess how people won't understand something in
advance while preparing a presentation. Consequently, it's very common to lose
people. No worthwhile presenter wants that.Real understanding is precious. By
asking a question, you are really declaring "I want to understand", and
everyone should respect that.Asking a question wakes you up. I don't mean from
"asleep" to "awake" but from "awake" to "really awake". It's easy to drift
through something sort-of-understanding. When you ask a question, especially
because you are on the spot, you will do much better.Some of these effects
might seem mi</p><p>2 0.11084855 <a title="257-tfidf-2" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>Introduction: A little over 4 years ago,Sanjoymade a postsaying roughly "we should study
active learning theoretically, because not much is understood".At the time, we
did not understand basic things such as whether or not it was possible to PAC-
learn with an active algorithm without making strong assumptions about the
noise rate. In other words, the fundamental question was "can we do it?"The
nature of the question has fundamentally changed in my mind. The answer is to
the previous question is "yes", both information theoretically and
computationally, most places where supervised learning could be applied.In
many situation, the question has now changed to: "is it worth it?" Is the
programming and computational overhead low enough to make the label cost
savings of active learning worthwhile? Currently, there are situations where
this question could go either way. Much of the challenge for the future is in
figuring out how to make active learning easier or more worthwhile.At
theactive learning tutor</p><p>3 0.10317703 <a title="257-tfidf-3" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>Introduction: Normally, I don't indulge in posters forICML, but this year is naturally an
exception for me. If you want one, there are a small numberleft here, if you
sign up before February.It also seems worthwhile to give some sense of the
scope and reviewing criteria for ICML for authors considering submitting
papers. At ICML, the (very large) program committee does the reviewing which
informs final decisions by area chairs on most papers. Program chairs setup
the process, deal with exceptions or disagreements, and provide advice for the
reviewing process. Providing advice is tricky (and easily misleading) because
a conference is a community, and in the end the aggregate interests of the
community determine the conference. Nevertheless, as a program chair this year
it seems worthwhile to state the overall philosophy I have and what I plan to
encourage (and occasionally discourage).At the highest level, I believe ICML
exists to further research into machine learning, which I generally think of
as</p><p>4 0.10110172 <a title="257-tfidf-4" href="../hunch_net-2011/hunch_net-2011-02-17-What_does_Watson_mean%3F.html">424 hunch net-2011-02-17-What does Watson mean?</a></p>
<p>Introduction: Watsonconvincingly beat the best championJeopardy!players. The apparent
significance of this varies hugely, depending on your background knowledge
about the related machine learning, NLP, and search technology. For a random
person, this might seem evidence of serious machine intelligence, while for
people working on the system itself, it probably seems like a reasonably good
assemblage of existing technologies with several twists to make the entire
system work.Above all, I think we should congratulate the people who managed
to put together and execute this project--many years of effort by a diverse
set of highly skilled people were needed to make this happen. In academia,
it's pretty difficult for one professor to assemble that quantity of talent,
and in industry it's rarely the case that such a capable group has both a
worthwhile project and the support needed to pursue something like this for
several years before success.Alinainvited me to the Jeopardy watching party
atIBM, which was</p><p>5 0.096458413 <a title="257-tfidf-5" href="../hunch_net-2005/hunch_net-2005-01-24-Holy_grails_of_machine_learning%3F.html">2 hunch net-2005-01-24-Holy grails of machine learning?</a></p>
<p>Introduction: Let me kick things off by posing this question to ML researchers:What do you
think are some important holy grails of machine learning?For example:- "A
classifier with SVM-level performance but much more scalable"- "Practical
confidence bounds (or learning bounds) for classification"- "A reinforcement
learning algorithm that can handle the ___ problem"- "Understanding
theoretically why ___ works so well in practice"etc.I pose this question
because I believe that when goals are stated explicitly and well (thus
providing clarity as well as opening up the problems to more people), rather
than left implicit, they are likely to be achieved much more quickly. I would
also like to know more about the internal goals of the various machine
learning sub-areas (theory, kernel methods, graphical models, reinforcement
learning, etc) as stated by people in these respective areas. This could help
people cross sub-areas.</p><p>6 0.087984927 <a title="257-tfidf-6" href="../hunch_net-2005/hunch_net-2005-03-13-Avoiding_Bad_Reviewing.html">40 hunch net-2005-03-13-Avoiding Bad Reviewing</a></p>
<p>7 0.076575123 <a title="257-tfidf-7" href="../hunch_net-2007/hunch_net-2007-08-25-The_Privacy_Problem.html">260 hunch net-2007-08-25-The Privacy Problem</a></p>
<p>8 0.075503185 <a title="257-tfidf-8" href="../hunch_net-2008/hunch_net-2008-04-12-It_Doesn%26%238217%3Bt_Stop.html">295 hunch net-2008-04-12-It Doesn&#8217;t Stop</a></p>
<p>9 0.074110359 <a title="257-tfidf-9" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>10 0.07163921 <a title="257-tfidf-10" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>11 0.069587044 <a title="257-tfidf-11" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>12 0.066384353 <a title="257-tfidf-12" href="../hunch_net-2007/hunch_net-2007-06-21-Presentation_Preparation.html">249 hunch net-2007-06-21-Presentation Preparation</a></p>
<p>13 0.065014474 <a title="257-tfidf-13" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>14 0.06379354 <a title="257-tfidf-14" href="../hunch_net-2006/hunch_net-2006-01-18-Is_Multitask_Learning_Black-Boxable%3F.html">149 hunch net-2006-01-18-Is Multitask Learning Black-Boxable?</a></p>
<p>15 0.06349986 <a title="257-tfidf-15" href="../hunch_net-2007/hunch_net-2007-01-15-The_Machine_Learning_Department.html">228 hunch net-2007-01-15-The Machine Learning Department</a></p>
<p>16 0.062847584 <a title="257-tfidf-16" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>17 0.061358117 <a title="257-tfidf-17" href="../hunch_net-2008/hunch_net-2008-08-24-Mass_Customized_Medicine_in_the_Future%3F.html">314 hunch net-2008-08-24-Mass Customized Medicine in the Future?</a></p>
<p>18 0.060710613 <a title="257-tfidf-18" href="../hunch_net-2010/hunch_net-2010-04-26-Compassionate_Reviewing.html">395 hunch net-2010-04-26-Compassionate Reviewing</a></p>
<p>19 0.060511351 <a title="257-tfidf-19" href="../hunch_net-2006/hunch_net-2006-01-25-1_year.html">151 hunch net-2006-01-25-1 year</a></p>
<p>20 0.058822721 <a title="257-tfidf-20" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.148), (1, 0.016), (2, 0.039), (3, -0.081), (4, 0.047), (5, 0.017), (6, -0.02), (7, 0.001), (8, 0.004), (9, 0.021), (10, -0.006), (11, 0.023), (12, 0.016), (13, -0.012), (14, -0.021), (15, 0.003), (16, 0.005), (17, 0.011), (18, 0.036), (19, -0.002), (20, 0.043), (21, 0.084), (22, -0.006), (23, 0.048), (24, 0.022), (25, -0.015), (26, -0.046), (27, -0.054), (28, -0.003), (29, 0.041), (30, -0.005), (31, 0.007), (32, -0.089), (33, 0.024), (34, 0.008), (35, -0.007), (36, -0.039), (37, -0.062), (38, -0.119), (39, 0.022), (40, 0.001), (41, -0.046), (42, -0.039), (43, 0.031), (44, -0.094), (45, 0.05), (46, 0.059), (47, -0.02), (48, 0.034), (49, 0.088)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9808653 <a title="257-lsi-1" href="../hunch_net-2007/hunch_net-2007-07-28-Asking_questions.html">257 hunch net-2007-07-28-Asking questions</a></p>
<p>Introduction: There are very substantial differences in how question asking is viewed
culturally. For example, all of the following are common:If no one asks a
question, then no one is paying attention.To ask a question is disrespectful
of the speaker.Asking a question is admitting your own ignorance.The first
view seems to be the right one for research, for several reasons.Research is
quite hard--it's difficult to guess how people won't understand something in
advance while preparing a presentation. Consequently, it's very common to lose
people. No worthwhile presenter wants that.Real understanding is precious. By
asking a question, you are really declaring "I want to understand", and
everyone should respect that.Asking a question wakes you up. I don't mean from
"asleep" to "awake" but from "awake" to "really awake". It's easy to drift
through something sort-of-understanding. When you ask a question, especially
because you are on the spot, you will do much better.Some of these effects
might seem mi</p><p>2 0.63234031 <a title="257-lsi-2" href="../hunch_net-2011/hunch_net-2011-02-17-What_does_Watson_mean%3F.html">424 hunch net-2011-02-17-What does Watson mean?</a></p>
<p>Introduction: Watsonconvincingly beat the best championJeopardy!players. The apparent
significance of this varies hugely, depending on your background knowledge
about the related machine learning, NLP, and search technology. For a random
person, this might seem evidence of serious machine intelligence, while for
people working on the system itself, it probably seems like a reasonably good
assemblage of existing technologies with several twists to make the entire
system work.Above all, I think we should congratulate the people who managed
to put together and execute this project--many years of effort by a diverse
set of highly skilled people were needed to make this happen. In academia,
it's pretty difficult for one professor to assemble that quantity of talent,
and in industry it's rarely the case that such a capable group has both a
worthwhile project and the support needed to pursue something like this for
several years before success.Alinainvited me to the Jeopardy watching party
atIBM, which was</p><p>3 0.55887651 <a title="257-lsi-3" href="../hunch_net-2011/hunch_net-2011-11-26-Giving_Thanks.html">449 hunch net-2011-11-26-Giving Thanks</a></p>
<p>Introduction: Thanksgiving is perhaps my favorite holiday, because pausing your life and
giving thanks provides a needed moment of perspective.As a researcher, I am
most thankful for my education, without which I could not function. I want to
share this, because it provides some sense of how a researcher starts.My long
term memory seems to function particularly well, which makes any education I
get is particularly useful.I am naturally obsessive, which makes me chase down
details until I fully understand things. Natural obsessiveness can go wrong,
of course, but it's a great ally when you absolutely must get things right.My
childhood was all in one hometown, which was a conscious sacrifice on the part
of my father, implying disruptions from moving around were eliminated. I'm not
sure how important this was since travel has it's own benefits, but it bears
thought.I had several great teachers in grade school, and naturally gravitated
towards teachers over classmates, as they seemed more interesting. I</p><p>4 0.5429346 <a title="257-lsi-4" href="../hunch_net-2006/hunch_net-2006-02-02-Introspectionism_as_a_Disease.html">153 hunch net-2006-02-02-Introspectionism as a Disease</a></p>
<p>Introduction: In the AI-related parts of machine learning, it is often tempting to examine
howyoudo things in order to imagine how a machine should do things. This is
introspection, and it can easily go awry. I will call introspection gone awry
introspectionism.Introspectionism is almost unique to AI (and the AI-related
parts of machine learning) and it can lead to huge wasted effort in research.
It's easiest to show how introspectionism arises by an example.Suppose we want
to solve the problem of navigating a robot from point A to point B given a
camera. Then, the following research action plan might seem natural when you
examine your own capabilities:Build an edge detector for still images.Build an
object recognition system given the edge detector.Build a system to predict
distance and orientation to objects given the object recognition system.Build
a system to plan a path through the scene you construct from {object
identification, distance, orientation} predictions.As you execute the above,
cons</p><p>5 0.53268379 <a title="257-lsi-5" href="../hunch_net-2006/hunch_net-2006-12-05-Recruitment_Conferences.html">222 hunch net-2006-12-05-Recruitment Conferences</a></p>
<p>Introduction: One of the subsidiary roles of conferences is recruitment.NIPSis optimally
placed in time for this because it falls right before the major recruitment
season.I personally found job hunting embarrassing, and was relatively inept
at it. I expect this is true of many people, because it is not something done
often.The basic rule is: make the plausible hirers aware of your interest.
Anycorporate sponsoris a "plausible", regardless of whether or not there is a
booth.CRAand theacm job centerare other reasonable sources.There are
substantial differences between the different possibilities. Putting some
effort into understanding the distinctions is a good idea, although you should
always remember where the other person is coming from.</p><p>6 0.5320648 <a title="257-lsi-6" href="../hunch_net-2007/hunch_net-2007-02-10-Best_Practices_for_Collaboration.html">231 hunch net-2007-02-10-Best Practices for Collaboration</a></p>
<p>7 0.52634919 <a title="257-lsi-7" href="../hunch_net-2005/hunch_net-2005-05-12-Math_on_the_Web.html">70 hunch net-2005-05-12-Math on the Web</a></p>
<p>8 0.52399951 <a title="257-lsi-8" href="../hunch_net-2010/hunch_net-2010-07-02-MetaOptimize.html">402 hunch net-2010-07-02-MetaOptimize</a></p>
<p>9 0.52216029 <a title="257-lsi-9" href="../hunch_net-2005/hunch_net-2005-05-17-A_Short_Guide_to_PhD_Graduate_Study.html">73 hunch net-2005-05-17-A Short Guide to PhD Graduate Study</a></p>
<p>10 0.50399983 <a title="257-lsi-10" href="../hunch_net-2006/hunch_net-2006-06-25-Presentation_of_Proofs_is_Hard..html">187 hunch net-2006-06-25-Presentation of Proofs is Hard.</a></p>
<p>11 0.49985504 <a title="257-lsi-11" href="../hunch_net-2005/hunch_net-2005-02-25-Solution%3A_Reinforcement_Learning_with_Classification.html">29 hunch net-2005-02-25-Solution: Reinforcement Learning with Classification</a></p>
<p>12 0.49917382 <a title="257-lsi-12" href="../hunch_net-2005/hunch_net-2005-07-10-Thinking_the_Unthought.html">91 hunch net-2005-07-10-Thinking the Unthought</a></p>
<p>13 0.48152298 <a title="257-lsi-13" href="../hunch_net-2010/hunch_net-2010-05-02-What%26%238217%3Bs_the_difference_between_gambling_and_rewarding_good_prediction%3F.html">397 hunch net-2010-05-02-What&#8217;s the difference between gambling and rewarding good prediction?</a></p>
<p>14 0.48009789 <a title="257-lsi-14" href="../hunch_net-2005/hunch_net-2005-07-07-The_Limits_of_Learning_Theory.html">90 hunch net-2005-07-07-The Limits of Learning Theory</a></p>
<p>15 0.47732788 <a title="257-lsi-15" href="../hunch_net-2006/hunch_net-2006-03-17-Multitask_learning_is_Black-Boxable.html">164 hunch net-2006-03-17-Multitask learning is Black-Boxable</a></p>
<p>16 0.47506374 <a title="257-lsi-16" href="../hunch_net-2006/hunch_net-2006-01-18-Is_Multitask_Learning_Black-Boxable%3F.html">149 hunch net-2006-01-18-Is Multitask Learning Black-Boxable?</a></p>
<p>17 0.47427824 <a title="257-lsi-17" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>18 0.46743277 <a title="257-lsi-18" href="../hunch_net-2005/hunch_net-2005-05-29-Bad_ideas.html">76 hunch net-2005-05-29-Bad ideas</a></p>
<p>19 0.46609271 <a title="257-lsi-19" href="../hunch_net-2008/hunch_net-2008-04-12-It_Doesn%26%238217%3Bt_Stop.html">295 hunch net-2008-04-12-It Doesn&#8217;t Stop</a></p>
<p>20 0.4622491 <a title="257-lsi-20" href="../hunch_net-2009/hunch_net-2009-04-02-Asymmophobia.html">348 hunch net-2009-04-02-Asymmophobia</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(11, 0.364), (35, 0.07), (42, 0.271), (68, 0.016), (74, 0.123), (95, 0.042)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.86771804 <a title="257-lda-1" href="../hunch_net-2007/hunch_net-2007-07-28-Asking_questions.html">257 hunch net-2007-07-28-Asking questions</a></p>
<p>Introduction: There are very substantial differences in how question asking is viewed
culturally. For example, all of the following are common:If no one asks a
question, then no one is paying attention.To ask a question is disrespectful
of the speaker.Asking a question is admitting your own ignorance.The first
view seems to be the right one for research, for several reasons.Research is
quite hard--it's difficult to guess how people won't understand something in
advance while preparing a presentation. Consequently, it's very common to lose
people. No worthwhile presenter wants that.Real understanding is precious. By
asking a question, you are really declaring "I want to understand", and
everyone should respect that.Asking a question wakes you up. I don't mean from
"asleep" to "awake" but from "awake" to "really awake". It's easy to drift
through something sort-of-understanding. When you ask a question, especially
because you are on the spot, you will do much better.Some of these effects
might seem mi</p><p>2 0.86325622 <a title="257-lda-2" href="../hunch_net-2013/hunch_net-2013-08-31-Extreme_Classification_workshop_at_NIPS.html">488 hunch net-2013-08-31-Extreme Classification workshop at NIPS</a></p>
<p>Introduction: Manikand I are organizing theextreme classificationworkshop at NIPS this year.
We have a number of good speakers lined up, but I would further encourage
anyone working in the area to submit an abstract by October 9. I believe this
is an idea whose time has now come.The NIPS website doesn't have other
workshops listed yet, but I expect several others to be of significant
interest.</p><p>3 0.77342296 <a title="257-lda-3" href="../hunch_net-2009/hunch_net-2009-06-01-Multitask_Poisoning.html">358 hunch net-2009-06-01-Multitask Poisoning</a></p>
<p>Introduction: There are many ways that interesting research gets done. For example it's
common at a conference for someone to discuss a problem with a partial
solution, and for someone else to know how to solve a piece of it, resulting
in a paper. In some sense, these are the easiest results we can achieve, so we
should ask: Can all research be this easy?The answer is certainly no for
fields where research inherently requires experimentation to discover how the
real world works. However, mathematics, including parts of physics, computer
science, statistics, etcâ&euro;Ś which are effectively mathematics don't require
experimentation. In effect, a paper can be simply a pure expression of
thinking. Can all mathematical-style research be this easy?What's going on
here is research-by-communication. Someone knows something, someone knows
something else, and as soon as someone knows both things, a problem is solved.
The interesting thing about research-by-communication is that it is becoming
radically easier with</p><p>4 0.66439539 <a title="257-lda-4" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>Introduction: A little over 4 years ago,Sanjoymade a postsaying roughly "we should study
active learning theoretically, because not much is understood".At the time, we
did not understand basic things such as whether or not it was possible to PAC-
learn with an active algorithm without making strong assumptions about the
noise rate. In other words, the fundamental question was "can we do it?"The
nature of the question has fundamentally changed in my mind. The answer is to
the previous question is "yes", both information theoretically and
computationally, most places where supervised learning could be applied.In
many situation, the question has now changed to: "is it worth it?" Is the
programming and computational overhead low enough to make the label cost
savings of active learning worthwhile? Currently, there are situations where
this question could go either way. Much of the challenge for the future is in
figuring out how to make active learning easier or more worthwhile.At
theactive learning tutor</p><p>5 0.63538903 <a title="257-lda-5" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>Introduction: The 2006 Machine Learning Summer School in Taipei, Taiwan ended on August 4,
2006. It has been a very exciting two weeks for a record crowd of 245
participants (including speakers and organizers) from 18 countries. We had a
lineup of speakers that is hard to match up for other similar events (see
ourWIKIfor more information). With this lineup, it is difficult for us as
organizers to screw it up too bad. Also, since we have pretty good
infrastructure for international meetings and experienced staff at NTUST and
Academia Sinica, plus the reputation established by previous MLSS series, it
was relatively easy for us to attract registrations and simply enjoyed this
two-week long party of machine learning.In the end of MLSS we distributed a
survey form for participants to fill in. I will report what we found from this
survey, together with the registration data and word-of-mouth from
participants.The first question is designed to find out how our participants
learned about MLSS 2006 Taipei.</p><p>6 0.63522446 <a title="257-lda-6" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>7 0.62837791 <a title="257-lda-7" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>8 0.62813693 <a title="257-lda-8" href="../hunch_net-2006/hunch_net-2006-05-05-An_ICML_reject.html">177 hunch net-2006-05-05-An ICML reject</a></p>
<p>9 0.62739748 <a title="257-lda-9" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>10 0.62683016 <a title="257-lda-10" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>11 0.62640488 <a title="257-lda-11" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>12 0.62597221 <a title="257-lda-12" href="../hunch_net-2005/hunch_net-2005-03-17-Going_all_the_Way%2C_Sometimes.html">42 hunch net-2005-03-17-Going all the Way, Sometimes</a></p>
<p>13 0.62583816 <a title="257-lda-13" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>14 0.62547511 <a title="257-lda-14" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">23 hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<p>15 0.62525004 <a title="257-lda-15" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>16 0.62487262 <a title="257-lda-16" href="../hunch_net-2013/hunch_net-2013-12-01-NIPS_tutorials_and_Vowpal_Wabbit_7.4.html">492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</a></p>
<p>17 0.62458485 <a title="257-lda-17" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>18 0.62426674 <a title="257-lda-18" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>19 0.62308872 <a title="257-lda-19" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>20 0.62284744 <a title="257-lda-20" href="../hunch_net-2007/hunch_net-2007-01-02-Retrospective.html">225 hunch net-2007-01-02-Retrospective</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
