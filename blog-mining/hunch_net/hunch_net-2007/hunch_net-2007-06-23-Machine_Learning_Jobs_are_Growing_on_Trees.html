<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>250 hunch net-2007-06-23-Machine Learning Jobs are Growing on Trees</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2007" href="../home/hunch_net-2007_home.html">hunch_net-2007</a> <a title="hunch_net-2007-250" href="#">hunch_net-2007-250</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>250 hunch net-2007-06-23-Machine Learning Jobs are Growing on Trees</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2007-250-html" href="http://hunch.net/?p=275">html</a></p><p>Introduction: The consensus of several discussions at ICML is that the number of jobs for people knowing machine learning well substantially exceeds supply.  This is my experience as well.  Demand comes from many places, but I’ve seen particularly strong demand from trading companies and internet startups.
 
Like all interest bursts, this one will probably pass because of economic recession or other distractions.  Nevertheless, the general outlook for machine learning in business seems to be good.  Machine learning is all about optimization when there is uncertainty and lots of data.  The quantity of data available is growing quickly as computer-run processes and sensors become more common, and the quality of the data is dropping since there is little editorial control in it’s collection.  Machine Learning is a difficult subject to master (*), so those who do should remain in demand over the long term.
 
(*) In fact, it would be reasonable to claim that no one has mastered it—there are just some peo</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The consensus of several discussions at ICML is that the number of jobs for people knowing machine learning well substantially exceeds supply. [sent-1, score-0.984]
</p><p>2 Demand comes from many places, but I’ve seen particularly strong demand from trading companies and internet startups. [sent-3, score-1.141]
</p><p>3 Like all interest bursts, this one will probably pass because of economic recession or other distractions. [sent-4, score-0.655]
</p><p>4 Nevertheless, the general outlook for machine learning in business seems to be good. [sent-5, score-0.264]
</p><p>5 Machine learning is all about optimization when there is uncertainty and lots of data. [sent-6, score-0.375]
</p><p>6 The quantity of data available is growing quickly as computer-run processes and sensors become more common, and the quality of the data is dropping since there is little editorial control in it’s collection. [sent-7, score-1.779]
</p><p>7 Machine Learning is a difficult subject to master (*), so those who do should remain in demand over the long term. [sent-8, score-0.928]
</p><p>8 (*) In fact, it would be reasonable to claim that no one has mastered it—there are just some people who know a bit more than others. [sent-9, score-0.35]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('demand', 0.425), ('editorial', 0.205), ('dropping', 0.205), ('sensors', 0.19), ('recession', 0.19), ('consensus', 0.179), ('mastered', 0.179), ('exceeds', 0.171), ('jobs', 0.159), ('trading', 0.154), ('economic', 0.154), ('business', 0.145), ('knowing', 0.138), ('pass', 0.138), ('uncertainty', 0.138), ('master', 0.138), ('lots', 0.138), ('quantity', 0.135), ('discussions', 0.133), ('companies', 0.13), ('remain', 0.13), ('control', 0.128), ('processes', 0.123), ('places', 0.123), ('quickly', 0.123), ('growing', 0.121), ('machine', 0.119), ('internet', 0.108), ('data', 0.105), ('claim', 0.104), ('optimization', 0.099), ('fact', 0.099), ('subject', 0.096), ('probably', 0.096), ('quality', 0.093), ('available', 0.089), ('comes', 0.089), ('seen', 0.088), ('nevertheless', 0.087), ('become', 0.085), ('substantially', 0.085), ('strong', 0.081), ('experience', 0.081), ('interest', 0.077), ('others', 0.077), ('long', 0.073), ('little', 0.072), ('reasonable', 0.067), ('particularly', 0.066), ('difficult', 0.066)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="250-tfidf-1" href="../hunch_net-2007/hunch_net-2007-06-23-Machine_Learning_Jobs_are_Growing_on_Trees.html">250 hunch net-2007-06-23-Machine Learning Jobs are Growing on Trees</a></p>
<p>Introduction: The consensus of several discussions at ICML is that the number of jobs for people knowing machine learning well substantially exceeds supply.  This is my experience as well.  Demand comes from many places, but I’ve seen particularly strong demand from trading companies and internet startups.
 
Like all interest bursts, this one will probably pass because of economic recession or other distractions.  Nevertheless, the general outlook for machine learning in business seems to be good.  Machine learning is all about optimization when there is uncertainty and lots of data.  The quantity of data available is growing quickly as computer-run processes and sensors become more common, and the quality of the data is dropping since there is little editorial control in it’s collection.  Machine Learning is a difficult subject to master (*), so those who do should remain in demand over the long term.
 
(*) In fact, it would be reasonable to claim that no one has mastered it—there are just some peo</p><p>2 0.11185073 <a title="250-tfidf-2" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<p>Introduction: I recently had fun discussions with both  Vikash Mansinghka  and  Thomas Breuel  about approaching AI with machine learning.  The general interest in taking a crack at AI with machine learning seems to be rising on many fronts including  DARPA .
 
As a matter of history, there was a great deal of interest in AI which died down before I began research.  There remain many projects and conferences spawned in this earlier AI wave, as well as a good bit of experience about what did not work, or at least did not work yet.  Here are a few examples of failure modes that people seem to run into:
  
  Supply/Product confusion .  Sometimes we think “Intelligences use X, so I’ll create X and have an Intelligence.”  An example of this is the  Cyc Project  which inspires some people as “intelligences use ontologies, so I’ll create an ontology and a system using it to have an Intelligence.”  The flaw here is that Intelligences  create  ontologies, which they use, and without the ability to create ont</p><p>3 0.099904396 <a title="250-tfidf-3" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>Introduction: Unfortunately, I ended up sick for much of this ICML.  I did manage to catch one interesting paper:
 
 Richard Socher ,  Cliff Lin ,  Andrew Y. Ng , and  Christopher D. Manning   Parsing Natural Scenes and Natural Language with Recursive Neural Networks .
 
I invited Richard to share his list of interesting papers, so hopefully we’ll hear from him soon.  In the meantime,  Paul  and  Hal  have posted some lists.
 
 the future 
 
 Joelle  and I are program chairs for ICML 2012 in  Edinburgh , which I previously enjoyed visiting in  2005 .  This is a huge responsibility, that we hope to accomplish well.  A part of this (perhaps the most fun part), is imagining how we can make ICML better.  A key and critical constraint is choosing things that can be accomplished.  So far we have:
  
  Colocation . The first thing we looked into was potential colocations.  We quickly discovered that many other conferences precomitted their location.  For the future, getting a colocation with  ACL  or  SIGI</p><p>4 0.092260152 <a title="250-tfidf-4" href="../hunch_net-2009/hunch_net-2009-02-22-Effective_Research_Funding.html">344 hunch net-2009-02-22-Effective Research Funding</a></p>
<p>Introduction: With a worldwide recession on, my impression is that the carnage in research has not been as severe as might be feared, at least in the United States.  I know of two notable negative impacts: 
  
 It’s quite difficult to get a job this year, as many companies and universities simply aren’t hiring.  This is particularly tough on graduating students. 
 Perhaps 10% of  IBM research  was fired. 
  
In contrast, around the time of the dot com bust,  ATnT Research  and  Lucent  had one or several 50% size firings wiping out much of the remainder of  Bell Labs , triggering a notable diaspora for the respected machine learning group there.  As the recession progresses, we may easily see more firings as companies in particular reach a point where they can no longer support research.
 
There are a couple positives to the recession as well.
  
 Both the implosion of Wall Street (which siphoned off smart people) and the general difficulty of getting a job coming out of an undergraduate education s</p><p>5 0.091322988 <a title="250-tfidf-5" href="../hunch_net-2005/hunch_net-2005-12-22-Yes_%2C_I_am_applying.html">142 hunch net-2005-12-22-Yes , I am applying</a></p>
<p>Introduction: Every year about now hundreds of applicants apply for a research/teaching job with the timing governed by the university recruitment schedule.  This time, it’s my turn—the hat’s in the ring, I am a contender, etc…  What I have heard is that this year is good in both directions—both an increased supply and an increased demand for machine learning expertise.
 
I consider this post a bit of an abuse as it is neither about general research nor machine learning.  Please forgive me this once.
 
My hope is that I will learn about new places interested in funding basic research—it’s easy to imagine that I have overlooked possibilities.
 
I am not dogmatic about where I end up in any particular way.  Several earlier posts detail what I think of as a good research environment, so I will avoid a repeat.  A few more details seem important:
  
 Application.  There is often a tension between basic research and immediate application.  This tension is not as strong as might be expected in my case.  As</p><p>6 0.08575806 <a title="250-tfidf-6" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>7 0.084662125 <a title="250-tfidf-7" href="../hunch_net-2009/hunch_net-2009-02-16-KDNuggets.html">342 hunch net-2009-02-16-KDNuggets</a></p>
<p>8 0.084335014 <a title="250-tfidf-8" href="../hunch_net-2009/hunch_net-2009-01-08-Predictive_Analytics_World.html">335 hunch net-2009-01-08-Predictive Analytics World</a></p>
<p>9 0.081914909 <a title="250-tfidf-9" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>10 0.081171587 <a title="250-tfidf-10" href="../hunch_net-2005/hunch_net-2005-09-08-Online_Learning_as_the_Mathematics_of_Accountability.html">109 hunch net-2005-09-08-Online Learning as the Mathematics of Accountability</a></p>
<p>11 0.078434035 <a title="250-tfidf-11" href="../hunch_net-2013/hunch_net-2013-01-07-NYU_Large_Scale_Machine_Learning_Class.html">478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</a></p>
<p>12 0.077796131 <a title="250-tfidf-12" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>13 0.073226735 <a title="250-tfidf-13" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>14 0.070713505 <a title="250-tfidf-14" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>15 0.070261613 <a title="250-tfidf-15" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>16 0.068283305 <a title="250-tfidf-16" href="../hunch_net-2007/hunch_net-2007-01-02-Retrospective.html">225 hunch net-2007-01-02-Retrospective</a></p>
<p>17 0.068019137 <a title="250-tfidf-17" href="../hunch_net-2009/hunch_net-2009-12-09-Inherent_Uncertainty.html">383 hunch net-2009-12-09-Inherent Uncertainty</a></p>
<p>18 0.067305155 <a title="250-tfidf-18" href="../hunch_net-2007/hunch_net-2007-08-25-The_Privacy_Problem.html">260 hunch net-2007-08-25-The Privacy Problem</a></p>
<p>19 0.067003325 <a title="250-tfidf-19" href="../hunch_net-2008/hunch_net-2008-11-28-A_Bumper_Crop_of_Machine_Learning_Graduates.html">329 hunch net-2008-11-28-A Bumper Crop of Machine Learning Graduates</a></p>
<p>20 0.066523589 <a title="250-tfidf-20" href="../hunch_net-2006/hunch_net-2006-07-09-The_Stock_Prediction_Machine_Learning_Problem.html">193 hunch net-2006-07-09-The Stock Prediction Machine Learning Problem</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.162), (1, -0.034), (2, -0.061), (3, 0.037), (4, 0.001), (5, 0.01), (6, -0.032), (7, 0.01), (8, -0.001), (9, -0.02), (10, -0.004), (11, -0.014), (12, -0.0), (13, 0.017), (14, -0.019), (15, 0.025), (16, -0.007), (17, -0.077), (18, 0.001), (19, -0.015), (20, 0.051), (21, -0.033), (22, -0.044), (23, 0.047), (24, -0.066), (25, 0.002), (26, 0.056), (27, -0.017), (28, 0.014), (29, 0.04), (30, -0.039), (31, -0.059), (32, -0.066), (33, 0.046), (34, 0.07), (35, -0.071), (36, -0.017), (37, -0.015), (38, 0.013), (39, -0.078), (40, 0.002), (41, 0.016), (42, 0.051), (43, 0.012), (44, -0.148), (45, -0.032), (46, -0.036), (47, -0.022), (48, -0.087), (49, 0.092)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9205156 <a title="250-lsi-1" href="../hunch_net-2007/hunch_net-2007-06-23-Machine_Learning_Jobs_are_Growing_on_Trees.html">250 hunch net-2007-06-23-Machine Learning Jobs are Growing on Trees</a></p>
<p>Introduction: The consensus of several discussions at ICML is that the number of jobs for people knowing machine learning well substantially exceeds supply.  This is my experience as well.  Demand comes from many places, but I’ve seen particularly strong demand from trading companies and internet startups.
 
Like all interest bursts, this one will probably pass because of economic recession or other distractions.  Nevertheless, the general outlook for machine learning in business seems to be good.  Machine learning is all about optimization when there is uncertainty and lots of data.  The quantity of data available is growing quickly as computer-run processes and sensors become more common, and the quality of the data is dropping since there is little editorial control in it’s collection.  Machine Learning is a difficult subject to master (*), so those who do should remain in demand over the long term.
 
(*) In fact, it would be reasonable to claim that no one has mastered it—there are just some peo</p><p>2 0.60779351 <a title="250-lsi-2" href="../hunch_net-2005/hunch_net-2005-12-07-Is_the_Google_way_the_way_for_machine_learning%3F.html">136 hunch net-2005-12-07-Is the Google way the way for machine learning?</a></p>
<p>Introduction: Urs Hoelzle  from  Google  gave an invited presentation at  NIPS .  In the presentation, he strongly advocates interacting with data in a particular scalable manner which is something like the following:
  
 Make a cluster of machines. 
 Build a unified filesystem.  (Google uses GFS, but NFS or other approaches work reasonably well for smaller clusters.) 
 Interact with data via  MapReduce . 
  
Creating a cluster of machines is, by this point, relatively straightforward.  
 
Unified filesystems are a little bit tricky—GFS is capable by design of essentially unlimited speed throughput to disk.  NFS can bottleneck because all of the data has to move through one machine.  Nevertheless,  this may not be a limiting factor for smaller clusters.
 
MapReduce is a programming paradigm.  Essentially, it is a combination of a data element transform (map) and an agreggator/selector (reduce).  These operations are highly parallelizable and the claim is that they support the forms of data interacti</p><p>3 0.59503728 <a title="250-lsi-3" href="../hunch_net-2009/hunch_net-2009-01-08-Predictive_Analytics_World.html">335 hunch net-2009-01-08-Predictive Analytics World</a></p>
<p>Introduction: Carla Vicens and  Eric Siegel  contacted me about  Predictive Analytics World  in San Francisco February 18&19, which I wasn’t familiar with.  A quick look at the  agenda  reveals several people I know working on applications of machine learning in businesses, covering deployed applications topics.  It’s interesting to see a business-focused machine learning conference, as it says that we are succeeding as a field.  If you are interested in deployed applications, you might attend.
 
Eric and I did a quick interview by email.
 
John > 
I’ve mostly published and participated in academic machine learning conferences like ICML, COLT, and NIPS.   When I look at the  set of speakers and subjects  for your conference  I think “machine learning for business”.  Is that your understanding of things? What I’m trying to ask is: what do you view as the primary goal for this conference?
 
Eric > 
 You got it.  This is the business event focused on the commercial deployment of technology developed at</p><p>4 0.58384657 <a title="250-lsi-4" href="../hunch_net-2005/hunch_net-2005-11-05-The_design_of_a_computing_cluster.html">128 hunch net-2005-11-05-The design of a computing cluster</a></p>
<p>Introduction: This is about the design of a computing cluster from the viewpoint of applied machine learning using current technology.  We just built a small one at TTI so this is some evidence of what is feasible and thoughts about the design choices.
  
  Architecture   There are several architectural choices.
 
 AMD Athlon64 based system.  This seems to have the cheapest bang/buck.  Maximum RAM is typically 2-3GB. 
 AMD Opteron based system. Opterons provide the additional capability to buy an SMP motherboard with two chips, and the motherboards often support 16GB of RAM.  The RAM is also the more expensive error correcting type. 
 Intel PIV or Xeon based system.  The PIV and Xeon based systems are the intel analog of the above 2.  Due to architectural design reasons, these chips tend to run a bit hotter and be a bit more expensive. 
 Dual core chips.  Both Intel and AMD have chips that actually have 2 processors embedded in them. 

In the end, we decided to go with option (2).  Roughly speaking,</p><p>5 0.54843086 <a title="250-lsi-5" href="../hunch_net-2007/hunch_net-2007-08-25-The_Privacy_Problem.html">260 hunch net-2007-08-25-The Privacy Problem</a></p>
<p>Introduction: Machine Learning is rising in importance because data is being collected for all sorts of tasks where it either wasn’t previously collected, or for tasks that did not previously exist.  While this is great for Machine Learning, it has a downside—the massive data collection which is so useful can also lead to substantial privacy problems.  
 
It’s important to understand that this is a much harder problem than many people appreciate.  The  AOL   data   release  is a good example.  To those doing machine learning, the following strategies might be obvious:
  
 Just delete any names or other obviously personally identifiable information.  The logic here seems to be “if I can’t easily find the person then no one can”.  That doesn’t work as demonstrated by the people who were found circumstantially from the AOL data. 
 … then just hash all the search terms!  The logic here is “if I can’t read it, then no one can”.  It’s also trivially broken by a dictionary attack—just hash all the strings</p><p>6 0.54636061 <a title="250-lsi-6" href="../hunch_net-2009/hunch_net-2009-08-03-Carbon_in_Computer_Science_Research.html">366 hunch net-2009-08-03-Carbon in Computer Science Research</a></p>
<p>7 0.54250938 <a title="250-lsi-7" href="../hunch_net-2006/hunch_net-2006-07-09-The_Stock_Prediction_Machine_Learning_Problem.html">193 hunch net-2006-07-09-The Stock Prediction Machine Learning Problem</a></p>
<p>8 0.53693438 <a title="250-lsi-8" href="../hunch_net-2005/hunch_net-2005-12-09-Machine_Learning_Thoughts.html">137 hunch net-2005-12-09-Machine Learning Thoughts</a></p>
<p>9 0.53267401 <a title="250-lsi-9" href="../hunch_net-2012/hunch_net-2012-10-26-ML_Symposium_and_Strata-Hadoop_World.html">475 hunch net-2012-10-26-ML Symposium and Strata-Hadoop World</a></p>
<p>10 0.51865733 <a title="250-lsi-10" href="../hunch_net-2005/hunch_net-2005-03-08-Fast_Physics_for_Learning.html">37 hunch net-2005-03-08-Fast Physics for Learning</a></p>
<p>11 0.50925851 <a title="250-lsi-11" href="../hunch_net-2007/hunch_net-2007-01-26-Parallel_Machine_Learning_Problems.html">229 hunch net-2007-01-26-Parallel Machine Learning Problems</a></p>
<p>12 0.50517714 <a title="250-lsi-12" href="../hunch_net-2010/hunch_net-2010-05-02-What%26%238217%3Bs_the_difference_between_gambling_and_rewarding_good_prediction%3F.html">397 hunch net-2010-05-02-What&#8217;s the difference between gambling and rewarding good prediction?</a></p>
<p>13 0.50445497 <a title="250-lsi-13" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>14 0.50116378 <a title="250-lsi-14" href="../hunch_net-2005/hunch_net-2005-12-22-Yes_%2C_I_am_applying.html">142 hunch net-2005-12-22-Yes , I am applying</a></p>
<p>15 0.49870503 <a title="250-lsi-15" href="../hunch_net-2011/hunch_net-2011-08-20-The_Large_Scale_Learning_Survey_Tutorial.html">442 hunch net-2011-08-20-The Large Scale Learning Survey Tutorial</a></p>
<p>16 0.4963412 <a title="250-lsi-16" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>17 0.49228024 <a title="250-lsi-17" href="../hunch_net-2008/hunch_net-2008-11-28-A_Bumper_Crop_of_Machine_Learning_Graduates.html">329 hunch net-2008-11-28-A Bumper Crop of Machine Learning Graduates</a></p>
<p>18 0.49098885 <a title="250-lsi-18" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<p>19 0.48594558 <a title="250-lsi-19" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>20 0.47563162 <a title="250-lsi-20" href="../hunch_net-2012/hunch_net-2012-07-09-Videolectures.html">469 hunch net-2012-07-09-Videolectures</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(27, 0.207), (55, 0.105), (56, 0.451), (94, 0.091), (95, 0.03)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.88752383 <a title="250-lda-1" href="../hunch_net-2006/hunch_net-2006-06-25-Presentation_of_Proofs_is_Hard..html">187 hunch net-2006-06-25-Presentation of Proofs is Hard.</a></p>
<p>Introduction: When presenting part of the  Reinforcement Learning theory tutorial  at  ICML 2006 , I was forcibly reminded of this.
 
There are several difficulties.
  
  When creating the presentation, the correct level of detail is tricky.  With too much detail, the proof takes too much time and people may be lost to boredom.  With too little detail, the steps of the proof involve too-great a jump. This is very difficult to judge.
 
 What may be an easy step in the careful thought of a quiet room is not so easy when you are occupied by the process of presentation. 
 What may be easy after having gone over this (and other) proofs is not so easy to follow in the first pass by a viewer. 
 

  These problems seem only correctable by process of repeated test-and-revise.
 
 When presenting the proof, simply speaking with sufficient precision is substantially harder than in normal conversation (where precision is not so critical).  Practice can help here. 
 When presenting the proof, going at the right p</p><p>same-blog 2 0.83810627 <a title="250-lda-2" href="../hunch_net-2007/hunch_net-2007-06-23-Machine_Learning_Jobs_are_Growing_on_Trees.html">250 hunch net-2007-06-23-Machine Learning Jobs are Growing on Trees</a></p>
<p>Introduction: The consensus of several discussions at ICML is that the number of jobs for people knowing machine learning well substantially exceeds supply.  This is my experience as well.  Demand comes from many places, but I’ve seen particularly strong demand from trading companies and internet startups.
 
Like all interest bursts, this one will probably pass because of economic recession or other distractions.  Nevertheless, the general outlook for machine learning in business seems to be good.  Machine learning is all about optimization when there is uncertainty and lots of data.  The quantity of data available is growing quickly as computer-run processes and sensors become more common, and the quality of the data is dropping since there is little editorial control in it’s collection.  Machine Learning is a difficult subject to master (*), so those who do should remain in demand over the long term.
 
(*) In fact, it would be reasonable to claim that no one has mastered it—there are just some peo</p><p>3 0.80081666 <a title="250-lda-3" href="../hunch_net-2008/hunch_net-2008-07-04-More_Presentation_Preparation.html">307 hunch net-2008-07-04-More Presentation Preparation</a></p>
<p>Introduction: We’ve discussed  presentation preparation before , but I have one more thing to add:  transitioning .  For a research presentation, it is substantially helpful for the audience if transitions are clear.  A common outline for a research presentation in machine leanring is:
  
  The problem .  Presentations which don’t describe the problem almost immediately lose people, because the context is missing to understand the detail. 
  Prior relevant work .  In many cases, a paper builds on some previous bit of work which must be understood in order to understand what the paper does.  A common failure mode seems to be spending too much time on prior work.  Discuss just the relevant aspects of prior work in the language of your work.  Sometimes this is missing when unneeded. 
  What we did . For theory papers in particular, it is often not possible to really cover the details.  Prioritizing what you present can be very important. 
  How it worked .  Many papers in Machine Learning have some sor</p><p>4 0.76296824 <a title="250-lda-4" href="../hunch_net-2012/hunch_net-2012-03-24-David_Waltz.html">460 hunch net-2012-03-24-David Waltz</a></p>
<p>Introduction: has  died .   He lived a full life.  I know him personally as a founder of the  Center for Computational Learning Systems  and the  New York Machine Learning Symposium , both of which have sheltered and promoted the advancement of machine learning.  I expect much of the New York area machine learning community will miss him, as well as many others around the world.</p><p>5 0.69745988 <a title="250-lda-5" href="../hunch_net-2006/hunch_net-2006-08-10-Precision_is_not_accuracy.html">202 hunch net-2006-08-10-Precision is not accuracy</a></p>
<p>Introduction: In my experience, there are two different groups of people who believe the same thing: the mathematics encountered in typical machine learning conference papers is often of questionable value. 
The two groups who agree on this are applied machine learning people who have given up on math, and mature theoreticians who understand the limits of theory. 
 
Partly, this is just a statement about where we are with respect to machine learning.  In particular, we have no mechanism capable of generating a prescription for how to solve all learning problems.  In the absence of such certainty, people try to come up with formalisms that partially describe and motivate how and why they do things.  This is natural and healthy—we might hope that it will eventually lead to just such a mechanism.
 
But, part of this is simply an emphasis on complexity over clarity.  A very natural and simple theoretical statement is often obscured by complexifications.  Common sources of complexification include:</p><p>6 0.66431952 <a title="250-lda-6" href="../hunch_net-2009/hunch_net-2009-05-24-2009_ICML_discussion_site.html">356 hunch net-2009-05-24-2009 ICML discussion site</a></p>
<p>7 0.5245108 <a title="250-lda-7" href="../hunch_net-2009/hunch_net-2009-11-23-ICML_2009_Workshops_%28and_Tutorials%29.html">379 hunch net-2009-11-23-ICML 2009 Workshops (and Tutorials)</a></p>
<p>8 0.49913219 <a title="250-lda-8" href="../hunch_net-2007/hunch_net-2007-06-21-Presentation_Preparation.html">249 hunch net-2007-06-21-Presentation Preparation</a></p>
<p>9 0.48634842 <a title="250-lda-9" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>10 0.47710118 <a title="250-lda-10" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>11 0.47262603 <a title="250-lda-11" href="../hunch_net-2010/hunch_net-2010-10-29-To_Vidoelecture_or_not.html">416 hunch net-2010-10-29-To Vidoelecture or not</a></p>
<p>12 0.46863785 <a title="250-lda-12" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>13 0.46773422 <a title="250-lda-13" href="../hunch_net-2008/hunch_net-2008-11-10-ICML_Reviewing_Criteria.html">325 hunch net-2008-11-10-ICML Reviewing Criteria</a></p>
<p>14 0.46686152 <a title="250-lda-14" href="../hunch_net-2007/hunch_net-2007-01-15-The_Machine_Learning_Department.html">228 hunch net-2007-01-15-The Machine Learning Department</a></p>
<p>15 0.46601301 <a title="250-lda-15" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>16 0.46503744 <a title="250-lda-16" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>17 0.46267515 <a title="250-lda-17" href="../hunch_net-2008/hunch_net-2008-09-03-Bidding_Problems.html">315 hunch net-2008-09-03-Bidding Problems</a></p>
<p>18 0.46263394 <a title="250-lda-18" href="../hunch_net-2007/hunch_net-2007-08-19-Choice_of_Metrics.html">259 hunch net-2007-08-19-Choice of Metrics</a></p>
<p>19 0.46226624 <a title="250-lda-19" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>20 0.46145287 <a title="250-lda-20" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
