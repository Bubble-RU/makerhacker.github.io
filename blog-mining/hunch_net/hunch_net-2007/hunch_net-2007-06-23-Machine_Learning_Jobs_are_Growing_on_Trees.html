<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>250 hunch net-2007-06-23-Machine Learning Jobs are Growing on Trees</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2007" href="../home/hunch_net-2007_home.html">hunch_net-2007</a> <a title="hunch_net-2007-250" href="#">hunch_net-2007-250</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>250 hunch net-2007-06-23-Machine Learning Jobs are Growing on Trees</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2007-250-html" href="http://hunch.net/?p=275">html</a></p><p>Introduction: The consensus of several discussions at ICML is that the number of jobs for
people knowing machine learning well substantially exceeds supply. This is my
experience as well. Demand comes from many places, but I've seen particularly
strong demand from trading companies and internet startups.Like all interest
bursts, this one will probably pass because of economic recession or other
distractions. Nevertheless, the general outlook for machine learning in
business seems to be good. Machine learning is all about optimization when
there is uncertainty and lots of data. The quantity of data available is
growing quickly as computer-run processes and sensors become more common, and
the quality of the data is dropping since there is little editorial control in
it's collection. Machine Learning is a difficult subject to master (*), so
those who do should remain in demand over the long term.(*) In fact, it would
be reasonable to claim that no one has mastered it--there are just some people
who kno</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The consensus of several discussions at ICML is that the number of jobs for people knowing machine learning well substantially exceeds supply. [sent-1, score-0.982]
</p><p>2 Demand comes from many places, but I've seen particularly strong demand from trading companies and internet startups. [sent-3, score-1.163]
</p><p>3 Like all interest bursts, this one will probably pass because of economic recession or other distractions. [sent-4, score-0.664]
</p><p>4 Nevertheless, the general outlook for machine learning in business seems to be good. [sent-5, score-0.269]
</p><p>5 Machine learning is all about optimization when there is uncertainty and lots of data. [sent-6, score-0.373]
</p><p>6 The quantity of data available is growing quickly as computer-run processes and sensors become more common, and the quality of the data is dropping since there is little editorial control in it's collection. [sent-7, score-1.771]
</p><p>7 Machine Learning is a difficult subject to master (*), so those who do should remain in demand over the long term. [sent-8, score-0.852]
</p><p>8 (*) In fact, it would be reasonable to claim that no one has mastered it--there are just some people who know a bit more than others. [sent-9, score-0.348]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('demand', 0.419), ('editorial', 0.202), ('dropping', 0.202), ('recession', 0.202), ('sensors', 0.187), ('trading', 0.187), ('consensus', 0.177), ('mastered', 0.177), ('exceeds', 0.169), ('jobs', 0.156), ('economic', 0.151), ('business', 0.143), ('pass', 0.14), ('knowing', 0.137), ('uncertainty', 0.137), ('master', 0.137), ('lots', 0.137), ('discussions', 0.134), ('quantity', 0.134), ('companies', 0.128), ('remain', 0.128), ('processes', 0.126), ('control', 0.126), ('machine', 0.126), ('places', 0.124), ('quickly', 0.122), ('growing', 0.12), ('data', 0.108), ('internet', 0.108), ('claim', 0.104), ('optimization', 0.099), ('fact', 0.098), ('subject', 0.095), ('probably', 0.095), ('quality', 0.092), ('available', 0.089), ('comes', 0.088), ('nevertheless', 0.088), ('seen', 0.087), ('become', 0.084), ('icml', 0.083), ('substantially', 0.083), ('strong', 0.081), ('experience', 0.08), ('others', 0.079), ('interest', 0.076), ('long', 0.073), ('little', 0.071), ('reasonable', 0.067), ('particularly', 0.065)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="250-tfidf-1" href="../hunch_net-2007/hunch_net-2007-06-23-Machine_Learning_Jobs_are_Growing_on_Trees.html">250 hunch net-2007-06-23-Machine Learning Jobs are Growing on Trees</a></p>
<p>Introduction: The consensus of several discussions at ICML is that the number of jobs for
people knowing machine learning well substantially exceeds supply. This is my
experience as well. Demand comes from many places, but I've seen particularly
strong demand from trading companies and internet startups.Like all interest
bursts, this one will probably pass because of economic recession or other
distractions. Nevertheless, the general outlook for machine learning in
business seems to be good. Machine learning is all about optimization when
there is uncertainty and lots of data. The quantity of data available is
growing quickly as computer-run processes and sensors become more common, and
the quality of the data is dropping since there is little editorial control in
it's collection. Machine Learning is a difficult subject to master (*), so
those who do should remain in demand over the long term.(*) In fact, it would
be reasonable to claim that no one has mastered it--there are just some people
who kno</p><p>2 0.11254131 <a title="250-tfidf-2" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<p>Introduction: I recently had fun discussions with bothVikash MansinghkaandThomas Breuelabout
approaching AI with machine learning. The general interest in taking a crack
at AI with machine learning seems to be rising on many fronts
includingDARPA.As a matter of history, there was a great deal of interest in
AI which died down before I began research. There remain many projects and
conferences spawned in this earlier AI wave, as well as a good bit of
experience about what did not work, or at least did not work yet. Here are a
few examples of failure modes that people seem to run into:Supply/Product
confusion. Sometimes we think "Intelligences use X, so I'll create X and have
an Intelligence." An example of this is theCyc Projectwhich inspires some
people as "intelligences use ontologies, so I'll create an ontology and a
system using it to have an Intelligence." The flaw here is that
Intelligencescreateontologies, which they use, and without the ability to
create ontologies you don't have an Intellige</p><p>3 0.10347472 <a title="250-tfidf-3" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>Introduction: Unfortunately, I ended up sick for much of this ICML. I did manage to catch
one interesting paper:Richard Socher,Cliff Lin,Andrew Y. Ng, andChristopher D.
ManningParsing Natural Scenes and Natural Language with Recursive Neural
Networks.I invited Richard to share his list of interesting papers, so
hopefully we'll hear from him soon. In the meantime,PaulandHalhave posted some
lists.the futureJoelleand I are program chairs for ICML 2012 inEdinburgh,
which I previously enjoyed visiting in2005. This is a huge responsibility,
that we hope to accomplish well. A part of this (perhaps the most fun part),
is imagining how we can make ICML better. A key and critical constraint is
choosing things that can be accomplished. So far we have:Colocation. The first
thing we looked into was potential colocations. We quickly discovered that
many other conferences precomitted their location. For the future, getting a
colocation withACLorSIGIR, seems to require more advanced planning. If that
can be done, I</p><p>4 0.10169263 <a title="250-tfidf-4" href="../hunch_net-2009/hunch_net-2009-02-22-Effective_Research_Funding.html">344 hunch net-2009-02-22-Effective Research Funding</a></p>
<p>Introduction: With a worldwide recession on, my impression is that the carnage in research
has not been as severe as might be feared, at least in the United States. I
know of two notable negative impacts:It's quite difficult to get a job this
year, as many companies and universities simply aren't hiring. This is
particularly tough on graduating students.Perhaps 10% ofIBM researchwas
fired.In contrast, around the time of the dot com bust,ATnT
ResearchandLucenthad one or several 50% size firings wiping out much of the
remainder ofBell Labs, triggering a notable diaspora for the respected machine
learning group there. As the recession progresses, we may easily see more
firings as companies in particular reach a point where they can no longer
support research.There are a couple positives to the recession as well.Both
the implosion of Wall Street (which siphoned off smart people) and the general
difficulty of getting a job coming out of an undergraduate education suggest
that the quality of admitted phd</p><p>5 0.091853231 <a title="250-tfidf-5" href="../hunch_net-2005/hunch_net-2005-12-22-Yes_%2C_I_am_applying.html">142 hunch net-2005-12-22-Yes , I am applying</a></p>
<p>Introduction: Every year about now hundreds of applicants apply for a research/teaching job
with the timing governed by the university recruitment schedule. This time,
it's my turn--the hat's in the ring, I am a contender, etcâ&euro;Ś What I have heard
is that this year is good in both directions--both an increased supply and an
increased demand for machine learning expertise.I consider this post a bit of
an abuse as it is neither about general research nor machine learning. Please
forgive me this once.My hope is that I will learn about new places interested
in funding basic research--it's easy to imagine that I have overlooked
possibilities.I am not dogmatic about where I end up in any particular way.
Several earlier posts detail what I think of as a good research environment,
so I will avoid a repeat. A few more details seem important:Application. There
is often a tension between basic research and immediate application. This
tension is not as strong as might be expected in my case. As evidence, many of</p><p>6 0.089613631 <a title="250-tfidf-6" href="../hunch_net-2009/hunch_net-2009-01-08-Predictive_Analytics_World.html">335 hunch net-2009-01-08-Predictive Analytics World</a></p>
<p>7 0.087678082 <a title="250-tfidf-7" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>8 0.087345451 <a title="250-tfidf-8" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>9 0.08502835 <a title="250-tfidf-9" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>10 0.084337443 <a title="250-tfidf-10" href="../hunch_net-2009/hunch_net-2009-02-16-KDNuggets.html">342 hunch net-2009-02-16-KDNuggets</a></p>
<p>11 0.082643569 <a title="250-tfidf-11" href="../hunch_net-2013/hunch_net-2013-01-07-NYU_Large_Scale_Machine_Learning_Class.html">478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</a></p>
<p>12 0.081498861 <a title="250-tfidf-12" href="../hunch_net-2005/hunch_net-2005-09-08-Online_Learning_as_the_Mathematics_of_Accountability.html">109 hunch net-2005-09-08-Online Learning as the Mathematics of Accountability</a></p>
<p>13 0.074443869 <a title="250-tfidf-13" href="../hunch_net-2007/hunch_net-2007-01-26-Parallel_Machine_Learning_Problems.html">229 hunch net-2007-01-26-Parallel Machine Learning Problems</a></p>
<p>14 0.07399863 <a title="250-tfidf-14" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>15 0.073948309 <a title="250-tfidf-15" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>16 0.071576454 <a title="250-tfidf-16" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>17 0.071451083 <a title="250-tfidf-17" href="../hunch_net-2008/hunch_net-2008-11-28-A_Bumper_Crop_of_Machine_Learning_Graduates.html">329 hunch net-2008-11-28-A Bumper Crop of Machine Learning Graduates</a></p>
<p>18 0.070768289 <a title="250-tfidf-18" href="../hunch_net-2007/hunch_net-2007-08-25-The_Privacy_Problem.html">260 hunch net-2007-08-25-The Privacy Problem</a></p>
<p>19 0.07007385 <a title="250-tfidf-19" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>20 0.06888327 <a title="250-tfidf-20" href="../hunch_net-2006/hunch_net-2006-07-09-The_Stock_Prediction_Machine_Learning_Problem.html">193 hunch net-2006-07-09-The Stock Prediction Machine Learning Problem</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.165), (1, 0.042), (2, 0.071), (3, -0.003), (4, 0.001), (5, 0.059), (6, -0.019), (7, -0.014), (8, 0.03), (9, -0.033), (10, -0.022), (11, 0.026), (12, 0.034), (13, -0.02), (14, 0.055), (15, -0.019), (16, -0.027), (17, 0.022), (18, 0.002), (19, 0.057), (20, 0.064), (21, -0.049), (22, -0.036), (23, -0.015), (24, 0.041), (25, -0.014), (26, 0.087), (27, 0.051), (28, 0.134), (29, -0.008), (30, -0.015), (31, -0.031), (32, 0.053), (33, 0.023), (34, -0.055), (35, -0.024), (36, -0.048), (37, 0.005), (38, 0.055), (39, -0.026), (40, 0.006), (41, 0.044), (42, -0.08), (43, 0.063), (44, 0.128), (45, -0.034), (46, 0.117), (47, -0.031), (48, 0.027), (49, -0.0)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9157325 <a title="250-lsi-1" href="../hunch_net-2007/hunch_net-2007-06-23-Machine_Learning_Jobs_are_Growing_on_Trees.html">250 hunch net-2007-06-23-Machine Learning Jobs are Growing on Trees</a></p>
<p>Introduction: The consensus of several discussions at ICML is that the number of jobs for
people knowing machine learning well substantially exceeds supply. This is my
experience as well. Demand comes from many places, but I've seen particularly
strong demand from trading companies and internet startups.Like all interest
bursts, this one will probably pass because of economic recession or other
distractions. Nevertheless, the general outlook for machine learning in
business seems to be good. Machine learning is all about optimization when
there is uncertainty and lots of data. The quantity of data available is
growing quickly as computer-run processes and sensors become more common, and
the quality of the data is dropping since there is little editorial control in
it's collection. Machine Learning is a difficult subject to master (*), so
those who do should remain in demand over the long term.(*) In fact, it would
be reasonable to claim that no one has mastered it--there are just some people
who kno</p><p>2 0.59083551 <a title="250-lsi-2" href="../hunch_net-2005/hunch_net-2005-12-07-Is_the_Google_way_the_way_for_machine_learning%3F.html">136 hunch net-2005-12-07-Is the Google way the way for machine learning?</a></p>
<p>Introduction: Urs HoelzlefromGooglegave an invited presentation atNIPS. In the presentation,
he strongly advocates interacting with data in a particular scalable manner
which is something like the following:Make a cluster of machines.Build a
unified filesystem. (Google uses GFS, but NFS or other approaches work
reasonably well for smaller clusters.)Interact with data viaMapReduce.Creating
a cluster of machines is, by this point, relatively straightforward.Unified
filesystems are a little bit tricky--GFS is capable by design of essentially
unlimited speed throughput to disk. NFS can bottleneck because all of the data
has to move through one machine. Nevertheless, this may not be a limiting
factor for smaller clusters.MapReduce is a programming paradigm. Essentially,
it is a combination of a data element transform (map) and an
agreggator/selector (reduce). These operations are highly parallelizable and
the claim is that they support the forms of data interaction which are
necessary.Apparently, theNutc</p><p>3 0.55726594 <a title="250-lsi-3" href="../hunch_net-2012/hunch_net-2012-02-20-Berkeley_Streaming_Data_Workshop.html">455 hunch net-2012-02-20-Berkeley Streaming Data Workshop</a></p>
<p>Introduction: TheFrom Data to Knowledgeworkshop May 7-11 atBerkeleyshould be of interest to
the many people encountering streaming data in different disciplines. It's run
by a group of astronomers who encounter streaming data all the time. I metJosh
Bloomrecently and he is broadly interested in a workshop covering all aspects
of Machine Learning on streaming data. The hope here is that techniques
developed in one area turn out useful in another which seems quite plausible.
Particularly if you are in the bay area, consider checking it out.</p><p>4 0.55026418 <a title="250-lsi-4" href="../hunch_net-2007/hunch_net-2007-08-25-The_Privacy_Problem.html">260 hunch net-2007-08-25-The Privacy Problem</a></p>
<p>Introduction: Machine Learning is rising in importance because data is being collected for
all sorts of tasks where it either wasn't previously collected, or for tasks
that did not previously exist. While this is great for Machine Learning, it
has a downside--the massive data collection which is so useful can also lead
to substantial privacy problems.It's important to understand that this is a
much harder problem than many people appreciate. TheAOLdatareleaseis a good
example. To those doing machine learning, the following strategies might be
obvious:Just delete any names or other obviously personally identifiable
information. The logic here seems to be "if I can't easily find the person
then no one can". That doesn't work as demonstrated by the people who were
found circumstantially from the AOL data.… then just hash all the search
terms! The logic here is "if I can't read it, then no one can". It's also
trivially broken by a dictionary attack--just hash all the strings that might
be in the data an</p><p>5 0.52199525 <a title="250-lsi-5" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<p>Introduction: I recently had fun discussions with bothVikash MansinghkaandThomas Breuelabout
approaching AI with machine learning. The general interest in taking a crack
at AI with machine learning seems to be rising on many fronts
includingDARPA.As a matter of history, there was a great deal of interest in
AI which died down before I began research. There remain many projects and
conferences spawned in this earlier AI wave, as well as a good bit of
experience about what did not work, or at least did not work yet. Here are a
few examples of failure modes that people seem to run into:Supply/Product
confusion. Sometimes we think "Intelligences use X, so I'll create X and have
an Intelligence." An example of this is theCyc Projectwhich inspires some
people as "intelligences use ontologies, so I'll create an ontology and a
system using it to have an Intelligence." The flaw here is that
Intelligencescreateontologies, which they use, and without the ability to
create ontologies you don't have an Intellige</p><p>6 0.51793271 <a title="250-lsi-6" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>7 0.51640075 <a title="250-lsi-7" href="../hunch_net-2013/hunch_net-2013-06-10-The_Large_Scale_Learning_class_notes.html">483 hunch net-2013-06-10-The Large Scale Learning class notes</a></p>
<p>8 0.51284337 <a title="250-lsi-8" href="../hunch_net-2006/hunch_net-2006-07-09-The_Stock_Prediction_Machine_Learning_Problem.html">193 hunch net-2006-07-09-The Stock Prediction Machine Learning Problem</a></p>
<p>9 0.50998771 <a title="250-lsi-9" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>10 0.50619209 <a title="250-lsi-10" href="../hunch_net-2007/hunch_net-2007-04-28-The_Coming_Patent_Apocalypse.html">241 hunch net-2007-04-28-The Coming Patent Apocalypse</a></p>
<p>11 0.50274342 <a title="250-lsi-11" href="../hunch_net-2009/hunch_net-2009-01-08-Predictive_Analytics_World.html">335 hunch net-2009-01-08-Predictive Analytics World</a></p>
<p>12 0.49495563 <a title="250-lsi-12" href="../hunch_net-2012/hunch_net-2012-07-09-Videolectures.html">469 hunch net-2012-07-09-Videolectures</a></p>
<p>13 0.4890551 <a title="250-lsi-13" href="../hunch_net-2005/hunch_net-2005-10-20-Machine_Learning_in_the_News.html">125 hunch net-2005-10-20-Machine Learning in the News</a></p>
<p>14 0.48765105 <a title="250-lsi-14" href="../hunch_net-2012/hunch_net-2012-10-26-ML_Symposium_and_Strata-Hadoop_World.html">475 hunch net-2012-10-26-ML Symposium and Strata-Hadoop World</a></p>
<p>15 0.48648891 <a title="250-lsi-15" href="../hunch_net-2012/hunch_net-2012-08-24-Patterns_for_research_in_machine_learning.html">471 hunch net-2012-08-24-Patterns for research in machine learning</a></p>
<p>16 0.48493052 <a title="250-lsi-16" href="../hunch_net-2005/hunch_net-2005-12-09-Machine_Learning_Thoughts.html">137 hunch net-2005-12-09-Machine Learning Thoughts</a></p>
<p>17 0.48018557 <a title="250-lsi-17" href="../hunch_net-2006/hunch_net-2006-04-09-Progress_in_Machine_Translation.html">171 hunch net-2006-04-09-Progress in Machine Translation</a></p>
<p>18 0.47988191 <a title="250-lsi-18" href="../hunch_net-2005/hunch_net-2005-01-24-The_Humanloop_Spectrum_of_Machine_Learning.html">3 hunch net-2005-01-24-The Humanloop Spectrum of Machine Learning</a></p>
<p>19 0.47889724 <a title="250-lsi-19" href="../hunch_net-2007/hunch_net-2007-11-14-BellKor_wins_Netflix.html">272 hunch net-2007-11-14-BellKor wins Netflix</a></p>
<p>20 0.47705549 <a title="250-lsi-20" href="../hunch_net-2013/hunch_net-2013-01-07-NYU_Large_Scale_Machine_Learning_Class.html">478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(35, 0.044), (42, 0.193), (45, 0.061), (68, 0.024), (74, 0.106), (79, 0.382), (95, 0.075)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.84454888 <a title="250-lda-1" href="../hunch_net-2007/hunch_net-2007-06-23-Machine_Learning_Jobs_are_Growing_on_Trees.html">250 hunch net-2007-06-23-Machine Learning Jobs are Growing on Trees</a></p>
<p>Introduction: The consensus of several discussions at ICML is that the number of jobs for
people knowing machine learning well substantially exceeds supply. This is my
experience as well. Demand comes from many places, but I've seen particularly
strong demand from trading companies and internet startups.Like all interest
bursts, this one will probably pass because of economic recession or other
distractions. Nevertheless, the general outlook for machine learning in
business seems to be good. Machine learning is all about optimization when
there is uncertainty and lots of data. The quantity of data available is
growing quickly as computer-run processes and sensors become more common, and
the quality of the data is dropping since there is little editorial control in
it's collection. Machine Learning is a difficult subject to master (*), so
those who do should remain in demand over the long term.(*) In fact, it would
be reasonable to claim that no one has mastered it--there are just some people
who kno</p><p>2 0.68359566 <a title="250-lda-2" href="../hunch_net-2005/hunch_net-2005-03-10-Breaking_Abstractions.html">39 hunch net-2005-03-10-Breaking Abstractions</a></p>
<p>Introduction: Sam Roweis's comment reminds me of a more general issue that comes up in doing
research: abstractions always break.Real number's aren't. Most real numbers
can not be represented with any machine. One implication of this is that many
real-number based algorithms have difficulties when implemented with floating
point numbers.The box on your desk is not a turing machine. A turing machine
can compute anything computable, given sufficient time. A typical computer
fails terribly when the state required for the computation exceeds some
limit.Nash equilibria aren't equilibria. This comes up when trying to predict
human behavior based on the result of the equilibria computation. Often, it
doesn't work.Theprobabilityisn't. Probability is an abstraction expressing
either our lack of knowledge (the Bayesian viewpoint) or fundamental
randomization (the frequentist viewpoint). From the frequentist viewpoint the
lack of knowledge typically precludes actually knowing the fundamental
randomization. Fro</p><p>3 0.54231036 <a title="250-lda-3" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>Introduction: Conferences exist as part of the process of doing research. They provide many
roles including "announcing research", "meeting people", and "point of
reference". Not all conferences are alike so a basic question is: "to what
extent do individual conferences attempt to aid research?" This question is
very difficult to answer in any satisfying way. What we can do is compare
details of the process across multiple conferences.CommentsThe average quality
of comments across conferences can vary dramatically. At one extreme, the
tradition in CS theory conferences is to provide essentially zero feedback. At
the other extreme, some conferences have a strong tradition of providing
detailed constructive feedback. Detailed feedback can give authors significant
guidance about how to improve research. This is the most subjective
entry.BlindVirtually all conferences offer single blind review where authors
do not know reviewers. Some also providedouble blindreview where reviewers do
not know authors. T</p><p>4 0.52014637 <a title="250-lda-4" href="../hunch_net-2005/hunch_net-2005-03-17-Going_all_the_Way%2C_Sometimes.html">42 hunch net-2005-03-17-Going all the Way, Sometimes</a></p>
<p>Introduction: At many points in research, you face a choice: should I keep on improving some
old piece of technology or should I do something new? For example:Should I
refine bounds to make them tighter?Should I take some learning theory and turn
it into a learning algorithm?Should I implement the learning algorithm?Should
I test the learning algorithm widely?Should I release the algorithm as source
code?Should I go see what problems people actually need to solve?The universal
temptation of people attracted to research is doing something new. That is
sometimes the right decision, but is also often not. I'd like to discuss some
reasons why not.ExpertiseOnce expertise are developed on some subject, you are
the right person to refine them.What is the real problem?Continually improving
a piece of technology is a mechanism forcing you to confront this question. In
many cases, this confrontation is uncomfortable because you discover that your
method has fundamental flaws with respect to solving the real p</p><p>5 0.5195778 <a title="250-lda-5" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>Introduction: At thelast ICML,Tom Dietterichasked me to look into systems for commenting on
papers. I've been slow getting to this, but it's relevant now.The essential
observation is that we now have many tools for online collaboration, but they
are not yet much used in academic research. If we can find the right way to
use them, then perhaps great things might happen, with extra kudos to the
first conference that manages to really create an online community. Various
conferences have been poking at this. For example,UAI has setup a wiki, COLT
hasstarted usingJoomla, with some dynamic content, and AAAI has been setting
up a "student blog". Similarly,Dinoj Surendransetup a twiki for theChicago
Machine Learning Summer School, which was quite useful for coordinating events
and other things.I believe the most important thing is a willingness to
experiment. A good place to start seems to be enhancing existing conference
websites. For example, theICML 2007 papers pageis basically only useful via
grep. A mu</p><p>6 0.51861537 <a title="250-lda-6" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>7 0.51760471 <a title="250-lda-7" href="../hunch_net-2006/hunch_net-2006-09-28-Programming_Languages_for_Machine_Learning_Implementations.html">210 hunch net-2006-09-28-Programming Languages for Machine Learning Implementations</a></p>
<p>8 0.5173012 <a title="250-lda-8" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>9 0.51405919 <a title="250-lda-9" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>10 0.51170897 <a title="250-lda-10" href="../hunch_net-2013/hunch_net-2013-12-01-NIPS_tutorials_and_Vowpal_Wabbit_7.4.html">492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</a></p>
<p>11 0.5117026 <a title="250-lda-11" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>12 0.51040035 <a title="250-lda-12" href="../hunch_net-2005/hunch_net-2005-09-26-Prediction_Bounds_as_the_Mathematics_of_Science.html">115 hunch net-2005-09-26-Prediction Bounds as the Mathematics of Science</a></p>
<p>13 0.51039594 <a title="250-lda-13" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>14 0.51038396 <a title="250-lda-14" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>15 0.50979775 <a title="250-lda-15" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>16 0.50977051 <a title="250-lda-16" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>17 0.50955659 <a title="250-lda-17" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>18 0.50887275 <a title="250-lda-18" href="../hunch_net-2005/hunch_net-2005-09-12-Fast_Gradient_Descent.html">111 hunch net-2005-09-12-Fast Gradient Descent</a></p>
<p>19 0.50864756 <a title="250-lda-19" href="../hunch_net-2009/hunch_net-2009-02-22-Effective_Research_Funding.html">344 hunch net-2009-02-22-Effective Research Funding</a></p>
<p>20 0.50754237 <a title="250-lda-20" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
