<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>232 hunch net-2007-02-11-24</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2007" href="../home/hunch_net-2007_home.html">hunch_net-2007</a> <a title="hunch_net-2007-232" href="#">hunch_net-2007-232</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>232 hunch net-2007-02-11-24</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2007-232-html" href="http://hunch.net/?p=252">html</a></p><p>Introduction: To commemorate the  Twenty Fourth Annual International Conference on Machine  Learning  (ICML-07), the FOX Network has decided to  launch a new spin-off series in prime time.  Through unofficial  sources, I have obtained the  story arc  for the first season, which appears frighteningly realistic.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 To commemorate the  Twenty Fourth Annual International Conference on Machine  Learning  (ICML-07), the FOX Network has decided to  launch a new spin-off series in prime time. [sent-1, score-1.124]
</p><p>2 Through unofficial  sources, I have obtained the  story arc  for the first season, which appears frighteningly realistic. [sent-2, score-1.077]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('launch', 0.321), ('frighteningly', 0.321), ('prime', 0.321), ('obtained', 0.298), ('realistic', 0.298), ('fourth', 0.281), ('season', 0.268), ('annual', 0.248), ('story', 0.233), ('series', 0.217), ('international', 0.217), ('decided', 0.208), ('sources', 0.184), ('network', 0.176), ('appears', 0.147), ('conference', 0.108), ('first', 0.078), ('new', 0.057), ('machine', 0.047), ('learning', 0.019)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="232-tfidf-1" href="../hunch_net-2007/hunch_net-2007-02-11-24.html">232 hunch net-2007-02-11-24</a></p>
<p>Introduction: To commemorate the  Twenty Fourth Annual International Conference on Machine  Learning  (ICML-07), the FOX Network has decided to  launch a new spin-off series in prime time.  Through unofficial  sources, I have obtained the  story arc  for the first season, which appears frighteningly realistic.</p><p>2 0.088557482 <a title="232-tfidf-2" href="../hunch_net-2007/hunch_net-2007-10-19-Second_Annual_Reinforcement_Learning_Competition.html">268 hunch net-2007-10-19-Second Annual Reinforcement Learning Competition</a></p>
<p>Introduction: The Second Annual Reinforcement Learning Competition is about to get started.  The aim of the competition is to facilitate direct comparisons between various learning methods on important and realistic domains.  This yearâ&euro;&trade;s event will feature well-known benchmark domains as well as more challenging problems of real-world complexity, such as helicopter control and robot soccer keepaway.
 
The competition begins on November 1st, 2007 when training software is released.   Results must be submitted by July 1st, 2008.  The competition will culminate in an event at ICML-08 in Helsinki, Finland, at which the winners will be announced.
 
For more information, visit   the competition website.</p><p>3 0.084702231 <a title="232-tfidf-3" href="../hunch_net-2005/hunch_net-2005-02-02-Paper_Deadlines.html">11 hunch net-2005-02-02-Paper Deadlines</a></p>
<p>Introduction: Itâ&euro;&trade;s conference season, and smell of budding papers is in the air.
 
 IJCAI 2005 , January 21  
 COLT 2005 , February 2  
 KDD 2005 , February 18  
 ICML 2005 , March 8  
 UAI 2005 , March 16  
 AAAI 2005 , March 18</p><p>4 0.059742708 <a title="232-tfidf-4" href="../hunch_net-2007/hunch_net-2007-07-13-The_View_From_China.html">255 hunch net-2007-07-13-The View From China</a></p>
<p>Introduction: I’m visiting Beijing for the  Pao-Lu Hsu Statistics Conference  on Machine Learning.
 
I had several discussions about the state of Chinese research.  Given the large population and economy, you might expect substantial research—more than has been observed at international conferences.  The fundamental problem seems to be the  Cultural Revolution  which  lobotimized higher education, and the research associated with it.  There has been a process of slow recovery since then, which has begun to be felt in the research world via increased participation in international conferences and (now) conferences in China.
 
The amount of effort going into construction in Beijing is very impressive—people are literally building a skyscraper at night outside the window of the hotel I’m staying at (and this is not unusual).  If a small fraction of this effort is later focused onto supporting research, the effect could be very substantial.  General growth in China’s research portfolio should be expecte</p><p>5 0.052125193 <a title="232-tfidf-5" href="../hunch_net-2007/hunch_net-2007-04-30-COLT_2007.html">242 hunch net-2007-04-30-COLT 2007</a></p>
<p>Introduction: Registration for COLT 2007 is now open.
 
The conference will take place on 13-15 June, 2007, in San Diego, California, as part of the 2007 Federated Computing Research Conference (FCRC), which includes STOC, Complexity, and EC.
 
The website for COLT: http://www.learningtheory.org/colt2007/index.html
 
The early registration deadline is May 11, and the cutoff date for discounted hotel rates is May 9.
 
Before registering, take note that the fees are substantially lower for members of ACM and/or SIGACT than for nonmembers. If youâ&euro;&trade;ve been contemplating joining either of these two societies (annual dues: $99 for ACM, $18 for SIGACT), now would be a good time!</p><p>6 0.051793631 <a title="232-tfidf-6" href="../hunch_net-2005/hunch_net-2005-02-09-Intuitions_from_applied_learning.html">16 hunch net-2005-02-09-Intuitions from applied learning</a></p>
<p>7 0.051270176 <a title="232-tfidf-7" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>8 0.051003985 <a title="232-tfidf-8" href="../hunch_net-2005/hunch_net-2005-05-28-Running_A_Machine_Learning_Summer_School.html">75 hunch net-2005-05-28-Running A Machine Learning Summer School</a></p>
<p>9 0.044610776 <a title="232-tfidf-9" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>10 0.042630173 <a title="232-tfidf-10" href="../hunch_net-2012/hunch_net-2012-10-18-7th_Annual_Machine_Learning_Symposium.html">474 hunch net-2012-10-18-7th Annual Machine Learning Symposium</a></p>
<p>11 0.03956778 <a title="232-tfidf-11" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>12 0.038643688 <a title="232-tfidf-12" href="../hunch_net-2009/hunch_net-2009-01-21-Nearly_all_natural_problems_require_nonlinearity.html">337 hunch net-2009-01-21-Nearly all natural problems require nonlinearity</a></p>
<p>13 0.038547322 <a title="232-tfidf-13" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>14 0.038214725 <a title="232-tfidf-14" href="../hunch_net-2009/hunch_net-2009-12-24-Top_graduates_this_season.html">384 hunch net-2009-12-24-Top graduates this season</a></p>
<p>15 0.038158119 <a title="232-tfidf-15" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>16 0.038012728 <a title="232-tfidf-16" href="../hunch_net-2005/hunch_net-2005-11-16-The_Everything_Ensemble_Edge.html">131 hunch net-2005-11-16-The Everything Ensemble Edge</a></p>
<p>17 0.037728496 <a title="232-tfidf-17" href="../hunch_net-2007/hunch_net-2007-01-04-2007_Summer_Machine_Learning_Conferences.html">226 hunch net-2007-01-04-2007 Summer Machine Learning Conferences</a></p>
<p>18 0.036916345 <a title="232-tfidf-18" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>19 0.035475284 <a title="232-tfidf-19" href="../hunch_net-2005/hunch_net-2005-05-11-Visa_Casualties.html">69 hunch net-2005-05-11-Visa Casualties</a></p>
<p>20 0.035441387 <a title="232-tfidf-20" href="../hunch_net-2009/hunch_net-2009-12-27-Interesting_things_at_NIPS_2009.html">385 hunch net-2009-12-27-Interesting things at NIPS 2009</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.044), (1, -0.032), (2, -0.035), (3, -0.023), (4, -0.001), (5, -0.032), (6, -0.012), (7, 0.02), (8, 0.013), (9, -0.007), (10, -0.024), (11, 0.0), (12, 0.013), (13, -0.002), (14, 0.024), (15, 0.032), (16, 0.017), (17, 0.029), (18, 0.014), (19, 0.029), (20, 0.019), (21, -0.043), (22, 0.013), (23, 0.017), (24, -0.009), (25, 0.027), (26, -0.059), (27, -0.007), (28, 0.007), (29, 0.022), (30, 0.005), (31, 0.06), (32, -0.016), (33, -0.03), (34, 0.0), (35, -0.015), (36, 0.015), (37, -0.015), (38, 0.02), (39, 0.025), (40, -0.002), (41, -0.009), (42, 0.044), (43, -0.016), (44, -0.032), (45, -0.044), (46, 0.022), (47, 0.004), (48, 0.049), (49, -0.051)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.90593952 <a title="232-lsi-1" href="../hunch_net-2007/hunch_net-2007-02-11-24.html">232 hunch net-2007-02-11-24</a></p>
<p>Introduction: To commemorate the  Twenty Fourth Annual International Conference on Machine  Learning  (ICML-07), the FOX Network has decided to  launch a new spin-off series in prime time.  Through unofficial  sources, I have obtained the  story arc  for the first season, which appears frighteningly realistic.</p><p>2 0.65327811 <a title="232-lsi-2" href="../hunch_net-2005/hunch_net-2005-07-13-%26%238220%3BSister_Conference%26%238221%3B_presentations.html">93 hunch net-2005-07-13-&#8220;Sister Conference&#8221; presentations</a></p>
<p>Introduction: Some of the “sister conference” presentations at  AAAI  have been great.  Roughly speaking, the conference organizers asked other conference organizers to come give a summary of their conference.  Many different AI-related conferences accepted.  The presenters typically discuss some of the background and goals of the conference then mention the results from a few papers they liked.  This is great because it provides a mechanism to get a digested overview of the work of several thousand researchers—something which is simply available nowhere else.
 
Based on these presentations, it looks like there is a significant component of (and opportunity for) applied machine learning in  AIIDE ,  IUI , and  ACL .
 
There was also some discussion of having a super-colocation event similar to  FCRC , but centered on AI & Learning.  This seems like a fine idea.  The field is fractured across so many different conferences that the mixing of a supercolocation seems likely helpful for research.</p><p>3 0.4967626 <a title="232-lsi-3" href="../hunch_net-2006/hunch_net-2006-01-06-MLTV.html">146 hunch net-2006-01-06-MLTV</a></p>
<p>Introduction: As part of a  PASCAL  project, the Slovenians have been filming various machine learning events and placing them on the web  here .  This includes, for example, the  Chicago 2005 Machine Learning Summer School  as well as a number of other summer schools, workshops, and conferences.
 
There are some significant caveats here—for example, I can’t access it from Linux.  Based upon the webserver logs, I expect that is a problem for most people—Computer scientists are particularly nonstandard in their choice of computing platform.
 
Nevertheless, the core idea here is excellent and details of compatibility can be fixed later.  With modern technology toys, there is no fundamental reason why the process of announcing new work at a conference should happen only once and only for the people who could make it to that room in that conference.  The problems solved include:
  
 The multitrack vs. single-track debate.  (“Sometimes the single track doesn’t interest me” vs. “When it’s multitrack I mis</p><p>4 0.48615959 <a title="232-lsi-4" href="../hunch_net-2010/hunch_net-2010-10-29-To_Vidoelecture_or_not.html">416 hunch net-2010-10-29-To Vidoelecture or not</a></p>
<p>Introduction: (update:  cross-posted  on  CACM )
 
For the first time in several years,  ICML 2010  did not have  videolectures  attending.  Luckily, the  tutorial on exploration and learning  which  Alina  and I put together can  be viewed , since we also presented at  KDD 2010 , which included videolecture support. 
 
ICML didn’t cover the cost of a videolecture, because  PASCAL  didn’t provide a grant for it this year.  On the other hand, KDD covered it out of registration costs.  The cost of videolectures isn’t cheap.  For  a workshop  the baseline quote we have is 270 euro per hour, plus a similar cost for the cameraman’s travel and accomodation.  This can be reduced substantially by having a volunteer with a camera handle the cameraman duties, uploading the video and slides to be processed for a quoted 216 euro per hour.
 
 Youtube  is the most predominant free video site with a cost of $0, but it turns out to be a poor alternative.   15 minute upload limits  do not match typical talk lengths.</p><p>5 0.48014456 <a title="232-lsi-5" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>Introduction: At the  last ICML ,  Tom Dietterich  asked me to look into systems for commenting on papers.  I’ve been slow getting to this, but it’s relevant now.
 
The essential observation is that we now have many tools for online collaboration, but they are not yet much used in academic research.  If we can find the right way to use them, then perhaps great things might happen, with extra kudos to the first conference that manages to really create an online community.  Various conferences have been poking at this.  For example,  UAI has setup a wiki , COLT has  started using   Joomla , with some dynamic content, and AAAI has been setting up a “ student blog “.  Similarly,  Dinoj Surendran  setup a twiki for the  Chicago Machine Learning Summer School , which was quite useful for coordinating events and other things.
 
I believe the most important thing is a willingness to experiment.  A good place to start seems to be enhancing existing conference websites.  For example, the  ICML 2007 papers pag</p><p>6 0.47989464 <a title="232-lsi-6" href="../hunch_net-2007/hunch_net-2007-10-19-Second_Annual_Reinforcement_Learning_Competition.html">268 hunch net-2007-10-19-Second Annual Reinforcement Learning Competition</a></p>
<p>7 0.45571166 <a title="232-lsi-7" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>8 0.44877476 <a title="232-lsi-8" href="../hunch_net-2005/hunch_net-2005-02-17-Learning_Research_Programs.html">21 hunch net-2005-02-17-Learning Research Programs</a></p>
<p>9 0.4402661 <a title="232-lsi-9" href="../hunch_net-2006/hunch_net-2006-04-27-Conferences%2C_Workshops%2C_and_Tutorials.html">174 hunch net-2006-04-27-Conferences, Workshops, and Tutorials</a></p>
<p>10 0.42345923 <a title="232-lsi-10" href="../hunch_net-2005/hunch_net-2005-05-03-Conference_attendance_is_mandatory.html">66 hunch net-2005-05-03-Conference attendance is mandatory</a></p>
<p>11 0.40340382 <a title="232-lsi-11" href="../hunch_net-2007/hunch_net-2007-11-02-The_Machine_Learning_Award_goes_to_%26%238230%3B.html">270 hunch net-2007-11-02-The Machine Learning Award goes to &#8230;</a></p>
<p>12 0.40222394 <a title="232-lsi-12" href="../hunch_net-2009/hunch_net-2009-01-08-Predictive_Analytics_World.html">335 hunch net-2009-01-08-Predictive Analytics World</a></p>
<p>13 0.39528227 <a title="232-lsi-13" href="../hunch_net-2009/hunch_net-2009-11-09-NYAS_ML_Symposium_this_year..html">377 hunch net-2009-11-09-NYAS ML Symposium this year.</a></p>
<p>14 0.38562155 <a title="232-lsi-14" href="../hunch_net-2009/hunch_net-2009-09-29-Machine_Learning_Protests_at_the_G20.html">372 hunch net-2009-09-29-Machine Learning Protests at the G20</a></p>
<p>15 0.38387197 <a title="232-lsi-15" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>16 0.38072318 <a title="232-lsi-16" href="../hunch_net-2005/hunch_net-2005-06-10-Workshops_are_not_Conferences.html">80 hunch net-2005-06-10-Workshops are not Conferences</a></p>
<p>17 0.37934059 <a title="232-lsi-17" href="../hunch_net-2008/hunch_net-2008-01-07-2008_Summer_Machine_Learning_Conference_Schedule.html">283 hunch net-2008-01-07-2008 Summer Machine Learning Conference Schedule</a></p>
<p>18 0.37441155 <a title="232-lsi-18" href="../hunch_net-2007/hunch_net-2007-04-30-COLT_2007.html">242 hunch net-2007-04-30-COLT 2007</a></p>
<p>19 0.37186816 <a title="232-lsi-19" href="../hunch_net-2006/hunch_net-2006-04-09-Progress_in_Machine_Translation.html">171 hunch net-2006-04-09-Progress in Machine Translation</a></p>
<p>20 0.36388102 <a title="232-lsi-20" href="../hunch_net-2006/hunch_net-2006-10-04-Health_of_Conferences_Wiki.html">212 hunch net-2006-10-04-Health of Conferences Wiki</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(27, 0.036), (48, 0.769)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95460808 <a title="232-lda-1" href="../hunch_net-2007/hunch_net-2007-02-11-24.html">232 hunch net-2007-02-11-24</a></p>
<p>Introduction: To commemorate the  Twenty Fourth Annual International Conference on Machine  Learning  (ICML-07), the FOX Network has decided to  launch a new spin-off series in prime time.  Through unofficial  sources, I have obtained the  story arc  for the first season, which appears frighteningly realistic.</p><p>2 0.54483128 <a title="232-lda-2" href="../hunch_net-2012/hunch_net-2012-06-29-ICML_survey_and_comments.html">468 hunch net-2012-06-29-ICML survey and comments</a></p>
<p>Introduction: Just about nothing could keep me from attending  ICML , except for  Dora  who arrived on Monday.  Consequently, I have only secondhand reports that the conference is going well.
 
For those who are remote (like me) or after the conference (like everyone),  Mark Reid  has setup the  ICML discussion  site where you can comment on any paper or subscribe to papers.  Authors are automatically subscribed to their own papers, so it should be possible to have a discussion significantly after the fact, as people desire.
 
We also conducted a survey before the conference and have the  survey results  now.  This can be compared with the  ICML 2010 survey results .  Looking at the comparable questions, we can sometimes order the answers to have scores ranging from 0 to 3 or 0 to 4 with 3 or 4 being best and 0 worst, then compute the average difference between 2012 and 2010.
 
Glancing through them, I see:
  
 Most people found the papers they reviewed a good fit for their expertise (-.037 w.r.t 20</p><p>3 0.50456381 <a title="232-lda-3" href="../hunch_net-2008/hunch_net-2008-06-09-The_Minimum_Sample_Complexity_of_Importance_Weighting.html">303 hunch net-2008-06-09-The Minimum Sample Complexity of Importance Weighting</a></p>
<p>Introduction: This post is about a trick that I learned from  Dale Schuurmans  which has been repeatedly useful for me over time.
 
The basic trick has to do with importance weighting for monte carlo integration.  Consider the problem of finding: 
 N = E x ~ D  f(x)  
given samples from  D  and knowledge of  f .
 
Often, we don’t have samples from  D  available.  Instead, we must make do with samples from some other distribution  Q .  In that case, we can still often solve the problem, as long as Q(x) isn’t 0 when D(x) is nonzero, using the importance weighting formula: 
 E x ~ Q  f(x) D(x)/Q(x) 
 
A basic question is: How many samples from  Q  are required in order to estimate  N  to some precision?  In general the convergence rate is not bounded, because  f(x) D(x)/Q(x)  is not bounded given the assumptions. 
Nevertheless, there is one special value  Q(x) = f(x) D(x) / N  where the sample complexity turns out to be  1 , which is typically substantially better than the sample complexity of the orig</p><p>4 0.49743608 <a title="232-lda-4" href="../hunch_net-2005/hunch_net-2005-03-24-The_Role_of_Workshops.html">46 hunch net-2005-03-24-The Role of Workshops</a></p>
<p>Introduction: A good workshop is often far more interesting than the papers at a conference.  This happens because a workshop has a much tighter focus than a conference.  Since you choose the workshops fitting your interest, the increased relevance can greatly enhance the level of your interest and attention.  Roughly speaking, a workshop program consists of elements related to a subject of your interest.  The main conference program consists of elements related to someoneâ&euro;&trade;s interest (which is rarely your own).  Workshops are more about doing research while conferences are more about presenting research.  
 
Several conferences have associated workshop programs, some with deadlines due shortly.
  
 
  ICML workshops  
 Due April 1 
 
 
  IJCAI workshops  
 Deadlines Vary 
 
 
 KDD workshops 
 Not yet finalized 
 
  
Anyone going to these conferences should examine the workshops and see if any are of interest.  (If none are, then maybe you should organize one next year.)</p><p>5 0.41683656 <a title="232-lda-5" href="../hunch_net-2008/hunch_net-2008-11-16-Observations_on_Linearity_for_Reductions_to_Regression.html">327 hunch net-2008-11-16-Observations on Linearity for Reductions to Regression</a></p>
<p>Introduction: Dean Foster  and  Daniel Hsu  had a couple observations about reductions to regression that I wanted to share.  This will make the most sense for people familiar with error correcting output codes (see the  tutorial, page 11 ).
 
Many people are comfortable using linear regression in a one-against-all style, where you try to predict the probability of choice  i  vs other classes, yet they are not comfortable with more complex error correcting codes because they fear that they create harder problems.  This fear turns out to be mathematically incoherent under a linear representation: comfort in the linear case should imply comfort with more complex codes.
 
In particular, If there exists a set of weight vectors  w i   such that  P(i|x)=, then for any invertible error correcting output code  C , there exists weight vectors  w c   which decode to perfectly predict the probability of each class.  The proof is simple and constructive: the weight vector  w c   can be constructed acc</p><p>6 0.39815226 <a title="232-lda-6" href="../hunch_net-2011/hunch_net-2011-09-28-Somebody%26%238217%3Bs_Eating_Your_Lunch.html">445 hunch net-2011-09-28-Somebody&#8217;s Eating Your Lunch</a></p>
<p>7 0.39586344 <a title="232-lda-7" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>8 0.15620601 <a title="232-lda-8" href="../hunch_net-2012/hunch_net-2012-06-05-ICML_acceptance_statistics.html">466 hunch net-2012-06-05-ICML acceptance statistics</a></p>
<p>9 0.12377405 <a title="232-lda-9" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>10 0.11549655 <a title="232-lda-10" href="../hunch_net-2011/hunch_net-2011-11-26-Giving_Thanks.html">449 hunch net-2011-11-26-Giving Thanks</a></p>
<p>11 0.11498248 <a title="232-lda-11" href="../hunch_net-2005/hunch_net-2005-05-28-Running_A_Machine_Learning_Summer_School.html">75 hunch net-2005-05-28-Running A Machine Learning Summer School</a></p>
<p>12 0.11333857 <a title="232-lda-12" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>13 0.1122718 <a title="232-lda-13" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>14 0.098891951 <a title="232-lda-14" href="../hunch_net-2012/hunch_net-2012-05-02-ICML%3A_Behind_the_Scenes.html">463 hunch net-2012-05-02-ICML: Behind the Scenes</a></p>
<p>15 0.09856236 <a title="232-lda-15" href="../hunch_net-2007/hunch_net-2007-08-19-Choice_of_Metrics.html">259 hunch net-2007-08-19-Choice of Metrics</a></p>
<p>16 0.09590216 <a title="232-lda-16" href="../hunch_net-2012/hunch_net-2012-04-09-ICML_author_feedback_is_open.html">461 hunch net-2012-04-09-ICML author feedback is open</a></p>
<p>17 0.095424414 <a title="232-lda-17" href="../hunch_net-2007/hunch_net-2007-12-20-Cool_and_Interesting_things_at_NIPS%2C_take_three.html">280 hunch net-2007-12-20-Cool and Interesting things at NIPS, take three</a></p>
<p>18 0.09422645 <a title="232-lda-18" href="../hunch_net-2013/hunch_net-2013-06-29-The_Benefits_of_Double-Blind_Review.html">485 hunch net-2013-06-29-The Benefits of Double-Blind Review</a></p>
<p>19 0.089891508 <a title="232-lda-19" href="../hunch_net-2012/hunch_net-2012-02-24-ICML%2B50%25.html">456 hunch net-2012-02-24-ICML+50%</a></p>
<p>20 0.089647993 <a title="232-lda-20" href="../hunch_net-2011/hunch_net-2011-04-11-The_Heritage_Health_Prize.html">430 hunch net-2011-04-11-The Heritage Health Prize</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
