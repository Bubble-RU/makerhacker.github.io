<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>232 hunch net-2007-02-11-24</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2007" href="../home/hunch_net-2007_home.html">hunch_net-2007</a> <a title="hunch_net-2007-232" href="#">hunch_net-2007-232</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>232 hunch net-2007-02-11-24</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2007-232-html" href="http://hunch.net/?p=252">html</a></p><p>Introduction: To commemorate theTwenty Fourth Annual International Conference on Machine
Learning(ICML-07), the FOX Network has decided to launch a new spin-off series
in prime time. Through unofficial sources, I have obtained thestory arcfor the
first season, which appears frighteningly realistic.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 To commemorate theTwenty Fourth Annual International Conference on Machine Learning(ICML-07), the FOX Network has decided to launch a new spin-off series in prime time. [sent-1, score-1.161]
</p><p>2 Through unofficial sources, I have obtained thestory arcfor the first season, which appears frighteningly realistic. [sent-2, score-0.864]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('launch', 0.327), ('frighteningly', 0.327), ('prime', 0.327), ('obtained', 0.303), ('realistic', 0.303), ('fourth', 0.286), ('season', 0.273), ('annual', 0.253), ('series', 0.232), ('international', 0.232), ('decided', 0.212), ('sources', 0.194), ('network', 0.188), ('appears', 0.149), ('conference', 0.114), ('first', 0.085), ('new', 0.063), ('machine', 0.051), ('learning', 0.021)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="232-tfidf-1" href="../hunch_net-2007/hunch_net-2007-02-11-24.html">232 hunch net-2007-02-11-24</a></p>
<p>Introduction: To commemorate theTwenty Fourth Annual International Conference on Machine
Learning(ICML-07), the FOX Network has decided to launch a new spin-off series
in prime time. Through unofficial sources, I have obtained thestory arcfor the
first season, which appears frighteningly realistic.</p><p>2 0.089517012 <a title="232-tfidf-2" href="../hunch_net-2007/hunch_net-2007-10-19-Second_Annual_Reinforcement_Learning_Competition.html">268 hunch net-2007-10-19-Second Annual Reinforcement Learning Competition</a></p>
<p>Introduction: The Second Annual Reinforcement Learning Competition is about to get started.
The aim of the competition is to facilitate direct comparisons between various
learning methods on important and realistic domains. This year's event will
feature well-known benchmark domains as well as more challenging problems of
real-world complexity, such as helicopter control and robot soccer
keepaway.The competition begins on November 1st, 2007 when training software
is released. Results must be submitted by July 1st, 2008. The competition will
culminate in an event at ICML-08 in Helsinki, Finland, at which the winners
will be announced.For more information, visitthe competition website.</p><p>3 0.089234978 <a title="232-tfidf-3" href="../hunch_net-2005/hunch_net-2005-02-02-Paper_Deadlines.html">11 hunch net-2005-02-02-Paper Deadlines</a></p>
<p>Introduction: It's conference season, and smell of budding papers is in the air.IJCAI 2005,
January 21COLT 2005, February 2KDD 2005, February 18ICML 2005, March 8UAI
2005, March 16AAAI 2005, March 18</p><p>4 0.062861383 <a title="232-tfidf-4" href="../hunch_net-2005/hunch_net-2005-02-09-Intuitions_from_applied_learning.html">16 hunch net-2005-02-09-Intuitions from applied learning</a></p>
<p>Introduction: Since learning is far from an exact science, it's good to pay attention to
basic intuitions of applied learning. Here are a few I've
collected.IntegrationIn Bayesian learning, the posterior is computed by an
integral, and the optimal thing to do is to predict according to this
integral. This phenomena seems to be far more general. Bagging, Boosting,
SVMs, and Neural Networks all take advantage of this idea to some extent. The
phenomena is more general: you can average over many differentclassification
predictorsto improve performance.
Sources:Zoubin,CaruanaDifferentiationDifferent pieces of an average should
differentiate to achieve good performance by different methods. This is know
as the 'symmetry breaking' problem for neural networks, and it's why weights
are initialized randomly. Boosting explicitly attempts to achieve good
differentiation by creating new, different, learning problems. Sources:Yann
LeCun,Phil LongDeep RepresentationHaving a deep representation is necessary
for hav</p><p>5 0.061544597 <a title="232-tfidf-5" href="../hunch_net-2007/hunch_net-2007-07-13-The_View_From_China.html">255 hunch net-2007-07-13-The View From China</a></p>
<p>Introduction: I'm visiting Beijing for thePao-Lu Hsu Statistics Conferenceon Machine
Learning.I had several discussions about the state of Chinese research. Given
the large population and economy, you might expect substantial research--more
than has been observed at international conferences. The fundamental problem
seems to be theCultural Revolutionwhich lobotimized higher education, and the
research associated with it. There has been a process of slow recovery since
then, which has begun to be felt in the research world via increased
participation in international conferences and (now) conferences in China.The
amount of effort going into construction in Beijing is very impressive--people
are literally building a skyscraper at night outside the window of the hotel
I'm staying at (and this is not unusual). If a small fraction of this effort
is later focused onto supporting research, the effect could be very
substantial. General growth in China's research portfolio should be expected.</p><p>6 0.061491448 <a title="232-tfidf-6" href="../hunch_net-2007/hunch_net-2007-01-04-2007_Summer_Machine_Learning_Conferences.html">226 hunch net-2007-01-04-2007 Summer Machine Learning Conferences</a></p>
<p>7 0.055025667 <a title="232-tfidf-7" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>8 0.053487808 <a title="232-tfidf-8" href="../hunch_net-2007/hunch_net-2007-04-30-COLT_2007.html">242 hunch net-2007-04-30-COLT 2007</a></p>
<p>9 0.049575508 <a title="232-tfidf-9" href="../hunch_net-2012/hunch_net-2012-10-18-7th_Annual_Machine_Learning_Symposium.html">474 hunch net-2012-10-18-7th Annual Machine Learning Symposium</a></p>
<p>10 0.049542837 <a title="232-tfidf-10" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>11 0.047726735 <a title="232-tfidf-11" href="../hunch_net-2009/hunch_net-2009-12-24-Top_graduates_this_season.html">384 hunch net-2009-12-24-Top graduates this season</a></p>
<p>12 0.045407638 <a title="232-tfidf-12" href="../hunch_net-2009/hunch_net-2009-12-27-Interesting_things_at_NIPS_2009.html">385 hunch net-2009-12-27-Interesting things at NIPS 2009</a></p>
<p>13 0.045307674 <a title="232-tfidf-13" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>14 0.042769995 <a title="232-tfidf-14" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>15 0.042572018 <a title="232-tfidf-15" href="../hunch_net-2008/hunch_net-2008-08-04-Electoralmarkets.com.html">312 hunch net-2008-08-04-Electoralmarkets.com</a></p>
<p>16 0.04132919 <a title="232-tfidf-16" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>17 0.037819847 <a title="232-tfidf-17" href="../hunch_net-2007/hunch_net-2007-02-22-Create_Your_Own_ICML_Workshop.html">234 hunch net-2007-02-22-Create Your Own ICML Workshop</a></p>
<p>18 0.036429882 <a title="232-tfidf-18" href="../hunch_net-2006/hunch_net-2006-04-27-Conferences%2C_Workshops%2C_and_Tutorials.html">174 hunch net-2006-04-27-Conferences, Workshops, and Tutorials</a></p>
<p>19 0.035376638 <a title="232-tfidf-19" href="../hunch_net-2005/hunch_net-2005-07-13-%26%238220%3BSister_Conference%26%238221%3B_presentations.html">93 hunch net-2005-07-13-&#8220;Sister Conference&#8221; presentations</a></p>
<p>20 0.035041526 <a title="232-tfidf-20" href="../hunch_net-2006/hunch_net-2006-01-06-MLTV.html">146 hunch net-2006-01-06-MLTV</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.046), (1, 0.028), (2, 0.036), (3, 0.04), (4, -0.006), (5, 0.008), (6, -0.046), (7, 0.012), (8, 0.014), (9, 0.015), (10, -0.022), (11, -0.051), (12, -0.006), (13, 0.024), (14, -0.053), (15, -0.004), (16, -0.006), (17, 0.046), (18, -0.015), (19, -0.027), (20, -0.01), (21, -0.007), (22, -0.004), (23, -0.05), (24, -0.065), (25, 0.036), (26, 0.02), (27, 0.027), (28, -0.028), (29, -0.071), (30, -0.017), (31, 0.065), (32, -0.02), (33, 0.027), (34, 0.018), (35, 0.011), (36, 0.03), (37, 0.008), (38, -0.024), (39, -0.044), (40, 0.01), (41, -0.065), (42, 0.067), (43, -0.039), (44, 0.085), (45, 0.017), (46, 0.029), (47, -0.017), (48, 0.031), (49, -0.021)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.91858816 <a title="232-lsi-1" href="../hunch_net-2007/hunch_net-2007-02-11-24.html">232 hunch net-2007-02-11-24</a></p>
<p>Introduction: To commemorate theTwenty Fourth Annual International Conference on Machine
Learning(ICML-07), the FOX Network has decided to launch a new spin-off series
in prime time. Through unofficial sources, I have obtained thestory arcfor the
first season, which appears frighteningly realistic.</p><p>2 0.57503843 <a title="232-lsi-2" href="../hunch_net-2005/hunch_net-2005-07-13-%26%238220%3BSister_Conference%26%238221%3B_presentations.html">93 hunch net-2005-07-13-&#8220;Sister Conference&#8221; presentations</a></p>
<p>Introduction: Some of the "sister conference" presentations atAAAIhave been great. Roughly
speaking, the conference organizers asked other conference organizers to come
give a summary of their conference. Many different AI-related conferences
accepted. The presenters typically discuss some of the background and goals of
the conference then mention the results from a few papers they liked. This is
great because it provides a mechanism to get a digested overview of the work
of several thousand researchers--something which is simply available nowhere
else.Based on these presentations, it looks like there is a significant
component of (and opportunity for) applied machine learning inAIIDE,IUI,
andACL.There was also some discussion of having a super-colocation event
similar toFCRC, but centered on AI & Learning. This seems like a fine idea.
The field is fractured across so many different conferences that the mixing of
a supercolocation seems likely helpful for research.</p><p>3 0.4950659 <a title="232-lsi-3" href="../hunch_net-2006/hunch_net-2006-01-06-MLTV.html">146 hunch net-2006-01-06-MLTV</a></p>
<p>Introduction: As part of aPASCALproject, the Slovenians have been filming various machine
learning events and placing them on the webhere. This includes, for example,
theChicago 2005 Machine Learning Summer Schoolas well as a number of other
summer schools, workshops, and conferences.There are some significant caveats
here--for example, I can't access it from Linux. Based upon the webserver
logs, I expect that is a problem for most people--Computer scientists are
particularly nonstandard in their choice of computing platform.Nevertheless,
the core idea here is excellent and details of compatibility can be fixed
later. With modern technology toys, there is no fundamental reason why the
process of announcing new work at a conference should happen only once and
only for the people who could make it to that room in that conference. The
problems solved include:The multitrack vs. single-track debate. ("Sometimes
the single track doesn't interest me" vs. "When it's multitrack I miss good
talks""I couldn't</p><p>4 0.48900265 <a title="232-lsi-4" href="../hunch_net-2005/hunch_net-2005-02-17-Learning_Research_Programs.html">21 hunch net-2005-02-17-Learning Research Programs</a></p>
<p>Introduction: This is an attempt to organize the broad research programs related to machine
learning currently underway. This isn't easy--this map is partial, the
categories often overlap, and there are many details left out. Nevertheless,
it is (perhaps) helpful to have some map of what is happening where. The word
'typical' should not be construed narrowly here.Learning TheoryFocuses on
analyzing mathematical models of learning, essentially no experiments. Typical
conference: COLT.Bayesian LearningBayes law is always used. Focus on methods
of speeding up or approximating integration, new probabilistic models, and
practical applications. Typical conferences: NIPS,UAIStructured
learningPredicting complex structured outputs, some applications. Typiical
conferences: NIPS, UAI, othersReinforcement LearningFocused on 'agent-in-the-
world' learning problems where the goal is optimizing reward. Typical
conferences: ICMLUnsupervised Learning/Clustering/Dimensionality
ReductionFocused on simpiflying data. T</p><p>5 0.46157882 <a title="232-lsi-5" href="../hunch_net-2009/hunch_net-2009-11-09-NYAS_ML_Symposium_this_year..html">377 hunch net-2009-11-09-NYAS ML Symposium this year.</a></p>
<p>Introduction: TheNYAS ML symposiumgrew again this year to 170 participants, despite the need
to outsmart or otherwise tunnel througha crowd.Perhaps the most distinct talk
was by Bob Bell on various aspects of theNetflix prizecompetition. I also
enjoyed several student posters includingMatt Hoffman's cool examples of blind
source separation for music.I'm somewhat surprised how much the workshop has
grown, as it is now comparable in size to a small conference, although in
style more similar to a workshop. At some point as an event grows, it becomes
owned by the community rather than the organizers, so if anyone has
suggestions on improving it, speak up and be heard.</p><p>6 0.45677739 <a title="232-lsi-6" href="../hunch_net-2010/hunch_net-2010-10-29-To_Vidoelecture_or_not.html">416 hunch net-2010-10-29-To Vidoelecture or not</a></p>
<p>7 0.44933695 <a title="232-lsi-7" href="../hunch_net-2013/hunch_net-2013-05-04-COLT_and_ICML_registration.html">482 hunch net-2013-05-04-COLT and ICML registration</a></p>
<p>8 0.43688276 <a title="232-lsi-8" href="../hunch_net-2007/hunch_net-2007-04-30-COLT_2007.html">242 hunch net-2007-04-30-COLT 2007</a></p>
<p>9 0.42940402 <a title="232-lsi-9" href="../hunch_net-2005/hunch_net-2005-02-02-Paper_Deadlines.html">11 hunch net-2005-02-02-Paper Deadlines</a></p>
<p>10 0.42872944 <a title="232-lsi-10" href="../hunch_net-2005/hunch_net-2005-06-10-Workshops_are_not_Conferences.html">80 hunch net-2005-06-10-Workshops are not Conferences</a></p>
<p>11 0.41214821 <a title="232-lsi-11" href="../hunch_net-2014/hunch_net-2014-03-11-The_New_York_ML_Symposium%2C_take_2.html">494 hunch net-2014-03-11-The New York ML Symposium, take 2</a></p>
<p>12 0.4093143 <a title="232-lsi-12" href="../hunch_net-2007/hunch_net-2007-10-19-Second_Annual_Reinforcement_Learning_Competition.html">268 hunch net-2007-10-19-Second Annual Reinforcement Learning Competition</a></p>
<p>13 0.39460698 <a title="232-lsi-13" href="../hunch_net-2006/hunch_net-2006-04-09-Progress_in_Machine_Translation.html">171 hunch net-2006-04-09-Progress in Machine Translation</a></p>
<p>14 0.39059222 <a title="232-lsi-14" href="../hunch_net-2009/hunch_net-2009-01-08-Predictive_Analytics_World.html">335 hunch net-2009-01-08-Predictive Analytics World</a></p>
<p>15 0.38512889 <a title="232-lsi-15" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>16 0.36158225 <a title="232-lsi-16" href="../hunch_net-2006/hunch_net-2006-10-04-Health_of_Conferences_Wiki.html">212 hunch net-2006-10-04-Health of Conferences Wiki</a></p>
<p>17 0.35383067 <a title="232-lsi-17" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>18 0.34906331 <a title="232-lsi-18" href="../hunch_net-2013/hunch_net-2013-09-20-No_NY_ML_Symposium_in_2013%2C_and_some_good_news.html">489 hunch net-2013-09-20-No NY ML Symposium in 2013, and some good news</a></p>
<p>19 0.34881365 <a title="232-lsi-19" href="../hunch_net-2007/hunch_net-2007-11-16-MLSS_2008.html">273 hunch net-2007-11-16-MLSS 2008</a></p>
<p>20 0.34809044 <a title="232-lsi-20" href="../hunch_net-2012/hunch_net-2012-10-26-ML_Symposium_and_Strata-Hadoop_World.html">475 hunch net-2012-10-26-ML Symposium and Strata-Hadoop World</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(42, 0.07), (95, 0.732)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96930683 <a title="232-lda-1" href="../hunch_net-2007/hunch_net-2007-02-11-24.html">232 hunch net-2007-02-11-24</a></p>
<p>Introduction: To commemorate theTwenty Fourth Annual International Conference on Machine
Learning(ICML-07), the FOX Network has decided to launch a new spin-off series
in prime time. Through unofficial sources, I have obtained thestory arcfor the
first season, which appears frighteningly realistic.</p><p>2 0.93123353 <a title="232-lda-2" href="../hunch_net-2009/hunch_net-2009-11-23-ICML_2009_Workshops_%28and_Tutorials%29.html">379 hunch net-2009-11-23-ICML 2009 Workshops (and Tutorials)</a></p>
<p>Introduction: I'm theworkshops chairforICMLthis year. As such, I would like to personally
encourage people to consider running a workshop.My general view of workshops
is that they are excellent as opportunities to discuss and develop research
directions--some of my best work has come from collaborations at workshops and
several workshops have substantially altered my thinking about various
problems. My experience running workshops is that setting them up and making
them fly often appears much harder than it actually is, and the workshops
often come off much better than expected in the end. Submissions are due
January 18, two weeks before papers.Similarly,Ben Taskaris looking for
goodtutorials, which is complementary. Workshops are about exploring a
subject, while a tutorial is about distilling it down into an easily taught
essence, a vital part of the research process. Tutorials are due February 13,
two weeks after papers.</p><p>3 0.92107892 <a title="232-lda-3" href="../hunch_net-2010/hunch_net-2010-11-18-ICML_2011_%26%238211%3B_Call_for_Tutorials.html">417 hunch net-2010-11-18-ICML 2011 &#8211; Call for Tutorials</a></p>
<p>Introduction: I would like to encourage people to consider giving a tutorial at next years
ICML. The ideal tutorial attracts a wide audience, provides a gentle and
easily taught introduction to the chosen research area, and also covers the
most important contributions in depth.Submissions are due January 14 Â (about
two weeks before paper
deadline).http://www.icml-2011.org/tutorials.phpRegards,Ulf</p><p>4 0.82688904 <a title="232-lda-4" href="../hunch_net-2008/hunch_net-2008-07-04-More_Presentation_Preparation.html">307 hunch net-2008-07-04-More Presentation Preparation</a></p>
<p>Introduction: We've discussedpresentation preparation before, but I have one more thing to
add:transitioning. For a research presentation, it is substantially helpful
for the audience if transitions are clear. A common outline for a research
presentation in machine leanring is:The problem. Presentations which don't
describe the problem almost immediately lose people, because the context is
missing to understand the detail.Prior relevant work. In many cases, a paper
builds on some previous bit of work which must be understood in order to
understand what the paper does. A common failure mode seems to be spending too
much time on prior work. Discuss just the relevant aspects of prior work in
the language of your work. Sometimes this is missing when unneeded.What we
did. For theory papers in particular, it is often not possible to really cover
the details. Prioritizing what you present can be very important.How it
worked. Many papers in Machine Learning have some sort of experimental test of
the algorit</p><p>5 0.75945973 <a title="232-lda-5" href="../hunch_net-2006/hunch_net-2006-01-08-Debugging_Your_Brain.html">147 hunch net-2006-01-08-Debugging Your Brain</a></p>
<p>Introduction: One part of doing research is debugging your understanding of reality. This is
hard work: How do you even discover where you misunderstand? If you discover a
misunderstanding, how do you go about removing it?The process of debugging
computer programs is quite analogous to debugging reality misunderstandings.
This is natural--a bug in a computer program is a misunderstanding between you
and the computer about what you said. Many of the familiar techniques from
debugging have exact parallels.DetailsWhen programming, there are often signs
that some bug exists like: "the graph my program output is shifted a little
bit" = maybe you have an indexing error. In debugging yourself, we often have
some impression that something is "not right". These impressions should be
addressed directly and immediately. (Some people have the habit of suppressing
worries in favor of excess certainty. That's not healthy for research.)Corner
CasesA "corner case" is an input to a program which is extreme in some w</p><p>6 0.71042329 <a title="232-lda-6" href="../hunch_net-2010/hunch_net-2010-12-02-Traffic_Prediction_Problem.html">418 hunch net-2010-12-02-Traffic Prediction Problem</a></p>
<p>7 0.68172878 <a title="232-lda-7" href="../hunch_net-2005/hunch_net-2005-05-10-Learning_Reductions_are_Reductionist.html">68 hunch net-2005-05-10-Learning Reductions are Reductionist</a></p>
<p>8 0.64423543 <a title="232-lda-8" href="../hunch_net-2006/hunch_net-2006-10-22-Exemplar_programming.html">215 hunch net-2006-10-22-Exemplar programming</a></p>
<p>9 0.62856656 <a title="232-lda-9" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>10 0.61812013 <a title="232-lda-10" href="../hunch_net-2012/hunch_net-2012-04-09-ICML_author_feedback_is_open.html">461 hunch net-2012-04-09-ICML author feedback is open</a></p>
<p>11 0.4775652 <a title="232-lda-11" href="../hunch_net-2005/hunch_net-2005-03-24-The_Role_of_Workshops.html">46 hunch net-2005-03-24-The Role of Workshops</a></p>
<p>12 0.43598729 <a title="232-lda-12" href="../hunch_net-2011/hunch_net-2011-05-09-CI_Fellows%2C_again.html">434 hunch net-2011-05-09-CI Fellows, again</a></p>
<p>13 0.37659553 <a title="232-lda-13" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>14 0.36212155 <a title="232-lda-14" href="../hunch_net-2007/hunch_net-2007-06-21-Presentation_Preparation.html">249 hunch net-2007-06-21-Presentation Preparation</a></p>
<p>15 0.35804552 <a title="232-lda-15" href="../hunch_net-2010/hunch_net-2010-01-19-Deadline_Season%2C_2010.html">387 hunch net-2010-01-19-Deadline Season, 2010</a></p>
<p>16 0.34228086 <a title="232-lda-16" href="../hunch_net-2005/hunch_net-2005-05-14-NIPS.html">71 hunch net-2005-05-14-NIPS</a></p>
<p>17 0.33536553 <a title="232-lda-17" href="../hunch_net-2011/hunch_net-2011-08-20-The_Large_Scale_Learning_Survey_Tutorial.html">442 hunch net-2011-08-20-The Large Scale Learning Survey Tutorial</a></p>
<p>18 0.33376595 <a title="232-lda-18" href="../hunch_net-2013/hunch_net-2013-06-10-The_Large_Scale_Learning_class_notes.html">483 hunch net-2013-06-10-The Large Scale Learning class notes</a></p>
<p>19 0.3145681 <a title="232-lda-19" href="../hunch_net-2009/hunch_net-2009-10-26-NIPS_workshops.html">375 hunch net-2009-10-26-NIPS workshops</a></p>
<p>20 0.31120503 <a title="232-lda-20" href="../hunch_net-2008/hunch_net-2008-11-09-A_Healthy__COLT.html">324 hunch net-2008-11-09-A Healthy  COLT</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
