<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>265 hunch net-2007-10-14-NIPS workshp: Learning Problem Design</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2007" href="../home/hunch_net-2007_home.html">hunch_net-2007</a> <a title="hunch_net-2007-265" href="#">hunch_net-2007-265</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>265 hunch net-2007-10-14-NIPS workshp: Learning Problem Design</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2007-265-html" href="http://hunch.net/?p=294">html</a></p><p>Introduction: Alinaand I are organizing a workshop onLearning Problem DesignatNIPS.What is
learning problem design?It's about being clever in creating learning problems
from otherwise unlabeled data. Read the webpage above for examples.I want to
participate!Email us before Nov. 1 with a description of what you want to talk
about.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Alinaand I are organizing a workshop onLearning Problem DesignatNIPS. [sent-1, score-0.46]
</p><p>2 It's about being clever in creating learning problems from otherwise unlabeled data. [sent-3, score-1.085]
</p><p>3 1 with a description of what you want to talk about. [sent-7, score-0.665]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('onlearning', 0.398), ('clever', 0.308), ('participate', 0.298), ('webpage', 0.29), ('organizing', 0.275), ('description', 0.253), ('email', 0.244), ('unlabeled', 0.244), ('want', 0.232), ('read', 0.202), ('otherwise', 0.198), ('creating', 0.193), ('workshop', 0.185), ('talk', 0.18), ('design', 0.166), ('problem', 0.15), ('us', 0.131), ('problems', 0.09), ('learning', 0.052)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="265-tfidf-1" href="../hunch_net-2007/hunch_net-2007-10-14-NIPS_workshp%3A_Learning_Problem_Design.html">265 hunch net-2007-10-14-NIPS workshp: Learning Problem Design</a></p>
<p>Introduction: Alinaand I are organizing a workshop onLearning Problem DesignatNIPS.What is
learning problem design?It's about being clever in creating learning problems
from otherwise unlabeled data. Read the webpage above for examples.I want to
participate!Email us before Nov. 1 with a description of what you want to talk
about.</p><p>2 0.13121709 <a title="265-tfidf-2" href="../hunch_net-2007/hunch_net-2007-02-22-Create_Your_Own_ICML_Workshop.html">234 hunch net-2007-02-22-Create Your Own ICML Workshop</a></p>
<p>Introduction: As usualICML 2007will be hosting aworkshop programto be held this year on June
24th. The success of the program depends on having researchers like you
propose interesting workshop topics and then organize the workshops. I'd like
to encourage all of you to consider sending a workshop proposal. The proposal
deadline has been extended to March 5. See the workshop web-site for
details.Organizing a workshop is a unique way to gather an international group
of researchers together to focus for an entire day on a topic of your
choosing. I've always found that the cost of organizing a workshop is not so
large, and very low compared to the benefits. The topic and format of a
workshop are limited only by your imagination (and the attractiveness to
potential participants) and need not follow the usual model of a mini-
conference on a particular ML sub-area. Hope to see some interesting proposals
rolling in.</p><p>3 0.11609053 <a title="265-tfidf-3" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>Introduction: AtKDDI enjoyedStephen Boyd's invited talk about optimization quite a bit.
However, the most interesting talk for me wasDavid Haussler's. His talk
started out with a formidable load of biological complexity. About half-way
through you start wondering, "can this be used to help with cancer?" And at
the end he connects it directly to use with a call to arms for the audience:
cure cancer. The core thesis here is that cancer is a complex set of diseases
which can be distentangled via genetic assays, allowing attacking the specific
signature of individual cancers. However, the data quantity and complex
dependencies within the data require systematic and relatively automatic
prediction and analysis algorithms of the kind that we are best familiar
with.Some of the papers which interested me are:Kai-Wei ChangandDan
Roth,Selective Block Minimization for Faster Convergence of Limited Memory
Large-Scale Linear Models, which is about effectively using a hard-example
cache to speedup learning.Leland</p><p>4 0.11160827 <a title="265-tfidf-4" href="../hunch_net-2010/hunch_net-2010-08-20-The_Workshop_on_Cores%2C_Clusters%2C_and_Clouds.html">404 hunch net-2010-08-20-The Workshop on Cores, Clusters, and Clouds</a></p>
<p>Introduction: Alekh,John,Ofer, and I are organizing aworkshopatNIPSthis year on learning in
parallel and distributed environments. The general interest level in parallel
learning seems to be growing rapidly, so I expect quite a bit of attendance.
Please join us if you are parallel-interested.And, if you are working in the
area of parallel learning, please considersubmitting an abstractdue Oct. 17
for presentation at the workshop.</p><p>5 0.11140691 <a title="265-tfidf-5" href="../hunch_net-2006/hunch_net-2006-07-12-Who_is_having_visa_problems_reaching_US_conferences%3F.html">195 hunch net-2006-07-12-Who is having visa problems reaching US conferences?</a></p>
<p>Introduction: Many of the large machine learning conferences were in the US this summer. A
common problem which students from abroad encounter is visa issues.Just
getting a visa to visit can be pretty rough: you stand around in lines,
sometimes for days. Even worse is the timing with respect to ticket buying.
Airplane tickets typically need to be bought well in advance on nonrefundable
terms to secure a reasonable rate for air travel. When a visa is denied, as
happens reasonably often, a very expensive ticket is burnt.A serious effort is
under way to raise this as in issue in need of fixing. Over the long term,
effectively driving research conferences to locate outside of the US seems an
unwise policy.Robert Schapireis planning to talk to a congressman.Sally
Goldmansuggested putting together a list of problem cases, andPhil Longsetup
an email addressimmigration.and.confs@gmail.comto collect them.If you (or
someone you know) has had insurmountable difficulties reaching a conference in
the US, please</p><p>6 0.10919261 <a title="265-tfidf-6" href="../hunch_net-2008/hunch_net-2008-10-19-NIPS_2008_workshop_on_Kernel_Learning.html">321 hunch net-2008-10-19-NIPS 2008 workshop on Kernel Learning</a></p>
<p>7 0.10536183 <a title="265-tfidf-7" href="../hunch_net-2007/hunch_net-2007-12-12-Workshop_Summary%26%238212%3BPrinciples_of_Learning_Problem_Design.html">277 hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</a></p>
<p>8 0.10207035 <a title="265-tfidf-8" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>9 0.099290259 <a title="265-tfidf-9" href="../hunch_net-2010/hunch_net-2010-06-20-2010_ICML_discussion_site.html">401 hunch net-2010-06-20-2010 ICML discussion site</a></p>
<p>10 0.092118256 <a title="265-tfidf-10" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>11 0.081307165 <a title="265-tfidf-11" href="../hunch_net-2012/hunch_net-2012-03-06-COLT-ICML_Open_Questions_and_ICML_Instructions.html">458 hunch net-2012-03-06-COLT-ICML Open Questions and ICML Instructions</a></p>
<p>12 0.07950227 <a title="265-tfidf-12" href="../hunch_net-2006/hunch_net-2006-03-27-Gradients_everywhere.html">167 hunch net-2006-03-27-Gradients everywhere</a></p>
<p>13 0.077151202 <a title="265-tfidf-13" href="../hunch_net-2009/hunch_net-2009-11-06-Yisong_Yue_on_Self-improving_Systems.html">376 hunch net-2009-11-06-Yisong Yue on Self-improving Systems</a></p>
<p>14 0.07453794 <a title="265-tfidf-14" href="../hunch_net-2006/hunch_net-2006-03-05-%26%238220%3BStructural%26%238221%3B_Learning.html">161 hunch net-2006-03-05-&#8220;Structural&#8221; Learning</a></p>
<p>15 0.073899783 <a title="265-tfidf-15" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>16 0.072937064 <a title="265-tfidf-16" href="../hunch_net-2006/hunch_net-2006-07-25-Upcoming_conference.html">198 hunch net-2006-07-25-Upcoming conference</a></p>
<p>17 0.068227135 <a title="265-tfidf-17" href="../hunch_net-2005/hunch_net-2005-03-24-The_Role_of_Workshops.html">46 hunch net-2005-03-24-The Role of Workshops</a></p>
<p>18 0.0664986 <a title="265-tfidf-18" href="../hunch_net-2013/hunch_net-2013-08-31-Extreme_Classification_workshop_at_NIPS.html">488 hunch net-2013-08-31-Extreme Classification workshop at NIPS</a></p>
<p>19 0.066346988 <a title="265-tfidf-19" href="../hunch_net-2008/hunch_net-2008-10-01-NIPS_2008_workshop_on_%26%238216%3BLearning_over_Empirical_Hypothesis_Spaces%26%238217%3B.html">319 hunch net-2008-10-01-NIPS 2008 workshop on &#8216;Learning over Empirical Hypothesis Spaces&#8217;</a></p>
<p>20 0.066109002 <a title="265-tfidf-20" href="../hunch_net-2008/hunch_net-2008-04-27-Watchword%3A_Supervised_Learning.html">299 hunch net-2008-04-27-Watchword: Supervised Learning</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.106), (1, 0.009), (2, 0.061), (3, 0.062), (4, -0.036), (5, -0.047), (6, -0.055), (7, -0.031), (8, -0.04), (9, 0.114), (10, -0.017), (11, 0.126), (12, 0.044), (13, 0.115), (14, -0.042), (15, -0.069), (16, -0.004), (17, 0.013), (18, -0.012), (19, 0.056), (20, -0.077), (21, 0.067), (22, 0.06), (23, 0.056), (24, -0.168), (25, -0.062), (26, 0.021), (27, -0.145), (28, 0.034), (29, -0.036), (30, -0.056), (31, 0.062), (32, -0.051), (33, -0.023), (34, 0.004), (35, 0.05), (36, 0.033), (37, 0.025), (38, 0.091), (39, 0.02), (40, -0.021), (41, 0.087), (42, 0.062), (43, 0.017), (44, -0.024), (45, 0.09), (46, -0.034), (47, -0.001), (48, -0.099), (49, -0.021)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94628745 <a title="265-lsi-1" href="../hunch_net-2007/hunch_net-2007-10-14-NIPS_workshp%3A_Learning_Problem_Design.html">265 hunch net-2007-10-14-NIPS workshp: Learning Problem Design</a></p>
<p>Introduction: Alinaand I are organizing a workshop onLearning Problem DesignatNIPS.What is
learning problem design?It's about being clever in creating learning problems
from otherwise unlabeled data. Read the webpage above for examples.I want to
participate!Email us before Nov. 1 with a description of what you want to talk
about.</p><p>2 0.65955073 <a title="265-lsi-2" href="../hunch_net-2007/hunch_net-2007-02-22-Create_Your_Own_ICML_Workshop.html">234 hunch net-2007-02-22-Create Your Own ICML Workshop</a></p>
<p>Introduction: As usualICML 2007will be hosting aworkshop programto be held this year on June
24th. The success of the program depends on having researchers like you
propose interesting workshop topics and then organize the workshops. I'd like
to encourage all of you to consider sending a workshop proposal. The proposal
deadline has been extended to March 5. See the workshop web-site for
details.Organizing a workshop is a unique way to gather an international group
of researchers together to focus for an entire day on a topic of your
choosing. I've always found that the cost of organizing a workshop is not so
large, and very low compared to the benefits. The topic and format of a
workshop are limited only by your imagination (and the attractiveness to
potential participants) and need not follow the usual model of a mini-
conference on a particular ML sub-area. Hope to see some interesting proposals
rolling in.</p><p>3 0.62797278 <a title="265-lsi-3" href="../hunch_net-2005/hunch_net-2005-09-20-Workshop_Proposal%3A_Atomic_Learning.html">114 hunch net-2005-09-20-Workshop Proposal: Atomic Learning</a></p>
<p>Introduction: This is a proposal for a workshop. It may or may not happen depending on the
level of interest. If you are interested, feel free to indicate so (by email
or comments).Description:Assume(*) that any system for solving large difficult
learning problems must decompose into repeated use of basic elements (i.e.
atoms). There are many basic questions which remain:What are the viable basic
elements?What makes a basic element viable?What are the viable principles for
the composition of these basic elements?What are the viable principles for
learning in such systems?What problems can this approach handle?Hal Daume
adds:Can composition of atoms be (semi-) automatically constructed[?]When
atoms are constructed through reductions, is there some notion of the
"naturalness" of the created leaning problems?Other than Markov
fields/graphical models/Bayes nets, is there a good language for representing
atoms and their compositions?The answer to these and related questions remain
unclear to me. A worksh</p><p>4 0.58778358 <a title="265-lsi-4" href="../hunch_net-2008/hunch_net-2008-10-01-NIPS_2008_workshop_on_%26%238216%3BLearning_over_Empirical_Hypothesis_Spaces%26%238217%3B.html">319 hunch net-2008-10-01-NIPS 2008 workshop on &#8216;Learning over Empirical Hypothesis Spaces&#8217;</a></p>
<p>Introduction: This workshop asks for insights how far we may/can push the theoretical
boundary of using data in the design of learning machines. Can we express our
classification rule in terms of the sample, or do we have to stick to a core
assumption of classical statistical learning theory, namely that the
hypothesis space is to be defined independent from the sample? This workshop
is particularly interested in - but not restricted to - the 'luckiness
framework' and the recently introduced notion of 'compatibility functions' in
a semi-supervised learning context (more information can be found
athttp://www.kuleuven.be/wehys).</p><p>5 0.56575513 <a title="265-lsi-5" href="../hunch_net-2006/hunch_net-2006-07-25-Upcoming_conference.html">198 hunch net-2006-07-25-Upcoming conference</a></p>
<p>Introduction: The Workshop for Women in Machine Learning will be held in San Diego on
October 4, 2006.For details see the workshop
website:http://www.seas.upenn.edu/~wiml/</p><p>6 0.53210819 <a title="265-lsi-6" href="../hunch_net-2007/hunch_net-2007-12-12-Workshop_Summary%26%238212%3BPrinciples_of_Learning_Problem_Design.html">277 hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</a></p>
<p>7 0.51463467 <a title="265-lsi-7" href="../hunch_net-2008/hunch_net-2008-10-19-NIPS_2008_workshop_on_Kernel_Learning.html">321 hunch net-2008-10-19-NIPS 2008 workshop on Kernel Learning</a></p>
<p>8 0.51369703 <a title="265-lsi-8" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>9 0.51016277 <a title="265-lsi-9" href="../hunch_net-2006/hunch_net-2006-07-12-Who_is_having_visa_problems_reaching_US_conferences%3F.html">195 hunch net-2006-07-12-Who is having visa problems reaching US conferences?</a></p>
<p>10 0.49171916 <a title="265-lsi-10" href="../hunch_net-2009/hunch_net-2009-11-06-Yisong_Yue_on_Self-improving_Systems.html">376 hunch net-2009-11-06-Yisong Yue on Self-improving Systems</a></p>
<p>11 0.43150178 <a title="265-lsi-11" href="../hunch_net-2005/hunch_net-2005-06-10-Workshops_are_not_Conferences.html">80 hunch net-2005-06-10-Workshops are not Conferences</a></p>
<p>12 0.42207932 <a title="265-lsi-12" href="../hunch_net-2005/hunch_net-2005-09-19-NIPS_Workshops.html">113 hunch net-2005-09-19-NIPS Workshops</a></p>
<p>13 0.41326025 <a title="265-lsi-13" href="../hunch_net-2012/hunch_net-2012-03-06-COLT-ICML_Open_Questions_and_ICML_Instructions.html">458 hunch net-2012-03-06-COLT-ICML Open Questions and ICML Instructions</a></p>
<p>14 0.40505841 <a title="265-lsi-14" href="../hunch_net-2005/hunch_net-2005-10-19-Workshop%3A_Atomic_Learning.html">124 hunch net-2005-10-19-Workshop: Atomic Learning</a></p>
<p>15 0.40497082 <a title="265-lsi-15" href="../hunch_net-2010/hunch_net-2010-06-20-2010_ICML_discussion_site.html">401 hunch net-2010-06-20-2010 ICML discussion site</a></p>
<p>16 0.40063664 <a title="265-lsi-16" href="../hunch_net-2010/hunch_net-2010-08-20-The_Workshop_on_Cores%2C_Clusters%2C_and_Clouds.html">404 hunch net-2010-08-20-The Workshop on Cores, Clusters, and Clouds</a></p>
<p>17 0.38150281 <a title="265-lsi-17" href="../hunch_net-2006/hunch_net-2006-03-05-%26%238220%3BStructural%26%238221%3B_Learning.html">161 hunch net-2006-03-05-&#8220;Structural&#8221; Learning</a></p>
<p>18 0.37702641 <a title="265-lsi-18" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>19 0.36740893 <a title="265-lsi-19" href="../hunch_net-2011/hunch_net-2011-04-18-A_paper_not_at_Snowbird.html">431 hunch net-2011-04-18-A paper not at Snowbird</a></p>
<p>20 0.36222523 <a title="265-lsi-20" href="../hunch_net-2005/hunch_net-2005-03-24-The_Role_of_Workshops.html">46 hunch net-2005-03-24-The Role of Workshops</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(42, 0.25), (70, 0.557)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.85524106 <a title="265-lda-1" href="../hunch_net-2007/hunch_net-2007-10-14-NIPS_workshp%3A_Learning_Problem_Design.html">265 hunch net-2007-10-14-NIPS workshp: Learning Problem Design</a></p>
<p>Introduction: Alinaand I are organizing a workshop onLearning Problem DesignatNIPS.What is
learning problem design?It's about being clever in creating learning problems
from otherwise unlabeled data. Read the webpage above for examples.I want to
participate!Email us before Nov. 1 with a description of what you want to talk
about.</p><p>2 0.69339079 <a title="265-lda-2" href="../hunch_net-2006/hunch_net-2006-04-02-Mad_%28Neuro%29science.html">168 hunch net-2006-04-02-Mad (Neuro)science</a></p>
<p>Introduction: One of the questions facing machine learning as a field is "Can we produce a
generalized learning system that can solve a wide array of standard learning
problems?" The answer is trivial: "yes, just have children".Of course, that
wasn't really the question. The refined question is "Are there simple-to-
implement generalized learning systems that can solve a wide array of standard
learning problems?" The answer to this is less clear. The ability of animals
(and people ) to learn might be due to megabytes encoded in the DNA. If this
algorithmic complexity isnecessaryto solve machine learning, the field faces a
daunting task in replicating it on a computer.This observation suggests a
possibility: if you can show that few bits of DNA are needed for learning in
animals, then this provides evidence that machine learning (as a field) has a
hope of big success with relatively little effort.It is well known that
specific portions of the brain have specific functionality across individuals.
Ther</p><p>3 0.57568157 <a title="265-lda-3" href="../hunch_net-2005/hunch_net-2005-02-01-NIPS%3A_Online_Bayes.html">8 hunch net-2005-02-01-NIPS: Online Bayes</a></p>
<p>Introduction: One nice use for this blog is to consider and discuss papers that that have
appeared at recent conferences. I really enjoyed Andrew Ng and Sham Kakade's
paperOnline Bounds for Bayesian Algorithms. From the paper:The philosophy
taken in the Bayesian methodology is often at odds withthat in the online
learning community…. the online learning settingmakes rather minimal
assumptions on the conditions under which thedata are being presented to the
learner â€”usually, Nature could provideexamples in an adversarial manner. We
study the performance ofBayesian algorithms in a more adversarial setting… We
providecompetitive bounds when the cost function is the log loss, and
wecompare our performance to the best model in our model class (as inthe
experts setting).It's a very nice analysis of some of my favorite algorithms
that all hinges around a beautiful theorem:Let Q be any distribution over
parameters theta. Then for all sequences S:L_{Bayes}(S) leq L_Q(S) +
KL(Q|P)where P is our prior and th</p><p>4 0.51698762 <a title="265-lda-4" href="../hunch_net-2006/hunch_net-2006-04-05-What_is_state%3F.html">169 hunch net-2006-04-05-What is state?</a></p>
<p>Introduction: In reinforcement learning (and sometimes other settings), there is a notion of
"state". Based upon the state various predictions are made such as "Which
action should be taken next?" or "How much cumulative reward do I expect if I
take some action from this state?" Given the importance of state, it is
important to examine the meaning. There are actually several distinct options
and it turns out the definition variation is very important in motivating
different pieces of work.Newtonian State. State is the physical pose of the
world. Under this definition, there areverymany states, often too many for
explicit representation. This is also the definition typically used in
games.Abstracted State. State is an abstracted physical state of the world.
"Is the door open or closed?" "Are you in room A or not?" The number of states
is much smaller here. A basic issue here is: "How do you compute the state
from observations?"Mathematical State. State is a sufficient statistic of
observations for ma</p><p>5 0.50932908 <a title="265-lda-5" href="../hunch_net-2006/hunch_net-2006-12-04-Structural_Problems_in_NIPS_Decision_Making.html">221 hunch net-2006-12-04-Structural Problems in NIPS Decision Making</a></p>
<p>Introduction: This is a very difficult post to write, because it is about a perenially
touchy subject. Nevertheless, it is an important one which needs to be thought
about carefully.There are a few things which should be understood:The system
is changing and responsive. We-the-authors are we-the-reviewers, we-the-PC,
and even we-the-NIPS-board. NIPS has implemented 'secondary program chairs',
'author response', and 'double blind reviewing' in the last few years to help
with the decision process, and more changes may happen in the future.Agreement
creates a perception of correctness. When any PC meets and makes a group
decision about a paper, there is a strong tendency for the reinforcement
inherent in a group decision to create the perception of correctness. For the
many people who have been on the NIPS PC it's reasonable to entertain a
healthy skepticism in the face of this reinforcing certainty.This post is
about structural problems. What problems arise because of the structure of the
process? The</p><p>6 0.40923652 <a title="265-lda-6" href="../hunch_net-2008/hunch_net-2008-04-27-Watchword%3A_Supervised_Learning.html">299 hunch net-2008-04-27-Watchword: Supervised Learning</a></p>
<p>7 0.40894106 <a title="265-lda-7" href="../hunch_net-2006/hunch_net-2006-01-23-On_Coding_via_Mutual_Information_%26%23038%3B_Bayes_Nets.html">150 hunch net-2006-01-23-On Coding via Mutual Information &#038; Bayes Nets</a></p>
<p>8 0.40891486 <a title="265-lda-8" href="../hunch_net-2012/hunch_net-2012-10-26-ML_Symposium_and_Strata-Hadoop_World.html">475 hunch net-2012-10-26-ML Symposium and Strata-Hadoop World</a></p>
<p>9 0.40879977 <a title="265-lda-9" href="../hunch_net-2005/hunch_net-2005-05-06-Don%26%238217%3Bt_mix_the_solution_into_the_problem.html">67 hunch net-2005-05-06-Don&#8217;t mix the solution into the problem</a></p>
<p>10 0.40851015 <a title="265-lda-10" href="../hunch_net-2005/hunch_net-2005-12-09-Some_NIPS_papers.html">138 hunch net-2005-12-09-Some NIPS papers</a></p>
<p>11 0.40846902 <a title="265-lda-11" href="../hunch_net-2006/hunch_net-2006-06-15-IJCAI_is_out_of_season.html">184 hunch net-2006-06-15-IJCAI is out of season</a></p>
<p>12 0.40823376 <a title="265-lda-12" href="../hunch_net-2005/hunch_net-2005-03-29-Academic_Mechanism_Design.html">48 hunch net-2005-03-29-Academic Mechanism Design</a></p>
<p>13 0.40751481 <a title="265-lda-13" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>14 0.40742877 <a title="265-lda-14" href="../hunch_net-2005/hunch_net-2005-07-07-The_Limits_of_Learning_Theory.html">90 hunch net-2005-07-07-The Limits of Learning Theory</a></p>
<p>15 0.40411451 <a title="265-lda-15" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>16 0.40359166 <a title="265-lda-16" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<p>17 0.40001529 <a title="265-lda-17" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>18 0.39747322 <a title="265-lda-18" href="../hunch_net-2005/hunch_net-2005-02-23-Problem%3A_Reinforcement_Learning_with_Classification.html">27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</a></p>
<p>19 0.39584878 <a title="265-lda-19" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>20 0.3941763 <a title="265-lda-20" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
