<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>265 hunch net-2007-10-14-NIPS workshp: Learning Problem Design</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2007" href="../home/hunch_net-2007_home.html">hunch_net-2007</a> <a title="hunch_net-2007-265" href="#">hunch_net-2007-265</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>265 hunch net-2007-10-14-NIPS workshp: Learning Problem Design</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2007-265-html" href="http://hunch.net/?p=294">html</a></p><p>Introduction: Alina  and I are organizing a workshop on  Learning Problem Design  at  NIPS .  
 
 What is learning problem design?  Itâ&euro;&trade;s about being clever in creating learning problems from otherwise unlabeled data.  Read the webpage above for examples.
 
 I want to participate!  Email us before Nov. 1 with a description of what you want to talk about.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Alina  and I are organizing a workshop on  Learning Problem Design  at  NIPS . [sent-1, score-0.438]
</p><p>2 Itâ&euro;&trade;s about being clever in creating learning problems from otherwise unlabeled data. [sent-3, score-1.105]
</p><p>3 1 with a description of what you want to talk about. [sent-7, score-0.662]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('design', 0.324), ('clever', 0.304), ('webpage', 0.295), ('participate', 0.287), ('organizing', 0.28), ('alina', 0.253), ('description', 0.248), ('unlabeled', 0.248), ('email', 0.24), ('want', 0.237), ('read', 0.202), ('otherwise', 0.197), ('creating', 0.195), ('talk', 0.177), ('nips', 0.165), ('workshop', 0.158), ('problem', 0.147), ('us', 0.132), ('problems', 0.089), ('learning', 0.072)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="265-tfidf-1" href="../hunch_net-2007/hunch_net-2007-10-14-NIPS_workshp%3A_Learning_Problem_Design.html">265 hunch net-2007-10-14-NIPS workshp: Learning Problem Design</a></p>
<p>Introduction: Alina  and I are organizing a workshop on  Learning Problem Design  at  NIPS .  
 
 What is learning problem design?  Itâ&euro;&trade;s about being clever in creating learning problems from otherwise unlabeled data.  Read the webpage above for examples.
 
 I want to participate!  Email us before Nov. 1 with a description of what you want to talk about.</p><p>2 0.14460781 <a title="265-tfidf-2" href="../hunch_net-2007/hunch_net-2007-02-22-Create_Your_Own_ICML_Workshop.html">234 hunch net-2007-02-22-Create Your Own ICML Workshop</a></p>
<p>Introduction: As usual  ICML 2007  will be hosting a  workshop program  to be held this year on June 24th. The success of the program depends on having researchers like you propose interesting workshop topics and then organize the workshops. I’d like to encourage all of you to consider sending a workshop proposal. The proposal deadline has been extended to March 5. See the workshop web-site for details. 
 
Organizing a workshop is a unique way to gather an international group of researchers together to focus for an entire day on a topic of your choosing. I’ve always found that the cost of organizing a workshop is not so large, and very low compared to the benefits. The topic and format of a workshop are limited only by your imagination (and the attractiveness to potential participants) and need not follow the usual model of a mini-conference on a particular ML sub-area.  Hope to see some interesting proposals rolling in.</p><p>3 0.13377763 <a title="265-tfidf-3" href="../hunch_net-2013/hunch_net-2013-08-31-Extreme_Classification_workshop_at_NIPS.html">488 hunch net-2013-08-31-Extreme Classification workshop at NIPS</a></p>
<p>Introduction: Manik  and I are organizing the  extreme classification  workshop at NIPS this year.  We have a number of good speakers lined up, but I would further encourage anyone working in the area to submit an abstract by October 9.  I believe this is an idea whose time has now come.
 
The NIPS website doesnâ&euro;&trade;t have other workshops listed yet, but I expect several others to be of significant interest.</p><p>4 0.12754296 <a title="265-tfidf-4" href="../hunch_net-2010/hunch_net-2010-08-20-The_Workshop_on_Cores%2C_Clusters%2C_and_Clouds.html">404 hunch net-2010-08-20-The Workshop on Cores, Clusters, and Clouds</a></p>
<p>Introduction: Alekh ,  John ,  Ofer , and I are organizing a  workshop  at  NIPS  this year on learning in parallel and distributed environments.  The general interest level in parallel learning seems to be growing rapidly, so I expect quite a bit of attendance.  Please join us if you are parallel-interested.
 
And, if you are working in the area of parallel learning, please consider  submitting an abstract  due Oct. 17 for presentation at the workshop.</p><p>5 0.12502326 <a title="265-tfidf-5" href="../hunch_net-2007/hunch_net-2007-12-12-Workshop_Summary%26%238212%3BPrinciples_of_Learning_Problem_Design.html">277 hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</a></p>
<p>Introduction: This is a summary of the  workshop on Learning Problem Design  which  Alina  and I ran at  NIPS  this year.
 
The first question many people have is “What is learning problem design?”  This workshop is about admitting that solving learning problems does not start with labeled data, but rather somewhere before.  When humans are hired to produce labels, this is usually not a serious problem because you can tell them precisely what semantics you want the labels to have, and we can fix some set of features in advance.  However, when other methods are used this becomes more problematic.  This focus is important for Machine Learning because there are very large quantities of data which are not labeled by a hired human.
 
The title of the workshop was a bit ambitious, because a workshop is not long enough to synthesize a diversity of approaches into a coherent set of principles.  For me, the posters at the end of the workshop were quite helpful in getting approaches to gel.
 
Here are some an</p><p>6 0.10651372 <a title="265-tfidf-6" href="../hunch_net-2009/hunch_net-2009-10-26-NIPS_workshops.html">375 hunch net-2009-10-26-NIPS workshops</a></p>
<p>7 0.10388055 <a title="265-tfidf-7" href="../hunch_net-2006/hunch_net-2006-07-12-Who_is_having_visa_problems_reaching_US_conferences%3F.html">195 hunch net-2006-07-12-Who is having visa problems reaching US conferences?</a></p>
<p>8 0.10102824 <a title="265-tfidf-8" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>9 0.097831063 <a title="265-tfidf-9" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>10 0.093022093 <a title="265-tfidf-10" href="../hunch_net-2008/hunch_net-2008-10-19-NIPS_2008_workshop_on_Kernel_Learning.html">321 hunch net-2008-10-19-NIPS 2008 workshop on Kernel Learning</a></p>
<p>11 0.09007898 <a title="265-tfidf-11" href="../hunch_net-2010/hunch_net-2010-06-20-2010_ICML_discussion_site.html">401 hunch net-2010-06-20-2010 ICML discussion site</a></p>
<p>12 0.088343531 <a title="265-tfidf-12" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>13 0.087024882 <a title="265-tfidf-13" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>14 0.085947141 <a title="265-tfidf-14" href="../hunch_net-2010/hunch_net-2010-09-21-Regretting_the_dead.html">411 hunch net-2010-09-21-Regretting the dead</a></p>
<p>15 0.083807558 <a title="265-tfidf-15" href="../hunch_net-2005/hunch_net-2005-11-02-Progress_in_Active_Learning.html">127 hunch net-2005-11-02-Progress in Active Learning</a></p>
<p>16 0.080652043 <a title="265-tfidf-16" href="../hunch_net-2006/hunch_net-2006-03-27-Gradients_everywhere.html">167 hunch net-2006-03-27-Gradients everywhere</a></p>
<p>17 0.077115215 <a title="265-tfidf-17" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>18 0.073092744 <a title="265-tfidf-18" href="../hunch_net-2006/hunch_net-2006-03-05-%26%238220%3BStructural%26%238221%3B_Learning.html">161 hunch net-2006-03-05-&#8220;Structural&#8221; Learning</a></p>
<p>19 0.072221413 <a title="265-tfidf-19" href="../hunch_net-2005/hunch_net-2005-09-20-Workshop_Proposal%3A_Atomic_Learning.html">114 hunch net-2005-09-20-Workshop Proposal: Atomic Learning</a></p>
<p>20 0.071288921 <a title="265-tfidf-20" href="../hunch_net-2012/hunch_net-2012-03-06-COLT-ICML_Open_Questions_and_ICML_Instructions.html">458 hunch net-2012-03-06-COLT-ICML Open Questions and ICML Instructions</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.124), (1, -0.02), (2, -0.074), (3, -0.046), (4, 0.056), (5, 0.104), (6, 0.078), (7, 0.015), (8, 0.014), (9, -0.027), (10, 0.004), (11, -0.059), (12, -0.03), (13, 0.129), (14, -0.058), (15, -0.059), (16, -0.103), (17, 0.108), (18, -0.136), (19, 0.042), (20, -0.173), (21, -0.031), (22, -0.022), (23, -0.026), (24, 0.036), (25, 0.059), (26, 0.11), (27, 0.018), (28, 0.12), (29, -0.015), (30, -0.077), (31, 0.091), (32, -0.007), (33, -0.084), (34, -0.052), (35, 0.009), (36, -0.028), (37, 0.042), (38, -0.033), (39, -0.037), (40, 0.057), (41, 0.086), (42, 0.011), (43, 0.018), (44, 0.075), (45, 0.008), (46, -0.115), (47, -0.022), (48, 0.094), (49, 0.068)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95487511 <a title="265-lsi-1" href="../hunch_net-2007/hunch_net-2007-10-14-NIPS_workshp%3A_Learning_Problem_Design.html">265 hunch net-2007-10-14-NIPS workshp: Learning Problem Design</a></p>
<p>Introduction: Alina  and I are organizing a workshop on  Learning Problem Design  at  NIPS .  
 
 What is learning problem design?  Itâ&euro;&trade;s about being clever in creating learning problems from otherwise unlabeled data.  Read the webpage above for examples.
 
 I want to participate!  Email us before Nov. 1 with a description of what you want to talk about.</p><p>2 0.60834152 <a title="265-lsi-2" href="../hunch_net-2005/hunch_net-2005-09-20-Workshop_Proposal%3A_Atomic_Learning.html">114 hunch net-2005-09-20-Workshop Proposal: Atomic Learning</a></p>
<p>Introduction: This is a proposal for a workshop.  It may or may not happen depending on the level of interest.  If you are interested, feel free to indicate so (by email or comments).
 
Description: 
Assume(*) that any system for solving large difficult learning problems must decompose into repeated use of basic elements (i.e. atoms).  There are many basic questions which remain:
  
  What are the viable basic elements? 
  What makes a basic element viable? 
  What are the viable principles for the composition of these basic elements? 
  What are the viable principles for learning in such systems? 
  What problems can this approach handle? 
  
Hal Daume adds:
  
 Can composition of atoms be (semi-) automatically constructed[?] 
 When atoms are constructed through reductions, is there some notion of the “naturalness” of the created leaning problems? 
 Other than Markov fields/graphical models/Bayes nets, is there a good language for representing atoms and their compositions? 
  
The answer to these a</p><p>3 0.5915401 <a title="265-lsi-3" href="../hunch_net-2007/hunch_net-2007-02-22-Create_Your_Own_ICML_Workshop.html">234 hunch net-2007-02-22-Create Your Own ICML Workshop</a></p>
<p>Introduction: As usual  ICML 2007  will be hosting a  workshop program  to be held this year on June 24th. The success of the program depends on having researchers like you propose interesting workshop topics and then organize the workshops. I’d like to encourage all of you to consider sending a workshop proposal. The proposal deadline has been extended to March 5. See the workshop web-site for details. 
 
Organizing a workshop is a unique way to gather an international group of researchers together to focus for an entire day on a topic of your choosing. I’ve always found that the cost of organizing a workshop is not so large, and very low compared to the benefits. The topic and format of a workshop are limited only by your imagination (and the attractiveness to potential participants) and need not follow the usual model of a mini-conference on a particular ML sub-area.  Hope to see some interesting proposals rolling in.</p><p>4 0.59042877 <a title="265-lsi-4" href="../hunch_net-2007/hunch_net-2007-12-12-Workshop_Summary%26%238212%3BPrinciples_of_Learning_Problem_Design.html">277 hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</a></p>
<p>Introduction: This is a summary of the  workshop on Learning Problem Design  which  Alina  and I ran at  NIPS  this year.
 
The first question many people have is “What is learning problem design?”  This workshop is about admitting that solving learning problems does not start with labeled data, but rather somewhere before.  When humans are hired to produce labels, this is usually not a serious problem because you can tell them precisely what semantics you want the labels to have, and we can fix some set of features in advance.  However, when other methods are used this becomes more problematic.  This focus is important for Machine Learning because there are very large quantities of data which are not labeled by a hired human.
 
The title of the workshop was a bit ambitious, because a workshop is not long enough to synthesize a diversity of approaches into a coherent set of principles.  For me, the posters at the end of the workshop were quite helpful in getting approaches to gel.
 
Here are some an</p><p>5 0.58219093 <a title="265-lsi-5" href="../hunch_net-2008/hunch_net-2008-10-01-NIPS_2008_workshop_on_%26%238216%3BLearning_over_Empirical_Hypothesis_Spaces%26%238217%3B.html">319 hunch net-2008-10-01-NIPS 2008 workshop on &#8216;Learning over Empirical Hypothesis Spaces&#8217;</a></p>
<p>Introduction: This workshop asks for insights how far we may/can push the theoretical boundary of using data in the design of learning machines. Can we express our classification rule in terms of the sample, or do we have to stick to a core assumption of classical statistical learning theory, namely that the hypothesis space is to be defined independent from the sample? This workshop is particularly interested in – but not restricted to – the ‘luckiness framework’ and the recently introduced  notion of ‘compatibility functions’ in a semi-supervised learning context (more information can be found at  http://www.kuleuven.be/wehys ).</p><p>6 0.54603118 <a title="265-lsi-6" href="../hunch_net-2006/hunch_net-2006-07-25-Upcoming_conference.html">198 hunch net-2006-07-25-Upcoming conference</a></p>
<p>7 0.52694863 <a title="265-lsi-7" href="../hunch_net-2008/hunch_net-2008-10-19-NIPS_2008_workshop_on_Kernel_Learning.html">321 hunch net-2008-10-19-NIPS 2008 workshop on Kernel Learning</a></p>
<p>8 0.4799754 <a title="265-lsi-8" href="../hunch_net-2010/hunch_net-2010-09-21-Regretting_the_dead.html">411 hunch net-2010-09-21-Regretting the dead</a></p>
<p>9 0.44880515 <a title="265-lsi-9" href="../hunch_net-2009/hunch_net-2009-01-28-Nielsen%26%238217%3Bs_talk.html">340 hunch net-2009-01-28-Nielsen&#8217;s talk</a></p>
<p>10 0.44492757 <a title="265-lsi-10" href="../hunch_net-2005/hunch_net-2005-12-28-Yet_more_nips_thoughts.html">144 hunch net-2005-12-28-Yet more nips thoughts</a></p>
<p>11 0.43185309 <a title="265-lsi-11" href="../hunch_net-2005/hunch_net-2005-06-10-Workshops_are_not_Conferences.html">80 hunch net-2005-06-10-Workshops are not Conferences</a></p>
<p>12 0.42128235 <a title="265-lsi-12" href="../hunch_net-2006/hunch_net-2006-03-05-%26%238220%3BStructural%26%238221%3B_Learning.html">161 hunch net-2006-03-05-&#8220;Structural&#8221; Learning</a></p>
<p>13 0.42061916 <a title="265-lsi-13" href="../hunch_net-2010/hunch_net-2010-08-20-The_Workshop_on_Cores%2C_Clusters%2C_and_Clouds.html">404 hunch net-2010-08-20-The Workshop on Cores, Clusters, and Clouds</a></p>
<p>14 0.41743198 <a title="265-lsi-14" href="../hunch_net-2011/hunch_net-2011-04-18-A_paper_not_at_Snowbird.html">431 hunch net-2011-04-18-A paper not at Snowbird</a></p>
<p>15 0.41621608 <a title="265-lsi-15" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>16 0.41489008 <a title="265-lsi-16" href="../hunch_net-2005/hunch_net-2005-10-19-Workshop%3A_Atomic_Learning.html">124 hunch net-2005-10-19-Workshop: Atomic Learning</a></p>
<p>17 0.40669307 <a title="265-lsi-17" href="../hunch_net-2006/hunch_net-2006-07-12-Who_is_having_visa_problems_reaching_US_conferences%3F.html">195 hunch net-2006-07-12-Who is having visa problems reaching US conferences?</a></p>
<p>18 0.4028897 <a title="265-lsi-18" href="../hunch_net-2009/hunch_net-2009-12-07-Vowpal_Wabbit_version_4.0%2C_and_a_NIPS_heresy.html">381 hunch net-2009-12-07-Vowpal Wabbit version 4.0, and a NIPS heresy</a></p>
<p>19 0.39686728 <a title="265-lsi-19" href="../hunch_net-2013/hunch_net-2013-08-31-Extreme_Classification_workshop_at_NIPS.html">488 hunch net-2013-08-31-Extreme Classification workshop at NIPS</a></p>
<p>20 0.38509339 <a title="265-lsi-20" href="../hunch_net-2005/hunch_net-2005-09-19-NIPS_Workshops.html">113 hunch net-2005-09-19-NIPS Workshops</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(11, 0.522), (53, 0.178), (55, 0.116)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.8604719 <a title="265-lda-1" href="../hunch_net-2010/hunch_net-2010-07-02-MetaOptimize.html">402 hunch net-2010-07-02-MetaOptimize</a></p>
<p>Introduction: Joseph Turian  creates  MetaOptimize  for discussion of NLP and ML on big datasets.  This includes a  blog , but perhaps more importantly a  question and answer section .  Iâ&euro;&trade;m hopeful it will take off.</p><p>2 0.85926002 <a title="265-lda-2" href="../hunch_net-2013/hunch_net-2013-07-24-ICML_2012_videos_lost.html">487 hunch net-2013-07-24-ICML 2012 videos lost</a></p>
<p>Introduction: A big ouch—all the videos for ICML 2012 were lost in a shuffle.  Rajnish sends the below, but if anyone can help that would be greatly appreciated.
 
——————————————————————————
 
Sincere apologies to ICML community for loosing 2012 archived videos
 
What happened: In order to publish 2013 videos, we decided to move 2012 videos to another server. We have a weekly backup service from the provider but after removing the videos from the current server, when we tried to retrieve the 2012 videos from backup service, the backup did not work because of provider-specific requirements that we had ignored while removing the data from previous server.
 
What are we doing about this: At this point, we are still looking into raw footage to find if we can retrieve some of the videos, but following are the steps we are taking to make sure this does not happen again in future: 
(1) We are going to create a channel on Vimeo (and potentially on YouTube) and we will publish there the p-in-p- or slide-vers</p><p>same-blog 3 0.78562307 <a title="265-lda-3" href="../hunch_net-2007/hunch_net-2007-10-14-NIPS_workshp%3A_Learning_Problem_Design.html">265 hunch net-2007-10-14-NIPS workshp: Learning Problem Design</a></p>
<p>Introduction: Alina  and I are organizing a workshop on  Learning Problem Design  at  NIPS .  
 
 What is learning problem design?  Itâ&euro;&trade;s about being clever in creating learning problems from otherwise unlabeled data.  Read the webpage above for examples.
 
 I want to participate!  Email us before Nov. 1 with a description of what you want to talk about.</p><p>4 0.62767041 <a title="265-lda-4" href="../hunch_net-2008/hunch_net-2008-01-28-Sufficient_Computation.html">287 hunch net-2008-01-28-Sufficient Computation</a></p>
<p>Introduction: Do we have computer hardware sufficient for AI?  This question is difficult to answer, but here’s a try:
 
One way to achieve AI is by simulating a human brain.  A human brain has about 10 15  synapses which operate at about 10 2  per second implying about 10 17  bit ops per second.
 
A modern computer runs at 10 9  cycles/second and operates on 10 2  bits per cycle implying 10 11  bits processed per second.  
 
The gap here is only 6 orders of magnitude, which can be plausibly surpassed via cluster machines.  For example, the  BlueGene/L  operates 10 5  nodes (one order of magnitude short).  It’s peak recorded performance is about 0.5*10 15  FLOPS which translates to about 10 16  bit ops per second, which is nearly 10 17 .
 
There are many criticisms (both positive and negative) for this argument.
  
 Simulation of a human brain might require substantially more detail.  Perhaps an additional 10 2  is required per neuron. 
 We may not need to simulate a human brain to achieve AI.  Ther</p><p>5 0.38734704 <a title="265-lda-5" href="../hunch_net-2007/hunch_net-2007-09-30-NIPS_workshops_are_out..html">264 hunch net-2007-09-30-NIPS workshops are out.</a></p>
<p>Introduction: Here .  Iâ&euro;&trade;m particularly interested in the  Web Search ,  Efficient ML , and (of course)  Learning Problem Design  workshops but there are many others to check out as well. Workshops are a great chance to make progress on or learn about a topic.  Relevance and interaction amongst diverse people can sometimes be magical.</p><p>6 0.36843255 <a title="265-lda-6" href="../hunch_net-2005/hunch_net-2005-12-29-Deadline_Season.html">145 hunch net-2005-12-29-Deadline Season</a></p>
<p>7 0.31982142 <a title="265-lda-7" href="../hunch_net-2005/hunch_net-2005-09-05-Site_Update.html">107 hunch net-2005-09-05-Site Update</a></p>
<p>8 0.30859473 <a title="265-lda-8" href="../hunch_net-2005/hunch_net-2005-02-09-Intuitions_from_applied_learning.html">16 hunch net-2005-02-09-Intuitions from applied learning</a></p>
<p>9 0.29961517 <a title="265-lda-9" href="../hunch_net-2005/hunch_net-2005-07-10-Thinking_the_Unthought.html">91 hunch net-2005-07-10-Thinking the Unthought</a></p>
<p>10 0.29856449 <a title="265-lda-10" href="../hunch_net-2005/hunch_net-2005-04-14-Families_of_Learning_Theory_Statements.html">56 hunch net-2005-04-14-Families of Learning Theory Statements</a></p>
<p>11 0.29815277 <a title="265-lda-11" href="../hunch_net-2005/hunch_net-2005-01-24-Holy_grails_of_machine_learning%3F.html">2 hunch net-2005-01-24-Holy grails of machine learning?</a></p>
<p>12 0.29294723 <a title="265-lda-12" href="../hunch_net-2009/hunch_net-2009-08-16-Centmail_comments.html">367 hunch net-2009-08-16-Centmail comments</a></p>
<p>13 0.28237793 <a title="265-lda-13" href="../hunch_net-2008/hunch_net-2008-03-15-COLT_Open_Problems.html">292 hunch net-2008-03-15-COLT Open Problems</a></p>
<p>14 0.2756241 <a title="265-lda-14" href="../hunch_net-2008/hunch_net-2008-12-12-Summer_Conferences.html">331 hunch net-2008-12-12-Summer Conferences</a></p>
<p>15 0.26460326 <a title="265-lda-15" href="../hunch_net-2005/hunch_net-2005-02-17-Learning_Research_Programs.html">21 hunch net-2005-02-17-Learning Research Programs</a></p>
<p>16 0.2573117 <a title="265-lda-16" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>17 0.25456044 <a title="265-lda-17" href="../hunch_net-2009/hunch_net-2009-05-24-2009_ICML_discussion_site.html">356 hunch net-2009-05-24-2009 ICML discussion site</a></p>
<p>18 0.25269648 <a title="265-lda-18" href="../hunch_net-2011/hunch_net-2011-01-16-2011_Summer_Conference_Deadline_Season.html">422 hunch net-2011-01-16-2011 Summer Conference Deadline Season</a></p>
<p>19 0.25238106 <a title="265-lda-19" href="../hunch_net-2008/hunch_net-2008-06-30-ICML_has_a_comment_system.html">305 hunch net-2008-06-30-ICML has a comment system</a></p>
<p>20 0.25204951 <a title="265-lda-20" href="../hunch_net-2010/hunch_net-2010-01-19-Deadline_Season%2C_2010.html">387 hunch net-2010-01-19-Deadline Season, 2010</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
