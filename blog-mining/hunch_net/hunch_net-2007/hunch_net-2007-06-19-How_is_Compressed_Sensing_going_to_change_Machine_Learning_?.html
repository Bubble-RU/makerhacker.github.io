<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>248 hunch net-2007-06-19-How is Compressed Sensing going to change Machine Learning ?</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2007" href="../home/hunch_net-2007_home.html">hunch_net-2007</a> <a title="hunch_net-2007-248" href="#">hunch_net-2007-248</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>248 hunch net-2007-06-19-How is Compressed Sensing going to change Machine Learning ?</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2007-248-html" href="http://hunch.net/?p=273">html</a></p><p>Introduction: Compressed Sensing(CS) is a new framework developed byEmmanuel Candes,Terry
TaoandDavid Donoho. To summarize, if you acquire a signal in some basis that
is incoherent with the basis in which you know the signal to be sparse in, it
is very likely you will be able to reconstruct the signal from these
incoherent projections.Terry Tao, the recentFields medalist, does a very nice
job at explaining the frameworkhere. He goes further in the theory description
in thispostwhere he mentions the central issue of the Uniform Uncertainty
Principle. It so happens that random projections are on average incoherent,
within the UUP meaning, with most known basis (sines, polynomials, splines,
wavelets, curvelets …) and are therefore an ideal basis for Compressed
Sensing. [ For more in-depth information on the subject, the Rice group has
done a very good job at providing a central library of papers relevant to the
growing subject:http://www.dsp.ece.rice.edu/cs/]The Machine Learning community
has looked at</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('projections', 0.56), ('random', 0.39), ('cs', 0.209), ('compressed', 0.173), ('signal', 0.16), ('sensing', 0.151), ('wakin', 0.146), ('jl', 0.144), ('manifold', 0.144), ('incoherent', 0.13), ('basis', 0.119), ('reduction', 0.11), ('propose', 0.108), ('dimension', 0.1), ('baraniuk', 0.097), ('central', 0.097), ('dimensionality', 0.094), ('proportional', 0.086), ('signals', 0.086), ('lemma', 0.08)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999982 <a title="248-tfidf-1" href="../hunch_net-2007/hunch_net-2007-06-19-How_is_Compressed_Sensing_going_to_change_Machine_Learning_%3F.html">248 hunch net-2007-06-19-How is Compressed Sensing going to change Machine Learning ?</a></p>
<p>Introduction: Compressed Sensing(CS) is a new framework developed byEmmanuel Candes,Terry
TaoandDavid Donoho. To summarize, if you acquire a signal in some basis that
is incoherent with the basis in which you know the signal to be sparse in, it
is very likely you will be able to reconstruct the signal from these
incoherent projections.Terry Tao, the recentFields medalist, does a very nice
job at explaining the frameworkhere. He goes further in the theory description
in thispostwhere he mentions the central issue of the Uniform Uncertainty
Principle. It so happens that random projections are on average incoherent,
within the UUP meaning, with most known basis (sines, polynomials, splines,
wavelets, curvelets …) and are therefore an ideal basis for Compressed
Sensing. [ For more in-depth information on the subject, the Rice group has
done a very good job at providing a central library of papers relevant to the
growing subject:http://www.dsp.ece.rice.edu/cs/]The Machine Learning community
has looked at</p><p>2 0.16769655 <a title="248-tfidf-2" href="../hunch_net-2006/hunch_net-2006-07-26-Two_more_UAI_papers_of_interest.html">199 hunch net-2006-07-26-Two more UAI papers of interest</a></p>
<p>Introduction: In addition to Ed Snelson's paper, there were (at least) two other papers that
caught my eye at UAI.One wasthis paperby Sanjoy Dasgupta, Daniel Hsu and Nakul
Verma at UCSD which shows in a surprisingly general and strong way that almost
all linear projections of any jointly distributed vector random variable with
finite first and second moments look sphereical and unimodal (in fact look
like a scale mixture of Gaussians). Great result, as you'd expect from
Sanjoy.The other paper which I found intriguing but which I just haven't
groked yet isthis beastby Manfred and Dima Kuzmin.You can check out the
(beautiful)slidesif that helps. I feel like there is something deep here, but
my brain is too small to understand it. The COLT and last NIPS papers/slides
are also on Manfred's page. Hopefully someone here can illuminate.</p><p>3 0.12596656 <a title="248-tfidf-3" href="../hunch_net-2006/hunch_net-2006-11-22-Explicit_Randomization_in_Learning_algorithms.html">219 hunch net-2006-11-22-Explicit Randomization in Learning algorithms</a></p>
<p>Introduction: There are a number of learning algorithms which explicitly incorporate
randomness into their execution. This includes at amongst others:Neural
Networks. Neural networks use randomization to assign initial
weights.Boltzmann Machines/Deep Belief Networks. Boltzmann machines are
something like a stochastic version of multinode logistic regression. The use
of randomness is more essential in Boltzmann machines, because the predicted
value at test time also uses randomness.Bagging. Bagging is a process where a
learning algorithm is run several different times on several different
datasets, creating a final predictor which makes a majority vote.Policy
descent. Several algorithms in reinforcement learning such asConservative
Policy Iterationuse random bits to create stochastic policies.Experts
algorithms. Randomized weighted majority use random bits as a part of the
prediction process to achieve better theoretical guarantees.A basic question
is: "Should there be explicit randomization in learn</p><p>4 0.11970285 <a title="248-tfidf-4" href="../hunch_net-2005/hunch_net-2005-04-22-New_Blog%3A_%5BLowerbounds%2CUpperbounds%5D.html">59 hunch net-2005-04-22-New Blog: [Lowerbounds,Upperbounds]</a></p>
<p>Introduction: Maverick Woo and the Aladdin group at CMU have started a CS theory-related
bloghere.</p><p>5 0.11197843 <a title="248-tfidf-5" href="../hunch_net-2008/hunch_net-2008-08-18-Radford_Neal_starts_a_blog.html">313 hunch net-2008-08-18-Radford Neal starts a blog</a></p>
<p>Introduction: hereon statistics, ML, CS, and other things he knows well.</p><p>6 0.1095932 <a title="248-tfidf-6" href="../hunch_net-2005/hunch_net-2005-08-11-Why_Manifold-Based_Dimension_Reduction_Techniques%3F.html">102 hunch net-2005-08-11-Why Manifold-Based Dimension Reduction Techniques?</a></p>
<p>7 0.090119913 <a title="248-tfidf-7" href="../hunch_net-2009/hunch_net-2009-06-24-Interesting_papers_at_UAICMOLT_2009.html">361 hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</a></p>
<p>8 0.08874812 <a title="248-tfidf-8" href="../hunch_net-2010/hunch_net-2010-05-10-Aggregation_of_estimators%2C_sparsity_in_high_dimension_and_computational_feasibility.html">398 hunch net-2010-05-10-Aggregation of estimators, sparsity in high dimension and computational feasibility</a></p>
<p>9 0.088579752 <a title="248-tfidf-9" href="../hunch_net-2010/hunch_net-2010-03-15-The_Efficient_Robust_Conditional_Probability_Estimation_Problem.html">391 hunch net-2010-03-15-The Efficient Robust Conditional Probability Estimation Problem</a></p>
<p>10 0.081743412 <a title="248-tfidf-10" href="../hunch_net-2011/hunch_net-2011-08-01-Interesting_papers_at_COLT_2011.html">439 hunch net-2011-08-01-Interesting papers at COLT 2011</a></p>
<p>11 0.081622496 <a title="248-tfidf-11" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>12 0.081302874 <a title="248-tfidf-12" href="../hunch_net-2009/hunch_net-2009-01-21-Nearly_all_natural_problems_require_nonlinearity.html">337 hunch net-2009-01-21-Nearly all natural problems require nonlinearity</a></p>
<p>13 0.079365626 <a title="248-tfidf-13" href="../hunch_net-2011/hunch_net-2011-07-11-Interesting_Neural_Network_Papers_at_ICML_2011.html">438 hunch net-2011-07-11-Interesting Neural Network Papers at ICML 2011</a></p>
<p>14 0.0756905 <a title="248-tfidf-14" href="../hunch_net-2005/hunch_net-2005-03-22-Active_learning.html">45 hunch net-2005-03-22-Active learning</a></p>
<p>15 0.072301239 <a title="248-tfidf-15" href="../hunch_net-2008/hunch_net-2008-09-12-How_do_we_get_weak_action_dependence_for_learning_with_partial_observations%3F.html">317 hunch net-2008-09-12-How do we get weak action dependence for learning with partial observations?</a></p>
<p>16 0.071918406 <a title="248-tfidf-16" href="../hunch_net-2006/hunch_net-2006-01-23-On_Coding_via_Mutual_Information_%26%23038%3B_Bayes_Nets.html">150 hunch net-2006-01-23-On Coding via Mutual Information &#038; Bayes Nets</a></p>
<p>17 0.069832452 <a title="248-tfidf-17" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>18 0.068965338 <a title="248-tfidf-18" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>19 0.06868954 <a title="248-tfidf-19" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>20 0.068196669 <a title="248-tfidf-20" href="../hunch_net-2010/hunch_net-2010-03-26-A_Variance_only_Deviation_Bound.html">392 hunch net-2010-03-26-A Variance only Deviation Bound</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.158), (1, -0.022), (2, -0.019), (3, 0.038), (4, -0.078), (5, -0.009), (6, -0.02), (7, -0.052), (8, 0.032), (9, -0.04), (10, 0.065), (11, 0.097), (12, -0.002), (13, 0.064), (14, 0.019), (15, 0.034), (16, 0.001), (17, -0.013), (18, -0.002), (19, -0.075), (20, 0.003), (21, -0.054), (22, 0.031), (23, 0.031), (24, 0.009), (25, 0.081), (26, 0.013), (27, 0.032), (28, 0.086), (29, 0.034), (30, 0.118), (31, 0.04), (32, 0.07), (33, -0.134), (34, 0.065), (35, -0.015), (36, 0.102), (37, 0.093), (38, 0.022), (39, -0.007), (40, 0.02), (41, -0.041), (42, 0.028), (43, -0.075), (44, -0.057), (45, 0.009), (46, 0.065), (47, -0.031), (48, 0.048), (49, 0.114)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96958649 <a title="248-lsi-1" href="../hunch_net-2007/hunch_net-2007-06-19-How_is_Compressed_Sensing_going_to_change_Machine_Learning_%3F.html">248 hunch net-2007-06-19-How is Compressed Sensing going to change Machine Learning ?</a></p>
<p>Introduction: Compressed Sensing(CS) is a new framework developed byEmmanuel Candes,Terry
TaoandDavid Donoho. To summarize, if you acquire a signal in some basis that
is incoherent with the basis in which you know the signal to be sparse in, it
is very likely you will be able to reconstruct the signal from these
incoherent projections.Terry Tao, the recentFields medalist, does a very nice
job at explaining the frameworkhere. He goes further in the theory description
in thispostwhere he mentions the central issue of the Uniform Uncertainty
Principle. It so happens that random projections are on average incoherent,
within the UUP meaning, with most known basis (sines, polynomials, splines,
wavelets, curvelets …) and are therefore an ideal basis for Compressed
Sensing. [ For more in-depth information on the subject, the Rice group has
done a very good job at providing a central library of papers relevant to the
growing subject:http://www.dsp.ece.rice.edu/cs/]The Machine Learning community
has looked at</p><p>2 0.63250768 <a title="248-lsi-2" href="../hunch_net-2005/hunch_net-2005-04-22-New_Blog%3A_%5BLowerbounds%2CUpperbounds%5D.html">59 hunch net-2005-04-22-New Blog: [Lowerbounds,Upperbounds]</a></p>
<p>Introduction: Maverick Woo and the Aladdin group at CMU have started a CS theory-related
bloghere.</p><p>3 0.51386625 <a title="248-lsi-3" href="../hunch_net-2006/hunch_net-2006-07-05-more_icml_papers.html">189 hunch net-2006-07-05-more icml papers</a></p>
<p>Introduction: Here are a few other papers I enjoyed from ICML06.Topic Models:Dynamic Topic
ModelsDavid Blei, John LaffertyA nice model for how topics in LDA type models
can evolve over time,using a linear dynamical system on the natural parameters
and a veryclever structured variational approximation (in which the mean
fieldparameters are pseudo-observations of a virtual LDS). Like all
Bleipapers, he makes it look easy, but it is extremely impressive.Pachinko
AllocationWei Li, Andrew McCallumA very elegant (but computationally
challenging) model which inducescorrelation amongst topics using a multi-level
DAG whose interior nodesare "super-topics" and "sub-topics" and whose leaves
are thevocabulary words. Makes the slumbering monster of structure learning
stir.Sequence Analysis (I missed these talks since I was chairing another
session)Online Decoding of Markov Models with Latency ConstraintsMukund
Narasimhan, Paul Viola, Michael ShilmanAn "ah-ha!" paper showing how to trade
off latency and decodinga</p><p>4 0.50401092 <a title="248-lsi-4" href="../hunch_net-2006/hunch_net-2006-07-26-Two_more_UAI_papers_of_interest.html">199 hunch net-2006-07-26-Two more UAI papers of interest</a></p>
<p>Introduction: In addition to Ed Snelson's paper, there were (at least) two other papers that
caught my eye at UAI.One wasthis paperby Sanjoy Dasgupta, Daniel Hsu and Nakul
Verma at UCSD which shows in a surprisingly general and strong way that almost
all linear projections of any jointly distributed vector random variable with
finite first and second moments look sphereical and unimodal (in fact look
like a scale mixture of Gaussians). Great result, as you'd expect from
Sanjoy.The other paper which I found intriguing but which I just haven't
groked yet isthis beastby Manfred and Dima Kuzmin.You can check out the
(beautiful)slidesif that helps. I feel like there is something deep here, but
my brain is too small to understand it. The COLT and last NIPS papers/slides
are also on Manfred's page. Hopefully someone here can illuminate.</p><p>5 0.50399041 <a title="248-lsi-5" href="../hunch_net-2005/hunch_net-2005-12-11-More_NIPS_Papers.html">139 hunch net-2005-12-11-More NIPS Papers</a></p>
<p>Introduction: Let me add to John's post with a few of my own favouritesfrom this year's
conference. First, let me say thatSanjoy's talk,Coarse Sample Complexity
Bounds for ActiveLearningwas also one of my favourites, as was theForgettron
paper.I also really enjoyed the last third ofChristos'talkon the complexity of
finding Nash equilibria.And, speaking of tagging, I thinkthe U.Mass Citeseer
replacement systemRexafrom the demo track is very cool.Finally, let me add my
recommendations for specific papers:Z. Ghahramani, K. Heller:Bayesian Sets[no
preprint](A very elegant probabilistic information retrieval style modelof
which objects are "most like" a given subset of objects.)T. Griffiths, Z.
Ghahramani:Infinite Latent Feature Models andthe Indian Buffet
Process[preprint](A Dirichlet style prior over infinite binary matrices
withbeautiful exchangeability properties.)K. Weinberger, J. Blitzer, L.
Saul:Distance Metric Learning forLarge Margin Nearest Neighbor
Classification[preprint](A nice idea about ho</p><p>6 0.49744809 <a title="248-lsi-6" href="../hunch_net-2006/hunch_net-2006-01-23-On_Coding_via_Mutual_Information_%26%23038%3B_Bayes_Nets.html">150 hunch net-2006-01-23-On Coding via Mutual Information &#038; Bayes Nets</a></p>
<p>7 0.48755318 <a title="248-lsi-7" href="../hunch_net-2005/hunch_net-2005-04-25-Embeddings%3A_what_are_they_good_for%3F.html">61 hunch net-2005-04-25-Embeddings: what are they good for?</a></p>
<p>8 0.48094609 <a title="248-lsi-8" href="../hunch_net-2008/hunch_net-2008-09-12-How_do_we_get_weak_action_dependence_for_learning_with_partial_observations%3F.html">317 hunch net-2008-09-12-How do we get weak action dependence for learning with partial observations?</a></p>
<p>9 0.48041165 <a title="248-lsi-9" href="../hunch_net-2005/hunch_net-2005-12-14-More_NIPS_Papers_II.html">140 hunch net-2005-12-14-More NIPS Papers II</a></p>
<p>10 0.47797635 <a title="248-lsi-10" href="../hunch_net-2010/hunch_net-2010-03-26-A_Variance_only_Deviation_Bound.html">392 hunch net-2010-03-26-A Variance only Deviation Bound</a></p>
<p>11 0.45336479 <a title="248-lsi-11" href="../hunch_net-2008/hunch_net-2008-08-18-Radford_Neal_starts_a_blog.html">313 hunch net-2008-08-18-Radford Neal starts a blog</a></p>
<p>12 0.45255768 <a title="248-lsi-12" href="../hunch_net-2005/hunch_net-2005-05-29-Maximum_Margin_Mismatch%3F.html">77 hunch net-2005-05-29-Maximum Margin Mismatch?</a></p>
<p>13 0.44509083 <a title="248-lsi-13" href="../hunch_net-2010/hunch_net-2010-05-10-Aggregation_of_estimators%2C_sparsity_in_high_dimension_and_computational_feasibility.html">398 hunch net-2010-05-10-Aggregation of estimators, sparsity in high dimension and computational feasibility</a></p>
<p>14 0.44410396 <a title="248-lsi-14" href="../hunch_net-2006/hunch_net-2006-11-22-Explicit_Randomization_in_Learning_algorithms.html">219 hunch net-2006-11-22-Explicit Randomization in Learning algorithms</a></p>
<p>15 0.42007378 <a title="248-lsi-15" href="../hunch_net-2011/hunch_net-2011-07-11-Interesting_Neural_Network_Papers_at_ICML_2011.html">438 hunch net-2011-07-11-Interesting Neural Network Papers at ICML 2011</a></p>
<p>16 0.40543401 <a title="248-lsi-16" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>17 0.39842287 <a title="248-lsi-17" href="../hunch_net-2007/hunch_net-2007-12-20-Cool_and_Interesting_things_at_NIPS%2C_take_three.html">280 hunch net-2007-12-20-Cool and Interesting things at NIPS, take three</a></p>
<p>18 0.39716786 <a title="248-lsi-18" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>19 0.39272714 <a title="248-lsi-19" href="../hunch_net-2006/hunch_net-2006-06-16-Regularization_%3D_Robustness.html">185 hunch net-2006-06-16-Regularization = Robustness</a></p>
<p>20 0.39131618 <a title="248-lsi-20" href="../hunch_net-2009/hunch_net-2009-06-24-Interesting_papers_at_UAICMOLT_2009.html">361 hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(6, 0.013), (35, 0.042), (39, 0.013), (42, 0.171), (45, 0.038), (68, 0.063), (74, 0.101), (92, 0.361), (95, 0.064)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.92304391 <a title="248-lda-1" href="../hunch_net-2005/hunch_net-2005-12-14-More_NIPS_Papers_II.html">140 hunch net-2005-12-14-More NIPS Papers II</a></p>
<p>Introduction: I thought this was a very good NIPS with many excellent papers. The following
are a few NIPS papers which I liked and I hope to study more carefully when I
get the chance. The list is not exhaustive and in no particular
order…Preconditioner Approximations for Probabilistic Graphical
Models.Pradeeep Ravikumar and John Lafferty.I thought the use of
preconditioner methods from solving linear systems in the context of
approximate inference was novel and interesting. The results look good and I'd
like to understand the limitations.Rodeo: Sparse nonparametric regression in
high dimensions.John Lafferty and Larry Wasserman.A very interesting approach
to feature selection in nonparametric regression from a frequentist framework.
The use of lengthscale variables in each dimension reminds me a lot of
'Automatic Relevance Determination' in Gaussian process regression -- it would
be interesting to compare Rodeo to ARD in GPs.Interpolating between types and
tokens by estimating power law generators</p><p>2 0.90561664 <a title="248-lda-2" href="../hunch_net-2005/hunch_net-2005-05-12-Math_on_the_Web.html">70 hunch net-2005-05-12-Math on the Web</a></p>
<p>Introduction: Andrej Bauer has setup aMathematics and ComputationBlog. As a first step he
has tried to address the persistent and annoying problem of math on the web.
As a basic tool for precisely stating and transfering understanding of
technical subjects, mathematics is very necessary. Despite this necessity,
every mechanism for expressing mathematics on the web seems unnaturally
clumsy. Here are some of the methods and their drawbacks:MathMLThis was
supposed to be the answer, but it has two severe drawbacks: "Internet
Explorer" doesn't read it and the language is an example of push-XML-to-the-
limit which no one would ever consider writing in. (In contrast, html is easy
to write in.) It's also very annoying that math fonts must be installed
independent of the browser, even for mozilla based browsers.Create inline
images. This has several big drawbacks: font size is fixed for all viewers,
you can't cut & paste inside the images, and you can't hyperlink from (say)
symbol to definition.Math Worldis</p><p>same-blog 3 0.83015138 <a title="248-lda-3" href="../hunch_net-2007/hunch_net-2007-06-19-How_is_Compressed_Sensing_going_to_change_Machine_Learning_%3F.html">248 hunch net-2007-06-19-How is Compressed Sensing going to change Machine Learning ?</a></p>
<p>Introduction: Compressed Sensing(CS) is a new framework developed byEmmanuel Candes,Terry
TaoandDavid Donoho. To summarize, if you acquire a signal in some basis that
is incoherent with the basis in which you know the signal to be sparse in, it
is very likely you will be able to reconstruct the signal from these
incoherent projections.Terry Tao, the recentFields medalist, does a very nice
job at explaining the frameworkhere. He goes further in the theory description
in thispostwhere he mentions the central issue of the Uniform Uncertainty
Principle. It so happens that random projections are on average incoherent,
within the UUP meaning, with most known basis (sines, polynomials, splines,
wavelets, curvelets …) and are therefore an ideal basis for Compressed
Sensing. [ For more in-depth information on the subject, the Rice group has
done a very good job at providing a central library of papers relevant to the
growing subject:http://www.dsp.ece.rice.edu/cs/]The Machine Learning community
has looked at</p><p>4 0.6544739 <a title="248-lda-4" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>Introduction: "Search" is the other branch of AI research which has been succesful. Concrete
examples includeDeep Bluewhich beat the world chess champion andChinookthe
champion checkers program. A set of core search techniques exist including A*,
alpha-beta pruning, and others that can be applied to any of many different
search problems.Given this, it may be surprising to learn that there has been
relatively little succesful work on combining prediction and search. Given
also that humans typically solve search problems using a number of predictive
heuristics to narrow in on a solution, we might be surprised again. However,
the big successful search-based systems have typically not used "smart" search
algorithms. Insteady they have optimized for very fast search. This is not for
lack of tryingâ&euro;Ś many people have tried to synthesize search and prediction to
various degrees of success. For example,Knightcapachieves good-but-not-stellar
chess playing performance, andTD-gammonhas achieved near-optimal Bac</p><p>5 0.64964807 <a title="248-lda-5" href="../hunch_net-2005/hunch_net-2005-03-22-Active_learning.html">45 hunch net-2005-03-22-Active learning</a></p>
<p>Introduction: Often, unlabeled data is easy to come by but labels are expensive. For
instance, if you're building a speech recognizer, it's easy enough to get raw
speech samples -- just walk around with a microphone -- but labeling even one
of these samples is a tedious process in which a human must examine the speech
signal and carefully segment it into phonemes. In the field of active
learning, the goal is as usual to construct an accurate classifier, but the
labels of the data points are initially hidden and there is a charge for each
label you want revealed. The hope is that by intelligent adaptive querying,
you can get away with significantly fewer labels than you would need in a
regular supervised learning framework.Here's an example. Suppose the data lie
on the real line, and the classifiers are simple thresholding functions, H =
{hw}:hw(x) = 1 if x > w, and 0 otherwise.VC theory tells us that if the
underlying distribution P can be classified perfectly by some hypothesis in H
(called thereal</p><p>6 0.56072086 <a title="248-lda-6" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>7 0.54496664 <a title="248-lda-7" href="../hunch_net-2007/hunch_net-2007-09-18-It%26%238217%3Bs_MDL_Jim%2C_but_not_as_we_know_it%26%238230%3B%28on_Bayes%2C_MDL_and_consistency%29.html">263 hunch net-2007-09-18-It&#8217;s MDL Jim, but not as we know it&#8230;(on Bayes, MDL and consistency)</a></p>
<p>8 0.51276392 <a title="248-lda-8" href="../hunch_net-2012/hunch_net-2012-06-05-ICML_acceptance_statistics.html">466 hunch net-2012-06-05-ICML acceptance statistics</a></p>
<p>9 0.50655234 <a title="248-lda-9" href="../hunch_net-2005/hunch_net-2005-03-17-Going_all_the_Way%2C_Sometimes.html">42 hunch net-2005-03-17-Going all the Way, Sometimes</a></p>
<p>10 0.50611293 <a title="248-lda-10" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>11 0.5024572 <a title="248-lda-11" href="../hunch_net-2005/hunch_net-2005-09-12-Fast_Gradient_Descent.html">111 hunch net-2005-09-12-Fast Gradient Descent</a></p>
<p>12 0.50227827 <a title="248-lda-12" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>13 0.50095975 <a title="248-lda-13" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>14 0.50051105 <a title="248-lda-14" href="../hunch_net-2012/hunch_net-2012-02-24-ICML%2B50%25.html">456 hunch net-2012-02-24-ICML+50%</a></p>
<p>15 0.50034982 <a title="248-lda-15" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>16 0.49957672 <a title="248-lda-16" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>17 0.49930063 <a title="248-lda-17" href="../hunch_net-2009/hunch_net-2009-06-24-Interesting_papers_at_UAICMOLT_2009.html">361 hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</a></p>
<p>18 0.49909362 <a title="248-lda-18" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>19 0.49821678 <a title="248-lda-19" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>20 0.49815977 <a title="248-lda-20" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
