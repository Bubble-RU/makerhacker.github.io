<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>455 hunch net-2012-02-20-Berkeley Streaming Data Workshop</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2012" href="../home/hunch_net-2012_home.html">hunch_net-2012</a> <a title="hunch_net-2012-455" href="#">hunch_net-2012-455</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>455 hunch net-2012-02-20-Berkeley Streaming Data Workshop</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2012-455-html" href="http://hunch.net/?p=2278">html</a></p><p>Introduction: TheFrom Data to Knowledgeworkshop May 7-11 atBerkeleyshould be of interest to
the many people encountering streaming data in different disciplines. It's run
by a group of astronomers who encounter streaming data all the time. I metJosh
Bloomrecently and he is broadly interested in a workshop covering all aspects
of Machine Learning on streaming data. The hope here is that techniques
developed in one area turn out useful in another which seems quite plausible.
Particularly if you are in the bay area, consider checking it out.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 TheFrom Data to Knowledgeworkshop May 7-11 atBerkeleyshould be of interest to the many people encountering streaming data in different disciplines. [sent-1, score-1.38]
</p><p>2 It's run by a group of astronomers who encounter streaming data all the time. [sent-2, score-1.315]
</p><p>3 I metJosh Bloomrecently and he is broadly interested in a workshop covering all aspects of Machine Learning on streaming data. [sent-3, score-1.391]
</p><p>4 The hope here is that techniques developed in one area turn out useful in another which seems quite plausible. [sent-4, score-0.98]
</p><p>5 Particularly if you are in the bay area, consider checking it out. [sent-5, score-0.483]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('streaming', 0.723), ('encountering', 0.241), ('bay', 0.223), ('area', 0.2), ('data', 0.194), ('checking', 0.175), ('encounter', 0.163), ('broadly', 0.163), ('covering', 0.159), ('aspects', 0.15), ('developed', 0.145), ('turn', 0.145), ('group', 0.132), ('workshop', 0.112), ('run', 0.103), ('techniques', 0.103), ('interest', 0.091), ('hope', 0.087), ('consider', 0.085), ('interested', 0.084), ('useful', 0.078), ('particularly', 0.078), ('another', 0.076), ('quite', 0.067), ('different', 0.062), ('may', 0.05), ('seems', 0.049), ('people', 0.038), ('machine', 0.037), ('many', 0.031), ('one', 0.03), ('learning', 0.016)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="455-tfidf-1" href="../hunch_net-2012/hunch_net-2012-02-20-Berkeley_Streaming_Data_Workshop.html">455 hunch net-2012-02-20-Berkeley Streaming Data Workshop</a></p>
<p>Introduction: TheFrom Data to Knowledgeworkshop May 7-11 atBerkeleyshould be of interest to
the many people encountering streaming data in different disciplines. It's run
by a group of astronomers who encounter streaming data all the time. I metJosh
Bloomrecently and he is broadly interested in a workshop covering all aspects
of Machine Learning on streaming data. The hope here is that techniques
developed in one area turn out useful in another which seems quite plausible.
Particularly if you are in the bay area, consider checking it out.</p><p>2 0.30897227 <a title="455-tfidf-2" href="../hunch_net-2012/hunch_net-2012-04-20-Both_new%3A_STOC_workshops_and_NEML.html">462 hunch net-2012-04-20-Both new: STOC workshops and NEML</a></p>
<p>Introduction: May 16 in Cambridge, is theNew England Machine Learning Day, a first regional
workshop/symposium on machine learning. To present a poster, submit an
abstract byMay 5.May 19 in New York,STOCis coming to town and rather
surprisingly havingworkshopswhich should be quite a bit of fun. I'll be
speaking atAlgorithms for Distributed and Streaming Data.</p><p>3 0.15549456 <a title="455-tfidf-3" href="../hunch_net-2009/hunch_net-2009-04-23-Jonathan_Chang_at_Slycoder.html">350 hunch net-2009-04-23-Jonathan Chang at Slycoder</a></p>
<p>Introduction: Jonathan Changhas aresearch blogon aspects of machine learning.</p><p>4 0.082931586 <a title="455-tfidf-4" href="../hunch_net-2007/hunch_net-2007-02-22-Create_Your_Own_ICML_Workshop.html">234 hunch net-2007-02-22-Create Your Own ICML Workshop</a></p>
<p>Introduction: As usualICML 2007will be hosting aworkshop programto be held this year on June
24th. The success of the program depends on having researchers like you
propose interesting workshop topics and then organize the workshops. I'd like
to encourage all of you to consider sending a workshop proposal. The proposal
deadline has been extended to March 5. See the workshop web-site for
details.Organizing a workshop is a unique way to gather an international group
of researchers together to focus for an entire day on a topic of your
choosing. I've always found that the cost of organizing a workshop is not so
large, and very low compared to the benefits. The topic and format of a
workshop are limited only by your imagination (and the attractiveness to
potential participants) and need not follow the usual model of a mini-
conference on a particular ML sub-area. Hope to see some interesting proposals
rolling in.</p><p>5 0.076997817 <a title="455-tfidf-5" href="../hunch_net-2012/hunch_net-2012-08-24-Patterns_for_research_in_machine_learning.html">471 hunch net-2012-08-24-Patterns for research in machine learning</a></p>
<p>Introduction: There area handful of basic code patternsthat I wish I was more aware of when
I started research in machine learning. Each on its own may seem pointless,
but collectively they go a long way towards making the typical research
workflow more efficient. Here they are:Separate code from data.Separate input
data, working data and output data.Save everything to disk frequently.Separate
options from parameters.Do not use global variables.Record the options used to
generate each run of the algorithm.Make it easy to sweep options.Make it easy
to execute only portions of the code.Use checkpointing.Write demos and
tests.Clickherefor discussion and examples for each item. Also seeCharles
Sutton'sandHackerNews'thoughts on the same topic.My guess is that these
patterns will not only be useful for machine learning, but also any other
computational work that involves either a) processing large amounts of data,
or b) algorithms that take a significant amount of time to execute. Share this
list with you</p><p>6 0.074141689 <a title="455-tfidf-6" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>7 0.074043572 <a title="455-tfidf-7" href="../hunch_net-2008/hunch_net-2008-10-20-New_York%26%238217%3Bs_ML_Day.html">322 hunch net-2008-10-20-New York&#8217;s ML Day</a></p>
<p>8 0.069501989 <a title="455-tfidf-8" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>9 0.068289213 <a title="455-tfidf-9" href="../hunch_net-2010/hunch_net-2010-08-20-The_Workshop_on_Cores%2C_Clusters%2C_and_Clouds.html">404 hunch net-2010-08-20-The Workshop on Cores, Clusters, and Clouds</a></p>
<p>10 0.068099819 <a title="455-tfidf-10" href="../hunch_net-2007/hunch_net-2007-12-12-Workshop_Summary%26%238212%3BPrinciples_of_Learning_Problem_Design.html">277 hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</a></p>
<p>11 0.066867322 <a title="455-tfidf-11" href="../hunch_net-2007/hunch_net-2007-08-25-The_Privacy_Problem.html">260 hunch net-2007-08-25-The Privacy Problem</a></p>
<p>12 0.066191226 <a title="455-tfidf-12" href="../hunch_net-2008/hunch_net-2008-11-10-ICML_Reviewing_Criteria.html">325 hunch net-2008-11-10-ICML Reviewing Criteria</a></p>
<p>13 0.062345512 <a title="455-tfidf-13" href="../hunch_net-2005/hunch_net-2005-03-24-The_Role_of_Workshops.html">46 hunch net-2005-03-24-The Role of Workshops</a></p>
<p>14 0.061425753 <a title="455-tfidf-14" href="../hunch_net-2010/hunch_net-2010-08-24-Alex_Smola_starts_a_blog.html">408 hunch net-2010-08-24-Alex Smola starts a blog</a></p>
<p>15 0.061159298 <a title="455-tfidf-15" href="../hunch_net-2013/hunch_net-2013-12-01-NIPS_tutorials_and_Vowpal_Wabbit_7.4.html">492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</a></p>
<p>16 0.060091622 <a title="455-tfidf-16" href="../hunch_net-2012/hunch_net-2012-05-02-ICML%3A_Behind_the_Scenes.html">463 hunch net-2012-05-02-ICML: Behind the Scenes</a></p>
<p>17 0.059566945 <a title="455-tfidf-17" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>18 0.058787856 <a title="455-tfidf-18" href="../hunch_net-2005/hunch_net-2005-12-09-Some_NIPS_papers.html">138 hunch net-2005-12-09-Some NIPS papers</a></p>
<p>19 0.056388292 <a title="455-tfidf-19" href="../hunch_net-2008/hunch_net-2008-10-01-NIPS_2008_workshop_on_%26%238216%3BLearning_over_Empirical_Hypothesis_Spaces%26%238217%3B.html">319 hunch net-2008-10-01-NIPS 2008 workshop on &#8216;Learning over Empirical Hypothesis Spaces&#8217;</a></p>
<p>20 0.056251001 <a title="455-tfidf-20" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.107), (1, 0.028), (2, 0.073), (3, 0.1), (4, -0.054), (5, 0.014), (6, -0.07), (7, -0.01), (8, 0.017), (9, 0.018), (10, -0.036), (11, 0.181), (12, 0.038), (13, -0.043), (14, -0.066), (15, -0.046), (16, -0.04), (17, -0.043), (18, 0.079), (19, 0.046), (20, 0.026), (21, -0.081), (22, 0.076), (23, -0.028), (24, -0.048), (25, 0.049), (26, 0.04), (27, 0.007), (28, 0.057), (29, 0.052), (30, 0.017), (31, -0.033), (32, 0.114), (33, 0.015), (34, -0.086), (35, -0.159), (36, -0.055), (37, 0.014), (38, 0.031), (39, 0.11), (40, -0.072), (41, 0.019), (42, -0.018), (43, -0.002), (44, 0.028), (45, -0.031), (46, 0.065), (47, -0.065), (48, -0.04), (49, -0.06)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96208084 <a title="455-lsi-1" href="../hunch_net-2012/hunch_net-2012-02-20-Berkeley_Streaming_Data_Workshop.html">455 hunch net-2012-02-20-Berkeley Streaming Data Workshop</a></p>
<p>Introduction: TheFrom Data to Knowledgeworkshop May 7-11 atBerkeleyshould be of interest to
the many people encountering streaming data in different disciplines. It's run
by a group of astronomers who encounter streaming data all the time. I metJosh
Bloomrecently and he is broadly interested in a workshop covering all aspects
of Machine Learning on streaming data. The hope here is that techniques
developed in one area turn out useful in another which seems quite plausible.
Particularly if you are in the bay area, consider checking it out.</p><p>2 0.76471573 <a title="455-lsi-2" href="../hunch_net-2012/hunch_net-2012-04-20-Both_new%3A_STOC_workshops_and_NEML.html">462 hunch net-2012-04-20-Both new: STOC workshops and NEML</a></p>
<p>Introduction: May 16 in Cambridge, is theNew England Machine Learning Day, a first regional
workshop/symposium on machine learning. To present a poster, submit an
abstract byMay 5.May 19 in New York,STOCis coming to town and rather
surprisingly havingworkshopswhich should be quite a bit of fun. I'll be
speaking atAlgorithms for Distributed and Streaming Data.</p><p>3 0.65345621 <a title="455-lsi-3" href="../hunch_net-2010/hunch_net-2010-08-24-Alex_Smola_starts_a_blog.html">408 hunch net-2010-08-24-Alex Smola starts a blog</a></p>
<p>Introduction: Adventures in Data Land.</p><p>4 0.55579197 <a title="455-lsi-4" href="../hunch_net-2005/hunch_net-2005-12-07-Is_the_Google_way_the_way_for_machine_learning%3F.html">136 hunch net-2005-12-07-Is the Google way the way for machine learning?</a></p>
<p>Introduction: Urs HoelzlefromGooglegave an invited presentation atNIPS. In the presentation,
he strongly advocates interacting with data in a particular scalable manner
which is something like the following:Make a cluster of machines.Build a
unified filesystem. (Google uses GFS, but NFS or other approaches work
reasonably well for smaller clusters.)Interact with data viaMapReduce.Creating
a cluster of machines is, by this point, relatively straightforward.Unified
filesystems are a little bit tricky--GFS is capable by design of essentially
unlimited speed throughput to disk. NFS can bottleneck because all of the data
has to move through one machine. Nevertheless, this may not be a limiting
factor for smaller clusters.MapReduce is a programming paradigm. Essentially,
it is a combination of a data element transform (map) and an
agreggator/selector (reduce). These operations are highly parallelizable and
the claim is that they support the forms of data interaction which are
necessary.Apparently, theNutc</p><p>5 0.55207682 <a title="455-lsi-5" href="../hunch_net-2005/hunch_net-2005-01-31-Watchword%3A_Assumption.html">7 hunch net-2005-01-31-Watchword: Assumption</a></p>
<p>Introduction: "Assumption" is another word to be careful with in machine learning because it
is used in several ways.Assumption = BiasThere are several ways to see that
some form of 'bias' (= preferring of one solution over another) is necessary.
This is obvious in an adversarial setting. A good bit of work has been
expended explaining this in other settings with "no free lunch" theorems. This
is a usage specialized to learning which is particularly common when talking
about priors for Bayesian Learning.Assumption = "if" of a theoremThe
assumptions are the 'if' part of the 'if-then' in a theorem. This is a fairly
common usage.Assumption = AxiomThe assumptions are the things that we assume
are true, but which we cannot verify. Examples are "the IID assumption" or "my
problem is a DNF on a small number of bits". This is the usage which I
prefer.One difficulty with any use of the word "assumption" is that you often
encounter "ifassumptionthenconclusionso ifnot assumptionthennot conclusion".
This is inc</p><p>6 0.55100542 <a title="455-lsi-6" href="../hunch_net-2007/hunch_net-2007-06-23-Machine_Learning_Jobs_are_Growing_on_Trees.html">250 hunch net-2007-06-23-Machine Learning Jobs are Growing on Trees</a></p>
<p>7 0.54838681 <a title="455-lsi-7" href="../hunch_net-2007/hunch_net-2007-08-25-The_Privacy_Problem.html">260 hunch net-2007-08-25-The Privacy Problem</a></p>
<p>8 0.53415036 <a title="455-lsi-8" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>9 0.53356969 <a title="455-lsi-9" href="../hunch_net-2012/hunch_net-2012-08-24-Patterns_for_research_in_machine_learning.html">471 hunch net-2012-08-24-Patterns for research in machine learning</a></p>
<p>10 0.50671661 <a title="455-lsi-10" href="../hunch_net-2009/hunch_net-2009-04-23-Jonathan_Chang_at_Slycoder.html">350 hunch net-2009-04-23-Jonathan Chang at Slycoder</a></p>
<p>11 0.49705637 <a title="455-lsi-11" href="../hunch_net-2008/hunch_net-2008-10-20-New_York%26%238217%3Bs_ML_Day.html">322 hunch net-2008-10-20-New York&#8217;s ML Day</a></p>
<p>12 0.49461281 <a title="455-lsi-12" href="../hunch_net-2009/hunch_net-2009-08-27-New_York_Area_Machine_Learning_Events.html">369 hunch net-2009-08-27-New York Area Machine Learning Events</a></p>
<p>13 0.49305961 <a title="455-lsi-13" href="../hunch_net-2012/hunch_net-2012-10-18-7th_Annual_Machine_Learning_Symposium.html">474 hunch net-2012-10-18-7th Annual Machine Learning Symposium</a></p>
<p>14 0.46363944 <a title="455-lsi-14" href="../hunch_net-2005/hunch_net-2005-04-25-Embeddings%3A_what_are_they_good_for%3F.html">61 hunch net-2005-04-25-Embeddings: what are they good for?</a></p>
<p>15 0.4550634 <a title="455-lsi-15" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>16 0.44776842 <a title="455-lsi-16" href="../hunch_net-2007/hunch_net-2007-12-12-Workshop_Summary%26%238212%3BPrinciples_of_Learning_Problem_Design.html">277 hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</a></p>
<p>17 0.44215685 <a title="455-lsi-17" href="../hunch_net-2005/hunch_net-2005-10-20-Machine_Learning_in_the_News.html">125 hunch net-2005-10-20-Machine Learning in the News</a></p>
<p>18 0.43728745 <a title="455-lsi-18" href="../hunch_net-2010/hunch_net-2010-08-20-The_Workshop_on_Cores%2C_Clusters%2C_and_Clouds.html">404 hunch net-2010-08-20-The Workshop on Cores, Clusters, and Clouds</a></p>
<p>19 0.43352178 <a title="455-lsi-19" href="../hunch_net-2008/hunch_net-2008-10-01-NIPS_2008_workshop_on_%26%238216%3BLearning_over_Empirical_Hypothesis_Spaces%26%238217%3B.html">319 hunch net-2008-10-01-NIPS 2008 workshop on &#8216;Learning over Empirical Hypothesis Spaces&#8217;</a></p>
<p>20 0.433442 <a title="455-lsi-20" href="../hunch_net-2013/hunch_net-2013-08-31-Extreme_Classification_workshop_at_NIPS.html">488 hunch net-2013-08-31-Extreme Classification workshop at NIPS</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(36, 0.42), (42, 0.196), (68, 0.119), (74, 0.076)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.76690674 <a title="455-lda-1" href="../hunch_net-2012/hunch_net-2012-02-20-Berkeley_Streaming_Data_Workshop.html">455 hunch net-2012-02-20-Berkeley Streaming Data Workshop</a></p>
<p>Introduction: TheFrom Data to Knowledgeworkshop May 7-11 atBerkeleyshould be of interest to
the many people encountering streaming data in different disciplines. It's run
by a group of astronomers who encounter streaming data all the time. I metJosh
Bloomrecently and he is broadly interested in a workshop covering all aspects
of Machine Learning on streaming data. The hope here is that techniques
developed in one area turn out useful in another which seems quite plausible.
Particularly if you are in the bay area, consider checking it out.</p><p>2 0.49376765 <a title="455-lda-2" href="../hunch_net-2005/hunch_net-2005-09-10-%26%238220%3BFailure%26%238221%3B_is_an_option.html">110 hunch net-2005-09-10-&#8220;Failure&#8221; is an option</a></p>
<p>Introduction: This is about the hard choices that graduate students must make.The cultural
definition of success in academic research is to:Produce good research which
many other people appreciate.Produce many students who go on to do the
same.There are fundamental reasons why this is success in the local culture.
Good research appreciated by others means access to jobs. Many students
succesful in the same way implies that there are a number of people who think
in a similar way and appreciate your work.In order to graduate, a phd student
must live in an academic culture for a period of several years. It is common
to adopt the culture's definition of success during this time. It's also
common for many phd students discover they are not suited to an academic
research lifestyle. This collision of values and abilities naturally results
in depression.The most fundamental advice when this happens is: change
something. Pick a new advisor. Pick a new research topic. Or leave the program
(and do something el</p><p>3 0.48984608 <a title="455-lda-3" href="../hunch_net-2005/hunch_net-2005-08-22-Do_you_believe_in_induction%3F.html">104 hunch net-2005-08-22-Do you believe in induction?</a></p>
<p>Introduction: Foster Provostgave a talk at the ICMLmetalearning workshopon "metalearning"
and the "no free lunch theorem" which seems worth summarizing.As a review: the
no free lunch theorem is the most complicated way we know of to say that
abiasis required in order to learn. The simplest way to see this is in a
nonprobabilistic setting. If you are given examples of the form(x,y)and you
wish to predictyfromxthen any prediction mechanism errs half the time in
expectation over all sequences of examples. The proof of this is very simple:
on every example a predictor must make some prediction and by symmetry over
the set of sequences it will be wrong half the time and right half the time.
The basic idea of this proof has been applied to many other settings.The
simplistic interpretation of this theorem which many people jump to is
"machine learning is dead" since there can be no single learning algorithm
which can solve all learning problems. This is the wrong way to think about
it. In the real world, w</p><p>4 0.48964179 <a title="455-lda-4" href="../hunch_net-2008/hunch_net-2008-04-12-It_Doesn%26%238217%3Bt_Stop.html">295 hunch net-2008-04-12-It Doesn&#8217;t Stop</a></p>
<p>Introduction: I've enjoyed theTerminatormovies and show. Neglecting the whacky aspects (time
travel and associated paradoxes), there is an enduring topic of discussion:
how do people deal with intelligent machines (and vice versa)?In Terminator-
land, the primary method for dealing with intelligent machines is to prevent
them from being made. This approach works pretty badly, because a new angle on
building an intelligent machine keeps coming up. This is partly a ploy for
writer's to avoid writing themselves out of a job, but there is a fundamental
truth to it as well: preventing progress in research is hard.The United
States, has been experimenting with trying to stop research onstem cells. It
hasn't worked very well--the net effect has been retarding research programs a
bit, and exporting some research to other countries. Another less recent
example was encryption technology, for which the United States generally did
not encourage early public research and evendiscouraged as a munition. This
slowe</p><p>5 0.48454878 <a title="455-lda-5" href="../hunch_net-2012/hunch_net-2012-03-24-David_Waltz.html">460 hunch net-2012-03-24-David Waltz</a></p>
<p>Introduction: hasdied. He lived a full life. I know him personally as a founder of theCenter
for Computational Learning Systemsand theNew York Machine Learning Symposium,
both of which have sheltered and promoted the advancement of machine learning.
I expect much of the New York area machine learning community will miss him,
as well as many others around the world.</p><p>6 0.48185554 <a title="455-lda-6" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>7 0.48093414 <a title="455-lda-7" href="../hunch_net-2005/hunch_net-2005-06-08-Question%3A_%26%238220%3BWhen_is_the_right_time_to_insert_the_loss_function%3F%26%238221%3B.html">79 hunch net-2005-06-08-Question: &#8220;When is the right time to insert the loss function?&#8221;</a></p>
<p>8 0.47791153 <a title="455-lda-8" href="../hunch_net-2005/hunch_net-2005-02-01-Watchword%3A_Loss.html">9 hunch net-2005-02-01-Watchword: Loss</a></p>
<p>9 0.477377 <a title="455-lda-9" href="../hunch_net-2009/hunch_net-2009-01-21-Nearly_all_natural_problems_require_nonlinearity.html">337 hunch net-2009-01-21-Nearly all natural problems require nonlinearity</a></p>
<p>10 0.47177604 <a title="455-lda-10" href="../hunch_net-2007/hunch_net-2007-11-28-Computational_Consequences_of_Classification.html">274 hunch net-2007-11-28-Computational Consequences of Classification</a></p>
<p>11 0.46883315 <a title="455-lda-11" href="../hunch_net-2007/hunch_net-2007-08-12-Exponentiated_Gradient.html">258 hunch net-2007-08-12-Exponentiated Gradient</a></p>
<p>12 0.46638441 <a title="455-lda-12" href="../hunch_net-2011/hunch_net-2011-07-11-Interesting_Neural_Network_Papers_at_ICML_2011.html">438 hunch net-2011-07-11-Interesting Neural Network Papers at ICML 2011</a></p>
<p>13 0.46430445 <a title="455-lda-13" href="../hunch_net-2007/hunch_net-2007-05-12-Loss_Function_Semantics.html">245 hunch net-2007-05-12-Loss Function Semantics</a></p>
<p>14 0.46303526 <a title="455-lda-14" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>15 0.46239939 <a title="455-lda-15" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>16 0.46149355 <a title="455-lda-16" href="../hunch_net-2009/hunch_net-2009-02-04-Optimal_Proxy_Loss_for_Classification.html">341 hunch net-2009-02-04-Optimal Proxy Loss for Classification</a></p>
<p>17 0.46138668 <a title="455-lda-17" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<p>18 0.46017426 <a title="455-lda-18" href="../hunch_net-2005/hunch_net-2005-08-11-Why_Manifold-Based_Dimension_Reduction_Techniques%3F.html">102 hunch net-2005-08-11-Why Manifold-Based Dimension Reduction Techniques?</a></p>
<p>19 0.45955989 <a title="455-lda-19" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>20 0.45915318 <a title="455-lda-20" href="../hunch_net-2006/hunch_net-2006-11-22-Explicit_Randomization_in_Learning_algorithms.html">219 hunch net-2006-11-22-Explicit Randomization in Learning algorithms</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
