<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>472 hunch net-2012-08-27-NYAS ML 2012 and ICML 2013</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2012" href="../home/hunch_net-2012_home.html">hunch_net-2012</a> <a title="hunch_net-2012-472" href="#">hunch_net-2012-472</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>472 hunch net-2012-08-27-NYAS ML 2012 and ICML 2013</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2012-472-html" href="http://hunch.net/?p=2574">html</a></p><p>Introduction: TheNew York Machine Learning Symposiumis October 19 with a 2 page abstract
deadline due September 13 via email with subject "Machine Learning Poster
Submission" sent to physicalscience@nyas.org. Everyone is welcome to submit.
Last year's attendance was 246 and I expect more this year.The primary
experiment forICML 2013is multiple paper submission deadlines with rolling
review cycles. The key dates are October 1, December 15, and February 15. This
is an attempt to shift ICML further towards a journal style review process and
reduce peak load. The "not for proceedings" experiment from this year's ICML
is not continuing.Edit: Fixed second ICML deadline.</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('october', 0.29), ('submission', 0.276), ('icml', 0.247), ('experiment', 0.229), ('deadline', 0.222), ('rolling', 0.2), ('december', 0.2), ('september', 0.185), ('peak', 0.185), ('foricml', 0.185), ('symposiumis', 0.185), ('review', 0.177), ('proceedings', 0.175), ('sent', 0.16), ('journal', 0.154), ('february', 0.149), ('welcome', 0.149), ('deadlines', 0.141), ('poster', 0.132), ('attendance', 0.129)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="472-tfidf-1" href="../hunch_net-2012/hunch_net-2012-08-27-NYAS_ML_2012_and_ICML_2013.html">472 hunch net-2012-08-27-NYAS ML 2012 and ICML 2013</a></p>
<p>Introduction: TheNew York Machine Learning Symposiumis October 19 with a 2 page abstract
deadline due September 13 via email with subject "Machine Learning Poster
Submission" sent to physicalscience@nyas.org. Everyone is welcome to submit.
Last year's attendance was 246 and I expect more this year.The primary
experiment forICML 2013is multiple paper submission deadlines with rolling
review cycles. The key dates are October 1, December 15, and February 15. This
is an attempt to shift ICML further towards a journal style review process and
reduce peak load. The "not for proceedings" experiment from this year's ICML
is not continuing.Edit: Fixed second ICML deadline.</p><p>2 0.18407772 <a title="472-tfidf-2" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>Introduction: Here's a quick reference for summer ML-related conferences sorted by due
date:ConferenceDue dateLocationReviewingKDDFeb 10August 12-16, Beijing,
ChinaSingle BlindCOLTFeb 14June 25-June 27, Edinburgh, ScotlandSingle Blind?
(historically)ICMLFeb 24June 26-July 1, Edinburgh, ScotlandDouble Blind,
author response, zeroSPOFUAIMarch 30August 15-17, Catalina Islands,
CaliforniaDouble Blind, author responseGeographically, this is greatly
dispersed and the UAI/KDD conflict is unfortunate.Machine Learning conferences
are triannual now, betweenNIPS,AIStat, andICML. This has not always been the
case: the academic default is annual summer conferences, then NIPS started
with a December conference, and now AIStat has grown into an April
conference.However, the first claim is not quite correct. NIPS and AIStat have
few competing venues while ICML implicitly competes with many other
conferences accepting machine learning related papers. SinceJoelleand I are
taking a turn as program chairs this year, I</p><p>3 0.16241765 <a title="472-tfidf-3" href="../hunch_net-2010/hunch_net-2010-01-19-Deadline_Season%2C_2010.html">387 hunch net-2010-01-19-Deadline Season, 2010</a></p>
<p>Introduction: Many conference deadlines are coming soon.DeadlineDouble Blind / Author
FeedbackTime/PlaceICMLJanuary 18((workshops) / February 1 (Papers) / February
13 (Tutorials)Y/YHaifa, Israel, June 21-25KDDFebruary 1(Workshops) / February
2&5 (Papers) / February 26 (Tutorials & Panels)) / April 17
(Demos)N/SWashington DC, July 25-28COLTJanuary 18 (Workshops) / February 19
(Papers)N/SHaifa, Israel, June 25-29UAIMarch 11 (Papers)N?/YCatalina Island,
California, July 8-11ICML continues to experiment with the reviewing process,
although perhaps less so than last year.The S "sort-of" for COLT is because
author feedback occurs only after decisions are made.KDD is notable for being
the most comprehensive in terms of {Tutorials, Workshops, Challenges, Panels,
Papers (two tracks), Demos}. The S for KDD is because there is sometimes
author feedback at the decision of the SPC.The (past) January 18 deadline for
workshops at ICML is nominal, as I (as workshop chair) almost missed it myself
and we have space f</p><p>4 0.16192818 <a title="472-tfidf-4" href="../hunch_net-2005/hunch_net-2005-12-29-Deadline_Season.html">145 hunch net-2005-12-29-Deadline Season</a></p>
<p>Introduction: Many different paper deadlines are coming up soon so I made a little reference
table. Out of curiosity, I also computed the interval between submission
deadline and
conference.ConferenceLocationDateDeadlineintervalCOLTPittsburghJune
22-25January 21152ICMLPittsburghJune 26-28January 30/February 6140UAIMITJuly
13-16March 9/March 16119AAAIBostonJuly 16-20February
16/21145KDDPhiladelphiaAugust 23-26March 3/March 10166It looks like the
northeastern US is the big winner as far as location this year.</p><p>5 0.13899392 <a title="472-tfidf-5" href="../hunch_net-2011/hunch_net-2011-10-10-ML_Symposium_and_ICML_details.html">447 hunch net-2011-10-10-ML Symposium and ICML details</a></p>
<p>Introduction: Everyone should have received notice forNY ML Symposiumabstracts. Check
carefully, as one was lost by our system.The event itself is October 21, next
week.Leon Bottou,Stephen Boyd, andYoav Freundare giving the invited talks this
year, and there are many spotlights on local work spread throughout the
day.Chris Wigginshas setup 6(!) ML-interested startups to follow the
symposium, which should be of substantial interest to the employment
interested.I also wanted to give an update onICML 2012. Unlike last year, our
deadline is coordinated withAIStat(which is due this Friday). The paper
deadline for ICML has been pushed back to February 24 which should allow
significant time for finishing up papers after the winter break. Other details
may interest people as well:We settled on usingCMTafter checking out the
possibilities. I wasn't looking for this, because I've often found CMT clunky
in terms of easy access to the right information. Nevertheless, the breadth of
features and willingness to s</p><p>6 0.126571 <a title="472-tfidf-6" href="../hunch_net-2008/hunch_net-2008-09-04-Fall_ML_Conferences.html">316 hunch net-2008-09-04-Fall ML Conferences</a></p>
<p>7 0.12353697 <a title="472-tfidf-7" href="../hunch_net-2008/hunch_net-2008-06-30-ICML_has_a_comment_system.html">305 hunch net-2008-06-30-ICML has a comment system</a></p>
<p>8 0.12318647 <a title="472-tfidf-8" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<p>9 0.1222354 <a title="472-tfidf-9" href="../hunch_net-2005/hunch_net-2005-05-02-Reviewing_techniques_for_conferences.html">65 hunch net-2005-05-02-Reviewing techniques for conferences</a></p>
<p>10 0.11856478 <a title="472-tfidf-10" href="../hunch_net-2009/hunch_net-2009-05-24-2009_ICML_discussion_site.html">356 hunch net-2009-05-24-2009 ICML discussion site</a></p>
<p>11 0.11733191 <a title="472-tfidf-11" href="../hunch_net-2006/hunch_net-2006-06-15-IJCAI_is_out_of_season.html">184 hunch net-2006-06-15-IJCAI is out of season</a></p>
<p>12 0.11255359 <a title="472-tfidf-12" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>13 0.11249871 <a title="472-tfidf-13" href="../hunch_net-2008/hunch_net-2008-02-10-Complexity_Illness.html">288 hunch net-2008-02-10-Complexity Illness</a></p>
<p>14 0.1112039 <a title="472-tfidf-14" href="../hunch_net-2005/hunch_net-2005-07-11-AAAI_blog.html">92 hunch net-2005-07-11-AAAI blog</a></p>
<p>15 0.11047805 <a title="472-tfidf-15" href="../hunch_net-2013/hunch_net-2013-08-31-Extreme_Classification_workshop_at_NIPS.html">488 hunch net-2013-08-31-Extreme Classification workshop at NIPS</a></p>
<p>16 0.10673629 <a title="472-tfidf-16" href="../hunch_net-2009/hunch_net-2009-12-09-Future_Publication_Models_%40_NIPS.html">382 hunch net-2009-12-09-Future Publication Models @ NIPS</a></p>
<p>17 0.10447001 <a title="472-tfidf-17" href="../hunch_net-2005/hunch_net-2005-10-19-Workshop%3A_Atomic_Learning.html">124 hunch net-2005-10-19-Workshop: Atomic Learning</a></p>
<p>18 0.10008217 <a title="472-tfidf-18" href="../hunch_net-2009/hunch_net-2009-10-26-NIPS_workshops.html">375 hunch net-2009-10-26-NIPS workshops</a></p>
<p>19 0.099730819 <a title="472-tfidf-19" href="../hunch_net-2013/hunch_net-2013-11-09-Graduates_and_Postdocs.html">490 hunch net-2013-11-09-Graduates and Postdocs</a></p>
<p>20 0.09951517 <a title="472-tfidf-20" href="../hunch_net-2012/hunch_net-2012-04-20-Both_new%3A_STOC_workshops_and_NEML.html">462 hunch net-2012-04-20-Both new: STOC workshops and NEML</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.125), (1, 0.193), (2, 0.02), (3, 0.182), (4, -0.017), (5, -0.033), (6, -0.016), (7, -0.003), (8, 0.133), (9, -0.129), (10, 0.085), (11, -0.029), (12, 0.078), (13, -0.084), (14, 0.047), (15, -0.104), (16, 0.009), (17, -0.061), (18, -0.041), (19, 0.024), (20, 0.05), (21, -0.171), (22, -0.032), (23, 0.054), (24, -0.039), (25, -0.068), (26, -0.027), (27, -0.165), (28, -0.016), (29, 0.054), (30, 0.011), (31, 0.14), (32, 0.017), (33, 0.049), (34, -0.046), (35, -0.02), (36, -0.036), (37, 0.119), (38, -0.021), (39, -0.069), (40, 0.022), (41, -0.054), (42, -0.066), (43, 0.026), (44, -0.086), (45, 0.037), (46, 0.055), (47, 0.025), (48, -0.02), (49, -0.062)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97495079 <a title="472-lsi-1" href="../hunch_net-2012/hunch_net-2012-08-27-NYAS_ML_2012_and_ICML_2013.html">472 hunch net-2012-08-27-NYAS ML 2012 and ICML 2013</a></p>
<p>Introduction: TheNew York Machine Learning Symposiumis October 19 with a 2 page abstract
deadline due September 13 via email with subject "Machine Learning Poster
Submission" sent to physicalscience@nyas.org. Everyone is welcome to submit.
Last year's attendance was 246 and I expect more this year.The primary
experiment forICML 2013is multiple paper submission deadlines with rolling
review cycles. The key dates are October 1, December 15, and February 15. This
is an attempt to shift ICML further towards a journal style review process and
reduce peak load. The "not for proceedings" experiment from this year's ICML
is not continuing.Edit: Fixed second ICML deadline.</p><p>2 0.57207566 <a title="472-lsi-2" href="../hunch_net-2008/hunch_net-2008-09-04-Fall_ML_Conferences.html">316 hunch net-2008-09-04-Fall ML Conferences</a></p>
<p>Introduction: If you are in the New York area and interested in machine learning, consider
submitting a 2 page abstract to theML symposiumby tomorrow (Sept 5th)
midnight. It's a fun one day affair on October 10 in an awesome location
overlooking the world trade center site.A bit further off (but a real
conference) is theAI and Statsdeadline on November 5, to be held in Florida
April 16-19.</p><p>3 0.5406816 <a title="472-lsi-3" href="../hunch_net-2005/hunch_net-2005-12-29-Deadline_Season.html">145 hunch net-2005-12-29-Deadline Season</a></p>
<p>Introduction: Many different paper deadlines are coming up soon so I made a little reference
table. Out of curiosity, I also computed the interval between submission
deadline and
conference.ConferenceLocationDateDeadlineintervalCOLTPittsburghJune
22-25January 21152ICMLPittsburghJune 26-28January 30/February 6140UAIMITJuly
13-16March 9/March 16119AAAIBostonJuly 16-20February
16/21145KDDPhiladelphiaAugust 23-26March 3/March 10166It looks like the
northeastern US is the big winner as far as location this year.</p><p>4 0.52654338 <a title="472-lsi-4" href="../hunch_net-2011/hunch_net-2011-10-10-ML_Symposium_and_ICML_details.html">447 hunch net-2011-10-10-ML Symposium and ICML details</a></p>
<p>Introduction: Everyone should have received notice forNY ML Symposiumabstracts. Check
carefully, as one was lost by our system.The event itself is October 21, next
week.Leon Bottou,Stephen Boyd, andYoav Freundare giving the invited talks this
year, and there are many spotlights on local work spread throughout the
day.Chris Wigginshas setup 6(!) ML-interested startups to follow the
symposium, which should be of substantial interest to the employment
interested.I also wanted to give an update onICML 2012. Unlike last year, our
deadline is coordinated withAIStat(which is due this Friday). The paper
deadline for ICML has been pushed back to February 24 which should allow
significant time for finishing up papers after the winter break. Other details
may interest people as well:We settled on usingCMTafter checking out the
possibilities. I wasn't looking for this, because I've often found CMT clunky
in terms of easy access to the right information. Nevertheless, the breadth of
features and willingness to s</p><p>5 0.50457734 <a title="472-lsi-5" href="../hunch_net-2012/hunch_net-2012-07-17-MUCMD_and_BayLearn.html">470 hunch net-2012-07-17-MUCMD and BayLearn</a></p>
<p>Introduction: The workshop on theMeaningful Use of Complex Medical Datais happening again,
August 9-12 in LA, nearUAIon Catalina Island August 15-17. I enjoyed my visit
last year, and expect this year to be interesting also.The firstBay Area
Machine Learning Symposiumis August 30 atGoogle. Abstracts are due July 30.</p><p>6 0.50364608 <a title="472-lsi-6" href="../hunch_net-2006/hunch_net-2006-05-21-NIPS_paper_evaluation_criteria.html">180 hunch net-2006-05-21-NIPS paper evaluation criteria</a></p>
<p>7 0.49587762 <a title="472-lsi-7" href="../hunch_net-2006/hunch_net-2006-06-15-IJCAI_is_out_of_season.html">184 hunch net-2006-06-15-IJCAI is out of season</a></p>
<p>8 0.48226833 <a title="472-lsi-8" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>9 0.48046789 <a title="472-lsi-9" href="../hunch_net-2009/hunch_net-2009-05-24-2009_ICML_discussion_site.html">356 hunch net-2009-05-24-2009 ICML discussion site</a></p>
<p>10 0.4766193 <a title="472-lsi-10" href="../hunch_net-2008/hunch_net-2008-06-30-ICML_has_a_comment_system.html">305 hunch net-2008-06-30-ICML has a comment system</a></p>
<p>11 0.46220306 <a title="472-lsi-11" href="../hunch_net-2011/hunch_net-2011-09-03-Fall_Machine_Learning_Events.html">443 hunch net-2011-09-03-Fall Machine Learning Events</a></p>
<p>12 0.44078472 <a title="472-lsi-12" href="../hunch_net-2010/hunch_net-2010-01-19-Deadline_Season%2C_2010.html">387 hunch net-2010-01-19-Deadline Season, 2010</a></p>
<p>13 0.43065137 <a title="472-lsi-13" href="../hunch_net-2012/hunch_net-2012-04-20-Both_new%3A_STOC_workshops_and_NEML.html">462 hunch net-2012-04-20-Both new: STOC workshops and NEML</a></p>
<p>14 0.43063605 <a title="472-lsi-14" href="../hunch_net-2013/hunch_net-2013-04-15-NEML_II.html">481 hunch net-2013-04-15-NEML II</a></p>
<p>15 0.40912333 <a title="472-lsi-15" href="../hunch_net-2005/hunch_net-2005-07-11-AAAI_blog.html">92 hunch net-2005-07-11-AAAI blog</a></p>
<p>16 0.40662229 <a title="472-lsi-16" href="../hunch_net-2010/hunch_net-2010-11-18-ICML_2011_%26%238211%3B_Call_for_Tutorials.html">417 hunch net-2010-11-18-ICML 2011 &#8211; Call for Tutorials</a></p>
<p>17 0.40185139 <a title="472-lsi-17" href="../hunch_net-2005/hunch_net-2005-10-19-Workshop%3A_Atomic_Learning.html">124 hunch net-2005-10-19-Workshop: Atomic Learning</a></p>
<p>18 0.38900882 <a title="472-lsi-18" href="../hunch_net-2012/hunch_net-2012-03-24-David_Waltz.html">460 hunch net-2012-03-24-David Waltz</a></p>
<p>19 0.3883104 <a title="472-lsi-19" href="../hunch_net-2005/hunch_net-2005-05-02-Reviewing_techniques_for_conferences.html">65 hunch net-2005-05-02-Reviewing techniques for conferences</a></p>
<p>20 0.38828871 <a title="472-lsi-20" href="../hunch_net-2011/hunch_net-2011-01-16-2011_Summer_Conference_Deadline_Season.html">422 hunch net-2011-01-16-2011 Summer Conference Deadline Season</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(35, 0.04), (42, 0.23), (74, 0.126), (85, 0.43), (95, 0.043)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.86637998 <a title="472-lda-1" href="../hunch_net-2012/hunch_net-2012-08-27-NYAS_ML_2012_and_ICML_2013.html">472 hunch net-2012-08-27-NYAS ML 2012 and ICML 2013</a></p>
<p>Introduction: TheNew York Machine Learning Symposiumis October 19 with a 2 page abstract
deadline due September 13 via email with subject "Machine Learning Poster
Submission" sent to physicalscience@nyas.org. Everyone is welcome to submit.
Last year's attendance was 246 and I expect more this year.The primary
experiment forICML 2013is multiple paper submission deadlines with rolling
review cycles. The key dates are October 1, December 15, and February 15. This
is an attempt to shift ICML further towards a journal style review process and
reduce peak load. The "not for proceedings" experiment from this year's ICML
is not continuing.Edit: Fixed second ICML deadline.</p><p>2 0.85042083 <a title="472-lda-2" href="../hunch_net-2008/hunch_net-2008-08-04-Electoralmarkets.com.html">312 hunch net-2008-08-04-Electoralmarkets.com</a></p>
<p>Introduction: Lancereminded me aboutelectoralmarketstoday, which is cool enough that I want
to point it out explicitly here.Most people stilluse pollsto predict who wins,
while electoralmarkets uses people betting real money. They might use polling
information, but any other sources of information are implicitly also allowed.
A side-by-side comparison of how polls compare to prediction markets might be
fun in a few months.</p><p>3 0.72330415 <a title="472-lda-3" href="../hunch_net-2005/hunch_net-2005-08-18-SVM_Adaptability.html">103 hunch net-2005-08-18-SVM Adaptability</a></p>
<p>Introduction: Several recent papers have shown that SVM-like optimizations can be used to
handle several large family loss functions.This is a good thing because it is
implausible that thelossfunction imposed by the world can not be taken into
account in the process of solving a prediction problem. Even people used to
the hard-coreBayesianapproach to learning often note that some approximations
are almost inevitable in specifying apriorand/or integrating to achieve a
posterior. Taking into account how the system will be evaluated can allow both
computational effort and design effort to be focused so as to improve
performance.A current laundry list of capabilities includes:2002multiclass SVM
including arbitrary cost matricesICML 2003Hidden Markov ModelsNIPS 2003Markov
Networks(see somediscussion)EMNLP 2004Context free grammarsICML 2004Any loss
(with much computation)ICML 2005Anyconstrained linear prediction model(that's
my own name).ICML 2005Any loss dependent on a contingency tableI am personally
in</p><p>4 0.67187464 <a title="472-lda-4" href="../hunch_net-2007/hunch_net-2007-05-12-Loss_Function_Semantics.html">245 hunch net-2007-05-12-Loss Function Semantics</a></p>
<p>Introduction: Some loss functions have a meaning, which can be understood in a manner
independent of the loss function itself.Optimizing squared
losslsq(y,y')=(y-y')2means predicting the (conditional) mean ofy.Optimizing
absolute value losslav(y,y')=|y-y'|means predicting the (conditional) median
ofy. Variants canhandle other quantiles. 0/1 loss for classification is a
special case.Optimizing log lossllog(y,y')=log (1/Prz~y'(z=y))means minimizing
the description length ofy.The semantics (= meaning) of the loss are made
explicit by a theorem in each case. For squared loss, we can prove a theorem
of the form:For all distributionsDoverY, ify' = arg miny'Ey ~ Dlsq(y,y')theny'
= Ey~DySimilar theorems hold for the other examples above, and they can all be
extended to predictors ofy'for distributionsDover a contextXand a valueY.There
are 3 points to this post.Everyone doing general machine learning should be
aware of the laundry list above. They form a handy toolkit which can match
many of the problems nat</p><p>5 0.61829293 <a title="472-lda-5" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>Introduction: The 2006 Machine Learning Summer School in Taipei, Taiwan ended on August 4,
2006. It has been a very exciting two weeks for a record crowd of 245
participants (including speakers and organizers) from 18 countries. We had a
lineup of speakers that is hard to match up for other similar events (see
ourWIKIfor more information). With this lineup, it is difficult for us as
organizers to screw it up too bad. Also, since we have pretty good
infrastructure for international meetings and experienced staff at NTUST and
Academia Sinica, plus the reputation established by previous MLSS series, it
was relatively easy for us to attract registrations and simply enjoyed this
two-week long party of machine learning.In the end of MLSS we distributed a
survey form for participants to fill in. I will report what we found from this
survey, together with the registration data and word-of-mouth from
participants.The first question is designed to find out how our participants
learned about MLSS 2006 Taipei.</p><p>6 0.52608353 <a title="472-lda-6" href="../hunch_net-2005/hunch_net-2005-02-17-Learning_Research_Programs.html">21 hunch net-2005-02-17-Learning Research Programs</a></p>
<p>7 0.52284926 <a title="472-lda-7" href="../hunch_net-2012/hunch_net-2012-09-29-Vowpal_Wabbit%2C_version_7.0.html">473 hunch net-2012-09-29-Vowpal Wabbit, version 7.0</a></p>
<p>8 0.52164596 <a title="472-lda-8" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>9 0.51742262 <a title="472-lda-9" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>10 0.51558429 <a title="472-lda-10" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>11 0.51496738 <a title="472-lda-11" href="../hunch_net-2008/hunch_net-2008-11-10-ICML_Reviewing_Criteria.html">325 hunch net-2008-11-10-ICML Reviewing Criteria</a></p>
<p>12 0.51366419 <a title="472-lda-12" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>13 0.51237017 <a title="472-lda-13" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>14 0.51152855 <a title="472-lda-14" href="../hunch_net-2010/hunch_net-2010-08-23-Boosted_Decision_Trees_for_Deep_Learning.html">407 hunch net-2010-08-23-Boosted Decision Trees for Deep Learning</a></p>
<p>15 0.50974101 <a title="472-lda-15" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>16 0.50906843 <a title="472-lda-16" href="../hunch_net-2006/hunch_net-2006-05-05-An_ICML_reject.html">177 hunch net-2006-05-05-An ICML reject</a></p>
<p>17 0.50845212 <a title="472-lda-17" href="../hunch_net-2005/hunch_net-2005-03-17-Going_all_the_Way%2C_Sometimes.html">42 hunch net-2005-03-17-Going all the Way, Sometimes</a></p>
<p>18 0.50844896 <a title="472-lda-18" href="../hunch_net-2013/hunch_net-2013-12-01-NIPS_tutorials_and_Vowpal_Wabbit_7.4.html">492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</a></p>
<p>19 0.50837511 <a title="472-lda-19" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>20 0.50820929 <a title="472-lda-20" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
