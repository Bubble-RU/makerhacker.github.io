<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>471 hunch net-2012-08-24-Patterns for research in machine learning</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2012" href="../home/hunch_net-2012_home.html">hunch_net-2012</a> <a title="hunch_net-2012-471" href="#">hunch_net-2012-471</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>471 hunch net-2012-08-24-Patterns for research in machine learning</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2012-471-html" href="http://hunch.net/?p=2562">html</a></p><p>Introduction: There are  a handful of basic code patterns  that I wish I was more aware of when I started research in machine learning. Each on its own may seem pointless, but collectively they go a long way towards making the typical research workflow more efficient. Here they are:
  
 Separate code from data. 
 Separate input data, working data and output data. 
 Save everything to disk frequently. 
 Separate options from parameters. 
 Do not use global variables. 
 Record the options used to generate each run of the algorithm. 
 Make it easy to sweep options. 
 Make it easy to execute only portions of the code. 
 Use checkpointing. 
 Write demos and tests. 
  
Click  here  for discussion and examples for each item. Also see  Charles Sutton’s  and  HackerNews’  thoughts on the same topic. 
 
My guess is that these patterns will not only be useful for machine learning, but also any other computational work that involves either a) processing large amounts of data, or b) algorithms that take a signif</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 There are  a handful of basic code patterns  that I wish I was more aware of when I started research in machine learning. [sent-1, score-0.928]
</p><p>2 Each on its own may seem pointless, but collectively they go a long way towards making the typical research workflow more efficient. [sent-2, score-0.452]
</p><p>3 Separate input data, working data and output data. [sent-4, score-0.394]
</p><p>4 Record the options used to generate each run of the algorithm. [sent-8, score-0.511]
</p><p>5 Make it easy to execute only portions of the code. [sent-10, score-0.419]
</p><p>6 Click  here  for discussion and examples for each item. [sent-13, score-0.072]
</p><p>7 Also see  Charles Sutton’s  and  HackerNews’  thoughts on the same topic. [sent-14, score-0.119]
</p><p>8 My guess is that these patterns will not only be useful for machine learning, but also any other computational work that involves either a) processing large amounts of data, or b) algorithms that take a significant amount of time to execute. [sent-15, score-1.17]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('separate', 0.383), ('patterns', 0.36), ('options', 0.278), ('sutton', 0.18), ('code', 0.175), ('portions', 0.167), ('charles', 0.157), ('disk', 0.157), ('click', 0.157), ('generate', 0.157), ('demos', 0.15), ('trust', 0.144), ('execute', 0.144), ('processing', 0.139), ('guess', 0.139), ('save', 0.139), ('data', 0.138), ('wish', 0.128), ('amounts', 0.124), ('thoughts', 0.119), ('involves', 0.119), ('global', 0.114), ('record', 0.114), ('appreciate', 0.108), ('easy', 0.108), ('share', 0.107), ('write', 0.103), ('output', 0.098), ('aware', 0.098), ('everything', 0.096), ('input', 0.094), ('students', 0.093), ('typical', 0.088), ('started', 0.088), ('towards', 0.087), ('amount', 0.08), ('make', 0.08), ('research', 0.079), ('list', 0.078), ('use', 0.077), ('run', 0.076), ('either', 0.074), ('discussion', 0.072), ('ll', 0.071), ('computational', 0.069), ('go', 0.069), ('also', 0.066), ('making', 0.065), ('working', 0.064), ('long', 0.064)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="471-tfidf-1" href="../hunch_net-2012/hunch_net-2012-08-24-Patterns_for_research_in_machine_learning.html">471 hunch net-2012-08-24-Patterns for research in machine learning</a></p>
<p>Introduction: There are  a handful of basic code patterns  that I wish I was more aware of when I started research in machine learning. Each on its own may seem pointless, but collectively they go a long way towards making the typical research workflow more efficient. Here they are:
  
 Separate code from data. 
 Separate input data, working data and output data. 
 Save everything to disk frequently. 
 Separate options from parameters. 
 Do not use global variables. 
 Record the options used to generate each run of the algorithm. 
 Make it easy to sweep options. 
 Make it easy to execute only portions of the code. 
 Use checkpointing. 
 Write demos and tests. 
  
Click  here  for discussion and examples for each item. Also see  Charles Sutton’s  and  HackerNews’  thoughts on the same topic. 
 
My guess is that these patterns will not only be useful for machine learning, but also any other computational work that involves either a) processing large amounts of data, or b) algorithms that take a signif</p><p>2 0.12261014 <a title="471-tfidf-2" href="../hunch_net-2007/hunch_net-2007-09-16-Optimizing_Machine_Learning_Programs.html">262 hunch net-2007-09-16-Optimizing Machine Learning Programs</a></p>
<p>Introduction: Machine learning is often computationally bounded which implies that the ability to write fast code becomes important if you ever want to implement a machine learning algorithm.  Basic tactical optimizations are covered well  elsewhere , but I haven’t seen a reasonable guide to higher level optimizations, which are the most important in my experience.  Here are some of the higher level optimizations I’ve often found useful.
  
  Algorithmic Improvement First . This is Hard, but it is the most important consideration, and typically yields the most benefits.  Good optimizations here are publishable.  In the context of machine learning, you should be familiar with the arguments for online vs. batch learning. 
  Choice of Language . There are many arguments about the  choice of language .  Sometimes you don’t have a choice when interfacing with other people.  Personally, I favor C/C++ when I want to write fast code.  This (admittedly) makes me a slower programmer than when using higher lev</p><p>3 0.10534634 <a title="471-tfidf-3" href="../hunch_net-2009/hunch_net-2009-07-31-Vowpal_Wabbit_Open_Source_Project.html">365 hunch net-2009-07-31-Vowpal Wabbit Open Source Project</a></p>
<p>Introduction: Today brings a new release of the  Vowpal Wabbit  fast online learning software.  This time, unlike the previous release, the project itself is going open source, developing via  github .  For example, the lastest and greatest can be downloaded via:
  
git clone git://github.com/JohnLangford/vowpal_wabbit.git
  
If you aren’t familiar with  git , it’s a distributed version control system which supports quick and easy branching, as well as reconciliation.
 
This version of the code is confirmed to compile without complaint on at least some flavors of OSX as well as Linux boxes.
 
As much of the point of this project is pushing the limits of fast and effective machine learning, let me mention a few datapoints from my experience.
  
 The program can effectively scale up to batch-style training on sparse terafeature (i.e. 10 12  sparse feature) size datasets.  The limiting factor is typically i/o. 
 I started using the the real datasets from the  large-scale learning  workshop as a conve</p><p>4 0.091096401 <a title="471-tfidf-4" href="../hunch_net-2005/hunch_net-2005-01-26-Summer_Schools.html">4 hunch net-2005-01-26-Summer Schools</a></p>
<p>Introduction: There are several summer schools related to machine learning.
 
We are running a two week  machine learning summer school  in Chicago, USA May 16-27.  
 
IPAM is running a more focused three week summer school on  Intelligent Extraction of Information from Graphs and High Dimensional Data  in Los Angeles, USA July 11-29.
 
A broad one-week school on  analysis of patterns  will be held in Erice, Italy, Oct. 28-Nov 6.</p><p>5 0.088845603 <a title="471-tfidf-5" href="../hunch_net-2007/hunch_net-2007-10-15-NIPS_workshops_extended_to_3_days.html">266 hunch net-2007-10-15-NIPS workshops extended to 3 days</a></p>
<p>Introduction: (Unofficially, at least.)  The  Deep Learning Workshop  is being held the afternoon before the rest of the workshops in Vancouver, BC.  Separate registration is needed, and open.
 
Whatâ&euro;&trade;s happening fundamentally here is that there are too many interesting workshops to fit into 2 days.  Perhaps we can get it officially expanded to 3 days next year.</p><p>6 0.081344366 <a title="471-tfidf-6" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>7 0.079014637 <a title="471-tfidf-7" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>8 0.076444209 <a title="471-tfidf-8" href="../hunch_net-2007/hunch_net-2007-12-21-Vowpal_Wabbit_Code_Release.html">281 hunch net-2007-12-21-Vowpal Wabbit Code Release</a></p>
<p>9 0.070460476 <a title="471-tfidf-9" href="../hunch_net-2006/hunch_net-2006-05-05-An_ICML_reject.html">177 hunch net-2006-05-05-An ICML reject</a></p>
<p>10 0.067175314 <a title="471-tfidf-10" href="../hunch_net-2009/hunch_net-2009-03-18-Parallel_ML_primitives.html">346 hunch net-2009-03-18-Parallel ML primitives</a></p>
<p>11 0.066072136 <a title="471-tfidf-11" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>12 0.065850526 <a title="471-tfidf-12" href="../hunch_net-2005/hunch_net-2005-03-21-Research_Styles_in_Machine_Learning.html">44 hunch net-2005-03-21-Research Styles in Machine Learning</a></p>
<p>13 0.065390401 <a title="471-tfidf-13" href="../hunch_net-2007/hunch_net-2007-08-25-The_Privacy_Problem.html">260 hunch net-2007-08-25-The Privacy Problem</a></p>
<p>14 0.064020209 <a title="471-tfidf-14" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>15 0.062977806 <a title="471-tfidf-15" href="../hunch_net-2011/hunch_net-2011-08-15-Vowpal_Wabbit_6.0.html">441 hunch net-2011-08-15-Vowpal Wabbit 6.0</a></p>
<p>16 0.062741064 <a title="471-tfidf-16" href="../hunch_net-2006/hunch_net-2006-09-18-What_is_missing_for_online_collaborative_research%3F.html">208 hunch net-2006-09-18-What is missing for online collaborative research?</a></p>
<p>17 0.061624505 <a title="471-tfidf-17" href="../hunch_net-2005/hunch_net-2005-03-05-Funding_Research.html">36 hunch net-2005-03-05-Funding Research</a></p>
<p>18 0.061565835 <a title="471-tfidf-18" href="../hunch_net-2005/hunch_net-2005-09-10-%26%238220%3BFailure%26%238221%3B_is_an_option.html">110 hunch net-2005-09-10-&#8220;Failure&#8221; is an option</a></p>
<p>19 0.061565585 <a title="471-tfidf-19" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>20 0.060540061 <a title="471-tfidf-20" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.153), (1, 0.004), (2, -0.076), (3, 0.053), (4, -0.013), (5, -0.003), (6, -0.032), (7, -0.003), (8, -0.015), (9, 0.034), (10, -0.065), (11, -0.052), (12, 0.033), (13, -0.027), (14, -0.003), (15, -0.014), (16, 0.016), (17, 0.027), (18, -0.031), (19, -0.015), (20, 0.087), (21, -0.021), (22, 0.016), (23, -0.028), (24, -0.028), (25, -0.05), (26, -0.027), (27, -0.005), (28, -0.004), (29, 0.004), (30, -0.034), (31, 0.021), (32, 0.016), (33, 0.072), (34, 0.005), (35, -0.002), (36, 0.022), (37, 0.007), (38, 0.027), (39, -0.076), (40, -0.007), (41, 0.015), (42, -0.019), (43, 0.036), (44, 0.024), (45, 0.054), (46, 0.022), (47, 0.027), (48, -0.037), (49, -0.095)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95182759 <a title="471-lsi-1" href="../hunch_net-2012/hunch_net-2012-08-24-Patterns_for_research_in_machine_learning.html">471 hunch net-2012-08-24-Patterns for research in machine learning</a></p>
<p>Introduction: There are  a handful of basic code patterns  that I wish I was more aware of when I started research in machine learning. Each on its own may seem pointless, but collectively they go a long way towards making the typical research workflow more efficient. Here they are:
  
 Separate code from data. 
 Separate input data, working data and output data. 
 Save everything to disk frequently. 
 Separate options from parameters. 
 Do not use global variables. 
 Record the options used to generate each run of the algorithm. 
 Make it easy to sweep options. 
 Make it easy to execute only portions of the code. 
 Use checkpointing. 
 Write demos and tests. 
  
Click  here  for discussion and examples for each item. Also see  Charles Sutton’s  and  HackerNews’  thoughts on the same topic. 
 
My guess is that these patterns will not only be useful for machine learning, but also any other computational work that involves either a) processing large amounts of data, or b) algorithms that take a signif</p><p>2 0.73967075 <a title="471-lsi-2" href="../hunch_net-2007/hunch_net-2007-09-16-Optimizing_Machine_Learning_Programs.html">262 hunch net-2007-09-16-Optimizing Machine Learning Programs</a></p>
<p>Introduction: Machine learning is often computationally bounded which implies that the ability to write fast code becomes important if you ever want to implement a machine learning algorithm.  Basic tactical optimizations are covered well  elsewhere , but I haven’t seen a reasonable guide to higher level optimizations, which are the most important in my experience.  Here are some of the higher level optimizations I’ve often found useful.
  
  Algorithmic Improvement First . This is Hard, but it is the most important consideration, and typically yields the most benefits.  Good optimizations here are publishable.  In the context of machine learning, you should be familiar with the arguments for online vs. batch learning. 
  Choice of Language . There are many arguments about the  choice of language .  Sometimes you don’t have a choice when interfacing with other people.  Personally, I favor C/C++ when I want to write fast code.  This (admittedly) makes me a slower programmer than when using higher lev</p><p>3 0.67168576 <a title="471-lsi-3" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>Introduction: Suppose you have a dataset with 2 terafeatures (we only count nonzero entries in a datamatrix), and want to learn a good linear predictor in a reasonable amount of time.  How do you do it?  As a learning theorist, the first thing you do is pray that this is too much data for the number of parameters—but that’s not the case, there are around 16 billion examples, 16 million parameters, and people really care about a high quality predictor, so subsampling is not a good strategy.
 
 Alekh  visited us last summer, and we had a breakthrough (see  here  for details), coming up with the first learning algorithm I’ve seen that is provably faster than  any future  single machine learning algorithm.  The proof of this is simple: We can output a optimal-up-to-precision linear predictor faster than the data can be streamed through the network interface of any single machine involved in the computation.
 
It is necessary but not sufficient to have an effective communication infrastructure.  It is ne</p><p>4 0.62812144 <a title="471-lsi-4" href="../hunch_net-2009/hunch_net-2009-07-31-Vowpal_Wabbit_Open_Source_Project.html">365 hunch net-2009-07-31-Vowpal Wabbit Open Source Project</a></p>
<p>Introduction: Today brings a new release of the  Vowpal Wabbit  fast online learning software.  This time, unlike the previous release, the project itself is going open source, developing via  github .  For example, the lastest and greatest can be downloaded via:
  
git clone git://github.com/JohnLangford/vowpal_wabbit.git
  
If you aren’t familiar with  git , it’s a distributed version control system which supports quick and easy branching, as well as reconciliation.
 
This version of the code is confirmed to compile without complaint on at least some flavors of OSX as well as Linux boxes.
 
As much of the point of this project is pushing the limits of fast and effective machine learning, let me mention a few datapoints from my experience.
  
 The program can effectively scale up to batch-style training on sparse terafeature (i.e. 10 12  sparse feature) size datasets.  The limiting factor is typically i/o. 
 I started using the the real datasets from the  large-scale learning  workshop as a conve</p><p>5 0.59233797 <a title="471-lsi-5" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>Introduction: Muthu  invited me to the workshop on  algorithms in the field , with the goal of providing a sense of where near-term research should go.  When the time came though, I bargained for a post instead, which provides a chance for many other people to comment.
 
There are several things I didn’t fully understand when I went to Yahoo! about 5 years ago.  I’d like to repeat them as people in academia may not yet understand them intuitively.
  
 Almost all the big impact algorithms operate in pseudo-linear or better time.  Think about caching, hashing, sorting, filtering, etc… and you have a sense of what some of the most heavily used algorithms are.  This matters quite a bit to Machine Learning research, because people often work with superlinear time algorithms and languages.  Two very common examples of this are graphical models, where inference is often a superlinear operation—think about the  n 2   dependence on the number of states in a  Hidden Markov Model  and Kernelized  Support Vecto</p><p>6 0.58824748 <a title="471-lsi-6" href="../hunch_net-2011/hunch_net-2011-08-15-Vowpal_Wabbit_6.0.html">441 hunch net-2011-08-15-Vowpal Wabbit 6.0</a></p>
<p>7 0.576352 <a title="471-lsi-7" href="../hunch_net-2006/hunch_net-2006-02-02-Introspectionism_as_a_Disease.html">153 hunch net-2006-02-02-Introspectionism as a Disease</a></p>
<p>8 0.57543224 <a title="471-lsi-8" href="../hunch_net-2009/hunch_net-2009-01-21-Nearly_all_natural_problems_require_nonlinearity.html">337 hunch net-2009-01-21-Nearly all natural problems require nonlinearity</a></p>
<p>9 0.56256223 <a title="471-lsi-9" href="../hunch_net-2011/hunch_net-2011-08-20-The_Large_Scale_Learning_Survey_Tutorial.html">442 hunch net-2011-08-20-The Large Scale Learning Survey Tutorial</a></p>
<p>10 0.5614661 <a title="471-lsi-10" href="../hunch_net-2005/hunch_net-2005-02-04-JMLG.html">13 hunch net-2005-02-04-JMLG</a></p>
<p>11 0.55666292 <a title="471-lsi-11" href="../hunch_net-2009/hunch_net-2009-03-18-Parallel_ML_primitives.html">346 hunch net-2009-03-18-Parallel ML primitives</a></p>
<p>12 0.55566144 <a title="471-lsi-12" href="../hunch_net-2011/hunch_net-2011-11-26-Giving_Thanks.html">449 hunch net-2011-11-26-Giving Thanks</a></p>
<p>13 0.548603 <a title="471-lsi-13" href="../hunch_net-2007/hunch_net-2007-12-21-Vowpal_Wabbit_Code_Release.html">281 hunch net-2007-12-21-Vowpal Wabbit Code Release</a></p>
<p>14 0.54442286 <a title="471-lsi-14" href="../hunch_net-2006/hunch_net-2006-01-13-Benchmarks_for_RL.html">148 hunch net-2006-01-13-Benchmarks for RL</a></p>
<p>15 0.5428074 <a title="471-lsi-15" href="../hunch_net-2005/hunch_net-2005-12-07-Is_the_Google_way_the_way_for_machine_learning%3F.html">136 hunch net-2005-12-07-Is the Google way the way for machine learning?</a></p>
<p>16 0.53875583 <a title="471-lsi-16" href="../hunch_net-2006/hunch_net-2006-01-06-MLTV.html">146 hunch net-2006-01-06-MLTV</a></p>
<p>17 0.53780454 <a title="471-lsi-17" href="../hunch_net-2008/hunch_net-2008-04-30-Concerns_about_the_Large_Scale_Learning_Challenge.html">300 hunch net-2008-04-30-Concerns about the Large Scale Learning Challenge</a></p>
<p>18 0.53531194 <a title="471-lsi-18" href="../hunch_net-2012/hunch_net-2012-10-26-ML_Symposium_and_Strata-Hadoop_World.html">475 hunch net-2012-10-26-ML Symposium and Strata-Hadoop World</a></p>
<p>19 0.53409714 <a title="471-lsi-19" href="../hunch_net-2005/hunch_net-2005-04-25-Embeddings%3A_what_are_they_good_for%3F.html">61 hunch net-2005-04-25-Embeddings: what are they good for?</a></p>
<p>20 0.53289741 <a title="471-lsi-20" href="../hunch_net-2006/hunch_net-2006-08-10-Precision_is_not_accuracy.html">202 hunch net-2006-08-10-Precision is not accuracy</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(14, 0.402), (27, 0.167), (53, 0.042), (55, 0.08), (94, 0.146), (95, 0.044)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.88768971 <a title="471-lda-1" href="../hunch_net-2012/hunch_net-2012-08-24-Patterns_for_research_in_machine_learning.html">471 hunch net-2012-08-24-Patterns for research in machine learning</a></p>
<p>Introduction: There are  a handful of basic code patterns  that I wish I was more aware of when I started research in machine learning. Each on its own may seem pointless, but collectively they go a long way towards making the typical research workflow more efficient. Here they are:
  
 Separate code from data. 
 Separate input data, working data and output data. 
 Save everything to disk frequently. 
 Separate options from parameters. 
 Do not use global variables. 
 Record the options used to generate each run of the algorithm. 
 Make it easy to sweep options. 
 Make it easy to execute only portions of the code. 
 Use checkpointing. 
 Write demos and tests. 
  
Click  here  for discussion and examples for each item. Also see  Charles Sutton’s  and  HackerNews’  thoughts on the same topic. 
 
My guess is that these patterns will not only be useful for machine learning, but also any other computational work that involves either a) processing large amounts of data, or b) algorithms that take a signif</p><p>2 0.87308204 <a title="471-lda-2" href="../hunch_net-2005/hunch_net-2005-07-13-Text_Entailment_at_AAAI.html">94 hunch net-2005-07-13-Text Entailment at AAAI</a></p>
<p>Introduction: Rajat Raina  presented a paper on the technique they used for the  PASCAL   Recognizing Textual Entailment  challenge.  
 
“Text entailment” is the problem of deciding if one sentence implies another.  For example the previous sentence entails: 
  
 Text entailment is a decision problem. 
 One sentence can imply another. 
  
The challenge was of the form: given an original sentence and another sentence predict whether there was an entailment.  All current techniques for predicting correctness of an entailment are at the “flail” stage—accuracies of around 58% where humans could achieve near 100% accuracy, so there is much room to improve.   Apparently, there may be another PASCAL challenge on this problem in the near future.</p><p>3 0.74364978 <a title="471-lda-3" href="../hunch_net-2009/hunch_net-2009-11-29-AI_Safety.html">380 hunch net-2009-11-29-AI Safety</a></p>
<p>Introduction: Dan Reeves  introduced me to  Michael Vassar  who ran the  Singularity Summit  and educated me a bit on the subject of AI safety which the  Singularity Institute  has  small grants for .  
 
I still believe that  interstellar space travel is necessary for long term civilization survival, and the AI is necessary for interstellar space travel .  On these grounds alone, we could judge that developing AI is much more safe than not.  Nevertheless, there is a basic reasonable fear, as expressed by some commenters, that AI could go bad.
 
A basic scenario starts with someone inventing an AI and telling it to make as much money as possible.  The AI promptly starts trading in various markets to make money.  To improve, it crafts a virus that takes over most of the world’s computers using it as a surveillance network so that it can always make the right decision.  The AI also branches out into any form of distance work, taking over the entire outsourcing process for all jobs that are entirely di</p><p>4 0.71416909 <a title="471-lda-4" href="../hunch_net-2011/hunch_net-2011-04-11-The_Heritage_Health_Prize.html">430 hunch net-2011-04-11-The Heritage Health Prize</a></p>
<p>Introduction: The  Heritage Health Prize  is potentially the largest prediction prize yet at $3M, which is sure to get many people interested.  Several elements of the competition may be worth discussing.
  
 The most straightforward way for HPN to deploy this predictor is in determining who to cover with insurance.  This might easily cover the costs of running the contest itself, but the value to the health system of a whole is minimal, as people not covered still exist.  While HPN itself is a provider network, they have active relationships with a number of insurance companies, and the right to resell any entrant.  It’s worth keeping in mind that the research and development may nevertheless end up being useful in the longer term, especially as entrants also keep the right to their code. 
 The  judging metric  is something I haven’t seen previously.  If a patient has probability 0.5 of being in the hospital 0 days and probability 0.5 of being in the hospital ~53.6 days, the optimal prediction in e</p><p>5 0.71384764 <a title="471-lda-5" href="../hunch_net-2010/hunch_net-2010-08-23-Boosted_Decision_Trees_for_Deep_Learning.html">407 hunch net-2010-08-23-Boosted Decision Trees for Deep Learning</a></p>
<p>Introduction: About 4 years ago, I speculated that  decision trees qualify as a deep learning algorithm  because they can make decisions which are substantially nonlinear in the input representation.   Ping Li  has  proved this correct, empirically  at  UAI  by showing that boosted decision trees can beat deep belief networks on versions of  Mnist  which are artificially hardened so as to make them solvable only by deep learning algorithms.  
 
This is an important point, because the ability to solve these sorts of problems is probably the best objective definition of a deep learning algorithm we have.   Iâ&euro;&trade;m not that surprised.  In my experience, if you can accept the computational drawbacks of a boosted decision tree, they can achieve pretty good performance.
 
 Geoff Hinton  once told me that the great thing about deep belief networks is that they work.  I understand that Ping had very substantial difficulty in getting this published, so I hope some reviewers step up to the standard of valuing wha</p><p>6 0.50950783 <a title="471-lda-6" href="../hunch_net-2006/hunch_net-2006-12-04-Structural_Problems_in_NIPS_Decision_Making.html">221 hunch net-2006-12-04-Structural Problems in NIPS Decision Making</a></p>
<p>7 0.50149882 <a title="471-lda-7" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>8 0.50033146 <a title="471-lda-8" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>9 0.49845222 <a title="471-lda-9" href="../hunch_net-2007/hunch_net-2007-01-26-Parallel_Machine_Learning_Problems.html">229 hunch net-2007-01-26-Parallel Machine Learning Problems</a></p>
<p>10 0.49649751 <a title="471-lda-10" href="../hunch_net-2005/hunch_net-2005-12-07-Is_the_Google_way_the_way_for_machine_learning%3F.html">136 hunch net-2005-12-07-Is the Google way the way for machine learning?</a></p>
<p>11 0.4920243 <a title="471-lda-11" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>12 0.48537907 <a title="471-lda-12" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>13 0.48426634 <a title="471-lda-13" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>14 0.48415914 <a title="471-lda-14" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>15 0.48365778 <a title="471-lda-15" href="../hunch_net-2005/hunch_net-2005-07-21-Six_Months.html">96 hunch net-2005-07-21-Six Months</a></p>
<p>16 0.48310927 <a title="471-lda-16" href="../hunch_net-2009/hunch_net-2009-09-21-Netflix_finishes_%28and_starts%29.html">371 hunch net-2009-09-21-Netflix finishes (and starts)</a></p>
<p>17 0.48116419 <a title="471-lda-17" href="../hunch_net-2007/hunch_net-2007-12-21-Vowpal_Wabbit_Code_Release.html">281 hunch net-2007-12-21-Vowpal Wabbit Code Release</a></p>
<p>18 0.48099259 <a title="471-lda-18" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>19 0.48082677 <a title="471-lda-19" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<p>20 0.48030812 <a title="471-lda-20" href="../hunch_net-2009/hunch_net-2009-06-03-Functionally_defined_Nonlinear_Dynamic_Models.html">359 hunch net-2009-06-03-Functionally defined Nonlinear Dynamic Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
