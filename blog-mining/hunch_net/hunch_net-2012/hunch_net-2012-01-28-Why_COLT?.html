<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>453 hunch net-2012-01-28-Why COLT?</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2012" href="../home/hunch_net-2012_home.html">hunch_net-2012</a> <a title="hunch_net-2012-453" href="#">hunch_net-2012-453</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>453 hunch net-2012-01-28-Why COLT?</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2012-453-html" href="http://hunch.net/?p=2241">html</a></p><p>Introduction: ByShieandNatiFollowing John's advertisement for submitting to ICML, we thought
it appropriate to highlight the advantages of COLT, and the reasons it is
often the best place for theory papers. We would like to emphasize that we
both respect ICML, and are active in ICML, both as authors and as area chairs,
and certainly are not arguing that ICML is a bad place for your papers. For
many papers, ICML is the best venue. But for many theory papers, COLT is a
better and more appropriate place.Why should you submit to COLT?By-and-large,
theory papers go to COLT. This is the tradition of the field and most theory
papers are sent to COLT. This is the place to present your ground-breaking
theorems and new models that will shape the theory of machine learning. COLT
is more focused then ICML with a single track session. Unlike ICML, the norm
in COLT is for people to sit through most sessions, and hear most of the talks
presented. There is also often a lively discussion following paper
presentation</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ByShieandNatiFollowing John's advertisement for submitting to ICML, we thought it appropriate to highlight the advantages of COLT, and the reasons it is often the best place for theory papers. [sent-1, score-0.445]
</p><p>2 We would like to emphasize that we both respect ICML, and are active in ICML, both as authors and as area chairs, and certainly are not arguing that ICML is a bad place for your papers. [sent-2, score-0.366]
</p><p>3 This is the tradition of the field and most theory papers are sent to COLT. [sent-7, score-0.361]
</p><p>4 This is the place to present your ground-breaking theorems and new models that will shape the theory of machine learning. [sent-8, score-0.255]
</p><p>5 Additionally, this year COLT and ICML are tightly co-located, with joint plenary sessions (i. [sent-13, score-0.212]
</p><p>6 some COLT papers will be presented in a plenary session to the entire combined COLT/ICML audience, as will some ICML papers), and many other opportunities for exposure to the wider ICML audience. [sent-15, score-0.422]
</p><p>7 And so, by submitting to COLT, you have the potential of reaching both the captive theory audience at COLT and the wider ML audience at ICML. [sent-16, score-0.517]
</p><p>8 The COLT program committee is comprised entirely of established, mostly fairly senior, researchers. [sent-18, score-0.608]
</p><p>9 Program committee members read and review papers themselves, or potentially use a sub-reviewer that they know personally and carefully select for the paper, but still check and maintain responsibility for the review. [sent-19, score-0.974]
</p><p>10 Your paper will get reviewed by at least three program committee members, who will likely be experts on the topics covered by the paper. [sent-20, score-0.695]
</p><p>11 The reviewing process is less rushed and program committee members (and sub-reviewers were appropriate) are expected to do a careful job on each and every paper. [sent-23, score-1.038]
</p><p>12 All papers are then discussed by the program committee, and there is generally significant and meaningful discussions on papers. [sent-24, score-0.395]
</p><p>13 This also means the COLT reviewing process is far from having a "single point of failure", as the paper will be carefully considered and argued for by multiple (senior) program committee members. [sent-25, score-0.91]
</p><p>14 Program committee members have access to the author identities (as do area chairs in ICML), as this is essential in order to select sub-reviewers. [sent-28, score-0.989]
</p><p>15 However, the author names do not appear on the papers, both in order to reduce the effect of first impressions, and to allow program committee members to utilize reviewers who are truly blind to the author's identities. [sent-29, score-1.148]
</p><p>16 It should be noted that the COLT anonimization guidelines are a bit more relaxed, which we hope makes it easier to create an anonimized version for conference submission (authors are still allowed to, and even encouraged, to post their papers online, with their names on them of course). [sent-30, score-0.236]
</p><p>17 Frankly, with the higher quality, less random, reviews, we feel it is not needed, and the hassle to authors and program committee members is not worth it. [sent-32, score-1.097]
</p><p>18 However, the tradition in COLT, which we plan to follow, is to contact authors as needed during the review and discussion process to ask for clarification on issues that came up during review. [sent-33, score-0.502]
</p><p>19 In particular, if a concern is raised on the soundness or other technical aspect of a paper, the authors will be contacted to give them a chance to set things straight. [sent-34, score-0.231]
</p><p>20 But no, there is no generic author response where authors can argue and plead for acceptance. [sent-35, score-0.265]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('colt', 0.465), ('committee', 0.364), ('icml', 0.273), ('program', 0.244), ('members', 0.214), ('authors', 0.164), ('papers', 0.151), ('plenary', 0.124), ('theory', 0.118), ('audience', 0.112), ('author', 0.101), ('chairs', 0.098), ('review', 0.098), ('senior', 0.096), ('appropriate', 0.092), ('select', 0.092), ('tradition', 0.092), ('wider', 0.092), ('sessions', 0.088), ('paper', 0.087), ('names', 0.085), ('submitting', 0.083), ('place', 0.082), ('process', 0.081), ('reviewing', 0.079), ('reviewers', 0.077), ('single', 0.075), ('submit', 0.071), ('advantages', 0.07), ('area', 0.069), ('technical', 0.067), ('needed', 0.067), ('blind', 0.063), ('reviews', 0.062), ('less', 0.056), ('carefully', 0.055), ('hassle', 0.055), ('shape', 0.055), ('randomness', 0.055), ('dedicated', 0.055), ('relaxed', 0.055), ('rigorous', 0.055), ('exposure', 0.055), ('rebuttal', 0.055), ('seniority', 0.055), ('arguing', 0.051), ('considerations', 0.051), ('frequently', 0.051), ('impressions', 0.051), ('identities', 0.051)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999964 <a title="453-tfidf-1" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<p>Introduction: ByShieandNatiFollowing John's advertisement for submitting to ICML, we thought
it appropriate to highlight the advantages of COLT, and the reasons it is
often the best place for theory papers. We would like to emphasize that we
both respect ICML, and are active in ICML, both as authors and as area chairs,
and certainly are not arguing that ICML is a bad place for your papers. For
many papers, ICML is the best venue. But for many theory papers, COLT is a
better and more appropriate place.Why should you submit to COLT?By-and-large,
theory papers go to COLT. This is the tradition of the field and most theory
papers are sent to COLT. This is the place to present your ground-breaking
theorems and new models that will shape the theory of machine learning. COLT
is more focused then ICML with a single track session. Unlike ICML, the norm
in COLT is for people to sit through most sessions, and hear most of the talks
presented. There is also often a lively discussion following paper
presentation</p><p>2 0.34010443 <a title="453-tfidf-2" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>Introduction: Here's a quick reference for summer ML-related conferences sorted by due
date:ConferenceDue dateLocationReviewingKDDFeb 10August 12-16, Beijing,
ChinaSingle BlindCOLTFeb 14June 25-June 27, Edinburgh, ScotlandSingle Blind?
(historically)ICMLFeb 24June 26-July 1, Edinburgh, ScotlandDouble Blind,
author response, zeroSPOFUAIMarch 30August 15-17, Catalina Islands,
CaliforniaDouble Blind, author responseGeographically, this is greatly
dispersed and the UAI/KDD conflict is unfortunate.Machine Learning conferences
are triannual now, betweenNIPS,AIStat, andICML. This has not always been the
case: the academic default is annual summer conferences, then NIPS started
with a December conference, and now AIStat has grown into an April
conference.However, the first claim is not quite correct. NIPS and AIStat have
few competing venues while ICML implicitly competes with many other
conferences accepting machine learning related papers. SinceJoelleand I are
taking a turn as program chairs this year, I</p><p>3 0.31573686 <a title="453-tfidf-3" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>Introduction: Unfortunately, I ended up sick for much of this ICML. I did manage to catch
one interesting paper:Richard Socher,Cliff Lin,Andrew Y. Ng, andChristopher D.
ManningParsing Natural Scenes and Natural Language with Recursive Neural
Networks.I invited Richard to share his list of interesting papers, so
hopefully we'll hear from him soon. In the meantime,PaulandHalhave posted some
lists.the futureJoelleand I are program chairs for ICML 2012 inEdinburgh,
which I previously enjoyed visiting in2005. This is a huge responsibility,
that we hope to accomplish well. A part of this (perhaps the most fun part),
is imagining how we can make ICML better. A key and critical constraint is
choosing things that can be accomplished. So far we have:Colocation. The first
thing we looked into was potential colocations. We quickly discovered that
many other conferences precomitted their location. For the future, getting a
colocation withACLorSIGIR, seems to require more advanced planning. If that
can be done, I</p><p>4 0.30070972 <a title="453-tfidf-4" href="../hunch_net-2005/hunch_net-2005-07-04-The_Health_of_COLT.html">89 hunch net-2005-07-04-The Health of COLT</a></p>
<p>Introduction: The health ofCOLT(Conference on Learning Theory or Computational Learning
Theory depending on who you ask) has been questioned over the last few years.
Low points for the conference occurred whenEuroCOLTmerged with COLT in 2001,
and the attendance at the 2002 Sydney COLT fell to a new low. This occurred in
the general context of machine learning conferences rising in both number and
size over the last decade.Any discussion ofwhyCOLT has had difficulties is
inherently controversial as is any story about well-intentioned people making
the wrong decisions. Nevertheless, this may be worth discussing in the hope of
avoiding problems in the future and general understanding. In any such
discussion there is a strong tendency to identify with a conference/community
in a patriotic manner that is detrimental to thinking. Keep in mind that
conferences exist to further research.My understanding (I wasn't around) is
that COLT started as a subcommunity of the computer science theory community.
This i</p><p>5 0.25882283 <a title="453-tfidf-5" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>Introduction: When thinking about how best to review papers, it seems helpful to have some
conception of what good reviewing is. As far as I can tell, this is almost
always only discussed in the specific context of a paper (i.e. your rejected
paper), or at most an area (i.e. what a "good paper" looks like for that area)
rather than general principles. Neither individual papers or areas are
sufficiently general for a large conference--every paper differs in the
details, and what if you want to build a new area and/or cross areas?An
unavoidable reason for reviewing is that the community of research is too
large. In particular, it is not possible for a researcher to read every paper
which someone thinks might be of interest. This reason for reviewing exists
independent of constraints on rooms or scheduling formats of individual
conferences. Indeed, history suggests that physical constraints are relatively
meaningless over the long term -- growing conferences simply use more rooms
and/or change formats</p><p>6 0.24735326 <a title="453-tfidf-6" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>7 0.24532013 <a title="453-tfidf-7" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>8 0.23635659 <a title="453-tfidf-8" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>9 0.21874897 <a title="453-tfidf-9" href="../hunch_net-2005/hunch_net-2005-05-02-Reviewing_techniques_for_conferences.html">65 hunch net-2005-05-02-Reviewing techniques for conferences</a></p>
<p>10 0.2063729 <a title="453-tfidf-10" href="../hunch_net-2012/hunch_net-2012-04-09-ICML_author_feedback_is_open.html">461 hunch net-2012-04-09-ICML author feedback is open</a></p>
<p>11 0.20610575 <a title="453-tfidf-11" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>12 0.20075098 <a title="453-tfidf-12" href="../hunch_net-2008/hunch_net-2008-12-12-Summer_Conferences.html">331 hunch net-2008-12-12-Summer Conferences</a></p>
<p>13 0.19205599 <a title="453-tfidf-13" href="../hunch_net-2012/hunch_net-2012-05-02-ICML%3A_Behind_the_Scenes.html">463 hunch net-2012-05-02-ICML: Behind the Scenes</a></p>
<p>14 0.19041409 <a title="453-tfidf-14" href="../hunch_net-2008/hunch_net-2008-11-09-A_Healthy__COLT.html">324 hunch net-2008-11-09-A Healthy  COLT</a></p>
<p>15 0.18300763 <a title="453-tfidf-15" href="../hunch_net-2006/hunch_net-2006-12-04-Structural_Problems_in_NIPS_Decision_Making.html">221 hunch net-2006-12-04-Structural Problems in NIPS Decision Making</a></p>
<p>16 0.18297924 <a title="453-tfidf-16" href="../hunch_net-2005/hunch_net-2005-06-28-The_cross_validation_problem%3A_cash_reward.html">86 hunch net-2005-06-28-The cross validation problem: cash reward</a></p>
<p>17 0.1806294 <a title="453-tfidf-17" href="../hunch_net-2005/hunch_net-2005-03-13-Avoiding_Bad_Reviewing.html">40 hunch net-2005-03-13-Avoiding Bad Reviewing</a></p>
<p>18 0.18004273 <a title="453-tfidf-18" href="../hunch_net-2012/hunch_net-2012-06-29-ICML_survey_and_comments.html">468 hunch net-2012-06-29-ICML survey and comments</a></p>
<p>19 0.17716306 <a title="453-tfidf-19" href="../hunch_net-2011/hunch_net-2011-10-10-ML_Symposium_and_ICML_details.html">447 hunch net-2011-10-10-ML Symposium and ICML details</a></p>
<p>20 0.17676947 <a title="453-tfidf-20" href="../hunch_net-2008/hunch_net-2008-10-14-Who_is_Responsible_for_a_Bad_Review%3F.html">320 hunch net-2008-10-14-Who is Responsible for a Bad Review?</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.259), (1, 0.35), (2, -0.237), (3, 0.068), (4, -0.02), (5, 0.033), (6, 0.008), (7, 0.012), (8, 0.077), (9, -0.063), (10, -0.082), (11, -0.103), (12, 0.119), (13, 0.023), (14, -0.01), (15, 0.135), (16, 0.111), (17, 0.114), (18, 0.093), (19, 0.165), (20, -0.105), (21, -0.09), (22, 0.079), (23, -0.011), (24, 0.119), (25, -0.017), (26, -0.048), (27, -0.103), (28, -0.022), (29, 0.022), (30, -0.088), (31, -0.019), (32, 0.086), (33, -0.089), (34, -0.001), (35, 0.067), (36, 0.011), (37, -0.004), (38, 0.022), (39, 0.033), (40, -0.004), (41, 0.055), (42, -0.006), (43, -0.015), (44, 0.017), (45, 0.019), (46, 0.107), (47, 0.003), (48, -0.098), (49, 0.022)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99042243 <a title="453-lsi-1" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<p>Introduction: ByShieandNatiFollowing John's advertisement for submitting to ICML, we thought
it appropriate to highlight the advantages of COLT, and the reasons it is
often the best place for theory papers. We would like to emphasize that we
both respect ICML, and are active in ICML, both as authors and as area chairs,
and certainly are not arguing that ICML is a bad place for your papers. For
many papers, ICML is the best venue. But for many theory papers, COLT is a
better and more appropriate place.Why should you submit to COLT?By-and-large,
theory papers go to COLT. This is the tradition of the field and most theory
papers are sent to COLT. This is the place to present your ground-breaking
theorems and new models that will shape the theory of machine learning. COLT
is more focused then ICML with a single track session. Unlike ICML, the norm
in COLT is for people to sit through most sessions, and hear most of the talks
presented. There is also often a lively discussion following paper
presentation</p><p>2 0.76739442 <a title="453-lsi-2" href="../hunch_net-2008/hunch_net-2008-11-09-A_Healthy__COLT.html">324 hunch net-2008-11-09-A Healthy  COLT</a></p>
<p>Introduction: Awhile ago, we discussed the health ofCOLT.COLT 2008substantially addressed my
concerns. The papers were diverse and several were interesting. Attendance was
up, which is particularly notable in Europe. In my opinion, the colocation
with UAI and ICML was the best colocation since 1998.And, perhaps best of all,
registration ended up being free for all students due to various grants from
theAcademy of Finland,Google,IBM, andYahoo.A basic question is: what went
right? There seem to be several answers.Cost-wise, COLT had sufficient grants
to alleviate the high cost of the Euro and location at a university
substantially reduces the cost compared to a hotel.Organization-wise, the
Finns were great with hordes of volunteers helping set everything up. Having
too many volunteers is a good failure mode.Organization-wise, it was clear
that all 3 program chairs were cooperating in designing the program
.Facilities-wise, proximity in time and space made the colocation much more
real than many others</p><p>3 0.74361002 <a title="453-lsi-3" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>Introduction: Unfortunately, I ended up sick for much of this ICML. I did manage to catch
one interesting paper:Richard Socher,Cliff Lin,Andrew Y. Ng, andChristopher D.
ManningParsing Natural Scenes and Natural Language with Recursive Neural
Networks.I invited Richard to share his list of interesting papers, so
hopefully we'll hear from him soon. In the meantime,PaulandHalhave posted some
lists.the futureJoelleand I are program chairs for ICML 2012 inEdinburgh,
which I previously enjoyed visiting in2005. This is a huge responsibility,
that we hope to accomplish well. A part of this (perhaps the most fun part),
is imagining how we can make ICML better. A key and critical constraint is
choosing things that can be accomplished. So far we have:Colocation. The first
thing we looked into was potential colocations. We quickly discovered that
many other conferences precomitted their location. For the future, getting a
colocation withACLorSIGIR, seems to require more advanced planning. If that
can be done, I</p><p>4 0.67508733 <a title="453-lsi-4" href="../hunch_net-2012/hunch_net-2012-06-29-ICML_survey_and_comments.html">468 hunch net-2012-06-29-ICML survey and comments</a></p>
<p>Introduction: Just about nothing could keep me from attendingICML, except forDorawho arrived
on Monday. Consequently, I have only secondhand reports that the conference is
going well.For those who are remote (like me) or after the conference (like
everyone),Mark Reidhas setup theICML discussionsite where you can comment on
any paper or subscribe to papers. Authors are automatically subscribed to
their own papers, so it should be possible to have a discussion significantly
after the fact, as people desire.We also conducted a survey before the
conference and have thesurvey resultsnow. This can be compared with theICML
2010 survey results. Looking at the comparable questions, we can sometimes
order the answers to have scores ranging from 0 to 3 or 0 to 4 with 3 or 4
being best and 0 worst, then compute the average difference between 2012 and
2010.Glancing through them, I see:Most people found the papers they reviewed a
good fit for their expertise (-.037 w.r.t 2010). Achieving this was one of our
subgo</p><p>5 0.67468637 <a title="453-lsi-5" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>Introduction: Claireasked me to be on the SODA program committee this year, which was quite
a bit of work.I had a relatively light load--merely 49 theory papers. Many of
these papers were not on subjects that I was expert about, so (as is common
for theory conferences) I found various reviewers that I trusted to help
review the papers. I ended up reviewing about 1/3 personally. There were a
couple instances where I ended up overruling a subreviewer whose logic seemed
off, but otherwise I generally let their reviews stand.There are some
differences in standards for paper reviews between the machine learning and
theory communities. In machine learning it is expected that a review be
detailed, while in the theory community this is often not the case. Every
paper given to me ended up with a review varying between somewhat and very
detailed.I'm sure not every author was happy with the outcome. While we did
our best to make good decisions, they were difficult decisions to make. For
example, if there is a</p><p>6 0.67306262 <a title="453-lsi-6" href="../hunch_net-2005/hunch_net-2005-07-04-The_Health_of_COLT.html">89 hunch net-2005-07-04-The Health of COLT</a></p>
<p>7 0.66727638 <a title="453-lsi-7" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>8 0.65503067 <a title="453-lsi-8" href="../hunch_net-2010/hunch_net-2010-04-24-COLT_Treasurer_is_now_Phil_Long.html">394 hunch net-2010-04-24-COLT Treasurer is now Phil Long</a></p>
<p>9 0.61359429 <a title="453-lsi-9" href="../hunch_net-2011/hunch_net-2011-10-10-ML_Symposium_and_ICML_details.html">447 hunch net-2011-10-10-ML Symposium and ICML details</a></p>
<p>10 0.61275226 <a title="453-lsi-10" href="../hunch_net-2012/hunch_net-2012-04-09-ICML_author_feedback_is_open.html">461 hunch net-2012-04-09-ICML author feedback is open</a></p>
<p>11 0.58126736 <a title="453-lsi-11" href="../hunch_net-2012/hunch_net-2012-05-02-ICML%3A_Behind_the_Scenes.html">463 hunch net-2012-05-02-ICML: Behind the Scenes</a></p>
<p>12 0.57122737 <a title="453-lsi-12" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>13 0.56286919 <a title="453-lsi-13" href="../hunch_net-2005/hunch_net-2005-07-01-The_Role_of_Impromptu_Talks.html">88 hunch net-2005-07-01-The Role of Impromptu Talks</a></p>
<p>14 0.56270343 <a title="453-lsi-14" href="../hunch_net-2008/hunch_net-2008-12-12-Summer_Conferences.html">331 hunch net-2008-12-12-Summer Conferences</a></p>
<p>15 0.55983621 <a title="453-lsi-15" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>16 0.5572027 <a title="453-lsi-16" href="../hunch_net-2006/hunch_net-2006-12-04-Structural_Problems_in_NIPS_Decision_Making.html">221 hunch net-2006-12-04-Structural Problems in NIPS Decision Making</a></p>
<p>17 0.55338937 <a title="453-lsi-17" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>18 0.55285496 <a title="453-lsi-18" href="../hunch_net-2005/hunch_net-2005-05-02-Reviewing_techniques_for_conferences.html">65 hunch net-2005-05-02-Reviewing techniques for conferences</a></p>
<p>19 0.53678542 <a title="453-lsi-19" href="../hunch_net-2005/hunch_net-2005-06-28-The_cross_validation_problem%3A_cash_reward.html">86 hunch net-2005-06-28-The cross validation problem: cash reward</a></p>
<p>20 0.52760869 <a title="453-lsi-20" href="../hunch_net-2005/hunch_net-2005-03-09-Bad_Reviewing.html">38 hunch net-2005-03-09-Bad Reviewing</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(6, 0.01), (35, 0.019), (42, 0.158), (48, 0.012), (50, 0.013), (59, 0.014), (68, 0.023), (69, 0.282), (74, 0.255), (82, 0.052), (95, 0.068)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94513249 <a title="453-lda-1" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<p>Introduction: ByShieandNatiFollowing John's advertisement for submitting to ICML, we thought
it appropriate to highlight the advantages of COLT, and the reasons it is
often the best place for theory papers. We would like to emphasize that we
both respect ICML, and are active in ICML, both as authors and as area chairs,
and certainly are not arguing that ICML is a bad place for your papers. For
many papers, ICML is the best venue. But for many theory papers, COLT is a
better and more appropriate place.Why should you submit to COLT?By-and-large,
theory papers go to COLT. This is the tradition of the field and most theory
papers are sent to COLT. This is the place to present your ground-breaking
theorems and new models that will shape the theory of machine learning. COLT
is more focused then ICML with a single track session. Unlike ICML, the norm
in COLT is for people to sit through most sessions, and hear most of the talks
presented. There is also often a lively discussion following paper
presentation</p><p>2 0.92429912 <a title="453-lda-2" href="../hunch_net-2007/hunch_net-2007-04-30-COLT_2007.html">242 hunch net-2007-04-30-COLT 2007</a></p>
<p>Introduction: Registration for COLT 2007 is now open.The conference will take place on 13-15
June, 2007, in San Diego, California, as part of the 2007 Federated Computing
Research Conference (FCRC), which includes STOC, Complexity, and EC.The
website for COLT: http://www.learningtheory.org/colt2007/index.htmlThe early
registration deadline is May 11, and the cutoff date for discounted hotel
rates is May 9.Before registering, take note that the fees are substantially
lower for members of ACM and/or SIGACT than for nonmembers. If you've been
contemplating joining either of these two societies (annual dues: $99 for ACM,
$18 for SIGACT), now would be a good time!</p><p>3 0.83528608 <a title="453-lda-3" href="../hunch_net-2007/hunch_net-2007-07-20-Motivation_should_be_the_Responsibility_of_the_Reviewer.html">256 hunch net-2007-07-20-Motivation should be the Responsibility of the Reviewer</a></p>
<p>Introduction: The prevailing wisdom in machine learning seems to be that motivating a paper
is the responsibility of the author. I think this is a harmful view--instead,
it's healthier for the community to regard this as the responsibility of the
reviewer.There are lots of reasons to prefer a reviewer-responsibility
approach.Authors are the most biased possible source of information about the
motivation of the paper. Systems which rely upon very biased sources of
information are inherently unreliable.Authors are highly variable in their
ability and desire to express motivation for their work. This adds greatly to
variance on acceptance of an idea, and it can systematically discriminate or
accentuate careers. It's great if you have a career accentuated by awesome
wording choice, but wise decision making by reviewers is important for the
field.The motivation section in a paper doesn'tdoanything in some sense--it's
there to get the paper in. Reading the motivation of a paper is of little use
in helping</p><p>4 0.83105135 <a title="453-lda-4" href="../hunch_net-2005/hunch_net-2005-07-10-Thinking_the_Unthought.html">91 hunch net-2005-07-10-Thinking the Unthought</a></p>
<p>Introduction: One thing common to much research is that the researcher must be the first
personeverto have some thought. How do you think of something that has never
been thought of? There seems to be no methodical manner of doing this, but
there are some tricks.The easiest method is to just have some connection come
to you. There is a trick here however: you should write it down and fill out
the idea immediately because it can just as easily go away.A harder method is
to set aside a block of time and simply think about an idea. Distraction
elimination is essential here because thinking about the unthought is hard
work which your mind will avoid.Another common method is in conversation.
Sometimes the process of verbalizing implies new ideas come up and sometimes
whoever you are talking to replies just the right way. This method is
dangerous though--you must speak to someone who helps you think rather than
someone who occupies your thoughts.Try to rephrase the problem so the answer
is simple. This is</p><p>5 0.82490814 <a title="453-lda-5" href="../hunch_net-2005/hunch_net-2005-11-16-The_Everything_Ensemble_Edge.html">131 hunch net-2005-11-16-The Everything Ensemble Edge</a></p>
<p>Introduction: Rich Caruana,Alexandru Niculescu, Geoff Crew, and Alex Ksikes have donea lot
of empirical testingwhich shows thatusing all methods to make a predictionis
more powerful than using any single method. This is in rough agreement with
the Bayesian way of solving problems, but based upon a different (essentially
empirical) motivation. A rough summary is:Take all of {decision trees, boosted
decision trees, bagged decision trees, boosted decision stumps, K nearest
neighbors, neural networks, SVM} with all reasonable parameter settings.Run
the methods on each problem of 8 problems with a large test set, calibrating
margins using eithersigmoid fittingorisotonic regression.For each loss of
{accuracy, area under the ROC curve, cross entropy, squared error, etcâ&euro;Ś}
evaluate the average performance of the method.A series of conclusions can be
drawn from the observations.(Calibrated) boosted decision trees appear to
perform best, in general although support vector machines and neural networks
give cred</p><p>6 0.81571168 <a title="453-lda-6" href="../hunch_net-2011/hunch_net-2011-09-28-Somebody%26%238217%3Bs_Eating_Your_Lunch.html">445 hunch net-2011-09-28-Somebody&#8217;s Eating Your Lunch</a></p>
<p>7 0.81547695 <a title="453-lda-7" href="../hunch_net-2005/hunch_net-2005-10-08-We_have_a_winner.html">119 hunch net-2005-10-08-We have a winner</a></p>
<p>8 0.78776562 <a title="453-lda-8" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>9 0.77129507 <a title="453-lda-9" href="../hunch_net-2012/hunch_net-2012-08-24-Patterns_for_research_in_machine_learning.html">471 hunch net-2012-08-24-Patterns for research in machine learning</a></p>
<p>10 0.77084875 <a title="453-lda-10" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>11 0.75849664 <a title="453-lda-11" href="../hunch_net-2006/hunch_net-2006-03-24-NLPers.html">166 hunch net-2006-03-24-NLPers</a></p>
<p>12 0.75454968 <a title="453-lda-12" href="../hunch_net-2007/hunch_net-2007-10-15-NIPS_workshops_extended_to_3_days.html">266 hunch net-2007-10-15-NIPS workshops extended to 3 days</a></p>
<p>13 0.75291717 <a title="453-lda-13" href="../hunch_net-2012/hunch_net-2012-05-02-ICML%3A_Behind_the_Scenes.html">463 hunch net-2012-05-02-ICML: Behind the Scenes</a></p>
<p>14 0.7472434 <a title="453-lda-14" href="../hunch_net-2010/hunch_net-2010-08-24-Alex_Smola_starts_a_blog.html">408 hunch net-2010-08-24-Alex Smola starts a blog</a></p>
<p>15 0.7457568 <a title="453-lda-15" href="../hunch_net-2008/hunch_net-2008-10-14-Who_is_Responsible_for_a_Bad_Review%3F.html">320 hunch net-2008-10-14-Who is Responsible for a Bad Review?</a></p>
<p>16 0.74265873 <a title="453-lda-16" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>17 0.74148858 <a title="453-lda-17" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>18 0.73744553 <a title="453-lda-18" href="../hunch_net-2010/hunch_net-2010-04-26-Compassionate_Reviewing.html">395 hunch net-2010-04-26-Compassionate Reviewing</a></p>
<p>19 0.73616993 <a title="453-lda-19" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>20 0.72959107 <a title="453-lda-20" href="../hunch_net-2005/hunch_net-2005-07-04-The_Health_of_COLT.html">89 hunch net-2005-07-04-The Health of COLT</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
