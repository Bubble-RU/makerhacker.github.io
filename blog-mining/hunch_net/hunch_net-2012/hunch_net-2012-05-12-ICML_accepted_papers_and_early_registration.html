<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>465 hunch net-2012-05-12-ICML accepted papers and early registration</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2012" href="../home/hunch_net-2012_home.html">hunch_net-2012</a> <a title="hunch_net-2012-465" href="#">hunch_net-2012-465</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>465 hunch net-2012-05-12-ICML accepted papers and early registration</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2012-465-html" href="http://hunch.net/?p=2488">html</a></p><p>Introduction: Theaccepted papersare up in full detail. We are still struggling with the
precise program itself, but that's coming along. Also note theMay 13deadline
forearly registrationand room booking.</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('full', 0.426), ('precise', 0.426), ('room', 0.42), ('coming', 0.407), ('note', 0.33), ('still', 0.295), ('program', 0.285), ('also', 0.133)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="465-tfidf-1" href="../hunch_net-2012/hunch_net-2012-05-12-ICML_accepted_papers_and_early_registration.html">465 hunch net-2012-05-12-ICML accepted papers and early registration</a></p>
<p>Introduction: Theaccepted papersare up in full detail. We are still struggling with the
precise program itself, but that's coming along. Also note theMay 13deadline
forearly registrationand room booking.</p><p>2 0.09122137 <a title="465-tfidf-2" href="../hunch_net-2006/hunch_net-2006-03-05-%26%238220%3BStructural%26%238221%3B_Learning.html">161 hunch net-2006-03-05-&#8220;Structural&#8221; Learning</a></p>
<p>Introduction: Fernando Pereirapointed outAndo andZhang'spaperon "structural" learning.
Structural learning is multitask learning on subproblems created from
unlabeled data.The basic idea is to take a look at the unlabeled data and
create many supervised problems. On text data, which they test on, these
subproblems might be of the form "Given surrounding words predict the middle
word". The hope here is that successfully predicting on these subproblems is
relevant to the prediction of your core problem.In the long run, the precise
mechanism used (essentially, linear predictors with parameters tied by a
common matrix) and the precise problems formed may not be critical. What seems
critical is that the hope is realized: the technique provides a significant
edge in practice.Some basic questions about this approach are:Are there
effective automated mechanisms for creating the subproblems?Is it necessary to
use a shared representation?</p><p>3 0.090126425 <a title="465-tfidf-3" href="../hunch_net-2012/hunch_net-2012-03-24-David_Waltz.html">460 hunch net-2012-03-24-David Waltz</a></p>
<p>Introduction: hasdied. He lived a full life. I know him personally as a founder of theCenter
for Computational Learning Systemsand theNew York Machine Learning Symposium,
both of which have sheltered and promoted the advancement of machine learning.
I expect much of the New York area machine learning community will miss him,
as well as many others around the world.</p><p>4 0.089582451 <a title="465-tfidf-4" href="../hunch_net-2005/hunch_net-2005-12-29-Deadline_Season.html">145 hunch net-2005-12-29-Deadline Season</a></p>
<p>Introduction: Many different paper deadlines are coming up soon so I made a little reference
table. Out of curiosity, I also computed the interval between submission
deadline and
conference.ConferenceLocationDateDeadlineintervalCOLTPittsburghJune
22-25January 21152ICMLPittsburghJune 26-28January 30/February 6140UAIMITJuly
13-16March 9/March 16119AAAIBostonJuly 16-20February
16/21145KDDPhiladelphiaAugust 23-26March 3/March 10166It looks like the
northeastern US is the big winner as far as location this year.</p><p>5 0.088174693 <a title="465-tfidf-5" href="../hunch_net-2012/hunch_net-2012-04-20-Both_new%3A_STOC_workshops_and_NEML.html">462 hunch net-2012-04-20-Both new: STOC workshops and NEML</a></p>
<p>Introduction: May 16 in Cambridge, is theNew England Machine Learning Day, a first regional
workshop/symposium on machine learning. To present a poster, submit an
abstract byMay 5.May 19 in New York,STOCis coming to town and rather
surprisingly havingworkshopswhich should be quite a bit of fun. I'll be
speaking atAlgorithms for Distributed and Streaming Data.</p><p>6 0.08708626 <a title="465-tfidf-6" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<p>7 0.079723269 <a title="465-tfidf-7" href="../hunch_net-2011/hunch_net-2011-09-03-Fall_Machine_Learning_Events.html">443 hunch net-2011-09-03-Fall Machine Learning Events</a></p>
<p>8 0.078103423 <a title="465-tfidf-8" href="../hunch_net-2011/hunch_net-2011-08-06-Interesting_thing_at_UAI_2011.html">440 hunch net-2011-08-06-Interesting thing at UAI 2011</a></p>
<p>9 0.072711751 <a title="465-tfidf-9" href="../hunch_net-2005/hunch_net-2005-04-27-DARPA_project%3A_LAGR.html">63 hunch net-2005-04-27-DARPA project: LAGR</a></p>
<p>10 0.071131393 <a title="465-tfidf-10" href="../hunch_net-2006/hunch_net-2006-04-27-Conferences%2C_Workshops%2C_and_Tutorials.html">174 hunch net-2006-04-27-Conferences, Workshops, and Tutorials</a></p>
<p>11 0.070212051 <a title="465-tfidf-11" href="../hunch_net-2006/hunch_net-2006-01-08-Debugging_Your_Brain.html">147 hunch net-2006-01-08-Debugging Your Brain</a></p>
<p>12 0.070143461 <a title="465-tfidf-12" href="../hunch_net-2006/hunch_net-2006-02-02-Introspectionism_as_a_Disease.html">153 hunch net-2006-02-02-Introspectionism as a Disease</a></p>
<p>13 0.067531951 <a title="465-tfidf-13" href="../hunch_net-2010/hunch_net-2010-12-04-Vowpal_Wabbit%2C_version_5.0%2C_and_the_second_heresy.html">419 hunch net-2010-12-04-Vowpal Wabbit, version 5.0, and the second heresy</a></p>
<p>14 0.06382338 <a title="465-tfidf-14" href="../hunch_net-2006/hunch_net-2006-12-05-Recruitment_Conferences.html">222 hunch net-2006-12-05-Recruitment Conferences</a></p>
<p>15 0.062843494 <a title="465-tfidf-15" href="../hunch_net-2011/hunch_net-2011-05-09-CI_Fellows%2C_again.html">434 hunch net-2011-05-09-CI Fellows, again</a></p>
<p>16 0.061488818 <a title="465-tfidf-16" href="../hunch_net-2011/hunch_net-2011-03-27-Vowpal_Wabbit%2C_v5.1.html">428 hunch net-2011-03-27-Vowpal Wabbit, v5.1</a></p>
<p>17 0.06024199 <a title="465-tfidf-17" href="../hunch_net-2012/hunch_net-2012-04-09-ICML_author_feedback_is_open.html">461 hunch net-2012-04-09-ICML author feedback is open</a></p>
<p>18 0.059675097 <a title="465-tfidf-18" href="../hunch_net-2008/hunch_net-2008-11-09-A_Healthy__COLT.html">324 hunch net-2008-11-09-A Healthy  COLT</a></p>
<p>19 0.059102632 <a title="465-tfidf-19" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>20 0.057384167 <a title="465-tfidf-20" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.064), (1, 0.034), (2, 0.035), (3, 0.028), (4, 0.017), (5, 0.006), (6, 0.02), (7, 0.036), (8, 0.037), (9, -0.016), (10, 0.0), (11, 0.008), (12, 0.036), (13, -0.109), (14, -0.016), (15, 0.062), (16, 0.048), (17, 0.004), (18, 0.036), (19, -0.015), (20, -0.081), (21, 0.019), (22, 0.035), (23, -0.068), (24, 0.01), (25, 0.02), (26, -0.086), (27, -0.03), (28, -0.031), (29, 0.064), (30, -0.107), (31, -0.031), (32, 0.069), (33, -0.079), (34, -0.007), (35, -0.025), (36, -0.019), (37, -0.105), (38, -0.041), (39, 0.006), (40, -0.008), (41, -0.032), (42, 0.061), (43, -0.018), (44, 0.013), (45, -0.031), (46, 0.012), (47, -0.019), (48, -0.068), (49, 0.102)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99325413 <a title="465-lsi-1" href="../hunch_net-2012/hunch_net-2012-05-12-ICML_accepted_papers_and_early_registration.html">465 hunch net-2012-05-12-ICML accepted papers and early registration</a></p>
<p>Introduction: Theaccepted papersare up in full detail. We are still struggling with the
precise program itself, but that's coming along. Also note theMay 13deadline
forearly registrationand room booking.</p><p>2 0.45780045 <a title="465-lsi-2" href="../hunch_net-2006/hunch_net-2006-01-08-Debugging_Your_Brain.html">147 hunch net-2006-01-08-Debugging Your Brain</a></p>
<p>Introduction: One part of doing research is debugging your understanding of reality. This is
hard work: How do you even discover where you misunderstand? If you discover a
misunderstanding, how do you go about removing it?The process of debugging
computer programs is quite analogous to debugging reality misunderstandings.
This is natural--a bug in a computer program is a misunderstanding between you
and the computer about what you said. Many of the familiar techniques from
debugging have exact parallels.DetailsWhen programming, there are often signs
that some bug exists like: "the graph my program output is shifted a little
bit" = maybe you have an indexing error. In debugging yourself, we often have
some impression that something is "not right". These impressions should be
addressed directly and immediately. (Some people have the habit of suppressing
worries in favor of excess certainty. That's not healthy for research.)Corner
CasesA "corner case" is an input to a program which is extreme in some w</p><p>3 0.45284447 <a title="465-lsi-3" href="../hunch_net-2008/hunch_net-2008-11-09-A_Healthy__COLT.html">324 hunch net-2008-11-09-A Healthy  COLT</a></p>
<p>Introduction: Awhile ago, we discussed the health ofCOLT.COLT 2008substantially addressed my
concerns. The papers were diverse and several were interesting. Attendance was
up, which is particularly notable in Europe. In my opinion, the colocation
with UAI and ICML was the best colocation since 1998.And, perhaps best of all,
registration ended up being free for all students due to various grants from
theAcademy of Finland,Google,IBM, andYahoo.A basic question is: what went
right? There seem to be several answers.Cost-wise, COLT had sufficient grants
to alleviate the high cost of the Euro and location at a university
substantially reduces the cost compared to a hotel.Organization-wise, the
Finns were great with hordes of volunteers helping set everything up. Having
too many volunteers is a good failure mode.Organization-wise, it was clear
that all 3 program chairs were cooperating in designing the program
.Facilities-wise, proximity in time and space made the colocation much more
real than many others</p><p>4 0.4035368 <a title="465-lsi-4" href="../hunch_net-2007/hunch_net-2007-04-30-COLT_2007.html">242 hunch net-2007-04-30-COLT 2007</a></p>
<p>Introduction: Registration for COLT 2007 is now open.The conference will take place on 13-15
June, 2007, in San Diego, California, as part of the 2007 Federated Computing
Research Conference (FCRC), which includes STOC, Complexity, and EC.The
website for COLT: http://www.learningtheory.org/colt2007/index.htmlThe early
registration deadline is May 11, and the cutoff date for discounted hotel
rates is May 9.Before registering, take note that the fees are substantially
lower for members of ACM and/or SIGACT than for nonmembers. If you've been
contemplating joining either of these two societies (annual dues: $99 for ACM,
$18 for SIGACT), now would be a good time!</p><p>5 0.38629982 <a title="465-lsi-5" href="../hunch_net-2010/hunch_net-2010-10-28-NY_ML_Symposium_2010.html">415 hunch net-2010-10-28-NY ML Symposium 2010</a></p>
<p>Introduction: About 200 people attended the2010 NYAS ML Symposiumthis year. (It wasabout 170
last year.) I particularly enjoyed several talks.Yannhas a new live demo of
(limited) real-time object recognition learning.Sanjoygave a fairly convincing
and comprehensible explanation of why amodified form of single-linkage
clusteringis consistent in higher dimensions, and why consistency is a
critical feature for clustering algorithms. I'm curious how well this
algorithm works in practice.Matt Hoffman's poster covering online LDA seemed
pretty convincing to me as an algorithmic improvement.This year, we allocated
more time towards posters & poster spotlights.For next year, we are
considering some further changes. The format has traditionally been 4 invited
Professor speakers, with posters and poster spotlight for students. Demand
from other parties to participate is growing, for example from postdocs and
startups in the area. Another growing concern is the facility--the location is
exceptional, but fittin</p><p>6 0.38119766 <a title="465-lsi-6" href="../hunch_net-2011/hunch_net-2011-03-27-Vowpal_Wabbit%2C_v5.1.html">428 hunch net-2011-03-27-Vowpal Wabbit, v5.1</a></p>
<p>7 0.36645949 <a title="465-lsi-7" href="../hunch_net-2006/hunch_net-2006-09-19-Luis_von_Ahn_is_awarded_a_MacArthur_fellowship..html">209 hunch net-2006-09-19-Luis von Ahn is awarded a MacArthur fellowship.</a></p>
<p>8 0.3657071 <a title="465-lsi-8" href="../hunch_net-2005/hunch_net-2005-07-04-The_Health_of_COLT.html">89 hunch net-2005-07-04-The Health of COLT</a></p>
<p>9 0.36484903 <a title="465-lsi-9" href="../hunch_net-2010/hunch_net-2010-04-24-COLT_Treasurer_is_now_Phil_Long.html">394 hunch net-2010-04-24-COLT Treasurer is now Phil Long</a></p>
<p>10 0.36338252 <a title="465-lsi-10" href="../hunch_net-2006/hunch_net-2006-06-05-Server_Shift%2C_Site_Tweaks%2C_Suggestions%3F.html">182 hunch net-2006-06-05-Server Shift, Site Tweaks, Suggestions?</a></p>
<p>11 0.36259869 <a title="465-lsi-11" href="../hunch_net-2005/hunch_net-2005-07-01-The_Role_of_Impromptu_Talks.html">88 hunch net-2005-07-01-The Role of Impromptu Talks</a></p>
<p>12 0.36166906 <a title="465-lsi-12" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<p>13 0.35809079 <a title="465-lsi-13" href="../hunch_net-2005/hunch_net-2005-04-27-DARPA_project%3A_LAGR.html">63 hunch net-2005-04-27-DARPA project: LAGR</a></p>
<p>14 0.35594139 <a title="465-lsi-14" href="../hunch_net-2012/hunch_net-2012-04-20-Both_new%3A_STOC_workshops_and_NEML.html">462 hunch net-2012-04-20-Both new: STOC workshops and NEML</a></p>
<p>15 0.3421081 <a title="465-lsi-15" href="../hunch_net-2007/hunch_net-2007-09-16-Optimizing_Machine_Learning_Programs.html">262 hunch net-2007-09-16-Optimizing Machine Learning Programs</a></p>
<p>16 0.31209296 <a title="465-lsi-16" href="../hunch_net-2009/hunch_net-2009-12-07-Vowpal_Wabbit_version_4.0%2C_and_a_NIPS_heresy.html">381 hunch net-2009-12-07-Vowpal Wabbit version 4.0, and a NIPS heresy</a></p>
<p>17 0.31103301 <a title="465-lsi-17" href="../hunch_net-2006/hunch_net-2006-02-02-Introspectionism_as_a_Disease.html">153 hunch net-2006-02-02-Introspectionism as a Disease</a></p>
<p>18 0.30252105 <a title="465-lsi-18" href="../hunch_net-2005/hunch_net-2005-02-25-Solution%3A_Reinforcement_Learning_with_Classification.html">29 hunch net-2005-02-25-Solution: Reinforcement Learning with Classification</a></p>
<p>19 0.30006832 <a title="465-lsi-19" href="../hunch_net-2011/hunch_net-2011-05-09-CI_Fellows%2C_again.html">434 hunch net-2011-05-09-CI Fellows, again</a></p>
<p>20 0.29823989 <a title="465-lsi-20" href="../hunch_net-2010/hunch_net-2010-04-28-CI_Fellows_program_renewed.html">396 hunch net-2010-04-28-CI Fellows program renewed</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(10, 0.734)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.90041524 <a title="465-lda-1" href="../hunch_net-2012/hunch_net-2012-05-12-ICML_accepted_papers_and_early_registration.html">465 hunch net-2012-05-12-ICML accepted papers and early registration</a></p>
<p>Introduction: Theaccepted papersare up in full detail. We are still struggling with the
precise program itself, but that's coming along. Also note theMay 13deadline
forearly registrationand room booking.</p><p>2 0.83438045 <a title="465-lda-2" href="../hunch_net-2012/hunch_net-2012-02-29-Key_Scientific_Challenges_and_the_Franklin_Symposium.html">457 hunch net-2012-02-29-Key Scientific Challenges and the Franklin Symposium</a></p>
<p>Introduction: For graduate students, theYahoo!Key Scientific Challenges programincluding
inmachine learningis on again,due March 9. The application is easy and the $5K
award is high quality "no strings attached" funding. Consider submitting.Those
in Washington DC, Philadelphia, and New York, may consider attending
theFranklin Institute SymposiumApril 25which has several speakers and an award
forV. Attendance is free with an RSVP.</p><p>3 0.56416667 <a title="465-lda-3" href="../hunch_net-2005/hunch_net-2005-05-11-Visa_Casualties.html">69 hunch net-2005-05-11-Visa Casualties</a></p>
<p>Introduction: For theChicago 2005 machine learning summer schoolwe are organizing, at least
5 international students can not come due to visa issues. There seem to be two
aspects to visa issues:Inefficiency. The system rejected the student simply by
being incapable of evenstartingto evaluate their visa in less than 1 month of
time.Politics. Border controls became much tighter after the September 11
attack. Losing a big chunk of downtown of the largest city in a country will
do that.What I (and the students) learned is that (1) is a much larger problem
than (2). Only 1 prospective student seems to have achieved an explicit visa
rejection. Fixing problem (1) should be a no-brainer, because the lag time
almost surely indicates overload, and overload on border controls should worry
even people concerned with (2). The obvious fixes to overload are "spend more
money" and "make the system more efficient".With respect to (2), (which is a
more minor issue by the numbers) it is unclear that the political calc</p><p>4 0.56348896 <a title="465-lda-4" href="../hunch_net-2005/hunch_net-2005-02-02-Kolmogorov_Complexity_and_Googling.html">10 hunch net-2005-02-02-Kolmogorov Complexity and Googling</a></p>
<p>Introduction: Machine learning makes theNew Scientist. From the article:COMPUTERS can learn
the meaning of words simply by plugging into Google. The finding could bring
forward the day that true artificial intelligence is developedâ&euro;Ś.But Paul
Vitanyi and Rudi Cilibrasi of the National Institute for Mathematics and
Computer Science in Amsterdam, the Netherlands, realised that a Google search
can be used to measure how closely two words relate to each other. For
instance, imagine a computer needs to understand what a hat is.You can read
the paper atKC Google.Hat tip:Kolmogorov Mailing ListAny thoughts on the
paper?</p><p>5 0.3541052 <a title="465-lda-5" href="../hunch_net-2005/hunch_net-2005-01-19-Why_I_decided_to_run_a_weblog..html">1 hunch net-2005-01-19-Why I decided to run a weblog.</a></p>
<p>Introduction: I have decided to run a weblog on machine learning and learning theory
research. Here are some reasons:1) Weblogs enable new functionality:Public
comment on papers. No mechanism for this exists at conferences and most
journals. I have encountered it once for asciencepaper. Some communities have
mailing lists supporting this, but not machine learning or learning theory. I
have often read papers and found myself wishing there was some method to
consider other's questions and read the replies.Conference shortlists. One of
the most common conversations at a conference is "what did you find
interesting?" There is no explicit mechanism for sharing this information at
conferences, and it's easy to imagine that it would be handy to do
so.Evaluation and comment on research directions. Papers are almost
exclusively about new research, rather than evaluation (and consideration) of
research directions. This last role is satisfied by funding agencies to some
extent, but that is a private debate of</p><p>6 0.2091299 <a title="465-lda-6" href="../hunch_net-2006/hunch_net-2006-01-30-Should_the_Input_Representation_be_a_Vector%3F.html">152 hunch net-2006-01-30-Should the Input Representation be a Vector?</a></p>
<p>7 0.16973273 <a title="465-lda-7" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>8 0.042045154 <a title="465-lda-8" href="../hunch_net-2014/hunch_net-2014-02-16-Metacademy%3A_a_package_manager_for_knowledge.html">493 hunch net-2014-02-16-Metacademy: a package manager for knowledge</a></p>
<p>9 0.030422449 <a title="465-lda-9" href="../hunch_net-2011/hunch_net-2011-09-03-Fall_Machine_Learning_Events.html">443 hunch net-2011-09-03-Fall Machine Learning Events</a></p>
<p>10 0.029945668 <a title="465-lda-10" href="../hunch_net-2005/hunch_net-2005-09-04-Science_in_the_Government.html">106 hunch net-2005-09-04-Science in the Government</a></p>
<p>11 0.0 <a title="465-lda-11" href="../hunch_net-2005/hunch_net-2005-01-24-Holy_grails_of_machine_learning%3F.html">2 hunch net-2005-01-24-Holy grails of machine learning?</a></p>
<p>12 0.0 <a title="465-lda-12" href="../hunch_net-2005/hunch_net-2005-01-24-The_Humanloop_Spectrum_of_Machine_Learning.html">3 hunch net-2005-01-24-The Humanloop Spectrum of Machine Learning</a></p>
<p>13 0.0 <a title="465-lda-13" href="../hunch_net-2005/hunch_net-2005-01-26-Summer_Schools.html">4 hunch net-2005-01-26-Summer Schools</a></p>
<p>14 0.0 <a title="465-lda-14" href="../hunch_net-2005/hunch_net-2005-01-26-Watchword%3A_Probability.html">5 hunch net-2005-01-26-Watchword: Probability</a></p>
<p>15 0.0 <a title="465-lda-15" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>16 0.0 <a title="465-lda-16" href="../hunch_net-2005/hunch_net-2005-01-31-Watchword%3A_Assumption.html">7 hunch net-2005-01-31-Watchword: Assumption</a></p>
<p>17 0.0 <a title="465-lda-17" href="../hunch_net-2005/hunch_net-2005-02-01-NIPS%3A_Online_Bayes.html">8 hunch net-2005-02-01-NIPS: Online Bayes</a></p>
<p>18 0.0 <a title="465-lda-18" href="../hunch_net-2005/hunch_net-2005-02-01-Watchword%3A_Loss.html">9 hunch net-2005-02-01-Watchword: Loss</a></p>
<p>19 0.0 <a title="465-lda-19" href="../hunch_net-2005/hunch_net-2005-02-02-Paper_Deadlines.html">11 hunch net-2005-02-02-Paper Deadlines</a></p>
<p>20 0.0 <a title="465-lda-20" href="../hunch_net-2005/hunch_net-2005-02-03-Learning_Theory%2C_by_assumption.html">12 hunch net-2005-02-03-Learning Theory, by assumption</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
