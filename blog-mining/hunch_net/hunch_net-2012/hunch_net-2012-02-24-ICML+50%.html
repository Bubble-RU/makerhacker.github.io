<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>456 hunch net-2012-02-24-ICML+50%</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2012" href="../home/hunch_net-2012_home.html">hunch_net-2012</a> <a title="hunch_net-2012-456" href="#">hunch_net-2012-456</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>456 hunch net-2012-02-24-ICML+50%</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2012-456-html" href="http://hunch.net/?p=2289">html</a></p><p>Introduction: TheICMLpaper deadline has passed.Joelleand I were surprised to see the number
of submissions jump from last year by about 50% to around 900 submissions. A
tiny portion of these are immediate rejects(*), so this is a much larger set
of papers than expected. The number of workshop submissions also doubled
compared to last year, so ICML may grow significantly this year, if we can
manage to handle the load well. The prospect of making 900 good decisions is
fundamentally daunting, and success will rely heavily on theprogram
committeeandarea chairsat this point.For those who want to rubberneck a bit
more, here's a breakdown of submissions by primary topic of submitted
papers:66 Reinforcement Learning 52 Supervised Learning 51 Clustering 46
Kernel Methods 40 Optimization Algorithms 39 Feature Selection and
Dimensionality Reduction 33 Learning Theory 33 Graphical Models 33
Applications 29 Probabilistic Models 29 NN & Deep Learning 26 Transfer and
Multi-Task Learning 25 Online Learning 25 Activ</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('submissions', 0.247), ('mining', 0.243), ('methods', 0.227), ('models', 0.21), ('inductive', 0.18), ('relational', 0.18), ('deadlines', 0.138), ('inference', 0.131), ('game', 0.123), ('statistical', 0.119), ('learning', 0.114), ('deadline', 0.108), ('topic', 0.099), ('querying', 0.097), ('daunting', 0.097), ('theprogram', 0.097), ('bioinformatics', 0.097), ('detection', 0.097), ('magical', 0.097), ('doubled', 0.097)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999958 <a title="456-tfidf-1" href="../hunch_net-2012/hunch_net-2012-02-24-ICML%2B50%25.html">456 hunch net-2012-02-24-ICML+50%</a></p>
<p>Introduction: TheICMLpaper deadline has passed.Joelleand I were surprised to see the number
of submissions jump from last year by about 50% to around 900 submissions. A
tiny portion of these are immediate rejects(*), so this is a much larger set
of papers than expected. The number of workshop submissions also doubled
compared to last year, so ICML may grow significantly this year, if we can
manage to handle the load well. The prospect of making 900 good decisions is
fundamentally daunting, and success will rely heavily on theprogram
committeeandarea chairsat this point.For those who want to rubberneck a bit
more, here's a breakdown of submissions by primary topic of submitted
papers:66 Reinforcement Learning 52 Supervised Learning 51 Clustering 46
Kernel Methods 40 Optimization Algorithms 39 Feature Selection and
Dimensionality Reduction 33 Learning Theory 33 Graphical Models 33
Applications 29 Probabilistic Models 29 NN & Deep Learning 26 Transfer and
Multi-Task Learning 25 Online Learning 25 Activ</p><p>2 0.19160163 <a title="456-tfidf-2" href="../hunch_net-2012/hunch_net-2012-06-05-ICML_acceptance_statistics.html">466 hunch net-2012-06-05-ICML acceptance statistics</a></p>
<p>Introduction: People are naturally interested in slicing the ICML acceptance statistics in
various ways. Here's a rundown for the top categories.18/66 = 0.27in
(0.18,0.36)Reinforcement Learning10/52 = 0.19in (0.17,0.37)Supervised
Learning9/51 = 0.18not in (0.18, 0.37)Clustering12/46 = 0.26in (0.17,
0.37)Kernel Methods11/40 = 0.28in (0.15, 0.4)Optimization Algorithms8/33 =
0.24in (0.15, 0.39)Learning Theory14/33 = 0.42not in (0.15, 0.39)Graphical
Models10/32 = 0.31in (0.15, 0.41)Applications (+5 invited)8/29 = 0.28in (0.14,
0.41])Probabilistic Models13/29 = 0.45not in (0.14, 0.41)NN & Deep
Learning8/26 = 0.31in (0.12, 0.42)Transfer and Multi-Task Learning13/25 =
0.52not in (0.12, 0.44)Online Learning5/25 = 0.20in (0.12, 0.44)Active
Learning6/22 = 0.27in (0.14, 0.41)Semi-Supervised Learning7/20 = 0.35in (0.1,
0.45)Statistical Methods4/20 = 0.20in (0.1, 0.45)Sparsity and Compressed
Sensing1/19 = 0.05not in (0.11, 0.42)Ensemble Methods5/18 = 0.28in (0.11,
0.44)Structured Output Prediction4/18 = 0.22in (</p><p>3 0.12237984 <a title="456-tfidf-3" href="../hunch_net-2011/hunch_net-2011-07-11-Interesting_Neural_Network_Papers_at_ICML_2011.html">438 hunch net-2011-07-11-Interesting Neural Network Papers at ICML 2011</a></p>
<p>Introduction: Maybe it's too early to call, but with four separate Neural Network sessions
at this year'sICML, it looks like Neural Networks are making a comeback. Here
are my highlights of these sessions. In general, my feeling is that these
papers both demystify deep learning and show its broader applicability.The
first observation I made is that the once disreputable "Neural" nomenclature
is being used againin lieu of"deep learning". Maybe it's because Adam Coates
et al. showed that single layer networks can work surprisingly well.An
Analysis of Single-Layer Networks in Unsupervised Feature Learning,Adam
Coates,Honglak Lee,Andrew Y. Ng(AISTATS 2011)The Importance of Encoding Versus
Training with Sparse Coding and Vector Quantization,Adam Coates,Andrew Y.
Ng(ICML 2011)Another surprising result out of Andrew Ng's group comes from
Andrew Saxe et al. who show that certain convolutional pooling architectures
can obtain close to state-of-the-art performance with random weights (that is,
without actuall</p><p>4 0.11662918 <a title="456-tfidf-4" href="../hunch_net-2008/hunch_net-2008-10-19-NIPS_2008_workshop_on_Kernel_Learning.html">321 hunch net-2008-10-19-NIPS 2008 workshop on Kernel Learning</a></p>
<p>Introduction: We'd like to invite hunch.net readers to participate in the NIPS 2008 workshop
on kernel learning. While the main focus is on automatically learning kernels
from data, we are also also looking at the broader questions of feature
selection, multi-task learning and multi-view learning. There are no
restrictions on the learning problem being addressed (regression,
classification, etc), and both theoretical and applied work will be
considered. The deadline for submissions isOctober 24.More detail can be
foundhere.Corinna Cortes, Arthur Gretton, Gert Lanckriet, Mehryar Mohri,
Afshin Rostamizadeh</p><p>5 0.11198895 <a title="456-tfidf-5" href="../hunch_net-2007/hunch_net-2007-11-14-BellKor_wins_Netflix.html">272 hunch net-2007-11-14-BellKor wins Netflix</a></p>
<p>Introduction: â&euro;Ś but only the little prize. TheBellKor teamfocused on integrating predictions
from many different methods. The base methods consist of:Nearest Neighbor
MethodsMatrix Factorization Methods (asymmetric and symmetric)Linear
Regression on various feature spacesRestricted Boltzman MachinesThe final
predictor was an ensemble (as was reasonable to expect), although it's a
little bit more complicated than just a weighted average--it's essentially a
customized learning algorithm. Base approaches (1)-(3) seem like relatively
well-known approaches (although I haven't seen the asymmetric factorization
variant before). RBMs are the new approach.Thewriteupis pretty clear for more
details.The contestants are close to reaching the big prize, but the last 1.5%
is probably at least as hard as what's been done. A few new structurally
different methods for making predictions may need to be discovered and added
into the mixture. In other words, research may be required.</p><p>6 0.10648417 <a title="456-tfidf-6" href="../hunch_net-2009/hunch_net-2009-10-26-NIPS_workshops.html">375 hunch net-2009-10-26-NIPS workshops</a></p>
<p>7 0.10588897 <a title="456-tfidf-7" href="../hunch_net-2005/hunch_net-2005-08-01-Peekaboom.html">99 hunch net-2005-08-01-Peekaboom</a></p>
<p>8 0.1007801 <a title="456-tfidf-8" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>9 0.09743119 <a title="456-tfidf-9" href="../hunch_net-2005/hunch_net-2005-01-24-Holy_grails_of_machine_learning%3F.html">2 hunch net-2005-01-24-Holy grails of machine learning?</a></p>
<p>10 0.09545432 <a title="456-tfidf-10" href="../hunch_net-2006/hunch_net-2006-07-05-more_icml_papers.html">189 hunch net-2006-07-05-more icml papers</a></p>
<p>11 0.094060138 <a title="456-tfidf-11" href="../hunch_net-2005/hunch_net-2005-11-16-The_Everything_Ensemble_Edge.html">131 hunch net-2005-11-16-The Everything Ensemble Edge</a></p>
<p>12 0.092419535 <a title="456-tfidf-12" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>13 0.091391429 <a title="456-tfidf-13" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>14 0.088469364 <a title="456-tfidf-14" href="../hunch_net-2005/hunch_net-2005-07-04-The_Health_of_COLT.html">89 hunch net-2005-07-04-The Health of COLT</a></p>
<p>15 0.087874554 <a title="456-tfidf-15" href="../hunch_net-2005/hunch_net-2005-01-24-The_Humanloop_Spectrum_of_Machine_Learning.html">3 hunch net-2005-01-24-The Humanloop Spectrum of Machine Learning</a></p>
<p>16 0.086976141 <a title="456-tfidf-16" href="../hunch_net-2009/hunch_net-2009-08-27-New_York_Area_Machine_Learning_Events.html">369 hunch net-2009-08-27-New York Area Machine Learning Events</a></p>
<p>17 0.086565569 <a title="456-tfidf-17" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>18 0.086142801 <a title="456-tfidf-18" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>19 0.085418805 <a title="456-tfidf-19" href="../hunch_net-2007/hunch_net-2007-02-22-Create_Your_Own_ICML_Workshop.html">234 hunch net-2007-02-22-Create Your Own ICML Workshop</a></p>
<p>20 0.084536597 <a title="456-tfidf-20" href="../hunch_net-2006/hunch_net-2006-12-12-Interesting_Papers_at_NIPS_2006.html">224 hunch net-2006-12-12-Interesting Papers at NIPS 2006</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.206), (1, -0.004), (2, 0.023), (3, 0.097), (4, -0.148), (5, 0.052), (6, -0.122), (7, -0.045), (8, 0.062), (9, -0.042), (10, 0.089), (11, 0.022), (12, -0.045), (13, 0.104), (14, 0.109), (15, 0.018), (16, 0.023), (17, 0.026), (18, 0.003), (19, -0.032), (20, 0.035), (21, -0.075), (22, 0.073), (23, 0.071), (24, -0.013), (25, 0.005), (26, 0.083), (27, 0.071), (28, -0.026), (29, 0.025), (30, -0.042), (31, -0.037), (32, 0.001), (33, 0.023), (34, 0.095), (35, 0.064), (36, 0.043), (37, 0.023), (38, -0.005), (39, -0.076), (40, 0.077), (41, 0.024), (42, -0.008), (43, 0.054), (44, 0.029), (45, -0.051), (46, 0.038), (47, 0.112), (48, 0.03), (49, 0.019)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.92136133 <a title="456-lsi-1" href="../hunch_net-2012/hunch_net-2012-02-24-ICML%2B50%25.html">456 hunch net-2012-02-24-ICML+50%</a></p>
<p>Introduction: TheICMLpaper deadline has passed.Joelleand I were surprised to see the number
of submissions jump from last year by about 50% to around 900 submissions. A
tiny portion of these are immediate rejects(*), so this is a much larger set
of papers than expected. The number of workshop submissions also doubled
compared to last year, so ICML may grow significantly this year, if we can
manage to handle the load well. The prospect of making 900 good decisions is
fundamentally daunting, and success will rely heavily on theprogram
committeeandarea chairsat this point.For those who want to rubberneck a bit
more, here's a breakdown of submissions by primary topic of submitted
papers:66 Reinforcement Learning 52 Supervised Learning 51 Clustering 46
Kernel Methods 40 Optimization Algorithms 39 Feature Selection and
Dimensionality Reduction 33 Learning Theory 33 Graphical Models 33
Applications 29 Probabilistic Models 29 NN & Deep Learning 26 Transfer and
Multi-Task Learning 25 Online Learning 25 Activ</p><p>2 0.63422447 <a title="456-lsi-2" href="../hunch_net-2011/hunch_net-2011-07-11-Interesting_Neural_Network_Papers_at_ICML_2011.html">438 hunch net-2011-07-11-Interesting Neural Network Papers at ICML 2011</a></p>
<p>Introduction: Maybe it's too early to call, but with four separate Neural Network sessions
at this year'sICML, it looks like Neural Networks are making a comeback. Here
are my highlights of these sessions. In general, my feeling is that these
papers both demystify deep learning and show its broader applicability.The
first observation I made is that the once disreputable "Neural" nomenclature
is being used againin lieu of"deep learning". Maybe it's because Adam Coates
et al. showed that single layer networks can work surprisingly well.An
Analysis of Single-Layer Networks in Unsupervised Feature Learning,Adam
Coates,Honglak Lee,Andrew Y. Ng(AISTATS 2011)The Importance of Encoding Versus
Training with Sparse Coding and Vector Quantization,Adam Coates,Andrew Y.
Ng(ICML 2011)Another surprising result out of Andrew Ng's group comes from
Andrew Saxe et al. who show that certain convolutional pooling architectures
can obtain close to state-of-the-art performance with random weights (that is,
without actuall</p><p>3 0.59993351 <a title="456-lsi-3" href="../hunch_net-2008/hunch_net-2008-10-19-NIPS_2008_workshop_on_Kernel_Learning.html">321 hunch net-2008-10-19-NIPS 2008 workshop on Kernel Learning</a></p>
<p>Introduction: We'd like to invite hunch.net readers to participate in the NIPS 2008 workshop
on kernel learning. While the main focus is on automatically learning kernels
from data, we are also also looking at the broader questions of feature
selection, multi-task learning and multi-view learning. There are no
restrictions on the learning problem being addressed (regression,
classification, etc), and both theoretical and applied work will be
considered. The deadline for submissions isOctober 24.More detail can be
foundhere.Corinna Cortes, Arthur Gretton, Gert Lanckriet, Mehryar Mohri,
Afshin Rostamizadeh</p><p>4 0.59939778 <a title="456-lsi-4" href="../hunch_net-2012/hunch_net-2012-06-05-ICML_acceptance_statistics.html">466 hunch net-2012-06-05-ICML acceptance statistics</a></p>
<p>Introduction: People are naturally interested in slicing the ICML acceptance statistics in
various ways. Here's a rundown for the top categories.18/66 = 0.27in
(0.18,0.36)Reinforcement Learning10/52 = 0.19in (0.17,0.37)Supervised
Learning9/51 = 0.18not in (0.18, 0.37)Clustering12/46 = 0.26in (0.17,
0.37)Kernel Methods11/40 = 0.28in (0.15, 0.4)Optimization Algorithms8/33 =
0.24in (0.15, 0.39)Learning Theory14/33 = 0.42not in (0.15, 0.39)Graphical
Models10/32 = 0.31in (0.15, 0.41)Applications (+5 invited)8/29 = 0.28in (0.14,
0.41])Probabilistic Models13/29 = 0.45not in (0.14, 0.41)NN & Deep
Learning8/26 = 0.31in (0.12, 0.42)Transfer and Multi-Task Learning13/25 =
0.52not in (0.12, 0.44)Online Learning5/25 = 0.20in (0.12, 0.44)Active
Learning6/22 = 0.27in (0.14, 0.41)Semi-Supervised Learning7/20 = 0.35in (0.1,
0.45)Statistical Methods4/20 = 0.20in (0.1, 0.45)Sparsity and Compressed
Sensing1/19 = 0.05not in (0.11, 0.42)Ensemble Methods5/18 = 0.28in (0.11,
0.44)Structured Output Prediction4/18 = 0.22in (</p><p>5 0.57520789 <a title="456-lsi-5" href="../hunch_net-2008/hunch_net-2008-11-28-A_Bumper_Crop_of_Machine_Learning_Graduates.html">329 hunch net-2008-11-28-A Bumper Crop of Machine Learning Graduates</a></p>
<p>Introduction: My impression is that this is a particularly strong year for machine learning
graduates. Here's my short list of the strong graduates I know. Analpha (for
perversity's sake) by last name:Jenn Wortmann. When Jenn visited us for the
summer, she hadone,two,three,fourpapers. That is typical--she's smart,
capable, and follows up many directions of research. I believe approximately
all of her many papers are on different subjects.Ruslan Salakhutdinov.
AScience paper on bijective dimensionality reduction, mastered and improved on
deep belief nets which seems like an important flavor of nonlinear learning,
and in my experience he's very fast, capable and creative at problem
solving.Marc'Aurelio Ranzato. I haven't spoken with Marc very much, but he had
a great visit at Yahoo! this summer, and has an impressive portfolio of
applications and improvements on convolutional neural networks and other deep
learning algorithms.Lihong Li. Lihong developed theKWIK ("Knows what it
Knows") learning framewo</p><p>6 0.55386931 <a title="456-lsi-6" href="../hunch_net-2007/hunch_net-2007-11-14-BellKor_wins_Netflix.html">272 hunch net-2007-11-14-BellKor wins Netflix</a></p>
<p>7 0.54251707 <a title="456-lsi-7" href="../hunch_net-2005/hunch_net-2005-12-11-More_NIPS_Papers.html">139 hunch net-2005-12-11-More NIPS Papers</a></p>
<p>8 0.52047575 <a title="456-lsi-8" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>9 0.51887274 <a title="456-lsi-9" href="../hunch_net-2006/hunch_net-2006-07-05-more_icml_papers.html">189 hunch net-2006-07-05-more icml papers</a></p>
<p>10 0.51805121 <a title="456-lsi-10" href="../hunch_net-2008/hunch_net-2008-07-15-Interesting_papers_at_COLT_%28and_a_bit_of_UAI_%26%23038%3B_workshops%29.html">310 hunch net-2008-07-15-Interesting papers at COLT (and a bit of UAI &#038; workshops)</a></p>
<p>11 0.51472241 <a title="456-lsi-11" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>12 0.51148248 <a title="456-lsi-12" href="../hunch_net-2005/hunch_net-2005-01-24-The_Humanloop_Spectrum_of_Machine_Learning.html">3 hunch net-2005-01-24-The Humanloop Spectrum of Machine Learning</a></p>
<p>13 0.50366372 <a title="456-lsi-13" href="../hunch_net-2005/hunch_net-2005-05-29-Maximum_Margin_Mismatch%3F.html">77 hunch net-2005-05-29-Maximum Margin Mismatch?</a></p>
<p>14 0.50167096 <a title="456-lsi-14" href="../hunch_net-2007/hunch_net-2007-12-20-Cool_and_Interesting_things_at_NIPS%2C_take_three.html">280 hunch net-2007-12-20-Cool and Interesting things at NIPS, take three</a></p>
<p>15 0.49773335 <a title="456-lsi-15" href="../hunch_net-2007/hunch_net-2007-06-19-How_is_Compressed_Sensing_going_to_change_Machine_Learning_%3F.html">248 hunch net-2007-06-19-How is Compressed Sensing going to change Machine Learning ?</a></p>
<p>16 0.49425867 <a title="456-lsi-16" href="../hunch_net-2013/hunch_net-2013-01-01-Deep_Learning_2012.html">477 hunch net-2013-01-01-Deep Learning 2012</a></p>
<p>17 0.49417627 <a title="456-lsi-17" href="../hunch_net-2009/hunch_net-2009-01-21-Nearly_all_natural_problems_require_nonlinearity.html">337 hunch net-2009-01-21-Nearly all natural problems require nonlinearity</a></p>
<p>18 0.49161261 <a title="456-lsi-18" href="../hunch_net-2006/hunch_net-2006-06-30-ICML_papers.html">188 hunch net-2006-06-30-ICML papers</a></p>
<p>19 0.49135941 <a title="456-lsi-19" href="../hunch_net-2005/hunch_net-2005-02-27-Antilearning%3A_When_proximity_goes_bad.html">32 hunch net-2005-02-27-Antilearning: When proximity goes bad</a></p>
<p>20 0.48939174 <a title="456-lsi-20" href="../hunch_net-2006/hunch_net-2006-07-08-Some_recent_papers.html">192 hunch net-2006-07-08-Some recent papers</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(29, 0.018), (35, 0.033), (42, 0.187), (45, 0.06), (68, 0.054), (69, 0.011), (74, 0.104), (82, 0.041), (88, 0.016), (97, 0.391)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.81483495 <a title="456-lda-1" href="../hunch_net-2009/hunch_net-2009-01-19-Netflix_prize_within_epsilon.html">336 hunch net-2009-01-19-Netflix prize within epsilon</a></p>
<p>Introduction: The competitors for theNetflix Prizeare tantalizingly close winning the
million dollar prize. This year,BellKorandCommendo Researchsent a combined
solution that won theprogress prize. Reading thewriteups2is instructive.
Several aspects of solutions are taken for granted including stochastic
gradient descent, ensemble prediction, and targeting residuals (a form of
boosting). Relatively to last year, it appears that many approaches have added
parameterizations, especially for the purpose of modeling through time.The big
question is: will they make the big prize? At this point, the level of
complexity in entering the competition is prohibitive, so perhaps only the
existing competitors will continue to try. (This equation might change
drastically if the teams open source their existing solutions, including
parameter settings.) One fear is that the progress is asymptoting on the wrong
side of the 10% threshold. In the first year, the teams progressed through
84.3% of the 10% gap, and in the</p><p>2 0.81332952 <a title="456-lda-2" href="../hunch_net-2007/hunch_net-2007-04-28-The_Coming_Patent_Apocalypse.html">241 hunch net-2007-04-28-The Coming Patent Apocalypse</a></p>
<p>Introduction: Many people in computer science believe that patents are problematic. The
truth is even worse--the patent system in the US is fundamentally broken in
ways that will require much more significant reform thanis being considered
now.The myth of the patent is the following: Patents are a mechanism for
inventors to be compensated according to the value of their inventions while
making the invention available to all. This myth sounds pretty desirable, but
the reality is a strange distortion slowly leading towards collapse.There are
many problems associated with patents, but I would like to focus on just two
of them:Patent TrollsThe way that patents have generally worked over the last
several decades is that they were a tool of large companies. Large companies
would amass a large number of patents and then cross-license each other's
patents--in effect saying "we agree to owe each other nothing". Smaller
companies would sometimes lose in this game, essentially because they didn't
have enough p</p><p>same-blog 3 0.80956084 <a title="456-lda-3" href="../hunch_net-2012/hunch_net-2012-02-24-ICML%2B50%25.html">456 hunch net-2012-02-24-ICML+50%</a></p>
<p>Introduction: TheICMLpaper deadline has passed.Joelleand I were surprised to see the number
of submissions jump from last year by about 50% to around 900 submissions. A
tiny portion of these are immediate rejects(*), so this is a much larger set
of papers than expected. The number of workshop submissions also doubled
compared to last year, so ICML may grow significantly this year, if we can
manage to handle the load well. The prospect of making 900 good decisions is
fundamentally daunting, and success will rely heavily on theprogram
committeeandarea chairsat this point.For those who want to rubberneck a bit
more, here's a breakdown of submissions by primary topic of submitted
papers:66 Reinforcement Learning 52 Supervised Learning 51 Clustering 46
Kernel Methods 40 Optimization Algorithms 39 Feature Selection and
Dimensionality Reduction 33 Learning Theory 33 Graphical Models 33
Applications 29 Probabilistic Models 29 NN & Deep Learning 26 Transfer and
Multi-Task Learning 25 Online Learning 25 Activ</p><p>4 0.78205776 <a title="456-lda-4" href="../hunch_net-2006/hunch_net-2006-09-09-How_to_solve_an_NP_hard_problem_in_quadratic_time.html">206 hunch net-2006-09-09-How to solve an NP hard problem in quadratic time</a></p>
<p>Introduction: This title is a lie, but it is a special lie which has a bit of
truth.Ifnplayers each play each other, you have a tournament. How do you order
the players from weakest to strongest?The standard first attempt is "find the
ordering which agrees with the tournament on as many player pairs as
possible". This is called the "minimum feedback arcset" problem in the CS
theory literature and it is a well known NP-hard problem. A basic guarantee
holds for the solution to this problem: if there is some "true" intrinsic
ordering, and the outcome of the tournament disagreesktimes (due to noise for
instance), then the output ordering will disagree with the original ordering
on at most2kedges (and no solution can be better).One standard approach to
tractably solving an NP-hard problem is to find another algorithm with an
approximation guarantee. For example,Don Coppersmith,Lisa FleischerandAtri
Rudraproved thatordering players according to the number of wins is a
5-approximation to the NP-hard proble</p><p>5 0.73663771 <a title="456-lda-5" href="../hunch_net-2008/hunch_net-2008-03-15-COLT_Open_Problems.html">292 hunch net-2008-03-15-COLT Open Problems</a></p>
<p>Introduction: COLT has acall for open problemsdue March 21. I encourage anyone with a
specifiable open problem to write it down and send it in. Just the effort of
specifying an open problem precisely and concisely has been very helpful for
my own solutions, and there is a substantial chance others will solve it. To
increase the chance someone will take it up, you can even put a bounty on the
solution. (Perhaps I should raise the$500 bountyon theK-fold cross-validation
problemas it hasn't yet been solved).</p><p>6 0.71588182 <a title="456-lda-6" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<p>7 0.55763978 <a title="456-lda-7" href="../hunch_net-2012/hunch_net-2012-06-05-ICML_acceptance_statistics.html">466 hunch net-2012-06-05-ICML acceptance statistics</a></p>
<p>8 0.52163649 <a title="456-lda-8" href="../hunch_net-2007/hunch_net-2007-08-19-Choice_of_Metrics.html">259 hunch net-2007-08-19-Choice of Metrics</a></p>
<p>9 0.51788771 <a title="456-lda-9" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>10 0.50568587 <a title="456-lda-10" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>11 0.50396502 <a title="456-lda-11" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>12 0.50365371 <a title="456-lda-12" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<p>13 0.50236571 <a title="456-lda-13" href="../hunch_net-2005/hunch_net-2005-12-07-Is_the_Google_way_the_way_for_machine_learning%3F.html">136 hunch net-2005-12-07-Is the Google way the way for machine learning?</a></p>
<p>14 0.50158995 <a title="456-lda-14" href="../hunch_net-2009/hunch_net-2009-02-22-Effective_Research_Funding.html">344 hunch net-2009-02-22-Effective Research Funding</a></p>
<p>15 0.50144142 <a title="456-lda-15" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>16 0.50073236 <a title="456-lda-16" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>17 0.50069761 <a title="456-lda-17" href="../hunch_net-2005/hunch_net-2005-02-12-ROC_vs._Accuracy_vs._AROC.html">18 hunch net-2005-02-12-ROC vs. Accuracy vs. AROC</a></p>
<p>18 0.49999633 <a title="456-lda-18" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>19 0.49937367 <a title="456-lda-19" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>20 0.49817187 <a title="456-lda-20" href="../hunch_net-2011/hunch_net-2011-10-24-2011_ML_symposium_and_the_bears.html">448 hunch net-2011-10-24-2011 ML symposium and the bears</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
