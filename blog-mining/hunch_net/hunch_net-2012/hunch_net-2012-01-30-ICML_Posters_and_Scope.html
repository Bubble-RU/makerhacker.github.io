<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>454 hunch net-2012-01-30-ICML Posters and Scope</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2012" href="../home/hunch_net-2012_home.html">hunch_net-2012</a> <a title="hunch_net-2012-454" href="#">hunch_net-2012-454</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>454 hunch net-2012-01-30-ICML Posters and Scope</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2012-454-html" href="http://hunch.net/?p=2246">html</a></p><p>Introduction: Normally, I don't indulge in posters forICML, but this year is naturally an
exception for me. If you want one, there are a small numberleft here, if you
sign up before February.It also seems worthwhile to give some sense of the
scope and reviewing criteria for ICML for authors considering submitting
papers. At ICML, the (very large) program committee does the reviewing which
informs final decisions by area chairs on most papers. Program chairs setup
the process, deal with exceptions or disagreements, and provide advice for the
reviewing process. Providing advice is tricky (and easily misleading) because
a conference is a community, and in the end the aggregate interests of the
community determine the conference. Nevertheless, as a program chair this year
it seems worthwhile to state the overall philosophy I have and what I plan to
encourage (and occasionally discourage).At the highest level, I believe ICML
exists to further research into machine learning, which I generally think of
as</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 It also seems worthwhile to give some sense of the scope and reviewing criteria for ICML for authors considering submitting papers. [sent-3, score-0.517]
</p><p>2 Program chairs setup the process, deal with exceptions or disagreements, and provide advice for the reviewing process. [sent-5, score-0.418]
</p><p>3 " Exhibiting new things that you can do with ML provides good reference points for what is possible, provides a sense of what works, and compelling new ideas about what to work on can be valuable to the community. [sent-20, score-0.376]
</p><p>4 New AlgorithmsOften, authors find that existing learning algorithms for solving some problem are lacking in some way, so they propose new better algorithms. [sent-24, score-0.775]
</p><p>5 This is plausibly the most common category of paper at ICML, so there is quite a bit of variety. [sent-25, score-0.349]
</p><p>6 For these papers it's important to have an empirical comparison to existing baselines. [sent-27, score-0.539]
</p><p>7 Some authors use synthetic datasets which do not seem significant to me, because good results on such datasets may not transfer to real-world problems well as the real world tends to be quite a bit more complex than the synthetic processes which are natural to program. [sent-29, score-0.908]
</p><p>8 One problem with relying on real datasets is dataset selection--choosing the dataset for which your algorithm seems to perform best. [sent-31, score-0.469]
</p><p>9 This is tricky, because three reviewers might consider three different baselines the most natural one. [sent-34, score-0.388]
</p><p>10 Asking around a bit when developing the paper might help here, but in the end this can be a tough judgement call: Is the paper convincing enough that people interested in solving the problem should use this algorithm? [sent-35, score-0.735]
</p><p>11 Another class of new algorithms papers is new algorithms for new areas of machine learning, blending into the previous category. [sent-36, score-0.782]
</p><p>12 For papers like this, one way I've seen difficulties is when authors are very invested in a particular approach to solving the problem. [sent-38, score-0.438]
</p><p>13 Another difficulty I've observed is reviewers used to the well-studied problems reject an interesting paper because (essentially) they assume that the authors left out a good baseline which does not exist. [sent-40, score-0.6]
</p><p>14 To prevent the first, authors who ask around might get some valuable early feedback. [sent-41, score-0.426]
</p><p>15 Algorithmic studiesA relatively rare but potentially valuable form of paper is an algorithmic study. [sent-43, score-0.429]
</p><p>16 Here, the authors do not propose a new algorithm, but instead do a comprehensive empirical comparison of different algorithms. [sent-44, score-0.797]
</p><p>17 The standards here are quite high--the empirical comparison needs to be first-class to convince people, so the empirical comparison comments under new algorithms apply strongly. [sent-45, score-0.954]
</p><p>18 I am personally most interested in theory that helps us design new learning algorithms, but broadly interested in what is possible. [sent-48, score-0.447]
</p><p>19 In essence, authors who choose to analyze an existing algorithm are sometimes forced to make many unnatural assumptions for the theory to be correct. [sent-53, score-0.509]
</p><p>20 Reviewers can fall into a trap where they are most interested in 1 of the 4 questions answered above, and find 1/4 of the paper devoted to their question relatively weak compared to the paper that devotes all the pages to the same question. [sent-57, score-0.765]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('authors', 0.239), ('comparison', 0.187), ('icml', 0.173), ('exceptions', 0.157), ('empirical', 0.144), ('paper', 0.142), ('papers', 0.132), ('new', 0.127), ('datasets', 0.125), ('valuable', 0.122), ('answered', 0.12), ('reviewing', 0.115), ('reviewers', 0.112), ('theory', 0.11), ('interesting', 0.107), ('questions', 0.106), ('synthetic', 0.105), ('interested', 0.105), ('propose', 0.1), ('algorithms', 0.098), ('dataset', 0.096), ('rare', 0.093), ('generally', 0.09), ('criteria', 0.09), ('interests', 0.085), ('conference', 0.084), ('algorithm', 0.084), ('asking', 0.083), ('tricky', 0.081), ('question', 0.078), ('call', 0.078), ('existing', 0.076), ('broad', 0.076), ('advice', 0.075), ('encourage', 0.075), ('worthwhile', 0.073), ('areas', 0.073), ('help', 0.073), ('bit', 0.073), ('relatively', 0.072), ('program', 0.072), ('three', 0.071), ('chairs', 0.071), ('natural', 0.069), ('research', 0.068), ('problem', 0.068), ('solving', 0.067), ('plausibly', 0.067), ('quite', 0.067), ('might', 0.065)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000002 <a title="454-tfidf-1" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>Introduction: Normally, I don't indulge in posters forICML, but this year is naturally an
exception for me. If you want one, there are a small numberleft here, if you
sign up before February.It also seems worthwhile to give some sense of the
scope and reviewing criteria for ICML for authors considering submitting
papers. At ICML, the (very large) program committee does the reviewing which
informs final decisions by area chairs on most papers. Program chairs setup
the process, deal with exceptions or disagreements, and provide advice for the
reviewing process. Providing advice is tricky (and easily misleading) because
a conference is a community, and in the end the aggregate interests of the
community determine the conference. Nevertheless, as a program chair this year
it seems worthwhile to state the overall philosophy I have and what I plan to
encourage (and occasionally discourage).At the highest level, I believe ICML
exists to further research into machine learning, which I generally think of
as</p><p>2 0.28443658 <a title="454-tfidf-2" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>Introduction: Here's a quick reference for summer ML-related conferences sorted by due
date:ConferenceDue dateLocationReviewingKDDFeb 10August 12-16, Beijing,
ChinaSingle BlindCOLTFeb 14June 25-June 27, Edinburgh, ScotlandSingle Blind?
(historically)ICMLFeb 24June 26-July 1, Edinburgh, ScotlandDouble Blind,
author response, zeroSPOFUAIMarch 30August 15-17, Catalina Islands,
CaliforniaDouble Blind, author responseGeographically, this is greatly
dispersed and the UAI/KDD conflict is unfortunate.Machine Learning conferences
are triannual now, betweenNIPS,AIStat, andICML. This has not always been the
case: the academic default is annual summer conferences, then NIPS started
with a December conference, and now AIStat has grown into an April
conference.However, the first claim is not quite correct. NIPS and AIStat have
few competing venues while ICML implicitly competes with many other
conferences accepting machine learning related papers. SinceJoelleand I are
taking a turn as program chairs this year, I</p><p>3 0.25751624 <a title="454-tfidf-3" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>Introduction: When thinking about how best to review papers, it seems helpful to have some
conception of what good reviewing is. As far as I can tell, this is almost
always only discussed in the specific context of a paper (i.e. your rejected
paper), or at most an area (i.e. what a "good paper" looks like for that area)
rather than general principles. Neither individual papers or areas are
sufficiently general for a large conference--every paper differs in the
details, and what if you want to build a new area and/or cross areas?An
unavoidable reason for reviewing is that the community of research is too
large. In particular, it is not possible for a researcher to read every paper
which someone thinks might be of interest. This reason for reviewing exists
independent of constraints on rooms or scheduling formats of individual
conferences. Indeed, history suggests that physical constraints are relatively
meaningless over the long term -- growing conferences simply use more rooms
and/or change formats</p><p>4 0.25613692 <a title="454-tfidf-4" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>Introduction: Unfortunately, I ended up sick for much of this ICML. I did manage to catch
one interesting paper:Richard Socher,Cliff Lin,Andrew Y. Ng, andChristopher D.
ManningParsing Natural Scenes and Natural Language with Recursive Neural
Networks.I invited Richard to share his list of interesting papers, so
hopefully we'll hear from him soon. In the meantime,PaulandHalhave posted some
lists.the futureJoelleand I are program chairs for ICML 2012 inEdinburgh,
which I previously enjoyed visiting in2005. This is a huge responsibility,
that we hope to accomplish well. A part of this (perhaps the most fun part),
is imagining how we can make ICML better. A key and critical constraint is
choosing things that can be accomplished. So far we have:Colocation. The first
thing we looked into was potential colocations. We quickly discovered that
many other conferences precomitted their location. For the future, getting a
colocation withACLorSIGIR, seems to require more advanced planning. If that
can be done, I</p><p>5 0.24532013 <a title="454-tfidf-5" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<p>Introduction: ByShieandNatiFollowing John's advertisement for submitting to ICML, we thought
it appropriate to highlight the advantages of COLT, and the reasons it is
often the best place for theory papers. We would like to emphasize that we
both respect ICML, and are active in ICML, both as authors and as area chairs,
and certainly are not arguing that ICML is a bad place for your papers. For
many papers, ICML is the best venue. But for many theory papers, COLT is a
better and more appropriate place.Why should you submit to COLT?By-and-large,
theory papers go to COLT. This is the tradition of the field and most theory
papers are sent to COLT. This is the place to present your ground-breaking
theorems and new models that will shape the theory of machine learning. COLT
is more focused then ICML with a single track session. Unlike ICML, the norm
in COLT is for people to sit through most sessions, and hear most of the talks
presented. There is also often a lively discussion following paper
presentation</p><p>6 0.2327653 <a title="454-tfidf-6" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>7 0.22945641 <a title="454-tfidf-7" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>8 0.22293513 <a title="454-tfidf-8" href="../hunch_net-2008/hunch_net-2008-11-10-ICML_Reviewing_Criteria.html">325 hunch net-2008-11-10-ICML Reviewing Criteria</a></p>
<p>9 0.21987246 <a title="454-tfidf-9" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>10 0.21510346 <a title="454-tfidf-10" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>11 0.21228042 <a title="454-tfidf-11" href="../hunch_net-2010/hunch_net-2010-04-26-Compassionate_Reviewing.html">395 hunch net-2010-04-26-Compassionate Reviewing</a></p>
<p>12 0.19307859 <a title="454-tfidf-12" href="../hunch_net-2008/hunch_net-2008-10-14-Who_is_Responsible_for_a_Bad_Review%3F.html">320 hunch net-2008-10-14-Who is Responsible for a Bad Review?</a></p>
<p>13 0.19181184 <a title="454-tfidf-13" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>14 0.18943289 <a title="454-tfidf-14" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>15 0.18881555 <a title="454-tfidf-15" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>16 0.18782428 <a title="454-tfidf-16" href="../hunch_net-2012/hunch_net-2012-04-09-ICML_author_feedback_is_open.html">461 hunch net-2012-04-09-ICML author feedback is open</a></p>
<p>17 0.17850177 <a title="454-tfidf-17" href="../hunch_net-2008/hunch_net-2008-09-03-Bidding_Problems.html">315 hunch net-2008-09-03-Bidding Problems</a></p>
<p>18 0.17437105 <a title="454-tfidf-18" href="../hunch_net-2005/hunch_net-2005-03-13-Avoiding_Bad_Reviewing.html">40 hunch net-2005-03-13-Avoiding Bad Reviewing</a></p>
<p>19 0.17236017 <a title="454-tfidf-19" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>20 0.16430229 <a title="454-tfidf-20" href="../hunch_net-2005/hunch_net-2005-05-02-Reviewing_techniques_for_conferences.html">65 hunch net-2005-05-02-Reviewing techniques for conferences</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.444), (1, 0.198), (2, -0.097), (3, -0.043), (4, -0.058), (5, 0.026), (6, -0.023), (7, 0.018), (8, -0.01), (9, 0.015), (10, -0.035), (11, -0.021), (12, 0.087), (13, 0.02), (14, -0.001), (15, 0.057), (16, 0.033), (17, -0.066), (18, 0.091), (19, 0.049), (20, 0.038), (21, -0.051), (22, 0.053), (23, 0.016), (24, 0.009), (25, 0.016), (26, -0.004), (27, -0.003), (28, 0.002), (29, 0.006), (30, -0.009), (31, 0.038), (32, -0.088), (33, -0.009), (34, -0.083), (35, 0.017), (36, -0.052), (37, 0.063), (38, -0.014), (39, -0.052), (40, -0.037), (41, 0.036), (42, -0.001), (43, 0.008), (44, -0.014), (45, -0.002), (46, -0.033), (47, -0.099), (48, -0.024), (49, -0.003)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9759782 <a title="454-lsi-1" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>Introduction: Normally, I don't indulge in posters forICML, but this year is naturally an
exception for me. If you want one, there are a small numberleft here, if you
sign up before February.It also seems worthwhile to give some sense of the
scope and reviewing criteria for ICML for authors considering submitting
papers. At ICML, the (very large) program committee does the reviewing which
informs final decisions by area chairs on most papers. Program chairs setup
the process, deal with exceptions or disagreements, and provide advice for the
reviewing process. Providing advice is tricky (and easily misleading) because
a conference is a community, and in the end the aggregate interests of the
community determine the conference. Nevertheless, as a program chair this year
it seems worthwhile to state the overall philosophy I have and what I plan to
encourage (and occasionally discourage).At the highest level, I believe ICML
exists to further research into machine learning, which I generally think of
as</p><p>2 0.7990545 <a title="454-lsi-2" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>Introduction: Unfortunately, I ended up sick for much of this ICML. I did manage to catch
one interesting paper:Richard Socher,Cliff Lin,Andrew Y. Ng, andChristopher D.
ManningParsing Natural Scenes and Natural Language with Recursive Neural
Networks.I invited Richard to share his list of interesting papers, so
hopefully we'll hear from him soon. In the meantime,PaulandHalhave posted some
lists.the futureJoelleand I are program chairs for ICML 2012 inEdinburgh,
which I previously enjoyed visiting in2005. This is a huge responsibility,
that we hope to accomplish well. A part of this (perhaps the most fun part),
is imagining how we can make ICML better. A key and critical constraint is
choosing things that can be accomplished. So far we have:Colocation. The first
thing we looked into was potential colocations. We quickly discovered that
many other conferences precomitted their location. For the future, getting a
colocation withACLorSIGIR, seems to require more advanced planning. If that
can be done, I</p><p>3 0.78418195 <a title="454-lsi-3" href="../hunch_net-2008/hunch_net-2008-11-10-ICML_Reviewing_Criteria.html">325 hunch net-2008-11-10-ICML Reviewing Criteria</a></p>
<p>Introduction: Michael LittmanandLeon Bottouhave decided to use a franchise program chair
approach toreviewing at ICMLthis year. I'll be one of the area chairs, so I
wanted to mention a few things if you are thinking about naming me.I take
reviewing seriously. That means papers to be reviewed are read, the
implications are considered, and decisions are only made after that. I do my
best to be fair, and there are zero subjects that I consider categorical
rejects. I don't consider severalarguments for rejection-not-on-the-merits
reasonable.I am generally interested in papers that (a) analyze new models of
machine learning, (b) provide new algorithms, and (c) show that they work
empirically on plausibly real problems. If a paper has the trifecta, I'm
particularly interested. With 2 out of 3, I might be interested. I often find
papers with only one element harder to accept, including papers with just
(a).I'm a bit tough. I rarely jump-up-and-down about a paper, because I
believe that great progress is ra</p><p>4 0.77881795 <a title="454-lsi-4" href="../hunch_net-2007/hunch_net-2007-07-20-Motivation_should_be_the_Responsibility_of_the_Reviewer.html">256 hunch net-2007-07-20-Motivation should be the Responsibility of the Reviewer</a></p>
<p>Introduction: The prevailing wisdom in machine learning seems to be that motivating a paper
is the responsibility of the author. I think this is a harmful view--instead,
it's healthier for the community to regard this as the responsibility of the
reviewer.There are lots of reasons to prefer a reviewer-responsibility
approach.Authors are the most biased possible source of information about the
motivation of the paper. Systems which rely upon very biased sources of
information are inherently unreliable.Authors are highly variable in their
ability and desire to express motivation for their work. This adds greatly to
variance on acceptance of an idea, and it can systematically discriminate or
accentuate careers. It's great if you have a career accentuated by awesome
wording choice, but wise decision making by reviewers is important for the
field.The motivation section in a paper doesn'tdoanything in some sense--it's
there to get the paper in. Reading the motivation of a paper is of little use
in helping</p><p>5 0.77401912 <a title="454-lsi-5" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>Introduction: Claireasked me to be on the SODA program committee this year, which was quite
a bit of work.I had a relatively light load--merely 49 theory papers. Many of
these papers were not on subjects that I was expert about, so (as is common
for theory conferences) I found various reviewers that I trusted to help
review the papers. I ended up reviewing about 1/3 personally. There were a
couple instances where I ended up overruling a subreviewer whose logic seemed
off, but otherwise I generally let their reviews stand.There are some
differences in standards for paper reviews between the machine learning and
theory communities. In machine learning it is expected that a review be
detailed, while in the theory community this is often not the case. Every
paper given to me ended up with a review varying between somewhat and very
detailed.I'm sure not every author was happy with the outcome. While we did
our best to make good decisions, they were difficult decisions to make. For
example, if there is a</p><p>6 0.76992965 <a title="454-lsi-6" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>7 0.76486671 <a title="454-lsi-7" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>8 0.75460172 <a title="454-lsi-8" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>9 0.74377447 <a title="454-lsi-9" href="../hunch_net-2010/hunch_net-2010-04-26-Compassionate_Reviewing.html">395 hunch net-2010-04-26-Compassionate Reviewing</a></p>
<p>10 0.73075044 <a title="454-lsi-10" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>11 0.72282261 <a title="454-lsi-11" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>12 0.72087795 <a title="454-lsi-12" href="../hunch_net-2006/hunch_net-2006-09-12-Incentive_Compatible_Reviewing.html">207 hunch net-2006-09-12-Incentive Compatible Reviewing</a></p>
<p>13 0.71732354 <a title="454-lsi-13" href="../hunch_net-2008/hunch_net-2008-09-03-Bidding_Problems.html">315 hunch net-2008-09-03-Bidding Problems</a></p>
<p>14 0.71393937 <a title="454-lsi-14" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>15 0.70824313 <a title="454-lsi-15" href="../hunch_net-2005/hunch_net-2005-04-04-Grounds_for_Rejection.html">52 hunch net-2005-04-04-Grounds for Rejection</a></p>
<p>16 0.70629799 <a title="454-lsi-16" href="../hunch_net-2007/hunch_net-2007-02-16-The_Forgetting.html">233 hunch net-2007-02-16-The Forgetting</a></p>
<p>17 0.70441514 <a title="454-lsi-17" href="../hunch_net-2008/hunch_net-2008-10-14-Who_is_Responsible_for_a_Bad_Review%3F.html">320 hunch net-2008-10-14-Who is Responsible for a Bad Review?</a></p>
<p>18 0.70402896 <a title="454-lsi-18" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>19 0.70203006 <a title="454-lsi-19" href="../hunch_net-2006/hunch_net-2006-05-05-An_ICML_reject.html">177 hunch net-2006-05-05-An ICML reject</a></p>
<p>20 0.69182593 <a title="454-lsi-20" href="../hunch_net-2006/hunch_net-2006-12-04-Structural_Problems_in_NIPS_Decision_Making.html">221 hunch net-2006-12-04-Structural Problems in NIPS Decision Making</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.024), (35, 0.042), (42, 0.26), (45, 0.051), (50, 0.024), (68, 0.038), (69, 0.015), (74, 0.194), (76, 0.185), (82, 0.023), (83, 0.01), (88, 0.013), (95, 0.056)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.98096496 <a title="454-lda-1" href="../hunch_net-2008/hunch_net-2008-07-02-Proprietary_Data_in_Academic_Research%3F.html">306 hunch net-2008-07-02-Proprietary Data in Academic Research?</a></p>
<p>Introduction: Should results of experiments on proprietary datasets be in the academic
research literature?The arguments I can imagine in the "against" column
are:Experiments are not repeatable. Repeatability in experiments is essential
to science because it allows others to compare new methods with old and
discover which is better.It's unfair. Academics who don't have insider access
to proprietary data are at a substantial disadvantage when competing with
others who do.I'm unsympathetic to argument (2). To me, it looks like their
are simply some resource constraints, and these should not prevent research
progress. For example, we wouldn't prevent publishing about particle
accelerator experiments by physicists atCERNbecause physicists atCMUcouldn't
run their own experiments.Argument (1) seems like a real issue.The argument
for is:Yes, they are another form of evidence that an algorithm is good. The
degree to which they are evidence is less than for publicly repeatable
experiments, but greater than n</p><p>2 0.94616115 <a title="454-lda-2" href="../hunch_net-2006/hunch_net-2006-02-18-Multiplication_of_Learned_Probabilities_is_Dangerous.html">157 hunch net-2006-02-18-Multiplication of Learned Probabilities is Dangerous</a></p>
<p>Introduction: This is about a design flaw in several learning algorithms such as the Naive
Bayes classifier and Hidden Markov Models. A number of people are aware of it,
but it seems that not everyone is.Several learning systems have the property
that they estimate some conditional probabilitiesP(event | other events)either
explicitly or implicitly. Then, at prediction time, these learned
probabilities are multiplied together according to some formula to produce a
final prediction. The Naive Bayes classifier for binary data is the simplest
of these, so it seems like a good example.When Naive Bayes is used, a set of
probabilities of the formPr'(feature i | label)are estimated via counting
statistics and some prior. Predictions are made according to the label
maximizing:Pr'(label) * Productfeatures iPr'(feature i | label)(ThePr'notation
indicates these are estimated values.)There is nothing wrong with this method
as long as (a) the prior for the sample counts is very strong and (b) the
prior (on the c</p><p>same-blog 3 0.93245244 <a title="454-lda-3" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>Introduction: Normally, I don't indulge in posters forICML, but this year is naturally an
exception for me. If you want one, there are a small numberleft here, if you
sign up before February.It also seems worthwhile to give some sense of the
scope and reviewing criteria for ICML for authors considering submitting
papers. At ICML, the (very large) program committee does the reviewing which
informs final decisions by area chairs on most papers. Program chairs setup
the process, deal with exceptions or disagreements, and provide advice for the
reviewing process. Providing advice is tricky (and easily misleading) because
a conference is a community, and in the end the aggregate interests of the
community determine the conference. Nevertheless, as a program chair this year
it seems worthwhile to state the overall philosophy I have and what I plan to
encourage (and occasionally discourage).At the highest level, I believe ICML
exists to further research into machine learning, which I generally think of
as</p><p>4 0.93206084 <a title="454-lda-4" href="../hunch_net-2006/hunch_net-2006-03-23-The_Approximation_Argument.html">165 hunch net-2006-03-23-The Approximation Argument</a></p>
<p>Introduction: An argument is sometimes made that the Bayesian way is the "right" way to do
machine learning. This is a serious argument which deserves a serious reply.
The approximation argument is a serious reply for which I have not yet seen a
reply2.The idea for the Bayesian approach is quite simple, elegant, and
general. Essentially, you first specify a priorP(D)over possible
processesDproducing the data, observe the data, then condition on the data
according to Bayes law to construct a posterior:P(D|x) = P(x|D)P(D)/P(x)After
this, hard decisions are made (such as "turn left" or "turn right") by
choosing the one which minimizes the expected (with respect to the posterior)
loss.This basic idea is reused thousands of times with various choices
ofP(D)and loss functions which is unsurprising given the many nice
properties:There is an extremely strong associated guarantee: If the actual
distribution generating the data is drawn fromP(D)there is no better method.
One way to think about this is that in</p><p>5 0.92666078 <a title="454-lda-5" href="../hunch_net-2010/hunch_net-2010-10-28-NY_ML_Symposium_2010.html">415 hunch net-2010-10-28-NY ML Symposium 2010</a></p>
<p>Introduction: About 200 people attended the2010 NYAS ML Symposiumthis year. (It wasabout 170
last year.) I particularly enjoyed several talks.Yannhas a new live demo of
(limited) real-time object recognition learning.Sanjoygave a fairly convincing
and comprehensible explanation of why amodified form of single-linkage
clusteringis consistent in higher dimensions, and why consistency is a
critical feature for clustering algorithms. I'm curious how well this
algorithm works in practice.Matt Hoffman's poster covering online LDA seemed
pretty convincing to me as an algorithmic improvement.This year, we allocated
more time towards posters & poster spotlights.For next year, we are
considering some further changes. The format has traditionally been 4 invited
Professor speakers, with posters and poster spotlight for students. Demand
from other parties to participate is growing, for example from postdocs and
startups in the area. Another growing concern is the facility--the location is
exceptional, but fittin</p><p>6 0.92206115 <a title="454-lda-6" href="../hunch_net-2009/hunch_net-2009-06-03-Functionally_defined_Nonlinear_Dynamic_Models.html">359 hunch net-2009-06-03-Functionally defined Nonlinear Dynamic Models</a></p>
<p>7 0.8812027 <a title="454-lda-7" href="../hunch_net-2009/hunch_net-2009-02-22-Effective_Research_Funding.html">344 hunch net-2009-02-22-Effective Research Funding</a></p>
<p>8 0.87806702 <a title="454-lda-8" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>9 0.87639612 <a title="454-lda-9" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>10 0.87454081 <a title="454-lda-10" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>11 0.87176073 <a title="454-lda-11" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>12 0.86803758 <a title="454-lda-12" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>13 0.86756945 <a title="454-lda-13" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>14 0.86561042 <a title="454-lda-14" href="../hunch_net-2008/hunch_net-2008-05-23-Three_levels_of_addressing_the_Netflix_Prize.html">301 hunch net-2008-05-23-Three levels of addressing the Netflix Prize</a></p>
<p>15 0.8649419 <a title="454-lda-15" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>16 0.86339146 <a title="454-lda-16" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>17 0.86186922 <a title="454-lda-17" href="../hunch_net-2008/hunch_net-2008-11-10-ICML_Reviewing_Criteria.html">325 hunch net-2008-11-10-ICML Reviewing Criteria</a></p>
<p>18 0.85951507 <a title="454-lda-18" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>19 0.85897928 <a title="454-lda-19" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>20 0.85705638 <a title="454-lda-20" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
