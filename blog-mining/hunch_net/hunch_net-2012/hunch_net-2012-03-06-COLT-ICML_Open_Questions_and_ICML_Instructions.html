<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>458 hunch net-2012-03-06-COLT-ICML Open Questions and ICML Instructions</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2012" href="../home/hunch_net-2012_home.html">hunch_net-2012</a> <a title="hunch_net-2012-458" href="#">hunch_net-2012-458</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>458 hunch net-2012-03-06-COLT-ICML Open Questions and ICML Instructions</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2012-458-html" href="http://hunch.net/?p=2313">html</a></p><p>Introduction: Sasha  is the  open problems  chair for both  COLT   and   ICML .  Open problems will be presented in a joint session in the evening of the COLT/ICML overlap day.   COLT has a history of open sessions, but this is new for ICML.  If you have a difficult theoretically definable problem in machine learning, consider submitting it for review,  due March 16 .  You’ll benefit three ways: 
  
 The effort of writing down a precise formulation of what you want often helps you understand the nature of the problem. 
 Your problem will be officially published and citable. 
 You might have it solved by some very intelligent bored people. 
  
The general idea could easily be applied to any problem which can be crisply stated with an easily verifiable solution, and we may consider expanding this in later years, but for this year all problems need to be of a theoretical variety.
 
 Joelle  and I (and  Mahdi , and  Laurent ) finished an initial assignment of  Program Committee  and  Area Chairs  to pap</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Sasha  is the  open problems  chair for both  COLT   and   ICML . [sent-1, score-0.452]
</p><p>2 Open problems will be presented in a joint session in the evening of the COLT/ICML overlap day. [sent-2, score-0.606]
</p><p>3 COLT has a history of open sessions, but this is new for ICML. [sent-3, score-0.362]
</p><p>4 If you have a difficult theoretically definable problem in machine learning, consider submitting it for review,  due March 16 . [sent-4, score-0.485]
</p><p>5 You’ll benefit three ways:      The effort of writing down a precise formulation of what you want often helps you understand the nature of the problem. [sent-5, score-0.784]
</p><p>6 Your problem will be officially published and citable. [sent-6, score-0.39]
</p><p>7 You might have it solved by some very intelligent bored people. [sent-7, score-0.121]
</p><p>8 The general idea could easily be applied to any problem which can be crisply stated with an easily verifiable solution, and we may consider expanding this in later years, but for this year all problems need to be of a theoretical variety. [sent-8, score-1.275]
</p><p>9 Joelle  and I (and  Mahdi , and  Laurent ) finished an initial assignment of  Program Committee  and  Area Chairs  to papers. [sent-9, score-0.389]
</p><p>10 We’ll be updating  instructions for the PC   and ACs  as we field questions. [sent-10, score-0.307]
</p><p>11 Feel free to comment here on things of plausible general interest, but email us directly with specific concerns. [sent-11, score-0.49]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('open', 0.228), ('verifiable', 0.18), ('officially', 0.18), ('mahdi', 0.18), ('formulation', 0.166), ('sasha', 0.166), ('expanding', 0.166), ('instructions', 0.157), ('laurent', 0.157), ('finished', 0.15), ('updating', 0.15), ('acs', 0.15), ('colt', 0.148), ('sessions', 0.144), ('ll', 0.142), ('easily', 0.14), ('joelle', 0.139), ('history', 0.134), ('assignment', 0.134), ('theoretically', 0.131), ('overlap', 0.131), ('submitting', 0.131), ('joint', 0.127), ('consider', 0.125), ('session', 0.124), ('intelligent', 0.121), ('pc', 0.119), ('problems', 0.118), ('concerns', 0.116), ('benefit', 0.114), ('published', 0.112), ('committee', 0.112), ('march', 0.11), ('precise', 0.108), ('stated', 0.106), ('presented', 0.106), ('later', 0.106), ('email', 0.106), ('chair', 0.106), ('initial', 0.105), ('chairs', 0.103), ('three', 0.101), ('nature', 0.099), ('writing', 0.099), ('specific', 0.099), ('problem', 0.098), ('comment', 0.097), ('helps', 0.097), ('general', 0.096), ('plausible', 0.092)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="458-tfidf-1" href="../hunch_net-2012/hunch_net-2012-03-06-COLT-ICML_Open_Questions_and_ICML_Instructions.html">458 hunch net-2012-03-06-COLT-ICML Open Questions and ICML Instructions</a></p>
<p>Introduction: Sasha  is the  open problems  chair for both  COLT   and   ICML .  Open problems will be presented in a joint session in the evening of the COLT/ICML overlap day.   COLT has a history of open sessions, but this is new for ICML.  If you have a difficult theoretically definable problem in machine learning, consider submitting it for review,  due March 16 .  You’ll benefit three ways: 
  
 The effort of writing down a precise formulation of what you want often helps you understand the nature of the problem. 
 Your problem will be officially published and citable. 
 You might have it solved by some very intelligent bored people. 
  
The general idea could easily be applied to any problem which can be crisply stated with an easily verifiable solution, and we may consider expanding this in later years, but for this year all problems need to be of a theoretical variety.
 
 Joelle  and I (and  Mahdi , and  Laurent ) finished an initial assignment of  Program Committee  and  Area Chairs  to pap</p><p>2 0.21029922 <a title="458-tfidf-2" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<p>Introduction: By  Shie  and  Nati 
 
Following John’s advertisement for submitting to ICML, we thought it appropriate to highlight the advantages of COLT, and the reasons it is often the best place for theory papers.  We would like to emphasize that we both respect ICML, and are active in ICML, both as authors and as area chairs, and certainly are not arguing that ICML is a bad place for your papers.  For many papers, ICML is the best venue.  But for many theory papers, COLT is a better and more appropriate place.
 
Why should you submit to COLT?
 
By-and-large, theory papers go to COLT. This is the tradition of the field and most theory papers are sent to COLT. This is the place to present your ground-breaking theorems and new models that will shape the theory of machine learning. COLT is more focused then ICML with a single track session.  Unlike ICML, the norm in COLT is for people to sit through most sessions, and hear most of the talks presented.  There is also often a lively discussion followi</p><p>3 0.1496345 <a title="458-tfidf-3" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>Introduction: When thinking about how best to review papers, it seems helpful to have some conception of what good reviewing is.  As far as I can tell, this is almost always only discussed in the specific context of a paper (i.e. your rejected paper), or at most an area (i.e. what a “good paper” looks like for that area) rather than general principles.  Neither individual papers or areas are sufficiently general for a large conference—every paper differs in the details, and what if you want to build a new area and/or cross areas?
 
An unavoidable reason for reviewing is that the community of research is too large.  In particular, it is not possible for a researcher to read every paper which someone thinks might be of interest.  This reason for reviewing exists independent of constraints on rooms or scheduling formats of individual conferences.  Indeed, history suggests that physical constraints are relatively meaningless over the long term — growing conferences simply use more rooms and/or change fo</p><p>4 0.14378303 <a title="458-tfidf-4" href="../hunch_net-2008/hunch_net-2008-03-15-COLT_Open_Problems.html">292 hunch net-2008-03-15-COLT Open Problems</a></p>
<p>Introduction: COLT has a  call for open problems  due March 21.  I encourage anyone with a specifiable open problem to write it down and send it in.  Just the effort of specifying an open problem precisely and concisely has been very helpful for my own solutions, and there is a substantial chance others will solve it.  To increase the chance someone will take it up, you can even put a bounty on the solution.  (Perhaps I should raise the  $500 bounty  on the  K-fold cross-validation problem  as it hasnâ&euro;&trade;t yet been solved).</p><p>5 0.13675334 <a title="458-tfidf-5" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>Introduction: Unfortunately, I ended up sick for much of this ICML.  I did manage to catch one interesting paper:
 
 Richard Socher ,  Cliff Lin ,  Andrew Y. Ng , and  Christopher D. Manning   Parsing Natural Scenes and Natural Language with Recursive Neural Networks .
 
I invited Richard to share his list of interesting papers, so hopefully we’ll hear from him soon.  In the meantime,  Paul  and  Hal  have posted some lists.
 
 the future 
 
 Joelle  and I are program chairs for ICML 2012 in  Edinburgh , which I previously enjoyed visiting in  2005 .  This is a huge responsibility, that we hope to accomplish well.  A part of this (perhaps the most fun part), is imagining how we can make ICML better.  A key and critical constraint is choosing things that can be accomplished.  So far we have:
  
  Colocation . The first thing we looked into was potential colocations.  We quickly discovered that many other conferences precomitted their location.  For the future, getting a colocation with  ACL  or  SIGI</p><p>6 0.13186716 <a title="458-tfidf-6" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>7 0.12100601 <a title="458-tfidf-7" href="../hunch_net-2006/hunch_net-2006-12-04-Structural_Problems_in_NIPS_Decision_Making.html">221 hunch net-2006-12-04-Structural Problems in NIPS Decision Making</a></p>
<p>8 0.11403411 <a title="458-tfidf-8" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>9 0.11232358 <a title="458-tfidf-9" href="../hunch_net-2005/hunch_net-2005-06-28-The_cross_validation_problem%3A_cash_reward.html">86 hunch net-2005-06-28-The cross validation problem: cash reward</a></p>
<p>10 0.11030403 <a title="458-tfidf-10" href="../hunch_net-2005/hunch_net-2005-07-04-The_Health_of_COLT.html">89 hunch net-2005-07-04-The Health of COLT</a></p>
<p>11 0.10189775 <a title="458-tfidf-11" href="../hunch_net-2008/hunch_net-2008-10-14-Who_is_Responsible_for_a_Bad_Review%3F.html">320 hunch net-2008-10-14-Who is Responsible for a Bad Review?</a></p>
<p>12 0.098209992 <a title="458-tfidf-12" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<p>13 0.097593054 <a title="458-tfidf-13" href="../hunch_net-2007/hunch_net-2007-11-16-MLSS_2008.html">273 hunch net-2007-11-16-MLSS 2008</a></p>
<p>14 0.097455129 <a title="458-tfidf-14" href="../hunch_net-2011/hunch_net-2011-10-10-ML_Symposium_and_ICML_details.html">447 hunch net-2011-10-10-ML Symposium and ICML details</a></p>
<p>15 0.096993774 <a title="458-tfidf-15" href="../hunch_net-2012/hunch_net-2012-05-02-ICML%3A_Behind_the_Scenes.html">463 hunch net-2012-05-02-ICML: Behind the Scenes</a></p>
<p>16 0.095342584 <a title="458-tfidf-16" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>17 0.095324591 <a title="458-tfidf-17" href="../hunch_net-2011/hunch_net-2011-04-06-COLT_open_questions.html">429 hunch net-2011-04-06-COLT open questions</a></p>
<p>18 0.093816184 <a title="458-tfidf-18" href="../hunch_net-2005/hunch_net-2005-08-04-Why_Reinforcement_Learning_is_Important.html">100 hunch net-2005-08-04-Why Reinforcement Learning is Important</a></p>
<p>19 0.091433875 <a title="458-tfidf-19" href="../hunch_net-2005/hunch_net-2005-02-02-Paper_Deadlines.html">11 hunch net-2005-02-02-Paper Deadlines</a></p>
<p>20 0.083606742 <a title="458-tfidf-20" href="../hunch_net-2007/hunch_net-2007-12-19-Cool_and_interesting_things_seen_at_NIPS.html">279 hunch net-2007-12-19-Cool and interesting things seen at NIPS</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.194), (1, -0.132), (2, 0.017), (3, -0.01), (4, -0.001), (5, -0.018), (6, 0.024), (7, 0.025), (8, -0.069), (9, -0.082), (10, 0.024), (11, -0.044), (12, -0.029), (13, 0.125), (14, 0.094), (15, -0.037), (16, 0.099), (17, -0.081), (18, -0.103), (19, 0.021), (20, -0.088), (21, 0.181), (22, 0.045), (23, -0.068), (24, -0.017), (25, 0.104), (26, 0.01), (27, 0.043), (28, 0.134), (29, 0.032), (30, 0.066), (31, 0.064), (32, 0.018), (33, -0.001), (34, -0.022), (35, 0.061), (36, 0.01), (37, -0.037), (38, 0.023), (39, -0.031), (40, 0.048), (41, -0.014), (42, -0.017), (43, 0.004), (44, 0.035), (45, -0.065), (46, -0.055), (47, -0.051), (48, -0.0), (49, 0.006)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97724485 <a title="458-lsi-1" href="../hunch_net-2012/hunch_net-2012-03-06-COLT-ICML_Open_Questions_and_ICML_Instructions.html">458 hunch net-2012-03-06-COLT-ICML Open Questions and ICML Instructions</a></p>
<p>Introduction: Sasha  is the  open problems  chair for both  COLT   and   ICML .  Open problems will be presented in a joint session in the evening of the COLT/ICML overlap day.   COLT has a history of open sessions, but this is new for ICML.  If you have a difficult theoretically definable problem in machine learning, consider submitting it for review,  due March 16 .  You’ll benefit three ways: 
  
 The effort of writing down a precise formulation of what you want often helps you understand the nature of the problem. 
 Your problem will be officially published and citable. 
 You might have it solved by some very intelligent bored people. 
  
The general idea could easily be applied to any problem which can be crisply stated with an easily verifiable solution, and we may consider expanding this in later years, but for this year all problems need to be of a theoretical variety.
 
 Joelle  and I (and  Mahdi , and  Laurent ) finished an initial assignment of  Program Committee  and  Area Chairs  to pap</p><p>2 0.77224433 <a title="458-lsi-2" href="../hunch_net-2008/hunch_net-2008-03-15-COLT_Open_Problems.html">292 hunch net-2008-03-15-COLT Open Problems</a></p>
<p>Introduction: COLT has a  call for open problems  due March 21.  I encourage anyone with a specifiable open problem to write it down and send it in.  Just the effort of specifying an open problem precisely and concisely has been very helpful for my own solutions, and there is a substantial chance others will solve it.  To increase the chance someone will take it up, you can even put a bounty on the solution.  (Perhaps I should raise the  $500 bounty  on the  K-fold cross-validation problem  as it hasnâ&euro;&trade;t yet been solved).</p><p>3 0.6841647 <a title="458-lsi-3" href="../hunch_net-2011/hunch_net-2011-04-06-COLT_open_questions.html">429 hunch net-2011-04-06-COLT open questions</a></p>
<p>Introduction: Alina  and  Jake  point out the COLT  Call for Open Questions  due May 11.  In general, this is cool, and worth doing if you can come up with a crisp question.  In my case, I particularly enjoyed  crafting an open question  with precisely a form such that a  critic targeting my papers  would be forced to confront their fallacy or make a case for the reward.  But less esoterically, this is a way to get the attention of some very smart people focused on a problem that really matters, which is the real value.</p><p>4 0.63550413 <a title="458-lsi-4" href="../hunch_net-2008/hunch_net-2008-11-09-A_Healthy__COLT.html">324 hunch net-2008-11-09-A Healthy  COLT</a></p>
<p>Introduction: A  while ago , we discussed the health of  COLT .   COLT 2008  substantially addressed my concerns.  The papers were diverse and several were interesting.  Attendance was up, which is particularly notable in Europe.  In my opinion, the colocation with UAI and ICML was the best colocation since 1998.
 
And, perhaps best of all, registration ended up being free for all students due to various grants from the  Academy of Finland ,  Google ,  IBM , and  Yahoo .
 
A basic question is: what went right?  There seem to be several answers.
  
 Cost-wise, COLT had sufficient grants to alleviate the high cost of the Euro and location at a university substantially reduces the cost compared to a hotel. 
 Organization-wise, the Finns were great with hordes of volunteers helping set everything up.  Having too many volunteers is a good failure mode. 
 Organization-wise, it was clear that all 3 program chairs were cooperating in designing the program. 
 Facilities-wise, proximity in time and space made</p><p>5 0.62406164 <a title="458-lsi-5" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<p>Introduction: By  Shie  and  Nati 
 
Following John’s advertisement for submitting to ICML, we thought it appropriate to highlight the advantages of COLT, and the reasons it is often the best place for theory papers.  We would like to emphasize that we both respect ICML, and are active in ICML, both as authors and as area chairs, and certainly are not arguing that ICML is a bad place for your papers.  For many papers, ICML is the best venue.  But for many theory papers, COLT is a better and more appropriate place.
 
Why should you submit to COLT?
 
By-and-large, theory papers go to COLT. This is the tradition of the field and most theory papers are sent to COLT. This is the place to present your ground-breaking theorems and new models that will shape the theory of machine learning. COLT is more focused then ICML with a single track session.  Unlike ICML, the norm in COLT is for people to sit through most sessions, and hear most of the talks presented.  There is also often a lively discussion followi</p><p>6 0.59799474 <a title="458-lsi-6" href="../hunch_net-2005/hunch_net-2005-06-17-Reopening_RL-%3EClassification.html">82 hunch net-2005-06-17-Reopening RL->Classification</a></p>
<p>7 0.59309947 <a title="458-lsi-7" href="../hunch_net-2005/hunch_net-2005-06-28-The_cross_validation_problem%3A_cash_reward.html">86 hunch net-2005-06-28-The cross validation problem: cash reward</a></p>
<p>8 0.58918148 <a title="458-lsi-8" href="../hunch_net-2005/hunch_net-2005-03-28-Open_Problems_for_Colt.html">47 hunch net-2005-03-28-Open Problems for Colt</a></p>
<p>9 0.5353775 <a title="458-lsi-9" href="../hunch_net-2005/hunch_net-2005-07-04-The_Health_of_COLT.html">89 hunch net-2005-07-04-The Health of COLT</a></p>
<p>10 0.51177806 <a title="458-lsi-10" href="../hunch_net-2007/hunch_net-2007-11-16-MLSS_2008.html">273 hunch net-2007-11-16-MLSS 2008</a></p>
<p>11 0.50687957 <a title="458-lsi-11" href="../hunch_net-2005/hunch_net-2005-07-01-The_Role_of_Impromptu_Talks.html">88 hunch net-2005-07-01-The Role of Impromptu Talks</a></p>
<p>12 0.50290096 <a title="458-lsi-12" href="../hunch_net-2010/hunch_net-2010-04-24-COLT_Treasurer_is_now_Phil_Long.html">394 hunch net-2010-04-24-COLT Treasurer is now Phil Long</a></p>
<p>13 0.50011051 <a title="458-lsi-13" href="../hunch_net-2011/hunch_net-2011-10-10-ML_Symposium_and_ICML_details.html">447 hunch net-2011-10-10-ML Symposium and ICML details</a></p>
<p>14 0.49877575 <a title="458-lsi-14" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>15 0.49642649 <a title="458-lsi-15" href="../hunch_net-2006/hunch_net-2006-01-08-Debugging_Your_Brain.html">147 hunch net-2006-01-08-Debugging Your Brain</a></p>
<p>16 0.49002722 <a title="458-lsi-16" href="../hunch_net-2005/hunch_net-2005-08-04-Why_Reinforcement_Learning_is_Important.html">100 hunch net-2005-08-04-Why Reinforcement Learning is Important</a></p>
<p>17 0.48693985 <a title="458-lsi-17" href="../hunch_net-2006/hunch_net-2006-12-04-Structural_Problems_in_NIPS_Decision_Making.html">221 hunch net-2006-12-04-Structural Problems in NIPS Decision Making</a></p>
<p>18 0.46724275 <a title="458-lsi-18" href="../hunch_net-2006/hunch_net-2006-10-22-Exemplar_programming.html">215 hunch net-2006-10-22-Exemplar programming</a></p>
<p>19 0.46609846 <a title="458-lsi-19" href="../hunch_net-2008/hunch_net-2008-10-14-Who_is_Responsible_for_a_Bad_Review%3F.html">320 hunch net-2008-10-14-Who is Responsible for a Bad Review?</a></p>
<p>20 0.46416172 <a title="458-lsi-20" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(27, 0.233), (53, 0.111), (54, 0.307), (55, 0.147), (94, 0.024), (95, 0.078)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.9396373 <a title="458-lda-1" href="../hunch_net-2014/hunch_net-2014-03-11-The_New_York_ML_Symposium%2C_take_2.html">494 hunch net-2014-03-11-The New York ML Symposium, take 2</a></p>
<p>Introduction: The 20 13 14 is  New York Machine Learning Symposium  is finally happening on March 28th at the  New York Academy of Science .  Every invited speaker interests me personally.  They are:
  
  Rayid Ghani  (Chief Scientist at Obama 2012) 
  Brian Kingsbury  (Speech Recognition @ IBM) 
  Jorge Nocedal  (who did LBFGS) 
  
Weâ&euro;&trade;ve been somewhat disorganized in advertising this.  As a consequence, anyone who has not submitted an abstract but would like to do so may send one directly to me (jl@hunch.net title NYASMLS) by Friday March 14.  I will forward them to the rest of the committee for consideration.</p><p>2 0.93193364 <a title="458-lda-2" href="../hunch_net-2009/hunch_net-2009-01-08-Predictive_Analytics_World.html">335 hunch net-2009-01-08-Predictive Analytics World</a></p>
<p>Introduction: Carla Vicens and  Eric Siegel  contacted me about  Predictive Analytics World  in San Francisco February 18&19, which I wasn’t familiar with.  A quick look at the  agenda  reveals several people I know working on applications of machine learning in businesses, covering deployed applications topics.  It’s interesting to see a business-focused machine learning conference, as it says that we are succeeding as a field.  If you are interested in deployed applications, you might attend.
 
Eric and I did a quick interview by email.
 
John > 
I’ve mostly published and participated in academic machine learning conferences like ICML, COLT, and NIPS.   When I look at the  set of speakers and subjects  for your conference  I think “machine learning for business”.  Is that your understanding of things? What I’m trying to ask is: what do you view as the primary goal for this conference?
 
Eric > 
 You got it.  This is the business event focused on the commercial deployment of technology developed at</p><p>3 0.89706314 <a title="458-lda-3" href="../hunch_net-2009/hunch_net-2009-01-19-Netflix_prize_within_epsilon.html">336 hunch net-2009-01-19-Netflix prize within epsilon</a></p>
<p>Introduction: The competitors for the  Netflix Prize  are tantalizingly close winning the million dollar prize.  This year,  BellKor  and  Commendo Research  sent a combined solution that won the  progress prize .  Reading the  writeups   2  is instructive.  Several aspects of solutions are taken for granted including stochastic gradient descent, ensemble prediction, and targeting residuals (a form of boosting).  Relatively to last year, it appears that many approaches have added parameterizations, especially for the purpose of modeling through time.
 
The big question is: will they make the big prize?  At this point, the level of complexity in entering the competition is prohibitive, so perhaps only the existing competitors will continue to try.  (This equation might change drastically if the teams open source their existing solutions, including parameter settings.) One fear is that the progress is asymptoting on the wrong side of the 10% threshold.  In the first year, the teams progressed through</p><p>same-blog 4 0.88049102 <a title="458-lda-4" href="../hunch_net-2012/hunch_net-2012-03-06-COLT-ICML_Open_Questions_and_ICML_Instructions.html">458 hunch net-2012-03-06-COLT-ICML Open Questions and ICML Instructions</a></p>
<p>Introduction: Sasha  is the  open problems  chair for both  COLT   and   ICML .  Open problems will be presented in a joint session in the evening of the COLT/ICML overlap day.   COLT has a history of open sessions, but this is new for ICML.  If you have a difficult theoretically definable problem in machine learning, consider submitting it for review,  due March 16 .  You’ll benefit three ways: 
  
 The effort of writing down a precise formulation of what you want often helps you understand the nature of the problem. 
 Your problem will be officially published and citable. 
 You might have it solved by some very intelligent bored people. 
  
The general idea could easily be applied to any problem which can be crisply stated with an easily verifiable solution, and we may consider expanding this in later years, but for this year all problems need to be of a theoretical variety.
 
 Joelle  and I (and  Mahdi , and  Laurent ) finished an initial assignment of  Program Committee  and  Area Chairs  to pap</p><p>5 0.86671084 <a title="458-lda-5" href="../hunch_net-2009/hunch_net-2009-11-06-Yisong_Yue_on_Self-improving_Systems.html">376 hunch net-2009-11-06-Yisong Yue on Self-improving Systems</a></p>
<p>Introduction: I’d like to point out  Yisong Yue ‘s  post on Self-improving systems , which is a nicely readable description of the necessity and potential of interactive learning to deal with the information overload problem that is endemic to the modern internet.</p><p>6 0.78320491 <a title="458-lda-6" href="../hunch_net-2005/hunch_net-2005-02-28-Regularization.html">33 hunch net-2005-02-28-Regularization</a></p>
<p>7 0.68508506 <a title="458-lda-7" href="../hunch_net-2007/hunch_net-2007-01-02-Retrospective.html">225 hunch net-2007-01-02-Retrospective</a></p>
<p>8 0.67713559 <a title="458-lda-8" href="../hunch_net-2012/hunch_net-2012-06-05-ICML_acceptance_statistics.html">466 hunch net-2012-06-05-ICML acceptance statistics</a></p>
<p>9 0.67256182 <a title="458-lda-9" href="../hunch_net-2006/hunch_net-2006-01-25-1_year.html">151 hunch net-2006-01-25-1 year</a></p>
<p>10 0.66775739 <a title="458-lda-10" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>11 0.66534227 <a title="458-lda-11" href="../hunch_net-2012/hunch_net-2012-10-26-ML_Symposium_and_Strata-Hadoop_World.html">475 hunch net-2012-10-26-ML Symposium and Strata-Hadoop World</a></p>
<p>12 0.66527593 <a title="458-lda-12" href="../hunch_net-2013/hunch_net-2013-01-07-NYU_Large_Scale_Machine_Learning_Class.html">478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</a></p>
<p>13 0.66482472 <a title="458-lda-13" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>14 0.66409266 <a title="458-lda-14" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>15 0.6637488 <a title="458-lda-15" href="../hunch_net-2005/hunch_net-2005-07-04-The_Health_of_COLT.html">89 hunch net-2005-07-04-The Health of COLT</a></p>
<p>16 0.6629045 <a title="458-lda-16" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>17 0.66265392 <a title="458-lda-17" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>18 0.66239738 <a title="458-lda-18" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>19 0.65925747 <a title="458-lda-19" href="../hunch_net-2006/hunch_net-2006-07-11-New_Models.html">194 hunch net-2006-07-11-New Models</a></p>
<p>20 0.65885013 <a title="458-lda-20" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
