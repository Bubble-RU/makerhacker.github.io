<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>469 hunch net-2012-07-09-Videolectures</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2012" href="../home/hunch_net-2012_home.html">hunch_net-2012</a> <a title="hunch_net-2012-469" href="#">hunch_net-2012-469</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>469 hunch net-2012-07-09-Videolectures</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2012-469-html" href="http://hunch.net/?p=2546">html</a></p><p>Introduction: Yaserpoints out some nicelyvideotaped machine learning lecturesatCaltech.
Yaser taught me machine learning, and I always found the lectures clear and
interesting, so I expect many people can benefit from watching. Relative
toAndrew Ng'sML classthere are somewhat different areas of emphasis but the
topic is the same, so picking and choosing the union may be helpful.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Yaser taught me machine learning, and I always found the lectures clear and interesting, so I expect many people can benefit from watching. [sent-2, score-1.726]
</p><p>2 Relative toAndrew Ng'sML classthere are somewhat different areas of emphasis but the topic is the same, so picking and choosing the union may be helpful. [sent-3, score-1.623]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('toandrew', 0.399), ('lectures', 0.333), ('picking', 0.29), ('emphasis', 0.283), ('taught', 0.283), ('benefit', 0.253), ('areas', 0.244), ('relative', 0.229), ('choosing', 0.219), ('topic', 0.203), ('somewhat', 0.198), ('helpful', 0.169), ('clear', 0.168), ('expect', 0.154), ('found', 0.149), ('always', 0.146), ('machine', 0.124), ('interesting', 0.119), ('different', 0.103), ('may', 0.083), ('people', 0.064), ('learning', 0.052), ('many', 0.052)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="469-tfidf-1" href="../hunch_net-2012/hunch_net-2012-07-09-Videolectures.html">469 hunch net-2012-07-09-Videolectures</a></p>
<p>Introduction: Yaserpoints out some nicelyvideotaped machine learning lecturesatCaltech.
Yaser taught me machine learning, and I always found the lectures clear and
interesting, so I expect many people can benefit from watching. Relative
toAndrew Ng'sML classthere are somewhat different areas of emphasis but the
topic is the same, so picking and choosing the union may be helpful.</p><p>2 0.13409153 <a title="469-tfidf-2" href="../hunch_net-2006/hunch_net-2006-02-04-Research_Budget_Changes.html">154 hunch net-2006-02-04-Research Budget Changes</a></p>
<p>Introduction: The announcement of an increase in funding for basic research in the US is
encouraging. There is some discussion of this at theComputing Research
Policyblog.One part of this discussion has a graph of NSF funding over time,
presumably in dollar budgets. I don't believe that dollar budgets are the
right way to judge the impact of funding changes on researchers. A better way
to judge seems to be in terms of dollar budget divided by GDP which provides a
measure of the relative emphasis on research.This graph was assembled by
dividing theNSF budgetby theUS GDP. For 2005 GDP, I used thecurrent
estimateand for 2006 and 2007 assumed an increase by a factor of 1.04 per
year. The 2007 number also uses the requested 2007 budget which is certain to
change.This graph makes it clear why researchers were upset: research funding
emphasis has fallen for 3 years in a row. The reality has been significantly
more severe due toDARPA decreasing fundingand industrial research labs (ATnT
and Lucent for exampl</p><p>3 0.11460534 <a title="469-tfidf-3" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>Introduction: There are several different flavors of Machine Learning classes. Many classes
are of the 'zoo' sort: many different learning algorithms are presented.
Others avoid the zoo by not covering the full scope of machine learning.This
is my view of what makes a good machine learning class, along with why. I'd
like to specifically invite comment on whether things are missing,
misemphasized, or misplaced.PhaseSubjectWhy?IntroductionWhat is a machine
learning problem?A good understanding of the characteristics of machine
learning problems seems essential. Characteristics include: a data source,
some hope the data is predictive, and a need for generalization. This is
probably best taught in a case study manner: lay out the specifics of some
problem and then ask "Is this a machine learning problem?"IntroductionMachine
Learning Problem IdentificationIdentification and recognition of the type of
learning problems is (obviously) a very important step in solving such
problems. People need to be famili</p><p>4 0.10617296 <a title="469-tfidf-4" href="../hunch_net-2005/hunch_net-2005-03-08-Fast_Physics_for_Learning.html">37 hunch net-2005-03-08-Fast Physics for Learning</a></p>
<p>Introduction: While everyone is silently working on ICML submissions, I found this
discussion about afast physics simulatorchip interesting from a learning
viewpoint. In many cases, learning attempts to predict the outcome of physical
processes. Access to a fast simulator for these processes might be quite
helpful in predicting the outcome. Bayesian learning in particular may
directly benefit while many other algorithms (like support vector machines)
might have their speed greatly increased.The biggest drawback is that writing
software for these odd architectures is always difficult and time consuming,
but a several-orders-of-magnitude speedup might make that worthwhile.</p><p>5 0.09695673 <a title="469-tfidf-5" href="../hunch_net-2013/hunch_net-2013-01-07-NYU_Large_Scale_Machine_Learning_Class.html">478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</a></p>
<p>Introduction: Yann LeCunand I are coteaching a class onLarge Scale Machine Learningstarting
late Januaryat NYU. This class will cover many tricks to get machine learning
working well on datasets with many features, examples, and classes, along with
several elements of deep learning and support systems enabling the
previous.This is not a beginning class--you really need to have taken a basic
machine learning class previously to follow along. Students will be able to
run and experiment with large scale learning algorithms sinceYahoo!has donated
servers which are being configured into a small scaleHadoopcluster. We are
planning to cover the frontier of research in scalable learning algorithms, so
good class projects could easily lead to papers.For me, this is a chance to
teach on many topics of past research. In general, it seems like researchers
should engage in at least occasional teaching of research, both as a proof of
teachability and to see their own research through that lens. More generally,
I</p><p>6 0.095552683 <a title="469-tfidf-6" href="../hunch_net-2006/hunch_net-2006-07-09-The_Stock_Prediction_Machine_Learning_Problem.html">193 hunch net-2006-07-09-The Stock Prediction Machine Learning Problem</a></p>
<p>7 0.094494537 <a title="469-tfidf-7" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>8 0.092859589 <a title="469-tfidf-8" href="../hunch_net-2006/hunch_net-2006-04-17-Rexa_is_live.html">173 hunch net-2006-04-17-Rexa is live</a></p>
<p>9 0.08602047 <a title="469-tfidf-9" href="../hunch_net-2006/hunch_net-2006-11-02-2006_NIPS_workshops.html">216 hunch net-2006-11-02-2006 NIPS workshops</a></p>
<p>10 0.076814197 <a title="469-tfidf-10" href="../hunch_net-2007/hunch_net-2007-01-15-The_Machine_Learning_Department.html">228 hunch net-2007-01-15-The Machine Learning Department</a></p>
<p>11 0.07438416 <a title="469-tfidf-11" href="../hunch_net-2008/hunch_net-2008-11-04-Rise_of_the_Machines.html">323 hunch net-2008-11-04-Rise of the Machines</a></p>
<p>12 0.073896654 <a title="469-tfidf-12" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>13 0.071615413 <a title="469-tfidf-13" href="../hunch_net-2007/hunch_net-2007-02-22-Create_Your_Own_ICML_Workshop.html">234 hunch net-2007-02-22-Create Your Own ICML Workshop</a></p>
<p>14 0.07055521 <a title="469-tfidf-14" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>15 0.066919826 <a title="469-tfidf-15" href="../hunch_net-2013/hunch_net-2013-06-10-The_Large_Scale_Learning_class_notes.html">483 hunch net-2013-06-10-The Large Scale Learning class notes</a></p>
<p>16 0.066070586 <a title="469-tfidf-16" href="../hunch_net-2010/hunch_net-2010-11-18-ICML_2011_%26%238211%3B_Call_for_Tutorials.html">417 hunch net-2010-11-18-ICML 2011 &#8211; Call for Tutorials</a></p>
<p>17 0.063522093 <a title="469-tfidf-17" href="../hunch_net-2005/hunch_net-2005-09-14-The_Predictionist_Viewpoint.html">112 hunch net-2005-09-14-The Predictionist Viewpoint</a></p>
<p>18 0.062204733 <a title="469-tfidf-18" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>19 0.061450504 <a title="469-tfidf-19" href="../hunch_net-2005/hunch_net-2005-09-04-Science_in_the_Government.html">106 hunch net-2005-09-04-Science in the Government</a></p>
<p>20 0.061294176 <a title="469-tfidf-20" href="../hunch_net-2006/hunch_net-2006-01-13-Benchmarks_for_RL.html">148 hunch net-2006-01-13-Benchmarks for RL</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.108), (1, 0.035), (2, 0.08), (3, 0.005), (4, -0.003), (5, -0.005), (6, -0.007), (7, 0.008), (8, -0.004), (9, -0.013), (10, 0.005), (11, 0.025), (12, 0.023), (13, -0.043), (14, 0.014), (15, -0.009), (16, 0.003), (17, 0.043), (18, 0.017), (19, 0.019), (20, 0.097), (21, -0.056), (22, -0.066), (23, -0.062), (24, -0.063), (25, -0.087), (26, 0.066), (27, 0.095), (28, -0.055), (29, -0.077), (30, 0.024), (31, -0.036), (32, 0.043), (33, 0.018), (34, 0.035), (35, -0.008), (36, -0.028), (37, 0.179), (38, -0.046), (39, 0.007), (40, -0.004), (41, -0.038), (42, -0.004), (43, 0.033), (44, 0.057), (45, 0.084), (46, 0.12), (47, 0.119), (48, -0.006), (49, 0.011)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.93293709 <a title="469-lsi-1" href="../hunch_net-2012/hunch_net-2012-07-09-Videolectures.html">469 hunch net-2012-07-09-Videolectures</a></p>
<p>Introduction: Yaserpoints out some nicelyvideotaped machine learning lecturesatCaltech.
Yaser taught me machine learning, and I always found the lectures clear and
interesting, so I expect many people can benefit from watching. Relative
toAndrew Ng'sML classthere are somewhat different areas of emphasis but the
topic is the same, so picking and choosing the union may be helpful.</p><p>2 0.65902781 <a title="469-lsi-2" href="../hunch_net-2006/hunch_net-2006-07-09-The_Stock_Prediction_Machine_Learning_Problem.html">193 hunch net-2006-07-09-The Stock Prediction Machine Learning Problem</a></p>
<p>Introduction: …is discussed inthis nytimes article. I generally expect such approaches to
become more common since computers are getting faster, machine learning is
getting better, and data is becoming more plentiful. This is another example
where machine learning technology may have a huge economic impact. Some side
notes:We-in-research know almost nothing about how these things are done
(because it is typically a corporate secret).… but the limited discussion in
the article seem naive from a machine learning viewpoint.The learning process
used apparently often fails to take into account transaction costs.What little
of the approaches is discussed appears modeling based. It seems plausible that
more direct prediction methods can yield an edge.One difficulty with stock
picking as a research topic is that it is inherently a zero sum game (for
every winner, there is a loser). Much of the rest of research is positive sum
(basically, everyone wins).</p><p>3 0.64096862 <a title="469-lsi-3" href="../hunch_net-2005/hunch_net-2005-03-08-Fast_Physics_for_Learning.html">37 hunch net-2005-03-08-Fast Physics for Learning</a></p>
<p>Introduction: While everyone is silently working on ICML submissions, I found this
discussion about afast physics simulatorchip interesting from a learning
viewpoint. In many cases, learning attempts to predict the outcome of physical
processes. Access to a fast simulator for these processes might be quite
helpful in predicting the outcome. Bayesian learning in particular may
directly benefit while many other algorithms (like support vector machines)
might have their speed greatly increased.The biggest drawback is that writing
software for these odd architectures is always difficult and time consuming,
but a several-orders-of-magnitude speedup might make that worthwhile.</p><p>4 0.60639465 <a title="469-lsi-4" href="../hunch_net-2008/hunch_net-2008-11-04-Rise_of_the_Machines.html">323 hunch net-2008-11-04-Rise of the Machines</a></p>
<p>Introduction: On theenduring topic of how people deal with intelligent machines, we have
this importantelection bulletin.</p><p>5 0.55419642 <a title="469-lsi-5" href="../hunch_net-2008/hunch_net-2008-11-26-Efficient_Reinforcement_Learning_in_MDPs.html">328 hunch net-2008-11-26-Efficient Reinforcement Learning in MDPs</a></p>
<p>Introduction: Claude Sammutis attempting to put together anEncyclopedia of Machine Learning.
I volunteered to write one article onEfficient RL in MDPs, which I would like
to invite comment on. Is something critical missing?</p><p>6 0.52299684 <a title="469-lsi-6" href="../hunch_net-2006/hunch_net-2006-04-17-Rexa_is_live.html">173 hunch net-2006-04-17-Rexa is live</a></p>
<p>7 0.51294011 <a title="469-lsi-7" href="../hunch_net-2009/hunch_net-2009-04-23-Jonathan_Chang_at_Slycoder.html">350 hunch net-2009-04-23-Jonathan Chang at Slycoder</a></p>
<p>8 0.50573742 <a title="469-lsi-8" href="../hunch_net-2007/hunch_net-2007-06-23-Machine_Learning_Jobs_are_Growing_on_Trees.html">250 hunch net-2007-06-23-Machine Learning Jobs are Growing on Trees</a></p>
<p>9 0.50185061 <a title="469-lsi-9" href="../hunch_net-2007/hunch_net-2007-01-15-The_Machine_Learning_Department.html">228 hunch net-2007-01-15-The Machine Learning Department</a></p>
<p>10 0.49905336 <a title="469-lsi-10" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>11 0.48798469 <a title="469-lsi-11" href="../hunch_net-2011/hunch_net-2011-08-20-The_Large_Scale_Learning_Survey_Tutorial.html">442 hunch net-2011-08-20-The Large Scale Learning Survey Tutorial</a></p>
<p>12 0.4779785 <a title="469-lsi-12" href="../hunch_net-2013/hunch_net-2013-01-07-NYU_Large_Scale_Machine_Learning_Class.html">478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</a></p>
<p>13 0.46460187 <a title="469-lsi-13" href="../hunch_net-2006/hunch_net-2006-04-09-Progress_in_Machine_Translation.html">171 hunch net-2006-04-09-Progress in Machine Translation</a></p>
<p>14 0.45523572 <a title="469-lsi-14" href="../hunch_net-2005/hunch_net-2005-09-14-The_Predictionist_Viewpoint.html">112 hunch net-2005-09-14-The Predictionist Viewpoint</a></p>
<p>15 0.45111564 <a title="469-lsi-15" href="../hunch_net-2006/hunch_net-2006-11-02-2006_NIPS_workshops.html">216 hunch net-2006-11-02-2006 NIPS workshops</a></p>
<p>16 0.45086476 <a title="469-lsi-16" href="../hunch_net-2011/hunch_net-2011-06-22-Ultra_LDA.html">436 hunch net-2011-06-22-Ultra LDA</a></p>
<p>17 0.44908926 <a title="469-lsi-17" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>18 0.44368351 <a title="469-lsi-18" href="../hunch_net-2008/hunch_net-2008-08-24-Mass_Customized_Medicine_in_the_Future%3F.html">314 hunch net-2008-08-24-Mass Customized Medicine in the Future?</a></p>
<p>19 0.43914023 <a title="469-lsi-19" href="../hunch_net-2006/hunch_net-2006-04-02-Mad_%28Neuro%29science.html">168 hunch net-2006-04-02-Mad (Neuro)science</a></p>
<p>20 0.43689638 <a title="469-lsi-20" href="../hunch_net-2009/hunch_net-2009-01-08-Predictive_Analytics_World.html">335 hunch net-2009-01-08-Predictive Analytics World</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(42, 0.23), (60, 0.586)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.99436957 <a title="469-lda-1" href="../hunch_net-2006/hunch_net-2006-07-25-Upcoming_conference.html">198 hunch net-2006-07-25-Upcoming conference</a></p>
<p>Introduction: The Workshop for Women in Machine Learning will be held in San Diego on
October 4, 2006.For details see the workshop
website:http://www.seas.upenn.edu/~wiml/</p><p>same-blog 2 0.87208521 <a title="469-lda-2" href="../hunch_net-2012/hunch_net-2012-07-09-Videolectures.html">469 hunch net-2012-07-09-Videolectures</a></p>
<p>Introduction: Yaserpoints out some nicelyvideotaped machine learning lecturesatCaltech.
Yaser taught me machine learning, and I always found the lectures clear and
interesting, so I expect many people can benefit from watching. Relative
toAndrew Ng'sML classthere are somewhat different areas of emphasis but the
topic is the same, so picking and choosing the union may be helpful.</p><p>3 0.74885547 <a title="469-lda-3" href="../hunch_net-2005/hunch_net-2005-05-29-Bad_ideas.html">76 hunch net-2005-05-29-Bad ideas</a></p>
<p>Introduction: I found these two essays on bad ideas interesting. Neither of these is written
from the viewpoint of research, but they are both highly relevant.Why smart
people have bad ideasby Paul GrahamWhy smart people defend bad ideasby Scott
Berkun (which appeared onslashdot)In my experience, bad ideas are
commonandover confidence in ideas is common. This overconfidence can take
either the form of excessive condemnation or excessive praise. Some of this is
necessary to the process of research. For example, some overconfidence in the
value of your own research is expected and probably necessary to motivate your
own investigation. Since research is a rather risky business, much of it does
not pan out. Learning to accept when something does not pan out is a critical
skill which is sometimes never acquired.Excessive condemnation can be a real
ill when it's encountered. This has two effects:When the penalty for being
wrong is too large, it means people have a great investment in defending
"their" ide</p><p>4 0.56132507 <a title="469-lda-4" href="../hunch_net-2005/hunch_net-2005-10-26-Fallback_Analysis_is_a_Secret_to_Useful_Algorithms.html">126 hunch net-2005-10-26-Fallback Analysis is a Secret to Useful Algorithms</a></p>
<p>Introduction: The ideal of theoretical algorithm analysis is to construct an algorithm with
accompanying optimality theorems proving that it is a useful algorithm. This
ideal often fails, particularly for learning algorithms and theory. The
general form of a theorem is:IfpreconditionsThenpostconditionsWhen we design
learning algorithms it is very common to come up with precondition assumptions
such as "the data is IID", "the learning problem is drawn from a known
distribution over learning problems", or "there is a perfect classifier". All
of these example preconditions can be false for real-world problems in ways
that are not easily detectable. This means that algorithms derived and
justified by these very common forms of analysis may be prone to catastrophic
failure in routine (mis)application.Wecanhope for better. Several different
kinds of learning algorithm analysis have been developed some of which have
fewer preconditions. Simply demanding that these forms of analysis be used may
be too stron</p><p>5 0.39350048 <a title="469-lda-5" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>Introduction: Claireasked me to be on the SODA program committee this year, which was quite
a bit of work.I had a relatively light load--merely 49 theory papers. Many of
these papers were not on subjects that I was expert about, so (as is common
for theory conferences) I found various reviewers that I trusted to help
review the papers. I ended up reviewing about 1/3 personally. There were a
couple instances where I ended up overruling a subreviewer whose logic seemed
off, but otherwise I generally let their reviews stand.There are some
differences in standards for paper reviews between the machine learning and
theory communities. In machine learning it is expected that a review be
detailed, while in the theory community this is often not the case. Every
paper given to me ended up with a review varying between somewhat and very
detailed.I'm sure not every author was happy with the outcome. While we did
our best to make good decisions, they were difficult decisions to make. For
example, if there is a</p><p>6 0.36546752 <a title="469-lda-6" href="../hunch_net-2008/hunch_net-2008-04-27-Watchword%3A_Supervised_Learning.html">299 hunch net-2008-04-27-Watchword: Supervised Learning</a></p>
<p>7 0.36520365 <a title="469-lda-7" href="../hunch_net-2006/hunch_net-2006-01-23-On_Coding_via_Mutual_Information_%26%23038%3B_Bayes_Nets.html">150 hunch net-2006-01-23-On Coding via Mutual Information &#038; Bayes Nets</a></p>
<p>8 0.36518025 <a title="469-lda-8" href="../hunch_net-2012/hunch_net-2012-10-26-ML_Symposium_and_Strata-Hadoop_World.html">475 hunch net-2012-10-26-ML Symposium and Strata-Hadoop World</a></p>
<p>9 0.36507747 <a title="469-lda-9" href="../hunch_net-2005/hunch_net-2005-05-06-Don%26%238217%3Bt_mix_the_solution_into_the_problem.html">67 hunch net-2005-05-06-Don&#8217;t mix the solution into the problem</a></p>
<p>10 0.36481884 <a title="469-lda-10" href="../hunch_net-2005/hunch_net-2005-12-09-Some_NIPS_papers.html">138 hunch net-2005-12-09-Some NIPS papers</a></p>
<p>11 0.36478209 <a title="469-lda-11" href="../hunch_net-2006/hunch_net-2006-06-15-IJCAI_is_out_of_season.html">184 hunch net-2006-06-15-IJCAI is out of season</a></p>
<p>12 0.36457199 <a title="469-lda-12" href="../hunch_net-2005/hunch_net-2005-03-29-Academic_Mechanism_Design.html">48 hunch net-2005-03-29-Academic Mechanism Design</a></p>
<p>13 0.36392993 <a title="469-lda-13" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>14 0.3638531 <a title="469-lda-14" href="../hunch_net-2005/hunch_net-2005-07-07-The_Limits_of_Learning_Theory.html">90 hunch net-2005-07-07-The Limits of Learning Theory</a></p>
<p>15 0.36089331 <a title="469-lda-15" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>16 0.3604264 <a title="469-lda-16" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<p>17 0.35723251 <a title="469-lda-17" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>18 0.35496232 <a title="469-lda-18" href="../hunch_net-2005/hunch_net-2005-02-23-Problem%3A_Reinforcement_Learning_with_Classification.html">27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</a></p>
<p>19 0.3535116 <a title="469-lda-19" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>20 0.352018 <a title="469-lda-20" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
