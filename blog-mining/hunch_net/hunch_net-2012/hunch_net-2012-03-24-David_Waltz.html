<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>460 hunch net-2012-03-24-David Waltz</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2012" href="../home/hunch_net-2012_home.html">hunch_net-2012</a> <a title="hunch_net-2012-460" href="#">hunch_net-2012-460</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>460 hunch net-2012-03-24-David Waltz</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2012-460-html" href="http://hunch.net/?p=2334">html</a></p><p>Introduction: hasdied. He lived a full life. I know him personally as a founder of theCenter
for Computational Learning Systemsand theNew York Machine Learning Symposium,
both of which have sheltered and promoted the advancement of machine learning.
I expect much of the New York area machine learning community will miss him,
as well as many others around the world.</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('york', 0.391), ('founder', 0.352), ('thecenter', 0.352), ('advancement', 0.352), ('symposium', 0.272), ('miss', 0.249), ('thenew', 0.223), ('full', 0.211), ('personally', 0.208), ('machine', 0.164), ('community', 0.162), ('around', 0.159), ('area', 0.146), ('others', 0.137), ('expect', 0.136), ('computational', 0.136), ('world', 0.132), ('know', 0.094), ('much', 0.072), ('well', 0.07)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="460-tfidf-1" href="../hunch_net-2012/hunch_net-2012-03-24-David_Waltz.html">460 hunch net-2012-03-24-David Waltz</a></p>
<p>Introduction: hasdied. He lived a full life. I know him personally as a founder of theCenter
for Computational Learning Systemsand theNew York Machine Learning Symposium,
both of which have sheltered and promoted the advancement of machine learning.
I expect much of the New York area machine learning community will miss him,
as well as many others around the world.</p><p>2 0.21969919 <a title="460-tfidf-2" href="../hunch_net-2013/hunch_net-2013-09-20-No_NY_ML_Symposium_in_2013%2C_and_some_good_news.html">489 hunch net-2013-09-20-No NY ML Symposium in 2013, and some good news</a></p>
<p>Introduction: There will be no New York ML Symposium this year. The core issue is thatNYASis
disorganized by people leaving, pushing back the date, with the current
candidate a spring symposium on March 28.Gunnarand I were outvoted here--we
were gung ho on organizing a fall symposium, but the rest of the committee
wants to wait.In some good news, most of theICML 2012 videoshave been restored
from a deep backup.</p><p>3 0.14724748 <a title="460-tfidf-3" href="../hunch_net-2010/hunch_net-2010-09-17-New_York_Area_Machine_Learning_Events.html">410 hunch net-2010-09-17-New York Area Machine Learning Events</a></p>
<p>Introduction: On Sept 21, there is anothermachine learning meetupwhere I'll be speaking.
Although the topic is contextual bandits, I think of it as "the future of
machine learning". In particular, it's all about how to learn in an
interactive environment, such as for ad display, trading, news recommendation,
etcâ&euro;ŚOn Sept 24, abstracts for theNew York Machine Learning Symposiumare due.
This is the largest Machine Learning event in the area, so it's a great way to
have a conversation with other people.On Oct 22, the NY ML Symposium actually
happens. This year, we are expanding the spotlights, and trying to have more
time for posters. In addition, we have a strong set of invited speakers:David
Blei,Sanjoy Dasgupta,Tommi Jaakkola, andYann LeCun. After the meeting, a
latehackNYrelated event is planned where students and startups can meet.I'd
also like to point out the relatedCS/Econ symposiumas I have interests there
as well.</p><p>4 0.14638828 <a title="460-tfidf-4" href="../hunch_net-2009/hunch_net-2009-08-27-New_York_Area_Machine_Learning_Events.html">369 hunch net-2009-08-27-New York Area Machine Learning Events</a></p>
<p>Introduction: Several events are happening in the NY area.Barriers in Computational Learning
Theory Workshop, Aug 28.That's tomorrow near Princeton. I'm looking forward to
speaking at this one on "Getting around Barriers in Learning Theory", but
several other talks are of interest, particularly to the CS theory
inclined.Claudia Perlichis running theINFORMS Data Mining Contestwith a
deadline of Sept. 25. This is a contest using real health record data (they
partnered withHealthCare Intelligence) to predict transfers and mortality. In
the current US health care reform debate, the case studies of high costs we
hear strongly suggest machine learning & statistics can save many billions.The
Singularity Summit October 3&4\. This is for the AIists out there. Several of
the talks look interesting, although unfortunately I'll miss it
forALT.Predictive Analytics World, Oct 20-21. This is stretching the
definition of "New York Area" a bit, but the train to DC is reasonable. This
is a conference of case studies</p><p>5 0.12956911 <a title="460-tfidf-5" href="../hunch_net-2014/hunch_net-2014-03-11-The_New_York_ML_Symposium%2C_take_2.html">494 hunch net-2014-03-11-The New York ML Symposium, take 2</a></p>
<p>Introduction: The 201314 isNew York Machine Learning Symposiumis finally happening on March
28th at theNew York Academy of Science. Every invited speaker interests me
personally. They are:Rayid Ghani(Chief Scientist at Obama 2012)Brian
Kingsbury(Speech Recognition @ IBM)Jorge Nocedal(who did LBFGS)We've been
somewhat disorganized in advertising this. As a consequence, anyone who has
not submitted an abstract but would like to do so may send one directly to me
(jl@hunch.net title NYASMLS) by Friday March 14. I will forward them to the
rest of the committee for consideration.</p><p>6 0.11072315 <a title="460-tfidf-6" href="../hunch_net-2008/hunch_net-2008-09-04-Fall_ML_Conferences.html">316 hunch net-2008-09-04-Fall ML Conferences</a></p>
<p>7 0.10075253 <a title="460-tfidf-7" href="../hunch_net-2006/hunch_net-2006-09-19-Luis_von_Ahn_is_awarded_a_MacArthur_fellowship..html">209 hunch net-2006-09-19-Luis von Ahn is awarded a MacArthur fellowship.</a></p>
<p>8 0.099723071 <a title="460-tfidf-8" href="../hunch_net-2012/hunch_net-2012-10-18-7th_Annual_Machine_Learning_Symposium.html">474 hunch net-2012-10-18-7th Annual Machine Learning Symposium</a></p>
<p>9 0.094144121 <a title="460-tfidf-9" href="../hunch_net-2012/hunch_net-2012-08-27-NYAS_ML_2012_and_ICML_2013.html">472 hunch net-2012-08-27-NYAS ML 2012 and ICML 2013</a></p>
<p>10 0.091834851 <a title="460-tfidf-10" href="../hunch_net-2011/hunch_net-2011-09-03-Fall_Machine_Learning_Events.html">443 hunch net-2011-09-03-Fall Machine Learning Events</a></p>
<p>11 0.090126425 <a title="460-tfidf-11" href="../hunch_net-2012/hunch_net-2012-05-12-ICML_accepted_papers_and_early_registration.html">465 hunch net-2012-05-12-ICML accepted papers and early registration</a></p>
<p>12 0.085463941 <a title="460-tfidf-12" href="../hunch_net-2006/hunch_net-2006-05-08-Big_machine_learning.html">178 hunch net-2006-05-08-Big machine learning</a></p>
<p>13 0.085403822 <a title="460-tfidf-13" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>14 0.08044254 <a title="460-tfidf-14" href="../hunch_net-2007/hunch_net-2007-01-15-The_Machine_Learning_Department.html">228 hunch net-2007-01-15-The Machine Learning Department</a></p>
<p>15 0.079362884 <a title="460-tfidf-15" href="../hunch_net-2012/hunch_net-2012-04-20-Both_new%3A_STOC_workshops_and_NEML.html">462 hunch net-2012-04-20-Both new: STOC workshops and NEML</a></p>
<p>16 0.07574676 <a title="460-tfidf-16" href="../hunch_net-2010/hunch_net-2010-10-28-NY_ML_Symposium_2010.html">415 hunch net-2010-10-28-NY ML Symposium 2010</a></p>
<p>17 0.074408546 <a title="460-tfidf-17" href="../hunch_net-2011/hunch_net-2011-10-24-2011_ML_symposium_and_the_bears.html">448 hunch net-2011-10-24-2011 ML symposium and the bears</a></p>
<p>18 0.074219584 <a title="460-tfidf-18" href="../hunch_net-2013/hunch_net-2013-04-15-NEML_II.html">481 hunch net-2013-04-15-NEML II</a></p>
<p>19 0.071807161 <a title="460-tfidf-19" href="../hunch_net-2012/hunch_net-2012-10-26-ML_Symposium_and_Strata-Hadoop_World.html">475 hunch net-2012-10-26-ML Symposium and Strata-Hadoop World</a></p>
<p>20 0.071085118 <a title="460-tfidf-20" href="../hunch_net-2005/hunch_net-2005-10-20-Machine_Learning_in_the_News.html">125 hunch net-2005-10-20-Machine Learning in the News</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.106), (1, 0.042), (2, 0.105), (3, 0.08), (4, 0.011), (5, 0.01), (6, -0.049), (7, 0.021), (8, 0.103), (9, -0.185), (10, 0.071), (11, 0.101), (12, 0.132), (13, -0.135), (14, -0.103), (15, 0.053), (16, -0.146), (17, -0.046), (18, -0.008), (19, 0.069), (20, -0.028), (21, -0.089), (22, -0.035), (23, -0.105), (24, -0.025), (25, 0.033), (26, -0.002), (27, 0.065), (28, -0.086), (29, -0.009), (30, 0.019), (31, 0.085), (32, 0.014), (33, -0.013), (34, 0.009), (35, -0.026), (36, -0.031), (37, -0.048), (38, -0.022), (39, -0.079), (40, 0.018), (41, 0.018), (42, -0.018), (43, -0.056), (44, 0.041), (45, 0.046), (46, 0.036), (47, 0.009), (48, -0.051), (49, -0.056)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.91602647 <a title="460-lsi-1" href="../hunch_net-2012/hunch_net-2012-03-24-David_Waltz.html">460 hunch net-2012-03-24-David Waltz</a></p>
<p>Introduction: hasdied. He lived a full life. I know him personally as a founder of theCenter
for Computational Learning Systemsand theNew York Machine Learning Symposium,
both of which have sheltered and promoted the advancement of machine learning.
I expect much of the New York area machine learning community will miss him,
as well as many others around the world.</p><p>2 0.78522319 <a title="460-lsi-2" href="../hunch_net-2012/hunch_net-2012-10-18-7th_Annual_Machine_Learning_Symposium.html">474 hunch net-2012-10-18-7th Annual Machine Learning Symposium</a></p>
<p>Introduction: A reminder that theNew York Academy of Scienceswill be hosting the7th Annual
Machine Learning Symposiumtomorrow from 9:30am.The main program will feature
invited talks fromPeter Bartlett,William Freeman, andVladimir Vapnik, along
with numerous spotlight talks and a poster session. Following the main
program,hackNYandMicrosoft Researchare sponsoring a networking hour with talks
from machine learning practitioners at NYC startups
(specificallybit.ly,Buzzfeed,Chartbeat, andSense Networks,Visual Revenue).
This should be of great interest to everyone considering working in machine
learning.</p><p>3 0.75400817 <a title="460-lsi-3" href="../hunch_net-2014/hunch_net-2014-03-11-The_New_York_ML_Symposium%2C_take_2.html">494 hunch net-2014-03-11-The New York ML Symposium, take 2</a></p>
<p>Introduction: The 201314 isNew York Machine Learning Symposiumis finally happening on March
28th at theNew York Academy of Science. Every invited speaker interests me
personally. They are:Rayid Ghani(Chief Scientist at Obama 2012)Brian
Kingsbury(Speech Recognition @ IBM)Jorge Nocedal(who did LBFGS)We've been
somewhat disorganized in advertising this. As a consequence, anyone who has
not submitted an abstract but would like to do so may send one directly to me
(jl@hunch.net title NYASMLS) by Friday March 14. I will forward them to the
rest of the committee for consideration.</p><p>4 0.71605885 <a title="460-lsi-4" href="../hunch_net-2010/hunch_net-2010-09-17-New_York_Area_Machine_Learning_Events.html">410 hunch net-2010-09-17-New York Area Machine Learning Events</a></p>
<p>Introduction: On Sept 21, there is anothermachine learning meetupwhere I'll be speaking.
Although the topic is contextual bandits, I think of it as "the future of
machine learning". In particular, it's all about how to learn in an
interactive environment, such as for ad display, trading, news recommendation,
etcâ&euro;ŚOn Sept 24, abstracts for theNew York Machine Learning Symposiumare due.
This is the largest Machine Learning event in the area, so it's a great way to
have a conversation with other people.On Oct 22, the NY ML Symposium actually
happens. This year, we are expanding the spotlights, and trying to have more
time for posters. In addition, we have a strong set of invited speakers:David
Blei,Sanjoy Dasgupta,Tommi Jaakkola, andYann LeCun. After the meeting, a
latehackNYrelated event is planned where students and startups can meet.I'd
also like to point out the relatedCS/Econ symposiumas I have interests there
as well.</p><p>5 0.69129032 <a title="460-lsi-5" href="../hunch_net-2013/hunch_net-2013-09-20-No_NY_ML_Symposium_in_2013%2C_and_some_good_news.html">489 hunch net-2013-09-20-No NY ML Symposium in 2013, and some good news</a></p>
<p>Introduction: There will be no New York ML Symposium this year. The core issue is thatNYASis
disorganized by people leaving, pushing back the date, with the current
candidate a spring symposium on March 28.Gunnarand I were outvoted here--we
were gung ho on organizing a fall symposium, but the rest of the committee
wants to wait.In some good news, most of theICML 2012 videoshave been restored
from a deep backup.</p><p>6 0.64913005 <a title="460-lsi-6" href="../hunch_net-2012/hunch_net-2012-04-20-Both_new%3A_STOC_workshops_and_NEML.html">462 hunch net-2012-04-20-Both new: STOC workshops and NEML</a></p>
<p>7 0.64825326 <a title="460-lsi-7" href="../hunch_net-2008/hunch_net-2008-09-04-Fall_ML_Conferences.html">316 hunch net-2008-09-04-Fall ML Conferences</a></p>
<p>8 0.62939942 <a title="460-lsi-8" href="../hunch_net-2011/hunch_net-2011-09-03-Fall_Machine_Learning_Events.html">443 hunch net-2011-09-03-Fall Machine Learning Events</a></p>
<p>9 0.62564778 <a title="460-lsi-9" href="../hunch_net-2006/hunch_net-2006-05-08-Big_machine_learning.html">178 hunch net-2006-05-08-Big machine learning</a></p>
<p>10 0.60024887 <a title="460-lsi-10" href="../hunch_net-2009/hunch_net-2009-08-27-New_York_Area_Machine_Learning_Events.html">369 hunch net-2009-08-27-New York Area Machine Learning Events</a></p>
<p>11 0.59957027 <a title="460-lsi-11" href="../hunch_net-2010/hunch_net-2010-10-28-NY_ML_Symposium_2010.html">415 hunch net-2010-10-28-NY ML Symposium 2010</a></p>
<p>12 0.56729537 <a title="460-lsi-12" href="../hunch_net-2005/hunch_net-2005-10-20-Machine_Learning_in_the_News.html">125 hunch net-2005-10-20-Machine Learning in the News</a></p>
<p>13 0.55885017 <a title="460-lsi-13" href="../hunch_net-2006/hunch_net-2006-04-09-Progress_in_Machine_Translation.html">171 hunch net-2006-04-09-Progress in Machine Translation</a></p>
<p>14 0.55214375 <a title="460-lsi-14" href="../hunch_net-2008/hunch_net-2008-10-20-New_York%26%238217%3Bs_ML_Day.html">322 hunch net-2008-10-20-New York&#8217;s ML Day</a></p>
<p>15 0.52353734 <a title="460-lsi-15" href="../hunch_net-2012/hunch_net-2012-10-26-ML_Symposium_and_Strata-Hadoop_World.html">475 hunch net-2012-10-26-ML Symposium and Strata-Hadoop World</a></p>
<p>16 0.50163108 <a title="460-lsi-16" href="../hunch_net-2009/hunch_net-2009-04-23-Jonathan_Chang_at_Slycoder.html">350 hunch net-2009-04-23-Jonathan Chang at Slycoder</a></p>
<p>17 0.48022342 <a title="460-lsi-17" href="../hunch_net-2013/hunch_net-2013-04-15-NEML_II.html">481 hunch net-2013-04-15-NEML II</a></p>
<p>18 0.44460517 <a title="460-lsi-18" href="../hunch_net-2012/hunch_net-2012-08-27-NYAS_ML_2012_and_ICML_2013.html">472 hunch net-2012-08-27-NYAS ML 2012 and ICML 2013</a></p>
<p>19 0.43851379 <a title="460-lsi-19" href="../hunch_net-2008/hunch_net-2008-05-25-Inappropriate_Mathematics_for_Machine_Learning.html">302 hunch net-2008-05-25-Inappropriate Mathematics for Machine Learning</a></p>
<p>20 0.43582776 <a title="460-lsi-20" href="../hunch_net-2009/hunch_net-2009-01-08-Predictive_Analytics_World.html">335 hunch net-2009-01-08-Predictive Analytics World</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(42, 0.155), (68, 0.656)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.93702888 <a title="460-lda-1" href="../hunch_net-2005/hunch_net-2005-02-09-Intuitions_from_applied_learning.html">16 hunch net-2005-02-09-Intuitions from applied learning</a></p>
<p>Introduction: Since learning is far from an exact science, it's good to pay attention to
basic intuitions of applied learning. Here are a few I've
collected.IntegrationIn Bayesian learning, the posterior is computed by an
integral, and the optimal thing to do is to predict according to this
integral. This phenomena seems to be far more general. Bagging, Boosting,
SVMs, and Neural Networks all take advantage of this idea to some extent. The
phenomena is more general: you can average over many differentclassification
predictorsto improve performance.
Sources:Zoubin,CaruanaDifferentiationDifferent pieces of an average should
differentiate to achieve good performance by different methods. This is know
as the 'symmetry breaking' problem for neural networks, and it's why weights
are initialized randomly. Boosting explicitly attempts to achieve good
differentiation by creating new, different, learning problems. Sources:Yann
LeCun,Phil LongDeep RepresentationHaving a deep representation is necessary
for hav</p><p>2 0.92630839 <a title="460-lda-2" href="../hunch_net-2012/hunch_net-2012-03-13-The_Submodularity_workshop_and_Lucca_Professorship.html">459 hunch net-2012-03-13-The Submodularity workshop and Lucca Professorship</a></p>
<p>Introduction: Ninapoints out theSubmodularity WorkshopMarch 19-20next week atGeorgia Tech.
Many people want to make Submodularity the new Convexity in machine learning,
and it certainly seems worth exploring.Sara Olsonalso points out atenured
faculty positionatIMT Luccawith a deadline ofMay 15th. Lucca happens to be the
ancestral home of 1/4 of my heritage</p><p>3 0.89548659 <a title="460-lda-3" href="../hunch_net-2005/hunch_net-2005-06-10-Workshops_are_not_Conferences.html">80 hunch net-2005-06-10-Workshops are not Conferences</a></p>
<p>Introduction: â&euro;Ś and you should use that fact.A workshop differs from a conference in that it
is about a focused group of people worrying about a focused topic. It also
differs in that a workshop is typically a "one-time affair" rather than a
series. (TheSnowbird learning workshopcounts as a conference in this
respect.)A common failure mode of both organizers and speakers at a workshop
is to treat it as a conference. This is "ok", but it is not really taking
advantage of the situation. Here are some things I've learned:For speakers: A
smaller audience means it can be more interactive. Interactive means a better
chance to avoid losing your audience and a more interesting presentation
(because you can adapt to your audience). Greater focus amongst the
participants means you can get to the heart of the matter more easily, and
discuss tradeoffs more carefully. Unlike conferences, relevance is more valued
than newness.For organizers: Not everything needs to be in a conference style
presentation format (i.</p><p>4 0.87123597 <a title="460-lda-4" href="../hunch_net-2009/hunch_net-2009-02-04-Optimal_Proxy_Loss_for_Classification.html">341 hunch net-2009-02-04-Optimal Proxy Loss for Classification</a></p>
<p>Introduction: Many people in machine learning take advantage of the notion of a proxy loss:
A loss function which is much easier to optimize computationally than the loss
function imposed by the world. A canonical example is when we want to learn a
weight vectorwand predict according to a dot productfw(x)= sumiwixiwhere
optimizing squared loss(y-fw(x))2over many samples is much more tractable than
optimizing 0-1 lossI(y = Threshold(fw(x) - 0.5)).While the computational
advantages of optimizing a proxy loss are substantial, we are curious: which
proxy loss is best? The answer of course depends on what the real loss imposed
by the world is. For 0-1 loss classification, there are adherents to many
choices:Log loss. If we confine the prediction to[0,1], we can treat it as a
predicted probability that the label is1, and measure loss according tolog
1/p'(y|x)wherep'(y|x)is the predicted probability of the observed label. A
standard method for confining the prediction to[0,1]islogistic regressionwhich
expo</p><p>5 0.85096884 <a title="460-lda-5" href="../hunch_net-2006/hunch_net-2006-07-08-Some_recent_papers.html">192 hunch net-2006-07-08-Some recent papers</a></p>
<p>Introduction: It was a fine time for learning in Pittsburgh. John and Sam mentioned some of
my favorites. Here's a few more worth checking out:Online Multitask
LearningOfer Dekel, Phil Long, Yoram SingerThis is on my reading list.
Definitely an area I'm interested in.Maximum Entropy Distribution Estimation
with Generalized RegularizationMiroslav DudÃƒÂ­k, Robert E. SchapireLearning
near-optimal policies with Bellman-residual minimization based fitted policy
iteration and a single sample pathAndrÃƒÂ¡s Antos, Csaba SzepesvÃƒÂ¡ri,
RÃƒÂ©mi MunosAgain, on the list to read. I saw Csaba and Remi talk about this
and related work at an ICML Workshop on Kernel Reinforcement Learning. The big
question in my head is how this compares/contrasts with existing work
inreductions to reinforcement learning.Are there
advantages/disadvantages?Higher Order Learning On Graphs>by Sameer Agarwal,
Kristin Branson, and Serge Belongie, looks to be interesteding. They seem to
poo-poo "tensorization" of existing graph algorithm</p><p>6 0.83497053 <a title="460-lda-6" href="../hunch_net-2009/hunch_net-2009-01-21-Nearly_all_natural_problems_require_nonlinearity.html">337 hunch net-2009-01-21-Nearly all natural problems require nonlinearity</a></p>
<p>same-blog 7 0.81336814 <a title="460-lda-7" href="../hunch_net-2012/hunch_net-2012-03-24-David_Waltz.html">460 hunch net-2012-03-24-David Waltz</a></p>
<p>8 0.74212056 <a title="460-lda-8" href="../hunch_net-2008/hunch_net-2008-04-12-It_Doesn%26%238217%3Bt_Stop.html">295 hunch net-2008-04-12-It Doesn&#8217;t Stop</a></p>
<p>9 0.71659374 <a title="460-lda-9" href="../hunch_net-2005/hunch_net-2005-08-22-Do_you_believe_in_induction%3F.html">104 hunch net-2005-08-22-Do you believe in induction?</a></p>
<p>10 0.67596859 <a title="460-lda-10" href="../hunch_net-2007/hunch_net-2007-05-12-Loss_Function_Semantics.html">245 hunch net-2007-05-12-Loss Function Semantics</a></p>
<p>11 0.59918952 <a title="460-lda-11" href="../hunch_net-2011/hunch_net-2011-07-11-Interesting_Neural_Network_Papers_at_ICML_2011.html">438 hunch net-2011-07-11-Interesting Neural Network Papers at ICML 2011</a></p>
<p>12 0.59392291 <a title="460-lda-12" href="../hunch_net-2005/hunch_net-2005-09-10-%26%238220%3BFailure%26%238221%3B_is_an_option.html">110 hunch net-2005-09-10-&#8220;Failure&#8221; is an option</a></p>
<p>13 0.5920049 <a title="460-lda-13" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>14 0.59184647 <a title="460-lda-14" href="../hunch_net-2005/hunch_net-2005-11-07-Prediction_Competitions.html">129 hunch net-2005-11-07-Prediction Competitions</a></p>
<p>15 0.5904758 <a title="460-lda-15" href="../hunch_net-2007/hunch_net-2007-08-12-Exponentiated_Gradient.html">258 hunch net-2007-08-12-Exponentiated Gradient</a></p>
<p>16 0.5651058 <a title="460-lda-16" href="../hunch_net-2007/hunch_net-2007-11-28-Computational_Consequences_of_Classification.html">274 hunch net-2007-11-28-Computational Consequences of Classification</a></p>
<p>17 0.5508725 <a title="460-lda-17" href="../hunch_net-2005/hunch_net-2005-02-01-Watchword%3A_Loss.html">9 hunch net-2005-02-01-Watchword: Loss</a></p>
<p>18 0.51382923 <a title="460-lda-18" href="../hunch_net-2005/hunch_net-2005-06-08-Question%3A_%26%238220%3BWhen_is_the_right_time_to_insert_the_loss_function%3F%26%238221%3B.html">79 hunch net-2005-06-08-Question: &#8220;When is the right time to insert the loss function?&#8221;</a></p>
<p>19 0.48882854 <a title="460-lda-19" href="../hunch_net-2009/hunch_net-2009-10-10-ALT_2009.html">374 hunch net-2009-10-10-ALT 2009</a></p>
<p>20 0.46800768 <a title="460-lda-20" href="../hunch_net-2008/hunch_net-2008-11-16-Observations_on_Linearity_for_Reductions_to_Regression.html">327 hunch net-2008-11-16-Observations on Linearity for Reductions to Regression</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
