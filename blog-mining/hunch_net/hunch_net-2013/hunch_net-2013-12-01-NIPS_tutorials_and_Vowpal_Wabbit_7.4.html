<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2013" href="../home/hunch_net-2013_home.html">hunch_net-2013</a> <a title="hunch_net-2013-492" href="#">hunch_net-2013-492</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2013-492-html" href="http://hunch.net/?p=2689">html</a></p><p>Introduction: At NIPS I’m giving a  tutorial on Learning to Interact .  In essence this is about dealing with causality in a contextual bandit framework.  Relative to  previous tutorials , I’ll be covering several new results that changed my understanding of the nature of the problem.  Note that  Judea Pearl  and  Elias Bareinboim  have a  tutorial on causality .  This might appear similar, but is quite different in practice.  Pearl and Bareinboim’s tutorial will be about the general concepts while mine will be about total mastery of the simplest nontrivial case, including code.  Luckily, they have the right order.  I recommend going to both   
 
I also just released version 7.4 of  Vowpal Wabbit .  When I was a frustrated learning theorist, I did not understand why people were not using learning reductions to solve problems.  I’ve been slowly discovering why with VW, and addressing the issues.  One of the issues is that machine learning itself was not automatic enough, while another is that creatin</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 At NIPS I’m giving a  tutorial on Learning to Interact . [sent-1, score-0.271]
</p><p>2 In essence this is about dealing with causality in a contextual bandit framework. [sent-2, score-0.189]
</p><p>3 Note that  Judea Pearl  and  Elias Bareinboim  have a  tutorial on causality . [sent-4, score-0.46]
</p><p>4 Pearl and Bareinboim’s tutorial will be about the general concepts while mine will be about total mastery of the simplest nontrivial case, including code. [sent-6, score-0.674]
</p><p>5 When I was a frustrated learning theorist, I did not understand why people were not using learning reductions to solve problems. [sent-10, score-0.227]
</p><p>6 One of the issues is that machine learning itself was not automatic enough, while another is that creating a very low overhead process for doing learning reductions is vitally important. [sent-12, score-0.132]
</p><p>7 These have been addressed well enough that we are starting to see compelling results. [sent-13, score-0.141]
</p><p>8 Various changes:     The internal learning reduction interface has been substantially improved. [sent-14, score-0.182]
</p><p>9 It’s now pretty easy to write new learning reduction. [sent-15, score-0.086]
</p><p>10 This is a very simple reduction which just binarizes the prediction. [sent-18, score-0.182]
</p><p>11 More improvements are coming, but this is good enough that other people have started contributing reductions. [sent-19, score-0.229]
</p><p>12 Zhen Qin  had a very productive internship with  Vaclav Petricek  at  eharmony  resulting in several systemic modifications and some new reductions, including:    A direct hash inversion implementation for use in debugging. [sent-20, score-0.701]
</p><p>13 A holdout system which takes over for progressive validation when multiple passes over data are used. [sent-21, score-0.253]
</p><p>14 An online bootstrap mechanism system which efficiently provides some understanding of prediction variations and which can sometimes effectively trade computational time for increased accuracy via ensembling. [sent-23, score-0.326]
</p><p>15 This will be discussed at the  biglearn workshop  at NIPS. [sent-24, score-0.11]
</p><p>16 A top-k reduction which chooses the top-k of any set of base instances. [sent-25, score-0.182]
</p><p>17 Hal Daume  has a new implementation of  Searn  (and  Dagger , the codes are unified) which makes structured prediction solutions far more natural. [sent-26, score-0.122]
</p><p>18 He has optimized this quite thoroughly (exercising the reduction stack in the process), resulting in this pretty graph. [sent-27, score-0.688]
</p><p>19 Fully optimized code is typically rough, but this one is  less than 100 lines . [sent-30, score-0.214]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('crf', 0.284), ('tutorial', 0.271), ('bareinboim', 0.213), ('pearl', 0.213), ('causality', 0.189), ('reduction', 0.182), ('enough', 0.141), ('break', 0.134), ('resulting', 0.134), ('reductions', 0.132), ('vw', 0.128), ('optimized', 0.122), ('implementation', 0.122), ('workshop', 0.11), ('productive', 0.095), ('mastery', 0.095), ('honest', 0.095), ('frustrated', 0.095), ('holdout', 0.095), ('vaclav', 0.095), ('eharmony', 0.095), ('theorist', 0.095), ('dagger', 0.095), ('code', 0.092), ('systemic', 0.088), ('stack', 0.088), ('modifications', 0.088), ('contributing', 0.088), ('pretty', 0.086), ('bootstrap', 0.083), ('unified', 0.083), ('skiing', 0.083), ('luckily', 0.083), ('trade', 0.083), ('skip', 0.083), ('bay', 0.083), ('provides', 0.081), ('progressive', 0.079), ('passes', 0.079), ('mine', 0.079), ('keeps', 0.079), ('hash', 0.079), ('variations', 0.079), ('nips', 0.077), ('including', 0.077), ('thoroughly', 0.076), ('daume', 0.076), ('concepts', 0.076), ('nontrivial', 0.076), ('edit', 0.073)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000002 <a title="492-tfidf-1" href="../hunch_net-2013/hunch_net-2013-12-01-NIPS_tutorials_and_Vowpal_Wabbit_7.4.html">492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</a></p>
<p>Introduction: At NIPS I’m giving a  tutorial on Learning to Interact .  In essence this is about dealing with causality in a contextual bandit framework.  Relative to  previous tutorials , I’ll be covering several new results that changed my understanding of the nature of the problem.  Note that  Judea Pearl  and  Elias Bareinboim  have a  tutorial on causality .  This might appear similar, but is quite different in practice.  Pearl and Bareinboim’s tutorial will be about the general concepts while mine will be about total mastery of the simplest nontrivial case, including code.  Luckily, they have the right order.  I recommend going to both   
 
I also just released version 7.4 of  Vowpal Wabbit .  When I was a frustrated learning theorist, I did not understand why people were not using learning reductions to solve problems.  I’ve been slowly discovering why with VW, and addressing the issues.  One of the issues is that machine learning itself was not automatic enough, while another is that creatin</p><p>2 0.17245895 <a title="492-tfidf-2" href="../hunch_net-2012/hunch_net-2012-09-29-Vowpal_Wabbit%2C_version_7.0.html">473 hunch net-2012-09-29-Vowpal Wabbit, version 7.0</a></p>
<p>Introduction: A new version of  VW  is  out .  The primary changes are:
  
  Learning Reductions : I’ve wanted to get  learning reductions  working and we’ve finally done it.  Not everything is implemented yet, but VW now supports direct:
 
 Multiclass Classification  –oaa  or  –ect . 
 Cost Sensitive Multiclass Classification  –csoaa  or  –wap . 
 Contextual Bandit Classification  –cb . 
 Sequential Structured Prediction   –searn  or  –dagger  
 

In addition, it is now easy to build your own custom learning reductions for various plausible uses: feature diddling, custom structured prediction problems, or alternate learning reductions.  This effort is far from done, but it is now in a generally useful state.  Note that all learning reductions inherit the ability to do cluster parallel learning.

 
  Library interface :  VW now has a basic library interface.  The library provides most of the functionality of VW, with the limitation that it is monolithic and nonreentrant.  These will be improved over</p><p>3 0.14994986 <a title="492-tfidf-3" href="../hunch_net-2011/hunch_net-2011-08-20-The_Large_Scale_Learning_Survey_Tutorial.html">442 hunch net-2011-08-20-The Large Scale Learning Survey Tutorial</a></p>
<p>Introduction: Ron Bekkerman  initiated an effort to create an  edited book on parallel machine learning  that  Misha  and I have been helping with.  The breadth of efforts to parallelize machine learning surprised me: I was only aware of a small fraction initially.
 
This put us in a unique position, with knowledge of a wide array of different efforts, so it is natural to put together a  survey tutorial on the subject of parallel learning  for  KDD , tomorrow.  This tutorial is  not  limited to the book itself however, as several interesting new algorithms have come out since we started inviting chapters.  
 
This tutorial should interest anyone trying to use machine learning on significant quantities of data, anyone interested in developing algorithms for such, and of course who has bragging rights to the fastest learning algorithm on planet earth   
 
(Also note the Modeling with Hadoop tutorial just before ours which deals with one way of trying to speed up learning algorithms.  We have almost no</p><p>4 0.13581567 <a title="492-tfidf-4" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>Introduction: This post is partly meant as an advertisement for the  reductions tutorial   Alina ,  Bianca , and I are planning to do at  ICML .  Please come, if you are interested.
 
Many research programs can be thought of as finding and building new useful abstractions.  The running example I’ll use is  learning reductions  where I have experience.  The basic abstraction here is that we can build a learning algorithm capable of solving classification problems up to a small expected regret.   This is used repeatedly to solve more complex problems.
 
In working on a new abstraction, I think you typically run into many substantial problems of understanding, which make publishing particularly difficult.
  
 It is difficult to seriously discuss the reason behind or mechanism for abstraction in a conference paper with small page limits.  People rarely see such discussions and hence have little basis on which to think about new abstractions.    Another difficulty is that when building an abstraction, yo</p><p>5 0.12913397 <a title="492-tfidf-5" href="../hunch_net-2011/hunch_net-2011-12-13-Vowpal_Wabbit_version_6.1_%26%23038%3B_the_NIPS_tutorial.html">451 hunch net-2011-12-13-Vowpal Wabbit version 6.1 &#038; the NIPS tutorial</a></p>
<p>Introduction: I just made  version 6.1  of  Vowpal Wabbit .  Relative to  6.0 , there are few new features, but many refinements. 
  
 The cluster parallel learning code better supports multiple simultaneous runs, and other forms of parallelism have been mostly removed.  This incidentally significantly simplifies the learning core. 
 The online learning algorithms are more general, with support for l 1  (via a truncated gradient variant) and l 2  regularization, and a generalized form of variable metric learning. 
 There is a solid persistent server mode which can train online, as well as serve answers to many simultaneous queries, either in text or binary. 
  
This should be a very good release if you are just getting started, as we’ve made it compile more automatically out of the box, have several new  examples  and updated documentation.
 
As  per   tradition , we’re planning to do a tutorial at NIPS during the break at the  parallel learning workshop  at 2pm Spanish time Friday.  I’ll cover the</p><p>6 0.1281524 <a title="492-tfidf-6" href="../hunch_net-2010/hunch_net-2010-12-04-Vowpal_Wabbit%2C_version_5.0%2C_and_the_second_heresy.html">419 hunch net-2010-12-04-Vowpal Wabbit, version 5.0, and the second heresy</a></p>
<p>7 0.12782763 <a title="492-tfidf-7" href="../hunch_net-2005/hunch_net-2005-08-18-SVM_Adaptability.html">103 hunch net-2005-08-18-SVM Adaptability</a></p>
<p>8 0.12683631 <a title="492-tfidf-8" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>9 0.12285751 <a title="492-tfidf-9" href="../hunch_net-2011/hunch_net-2011-03-27-Vowpal_Wabbit%2C_v5.1.html">428 hunch net-2011-03-27-Vowpal Wabbit, v5.1</a></p>
<p>10 0.11912999 <a title="492-tfidf-10" href="../hunch_net-2009/hunch_net-2009-12-07-Vowpal_Wabbit_version_4.0%2C_and_a_NIPS_heresy.html">381 hunch net-2009-12-07-Vowpal Wabbit version 4.0, and a NIPS heresy</a></p>
<p>11 0.11388191 <a title="492-tfidf-11" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>12 0.11361447 <a title="492-tfidf-12" href="../hunch_net-2010/hunch_net-2010-11-18-ICML_2011_%26%238211%3B_Call_for_Tutorials.html">417 hunch net-2010-11-18-ICML 2011 &#8211; Call for Tutorials</a></p>
<p>13 0.11098382 <a title="492-tfidf-13" href="../hunch_net-2009/hunch_net-2009-07-31-Vowpal_Wabbit_Open_Source_Project.html">365 hunch net-2009-07-31-Vowpal Wabbit Open Source Project</a></p>
<p>14 0.10767895 <a title="492-tfidf-14" href="../hunch_net-2007/hunch_net-2007-03-15-Alternative_Machine_Learning_Reductions_Definitions.html">236 hunch net-2007-03-15-Alternative Machine Learning Reductions Definitions</a></p>
<p>15 0.10519683 <a title="492-tfidf-15" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>16 0.097587943 <a title="492-tfidf-16" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>17 0.09325248 <a title="492-tfidf-17" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>18 0.092703253 <a title="492-tfidf-18" href="../hunch_net-2007/hunch_net-2007-12-21-Vowpal_Wabbit_Code_Release.html">281 hunch net-2007-12-21-Vowpal Wabbit Code Release</a></p>
<p>19 0.092422836 <a title="492-tfidf-19" href="../hunch_net-2006/hunch_net-2006-05-05-An_ICML_reject.html">177 hunch net-2006-05-05-An ICML reject</a></p>
<p>20 0.091214001 <a title="492-tfidf-20" href="../hunch_net-2011/hunch_net-2011-08-15-Vowpal_Wabbit_6.0.html">441 hunch net-2011-08-15-Vowpal Wabbit 6.0</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.217), (1, 0.034), (2, -0.077), (3, -0.052), (4, 0.033), (5, 0.085), (6, 0.021), (7, -0.07), (8, -0.131), (9, 0.09), (10, -0.109), (11, -0.086), (12, 0.03), (13, 0.104), (14, -0.038), (15, -0.104), (16, 0.027), (17, 0.025), (18, 0.007), (19, -0.115), (20, 0.001), (21, 0.006), (22, -0.026), (23, -0.039), (24, -0.032), (25, -0.01), (26, -0.06), (27, -0.034), (28, 0.005), (29, 0.102), (30, 0.059), (31, -0.07), (32, 0.056), (33, -0.003), (34, -0.043), (35, 0.11), (36, 0.004), (37, 0.039), (38, 0.028), (39, 0.14), (40, -0.032), (41, 0.109), (42, -0.114), (43, -0.091), (44, 0.002), (45, 0.02), (46, 0.042), (47, 0.039), (48, -0.05), (49, -0.032)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94800133 <a title="492-lsi-1" href="../hunch_net-2013/hunch_net-2013-12-01-NIPS_tutorials_and_Vowpal_Wabbit_7.4.html">492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</a></p>
<p>Introduction: At NIPS I’m giving a  tutorial on Learning to Interact .  In essence this is about dealing with causality in a contextual bandit framework.  Relative to  previous tutorials , I’ll be covering several new results that changed my understanding of the nature of the problem.  Note that  Judea Pearl  and  Elias Bareinboim  have a  tutorial on causality .  This might appear similar, but is quite different in practice.  Pearl and Bareinboim’s tutorial will be about the general concepts while mine will be about total mastery of the simplest nontrivial case, including code.  Luckily, they have the right order.  I recommend going to both   
 
I also just released version 7.4 of  Vowpal Wabbit .  When I was a frustrated learning theorist, I did not understand why people were not using learning reductions to solve problems.  I’ve been slowly discovering why with VW, and addressing the issues.  One of the issues is that machine learning itself was not automatic enough, while another is that creatin</p><p>2 0.80097002 <a title="492-lsi-2" href="../hunch_net-2012/hunch_net-2012-09-29-Vowpal_Wabbit%2C_version_7.0.html">473 hunch net-2012-09-29-Vowpal Wabbit, version 7.0</a></p>
<p>Introduction: A new version of  VW  is  out .  The primary changes are:
  
  Learning Reductions : I’ve wanted to get  learning reductions  working and we’ve finally done it.  Not everything is implemented yet, but VW now supports direct:
 
 Multiclass Classification  –oaa  or  –ect . 
 Cost Sensitive Multiclass Classification  –csoaa  or  –wap . 
 Contextual Bandit Classification  –cb . 
 Sequential Structured Prediction   –searn  or  –dagger  
 

In addition, it is now easy to build your own custom learning reductions for various plausible uses: feature diddling, custom structured prediction problems, or alternate learning reductions.  This effort is far from done, but it is now in a generally useful state.  Note that all learning reductions inherit the ability to do cluster parallel learning.

 
  Library interface :  VW now has a basic library interface.  The library provides most of the functionality of VW, with the limitation that it is monolithic and nonreentrant.  These will be improved over</p><p>3 0.70340383 <a title="492-lsi-3" href="../hunch_net-2009/hunch_net-2009-12-07-Vowpal_Wabbit_version_4.0%2C_and_a_NIPS_heresy.html">381 hunch net-2009-12-07-Vowpal Wabbit version 4.0, and a NIPS heresy</a></p>
<p>Introduction: I’m releasing  version 4.0 ( tarball ) of  Vowpal Wabbit .  The biggest change (by far) in this release is experimental support for cluster parallelism, with notable help from  Daniel Hsu .  
 
I also took advantage of the major version number to introduce some incompatible changes, including switching to  murmurhash 2 , and other alterations to cachefiles.  You’ll need to delete and regenerate them.  In addition, the precise specification for a “tag” (i.e. string that can be used to identify an example) changed—you can’t have a space between the tag and the ‘|’ at the beginning of the feature namespace.  
 
And, of course, we made it faster.
 
For the future, I put up my  todo list  outlining the major future improvements I want to see in the code.  I’m planning to discuss the current mechanism and results of the cluster parallel implementation at the  large scale machine learning workshop  at  NIPS  later this week.  Several people have asked me to do a tutorial/walkthrough of VW, wh</p><p>4 0.61792701 <a title="492-lsi-4" href="../hunch_net-2011/hunch_net-2011-12-13-Vowpal_Wabbit_version_6.1_%26%23038%3B_the_NIPS_tutorial.html">451 hunch net-2011-12-13-Vowpal Wabbit version 6.1 &#038; the NIPS tutorial</a></p>
<p>Introduction: I just made  version 6.1  of  Vowpal Wabbit .  Relative to  6.0 , there are few new features, but many refinements. 
  
 The cluster parallel learning code better supports multiple simultaneous runs, and other forms of parallelism have been mostly removed.  This incidentally significantly simplifies the learning core. 
 The online learning algorithms are more general, with support for l 1  (via a truncated gradient variant) and l 2  regularization, and a generalized form of variable metric learning. 
 There is a solid persistent server mode which can train online, as well as serve answers to many simultaneous queries, either in text or binary. 
  
This should be a very good release if you are just getting started, as we’ve made it compile more automatically out of the box, have several new  examples  and updated documentation.
 
As  per   tradition , we’re planning to do a tutorial at NIPS during the break at the  parallel learning workshop  at 2pm Spanish time Friday.  I’ll cover the</p><p>5 0.60095352 <a title="492-lsi-5" href="../hunch_net-2010/hunch_net-2010-12-04-Vowpal_Wabbit%2C_version_5.0%2C_and_the_second_heresy.html">419 hunch net-2010-12-04-Vowpal Wabbit, version 5.0, and the second heresy</a></p>
<p>Introduction: I’ve released  version 5.0  of the  Vowpal Wabbit  online learning software.  The major number has changed since the  last release  because I regard all earlier versions as obsolete—there are several new algorithms & features including substantial changes and upgrades to the default learning algorithm.  
 
The biggest changes are new algorithms:
  
  Nikos  and I improved the default algorithm.  The basic update rule still uses gradient descent, but the size of the update is carefully controlled so that it’s impossible to overrun the label.  In addition, the normalization has changed.  Computationally, these changes are virtually free and yield better results, sometimes much better.  Less careful updates can be reenabled with –loss_function classic, although results are still not identical to previous due to normalization changes. 
 Nikos also implemented the per-feature learning rates as per these  two   papers .  Often, this works better than the default algorithm.  It isn’t the defa</p><p>6 0.56154567 <a title="492-lsi-6" href="../hunch_net-2011/hunch_net-2011-08-15-Vowpal_Wabbit_6.0.html">441 hunch net-2011-08-15-Vowpal Wabbit 6.0</a></p>
<p>7 0.55724168 <a title="492-lsi-7" href="../hunch_net-2009/hunch_net-2009-07-31-Vowpal_Wabbit_Open_Source_Project.html">365 hunch net-2009-07-31-Vowpal Wabbit Open Source Project</a></p>
<p>8 0.54510278 <a title="492-lsi-8" href="../hunch_net-2011/hunch_net-2011-08-20-The_Large_Scale_Learning_Survey_Tutorial.html">442 hunch net-2011-08-20-The Large Scale Learning Survey Tutorial</a></p>
<p>9 0.53738755 <a title="492-lsi-9" href="../hunch_net-2005/hunch_net-2005-08-18-SVM_Adaptability.html">103 hunch net-2005-08-18-SVM Adaptability</a></p>
<p>10 0.5248577 <a title="492-lsi-10" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>11 0.51406687 <a title="492-lsi-11" href="../hunch_net-2011/hunch_net-2011-06-22-Ultra_LDA.html">436 hunch net-2011-06-22-Ultra LDA</a></p>
<p>12 0.51334196 <a title="492-lsi-12" href="../hunch_net-2011/hunch_net-2011-03-27-Vowpal_Wabbit%2C_v5.1.html">428 hunch net-2011-03-27-Vowpal Wabbit, v5.1</a></p>
<p>13 0.48469132 <a title="492-lsi-13" href="../hunch_net-2010/hunch_net-2010-11-18-ICML_2011_%26%238211%3B_Call_for_Tutorials.html">417 hunch net-2010-11-18-ICML 2011 &#8211; Call for Tutorials</a></p>
<p>14 0.48188052 <a title="492-lsi-14" href="../hunch_net-2008/hunch_net-2008-11-16-Observations_on_Linearity_for_Reductions_to_Regression.html">327 hunch net-2008-11-16-Observations on Linearity for Reductions to Regression</a></p>
<p>15 0.47463602 <a title="492-lsi-15" href="../hunch_net-2005/hunch_net-2005-02-25-Solution%3A_Reinforcement_Learning_with_Classification.html">29 hunch net-2005-02-25-Solution: Reinforcement Learning with Classification</a></p>
<p>16 0.46420282 <a title="492-lsi-16" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>17 0.46327773 <a title="492-lsi-17" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>18 0.46090284 <a title="492-lsi-18" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>19 0.44969231 <a title="492-lsi-19" href="../hunch_net-2009/hunch_net-2009-01-21-Nearly_all_natural_problems_require_nonlinearity.html">337 hunch net-2009-01-21-Nearly all natural problems require nonlinearity</a></p>
<p>20 0.44486794 <a title="492-lsi-20" href="../hunch_net-2010/hunch_net-2010-09-28-Machined_Learnings.html">412 hunch net-2010-09-28-Machined Learnings</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.055), (3, 0.026), (13, 0.261), (27, 0.236), (30, 0.036), (38, 0.063), (48, 0.012), (53, 0.038), (55, 0.086), (94, 0.094)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.96279013 <a title="492-lda-1" href="../hunch_net-2006/hunch_net-2006-10-04-Health_of_Conferences_Wiki.html">212 hunch net-2006-10-04-Health of Conferences Wiki</a></p>
<p>Introduction: Aaron Hertzmann  points out the  health of conferences wiki , which has a great deal of information about how many different conferences function.</p><p>2 0.93723124 <a title="492-lda-2" href="../hunch_net-2005/hunch_net-2005-10-03-Not_ICML.html">117 hunch net-2005-10-03-Not ICML</a></p>
<p>Introduction: Alex Smola  showed me this  ICML 2006  webpage.  This is  NOT  the ICML we know, but rather some people at “Enformatika”.  Investigation shows that they registered with an anonymous yahoo email account from  dotregistrar.com  the “Home of the $6.79 wholesale domain!” and their nameservers are by  Turkticaret , a Turkish internet company.
 
It appears the website has since been altered to “ ICNL ” (the above link uses the google cache).
 
They say that imitation is the sincerest form of flattery, so the organizers of the real  ICML 2006  must feel quite flattered.</p><p>3 0.93433112 <a title="492-lda-3" href="../hunch_net-2010/hunch_net-2010-01-13-Sam_Roweis_died.html">386 hunch net-2010-01-13-Sam Roweis died</a></p>
<p>Introduction: and I can’t help but remember him.
 
I first met  Sam  as an undergraduate at  Caltech  where he was TA for  Hopfield ‘s class, and again when I visited  Gatsby , when he invited me to visit  Toronto , and at too many conferences to recount.  His personality was a combination of enthusiastic and thoughtful, with a great ability to phrase a problem so it’s solution must be understood.  With respect to my own work, Sam was the one who advised me to make  my first tutorial , leading to others, and to other things, all of which I’m grateful to him for.  In fact, my every interaction with Sam was positive, and that was his way.
 
His death is  being called a suicide  which is so incompatible with my understanding of Sam that it strains my credibility.  But we know that his many responsibilities were great, and it is well understood that basically all sane researchers have legions of inner doubts.  Having been depressed now and then myself, it’s helpful to understand at least intellectually</p><p>4 0.93021917 <a title="492-lda-4" href="../hunch_net-2005/hunch_net-2005-12-09-Machine_Learning_Thoughts.html">137 hunch net-2005-12-09-Machine Learning Thoughts</a></p>
<p>Introduction: I added a link to Olivier Bousquetâ&euro;&trade;s  machine learning thoughts  blog.  Several of the posts may be of interest.</p><p>same-blog 5 0.87586135 <a title="492-lda-5" href="../hunch_net-2013/hunch_net-2013-12-01-NIPS_tutorials_and_Vowpal_Wabbit_7.4.html">492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</a></p>
<p>Introduction: At NIPS I’m giving a  tutorial on Learning to Interact .  In essence this is about dealing with causality in a contextual bandit framework.  Relative to  previous tutorials , I’ll be covering several new results that changed my understanding of the nature of the problem.  Note that  Judea Pearl  and  Elias Bareinboim  have a  tutorial on causality .  This might appear similar, but is quite different in practice.  Pearl and Bareinboim’s tutorial will be about the general concepts while mine will be about total mastery of the simplest nontrivial case, including code.  Luckily, they have the right order.  I recommend going to both   
 
I also just released version 7.4 of  Vowpal Wabbit .  When I was a frustrated learning theorist, I did not understand why people were not using learning reductions to solve problems.  I’ve been slowly discovering why with VW, and addressing the issues.  One of the issues is that machine learning itself was not automatic enough, while another is that creatin</p><p>6 0.81525159 <a title="492-lda-6" href="../hunch_net-2006/hunch_net-2006-10-13-David_Pennock_starts_Oddhead.html">214 hunch net-2006-10-13-David Pennock starts Oddhead</a></p>
<p>7 0.80607259 <a title="492-lda-7" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>8 0.80097657 <a title="492-lda-8" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>9 0.72808313 <a title="492-lda-9" href="../hunch_net-2008/hunch_net-2008-08-04-Electoralmarkets.com.html">312 hunch net-2008-08-04-Electoralmarkets.com</a></p>
<p>10 0.70357251 <a title="492-lda-10" href="../hunch_net-2005/hunch_net-2005-11-28-A_question_of_quantification.html">133 hunch net-2005-11-28-A question of quantification</a></p>
<p>11 0.70296985 <a title="492-lda-11" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>12 0.70155096 <a title="492-lda-12" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>13 0.70143199 <a title="492-lda-13" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>14 0.69943976 <a title="492-lda-14" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>15 0.69853681 <a title="492-lda-15" href="../hunch_net-2007/hunch_net-2007-08-19-Choice_of_Metrics.html">259 hunch net-2007-08-19-Choice of Metrics</a></p>
<p>16 0.69847077 <a title="492-lda-16" href="../hunch_net-2007/hunch_net-2007-02-02-Thoughts_regarding_%26%238220%3BIs_machine_learning_different_from_statistics%3F%26%238221%3B.html">230 hunch net-2007-02-02-Thoughts regarding &#8220;Is machine learning different from statistics?&#8221;</a></p>
<p>17 0.69814676 <a title="492-lda-17" href="../hunch_net-2005/hunch_net-2005-06-08-Question%3A_%26%238220%3BWhen_is_the_right_time_to_insert_the_loss_function%3F%26%238221%3B.html">79 hunch net-2005-06-08-Question: &#8220;When is the right time to insert the loss function?&#8221;</a></p>
<p>18 0.69690847 <a title="492-lda-18" href="../hunch_net-2009/hunch_net-2009-06-03-Functionally_defined_Nonlinear_Dynamic_Models.html">359 hunch net-2009-06-03-Functionally defined Nonlinear Dynamic Models</a></p>
<p>19 0.69580662 <a title="492-lda-19" href="../hunch_net-2009/hunch_net-2009-11-23-ICML_2009_Workshops_%28and_Tutorials%29.html">379 hunch net-2009-11-23-ICML 2009 Workshops (and Tutorials)</a></p>
<p>20 0.69536072 <a title="492-lda-20" href="../hunch_net-2005/hunch_net-2005-03-15-The_State_of_Tight_Bounds.html">41 hunch net-2005-03-15-The State of Tight Bounds</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
