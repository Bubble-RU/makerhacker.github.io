<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>488 hunch net-2013-08-31-Extreme Classification workshop at NIPS</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2013" href="../home/hunch_net-2013_home.html">hunch_net-2013</a> <a title="hunch_net-2013-488" href="#">hunch_net-2013-488</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>488 hunch net-2013-08-31-Extreme Classification workshop at NIPS</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2013-488-html" href="http://hunch.net/?p=2668">html</a></p><p>Introduction: Manik  and I are organizing the  extreme classification  workshop at NIPS this year.  We have a number of good speakers lined up, but I would further encourage anyone working in the area to submit an abstract by October 9.  I believe this is an idea whose time has now come.
 
The NIPS website doesnâ&euro;&trade;t have other workshops listed yet, but I expect several others to be of significant interest.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Manik  and I are organizing the  extreme classification  workshop at NIPS this year. [sent-1, score-0.7]
</p><p>2 We have a number of good speakers lined up, but I would further encourage anyone working in the area to submit an abstract by October 9. [sent-2, score-1.841]
</p><p>3 I believe this is an idea whose time has now come. [sent-3, score-0.578]
</p><p>4 The NIPS website doesnâ&euro;&trade;t have other workshops listed yet, but I expect several others to be of significant interest. [sent-4, score-1.102]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lined', 0.344), ('listed', 0.287), ('nips', 0.279), ('whose', 0.266), ('organizing', 0.238), ('october', 0.238), ('speakers', 0.227), ('submit', 0.223), ('encourage', 0.214), ('abstract', 0.204), ('website', 0.204), ('extreme', 0.187), ('workshops', 0.171), ('anyone', 0.142), ('classification', 0.141), ('area', 0.141), ('doesn', 0.136), ('workshop', 0.134), ('expect', 0.132), ('interest', 0.129), ('others', 0.129), ('believe', 0.126), ('working', 0.123), ('idea', 0.117), ('significant', 0.113), ('yet', 0.111), ('would', 0.086), ('number', 0.082), ('time', 0.069), ('several', 0.066), ('good', 0.055)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="488-tfidf-1" href="../hunch_net-2013/hunch_net-2013-08-31-Extreme_Classification_workshop_at_NIPS.html">488 hunch net-2013-08-31-Extreme Classification workshop at NIPS</a></p>
<p>Introduction: Manik  and I are organizing the  extreme classification  workshop at NIPS this year.  We have a number of good speakers lined up, but I would further encourage anyone working in the area to submit an abstract by October 9.  I believe this is an idea whose time has now come.
 
The NIPS website doesnâ&euro;&trade;t have other workshops listed yet, but I expect several others to be of significant interest.</p><p>2 0.2063829 <a title="488-tfidf-2" href="../hunch_net-2010/hunch_net-2010-09-13-AIStats.html">409 hunch net-2010-09-13-AIStats</a></p>
<p>Introduction: Geoff Gordon  points out  AIStats 2011  in Ft. Lauderdale, Florida.  The  call for papers  is now out, due Nov. 1.  The plan is to  experiment with the review process  to encourage quality in several ways.  I expect to submit a paper and would encourage others with good research to do likewise.</p><p>3 0.20383562 <a title="488-tfidf-3" href="../hunch_net-2010/hunch_net-2010-08-20-The_Workshop_on_Cores%2C_Clusters%2C_and_Clouds.html">404 hunch net-2010-08-20-The Workshop on Cores, Clusters, and Clouds</a></p>
<p>Introduction: Alekh ,  John ,  Ofer , and I are organizing a  workshop  at  NIPS  this year on learning in parallel and distributed environments.  The general interest level in parallel learning seems to be growing rapidly, so I expect quite a bit of attendance.  Please join us if you are parallel-interested.
 
And, if you are working in the area of parallel learning, please consider  submitting an abstract  due Oct. 17 for presentation at the workshop.</p><p>4 0.19402702 <a title="488-tfidf-4" href="../hunch_net-2006/hunch_net-2006-11-02-2006_NIPS_workshops.html">216 hunch net-2006-11-02-2006 NIPS workshops</a></p>
<p>Introduction: I expect the  NIPS 2006 workshops  to be quite interesting, and recommend going for anyone interested in machine learning research.  (Most or all of the workshops webpages can be found two links deep.)</p><p>5 0.18639858 <a title="488-tfidf-5" href="../hunch_net-2008/hunch_net-2008-01-23-Why_Workshop%3F.html">285 hunch net-2008-01-23-Why Workshop?</a></p>
<p>Introduction: I second the  call for workshops at ICML/COLT/UAI .
 
 Several   times   before , details of why and how to run a  workshop have been mentioned.  
 
There is a simple reason to prefer workshops here: attendance.  The Helsinki colocation has placed workshops  directly between ICML and COLT/UAI , which is optimal for getting attendees from any conference.  In addition,  last year ICML had relatively few workshops  and NIPS workshops were overloaded.  In addition to  those that happened  a similar number were rejected.  The overload has strange consequences—for example,  the best attended workshop  wasn’t an official NIPS workshop.  Aside from intrinsic interest, the Deep Learning workshop benefited greatly from being off schedule.</p><p>6 0.17307028 <a title="488-tfidf-6" href="../hunch_net-2006/hunch_net-2006-07-25-Upcoming_conference.html">198 hunch net-2006-07-25-Upcoming conference</a></p>
<p>7 0.16768134 <a title="488-tfidf-7" href="../hunch_net-2005/hunch_net-2005-03-24-The_Role_of_Workshops.html">46 hunch net-2005-03-24-The Role of Workshops</a></p>
<p>8 0.16498357 <a title="488-tfidf-8" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>9 0.15101051 <a title="488-tfidf-9" href="../hunch_net-2009/hunch_net-2009-08-27-New_York_Area_Machine_Learning_Events.html">369 hunch net-2009-08-27-New York Area Machine Learning Events</a></p>
<p>10 0.14625829 <a title="488-tfidf-10" href="../hunch_net-2007/hunch_net-2007-02-22-Create_Your_Own_ICML_Workshop.html">234 hunch net-2007-02-22-Create Your Own ICML Workshop</a></p>
<p>11 0.14423376 <a title="488-tfidf-11" href="../hunch_net-2009/hunch_net-2009-10-26-NIPS_workshops.html">375 hunch net-2009-10-26-NIPS workshops</a></p>
<p>12 0.14122836 <a title="488-tfidf-12" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>13 0.13518032 <a title="488-tfidf-13" href="../hunch_net-2005/hunch_net-2005-05-14-NIPS.html">71 hunch net-2005-05-14-NIPS</a></p>
<p>14 0.13377763 <a title="488-tfidf-14" href="../hunch_net-2007/hunch_net-2007-10-14-NIPS_workshp%3A_Learning_Problem_Design.html">265 hunch net-2007-10-14-NIPS workshp: Learning Problem Design</a></p>
<p>15 0.13288893 <a title="488-tfidf-15" href="../hunch_net-2009/hunch_net-2009-11-23-ICML_2009_Workshops_%28and_Tutorials%29.html">379 hunch net-2009-11-23-ICML 2009 Workshops (and Tutorials)</a></p>
<p>16 0.13064414 <a title="488-tfidf-16" href="../hunch_net-2005/hunch_net-2005-09-19-NIPS_Workshops.html">113 hunch net-2005-09-19-NIPS Workshops</a></p>
<p>17 0.11738339 <a title="488-tfidf-17" href="../hunch_net-2011/hunch_net-2011-09-03-Fall_Machine_Learning_Events.html">443 hunch net-2011-09-03-Fall Machine Learning Events</a></p>
<p>18 0.115345 <a title="488-tfidf-18" href="../hunch_net-2012/hunch_net-2012-04-20-Both_new%3A_STOC_workshops_and_NEML.html">462 hunch net-2012-04-20-Both new: STOC workshops and NEML</a></p>
<p>19 0.10473908 <a title="488-tfidf-19" href="../hunch_net-2012/hunch_net-2012-08-27-NYAS_ML_2012_and_ICML_2013.html">472 hunch net-2012-08-27-NYAS ML 2012 and ICML 2013</a></p>
<p>20 0.10290992 <a title="488-tfidf-20" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.144), (1, -0.147), (2, -0.137), (3, -0.162), (4, 0.032), (5, 0.261), (6, 0.146), (7, 0.026), (8, -0.002), (9, -0.038), (10, -0.005), (11, -0.045), (12, 0.099), (13, 0.015), (14, -0.002), (15, 0.005), (16, -0.008), (17, -0.002), (18, -0.023), (19, 0.002), (20, -0.114), (21, 0.035), (22, -0.034), (23, -0.044), (24, -0.012), (25, 0.071), (26, 0.018), (27, 0.062), (28, 0.01), (29, -0.021), (30, 0.009), (31, 0.015), (32, -0.028), (33, -0.035), (34, 0.021), (35, 0.07), (36, -0.023), (37, 0.006), (38, 0.081), (39, 0.024), (40, 0.059), (41, 0.049), (42, -0.006), (43, 0.058), (44, -0.015), (45, -0.015), (46, 0.094), (47, -0.11), (48, -0.088), (49, -0.013)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99155188 <a title="488-lsi-1" href="../hunch_net-2013/hunch_net-2013-08-31-Extreme_Classification_workshop_at_NIPS.html">488 hunch net-2013-08-31-Extreme Classification workshop at NIPS</a></p>
<p>Introduction: Manik  and I are organizing the  extreme classification  workshop at NIPS this year.  We have a number of good speakers lined up, but I would further encourage anyone working in the area to submit an abstract by October 9.  I believe this is an idea whose time has now come.
 
The NIPS website doesnâ&euro;&trade;t have other workshops listed yet, but I expect several others to be of significant interest.</p><p>2 0.67488796 <a title="488-lsi-2" href="../hunch_net-2005/hunch_net-2005-03-24-The_Role_of_Workshops.html">46 hunch net-2005-03-24-The Role of Workshops</a></p>
<p>Introduction: A good workshop is often far more interesting than the papers at a conference.  This happens because a workshop has a much tighter focus than a conference.  Since you choose the workshops fitting your interest, the increased relevance can greatly enhance the level of your interest and attention.  Roughly speaking, a workshop program consists of elements related to a subject of your interest.  The main conference program consists of elements related to someoneâ&euro;&trade;s interest (which is rarely your own).  Workshops are more about doing research while conferences are more about presenting research.  
 
Several conferences have associated workshop programs, some with deadlines due shortly.
  
 
  ICML workshops  
 Due April 1 
 
 
  IJCAI workshops  
 Deadlines Vary 
 
 
 KDD workshops 
 Not yet finalized 
 
  
Anyone going to these conferences should examine the workshops and see if any are of interest.  (If none are, then maybe you should organize one next year.)</p><p>3 0.65988338 <a title="488-lsi-3" href="../hunch_net-2005/hunch_net-2005-09-19-NIPS_Workshops.html">113 hunch net-2005-09-19-NIPS Workshops</a></p>
<p>Introduction: Attendance at the  NIPS workshops  is highly recommended for both research and learning.   Unfortunately, there does not yet appear to be a public list of workshops. However, I found the following workshop webpages of interest:
  
  Machine Learning in Finance  
  Learning to Rank  
  Foundations of Active Learning  
  Machine Learning Based Robotics in Unstructured Environments  
  
There are  many  more workshops.  In fact, there are so many that it is not plausible anyone can attend every workshop they are interested in.  Maybe in future years the organizers can spread them out over more days to reduce overlap. 
 
Many of these workshops are accepting presentation proposals (due mid-October).</p><p>4 0.64672321 <a title="488-lsi-4" href="../hunch_net-2008/hunch_net-2008-01-23-Why_Workshop%3F.html">285 hunch net-2008-01-23-Why Workshop?</a></p>
<p>Introduction: I second the  call for workshops at ICML/COLT/UAI .
 
 Several   times   before , details of why and how to run a  workshop have been mentioned.  
 
There is a simple reason to prefer workshops here: attendance.  The Helsinki colocation has placed workshops  directly between ICML and COLT/UAI , which is optimal for getting attendees from any conference.  In addition,  last year ICML had relatively few workshops  and NIPS workshops were overloaded.  In addition to  those that happened  a similar number were rejected.  The overload has strange consequences—for example,  the best attended workshop  wasn’t an official NIPS workshop.  Aside from intrinsic interest, the Deep Learning workshop benefited greatly from being off schedule.</p><p>5 0.61003435 <a title="488-lsi-5" href="../hunch_net-2011/hunch_net-2011-09-03-Fall_Machine_Learning_Events.html">443 hunch net-2011-09-03-Fall Machine Learning Events</a></p>
<p>Introduction: Many Machine Learning related events are coming up this fall.
  
  September 9 ,  abstracts for the New York Machine Learning Symposium  are due.  Send a 2 page pdf, if interested, and note that we:
 
 widened submissions to be from anybody rather than students. 
 set aside a larger fraction of time for contributed submissions.  
 
 
  September 15 , there is a  machine learning meetup , where I’ll be discussing terascale learning at AOL. 
  September 16 , there is a  CS&Econ; day  at New York Academy of Sciences.  This is not ML focused, but it’s easy to imagine interest. 
  September 23 and later   NIPS workshop  submissions start coming due.  As usual, there are too many good ones, so I won’t be able to attend all those that interest me.  I do hope some workshop makers consider ICML this coming summer, as we are increasing to a 2 day format for you.  Here are a few that interest me:
 
  Big Learning  is about dealing with lots of data.  Abstracts are due  September 30 . 
 The  Bayes</p><p>6 0.59736699 <a title="488-lsi-6" href="../hunch_net-2006/hunch_net-2006-07-25-Upcoming_conference.html">198 hunch net-2006-07-25-Upcoming conference</a></p>
<p>7 0.59442687 <a title="488-lsi-7" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>8 0.59016359 <a title="488-lsi-8" href="../hunch_net-2010/hunch_net-2010-08-20-The_Workshop_on_Cores%2C_Clusters%2C_and_Clouds.html">404 hunch net-2010-08-20-The Workshop on Cores, Clusters, and Clouds</a></p>
<p>9 0.58783728 <a title="488-lsi-9" href="../hunch_net-2006/hunch_net-2006-11-02-2006_NIPS_workshops.html">216 hunch net-2006-11-02-2006 NIPS workshops</a></p>
<p>10 0.55187267 <a title="488-lsi-10" href="../hunch_net-2007/hunch_net-2007-02-22-Create_Your_Own_ICML_Workshop.html">234 hunch net-2007-02-22-Create Your Own ICML Workshop</a></p>
<p>11 0.55154568 <a title="488-lsi-11" href="../hunch_net-2005/hunch_net-2005-10-19-Workshop%3A_Atomic_Learning.html">124 hunch net-2005-10-19-Workshop: Atomic Learning</a></p>
<p>12 0.5453521 <a title="488-lsi-12" href="../hunch_net-2005/hunch_net-2005-05-14-NIPS.html">71 hunch net-2005-05-14-NIPS</a></p>
<p>13 0.53958744 <a title="488-lsi-13" href="../hunch_net-2008/hunch_net-2008-10-19-NIPS_2008_workshop_on_Kernel_Learning.html">321 hunch net-2008-10-19-NIPS 2008 workshop on Kernel Learning</a></p>
<p>14 0.515104 <a title="488-lsi-14" href="../hunch_net-2009/hunch_net-2009-10-26-NIPS_workshops.html">375 hunch net-2009-10-26-NIPS workshops</a></p>
<p>15 0.51314795 <a title="488-lsi-15" href="../hunch_net-2009/hunch_net-2009-11-23-ICML_2009_Workshops_%28and_Tutorials%29.html">379 hunch net-2009-11-23-ICML 2009 Workshops (and Tutorials)</a></p>
<p>16 0.50251073 <a title="488-lsi-16" href="../hunch_net-2007/hunch_net-2007-10-15-NIPS_workshops_extended_to_3_days.html">266 hunch net-2007-10-15-NIPS workshops extended to 3 days</a></p>
<p>17 0.49143586 <a title="488-lsi-17" href="../hunch_net-2012/hunch_net-2012-02-20-Berkeley_Streaming_Data_Workshop.html">455 hunch net-2012-02-20-Berkeley Streaming Data Workshop</a></p>
<p>18 0.47353494 <a title="488-lsi-18" href="../hunch_net-2010/hunch_net-2010-09-13-AIStats.html">409 hunch net-2010-09-13-AIStats</a></p>
<p>19 0.47308064 <a title="488-lsi-19" href="../hunch_net-2006/hunch_net-2006-05-21-NIPS_paper_evaluation_criteria.html">180 hunch net-2006-05-21-NIPS paper evaluation criteria</a></p>
<p>20 0.44983682 <a title="488-lsi-20" href="../hunch_net-2012/hunch_net-2012-04-20-Both_new%3A_STOC_workshops_and_NEML.html">462 hunch net-2012-04-20-Both new: STOC workshops and NEML</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(27, 0.048), (38, 0.636), (55, 0.157)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97251219 <a title="488-lda-1" href="../hunch_net-2013/hunch_net-2013-08-31-Extreme_Classification_workshop_at_NIPS.html">488 hunch net-2013-08-31-Extreme Classification workshop at NIPS</a></p>
<p>Introduction: Manik  and I are organizing the  extreme classification  workshop at NIPS this year.  We have a number of good speakers lined up, but I would further encourage anyone working in the area to submit an abstract by October 9.  I believe this is an idea whose time has now come.
 
The NIPS website doesnâ&euro;&trade;t have other workshops listed yet, but I expect several others to be of significant interest.</p><p>2 0.9653157 <a title="488-lda-2" href="../hunch_net-2005/hunch_net-2005-10-20-Machine_Learning_in_the_News.html">125 hunch net-2005-10-20-Machine Learning in the News</a></p>
<p>Introduction: The New York Times had a  short interview  about machine learning in datamining being used pervasively by the IRS and large corporations to predict who to audit and who to target for various marketing campaigns.  This is a big application area of machine learning.  It can be harmful (learning + databases = another way to invade privacy) or beneficial (as google demonstrates, better targeting of marketing campaigns is far less annoying).  This is yet more evidence that we can not rely upon “I’m just another fish in the school” logic for our expectations about treatment by government and large corporations.</p><p>3 0.9088307 <a title="488-lda-3" href="../hunch_net-2009/hunch_net-2009-01-27-Key_Scientific_Challenges.html">339 hunch net-2009-01-27-Key Scientific Challenges</a></p>
<p>Introduction: Yahoo released the  Key Scientific Challenges  program.  There is a  Machine Learning  list I worked on and a  Statistics  list which  Deepak  worked on.
 
I’m hoping this is taken quite seriously by graduate students.  The primary value, is that it gave us a chance to sit down and publicly specify directions of research which would be valuable to make progress on.  A good strategy for a beginning graduate student is to pick one of these directions, pursue it, and make substantial advances for a PhD.  The directions are sufficiently general that I’m sure any serious advance has applications well beyond Yahoo.
 
A secondary point, (which I’m sure is primary for many    ) is that there is money for graduate students here.  It’s unrestricted, so you can use it for any reasonable travel, supplies, etc…</p><p>4 0.8502863 <a title="488-lda-4" href="../hunch_net-2006/hunch_net-2006-05-23-What_is_the_best_regret_transform_reduction_from_multiclass_to_binary%3F.html">181 hunch net-2006-05-23-What is the best regret transform reduction from multiclass to binary?</a></p>
<p>Introduction: This post is about an open problem in learning reductions.
 
 Background  A reduction might transform a a multiclass prediction problem where there are  k  possible labels into a binary learning problem where there are only 2 possible labels.   On this induced binary problem we might learn a binary classifier with some error rate  e .  After subtracting the minimum possible (Bayes) error rate  b , we get a regret  r = e – b .  The  PECOC (Probabilistic Error Correcting Output Code) reduction has the property that binary regret  r  implies multiclass regret at most  4r 0.5  .
 
 The problem  This is not the “rightest” answer.  Consider the  k=2  case, where we reduce binary to binary.  There exists a reduction (the identity) with the property that regret  r  implies regret  r .  This is substantially superior to the transform given by the PECOC reduction, which suggests that a better reduction may exist for general  k .  For example, we can not rule out the possibility that a reduction</p><p>5 0.84047788 <a title="488-lda-5" href="../hunch_net-2005/hunch_net-2005-06-18-Lower_Bounds_for_Learning_Reductions.html">83 hunch net-2005-06-18-Lower Bounds for Learning Reductions</a></p>
<p>Introduction: Learning reductions  transform a solver of one type of learning problem into a solver of another type of learning problem.  When we analyze these for robustness we can make statement of the form “Reduction  R  has the property that regret  r  (or loss) on subproblems of type  A  implies regret at most   f ( r )  on the original problem of type  B “.
 
A lower bound for a learning reduction would have the form “for all reductions  R , there exists a learning problem of type  B  and learning algorithm for problems of type  A  where regret  r  on induced problems implies  at least  regret  f ( r )  for  B “.
 
The pursuit of lower bounds is often questionable because, unlike upper bounds, they do not yield practical algorithms.  Nevertheless, they may be helpful as a tool for thinking about what is learnable and how learnable it is.  This has already come up  here  and  here .
 
At the moment, there is no coherent theory of lower bounds for learning reductions, and we have little understa</p><p>6 0.8163023 <a title="488-lda-6" href="../hunch_net-2006/hunch_net-2006-04-06-Bounds_greater_than_1.html">170 hunch net-2006-04-06-Bounds greater than 1</a></p>
<p>7 0.71736979 <a title="488-lda-7" href="../hunch_net-2007/hunch_net-2007-02-16-The_Forgetting.html">233 hunch net-2007-02-16-The Forgetting</a></p>
<p>8 0.70525992 <a title="488-lda-8" href="../hunch_net-2009/hunch_net-2009-05-08-Computability_in_Artificial_Intelligence.html">353 hunch net-2009-05-08-Computability in Artificial Intelligence</a></p>
<p>9 0.63110125 <a title="488-lda-9" href="../hunch_net-2007/hunch_net-2007-03-15-Alternative_Machine_Learning_Reductions_Definitions.html">236 hunch net-2007-03-15-Alternative Machine Learning Reductions Definitions</a></p>
<p>10 0.60815126 <a title="488-lda-10" href="../hunch_net-2007/hunch_net-2007-06-24-Interesting_Papers_at_ICML_2007.html">251 hunch net-2007-06-24-Interesting Papers at ICML 2007</a></p>
<p>11 0.59969628 <a title="488-lda-11" href="../hunch_net-2008/hunch_net-2008-01-18-Datasets.html">284 hunch net-2008-01-18-Datasets</a></p>
<p>12 0.58951169 <a title="488-lda-12" href="../hunch_net-2007/hunch_net-2007-04-18-%2450K_Spock_Challenge.html">239 hunch net-2007-04-18-$50K Spock Challenge</a></p>
<p>13 0.54706788 <a title="488-lda-13" href="../hunch_net-2010/hunch_net-2010-09-13-AIStats.html">409 hunch net-2010-09-13-AIStats</a></p>
<p>14 0.50804096 <a title="488-lda-14" href="../hunch_net-2005/hunch_net-2005-05-16-Regret_minimizing_vs_error_limiting_reductions.html">72 hunch net-2005-05-16-Regret minimizing vs error limiting reductions</a></p>
<p>15 0.46430019 <a title="488-lda-15" href="../hunch_net-2005/hunch_net-2005-06-17-Reopening_RL-%3EClassification.html">82 hunch net-2005-06-17-Reopening RL->Classification</a></p>
<p>16 0.45927042 <a title="488-lda-16" href="../hunch_net-2007/hunch_net-2007-09-30-NIPS_workshops_are_out..html">264 hunch net-2007-09-30-NIPS workshops are out.</a></p>
<p>17 0.4536052 <a title="488-lda-17" href="../hunch_net-2005/hunch_net-2005-02-21-Problem%3A_Cross_Validation.html">26 hunch net-2005-02-21-Problem: Cross Validation</a></p>
<p>18 0.42487881 <a title="488-lda-18" href="../hunch_net-2011/hunch_net-2011-08-01-Interesting_papers_at_COLT_2011.html">439 hunch net-2011-08-01-Interesting papers at COLT 2011</a></p>
<p>19 0.41719896 <a title="488-lda-19" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>20 0.41333303 <a title="488-lda-20" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
