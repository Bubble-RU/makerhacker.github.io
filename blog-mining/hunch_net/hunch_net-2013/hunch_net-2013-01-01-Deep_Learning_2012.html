<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>477 hunch net-2013-01-01-Deep Learning 2012</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2013" href="../home/hunch_net-2013_home.html">hunch_net-2013</a> <a title="hunch_net-2013-477" href="#">hunch_net-2013-477</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>477 hunch net-2013-01-01-Deep Learning 2012</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2013-477-html" href="http://hunch.net/?p=2609">html</a></p><p>Introduction: 2012 was a tumultuous year for me, but it was undeniably a great year for deep
learning efforts. Signs of this include:Winning aKaggle competition.Wide
adoption ofdeep learning for speech recognition.Significantindustry
support.Gains inimagerecognition.This is a rare event in research: a
significant capability breakout. Congratulations are definitely in order for
those who managed to achieve it. At this point, deep learning algorithms seem
like a choice undeniably worth investigating for real applications with
significant data.</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('undeniably', 0.515), ('deep', 0.267), ('investigating', 0.243), ('signs', 0.243), ('winning', 0.243), ('adoption', 0.243), ('capability', 0.215), ('rare', 0.215), ('definitely', 0.188), ('significant', 0.185), ('managed', 0.18), ('speech', 0.18), ('year', 0.172), ('event', 0.162), ('worth', 0.147), ('include', 0.14), ('achieve', 0.129), ('choice', 0.114), ('applications', 0.11), ('order', 0.11)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="477-tfidf-1" href="../hunch_net-2013/hunch_net-2013-01-01-Deep_Learning_2012.html">477 hunch net-2013-01-01-Deep Learning 2012</a></p>
<p>Introduction: 2012 was a tumultuous year for me, but it was undeniably a great year for deep
learning efforts. Signs of this include:Winning aKaggle competition.Wide
adoption ofdeep learning for speech recognition.Significantindustry
support.Gains inimagerecognition.This is a rare event in research: a
significant capability breakout. Congratulations are definitely in order for
those who managed to achieve it. At this point, deep learning algorithms seem
like a choice undeniably worth investigating for real applications with
significant data.</p><p>2 0.16563594 <a title="477-tfidf-2" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>Introduction: Many learning algorithms used in practice are fairly simple. Viewed
representationally, many prediction algorithms either compute a linear
separator of basic features (perceptron, winnow, weighted majority, SVM) or
perhaps a linear separator of slightly more complex features (2-layer neural
networks or kernelized SVMs). Should we go beyond this, and start using "deep"
representations?What is deep learning?Intuitively, deep learning is about
learning to predict in ways which can involve complex dependencies between the
input (observed) features.Specifying this more rigorously turns out to be
rather difficult. Consider the following cases:SVM with Gaussian Kernel. This
is not considered deep learning, because an SVM with a gaussian kernel can't
succinctly represent certain decision surfaces. One ofYann LeCun's examples is
recognizing objects based on pixel values. An SVM will need a new support
vector for each significantly different background. Since the number of
distinct backgrounds i</p><p>3 0.14802854 <a title="477-tfidf-3" href="../hunch_net-2010/hunch_net-2010-08-23-Boosted_Decision_Trees_for_Deep_Learning.html">407 hunch net-2010-08-23-Boosted Decision Trees for Deep Learning</a></p>
<p>Introduction: About 4 years ago, I speculated thatdecision trees qualify as a deep learning
algorithmbecause they can make decisions which are substantially nonlinear in
the input representation.Ping Lihasproved this correct, empiricallyatUAIby
showing that boosted decision trees can beat deep belief networks on versions
ofMnistwhich are artificially hardened so as to make them solvable only by
deep learning algorithms.This is an important point, because the ability to
solve these sorts of problems is probably the best objective definition of a
deep learning algorithm we have. I'm not that surprised. In my experience, if
you can accept the computational drawbacks of a boosted decision tree, they
can achieve pretty good performance.Geoff Hintononce told me that the great
thing about deep belief networks is that they work. I understand that Ping had
very substantial difficulty in getting this published, so I hope some
reviewers step up to the standard of valuing what works.</p><p>4 0.10256583 <a title="477-tfidf-4" href="../hunch_net-2007/hunch_net-2007-01-10-A_Deep_Belief_Net_Learning_Problem.html">227 hunch net-2007-01-10-A Deep Belief Net Learning Problem</a></p>
<p>Introduction: "Deep learning" is used to describe learning architectures which have
significant depth (as a circuit).One claimis that shallow architectures (one
or two layers) can not concisely represent some functions while a circuit with
more depth can concisely represent these same functions. Proving lower bounds
on the size of a circuit is substantially harder than upper bounds (which are
constructive), but some results are known.Luca Trevisan'sclass notesdetail how
XOR is not concisely representable by "AC0â&euro;ł (= constant depth unbounded fan-in
AND, OR, NOT gates). This doesn't quite prove that depth is necessary for the
representations commonly used in learning (such as a thresholded weighted
sum), but it is strongly suggestive that this is so.Examples like this are a
bit disheartening because existing algorithms for deep learning (deep belief
nets, gradient descent on deep neural networks, and a perhaps decision trees
depending on who you ask) can't learn XOR very easily. Evidence so far
sugges</p><p>5 0.093751229 <a title="477-tfidf-5" href="../hunch_net-2013/hunch_net-2013-11-09-Graduates_and_Postdocs.html">490 hunch net-2013-11-09-Graduates and Postdocs</a></p>
<p>Introduction: Several strong graduates are on the job market this year.Alekh Agarwalmade
themost scalable public learning algorithmas an intern two years ago. He has a
deep and broad understanding of optimization and learning as well as the
ability and will to make things happen programming-wise. I've been privileged
to have Alekh visiting me in NY where he will be sorely missed.John
DuchicreatedAdagradwhich is a commonly helpful improvement over online
gradient descent that is seeing wide adoption, including inVowpal Wabbit. He
has a similarly deep and broad understanding of optimization and learning with
significant industry experience atGoogle. Alekh and John have often coauthored
together.Stephane Rossvisited me a year ago over the summer, implementing many
new algorithms and working out the firstscale free online update rulewhich is
now the default in Vowpal Wabbit. Stephane isnoton the market--Google robbed
the cradle successfullyI'm sure that he will do great things.Anna
Choromanskavisited me</p><p>6 0.090382405 <a title="477-tfidf-6" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<p>7 0.078223981 <a title="477-tfidf-7" href="../hunch_net-2008/hunch_net-2008-11-28-A_Bumper_Crop_of_Machine_Learning_Graduates.html">329 hunch net-2008-11-28-A Bumper Crop of Machine Learning Graduates</a></p>
<p>8 0.07275831 <a title="477-tfidf-8" href="../hunch_net-2011/hunch_net-2011-04-18-A_paper_not_at_Snowbird.html">431 hunch net-2011-04-18-A paper not at Snowbird</a></p>
<p>9 0.071129143 <a title="477-tfidf-9" href="../hunch_net-2009/hunch_net-2009-06-01-Multitask_Poisoning.html">358 hunch net-2009-06-01-Multitask Poisoning</a></p>
<p>10 0.070511915 <a title="477-tfidf-10" href="../hunch_net-2007/hunch_net-2007-12-19-Cool_and_interesting_things_seen_at_NIPS.html">279 hunch net-2007-12-19-Cool and interesting things seen at NIPS</a></p>
<p>11 0.070192248 <a title="477-tfidf-11" href="../hunch_net-2009/hunch_net-2009-09-29-Machine_Learning_Protests_at_the_G20.html">372 hunch net-2009-09-29-Machine Learning Protests at the G20</a></p>
<p>12 0.069019139 <a title="477-tfidf-12" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>13 0.063692234 <a title="477-tfidf-13" href="../hunch_net-2010/hunch_net-2010-09-17-New_York_Area_Machine_Learning_Events.html">410 hunch net-2010-09-17-New York Area Machine Learning Events</a></p>
<p>14 0.062412929 <a title="477-tfidf-14" href="../hunch_net-2006/hunch_net-2006-12-12-Interesting_Papers_at_NIPS_2006.html">224 hunch net-2006-12-12-Interesting Papers at NIPS 2006</a></p>
<p>15 0.058127776 <a title="477-tfidf-15" href="../hunch_net-2009/hunch_net-2009-05-24-2009_ICML_discussion_site.html">356 hunch net-2009-05-24-2009 ICML discussion site</a></p>
<p>16 0.056068774 <a title="477-tfidf-16" href="../hunch_net-2009/hunch_net-2009-01-19-Netflix_prize_within_epsilon.html">336 hunch net-2009-01-19-Netflix prize within epsilon</a></p>
<p>17 0.055083372 <a title="477-tfidf-17" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>18 0.054882642 <a title="477-tfidf-18" href="../hunch_net-2005/hunch_net-2005-03-22-Active_learning.html">45 hunch net-2005-03-22-Active learning</a></p>
<p>19 0.054169651 <a title="477-tfidf-19" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>20 0.05381459 <a title="477-tfidf-20" href="../hunch_net-2005/hunch_net-2005-03-05-Funding_Research.html">36 hunch net-2005-03-05-Funding Research</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.112), (1, 0.006), (2, 0.064), (3, 0.002), (4, -0.037), (5, 0.049), (6, -0.074), (7, 0.0), (8, 0.055), (9, -0.05), (10, 0.159), (11, -0.028), (12, -0.076), (13, 0.076), (14, 0.067), (15, -0.043), (16, -0.056), (17, 0.113), (18, 0.033), (19, 0.068), (20, -0.012), (21, -0.027), (22, -0.026), (23, 0.018), (24, 0.063), (25, 0.046), (26, -0.017), (27, -0.007), (28, -0.006), (29, 0.004), (30, -0.0), (31, -0.06), (32, 0.025), (33, 0.015), (34, -0.072), (35, -0.085), (36, -0.053), (37, -0.002), (38, 0.029), (39, -0.023), (40, 0.021), (41, 0.054), (42, 0.011), (43, -0.039), (44, 0.09), (45, 0.003), (46, -0.108), (47, 0.006), (48, 0.068), (49, -0.037)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9483577 <a title="477-lsi-1" href="../hunch_net-2013/hunch_net-2013-01-01-Deep_Learning_2012.html">477 hunch net-2013-01-01-Deep Learning 2012</a></p>
<p>Introduction: 2012 was a tumultuous year for me, but it was undeniably a great year for deep
learning efforts. Signs of this include:Winning aKaggle competition.Wide
adoption ofdeep learning for speech recognition.Significantindustry
support.Gains inimagerecognition.This is a rare event in research: a
significant capability breakout. Congratulations are definitely in order for
those who managed to achieve it. At this point, deep learning algorithms seem
like a choice undeniably worth investigating for real applications with
significant data.</p><p>2 0.76763344 <a title="477-lsi-2" href="../hunch_net-2010/hunch_net-2010-08-23-Boosted_Decision_Trees_for_Deep_Learning.html">407 hunch net-2010-08-23-Boosted Decision Trees for Deep Learning</a></p>
<p>Introduction: About 4 years ago, I speculated thatdecision trees qualify as a deep learning
algorithmbecause they can make decisions which are substantially nonlinear in
the input representation.Ping Lihasproved this correct, empiricallyatUAIby
showing that boosted decision trees can beat deep belief networks on versions
ofMnistwhich are artificially hardened so as to make them solvable only by
deep learning algorithms.This is an important point, because the ability to
solve these sorts of problems is probably the best objective definition of a
deep learning algorithm we have. I'm not that surprised. In my experience, if
you can accept the computational drawbacks of a boosted decision tree, they
can achieve pretty good performance.Geoff Hintononce told me that the great
thing about deep belief networks is that they work. I understand that Ping had
very substantial difficulty in getting this published, so I hope some
reviewers step up to the standard of valuing what works.</p><p>3 0.74503124 <a title="477-lsi-3" href="../hunch_net-2011/hunch_net-2011-04-18-A_paper_not_at_Snowbird.html">431 hunch net-2011-04-18-A paper not at Snowbird</a></p>
<p>Introduction: Unfortunately, a scheduling failure meant I missed all ofAIStatand most of
thelearning workshop, otherwise known as Snowbird, when it's atSnowbird.At
snowbird, the talk onSum-Productnetworks byHoifung Poonstood out to me (Pedro
Domingosis a coauthor.). The basic point was that by appropriately
constructing networks based on sums and products, the normalization problem in
probabilistic models is eliminated, yielding a highly tractable yet flexible
representation+learning algorithm. As an algorithm, this is noticeably cleaner
than deep belief networks with a claim to being an order of magnitude faster
and working better on an image completion task.Snowbird doesn't have real
papers--just the abstract above. I look forward to seeing the paper. (added:
Rodrigo points out the deep learning workshopdraft.)</p><p>4 0.72583926 <a title="477-lsi-4" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>Introduction: Many learning algorithms used in practice are fairly simple. Viewed
representationally, many prediction algorithms either compute a linear
separator of basic features (perceptron, winnow, weighted majority, SVM) or
perhaps a linear separator of slightly more complex features (2-layer neural
networks or kernelized SVMs). Should we go beyond this, and start using "deep"
representations?What is deep learning?Intuitively, deep learning is about
learning to predict in ways which can involve complex dependencies between the
input (observed) features.Specifying this more rigorously turns out to be
rather difficult. Consider the following cases:SVM with Gaussian Kernel. This
is not considered deep learning, because an SVM with a gaussian kernel can't
succinctly represent certain decision surfaces. One ofYann LeCun's examples is
recognizing objects based on pixel values. An SVM will need a new support
vector for each significantly different background. Since the number of
distinct backgrounds i</p><p>5 0.61095434 <a title="477-lsi-5" href="../hunch_net-2007/hunch_net-2007-01-10-A_Deep_Belief_Net_Learning_Problem.html">227 hunch net-2007-01-10-A Deep Belief Net Learning Problem</a></p>
<p>Introduction: "Deep learning" is used to describe learning architectures which have
significant depth (as a circuit).One claimis that shallow architectures (one
or two layers) can not concisely represent some functions while a circuit with
more depth can concisely represent these same functions. Proving lower bounds
on the size of a circuit is substantially harder than upper bounds (which are
constructive), but some results are known.Luca Trevisan'sclass notesdetail how
XOR is not concisely representable by "AC0â&euro;ł (= constant depth unbounded fan-in
AND, OR, NOT gates). This doesn't quite prove that depth is necessary for the
representations commonly used in learning (such as a thresholded weighted
sum), but it is strongly suggestive that this is so.Examples like this are a
bit disheartening because existing algorithms for deep learning (deep belief
nets, gradient descent on deep neural networks, and a perhaps decision trees
depending on who you ask) can't learn XOR very easily. Evidence so far
sugges</p><p>6 0.58189315 <a title="477-lsi-6" href="../hunch_net-2006/hunch_net-2006-12-12-Interesting_Papers_at_NIPS_2006.html">224 hunch net-2006-12-12-Interesting Papers at NIPS 2006</a></p>
<p>7 0.56585139 <a title="477-lsi-7" href="../hunch_net-2011/hunch_net-2011-07-11-Interesting_Neural_Network_Papers_at_ICML_2011.html">438 hunch net-2011-07-11-Interesting Neural Network Papers at ICML 2011</a></p>
<p>8 0.50319344 <a title="477-lsi-8" href="../hunch_net-2008/hunch_net-2008-11-28-A_Bumper_Crop_of_Machine_Learning_Graduates.html">329 hunch net-2008-11-28-A Bumper Crop of Machine Learning Graduates</a></p>
<p>9 0.44025499 <a title="477-lsi-9" href="../hunch_net-2005/hunch_net-2005-02-09-Intuitions_from_applied_learning.html">16 hunch net-2005-02-09-Intuitions from applied learning</a></p>
<p>10 0.43057919 <a title="477-lsi-10" href="../hunch_net-2009/hunch_net-2009-09-29-Machine_Learning_Protests_at_the_G20.html">372 hunch net-2009-09-29-Machine Learning Protests at the G20</a></p>
<p>11 0.42998898 <a title="477-lsi-11" href="../hunch_net-2013/hunch_net-2013-11-09-Graduates_and_Postdocs.html">490 hunch net-2013-11-09-Graduates and Postdocs</a></p>
<p>12 0.40450299 <a title="477-lsi-12" href="../hunch_net-2007/hunch_net-2007-11-05-CMU_wins_DARPA_Urban_Challenge.html">271 hunch net-2007-11-05-CMU wins DARPA Urban Challenge</a></p>
<p>13 0.3961921 <a title="477-lsi-13" href="../hunch_net-2012/hunch_net-2012-02-24-ICML%2B50%25.html">456 hunch net-2012-02-24-ICML+50%</a></p>
<p>14 0.39011943 <a title="477-lsi-14" href="../hunch_net-2005/hunch_net-2005-02-27-Antilearning%3A_When_proximity_goes_bad.html">32 hunch net-2005-02-27-Antilearning: When proximity goes bad</a></p>
<p>15 0.3748911 <a title="477-lsi-15" href="../hunch_net-2010/hunch_net-2010-10-17-Partha_Niyogi_has_died.html">414 hunch net-2010-10-17-Partha Niyogi has died</a></p>
<p>16 0.37228918 <a title="477-lsi-16" href="../hunch_net-2005/hunch_net-2005-04-01-Basic_computer_science_research_takes_a_hit.html">50 hunch net-2005-04-01-Basic computer science research takes a hit</a></p>
<p>17 0.36968392 <a title="477-lsi-17" href="../hunch_net-2010/hunch_net-2010-05-20-Google_Predict.html">399 hunch net-2010-05-20-Google Predict</a></p>
<p>18 0.36401677 <a title="477-lsi-18" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>19 0.3565709 <a title="477-lsi-19" href="../hunch_net-2005/hunch_net-2005-01-24-The_Humanloop_Spectrum_of_Machine_Learning.html">3 hunch net-2005-01-24-The Humanloop Spectrum of Machine Learning</a></p>
<p>20 0.34720111 <a title="477-lsi-20" href="../hunch_net-2006/hunch_net-2006-11-22-Explicit_Randomization_in_Learning_algorithms.html">219 hunch net-2006-11-22-Explicit Randomization in Learning algorithms</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.557), (42, 0.153), (74, 0.119)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.97035468 <a title="477-lda-1" href="../hunch_net-2011/hunch_net-2011-02-25-Yahoo%21_Machine_Learning_grant_due_March_11.html">425 hunch net-2011-02-25-Yahoo! Machine Learning grant due March 11</a></p>
<p>Introduction: Yahoo!'sKey Scientific ChallengesforMachine Learninggrant applications are due
March 11. If you are a student working on relevant research, please consider
applying. It's for $5K of unrestricted funding.</p><p>2 0.8813315 <a title="477-lda-2" href="../hunch_net-2005/hunch_net-2005-12-29-Deadline_Season.html">145 hunch net-2005-12-29-Deadline Season</a></p>
<p>Introduction: Many different paper deadlines are coming up soon so I made a little reference
table. Out of curiosity, I also computed the interval between submission
deadline and
conference.ConferenceLocationDateDeadlineintervalCOLTPittsburghJune
22-25January 21152ICMLPittsburghJune 26-28January 30/February 6140UAIMITJuly
13-16March 9/March 16119AAAIBostonJuly 16-20February
16/21145KDDPhiladelphiaAugust 23-26March 3/March 10166It looks like the
northeastern US is the big winner as far as location this year.</p><p>same-blog 3 0.87915397 <a title="477-lda-3" href="../hunch_net-2013/hunch_net-2013-01-01-Deep_Learning_2012.html">477 hunch net-2013-01-01-Deep Learning 2012</a></p>
<p>Introduction: 2012 was a tumultuous year for me, but it was undeniably a great year for deep
learning efforts. Signs of this include:Winning aKaggle competition.Wide
adoption ofdeep learning for speech recognition.Significantindustry
support.Gains inimagerecognition.This is a rare event in research: a
significant capability breakout. Congratulations are definitely in order for
those who managed to achieve it. At this point, deep learning algorithms seem
like a choice undeniably worth investigating for real applications with
significant data.</p><p>4 0.82665634 <a title="477-lda-4" href="../hunch_net-2010/hunch_net-2010-09-17-New_York_Area_Machine_Learning_Events.html">410 hunch net-2010-09-17-New York Area Machine Learning Events</a></p>
<p>Introduction: On Sept 21, there is anothermachine learning meetupwhere I'll be speaking.
Although the topic is contextual bandits, I think of it as "the future of
machine learning". In particular, it's all about how to learn in an
interactive environment, such as for ad display, trading, news recommendation,
etcâ&euro;ŚOn Sept 24, abstracts for theNew York Machine Learning Symposiumare due.
This is the largest Machine Learning event in the area, so it's a great way to
have a conversation with other people.On Oct 22, the NY ML Symposium actually
happens. This year, we are expanding the spotlights, and trying to have more
time for posters. In addition, we have a strong set of invited speakers:David
Blei,Sanjoy Dasgupta,Tommi Jaakkola, andYann LeCun. After the meeting, a
latehackNYrelated event is planned where students and startups can meet.I'd
also like to point out the relatedCS/Econ symposiumas I have interests there
as well.</p><p>5 0.66090357 <a title="477-lda-5" href="../hunch_net-2006/hunch_net-2006-07-26-Two_more_UAI_papers_of_interest.html">199 hunch net-2006-07-26-Two more UAI papers of interest</a></p>
<p>Introduction: In addition to Ed Snelson's paper, there were (at least) two other papers that
caught my eye at UAI.One wasthis paperby Sanjoy Dasgupta, Daniel Hsu and Nakul
Verma at UCSD which shows in a surprisingly general and strong way that almost
all linear projections of any jointly distributed vector random variable with
finite first and second moments look sphereical and unimodal (in fact look
like a scale mixture of Gaussians). Great result, as you'd expect from
Sanjoy.The other paper which I found intriguing but which I just haven't
groked yet isthis beastby Manfred and Dima Kuzmin.You can check out the
(beautiful)slidesif that helps. I feel like there is something deep here, but
my brain is too small to understand it. The COLT and last NIPS papers/slides
are also on Manfred's page. Hopefully someone here can illuminate.</p><p>6 0.54068583 <a title="477-lda-6" href="../hunch_net-2005/hunch_net-2005-04-01-Basic_computer_science_research_takes_a_hit.html">50 hunch net-2005-04-01-Basic computer science research takes a hit</a></p>
<p>7 0.4991836 <a title="477-lda-7" href="../hunch_net-2008/hunch_net-2008-05-23-Three_levels_of_addressing_the_Netflix_Prize.html">301 hunch net-2008-05-23-Three levels of addressing the Netflix Prize</a></p>
<p>8 0.38732862 <a title="477-lda-8" href="../hunch_net-2009/hunch_net-2009-01-27-Key_Scientific_Challenges.html">339 hunch net-2009-01-27-Key Scientific Challenges</a></p>
<p>9 0.34439474 <a title="477-lda-9" href="../hunch_net-2005/hunch_net-2005-02-15-ESPgame_and_image_labeling.html">20 hunch net-2005-02-15-ESPgame and image labeling</a></p>
<p>10 0.32462275 <a title="477-lda-10" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>11 0.32318413 <a title="477-lda-11" href="../hunch_net-2011/hunch_net-2011-10-10-ML_Symposium_and_ICML_details.html">447 hunch net-2011-10-10-ML Symposium and ICML details</a></p>
<p>12 0.31877875 <a title="477-lda-12" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>13 0.31694293 <a title="477-lda-13" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>14 0.31669328 <a title="477-lda-14" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>15 0.31638888 <a title="477-lda-15" href="../hunch_net-2008/hunch_net-2008-11-10-ICML_Reviewing_Criteria.html">325 hunch net-2008-11-10-ICML Reviewing Criteria</a></p>
<p>16 0.31486076 <a title="477-lda-16" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>17 0.31375745 <a title="477-lda-17" href="../hunch_net-2010/hunch_net-2010-02-26-Yahoo%21_ML_events.html">389 hunch net-2010-02-26-Yahoo! ML events</a></p>
<p>18 0.31337339 <a title="477-lda-18" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>19 0.31310862 <a title="477-lda-19" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>20 0.31275469 <a title="477-lda-20" href="../hunch_net-2007/hunch_net-2007-01-26-Parallel_Machine_Learning_Problems.html">229 hunch net-2007-01-26-Parallel Machine Learning Problems</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
