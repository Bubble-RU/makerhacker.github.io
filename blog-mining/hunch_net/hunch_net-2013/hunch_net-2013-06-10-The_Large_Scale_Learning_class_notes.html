<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>483 hunch net-2013-06-10-The Large Scale Learning class notes</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2013" href="../home/hunch_net-2013_home.html">hunch_net-2013</a> <a title="hunch_net-2013-483" href="#">hunch_net-2013-483</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>483 hunch net-2013-06-10-The Large Scale Learning class notes</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2013-483-html" href="http://hunch.net/?p=2637">html</a></p><p>Introduction: Thelarge scale machine learning classI taught withYann LeCunhas finished. As I
expected, it took quite a bit of time. We had about 25 people attending in
person on average and 400 regularly watching therecorded lectureswhich is
substantially more sustained interest than I expected for an advanced ML
class. We also had some fun with class projects--I'm hopeful that several will
eventually turn into papers.I expect there are a number of professors
interested in lecturing on this and related topics. Everyone will have their
personal taste in subjects of course, but hopefully there will be some
convergence to common course materials as well. To help with this, I am making
thesources to my presentations available. Feel free to
use/improve/embelish/ridicule/etcâ&euro;Ś in the pursuit of the perfect course.</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('course', 0.316), ('lecunhas', 0.229), ('taste', 0.229), ('expected', 0.215), ('watching', 0.212), ('regularly', 0.2), ('pursuit', 0.2), ('hopeful', 0.2), ('professors', 0.191), ('materials', 0.191), ('thelarge', 0.191), ('advanced', 0.171), ('perfect', 0.162), ('took', 0.162), ('taught', 0.162), ('attending', 0.158), ('subjects', 0.151), ('convergence', 0.151), ('eventually', 0.148), ('presentations', 0.148)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="483-tfidf-1" href="../hunch_net-2013/hunch_net-2013-06-10-The_Large_Scale_Learning_class_notes.html">483 hunch net-2013-06-10-The Large Scale Learning class notes</a></p>
<p>Introduction: Thelarge scale machine learning classI taught withYann LeCunhas finished. As I
expected, it took quite a bit of time. We had about 25 people attending in
person on average and 400 regularly watching therecorded lectureswhich is
substantially more sustained interest than I expected for an advanced ML
class. We also had some fun with class projects--I'm hopeful that several will
eventually turn into papers.I expect there are a number of professors
interested in lecturing on this and related topics. Everyone will have their
personal taste in subjects of course, but hopefully there will be some
convergence to common course materials as well. To help with this, I am making
thesources to my presentations available. Feel free to
use/improve/embelish/ridicule/etcâ&euro;Ś in the pursuit of the perfect course.</p><p>2 0.12545274 <a title="483-tfidf-2" href="../hunch_net-2010/hunch_net-2010-07-02-MetaOptimize.html">402 hunch net-2010-07-02-MetaOptimize</a></p>
<p>Introduction: Joseph TuriancreatesMetaOptimizefor discussion of NLP and ML on big datasets.
This includes ablog, but perhaps more importantly aquestion and answer
section. I'm hopeful it will take off.</p><p>3 0.10246345 <a title="483-tfidf-3" href="../hunch_net-2013/hunch_net-2013-01-07-NYU_Large_Scale_Machine_Learning_Class.html">478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</a></p>
<p>Introduction: Yann LeCunand I are coteaching a class onLarge Scale Machine Learningstarting
late Januaryat NYU. This class will cover many tricks to get machine learning
working well on datasets with many features, examples, and classes, along with
several elements of deep learning and support systems enabling the
previous.This is not a beginning class--you really need to have taken a basic
machine learning class previously to follow along. Students will be able to
run and experiment with large scale learning algorithms sinceYahoo!has donated
servers which are being configured into a small scaleHadoopcluster. We are
planning to cover the frontier of research in scalable learning algorithms, so
good class projects could easily lead to papers.For me, this is a chance to
teach on many topics of past research. In general, it seems like researchers
should engage in at least occasional teaching of research, both as a proof of
teachability and to see their own research through that lens. More generally,
I</p><p>4 0.10117522 <a title="483-tfidf-4" href="../hunch_net-2007/hunch_net-2007-09-30-NIPS_workshops_are_out..html">264 hunch net-2007-09-30-NIPS workshops are out.</a></p>
<p>Introduction: Here. I'm particularly interested in theWeb Search,Efficient ML, and (of
course)Learning Problem Designworkshops but there are many others to check out
as well. Workshops are a great chance to make progress on or learn about a
topic. Relevance and interaction amongst diverse people can sometimes be
magical.</p><p>5 0.096360698 <a title="483-tfidf-5" href="../hunch_net-2009/hunch_net-2009-12-07-Vowpal_Wabbit_version_4.0%2C_and_a_NIPS_heresy.html">381 hunch net-2009-12-07-Vowpal Wabbit version 4.0, and a NIPS heresy</a></p>
<p>Introduction: I'm releasingversion 4.0(tarball) ofVowpal Wabbit. The biggest change (by far)
in this release is experimental support for cluster parallelism, with notable
help fromDaniel Hsu.I also took advantage of the major version number to
introduce some incompatible changes, including switching tomurmurhash 2, and
other alterations to cachefiles. You'll need to delete and regenerate them. In
addition, the precise specification for a "tag" (i.e. string that can be used
to identify an example) changed--you can't have a space between the tag and
the '|' at the beginning of the feature namespace.And, of course, we made it
faster.For the future, I put up mytodo listoutlining the major future
improvements I want to see in the code. I'm planning to discuss the current
mechanism and results of the cluster parallel implementation at thelarge scale
machine learning workshopatNIPSlater this week. Several people have asked me
to do a tutorial/walkthrough of VW, which is arranged for friday 2pm in the
works</p><p>6 0.085458614 <a title="483-tfidf-6" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>7 0.078020059 <a title="483-tfidf-7" href="../hunch_net-2005/hunch_net-2005-07-11-AAAI_blog.html">92 hunch net-2005-07-11-AAAI blog</a></p>
<p>8 0.074190207 <a title="483-tfidf-8" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>9 0.070391536 <a title="483-tfidf-9" href="../hunch_net-2009/hunch_net-2009-03-18-Parallel_ML_primitives.html">346 hunch net-2009-03-18-Parallel ML primitives</a></p>
<p>10 0.069325581 <a title="483-tfidf-10" href="../hunch_net-2011/hunch_net-2011-09-28-Somebody%26%238217%3Bs_Eating_Your_Lunch.html">445 hunch net-2011-09-28-Somebody&#8217;s Eating Your Lunch</a></p>
<p>11 0.066919826 <a title="483-tfidf-11" href="../hunch_net-2012/hunch_net-2012-07-09-Videolectures.html">469 hunch net-2012-07-09-Videolectures</a></p>
<p>12 0.065619856 <a title="483-tfidf-12" href="../hunch_net-2007/hunch_net-2007-04-21-Videolectures.net.html">240 hunch net-2007-04-21-Videolectures.net</a></p>
<p>13 0.062915765 <a title="483-tfidf-13" href="../hunch_net-2014/hunch_net-2014-02-16-Metacademy%3A_a_package_manager_for_knowledge.html">493 hunch net-2014-02-16-Metacademy: a package manager for knowledge</a></p>
<p>14 0.062571757 <a title="483-tfidf-14" href="../hunch_net-2008/hunch_net-2008-04-30-Concerns_about_the_Large_Scale_Learning_Challenge.html">300 hunch net-2008-04-30-Concerns about the Large Scale Learning Challenge</a></p>
<p>15 0.062194802 <a title="483-tfidf-15" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>16 0.060975771 <a title="483-tfidf-16" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>17 0.059808664 <a title="483-tfidf-17" href="../hunch_net-2008/hunch_net-2008-01-07-2008_Summer_Machine_Learning_Conference_Schedule.html">283 hunch net-2008-01-07-2008 Summer Machine Learning Conference Schedule</a></p>
<p>18 0.057356402 <a title="483-tfidf-18" href="../hunch_net-2009/hunch_net-2009-11-23-ICML_2009_Workshops_%28and_Tutorials%29.html">379 hunch net-2009-11-23-ICML 2009 Workshops (and Tutorials)</a></p>
<p>19 0.056118853 <a title="483-tfidf-19" href="../hunch_net-2008/hunch_net-2008-11-10-ICML_Reviewing_Criteria.html">325 hunch net-2008-11-10-ICML Reviewing Criteria</a></p>
<p>20 0.055079099 <a title="483-tfidf-20" href="../hunch_net-2007/hunch_net-2007-06-24-Interesting_Papers_at_ICML_2007.html">251 hunch net-2007-06-24-Interesting Papers at ICML 2007</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.115), (1, 0.035), (2, 0.061), (3, 0.043), (4, -0.001), (5, 0.039), (6, 0.022), (7, -0.003), (8, 0.03), (9, 0.011), (10, -0.022), (11, 0.058), (12, 0.001), (13, -0.026), (14, 0.006), (15, 0.043), (16, -0.021), (17, -0.02), (18, 0.0), (19, -0.029), (20, 0.118), (21, -0.022), (22, -0.067), (23, 0.085), (24, 0.093), (25, -0.085), (26, -0.083), (27, 0.074), (28, 0.08), (29, -0.081), (30, 0.032), (31, -0.064), (32, 0.067), (33, 0.053), (34, -0.047), (35, 0.035), (36, 0.046), (37, 0.011), (38, -0.094), (39, -0.001), (40, -0.043), (41, 0.003), (42, 0.059), (43, 0.122), (44, 0.008), (45, -0.005), (46, 0.041), (47, -0.012), (48, 0.032), (49, 0.061)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97120875 <a title="483-lsi-1" href="../hunch_net-2013/hunch_net-2013-06-10-The_Large_Scale_Learning_class_notes.html">483 hunch net-2013-06-10-The Large Scale Learning class notes</a></p>
<p>Introduction: Thelarge scale machine learning classI taught withYann LeCunhas finished. As I
expected, it took quite a bit of time. We had about 25 people attending in
person on average and 400 regularly watching therecorded lectureswhich is
substantially more sustained interest than I expected for an advanced ML
class. We also had some fun with class projects--I'm hopeful that several will
eventually turn into papers.I expect there are a number of professors
interested in lecturing on this and related topics. Everyone will have their
personal taste in subjects of course, but hopefully there will be some
convergence to common course materials as well. To help with this, I am making
thesources to my presentations available. Feel free to
use/improve/embelish/ridicule/etcâ&euro;Ś in the pursuit of the perfect course.</p><p>2 0.54141378 <a title="483-lsi-2" href="../hunch_net-2013/hunch_net-2013-01-07-NYU_Large_Scale_Machine_Learning_Class.html">478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</a></p>
<p>Introduction: Yann LeCunand I are coteaching a class onLarge Scale Machine Learningstarting
late Januaryat NYU. This class will cover many tricks to get machine learning
working well on datasets with many features, examples, and classes, along with
several elements of deep learning and support systems enabling the
previous.This is not a beginning class--you really need to have taken a basic
machine learning class previously to follow along. Students will be able to
run and experiment with large scale learning algorithms sinceYahoo!has donated
servers which are being configured into a small scaleHadoopcluster. We are
planning to cover the frontier of research in scalable learning algorithms, so
good class projects could easily lead to papers.For me, this is a chance to
teach on many topics of past research. In general, it seems like researchers
should engage in at least occasional teaching of research, both as a proof of
teachability and to see their own research through that lens. More generally,
I</p><p>3 0.51879334 <a title="483-lsi-3" href="../hunch_net-2013/hunch_net-2013-01-31-Remote_large_scale_learning_class_participation.html">479 hunch net-2013-01-31-Remote large scale learning class participation</a></p>
<p>Introduction: Yann and I have arranged so that people who are interested in ourlarge scale
machine learning classand not able to attend in person can follow along via
two methods.Videoswill be posted with about a 1 day delay ontechtalks. This is
a side-by-side capture of video+slides fromWeyond.We are experimenting
withPiazzaas a discussion forum. Anyone is welcome to subscribe to Piazza and
ask questions there, where I will be monitoring things.update2: Sign
uphere.The first lecture is up now, including therevised version of the
slideswhich fixes a few typos and rounds out references.</p><p>4 0.50133455 <a title="483-lsi-4" href="../hunch_net-2007/hunch_net-2007-09-30-NIPS_workshops_are_out..html">264 hunch net-2007-09-30-NIPS workshops are out.</a></p>
<p>Introduction: Here. I'm particularly interested in theWeb Search,Efficient ML, and (of
course)Learning Problem Designworkshops but there are many others to check out
as well. Workshops are a great chance to make progress on or learn about a
topic. Relevance and interaction amongst diverse people can sometimes be
magical.</p><p>5 0.45141748 <a title="483-lsi-5" href="../hunch_net-2011/hunch_net-2011-09-28-Somebody%26%238217%3Bs_Eating_Your_Lunch.html">445 hunch net-2011-09-28-Somebody&#8217;s Eating Your Lunch</a></p>
<p>Introduction: Since we last discussedthe other online learning,Stanfordhas very visibly
started pushing mass teaching inAI,Machine Learning, andDatabases. In
retrospect, it's not too surprising that the next step up in serious online
teaching experiments are occurring at the computer science department of a
university embedded in the land of startups. Numbers on the order of100000are
quite significant--similar in scale to the number ofcomputer science
undergraduate students/yearin the US. Although these populations surely
differ, the fact that theycouldoverlap is worth considering for the
future.It's too soon to say how successful these classes will be and there are
many easy criticisms to make:Registration != Learning… but if only 1/10th
complete these classes, the scale of teaching still surpasses the scale of any
traditional process.1st year excitement != nth year routine… but if only
1/10th take future classes, the scale of teaching still surpasses the scale of
any traditional process.Hello, che</p><p>6 0.43078524 <a title="483-lsi-6" href="../hunch_net-2009/hunch_net-2009-12-07-Vowpal_Wabbit_version_4.0%2C_and_a_NIPS_heresy.html">381 hunch net-2009-12-07-Vowpal Wabbit version 4.0, and a NIPS heresy</a></p>
<p>7 0.41918543 <a title="483-lsi-7" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>8 0.40522069 <a title="483-lsi-8" href="../hunch_net-2014/hunch_net-2014-02-16-Metacademy%3A_a_package_manager_for_knowledge.html">493 hunch net-2014-02-16-Metacademy: a package manager for knowledge</a></p>
<p>9 0.40470117 <a title="483-lsi-9" href="../hunch_net-2005/hunch_net-2005-03-08-Fast_Physics_for_Learning.html">37 hunch net-2005-03-08-Fast Physics for Learning</a></p>
<p>10 0.39860731 <a title="483-lsi-10" href="../hunch_net-2005/hunch_net-2005-07-11-AAAI_blog.html">92 hunch net-2005-07-11-AAAI blog</a></p>
<p>11 0.39820528 <a title="483-lsi-11" href="../hunch_net-2007/hunch_net-2007-06-23-Machine_Learning_Jobs_are_Growing_on_Trees.html">250 hunch net-2007-06-23-Machine Learning Jobs are Growing on Trees</a></p>
<p>12 0.39206257 <a title="483-lsi-12" href="../hunch_net-2013/hunch_net-2013-03-22-I%26%238217%3Bm_a_bandit.html">480 hunch net-2013-03-22-I&#8217;m a bandit</a></p>
<p>13 0.39003143 <a title="483-lsi-13" href="../hunch_net-2007/hunch_net-2007-04-21-Videolectures.net.html">240 hunch net-2007-04-21-Videolectures.net</a></p>
<p>14 0.36145771 <a title="483-lsi-14" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>15 0.360315 <a title="483-lsi-15" href="../hunch_net-2010/hunch_net-2010-08-21-Rob_Schapire_at_NYC_ML_Meetup.html">405 hunch net-2010-08-21-Rob Schapire at NYC ML Meetup</a></p>
<p>16 0.35688704 <a title="483-lsi-16" href="../hunch_net-2006/hunch_net-2006-07-08-Some_recent_papers.html">192 hunch net-2006-07-08-Some recent papers</a></p>
<p>17 0.35493529 <a title="483-lsi-17" href="../hunch_net-2011/hunch_net-2011-12-13-Vowpal_Wabbit_version_6.1_%26%23038%3B_the_NIPS_tutorial.html">451 hunch net-2011-12-13-Vowpal Wabbit version 6.1 &#038; the NIPS tutorial</a></p>
<p>18 0.35272276 <a title="483-lsi-18" href="../hunch_net-2005/hunch_net-2005-04-08-Fast_SVMs.html">54 hunch net-2005-04-08-Fast SVMs</a></p>
<p>19 0.35149914 <a title="483-lsi-19" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>20 0.35038576 <a title="483-lsi-20" href="../hunch_net-2006/hunch_net-2006-06-25-Presentation_of_Proofs_is_Hard..html">187 hunch net-2006-06-25-Presentation of Proofs is Hard.</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(35, 0.072), (42, 0.189), (68, 0.046), (73, 0.425), (74, 0.023), (95, 0.115)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.91141051 <a title="483-lda-1" href="../hunch_net-2013/hunch_net-2013-06-10-The_Large_Scale_Learning_class_notes.html">483 hunch net-2013-06-10-The Large Scale Learning class notes</a></p>
<p>Introduction: Thelarge scale machine learning classI taught withYann LeCunhas finished. As I
expected, it took quite a bit of time. We had about 25 people attending in
person on average and 400 regularly watching therecorded lectureswhich is
substantially more sustained interest than I expected for an advanced ML
class. We also had some fun with class projects--I'm hopeful that several will
eventually turn into papers.I expect there are a number of professors
interested in lecturing on this and related topics. Everyone will have their
personal taste in subjects of course, but hopefully there will be some
convergence to common course materials as well. To help with this, I am making
thesources to my presentations available. Feel free to
use/improve/embelish/ridicule/etcâ&euro;Ś in the pursuit of the perfect course.</p><p>2 0.84007639 <a title="483-lda-2" href="../hunch_net-2005/hunch_net-2005-07-13-Text_Entailment_at_AAAI.html">94 hunch net-2005-07-13-Text Entailment at AAAI</a></p>
<p>Introduction: Rajat Rainapresented a paper on the technique they used for
thePASCALRecognizing Textual Entailmentchallenge."Text entailment" is the
problem of deciding if one sentence implies another. For example the previous
sentence entails:Text entailment is a decision problem.One sentence can imply
another.The challenge was of the form: given an original sentence and another
sentence predict whether there was an entailment. All current techniques for
predicting correctness of an entailment are at the "flail" stage--accuracies
of around 58% where humans could achieve near 100% accuracy, so there is much
room to improve. Apparently, there may be another PASCAL challenge on this
problem in the near future.</p><p>3 0.81304431 <a title="483-lda-3" href="../hunch_net-2005/hunch_net-2005-12-28-Yet_more_nips_thoughts.html">144 hunch net-2005-12-28-Yet more nips thoughts</a></p>
<p>Introduction: I only managed to make it out to the NIPS workshops this year soI'll give my
comments on what I saw there.The Learing and Robotics workshops lives again. I
hope itcontinues and gets more high quality papers in the future. Themost
interesting talk for me was Larry Jackel's on the LAGRprogram (see John's
previous post on said program). I got someideas as to what progress has been
made. Larry really explainedthe types of benchmarks and the tradeoffs that had
to be made tomake the goals achievable but challenging.Hal Daume gave a very
interesting talk about structuredprediction using RL techniques, something
near and dear to my ownheart. He achieved rather impressive results using only
a verygreedy search.The non-parametric Bayes workshop was great. I enjoyed the
entiremorning session I spent there, and particularly (the usuallydesultory)
discussion periods. One interesting topic was theGibbs/Variational inference
divide. I won't try to summarizeespecially as no conclusion was reached. It</p><p>4 0.79202473 <a title="483-lda-4" href="../hunch_net-2007/hunch_net-2007-11-16-MLSS_2008.html">273 hunch net-2007-11-16-MLSS 2008</a></p>
<p>Introduction: â&euro;Ś is in Kioloa, Australia from March 3 to March 14. It's a great chance to
learn something about Machine Learning and I've enjoyed severalprevious
Machine Learning Summer Schools.Thewebsite has many more details, but
registration is open now for the first 80 to sign up.</p><p>5 0.72176725 <a title="483-lda-5" href="../hunch_net-2008/hunch_net-2008-10-20-New_York%26%238217%3Bs_ML_Day.html">322 hunch net-2008-10-20-New York&#8217;s ML Day</a></p>
<p>Introduction: I'm not as naturally exuberant asMuthu2orDavidaboutCS/Econday, but I believe
it andML daywere certainly successful.At the CS/Econ day, I particularly
enjoyedToumas Sandholm'stalk which showed a commanding depth of understanding
and application in automated auctions.For the machine learning day, I enjoyed
several talks and posters (I better, I helped pick them.). What stood out to
me was number of people attending: 158 registered, a level qualifying as
"scramble to find seats". My rule of thumb for workshops/conferences is that
the number of attendees is often something like the number of submissions.
That isn't the case here, where there were just 4 invited speakers and 30-or-
so posters. Presumably, the difference is due to a critical mass of Machine
Learning interested people in the area and the ease of their attendance.Are
there other areas where a local Machine Learning day would fly? It's easy to
imagine something working out in the San Francisco bay area and possibly
Germany or E</p><p>6 0.69827783 <a title="483-lda-6" href="../hunch_net-2011/hunch_net-2011-04-11-The_Heritage_Health_Prize.html">430 hunch net-2011-04-11-The Heritage Health Prize</a></p>
<p>7 0.61164868 <a title="483-lda-7" href="../hunch_net-2005/hunch_net-2005-03-15-The_State_of_Tight_Bounds.html">41 hunch net-2005-03-15-The State of Tight Bounds</a></p>
<p>8 0.46369067 <a title="483-lda-8" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>9 0.4633714 <a title="483-lda-9" href="../hunch_net-2005/hunch_net-2005-05-10-Learning_Reductions_are_Reductionist.html">68 hunch net-2005-05-10-Learning Reductions are Reductionist</a></p>
<p>10 0.46164018 <a title="483-lda-10" href="../hunch_net-2006/hunch_net-2006-10-22-Exemplar_programming.html">215 hunch net-2006-10-22-Exemplar programming</a></p>
<p>11 0.44945347 <a title="483-lda-11" href="../hunch_net-2010/hunch_net-2010-12-02-Traffic_Prediction_Problem.html">418 hunch net-2010-12-02-Traffic Prediction Problem</a></p>
<p>12 0.44708368 <a title="483-lda-12" href="../hunch_net-2005/hunch_net-2005-09-26-Prediction_Bounds_as_the_Mathematics_of_Science.html">115 hunch net-2005-09-26-Prediction Bounds as the Mathematics of Science</a></p>
<p>13 0.4364261 <a title="483-lda-13" href="../hunch_net-2006/hunch_net-2006-01-08-Debugging_Your_Brain.html">147 hunch net-2006-01-08-Debugging Your Brain</a></p>
<p>14 0.43510073 <a title="483-lda-14" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>15 0.4348903 <a title="483-lda-15" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>16 0.43310007 <a title="483-lda-16" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>17 0.43295068 <a title="483-lda-17" href="../hunch_net-2007/hunch_net-2007-08-19-Choice_of_Metrics.html">259 hunch net-2007-08-19-Choice of Metrics</a></p>
<p>18 0.43206674 <a title="483-lda-18" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>19 0.4320389 <a title="483-lda-19" href="../hunch_net-2005/hunch_net-2005-03-17-Going_all_the_Way%2C_Sometimes.html">42 hunch net-2005-03-17-Going all the Way, Sometimes</a></p>
<p>20 0.4312461 <a title="483-lda-20" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
