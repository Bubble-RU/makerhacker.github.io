<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2013" href="../home/hunch_net-2013_home.html">hunch_net-2013</a> <a title="hunch_net-2013-478" href="#">hunch_net-2013-478</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2013-478-html" href="http://hunch.net/?p=2616">html</a></p><p>Introduction: Yann LeCunand I are coteaching a class onLarge Scale Machine Learningstarting
late Januaryat NYU. This class will cover many tricks to get machine learning
working well on datasets with many features, examples, and classes, along with
several elements of deep learning and support systems enabling the
previous.This is not a beginning class--you really need to have taken a basic
machine learning class previously to follow along. Students will be able to
run and experiment with large scale learning algorithms sinceYahoo!has donated
servers which are being configured into a small scaleHadoopcluster. We are
planning to cover the frontier of research in scalable learning algorithms, so
good class projects could easily lead to papers.For me, this is a chance to
teach on many topics of past research. In general, it seems like researchers
should engage in at least occasional teaching of research, both as a proof of
teachability and to see their own research through that lens. More generally,
I</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('class', 0.473), ('nyu', 0.283), ('yann', 0.194), ('figuring', 0.183), ('cover', 0.156), ('students', 0.129), ('frontier', 0.126), ('videotape', 0.126), ('support', 0.119), ('scale', 0.118), ('engage', 0.116), ('obstacles', 0.116), ('occasional', 0.116), ('turning', 0.116), ('teach', 0.11), ('enable', 0.105), ('lectures', 0.105), ('enabling', 0.105), ('scalable', 0.101), ('viewpoints', 0.101)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999982 <a title="478-tfidf-1" href="../hunch_net-2013/hunch_net-2013-01-07-NYU_Large_Scale_Machine_Learning_Class.html">478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</a></p>
<p>Introduction: Yann LeCunand I are coteaching a class onLarge Scale Machine Learningstarting
late Januaryat NYU. This class will cover many tricks to get machine learning
working well on datasets with many features, examples, and classes, along with
several elements of deep learning and support systems enabling the
previous.This is not a beginning class--you really need to have taken a basic
machine learning class previously to follow along. Students will be able to
run and experiment with large scale learning algorithms sinceYahoo!has donated
servers which are being configured into a small scaleHadoopcluster. We are
planning to cover the frontier of research in scalable learning algorithms, so
good class projects could easily lead to papers.For me, this is a chance to
teach on many topics of past research. In general, it seems like researchers
should engage in at least occasional teaching of research, both as a proof of
teachability and to see their own research through that lens. More generally,
I</p><p>2 0.1827341 <a title="478-tfidf-2" href="../hunch_net-2011/hunch_net-2011-09-28-Somebody%26%238217%3Bs_Eating_Your_Lunch.html">445 hunch net-2011-09-28-Somebody&#8217;s Eating Your Lunch</a></p>
<p>Introduction: Since we last discussedthe other online learning,Stanfordhas very visibly
started pushing mass teaching inAI,Machine Learning, andDatabases. In
retrospect, it's not too surprising that the next step up in serious online
teaching experiments are occurring at the computer science department of a
university embedded in the land of startups. Numbers on the order of100000are
quite significant--similar in scale to the number ofcomputer science
undergraduate students/yearin the US. Although these populations surely
differ, the fact that theycouldoverlap is worth considering for the
future.It's too soon to say how successful these classes will be and there are
many easy criticisms to make:Registration != Learning… but if only 1/10th
complete these classes, the scale of teaching still surpasses the scale of any
traditional process.1st year excitement != nth year routine… but if only
1/10th take future classes, the scale of teaching still surpasses the scale of
any traditional process.Hello, che</p><p>3 0.17039631 <a title="478-tfidf-3" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>Introduction: Let's define a learning problem as making predictions given past data. There
are several ways to attack the learning problem which seem to be equivalent to
solving the learning problem.Find the InvariantThis viewpoint says that
learning is all about learning (or incorporating) transformations of objects
that do not change the correct prediction. The best possible invariant is the
one which says "all things of the same class are the same". Finding this is
equivalent to learning. This viewpoint is particularly common when working
with image features.Feature SelectionThis viewpoint says that the way to learn
is by finding the right features to input to a learning algorithm. The best
feature is the one which is the class to predict. Finding this is equivalent
to learning for all reasonable learning algorithms. This viewpoint is common
in several applications of machine learning. SeeGilad's and Bianca's
comments.Find the RepresentationThis is almost the same as feature selection,
except int</p><p>4 0.14448516 <a title="478-tfidf-4" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>Introduction: If you search for "online learning" with anymajorsearchengine, it's
interesting to note that zero of the results are for online machine learning.
This may not be a mistake if you are committed to a global ordering. In other
words, the number of people specifically interested in the least interesting
top-10 online human learning result might exceed the number of people
interested in online machine learning, even given the presence of the other 9
results. The essential observation here is that the process of human learning
is a big business (around 5% of GDP) effecting virtually everyone.The internet
is changing this dramatically, by altering the economics of teaching. Consider
two possibilities:The classroom-style teaching environment continues as is,
with many teachers for the same subject.All the teachers for one subject get
together, along with perhaps a factor of 2 more people who are experts in
online delivery. They spend a factor of 4 more time designing the perfect
lecture & lear</p><p>5 0.14023088 <a title="478-tfidf-5" href="../hunch_net-2005/hunch_net-2005-02-26-Problem%3A_Reductions_and_Relative_Ranking_Metrics.html">31 hunch net-2005-02-26-Problem: Reductions and Relative Ranking Metrics</a></p>
<p>Introduction: This, again, is something of a research direction rather than a single
problem.There are several metrics people care about which depend upon the
relative ranking of examples and there are sometimes good reasons to care
about such metrics. Examples includeAROC, "F1â&euro;ł, the proportion of the time
that the top ranked element is in some class, the proportion of the top 10
examples in some class (google's problem), the lowest ranked example of some
class, and the "sort distance" from a predicted ranking to a correct ranking.
Seeherefor an example of some of these.ProblemWhat does the ability to
classify well imply about performance under these metrics?Past
WorkProbabilistic classification under squared errorcan be solved with a
classifier. A counterexample shows this does not imply a good AROC.Sample
complexity bounds forAROC(andhere).A paper on "Learning to Order
Things".DifficultySeveral of these may be easy. Some of them may be
hard.ImpactPositive or negative results will broaden our under</p><p>6 0.12450161 <a title="478-tfidf-6" href="../hunch_net-2013/hunch_net-2013-01-31-Remote_large_scale_learning_class_participation.html">479 hunch net-2013-01-31-Remote large scale learning class participation</a></p>
<p>7 0.11689886 <a title="478-tfidf-7" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>8 0.11527553 <a title="478-tfidf-8" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>9 0.10437173 <a title="478-tfidf-9" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>10 0.10246345 <a title="478-tfidf-10" href="../hunch_net-2013/hunch_net-2013-06-10-The_Large_Scale_Learning_class_notes.html">483 hunch net-2013-06-10-The Large Scale Learning class notes</a></p>
<p>11 0.098820195 <a title="478-tfidf-11" href="../hunch_net-2011/hunch_net-2011-12-13-Vowpal_Wabbit_version_6.1_%26%23038%3B_the_NIPS_tutorial.html">451 hunch net-2011-12-13-Vowpal Wabbit version 6.1 &#038; the NIPS tutorial</a></p>
<p>12 0.09695673 <a title="478-tfidf-12" href="../hunch_net-2012/hunch_net-2012-07-09-Videolectures.html">469 hunch net-2012-07-09-Videolectures</a></p>
<p>13 0.096175045 <a title="478-tfidf-13" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>14 0.096155122 <a title="478-tfidf-14" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>15 0.096068524 <a title="478-tfidf-15" href="../hunch_net-2011/hunch_net-2011-03-27-Vowpal_Wabbit%2C_v5.1.html">428 hunch net-2011-03-27-Vowpal Wabbit, v5.1</a></p>
<p>16 0.095688879 <a title="478-tfidf-16" href="../hunch_net-2009/hunch_net-2009-02-22-Effective_Research_Funding.html">344 hunch net-2009-02-22-Effective Research Funding</a></p>
<p>17 0.09438476 <a title="478-tfidf-17" href="../hunch_net-2007/hunch_net-2007-10-17-Online_as_the_new_adjective.html">267 hunch net-2007-10-17-Online as the new adjective</a></p>
<p>18 0.093575597 <a title="478-tfidf-18" href="../hunch_net-2005/hunch_net-2005-05-28-Running_A_Machine_Learning_Summer_School.html">75 hunch net-2005-05-28-Running A Machine Learning Summer School</a></p>
<p>19 0.093519159 <a title="478-tfidf-19" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>20 0.090853482 <a title="478-tfidf-20" href="../hunch_net-2009/hunch_net-2009-12-09-Future_Publication_Models_%40_NIPS.html">382 hunch net-2009-12-09-Future Publication Models @ NIPS</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.209), (1, 0.019), (2, 0.142), (3, 0.004), (4, 0.006), (5, 0.072), (6, 0.061), (7, 0.036), (8, -0.0), (9, -0.007), (10, 0.049), (11, 0.001), (12, -0.05), (13, -0.016), (14, -0.051), (15, 0.009), (16, -0.023), (17, -0.013), (18, 0.065), (19, -0.012), (20, 0.129), (21, -0.066), (22, -0.019), (23, 0.019), (24, -0.015), (25, -0.203), (26, -0.048), (27, 0.045), (28, -0.024), (29, -0.057), (30, -0.095), (31, -0.058), (32, 0.047), (33, 0.029), (34, -0.096), (35, 0.01), (36, 0.022), (37, 0.096), (38, -0.036), (39, 0.074), (40, 0.116), (41, 0.029), (42, -0.007), (43, 0.081), (44, -0.031), (45, -0.046), (46, 0.016), (47, -0.063), (48, 0.084), (49, 0.086)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9504599 <a title="478-lsi-1" href="../hunch_net-2013/hunch_net-2013-01-07-NYU_Large_Scale_Machine_Learning_Class.html">478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</a></p>
<p>Introduction: Yann LeCunand I are coteaching a class onLarge Scale Machine Learningstarting
late Januaryat NYU. This class will cover many tricks to get machine learning
working well on datasets with many features, examples, and classes, along with
several elements of deep learning and support systems enabling the
previous.This is not a beginning class--you really need to have taken a basic
machine learning class previously to follow along. Students will be able to
run and experiment with large scale learning algorithms sinceYahoo!has donated
servers which are being configured into a small scaleHadoopcluster. We are
planning to cover the frontier of research in scalable learning algorithms, so
good class projects could easily lead to papers.For me, this is a chance to
teach on many topics of past research. In general, it seems like researchers
should engage in at least occasional teaching of research, both as a proof of
teachability and to see their own research through that lens. More generally,
I</p><p>2 0.72205567 <a title="478-lsi-2" href="../hunch_net-2011/hunch_net-2011-09-28-Somebody%26%238217%3Bs_Eating_Your_Lunch.html">445 hunch net-2011-09-28-Somebody&#8217;s Eating Your Lunch</a></p>
<p>Introduction: Since we last discussedthe other online learning,Stanfordhas very visibly
started pushing mass teaching inAI,Machine Learning, andDatabases. In
retrospect, it's not too surprising that the next step up in serious online
teaching experiments are occurring at the computer science department of a
university embedded in the land of startups. Numbers on the order of100000are
quite significant--similar in scale to the number ofcomputer science
undergraduate students/yearin the US. Although these populations surely
differ, the fact that theycouldoverlap is worth considering for the
future.It's too soon to say how successful these classes will be and there are
many easy criticisms to make:Registration != Learning… but if only 1/10th
complete these classes, the scale of teaching still surpasses the scale of any
traditional process.1st year excitement != nth year routine… but if only
1/10th take future classes, the scale of teaching still surpasses the scale of
any traditional process.Hello, che</p><p>3 0.66347867 <a title="478-lsi-3" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>Introduction: If you search for "online learning" with anymajorsearchengine, it's
interesting to note that zero of the results are for online machine learning.
This may not be a mistake if you are committed to a global ordering. In other
words, the number of people specifically interested in the least interesting
top-10 online human learning result might exceed the number of people
interested in online machine learning, even given the presence of the other 9
results. The essential observation here is that the process of human learning
is a big business (around 5% of GDP) effecting virtually everyone.The internet
is changing this dramatically, by altering the economics of teaching. Consider
two possibilities:The classroom-style teaching environment continues as is,
with many teachers for the same subject.All the teachers for one subject get
together, along with perhaps a factor of 2 more people who are experts in
online delivery. They spend a factor of 4 more time designing the perfect
lecture & lear</p><p>4 0.66163516 <a title="478-lsi-4" href="../hunch_net-2014/hunch_net-2014-02-16-Metacademy%3A_a_package_manager_for_knowledge.html">493 hunch net-2014-02-16-Metacademy: a package manager for knowledge</a></p>
<p>Introduction: In recent years, there’s been an explosion of free educational resources that
make high-level knowledge and skills accessible to an ever-wider group of
people. In your own field, you probably have a good idea of where to look for
the answer to any particular question. But outside your areas of expertise,
sifting through textbooks, Wikipedia articles, research papers, and online
lectures can be bewildering (unless you’re fortunate enough to have a
knowledgeable colleague to consult). What are the key concepts in the field,
how do they relate to each other, which ones should you learn, and where
should you learn them?Courses are a major vehicle for packaging educational
materials for a broad audience. The trouble is that they’re typically meant to
be consumed linearly, regardless of your specific background or goals. Also,
unless thousands of other people have had the same background and learning
goals, there may not even be a course that fits your needs. Recently, we
(Roger GrosseandCol</p><p>5 0.65520304 <a title="478-lsi-5" href="../hunch_net-2013/hunch_net-2013-06-10-The_Large_Scale_Learning_class_notes.html">483 hunch net-2013-06-10-The Large Scale Learning class notes</a></p>
<p>Introduction: Thelarge scale machine learning classI taught withYann LeCunhas finished. As I
expected, it took quite a bit of time. We had about 25 people attending in
person on average and 400 regularly watching therecorded lectureswhich is
substantially more sustained interest than I expected for an advanced ML
class. We also had some fun with class projects--I'm hopeful that several will
eventually turn into papers.I expect there are a number of professors
interested in lecturing on this and related topics. Everyone will have their
personal taste in subjects of course, but hopefully there will be some
convergence to common course materials as well. To help with this, I am making
thesources to my presentations available. Feel free to
use/improve/embelish/ridicule/etcâ&euro;Ś in the pursuit of the perfect course.</p><p>6 0.64630944 <a title="478-lsi-6" href="../hunch_net-2013/hunch_net-2013-01-31-Remote_large_scale_learning_class_participation.html">479 hunch net-2013-01-31-Remote large scale learning class participation</a></p>
<p>7 0.59883201 <a title="478-lsi-7" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>8 0.55895489 <a title="478-lsi-8" href="../hunch_net-2005/hunch_net-2005-02-04-JMLG.html">13 hunch net-2005-02-04-JMLG</a></p>
<p>9 0.55540466 <a title="478-lsi-9" href="../hunch_net-2005/hunch_net-2005-02-26-Problem%3A_Reductions_and_Relative_Ranking_Metrics.html">31 hunch net-2005-02-26-Problem: Reductions and Relative Ranking Metrics</a></p>
<p>10 0.55125922 <a title="478-lsi-10" href="../hunch_net-2009/hunch_net-2009-12-07-Vowpal_Wabbit_version_4.0%2C_and_a_NIPS_heresy.html">381 hunch net-2009-12-07-Vowpal Wabbit version 4.0, and a NIPS heresy</a></p>
<p>11 0.54545277 <a title="478-lsi-11" href="../hunch_net-2011/hunch_net-2011-06-22-Ultra_LDA.html">436 hunch net-2011-06-22-Ultra LDA</a></p>
<p>12 0.54124314 <a title="478-lsi-12" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>13 0.53293657 <a title="478-lsi-13" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>14 0.5301168 <a title="478-lsi-14" href="../hunch_net-2011/hunch_net-2011-08-20-The_Large_Scale_Learning_Survey_Tutorial.html">442 hunch net-2011-08-20-The Large Scale Learning Survey Tutorial</a></p>
<p>15 0.52209169 <a title="478-lsi-15" href="../hunch_net-2013/hunch_net-2013-12-01-NIPS_tutorials_and_Vowpal_Wabbit_7.4.html">492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</a></p>
<p>16 0.51010412 <a title="478-lsi-16" href="../hunch_net-2011/hunch_net-2011-12-13-Vowpal_Wabbit_version_6.1_%26%23038%3B_the_NIPS_tutorial.html">451 hunch net-2011-12-13-Vowpal Wabbit version 6.1 &#038; the NIPS tutorial</a></p>
<p>17 0.5064798 <a title="478-lsi-17" href="../hunch_net-2013/hunch_net-2013-11-09-Graduates_and_Postdocs.html">490 hunch net-2013-11-09-Graduates and Postdocs</a></p>
<p>18 0.481516 <a title="478-lsi-18" href="../hunch_net-2007/hunch_net-2007-06-23-Machine_Learning_Jobs_are_Growing_on_Trees.html">250 hunch net-2007-06-23-Machine Learning Jobs are Growing on Trees</a></p>
<p>19 0.47921583 <a title="478-lsi-19" href="../hunch_net-2005/hunch_net-2005-01-19-Why_I_decided_to_run_a_weblog..html">1 hunch net-2005-01-19-Why I decided to run a weblog.</a></p>
<p>20 0.47586718 <a title="478-lsi-20" href="../hunch_net-2009/hunch_net-2009-07-31-Vowpal_Wabbit_Open_Source_Project.html">365 hunch net-2009-07-31-Vowpal Wabbit Open Source Project</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(35, 0.045), (42, 0.314), (44, 0.02), (45, 0.012), (68, 0.059), (74, 0.085), (82, 0.011), (83, 0.018), (88, 0.287), (95, 0.057)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.98841995 <a title="478-lda-1" href="../hunch_net-2007/hunch_net-2007-05-08-Conditional_Tournaments_for_Multiclass_to_Binary.html">243 hunch net-2007-05-08-Conditional Tournaments for Multiclass to Binary</a></p>
<p>Introduction: Thisproblemhas been cracked (but not quite completely solved) byAlina,Pradeep,
andI. The problem is essentially finding a better way to reduce multiclass
classification to binary classification. The solution is to use a carefully
crafted tournament, the simplest version of which is asingle elimination
tournamentwhere the "players" are the different classes. An example of the
structure is here:For the single elimination tournament, we can prove that:For
all multiclass problemsD, for all learned binary classifiersc, the regret of
an induced multiclass classifier is bounded by the regret of the binary
classifier timeslog2k. Restated:regmulticlass(D,Filter_tree_test(c)) <=
regbinary(Filter_tree_train(D),c)Here:Filter_tree_train(D)is the induced
binary classification problemFilter_tree_test(c)is the induced multiclass
classifier.regmulticlassis the multiclass regret (= difference between error
rate and minimum possible error rate)regbinaryis the binary regretThis result
has a slight depende</p><p>2 0.98135829 <a title="478-lda-2" href="../hunch_net-2012/hunch_net-2012-12-29-Simons_Institute_Big_Data_Program.html">476 hunch net-2012-12-29-Simons Institute Big Data Program</a></p>
<p>Introduction: Michael Jordansends the below:The newSimons Institute for the Theory of
Computingwill begin organizing semester-long programs starting in 2013.One of
our first programs, set for Fall 2013, will be on the "Theoretical
Foundationsof Big Data Analysis". The organizers of this program are Michael
Jordan (chair),Stephen Boyd, Peter Buehlmann, Ravi Kannan, Michael Mahoney,
and Muthu
Muthukrishnan.Seehttp://simons.berkeley.edu/program_bigdata2013.htmlfor more
information onthe program.The Simons Institute has created a number of
"Research Fellowships" for youngresearchers (within at most six years of the
award of their PhD) who wish toparticipate in Institute programs, including
the Big Data program. Individualswho already hold postdoctoral positions or
who are junior faculty are welcometo apply, as are finishing PhDs.Please note
that the application deadline is January 15, 2013. Further detailsare
available athttp://simons.berkeley.edu/fellows.html.Mike Jordan</p><p>3 0.97929204 <a title="478-lda-3" href="../hunch_net-2006/hunch_net-2006-07-06-Branch_Prediction_Competition.html">190 hunch net-2006-07-06-Branch Prediction Competition</a></p>
<p>Introduction: Alan Fernpoints out thesecond branch prediction challenge(due September 29)
which is a follow up to thefirst branch prediction competition. Branch
prediction is one of the fundamental learning problems of the computer age:
without it our computers might run an order of magnitude slower. This is a
tough problem since there are sharp constraints on time and space complexity
in an online environment. For machine learning, the "idealistic track" may fit
well. Essentially, they remove these constraints to gain a weak upper bound on
what might be done.</p><p>4 0.96437448 <a title="478-lda-4" href="../hunch_net-2011/hunch_net-2011-12-13-Vowpal_Wabbit_version_6.1_%26%23038%3B_the_NIPS_tutorial.html">451 hunch net-2011-12-13-Vowpal Wabbit version 6.1 &#038; the NIPS tutorial</a></p>
<p>Introduction: I just madeversion 6.1ofVowpal Wabbit. Relative to6.0, there are few new
features, but many refinements.The cluster parallel learning code better
supports multiple simultaneous runs, and other forms of parallelism have been
mostly removed. This incidentally significantly simplifies the learning
core.The online learning algorithms are more general, with support for l1(via
a truncated gradient variant) and l2regularization, and a generalized form of
variable metric learning.There is a solid persistent server mode which can
train online, as well as serve answers to many simultaneous queries, either in
text or binary.This should be a very good release if you are just getting
started, as we've made it compile more automatically out of the box, have
several newexamplesand updated documentation.Aspertradition, we're planning to
do a tutorial at NIPS during the break at theparallel learning workshopat 2pm
Spanish time Friday. I'll cover the basics, leaving the fun stuff for
others.Mirowill cov</p><p>5 0.96407521 <a title="478-lda-5" href="../hunch_net-2006/hunch_net-2006-11-20-Context_and_the_calculation_misperception.html">218 hunch net-2006-11-20-Context and the calculation misperception</a></p>
<p>Introduction: This post is really for peoplenotin machine learning (or related fields). It
is about a common misperception which affects people who have not thought
about the process of trying to predict somethinng. Hopefully, by precisely
stating it, we can remove it.Suppose we have a set of events, each described
by a vector of features.01011101011101000111110011000101110Suppose we want to
predict the value of the first feature given the others. One approach is to
bin the data byonefeature. For the above example, we might partition the data
according to feature 2, then observe that when feature 2 is 0 the label
(feature 1) is mostly 1. On the other hand, when feature 2 is 1, the label
(feature 1) is mostly 0. Using this simple rule we get an observed error rate
of 3/7.There are two issues here. The first is that this is really a training
error rate, and (hence) may be an overoptimistic prediction. This is not a
very serious issue as long as there are a reasonable number of representative
examples.</p><p>6 0.94892645 <a title="478-lda-6" href="../hunch_net-2011/hunch_net-2011-04-06-COLT_open_questions.html">429 hunch net-2011-04-06-COLT open questions</a></p>
<p>same-blog 7 0.91781771 <a title="478-lda-7" href="../hunch_net-2013/hunch_net-2013-01-07-NYU_Large_Scale_Machine_Learning_Class.html">478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</a></p>
<p>8 0.83575004 <a title="478-lda-8" href="../hunch_net-2005/hunch_net-2005-05-16-Regret_minimizing_vs_error_limiting_reductions.html">72 hunch net-2005-05-16-Regret minimizing vs error limiting reductions</a></p>
<p>9 0.82064939 <a title="478-lda-9" href="../hunch_net-2006/hunch_net-2006-05-23-What_is_the_best_regret_transform_reduction_from_multiclass_to_binary%3F.html">181 hunch net-2006-05-23-What is the best regret transform reduction from multiclass to binary?</a></p>
<p>10 0.80861479 <a title="478-lda-10" href="../hunch_net-2005/hunch_net-2005-04-08-Fast_SVMs.html">54 hunch net-2005-04-08-Fast SVMs</a></p>
<p>11 0.7984367 <a title="478-lda-11" href="../hunch_net-2011/hunch_net-2011-03-27-Vowpal_Wabbit%2C_v5.1.html">428 hunch net-2011-03-27-Vowpal Wabbit, v5.1</a></p>
<p>12 0.79209119 <a title="478-lda-12" href="../hunch_net-2010/hunch_net-2010-03-15-The_Efficient_Robust_Conditional_Probability_Estimation_Problem.html">391 hunch net-2010-03-15-The Efficient Robust Conditional Probability Estimation Problem</a></p>
<p>13 0.7882393 <a title="478-lda-13" href="../hunch_net-2011/hunch_net-2011-02-17-What_does_Watson_mean%3F.html">424 hunch net-2011-02-17-What does Watson mean?</a></p>
<p>14 0.78685254 <a title="478-lda-14" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<p>15 0.78653342 <a title="478-lda-15" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>16 0.77818048 <a title="478-lda-16" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>17 0.77710587 <a title="478-lda-17" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>18 0.77519011 <a title="478-lda-18" href="../hunch_net-2005/hunch_net-2005-03-17-Going_all_the_Way%2C_Sometimes.html">42 hunch net-2005-03-17-Going all the Way, Sometimes</a></p>
<p>19 0.77157682 <a title="478-lda-19" href="../hunch_net-2008/hunch_net-2008-03-15-COLT_Open_Problems.html">292 hunch net-2008-03-15-COLT Open Problems</a></p>
<p>20 0.77126575 <a title="478-lda-20" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
