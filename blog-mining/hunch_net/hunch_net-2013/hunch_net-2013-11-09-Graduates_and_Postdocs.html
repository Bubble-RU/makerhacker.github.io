<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>490 hunch net-2013-11-09-Graduates and Postdocs</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2013" href="../home/hunch_net-2013_home.html">hunch_net-2013</a> <a title="hunch_net-2013-490" href="#">hunch_net-2013-490</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>490 hunch net-2013-11-09-Graduates and Postdocs</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2013-490-html" href="http://hunch.net/?p=2680">html</a></p><p>Introduction: Several strong graduates are on the job market this year.
  
  Alekh Agarwal  made the  most scalable public learning algorithm  as an intern two years ago.  He has a deep and broad understanding of optimization and learning as well as the ability and will to make things happen programming-wise.  I’ve been privileged to have Alekh visiting me in NY where he will be sorely missed. 
  John Duchi  created  Adagrad  which is a commonly helpful improvement over online gradient descent that is seeing wide adoption, including in  Vowpal Wabbit .  He has a similarly deep and broad understanding of optimization and learning with significant industry experience at  Google .  Alekh and John have often coauthored together. 
  Stephane Ross  visited me a year ago over the summer, implementing many new algorithms and working out the first  scale free online update rule  which is now the default in Vowpal Wabbit.  Stephane is  not  on the market—Google robbed the cradle successfully    I’m sure that</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Several strong graduates are on the job market this year. [sent-1, score-0.249]
</p><p>2 Alekh Agarwal  made the  most scalable public learning algorithm  as an intern two years ago. [sent-2, score-0.087]
</p><p>3 He has a deep and broad understanding of optimization and learning as well as the ability and will to make things happen programming-wise. [sent-3, score-0.419]
</p><p>4 I’ve been privileged to have Alekh visiting me in NY where he will be sorely missed. [sent-4, score-0.2]
</p><p>5 John Duchi  created  Adagrad  which is a commonly helpful improvement over online gradient descent that is seeing wide adoption, including in  Vowpal Wabbit . [sent-5, score-0.079]
</p><p>6 He has a similarly deep and broad understanding of optimization and learning with significant industry experience at  Google . [sent-6, score-0.51]
</p><p>7 Stephane Ross  visited me a year ago over the summer, implementing many new algorithms and working out the first  scale free online update rule  which is now the default in Vowpal Wabbit. [sent-8, score-0.308]
</p><p>8 Stephane is  not  on the market—Google robbed the cradle successfully    I’m sure that he will do great things. [sent-9, score-0.091]
</p><p>9 Anna Choromanska  visited me this summer, where we worked on  extreme multiclass classification . [sent-10, score-0.154]
</p><p>10 She is very good at focusing on a problem and grinding it into submission both in theory and in practice—I can see why she wins awards for her work. [sent-11, score-0.281]
</p><p>11 I also wanted to mention some postdoc openings in machine learning. [sent-13, score-0.408]
</p><p>12 In New York  Leon Bottou ,  Miro Dudik , and I are  looking for someone . [sent-14, score-0.268]
</p><p>13 In New England,  Sham Kakade  and  Adam Kalai  are  looking for someone . [sent-16, score-0.268]
</p><p>14 Also in the New York area,  Daniel Hsu  and  Tong Zhang  may both be considering a postdoc with no particular deadline. [sent-18, score-0.218]
</p><p>15 In England, Peter Flach is  looking for two postdocs  on a health & machine learning project with a deadline of  December 2 . [sent-19, score-0.506]
</p><p>16 I consider machine learning for healthcare of critical importance in the future. [sent-20, score-0.109]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('december', 0.303), ('alekh', 0.272), ('anna', 0.245), ('postdoc', 0.218), ('stephane', 0.218), ('england', 0.19), ('looking', 0.168), ('deadline', 0.166), ('market', 0.154), ('visited', 0.154), ('broad', 0.138), ('vowpal', 0.133), ('google', 0.123), ('york', 0.12), ('summer', 0.116), ('john', 0.115), ('openings', 0.109), ('privileged', 0.109), ('healthcare', 0.109), ('optimization', 0.105), ('duchi', 0.101), ('miro', 0.101), ('someone', 0.1), ('deep', 0.099), ('awards', 0.095), ('agarwal', 0.095), ('wins', 0.095), ('graduates', 0.095), ('postdocs', 0.095), ('adoption', 0.095), ('focusing', 0.091), ('visiting', 0.091), ('successfully', 0.091), ('ny', 0.091), ('industry', 0.091), ('scalable', 0.087), ('kalai', 0.084), ('future', 0.082), ('bottou', 0.081), ('tong', 0.081), ('mention', 0.081), ('kakade', 0.079), ('seeing', 0.079), ('zhang', 0.079), ('new', 0.077), ('peter', 0.077), ('implementing', 0.077), ('health', 0.077), ('understanding', 0.077), ('adam', 0.075)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="490-tfidf-1" href="../hunch_net-2013/hunch_net-2013-11-09-Graduates_and_Postdocs.html">490 hunch net-2013-11-09-Graduates and Postdocs</a></p>
<p>Introduction: Several strong graduates are on the job market this year.
  
  Alekh Agarwal  made the  most scalable public learning algorithm  as an intern two years ago.  He has a deep and broad understanding of optimization and learning as well as the ability and will to make things happen programming-wise.  I’ve been privileged to have Alekh visiting me in NY where he will be sorely missed. 
  John Duchi  created  Adagrad  which is a commonly helpful improvement over online gradient descent that is seeing wide adoption, including in  Vowpal Wabbit .  He has a similarly deep and broad understanding of optimization and learning with significant industry experience at  Google .  Alekh and John have often coauthored together. 
  Stephane Ross  visited me a year ago over the summer, implementing many new algorithms and working out the first  scale free online update rule  which is now the default in Vowpal Wabbit.  Stephane is  not  on the market—Google robbed the cradle successfully    I’m sure that</p><p>2 0.14049685 <a title="490-tfidf-2" href="../hunch_net-2012/hunch_net-2012-08-27-NYAS_ML_2012_and_ICML_2013.html">472 hunch net-2012-08-27-NYAS ML 2012 and ICML 2013</a></p>
<p>Introduction: The  New York Machine Learning Symposium  is October 19 with a 2 page abstract deadline due September 13 via email with subject “Machine Learning Poster Submission” sent to physicalscience@nyas.org.  Everyone is welcome to submit.  Last year’s attendance was 246 and I expect more this year.
 
The primary experiment for  ICML 2013  is multiple paper submission deadlines with rolling review cycles.  The key dates are October 1, December 15, and February 15.  This is an attempt to shift ICML further towards a journal style review process and reduce peak load.   The “not for proceedings” experiment from this year’s ICML is not continuing.
 
Edit: Fixed second ICML deadline.</p><p>3 0.1226131 <a title="490-tfidf-3" href="../hunch_net-2013/hunch_net-2013-04-15-NEML_II.html">481 hunch net-2013-04-15-NEML II</a></p>
<p>Introduction: Adam Kalai  points out the  New England Machine Learning Day  May 1 at MSR New England.  There is a poster session with abstracts due April 19.  I understand last year’s  NEML  went well and it’s great to meet your neighbors at regional workshops like this.</p><p>4 0.10999987 <a title="490-tfidf-4" href="../hunch_net-2012/hunch_net-2012-09-29-Vowpal_Wabbit%2C_version_7.0.html">473 hunch net-2012-09-29-Vowpal Wabbit, version 7.0</a></p>
<p>Introduction: A new version of  VW  is  out .  The primary changes are:
  
  Learning Reductions : I’ve wanted to get  learning reductions  working and we’ve finally done it.  Not everything is implemented yet, but VW now supports direct:
 
 Multiclass Classification  –oaa  or  –ect . 
 Cost Sensitive Multiclass Classification  –csoaa  or  –wap . 
 Contextual Bandit Classification  –cb . 
 Sequential Structured Prediction   –searn  or  –dagger  
 

In addition, it is now easy to build your own custom learning reductions for various plausible uses: feature diddling, custom structured prediction problems, or alternate learning reductions.  This effort is far from done, but it is now in a generally useful state.  Note that all learning reductions inherit the ability to do cluster parallel learning.

 
  Library interface :  VW now has a basic library interface.  The library provides most of the functionality of VW, with the limitation that it is monolithic and nonreentrant.  These will be improved over</p><p>5 0.10939591 <a title="490-tfidf-5" href="../hunch_net-2009/hunch_net-2009-08-27-New_York_Area_Machine_Learning_Events.html">369 hunch net-2009-08-27-New York Area Machine Learning Events</a></p>
<p>Introduction: Several events are happening in the NY area.
  
  Barriers in Computational Learning Theory Workshop, Aug 28.   That’s tomorrow near Princeton.  I’m looking forward to speaking at this one on “Getting around Barriers in Learning Theory”, but several other talks are of interest, particularly to the CS theory inclined. 
  Claudia Perlich  is running the  INFORMS Data Mining Contest  with a deadline of Sept. 25. This is a contest using real health record data (they partnered with  HealthCare Intelligence ) to predict transfers and mortality. In the current US health care reform debate, the case studies of high costs we hear strongly suggest machine learning & statistics can save many billions. 
  The Singularity Summit October 3&4 .  This is for the AIists out there.  Several of the talks look interesting, although unfortunately I’ll miss it for  ALT . 
  Predictive Analytics World, Oct 20-21 .  This is stretching the definition of “New York Area” a bit, but the train to DC is reasonable.</p><p>6 0.10361937 <a title="490-tfidf-6" href="../hunch_net-2010/hunch_net-2010-08-20-The_Workshop_on_Cores%2C_Clusters%2C_and_Clouds.html">404 hunch net-2010-08-20-The Workshop on Cores, Clusters, and Clouds</a></p>
<p>7 0.10173482 <a title="490-tfidf-7" href="../hunch_net-2012/hunch_net-2012-04-20-Both_new%3A_STOC_workshops_and_NEML.html">462 hunch net-2012-04-20-Both new: STOC workshops and NEML</a></p>
<p>8 0.10156055 <a title="490-tfidf-8" href="../hunch_net-2011/hunch_net-2011-12-13-Vowpal_Wabbit_version_6.1_%26%23038%3B_the_NIPS_tutorial.html">451 hunch net-2011-12-13-Vowpal Wabbit version 6.1 &#038; the NIPS tutorial</a></p>
<p>9 0.097763345 <a title="490-tfidf-9" href="../hunch_net-2009/hunch_net-2009-05-19-CI_Fellows.html">355 hunch net-2009-05-19-CI Fellows</a></p>
<p>10 0.097044721 <a title="490-tfidf-10" href="../hunch_net-2013/hunch_net-2013-01-01-Deep_Learning_2012.html">477 hunch net-2013-01-01-Deep Learning 2012</a></p>
<p>11 0.096452214 <a title="490-tfidf-11" href="../hunch_net-2008/hunch_net-2008-11-28-A_Bumper_Crop_of_Machine_Learning_Graduates.html">329 hunch net-2008-11-28-A Bumper Crop of Machine Learning Graduates</a></p>
<p>12 0.093481719 <a title="490-tfidf-12" href="../hunch_net-2007/hunch_net-2007-12-21-Vowpal_Wabbit_Code_Release.html">281 hunch net-2007-12-21-Vowpal Wabbit Code Release</a></p>
<p>13 0.088539481 <a title="490-tfidf-13" href="../hunch_net-2011/hunch_net-2011-08-15-Vowpal_Wabbit_6.0.html">441 hunch net-2011-08-15-Vowpal Wabbit 6.0</a></p>
<p>14 0.085617401 <a title="490-tfidf-14" href="../hunch_net-2006/hunch_net-2006-06-24-Online_convex_optimization_at_COLT.html">186 hunch net-2006-06-24-Online convex optimization at COLT</a></p>
<p>15 0.084760465 <a title="490-tfidf-15" href="../hunch_net-2006/hunch_net-2006-04-27-Conferences%2C_Workshops%2C_and_Tutorials.html">174 hunch net-2006-04-27-Conferences, Workshops, and Tutorials</a></p>
<p>16 0.082631335 <a title="490-tfidf-16" href="../hunch_net-2009/hunch_net-2009-12-24-Top_graduates_this_season.html">384 hunch net-2009-12-24-Top graduates this season</a></p>
<p>17 0.082127988 <a title="490-tfidf-17" href="../hunch_net-2007/hunch_net-2007-10-17-Online_as_the_new_adjective.html">267 hunch net-2007-10-17-Online as the new adjective</a></p>
<p>18 0.080897123 <a title="490-tfidf-18" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>19 0.080734015 <a title="490-tfidf-19" href="../hunch_net-2010/hunch_net-2010-12-04-Vowpal_Wabbit%2C_version_5.0%2C_and_the_second_heresy.html">419 hunch net-2010-12-04-Vowpal Wabbit, version 5.0, and the second heresy</a></p>
<p>20 0.079038933 <a title="490-tfidf-20" href="../hunch_net-2011/hunch_net-2011-10-24-2011_ML_symposium_and_the_bears.html">448 hunch net-2011-10-24-2011 ML symposium and the bears</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.172), (1, -0.047), (2, -0.124), (3, -0.065), (4, 0.046), (5, -0.005), (6, -0.084), (7, -0.034), (8, -0.159), (9, -0.009), (10, 0.043), (11, -0.083), (12, 0.026), (13, -0.118), (14, 0.023), (15, 0.016), (16, 0.054), (17, -0.026), (18, 0.013), (19, 0.002), (20, 0.056), (21, -0.003), (22, -0.002), (23, -0.019), (24, -0.068), (25, 0.001), (26, 0.023), (27, 0.032), (28, -0.072), (29, -0.022), (30, 0.09), (31, -0.014), (32, -0.081), (33, 0.013), (34, 0.038), (35, 0.019), (36, -0.0), (37, -0.088), (38, -0.117), (39, 0.106), (40, 0.012), (41, 0.019), (42, -0.089), (43, 0.001), (44, 0.035), (45, 0.11), (46, 0.1), (47, -0.007), (48, 0.031), (49, -0.078)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95271039 <a title="490-lsi-1" href="../hunch_net-2013/hunch_net-2013-11-09-Graduates_and_Postdocs.html">490 hunch net-2013-11-09-Graduates and Postdocs</a></p>
<p>Introduction: Several strong graduates are on the job market this year.
  
  Alekh Agarwal  made the  most scalable public learning algorithm  as an intern two years ago.  He has a deep and broad understanding of optimization and learning as well as the ability and will to make things happen programming-wise.  I’ve been privileged to have Alekh visiting me in NY where he will be sorely missed. 
  John Duchi  created  Adagrad  which is a commonly helpful improvement over online gradient descent that is seeing wide adoption, including in  Vowpal Wabbit .  He has a similarly deep and broad understanding of optimization and learning with significant industry experience at  Google .  Alekh and John have often coauthored together. 
  Stephane Ross  visited me a year ago over the summer, implementing many new algorithms and working out the first  scale free online update rule  which is now the default in Vowpal Wabbit.  Stephane is  not  on the market—Google robbed the cradle successfully    I’m sure that</p><p>2 0.60381061 <a title="490-lsi-2" href="../hunch_net-2012/hunch_net-2012-04-20-Both_new%3A_STOC_workshops_and_NEML.html">462 hunch net-2012-04-20-Both new: STOC workshops and NEML</a></p>
<p>Introduction: May 16 in Cambridge , is the  New England Machine Learning Day , a first regional workshop/symposium on machine learning.  To present a poster, submit an abstract by  May 5 .
 
 May 19 in New York ,  STOC  is coming to town and  rather surprisingly having  workshops  which should be quite a bit of fun.  Iâ&euro;&trade;ll be speaking at  Algorithms for Distributed and Streaming Data .</p><p>3 0.57013953 <a title="490-lsi-3" href="../hunch_net-2013/hunch_net-2013-04-15-NEML_II.html">481 hunch net-2013-04-15-NEML II</a></p>
<p>Introduction: Adam Kalai  points out the  New England Machine Learning Day  May 1 at MSR New England.  There is a poster session with abstracts due April 19.  I understand last year’s  NEML  went well and it’s great to meet your neighbors at regional workshops like this.</p><p>4 0.56367636 <a title="490-lsi-4" href="../hunch_net-2010/hunch_net-2010-12-04-Vowpal_Wabbit%2C_version_5.0%2C_and_the_second_heresy.html">419 hunch net-2010-12-04-Vowpal Wabbit, version 5.0, and the second heresy</a></p>
<p>Introduction: I’ve released  version 5.0  of the  Vowpal Wabbit  online learning software.  The major number has changed since the  last release  because I regard all earlier versions as obsolete—there are several new algorithms & features including substantial changes and upgrades to the default learning algorithm.  
 
The biggest changes are new algorithms:
  
  Nikos  and I improved the default algorithm.  The basic update rule still uses gradient descent, but the size of the update is carefully controlled so that it’s impossible to overrun the label.  In addition, the normalization has changed.  Computationally, these changes are virtually free and yield better results, sometimes much better.  Less careful updates can be reenabled with –loss_function classic, although results are still not identical to previous due to normalization changes. 
 Nikos also implemented the per-feature learning rates as per these  two   papers .  Often, this works better than the default algorithm.  It isn’t the defa</p><p>5 0.53798449 <a title="490-lsi-5" href="../hunch_net-2011/hunch_net-2011-12-13-Vowpal_Wabbit_version_6.1_%26%23038%3B_the_NIPS_tutorial.html">451 hunch net-2011-12-13-Vowpal Wabbit version 6.1 &#038; the NIPS tutorial</a></p>
<p>Introduction: I just made  version 6.1  of  Vowpal Wabbit .  Relative to  6.0 , there are few new features, but many refinements. 
  
 The cluster parallel learning code better supports multiple simultaneous runs, and other forms of parallelism have been mostly removed.  This incidentally significantly simplifies the learning core. 
 The online learning algorithms are more general, with support for l 1  (via a truncated gradient variant) and l 2  regularization, and a generalized form of variable metric learning. 
 There is a solid persistent server mode which can train online, as well as serve answers to many simultaneous queries, either in text or binary. 
  
This should be a very good release if you are just getting started, as we’ve made it compile more automatically out of the box, have several new  examples  and updated documentation.
 
As  per   tradition , we’re planning to do a tutorial at NIPS during the break at the  parallel learning workshop  at 2pm Spanish time Friday.  I’ll cover the</p><p>6 0.53504384 <a title="490-lsi-6" href="../hunch_net-2008/hunch_net-2008-11-28-A_Bumper_Crop_of_Machine_Learning_Graduates.html">329 hunch net-2008-11-28-A Bumper Crop of Machine Learning Graduates</a></p>
<p>7 0.51938546 <a title="490-lsi-7" href="../hunch_net-2007/hunch_net-2007-12-21-Vowpal_Wabbit_Code_Release.html">281 hunch net-2007-12-21-Vowpal Wabbit Code Release</a></p>
<p>8 0.51596701 <a title="490-lsi-8" href="../hunch_net-2012/hunch_net-2012-03-24-David_Waltz.html">460 hunch net-2012-03-24-David Waltz</a></p>
<p>9 0.51227689 <a title="490-lsi-9" href="../hunch_net-2012/hunch_net-2012-09-29-Vowpal_Wabbit%2C_version_7.0.html">473 hunch net-2012-09-29-Vowpal Wabbit, version 7.0</a></p>
<p>10 0.50017709 <a title="490-lsi-10" href="../hunch_net-2011/hunch_net-2011-06-22-Ultra_LDA.html">436 hunch net-2011-06-22-Ultra LDA</a></p>
<p>11 0.49888009 <a title="490-lsi-11" href="../hunch_net-2010/hunch_net-2010-10-28-NY_ML_Symposium_2010.html">415 hunch net-2010-10-28-NY ML Symposium 2010</a></p>
<p>12 0.48827761 <a title="490-lsi-12" href="../hunch_net-2013/hunch_net-2013-01-01-Deep_Learning_2012.html">477 hunch net-2013-01-01-Deep Learning 2012</a></p>
<p>13 0.48359659 <a title="490-lsi-13" href="../hunch_net-2011/hunch_net-2011-08-15-Vowpal_Wabbit_6.0.html">441 hunch net-2011-08-15-Vowpal Wabbit 6.0</a></p>
<p>14 0.47461125 <a title="490-lsi-14" href="../hunch_net-2006/hunch_net-2006-05-08-Big_machine_learning.html">178 hunch net-2006-05-08-Big machine learning</a></p>
<p>15 0.47053859 <a title="490-lsi-15" href="../hunch_net-2007/hunch_net-2007-06-13-Not_Posting.html">246 hunch net-2007-06-13-Not Posting</a></p>
<p>16 0.44937876 <a title="490-lsi-16" href="../hunch_net-2013/hunch_net-2013-12-01-NIPS_tutorials_and_Vowpal_Wabbit_7.4.html">492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</a></p>
<p>17 0.44931534 <a title="490-lsi-17" href="../hunch_net-2012/hunch_net-2012-08-27-NYAS_ML_2012_and_ICML_2013.html">472 hunch net-2012-08-27-NYAS ML 2012 and ICML 2013</a></p>
<p>18 0.4468289 <a title="490-lsi-18" href="../hunch_net-2011/hunch_net-2011-09-03-Fall_Machine_Learning_Events.html">443 hunch net-2011-09-03-Fall Machine Learning Events</a></p>
<p>19 0.44473791 <a title="490-lsi-19" href="../hunch_net-2009/hunch_net-2009-03-18-Parallel_ML_primitives.html">346 hunch net-2009-03-18-Parallel ML primitives</a></p>
<p>20 0.44054928 <a title="490-lsi-20" href="../hunch_net-2009/hunch_net-2009-05-30-Many_ways_to_Learn_this_summer.html">357 hunch net-2009-05-30-Many ways to Learn this summer</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(7, 0.02), (9, 0.359), (10, 0.014), (27, 0.103), (38, 0.022), (51, 0.013), (53, 0.106), (55, 0.139), (86, 0.025), (94, 0.095), (95, 0.016)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.89036137 <a title="490-lda-1" href="../hunch_net-2013/hunch_net-2013-11-09-Graduates_and_Postdocs.html">490 hunch net-2013-11-09-Graduates and Postdocs</a></p>
<p>Introduction: Several strong graduates are on the job market this year.
  
  Alekh Agarwal  made the  most scalable public learning algorithm  as an intern two years ago.  He has a deep and broad understanding of optimization and learning as well as the ability and will to make things happen programming-wise.  I’ve been privileged to have Alekh visiting me in NY where he will be sorely missed. 
  John Duchi  created  Adagrad  which is a commonly helpful improvement over online gradient descent that is seeing wide adoption, including in  Vowpal Wabbit .  He has a similarly deep and broad understanding of optimization and learning with significant industry experience at  Google .  Alekh and John have often coauthored together. 
  Stephane Ross  visited me a year ago over the summer, implementing many new algorithms and working out the first  scale free online update rule  which is now the default in Vowpal Wabbit.  Stephane is  not  on the market—Google robbed the cradle successfully    I’m sure that</p><p>2 0.87479961 <a title="490-lda-2" href="../hunch_net-2006/hunch_net-2006-04-27-Conferences%2C_Workshops%2C_and_Tutorials.html">174 hunch net-2006-04-27-Conferences, Workshops, and Tutorials</a></p>
<p>Introduction: This is a reminder that many deadlines for summer conference registration are coming up, and attendance is a very good idea.  
  
 It’s entirely reasonable for anyone to visit a conference once, even when they don’t have a paper.  For students, visiting a conference is almost a ‘must’—there is no where else that a broad cross-section of research is on display. 
 Workshops are also a very good idea.   ICML has 11 ,  KDD has 9 , and  AAAI has 19 .  Workshops provide an opportunity to get a good understanding of some current area of research.  They are probably the forum most conducive to starting new lines of research because they are so interactive. 
 Tutorials are a good way to gain some understanding of a long-standing direction of research.  They are generally more coherent than workshops.   ICML has 7  and  AAAI has 15 .</p><p>3 0.86862361 <a title="490-lda-3" href="../hunch_net-2010/hunch_net-2010-05-20-Google_Predict.html">399 hunch net-2010-05-20-Google Predict</a></p>
<p>Introduction: Slashdot  points out  Google Predict .  I’m not privy to the details, but this has the potential to be extremely useful, as in many applications simply having an easy mechanism to apply existing learning algorithms can be extremely helpful.  This differs goalwise from  MLcomp —instead of public comparisons for research purposes, it’s about private utilization of good existing algorithms.  It also differs infrastructurally, since a system designed to do this is much less awkward than using Amazon’s cloud computing.  The latter implies that datasets several order of magnitude larger can be handled up to limits imposed by network and storage.</p><p>4 0.79688805 <a title="490-lda-4" href="../hunch_net-2009/hunch_net-2009-06-24-Interesting_papers_at_UAICMOLT_2009.html">361 hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</a></p>
<p>Introduction: Here’s a list of papers that I found interesting at  ICML / COLT / UAI  in 2009.
  
  Elad Hazan  and  Comandur Seshadhri   Efficient learning algorithms for changing environments  at ICML.  This paper shows how to adapt learning algorithms that compete with fixed predictors to compete with changing policies.  The definition of regret they deal with seems particularly useful in many situation. 
  Hal Daume ,  Unsupervised Search-based Structured Prediction  at ICML.  This paper shows a technique for reducing unsupervised learning to supervised learning which (a) make a fast unsupervised learning algorithm and (b)  makes semisupervised learning both easy and highly effective.  
 There were two papers with similar results on active learning in the KWIK framework for linear regression, both reducing the sample complexity to .  One was  Nicolo Cesa-Bianchi ,  Claudio Gentile , and  Francesco Orabona   Robust Bounds for Classification via Selective Sampling  at ICML and the other was  Thoma</p><p>5 0.7529062 <a title="490-lda-5" href="../hunch_net-2008/hunch_net-2008-12-07-A_NIPS_paper.html">330 hunch net-2008-12-07-A NIPS paper</a></p>
<p>Introduction: I’m skipping NIPS this year in favor of  Ada , but I wanted to point out  this paper  by  Andriy Mnih  and  Geoff Hinton .  The basic claim of the paper is that by carefully but automatically constructing a binary tree over words, it’s possible to predict words well with huge computational resource savings over unstructured approaches.
 
I’m interested in this beyond the application to word prediction because it is relevant to the general normalization problem: If you want to predict the probability of one of a large number of events, often you must compute a predicted score for all the events and then normalize, a computationally inefficient operation.  The problem comes up in many places using probabilistic models, but I’ve run into it with high-dimensional regression.
 
There are a couple workarounds for this computational bug:
  
 Approximate. There are many ways.  Often the approximations are uncontrolled (i.e. can be arbitrarily bad), and hence finicky in application. 
 Avoid.  Y</p><p>6 0.75121307 <a title="490-lda-6" href="../hunch_net-2006/hunch_net-2006-02-18-Multiplication_of_Learned_Probabilities_is_Dangerous.html">157 hunch net-2006-02-18-Multiplication of Learned Probabilities is Dangerous</a></p>
<p>7 0.57532299 <a title="490-lda-7" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>8 0.52212119 <a title="490-lda-8" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>9 0.50246781 <a title="490-lda-9" href="../hunch_net-2006/hunch_net-2006-09-12-Incentive_Compatible_Reviewing.html">207 hunch net-2006-09-12-Incentive Compatible Reviewing</a></p>
<p>10 0.48865807 <a title="490-lda-10" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>11 0.48755658 <a title="490-lda-11" href="../hunch_net-2006/hunch_net-2006-03-02-Why_do_people_count_for_learning%3F.html">160 hunch net-2006-03-02-Why do people count for learning?</a></p>
<p>12 0.48503798 <a title="490-lda-12" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>13 0.48474759 <a title="490-lda-13" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>14 0.48401958 <a title="490-lda-14" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>15 0.48366433 <a title="490-lda-15" href="../hunch_net-2005/hunch_net-2005-03-13-Avoiding_Bad_Reviewing.html">40 hunch net-2005-03-13-Avoiding Bad Reviewing</a></p>
<p>16 0.48091939 <a title="490-lda-16" href="../hunch_net-2012/hunch_net-2012-05-02-ICML%3A_Behind_the_Scenes.html">463 hunch net-2012-05-02-ICML: Behind the Scenes</a></p>
<p>17 0.48006287 <a title="490-lda-17" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<p>18 0.4775528 <a title="490-lda-18" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>19 0.47678423 <a title="490-lda-19" href="../hunch_net-2005/hunch_net-2005-12-14-More_NIPS_Papers_II.html">140 hunch net-2005-12-14-More NIPS Papers II</a></p>
<p>20 0.47185832 <a title="490-lda-20" href="../hunch_net-2010/hunch_net-2010-04-26-Compassionate_Reviewing.html">395 hunch net-2010-04-26-Compassionate Reviewing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
