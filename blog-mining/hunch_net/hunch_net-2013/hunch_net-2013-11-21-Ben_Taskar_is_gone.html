<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>491 hunch net-2013-11-21-Ben Taskar is gone</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2013" href="../home/hunch_net-2013_home.html">hunch_net-2013</a> <a title="hunch_net-2013-491" href="#">hunch_net-2013-491</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>491 hunch net-2013-11-21-Ben Taskar is gone</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2013-491-html" href="http://hunch.net/?p=2684">html</a></p><p>Introduction: I was not as personally close toBenasSam, but the level of tragedy is similar
and I can't help but be greatly saddened by the loss.Variousnewsstorieshave
coverage, but the synopsis is that he had a heart attack on Sunday and is
survived by his wife Anat and daughter Aviv. There is discussion of creating a
memorial fund for them, which I hope comes to fruition, and plan to contribute
to.I will remember Ben as someone who thought carefully and comprehensively
about new ways to do things, then fought hard and successfully for what he
believed in. It is an ideal we strive for, that Ben accomplished.Edit:
donationsgo here, and more information ishere.</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ben', 0.435), ('sunday', 0.235), ('fought', 0.235), ('strive', 0.235), ('wife', 0.235), ('believed', 0.235), ('ishere', 0.218), ('coverage', 0.218), ('heart', 0.205), ('successfully', 0.205), ('fund', 0.196), ('contribute', 0.176), ('attack', 0.171), ('ideal', 0.158), ('remember', 0.146), ('plan', 0.144), ('personally', 0.139), ('close', 0.137), ('carefully', 0.118), ('greatly', 0.115)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="491-tfidf-1" href="../hunch_net-2013/hunch_net-2013-11-21-Ben_Taskar_is_gone.html">491 hunch net-2013-11-21-Ben Taskar is gone</a></p>
<p>Introduction: I was not as personally close toBenasSam, but the level of tragedy is similar
and I can't help but be greatly saddened by the loss.Variousnewsstorieshave
coverage, but the synopsis is that he had a heart attack on Sunday and is
survived by his wife Anat and daughter Aviv. There is discussion of creating a
memorial fund for them, which I hope comes to fruition, and plan to contribute
to.I will remember Ben as someone who thought carefully and comprehensively
about new ways to do things, then fought hard and successfully for what he
believed in. It is an ideal we strive for, that Ben accomplished.Edit:
donationsgo here, and more information ishere.</p><p>2 0.075265534 <a title="491-tfidf-2" href="../hunch_net-2010/hunch_net-2010-01-13-Sam_Roweis_died.html">386 hunch net-2010-01-13-Sam Roweis died</a></p>
<p>Introduction: and I can't help but remember him.I first metSamas an undergraduate
atCaltechwhere he was TA forHopfield's class, and again when I visitedGatsby,
when he invited me to visitToronto, and at too many conferences to recount.
His personality was a combination of enthusiastic and thoughtful, with a great
ability to phrase a problem so it's solution must be understood. With respect
to my own work, Sam was the one who advised me to makemy first tutorial,
leading to others, and to other things, all of which I'm grateful to him for.
In fact, my every interaction with Sam was positive, and that was his way.His
death isbeing called a suicidewhich is so incompatible with my understanding
of Sam that it strains my credibility. But we know that his many
responsibilities were great, and it is well understood that basically all sane
researchers have legions of inner doubts. Having been depressed now and then
myself, it's helpful to understand at least intellectually that the true
darkness of the now i</p><p>3 0.063624173 <a title="491-tfidf-3" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>Introduction: Consider the contextual bandit setting where, repeatedly:A contextxis
observed.An actionais taken given the contextx.A rewardris observed, dependent
onxanda.Where the goal of a learning agent is to find a policy for step 2
achieving a large expected reward.This setting is of obvious importance,
because in the real world we typically make decisions based on some set of
information and then get feedback only about the single action taken. It also
fundamentally differs from supervised learning settings because knowing the
value of one action is not equivalent to knowing the value of all actions.A
decade ago the best machine learning techniques for this setting where
implausibly inefficient.Dean Fosteronce told me he thought the area was a
research sinkhole with little progress to be expected. Now we are on the verge
of being able to routinely attack these problems, in almost exactly the same
sense that we routinely attack bread and butter supervised learning problems.
Just as for supervis</p><p>4 0.057206206 <a title="491-tfidf-4" href="../hunch_net-2006/hunch_net-2006-03-27-Gradients_everywhere.html">167 hunch net-2006-03-27-Gradients everywhere</a></p>
<p>Introduction: One of the basic observations from theatomic learning workshopis that
gradient-based optimization is pervasive. For example, at least 7 (of 12)
speakers used the word 'gradient' in their talk and several others may be
approximating a gradient. The essential useful quality of a gradient is that
it decouples local updates from global optimization. Restated: Given a
gradient, we can determine how to change individual parameters of the system
so as to improve overall performance.It's easy to feel depressed about this
and think "nothing has happened", but that appears untrue. Many of the talks
were about clever techniques for computing gradients where your calculus
textbook breaks down.Sometimes there are clever approximations of the
gradient. (Simon Osindero)Sometimes we can compute constrained gradients via
iterated gradient/project steps. (Ben Taskar)Sometimes we can compute
gradients anyways over mildly nondifferentiable functions. (Drew Bagnell)Even
given a gradient, the choice of upda</p><p>5 0.053284783 <a title="491-tfidf-5" href="../hunch_net-2005/hunch_net-2005-03-28-Open_Problems_for_Colt.html">47 hunch net-2005-03-28-Open Problems for Colt</a></p>
<p>Introduction: Adam KlivansandRocco Servedioare looking foropen (learning theory)
problemsforCOLT. This is a good idea in the same way that the KDDcup challenge
is a good idea: crisp problem definitions that anyone can attack yield
solutions that advance science.</p><p>6 0.052598238 <a title="491-tfidf-6" href="../hunch_net-2006/hunch_net-2006-03-05-%26%238220%3BStructural%26%238221%3B_Learning.html">161 hunch net-2006-03-05-&#8220;Structural&#8221; Learning</a></p>
<p>7 0.048928767 <a title="491-tfidf-7" href="../hunch_net-2005/hunch_net-2005-05-29-Maximum_Margin_Mismatch%3F.html">77 hunch net-2005-05-29-Maximum Margin Mismatch?</a></p>
<p>8 0.047403619 <a title="491-tfidf-8" href="../hunch_net-2006/hunch_net-2006-12-05-Recruitment_Conferences.html">222 hunch net-2006-12-05-Recruitment Conferences</a></p>
<p>9 0.045539841 <a title="491-tfidf-9" href="../hunch_net-2009/hunch_net-2009-12-24-Top_graduates_this_season.html">384 hunch net-2009-12-24-Top graduates this season</a></p>
<p>10 0.044871069 <a title="491-tfidf-10" href="../hunch_net-2007/hunch_net-2007-07-20-Motivation_should_be_the_Responsibility_of_the_Reviewer.html">256 hunch net-2007-07-20-Motivation should be the Responsibility of the Reviewer</a></p>
<p>11 0.041616548 <a title="491-tfidf-11" href="../hunch_net-2005/hunch_net-2005-09-20-Workshop_Proposal%3A_Atomic_Learning.html">114 hunch net-2005-09-20-Workshop Proposal: Atomic Learning</a></p>
<p>12 0.041289706 <a title="491-tfidf-12" href="../hunch_net-2007/hunch_net-2007-02-16-The_Forgetting.html">233 hunch net-2007-02-16-The Forgetting</a></p>
<p>13 0.040963307 <a title="491-tfidf-13" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>14 0.04089402 <a title="491-tfidf-14" href="../hunch_net-2005/hunch_net-2005-03-05-Funding_Research.html">36 hunch net-2005-03-05-Funding Research</a></p>
<p>15 0.040696595 <a title="491-tfidf-15" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>16 0.040320165 <a title="491-tfidf-16" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>17 0.038802702 <a title="491-tfidf-17" href="../hunch_net-2008/hunch_net-2008-05-23-Three_levels_of_addressing_the_Netflix_Prize.html">301 hunch net-2008-05-23-Three levels of addressing the Netflix Prize</a></p>
<p>18 0.03857198 <a title="491-tfidf-18" href="../hunch_net-2010/hunch_net-2010-09-13-AIStats.html">409 hunch net-2010-09-13-AIStats</a></p>
<p>19 0.038351677 <a title="491-tfidf-19" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>20 0.037745643 <a title="491-tfidf-20" href="../hunch_net-2005/hunch_net-2005-04-21-Dynamic_Programming_Generalizations_and_Their_Use.html">58 hunch net-2005-04-21-Dynamic Programming Generalizations and Their Use</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.07), (1, 0.024), (2, 0.025), (3, -0.035), (4, 0.017), (5, -0.016), (6, -0.006), (7, -0.011), (8, -0.003), (9, 0.03), (10, -0.01), (11, 0.022), (12, 0.016), (13, -0.004), (14, -0.02), (15, 0.027), (16, -0.015), (17, 0.008), (18, -0.026), (19, 0.009), (20, 0.023), (21, 0.004), (22, 0.007), (23, -0.02), (24, -0.001), (25, -0.016), (26, -0.063), (27, 0.017), (28, 0.016), (29, 0.039), (30, 0.046), (31, -0.012), (32, -0.01), (33, -0.034), (34, 0.038), (35, -0.008), (36, 0.045), (37, 0.036), (38, -0.029), (39, 0.013), (40, -0.053), (41, 0.032), (42, -0.013), (43, -0.056), (44, -0.019), (45, -0.01), (46, 0.013), (47, 0.036), (48, -0.042), (49, 0.002)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97321606 <a title="491-lsi-1" href="../hunch_net-2013/hunch_net-2013-11-21-Ben_Taskar_is_gone.html">491 hunch net-2013-11-21-Ben Taskar is gone</a></p>
<p>Introduction: I was not as personally close toBenasSam, but the level of tragedy is similar
and I can't help but be greatly saddened by the loss.Variousnewsstorieshave
coverage, but the synopsis is that he had a heart attack on Sunday and is
survived by his wife Anat and daughter Aviv. There is discussion of creating a
memorial fund for them, which I hope comes to fruition, and plan to contribute
to.I will remember Ben as someone who thought carefully and comprehensively
about new ways to do things, then fought hard and successfully for what he
believed in. It is an ideal we strive for, that Ben accomplished.Edit:
donationsgo here, and more information ishere.</p><p>2 0.63352668 <a title="491-lsi-2" href="../hunch_net-2007/hunch_net-2007-02-10-Best_Practices_for_Collaboration.html">231 hunch net-2007-02-10-Best Practices for Collaboration</a></p>
<p>Introduction: Many people, especially students, haven't had an opportunity to collaborate
with other researchers. Collaboration, especially with remote people can be
tricky. Here are some observations of what has worked for me on collaborations
involving a few people.Travel and DiscussAlmost all collaborations start with
in-person discussion. This implies that travel is often necessary. We can hope
that in the future we'll have better systems for starting collaborations
remotely (such as blogs), but we aren't quite there yet.Enable your
collaborator. A collaboration can fall apart because one collaborator disables
another. This sounds stupid (and it is), but it's far easier than you might
think.Avoid Duplication. Discovering that you and a collaborator have been
editing the same thing and now need to waste time reconciling changes is
annoying. The best way to avoid this to be explicit about who has write
permission to what. Most of the time, a write lock is held for the entire
document, just to be s</p><p>3 0.58310777 <a title="491-lsi-3" href="../hunch_net-2007/hunch_net-2007-06-21-Presentation_Preparation.html">249 hunch net-2007-06-21-Presentation Preparation</a></p>
<p>Introduction: A big part of doing research is presenting it at a conference. Since many
people start out shy of public presentations, this can be a substantial
challenge. Here are a few notes which might be helpful when thinking about
preparing a presentation on research.Motivate. Talks which don't start by
describing the problem to solve cause many people to zone out.Prioritize. It
is typical that you have more things to say than time to say them, and many
presenters fall into the failure mode of trying to say too much. This is an
easy-to-understand failure mode as it's very natural to want to include
everything. A basic fact is: you can't. Example of this are:Your slides are so
densely full of equations and words that you can't cover them.Your talk runs
over and a moderator prioritizes for you by cutting you off.You motor-mouth
through the presentation, and the information absorption rate of the audience
prioritizes in some uncontrolled fashion.The rate of flow of concepts simply
exceeds the infor</p><p>4 0.51701391 <a title="491-lsi-4" href="../hunch_net-2005/hunch_net-2005-07-10-Thinking_the_Unthought.html">91 hunch net-2005-07-10-Thinking the Unthought</a></p>
<p>Introduction: One thing common to much research is that the researcher must be the first
personeverto have some thought. How do you think of something that has never
been thought of? There seems to be no methodical manner of doing this, but
there are some tricks.The easiest method is to just have some connection come
to you. There is a trick here however: you should write it down and fill out
the idea immediately because it can just as easily go away.A harder method is
to set aside a block of time and simply think about an idea. Distraction
elimination is essential here because thinking about the unthought is hard
work which your mind will avoid.Another common method is in conversation.
Sometimes the process of verbalizing implies new ideas come up and sometimes
whoever you are talking to replies just the right way. This method is
dangerous though--you must speak to someone who helps you think rather than
someone who occupies your thoughts.Try to rephrase the problem so the answer
is simple. This is</p><p>5 0.51561815 <a title="491-lsi-5" href="../hunch_net-2009/hunch_net-2009-06-01-Multitask_Poisoning.html">358 hunch net-2009-06-01-Multitask Poisoning</a></p>
<p>Introduction: There are many ways that interesting research gets done. For example it's
common at a conference for someone to discuss a problem with a partial
solution, and for someone else to know how to solve a piece of it, resulting
in a paper. In some sense, these are the easiest results we can achieve, so we
should ask: Can all research be this easy?The answer is certainly no for
fields where research inherently requires experimentation to discover how the
real world works. However, mathematics, including parts of physics, computer
science, statistics, etcâ&euro;Ś which are effectively mathematics don't require
experimentation. In effect, a paper can be simply a pure expression of
thinking. Can all mathematical-style research be this easy?What's going on
here is research-by-communication. Someone knows something, someone knows
something else, and as soon as someone knows both things, a problem is solved.
The interesting thing about research-by-communication is that it is becoming
radically easier with</p><p>6 0.49528593 <a title="491-lsi-6" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>7 0.47136119 <a title="491-lsi-7" href="../hunch_net-2005/hunch_net-2005-06-13-Wikis_for_Summer_Schools_and_Workshops.html">81 hunch net-2005-06-13-Wikis for Summer Schools and Workshops</a></p>
<p>8 0.46977344 <a title="491-lsi-8" href="../hunch_net-2010/hunch_net-2010-01-13-Sam_Roweis_died.html">386 hunch net-2010-01-13-Sam Roweis died</a></p>
<p>9 0.46087262 <a title="491-lsi-9" href="../hunch_net-2006/hunch_net-2006-03-09-Use_of_Notation.html">162 hunch net-2006-03-09-Use of Notation</a></p>
<p>10 0.4528431 <a title="491-lsi-10" href="../hunch_net-2008/hunch_net-2008-07-04-More_Presentation_Preparation.html">307 hunch net-2008-07-04-More Presentation Preparation</a></p>
<p>11 0.45250812 <a title="491-lsi-11" href="../hunch_net-2005/hunch_net-2005-05-28-Running_A_Machine_Learning_Summer_School.html">75 hunch net-2005-05-28-Running A Machine Learning Summer School</a></p>
<p>12 0.44799253 <a title="491-lsi-12" href="../hunch_net-2005/hunch_net-2005-03-29-Academic_Mechanism_Design.html">48 hunch net-2005-03-29-Academic Mechanism Design</a></p>
<p>13 0.44783187 <a title="491-lsi-13" href="../hunch_net-2006/hunch_net-2006-01-08-Debugging_Your_Brain.html">147 hunch net-2006-01-08-Debugging Your Brain</a></p>
<p>14 0.44698793 <a title="491-lsi-14" href="../hunch_net-2006/hunch_net-2006-09-18-What_is_missing_for_online_collaborative_research%3F.html">208 hunch net-2006-09-18-What is missing for online collaborative research?</a></p>
<p>15 0.44099775 <a title="491-lsi-15" href="../hunch_net-2008/hunch_net-2008-04-21-The_Science_2.0_article.html">296 hunch net-2008-04-21-The Science 2.0 article</a></p>
<p>16 0.43991905 <a title="491-lsi-16" href="../hunch_net-2008/hunch_net-2008-08-24-Mass_Customized_Medicine_in_the_Future%3F.html">314 hunch net-2008-08-24-Mass Customized Medicine in the Future?</a></p>
<p>17 0.43354857 <a title="491-lsi-17" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>18 0.42866552 <a title="491-lsi-18" href="../hunch_net-2006/hunch_net-2006-06-25-Presentation_of_Proofs_is_Hard..html">187 hunch net-2006-06-25-Presentation of Proofs is Hard.</a></p>
<p>19 0.42526874 <a title="491-lsi-19" href="../hunch_net-2005/hunch_net-2005-08-11-Why_Manifold-Based_Dimension_Reduction_Techniques%3F.html">102 hunch net-2005-08-11-Why Manifold-Based Dimension Reduction Techniques?</a></p>
<p>20 0.42477518 <a title="491-lsi-20" href="../hunch_net-2005/hunch_net-2005-07-01-The_Role_of_Impromptu_Talks.html">88 hunch net-2005-07-01-The Role of Impromptu Talks</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(42, 0.123), (68, 0.076), (74, 0.105), (98, 0.543)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95200545 <a title="491-lda-1" href="../hunch_net-2013/hunch_net-2013-11-21-Ben_Taskar_is_gone.html">491 hunch net-2013-11-21-Ben Taskar is gone</a></p>
<p>Introduction: I was not as personally close toBenasSam, but the level of tragedy is similar
and I can't help but be greatly saddened by the loss.Variousnewsstorieshave
coverage, but the synopsis is that he had a heart attack on Sunday and is
survived by his wife Anat and daughter Aviv. There is discussion of creating a
memorial fund for them, which I hope comes to fruition, and plan to contribute
to.I will remember Ben as someone who thought carefully and comprehensively
about new ways to do things, then fought hard and successfully for what he
believed in. It is an ideal we strive for, that Ben accomplished.Edit:
donationsgo here, and more information ishere.</p><p>2 0.80243903 <a title="491-lda-2" href="../hunch_net-2008/hunch_net-2008-06-09-The_Minimum_Sample_Complexity_of_Importance_Weighting.html">303 hunch net-2008-06-09-The Minimum Sample Complexity of Importance Weighting</a></p>
<p>Introduction: This post is about a trick that I learned fromDale Schuurmanswhich has been
repeatedly useful for me over time.The basic trick has to do with importance
weighting for monte carlo integration. Consider the problem of finding:N = Ex
~ Df(x)given samples fromDand knowledge off.Often, we don't have samples
fromDavailable. Instead, we must make do with samples from some other
distributionQ. In that case, we can still often solve the problem, as long as
Q(x) isn't 0 when D(x) is nonzero, using the importance weighting formula:Ex ~
Qf(x) D(x)/Q(x)A basic question is: How many samples fromQare required in
order to estimateNto some precision? In general the convergence rate is not
bounded, becausef(x) D(x)/Q(x)is not bounded given the
assumptions.Nevertheless, there is one special valueQ(x) = f(x) D(x) / Nwhere
the sample complexity turns out to be1, which is typically substantially
better than the sample complexity of the original problem.This observation
underlies the motivation for voluntary</p><p>3 0.71218526 <a title="491-lda-3" href="../hunch_net-2005/hunch_net-2005-12-22-Yes_%2C_I_am_applying.html">142 hunch net-2005-12-22-Yes , I am applying</a></p>
<p>Introduction: Every year about now hundreds of applicants apply for a research/teaching job
with the timing governed by the university recruitment schedule. This time,
it's my turn--the hat's in the ring, I am a contender, etcâ&euro;Ś What I have heard
is that this year is good in both directions--both an increased supply and an
increased demand for machine learning expertise.I consider this post a bit of
an abuse as it is neither about general research nor machine learning. Please
forgive me this once.My hope is that I will learn about new places interested
in funding basic research--it's easy to imagine that I have overlooked
possibilities.I am not dogmatic about where I end up in any particular way.
Several earlier posts detail what I think of as a good research environment,
so I will avoid a repeat. A few more details seem important:Application. There
is often a tension between basic research and immediate application. This
tension is not as strong as might be expected in my case. As evidence, many of</p><p>4 0.52346396 <a title="491-lda-4" href="../hunch_net-2005/hunch_net-2005-11-28-A_question_of_quantification.html">133 hunch net-2005-11-28-A question of quantification</a></p>
<p>Introduction: This is about methods for phrasing and think about the scope of some theorems
in learning theory. The basic claim is that there are several different ways
of quantifying the scope which sound different yet are essentially the
same.For all sequences of examples. This is the standard quantification in
online learning analysis. Standard theorems would say something like "for all
sequences of predictions by experts, the algorithm A will perform almost as
well as the best expert."For all training sets. This is the standard
quantification for boosting analysis such asadaboostormulticlass
boosting.Standard theorems have the form "for all training sets the error rate
inequalities … hold".For all distributions over examples. This is the one that
we have been using for reductions analysis. Standard theorem statements have
the form "For all distributions over examples, the error rate inequalities …
hold".It is not quite true that each of these is equivalent. For example, in
the online learning se</p><p>5 0.52174091 <a title="491-lda-5" href="../hunch_net-2005/hunch_net-2005-01-24-Holy_grails_of_machine_learning%3F.html">2 hunch net-2005-01-24-Holy grails of machine learning?</a></p>
<p>Introduction: Let me kick things off by posing this question to ML researchers:What do you
think are some important holy grails of machine learning?For example:- "A
classifier with SVM-level performance but much more scalable"- "Practical
confidence bounds (or learning bounds) for classification"- "A reinforcement
learning algorithm that can handle the ___ problem"- "Understanding
theoretically why ___ works so well in practice"etc.I pose this question
because I believe that when goals are stated explicitly and well (thus
providing clarity as well as opening up the problems to more people), rather
than left implicit, they are likely to be achieved much more quickly. I would
also like to know more about the internal goals of the various machine
learning sub-areas (theory, kernel methods, graphical models, reinforcement
learning, etc) as stated by people in these respective areas. This could help
people cross sub-areas.</p><p>6 0.45214748 <a title="491-lda-6" href="../hunch_net-2008/hunch_net-2008-03-23-Interactive_Machine_Learning.html">293 hunch net-2008-03-23-Interactive Machine Learning</a></p>
<p>7 0.36166602 <a title="491-lda-7" href="../hunch_net-2006/hunch_net-2006-03-27-Gradients_everywhere.html">167 hunch net-2006-03-27-Gradients everywhere</a></p>
<p>8 0.33442006 <a title="491-lda-8" href="../hunch_net-2007/hunch_net-2007-06-24-Interesting_Papers_at_ICML_2007.html">251 hunch net-2007-06-24-Interesting Papers at ICML 2007</a></p>
<p>9 0.33263302 <a title="491-lda-9" href="../hunch_net-2005/hunch_net-2005-03-02-Prior%2C_%26%238220%3BPrior%26%238221%3B_and_Bias.html">34 hunch net-2005-03-02-Prior, &#8220;Prior&#8221; and Bias</a></p>
<p>10 0.31414539 <a title="491-lda-10" href="../hunch_net-2006/hunch_net-2006-04-05-What_is_state%3F.html">169 hunch net-2006-04-05-What is state?</a></p>
<p>11 0.31195408 <a title="491-lda-11" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>12 0.31117782 <a title="491-lda-12" href="../hunch_net-2006/hunch_net-2006-04-06-Bounds_greater_than_1.html">170 hunch net-2006-04-06-Bounds greater than 1</a></p>
<p>13 0.30787829 <a title="491-lda-13" href="../hunch_net-2007/hunch_net-2007-12-20-Cool_and_Interesting_things_at_NIPS%2C_take_three.html">280 hunch net-2007-12-20-Cool and Interesting things at NIPS, take three</a></p>
<p>14 0.30494255 <a title="491-lda-14" href="../hunch_net-2006/hunch_net-2006-05-05-An_ICML_reject.html">177 hunch net-2006-05-05-An ICML reject</a></p>
<p>15 0.30445164 <a title="491-lda-15" href="../hunch_net-2006/hunch_net-2006-03-09-Use_of_Notation.html">162 hunch net-2006-03-09-Use of Notation</a></p>
<p>16 0.3013624 <a title="491-lda-16" href="../hunch_net-2005/hunch_net-2005-06-18-Lower_Bounds_for_Learning_Reductions.html">83 hunch net-2005-06-18-Lower Bounds for Learning Reductions</a></p>
<p>17 0.3009544 <a title="491-lda-17" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>18 0.29788131 <a title="491-lda-18" href="../hunch_net-2005/hunch_net-2005-02-01-NIPS%3A_Online_Bayes.html">8 hunch net-2005-02-01-NIPS: Online Bayes</a></p>
<p>19 0.29738426 <a title="491-lda-19" href="../hunch_net-2009/hunch_net-2009-06-24-Interesting_papers_at_UAICMOLT_2009.html">361 hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</a></p>
<p>20 0.29686925 <a title="491-lda-20" href="../hunch_net-2010/hunch_net-2010-03-15-The_Efficient_Robust_Conditional_Probability_Estimation_Problem.html">391 hunch net-2010-03-15-The Efficient Robust Conditional Probability Estimation Problem</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
