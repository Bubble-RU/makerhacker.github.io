<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>404 hunch net-2010-08-20-The Workshop on Cores, Clusters, and Clouds</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2010" href="../home/hunch_net-2010_home.html">hunch_net-2010</a> <a title="hunch_net-2010-404" href="#">hunch_net-2010-404</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>404 hunch net-2010-08-20-The Workshop on Cores, Clusters, and Clouds</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2010-404-html" href="http://hunch.net/?p=1446">html</a></p><p>Introduction: Alekh,John,Ofer, and I are organizing aworkshopatNIPSthis year on learning in
parallel and distributed environments. The general interest level in parallel
learning seems to be growing rapidly, so I expect quite a bit of attendance.
Please join us if you are parallel-interested.And, if you are working in the
area of parallel learning, please considersubmitting an abstractdue Oct. 17
for presentation at the workshop.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Alekh,John,Ofer, and I are organizing aworkshopatNIPSthis year on learning in parallel and distributed environments. [sent-1, score-1.246]
</p><p>2 The general interest level in parallel learning seems to be growing rapidly, so I expect quite a bit of attendance. [sent-2, score-1.648]
</p><p>3 And, if you are working in the area of parallel learning, please considersubmitting an abstractdue Oct. [sent-4, score-1.247]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('parallel', 0.622), ('please', 0.362), ('join', 0.271), ('organizing', 0.234), ('rapidly', 0.234), ('distributed', 0.219), ('growing', 0.2), ('presentation', 0.191), ('workshop', 0.158), ('level', 0.142), ('area', 0.141), ('expect', 0.131), ('interest', 0.128), ('working', 0.122), ('us', 0.111), ('year', 0.105), ('bit', 0.103), ('quite', 0.094), ('general', 0.093), ('seems', 0.069), ('learning', 0.066)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="404-tfidf-1" href="../hunch_net-2010/hunch_net-2010-08-20-The_Workshop_on_Cores%2C_Clusters%2C_and_Clouds.html">404 hunch net-2010-08-20-The Workshop on Cores, Clusters, and Clouds</a></p>
<p>Introduction: Alekh,John,Ofer, and I are organizing aworkshopatNIPSthis year on learning in
parallel and distributed environments. The general interest level in parallel
learning seems to be growing rapidly, so I expect quite a bit of attendance.
Please join us if you are parallel-interested.And, if you are working in the
area of parallel learning, please considersubmitting an abstractdue Oct. 17
for presentation at the workshop.</p><p>2 0.18155824 <a title="404-tfidf-2" href="../hunch_net-2009/hunch_net-2009-03-18-Parallel_ML_primitives.html">346 hunch net-2009-03-18-Parallel ML primitives</a></p>
<p>Introduction: Previously, we discussedparallel machine learninga bit. As parallel ML is
rather difficult, I'd like to describe my thinking at the moment, and ask for
advice from the rest of the world. This is particularly relevant right now, as
I'm attending a workshop tomorrow on parallel ML.Parallelizing slow algorithms
seems uncompelling. Parallelizing many algorithms also seems uncompelling,
because the effort required to parallelize is substantial. This leaves the
question: Which one fast algorithm is the best to parallelize? What is a
substantially different second?One compellingly fast simple algorithm is
online gradient descent on a linear representation. This is the core of
Leon'ssgdcode andVowpal Wabbit.Antoine Bordesshowed a variant was competitive
in thelarge scale learning challenge. It's also a decades old primitive which
has been reused in many algorithms, and continues to be reused. It also
applies to onlinelearningrather than just onlineoptimization, implying the
algorithm can be us</p><p>3 0.15513843 <a title="404-tfidf-3" href="../hunch_net-2007/hunch_net-2007-01-26-Parallel_Machine_Learning_Problems.html">229 hunch net-2007-01-26-Parallel Machine Learning Problems</a></p>
<p>Introduction: Parallel machine learning is a subject rarely addressed at machine learning
conferences. Nevertheless, it seems likely to increase in importance
because:Data set sizes appear to be growing substantially faster than
computation. Essentially, this happens because more and more sensors of
various sorts are being hooked up to the internet.Serial speedups of
processors seem are relatively stalled. The new trend is to make processors
more powerful by making themmulticore.BothAMDandIntelare making dual core
designs standard, with plans for more parallelism in the future.IBM'sCell
processorhas (essentially) 9 cores.Modern graphics chips can have an order of
magnitude more separate execution units.The meaning of 'core' varies a bit
from processor to processor, but the overall trend seems quite clear.So, how
do we parallelize machine learning algorithms?The simplest and most common
technique is to simply run the same learning algorithm with different
parameters on different processors. Cluster m</p><p>4 0.14921388 <a title="404-tfidf-4" href="../hunch_net-2011/hunch_net-2011-08-20-The_Large_Scale_Learning_Survey_Tutorial.html">442 hunch net-2011-08-20-The Large Scale Learning Survey Tutorial</a></p>
<p>Introduction: Ron Bekkermaninitiated an effort to create anedited book on parallel machine
learningthatMishaand I have been helping with. The breadth of efforts to
parallelize machine learning surprised me: I was only aware of a small
fraction initially.This put us in a unique position, with knowledge of a wide
array of different efforts, so it is natural to put together asurvey tutorial
on the subject of parallel learningforKDD, tomorrow. This tutorial
isnotlimited to the book itself however, as several interesting new algorithms
have come out since we started inviting chapters.This tutorial should interest
anyone trying to use machine learning on significant quantities of data,
anyone interested in developing algorithms for such, and of course who has
bragging rights to the fastest learning algorithm on planet earth(Also note
the Modeling with Hadoop tutorial just before ours which deals with one way of
trying to speed up learning algorithms. We have almost no overlap.)</p><p>5 0.13669243 <a title="404-tfidf-5" href="../hunch_net-2008/hunch_net-2008-04-30-Concerns_about_the_Large_Scale_Learning_Challenge.html">300 hunch net-2008-04-30-Concerns about the Large Scale Learning Challenge</a></p>
<p>Introduction: Thelarge scale learning challengefor ICML interests me a great deal, although
I have concerns about the way it is structured.From theinstructions page,
several issues come up:Large DefinitionMy personal definition of dataset size
is:smallA dataset is small if a human could look at the dataset and plausibly
find a good solution.mediumA dataset is mediumsize if it fits in the RAM of a
reasonably priced computer.largeA large dataset does not fit in the RAM of a
reasonably priced computer.By this definition, all of the datasets are medium
sized. This might sound like a pissing match over dataset size, but I believe
it is more than that.The fundamental reason for these definitions is that they
correspond to transitions in the sorts of approaches which are feasible. From
small to medium, the ability to use a human as the learning algorithm
degrades. From medium to large, it becomes essential to have learning
algorithms that don't require random access to examples.No Loading TimeThe
medium sc</p><p>6 0.13109206 <a title="404-tfidf-6" href="../hunch_net-2011/hunch_net-2011-12-13-Vowpal_Wabbit_version_6.1_%26%23038%3B_the_NIPS_tutorial.html">451 hunch net-2011-12-13-Vowpal Wabbit version 6.1 &#038; the NIPS tutorial</a></p>
<p>7 0.12910064 <a title="404-tfidf-7" href="../hunch_net-2009/hunch_net-2009-08-03-Carbon_in_Computer_Science_Research.html">366 hunch net-2009-08-03-Carbon in Computer Science Research</a></p>
<p>8 0.12699926 <a title="404-tfidf-8" href="../hunch_net-2013/hunch_net-2013-08-31-Extreme_Classification_workshop_at_NIPS.html">488 hunch net-2013-08-31-Extreme Classification workshop at NIPS</a></p>
<p>9 0.12458058 <a title="404-tfidf-9" href="../hunch_net-2009/hunch_net-2009-12-07-Vowpal_Wabbit_version_4.0%2C_and_a_NIPS_heresy.html">381 hunch net-2009-12-07-Vowpal Wabbit version 4.0, and a NIPS heresy</a></p>
<p>10 0.11769655 <a title="404-tfidf-10" href="../hunch_net-2007/hunch_net-2007-02-22-Create_Your_Own_ICML_Workshop.html">234 hunch net-2007-02-22-Create Your Own ICML Workshop</a></p>
<p>11 0.11475326 <a title="404-tfidf-11" href="../hunch_net-2011/hunch_net-2011-02-25-Yahoo%21_Machine_Learning_grant_due_March_11.html">425 hunch net-2011-02-25-Yahoo! Machine Learning grant due March 11</a></p>
<p>12 0.11160827 <a title="404-tfidf-12" href="../hunch_net-2007/hunch_net-2007-10-14-NIPS_workshp%3A_Learning_Problem_Design.html">265 hunch net-2007-10-14-NIPS workshp: Learning Problem Design</a></p>
<p>13 0.10439514 <a title="404-tfidf-13" href="../hunch_net-2009/hunch_net-2009-05-24-2009_ICML_discussion_site.html">356 hunch net-2009-05-24-2009 ICML discussion site</a></p>
<p>14 0.098859228 <a title="404-tfidf-14" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>15 0.098376453 <a title="404-tfidf-15" href="../hunch_net-2005/hunch_net-2005-03-24-The_Role_of_Workshops.html">46 hunch net-2005-03-24-The Role of Workshops</a></p>
<p>16 0.084587708 <a title="404-tfidf-16" href="../hunch_net-2006/hunch_net-2006-07-12-Who_is_having_visa_problems_reaching_US_conferences%3F.html">195 hunch net-2006-07-12-Who is having visa problems reaching US conferences?</a></p>
<p>17 0.084580988 <a title="404-tfidf-17" href="../hunch_net-2005/hunch_net-2005-09-19-NIPS_Workshops.html">113 hunch net-2005-09-19-NIPS Workshops</a></p>
<p>18 0.083430558 <a title="404-tfidf-18" href="../hunch_net-2008/hunch_net-2008-06-30-ICML_has_a_comment_system.html">305 hunch net-2008-06-30-ICML has a comment system</a></p>
<p>19 0.082721792 <a title="404-tfidf-19" href="../hunch_net-2008/hunch_net-2008-07-04-More_Presentation_Preparation.html">307 hunch net-2008-07-04-More Presentation Preparation</a></p>
<p>20 0.076948926 <a title="404-tfidf-20" href="../hunch_net-2012/hunch_net-2012-04-20-Both_new%3A_STOC_workshops_and_NEML.html">462 hunch net-2012-04-20-Both new: STOC workshops and NEML</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.113), (1, 0.053), (2, 0.12), (3, 0.14), (4, -0.038), (5, 0.078), (6, 0.064), (7, 0.021), (8, 0.039), (9, 0.094), (10, -0.02), (11, 0.152), (12, 0.005), (13, -0.057), (14, -0.019), (15, -0.068), (16, 0.045), (17, -0.088), (18, 0.066), (19, 0.121), (20, -0.098), (21, -0.01), (22, -0.007), (23, 0.144), (24, -0.119), (25, -0.043), (26, -0.044), (27, 0.086), (28, 0.116), (29, -0.009), (30, -0.045), (31, 0.013), (32, 0.024), (33, 0.083), (34, 0.114), (35, 0.007), (36, -0.048), (37, 0.051), (38, 0.014), (39, 0.091), (40, 0.063), (41, -0.007), (42, 0.107), (43, -0.044), (44, 0.06), (45, -0.055), (46, -0.077), (47, -0.007), (48, -0.062), (49, -0.034)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95915478 <a title="404-lsi-1" href="../hunch_net-2010/hunch_net-2010-08-20-The_Workshop_on_Cores%2C_Clusters%2C_and_Clouds.html">404 hunch net-2010-08-20-The Workshop on Cores, Clusters, and Clouds</a></p>
<p>Introduction: Alekh,John,Ofer, and I are organizing aworkshopatNIPSthis year on learning in
parallel and distributed environments. The general interest level in parallel
learning seems to be growing rapidly, so I expect quite a bit of attendance.
Please join us if you are parallel-interested.And, if you are working in the
area of parallel learning, please considersubmitting an abstractdue Oct. 17
for presentation at the workshop.</p><p>2 0.60873377 <a title="404-lsi-2" href="../hunch_net-2009/hunch_net-2009-12-07-Vowpal_Wabbit_version_4.0%2C_and_a_NIPS_heresy.html">381 hunch net-2009-12-07-Vowpal Wabbit version 4.0, and a NIPS heresy</a></p>
<p>Introduction: I'm releasingversion 4.0(tarball) ofVowpal Wabbit. The biggest change (by far)
in this release is experimental support for cluster parallelism, with notable
help fromDaniel Hsu.I also took advantage of the major version number to
introduce some incompatible changes, including switching tomurmurhash 2, and
other alterations to cachefiles. You'll need to delete and regenerate them. In
addition, the precise specification for a "tag" (i.e. string that can be used
to identify an example) changed--you can't have a space between the tag and
the '|' at the beginning of the feature namespace.And, of course, we made it
faster.For the future, I put up mytodo listoutlining the major future
improvements I want to see in the code. I'm planning to discuss the current
mechanism and results of the cluster parallel implementation at thelarge scale
machine learning workshopatNIPSlater this week. Several people have asked me
to do a tutorial/walkthrough of VW, which is arranged for friday 2pm in the
works</p><p>3 0.59976274 <a title="404-lsi-3" href="../hunch_net-2011/hunch_net-2011-08-20-The_Large_Scale_Learning_Survey_Tutorial.html">442 hunch net-2011-08-20-The Large Scale Learning Survey Tutorial</a></p>
<p>Introduction: Ron Bekkermaninitiated an effort to create anedited book on parallel machine
learningthatMishaand I have been helping with. The breadth of efforts to
parallelize machine learning surprised me: I was only aware of a small
fraction initially.This put us in a unique position, with knowledge of a wide
array of different efforts, so it is natural to put together asurvey tutorial
on the subject of parallel learningforKDD, tomorrow. This tutorial
isnotlimited to the book itself however, as several interesting new algorithms
have come out since we started inviting chapters.This tutorial should interest
anyone trying to use machine learning on significant quantities of data,
anyone interested in developing algorithms for such, and of course who has
bragging rights to the fastest learning algorithm on planet earth(Also note
the Modeling with Hadoop tutorial just before ours which deals with one way of
trying to speed up learning algorithms. We have almost no overlap.)</p><p>4 0.53207189 <a title="404-lsi-4" href="../hunch_net-2011/hunch_net-2011-12-13-Vowpal_Wabbit_version_6.1_%26%23038%3B_the_NIPS_tutorial.html">451 hunch net-2011-12-13-Vowpal Wabbit version 6.1 &#038; the NIPS tutorial</a></p>
<p>Introduction: I just madeversion 6.1ofVowpal Wabbit. Relative to6.0, there are few new
features, but many refinements.The cluster parallel learning code better
supports multiple simultaneous runs, and other forms of parallelism have been
mostly removed. This incidentally significantly simplifies the learning
core.The online learning algorithms are more general, with support for l1(via
a truncated gradient variant) and l2regularization, and a generalized form of
variable metric learning.There is a solid persistent server mode which can
train online, as well as serve answers to many simultaneous queries, either in
text or binary.This should be a very good release if you are just getting
started, as we've made it compile more automatically out of the box, have
several newexamplesand updated documentation.Aspertradition, we're planning to
do a tutorial at NIPS during the break at theparallel learning workshopat 2pm
Spanish time Friday. I'll cover the basics, leaving the fun stuff for
others.Mirowill cov</p><p>5 0.48336416 <a title="404-lsi-5" href="../hunch_net-2006/hunch_net-2006-07-25-Upcoming_conference.html">198 hunch net-2006-07-25-Upcoming conference</a></p>
<p>Introduction: The Workshop for Women in Machine Learning will be held in San Diego on
October 4, 2006.For details see the workshop
website:http://www.seas.upenn.edu/~wiml/</p><p>6 0.47772285 <a title="404-lsi-6" href="../hunch_net-2009/hunch_net-2009-03-18-Parallel_ML_primitives.html">346 hunch net-2009-03-18-Parallel ML primitives</a></p>
<p>7 0.45852643 <a title="404-lsi-7" href="../hunch_net-2005/hunch_net-2005-10-19-Workshop%3A_Atomic_Learning.html">124 hunch net-2005-10-19-Workshop: Atomic Learning</a></p>
<p>8 0.43648666 <a title="404-lsi-8" href="../hunch_net-2005/hunch_net-2005-09-19-NIPS_Workshops.html">113 hunch net-2005-09-19-NIPS Workshops</a></p>
<p>9 0.42598432 <a title="404-lsi-9" href="../hunch_net-2007/hunch_net-2007-01-26-Parallel_Machine_Learning_Problems.html">229 hunch net-2007-01-26-Parallel Machine Learning Problems</a></p>
<p>10 0.42500955 <a title="404-lsi-10" href="../hunch_net-2008/hunch_net-2008-10-01-NIPS_2008_workshop_on_%26%238216%3BLearning_over_Empirical_Hypothesis_Spaces%26%238217%3B.html">319 hunch net-2008-10-01-NIPS 2008 workshop on &#8216;Learning over Empirical Hypothesis Spaces&#8217;</a></p>
<p>11 0.42228225 <a title="404-lsi-11" href="../hunch_net-2007/hunch_net-2007-02-22-Create_Your_Own_ICML_Workshop.html">234 hunch net-2007-02-22-Create Your Own ICML Workshop</a></p>
<p>12 0.41973728 <a title="404-lsi-12" href="../hunch_net-2013/hunch_net-2013-12-01-NIPS_tutorials_and_Vowpal_Wabbit_7.4.html">492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</a></p>
<p>13 0.41654724 <a title="404-lsi-13" href="../hunch_net-2009/hunch_net-2009-08-03-Carbon_in_Computer_Science_Research.html">366 hunch net-2009-08-03-Carbon in Computer Science Research</a></p>
<p>14 0.40786353 <a title="404-lsi-14" href="../hunch_net-2012/hunch_net-2012-02-20-Berkeley_Streaming_Data_Workshop.html">455 hunch net-2012-02-20-Berkeley Streaming Data Workshop</a></p>
<p>15 0.40302071 <a title="404-lsi-15" href="../hunch_net-2012/hunch_net-2012-07-17-MUCMD_and_BayLearn.html">470 hunch net-2012-07-17-MUCMD and BayLearn</a></p>
<p>16 0.40179464 <a title="404-lsi-16" href="../hunch_net-2008/hunch_net-2008-04-30-Concerns_about_the_Large_Scale_Learning_Challenge.html">300 hunch net-2008-04-30-Concerns about the Large Scale Learning Challenge</a></p>
<p>17 0.39767161 <a title="404-lsi-17" href="../hunch_net-2012/hunch_net-2012-09-29-Vowpal_Wabbit%2C_version_7.0.html">473 hunch net-2012-09-29-Vowpal Wabbit, version 7.0</a></p>
<p>18 0.39321607 <a title="404-lsi-18" href="../hunch_net-2005/hunch_net-2005-04-08-Fast_SVMs.html">54 hunch net-2005-04-08-Fast SVMs</a></p>
<p>19 0.38076729 <a title="404-lsi-19" href="../hunch_net-2007/hunch_net-2007-10-14-NIPS_workshp%3A_Learning_Problem_Design.html">265 hunch net-2007-10-14-NIPS workshp: Learning Problem Design</a></p>
<p>20 0.35997933 <a title="404-lsi-20" href="../hunch_net-2009/hunch_net-2009-05-17-Server_Update.html">354 hunch net-2009-05-17-Server Update</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(42, 0.196), (63, 0.469), (74, 0.133)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.89261693 <a title="404-lda-1" href="../hunch_net-2013/hunch_net-2013-07-10-Thoughts_on_Artificial_Intelligence.html">486 hunch net-2013-07-10-Thoughts on Artificial Intelligence</a></p>
<p>Introduction: David McAllesterstarts a blog.</p><p>same-blog 2 0.87309188 <a title="404-lda-2" href="../hunch_net-2010/hunch_net-2010-08-20-The_Workshop_on_Cores%2C_Clusters%2C_and_Clouds.html">404 hunch net-2010-08-20-The Workshop on Cores, Clusters, and Clouds</a></p>
<p>Introduction: Alekh,John,Ofer, and I are organizing aworkshopatNIPSthis year on learning in
parallel and distributed environments. The general interest level in parallel
learning seems to be growing rapidly, so I expect quite a bit of attendance.
Please join us if you are parallel-interested.And, if you are working in the
area of parallel learning, please considersubmitting an abstractdue Oct. 17
for presentation at the workshop.</p><p>3 0.85386527 <a title="404-lda-3" href="../hunch_net-2008/hunch_net-2008-04-21-The_Science_2.0_article.html">296 hunch net-2008-04-21-The Science 2.0 article</a></p>
<p>Introduction: I found the article aboutscience using modern tools interesting, especially
the part about 'blogophobia', which in my experience is often a substantial
issue: many potential guest posters aren't quite ready, because of the fear of
a permanent public mistake, because it is particularly hard to write about the
unknown (the essence of research), and because the system for public credit
doesn't yet really handle blog posts.So far, science has been relatively
resistant to discussing research on blogs. Some things need to change to get
there. Public tolerance of the occasional mistake is essential, as is a
willingness to cite (and credit) blogs as freely as papers.I've often run into
another reason for holding back myself: I don't want to overtalk my own
research. Nevertheless, I'm slowly changing to the opinion that I'm holding
back too much: the real power of a blog in research is that it can be used to
confer with many people, and that just makes research work better.</p><p>4 0.70869029 <a title="404-lda-4" href="../hunch_net-2009/hunch_net-2009-08-26-Another_10-year_paper_in_Machine_Learning.html">368 hunch net-2009-08-26-Another 10-year paper in Machine Learning</a></p>
<p>Introduction: When I was thinking about the best "10 year paper" forICML, I also took a look
at a few other conferences. Here is one from 10 years ago that interested
me:David McAllesterPAC-Bayesian Model Averaging,COLT1999.2001 Journal
Draft.Prior to this paper, the only mechanism known for controlling or
estimating the necessary sample complexity for learning over continuously
parameterized predictors was VC theory and variants, all of which suffered
from a basic problem: they were incredibly pessimistic in practice. This meant
that only very gross guidance could be provided for learning algorithm design.
The PAC-Bayes bound provided an alternative approach to sample complexity
bounds which was radically tighter, quantitatively. It also imported and
explained many of the motivations for Bayesian learning in a way that learning
theory and perhaps optimization people might appreciate. Since this paper came
out, there have been a number of moderately successful attempts to drive
algorithms directly b</p><p>5 0.64327598 <a title="404-lda-5" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>Introduction: The papers which interested me most atICMLandCOLT2010 were:Thomas
Walsh,Kaushik Subramanian,Michael LittmanandCarlos DiukGeneralizing
Apprenticeship Learning across Hypothesis Classes. This paper formalizes and
provides algorithms with guarantees for mixed-mode apprenticeship and
traditional reinforcement learning algorithms, allowing RL algorithms that
perform better than for either setting alone.István SzitaandCsaba
SzepesváriModel-based reinforcement learning with nearly tight exploration
complexity bounds. This paper andanotherrepresent the frontier of best-known
algorithm for Reinforcement Learning in a Markov Decision Process.James
MartensDeep learning via Hessian-free optimization. About a new not-quite-
online second order gradient algorithm for learning deep functional
structures. Potentially this is very powerful because while people have often
talked about end-to-end learning, it has rarely worked in practice.Chrisoph
Sawade,Niels Landwehr,Steffen Bickel. andTobias SchefferA</p><p>6 0.64201772 <a title="404-lda-6" href="../hunch_net-2008/hunch_net-2008-01-06-Research_Political_Issues.html">282 hunch net-2008-01-06-Research Political Issues</a></p>
<p>7 0.49866885 <a title="404-lda-7" href="../hunch_net-2005/hunch_net-2005-02-15-ESPgame_and_image_labeling.html">20 hunch net-2005-02-15-ESPgame and image labeling</a></p>
<p>8 0.47732762 <a title="404-lda-8" href="../hunch_net-2007/hunch_net-2007-01-02-Retrospective.html">225 hunch net-2007-01-02-Retrospective</a></p>
<p>9 0.46065393 <a title="404-lda-9" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>10 0.44967896 <a title="404-lda-10" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>11 0.44178694 <a title="404-lda-11" href="../hunch_net-2007/hunch_net-2007-02-16-The_Forgetting.html">233 hunch net-2007-02-16-The Forgetting</a></p>
<p>12 0.4400664 <a title="404-lda-12" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>13 0.43919533 <a title="404-lda-13" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>14 0.43781659 <a title="404-lda-14" href="../hunch_net-2008/hunch_net-2008-11-10-ICML_Reviewing_Criteria.html">325 hunch net-2008-11-10-ICML Reviewing Criteria</a></p>
<p>15 0.43208694 <a title="404-lda-15" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>16 0.43117163 <a title="404-lda-16" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>17 0.43064126 <a title="404-lda-17" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>18 0.43026116 <a title="404-lda-18" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>19 0.4299542 <a title="404-lda-19" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>20 0.42868859 <a title="404-lda-20" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
