<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>400 hunch net-2010-06-13-The Good News on Exploration and Learning</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2010" href="../home/hunch_net-2010_home.html">hunch_net-2010</a> <a title="hunch_net-2010-400" href="#">hunch_net-2010-400</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>400 hunch net-2010-06-13-The Good News on Exploration and Learning</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2010-400-html" href="http://hunch.net/?p=1389">html</a></p><p>Introduction: Consider the contextual bandit setting where, repeatedly:
  
 A context  x  is observed. 
 An action  a  is taken given the context  x .  
 A reward  r  is observed, dependent on  x  and  a . 
  
Where the goal of a learning agent is to find a policy for step 2 achieving a large expected reward.  
 
This setting is of obvious importance, because in the real world we typically make decisions based on some set of information and then get feedback only about the single action taken.  It also fundamentally differs from supervised learning settings because knowing the value of one action is not equivalent to knowing the value of all actions.
 
A decade ago the best machine learning techniques for this setting where implausibly inefficient.   Dean Foster  once told me he thought the area was a research sinkhole with little progress to be expected.  Now we are on the verge of being able to routinely attack these problems, in almost exactly the same sense that we routinely attack bread and but</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Consider the contextual bandit setting where, repeatedly:     A context  x  is observed. [sent-1, score-0.678]
</p><p>2 Where the goal of a learning agent is to find a policy for step 2 achieving a large expected reward. [sent-4, score-0.074]
</p><p>3 This setting is of obvious importance, because in the real world we typically make decisions based on some set of information and then get feedback only about the single action taken. [sent-5, score-0.343]
</p><p>4 It also fundamentally differs from supervised learning settings because knowing the value of one action is not equivalent to knowing the value of all actions. [sent-6, score-1.009]
</p><p>5 A decade ago the best machine learning techniques for this setting where implausibly inefficient. [sent-7, score-0.456]
</p><p>6 Dean Foster  once told me he thought the area was a research sinkhole with little progress to be expected. [sent-8, score-0.078]
</p><p>7 Now we are on the verge of being able to routinely attack these problems, in almost exactly the same sense that we routinely attack bread and butter supervised learning problems. [sent-9, score-1.107]
</p><p>8 Just as for supervised learning, we know how to create and reuse datasets, how to benchmark algorithms, how to reuse existing supervised learning algorithms in this setting, and how to achieve optimal rates of learning quantitatively similar to supervised learning. [sent-10, score-1.494]
</p><p>9 This is also one of the times when understanding the basic theory can make a huge difference in your success. [sent-11, score-0.261]
</p><p>10 There are many wrong ways to attack contextual bandit problems or prepare datasets, and taking a wrong turn can easily mean the difference between failure and success. [sent-12, score-1.318]
</p><p>11 Understanding how contextual bandit problems differ from basic supervised learning problems is critical to routine success here. [sent-13, score-1.127]
</p><p>12 All of the above is not meant to claim that everything is done research-wise here so weâ&euro;&trade;ll try to outline where the current boundary of research lies as best we can. [sent-14, score-0.292]
</p><p>13 Given the above,  Alina  and I decided to  prepare a tutorial  to be given at  Yahoo! [sent-16, score-0.348]
</p><p>14 The subjects we plan to cover are essentially the keys to the kingdom of solving shallow interactive learning problems. [sent-21, score-0.194]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('supervised', 0.315), ('attack', 0.222), ('contextual', 0.211), ('bandit', 0.207), ('prepare', 0.193), ('action', 0.188), ('routinely', 0.174), ('reuse', 0.174), ('setting', 0.155), ('knowing', 0.141), ('techniques', 0.13), ('surely', 0.13), ('problems', 0.114), ('datasets', 0.107), ('difference', 0.106), ('context', 0.105), ('supply', 0.104), ('trip', 0.104), ('outline', 0.104), ('quantitatively', 0.104), ('keys', 0.097), ('implausibly', 0.097), ('lies', 0.097), ('benchmark', 0.097), ('shallow', 0.097), ('wrong', 0.092), ('india', 0.091), ('boundary', 0.091), ('medical', 0.091), ('foster', 0.087), ('given', 0.087), ('reliable', 0.084), ('dean', 0.084), ('routine', 0.084), ('advertising', 0.084), ('basic', 0.082), ('applications', 0.082), ('easily', 0.081), ('methodology', 0.081), ('labs', 0.081), ('join', 0.081), ('told', 0.078), ('differs', 0.076), ('value', 0.074), ('decade', 0.074), ('shifting', 0.074), ('agent', 0.074), ('understanding', 0.073), ('demand', 0.072), ('decided', 0.068)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="400-tfidf-1" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>Introduction: Consider the contextual bandit setting where, repeatedly:
  
 A context  x  is observed. 
 An action  a  is taken given the context  x .  
 A reward  r  is observed, dependent on  x  and  a . 
  
Where the goal of a learning agent is to find a policy for step 2 achieving a large expected reward.  
 
This setting is of obvious importance, because in the real world we typically make decisions based on some set of information and then get feedback only about the single action taken.  It also fundamentally differs from supervised learning settings because knowing the value of one action is not equivalent to knowing the value of all actions.
 
A decade ago the best machine learning techniques for this setting where implausibly inefficient.   Dean Foster  once told me he thought the area was a research sinkhole with little progress to be expected.  Now we are on the verge of being able to routinely attack these problems, in almost exactly the same sense that we routinely attack bread and but</p><p>2 0.25203091 <a title="400-tfidf-2" href="../hunch_net-2007/hunch_net-2007-10-24-Contextual_Bandits.html">269 hunch net-2007-10-24-Contextual Bandits</a></p>
<p>Introduction: One of the fundamental underpinnings of the internet is advertising based content.  This has become much more effective due to targeted advertising where ads are specifically matched to interests.  Everyone is familiar with this, because everyone uses search engines and all search engines try to make money this way.
 
The problem of matching ads to interests is a natural machine learning problem in some ways since there is much information in who clicks on what.  A fundamental problem with this information is that it is not supervised—in particular a click-or-not on one ad doesn’t generally tell you if a different ad would have been clicked on.  This implies we have a fundamental exploration problem.
 
A standard mathematical setting for this situation is “ k -Armed Bandits”, often with various relevant embellishments.  The  k -Armed Bandit setting works on a round-by-round basis.  On each round:
  
 A policy chooses arm  a  from  1  of  k  arms (i.e. 1 of k ads). 
 The world reveals t</p><p>3 0.23362526 <a title="400-tfidf-3" href="../hunch_net-2008/hunch_net-2008-03-23-Interactive_Machine_Learning.html">293 hunch net-2008-03-23-Interactive Machine Learning</a></p>
<p>Introduction: A new direction of research seems to be arising in machine learning: Interactive Machine Learning.  This isn’t a familiar term, although it does include some familiar subjects.
 
 What is Interactive Machine Learning?   The fundamental requirement is (a) learning algorithms which interact with the world and (b) learn.  
 
For our purposes, let’s define learning as efficiently competing with a large set of possible predictors.  Examples include:
  
 Online learning against an adversary ( Avrim’s Notes ).  The interaction is almost trivial: the learning algorithm makes a prediction and then receives feedback. The learning is choosing based upon the advice of many experts. 
   Active Learning  .  In active learning, the interaction is choosing which examples to label, and the learning is choosing from amongst a large set of hypotheses. 
   Contextual Bandits  .  The interaction is choosing one of several actions and learning only the value of the chosen action (weaker than active learning</p><p>4 0.19247508 <a title="400-tfidf-4" href="../hunch_net-2008/hunch_net-2008-09-12-How_do_we_get_weak_action_dependence_for_learning_with_partial_observations%3F.html">317 hunch net-2008-09-12-How do we get weak action dependence for learning with partial observations?</a></p>
<p>Introduction: This post is about contextual bandit problems where, repeatedly:
  
 The world chooses features  x  and rewards for each action  r 1 ,…,r k   then announces the features  x  (but not the rewards). 
 A policy chooses an action  a . 
 The world announces the reward  r a   
  
The goal in these situations is to learn a policy which maximizes  r a   in expectation efficiently.  I’m thinking about all situations which fit the above setting, whether they are drawn IID or adversarially from round to round and whether they involve past logged data or rapidly learning via interaction.
 
One common drawback of all algorithms for solving this setting, is that they have a poor dependence on the number of actions.  For example if  k  is the number of actions,  EXP4 (page 66)  has a dependence on  k 0.5  ,  epoch-greedy  (and the simpler epsilon greedy) have a dependence on  k 1/3  , and the  offset tree  has a dependence on  k-1 .  These results aren’t directly comparable because different things a</p><p>5 0.18716422 <a title="400-tfidf-5" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>Introduction: One thing which is clear on a little reflection is that there exists a single master learning problem capable of encoding essentially all learning problems.  This problem is of course a very general sort of reinforcement learning where the world interacts with an agent as:
  
 The world announces an observation  x . 
 The agent makes a choice  a . 
 The world announces a reward  r . 
  
The goal here is to maximize the sum of the rewards over the time of the agent.  No particular structure relating  x  to  a  or  a  to  r  is implied by this setting so we do not know effective general algorithms for the agent.  It’s very easy to prove lower bounds showing that an agent cannot hope to succeed here—just consider the case where actions are unrelated to rewards.  Nevertheless, there is a real sense in which essentially all forms of life are agents operating in this setting, somehow succeeding.  The gap between these observations drives research—How can we find tractable specializations of</p><p>6 0.16223332 <a title="400-tfidf-6" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>7 0.15823349 <a title="400-tfidf-7" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<p>8 0.133288 <a title="400-tfidf-8" href="../hunch_net-2008/hunch_net-2008-04-27-Watchword%3A_Supervised_Learning.html">299 hunch net-2008-04-27-Watchword: Supervised Learning</a></p>
<p>9 0.1213557 <a title="400-tfidf-9" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>10 0.11953644 <a title="400-tfidf-10" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>11 0.11855456 <a title="400-tfidf-11" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>12 0.11037483 <a title="400-tfidf-12" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<p>13 0.10977628 <a title="400-tfidf-13" href="../hunch_net-2006/hunch_net-2006-01-13-Benchmarks_for_RL.html">148 hunch net-2006-01-13-Benchmarks for RL</a></p>
<p>14 0.10683425 <a title="400-tfidf-14" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>15 0.10574142 <a title="400-tfidf-15" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>16 0.10255069 <a title="400-tfidf-16" href="../hunch_net-2006/hunch_net-2006-02-11-Yahoo%26%238217%3Bs_Learning_Problems..html">156 hunch net-2006-02-11-Yahoo&#8217;s Learning Problems.</a></p>
<p>17 0.095846951 <a title="400-tfidf-17" href="../hunch_net-2011/hunch_net-2011-04-11-The_Heritage_Health_Prize.html">430 hunch net-2011-04-11-The Heritage Health Prize</a></p>
<p>18 0.093589328 <a title="400-tfidf-18" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>19 0.09124691 <a title="400-tfidf-19" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>20 0.091190197 <a title="400-tfidf-20" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.24), (1, 0.044), (2, -0.067), (3, 0.016), (4, 0.062), (5, -0.071), (6, 0.0), (7, 0.015), (8, -0.042), (9, 0.088), (10, 0.149), (11, 0.056), (12, 0.022), (13, 0.164), (14, -0.173), (15, 0.128), (16, -0.008), (17, -0.068), (18, 0.002), (19, 0.079), (20, -0.018), (21, -0.066), (22, 0.058), (23, -0.022), (24, -0.097), (25, 0.097), (26, -0.036), (27, -0.044), (28, -0.057), (29, 0.058), (30, -0.033), (31, -0.06), (32, 0.082), (33, 0.054), (34, 0.058), (35, -0.04), (36, 0.006), (37, -0.027), (38, -0.06), (39, 0.052), (40, -0.037), (41, -0.026), (42, -0.058), (43, 0.051), (44, -0.042), (45, 0.052), (46, 0.033), (47, -0.05), (48, 0.063), (49, 0.059)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9590745 <a title="400-lsi-1" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>Introduction: Consider the contextual bandit setting where, repeatedly:
  
 A context  x  is observed. 
 An action  a  is taken given the context  x .  
 A reward  r  is observed, dependent on  x  and  a . 
  
Where the goal of a learning agent is to find a policy for step 2 achieving a large expected reward.  
 
This setting is of obvious importance, because in the real world we typically make decisions based on some set of information and then get feedback only about the single action taken.  It also fundamentally differs from supervised learning settings because knowing the value of one action is not equivalent to knowing the value of all actions.
 
A decade ago the best machine learning techniques for this setting where implausibly inefficient.   Dean Foster  once told me he thought the area was a research sinkhole with little progress to be expected.  Now we are on the verge of being able to routinely attack these problems, in almost exactly the same sense that we routinely attack bread and but</p><p>2 0.85012186 <a title="400-lsi-2" href="../hunch_net-2007/hunch_net-2007-10-24-Contextual_Bandits.html">269 hunch net-2007-10-24-Contextual Bandits</a></p>
<p>Introduction: One of the fundamental underpinnings of the internet is advertising based content.  This has become much more effective due to targeted advertising where ads are specifically matched to interests.  Everyone is familiar with this, because everyone uses search engines and all search engines try to make money this way.
 
The problem of matching ads to interests is a natural machine learning problem in some ways since there is much information in who clicks on what.  A fundamental problem with this information is that it is not supervised—in particular a click-or-not on one ad doesn’t generally tell you if a different ad would have been clicked on.  This implies we have a fundamental exploration problem.
 
A standard mathematical setting for this situation is “ k -Armed Bandits”, often with various relevant embellishments.  The  k -Armed Bandit setting works on a round-by-round basis.  On each round:
  
 A policy chooses arm  a  from  1  of  k  arms (i.e. 1 of k ads). 
 The world reveals t</p><p>3 0.82080972 <a title="400-lsi-3" href="../hunch_net-2008/hunch_net-2008-09-12-How_do_we_get_weak_action_dependence_for_learning_with_partial_observations%3F.html">317 hunch net-2008-09-12-How do we get weak action dependence for learning with partial observations?</a></p>
<p>Introduction: This post is about contextual bandit problems where, repeatedly:
  
 The world chooses features  x  and rewards for each action  r 1 ,…,r k   then announces the features  x  (but not the rewards). 
 A policy chooses an action  a . 
 The world announces the reward  r a   
  
The goal in these situations is to learn a policy which maximizes  r a   in expectation efficiently.  I’m thinking about all situations which fit the above setting, whether they are drawn IID or adversarially from round to round and whether they involve past logged data or rapidly learning via interaction.
 
One common drawback of all algorithms for solving this setting, is that they have a poor dependence on the number of actions.  For example if  k  is the number of actions,  EXP4 (page 66)  has a dependence on  k 0.5  ,  epoch-greedy  (and the simpler epsilon greedy) have a dependence on  k 1/3  , and the  offset tree  has a dependence on  k-1 .  These results aren’t directly comparable because different things a</p><p>4 0.74806368 <a title="400-lsi-4" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>Introduction: One thing which is clear on a little reflection is that there exists a single master learning problem capable of encoding essentially all learning problems.  This problem is of course a very general sort of reinforcement learning where the world interacts with an agent as:
  
 The world announces an observation  x . 
 The agent makes a choice  a . 
 The world announces a reward  r . 
  
The goal here is to maximize the sum of the rewards over the time of the agent.  No particular structure relating  x  to  a  or  a  to  r  is implied by this setting so we do not know effective general algorithms for the agent.  It’s very easy to prove lower bounds showing that an agent cannot hope to succeed here—just consider the case where actions are unrelated to rewards.  Nevertheless, there is a real sense in which essentially all forms of life are agents operating in this setting, somehow succeeding.  The gap between these observations drives research—How can we find tractable specializations of</p><p>5 0.71652943 <a title="400-lsi-5" href="../hunch_net-2008/hunch_net-2008-03-23-Interactive_Machine_Learning.html">293 hunch net-2008-03-23-Interactive Machine Learning</a></p>
<p>Introduction: A new direction of research seems to be arising in machine learning: Interactive Machine Learning.  This isn’t a familiar term, although it does include some familiar subjects.
 
 What is Interactive Machine Learning?   The fundamental requirement is (a) learning algorithms which interact with the world and (b) learn.  
 
For our purposes, let’s define learning as efficiently competing with a large set of possible predictors.  Examples include:
  
 Online learning against an adversary ( Avrim’s Notes ).  The interaction is almost trivial: the learning algorithm makes a prediction and then receives feedback. The learning is choosing based upon the advice of many experts. 
   Active Learning  .  In active learning, the interaction is choosing which examples to label, and the learning is choosing from amongst a large set of hypotheses. 
   Contextual Bandits  .  The interaction is choosing one of several actions and learning only the value of the chosen action (weaker than active learning</p><p>6 0.63154513 <a title="400-lsi-6" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<p>7 0.58136487 <a title="400-lsi-7" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>8 0.57479692 <a title="400-lsi-8" href="../hunch_net-2008/hunch_net-2008-04-27-Watchword%3A_Supervised_Learning.html">299 hunch net-2008-04-27-Watchword: Supervised Learning</a></p>
<p>9 0.56288445 <a title="400-lsi-9" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>10 0.56213897 <a title="400-lsi-10" href="../hunch_net-2010/hunch_net-2010-03-26-A_Variance_only_Deviation_Bound.html">392 hunch net-2010-03-26-A Variance only Deviation Bound</a></p>
<p>11 0.55145025 <a title="400-lsi-11" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>12 0.55083227 <a title="400-lsi-12" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>13 0.53679353 <a title="400-lsi-13" href="../hunch_net-2006/hunch_net-2006-01-13-Benchmarks_for_RL.html">148 hunch net-2006-01-13-Benchmarks for RL</a></p>
<p>14 0.52638465 <a title="400-lsi-14" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<p>15 0.51609248 <a title="400-lsi-15" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>16 0.47255945 <a title="400-lsi-16" href="../hunch_net-2006/hunch_net-2006-02-11-Yahoo%26%238217%3Bs_Learning_Problems..html">156 hunch net-2006-02-11-Yahoo&#8217;s Learning Problems.</a></p>
<p>17 0.46341872 <a title="400-lsi-17" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>18 0.4583343 <a title="400-lsi-18" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>19 0.4551357 <a title="400-lsi-19" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>20 0.45339265 <a title="400-lsi-20" href="../hunch_net-2009/hunch_net-2009-10-03-Static_vs._Dynamic_multiclass_prediction.html">373 hunch net-2009-10-03-Static vs. Dynamic multiclass prediction</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(27, 0.738), (38, 0.016), (53, 0.026), (77, 0.027), (94, 0.058), (95, 0.031)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99951738 <a title="400-lda-1" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>Introduction: Consider the contextual bandit setting where, repeatedly:
  
 A context  x  is observed. 
 An action  a  is taken given the context  x .  
 A reward  r  is observed, dependent on  x  and  a . 
  
Where the goal of a learning agent is to find a policy for step 2 achieving a large expected reward.  
 
This setting is of obvious importance, because in the real world we typically make decisions based on some set of information and then get feedback only about the single action taken.  It also fundamentally differs from supervised learning settings because knowing the value of one action is not equivalent to knowing the value of all actions.
 
A decade ago the best machine learning techniques for this setting where implausibly inefficient.   Dean Foster  once told me he thought the area was a research sinkhole with little progress to be expected.  Now we are on the verge of being able to routinely attack these problems, in almost exactly the same sense that we routinely attack bread and but</p><p>2 0.99456608 <a title="400-lda-2" href="../hunch_net-2005/hunch_net-2005-03-22-Active_learning.html">45 hunch net-2005-03-22-Active learning</a></p>
<p>Introduction: Often, unlabeled data is easy to come by but labels are expensive. For instance, if you’re building a speech recognizer, it’s easy enough to get raw speech samples — just walk around with a microphone — but labeling even one of these samples is a tedious process in which a human must examine the speech signal and carefully segment it into phonemes. In the field of active learning, the goal is as usual to construct an accurate classifier, but the labels of the data points are initially hidden and there is a charge for each label you want revealed. The hope is that by intelligent adaptive querying, you can get away with significantly fewer labels than you would need in a regular supervised learning framework.
 
Here’s an example. Suppose the data lie on the real line, and the classifiers are simple thresholding functions, H = {h w }: 
  h w (x) = 1 if x > w, and 0 otherwise.  
 
VC theory tells us that if the underlying distribution P can be classified perfectly by some hypothesis in H (</p><p>3 0.99451351 <a title="400-lda-3" href="../hunch_net-2006/hunch_net-2006-03-24-NLPers.html">166 hunch net-2006-03-24-NLPers</a></p>
<p>Introduction: Hal Daume  has started the  NLPers  blog to discuss learning for language problems.</p><p>4 0.99451351 <a title="400-lda-4" href="../hunch_net-2007/hunch_net-2007-06-13-Not_Posting.html">246 hunch net-2007-06-13-Not Posting</a></p>
<p>Introduction: If you have been disappointed by the lack of a post for the last month, consider  contributing your own  (Iâ&euro;&trade;ve been busy+uninspired).  Also, keep in mind that there is a community of machine learning blogs (see the sidebar).</p><p>5 0.99451351 <a title="400-lda-5" href="../hunch_net-2010/hunch_net-2010-12-02-Traffic_Prediction_Problem.html">418 hunch net-2010-12-02-Traffic Prediction Problem</a></p>
<p>Introduction: Slashdot  points out the  Traffic Prediction Challenge  which looks pretty fun.  The temporal aspect seems to be very common in many real-world problems and somewhat understudied.</p><p>6 0.99392056 <a title="400-lda-6" href="../hunch_net-2007/hunch_net-2007-11-28-Computational_Consequences_of_Classification.html">274 hunch net-2007-11-28-Computational Consequences of Classification</a></p>
<p>7 0.99382383 <a title="400-lda-7" href="../hunch_net-2008/hunch_net-2008-07-06-To_Dual_or_Not.html">308 hunch net-2008-07-06-To Dual or Not</a></p>
<p>8 0.99347365 <a title="400-lda-8" href="../hunch_net-2007/hunch_net-2007-06-14-Interesting_Papers_at_COLT_2007.html">247 hunch net-2007-06-14-Interesting Papers at COLT 2007</a></p>
<p>9 0.99301702 <a title="400-lda-9" href="../hunch_net-2006/hunch_net-2006-04-14-JMLR_is_a_success.html">172 hunch net-2006-04-14-JMLR is a success</a></p>
<p>10 0.99213284 <a title="400-lda-10" href="../hunch_net-2007/hunch_net-2007-05-12-Loss_Function_Semantics.html">245 hunch net-2007-05-12-Loss Function Semantics</a></p>
<p>11 0.98839885 <a title="400-lda-11" href="../hunch_net-2008/hunch_net-2008-02-10-Complexity_Illness.html">288 hunch net-2008-02-10-Complexity Illness</a></p>
<p>12 0.98276132 <a title="400-lda-12" href="../hunch_net-2005/hunch_net-2005-02-01-Watchword%3A_Loss.html">9 hunch net-2005-02-01-Watchword: Loss</a></p>
<p>13 0.98224193 <a title="400-lda-13" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<p>14 0.97880256 <a title="400-lda-14" href="../hunch_net-2009/hunch_net-2009-02-04-Optimal_Proxy_Loss_for_Classification.html">341 hunch net-2009-02-04-Optimal Proxy Loss for Classification</a></p>
<p>15 0.96903706 <a title="400-lda-15" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>16 0.96731466 <a title="400-lda-16" href="../hunch_net-2006/hunch_net-2006-07-13-Regression_vs._Classification_as_a_Primitive.html">196 hunch net-2006-07-13-Regression vs. Classification as a Primitive</a></p>
<p>17 0.95961326 <a title="400-lda-17" href="../hunch_net-2013/hunch_net-2013-06-10-The_Large_Scale_Learning_class_notes.html">483 hunch net-2013-06-10-The Large Scale Learning class notes</a></p>
<p>18 0.95670527 <a title="400-lda-18" href="../hunch_net-2007/hunch_net-2007-05-09-The_Missing_Bound.html">244 hunch net-2007-05-09-The Missing Bound</a></p>
<p>19 0.95131391 <a title="400-lda-19" href="../hunch_net-2008/hunch_net-2008-03-23-Interactive_Machine_Learning.html">293 hunch net-2008-03-23-Interactive Machine Learning</a></p>
<p>20 0.95037359 <a title="400-lda-20" href="../hunch_net-2005/hunch_net-2005-02-01-NIPS%3A_Online_Bayes.html">8 hunch net-2005-02-01-NIPS: Online Bayes</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
