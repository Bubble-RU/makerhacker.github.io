<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>415 hunch net-2010-10-28-NY ML Symposium 2010</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2010" href="../home/hunch_net-2010_home.html">hunch_net-2010</a> <a title="hunch_net-2010-415" href="#">hunch_net-2010-415</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>415 hunch net-2010-10-28-NY ML Symposium 2010</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2010-415-html" href="http://hunch.net/?p=1560">html</a></p><p>Introduction: About 200 people attended the2010 NYAS ML Symposiumthis year. (It wasabout 170
last year.) I particularly enjoyed several talks.Yannhas a new live demo of
(limited) real-time object recognition learning.Sanjoygave a fairly convincing
and comprehensible explanation of why amodified form of single-linkage
clusteringis consistent in higher dimensions, and why consistency is a
critical feature for clustering algorithms. I'm curious how well this
algorithm works in practice.Matt Hoffman's poster covering online LDA seemed
pretty convincing to me as an algorithmic improvement.This year, we allocated
more time towards posters & poster spotlights.For next year, we are
considering some further changes. The format has traditionally been 4 invited
Professor speakers, with posters and poster spotlight for students. Demand
from other parties to participate is growing, for example from postdocs and
startups in the area. Another growing concern is the facility--the location is
exceptional, but fittin</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('meetup', 0.43), ('regional', 0.282), ('posters', 0.234), ('poster', 0.213), ('groups', 0.201), ('california', 0.179), ('participation', 0.166), ('symposium', 0.166), ('convincing', 0.152), ('location', 0.145), ('growing', 0.127), ('relative', 0.123), ('higher', 0.123), ('explanation', 0.107), ('sydney', 0.107), ('demo', 0.107), ('hoffman', 0.107), ('symposiumthis', 0.107), ('australia', 0.099), ('postdocs', 0.099)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="415-tfidf-1" href="../hunch_net-2010/hunch_net-2010-10-28-NY_ML_Symposium_2010.html">415 hunch net-2010-10-28-NY ML Symposium 2010</a></p>
<p>Introduction: About 200 people attended the2010 NYAS ML Symposiumthis year. (It wasabout 170
last year.) I particularly enjoyed several talks.Yannhas a new live demo of
(limited) real-time object recognition learning.Sanjoygave a fairly convincing
and comprehensible explanation of why amodified form of single-linkage
clusteringis consistent in higher dimensions, and why consistency is a
critical feature for clustering algorithms. I'm curious how well this
algorithm works in practice.Matt Hoffman's poster covering online LDA seemed
pretty convincing to me as an algorithmic improvement.This year, we allocated
more time towards posters & poster spotlights.For next year, we are
considering some further changes. The format has traditionally been 4 invited
Professor speakers, with posters and poster spotlight for students. Demand
from other parties to participate is growing, for example from postdocs and
startups in the area. Another growing concern is the facility--the location is
exceptional, but fittin</p><p>2 0.15580907 <a title="415-tfidf-2" href="../hunch_net-2013/hunch_net-2013-04-15-NEML_II.html">481 hunch net-2013-04-15-NEML II</a></p>
<p>Introduction: Adam Kalaipoints out theNew England Machine Learning DayMay 1 at MSR New
England. There is a poster session with abstracts due April 19. I understand
last year'sNEMLwent well and it's great to meet your neighbors at regional
workshops like this.</p><p>3 0.1539295 <a title="415-tfidf-3" href="../hunch_net-2012/hunch_net-2012-04-20-Both_new%3A_STOC_workshops_and_NEML.html">462 hunch net-2012-04-20-Both new: STOC workshops and NEML</a></p>
<p>Introduction: May 16 in Cambridge, is theNew England Machine Learning Day, a first regional
workshop/symposium on machine learning. To present a poster, submit an
abstract byMay 5.May 19 in New York,STOCis coming to town and rather
surprisingly havingworkshopswhich should be quite a bit of fun. I'll be
speaking atAlgorithms for Distributed and Streaming Data.</p><p>4 0.11769666 <a title="415-tfidf-4" href="../hunch_net-2013/hunch_net-2013-09-20-No_NY_ML_Symposium_in_2013%2C_and_some_good_news.html">489 hunch net-2013-09-20-No NY ML Symposium in 2013, and some good news</a></p>
<p>Introduction: There will be no New York ML Symposium this year. The core issue is thatNYASis
disorganized by people leaving, pushing back the date, with the current
candidate a spring symposium on March 28.Gunnarand I were outvoted here--we
were gung ho on organizing a fall symposium, but the rest of the committee
wants to wait.In some good news, most of theICML 2012 videoshave been restored
from a deep backup.</p><p>5 0.11503208 <a title="415-tfidf-5" href="../hunch_net-2008/hunch_net-2008-10-20-New_York%26%238217%3Bs_ML_Day.html">322 hunch net-2008-10-20-New York&#8217;s ML Day</a></p>
<p>Introduction: I'm not as naturally exuberant asMuthu2orDavidaboutCS/Econday, but I believe
it andML daywere certainly successful.At the CS/Econ day, I particularly
enjoyedToumas Sandholm'stalk which showed a commanding depth of understanding
and application in automated auctions.For the machine learning day, I enjoyed
several talks and posters (I better, I helped pick them.). What stood out to
me was number of people attending: 158 registered, a level qualifying as
"scramble to find seats". My rule of thumb for workshops/conferences is that
the number of attendees is often something like the number of submissions.
That isn't the case here, where there were just 4 invited speakers and 30-or-
so posters. Presumably, the difference is due to a critical mass of Machine
Learning interested people in the area and the ease of their attendance.Are
there other areas where a local Machine Learning day would fly? It's easy to
imagine something working out in the San Francisco bay area and possibly
Germany or E</p><p>6 0.11078061 <a title="415-tfidf-6" href="../hunch_net-2009/hunch_net-2009-11-09-NYAS_ML_Symposium_this_year..html">377 hunch net-2009-11-09-NYAS ML Symposium this year.</a></p>
<p>7 0.094791204 <a title="415-tfidf-7" href="../hunch_net-2012/hunch_net-2012-10-18-7th_Annual_Machine_Learning_Symposium.html">474 hunch net-2012-10-18-7th Annual Machine Learning Symposium</a></p>
<p>8 0.090245984 <a title="415-tfidf-8" href="../hunch_net-2010/hunch_net-2010-09-17-New_York_Area_Machine_Learning_Events.html">410 hunch net-2010-09-17-New York Area Machine Learning Events</a></p>
<p>9 0.084417015 <a title="415-tfidf-9" href="../hunch_net-2011/hunch_net-2011-01-16-2011_Summer_Conference_Deadline_Season.html">422 hunch net-2011-01-16-2011 Summer Conference Deadline Season</a></p>
<p>10 0.077252522 <a title="415-tfidf-10" href="../hunch_net-2011/hunch_net-2011-09-03-Fall_Machine_Learning_Events.html">443 hunch net-2011-09-03-Fall Machine Learning Events</a></p>
<p>11 0.076892339 <a title="415-tfidf-11" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>12 0.076427728 <a title="415-tfidf-12" href="../hunch_net-2011/hunch_net-2011-10-10-ML_Symposium_and_ICML_details.html">447 hunch net-2011-10-10-ML Symposium and ICML details</a></p>
<p>13 0.07574676 <a title="415-tfidf-13" href="../hunch_net-2012/hunch_net-2012-03-24-David_Waltz.html">460 hunch net-2012-03-24-David Waltz</a></p>
<p>14 0.070641398 <a title="415-tfidf-14" href="../hunch_net-2009/hunch_net-2009-05-19-CI_Fellows.html">355 hunch net-2009-05-19-CI Fellows</a></p>
<p>15 0.0703618 <a title="415-tfidf-15" href="../hunch_net-2005/hunch_net-2005-02-19-Machine_learning_reading_groups.html">24 hunch net-2005-02-19-Machine learning reading groups</a></p>
<p>16 0.068697594 <a title="415-tfidf-16" href="../hunch_net-2007/hunch_net-2007-07-13-The_View_From_China.html">255 hunch net-2007-07-13-The View From China</a></p>
<p>17 0.067602456 <a title="415-tfidf-17" href="../hunch_net-2008/hunch_net-2008-09-04-Fall_ML_Conferences.html">316 hunch net-2008-09-04-Fall ML Conferences</a></p>
<p>18 0.067320757 <a title="415-tfidf-18" href="../hunch_net-2011/hunch_net-2011-06-22-Ultra_LDA.html">436 hunch net-2011-06-22-Ultra LDA</a></p>
<p>19 0.067197889 <a title="415-tfidf-19" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>20 0.066765502 <a title="415-tfidf-20" href="../hunch_net-2011/hunch_net-2011-10-24-2011_ML_symposium_and_the_bears.html">448 hunch net-2011-10-24-2011 ML symposium and the bears</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.121), (1, 0.069), (2, 0.092), (3, 0.121), (4, -0.003), (5, 0.006), (6, 0.008), (7, -0.002), (8, 0.079), (9, -0.085), (10, 0.052), (11, 0.072), (12, 0.045), (13, -0.088), (14, -0.087), (15, 0.064), (16, -0.035), (17, 0.028), (18, 0.007), (19, 0.073), (20, 0.006), (21, -0.005), (22, 0.049), (23, -0.052), (24, -0.023), (25, 0.082), (26, 0.027), (27, -0.013), (28, -0.082), (29, 0.081), (30, 0.079), (31, -0.02), (32, 0.014), (33, 0.079), (34, -0.046), (35, -0.036), (36, -0.026), (37, -0.082), (38, -0.036), (39, 0.013), (40, 0.006), (41, -0.024), (42, 0.059), (43, -0.042), (44, -0.025), (45, -0.028), (46, 0.034), (47, 0.018), (48, 0.06), (49, 0.042)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97410762 <a title="415-lsi-1" href="../hunch_net-2010/hunch_net-2010-10-28-NY_ML_Symposium_2010.html">415 hunch net-2010-10-28-NY ML Symposium 2010</a></p>
<p>Introduction: About 200 people attended the2010 NYAS ML Symposiumthis year. (It wasabout 170
last year.) I particularly enjoyed several talks.Yannhas a new live demo of
(limited) real-time object recognition learning.Sanjoygave a fairly convincing
and comprehensible explanation of why amodified form of single-linkage
clusteringis consistent in higher dimensions, and why consistency is a
critical feature for clustering algorithms. I'm curious how well this
algorithm works in practice.Matt Hoffman's poster covering online LDA seemed
pretty convincing to me as an algorithmic improvement.This year, we allocated
more time towards posters & poster spotlights.For next year, we are
considering some further changes. The format has traditionally been 4 invited
Professor speakers, with posters and poster spotlight for students. Demand
from other parties to participate is growing, for example from postdocs and
startups in the area. Another growing concern is the facility--the location is
exceptional, but fittin</p><p>2 0.73881727 <a title="415-lsi-2" href="../hunch_net-2012/hunch_net-2012-10-18-7th_Annual_Machine_Learning_Symposium.html">474 hunch net-2012-10-18-7th Annual Machine Learning Symposium</a></p>
<p>Introduction: A reminder that theNew York Academy of Scienceswill be hosting the7th Annual
Machine Learning Symposiumtomorrow from 9:30am.The main program will feature
invited talks fromPeter Bartlett,William Freeman, andVladimir Vapnik, along
with numerous spotlight talks and a poster session. Following the main
program,hackNYandMicrosoft Researchare sponsoring a networking hour with talks
from machine learning practitioners at NYC startups
(specificallybit.ly,Buzzfeed,Chartbeat, andSense Networks,Visual Revenue).
This should be of great interest to everyone considering working in machine
learning.</p><p>3 0.72847903 <a title="415-lsi-3" href="../hunch_net-2011/hunch_net-2011-09-03-Fall_Machine_Learning_Events.html">443 hunch net-2011-09-03-Fall Machine Learning Events</a></p>
<p>Introduction: Many Machine Learning related events are coming up this fall.September
9,abstracts for the New York Machine Learning Symposiumare due. Send a 2 page
pdf, if interested, and note that we:widened submissions to be from anybody
rather than students.set aside a larger fraction of time for contributed
submissions.September 15, there is amachine learning meetup, where I'll be
discussing terascale learning at AOL.September 16, there is aCS&Econ; dayat New
York Academy of Sciences. This is not ML focused, but it's easy to imagine
interest.September 23 and laterNIPS workshopsubmissions start coming due. As
usual, there are too many good ones, so I won't be able to attend all those
that interest me. I do hope some workshop makers consider ICML this coming
summer, as we are increasing to a 2 day format for you. Here are a few that
interest me:Big Learningis about dealing with lots of data. Abstracts are
dueSeptember 30.TheBayes Banditsworkshop. Abstracts are dueSeptember
23.ThePersonalized Medicin</p><p>4 0.7081126 <a title="415-lsi-4" href="../hunch_net-2012/hunch_net-2012-04-20-Both_new%3A_STOC_workshops_and_NEML.html">462 hunch net-2012-04-20-Both new: STOC workshops and NEML</a></p>
<p>Introduction: May 16 in Cambridge, is theNew England Machine Learning Day, a first regional
workshop/symposium on machine learning. To present a poster, submit an
abstract byMay 5.May 19 in New York,STOCis coming to town and rather
surprisingly havingworkshopswhich should be quite a bit of fun. I'll be
speaking atAlgorithms for Distributed and Streaming Data.</p><p>5 0.65574592 <a title="415-lsi-5" href="../hunch_net-2013/hunch_net-2013-04-15-NEML_II.html">481 hunch net-2013-04-15-NEML II</a></p>
<p>Introduction: Adam Kalaipoints out theNew England Machine Learning DayMay 1 at MSR New
England. There is a poster session with abstracts due April 19. I understand
last year'sNEMLwent well and it's great to meet your neighbors at regional
workshops like this.</p><p>6 0.63918078 <a title="415-lsi-6" href="../hunch_net-2010/hunch_net-2010-09-17-New_York_Area_Machine_Learning_Events.html">410 hunch net-2010-09-17-New York Area Machine Learning Events</a></p>
<p>7 0.63250625 <a title="415-lsi-7" href="../hunch_net-2008/hunch_net-2008-10-20-New_York%26%238217%3Bs_ML_Day.html">322 hunch net-2008-10-20-New York&#8217;s ML Day</a></p>
<p>8 0.59141725 <a title="415-lsi-8" href="../hunch_net-2012/hunch_net-2012-10-26-ML_Symposium_and_Strata-Hadoop_World.html">475 hunch net-2012-10-26-ML Symposium and Strata-Hadoop World</a></p>
<p>9 0.57839864 <a title="415-lsi-9" href="../hunch_net-2012/hunch_net-2012-03-24-David_Waltz.html">460 hunch net-2012-03-24-David Waltz</a></p>
<p>10 0.57132626 <a title="415-lsi-10" href="../hunch_net-2011/hunch_net-2011-10-24-2011_ML_symposium_and_the_bears.html">448 hunch net-2011-10-24-2011 ML symposium and the bears</a></p>
<p>11 0.51872122 <a title="415-lsi-11" href="../hunch_net-2008/hunch_net-2008-09-04-Fall_ML_Conferences.html">316 hunch net-2008-09-04-Fall ML Conferences</a></p>
<p>12 0.48918447 <a title="415-lsi-12" href="../hunch_net-2014/hunch_net-2014-03-11-The_New_York_ML_Symposium%2C_take_2.html">494 hunch net-2014-03-11-The New York ML Symposium, take 2</a></p>
<p>13 0.4842113 <a title="415-lsi-13" href="../hunch_net-2006/hunch_net-2006-05-08-Big_machine_learning.html">178 hunch net-2006-05-08-Big machine learning</a></p>
<p>14 0.46709406 <a title="415-lsi-14" href="../hunch_net-2009/hunch_net-2009-11-09-NYAS_ML_Symposium_this_year..html">377 hunch net-2009-11-09-NYAS ML Symposium this year.</a></p>
<p>15 0.44277796 <a title="415-lsi-15" href="../hunch_net-2005/hunch_net-2005-10-08-We_have_a_winner.html">119 hunch net-2005-10-08-We have a winner</a></p>
<p>16 0.42990702 <a title="415-lsi-16" href="../hunch_net-2011/hunch_net-2011-10-10-ML_Symposium_and_ICML_details.html">447 hunch net-2011-10-10-ML Symposium and ICML details</a></p>
<p>17 0.42612654 <a title="415-lsi-17" href="../hunch_net-2009/hunch_net-2009-01-08-Predictive_Analytics_World.html">335 hunch net-2009-01-08-Predictive Analytics World</a></p>
<p>18 0.42072251 <a title="415-lsi-18" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>19 0.4191303 <a title="415-lsi-19" href="../hunch_net-2006/hunch_net-2006-04-09-Progress_in_Machine_Translation.html">171 hunch net-2006-04-09-Progress in Machine Translation</a></p>
<p>20 0.41773218 <a title="415-lsi-20" href="../hunch_net-2012/hunch_net-2012-07-17-MUCMD_and_BayLearn.html">470 hunch net-2012-07-17-MUCMD and BayLearn</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(42, 0.135), (68, 0.076), (73, 0.021), (74, 0.123), (76, 0.542)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.91587019 <a title="415-lda-1" href="../hunch_net-2010/hunch_net-2010-10-28-NY_ML_Symposium_2010.html">415 hunch net-2010-10-28-NY ML Symposium 2010</a></p>
<p>Introduction: About 200 people attended the2010 NYAS ML Symposiumthis year. (It wasabout 170
last year.) I particularly enjoyed several talks.Yannhas a new live demo of
(limited) real-time object recognition learning.Sanjoygave a fairly convincing
and comprehensible explanation of why amodified form of single-linkage
clusteringis consistent in higher dimensions, and why consistency is a
critical feature for clustering algorithms. I'm curious how well this
algorithm works in practice.Matt Hoffman's poster covering online LDA seemed
pretty convincing to me as an algorithmic improvement.This year, we allocated
more time towards posters & poster spotlights.For next year, we are
considering some further changes. The format has traditionally been 4 invited
Professor speakers, with posters and poster spotlight for students. Demand
from other parties to participate is growing, for example from postdocs and
startups in the area. Another growing concern is the facility--the location is
exceptional, but fittin</p><p>2 0.74332654 <a title="415-lda-2" href="../hunch_net-2006/hunch_net-2006-02-18-Multiplication_of_Learned_Probabilities_is_Dangerous.html">157 hunch net-2006-02-18-Multiplication of Learned Probabilities is Dangerous</a></p>
<p>Introduction: This is about a design flaw in several learning algorithms such as the Naive
Bayes classifier and Hidden Markov Models. A number of people are aware of it,
but it seems that not everyone is.Several learning systems have the property
that they estimate some conditional probabilitiesP(event | other events)either
explicitly or implicitly. Then, at prediction time, these learned
probabilities are multiplied together according to some formula to produce a
final prediction. The Naive Bayes classifier for binary data is the simplest
of these, so it seems like a good example.When Naive Bayes is used, a set of
probabilities of the formPr'(feature i | label)are estimated via counting
statistics and some prior. Predictions are made according to the label
maximizing:Pr'(label) * Productfeatures iPr'(feature i | label)(ThePr'notation
indicates these are estimated values.)There is nothing wrong with this method
as long as (a) the prior for the sample counts is very strong and (b) the
prior (on the c</p><p>3 0.74111104 <a title="415-lda-3" href="../hunch_net-2009/hunch_net-2009-11-06-Yisong_Yue_on_Self-improving_Systems.html">376 hunch net-2009-11-06-Yisong Yue on Self-improving Systems</a></p>
<p>Introduction: I'd like to point outYisong Yue'spost on Self-improving systems, which is a
nicely readable description of the necessity and potential of interactive
learning to deal with the information overload problem that is endemic to the
modern internet.</p><p>4 0.70941347 <a title="415-lda-4" href="../hunch_net-2008/hunch_net-2008-07-02-Proprietary_Data_in_Academic_Research%3F.html">306 hunch net-2008-07-02-Proprietary Data in Academic Research?</a></p>
<p>Introduction: Should results of experiments on proprietary datasets be in the academic
research literature?The arguments I can imagine in the "against" column
are:Experiments are not repeatable. Repeatability in experiments is essential
to science because it allows others to compare new methods with old and
discover which is better.It's unfair. Academics who don't have insider access
to proprietary data are at a substantial disadvantage when competing with
others who do.I'm unsympathetic to argument (2). To me, it looks like their
are simply some resource constraints, and these should not prevent research
progress. For example, we wouldn't prevent publishing about particle
accelerator experiments by physicists atCERNbecause physicists atCMUcouldn't
run their own experiments.Argument (1) seems like a real issue.The argument
for is:Yes, they are another form of evidence that an algorithm is good. The
degree to which they are evidence is less than for publicly repeatable
experiments, but greater than n</p><p>5 0.68239927 <a title="415-lda-5" href="../hunch_net-2006/hunch_net-2006-03-23-The_Approximation_Argument.html">165 hunch net-2006-03-23-The Approximation Argument</a></p>
<p>Introduction: An argument is sometimes made that the Bayesian way is the "right" way to do
machine learning. This is a serious argument which deserves a serious reply.
The approximation argument is a serious reply for which I have not yet seen a
reply2.The idea for the Bayesian approach is quite simple, elegant, and
general. Essentially, you first specify a priorP(D)over possible
processesDproducing the data, observe the data, then condition on the data
according to Bayes law to construct a posterior:P(D|x) = P(x|D)P(D)/P(x)After
this, hard decisions are made (such as "turn left" or "turn right") by
choosing the one which minimizes the expected (with respect to the posterior)
loss.This basic idea is reused thousands of times with various choices
ofP(D)and loss functions which is unsurprising given the many nice
properties:There is an extremely strong associated guarantee: If the actual
distribution generating the data is drawn fromP(D)there is no better method.
One way to think about this is that in</p><p>6 0.60150254 <a title="415-lda-6" href="../hunch_net-2009/hunch_net-2009-06-03-Functionally_defined_Nonlinear_Dynamic_Models.html">359 hunch net-2009-06-03-Functionally defined Nonlinear Dynamic Models</a></p>
<p>7 0.47093982 <a title="415-lda-7" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>8 0.40773189 <a title="415-lda-8" href="../hunch_net-2006/hunch_net-2006-07-08-MaxEnt_contradicts_Bayes_Rule%3F.html">191 hunch net-2006-07-08-MaxEnt contradicts Bayes Rule?</a></p>
<p>9 0.39605439 <a title="415-lda-9" href="../hunch_net-2005/hunch_net-2005-08-11-Why_Manifold-Based_Dimension_Reduction_Techniques%3F.html">102 hunch net-2005-08-11-Why Manifold-Based Dimension Reduction Techniques?</a></p>
<p>10 0.39480609 <a title="415-lda-10" href="../hunch_net-2008/hunch_net-2008-05-23-Three_levels_of_addressing_the_Netflix_Prize.html">301 hunch net-2008-05-23-Three levels of addressing the Netflix Prize</a></p>
<p>11 0.3918317 <a title="415-lda-11" href="../hunch_net-2005/hunch_net-2005-02-12-ROC_vs._Accuracy_vs._AROC.html">18 hunch net-2005-02-12-ROC vs. Accuracy vs. AROC</a></p>
<p>12 0.39102569 <a title="415-lda-12" href="../hunch_net-2006/hunch_net-2006-08-10-Precision_is_not_accuracy.html">202 hunch net-2006-08-10-Precision is not accuracy</a></p>
<p>13 0.38137928 <a title="415-lda-13" href="../hunch_net-2005/hunch_net-2005-01-24-The_Humanloop_Spectrum_of_Machine_Learning.html">3 hunch net-2005-01-24-The Humanloop Spectrum of Machine Learning</a></p>
<p>14 0.38066196 <a title="415-lda-14" href="../hunch_net-2007/hunch_net-2007-09-18-It%26%238217%3Bs_MDL_Jim%2C_but_not_as_we_know_it%26%238230%3B%28on_Bayes%2C_MDL_and_consistency%29.html">263 hunch net-2007-09-18-It&#8217;s MDL Jim, but not as we know it&#8230;(on Bayes, MDL and consistency)</a></p>
<p>15 0.3803753 <a title="415-lda-15" href="../hunch_net-2006/hunch_net-2006-03-02-Why_do_people_count_for_learning%3F.html">160 hunch net-2006-03-02-Why do people count for learning?</a></p>
<p>16 0.3800137 <a title="415-lda-16" href="../hunch_net-2005/hunch_net-2005-10-12-The_unrealized_potential_of_the_research_lab.html">121 hunch net-2005-10-12-The unrealized potential of the research lab</a></p>
<p>17 0.37588489 <a title="415-lda-17" href="../hunch_net-2005/hunch_net-2005-01-19-Why_I_decided_to_run_a_weblog..html">1 hunch net-2005-01-19-Why I decided to run a weblog.</a></p>
<p>18 0.3758004 <a title="415-lda-18" href="../hunch_net-2006/hunch_net-2006-11-06-Data_Linkage_Problems.html">217 hunch net-2006-11-06-Data Linkage Problems</a></p>
<p>19 0.37187213 <a title="415-lda-19" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>20 0.36657926 <a title="415-lda-20" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
