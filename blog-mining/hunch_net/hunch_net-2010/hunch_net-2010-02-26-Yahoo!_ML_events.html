<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>389 hunch net-2010-02-26-Yahoo! ML events</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2010" href="../home/hunch_net-2010_home.html">hunch_net-2010</a> <a title="hunch_net-2010-389" href="#">hunch_net-2010-389</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>389 hunch net-2010-02-26-Yahoo! ML events</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2010-389-html" href="http://hunch.net/?p=1236">html</a></p><p>Introduction: Yahoo! is sponsoring two machine learning events that might interest people.  
  
 The  Key Scientific Challenges  program (due March 5) for  Machine Learning  and  Statistics  offers $5K (plus bonuses) for graduate students working on a core problem of interest to Y!  If you are already working on one of these problems, there is no reason not to submit, and if you arenâ&euro;&trade;t you might want to think about it for next year, as I am confident they all press the boundary of the possible in Machine Learning.   There are 7 days left. 
 The  Learning to Rank challenge  (due May 31) offers an $8K first prize for the best ranking algorithm on a real (and really used) dataset for search ranking, with presentations at an ICML workshop.  Unlike the Netflix competition, there are prizes for 2nd, 3rd, and 4th place, perhaps avoiding the heartbreak  the ensemble  encountered.  If you think you know how to rank, you should give it a try, and we might all learn something.  There are 3 months left.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 is sponsoring two machine learning events that might interest people. [sent-2, score-0.64]
</p><p>2 The  Key Scientific Challenges  program (due March 5) for  Machine Learning  and  Statistics  offers $5K (plus bonuses) for graduate students working on a core problem of interest to Y! [sent-3, score-0.977]
</p><p>3 If you are already working on one of these problems, there is no reason not to submit, and if you arenâ&euro;&trade;t you might want to think about it for next year, as I am confident they all press the boundary of the possible in Machine Learning. [sent-4, score-1.158]
</p><p>4 The  Learning to Rank challenge  (due May 31) offers an $8K first prize for the best ranking algorithm on a real (and really used) dataset for search ranking, with presentations at an ICML workshop. [sent-6, score-1.18]
</p><p>5 Unlike the Netflix competition, there are prizes for 2nd, 3rd, and 4th place, perhaps avoiding the heartbreak  the ensemble  encountered. [sent-7, score-0.426]
</p><p>6 If you think you know how to rank, you should give it a try, and we might all learn something. [sent-8, score-0.378]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('rank', 0.321), ('offers', 0.308), ('ranking', 0.249), ('bonuses', 0.192), ('press', 0.192), ('confident', 0.178), ('sponsoring', 0.168), ('boundary', 0.168), ('prizes', 0.16), ('plus', 0.154), ('interest', 0.144), ('scientific', 0.14), ('working', 0.137), ('months', 0.133), ('challenges', 0.133), ('avoiding', 0.133), ('ensemble', 0.133), ('due', 0.127), ('prize', 0.127), ('might', 0.126), ('days', 0.124), ('submit', 0.124), ('presentations', 0.124), ('graduate', 0.124), ('netflix', 0.124), ('unlike', 0.12), ('events', 0.118), ('march', 0.118), ('competition', 0.114), ('left', 0.112), ('yahoo', 0.112), ('think', 0.106), ('search', 0.103), ('statistics', 0.103), ('challenge', 0.1), ('key', 0.099), ('students', 0.099), ('dataset', 0.098), ('place', 0.094), ('core', 0.091), ('already', 0.091), ('aren', 0.089), ('machine', 0.084), ('give', 0.083), ('next', 0.081), ('try', 0.079), ('reason', 0.079), ('program', 0.074), ('really', 0.071), ('learn', 0.063)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="389-tfidf-1" href="../hunch_net-2010/hunch_net-2010-02-26-Yahoo%21_ML_events.html">389 hunch net-2010-02-26-Yahoo! ML events</a></p>
<p>Introduction: Yahoo! is sponsoring two machine learning events that might interest people.  
  
 The  Key Scientific Challenges  program (due March 5) for  Machine Learning  and  Statistics  offers $5K (plus bonuses) for graduate students working on a core problem of interest to Y!  If you are already working on one of these problems, there is no reason not to submit, and if you arenâ&euro;&trade;t you might want to think about it for next year, as I am confident they all press the boundary of the possible in Machine Learning.   There are 7 days left. 
 The  Learning to Rank challenge  (due May 31) offers an $8K first prize for the best ranking algorithm on a real (and really used) dataset for search ranking, with presentations at an ICML workshop.  Unlike the Netflix competition, there are prizes for 2nd, 3rd, and 4th place, perhaps avoiding the heartbreak  the ensemble  encountered.  If you think you know how to rank, you should give it a try, and we might all learn something.  There are 3 months left.</p><p>2 0.21283787 <a title="389-tfidf-2" href="../hunch_net-2011/hunch_net-2011-02-25-Yahoo%21_Machine_Learning_grant_due_March_11.html">425 hunch net-2011-02-25-Yahoo! Machine Learning grant due March 11</a></p>
<p>Introduction: Yahoo!’s  Key Scientific Challenges  for  Machine Learning  grant applications are due March 11.  If you are a student working on relevant research, please consider applying.  It’s for $5K of unrestricted funding.</p><p>3 0.16203298 <a title="389-tfidf-3" href="../hunch_net-2012/hunch_net-2012-02-29-Key_Scientific_Challenges_and_the_Franklin_Symposium.html">457 hunch net-2012-02-29-Key Scientific Challenges and the Franklin Symposium</a></p>
<p>Introduction: For graduate students, the  Yahoo!   Key Scientific Challenges program  including in  machine learning  is on again,  due March 9 .  The application is easy and the $5K award is high quality “no strings attached” funding.   Consider submitting.
 
Those in Washington DC, Philadelphia, and New York, may consider attending the  Franklin Institute Symposium   April 25  which has several speakers and an award for  V .  Attendance is free with an RSVP.</p><p>4 0.13231359 <a title="389-tfidf-4" href="../hunch_net-2009/hunch_net-2009-09-21-Netflix_finishes_%28and_starts%29.html">371 hunch net-2009-09-21-Netflix finishes (and starts)</a></p>
<p>Introduction: I attended the  Netflix prize  ceremony this morning.  The press conference part is  covered fine elsewhere , with the basic outcome being that  BellKor’s Pragmatic Chaos  won over  The Ensemble  by 15-20  minutes , because they were tied in performance on the ultimate holdout set.  I’m sure the individual participants will have many chances to speak about the solution.  One of these is Bell at the  NYAS ML symposium on Nov. 6 .
 
Several additional details may interest ML people.
  
 The degree of overfitting exhibited by the difference in performance on the  leaderboard test set  and the ultimate hold out set was small, but determining at .02 to .03%. 
 A tie was possible, because the rules cut off measurements below the fourth digit based on significance concerns.  In actuality, of course, the scores do differ before rounding, but everyone I spoke to claimed not to know how.  The complete dataset has been  released on UCI , so each team could compute their own score to whatever accu</p><p>5 0.12948102 <a title="389-tfidf-5" href="../hunch_net-2006/hunch_net-2006-02-11-Yahoo%26%238217%3Bs_Learning_Problems..html">156 hunch net-2006-02-11-Yahoo&#8217;s Learning Problems.</a></p>
<p>Introduction: I just visited  Yahoo Research  which has several fundamental learning problems near to (or beyond) the set of problems we know how to solve well.  Here are 3 of them.
  
  Ranking   This is the canonical problem of all search engines.  It is made extra difficult for several reasons.
 
 There is relatively little “good” supervised learning data and a great deal of data with some signal (such as click through rates). 
 The learning must occur in a partially adversarial environment. Many people very actively attempt to place themselves at the top of 
rankings. 
 It is not even quite clear whether the problem should be posed as ‘ranking’ or as ‘regression’ which is then used to produce a 
ranking. 
 
 
  Collaborative filtering  Yahoo has a large number of recommendation systems for music, movies, etc…  In these sorts of systems, users specify how they liked a set of things, and then the system can (hopefully) find some more examples of things they might like 
by reasoning across multiple</p><p>6 0.12902462 <a title="389-tfidf-6" href="../hunch_net-2009/hunch_net-2009-01-27-Key_Scientific_Challenges.html">339 hunch net-2009-01-27-Key Scientific Challenges</a></p>
<p>7 0.11749841 <a title="389-tfidf-7" href="../hunch_net-2005/hunch_net-2005-09-19-NIPS_Workshops.html">113 hunch net-2005-09-19-NIPS Workshops</a></p>
<p>8 0.11056939 <a title="389-tfidf-8" href="../hunch_net-2008/hunch_net-2008-05-23-Three_levels_of_addressing_the_Netflix_Prize.html">301 hunch net-2008-05-23-Three levels of addressing the Netflix Prize</a></p>
<p>9 0.1075526 <a title="389-tfidf-9" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>10 0.10071271 <a title="389-tfidf-10" href="../hunch_net-2009/hunch_net-2009-06-26-Netflix_nearly_done.html">362 hunch net-2009-06-26-Netflix nearly done</a></p>
<p>11 0.10034648 <a title="389-tfidf-11" href="../hunch_net-2008/hunch_net-2008-07-15-Interesting_papers_at_COLT_%28and_a_bit_of_UAI_%26%23038%3B_workshops%29.html">310 hunch net-2008-07-15-Interesting papers at COLT (and a bit of UAI &#038; workshops)</a></p>
<p>12 0.096719794 <a title="389-tfidf-12" href="../hunch_net-2009/hunch_net-2009-01-19-Netflix_prize_within_epsilon.html">336 hunch net-2009-01-19-Netflix prize within epsilon</a></p>
<p>13 0.096209273 <a title="389-tfidf-13" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>14 0.094701849 <a title="389-tfidf-14" href="../hunch_net-2007/hunch_net-2007-01-15-The_Machine_Learning_Department.html">228 hunch net-2007-01-15-The Machine Learning Department</a></p>
<p>15 0.092388511 <a title="389-tfidf-15" href="../hunch_net-2006/hunch_net-2006-04-30-John_Langford_%26%238211%3B%3E_Yahoo_Research%2C_NY.html">175 hunch net-2006-04-30-John Langford &#8211;> Yahoo Research, NY</a></p>
<p>16 0.090622015 <a title="389-tfidf-16" href="../hunch_net-2009/hunch_net-2009-10-26-NIPS_workshops.html">375 hunch net-2009-10-26-NIPS workshops</a></p>
<p>17 0.084290884 <a title="389-tfidf-17" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>18 0.083217286 <a title="389-tfidf-18" href="../hunch_net-2007/hunch_net-2007-11-16-MLSS_2008.html">273 hunch net-2007-11-16-MLSS 2008</a></p>
<p>19 0.08267232 <a title="389-tfidf-19" href="../hunch_net-2007/hunch_net-2007-04-18-%2450K_Spock_Challenge.html">239 hunch net-2007-04-18-$50K Spock Challenge</a></p>
<p>20 0.078596093 <a title="389-tfidf-20" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.176), (1, -0.071), (2, -0.12), (3, -0.026), (4, -0.07), (5, 0.004), (6, -0.044), (7, 0.05), (8, -0.085), (9, -0.126), (10, 0.001), (11, 0.192), (12, 0.009), (13, 0.038), (14, -0.001), (15, 0.089), (16, 0.013), (17, -0.074), (18, -0.028), (19, -0.106), (20, 0.004), (21, 0.076), (22, 0.077), (23, -0.134), (24, 0.042), (25, 0.059), (26, 0.081), (27, -0.125), (28, -0.028), (29, 0.089), (30, -0.068), (31, 0.141), (32, -0.044), (33, -0.052), (34, -0.07), (35, 0.013), (36, -0.061), (37, 0.015), (38, 0.012), (39, -0.009), (40, -0.009), (41, -0.049), (42, 0.136), (43, 0.034), (44, -0.027), (45, 0.07), (46, 0.004), (47, -0.061), (48, 0.063), (49, -0.004)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95007765 <a title="389-lsi-1" href="../hunch_net-2010/hunch_net-2010-02-26-Yahoo%21_ML_events.html">389 hunch net-2010-02-26-Yahoo! ML events</a></p>
<p>Introduction: Yahoo! is sponsoring two machine learning events that might interest people.  
  
 The  Key Scientific Challenges  program (due March 5) for  Machine Learning  and  Statistics  offers $5K (plus bonuses) for graduate students working on a core problem of interest to Y!  If you are already working on one of these problems, there is no reason not to submit, and if you arenâ&euro;&trade;t you might want to think about it for next year, as I am confident they all press the boundary of the possible in Machine Learning.   There are 7 days left. 
 The  Learning to Rank challenge  (due May 31) offers an $8K first prize for the best ranking algorithm on a real (and really used) dataset for search ranking, with presentations at an ICML workshop.  Unlike the Netflix competition, there are prizes for 2nd, 3rd, and 4th place, perhaps avoiding the heartbreak  the ensemble  encountered.  If you think you know how to rank, you should give it a try, and we might all learn something.  There are 3 months left.</p><p>2 0.71852237 <a title="389-lsi-2" href="../hunch_net-2012/hunch_net-2012-02-29-Key_Scientific_Challenges_and_the_Franklin_Symposium.html">457 hunch net-2012-02-29-Key Scientific Challenges and the Franklin Symposium</a></p>
<p>Introduction: For graduate students, the  Yahoo!   Key Scientific Challenges program  including in  machine learning  is on again,  due March 9 .  The application is easy and the $5K award is high quality “no strings attached” funding.   Consider submitting.
 
Those in Washington DC, Philadelphia, and New York, may consider attending the  Franklin Institute Symposium   April 25  which has several speakers and an award for  V .  Attendance is free with an RSVP.</p><p>3 0.68090075 <a title="389-lsi-3" href="../hunch_net-2009/hunch_net-2009-01-27-Key_Scientific_Challenges.html">339 hunch net-2009-01-27-Key Scientific Challenges</a></p>
<p>Introduction: Yahoo released the  Key Scientific Challenges  program.  There is a  Machine Learning  list I worked on and a  Statistics  list which  Deepak  worked on.
 
I’m hoping this is taken quite seriously by graduate students.  The primary value, is that it gave us a chance to sit down and publicly specify directions of research which would be valuable to make progress on.  A good strategy for a beginning graduate student is to pick one of these directions, pursue it, and make substantial advances for a PhD.  The directions are sufficiently general that I’m sure any serious advance has applications well beyond Yahoo.
 
A secondary point, (which I’m sure is primary for many    ) is that there is money for graduate students here.  It’s unrestricted, so you can use it for any reasonable travel, supplies, etc…</p><p>4 0.67533672 <a title="389-lsi-4" href="../hunch_net-2011/hunch_net-2011-02-25-Yahoo%21_Machine_Learning_grant_due_March_11.html">425 hunch net-2011-02-25-Yahoo! Machine Learning grant due March 11</a></p>
<p>Introduction: Yahoo!’s  Key Scientific Challenges  for  Machine Learning  grant applications are due March 11.  If you are a student working on relevant research, please consider applying.  It’s for $5K of unrestricted funding.</p><p>5 0.53112584 <a title="389-lsi-5" href="../hunch_net-2006/hunch_net-2006-02-11-Yahoo%26%238217%3Bs_Learning_Problems..html">156 hunch net-2006-02-11-Yahoo&#8217;s Learning Problems.</a></p>
<p>Introduction: I just visited  Yahoo Research  which has several fundamental learning problems near to (or beyond) the set of problems we know how to solve well.  Here are 3 of them.
  
  Ranking   This is the canonical problem of all search engines.  It is made extra difficult for several reasons.
 
 There is relatively little “good” supervised learning data and a great deal of data with some signal (such as click through rates). 
 The learning must occur in a partially adversarial environment. Many people very actively attempt to place themselves at the top of 
rankings. 
 It is not even quite clear whether the problem should be posed as ‘ranking’ or as ‘regression’ which is then used to produce a 
ranking. 
 
 
  Collaborative filtering  Yahoo has a large number of recommendation systems for music, movies, etc…  In these sorts of systems, users specify how they liked a set of things, and then the system can (hopefully) find some more examples of things they might like 
by reasoning across multiple</p><p>6 0.5192067 <a title="389-lsi-6" href="../hunch_net-2009/hunch_net-2009-01-08-Predictive_Analytics_World.html">335 hunch net-2009-01-08-Predictive Analytics World</a></p>
<p>7 0.48196194 <a title="389-lsi-7" href="../hunch_net-2006/hunch_net-2006-04-09-Progress_in_Machine_Translation.html">171 hunch net-2006-04-09-Progress in Machine Translation</a></p>
<p>8 0.46699569 <a title="389-lsi-8" href="../hunch_net-2009/hunch_net-2009-01-19-Netflix_prize_within_epsilon.html">336 hunch net-2009-01-19-Netflix prize within epsilon</a></p>
<p>9 0.4666357 <a title="389-lsi-9" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>10 0.46359083 <a title="389-lsi-10" href="../hunch_net-2008/hunch_net-2008-02-27-The_Stats_Handicap.html">290 hunch net-2008-02-27-The Stats Handicap</a></p>
<p>11 0.45278463 <a title="389-lsi-11" href="../hunch_net-2007/hunch_net-2007-11-02-The_Machine_Learning_Award_goes_to_%26%238230%3B.html">270 hunch net-2007-11-02-The Machine Learning Award goes to &#8230;</a></p>
<p>12 0.43474641 <a title="389-lsi-12" href="../hunch_net-2005/hunch_net-2005-04-27-DARPA_project%3A_LAGR.html">63 hunch net-2005-04-27-DARPA project: LAGR</a></p>
<p>13 0.43378609 <a title="389-lsi-13" href="../hunch_net-2009/hunch_net-2009-10-26-NIPS_workshops.html">375 hunch net-2009-10-26-NIPS workshops</a></p>
<p>14 0.42199039 <a title="389-lsi-14" href="../hunch_net-2006/hunch_net-2006-04-30-John_Langford_%26%238211%3B%3E_Yahoo_Research%2C_NY.html">175 hunch net-2006-04-30-John Langford &#8211;> Yahoo Research, NY</a></p>
<p>15 0.41639817 <a title="389-lsi-15" href="../hunch_net-2006/hunch_net-2006-05-08-Big_machine_learning.html">178 hunch net-2006-05-08-Big machine learning</a></p>
<p>16 0.39430597 <a title="389-lsi-16" href="../hunch_net-2007/hunch_net-2007-11-16-MLSS_2008.html">273 hunch net-2007-11-16-MLSS 2008</a></p>
<p>17 0.39168212 <a title="389-lsi-17" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>18 0.39013287 <a title="389-lsi-18" href="../hunch_net-2006/hunch_net-2006-04-14-JMLR_is_a_success.html">172 hunch net-2006-04-14-JMLR is a success</a></p>
<p>19 0.38729405 <a title="389-lsi-19" href="../hunch_net-2012/hunch_net-2012-03-06-COLT-ICML_Open_Questions_and_ICML_Instructions.html">458 hunch net-2012-03-06-COLT-ICML Open Questions and ICML Instructions</a></p>
<p>20 0.37871316 <a title="389-lsi-20" href="../hunch_net-2009/hunch_net-2009-09-21-Netflix_finishes_%28and_starts%29.html">371 hunch net-2009-09-21-Netflix finishes (and starts)</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(27, 0.135), (55, 0.102), (94, 0.054), (95, 0.592)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.98962092 <a title="389-lda-1" href="../hunch_net-2013/hunch_net-2013-01-31-Remote_large_scale_learning_class_participation.html">479 hunch net-2013-01-31-Remote large scale learning class participation</a></p>
<p>Introduction: Yann and I have arranged so that people who are interested in our  large scale machine learning class  and not able to attend in person can follow along via two methods.
  
  Videos  will be posted with about a 1 day delay on  techtalks .  This is a side-by-side capture of video+slides from  Weyond . 
 We are experimenting with  Piazza  as a discussion forum.  Anyone is welcome to subscribe to Piazza and ask questions there, where I will be monitoring things.   update2 : Sign up  here . 
  
The first lecture is up now, including the  revised version of the slides  which fixes a few typos and rounds out references.</p><p>2 0.98587704 <a title="389-lda-2" href="../hunch_net-2010/hunch_net-2010-03-12-Netflix_Challenge_2_Canceled.html">390 hunch net-2010-03-12-Netflix Challenge 2 Canceled</a></p>
<p>Introduction: The  second Netflix prize is canceled  due to  privacy problems .  I continue to believe my original assessment of this paper, that the privacy break was somewhat overstated.  I still haven’t seen any serious privacy failures on the scale of the  AOL search log release .
 
I expect privacy concerns to continue to be a big issue when dealing with data releases by companies or governments.  The theory of maintaining privacy while using data is improving, but it is not yet in a state where the limits of what’s possible are clear let alone how to achieve these limits in a manner friendly to a prediction competition.</p><p>3 0.96998632 <a title="389-lda-3" href="../hunch_net-2008/hunch_net-2008-10-01-NIPS_2008_workshop_on_%26%238216%3BLearning_over_Empirical_Hypothesis_Spaces%26%238217%3B.html">319 hunch net-2008-10-01-NIPS 2008 workshop on &#8216;Learning over Empirical Hypothesis Spaces&#8217;</a></p>
<p>Introduction: This workshop asks for insights how far we may/can push the theoretical boundary of using data in the design of learning machines. Can we express our classification rule in terms of the sample, or do we have to stick to a core assumption of classical statistical learning theory, namely that the hypothesis space is to be defined independent from the sample? This workshop is particularly interested in – but not restricted to – the ‘luckiness framework’ and the recently introduced  notion of ‘compatibility functions’ in a semi-supervised learning context (more information can be found at  http://www.kuleuven.be/wehys ).</p><p>4 0.95182276 <a title="389-lda-4" href="../hunch_net-2005/hunch_net-2005-02-25-Why_Papers%3F.html">30 hunch net-2005-02-25-Why Papers?</a></p>
<p>Introduction: Makc asked a good  question  in comments—”Why bother to make a paper, at all?”  There are several reasons for writing papers which may not be immediately obvious to people not in academia.
 
The basic idea is that papers have considerably more utility than the obvious “present an idea”.
  
 Papers are a formalized units of work. Academics (especially young ones) are often judged on the number of papers they produce. 
 Papers have a formalized method of citing and crediting other—the bibliography.  Academics (especially older ones) are often judged on the number of citations they receive. 
 Papers enable a “more fair” anonymous review.  Conferences receive  many  papers, from which a subset are selected.  Discussion forums are inherently not anonymous for anyone who wants to build a reputation for good work. 
 Papers are an excuse to meet your friends.  Papers are the content of conferences, but much of what you do is talk to friends about interesting problems while there.  Sometimes yo</p><p>same-blog 5 0.93100131 <a title="389-lda-5" href="../hunch_net-2010/hunch_net-2010-02-26-Yahoo%21_ML_events.html">389 hunch net-2010-02-26-Yahoo! ML events</a></p>
<p>Introduction: Yahoo! is sponsoring two machine learning events that might interest people.  
  
 The  Key Scientific Challenges  program (due March 5) for  Machine Learning  and  Statistics  offers $5K (plus bonuses) for graduate students working on a core problem of interest to Y!  If you are already working on one of these problems, there is no reason not to submit, and if you arenâ&euro;&trade;t you might want to think about it for next year, as I am confident they all press the boundary of the possible in Machine Learning.   There are 7 days left. 
 The  Learning to Rank challenge  (due May 31) offers an $8K first prize for the best ranking algorithm on a real (and really used) dataset for search ranking, with presentations at an ICML workshop.  Unlike the Netflix competition, there are prizes for 2nd, 3rd, and 4th place, perhaps avoiding the heartbreak  the ensemble  encountered.  If you think you know how to rank, you should give it a try, and we might all learn something.  There are 3 months left.</p><p>6 0.8764652 <a title="389-lda-6" href="../hunch_net-2012/hunch_net-2012-02-24-ICML%2B50%25.html">456 hunch net-2012-02-24-ICML+50%</a></p>
<p>7 0.86758482 <a title="389-lda-7" href="../hunch_net-2005/hunch_net-2005-11-02-Progress_in_Active_Learning.html">127 hunch net-2005-11-02-Progress in Active Learning</a></p>
<p>8 0.80962098 <a title="389-lda-8" href="../hunch_net-2009/hunch_net-2009-02-22-Effective_Research_Funding.html">344 hunch net-2009-02-22-Effective Research Funding</a></p>
<p>9 0.78106761 <a title="389-lda-9" href="../hunch_net-2009/hunch_net-2009-10-03-Static_vs._Dynamic_multiclass_prediction.html">373 hunch net-2009-10-03-Static vs. Dynamic multiclass prediction</a></p>
<p>10 0.7750867 <a title="389-lda-10" href="../hunch_net-2012/hunch_net-2012-04-20-Both_new%3A_STOC_workshops_and_NEML.html">462 hunch net-2012-04-20-Both new: STOC workshops and NEML</a></p>
<p>11 0.71962851 <a title="389-lda-11" href="../hunch_net-2007/hunch_net-2007-02-22-Create_Your_Own_ICML_Workshop.html">234 hunch net-2007-02-22-Create Your Own ICML Workshop</a></p>
<p>12 0.69977236 <a title="389-lda-12" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<p>13 0.57622504 <a title="389-lda-13" href="../hunch_net-2005/hunch_net-2005-01-31-Watchword%3A_Assumption.html">7 hunch net-2005-01-31-Watchword: Assumption</a></p>
<p>14 0.54079187 <a title="389-lda-14" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>15 0.53976953 <a title="389-lda-15" href="../hunch_net-2012/hunch_net-2012-02-20-Berkeley_Streaming_Data_Workshop.html">455 hunch net-2012-02-20-Berkeley Streaming Data Workshop</a></p>
<p>16 0.53835344 <a title="389-lda-16" href="../hunch_net-2011/hunch_net-2011-09-28-Somebody%26%238217%3Bs_Eating_Your_Lunch.html">445 hunch net-2011-09-28-Somebody&#8217;s Eating Your Lunch</a></p>
<p>17 0.53131974 <a title="389-lda-17" href="../hunch_net-2007/hunch_net-2007-11-29-The_Netflix_Crack.html">275 hunch net-2007-11-29-The Netflix Crack</a></p>
<p>18 0.51789701 <a title="389-lda-18" href="../hunch_net-2006/hunch_net-2006-11-02-2006_NIPS_workshops.html">216 hunch net-2006-11-02-2006 NIPS workshops</a></p>
<p>19 0.51786745 <a title="389-lda-19" href="../hunch_net-2005/hunch_net-2005-03-05-Funding_Research.html">36 hunch net-2005-03-05-Funding Research</a></p>
<p>20 0.51722622 <a title="389-lda-20" href="../hunch_net-2012/hunch_net-2012-06-05-ICML_acceptance_statistics.html">466 hunch net-2012-06-05-ICML acceptance statistics</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
