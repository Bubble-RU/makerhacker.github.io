<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>387 hunch net-2010-01-19-Deadline Season, 2010</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2010" href="../home/hunch_net-2010_home.html">hunch_net-2010</a> <a title="hunch_net-2010-387" href="#">hunch_net-2010-387</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>387 hunch net-2010-01-19-Deadline Season, 2010</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2010-387-html" href="http://hunch.net/?p=1182">html</a></p><p>Introduction: Many conference deadlines are coming soon.DeadlineDouble Blind / Author
FeedbackTime/PlaceICMLJanuary 18((workshops) / February 1 (Papers) / February
13 (Tutorials)Y/YHaifa, Israel, June 21-25KDDFebruary 1(Workshops) / February
2&5 (Papers) / February 26 (Tutorials & Panels)) / April 17
(Demos)N/SWashington DC, July 25-28COLTJanuary 18 (Workshops) / February 19
(Papers)N/SHaifa, Israel, June 25-29UAIMarch 11 (Papers)N?/YCatalina Island,
California, July 8-11ICML continues to experiment with the reviewing process,
although perhaps less so than last year.The S "sort-of" for COLT is because
author feedback occurs only after decisions are made.KDD is notable for being
the most comprehensive in terms of {Tutorials, Workshops, Challenges, Panels,
Papers (two tracks), Demos}. The S for KDD is because there is sometimes
author feedback at the decision of the SPC.The (past) January 18 deadline for
workshops at ICML is nominal, as I (as workshop chair) almost missed it myself
and we have space f</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('february', 0.439), ('workshops', 0.302), ('israel', 0.264), ('panels', 0.264), ('tutorials', 0.249), ('demos', 0.195), ('papers', 0.193), ('june', 0.181), ('july', 0.175), ('missed', 0.162), ('author', 0.16), ('chair', 0.143), ('deadline', 0.131), ('island', 0.117), ('tracks', 0.117), ('spc', 0.109), ('interpreted', 0.109), ('dc', 0.109), ('feedback', 0.107), ('oops', 0.102)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="387-tfidf-1" href="../hunch_net-2010/hunch_net-2010-01-19-Deadline_Season%2C_2010.html">387 hunch net-2010-01-19-Deadline Season, 2010</a></p>
<p>Introduction: Many conference deadlines are coming soon.DeadlineDouble Blind / Author
FeedbackTime/PlaceICMLJanuary 18((workshops) / February 1 (Papers) / February
13 (Tutorials)Y/YHaifa, Israel, June 21-25KDDFebruary 1(Workshops) / February
2&5 (Papers) / February 26 (Tutorials & Panels)) / April 17
(Demos)N/SWashington DC, July 25-28COLTJanuary 18 (Workshops) / February 19
(Papers)N/SHaifa, Israel, June 25-29UAIMarch 11 (Papers)N?/YCatalina Island,
California, July 8-11ICML continues to experiment with the reviewing process,
although perhaps less so than last year.The S "sort-of" for COLT is because
author feedback occurs only after decisions are made.KDD is notable for being
the most comprehensive in terms of {Tutorials, Workshops, Challenges, Panels,
Papers (two tracks), Demos}. The S for KDD is because there is sometimes
author feedback at the decision of the SPC.The (past) January 18 deadline for
workshops at ICML is nominal, as I (as workshop chair) almost missed it myself
and we have space f</p><p>2 0.30326903 <a title="387-tfidf-2" href="../hunch_net-2009/hunch_net-2009-11-23-ICML_2009_Workshops_%28and_Tutorials%29.html">379 hunch net-2009-11-23-ICML 2009 Workshops (and Tutorials)</a></p>
<p>Introduction: I'm theworkshops chairforICMLthis year. As such, I would like to personally
encourage people to consider running a workshop.My general view of workshops
is that they are excellent as opportunities to discuss and develop research
directions--some of my best work has come from collaborations at workshops and
several workshops have substantially altered my thinking about various
problems. My experience running workshops is that setting them up and making
them fly often appears much harder than it actually is, and the workshops
often come off much better than expected in the end. Submissions are due
January 18, two weeks before papers.Similarly,Ben Taskaris looking for
goodtutorials, which is complementary. Workshops are about exploring a
subject, while a tutorial is about distilling it down into an easily taught
essence, a vital part of the research process. Tutorials are due February 13,
two weeks after papers.</p><p>3 0.26811177 <a title="387-tfidf-3" href="../hunch_net-2005/hunch_net-2005-02-02-Paper_Deadlines.html">11 hunch net-2005-02-02-Paper Deadlines</a></p>
<p>Introduction: It's conference season, and smell of budding papers is in the air.IJCAI 2005,
January 21COLT 2005, February 2KDD 2005, February 18ICML 2005, March 8UAI
2005, March 16AAAI 2005, March 18</p><p>4 0.23092456 <a title="387-tfidf-4" href="../hunch_net-2005/hunch_net-2005-11-16-MLSS_2006.html">130 hunch net-2005-11-16-MLSS 2006</a></p>
<p>Introduction: There will be twomachine learning summer schoolsin 2006.One is inCanberra,
Australiafrom February 6 to February 17 (Aussie summer). The webpage is fully
'live' so you should actively consider it now.The other is inTaipei,
Taiwanfrom July 24 to August 4. This one is still in the planning phase, but
that should be settled soon.Attending an MLSS is probably the quickest and
easiest way to bootstrap yourself into a reasonable initial understanding of
the field of machine learning.</p><p>5 0.22806975 <a title="387-tfidf-5" href="../hunch_net-2007/hunch_net-2007-01-04-2007_Summer_Machine_Learning_Conferences.html">226 hunch net-2007-01-04-2007 Summer Machine Learning Conferences</a></p>
<p>Introduction: It's conference season once again.ConferenceDue?When?Where?double blind?author
feedback?Workshops?AAAIFebruary 1/6 (and 27)July 22-26Vancouver, British
ColumbiaYesYesDoneUAIFebruary 28/March 2July 19-22Vancouver, British
ColumbiaNoNoNoCOLTJanuary 16June 13-15San Diego, California
(withFCRC)NoNoNoICMLFebruary 7/9June 20-24Corvallis, OregonYesYesFebruary
16KDDFebruary 23/28August 12-15San Jose, CaliforniaYesNo?February 28The
geowinner this year is the west coast of North America.Last year's geowinner
was the Northeastern US, and theyear beforeit was mostly Europe. It's notable
how tightly the conferences cluster, even when they don't colocate.</p><p>6 0.1872731 <a title="387-tfidf-6" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>7 0.17153488 <a title="387-tfidf-7" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>8 0.17113884 <a title="387-tfidf-8" href="../hunch_net-2005/hunch_net-2005-05-14-NIPS.html">71 hunch net-2005-05-14-NIPS</a></p>
<p>9 0.16241765 <a title="387-tfidf-9" href="../hunch_net-2012/hunch_net-2012-08-27-NYAS_ML_2012_and_ICML_2013.html">472 hunch net-2012-08-27-NYAS ML 2012 and ICML 2013</a></p>
<p>10 0.15756862 <a title="387-tfidf-10" href="../hunch_net-2005/hunch_net-2005-03-24-The_Role_of_Workshops.html">46 hunch net-2005-03-24-The Role of Workshops</a></p>
<p>11 0.15500496 <a title="387-tfidf-11" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>12 0.1536302 <a title="387-tfidf-12" href="../hunch_net-2008/hunch_net-2008-11-11-COLT_CFP.html">326 hunch net-2008-11-11-COLT CFP</a></p>
<p>13 0.1480899 <a title="387-tfidf-13" href="../hunch_net-2008/hunch_net-2008-12-12-Summer_Conferences.html">331 hunch net-2008-12-12-Summer Conferences</a></p>
<p>14 0.14445232 <a title="387-tfidf-14" href="../hunch_net-2011/hunch_net-2011-10-10-ML_Symposium_and_ICML_details.html">447 hunch net-2011-10-10-ML Symposium and ICML details</a></p>
<p>15 0.14289701 <a title="387-tfidf-15" href="../hunch_net-2008/hunch_net-2008-01-23-Why_Workshop%3F.html">285 hunch net-2008-01-23-Why Workshop?</a></p>
<p>16 0.13901156 <a title="387-tfidf-16" href="../hunch_net-2005/hunch_net-2005-05-02-Reviewing_techniques_for_conferences.html">65 hunch net-2005-05-02-Reviewing techniques for conferences</a></p>
<p>17 0.13533209 <a title="387-tfidf-17" href="../hunch_net-2009/hunch_net-2009-05-30-Many_ways_to_Learn_this_summer.html">357 hunch net-2009-05-30-Many ways to Learn this summer</a></p>
<p>18 0.12572479 <a title="387-tfidf-18" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<p>19 0.12181559 <a title="387-tfidf-19" href="../hunch_net-2006/hunch_net-2006-06-15-IJCAI_is_out_of_season.html">184 hunch net-2006-06-15-IJCAI is out of season</a></p>
<p>20 0.11989737 <a title="387-tfidf-20" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.146), (1, 0.273), (2, -0.035), (3, 0.303), (4, -0.041), (5, -0.123), (6, -0.08), (7, 0.036), (8, -0.058), (9, 0.023), (10, 0.01), (11, -0.231), (12, -0.025), (13, -0.032), (14, 0.089), (15, -0.105), (16, 0.011), (17, -0.077), (18, 0.014), (19, -0.148), (20, 0.008), (21, 0.031), (22, -0.036), (23, -0.108), (24, 0.045), (25, -0.098), (26, 0.046), (27, 0.083), (28, -0.025), (29, 0.162), (30, 0.003), (31, 0.112), (32, -0.045), (33, -0.078), (34, -0.032), (35, -0.034), (36, 0.035), (37, 0.039), (38, 0.042), (39, -0.096), (40, -0.056), (41, 0.039), (42, 0.034), (43, 0.0), (44, -0.061), (45, -0.029), (46, 0.021), (47, 0.041), (48, 0.028), (49, -0.017)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97976178 <a title="387-lsi-1" href="../hunch_net-2010/hunch_net-2010-01-19-Deadline_Season%2C_2010.html">387 hunch net-2010-01-19-Deadline Season, 2010</a></p>
<p>Introduction: Many conference deadlines are coming soon.DeadlineDouble Blind / Author
FeedbackTime/PlaceICMLJanuary 18((workshops) / February 1 (Papers) / February
13 (Tutorials)Y/YHaifa, Israel, June 21-25KDDFebruary 1(Workshops) / February
2&5 (Papers) / February 26 (Tutorials & Panels)) / April 17
(Demos)N/SWashington DC, July 25-28COLTJanuary 18 (Workshops) / February 19
(Papers)N/SHaifa, Israel, June 25-29UAIMarch 11 (Papers)N?/YCatalina Island,
California, July 8-11ICML continues to experiment with the reviewing process,
although perhaps less so than last year.The S "sort-of" for COLT is because
author feedback occurs only after decisions are made.KDD is notable for being
the most comprehensive in terms of {Tutorials, Workshops, Challenges, Panels,
Papers (two tracks), Demos}. The S for KDD is because there is sometimes
author feedback at the decision of the SPC.The (past) January 18 deadline for
workshops at ICML is nominal, as I (as workshop chair) almost missed it myself
and we have space f</p><p>2 0.72932768 <a title="387-lsi-2" href="../hunch_net-2007/hunch_net-2007-01-04-2007_Summer_Machine_Learning_Conferences.html">226 hunch net-2007-01-04-2007 Summer Machine Learning Conferences</a></p>
<p>Introduction: It's conference season once again.ConferenceDue?When?Where?double blind?author
feedback?Workshops?AAAIFebruary 1/6 (and 27)July 22-26Vancouver, British
ColumbiaYesYesDoneUAIFebruary 28/March 2July 19-22Vancouver, British
ColumbiaNoNoNoCOLTJanuary 16June 13-15San Diego, California
(withFCRC)NoNoNoICMLFebruary 7/9June 20-24Corvallis, OregonYesYesFebruary
16KDDFebruary 23/28August 12-15San Jose, CaliforniaYesNo?February 28The
geowinner this year is the west coast of North America.Last year's geowinner
was the Northeastern US, and theyear beforeit was mostly Europe. It's notable
how tightly the conferences cluster, even when they don't colocate.</p><p>3 0.69846714 <a title="387-lsi-3" href="../hunch_net-2009/hunch_net-2009-11-23-ICML_2009_Workshops_%28and_Tutorials%29.html">379 hunch net-2009-11-23-ICML 2009 Workshops (and Tutorials)</a></p>
<p>Introduction: I'm theworkshops chairforICMLthis year. As such, I would like to personally
encourage people to consider running a workshop.My general view of workshops
is that they are excellent as opportunities to discuss and develop research
directions--some of my best work has come from collaborations at workshops and
several workshops have substantially altered my thinking about various
problems. My experience running workshops is that setting them up and making
them fly often appears much harder than it actually is, and the workshops
often come off much better than expected in the end. Submissions are due
January 18, two weeks before papers.Similarly,Ben Taskaris looking for
goodtutorials, which is complementary. Workshops are about exploring a
subject, while a tutorial is about distilling it down into an easily taught
essence, a vital part of the research process. Tutorials are due February 13,
two weeks after papers.</p><p>4 0.61797106 <a title="387-lsi-4" href="../hunch_net-2007/hunch_net-2007-10-15-NIPS_workshops_extended_to_3_days.html">266 hunch net-2007-10-15-NIPS workshops extended to 3 days</a></p>
<p>Introduction: (Unofficially, at least.) TheDeep Learning Workshopis being held the afternoon
before the rest of the workshops in Vancouver, BC. Separate registration is
needed, and open.What's happening fundamentally here is that there are too
many interesting workshops to fit into 2 days. Perhaps we can get it
officially expanded to 3 days next year.</p><p>5 0.596039 <a title="387-lsi-5" href="../hunch_net-2005/hunch_net-2005-05-14-NIPS.html">71 hunch net-2005-05-14-NIPS</a></p>
<p>Introduction: NIPSis the big winter conference of learning.Paper due date: June 3rd.
(Tweaked thanks toFei Sha.)Location: Vancouver (main program) Dec. 5-8 and
Whistler (workshops) Dec 9-10, BC, CanadaNIPS is larger than all of the other
learning conferences, partly because it's the only one at that time of year. I
recommend the workshops which are often quite interesting and energetic.</p><p>6 0.54712826 <a title="387-lsi-6" href="../hunch_net-2005/hunch_net-2005-02-02-Paper_Deadlines.html">11 hunch net-2005-02-02-Paper Deadlines</a></p>
<p>7 0.54202867 <a title="387-lsi-7" href="../hunch_net-2006/hunch_net-2006-06-15-IJCAI_is_out_of_season.html">184 hunch net-2006-06-15-IJCAI is out of season</a></p>
<p>8 0.5309999 <a title="387-lsi-8" href="../hunch_net-2008/hunch_net-2008-01-23-Why_Workshop%3F.html">285 hunch net-2008-01-23-Why Workshop?</a></p>
<p>9 0.50467789 <a title="387-lsi-9" href="../hunch_net-2005/hunch_net-2005-11-16-MLSS_2006.html">130 hunch net-2005-11-16-MLSS 2006</a></p>
<p>10 0.48459464 <a title="387-lsi-10" href="../hunch_net-2008/hunch_net-2008-12-12-Summer_Conferences.html">331 hunch net-2008-12-12-Summer Conferences</a></p>
<p>11 0.45596084 <a title="387-lsi-11" href="../hunch_net-2011/hunch_net-2011-01-16-2011_Summer_Conference_Deadline_Season.html">422 hunch net-2011-01-16-2011 Summer Conference Deadline Season</a></p>
<p>12 0.45155504 <a title="387-lsi-12" href="../hunch_net-2008/hunch_net-2008-11-11-COLT_CFP.html">326 hunch net-2008-11-11-COLT CFP</a></p>
<p>13 0.44671223 <a title="387-lsi-13" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>14 0.42587343 <a title="387-lsi-14" href="../hunch_net-2012/hunch_net-2012-08-27-NYAS_ML_2012_and_ICML_2013.html">472 hunch net-2012-08-27-NYAS ML 2012 and ICML 2013</a></p>
<p>15 0.42559776 <a title="387-lsi-15" href="../hunch_net-2011/hunch_net-2011-10-10-ML_Symposium_and_ICML_details.html">447 hunch net-2011-10-10-ML Symposium and ICML details</a></p>
<p>16 0.41917473 <a title="387-lsi-16" href="../hunch_net-2013/hunch_net-2013-05-04-COLT_and_ICML_registration.html">482 hunch net-2013-05-04-COLT and ICML registration</a></p>
<p>17 0.4097608 <a title="387-lsi-17" href="../hunch_net-2012/hunch_net-2012-07-17-MUCMD_and_BayLearn.html">470 hunch net-2012-07-17-MUCMD and BayLearn</a></p>
<p>18 0.40315974 <a title="387-lsi-18" href="../hunch_net-2005/hunch_net-2005-03-24-The_Role_of_Workshops.html">46 hunch net-2005-03-24-The Role of Workshops</a></p>
<p>19 0.39941862 <a title="387-lsi-19" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>20 0.39712822 <a title="387-lsi-20" href="../hunch_net-2009/hunch_net-2009-05-30-Many_ways_to_Learn_this_summer.html">357 hunch net-2009-05-30-Many ways to Learn this summer</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.052), (42, 0.142), (57, 0.017), (74, 0.144), (82, 0.076), (83, 0.036), (93, 0.304), (95, 0.105)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.92396241 <a title="387-lda-1" href="../hunch_net-2006/hunch_net-2006-04-09-Progress_in_Machine_Translation.html">171 hunch net-2006-04-09-Progress in Machine Translation</a></p>
<p>Introduction: I just visitedISIwhereDaniel Marcuand others are working on machine
translation. Apparently, machine translation is rapidly improving. A
particularly dramatic year was 2002->2003 when systems switched from word-
based translation to phrase-based translation. From a (now famous) slide by
Charles Wayne atDARPA(which funds much of the work on machine translation)
here is some anecdotal evidence:20022003insistent Wednesday may recurred her
trips to Libya tomorrow for flying.Cairo 6-4 ( AFP ) - An official announced
today in the Egyptian lines company for flying Tuesday is a company "insistent
for flying" may resumed a consideration of a day Wednesday tomorrow her trips
to Libya of Security Council decision trace international the imposed ban
comment.And said the official "the institution sent a speech to Ministry of
Foreign Affairs of lifting on Libya air, a situation her recieving replying
are so a trip will pull to Libya a morning Wednesday."Egyptair has tomorrow to
Resume Its flight to</p><p>same-blog 2 0.87894481 <a title="387-lda-2" href="../hunch_net-2010/hunch_net-2010-01-19-Deadline_Season%2C_2010.html">387 hunch net-2010-01-19-Deadline Season, 2010</a></p>
<p>Introduction: Many conference deadlines are coming soon.DeadlineDouble Blind / Author
FeedbackTime/PlaceICMLJanuary 18((workshops) / February 1 (Papers) / February
13 (Tutorials)Y/YHaifa, Israel, June 21-25KDDFebruary 1(Workshops) / February
2&5 (Papers) / February 26 (Tutorials & Panels)) / April 17
(Demos)N/SWashington DC, July 25-28COLTJanuary 18 (Workshops) / February 19
(Papers)N/SHaifa, Israel, June 25-29UAIMarch 11 (Papers)N?/YCatalina Island,
California, July 8-11ICML continues to experiment with the reviewing process,
although perhaps less so than last year.The S "sort-of" for COLT is because
author feedback occurs only after decisions are made.KDD is notable for being
the most comprehensive in terms of {Tutorials, Workshops, Challenges, Panels,
Papers (two tracks), Demos}. The S for KDD is because there is sometimes
author feedback at the decision of the SPC.The (past) January 18 deadline for
workshops at ICML is nominal, as I (as workshop chair) almost missed it myself
and we have space f</p><p>3 0.79618692 <a title="387-lda-3" href="../hunch_net-2005/hunch_net-2005-04-28-Science_Fiction_and_Research.html">64 hunch net-2005-04-28-Science Fiction and Research</a></p>
<p>Introduction: A big part of doing research is imagining how things could be different, and
then trying to figure out how to get there.A big part of science fiction is
imagining how things could be different, and then working through the
implications.Because of the similarity here, reading science fiction can
sometimes be helpful in understanding and doing research. (And, hey, it's
fun.) Here's some list of science fiction books I enjoyed which seem
particularly relevant to computer science and (sometimes) learning
systems:Vernor Vinge, "True Names", "A Fire Upon the Deep"Marc Stiegler,
"David's Sling", "Earthweb"Charles Stross, "Singularity Sky"Greg Egan,
"Diaspora"Joe Haldeman, "Forever Peace"(There are surely many
others.)Incidentally, the nature of science fiction itself has changed.
Decades ago, science fiction projected great increases in the power humans
control (example: E.E. Smith Lensman series). That didn't really happen in the
last 50 years. Instead, we gradually refined the degree to whi</p><p>4 0.71083963 <a title="387-lda-4" href="../hunch_net-2006/hunch_net-2006-04-06-Bounds_greater_than_1.html">170 hunch net-2006-04-06-Bounds greater than 1</a></p>
<p>Introduction: Nati SrebroandShai Ben-Davidhave apaperatCOLTwhich, in the appendix, proves
something very striking: several previous error bounds arealwaysgreater than
1.BackgroundOne branch of learning theory focuses on theorems whichAssume
samples are drawn IID from an unknown distributionD.Fix a set of
classifiersFind a high probability bound on the maximum true error rate (with
respect toD) as a function of the empirical error rate on the training
set.Many of these bounds become extremely complex and hairy.CurrentEveryone
working on this subject wants "tighter bounds", however there are different
definitions of "tighter". Some groups focus on "functional tightness" (getting
the right functional dependency between the size of the training set and a
parameterization of the hypothesis space) whileothersfocus on "practical
tightness" (finding bounds which work well on practical problems). (I am
definitely in the second camp.)One of the dangers of striving for "functional
tightness" is that the bound</p><p>5 0.66255736 <a title="387-lda-5" href="../hunch_net-2006/hunch_net-2006-07-13-Regression_vs._Classification_as_a_Primitive.html">196 hunch net-2006-07-13-Regression vs. Classification as a Primitive</a></p>
<p>Introduction: Forlearning reductionswe have been concentrating on reducing various complex
learning problems to binary classification. This choice needs to be actively
questioned, because it was not carefully considered.Binary clasification is
learning a classifierc:X -> {0,1}so as to minimize the probability of being
wrong,Prx,y~D(c(x)y).The primary alternative candidate seems to be squared
error regression. In squared error regression, you learn a regressors:X ->
[0,1]so as to minimize squared error,Ex,y~D(s(x)-y)2.It is difficult to judge
one primitive against another. The judgement must at least partially be made
on nontheoretical grounds because (essentially) we are evaluating a choice
between two axioms/assumptions.These two primitives are significantly related.
Classification can be reduced to regression in the obvious way: you use the
regressor to predictD(y=1|x), then threshold at0.5. For this simple reduction
a squared error regret ofrimplies a classification regret of at mostr0.5.
Regress</p><p>6 0.59781951 <a title="387-lda-6" href="../hunch_net-2012/hunch_net-2012-04-09-ICML_author_feedback_is_open.html">461 hunch net-2012-04-09-ICML author feedback is open</a></p>
<p>7 0.57251698 <a title="387-lda-7" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>8 0.5655148 <a title="387-lda-8" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>9 0.56361389 <a title="387-lda-9" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>10 0.56147224 <a title="387-lda-10" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>11 0.55589002 <a title="387-lda-11" href="../hunch_net-2006/hunch_net-2006-03-02-Why_do_people_count_for_learning%3F.html">160 hunch net-2006-03-02-Why do people count for learning?</a></p>
<p>12 0.55536461 <a title="387-lda-12" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>13 0.55057615 <a title="387-lda-13" href="../hunch_net-2010/hunch_net-2010-04-26-Compassionate_Reviewing.html">395 hunch net-2010-04-26-Compassionate Reviewing</a></p>
<p>14 0.55027223 <a title="387-lda-14" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>15 0.54836023 <a title="387-lda-15" href="../hunch_net-2006/hunch_net-2006-12-04-Structural_Problems_in_NIPS_Decision_Making.html">221 hunch net-2006-12-04-Structural Problems in NIPS Decision Making</a></p>
<p>16 0.54653919 <a title="387-lda-16" href="../hunch_net-2012/hunch_net-2012-06-29-ICML_survey_and_comments.html">468 hunch net-2012-06-29-ICML survey and comments</a></p>
<p>17 0.54517943 <a title="387-lda-17" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>18 0.54194599 <a title="387-lda-18" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>19 0.54167181 <a title="387-lda-19" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>20 0.53580141 <a title="387-lda-20" href="../hunch_net-2005/hunch_net-2005-03-13-Avoiding_Bad_Reviewing.html">40 hunch net-2005-03-13-Avoiding Bad Reviewing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
