<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>420 hunch net-2010-12-26-NIPS 2010</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2010" href="../home/hunch_net-2010_home.html">hunch_net-2010</a> <a title="hunch_net-2010-420" href="#">hunch_net-2010-420</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>420 hunch net-2010-12-26-NIPS 2010</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2010-420-html" href="http://hunch.net/?p=1614">html</a></p><p>Introduction: I enjoyed attendingNIPSthis year, with several things interesting me. For the
conference itself:Peter Welinder,Steve Branson,Serge Belongie, andPietro
Perona,The Multidimensional Wisdom of Crowds. This paper is about
usingmechanical turkto get label information, with results superior to a
majority vote approach.David McAllester,Tamir Hazan, andJoseph KeshetDirect
Loss Minimization for Structured Prediction. This is about another technique
for directly optimizing the loss in structured prediction, with an application
to speech recognition.Mohammad SaberianandNuno VasconcelosBoosting Classifier
Cascades. This is about an algorithm for simultaneously optimizing loss and
computation in a classifier cascade construction. There were several other
papers on cascades which are worth looking at if interested.Alan FernandPrasad
Tadepalli,A Computational Decision Theory for Interactive Assistants. This
paper carves out some forms of natural not-MDP problems and shows their RL-
style solution is t</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('partition', 0.226), ('sublinearly', 0.203), ('differential', 0.167), ('wisdom', 0.158), ('absolute', 0.158), ('leverage', 0.151), ('moving', 0.14), ('agreement', 0.135), ('fast', 0.129), ('scale', 0.128), ('amounts', 0.125), ('features', 0.118), ('optimizing', 0.111), ('loss', 0.11), ('structured', 0.107), ('search', 0.098), ('hope', 0.098), ('implying', 0.097), ('belief', 0.095), ('results', 0.095)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="420-tfidf-1" href="../hunch_net-2010/hunch_net-2010-12-26-NIPS_2010.html">420 hunch net-2010-12-26-NIPS 2010</a></p>
<p>Introduction: I enjoyed attendingNIPSthis year, with several things interesting me. For the
conference itself:Peter Welinder,Steve Branson,Serge Belongie, andPietro
Perona,The Multidimensional Wisdom of Crowds. This paper is about
usingmechanical turkto get label information, with results superior to a
majority vote approach.David McAllester,Tamir Hazan, andJoseph KeshetDirect
Loss Minimization for Structured Prediction. This is about another technique
for directly optimizing the loss in structured prediction, with an application
to speech recognition.Mohammad SaberianandNuno VasconcelosBoosting Classifier
Cascades. This is about an algorithm for simultaneously optimizing loss and
computation in a classifier cascade construction. There were several other
papers on cascades which are worth looking at if interested.Alan FernandPrasad
Tadepalli,A Computational Decision Theory for Interactive Assistants. This
paper carves out some forms of natural not-MDP problems and shows their RL-
style solution is t</p><p>2 0.22393772 <a title="420-tfidf-2" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>Introduction: At NIPS,Andrew Ngasked me what should be in a large scale learning class.
After some discussion with him andNandoand mulling it over a bit, these are
the topics that I think should be covered.There are many different kinds of
scaling.Scaling in examplesThis is the most basic kind of scaling.Online
Gradient DescentThis is an old algorithm--I'm not sure if anyone can be
credited with it in particular. Perhaps thePerceptronis a good precursor, but
substantial improvements come from the notion of a loss function of
whichsquared loss,logistic loss, Hinge Loss, andQuantile Lossare all worth
covering. It's important to cover thesemanticsof these loss functions as
well.Vowpal Wabbitis a reasonably fast codebase implementing these.Second
Order Gradient Descent methodsFor some problems, methods taking into account
second derivative information can be more effective. I've seen preconditioned
conjugate gradient work well, for whichJonathan Shewchuck'swriteupis
reasonable. Nando likesL-BFGSwhich I</p><p>3 0.16648819 <a title="420-tfidf-3" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>Introduction: Many people in Machine Learning don't fully understand the impact of
computation, as demonstrated by a lack ofbig-Oanalysis of new learning
algorithms. This is important--some current active research programs are
fundamentally flawed w.r.t. computation, and other research programs are
directly motivated by it. When considering a learning algorithm, I think about
the following questions:How does the learning algorithm scale with the number
of examplesm? Any algorithm using all of the data is at leastO(m), but in many
cases this isO(m2)(naive nearest neighbor for self-prediction) or unknown
(k-means or many other optimization algorithms). The unknown case is very
common, and it can mean (for example) that the algorithm isn't convergent or
simply that the amount of computation isn't controlled.The above question can
also be asked for test cases. In some applications, test-time performance is
of great importance.How does the algorithm scale with the number of
featuresnper example? Many sec</p><p>4 0.14638726 <a title="420-tfidf-4" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>Introduction: Muthuinvited me to the workshop onalgorithms in the field, with the goal of
providing a sense of where near-term research should go. When the time came
though, I bargained for a post instead, which provides a chance for many other
people to comment.There are several things I didn't fully understand when I
went to Yahoo! about 5 years ago. I'd like to repeat them as people in
academia may not yet understand them intuitively.Almost all the big impact
algorithms operate in pseudo-linear or better time. Think about caching,
hashing, sorting, filtering, etcâ&euro;Ś and you have a sense of what some of the
most heavily used algorithms are. This matters quite a bit to Machine Learning
research, because people often work with superlinear time algorithms and
languages. Two very common examples of this are graphical models, where
inference is often a superlinear operation--think about then2dependence on the
number of states in aHidden Markov Modeland KernelizedSupport Vector
Machineswhere optimization</p><p>5 0.14333972 <a title="420-tfidf-5" href="../hunch_net-2005/hunch_net-2005-06-08-Question%3A_%26%238220%3BWhen_is_the_right_time_to_insert_the_loss_function%3F%26%238221%3B.html">79 hunch net-2005-06-08-Question: &#8220;When is the right time to insert the loss function?&#8221;</a></p>
<p>Introduction: Halasksa very good question: "When is the right time to insert the loss
function?" In particular, should it be used at testing time or at training
time?When the world imposes a loss on us, the standard Bayesian recipe is to
predict the (conditional) probability of each possibility and then choose the
possibility which minimizes the expected loss. In contrast, as
theconfusionover "loss = money lost" or "loss = the thing you optimize" might
indicate, many people ignore the Bayesian approach and simply optimize their
loss (or a close proxy for their loss) over the representation on the training
set.The best answer I can give is "it's unclear, but I prefer optimizing the
loss at training time". My experience is that optimizing the loss in the most
direct manner possible typically yields best performance. This question is
related to a basic principle which bothYann LeCun(applied) andVladimir
Vapnik(theoretical) advocate: "solve the simplest prediction problem that
solves the problem". (One</p><p>6 0.13971485 <a title="420-tfidf-6" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>7 0.13949285 <a title="420-tfidf-7" href="../hunch_net-2007/hunch_net-2007-12-12-Workshop_Summary%26%238212%3BPrinciples_of_Learning_Problem_Design.html">277 hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</a></p>
<p>8 0.13095318 <a title="420-tfidf-8" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>9 0.12942134 <a title="420-tfidf-9" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>10 0.12476504 <a title="420-tfidf-10" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>11 0.1242264 <a title="420-tfidf-11" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>12 0.12377696 <a title="420-tfidf-12" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>13 0.1207324 <a title="420-tfidf-13" href="../hunch_net-2007/hunch_net-2007-12-19-Cool_and_interesting_things_seen_at_NIPS.html">279 hunch net-2007-12-19-Cool and interesting things seen at NIPS</a></p>
<p>14 0.12066042 <a title="420-tfidf-14" href="../hunch_net-2005/hunch_net-2005-03-17-Going_all_the_Way%2C_Sometimes.html">42 hunch net-2005-03-17-Going all the Way, Sometimes</a></p>
<p>15 0.12008051 <a title="420-tfidf-15" href="../hunch_net-2009/hunch_net-2009-02-04-Optimal_Proxy_Loss_for_Classification.html">341 hunch net-2009-02-04-Optimal Proxy Loss for Classification</a></p>
<p>16 0.11857468 <a title="420-tfidf-16" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>17 0.11783238 <a title="420-tfidf-17" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>18 0.11680833 <a title="420-tfidf-18" href="../hunch_net-2006/hunch_net-2006-05-05-An_ICML_reject.html">177 hunch net-2006-05-05-An ICML reject</a></p>
<p>19 0.11663096 <a title="420-tfidf-19" href="../hunch_net-2009/hunch_net-2009-03-18-Parallel_ML_primitives.html">346 hunch net-2009-03-18-Parallel ML primitives</a></p>
<p>20 0.11567967 <a title="420-tfidf-20" href="../hunch_net-2008/hunch_net-2008-07-15-Interesting_papers_at_COLT_%28and_a_bit_of_UAI_%26%23038%3B_workshops%29.html">310 hunch net-2008-07-15-Interesting papers at COLT (and a bit of UAI &#038; workshops)</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.296), (1, -0.079), (2, 0.006), (3, 0.117), (4, 0.013), (5, 0.129), (6, -0.007), (7, -0.033), (8, -0.058), (9, 0.049), (10, 0.002), (11, 0.101), (12, 0.031), (13, 0.049), (14, 0.018), (15, -0.006), (16, -0.009), (17, 0.048), (18, 0.043), (19, -0.039), (20, -0.055), (21, -0.009), (22, -0.028), (23, -0.081), (24, -0.01), (25, -0.002), (26, 0.015), (27, 0.017), (28, 0.066), (29, 0.034), (30, -0.01), (31, -0.009), (32, -0.123), (33, 0.065), (34, -0.08), (35, -0.077), (36, -0.004), (37, 0.019), (38, -0.015), (39, -0.015), (40, 0.021), (41, 0.1), (42, -0.03), (43, 0.01), (44, -0.111), (45, -0.028), (46, 0.02), (47, 0.037), (48, 0.057), (49, 0.011)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95899636 <a title="420-lsi-1" href="../hunch_net-2010/hunch_net-2010-12-26-NIPS_2010.html">420 hunch net-2010-12-26-NIPS 2010</a></p>
<p>Introduction: I enjoyed attendingNIPSthis year, with several things interesting me. For the
conference itself:Peter Welinder,Steve Branson,Serge Belongie, andPietro
Perona,The Multidimensional Wisdom of Crowds. This paper is about
usingmechanical turkto get label information, with results superior to a
majority vote approach.David McAllester,Tamir Hazan, andJoseph KeshetDirect
Loss Minimization for Structured Prediction. This is about another technique
for directly optimizing the loss in structured prediction, with an application
to speech recognition.Mohammad SaberianandNuno VasconcelosBoosting Classifier
Cascades. This is about an algorithm for simultaneously optimizing loss and
computation in a classifier cascade construction. There were several other
papers on cascades which are worth looking at if interested.Alan FernandPrasad
Tadepalli,A Computational Decision Theory for Interactive Assistants. This
paper carves out some forms of natural not-MDP problems and shows their RL-
style solution is t</p><p>2 0.80870408 <a title="420-lsi-2" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>Introduction: At NIPS,Andrew Ngasked me what should be in a large scale learning class.
After some discussion with him andNandoand mulling it over a bit, these are
the topics that I think should be covered.There are many different kinds of
scaling.Scaling in examplesThis is the most basic kind of scaling.Online
Gradient DescentThis is an old algorithm--I'm not sure if anyone can be
credited with it in particular. Perhaps thePerceptronis a good precursor, but
substantial improvements come from the notion of a loss function of
whichsquared loss,logistic loss, Hinge Loss, andQuantile Lossare all worth
covering. It's important to cover thesemanticsof these loss functions as
well.Vowpal Wabbitis a reasonably fast codebase implementing these.Second
Order Gradient Descent methodsFor some problems, methods taking into account
second derivative information can be more effective. I've seen preconditioned
conjugate gradient work well, for whichJonathan Shewchuck'swriteupis
reasonable. Nando likesL-BFGSwhich I</p><p>3 0.65207678 <a title="420-lsi-3" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>Introduction: AtKDDI enjoyedStephen Boyd's invited talk about optimization quite a bit.
However, the most interesting talk for me wasDavid Haussler's. His talk
started out with a formidable load of biological complexity. About half-way
through you start wondering, "can this be used to help with cancer?" And at
the end he connects it directly to use with a call to arms for the audience:
cure cancer. The core thesis here is that cancer is a complex set of diseases
which can be distentangled via genetic assays, allowing attacking the specific
signature of individual cancers. However, the data quantity and complex
dependencies within the data require systematic and relatively automatic
prediction and analysis algorithms of the kind that we are best familiar
with.Some of the papers which interested me are:Kai-Wei ChangandDan
Roth,Selective Block Minimization for Faster Convergence of Limited Memory
Large-Scale Linear Models, which is about effectively using a hard-example
cache to speedup learning.Leland</p><p>4 0.62907177 <a title="420-lsi-4" href="../hunch_net-2007/hunch_net-2007-12-12-Workshop_Summary%26%238212%3BPrinciples_of_Learning_Problem_Design.html">277 hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</a></p>
<p>Introduction: This is a summary of theworkshop on Learning Problem DesignwhichAlinaand I ran
atNIPSthis year.The first question many people have is "What is learning
problem design?" This workshop is about admitting that solving learning
problems does not start with labeled data, but rather somewhere before. When
humans are hired to produce labels, this is usually not a serious problem
because you can tell them precisely what semantics you want the labels to
have, and we can fix some set of features in advance. However, when other
methods are used this becomes more problematic. This focus is important for
Machine Learning because there are very large quantities of data which are not
labeled by a hired human.The title of the workshop was a bit ambitious,
because a workshop is not long enough to synthesize a diversity of approaches
into a coherent set of principles. For me, the posters at the end of the
workshop were quite helpful in getting approaches to gel.Here are some answers
to "where do the lab</p><p>5 0.62761956 <a title="420-lsi-5" href="../hunch_net-2007/hunch_net-2007-08-25-The_Privacy_Problem.html">260 hunch net-2007-08-25-The Privacy Problem</a></p>
<p>Introduction: Machine Learning is rising in importance because data is being collected for
all sorts of tasks where it either wasn't previously collected, or for tasks
that did not previously exist. While this is great for Machine Learning, it
has a downside--the massive data collection which is so useful can also lead
to substantial privacy problems.It's important to understand that this is a
much harder problem than many people appreciate. TheAOLdatareleaseis a good
example. To those doing machine learning, the following strategies might be
obvious:Just delete any names or other obviously personally identifiable
information. The logic here seems to be "if I can't easily find the person
then no one can". That doesn't work as demonstrated by the people who were
found circumstantially from the AOL data.… then just hash all the search
terms! The logic here is "if I can't read it, then no one can". It's also
trivially broken by a dictionary attack--just hash all the strings that might
be in the data an</p><p>6 0.59952712 <a title="420-lsi-6" href="../hunch_net-2009/hunch_net-2009-04-02-Asymmophobia.html">348 hunch net-2009-04-02-Asymmophobia</a></p>
<p>7 0.58821619 <a title="420-lsi-7" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>8 0.58466202 <a title="420-lsi-8" href="../hunch_net-2005/hunch_net-2005-12-07-Is_the_Google_way_the_way_for_machine_learning%3F.html">136 hunch net-2005-12-07-Is the Google way the way for machine learning?</a></p>
<p>9 0.58050752 <a title="420-lsi-9" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>10 0.57364553 <a title="420-lsi-10" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>11 0.56478006 <a title="420-lsi-11" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>12 0.56234074 <a title="420-lsi-12" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>13 0.55631071 <a title="420-lsi-13" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>14 0.55598754 <a title="420-lsi-14" href="../hunch_net-2010/hunch_net-2010-05-02-What%26%238217%3Bs_the_difference_between_gambling_and_rewarding_good_prediction%3F.html">397 hunch net-2010-05-02-What&#8217;s the difference between gambling and rewarding good prediction?</a></p>
<p>15 0.54821545 <a title="420-lsi-15" href="../hunch_net-2007/hunch_net-2007-12-21-Vowpal_Wabbit_Code_Release.html">281 hunch net-2007-12-21-Vowpal Wabbit Code Release</a></p>
<p>16 0.54545826 <a title="420-lsi-16" href="../hunch_net-2007/hunch_net-2007-08-12-Exponentiated_Gradient.html">258 hunch net-2007-08-12-Exponentiated Gradient</a></p>
<p>17 0.54389948 <a title="420-lsi-17" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>18 0.54343146 <a title="420-lsi-18" href="../hunch_net-2007/hunch_net-2007-07-06-Idempotent-capable_Predictors.html">253 hunch net-2007-07-06-Idempotent-capable Predictors</a></p>
<p>19 0.54338199 <a title="420-lsi-19" href="../hunch_net-2009/hunch_net-2009-01-21-Nearly_all_natural_problems_require_nonlinearity.html">337 hunch net-2009-01-21-Nearly all natural problems require nonlinearity</a></p>
<p>20 0.53862631 <a title="420-lsi-20" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.024), (35, 0.053), (38, 0.018), (42, 0.224), (44, 0.017), (45, 0.08), (64, 0.271), (68, 0.078), (74, 0.109), (77, 0.012), (95, 0.047)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.90473896 <a title="420-lda-1" href="../hunch_net-2011/hunch_net-2011-08-15-Vowpal_Wabbit_6.0.html">441 hunch net-2011-08-15-Vowpal Wabbit 6.0</a></p>
<p>Introduction: I just releasedVowpal Wabbit 6.0. Since the last version:VW is now 2-3 orders
of magnitude faster at linear learning, primarily thanks toAlekh. Given the
baseline, this is loads of fun, allowing us to easily deal with terafeature
datasets, and dwarfing the scale of any other open source projects. The core
improvement here comes from effective parallelization over kilonode clusters
(eitherHadoopor not). This code is highly scalable, so it even helps with
clusters of size 2 (and doesn't hurt for clusters of size 1). The core
allreduce technique appears widely and easily reused--we've already used it to
parallelize Conjugate Gradient, LBFGS, and two variants of online learning.
We'll be documenting how to do this more thoroughly, but for now
"README_cluster" and associated scripts should provide a good starting
point.The newLBFGScode fromMiroseems to commonly dominate the existing
conjugate gradient code in time/quality tradeoffs.The new matrix factorization
code fromJakeadds a core algor</p><p>2 0.88400358 <a title="420-lda-2" href="../hunch_net-2010/hunch_net-2010-02-26-Yahoo%21_ML_events.html">389 hunch net-2010-02-26-Yahoo! ML events</a></p>
<p>Introduction: Yahoo! is sponsoring two machine learning events that might interest
people.TheKey Scientific Challengesprogram (due March 5) forMachine
LearningandStatisticsoffers $5K (plus bonuses) for graduate students working
on a core problem of interest to Y! If you are already working on one of these
problems, there is no reason not to submit, and if you aren't you might want
to think about it for next year, as I am confident they all press the boundary
of the possible in Machine Learning. There are 7 days left.TheLearning to Rank
challenge(due May 31) offers an $8K first prize for the best ranking algorithm
on a real (and really used) dataset for search ranking, with presentations at
an ICML workshop. Unlike the Netflix competition, there are prizes for 2nd,
3rd, and 4th place, perhaps avoiding the heartbreakthe ensembleencountered. If
you think you know how to rank, you should give it a try, and we might all
learn something. There are 3 months left.</p><p>3 0.88186389 <a title="420-lda-3" href="../hunch_net-2008/hunch_net-2008-05-25-Inappropriate_Mathematics_for_Machine_Learning.html">302 hunch net-2008-05-25-Inappropriate Mathematics for Machine Learning</a></p>
<p>Introduction: Reviewers and students are sometimes greatly concerned by the distinction
between:Anopen setand aclosed set.ASupremumand aMaximum.An event which happens
with probability 1 and an event that always happens.I don't appreciate this
distinction in machine learning & learning theory. All machine learning takes
place (by definition) on a machine where every parameter has finite precision.
Consequently, every set is closed, a maximal element always exists, and
probability 1 events always happen.The fundamental issue here is that
substantial parts of mathematics don't appear well-matched to computation in
the physical world, because the mathematics has concerns which are unphysical.
This mismatched mathematics makes irrelevant distinctions. We can ask "what
mathematics is appropriate to computation?"Andrejhas convinced me that a
pretty good answer to this question isconstructive mathematics.So, here's a
basic challenge: Can anyone name a situation where any of the distinctions
above (or simila</p><p>same-blog 4 0.84427238 <a title="420-lda-4" href="../hunch_net-2010/hunch_net-2010-12-26-NIPS_2010.html">420 hunch net-2010-12-26-NIPS 2010</a></p>
<p>Introduction: I enjoyed attendingNIPSthis year, with several things interesting me. For the
conference itself:Peter Welinder,Steve Branson,Serge Belongie, andPietro
Perona,The Multidimensional Wisdom of Crowds. This paper is about
usingmechanical turkto get label information, with results superior to a
majority vote approach.David McAllester,Tamir Hazan, andJoseph KeshetDirect
Loss Minimization for Structured Prediction. This is about another technique
for directly optimizing the loss in structured prediction, with an application
to speech recognition.Mohammad SaberianandNuno VasconcelosBoosting Classifier
Cascades. This is about an algorithm for simultaneously optimizing loss and
computation in a classifier cascade construction. There were several other
papers on cascades which are worth looking at if interested.Alan FernandPrasad
Tadepalli,A Computational Decision Theory for Interactive Assistants. This
paper carves out some forms of natural not-MDP problems and shows their RL-
style solution is t</p><p>5 0.83878911 <a title="420-lda-5" href="../hunch_net-2008/hunch_net-2008-11-04-Rise_of_the_Machines.html">323 hunch net-2008-11-04-Rise of the Machines</a></p>
<p>Introduction: On theenduring topic of how people deal with intelligent machines, we have
this importantelection bulletin.</p><p>6 0.82221138 <a title="420-lda-6" href="../hunch_net-2009/hunch_net-2009-04-23-Jonathan_Chang_at_Slycoder.html">350 hunch net-2009-04-23-Jonathan Chang at Slycoder</a></p>
<p>7 0.79186672 <a title="420-lda-7" href="../hunch_net-2007/hunch_net-2007-06-24-Interesting_Papers_at_ICML_2007.html">251 hunch net-2007-06-24-Interesting Papers at ICML 2007</a></p>
<p>8 0.71657139 <a title="420-lda-8" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>9 0.70700914 <a title="420-lda-9" href="../hunch_net-2009/hunch_net-2009-07-31-Vowpal_Wabbit_Open_Source_Project.html">365 hunch net-2009-07-31-Vowpal Wabbit Open Source Project</a></p>
<p>10 0.69312769 <a title="420-lda-10" href="../hunch_net-2005/hunch_net-2005-09-12-Fast_Gradient_Descent.html">111 hunch net-2005-09-12-Fast Gradient Descent</a></p>
<p>11 0.69309789 <a title="420-lda-11" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>12 0.6898033 <a title="420-lda-12" href="../hunch_net-2005/hunch_net-2005-03-17-Going_all_the_Way%2C_Sometimes.html">42 hunch net-2005-03-17-Going all the Way, Sometimes</a></p>
<p>13 0.68801391 <a title="420-lda-13" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>14 0.68800789 <a title="420-lda-14" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<p>15 0.68694538 <a title="420-lda-15" href="../hunch_net-2007/hunch_net-2007-12-21-Vowpal_Wabbit_Code_Release.html">281 hunch net-2007-12-21-Vowpal Wabbit Code Release</a></p>
<p>16 0.68673879 <a title="420-lda-16" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>17 0.68666577 <a title="420-lda-17" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>18 0.68400395 <a title="420-lda-18" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>19 0.68202746 <a title="420-lda-19" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>20 0.68196231 <a title="420-lda-20" href="../hunch_net-2005/hunch_net-2005-12-07-Is_the_Google_way_the_way_for_machine_learning%3F.html">136 hunch net-2005-12-07-Is the Google way the way for machine learning?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
