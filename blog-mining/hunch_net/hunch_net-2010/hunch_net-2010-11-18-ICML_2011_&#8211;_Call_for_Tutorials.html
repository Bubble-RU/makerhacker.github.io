<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>417 hunch net-2010-11-18-ICML 2011 &#8211; Call for Tutorials</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2010" href="../home/hunch_net-2010_home.html">hunch_net-2010</a> <a title="hunch_net-2010-417" href="#">hunch_net-2010-417</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>417 hunch net-2010-11-18-ICML 2011 &#8211; Call for Tutorials</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2010-417-html" href="http://hunch.net/?p=1578">html</a></p><p>Introduction: I would like to encourage people to consider giving a tutorial at next years ICML. The ideal tutorial attracts a wide audience, provides a gentle and easily taught introduction to the chosen research area, and also covers the most important contributions in depth.
 
Submissions are due January 14 Â (about two weeks before paper deadline). 
 http://www.icml-2011.org/tutorials.php 
 
Regards, 
Ulf</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 I would like to encourage people to consider giving a tutorial at next years ICML. [sent-1, score-1.233]
</p><p>2 The ideal tutorial attracts a wide audience, provides a gentle and easily taught introduction to the chosen research area, and also covers the most important contributions in depth. [sent-2, score-2.383]
</p><p>3 Submissions are due January 14 Â (about two weeks before paper deadline). [sent-3, score-0.443]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tutorial', 0.342), ('regards', 0.298), ('covers', 0.298), ('contributions', 0.248), ('introduction', 0.239), ('http', 0.223), ('taught', 0.211), ('weeks', 0.206), ('january', 0.201), ('audience', 0.201), ('ideal', 0.201), ('chosen', 0.201), ('wide', 0.193), ('encourage', 0.186), ('submissions', 0.182), ('giving', 0.161), ('deadline', 0.152), ('years', 0.136), ('provides', 0.128), ('next', 0.126), ('area', 0.122), ('easily', 0.116), ('consider', 0.103), ('due', 0.098), ('important', 0.086), ('would', 0.074), ('paper', 0.071), ('two', 0.068), ('research', 0.066), ('like', 0.059), ('also', 0.054), ('people', 0.046)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="417-tfidf-1" href="../hunch_net-2010/hunch_net-2010-11-18-ICML_2011_%26%238211%3B_Call_for_Tutorials.html">417 hunch net-2010-11-18-ICML 2011 &#8211; Call for Tutorials</a></p>
<p>Introduction: I would like to encourage people to consider giving a tutorial at next years ICML. The ideal tutorial attracts a wide audience, provides a gentle and easily taught introduction to the chosen research area, and also covers the most important contributions in depth.
 
Submissions are due January 14 Â (about two weeks before paper deadline). 
 http://www.icml-2011.org/tutorials.php 
 
Regards, 
Ulf</p><p>2 0.21280316 <a title="417-tfidf-2" href="../hunch_net-2009/hunch_net-2009-11-23-ICML_2009_Workshops_%28and_Tutorials%29.html">379 hunch net-2009-11-23-ICML 2009 Workshops (and Tutorials)</a></p>
<p>Introduction: I’m the  workshops chair  for  ICML  this year.  As such, I would like to personally encourage people to consider running a workshop.
 
My general view of workshops is that they are excellent as opportunities to discuss and develop research directions—some of my best work has come from collaborations at workshops and several workshops have substantially altered my thinking about various problems.  My experience running workshops is that setting them up and making them fly often appears much harder than it actually is, and the workshops often come off much better than expected in the end.  Submissions are due January 18, two weeks before papers.
 
Similarly,  Ben Taskar  is looking for good  tutorials , which is complementary.  Workshops are about exploring a subject, while a tutorial is about distilling it down into an easily taught essence, a vital part of the research process.  Tutorials are due February 13, two weeks after papers.</p><p>3 0.15891263 <a title="417-tfidf-3" href="../hunch_net-2011/hunch_net-2011-08-20-The_Large_Scale_Learning_Survey_Tutorial.html">442 hunch net-2011-08-20-The Large Scale Learning Survey Tutorial</a></p>
<p>Introduction: Ron Bekkerman  initiated an effort to create an  edited book on parallel machine learning  that  Misha  and I have been helping with.  The breadth of efforts to parallelize machine learning surprised me: I was only aware of a small fraction initially.
 
This put us in a unique position, with knowledge of a wide array of different efforts, so it is natural to put together a  survey tutorial on the subject of parallel learning  for  KDD , tomorrow.  This tutorial is  not  limited to the book itself however, as several interesting new algorithms have come out since we started inviting chapters.  
 
This tutorial should interest anyone trying to use machine learning on significant quantities of data, anyone interested in developing algorithms for such, and of course who has bragging rights to the fastest learning algorithm on planet earth   
 
(Also note the Modeling with Hadoop tutorial just before ours which deals with one way of trying to speed up learning algorithms.  We have almost no</p><p>4 0.1170828 <a title="417-tfidf-4" href="../hunch_net-2010/hunch_net-2010-09-13-AIStats.html">409 hunch net-2010-09-13-AIStats</a></p>
<p>Introduction: Geoff Gordon  points out  AIStats 2011  in Ft. Lauderdale, Florida.  The  call for papers  is now out, due Nov. 1.  The plan is to  experiment with the review process  to encourage quality in several ways.  I expect to submit a paper and would encourage others with good research to do likewise.</p><p>5 0.11361447 <a title="417-tfidf-5" href="../hunch_net-2013/hunch_net-2013-12-01-NIPS_tutorials_and_Vowpal_Wabbit_7.4.html">492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</a></p>
<p>Introduction: At NIPS I’m giving a  tutorial on Learning to Interact .  In essence this is about dealing with causality in a contextual bandit framework.  Relative to  previous tutorials , I’ll be covering several new results that changed my understanding of the nature of the problem.  Note that  Judea Pearl  and  Elias Bareinboim  have a  tutorial on causality .  This might appear similar, but is quite different in practice.  Pearl and Bareinboim’s tutorial will be about the general concepts while mine will be about total mastery of the simplest nontrivial case, including code.  Luckily, they have the right order.  I recommend going to both   
 
I also just released version 7.4 of  Vowpal Wabbit .  When I was a frustrated learning theorist, I did not understand why people were not using learning reductions to solve problems.  I’ve been slowly discovering why with VW, and addressing the issues.  One of the issues is that machine learning itself was not automatic enough, while another is that creatin</p><p>6 0.0994533 <a title="417-tfidf-6" href="../hunch_net-2011/hunch_net-2011-04-23-ICML_workshops_due.html">433 hunch net-2011-04-23-ICML workshops due</a></p>
<p>7 0.099338353 <a title="417-tfidf-7" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>8 0.090945266 <a title="417-tfidf-8" href="../hunch_net-2005/hunch_net-2005-12-29-Deadline_Season.html">145 hunch net-2005-12-29-Deadline Season</a></p>
<p>9 0.088758178 <a title="417-tfidf-9" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>10 0.086767748 <a title="417-tfidf-10" href="../hunch_net-2012/hunch_net-2012-12-29-Simons_Institute_Big_Data_Program.html">476 hunch net-2012-12-29-Simons Institute Big Data Program</a></p>
<p>11 0.077879921 <a title="417-tfidf-11" href="../hunch_net-2009/hunch_net-2009-10-26-NIPS_workshops.html">375 hunch net-2009-10-26-NIPS workshops</a></p>
<p>12 0.075845763 <a title="417-tfidf-12" href="../hunch_net-2005/hunch_net-2005-06-10-Workshops_are_not_Conferences.html">80 hunch net-2005-06-10-Workshops are not Conferences</a></p>
<p>13 0.074899554 <a title="417-tfidf-13" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>14 0.074340031 <a title="417-tfidf-14" href="../hunch_net-2010/hunch_net-2010-01-19-Deadline_Season%2C_2010.html">387 hunch net-2010-01-19-Deadline Season, 2010</a></p>
<p>15 0.074289434 <a title="417-tfidf-15" href="../hunch_net-2006/hunch_net-2006-07-25-Upcoming_conference.html">198 hunch net-2006-07-25-Upcoming conference</a></p>
<p>16 0.074127302 <a title="417-tfidf-16" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>17 0.069360606 <a title="417-tfidf-17" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>18 0.068067275 <a title="417-tfidf-18" href="../hunch_net-2006/hunch_net-2006-06-15-IJCAI_is_out_of_season.html">184 hunch net-2006-06-15-IJCAI is out of season</a></p>
<p>19 0.067631319 <a title="417-tfidf-19" href="../hunch_net-2012/hunch_net-2012-02-24-ICML%2B50%25.html">456 hunch net-2012-02-24-ICML+50%</a></p>
<p>20 0.064551286 <a title="417-tfidf-20" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.11), (1, -0.105), (2, -0.053), (3, -0.05), (4, 0.001), (5, 0.027), (6, 0.026), (7, 0.007), (8, -0.035), (9, -0.003), (10, 0.028), (11, -0.074), (12, 0.003), (13, 0.005), (14, -0.022), (15, 0.022), (16, 0.022), (17, -0.026), (18, -0.084), (19, -0.098), (20, -0.041), (21, 0.037), (22, -0.036), (23, -0.031), (24, 0.025), (25, 0.039), (26, 0.026), (27, -0.151), (28, -0.082), (29, 0.006), (30, -0.001), (31, -0.153), (32, 0.064), (33, -0.048), (34, -0.021), (35, 0.019), (36, 0.075), (37, 0.058), (38, 0.06), (39, 0.113), (40, -0.006), (41, 0.111), (42, -0.043), (43, -0.022), (44, -0.137), (45, 0.007), (46, 0.093), (47, -0.046), (48, -0.019), (49, -0.006)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97761267 <a title="417-lsi-1" href="../hunch_net-2010/hunch_net-2010-11-18-ICML_2011_%26%238211%3B_Call_for_Tutorials.html">417 hunch net-2010-11-18-ICML 2011 &#8211; Call for Tutorials</a></p>
<p>Introduction: I would like to encourage people to consider giving a tutorial at next years ICML. The ideal tutorial attracts a wide audience, provides a gentle and easily taught introduction to the chosen research area, and also covers the most important contributions in depth.
 
Submissions are due January 14 Â (about two weeks before paper deadline). 
 http://www.icml-2011.org/tutorials.php 
 
Regards, 
Ulf</p><p>2 0.57566589 <a title="417-lsi-2" href="../hunch_net-2012/hunch_net-2012-12-29-Simons_Institute_Big_Data_Program.html">476 hunch net-2012-12-29-Simons Institute Big Data Program</a></p>
<p>Introduction: Michael Jordan  sends the below:
 
The new  Simons Institute for the Theory of Computing  
will begin organizing semester-long programs starting in 2013.
 
One of our first programs, set for Fall 2013, will be on the “Theoretical Foundations 
of Big Data Analysis”.  The organizers of this program are Michael Jordan (chair), 
Stephen Boyd, Peter Buehlmann, Ravi Kannan, Michael Mahoney, and Muthu Muthukrishnan.
 
See  http://simons.berkeley.edu/program_bigdata2013.html  for more information on 
the program.
 
The Simons Institute has created a number of “Research Fellowships” for young 
researchers (within at most six years of the award of their PhD) who wish to 
participate in Institute programs, including the Big Data program.  Individuals 
who already hold postdoctoral positions or who are junior faculty are welcome 
to apply, as are finishing PhDs.
 
Please note that the application deadline is January 15, 2013.  Further details 
are available at  http://simons.berkeley.edu/fellows.h</p><p>3 0.52400964 <a title="417-lsi-3" href="../hunch_net-2008/hunch_net-2008-10-19-NIPS_2008_workshop_on_Kernel_Learning.html">321 hunch net-2008-10-19-NIPS 2008 workshop on Kernel Learning</a></p>
<p>Introduction: Weâ&euro;&trade;d like to invite hunch.net readers to participate in the NIPS 2008 workshop on kernel learning.  While the main focus is on automatically learning kernels from data, we are also also looking at the broader questions of feature selection, multi-task learning and multi-view learning. There are no restrictions on the learning problem being addressed (regression, classification, etc), and both theoretical and applied work will be considered. The deadline for submissions is  October 24 .
 
More detail can be found  here .
 
Corinna Cortes, Arthur Gretton, Gert Lanckriet, Mehryar Mohri, Afshin Rostamizadeh</p><p>4 0.51444769 <a title="417-lsi-4" href="../hunch_net-2010/hunch_net-2010-09-13-AIStats.html">409 hunch net-2010-09-13-AIStats</a></p>
<p>Introduction: Geoff Gordon  points out  AIStats 2011  in Ft. Lauderdale, Florida.  The  call for papers  is now out, due Nov. 1.  The plan is to  experiment with the review process  to encourage quality in several ways.  I expect to submit a paper and would encourage others with good research to do likewise.</p><p>5 0.49872985 <a title="417-lsi-5" href="../hunch_net-2011/hunch_net-2011-08-20-The_Large_Scale_Learning_Survey_Tutorial.html">442 hunch net-2011-08-20-The Large Scale Learning Survey Tutorial</a></p>
<p>Introduction: Ron Bekkerman  initiated an effort to create an  edited book on parallel machine learning  that  Misha  and I have been helping with.  The breadth of efforts to parallelize machine learning surprised me: I was only aware of a small fraction initially.
 
This put us in a unique position, with knowledge of a wide array of different efforts, so it is natural to put together a  survey tutorial on the subject of parallel learning  for  KDD , tomorrow.  This tutorial is  not  limited to the book itself however, as several interesting new algorithms have come out since we started inviting chapters.  
 
This tutorial should interest anyone trying to use machine learning on significant quantities of data, anyone interested in developing algorithms for such, and of course who has bragging rights to the fastest learning algorithm on planet earth   
 
(Also note the Modeling with Hadoop tutorial just before ours which deals with one way of trying to speed up learning algorithms.  We have almost no</p><p>6 0.47269517 <a title="417-lsi-6" href="../hunch_net-2013/hunch_net-2013-12-01-NIPS_tutorials_and_Vowpal_Wabbit_7.4.html">492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</a></p>
<p>7 0.44837907 <a title="417-lsi-7" href="../hunch_net-2007/hunch_net-2007-02-22-Create_Your_Own_ICML_Workshop.html">234 hunch net-2007-02-22-Create Your Own ICML Workshop</a></p>
<p>8 0.44569287 <a title="417-lsi-8" href="../hunch_net-2005/hunch_net-2005-10-19-Workshop%3A_Atomic_Learning.html">124 hunch net-2005-10-19-Workshop: Atomic Learning</a></p>
<p>9 0.43814251 <a title="417-lsi-9" href="../hunch_net-2006/hunch_net-2006-05-21-NIPS_paper_evaluation_criteria.html">180 hunch net-2006-05-21-NIPS paper evaluation criteria</a></p>
<p>10 0.4284313 <a title="417-lsi-10" href="../hunch_net-2006/hunch_net-2006-07-25-Upcoming_conference.html">198 hunch net-2006-07-25-Upcoming conference</a></p>
<p>11 0.40413055 <a title="417-lsi-11" href="../hunch_net-2010/hunch_net-2010-08-20-The_Workshop_on_Cores%2C_Clusters%2C_and_Clouds.html">404 hunch net-2010-08-20-The Workshop on Cores, Clusters, and Clouds</a></p>
<p>12 0.40070975 <a title="417-lsi-12" href="../hunch_net-2012/hunch_net-2012-08-27-NYAS_ML_2012_and_ICML_2013.html">472 hunch net-2012-08-27-NYAS ML 2012 and ICML 2013</a></p>
<p>13 0.39825746 <a title="417-lsi-13" href="../hunch_net-2006/hunch_net-2006-06-15-IJCAI_is_out_of_season.html">184 hunch net-2006-06-15-IJCAI is out of season</a></p>
<p>14 0.39781123 <a title="417-lsi-14" href="../hunch_net-2005/hunch_net-2005-12-22-Yes_%2C_I_am_applying.html">142 hunch net-2005-12-22-Yes , I am applying</a></p>
<p>15 0.39544886 <a title="417-lsi-15" href="../hunch_net-2013/hunch_net-2013-08-31-Extreme_Classification_workshop_at_NIPS.html">488 hunch net-2013-08-31-Extreme Classification workshop at NIPS</a></p>
<p>16 0.37869468 <a title="417-lsi-16" href="../hunch_net-2008/hunch_net-2008-11-04-Rise_of_the_Machines.html">323 hunch net-2008-11-04-Rise of the Machines</a></p>
<p>17 0.3747353 <a title="417-lsi-17" href="../hunch_net-2011/hunch_net-2011-01-03-Herman_Goldstine_2011.html">421 hunch net-2011-01-03-Herman Goldstine 2011</a></p>
<p>18 0.36822924 <a title="417-lsi-18" href="../hunch_net-2008/hunch_net-2008-07-02-Proprietary_Data_in_Academic_Research%3F.html">306 hunch net-2008-07-02-Proprietary Data in Academic Research?</a></p>
<p>19 0.36700344 <a title="417-lsi-19" href="../hunch_net-2013/hunch_net-2013-01-07-NYU_Large_Scale_Machine_Learning_Class.html">478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</a></p>
<p>20 0.36255535 <a title="417-lsi-20" href="../hunch_net-2009/hunch_net-2009-11-23-ICML_2009_Workshops_%28and_Tutorials%29.html">379 hunch net-2009-11-23-ICML 2009 Workshops (and Tutorials)</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(27, 0.126), (38, 0.014), (55, 0.126), (71, 0.577)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.88868022 <a title="417-lda-1" href="../hunch_net-2010/hunch_net-2010-11-18-ICML_2011_%26%238211%3B_Call_for_Tutorials.html">417 hunch net-2010-11-18-ICML 2011 &#8211; Call for Tutorials</a></p>
<p>Introduction: I would like to encourage people to consider giving a tutorial at next years ICML. The ideal tutorial attracts a wide audience, provides a gentle and easily taught introduction to the chosen research area, and also covers the most important contributions in depth.
 
Submissions are due January 14 Â (about two weeks before paper deadline). 
 http://www.icml-2011.org/tutorials.php 
 
Regards, 
Ulf</p><p>2 0.73725551 <a title="417-lda-2" href="../hunch_net-2006/hunch_net-2006-03-05-%26%238220%3BStructural%26%238221%3B_Learning.html">161 hunch net-2006-03-05-&#8220;Structural&#8221; Learning</a></p>
<p>Introduction: Fernando Pereira   pointed out  Ando and  Zhang ‘s  paper  on “structural” learning.  Structural learning is multitask learning on subproblems created from unlabeled data.  
 
The basic idea is to take a look at the unlabeled data and create many supervised problems.  On text data, which they test on, these subproblems might be of the form “Given surrounding words predict the middle word”.   The hope here is that successfully predicting on these subproblems is relevant to the prediction of your core problem.
 
In the long run, the precise mechanism used (essentially, linear predictors with parameters tied by a common matrix) and the precise problems formed may not be critical.  What seems critical is that the hope is realized: the technique provides a significant edge in practice.
 
Some basic questions about this approach are:
  
 Are there effective automated mechanisms for creating the subproblems? 
 Is it necessary to use a shared representation?</p><p>3 0.60122567 <a title="417-lda-3" href="../hunch_net-2006/hunch_net-2006-01-08-Debugging_Your_Brain.html">147 hunch net-2006-01-08-Debugging Your Brain</a></p>
<p>Introduction: One part of doing research is debugging your understanding of reality.  This is hard work: How do you even discover where you misunderstand?  If you discover a misunderstanding, how do you go about removing it?
 
The process of debugging computer programs is quite analogous to debugging reality misunderstandings.   This is natural—a bug in a computer program is a misunderstanding between you and the computer about what you said.  Many of the familiar techniques from debugging have exact parallels.
  
  Details  When programming, there are often signs that some bug exists like: “the graph my program output is shifted a little bit” = maybe you have an indexing error.  In debugging yourself, we often have some impression that something is “not right”. These impressions should be addressed directly and immediately.  (Some people have the habit of suppressing worries in favor of excess certainty.  That’s not healthy for research.) 
  Corner Cases  A “corner case” is an input to a program wh</p><p>4 0.45715451 <a title="417-lda-4" href="../hunch_net-2006/hunch_net-2006-01-30-Should_the_Input_Representation_be_a_Vector%3F.html">152 hunch net-2006-01-30-Should the Input Representation be a Vector?</a></p>
<p>Introduction: Let’s suppose that we are trying to create a general purpose machine learning box.  The box is fed many examples of the function it is supposed to learn and (hopefully) succeeds.
 
To date, most such attempts to produce a box of this form take a vector as input.  The elements of the vector might be bits, real numbers, or ‘categorical’ data (a discrete set of values).
 
On the other hand, there are a number of succesful applications of machine learning which do not seem to use a vector representation as input.  For example, in vision,   convolutional neural networks  have been used to solve several vision problems.  The input to the convolutional neural network is essentially the raw camera image as a  matrix .  In learning for natural languages, several people have had success on problems like parts-of-speech tagging using predictors restricted to a window surrounding the word to be predicted.  
 
A vector window and a matrix both imply a notion of locality which is being actively and</p><p>5 0.44731686 <a title="417-lda-5" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>Introduction: Suppose you have a dataset with 2 terafeatures (we only count nonzero entries in a datamatrix), and want to learn a good linear predictor in a reasonable amount of time.  How do you do it?  As a learning theorist, the first thing you do is pray that this is too much data for the number of parameters—but that’s not the case, there are around 16 billion examples, 16 million parameters, and people really care about a high quality predictor, so subsampling is not a good strategy.
 
 Alekh  visited us last summer, and we had a breakthrough (see  here  for details), coming up with the first learning algorithm I’ve seen that is provably faster than  any future  single machine learning algorithm.  The proof of this is simple: We can output a optimal-up-to-precision linear predictor faster than the data can be streamed through the network interface of any single machine involved in the computation.
 
It is necessary but not sufficient to have an effective communication infrastructure.  It is ne</p><p>6 0.34157786 <a title="417-lda-6" href="../hunch_net-2009/hunch_net-2009-11-23-ICML_2009_Workshops_%28and_Tutorials%29.html">379 hunch net-2009-11-23-ICML 2009 Workshops (and Tutorials)</a></p>
<p>7 0.28513938 <a title="417-lda-7" href="../hunch_net-2007/hunch_net-2007-11-02-The_Machine_Learning_Award_goes_to_%26%238230%3B.html">270 hunch net-2007-11-02-The Machine Learning Award goes to &#8230;</a></p>
<p>8 0.28315884 <a title="417-lda-8" href="../hunch_net-2010/hunch_net-2010-04-26-Compassionate_Reviewing.html">395 hunch net-2010-04-26-Compassionate Reviewing</a></p>
<p>9 0.27878976 <a title="417-lda-9" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>10 0.27818561 <a title="417-lda-10" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<p>11 0.2751458 <a title="417-lda-11" href="../hunch_net-2005/hunch_net-2005-07-07-The_Limits_of_Learning_Theory.html">90 hunch net-2005-07-07-The Limits of Learning Theory</a></p>
<p>12 0.27029961 <a title="417-lda-12" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>13 0.26976177 <a title="417-lda-13" href="../hunch_net-2005/hunch_net-2005-07-04-The_Health_of_COLT.html">89 hunch net-2005-07-04-The Health of COLT</a></p>
<p>14 0.26925129 <a title="417-lda-14" href="../hunch_net-2005/hunch_net-2005-03-13-Avoiding_Bad_Reviewing.html">40 hunch net-2005-03-13-Avoiding Bad Reviewing</a></p>
<p>15 0.26853302 <a title="417-lda-15" href="../hunch_net-2007/hunch_net-2007-09-30-NIPS_workshops_are_out..html">264 hunch net-2007-09-30-NIPS workshops are out.</a></p>
<p>16 0.2661185 <a title="417-lda-16" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>17 0.26514101 <a title="417-lda-17" href="../hunch_net-2005/hunch_net-2005-02-15-ESPgame_and_image_labeling.html">20 hunch net-2005-02-15-ESPgame and image labeling</a></p>
<p>18 0.2651062 <a title="417-lda-18" href="../hunch_net-2009/hunch_net-2009-10-26-NIPS_workshops.html">375 hunch net-2009-10-26-NIPS workshops</a></p>
<p>19 0.26504904 <a title="417-lda-19" href="../hunch_net-2005/hunch_net-2005-05-02-Reviewing_techniques_for_conferences.html">65 hunch net-2005-05-02-Reviewing techniques for conferences</a></p>
<p>20 0.264842 <a title="417-lda-20" href="../hunch_net-2011/hunch_net-2011-10-24-2011_ML_symposium_and_the_bears.html">448 hunch net-2011-10-24-2011 ML symposium and the bears</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
