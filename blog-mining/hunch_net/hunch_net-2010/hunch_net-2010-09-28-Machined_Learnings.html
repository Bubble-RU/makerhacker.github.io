<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>412 hunch net-2010-09-28-Machined Learnings</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2010" href="../home/hunch_net-2010_home.html">hunch_net-2010</a> <a title="hunch_net-2010-412" href="#">hunch_net-2010-412</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>412 hunch net-2010-09-28-Machined Learnings</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2010-412-html" href="http://hunch.net/?p=1488">html</a></p><p>Introduction: Paul Mineirohas startedMachined Learningswhere he's seriously attempting to do
ML research in public. I personally need to read through in greater detail, as
much of it is learning reduction related, trying to deal with the sorts of
complex source problems that come up in practice.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Paul Mineirohas startedMachined Learningswhere he's seriously attempting to do ML research in public. [sent-1, score-0.729]
</p><p>2 I personally need to read through in greater detail, as much of it is learning reduction related, trying to deal with the sorts of complex source problems that come up in practice. [sent-2, score-2.418]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('paul', 0.347), ('attempting', 0.333), ('detail', 0.302), ('seriously', 0.302), ('sorts', 0.264), ('personally', 0.246), ('greater', 0.238), ('read', 0.211), ('reduction', 0.211), ('complex', 0.202), ('practice', 0.198), ('ml', 0.192), ('source', 0.184), ('trying', 0.181), ('deal', 0.17), ('related', 0.159), ('need', 0.154), ('come', 0.15), ('research', 0.094), ('problems', 0.094), ('much', 0.086), ('learning', 0.027)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="412-tfidf-1" href="../hunch_net-2010/hunch_net-2010-09-28-Machined_Learnings.html">412 hunch net-2010-09-28-Machined Learnings</a></p>
<p>Introduction: Paul Mineirohas startedMachined Learningswhere he's seriously attempting to do
ML research in public. I personally need to read through in greater detail, as
much of it is learning reduction related, trying to deal with the sorts of
complex source problems that come up in practice.</p><p>2 0.14365569 <a title="412-tfidf-2" href="../hunch_net-2006/hunch_net-2006-06-25-Presentation_of_Proofs_is_Hard..html">187 hunch net-2006-06-25-Presentation of Proofs is Hard.</a></p>
<p>Introduction: When presenting part of theReinforcement Learning theory tutorialatICML 2006,
I was forcibly reminded of this.There are several difficulties.When creating
the presentation, the correct level of detail is tricky. With too much detail,
the proof takes too much time and people may be lost to boredom. With too
little detail, the steps of the proof involve too-great a jump. This is very
difficult to judge.What may be an easy step in the careful thought of a quiet
room is not so easy when you are occupied by the process of presentation.What
may be easy after having gone over this (and other) proofs is not so easy to
follow in the first pass by a viewer.These problems seem only correctable by
process of repeated test-and-revise.When presenting the proof, simply speaking
with sufficient precision is substantially harder than in normal conversation
(where precision is not so critical). Practice can help here.When presenting
the proof, going at the right pace for understanding is difficult. When</p><p>3 0.12748197 <a title="412-tfidf-3" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>Introduction: Researchers are typically confronted with big problems that they have no idea
how to solve. In trying to come up with a solution, a natural approach is to
decompose the big problem into a set of subproblems whose solution yields a
solution to the larger problem. This approach can go wrong in several
ways.Decomposition failure. The solution to the decomposition does not in fact
yield a solution to the overall problem.Artificial hardness. The subproblems
created are sufficient if solved to solve the overall problem, but they are
harder than necessary.As you can see, computational complexity forms a
relatively new (in research-history) razor by which to judge an approach
sufficient but not necessary.In my experience, the artificial hardness problem
is very common. Many researchers abdicate the responsibility of choosing a
problem to work on to other people. This process starts very naturally as a
graduate student, when an incoming student might have relatively little idea
about how to do</p><p>4 0.10706894 <a title="412-tfidf-4" href="../hunch_net-2008/hunch_net-2008-11-26-Efficient_Reinforcement_Learning_in_MDPs.html">328 hunch net-2008-11-26-Efficient Reinforcement Learning in MDPs</a></p>
<p>Introduction: Claude Sammutis attempting to put together anEncyclopedia of Machine Learning.
I volunteered to write one article onEfficient RL in MDPs, which I would like
to invite comment on. Is something critical missing?</p><p>5 0.10416304 <a title="412-tfidf-5" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>Introduction: This post is partly meant as an advertisement for thereductions
tutorialAlina,Bianca, and I are planning to do atICML. Please come, if you are
interested.Many research programs can be thought of as finding and building
new useful abstractions. The running example I'll use islearning
reductionswhere I have experience. The basic abstraction here is that we can
build a learning algorithm capable of solving classification problems up to a
small expected regret. This is used repeatedly to solve more complex
problems.In working on a new abstraction, I think you typically run into many
substantial problems of understanding, which make publishing particularly
difficult.It is difficult to seriously discuss the reason behind or mechanism
for abstraction in a conference paper with small page limits. People rarely
see such discussions and hence have little basis on which to think about new
abstractions. Another difficulty is that when building an abstraction, you
often don't know the right way to</p><p>6 0.094426967 <a title="412-tfidf-6" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<p>7 0.093977377 <a title="412-tfidf-7" href="../hunch_net-2005/hunch_net-2005-02-02-Kolmogorov_Complexity_and_Googling.html">10 hunch net-2005-02-02-Kolmogorov Complexity and Googling</a></p>
<p>8 0.090910204 <a title="412-tfidf-8" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>9 0.086211175 <a title="412-tfidf-9" href="../hunch_net-2006/hunch_net-2006-05-23-What_is_the_best_regret_transform_reduction_from_multiclass_to_binary%3F.html">181 hunch net-2006-05-23-What is the best regret transform reduction from multiclass to binary?</a></p>
<p>10 0.084710702 <a title="412-tfidf-10" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>11 0.070788994 <a title="412-tfidf-11" href="../hunch_net-2005/hunch_net-2005-12-22-Yes_%2C_I_am_applying.html">142 hunch net-2005-12-22-Yes , I am applying</a></p>
<p>12 0.070692882 <a title="412-tfidf-12" href="../hunch_net-2008/hunch_net-2008-10-19-NIPS_2008_workshop_on_Kernel_Learning.html">321 hunch net-2008-10-19-NIPS 2008 workshop on Kernel Learning</a></p>
<p>13 0.068956308 <a title="412-tfidf-13" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>14 0.068582714 <a title="412-tfidf-14" href="../hunch_net-2008/hunch_net-2008-08-18-Radford_Neal_starts_a_blog.html">313 hunch net-2008-08-18-Radford Neal starts a blog</a></p>
<p>15 0.066188484 <a title="412-tfidf-15" href="../hunch_net-2005/hunch_net-2005-09-06-A_link.html">108 hunch net-2005-09-06-A link</a></p>
<p>16 0.065267675 <a title="412-tfidf-16" href="../hunch_net-2007/hunch_net-2007-12-19-Cool_and_interesting_things_seen_at_NIPS.html">279 hunch net-2007-12-19-Cool and interesting things seen at NIPS</a></p>
<p>17 0.064017005 <a title="412-tfidf-17" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>18 0.063298702 <a title="412-tfidf-18" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>19 0.063135214 <a title="412-tfidf-19" href="../hunch_net-2009/hunch_net-2009-11-23-ICML_2009_Workshops_%28and_Tutorials%29.html">379 hunch net-2009-11-23-ICML 2009 Workshops (and Tutorials)</a></p>
<p>20 0.061104506 <a title="412-tfidf-20" href="../hunch_net-2005/hunch_net-2005-03-05-Funding_Research.html">36 hunch net-2005-03-05-Funding Research</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.113), (1, 0.004), (2, 0.069), (3, -0.032), (4, 0.03), (5, -0.054), (6, 0.041), (7, 0.006), (8, -0.0), (9, 0.062), (10, 0.045), (11, 0.056), (12, 0.065), (13, 0.058), (14, -0.002), (15, 0.051), (16, -0.025), (17, -0.028), (18, 0.036), (19, -0.044), (20, 0.046), (21, -0.039), (22, -0.012), (23, 0.064), (24, 0.038), (25, 0.054), (26, 0.037), (27, 0.1), (28, -0.031), (29, 0.001), (30, 0.074), (31, -0.055), (32, -0.063), (33, 0.002), (34, -0.029), (35, -0.028), (36, 0.088), (37, -0.017), (38, 0.054), (39, -0.055), (40, -0.0), (41, 0.027), (42, 0.069), (43, -0.008), (44, -0.117), (45, -0.016), (46, 0.084), (47, 0.03), (48, -0.012), (49, 0.055)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96824992 <a title="412-lsi-1" href="../hunch_net-2010/hunch_net-2010-09-28-Machined_Learnings.html">412 hunch net-2010-09-28-Machined Learnings</a></p>
<p>Introduction: Paul Mineirohas startedMachined Learningswhere he's seriously attempting to do
ML research in public. I personally need to read through in greater detail, as
much of it is learning reduction related, trying to deal with the sorts of
complex source problems that come up in practice.</p><p>2 0.54316247 <a title="412-lsi-2" href="../hunch_net-2008/hunch_net-2008-08-18-Radford_Neal_starts_a_blog.html">313 hunch net-2008-08-18-Radford Neal starts a blog</a></p>
<p>Introduction: hereon statistics, ML, CS, and other things he knows well.</p><p>3 0.53559899 <a title="412-lsi-3" href="../hunch_net-2009/hunch_net-2009-12-24-Top_graduates_this_season.html">384 hunch net-2009-12-24-Top graduates this season</a></p>
<p>Introduction: I would like to point out 3 graduates this season as having my confidence they
are capable of doing great things.Daniel Hsuhas diverse papers with diverse
coauthors on {active learning, mulitlabeling, temporal learning, â&euro;Ś} each
covering new algorithms and methods of analysis. He is also a capable
programmer, having helped me with some nitty-gritty details of cluster
parallelVowpal Wabbitthis summer. He has an excellent tendency to just get
things done.Nicolas Lambertdoesn't nominally work in machine learning, but
I've found his work inelicitationrelevant nevertheless. In essence, elicitable
properties are closely related to learnable properties, and the elicitation
complexity is related to a notion of learning complexity. See theSurrogate
regret bounds paperfor some related discussion. Few people successfully work
at such a general level that it crosses fields, but he's one of them.Yisong
Yueis deeply focused on interactive learning, which he has attacked at all
levels: theory, algorit</p><p>4 0.5306564 <a title="412-lsi-4" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>Introduction: This post is some combination of belaboring the obvious and speculating wildly
about the future. The basic issue to be addressed is how to think about
machine learning in terms given to us from Programming Language theory.Types
and ReductionsJohn's research programme (I feel this should be in British
spelling to reflect the grandiousness of the ideaâ&euro;Ś) of machine learning
reductionsStateOfReductionis at some essential level type-theoretic in nature.
The fundamental elements are the classifier, a function f: alpha -> beta, and
the corresponding classifier trainer g: List of (alpha,beta) -> (alpha ->
beta). The research goal is to create *combinators* that produce new f's and
g's given existing ones. John (probably quite rightly) seems unwilling at the
moment to commit to any notion stronger than these combinators are correctly
typed. One way to see the result of a reduction is something typed like: (For
those denied the joy of the Hindly-Milner type system, "simple" is probably
wildly wr</p><p>5 0.49556476 <a title="412-lsi-5" href="../hunch_net-2005/hunch_net-2005-05-29-Bad_ideas.html">76 hunch net-2005-05-29-Bad ideas</a></p>
<p>Introduction: I found these two essays on bad ideas interesting. Neither of these is written
from the viewpoint of research, but they are both highly relevant.Why smart
people have bad ideasby Paul GrahamWhy smart people defend bad ideasby Scott
Berkun (which appeared onslashdot)In my experience, bad ideas are
commonandover confidence in ideas is common. This overconfidence can take
either the form of excessive condemnation or excessive praise. Some of this is
necessary to the process of research. For example, some overconfidence in the
value of your own research is expected and probably necessary to motivate your
own investigation. Since research is a rather risky business, much of it does
not pan out. Learning to accept when something does not pan out is a critical
skill which is sometimes never acquired.Excessive condemnation can be a real
ill when it's encountered. This has two effects:When the penalty for being
wrong is too large, it means people have a great investment in defending
"their" ide</p><p>6 0.47920659 <a title="412-lsi-6" href="../hunch_net-2007/hunch_net-2007-02-10-Best_Practices_for_Collaboration.html">231 hunch net-2007-02-10-Best Practices for Collaboration</a></p>
<p>7 0.46787924 <a title="412-lsi-7" href="../hunch_net-2005/hunch_net-2005-01-24-Holy_grails_of_machine_learning%3F.html">2 hunch net-2005-01-24-Holy grails of machine learning?</a></p>
<p>8 0.46556547 <a title="412-lsi-8" href="../hunch_net-2008/hunch_net-2008-11-04-Rise_of_the_Machines.html">323 hunch net-2008-11-04-Rise of the Machines</a></p>
<p>9 0.46525624 <a title="412-lsi-9" href="../hunch_net-2009/hunch_net-2009-06-01-Multitask_Poisoning.html">358 hunch net-2009-06-01-Multitask Poisoning</a></p>
<p>10 0.45526356 <a title="412-lsi-10" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>11 0.45434359 <a title="412-lsi-11" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>12 0.45424744 <a title="412-lsi-12" href="../hunch_net-2005/hunch_net-2005-02-25-Solution%3A_Reinforcement_Learning_with_Classification.html">29 hunch net-2005-02-25-Solution: Reinforcement Learning with Classification</a></p>
<p>13 0.42587933 <a title="412-lsi-13" href="../hunch_net-2006/hunch_net-2006-06-25-Presentation_of_Proofs_is_Hard..html">187 hunch net-2006-06-25-Presentation of Proofs is Hard.</a></p>
<p>14 0.42580658 <a title="412-lsi-14" href="../hunch_net-2008/hunch_net-2008-11-26-Efficient_Reinforcement_Learning_in_MDPs.html">328 hunch net-2008-11-26-Efficient Reinforcement Learning in MDPs</a></p>
<p>15 0.4198994 <a title="412-lsi-15" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>16 0.41867974 <a title="412-lsi-16" href="../hunch_net-2007/hunch_net-2007-09-30-NIPS_workshops_are_out..html">264 hunch net-2007-09-30-NIPS workshops are out.</a></p>
<p>17 0.41630271 <a title="412-lsi-17" href="../hunch_net-2005/hunch_net-2005-04-22-New_Blog%3A_%5BLowerbounds%2CUpperbounds%5D.html">59 hunch net-2005-04-22-New Blog: [Lowerbounds,Upperbounds]</a></p>
<p>18 0.41320577 <a title="412-lsi-18" href="../hunch_net-2010/hunch_net-2010-01-13-Sam_Roweis_died.html">386 hunch net-2010-01-13-Sam Roweis died</a></p>
<p>19 0.41270331 <a title="412-lsi-19" href="../hunch_net-2007/hunch_net-2007-06-19-How_is_Compressed_Sensing_going_to_change_Machine_Learning_%3F.html">248 hunch net-2007-06-19-How is Compressed Sensing going to change Machine Learning ?</a></p>
<p>20 0.40519109 <a title="412-lsi-20" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(22, 0.379), (42, 0.308), (74, 0.132)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94603187 <a title="412-lda-1" href="../hunch_net-2010/hunch_net-2010-09-28-Machined_Learnings.html">412 hunch net-2010-09-28-Machined Learnings</a></p>
<p>Introduction: Paul Mineirohas startedMachined Learningswhere he's seriously attempting to do
ML research in public. I personally need to read through in greater detail, as
much of it is learning reduction related, trying to deal with the sorts of
complex source problems that come up in practice.</p><p>2 0.92286903 <a title="412-lda-2" href="../hunch_net-2006/hunch_net-2006-04-14-JMLR_is_a_success.html">172 hunch net-2006-04-14-JMLR is a success</a></p>
<p>Introduction: In 2001, the "Journal of Machine Learning Research" was created in reaction to
unadaptive publisher policies atMLJ. Essentially, with the creation of the
internet, the bottleneck in publishing research shifted from publishing to
research. Thedeclaration of independenceaccompanying this move expresses the
reasons why in greater detail.MLJ has strongly changed its policy in reaction
to this. In particular, there is no longer an assignment of copyright to the
publisher (*), and MLJ regularly sponsors many student "best paper awards"
across several conferences with cash prizes. This is an advantage of MLJ over
JMLR: MLJ can afford to sponsor cash prizes for the machine learning
community. The remaining disadvantage is that reading papers in MLJ sometimes
requires searching for the author's website where the free version is
available. In contrast, JMLR articles are freely available to everyone off the
JMLR website. Whether or not this disadvantage cancels the advantage is
debatable, but ess</p><p>3 0.64346135 <a title="412-lda-3" href="../hunch_net-2008/hunch_net-2008-11-10-ICML_Reviewing_Criteria.html">325 hunch net-2008-11-10-ICML Reviewing Criteria</a></p>
<p>Introduction: Michael LittmanandLeon Bottouhave decided to use a franchise program chair
approach toreviewing at ICMLthis year. I'll be one of the area chairs, so I
wanted to mention a few things if you are thinking about naming me.I take
reviewing seriously. That means papers to be reviewed are read, the
implications are considered, and decisions are only made after that. I do my
best to be fair, and there are zero subjects that I consider categorical
rejects. I don't consider severalarguments for rejection-not-on-the-merits
reasonable.I am generally interested in papers that (a) analyze new models of
machine learning, (b) provide new algorithms, and (c) show that they work
empirically on plausibly real problems. If a paper has the trifecta, I'm
particularly interested. With 2 out of 3, I might be interested. I often find
papers with only one element harder to accept, including papers with just
(a).I'm a bit tough. I rarely jump-up-and-down about a paper, because I
believe that great progress is ra</p><p>4 0.64296925 <a title="412-lda-4" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>Introduction: I wanted to expand on thispostand some of the previousproblems/research
directionsabout where learning theory might make large strides.Why theory?The
essential reason for theory is "intuition extension". A very good applied
learning person can master some particular application domain yielding the
best computer algorithms for solving that problem. A very good theory can take
the intuitions discovered by this and other applied learning people and extend
them to new domains in a relatively automatic fashion. To do this, we take
these basic intuitions and try to find a mathematical model that:Explains the
basic intuitions.Makes new testable predictions about how to learn.Succeeds in
so learning.This is "intuition extension": taking what we have learned
somewhere else and applying it in new domains. It is fundamentally useful to
everyone because it increases the level of automation in solving
problems.Where next for learning theory?I like the analogy with physics. Back
before we-the-humans</p><p>5 0.64203387 <a title="412-lda-5" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>Introduction: I want to try to describe what doing research means, especially from the point
of view of an undergraduate. The shift from a class-taking mentality to a
research mentality is very significant and not easy.Problem PosingPosing the
right problem is often as important as solving them. Many people can get by in
research by solving problems others have posed, but that's not sufficient for
really inspiring research. For learning in particular, there is a strong
feeling that we just haven't figured out which questions are the right ones to
ask. You can see this, because the answers we have do not seem
convincing.Gambling your lifeWhen you do research, you think very hard about
new ways of solving problems, new problems, and new solutions. Many
conversations are of the form "I wonder what would happen if…" These processes
can be short (days or weeks) or years-long endeavours. The worst part is that
you'll only know if you were succesful at the end of the process (and
sometimes not even then be</p><p>6 0.64115387 <a title="412-lda-6" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>7 0.64082915 <a title="412-lda-7" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>8 0.64013398 <a title="412-lda-8" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>9 0.63994908 <a title="412-lda-9" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>10 0.63958681 <a title="412-lda-10" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>11 0.63912421 <a title="412-lda-11" href="../hunch_net-2005/hunch_net-2005-03-21-Research_Styles_in_Machine_Learning.html">44 hunch net-2005-03-21-Research Styles in Machine Learning</a></p>
<p>12 0.63884389 <a title="412-lda-12" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>13 0.63826251 <a title="412-lda-13" href="../hunch_net-2006/hunch_net-2006-04-05-What_is_state%3F.html">169 hunch net-2006-04-05-What is state?</a></p>
<p>14 0.63794565 <a title="412-lda-14" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>15 0.63778102 <a title="412-lda-15" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>16 0.63763213 <a title="412-lda-16" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>17 0.63746446 <a title="412-lda-17" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>18 0.63723093 <a title="412-lda-18" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>19 0.6372025 <a title="412-lda-19" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>20 0.63668245 <a title="412-lda-20" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
