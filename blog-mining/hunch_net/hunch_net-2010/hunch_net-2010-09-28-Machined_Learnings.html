<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>412 hunch net-2010-09-28-Machined Learnings</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2010" href="../home/hunch_net-2010_home.html">hunch_net-2010</a> <a title="hunch_net-2010-412" href="#">hunch_net-2010-412</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>412 hunch net-2010-09-28-Machined Learnings</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2010-412-html" href="http://hunch.net/?p=1488">html</a></p><p>Introduction: Paul Mineiro  has started  Machined Learnings  where heâ&euro;&trade;s seriously attempting to do ML research in public.  I personally need to read through in greater detail, as much of it is learning reduction related, trying to deal with the sorts of complex source problems that come up in practice.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Paul Mineiro  has started  Machined Learnings  where heâ&euro;&trade;s seriously attempting to do ML research in public. [sent-1, score-0.933]
</p><p>2 I personally need to read through in greater detail, as much of it is learning reduction related, trying to deal with the sorts of complex source problems that come up in practice. [sent-2, score-2.393]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('attempting', 0.334), ('paul', 0.304), ('seriously', 0.304), ('detail', 0.296), ('sorts', 0.26), ('personally', 0.247), ('greater', 0.239), ('read', 0.208), ('complex', 0.203), ('started', 0.203), ('reduction', 0.201), ('practice', 0.197), ('ml', 0.189), ('source', 0.182), ('trying', 0.178), ('deal', 0.169), ('related', 0.159), ('need', 0.155), ('come', 0.15), ('research', 0.092), ('problems', 0.091), ('much', 0.085), ('learning', 0.025)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="412-tfidf-1" href="../hunch_net-2010/hunch_net-2010-09-28-Machined_Learnings.html">412 hunch net-2010-09-28-Machined Learnings</a></p>
<p>Introduction: Paul Mineiro  has started  Machined Learnings  where heâ&euro;&trade;s seriously attempting to do ML research in public.  I personally need to read through in greater detail, as much of it is learning reduction related, trying to deal with the sorts of complex source problems that come up in practice.</p><p>2 0.13750234 <a title="412-tfidf-2" href="../hunch_net-2006/hunch_net-2006-06-25-Presentation_of_Proofs_is_Hard..html">187 hunch net-2006-06-25-Presentation of Proofs is Hard.</a></p>
<p>Introduction: When presenting part of the  Reinforcement Learning theory tutorial  at  ICML 2006 , I was forcibly reminded of this.
 
There are several difficulties.
  
  When creating the presentation, the correct level of detail is tricky.  With too much detail, the proof takes too much time and people may be lost to boredom.  With too little detail, the steps of the proof involve too-great a jump. This is very difficult to judge.
 
 What may be an easy step in the careful thought of a quiet room is not so easy when you are occupied by the process of presentation. 
 What may be easy after having gone over this (and other) proofs is not so easy to follow in the first pass by a viewer. 
 

  These problems seem only correctable by process of repeated test-and-revise.
 
 When presenting the proof, simply speaking with sufficient precision is substantially harder than in normal conversation (where precision is not so critical).  Practice can help here. 
 When presenting the proof, going at the right p</p><p>3 0.12247958 <a title="412-tfidf-3" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>Introduction: Researchers are typically confronted with big problems that they have no idea how to solve.  In trying to come up with a solution, a natural approach is to decompose the big problem into a set of subproblems whose solution yields a solution to the larger problem.  This approach can go wrong in several ways. 
  
  Decomposition failure .  The solution to the decomposition does not in fact yield a solution to the overall problem. 
  Artificial hardness .  The subproblems created are sufficient if solved to solve the overall problem, but they are harder than necessary. 
  
As you can see, computational complexity forms a relatively new (in research-history) razor by which to judge an approach sufficient but not necessary.
 
In my experience, the artificial hardness problem is very common.  Many researchers abdicate the responsibility of choosing a problem to work on to other people.  This process starts very naturally as a graduate student, when an incoming student might have relatively l</p><p>4 0.10725865 <a title="412-tfidf-4" href="../hunch_net-2008/hunch_net-2008-11-26-Efficient_Reinforcement_Learning_in_MDPs.html">328 hunch net-2008-11-26-Efficient Reinforcement Learning in MDPs</a></p>
<p>Introduction: Claude Sammut  is attempting to put together an  Encyclopedia of Machine Learning .  I volunteered to write one article on  Efficient RL in MDPs , which I would like to invite comment on.  Is something critical missing?</p><p>5 0.10276197 <a title="412-tfidf-5" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>Introduction: This post is partly meant as an advertisement for the  reductions tutorial   Alina ,  Bianca , and I are planning to do at  ICML .  Please come, if you are interested.
 
Many research programs can be thought of as finding and building new useful abstractions.  The running example I’ll use is  learning reductions  where I have experience.  The basic abstraction here is that we can build a learning algorithm capable of solving classification problems up to a small expected regret.   This is used repeatedly to solve more complex problems.
 
In working on a new abstraction, I think you typically run into many substantial problems of understanding, which make publishing particularly difficult.
  
 It is difficult to seriously discuss the reason behind or mechanism for abstraction in a conference paper with small page limits.  People rarely see such discussions and hence have little basis on which to think about new abstractions.    Another difficulty is that when building an abstraction, yo</p><p>6 0.087962419 <a title="412-tfidf-6" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<p>7 0.083962545 <a title="412-tfidf-7" href="../hunch_net-2006/hunch_net-2006-05-23-What_is_the_best_regret_transform_reduction_from_multiclass_to_binary%3F.html">181 hunch net-2006-05-23-What is the best regret transform reduction from multiclass to binary?</a></p>
<p>8 0.081453986 <a title="412-tfidf-8" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>9 0.080861919 <a title="412-tfidf-9" href="../hunch_net-2006/hunch_net-2006-03-24-NLPers.html">166 hunch net-2006-03-24-NLPers</a></p>
<p>10 0.074096441 <a title="412-tfidf-10" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>11 0.073305458 <a title="412-tfidf-11" href="../hunch_net-2005/hunch_net-2005-04-22-New_Blog%3A_%5BLowerbounds%2CUpperbounds%5D.html">59 hunch net-2005-04-22-New Blog: [Lowerbounds,Upperbounds]</a></p>
<p>12 0.070472024 <a title="412-tfidf-12" href="../hunch_net-2006/hunch_net-2006-04-06-Bounds_greater_than_1.html">170 hunch net-2006-04-06-Bounds greater than 1</a></p>
<p>13 0.070324764 <a title="412-tfidf-13" href="../hunch_net-2005/hunch_net-2005-12-22-Yes_%2C_I_am_applying.html">142 hunch net-2005-12-22-Yes , I am applying</a></p>
<p>14 0.070260853 <a title="412-tfidf-14" href="../hunch_net-2005/hunch_net-2005-02-02-Kolmogorov_Complexity_and_Googling.html">10 hunch net-2005-02-02-Kolmogorov Complexity and Googling</a></p>
<p>15 0.066739708 <a title="412-tfidf-15" href="../hunch_net-2008/hunch_net-2008-08-18-Radford_Neal_starts_a_blog.html">313 hunch net-2008-08-18-Radford Neal starts a blog</a></p>
<p>16 0.066417493 <a title="412-tfidf-16" href="../hunch_net-2011/hunch_net-2011-10-24-2011_ML_symposium_and_the_bears.html">448 hunch net-2011-10-24-2011 ML symposium and the bears</a></p>
<p>17 0.066003658 <a title="412-tfidf-17" href="../hunch_net-2011/hunch_net-2011-08-15-Vowpal_Wabbit_6.0.html">441 hunch net-2011-08-15-Vowpal Wabbit 6.0</a></p>
<p>18 0.065528348 <a title="412-tfidf-18" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>19 0.062233903 <a title="412-tfidf-19" href="../hunch_net-2006/hunch_net-2006-03-17-Multitask_learning_is_Black-Boxable.html">164 hunch net-2006-03-17-Multitask learning is Black-Boxable</a></p>
<p>20 0.058845095 <a title="412-tfidf-20" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.114), (1, 0.002), (2, -0.069), (3, 0.048), (4, -0.045), (5, -0.002), (6, 0.063), (7, -0.085), (8, -0.052), (9, -0.01), (10, -0.026), (11, -0.033), (12, 0.02), (13, 0.057), (14, -0.026), (15, 0.026), (16, 0.044), (17, 0.058), (18, -0.059), (19, -0.065), (20, 0.03), (21, 0.018), (22, 0.014), (23, -0.007), (24, 0.027), (25, -0.119), (26, -0.07), (27, 0.014), (28, -0.036), (29, 0.069), (30, 0.022), (31, -0.03), (32, -0.03), (33, 0.02), (34, -0.051), (35, -0.007), (36, 0.013), (37, -0.0), (38, 0.001), (39, 0.063), (40, 0.014), (41, 0.048), (42, -0.012), (43, 0.032), (44, -0.01), (45, -0.102), (46, -0.029), (47, 0.147), (48, 0.039), (49, 0.064)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97950464 <a title="412-lsi-1" href="../hunch_net-2010/hunch_net-2010-09-28-Machined_Learnings.html">412 hunch net-2010-09-28-Machined Learnings</a></p>
<p>Introduction: Paul Mineiro  has started  Machined Learnings  where heâ&euro;&trade;s seriously attempting to do ML research in public.  I personally need to read through in greater detail, as much of it is learning reduction related, trying to deal with the sorts of complex source problems that come up in practice.</p><p>2 0.59273344 <a title="412-lsi-2" href="../hunch_net-2005/hunch_net-2005-02-25-Solution%3A_Reinforcement_Learning_with_Classification.html">29 hunch net-2005-02-25-Solution: Reinforcement Learning with Classification</a></p>
<p>Introduction: I realized that the tools needed to solve the  problem just posted  were just created.  I tried to sketch out the solution  here  (also in  .lyx  and  .tex ).  It is still quite sketchy (and probably only the few people who understand reductions well can follow).
 
One of the reasons why I started this weblog was to experiment with “research in the open”, and this is an opportunity to do so.  Over the next few days, I’ll be filling in details and trying to get things to make sense.  If you have additions or ideas, please propose them.</p><p>3 0.56425107 <a title="412-lsi-3" href="../hunch_net-2008/hunch_net-2008-08-18-Radford_Neal_starts_a_blog.html">313 hunch net-2008-08-18-Radford Neal starts a blog</a></p>
<p>Introduction: here  on statistics, ML, CS, and other things he knows well.</p><p>4 0.51575553 <a title="412-lsi-4" href="../hunch_net-2009/hunch_net-2009-12-24-Top_graduates_this_season.html">384 hunch net-2009-12-24-Top graduates this season</a></p>
<p>Introduction: I would like to point out 3 graduates this season as having my confidence they are capable of doing great things. 
  
  Daniel Hsu  has diverse papers with diverse coauthors on {active learning, mulitlabeling, temporal learning, …} each covering new algorithms and methods of analysis.  He is also a capable programmer, having helped me with some nitty-gritty details of cluster parallel  Vowpal Wabbit  this summer.  He has an excellent tendency to just get things done. 
  Nicolas Lambert  doesn’t nominally work in machine learning, but I’ve found his work in  elicitation  relevant nevertheless.  In essence, elicitable properties are closely related to learnable properties, and the elicitation complexity is related to a notion of learning complexity.  See the  Surrogate regret bounds paper  for some related discussion.  Few people successfully work at such a general level that it crosses fields, but he’s one of them. 
  Yisong Yue  is deeply focused on interactive learning, which he has a</p><p>5 0.51118106 <a title="412-lsi-5" href="../hunch_net-2008/hunch_net-2008-04-21-The_Science_2.0_article.html">296 hunch net-2008-04-21-The Science 2.0 article</a></p>
<p>Introduction: I found the article about  science using modern tools interesting , especially the part about ‘blogophobia’, which in my experience is often a substantial issue: many potential guest posters aren’t quite ready, because of the fear of a permanent public mistake, because it is particularly hard to write about the unknown (the essence of research), and because the system for public credit doesn’t yet really handle blog posts.
 
So far, science has been relatively resistant to discussing research on blogs.  Some things need to change to get there.  Public tolerance of the occasional mistake is essential, as is a willingness to cite (and credit) blogs as freely as papers.  
 
I’ve often run into another reason for holding back myself: I don’t want to overtalk my own research.  Nevertheless, I’m slowly changing to the opinion that I’m holding back too much: the real power of a blog in research is that it can be used to confer with many people, and that just makes research work better.</p><p>6 0.48553166 <a title="412-lsi-6" href="../hunch_net-2013/hunch_net-2013-12-01-NIPS_tutorials_and_Vowpal_Wabbit_7.4.html">492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</a></p>
<p>7 0.48068538 <a title="412-lsi-7" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>8 0.47762501 <a title="412-lsi-8" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>9 0.46652943 <a title="412-lsi-9" href="../hunch_net-2005/hunch_net-2005-05-29-Bad_ideas.html">76 hunch net-2005-05-29-Bad ideas</a></p>
<p>10 0.45328993 <a title="412-lsi-10" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>11 0.44401401 <a title="412-lsi-11" href="../hunch_net-2010/hunch_net-2010-08-21-Rob_Schapire_at_NYC_ML_Meetup.html">405 hunch net-2010-08-21-Rob Schapire at NYC ML Meetup</a></p>
<p>12 0.44061792 <a title="412-lsi-12" href="../hunch_net-2007/hunch_net-2007-11-02-The_Machine_Learning_Award_goes_to_%26%238230%3B.html">270 hunch net-2007-11-02-The Machine Learning Award goes to &#8230;</a></p>
<p>13 0.44033274 <a title="412-lsi-13" href="../hunch_net-2005/hunch_net-2005-01-24-Holy_grails_of_machine_learning%3F.html">2 hunch net-2005-01-24-Holy grails of machine learning?</a></p>
<p>14 0.43790674 <a title="412-lsi-14" href="../hunch_net-2006/hunch_net-2006-06-25-Presentation_of_Proofs_is_Hard..html">187 hunch net-2006-06-25-Presentation of Proofs is Hard.</a></p>
<p>15 0.43488777 <a title="412-lsi-15" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>16 0.4340812 <a title="412-lsi-16" href="../hunch_net-2007/hunch_net-2007-07-13-The_View_From_China.html">255 hunch net-2007-07-13-The View From China</a></p>
<p>17 0.43007812 <a title="412-lsi-17" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>18 0.42904359 <a title="412-lsi-18" href="../hunch_net-2010/hunch_net-2010-01-13-Sam_Roweis_died.html">386 hunch net-2010-01-13-Sam Roweis died</a></p>
<p>19 0.42798069 <a title="412-lsi-19" href="../hunch_net-2010/hunch_net-2010-10-17-Partha_Niyogi_has_died.html">414 hunch net-2010-10-17-Partha Niyogi has died</a></p>
<p>20 0.42014125 <a title="412-lsi-20" href="../hunch_net-2005/hunch_net-2005-08-23-%28Dis%29similarities_between_academia_and_open_source_programmers.html">105 hunch net-2005-08-23-(Dis)similarities between academia and open source programmers</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(27, 0.195), (53, 0.127), (61, 0.5)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.77871245 <a title="412-lda-1" href="../hunch_net-2010/hunch_net-2010-09-28-Machined_Learnings.html">412 hunch net-2010-09-28-Machined Learnings</a></p>
<p>Introduction: Paul Mineiro  has started  Machined Learnings  where heâ&euro;&trade;s seriously attempting to do ML research in public.  I personally need to read through in greater detail, as much of it is learning reduction related, trying to deal with the sorts of complex source problems that come up in practice.</p><p>2 0.67136896 <a title="412-lda-2" href="../hunch_net-2005/hunch_net-2005-09-04-Science_in_the_Government.html">106 hunch net-2005-09-04-Science in the Government</a></p>
<p>Introduction: I found the article on “ Political Science ” at the  New York Times  interesting.  Essentially the article is about allegations that the US government has been systematically distorting scientific views.   With a  petition  by some  7000+ scientists  alleging such behavior this is clearly a significant concern.
 
One thing not mentioned explicitly in this discussion is that there are fundamental cultural differences between academic research and the rest of the world.  In academic research, careful, clear thought is valued.  This value is achieved by both formal and informal mechanisms.  One example of a formal mechanism is peer review.
 
In contrast, in the land of politics, the basic value is agreement.  It is only with some amount of agreement that a new law can be passed or other actions can be taken.  Since Science (with a capitol ‘S’) has accomplished many things, it can be a significant tool in persuading people.  This makes it compelling for a politician to use science as a mec</p><p>3 0.62714112 <a title="412-lda-3" href="../hunch_net-2010/hunch_net-2010-10-29-To_Vidoelecture_or_not.html">416 hunch net-2010-10-29-To Vidoelecture or not</a></p>
<p>Introduction: (update:  cross-posted  on  CACM )
 
For the first time in several years,  ICML 2010  did not have  videolectures  attending.  Luckily, the  tutorial on exploration and learning  which  Alina  and I put together can  be viewed , since we also presented at  KDD 2010 , which included videolecture support. 
 
ICML didn’t cover the cost of a videolecture, because  PASCAL  didn’t provide a grant for it this year.  On the other hand, KDD covered it out of registration costs.  The cost of videolectures isn’t cheap.  For  a workshop  the baseline quote we have is 270 euro per hour, plus a similar cost for the cameraman’s travel and accomodation.  This can be reduced substantially by having a volunteer with a camera handle the cameraman duties, uploading the video and slides to be processed for a quoted 216 euro per hour.
 
 Youtube  is the most predominant free video site with a cost of $0, but it turns out to be a poor alternative.   15 minute upload limits  do not match typical talk lengths.</p><p>4 0.54899186 <a title="412-lda-4" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>Introduction: Many learning algorithms used in practice are fairly simple.  Viewed representationally, many prediction algorithms either compute a linear separator of basic features (perceptron, winnow, weighted majority, SVM) or perhaps a linear separator of slightly more complex features (2-layer neural networks or kernelized SVMs).  Should we go beyond this, and start using “deep” representations?
 
 What is deep learning?  
Intuitively, deep learning is about learning to predict in ways which can involve complex dependencies between the input (observed) features.
 
Specifying this more rigorously turns out to be rather difficult.  Consider the following cases:
  
 SVM with Gaussian Kernel.  This is not considered deep learning, because an SVM with a gaussian kernel can’t succinctly represent certain decision surfaces.  One of  Yann LeCun ‘s examples is recognizing objects based on pixel values.  An SVM will need a new support vector for each significantly different background.  Since the number</p><p>5 0.39810497 <a title="412-lda-5" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>Introduction: Let’s define a learning problem as making predictions given past data. There are several ways to attack the learning problem which seem to be equivalent to solving the learning problem.
  
  Find the Invariant  This viewpoint says that learning is all about learning (or incorporating) transformations of objects that do not change the correct prediction. The best possible invariant is the one which says “all things of the same class are the same”.  Finding this is equivalent to learning.  This viewpoint is particularly common when working with image features.  
  Feature Selection  This viewpoint says that the way to learn is by finding the right features to input to a learning algorithm.  The best feature is the one which is the class to predict.  Finding this is equivalent to learning for all reasonable learning algorithms.  This viewpoint is common in several  applications of machine learning.  See  Gilad’s and Bianca’s comments . 
  Find the Representation  This is almost the same a</p><p>6 0.39135665 <a title="412-lda-6" href="../hunch_net-2013/hunch_net-2013-06-10-The_Large_Scale_Learning_class_notes.html">483 hunch net-2013-06-10-The Large Scale Learning class notes</a></p>
<p>7 0.38414311 <a title="412-lda-7" href="../hunch_net-2007/hunch_net-2007-01-10-A_Deep_Belief_Net_Learning_Problem.html">227 hunch net-2007-01-10-A Deep Belief Net Learning Problem</a></p>
<p>8 0.38279158 <a title="412-lda-8" href="../hunch_net-2006/hunch_net-2006-01-30-Should_the_Input_Representation_be_a_Vector%3F.html">152 hunch net-2006-01-30-Should the Input Representation be a Vector?</a></p>
<p>9 0.38216224 <a title="412-lda-9" href="../hunch_net-2005/hunch_net-2005-04-23-Advantages_and_Disadvantages_of_Bayesian_Learning.html">60 hunch net-2005-04-23-Advantages and Disadvantages of Bayesian Learning</a></p>
<p>10 0.37976512 <a title="412-lda-10" href="../hunch_net-2005/hunch_net-2005-02-01-Watchword%3A_Loss.html">9 hunch net-2005-02-01-Watchword: Loss</a></p>
<p>11 0.37938532 <a title="412-lda-11" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>12 0.377749 <a title="412-lda-12" href="../hunch_net-2013/hunch_net-2013-01-07-NYU_Large_Scale_Machine_Learning_Class.html">478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</a></p>
<p>13 0.37691477 <a title="412-lda-13" href="../hunch_net-2005/hunch_net-2005-05-06-Don%26%238217%3Bt_mix_the_solution_into_the_problem.html">67 hunch net-2005-05-06-Don&#8217;t mix the solution into the problem</a></p>
<p>14 0.37472695 <a title="412-lda-14" href="../hunch_net-2005/hunch_net-2005-03-15-The_State_of_Tight_Bounds.html">41 hunch net-2005-03-15-The State of Tight Bounds</a></p>
<p>15 0.37458235 <a title="412-lda-15" href="../hunch_net-2005/hunch_net-2005-02-23-Problem%3A_Reinforcement_Learning_with_Classification.html">27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</a></p>
<p>16 0.37439787 <a title="412-lda-16" href="../hunch_net-2005/hunch_net-2005-02-03-Learning_Theory%2C_by_assumption.html">12 hunch net-2005-02-03-Learning Theory, by assumption</a></p>
<p>17 0.37418139 <a title="412-lda-17" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>18 0.37415677 <a title="412-lda-18" href="../hunch_net-2005/hunch_net-2005-11-16-The_Everything_Ensemble_Edge.html">131 hunch net-2005-11-16-The Everything Ensemble Edge</a></p>
<p>19 0.37278551 <a title="412-lda-19" href="../hunch_net-2009/hunch_net-2009-08-16-Centmail_comments.html">367 hunch net-2009-08-16-Centmail comments</a></p>
<p>20 0.37153113 <a title="412-lda-20" href="../hunch_net-2005/hunch_net-2005-01-24-The_Humanloop_Spectrum_of_Machine_Learning.html">3 hunch net-2005-01-24-The Humanloop Spectrum of Machine Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
