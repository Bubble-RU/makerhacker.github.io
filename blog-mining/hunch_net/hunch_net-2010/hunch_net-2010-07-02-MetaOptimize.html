<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>402 hunch net-2010-07-02-MetaOptimize</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2010" href="../home/hunch_net-2010_home.html">hunch_net-2010</a> <a title="hunch_net-2010-402" href="#">hunch_net-2010-402</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>402 hunch net-2010-07-02-MetaOptimize</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2010-402-html" href="http://hunch.net/?p=1425">html</a></p><p>Introduction: Joseph TuriancreatesMetaOptimizefor discussion of NLP and ML on big datasets.
This includes ablog, but perhaps more importantly aquestion and answer
section. I'm hopeful it will take off.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Joseph TuriancreatesMetaOptimizefor discussion of NLP and ML on big datasets. [sent-1, score-0.448]
</p><p>2 This includes ablog, but perhaps more importantly aquestion and answer section. [sent-2, score-1.152]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hopeful', 0.491), ('nlp', 0.434), ('importantly', 0.408), ('includes', 0.356), ('ml', 0.259), ('discussion', 0.235), ('big', 0.213), ('answer', 0.207), ('take', 0.187), ('perhaps', 0.181)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="402-tfidf-1" href="../hunch_net-2010/hunch_net-2010-07-02-MetaOptimize.html">402 hunch net-2010-07-02-MetaOptimize</a></p>
<p>Introduction: Joseph TuriancreatesMetaOptimizefor discussion of NLP and ML on big datasets.
This includes ablog, but perhaps more importantly aquestion and answer
section. I'm hopeful it will take off.</p><p>2 0.12545274 <a title="402-tfidf-2" href="../hunch_net-2013/hunch_net-2013-06-10-The_Large_Scale_Learning_class_notes.html">483 hunch net-2013-06-10-The Large Scale Learning class notes</a></p>
<p>Introduction: Thelarge scale machine learning classI taught withYann LeCunhas finished. As I
expected, it took quite a bit of time. We had about 25 people attending in
person on average and 400 regularly watching therecorded lectureswhich is
substantially more sustained interest than I expected for an advanced ML
class. We also had some fun with class projects--I'm hopeful that several will
eventually turn into papers.I expect there are a number of professors
interested in lecturing on this and related topics. Everyone will have their
personal taste in subjects of course, but hopefully there will be some
convergence to common course materials as well. To help with this, I am making
thesources to my presentations available. Feel free to
use/improve/embelish/ridicule/etcâ&euro;Ś in the pursuit of the perfect course.</p><p>3 0.092549548 <a title="402-tfidf-3" href="../hunch_net-2008/hunch_net-2008-08-18-Radford_Neal_starts_a_blog.html">313 hunch net-2008-08-18-Radford Neal starts a blog</a></p>
<p>Introduction: hereon statistics, ML, CS, and other things he knows well.</p><p>4 0.074709259 <a title="402-tfidf-4" href="../hunch_net-2007/hunch_net-2007-07-20-Motivation_should_be_the_Responsibility_of_the_Reviewer.html">256 hunch net-2007-07-20-Motivation should be the Responsibility of the Reviewer</a></p>
<p>Introduction: The prevailing wisdom in machine learning seems to be that motivating a paper
is the responsibility of the author. I think this is a harmful view--instead,
it's healthier for the community to regard this as the responsibility of the
reviewer.There are lots of reasons to prefer a reviewer-responsibility
approach.Authors are the most biased possible source of information about the
motivation of the paper. Systems which rely upon very biased sources of
information are inherently unreliable.Authors are highly variable in their
ability and desire to express motivation for their work. This adds greatly to
variance on acceptance of an idea, and it can systematically discriminate or
accentuate careers. It's great if you have a career accentuated by awesome
wording choice, but wise decision making by reviewers is important for the
field.The motivation section in a paper doesn'tdoanything in some sense--it's
there to get the paper in. Reading the motivation of a paper is of little use
in helping</p><p>5 0.068011887 <a title="402-tfidf-5" href="../hunch_net-2013/hunch_net-2013-03-22-I%26%238217%3Bm_a_bandit.html">480 hunch net-2013-03-22-I&#8217;m a bandit</a></p>
<p>Introduction: Sebastien Bubeck has anew ML blogfocused on optimization and partial feedback
which may interest people.</p><p>6 0.068002895 <a title="402-tfidf-6" href="../hunch_net-2007/hunch_net-2007-04-30-COLT_2007.html">242 hunch net-2007-04-30-COLT 2007</a></p>
<p>7 0.060572583 <a title="402-tfidf-7" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>8 0.05871797 <a title="402-tfidf-8" href="../hunch_net-2008/hunch_net-2008-07-15-Interesting_papers_at_COLT_%28and_a_bit_of_UAI_%26%23038%3B_workshops%29.html">310 hunch net-2008-07-15-Interesting papers at COLT (and a bit of UAI &#038; workshops)</a></p>
<p>9 0.058040168 <a title="402-tfidf-9" href="../hunch_net-2011/hunch_net-2011-10-24-2011_ML_symposium_and_the_bears.html">448 hunch net-2011-10-24-2011 ML symposium and the bears</a></p>
<p>10 0.05735549 <a title="402-tfidf-10" href="../hunch_net-2005/hunch_net-2005-07-07-The_Limits_of_Learning_Theory.html">90 hunch net-2005-07-07-The Limits of Learning Theory</a></p>
<p>11 0.056167632 <a title="402-tfidf-11" href="../hunch_net-2005/hunch_net-2005-07-04-The_Health_of_COLT.html">89 hunch net-2005-07-04-The Health of COLT</a></p>
<p>12 0.054720905 <a title="402-tfidf-12" href="../hunch_net-2008/hunch_net-2008-04-26-Eliminating_the_Birthday_Paradox_for_Universal_Features.html">298 hunch net-2008-04-26-Eliminating the Birthday Paradox for Universal Features</a></p>
<p>13 0.053620216 <a title="402-tfidf-13" href="../hunch_net-2010/hunch_net-2010-06-20-2010_ICML_discussion_site.html">401 hunch net-2010-06-20-2010 ICML discussion site</a></p>
<p>14 0.053385653 <a title="402-tfidf-14" href="../hunch_net-2006/hunch_net-2006-01-18-Is_Multitask_Learning_Black-Boxable%3F.html">149 hunch net-2006-01-18-Is Multitask Learning Black-Boxable?</a></p>
<p>15 0.052623358 <a title="402-tfidf-15" href="../hunch_net-2007/hunch_net-2007-09-30-NIPS_workshops_are_out..html">264 hunch net-2007-09-30-NIPS workshops are out.</a></p>
<p>16 0.049583741 <a title="402-tfidf-16" href="../hunch_net-2010/hunch_net-2010-09-28-Machined_Learnings.html">412 hunch net-2010-09-28-Machined Learnings</a></p>
<p>17 0.049153794 <a title="402-tfidf-17" href="../hunch_net-2011/hunch_net-2011-02-17-What_does_Watson_mean%3F.html">424 hunch net-2011-02-17-What does Watson mean?</a></p>
<p>18 0.046980761 <a title="402-tfidf-18" href="../hunch_net-2006/hunch_net-2006-09-18-What_is_missing_for_online_collaborative_research%3F.html">208 hunch net-2006-09-18-What is missing for online collaborative research?</a></p>
<p>19 0.046508711 <a title="402-tfidf-19" href="../hunch_net-2008/hunch_net-2008-02-10-Complexity_Illness.html">288 hunch net-2008-02-10-Complexity Illness</a></p>
<p>20 0.046427809 <a title="402-tfidf-20" href="../hunch_net-2007/hunch_net-2007-11-02-The_Machine_Learning_Award_goes_to_%26%238230%3B.html">270 hunch net-2007-11-02-The Machine Learning Award goes to &#8230;</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.059), (1, 0.026), (2, 0.034), (3, -0.008), (4, 0.007), (5, 0.012), (6, -0.025), (7, -0.013), (8, 0.06), (9, 0.01), (10, -0.002), (11, 0.008), (12, 0.033), (13, 0.008), (14, 0.013), (15, 0.043), (16, -0.016), (17, 0.076), (18, 0.015), (19, -0.007), (20, 0.054), (21, 0.02), (22, -0.025), (23, 0.059), (24, 0.029), (25, 0.009), (26, 0.037), (27, 0.045), (28, -0.007), (29, 0.022), (30, 0.079), (31, 0.009), (32, -0.038), (33, -0.11), (34, 0.086), (35, 0.026), (36, 0.017), (37, -0.066), (38, -0.12), (39, -0.001), (40, -0.021), (41, -0.031), (42, 0.086), (43, 0.069), (44, -0.125), (45, 0.053), (46, 0.02), (47, -0.026), (48, 0.069), (49, 0.02)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99473673 <a title="402-lsi-1" href="../hunch_net-2010/hunch_net-2010-07-02-MetaOptimize.html">402 hunch net-2010-07-02-MetaOptimize</a></p>
<p>Introduction: Joseph TuriancreatesMetaOptimizefor discussion of NLP and ML on big datasets.
This includes ablog, but perhaps more importantly aquestion and answer
section. I'm hopeful it will take off.</p><p>2 0.64357734 <a title="402-lsi-2" href="../hunch_net-2008/hunch_net-2008-08-18-Radford_Neal_starts_a_blog.html">313 hunch net-2008-08-18-Radford Neal starts a blog</a></p>
<p>Introduction: hereon statistics, ML, CS, and other things he knows well.</p><p>3 0.49954835 <a title="402-lsi-3" href="../hunch_net-2013/hunch_net-2013-03-22-I%26%238217%3Bm_a_bandit.html">480 hunch net-2013-03-22-I&#8217;m a bandit</a></p>
<p>Introduction: Sebastien Bubeck has anew ML blogfocused on optimization and partial feedback
which may interest people.</p><p>4 0.48670354 <a title="402-lsi-4" href="../hunch_net-2007/hunch_net-2007-07-28-Asking_questions.html">257 hunch net-2007-07-28-Asking questions</a></p>
<p>Introduction: There are very substantial differences in how question asking is viewed
culturally. For example, all of the following are common:If no one asks a
question, then no one is paying attention.To ask a question is disrespectful
of the speaker.Asking a question is admitting your own ignorance.The first
view seems to be the right one for research, for several reasons.Research is
quite hard--it's difficult to guess how people won't understand something in
advance while preparing a presentation. Consequently, it's very common to lose
people. No worthwhile presenter wants that.Real understanding is precious. By
asking a question, you are really declaring "I want to understand", and
everyone should respect that.Asking a question wakes you up. I don't mean from
"asleep" to "awake" but from "awake" to "really awake". It's easy to drift
through something sort-of-understanding. When you ask a question, especially
because you are on the spot, you will do much better.Some of these effects
might seem mi</p><p>5 0.473032 <a title="402-lsi-5" href="../hunch_net-2006/hunch_net-2006-03-17-Multitask_learning_is_Black-Boxable.html">164 hunch net-2006-03-17-Multitask learning is Black-Boxable</a></p>
<p>Introduction: Multitask learning is the problem of jointly predicting multiple labels
simultaneously with one system. A basic question iswhether or not multitask
learning can be decomposed into one (or more) single prediction problems. It
seems the answer to this is "yes", in a fairly straightforward manner.The
basic idea is that a controlled input feature is equivalent to an extra
output. Suppose we have some process generating examples:(x,y1,y2) in
Swherey1andy2are labels for two different tasks. Then, we could reprocess the
data to the formSb(S) = {((x,i),yi): (x,y1,y2) in S, i in {1,2}}and then learn
a classifierc:X x {1,2} -> Y. Note that(x,i)is the (composite) input. At
testing time, given an inputx, we can querycfor the predicted values of y1and
y2using(x,1)and(x,2).A strong form of equivalence can be stated between these
tasks. In particular, suppose we have a multitask learning algorithmMLwhich
learns a multitask predictorm:X -> Y x Y. Then the following theorem can be
proved:For allMLfor a</p><p>6 0.4378399 <a title="402-lsi-6" href="../hunch_net-2006/hunch_net-2006-01-18-Is_Multitask_Learning_Black-Boxable%3F.html">149 hunch net-2006-01-18-Is Multitask Learning Black-Boxable?</a></p>
<p>7 0.40681538 <a title="402-lsi-7" href="../hunch_net-2011/hunch_net-2011-10-24-2011_ML_symposium_and_the_bears.html">448 hunch net-2011-10-24-2011 ML symposium and the bears</a></p>
<p>8 0.40552855 <a title="402-lsi-8" href="../hunch_net-2010/hunch_net-2010-08-21-Rob_Schapire_at_NYC_ML_Meetup.html">405 hunch net-2010-08-21-Rob Schapire at NYC ML Meetup</a></p>
<p>9 0.39517778 <a title="402-lsi-9" href="../hunch_net-2005/hunch_net-2005-04-22-New_Blog%3A_%5BLowerbounds%2CUpperbounds%5D.html">59 hunch net-2005-04-22-New Blog: [Lowerbounds,Upperbounds]</a></p>
<p>10 0.3330549 <a title="402-lsi-10" href="../hunch_net-2006/hunch_net-2006-12-05-Recruitment_Conferences.html">222 hunch net-2006-12-05-Recruitment Conferences</a></p>
<p>11 0.32325074 <a title="402-lsi-11" href="../hunch_net-2010/hunch_net-2010-09-28-Machined_Learnings.html">412 hunch net-2010-09-28-Machined Learnings</a></p>
<p>12 0.31715372 <a title="402-lsi-12" href="../hunch_net-2009/hunch_net-2009-01-19-Netflix_prize_within_epsilon.html">336 hunch net-2009-01-19-Netflix prize within epsilon</a></p>
<p>13 0.31295368 <a title="402-lsi-13" href="../hunch_net-2009/hunch_net-2009-12-09-Future_Publication_Models_%40_NIPS.html">382 hunch net-2009-12-09-Future Publication Models @ NIPS</a></p>
<p>14 0.30877998 <a title="402-lsi-14" href="../hunch_net-2013/hunch_net-2013-01-31-Remote_large_scale_learning_class_participation.html">479 hunch net-2013-01-31-Remote large scale learning class participation</a></p>
<p>15 0.3084963 <a title="402-lsi-15" href="../hunch_net-2013/hunch_net-2013-06-10-The_Large_Scale_Learning_class_notes.html">483 hunch net-2013-06-10-The Large Scale Learning class notes</a></p>
<p>16 0.3067832 <a title="402-lsi-16" href="../hunch_net-2010/hunch_net-2010-06-20-2010_ICML_discussion_site.html">401 hunch net-2010-06-20-2010 ICML discussion site</a></p>
<p>17 0.30371556 <a title="402-lsi-17" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>18 0.30052489 <a title="402-lsi-18" href="../hunch_net-2007/hunch_net-2007-06-19-How_is_Compressed_Sensing_going_to_change_Machine_Learning_%3F.html">248 hunch net-2007-06-19-How is Compressed Sensing going to change Machine Learning ?</a></p>
<p>19 0.2986418 <a title="402-lsi-19" href="../hunch_net-2005/hunch_net-2005-01-24-Holy_grails_of_machine_learning%3F.html">2 hunch net-2005-01-24-Holy grails of machine learning?</a></p>
<p>20 0.29816028 <a title="402-lsi-20" href="../hunch_net-2008/hunch_net-2008-02-27-The_Stats_Handicap.html">290 hunch net-2008-02-27-The Stats Handicap</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(42, 0.212), (65, 0.541)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.84882772 <a title="402-lda-1" href="../hunch_net-2010/hunch_net-2010-07-02-MetaOptimize.html">402 hunch net-2010-07-02-MetaOptimize</a></p>
<p>Introduction: Joseph TuriancreatesMetaOptimizefor discussion of NLP and ML on big datasets.
This includes ablog, but perhaps more importantly aquestion and answer
section. I'm hopeful it will take off.</p><p>2 0.73140591 <a title="402-lda-2" href="../hunch_net-2008/hunch_net-2008-11-16-Observations_on_Linearity_for_Reductions_to_Regression.html">327 hunch net-2008-11-16-Observations on Linearity for Reductions to Regression</a></p>
<p>Introduction: Dean FosterandDaniel Hsuhad a couple observations about reductions to
regression that I wanted to share. This will make the most sense for people
familiar with error correcting output codes (see thetutorial, page 11).Many
people are comfortable using linear regression in a one-against-all style,
where you try to predict the probability of choiceivs other classes, yet they
are not comfortable with more complex error correcting codes because they fear
that they create harder problems. This fear turns out to be mathematically
incoherent under a linear representation: comfort in the linear case should
imply comfort with more complex codes.In particular, If there exists a set of
weight vectorswisuch thatP(i|x)=, then for any invertible error
correcting output codeC, there exists weight vectorswcwhich decode to
perfectly predict the probability of each class. The proof is simple and
constructive: the weight vectorwccan be constructed according to the linear
superposition ofwiimplied b</p><p>3 0.64319241 <a title="402-lda-3" href="../hunch_net-2009/hunch_net-2009-11-06-Yisong_Yue_on_Self-improving_Systems.html">376 hunch net-2009-11-06-Yisong Yue on Self-improving Systems</a></p>
<p>Introduction: I'd like to point outYisong Yue'spost on Self-improving systems, which is a
nicely readable description of the necessity and potential of interactive
learning to deal with the information overload problem that is endemic to the
modern internet.</p><p>4 0.5550819 <a title="402-lda-4" href="../hunch_net-2005/hunch_net-2005-07-21-Six_Months.html">96 hunch net-2005-07-21-Six Months</a></p>
<p>Introduction: This is the6 month pointin the "run a research blog" experiment, so it seems
like a good point to take stock and assess.One fundamental question is: "Is it
worth it?" The idea of running a research blog will never become widely
popular and useful unless it actually aids research. On the negative side,
composing ideas for a post and maintaining a blog takes a significant amount
of time. On the positive side, the process might yield better research because
there is an opportunity for better, faster feedback implying better, faster
thinking.My answer at the moment is a provisional "yes". Running the blog has
been incidentally helpful in several ways:It is sometimes
educational.exampleMore often, the process of composing thoughts well enough
to post simply aids thinking. This has resulted in a couple solutions to
problems ofinterest(and perhaps more over time). If you really want to solve a
problem, letting the world know is helpful. This isn't necessarily because the
world will help you s</p><p>5 0.38053209 <a title="402-lda-5" href="../hunch_net-2005/hunch_net-2005-05-16-Regret_minimizing_vs_error_limiting_reductions.html">72 hunch net-2005-05-16-Regret minimizing vs error limiting reductions</a></p>
<p>Introduction: This post is about a reductions-related problem that I find mysterious. There
are two kinds of reductions analysis currently under consideration.Error
limiting reductions. Here, the goal is to bound the error rate of the created
classifier in terms of the error rate of the binary classifiers that you
reduce to. A very simple example of this is thaterror correcting output
codeswhere it is possible to prove that for certain codes, the multiclass
error rate is at most 4 * the binary classifier error rate.Regret minimizing
reductions. Here, the goal is to bound theregretof the created classifier in
terms of theregretof the binary classifiers reduced to. The regret is the
error rate minus the minimum error rate. When the learning problem is noisy
the minimum error rate may not be0. An analagous result for reget is that for
aprobabilistic error correcting output code, multiclass regret is at most 4 *
(binary regret)0.5.The use of "regret" is more desirable than the use of error
rates, becaus</p><p>6 0.36451951 <a title="402-lda-6" href="../hunch_net-2008/hunch_net-2008-04-27-Watchword%3A_Supervised_Learning.html">299 hunch net-2008-04-27-Watchword: Supervised Learning</a></p>
<p>7 0.36425632 <a title="402-lda-7" href="../hunch_net-2006/hunch_net-2006-01-23-On_Coding_via_Mutual_Information_%26%23038%3B_Bayes_Nets.html">150 hunch net-2006-01-23-On Coding via Mutual Information &#038; Bayes Nets</a></p>
<p>8 0.36423299 <a title="402-lda-8" href="../hunch_net-2012/hunch_net-2012-10-26-ML_Symposium_and_Strata-Hadoop_World.html">475 hunch net-2012-10-26-ML Symposium and Strata-Hadoop World</a></p>
<p>9 0.36413047 <a title="402-lda-9" href="../hunch_net-2005/hunch_net-2005-05-06-Don%26%238217%3Bt_mix_the_solution_into_the_problem.html">67 hunch net-2005-05-06-Don&#8217;t mix the solution into the problem</a></p>
<p>10 0.36387253 <a title="402-lda-10" href="../hunch_net-2005/hunch_net-2005-12-09-Some_NIPS_papers.html">138 hunch net-2005-12-09-Some NIPS papers</a></p>
<p>11 0.36383587 <a title="402-lda-11" href="../hunch_net-2006/hunch_net-2006-06-15-IJCAI_is_out_of_season.html">184 hunch net-2006-06-15-IJCAI is out of season</a></p>
<p>12 0.36362633 <a title="402-lda-12" href="../hunch_net-2005/hunch_net-2005-03-29-Academic_Mechanism_Design.html">48 hunch net-2005-03-29-Academic Mechanism Design</a></p>
<p>13 0.36298591 <a title="402-lda-13" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>14 0.36290929 <a title="402-lda-14" href="../hunch_net-2005/hunch_net-2005-07-07-The_Limits_of_Learning_Theory.html">90 hunch net-2005-07-07-The Limits of Learning Theory</a></p>
<p>15 0.35995719 <a title="402-lda-15" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>16 0.35949147 <a title="402-lda-16" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<p>17 0.3563059 <a title="402-lda-17" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>18 0.35404158 <a title="402-lda-18" href="../hunch_net-2005/hunch_net-2005-02-23-Problem%3A_Reinforcement_Learning_with_Classification.html">27 hunch net-2005-02-23-Problem: Reinforcement Learning with Classification</a></p>
<p>19 0.35259461 <a title="402-lda-19" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>20 0.35110489 <a title="402-lda-20" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
