<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>388 hunch net-2010-01-24-Specializations of the Master Problem</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2010" href="../home/hunch_net-2010_home.html">hunch_net-2010</a> <a title="hunch_net-2010-388" href="#">hunch_net-2010-388</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>388 hunch net-2010-01-24-Specializations of the Master Problem</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2010-388-html" href="http://hunch.net/?p=1167">html</a></p><p>Introduction: One thing which is clear on a little reflection is that there exists a single
master learning problem capable of encoding essentially all learning problems.
This problem is of course a very general sort of reinforcement learning where
the world interacts with an agent as:The world announces an observationx.The
agent makes a choicea.The world announces a rewardr.The goal here is to
maximize the sum of the rewards over the time of the agent. No particular
structure relatingxtoaoratoris implied by this setting so we do not know
effective general algorithms for the agent. It's very easy to prove lower
bounds showing that an agent cannot hope to succeed here--just consider the
case where actions are unrelated to rewards. Nevertheless, there is a real
sense in which essentially all forms of life are agents operating in this
setting, somehow succeeding. The gap between these observations drives
research--How can we find tractable specializations of the master problem
general enough to provide</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('setting', 0.319), ('dependence', 0.222), ('oracle', 0.196), ('tractable', 0.169), ('step', 0.16), ('mdp', 0.157), ('observations', 0.143), ('agent', 0.139), ('compete', 0.135), ('policy', 0.133), ('states', 0.118), ('observation', 0.117), ('world', 0.111), ('canonical', 0.11), ('policies', 0.107), ('master', 0.099), ('computationally', 0.099), ('specialize', 0.098), ('onlyo', 0.098), ('statistic', 0.098)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999958 <a title="388-tfidf-1" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>Introduction: One thing which is clear on a little reflection is that there exists a single
master learning problem capable of encoding essentially all learning problems.
This problem is of course a very general sort of reinforcement learning where
the world interacts with an agent as:The world announces an observationx.The
agent makes a choicea.The world announces a rewardr.The goal here is to
maximize the sum of the rewards over the time of the agent. No particular
structure relatingxtoaoratoris implied by this setting so we do not know
effective general algorithms for the agent. It's very easy to prove lower
bounds showing that an agent cannot hope to succeed here--just consider the
case where actions are unrelated to rewards. Nevertheless, there is a real
sense in which essentially all forms of life are agents operating in this
setting, somehow succeeding. The gap between these observations drives
research--How can we find tractable specializations of the master problem
general enough to provide</p><p>2 0.33584365 <a title="388-tfidf-2" href="../hunch_net-2008/hunch_net-2008-09-12-How_do_we_get_weak_action_dependence_for_learning_with_partial_observations%3F.html">317 hunch net-2008-09-12-How do we get weak action dependence for learning with partial observations?</a></p>
<p>Introduction: This post is about contextual bandit problems where, repeatedly:The world
chooses featuresxand rewards for each actionr1,â&euro;Ś,rkthen announces the
featuresx(but not the rewards).A policy chooses an actiona.The world announces
the rewardraThe goal in these situations is to learn a policy which
maximizesrain expectation efficiently. I'm thinking about all situations which
fit the above setting, whether they are drawn IID or adversarially from round
to round and whether they involve past logged data or rapidly learning via
interaction.One common drawback of all algorithms for solving this setting, is
that they have a poor dependence on the number of actions. For example ifkis
the number of actions,EXP4 (page 66)has a dependence onk0.5,epoch-greedy(and
the simpler epsilon greedy) have a dependence onk1/3, and theoffset treehas a
dependence onk-1. These results aren't directly comparable because different
things are being analyzed. The fact thatallanalyses have poor dependence onkis
troublesom</p><p>3 0.22515517 <a title="388-tfidf-3" href="../hunch_net-2007/hunch_net-2007-10-24-Contextual_Bandits.html">269 hunch net-2007-10-24-Contextual Bandits</a></p>
<p>Introduction: One of the fundamental underpinnings of the internet is advertising based
content. This has become much more effective due to targeted advertising where
ads are specifically matched to interests. Everyone is familiar with this,
because everyone uses search engines and all search engines try to make money
this way.The problem of matching ads to interests is a natural machine
learning problem in some ways since there is much information in who clicks on
what. A fundamental problem with this information is that it is not supervised
--in particular a click-or-not on one ad doesn't generally tell you if a
different ad would have been clicked on. This implies we have a fundamental
exploration problem.A standard mathematical setting for this situation is
"k-Armed Bandits", often with various relevant embellishments. Thek-Armed
Bandit setting works on a round-by-round basis. On each round:A policy chooses
armafrom1ofkarms (i.e. 1 of k ads).The world reveals the rewardraof the chosen
arm (i.e.</p><p>4 0.22458285 <a title="388-tfidf-4" href="../hunch_net-2010/hunch_net-2010-03-15-The_Efficient_Robust_Conditional_Probability_Estimation_Problem.html">391 hunch net-2010-03-15-The Efficient Robust Conditional Probability Estimation Problem</a></p>
<p>Introduction: I'm offering a reward of $1000 for a solution to this problem. This joins
thecross validation problemwhich I'm offering a$500 rewardfor. I believe both
of these problems are hard but plausibly solvable, and plausibly with a
solution of substantial practical value. While it's unlikely these rewards are
worth your time on an hourly wage basis, the recognition for solving them
definitely should beThe ProblemThe problem is finding a general, robust, and
efficient mechanism for estimating a conditional probabilityP(y|x)where
robustness and efficiency are measured using techniques from learning
reductions.In particular, suppose we have access to a binary regression
oracleBwhich has two interfaces--one for specifying training information and
one for testing. Training information is specified asB(x',y')wherex'is a
feature vector andy'is a scalar in[0,1]with no value returned. Testing is done
according toB(x')with a value in[0,1]returned.A learning reduction consists of
two algorithmsRandR-1whi</p><p>5 0.2192906 <a title="388-tfidf-5" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>Introduction: This post is about a general technique for problem solving which I've never
seen taught (in full generality), but which I've found very useful.Many
problems in computer science turn out to be discretely difficult. The best
known version of such problems are NP-hard problems, but I mean 'discretely
difficult' in a much more general way, which I only know how to capture by
examples.ERMIn empirical risk minimization, you choose a minimum error rate
classifier from a set of classifiers. This is NP hard for common sets, but it
can be much harder, depending on the set.ExpertsIn the online learning with
experts setting, you try to predict well so as to compete with a set of
(adversarial) experts. Here the alternating quantifiers of you and an
adversary playing out a game can yield a dynamic programming problem that
grows exponentially.Policy IterationThe problem with policy iteration is that
you learn a new policy with respect to an old policy, which implies that
simply adopting the new polic</p><p>6 0.21272756 <a title="388-tfidf-6" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>7 0.19682485 <a title="388-tfidf-7" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>8 0.1917264 <a title="388-tfidf-8" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>9 0.18542169 <a title="388-tfidf-9" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>10 0.18187343 <a title="388-tfidf-10" href="../hunch_net-2005/hunch_net-2005-11-02-Progress_in_Active_Learning.html">127 hunch net-2005-11-02-Progress in Active Learning</a></p>
<p>11 0.17262055 <a title="388-tfidf-11" href="../hunch_net-2007/hunch_net-2007-07-01-Watchword%3A_Online_Learning.html">252 hunch net-2007-07-01-Watchword: Online Learning</a></p>
<p>12 0.17151417 <a title="388-tfidf-12" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>13 0.16562721 <a title="388-tfidf-13" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<p>14 0.16109397 <a title="388-tfidf-14" href="../hunch_net-2006/hunch_net-2006-04-05-What_is_state%3F.html">169 hunch net-2006-04-05-What is state?</a></p>
<p>15 0.16029814 <a title="388-tfidf-15" href="../hunch_net-2009/hunch_net-2009-06-03-Functionally_defined_Nonlinear_Dynamic_Models.html">359 hunch net-2009-06-03-Functionally defined Nonlinear Dynamic Models</a></p>
<p>16 0.15385604 <a title="388-tfidf-16" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>17 0.14928411 <a title="388-tfidf-17" href="../hunch_net-2005/hunch_net-2005-09-08-Online_Learning_as_the_Mathematics_of_Accountability.html">109 hunch net-2005-09-08-Online Learning as the Mathematics of Accountability</a></p>
<p>18 0.14737275 <a title="388-tfidf-18" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>19 0.13859609 <a title="388-tfidf-19" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>20 0.13694654 <a title="388-tfidf-20" href="../hunch_net-2005/hunch_net-2005-08-08-Apprenticeship_Reinforcement_Learning_for_Control.html">101 hunch net-2005-08-08-Apprenticeship Reinforcement Learning for Control</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.342), (1, -0.194), (2, -0.042), (3, -0.037), (4, -0.142), (5, -0.071), (6, 0.11), (7, 0.01), (8, -0.163), (9, -0.098), (10, -0.008), (11, -0.105), (12, 0.123), (13, 0.03), (14, 0.056), (15, 0.023), (16, -0.089), (17, 0.019), (18, -0.248), (19, 0.155), (20, -0.042), (21, -0.007), (22, 0.013), (23, -0.036), (24, -0.053), (25, 0.049), (26, -0.101), (27, 0.084), (28, 0.135), (29, 0.041), (30, 0.07), (31, 0.092), (32, 0.102), (33, -0.084), (34, -0.047), (35, 0.041), (36, 0.02), (37, 0.021), (38, 0.019), (39, 0.019), (40, 0.015), (41, -0.055), (42, 0.054), (43, -0.001), (44, 0.03), (45, 0.027), (46, -0.012), (47, -0.009), (48, -0.043), (49, 0.002)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9690963 <a title="388-lsi-1" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>Introduction: One thing which is clear on a little reflection is that there exists a single
master learning problem capable of encoding essentially all learning problems.
This problem is of course a very general sort of reinforcement learning where
the world interacts with an agent as:The world announces an observationx.The
agent makes a choicea.The world announces a rewardr.The goal here is to
maximize the sum of the rewards over the time of the agent. No particular
structure relatingxtoaoratoris implied by this setting so we do not know
effective general algorithms for the agent. It's very easy to prove lower
bounds showing that an agent cannot hope to succeed here--just consider the
case where actions are unrelated to rewards. Nevertheless, there is a real
sense in which essentially all forms of life are agents operating in this
setting, somehow succeeding. The gap between these observations drives
research--How can we find tractable specializations of the master problem
general enough to provide</p><p>2 0.8639493 <a title="388-lsi-2" href="../hunch_net-2008/hunch_net-2008-09-12-How_do_we_get_weak_action_dependence_for_learning_with_partial_observations%3F.html">317 hunch net-2008-09-12-How do we get weak action dependence for learning with partial observations?</a></p>
<p>Introduction: This post is about contextual bandit problems where, repeatedly:The world
chooses featuresxand rewards for each actionr1,â&euro;Ś,rkthen announces the
featuresx(but not the rewards).A policy chooses an actiona.The world announces
the rewardraThe goal in these situations is to learn a policy which
maximizesrain expectation efficiently. I'm thinking about all situations which
fit the above setting, whether they are drawn IID or adversarially from round
to round and whether they involve past logged data or rapidly learning via
interaction.One common drawback of all algorithms for solving this setting, is
that they have a poor dependence on the number of actions. For example ifkis
the number of actions,EXP4 (page 66)has a dependence onk0.5,epoch-greedy(and
the simpler epsilon greedy) have a dependence onk1/3, and theoffset treehas a
dependence onk-1. These results aren't directly comparable because different
things are being analyzed. The fact thatallanalyses have poor dependence onkis
troublesom</p><p>3 0.82715362 <a title="388-lsi-3" href="../hunch_net-2007/hunch_net-2007-10-24-Contextual_Bandits.html">269 hunch net-2007-10-24-Contextual Bandits</a></p>
<p>Introduction: One of the fundamental underpinnings of the internet is advertising based
content. This has become much more effective due to targeted advertising where
ads are specifically matched to interests. Everyone is familiar with this,
because everyone uses search engines and all search engines try to make money
this way.The problem of matching ads to interests is a natural machine
learning problem in some ways since there is much information in who clicks on
what. A fundamental problem with this information is that it is not supervised
--in particular a click-or-not on one ad doesn't generally tell you if a
different ad would have been clicked on. This implies we have a fundamental
exploration problem.A standard mathematical setting for this situation is
"k-Armed Bandits", often with various relevant embellishments. Thek-Armed
Bandit setting works on a round-by-round basis. On each round:A policy chooses
armafrom1ofkarms (i.e. 1 of k ads).The world reveals the rewardraof the chosen
arm (i.e.</p><p>4 0.75890452 <a title="388-lsi-4" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<p>Introduction: There weretwopapersat ICML presenting learning algorithms for acontextual
bandit-style setting, where the loss for all labels is not known, but the loss
for one label is known. (The first might require aexploration
scavengingviewpoint to understand if the experimental assignment was
nonrandom.) I strongly approve of these papers and further work in this
setting and its variants, because I expect it to become more important than
supervised learning. As a quick review, we are thinking about situations where
repeatedly:The world reveals feature values (aka context information).A policy
chooses an action.The world provides a reward.Sometimes this is done in an
online fashion where the policy can change based on immediate feedback and
sometimes it's done in a batch setting where many samples are collected before
the policy can change. If you haven't spent time thinking about the setting,
you might want to because there are many natural applications.I'm going to
pick on the Banditron paper (</p><p>5 0.73762071 <a title="388-lsi-5" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>Introduction: Consider the contextual bandit setting where, repeatedly:A contextxis
observed.An actionais taken given the contextx.A rewardris observed, dependent
onxanda.Where the goal of a learning agent is to find a policy for step 2
achieving a large expected reward.This setting is of obvious importance,
because in the real world we typically make decisions based on some set of
information and then get feedback only about the single action taken. It also
fundamentally differs from supervised learning settings because knowing the
value of one action is not equivalent to knowing the value of all actions.A
decade ago the best machine learning techniques for this setting where
implausibly inefficient.Dean Fosteronce told me he thought the area was a
research sinkhole with little progress to be expected. Now we are on the verge
of being able to routinely attack these problems, in almost exactly the same
sense that we routinely attack bread and butter supervised learning problems.
Just as for supervis</p><p>6 0.73198193 <a title="388-lsi-6" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>7 0.70267993 <a title="388-lsi-7" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>8 0.68081433 <a title="388-lsi-8" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>9 0.66053659 <a title="388-lsi-9" href="../hunch_net-2007/hunch_net-2007-07-01-Watchword%3A_Online_Learning.html">252 hunch net-2007-07-01-Watchword: Online Learning</a></p>
<p>10 0.63508862 <a title="388-lsi-10" href="../hunch_net-2006/hunch_net-2006-11-22-Explicit_Randomization_in_Learning_algorithms.html">219 hunch net-2006-11-22-Explicit Randomization in Learning algorithms</a></p>
<p>11 0.61630392 <a title="388-lsi-11" href="../hunch_net-2006/hunch_net-2006-04-05-What_is_state%3F.html">169 hunch net-2006-04-05-What is state?</a></p>
<p>12 0.61478776 <a title="388-lsi-12" href="../hunch_net-2009/hunch_net-2009-10-03-Static_vs._Dynamic_multiclass_prediction.html">373 hunch net-2009-10-03-Static vs. Dynamic multiclass prediction</a></p>
<p>13 0.58251584 <a title="388-lsi-13" href="../hunch_net-2005/hunch_net-2005-08-08-Apprenticeship_Reinforcement_Learning_for_Control.html">101 hunch net-2005-08-08-Apprenticeship Reinforcement Learning for Control</a></p>
<p>14 0.58218056 <a title="388-lsi-14" href="../hunch_net-2010/hunch_net-2010-03-15-The_Efficient_Robust_Conditional_Probability_Estimation_Problem.html">391 hunch net-2010-03-15-The Efficient Robust Conditional Probability Estimation Problem</a></p>
<p>15 0.56818688 <a title="388-lsi-15" href="../hunch_net-2005/hunch_net-2005-02-25-Problem%3A_Online_Learning.html">28 hunch net-2005-02-25-Problem: Online Learning</a></p>
<p>16 0.53674436 <a title="388-lsi-16" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>17 0.53540111 <a title="388-lsi-17" href="../hunch_net-2005/hunch_net-2005-10-26-Fallback_Analysis_is_a_Secret_to_Useful_Algorithms.html">126 hunch net-2005-10-26-Fallback Analysis is a Secret to Useful Algorithms</a></p>
<p>18 0.53276831 <a title="388-lsi-18" href="../hunch_net-2005/hunch_net-2005-09-08-Online_Learning_as_the_Mathematics_of_Accountability.html">109 hunch net-2005-09-08-Online Learning as the Mathematics of Accountability</a></p>
<p>19 0.5276891 <a title="388-lsi-19" href="../hunch_net-2005/hunch_net-2005-04-16-Which_Assumptions_are_Reasonable%3F.html">57 hunch net-2005-04-16-Which Assumptions are Reasonable?</a></p>
<p>20 0.52564782 <a title="388-lsi-20" href="../hunch_net-2006/hunch_net-2006-01-13-Benchmarks_for_RL.html">148 hunch net-2006-01-13-Benchmarks for RL</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(6, 0.035), (16, 0.193), (35, 0.037), (38, 0.025), (42, 0.265), (45, 0.051), (68, 0.061), (69, 0.036), (74, 0.104), (76, 0.03), (81, 0.014), (91, 0.016), (95, 0.052)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.96361089 <a title="388-lda-1" href="../hunch_net-2006/hunch_net-2006-05-08-Big_machine_learning.html">178 hunch net-2006-05-08-Big machine learning</a></p>
<p>Introduction: According to theNew York Times,Yahoo is releasing Project Panama shortly.
Project Panama is about better predicting which advertisements are relevant to
a search, implying a higher click through rate, implying larger income
forYahoo. There are two things that seem interesting here:A significant
portion of that improved accuracy is almost certainly machine learning at
work.The quantitative effect is huge--the estimate in the article is
$600*106.Googlealready has such improvements andMicrosoft Searchis surely
working on them, which suggest this is (perhaps) a $109per year machine
learning problem.The exact methodology under use is unlikely to be publicly
discussed in the near future because of the competitive enivironment.
Hopefully we'll have some public "war stories" at some point in the future
when this information becomes less sensitive. For now, it's reassuring to
simply note that machine learning is having a big impact.</p><p>2 0.96202046 <a title="388-lda-2" href="../hunch_net-2008/hunch_net-2008-02-17-The_Meaning_of_Confidence.html">289 hunch net-2008-02-17-The Meaning of Confidence</a></p>
<p>Introduction: In many machine learning papers experiments are done and little confidence
bars are reported for the results. This often seems quite clear, until you
actually try to figure out what it means. There are several different kinds of
'confidence' being used, and it's easy to become confused.Confidence =
Probability. For those who haven't worried about confidence for a long time,
confidence is simply the probability of some event. You are confident about
events which have a large probability. This meaning of confidence is
inadequate in many applications because we want to reason about how much more
information we have, how much more is needed, and where to get it. As an
example, a learning algorithm might predict that the probability of an event
is0.5, but it's unclear if the probability is0.5because no examples have been
provided or0.5because many examples have been provided and the event is simply
fundamentally uncertain.Classical Confidence Intervals. These are common in
learning theory.</p><p>3 0.95787513 <a title="388-lda-3" href="../hunch_net-2005/hunch_net-2005-03-18-Binomial_Weighting.html">43 hunch net-2005-03-18-Binomial Weighting</a></p>
<p>Introduction: Suppose we have a set of classifierscmaking binary predictions from an
inputxand we see examples in an online fashion. In particular, we repeatedly
see an unlabeled examplex, make a predictiony'(possibly based on the
classifiersc), and then see the correct labely.When one of these classifiers
is perfect, there is a great algorithm available: predict according to the
majority vote over every classifier consistent with every previous example.
This is called the Halving algorithm. It makes at mostlog2|c|mistakes since on
any mistake, at least half of the classifiers are eliminated.Obviously, we
can't generally hope that the there exists a classifier which never errs.
TheBinomial Weighting algorithmis an elegant technique allowing a variant
Halving algorithm to cope with errors by creating a set of virtual classifiers
for every classifier which occasionally disagree with the original classifier.
The Halving algorithm on this set of virtual classifiers satisfies a theorem
of the form:errors</p><p>4 0.95645529 <a title="388-lda-4" href="../hunch_net-2006/hunch_net-2006-10-08-Incompatibilities_between_classical_confidence_intervals_and_learning..html">213 hunch net-2006-10-08-Incompatibilities between classical confidence intervals and learning.</a></p>
<p>Introduction: Classical confidence intervals satisfy a theorem of the form: For some data
sourcesD,PrS ~ D(f(D) > g(S)) > 1-dwherefis some function of the distribution
(such as the mean) andgis some function of the observed sampleS. The
constraints onDcan vary between "Independent and identically distributed (IID)
samples from a gaussian with an unknown mean" to "IID samples from an
arbitrary distributionD". There are even some confidence intervals which do
not require IID samples.Classical confidence intervals often confuse people.
They donotsay "with high probability, for my observed sample, the bounds
holds". Instead, they tell you that if you reason according to the confidence
interval in the future (and the constraints onDare satisfied), then you are
not often wrong. Restated, they tell you something about what a safe procedure
is in a stochastic world wheredis the safety parameter.There are a number of
results in theoretical machine learning which use confidence intervals. For
example,TheE3alg</p><p>5 0.92743844 <a title="388-lda-5" href="../hunch_net-2005/hunch_net-2005-02-26-Problem%3A_Reductions_and_Relative_Ranking_Metrics.html">31 hunch net-2005-02-26-Problem: Reductions and Relative Ranking Metrics</a></p>
<p>Introduction: This, again, is something of a research direction rather than a single
problem.There are several metrics people care about which depend upon the
relative ranking of examples and there are sometimes good reasons to care
about such metrics. Examples includeAROC, "F1â&euro;ł, the proportion of the time
that the top ranked element is in some class, the proportion of the top 10
examples in some class (google's problem), the lowest ranked example of some
class, and the "sort distance" from a predicted ranking to a correct ranking.
Seeherefor an example of some of these.ProblemWhat does the ability to
classify well imply about performance under these metrics?Past
WorkProbabilistic classification under squared errorcan be solved with a
classifier. A counterexample shows this does not imply a good AROC.Sample
complexity bounds forAROC(andhere).A paper on "Learning to Order
Things".DifficultySeveral of these may be easy. Some of them may be
hard.ImpactPositive or negative results will broaden our under</p><p>same-blog 6 0.92234218 <a title="388-lda-6" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>7 0.87857574 <a title="388-lda-7" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>8 0.85223132 <a title="388-lda-8" href="../hunch_net-2006/hunch_net-2006-04-05-What_is_state%3F.html">169 hunch net-2006-04-05-What is state?</a></p>
<p>9 0.84775811 <a title="388-lda-9" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>10 0.84396696 <a title="388-lda-10" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>11 0.84150642 <a title="388-lda-11" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>12 0.83774108 <a title="388-lda-12" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>13 0.83698642 <a title="388-lda-13" href="../hunch_net-2007/hunch_net-2007-08-19-Choice_of_Metrics.html">259 hunch net-2007-08-19-Choice of Metrics</a></p>
<p>14 0.83669388 <a title="388-lda-14" href="../hunch_net-2005/hunch_net-2005-03-17-Going_all_the_Way%2C_Sometimes.html">42 hunch net-2005-03-17-Going all the Way, Sometimes</a></p>
<p>15 0.8293767 <a title="388-lda-15" href="../hunch_net-2005/hunch_net-2005-02-25-Problem%3A_Online_Learning.html">28 hunch net-2005-02-25-Problem: Online Learning</a></p>
<p>16 0.82727998 <a title="388-lda-16" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>17 0.82383966 <a title="388-lda-17" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>18 0.82213587 <a title="388-lda-18" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>19 0.81926537 <a title="388-lda-19" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>20 0.81904024 <a title="388-lda-20" href="../hunch_net-2008/hunch_net-2008-09-12-How_do_we_get_weak_action_dependence_for_learning_with_partial_observations%3F.html">317 hunch net-2008-09-12-How do we get weak action dependence for learning with partial observations?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
