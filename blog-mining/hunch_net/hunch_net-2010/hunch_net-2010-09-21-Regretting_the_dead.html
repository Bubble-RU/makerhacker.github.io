<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>411 hunch net-2010-09-21-Regretting the dead</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2010" href="../home/hunch_net-2010_home.html">hunch_net-2010</a> <a title="hunch_net-2010-411" href="#">hunch_net-2010-411</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>411 hunch net-2010-09-21-Regretting the dead</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2010-411-html" href="http://hunch.net/?p=1483">html</a></p><p>Introduction: Nikospointed out thisnew york timesarticle aboutpoor clinical design killing
people. For those of us who study learning from exploration information this
is a reminder that low regret algorithms are particularly important, as regret
in clinical trials is measured by patient deaths.Two obvious improvements on
the experimental design are:With reasonable record keeping of existing
outcomes for the standard treatments, there is no need to explicitly assign
people to a control group with the standard treatment, as that approach is
effectively explored with great certainty. Asserting otherwise would imply
that the nature of effective treatments for cancer has changed between now and
a year ago, which denies the value of any clinical trial.An optimal
experimental design will smoothly phase between exploration and exploitation
as evidence for a new treatment shows that it can be effective. This is old
tech, for example in theEXP3.P algorithm (page 12 aka 59)although I prefer the
generalized an</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('clinical', 0.453), ('treatments', 0.42), ('treatment', 0.302), ('design', 0.226), ('pool', 0.204), ('trial', 0.159), ('phase', 0.151), ('partly', 0.107), ('exploration', 0.107), ('experimental', 0.1), ('failure', 0.096), ('asserting', 0.091), ('moral', 0.091), ('expanded', 0.091), ('tech', 0.091), ('exploitation', 0.091), ('cancer', 0.091), ('saving', 0.091), ('regret', 0.09), ('bonus', 0.084)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999982 <a title="411-tfidf-1" href="../hunch_net-2010/hunch_net-2010-09-21-Regretting_the_dead.html">411 hunch net-2010-09-21-Regretting the dead</a></p>
<p>Introduction: Nikospointed out thisnew york timesarticle aboutpoor clinical design killing
people. For those of us who study learning from exploration information this
is a reminder that low regret algorithms are particularly important, as regret
in clinical trials is measured by patient deaths.Two obvious improvements on
the experimental design are:With reasonable record keeping of existing
outcomes for the standard treatments, there is no need to explicitly assign
people to a control group with the standard treatment, as that approach is
effectively explored with great certainty. Asserting otherwise would imply
that the nature of effective treatments for cancer has changed between now and
a year ago, which denies the value of any clinical trial.An optimal
experimental design will smoothly phase between exploration and exploitation
as evidence for a new treatment shows that it can be effective. This is old
tech, for example in theEXP3.P algorithm (page 12 aka 59)although I prefer the
generalized an</p><p>2 0.18920937 <a title="411-tfidf-2" href="../hunch_net-2008/hunch_net-2008-08-24-Mass_Customized_Medicine_in_the_Future%3F.html">314 hunch net-2008-08-24-Mass Customized Medicine in the Future?</a></p>
<p>Introduction: This post is about a technology which could develop in the future.Right now, a
new drug might be tested by finding patients with some diagnosis and giving or
not giving them a drug according to a secret randomization. The outcome is
observed, and if the average outcome for those treated is measurably better
than the average outcome for those not treated, the drug might become a
standard treatment.Generalizing this, a filterFsorts people into two groups:
those for treatmentAand those not for treatmentBbased upon observationsx. To
measure the outcome, you randomize between treatment and nontreatment of
groupAand measure the relative performance of the treatment.A problem often
arises: in many cases the treated group does not do better than the nontreated
group. A basic question is: does this mean the treatment is bad? With respect
to the filterFit may mean that, but with respect to another filterF', the
treatment might be very effective. For example, a drug might work great for
people wh</p><p>3 0.089319237 <a title="411-tfidf-3" href="../hunch_net-2006/hunch_net-2006-09-07-Objective_and_subjective_interpretations_of_probability.html">205 hunch net-2006-09-07-Objective and subjective interpretations of probability</a></p>
<p>Introduction: An amusing tidbit (reproduced without permission) from Herman Chernoff's
delightful monograph, "Sequential analysis and optimal design":The use of
randomization raises a philosophical question which is articulated by the
following probably apocryphal anecdote.The metallurgist told his friend the
statistician how he planned to test the effect of heat on the strength of a
metal bar by sawing the bar into six pieces. The first two would go into the
hot oven, the next two into the medium oven, and the last two into the cool
oven. The statistician, horrified, explained how he should randomize to avoid
the effect of a possible gradient of strength in the metal bar. The method of
randomization was applied, and it turned out that the randomized experiment
called for putting the first two pieces into the hot oven, the next two into
the medium oven, and the last two into the cool oven. "Obviously, we can't do
that," said the metallurgist. "On the contrary, you have to do that," said the
statisti</p><p>4 0.084218524 <a title="411-tfidf-4" href="../hunch_net-2006/hunch_net-2006-11-06-Data_Linkage_Problems.html">217 hunch net-2006-11-06-Data Linkage Problems</a></p>
<p>Introduction: Data linkage is a problem which seems to come up in various applied machine
learning problems. I have heard it mentioned in various data mining contexts,
but it seems relatively less studied for systemic reasons.A very simple
version of the data linkage problem is a cross hospital patient record merge.
Suppose a patient (John Doe) is admitted to a hospital (General Health),
treated, and released. Later, John Doe is admitted to a second hospital
(Health General), treated, and released. Given a large number of records of
this sort, it becomes very tempting to try and predict the outcomes of
treatments. This is reasonably straightforward as a machine learning problem
if there is a shared unique identifier for John Doe used by General Health and
Health General along with time stamps. We can merge the records and create
examples of the form "Given symptoms and treatment, did the patient come back
to a hospital within the next year?" These examples could be fed into a
learning algorithm, and</p><p>5 0.083885394 <a title="411-tfidf-5" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>Introduction: One view of machine learning is that it's about how to program computers to
predict well. This suggests a broader research program centered around the
more pervasive goal of simply predicting well.There are many distinct strands
of this broader research program which are only partially unified. Here are
the ones that I know of:Learning Theory. Learning theory focuses on several
topics related to the dynamics and process of prediction. Convergence bounds
like theVC boundgive an intellectual foundation to many learning algorithms.
Online learning algorithms likeWeighted Majorityprovide an alternate purely
game theoretic foundation for learning.Boosting algorithmsyield algorithms for
purifying prediction abiliity.Reduction algorithmsprovide means for changing
esoteric problems into well known ones.Machine Learning. A great deal of
experience has accumulated in practical algorithm design from a mixture of
paradigms, including bayesian, biological, optimization, and
theoretical.Mechanism De</p><p>6 0.081390619 <a title="411-tfidf-6" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>7 0.080677241 <a title="411-tfidf-7" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>8 0.078719482 <a title="411-tfidf-8" href="../hunch_net-2005/hunch_net-2005-03-29-Academic_Mechanism_Design.html">48 hunch net-2005-03-29-Academic Mechanism Design</a></p>
<p>9 0.076301977 <a title="411-tfidf-9" href="../hunch_net-2005/hunch_net-2005-09-26-Prediction_Bounds_as_the_Mathematics_of_Science.html">115 hunch net-2005-09-26-Prediction Bounds as the Mathematics of Science</a></p>
<p>10 0.072273046 <a title="411-tfidf-10" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>11 0.071750432 <a title="411-tfidf-11" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>12 0.071308196 <a title="411-tfidf-12" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>13 0.0678339 <a title="411-tfidf-13" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>14 0.064495377 <a title="411-tfidf-14" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>15 0.063011803 <a title="411-tfidf-15" href="../hunch_net-2005/hunch_net-2005-10-20-Machine_Learning_in_the_News.html">125 hunch net-2005-10-20-Machine Learning in the News</a></p>
<p>16 0.057163697 <a title="411-tfidf-16" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>17 0.05666798 <a title="411-tfidf-17" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<p>18 0.056642849 <a title="411-tfidf-18" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>19 0.054708891 <a title="411-tfidf-19" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>20 0.05443434 <a title="411-tfidf-20" href="../hunch_net-2007/hunch_net-2007-10-14-NIPS_workshp%3A_Learning_Problem_Design.html">265 hunch net-2007-10-14-NIPS workshp: Learning Problem Design</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.142), (1, -0.008), (2, 0.037), (3, -0.016), (4, 0.002), (5, -0.015), (6, 0.008), (7, 0.003), (8, -0.0), (9, -0.02), (10, -0.019), (11, 0.03), (12, 0.015), (13, 0.032), (14, -0.026), (15, -0.025), (16, -0.011), (17, 0.021), (18, -0.007), (19, -0.005), (20, -0.028), (21, 0.015), (22, 0.014), (23, -0.062), (24, -0.018), (25, 0.061), (26, 0.029), (27, 0.018), (28, 0.053), (29, -0.004), (30, 0.032), (31, 0.047), (32, -0.041), (33, 0.008), (34, 0.034), (35, -0.023), (36, 0.022), (37, 0.028), (38, -0.051), (39, -0.085), (40, -0.051), (41, 0.051), (42, 0.007), (43, -0.022), (44, 0.03), (45, 0.035), (46, -0.027), (47, -0.08), (48, -0.074), (49, 0.107)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95750535 <a title="411-lsi-1" href="../hunch_net-2010/hunch_net-2010-09-21-Regretting_the_dead.html">411 hunch net-2010-09-21-Regretting the dead</a></p>
<p>Introduction: Nikospointed out thisnew york timesarticle aboutpoor clinical design killing
people. For those of us who study learning from exploration information this
is a reminder that low regret algorithms are particularly important, as regret
in clinical trials is measured by patient deaths.Two obvious improvements on
the experimental design are:With reasonable record keeping of existing
outcomes for the standard treatments, there is no need to explicitly assign
people to a control group with the standard treatment, as that approach is
effectively explored with great certainty. Asserting otherwise would imply
that the nature of effective treatments for cancer has changed between now and
a year ago, which denies the value of any clinical trial.An optimal
experimental design will smoothly phase between exploration and exploitation
as evidence for a new treatment shows that it can be effective. This is old
tech, for example in theEXP3.P algorithm (page 12 aka 59)although I prefer the
generalized an</p><p>2 0.75182009 <a title="411-lsi-2" href="../hunch_net-2006/hunch_net-2006-09-07-Objective_and_subjective_interpretations_of_probability.html">205 hunch net-2006-09-07-Objective and subjective interpretations of probability</a></p>
<p>Introduction: An amusing tidbit (reproduced without permission) from Herman Chernoff's
delightful monograph, "Sequential analysis and optimal design":The use of
randomization raises a philosophical question which is articulated by the
following probably apocryphal anecdote.The metallurgist told his friend the
statistician how he planned to test the effect of heat on the strength of a
metal bar by sawing the bar into six pieces. The first two would go into the
hot oven, the next two into the medium oven, and the last two into the cool
oven. The statistician, horrified, explained how he should randomize to avoid
the effect of a possible gradient of strength in the metal bar. The method of
randomization was applied, and it turned out that the randomized experiment
called for putting the first two pieces into the hot oven, the next two into
the medium oven, and the last two into the cool oven. "Obviously, we can't do
that," said the metallurgist. "On the contrary, you have to do that," said the
statisti</p><p>3 0.60010719 <a title="411-lsi-3" href="../hunch_net-2005/hunch_net-2005-03-29-Academic_Mechanism_Design.html">48 hunch net-2005-03-29-Academic Mechanism Design</a></p>
<p>Introduction: From game theory, there is a notion of "mechanism design": setting up the
structure of the world so that participants have some incentive to do sane
things (rather than obviously counterproductive things). Application of this
principle to academic research may be fruitful.What is misdesigned about
academic research?TheJMLGguides give many hints.The common nature ofbad
reviewingalso suggests the system isn't working optimally.There are many ways
to experimentally"cheat" in machine learning.Funding Prisoner's Delimma. Good
researchers often write grant proposals for funding rather than doing
research. Since the pool of grant money is finite, this means that grant
proposals are often rejected, implying that more must be written. This is
essentially a "prisoner's delimma": anyone not writing grant proposals loses,
but the entire process of doing research is slowed by distraction. If everyone
wrote 1/2 as many grant proposals, roughly the same distribution of funding
would occur, and time w</p><p>4 0.58442718 <a title="411-lsi-4" href="../hunch_net-2008/hunch_net-2008-08-24-Mass_Customized_Medicine_in_the_Future%3F.html">314 hunch net-2008-08-24-Mass Customized Medicine in the Future?</a></p>
<p>Introduction: This post is about a technology which could develop in the future.Right now, a
new drug might be tested by finding patients with some diagnosis and giving or
not giving them a drug according to a secret randomization. The outcome is
observed, and if the average outcome for those treated is measurably better
than the average outcome for those not treated, the drug might become a
standard treatment.Generalizing this, a filterFsorts people into two groups:
those for treatmentAand those not for treatmentBbased upon observationsx. To
measure the outcome, you randomize between treatment and nontreatment of
groupAand measure the relative performance of the treatment.A problem often
arises: in many cases the treated group does not do better than the nontreated
group. A basic question is: does this mean the treatment is bad? With respect
to the filterFit may mean that, but with respect to another filterF', the
treatment might be very effective. For example, a drug might work great for
people wh</p><p>5 0.52850044 <a title="411-lsi-5" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>Introduction: One view of machine learning is that it's about how to program computers to
predict well. This suggests a broader research program centered around the
more pervasive goal of simply predicting well.There are many distinct strands
of this broader research program which are only partially unified. Here are
the ones that I know of:Learning Theory. Learning theory focuses on several
topics related to the dynamics and process of prediction. Convergence bounds
like theVC boundgive an intellectual foundation to many learning algorithms.
Online learning algorithms likeWeighted Majorityprovide an alternate purely
game theoretic foundation for learning.Boosting algorithmsyield algorithms for
purifying prediction abiliity.Reduction algorithmsprovide means for changing
esoteric problems into well known ones.Machine Learning. A great deal of
experience has accumulated in practical algorithm design from a mixture of
paradigms, including bayesian, biological, optimization, and
theoretical.Mechanism De</p><p>6 0.52573419 <a title="411-lsi-6" href="../hunch_net-2012/hunch_net-2012-10-26-ML_Symposium_and_Strata-Hadoop_World.html">475 hunch net-2012-10-26-ML Symposium and Strata-Hadoop World</a></p>
<p>7 0.501674 <a title="411-lsi-7" href="../hunch_net-2005/hunch_net-2005-11-05-The_design_of_a_computing_cluster.html">128 hunch net-2005-11-05-The design of a computing cluster</a></p>
<p>8 0.50052482 <a title="411-lsi-8" href="../hunch_net-2007/hunch_net-2007-04-28-The_Coming_Patent_Apocalypse.html">241 hunch net-2007-04-28-The Coming Patent Apocalypse</a></p>
<p>9 0.49659604 <a title="411-lsi-9" href="../hunch_net-2010/hunch_net-2010-05-02-What%26%238217%3Bs_the_difference_between_gambling_and_rewarding_good_prediction%3F.html">397 hunch net-2010-05-02-What&#8217;s the difference between gambling and rewarding good prediction?</a></p>
<p>10 0.48987573 <a title="411-lsi-10" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>11 0.47973311 <a title="411-lsi-11" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>12 0.47772002 <a title="411-lsi-12" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>13 0.47653463 <a title="411-lsi-13" href="../hunch_net-2007/hunch_net-2007-02-10-Best_Practices_for_Collaboration.html">231 hunch net-2007-02-10-Best Practices for Collaboration</a></p>
<p>14 0.47060066 <a title="411-lsi-14" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>15 0.46812737 <a title="411-lsi-15" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>16 0.45206627 <a title="411-lsi-16" href="../hunch_net-2005/hunch_net-2005-01-19-Why_I_decided_to_run_a_weblog..html">1 hunch net-2005-01-19-Why I decided to run a weblog.</a></p>
<p>17 0.44985753 <a title="411-lsi-17" href="../hunch_net-2005/hunch_net-2005-09-04-Science_in_the_Government.html">106 hunch net-2005-09-04-Science in the Government</a></p>
<p>18 0.4485845 <a title="411-lsi-18" href="../hunch_net-2009/hunch_net-2009-06-03-Functionally_defined_Nonlinear_Dynamic_Models.html">359 hunch net-2009-06-03-Functionally defined Nonlinear Dynamic Models</a></p>
<p>19 0.44671479 <a title="411-lsi-19" href="../hunch_net-2008/hunch_net-2008-09-12-How_do_we_get_weak_action_dependence_for_learning_with_partial_observations%3F.html">317 hunch net-2008-09-12-How do we get weak action dependence for learning with partial observations?</a></p>
<p>20 0.44649354 <a title="411-lsi-20" href="../hunch_net-2007/hunch_net-2007-06-19-How_is_Compressed_Sensing_going_to_change_Machine_Learning_%3F.html">248 hunch net-2007-06-19-How is Compressed Sensing going to change Machine Learning ?</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(38, 0.027), (42, 0.247), (45, 0.036), (62, 0.343), (68, 0.044), (69, 0.031), (74, 0.043), (82, 0.049), (95, 0.081)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.93264425 <a title="411-lda-1" href="../hunch_net-2005/hunch_net-2005-12-09-Machine_Learning_Thoughts.html">137 hunch net-2005-12-09-Machine Learning Thoughts</a></p>
<p>Introduction: I added a link to Olivier Bousquet'smachine learning thoughtsblog. Several of
the posts may be of interest.</p><p>2 0.90844077 <a title="411-lda-2" href="../hunch_net-2011/hunch_net-2011-09-03-Fall_Machine_Learning_Events.html">443 hunch net-2011-09-03-Fall Machine Learning Events</a></p>
<p>Introduction: Many Machine Learning related events are coming up this fall.September
9,abstracts for the New York Machine Learning Symposiumare due. Send a 2 page
pdf, if interested, and note that we:widened submissions to be from anybody
rather than students.set aside a larger fraction of time for contributed
submissions.September 15, there is amachine learning meetup, where I'll be
discussing terascale learning at AOL.September 16, there is aCS&Econ; dayat New
York Academy of Sciences. This is not ML focused, but it's easy to imagine
interest.September 23 and laterNIPS workshopsubmissions start coming due. As
usual, there are too many good ones, so I won't be able to attend all those
that interest me. I do hope some workshop makers consider ICML this coming
summer, as we are increasing to a 2 day format for you. Here are a few that
interest me:Big Learningis about dealing with lots of data. Abstracts are
dueSeptember 30.TheBayes Banditsworkshop. Abstracts are dueSeptember
23.ThePersonalized Medicin</p><p>same-blog 3 0.86752886 <a title="411-lda-3" href="../hunch_net-2010/hunch_net-2010-09-21-Regretting_the_dead.html">411 hunch net-2010-09-21-Regretting the dead</a></p>
<p>Introduction: Nikospointed out thisnew york timesarticle aboutpoor clinical design killing
people. For those of us who study learning from exploration information this
is a reminder that low regret algorithms are particularly important, as regret
in clinical trials is measured by patient deaths.Two obvious improvements on
the experimental design are:With reasonable record keeping of existing
outcomes for the standard treatments, there is no need to explicitly assign
people to a control group with the standard treatment, as that approach is
effectively explored with great certainty. Asserting otherwise would imply
that the nature of effective treatments for cancer has changed between now and
a year ago, which denies the value of any clinical trial.An optimal
experimental design will smoothly phase between exploration and exploitation
as evidence for a new treatment shows that it can be effective. This is old
tech, for example in theEXP3.P algorithm (page 12 aka 59)although I prefer the
generalized an</p><p>4 0.84867817 <a title="411-lda-4" href="../hunch_net-2010/hunch_net-2010-10-17-Partha_Niyogi_has_died.html">414 hunch net-2010-10-17-Partha Niyogi has died</a></p>
<p>Introduction: from brain cancer. I askedMishawho worked with him to write about it.Partha
Niyogi, Louis Block Professor in Computer Science and Statistics at the
University of Chicago passed away on October 1, 2010, aged 43.I first met
Partha Niyogi almost exactly ten years ago when I was a graduate student in
math and he had just started as a faculty in Computer Science and Statistics
at the University of Chicago. Strangely, we first talked at length due to a
somewhat convoluted mathematical argument in a paper on pattern recognition. I
asked him some questions about the paper, and, even though the topic was new
to him, he had put serious thought into it and we started regular meetings. We
made significant progress and developed a line of research stemming initially
just from trying to understand that one paper and to simplify one derivation.
I think this was typical of Partha, showing both his intellectual curiosity
and his intuition for the serendipitous; having a sense and focus for
inquiries wo</p><p>5 0.84696752 <a title="411-lda-5" href="../hunch_net-2005/hunch_net-2005-11-05-The_design_of_a_computing_cluster.html">128 hunch net-2005-11-05-The design of a computing cluster</a></p>
<p>Introduction: This is about the design of a computing cluster from the viewpoint of applied
machine learning using current technology. We just built a small one at TTI so
this is some evidence of what is feasible and thoughts about the design
choices.ArchitectureThere are several architectural choices.AMD Athlon64 based
system. This seems to have the cheapest bang/buck. Maximum RAM is typically
2-3GB.AMD Opteron based system. Opterons provide the additional capability to
buy an SMP motherboard with two chips, and the motherboards often support 16GB
of RAM. The RAM is also the more expensive error correcting type.Intel PIV or
Xeon based system. The PIV and Xeon based systems are the intel analog of the
above 2. Due to architectural design reasons, these chips tend to run a bit
hotter and be a bit more expensive.Dual core chips. Both Intel and AMD have
chips that actually have 2 processors embedded in them.In the end, we decided
to go with option (2). Roughly speaking, the AMD system seemed like a bet</p><p>6 0.80603027 <a title="411-lda-6" href="../hunch_net-2010/hunch_net-2010-10-29-To_Vidoelecture_or_not.html">416 hunch net-2010-10-29-To Vidoelecture or not</a></p>
<p>7 0.70220506 <a title="411-lda-7" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>8 0.59528452 <a title="411-lda-8" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>9 0.59475207 <a title="411-lda-9" href="../hunch_net-2007/hunch_net-2007-08-19-Choice_of_Metrics.html">259 hunch net-2007-08-19-Choice of Metrics</a></p>
<p>10 0.59409755 <a title="411-lda-10" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>11 0.59385848 <a title="411-lda-11" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>12 0.59219283 <a title="411-lda-12" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>13 0.59187335 <a title="411-lda-13" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>14 0.58971012 <a title="411-lda-14" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>15 0.58961344 <a title="411-lda-15" href="../hunch_net-2013/hunch_net-2013-12-01-NIPS_tutorials_and_Vowpal_Wabbit_7.4.html">492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</a></p>
<p>16 0.58827323 <a title="411-lda-16" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>17 0.58814132 <a title="411-lda-17" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>18 0.58802235 <a title="411-lda-18" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>19 0.58758736 <a title="411-lda-19" href="../hunch_net-2007/hunch_net-2007-12-12-Workshop_Summary%26%238212%3BPrinciples_of_Learning_Problem_Design.html">277 hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</a></p>
<p>20 0.5873397 <a title="411-lda-20" href="../hunch_net-2006/hunch_net-2006-01-13-Benchmarks_for_RL.html">148 hunch net-2006-01-13-Benchmarks for RL</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
