<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>395 hunch net-2010-04-26-Compassionate Reviewing</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2010" href="../home/hunch_net-2010_home.html">hunch_net-2010</a> <a title="hunch_net-2010-395" href="#">hunch_net-2010-395</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>395 hunch net-2010-04-26-Compassionate Reviewing</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2010-395-html" href="http://hunch.net/?p=1336">html</a></p><p>Introduction: Most long conversations between academics seem to converge on the topic of reviewing where almost no one is happy.  A basic question is: Should most people be happy?
 
The case against is straightforward.  Anyone who watches the flow of papers realizes that most papers amount to little in the longer term.  By it’s nature research is brutal, where the second-best method is worthless, and the second person to discover things typically gets no credit.  If you think about this for a moment, it’s very different from most other human endeavors.  The second best migrant laborer, construction worker, manager, conductor, quarterback, etc… all can manage quite well. If a reviewer has even a vaguely predictive sense of what’s important in the longer term, then most people submitting papers will be unhappy.
 
But this argument unravels, in my experience.  Perhaps half of reviews are thoughtless or simply wrong with a small part being simply malicious.  And yet, I’m sure that most reviewers genuine</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Most long conversations between academics seem to converge on the topic of reviewing where almost no one is happy. [sent-1, score-0.693]
</p><p>2 Anyone who watches the flow of papers realizes that most papers amount to little in the longer term. [sent-4, score-0.576]
</p><p>3 If a reviewer has even a vaguely predictive sense of what’s important in the longer term, then most people submitting papers will be unhappy. [sent-8, score-0.411]
</p><p>4 And yet, I’m sure that most reviewers genuinely believe they can predict what will and will not be useful in the longer term. [sent-11, score-0.225]
</p><p>5 When academics have conversations about reviewing, the presumption of participants in each conversation is that they all share about the same beliefs about what will be useful, and what will take off. [sent-13, score-0.446]
</p><p>6 Such conversations rarely go into specifics, because the specifics are boring in particular, technical, and because their is a real chance of disagreement on the specifics themselves. [sent-14, score-0.547]
</p><p>7 When double blind reviewing was first being considered for ICML, I remember speaking about the experience in the Crypto community, where in my estimate the reviewing was both fairer and less happy. [sent-15, score-1.385]
</p><p>8 Without double blind reviewing, it is common to have an “in” crowd who everyone respects and whose papers are virtually always accepted. [sent-17, score-0.793]
</p><p>9 With double blind reviewing, everyone suffers substantial rejections. [sent-19, score-0.609]
</p><p>10 From a viewpoint external to the community, when the reviewing is poor and the viewpoint of people in the community highly contradictory, nothing good happens. [sent-21, score-0.927]
</p><p>11 most people) viewing the acrimony choose some other way to solve problems, proposals don’t get funded, and the community itself tends to fracture. [sent-24, score-0.288]
</p><p>12 For example, in cryptography,  TCC  (not double blind) has started, presumably because the top theory people got tired of having their papers rejected at  Crypto  (double blind). [sent-25, score-0.476]
</p><p>13 What seems to be lost with double blind reviewing is some amount of compassion, unfairly allocated. [sent-27, score-1.026]
</p><p>14 In a double blind system, any given paper is plausibly from someone you don’t know, and since most papers go nowhere, plausibly not going anywhere. [sent-28, score-0.823]
</p><p>15 Some time ago, I discussed how I thought  motivation should be the responsibility of the reviewer . [sent-30, score-0.265]
</p><p>16 ”  In a healthy community, reviewers will actively understand why a piece of work is or is not important, filling in and extending the motivation as they consider the problem. [sent-35, score-0.4]
</p><p>17 Reducing reviewing load is certainly helpful, but it is not sufficient alone, because many people naturally interpret a reduced reviewing load as time to work on other things. [sent-39, score-1.184]
</p><p>18 For example, the two-phase reviewing process that  ICML  currently uses might save 0. [sent-41, score-0.421]
</p><p>19 5 reviews/paper, while guaranteeing that for half of the papers, the deciding review is done hastily with no author feedback, a recipe for mistakes. [sent-42, score-0.344]
</p><p>20 A natural conversation helps (the current method of single round response tends to be very stilted). [sent-45, score-0.277]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('reviewing', 0.421), ('blind', 0.279), ('double', 0.264), ('specifics', 0.193), ('community', 0.19), ('conversations', 0.161), ('compassion', 0.157), ('longer', 0.128), ('papers', 0.126), ('viewpoint', 0.115), ('academics', 0.111), ('crypto', 0.111), ('conversation', 0.104), ('happy', 0.104), ('responsibility', 0.104), ('tends', 0.098), ('reviewers', 0.097), ('half', 0.096), ('load', 0.096), ('motivation', 0.09), ('people', 0.086), ('actively', 0.085), ('fair', 0.085), ('plausibly', 0.077), ('helps', 0.075), ('reviewer', 0.071), ('contradictory', 0.07), ('realizes', 0.07), ('witness', 0.07), ('presumption', 0.07), ('disparity', 0.07), ('everyone', 0.066), ('question', 0.065), ('filling', 0.064), ('recipe', 0.064), ('extending', 0.064), ('nowhere', 0.064), ('manager', 0.064), ('flow', 0.064), ('interpret', 0.064), ('author', 0.062), ('amount', 0.062), ('review', 0.061), ('guaranteeing', 0.061), ('aaron', 0.061), ('suboptimal', 0.061), ('disagreed', 0.061), ('career', 0.058), ('grounds', 0.058), ('crowd', 0.058)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="395-tfidf-1" href="../hunch_net-2010/hunch_net-2010-04-26-Compassionate_Reviewing.html">395 hunch net-2010-04-26-Compassionate Reviewing</a></p>
<p>Introduction: Most long conversations between academics seem to converge on the topic of reviewing where almost no one is happy.  A basic question is: Should most people be happy?
 
The case against is straightforward.  Anyone who watches the flow of papers realizes that most papers amount to little in the longer term.  By it’s nature research is brutal, where the second-best method is worthless, and the second person to discover things typically gets no credit.  If you think about this for a moment, it’s very different from most other human endeavors.  The second best migrant laborer, construction worker, manager, conductor, quarterback, etc… all can manage quite well. If a reviewer has even a vaguely predictive sense of what’s important in the longer term, then most people submitting papers will be unhappy.
 
But this argument unravels, in my experience.  Perhaps half of reviews are thoughtless or simply wrong with a small part being simply malicious.  And yet, I’m sure that most reviewers genuine</p><p>2 0.35897681 <a title="395-tfidf-2" href="../hunch_net-2005/hunch_net-2005-03-13-Avoiding_Bad_Reviewing.html">40 hunch net-2005-03-13-Avoiding Bad Reviewing</a></p>
<p>Introduction: If we accept that bad reviewing often occurs and want to fix it, the question is “how”?
 
Reviewing is done by paper writers just like yourself, so a good proxy for this question is asking “How can I be a better reviewer?”  Here are a few things I’ve learned by trial (and error), as a paper writer, and as a reviewer.
  
 The secret ingredient is careful thought.  There is no good substitution for a deep and careful understanding. 
 Avoid reviewing papers that you feel competitive about.  You almost certainly will be asked to review papers that feel competitive if you work on subjects of common interest.  But, the feeling of competition can easily lead to bad judgement. 
 If you feel biased for some other reason, then you should avoid reviewing.  For example… 
 Feeling angry or threatened by a paper is a form of bias.  See above. 
 Double blind yourself (avoid looking at the name even in a single-blind situation).  The significant effect of a name you recognize is making you pay close a</p><p>3 0.33740813 <a title="395-tfidf-3" href="../hunch_net-2005/hunch_net-2005-05-02-Reviewing_techniques_for_conferences.html">65 hunch net-2005-05-02-Reviewing techniques for conferences</a></p>
<p>Introduction: The many reviews following the many paper deadlines are just about over.  AAAI and ICML in particular were experimenting with several reviewing techniques.  
  
 Double Blind: AAAI and ICML were both double blind this year.  It seemed (overall) beneficial, but two problems arose.
 
 For theoretical papers, with a lot to say, authors often leave out the proofs.  This is very hard to cope with under a double blind review because (1) you can not trust the authors got the proof right but (2) a blanket “reject” hits many probably-good papers.  Perhaps authors should more strongly favor proof-complete papers sent to double blind conferences. 
 On the author side, double blind reviewing is actually somewhat disruptive to research.  In particular, it discourages the author from talking about the subject, which is one of the mechanisms of research.  This is not a great drawback, but it is one not previously appreciated. 
 
 
 Author feedback: AAAI and ICML did author feedback this year. It seem</p><p>4 0.32001153 <a title="395-tfidf-4" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>Introduction: When thinking about how best to review papers, it seems helpful to have some conception of what good reviewing is.  As far as I can tell, this is almost always only discussed in the specific context of a paper (i.e. your rejected paper), or at most an area (i.e. what a “good paper” looks like for that area) rather than general principles.  Neither individual papers or areas are sufficiently general for a large conference—every paper differs in the details, and what if you want to build a new area and/or cross areas?
 
An unavoidable reason for reviewing is that the community of research is too large.  In particular, it is not possible for a researcher to read every paper which someone thinks might be of interest.  This reason for reviewing exists independent of constraints on rooms or scheduling formats of individual conferences.  Indeed, history suggests that physical constraints are relatively meaningless over the long term — growing conferences simply use more rooms and/or change fo</p><p>5 0.2846576 <a title="395-tfidf-5" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>Introduction: Here’s a quick reference for summer ML-related conferences sorted by due date:
  
 
 Conference 
 Due date 
 Location 
 Reviewing 
 
 
  KDD  
 Feb 10 
 August 12-16, Beijing, China 
 Single Blind 
 
 
  COLT  
 Feb 14 
 June 25-June 27, Edinburgh, Scotland 
 Single Blind? (historically) 
 
 
  ICML  
 Feb 24 
 June 26-July 1, Edinburgh, Scotland 
 Double Blind, author response, zero  SPOF  
 
 
  UAI  
 March 30 
 August 15-17, Catalina Islands, California 
 Double Blind, author response 
 
  
Geographically, this is greatly dispersed and the UAI/KDD conflict is unfortunate.
 
Machine Learning conferences are triannual now, between  NIPS ,  AIStat , and  ICML .  This has not always been the case: the academic default is annual summer conferences, then NIPS started with a December conference, and now AIStat has grown into an April conference.  
 
However, the first claim is not quite correct.  NIPS and AIStat have few competing venues while ICML implicitly competes with many other conf</p><p>6 0.27962169 <a title="395-tfidf-6" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>7 0.24836966 <a title="395-tfidf-7" href="../hunch_net-2006/hunch_net-2006-09-12-Incentive_Compatible_Reviewing.html">207 hunch net-2006-09-12-Incentive Compatible Reviewing</a></p>
<p>8 0.23326327 <a title="395-tfidf-8" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>9 0.21492314 <a title="395-tfidf-9" href="../hunch_net-2008/hunch_net-2008-12-12-Summer_Conferences.html">331 hunch net-2008-12-12-Summer Conferences</a></p>
<p>10 0.20960131 <a title="395-tfidf-10" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>11 0.2055268 <a title="395-tfidf-11" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>12 0.20308201 <a title="395-tfidf-12" href="../hunch_net-2008/hunch_net-2008-09-03-Bidding_Problems.html">315 hunch net-2008-09-03-Bidding Problems</a></p>
<p>13 0.177967 <a title="395-tfidf-13" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>14 0.16313802 <a title="395-tfidf-14" href="../hunch_net-2007/hunch_net-2007-07-20-Motivation_should_be_the_Responsibility_of_the_Reviewer.html">256 hunch net-2007-07-20-Motivation should be the Responsibility of the Reviewer</a></p>
<p>15 0.16193335 <a title="395-tfidf-15" href="../hunch_net-2012/hunch_net-2012-04-09-ICML_author_feedback_is_open.html">461 hunch net-2012-04-09-ICML author feedback is open</a></p>
<p>16 0.15393205 <a title="395-tfidf-16" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<p>17 0.1539277 <a title="395-tfidf-17" href="../hunch_net-2008/hunch_net-2008-10-14-Who_is_Responsible_for_a_Bad_Review%3F.html">320 hunch net-2008-10-14-Who is Responsible for a Bad Review?</a></p>
<p>18 0.15334915 <a title="395-tfidf-18" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>19 0.15228869 <a title="395-tfidf-19" href="../hunch_net-2012/hunch_net-2012-06-29-ICML_survey_and_comments.html">468 hunch net-2012-06-29-ICML survey and comments</a></p>
<p>20 0.14869365 <a title="395-tfidf-20" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.291), (1, -0.247), (2, 0.272), (3, 0.125), (4, -0.015), (5, 0.012), (6, -0.007), (7, 0.031), (8, 0.041), (9, 0.033), (10, -0.06), (11, -0.002), (12, 0.095), (13, -0.01), (14, -0.083), (15, 0.027), (16, -0.029), (17, -0.053), (18, -0.001), (19, -0.017), (20, 0.005), (21, -0.053), (22, -0.005), (23, 0.025), (24, 0.11), (25, -0.073), (26, -0.036), (27, 0.085), (28, -0.108), (29, 0.066), (30, -0.022), (31, -0.016), (32, -0.012), (33, -0.021), (34, -0.041), (35, -0.069), (36, -0.022), (37, -0.121), (38, -0.045), (39, -0.002), (40, 0.01), (41, -0.055), (42, 0.047), (43, 0.051), (44, 0.043), (45, 0.029), (46, 0.017), (47, -0.0), (48, -0.041), (49, -0.008)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98231459 <a title="395-lsi-1" href="../hunch_net-2010/hunch_net-2010-04-26-Compassionate_Reviewing.html">395 hunch net-2010-04-26-Compassionate Reviewing</a></p>
<p>Introduction: Most long conversations between academics seem to converge on the topic of reviewing where almost no one is happy.  A basic question is: Should most people be happy?
 
The case against is straightforward.  Anyone who watches the flow of papers realizes that most papers amount to little in the longer term.  By it’s nature research is brutal, where the second-best method is worthless, and the second person to discover things typically gets no credit.  If you think about this for a moment, it’s very different from most other human endeavors.  The second best migrant laborer, construction worker, manager, conductor, quarterback, etc… all can manage quite well. If a reviewer has even a vaguely predictive sense of what’s important in the longer term, then most people submitting papers will be unhappy.
 
But this argument unravels, in my experience.  Perhaps half of reviews are thoughtless or simply wrong with a small part being simply malicious.  And yet, I’m sure that most reviewers genuine</p><p>2 0.91498899 <a title="395-lsi-2" href="../hunch_net-2005/hunch_net-2005-03-13-Avoiding_Bad_Reviewing.html">40 hunch net-2005-03-13-Avoiding Bad Reviewing</a></p>
<p>Introduction: If we accept that bad reviewing often occurs and want to fix it, the question is “how”?
 
Reviewing is done by paper writers just like yourself, so a good proxy for this question is asking “How can I be a better reviewer?”  Here are a few things I’ve learned by trial (and error), as a paper writer, and as a reviewer.
  
 The secret ingredient is careful thought.  There is no good substitution for a deep and careful understanding. 
 Avoid reviewing papers that you feel competitive about.  You almost certainly will be asked to review papers that feel competitive if you work on subjects of common interest.  But, the feeling of competition can easily lead to bad judgement. 
 If you feel biased for some other reason, then you should avoid reviewing.  For example… 
 Feeling angry or threatened by a paper is a form of bias.  See above. 
 Double blind yourself (avoid looking at the name even in a single-blind situation).  The significant effect of a name you recognize is making you pay close a</p><p>3 0.89936447 <a title="395-lsi-3" href="../hunch_net-2005/hunch_net-2005-05-02-Reviewing_techniques_for_conferences.html">65 hunch net-2005-05-02-Reviewing techniques for conferences</a></p>
<p>Introduction: The many reviews following the many paper deadlines are just about over.  AAAI and ICML in particular were experimenting with several reviewing techniques.  
  
 Double Blind: AAAI and ICML were both double blind this year.  It seemed (overall) beneficial, but two problems arose.
 
 For theoretical papers, with a lot to say, authors often leave out the proofs.  This is very hard to cope with under a double blind review because (1) you can not trust the authors got the proof right but (2) a blanket “reject” hits many probably-good papers.  Perhaps authors should more strongly favor proof-complete papers sent to double blind conferences. 
 On the author side, double blind reviewing is actually somewhat disruptive to research.  In particular, it discourages the author from talking about the subject, which is one of the mechanisms of research.  This is not a great drawback, but it is one not previously appreciated. 
 
 
 Author feedback: AAAI and ICML did author feedback this year. It seem</p><p>4 0.88638568 <a title="395-lsi-4" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>Introduction: Conferences exist as part of the process of doing research.  They provide many roles including “announcing research”, “meeting people”, and  “point of reference”.  Not all conferences are alike so a basic question is: “to what extent do individual conferences attempt to aid research?”  This question is very difficult to answer in any satisfying way.  What we can do is compare details of the process across multiple conferences.
  
  Comments   The average quality of comments across conferences can vary dramatically.  At one extreme, the tradition in CS theory conferences is to provide essentially zero feedback.  At the other extreme, some conferences have a strong tradition of providing detailed constructive feedback.  Detailed feedback can give authors significant guidance about how to improve research.  This is the most subjective entry. 
  Blind  Virtually all conferences offer single blind review where authors do not know reviewers.  Some also provide  double blind  review where rev</p><p>5 0.7928012 <a title="395-lsi-5" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>Introduction: When thinking about how best to review papers, it seems helpful to have some conception of what good reviewing is.  As far as I can tell, this is almost always only discussed in the specific context of a paper (i.e. your rejected paper), or at most an area (i.e. what a “good paper” looks like for that area) rather than general principles.  Neither individual papers or areas are sufficiently general for a large conference—every paper differs in the details, and what if you want to build a new area and/or cross areas?
 
An unavoidable reason for reviewing is that the community of research is too large.  In particular, it is not possible for a researcher to read every paper which someone thinks might be of interest.  This reason for reviewing exists independent of constraints on rooms or scheduling formats of individual conferences.  Indeed, history suggests that physical constraints are relatively meaningless over the long term — growing conferences simply use more rooms and/or change fo</p><p>6 0.78089142 <a title="395-lsi-6" href="../hunch_net-2013/hunch_net-2013-06-29-The_Benefits_of_Double-Blind_Review.html">485 hunch net-2013-06-29-The Benefits of Double-Blind Review</a></p>
<p>7 0.77419943 <a title="395-lsi-7" href="../hunch_net-2006/hunch_net-2006-09-12-Incentive_Compatible_Reviewing.html">207 hunch net-2006-09-12-Incentive Compatible Reviewing</a></p>
<p>8 0.77268225 <a title="395-lsi-8" href="../hunch_net-2008/hunch_net-2008-09-03-Bidding_Problems.html">315 hunch net-2008-09-03-Bidding Problems</a></p>
<p>9 0.74049878 <a title="395-lsi-9" href="../hunch_net-2012/hunch_net-2012-04-09-ICML_author_feedback_is_open.html">461 hunch net-2012-04-09-ICML author feedback is open</a></p>
<p>10 0.72692835 <a title="395-lsi-10" href="../hunch_net-2009/hunch_net-2009-12-09-Future_Publication_Models_%40_NIPS.html">382 hunch net-2009-12-09-Future Publication Models @ NIPS</a></p>
<p>11 0.68264574 <a title="395-lsi-11" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>12 0.66552216 <a title="395-lsi-12" href="../hunch_net-2012/hunch_net-2012-06-29-ICML_survey_and_comments.html">468 hunch net-2012-06-29-ICML survey and comments</a></p>
<p>13 0.65716338 <a title="395-lsi-13" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>14 0.65524578 <a title="395-lsi-14" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>15 0.63919884 <a title="395-lsi-15" href="../hunch_net-2008/hunch_net-2008-10-14-Who_is_Responsible_for_a_Bad_Review%3F.html">320 hunch net-2008-10-14-Who is Responsible for a Bad Review?</a></p>
<p>16 0.63646442 <a title="395-lsi-16" href="../hunch_net-2008/hunch_net-2008-12-12-Summer_Conferences.html">331 hunch net-2008-12-12-Summer Conferences</a></p>
<p>17 0.62604862 <a title="395-lsi-17" href="../hunch_net-2009/hunch_net-2009-07-09-The_Machine_Learning_Forum.html">363 hunch net-2009-07-09-The Machine Learning Forum</a></p>
<p>18 0.61136693 <a title="395-lsi-18" href="../hunch_net-2008/hunch_net-2008-02-10-Complexity_Illness.html">288 hunch net-2008-02-10-Complexity Illness</a></p>
<p>19 0.61019087 <a title="395-lsi-19" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>20 0.60979122 <a title="395-lsi-20" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(10, 0.023), (27, 0.189), (38, 0.04), (48, 0.039), (53, 0.055), (55, 0.466), (68, 0.018), (94, 0.063), (95, 0.032)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.98973101 <a title="395-lda-1" href="../hunch_net-2005/hunch_net-2005-07-07-The_Limits_of_Learning_Theory.html">90 hunch net-2005-07-07-The Limits of Learning Theory</a></p>
<p>Introduction: Suppose we had an infinitely powerful mathematician sitting in a room and proving theorems about learning.  Could he solve machine learning?
 
The answer is “no”.   This answer is both obvious and sometimes underappreciated.   
 
There are several ways to conclude that some  bias  is necessary in order to succesfully learn.  For example, suppose we are trying to solve classification.  At prediction time, we observe some features  X  and want to make a prediction of either  0  or  1 .   Bias is what makes us prefer one answer over the other based on past experience.  In order to learn we must:
  
 Have a bias.  Always predicting  0  is as likely as  1  is useless. 
 Have the “right” bias.  Predicting  1  when the answer is  0  is also not helpful. 
  
The implication of “have a bias” is that we can not design effective learning algorithms with “a uniform prior over all possibilities”.  The implication of “have the ‘right’ bias” is that our mathematician fails since “right” is defined wi</p><p>2 0.98866642 <a title="395-lda-2" href="../hunch_net-2011/hunch_net-2011-10-24-2011_ML_symposium_and_the_bears.html">448 hunch net-2011-10-24-2011 ML symposium and the bears</a></p>
<p>Introduction: The  New York ML symposium  was last Friday.  Attendance was 268, significantly larger than  last year .  My impression was that the event mostly still fit the space, although it was crowded.  If anyone has suggestions for next year, speak up.
 
The best student paper award went to  Sergiu Goschin  for a cool video of how his system learned to play video games (I can’t find the paper online yet).  Choosing amongst the submitted talks was pretty difficult this year, as there were many similarly good ones.
 
By coincidence all the invited talks were (at least potentially) about faster learning algorithms.   Stephen Boyd  talked about  ADMM .  Leon Bottou  spoke on single pass online learning via  averaged SGD .   Yoav Freund  talked about  parameter-free hedging .  In Yoav’s case the talk was mostly about a better theoretical learning algorithm, but it has the potential to unlock an exponential computational complexity improvement via oraclization of experts algorithms… but some serious</p><p>3 0.98286241 <a title="395-lda-3" href="../hunch_net-2005/hunch_net-2005-02-15-ESPgame_and_image_labeling.html">20 hunch net-2005-02-15-ESPgame and image labeling</a></p>
<p>Introduction: Luis von Ahn  has been running the  espgame  for awhile now.  The espgame provides a picture to two randomly paired people across the web, and asks them to agree on a label.  It hasn’t managed to label the web yet, but it has produced a  large dataset  of (image, label) pairs.  I organized the dataset so you could  explore the implied bipartite graph  (requires much bandwidth).
 
Relative to other image datasets, this one is quite large—67000 images, 358,000 labels (average of 5/image with variation from 1 to 19), and 22,000 unique labels (one every 3 images).  The dataset is also very ‘natural’, consisting of images spidered from the internet.  The multiple label characteristic is intriguing because ‘learning to learn’ and metalearning techniques may be applicable.  The ‘natural’ quality means that this dataset varies greatly in difficulty from easy (predicting “red”) to hard (predicting “funny”) and potentially more rewarding to tackle.
 
The open problem here is, of course, to make</p><p>4 0.98126423 <a title="395-lda-4" href="../hunch_net-2008/hunch_net-2008-05-25-Inappropriate_Mathematics_for_Machine_Learning.html">302 hunch net-2008-05-25-Inappropriate Mathematics for Machine Learning</a></p>
<p>Introduction: Reviewers and students are sometimes greatly concerned by the distinction between:
  
  An  open set  and a  closed set . 
 A  Supremum  and a  Maximum . 
 An event which happens with probability 1 and an event that always happens. 
  
I don’t appreciate this distinction in machine learning & learning theory.  All machine learning takes place (by definition) on a machine where every parameter has finite precision.  Consequently, every set is closed, a maximal element always exists, and probability 1 events always happen.
 
The fundamental issue here is that substantial parts of mathematics don’t appear well-matched to computation in the physical world, because the mathematics has concerns which are unphysical.  This mismatched mathematics makes irrelevant distinctions.  We can ask “what mathematics is appropriate to computation?”   Andrej  has convinced me that a pretty good answer to this question is  constructive mathematics .
 
So, here’s a basic challenge: Can anyone name a situati</p><p>5 0.97552174 <a title="395-lda-5" href="../hunch_net-2007/hunch_net-2007-11-02-The_Machine_Learning_Award_goes_to_%26%238230%3B.html">270 hunch net-2007-11-02-The Machine Learning Award goes to &#8230;</a></p>
<p>Introduction: Perhaps the biggest CS prize for research is the  Turing Award , which has a $0.25M cash prize associated with it.  It appears none of the prizes so far have been for anything like machine learning (the closest are perhaps database awards).
 
In CS theory, there is the  GÃƒÂ¶del Prize  which is smaller and newer, offering a $5K prize along and perhaps (more importantly) recognition.  One such award has been given for Machine Learning, to  Robert Schapire  and  Yoav Freund  for Adaboost.
 
In Machine Learning, there seems to be no equivalent of these sorts of prizes.  There are several plausible reasons for this:
  
 
 There is no coherent community. 
  People drift in and out of the central conferences all the time.  Most of the author names from 10 years ago do not occur in the conferences of today.  In addition, the entire subject area is fairly new. 
 There are at least a core group of people who have stayed around. 
 
 
 Machine Learning work doesn’t last 
 Almost every paper is fo</p><p>6 0.97514164 <a title="395-lda-6" href="../hunch_net-2011/hunch_net-2011-10-03-Monday_announcements.html">446 hunch net-2011-10-03-Monday announcements</a></p>
<p>7 0.97506267 <a title="395-lda-7" href="../hunch_net-2007/hunch_net-2007-11-05-CMU_wins_DARPA_Urban_Challenge.html">271 hunch net-2007-11-05-CMU wins DARPA Urban Challenge</a></p>
<p>same-blog 8 0.97466117 <a title="395-lda-8" href="../hunch_net-2010/hunch_net-2010-04-26-Compassionate_Reviewing.html">395 hunch net-2010-04-26-Compassionate Reviewing</a></p>
<p>9 0.96029705 <a title="395-lda-9" href="../hunch_net-2008/hunch_net-2008-12-12-Summer_Conferences.html">331 hunch net-2008-12-12-Summer Conferences</a></p>
<p>10 0.95954865 <a title="395-lda-10" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<p>11 0.95296156 <a title="395-lda-11" href="../hunch_net-2012/hunch_net-2012-08-27-NYAS_ML_2012_and_ICML_2013.html">472 hunch net-2012-08-27-NYAS ML 2012 and ICML 2013</a></p>
<p>12 0.92413718 <a title="395-lda-12" href="../hunch_net-2005/hunch_net-2005-05-02-Reviewing_techniques_for_conferences.html">65 hunch net-2005-05-02-Reviewing techniques for conferences</a></p>
<p>13 0.91183913 <a title="395-lda-13" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>14 0.90528214 <a title="395-lda-14" href="../hunch_net-2008/hunch_net-2008-11-11-COLT_CFP.html">326 hunch net-2008-11-11-COLT CFP</a></p>
<p>15 0.90528214 <a title="395-lda-15" href="../hunch_net-2012/hunch_net-2012-05-12-ICML_accepted_papers_and_early_registration.html">465 hunch net-2012-05-12-ICML accepted papers and early registration</a></p>
<p>16 0.90526843 <a title="395-lda-16" href="../hunch_net-2010/hunch_net-2010-01-19-Deadline_Season%2C_2010.html">387 hunch net-2010-01-19-Deadline Season, 2010</a></p>
<p>17 0.8780899 <a title="395-lda-17" href="../hunch_net-2005/hunch_net-2005-03-13-Avoiding_Bad_Reviewing.html">40 hunch net-2005-03-13-Avoiding Bad Reviewing</a></p>
<p>18 0.875045 <a title="395-lda-18" href="../hunch_net-2005/hunch_net-2005-03-24-The_Role_of_Workshops.html">46 hunch net-2005-03-24-The Role of Workshops</a></p>
<p>19 0.86754215 <a title="395-lda-19" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>20 0.8644461 <a title="395-lda-20" href="../hunch_net-2009/hunch_net-2009-05-24-2009_ICML_discussion_site.html">356 hunch net-2009-05-24-2009 ICML discussion site</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
