<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>403 hunch net-2010-07-18-ICML &#038; COLT 2010</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2010" href="../home/hunch_net-2010_home.html">hunch_net-2010</a> <a title="hunch_net-2010-403" href="#">hunch_net-2010-403</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>403 hunch net-2010-07-18-ICML &#038; COLT 2010</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2010-403-html" href="http://hunch.net/?p=1431">html</a></p><p>Introduction: The papers which interested me most atICMLandCOLT2010 were:Thomas
Walsh,Kaushik Subramanian,Michael LittmanandCarlos DiukGeneralizing
Apprenticeship Learning across Hypothesis Classes. This paper formalizes and
provides algorithms with guarantees for mixed-mode apprenticeship and
traditional reinforcement learning algorithms, allowing RL algorithms that
perform better than for either setting alone.István SzitaandCsaba
SzepesváriModel-based reinforcement learning with nearly tight exploration
complexity bounds. This paper andanotherrepresent the frontier of best-known
algorithm for Reinforcement Learning in a Markov Decision Process.James
MartensDeep learning via Hessian-free optimization. About a new not-quite-
online second order gradient algorithm for learning deep functional
structures. Potentially this is very powerful because while people have often
talked about end-to-end learning, it has rarely worked in practice.Chrisoph
Sawade,Niels Landwehr,Steffen Bickel. andTobias SchefferA</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The papers which interested me most atICMLandCOLT2010 were:Thomas Walsh,Kaushik Subramanian,Michael LittmanandCarlos DiukGeneralizing Apprenticeship Learning across Hypothesis Classes. [sent-1, score-0.102]
</p><p>2 This paper formalizes and provides algorithms with guarantees for mixed-mode apprenticeship and traditional reinforcement learning algorithms, allowing RL algorithms that perform better than for either setting alone. [sent-2, score-0.709]
</p><p>3 István SzitaandCsaba SzepesváriModel-based reinforcement learning with nearly tight exploration complexity bounds. [sent-3, score-0.146]
</p><p>4 This paper andanotherrepresent the frontier of best-known algorithm for Reinforcement Learning in a Markov Decision Process. [sent-4, score-0.093]
</p><p>5 About a new not-quite- online second order gradient algorithm for learning deep functional structures. [sent-6, score-0.122]
</p><p>6 When a test set is not known in advance, the model can be used to safely aid test set evaluation using importance weighting techniques. [sent-10, score-0.162]
</p><p>7 Relative to the paper, placing a lower bound on p(y|x) is probably important in practice. [sent-11, score-0.088]
</p><p>8 These papers provide tractable online algorithms with regret guarantees over a family of metrics rather than just euclidean metrics. [sent-14, score-0.588]
</p><p>9 Nicolò Cesa-Bianchi,Claudio Gentile,Fabio Vitale,Giovanni Zappella,Active Learning on Trees and GraphsVarious subsets of these authors have other papers about actively learning graph-obeying functions which in total provide a good basis for understanding what's possible and how to learn. [sent-16, score-0.454]
</p><p>10 The results seem to suggest that participants generally agree with the current ICML process. [sent-18, score-0.108]
</p><p>11 I expect there is some amount of anchoring effect going on where participants have an apparent preference for the known status quo, although it's difficult to judge the degree of that. [sent-19, score-0.363]
</p><p>12 It would be interesting to know for which fraction of accepted papers reviewers had their mind changed, but that isn't there. [sent-22, score-0.339]
</p><p>13 4% of authors don't know if the reviewers read their response, believe they read and ignored it, or believe they didn't read it. [sent-24, score-0.822]
</p><p>14 6% support growing the conference with the largest fraction suggesting poster-only papers. [sent-27, score-0.188]
</p><p>15 This is pretty different from the standard colocation list for ICML. [sent-29, score-0.085]
</p><p>16 Many possibilities are precluded by scheduling, but AAAI, IJCAI, UAI, KDD, COLT, SIGIR are all serious possibilities some of which haven't been used much in the past. [sent-30, score-0.27]
</p><p>17 My experience withMark's newpaper discussion siteis generally positive--having comments emailed to interested parties really helps the discussion. [sent-31, score-0.323]
</p><p>18 There are a few comments that authors haven't responded to, so if you are an author you might want to sign up to receive comments. [sent-32, score-0.421]
</p><p>19 I think it would be great if ICML can continue to attract new challenge workshops in the future. [sent-36, score-0.182]
</p><p>20 If anyone else has comments about the workshops, I'd love to hear them. [sent-37, score-0.232]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sigir', 0.21), ('apprenticeship', 0.186), ('authors', 0.185), ('ijcai', 0.173), ('aaai', 0.163), ('icml', 0.154), ('comments', 0.151), ('reinforcement', 0.146), ('read', 0.142), ('possibilities', 0.135), ('kdd', 0.135), ('reviewers', 0.13), ('uai', 0.129), ('changed', 0.126), ('online', 0.122), ('guarantees', 0.11), ('participants', 0.108), ('fraction', 0.107), ('papers', 0.102), ('colt', 0.098), ('workshops', 0.096), ('emnlp', 0.093), ('szepesv', 0.093), ('formalizes', 0.093), ('anchoring', 0.093), ('frontier', 0.093), ('bound', 0.088), ('algorithms', 0.087), ('emailed', 0.086), ('attract', 0.086), ('rank', 0.086), ('dan', 0.086), ('hazan', 0.086), ('parties', 0.086), ('provide', 0.086), ('author', 0.085), ('pretty', 0.085), ('love', 0.081), ('euclidean', 0.081), ('subsets', 0.081), ('suggesting', 0.081), ('status', 0.081), ('apparent', 0.081), ('ignored', 0.081), ('aistats', 0.081), ('test', 0.081), ('communicating', 0.078), ('talked', 0.078), ('thelearning', 0.078), ('exception', 0.078)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000002 <a title="403-tfidf-1" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>Introduction: The papers which interested me most atICMLandCOLT2010 were:Thomas
Walsh,Kaushik Subramanian,Michael LittmanandCarlos DiukGeneralizing
Apprenticeship Learning across Hypothesis Classes. This paper formalizes and
provides algorithms with guarantees for mixed-mode apprenticeship and
traditional reinforcement learning algorithms, allowing RL algorithms that
perform better than for either setting alone.István SzitaandCsaba
SzepesváriModel-based reinforcement learning with nearly tight exploration
complexity bounds. This paper andanotherrepresent the frontier of best-known
algorithm for Reinforcement Learning in a Markov Decision Process.James
MartensDeep learning via Hessian-free optimization. About a new not-quite-
online second order gradient algorithm for learning deep functional
structures. Potentially this is very powerful because while people have often
talked about end-to-end learning, it has rarely worked in practice.Chrisoph
Sawade,Niels Landwehr,Steffen Bickel. andTobias SchefferA</p><p>2 0.23205796 <a title="403-tfidf-2" href="../hunch_net-2012/hunch_net-2012-06-29-ICML_survey_and_comments.html">468 hunch net-2012-06-29-ICML survey and comments</a></p>
<p>Introduction: Just about nothing could keep me from attendingICML, except forDorawho arrived
on Monday. Consequently, I have only secondhand reports that the conference is
going well.For those who are remote (like me) or after the conference (like
everyone),Mark Reidhas setup theICML discussionsite where you can comment on
any paper or subscribe to papers. Authors are automatically subscribed to
their own papers, so it should be possible to have a discussion significantly
after the fact, as people desire.We also conducted a survey before the
conference and have thesurvey resultsnow. This can be compared with theICML
2010 survey results. Looking at the comparable questions, we can sometimes
order the answers to have scores ranging from 0 to 3 or 0 to 4 with 3 or 4
being best and 0 worst, then compute the average difference between 2012 and
2010.Glancing through them, I see:Most people found the papers they reviewed a
good fit for their expertise (-.037 w.r.t 2010). Achieving this was one of our
subgo</p><p>3 0.21987246 <a title="403-tfidf-3" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>Introduction: Normally, I don't indulge in posters forICML, but this year is naturally an
exception for me. If you want one, there are a small numberleft here, if you
sign up before February.It also seems worthwhile to give some sense of the
scope and reviewing criteria for ICML for authors considering submitting
papers. At ICML, the (very large) program committee does the reviewing which
informs final decisions by area chairs on most papers. Program chairs setup
the process, deal with exceptions or disagreements, and provide advice for the
reviewing process. Providing advice is tricky (and easily misleading) because
a conference is a community, and in the end the aggregate interests of the
community determine the conference. Nevertheless, as a program chair this year
it seems worthwhile to state the overall philosophy I have and what I plan to
encourage (and occasionally discourage).At the highest level, I believe ICML
exists to further research into machine learning, which I generally think of
as</p><p>4 0.21026266 <a title="403-tfidf-4" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>Introduction: Unfortunately, I ended up sick for much of this ICML. I did manage to catch
one interesting paper:Richard Socher,Cliff Lin,Andrew Y. Ng, andChristopher D.
ManningParsing Natural Scenes and Natural Language with Recursive Neural
Networks.I invited Richard to share his list of interesting papers, so
hopefully we'll hear from him soon. In the meantime,PaulandHalhave posted some
lists.the futureJoelleand I are program chairs for ICML 2012 inEdinburgh,
which I previously enjoyed visiting in2005. This is a huge responsibility,
that we hope to accomplish well. A part of this (perhaps the most fun part),
is imagining how we can make ICML better. A key and critical constraint is
choosing things that can be accomplished. So far we have:Colocation. The first
thing we looked into was potential colocations. We quickly discovered that
many other conferences precomitted their location. For the future, getting a
colocation withACLorSIGIR, seems to require more advanced planning. If that
can be done, I</p><p>5 0.20610575 <a title="403-tfidf-5" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<p>Introduction: ByShieandNatiFollowing John's advertisement for submitting to ICML, we thought
it appropriate to highlight the advantages of COLT, and the reasons it is
often the best place for theory papers. We would like to emphasize that we
both respect ICML, and are active in ICML, both as authors and as area chairs,
and certainly are not arguing that ICML is a bad place for your papers. For
many papers, ICML is the best venue. But for many theory papers, COLT is a
better and more appropriate place.Why should you submit to COLT?By-and-large,
theory papers go to COLT. This is the tradition of the field and most theory
papers are sent to COLT. This is the place to present your ground-breaking
theorems and new models that will shape the theory of machine learning. COLT
is more focused then ICML with a single track session. Unlike ICML, the norm
in COLT is for people to sit through most sessions, and hear most of the talks
presented. There is also often a lively discussion following paper
presentation</p><p>6 0.19577846 <a title="403-tfidf-6" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>7 0.19390193 <a title="403-tfidf-7" href="../hunch_net-2005/hunch_net-2005-05-02-Reviewing_techniques_for_conferences.html">65 hunch net-2005-05-02-Reviewing techniques for conferences</a></p>
<p>8 0.19381058 <a title="403-tfidf-8" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>9 0.15264298 <a title="403-tfidf-9" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>10 0.15159576 <a title="403-tfidf-10" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>11 0.15082739 <a title="403-tfidf-11" href="../hunch_net-2012/hunch_net-2012-04-09-ICML_author_feedback_is_open.html">461 hunch net-2012-04-09-ICML author feedback is open</a></p>
<p>12 0.14891472 <a title="403-tfidf-12" href="../hunch_net-2008/hunch_net-2008-12-12-Summer_Conferences.html">331 hunch net-2008-12-12-Summer Conferences</a></p>
<p>13 0.14870669 <a title="403-tfidf-13" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>14 0.14058621 <a title="403-tfidf-14" href="../hunch_net-2008/hunch_net-2008-11-10-ICML_Reviewing_Criteria.html">325 hunch net-2008-11-10-ICML Reviewing Criteria</a></p>
<p>15 0.13936612 <a title="403-tfidf-15" href="../hunch_net-2008/hunch_net-2008-10-14-Who_is_Responsible_for_a_Bad_Review%3F.html">320 hunch net-2008-10-14-Who is Responsible for a Bad Review?</a></p>
<p>16 0.13211857 <a title="403-tfidf-16" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>17 0.13001247 <a title="403-tfidf-17" href="../hunch_net-2012/hunch_net-2012-05-02-ICML%3A_Behind_the_Scenes.html">463 hunch net-2012-05-02-ICML: Behind the Scenes</a></p>
<p>18 0.12810478 <a title="403-tfidf-18" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>19 0.12082864 <a title="403-tfidf-19" href="../hunch_net-2008/hunch_net-2008-01-23-Why_Workshop%3F.html">285 hunch net-2008-01-23-Why Workshop?</a></p>
<p>20 0.11989737 <a title="403-tfidf-20" href="../hunch_net-2010/hunch_net-2010-01-19-Deadline_Season%2C_2010.html">387 hunch net-2010-01-19-Deadline Season, 2010</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.307), (1, 0.164), (2, -0.156), (3, 0.089), (4, -0.126), (5, 0.016), (6, 0.062), (7, -0.005), (8, -0.035), (9, -0.001), (10, -0.017), (11, -0.089), (12, -0.02), (13, 0.076), (14, 0.018), (15, 0.076), (16, 0.032), (17, 0.041), (18, 0.025), (19, 0.122), (20, 0.005), (21, -0.051), (22, -0.046), (23, -0.029), (24, -0.07), (25, -0.125), (26, -0.008), (27, 0.053), (28, -0.065), (29, 0.035), (30, -0.045), (31, -0.025), (32, 0.035), (33, -0.02), (34, -0.084), (35, 0.01), (36, -0.028), (37, -0.037), (38, 0.033), (39, -0.052), (40, 0.011), (41, -0.018), (42, 0.029), (43, 0.011), (44, 0.034), (45, 0.078), (46, -0.009), (47, -0.049), (48, -0.009), (49, -0.025)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95945728 <a title="403-lsi-1" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>Introduction: The papers which interested me most atICMLandCOLT2010 were:Thomas
Walsh,Kaushik Subramanian,Michael LittmanandCarlos DiukGeneralizing
Apprenticeship Learning across Hypothesis Classes. This paper formalizes and
provides algorithms with guarantees for mixed-mode apprenticeship and
traditional reinforcement learning algorithms, allowing RL algorithms that
perform better than for either setting alone.István SzitaandCsaba
SzepesváriModel-based reinforcement learning with nearly tight exploration
complexity bounds. This paper andanotherrepresent the frontier of best-known
algorithm for Reinforcement Learning in a Markov Decision Process.James
MartensDeep learning via Hessian-free optimization. About a new not-quite-
online second order gradient algorithm for learning deep functional
structures. Potentially this is very powerful because while people have often
talked about end-to-end learning, it has rarely worked in practice.Chrisoph
Sawade,Niels Landwehr,Steffen Bickel. andTobias SchefferA</p><p>2 0.72573197 <a title="403-lsi-2" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>Introduction: Unfortunately, I ended up sick for much of this ICML. I did manage to catch
one interesting paper:Richard Socher,Cliff Lin,Andrew Y. Ng, andChristopher D.
ManningParsing Natural Scenes and Natural Language with Recursive Neural
Networks.I invited Richard to share his list of interesting papers, so
hopefully we'll hear from him soon. In the meantime,PaulandHalhave posted some
lists.the futureJoelleand I are program chairs for ICML 2012 inEdinburgh,
which I previously enjoyed visiting in2005. This is a huge responsibility,
that we hope to accomplish well. A part of this (perhaps the most fun part),
is imagining how we can make ICML better. A key and critical constraint is
choosing things that can be accomplished. So far we have:Colocation. The first
thing we looked into was potential colocations. We quickly discovered that
many other conferences precomitted their location. For the future, getting a
colocation withACLorSIGIR, seems to require more advanced planning. If that
can be done, I</p><p>3 0.70267463 <a title="403-lsi-3" href="../hunch_net-2012/hunch_net-2012-06-29-ICML_survey_and_comments.html">468 hunch net-2012-06-29-ICML survey and comments</a></p>
<p>Introduction: Just about nothing could keep me from attendingICML, except forDorawho arrived
on Monday. Consequently, I have only secondhand reports that the conference is
going well.For those who are remote (like me) or after the conference (like
everyone),Mark Reidhas setup theICML discussionsite where you can comment on
any paper or subscribe to papers. Authors are automatically subscribed to
their own papers, so it should be possible to have a discussion significantly
after the fact, as people desire.We also conducted a survey before the
conference and have thesurvey resultsnow. This can be compared with theICML
2010 survey results. Looking at the comparable questions, we can sometimes
order the answers to have scores ranging from 0 to 3 or 0 to 4 with 3 or 4
being best and 0 worst, then compute the average difference between 2012 and
2010.Glancing through them, I see:Most people found the papers they reviewed a
good fit for their expertise (-.037 w.r.t 2010). Achieving this was one of our
subgo</p><p>4 0.68577117 <a title="403-lsi-4" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<p>Introduction: ByShieandNatiFollowing John's advertisement for submitting to ICML, we thought
it appropriate to highlight the advantages of COLT, and the reasons it is
often the best place for theory papers. We would like to emphasize that we
both respect ICML, and are active in ICML, both as authors and as area chairs,
and certainly are not arguing that ICML is a bad place for your papers. For
many papers, ICML is the best venue. But for many theory papers, COLT is a
better and more appropriate place.Why should you submit to COLT?By-and-large,
theory papers go to COLT. This is the tradition of the field and most theory
papers are sent to COLT. This is the place to present your ground-breaking
theorems and new models that will shape the theory of machine learning. COLT
is more focused then ICML with a single track session. Unlike ICML, the norm
in COLT is for people to sit through most sessions, and hear most of the talks
presented. There is also often a lively discussion following paper
presentation</p><p>5 0.68472427 <a title="403-lsi-5" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>Introduction: Here's a quick reference for summer ML-related conferences sorted by due
date:ConferenceDue dateLocationReviewingKDDFeb 10August 12-16, Beijing,
ChinaSingle BlindCOLTFeb 14June 25-June 27, Edinburgh, ScotlandSingle Blind?
(historically)ICMLFeb 24June 26-July 1, Edinburgh, ScotlandDouble Blind,
author response, zeroSPOFUAIMarch 30August 15-17, Catalina Islands,
CaliforniaDouble Blind, author responseGeographically, this is greatly
dispersed and the UAI/KDD conflict is unfortunate.Machine Learning conferences
are triannual now, betweenNIPS,AIStat, andICML. This has not always been the
case: the academic default is annual summer conferences, then NIPS started
with a December conference, and now AIStat has grown into an April
conference.However, the first claim is not quite correct. NIPS and AIStat have
few competing venues while ICML implicitly competes with many other
conferences accepting machine learning related papers. SinceJoelleand I are
taking a turn as program chairs this year, I</p><p>6 0.6732834 <a title="403-lsi-6" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>7 0.65670854 <a title="403-lsi-7" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>8 0.63226956 <a title="403-lsi-8" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>9 0.62162989 <a title="403-lsi-9" href="../hunch_net-2006/hunch_net-2006-07-08-Some_recent_papers.html">192 hunch net-2006-07-08-Some recent papers</a></p>
<p>10 0.60898095 <a title="403-lsi-10" href="../hunch_net-2012/hunch_net-2012-05-02-ICML%3A_Behind_the_Scenes.html">463 hunch net-2012-05-02-ICML: Behind the Scenes</a></p>
<p>11 0.59619868 <a title="403-lsi-11" href="../hunch_net-2006/hunch_net-2006-06-30-ICML_papers.html">188 hunch net-2006-06-30-ICML papers</a></p>
<p>12 0.59433758 <a title="403-lsi-12" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>13 0.5789305 <a title="403-lsi-13" href="../hunch_net-2012/hunch_net-2012-04-09-ICML_author_feedback_is_open.html">461 hunch net-2012-04-09-ICML author feedback is open</a></p>
<p>14 0.57725024 <a title="403-lsi-14" href="../hunch_net-2006/hunch_net-2006-12-04-Structural_Problems_in_NIPS_Decision_Making.html">221 hunch net-2006-12-04-Structural Problems in NIPS Decision Making</a></p>
<p>15 0.56871384 <a title="403-lsi-15" href="../hunch_net-2008/hunch_net-2008-11-10-ICML_Reviewing_Criteria.html">325 hunch net-2008-11-10-ICML Reviewing Criteria</a></p>
<p>16 0.56661391 <a title="403-lsi-16" href="../hunch_net-2009/hunch_net-2009-06-24-Interesting_papers_at_UAICMOLT_2009.html">361 hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</a></p>
<p>17 0.56442118 <a title="403-lsi-17" href="../hunch_net-2009/hunch_net-2009-12-27-Interesting_things_at_NIPS_2009.html">385 hunch net-2009-12-27-Interesting things at NIPS 2009</a></p>
<p>18 0.56004769 <a title="403-lsi-18" href="../hunch_net-2005/hunch_net-2005-05-02-Reviewing_techniques_for_conferences.html">65 hunch net-2005-05-02-Reviewing techniques for conferences</a></p>
<p>19 0.55318284 <a title="403-lsi-19" href="../hunch_net-2008/hunch_net-2008-09-03-Bidding_Problems.html">315 hunch net-2008-09-03-Bidding Problems</a></p>
<p>20 0.55007929 <a title="403-lsi-20" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.04), (16, 0.016), (35, 0.058), (39, 0.033), (42, 0.272), (45, 0.038), (63, 0.241), (69, 0.017), (74, 0.163), (76, 0.012), (95, 0.035)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.98169291 <a title="403-lda-1" href="../hunch_net-2008/hunch_net-2008-04-21-The_Science_2.0_article.html">296 hunch net-2008-04-21-The Science 2.0 article</a></p>
<p>Introduction: I found the article aboutscience using modern tools interesting, especially
the part about 'blogophobia', which in my experience is often a substantial
issue: many potential guest posters aren't quite ready, because of the fear of
a permanent public mistake, because it is particularly hard to write about the
unknown (the essence of research), and because the system for public credit
doesn't yet really handle blog posts.So far, science has been relatively
resistant to discussing research on blogs. Some things need to change to get
there. Public tolerance of the occasional mistake is essential, as is a
willingness to cite (and credit) blogs as freely as papers.I've often run into
another reason for holding back myself: I don't want to overtalk my own
research. Nevertheless, I'm slowly changing to the opinion that I'm holding
back too much: the real power of a blog in research is that it can be used to
confer with many people, and that just makes research work better.</p><p>2 0.9560588 <a title="403-lda-2" href="../hunch_net-2010/hunch_net-2010-08-20-The_Workshop_on_Cores%2C_Clusters%2C_and_Clouds.html">404 hunch net-2010-08-20-The Workshop on Cores, Clusters, and Clouds</a></p>
<p>Introduction: Alekh,John,Ofer, and I are organizing aworkshopatNIPSthis year on learning in
parallel and distributed environments. The general interest level in parallel
learning seems to be growing rapidly, so I expect quite a bit of attendance.
Please join us if you are parallel-interested.And, if you are working in the
area of parallel learning, please considersubmitting an abstractdue Oct. 17
for presentation at the workshop.</p><p>same-blog 3 0.91837281 <a title="403-lda-3" href="../hunch_net-2010/hunch_net-2010-07-18-ICML_%26%23038%3B_COLT_2010.html">403 hunch net-2010-07-18-ICML &#038; COLT 2010</a></p>
<p>Introduction: The papers which interested me most atICMLandCOLT2010 were:Thomas
Walsh,Kaushik Subramanian,Michael LittmanandCarlos DiukGeneralizing
Apprenticeship Learning across Hypothesis Classes. This paper formalizes and
provides algorithms with guarantees for mixed-mode apprenticeship and
traditional reinforcement learning algorithms, allowing RL algorithms that
perform better than for either setting alone.István SzitaandCsaba
SzepesváriModel-based reinforcement learning with nearly tight exploration
complexity bounds. This paper andanotherrepresent the frontier of best-known
algorithm for Reinforcement Learning in a Markov Decision Process.James
MartensDeep learning via Hessian-free optimization. About a new not-quite-
online second order gradient algorithm for learning deep functional
structures. Potentially this is very powerful because while people have often
talked about end-to-end learning, it has rarely worked in practice.Chrisoph
Sawade,Niels Landwehr,Steffen Bickel. andTobias SchefferA</p><p>4 0.91416198 <a title="403-lda-4" href="../hunch_net-2008/hunch_net-2008-01-06-Research_Political_Issues.html">282 hunch net-2008-01-06-Research Political Issues</a></p>
<p>Introduction: I've avoided discussing politics here, although not for lack of interest. The
problem with discussing politics is that it's customary for people to say much
based upon little information. Nevertheless, politics can have a substantial
impact on science (and we might hope for the vice-versa). It's primary
election time in the United States, so the topic is timely, although the
issues are not.There are several policy decisions which substantially effect
development of science and technology in the US.EducationThe US has great
contrasts in education. The top universities are very good places, yet the
grade school education system produces mediocre results. For me, the contrast
between apublic educationandCaltechwas bracing. For many others attending
Caltech, it clearly was not. Upgrading the k-12 education system in the US is
a long-standing chronic problem which I know relatively little about. My own
experience is that a basic attitude of "no child unrealized" is better than
"no child lef</p><p>5 0.9081127 <a title="403-lda-5" href="../hunch_net-2009/hunch_net-2009-08-26-Another_10-year_paper_in_Machine_Learning.html">368 hunch net-2009-08-26-Another 10-year paper in Machine Learning</a></p>
<p>Introduction: When I was thinking about the best "10 year paper" forICML, I also took a look
at a few other conferences. Here is one from 10 years ago that interested
me:David McAllesterPAC-Bayesian Model Averaging,COLT1999.2001 Journal
Draft.Prior to this paper, the only mechanism known for controlling or
estimating the necessary sample complexity for learning over continuously
parameterized predictors was VC theory and variants, all of which suffered
from a basic problem: they were incredibly pessimistic in practice. This meant
that only very gross guidance could be provided for learning algorithm design.
The PAC-Bayes bound provided an alternative approach to sample complexity
bounds which was radically tighter, quantitatively. It also imported and
explained many of the motivations for Bayesian learning in a way that learning
theory and perhaps optimization people might appreciate. Since this paper came
out, there have been a number of moderately successful attempts to drive
algorithms directly b</p><p>6 0.79782152 <a title="403-lda-6" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>7 0.79770285 <a title="403-lda-7" href="../hunch_net-2007/hunch_net-2007-01-02-Retrospective.html">225 hunch net-2007-01-02-Retrospective</a></p>
<p>8 0.79356158 <a title="403-lda-8" href="../hunch_net-2005/hunch_net-2005-02-15-ESPgame_and_image_labeling.html">20 hunch net-2005-02-15-ESPgame and image labeling</a></p>
<p>9 0.78694755 <a title="403-lda-9" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>10 0.78500718 <a title="403-lda-10" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>11 0.78457886 <a title="403-lda-11" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>12 0.78342009 <a title="403-lda-12" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>13 0.78292441 <a title="403-lda-13" href="../hunch_net-2009/hunch_net-2009-02-22-Effective_Research_Funding.html">344 hunch net-2009-02-22-Effective Research Funding</a></p>
<p>14 0.78113425 <a title="403-lda-14" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>15 0.77909523 <a title="403-lda-15" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>16 0.77884424 <a title="403-lda-16" href="../hunch_net-2005/hunch_net-2005-02-17-Learning_Research_Programs.html">21 hunch net-2005-02-17-Learning Research Programs</a></p>
<p>17 0.77827495 <a title="403-lda-17" href="../hunch_net-2008/hunch_net-2008-11-10-ICML_Reviewing_Criteria.html">325 hunch net-2008-11-10-ICML Reviewing Criteria</a></p>
<p>18 0.77595741 <a title="403-lda-18" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>19 0.77562702 <a title="403-lda-19" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>20 0.77560723 <a title="403-lda-20" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
