<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>408 hunch net-2010-08-24-Alex Smola starts a blog</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2010" href="../home/hunch_net-2010_home.html">hunch_net-2010</a> <a title="hunch_net-2010-408" href="#">hunch_net-2010-408</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>408 hunch net-2010-08-24-Alex Smola starts a blog</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2010-408-html" href="http://hunch.net/?p=1470">html</a></p><p>Introduction: Adventures in Data Land.</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('land', 0.948), ('data', 0.317)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="408-tfidf-1" href="../hunch_net-2010/hunch_net-2010-08-24-Alex_Smola_starts_a_blog.html">408 hunch net-2010-08-24-Alex Smola starts a blog</a></p>
<p>Introduction: Adventures in Data Land.</p><p>2 0.11013076 <a title="408-tfidf-2" href="../hunch_net-2008/hunch_net-2008-04-12-It_Doesn%26%238217%3Bt_Stop.html">295 hunch net-2008-04-12-It Doesn&#8217;t Stop</a></p>
<p>Introduction: I've enjoyed theTerminatormovies and show. Neglecting the whacky aspects (time
travel and associated paradoxes), there is an enduring topic of discussion:
how do people deal with intelligent machines (and vice versa)?In Terminator-
land, the primary method for dealing with intelligent machines is to prevent
them from being made. This approach works pretty badly, because a new angle on
building an intelligent machine keeps coming up. This is partly a ploy for
writer's to avoid writing themselves out of a job, but there is a fundamental
truth to it as well: preventing progress in research is hard.The United
States, has been experimenting with trying to stop research onstem cells. It
hasn't worked very well--the net effect has been retarding research programs a
bit, and exporting some research to other countries. Another less recent
example was encryption technology, for which the United States generally did
not encourage early public research and evendiscouraged as a munition. This
slowe</p><p>3 0.10836475 <a title="408-tfidf-3" href="../hunch_net-2008/hunch_net-2008-04-26-Eliminating_the_Birthday_Paradox_for_Universal_Features.html">298 hunch net-2008-04-26-Eliminating the Birthday Paradox for Universal Features</a></p>
<p>Introduction: I want to expand onthis postwhich describes one of the core tricks for
makingVowpal Wabbitfast and easy to use when learning from text.The central
trick is converting a word (or any other parseable quantity) into a number via
a hash function.Kishoretells me this is a relatively old trick in NLP land,
but it has some added advantages when doing online learning, because you can
learn directly from the existing data without preprocessing the data to create
features (destroying the online property) or using an expensive hashtable
lookup (slowing things down).A central concern for this approach is
collisions, which create a loss of information. If you usemfeatures in an
index space of sizenthe birthday paradox suggests a collision ifm > n0.5,
essentially because there arem2pairs. This is pretty bad, because it says that
with a vocabulary of105features, you might need to have1010entries in your
table.It turns out that redundancy is great for dealing with
collisions.Alexand I worked out a cou</p><p>4 0.071977437 <a title="408-tfidf-4" href="../hunch_net-2007/hunch_net-2007-08-25-The_Privacy_Problem.html">260 hunch net-2007-08-25-The Privacy Problem</a></p>
<p>Introduction: Machine Learning is rising in importance because data is being collected for
all sorts of tasks where it either wasn't previously collected, or for tasks
that did not previously exist. While this is great for Machine Learning, it
has a downside--the massive data collection which is so useful can also lead
to substantial privacy problems.It's important to understand that this is a
much harder problem than many people appreciate. TheAOLdatareleaseis a good
example. To those doing machine learning, the following strategies might be
obvious:Just delete any names or other obviously personally identifiable
information. The logic here seems to be "if I can't easily find the person
then no one can". That doesn't work as demonstrated by the people who were
found circumstantially from the AOL data.â€¦ then just hash all the search
terms! The logic here is "if I can't read it, then no one can". It's also
trivially broken by a dictionary attack--just hash all the strings that might
be in the data an</p><p>5 0.069553971 <a title="408-tfidf-5" href="../hunch_net-2005/hunch_net-2005-12-07-Is_the_Google_way_the_way_for_machine_learning%3F.html">136 hunch net-2005-12-07-Is the Google way the way for machine learning?</a></p>
<p>Introduction: Urs HoelzlefromGooglegave an invited presentation atNIPS. In the presentation,
he strongly advocates interacting with data in a particular scalable manner
which is something like the following:Make a cluster of machines.Build a
unified filesystem. (Google uses GFS, but NFS or other approaches work
reasonably well for smaller clusters.)Interact with data viaMapReduce.Creating
a cluster of machines is, by this point, relatively straightforward.Unified
filesystems are a little bit tricky--GFS is capable by design of essentially
unlimited speed throughput to disk. NFS can bottleneck because all of the data
has to move through one machine. Nevertheless, this may not be a limiting
factor for smaller clusters.MapReduce is a programming paradigm. Essentially,
it is a combination of a data element transform (map) and an
agreggator/selector (reduce). These operations are highly parallelizable and
the claim is that they support the forms of data interaction which are
necessary.Apparently, theNutc</p><p>6 0.06747523 <a title="408-tfidf-6" href="../hunch_net-2005/hunch_net-2005-09-04-Science_in_the_Government.html">106 hunch net-2005-09-04-Science in the Government</a></p>
<p>7 0.065190002 <a title="408-tfidf-7" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>8 0.062745273 <a title="408-tfidf-8" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>9 0.061425753 <a title="408-tfidf-9" href="../hunch_net-2012/hunch_net-2012-02-20-Berkeley_Streaming_Data_Workshop.html">455 hunch net-2012-02-20-Berkeley Streaming Data Workshop</a></p>
<p>10 0.05924616 <a title="408-tfidf-10" href="../hunch_net-2012/hunch_net-2012-08-24-Patterns_for_research_in_machine_learning.html">471 hunch net-2012-08-24-Patterns for research in machine learning</a></p>
<p>11 0.057878211 <a title="408-tfidf-11" href="../hunch_net-2011/hunch_net-2011-09-28-Somebody%26%238217%3Bs_Eating_Your_Lunch.html">445 hunch net-2011-09-28-Somebody&#8217;s Eating Your Lunch</a></p>
<p>12 0.056324303 <a title="408-tfidf-12" href="../hunch_net-2009/hunch_net-2009-10-03-Static_vs._Dynamic_multiclass_prediction.html">373 hunch net-2009-10-03-Static vs. Dynamic multiclass prediction</a></p>
<p>13 0.054178767 <a title="408-tfidf-13" href="../hunch_net-2005/hunch_net-2005-02-03-Learning_Theory%2C_by_assumption.html">12 hunch net-2005-02-03-Learning Theory, by assumption</a></p>
<p>14 0.052149531 <a title="408-tfidf-14" href="../hunch_net-2006/hunch_net-2006-02-27-The_Peekaboom_Dataset.html">159 hunch net-2006-02-27-The Peekaboom Dataset</a></p>
<p>15 0.046261575 <a title="408-tfidf-15" href="../hunch_net-2005/hunch_net-2005-01-24-The_Humanloop_Spectrum_of_Machine_Learning.html">3 hunch net-2005-01-24-The Humanloop Spectrum of Machine Learning</a></p>
<p>16 0.042263247 <a title="408-tfidf-16" href="../hunch_net-2005/hunch_net-2005-04-25-Embeddings%3A_what_are_they_good_for%3F.html">61 hunch net-2005-04-25-Embeddings: what are they good for?</a></p>
<p>17 0.040388912 <a title="408-tfidf-17" href="../hunch_net-2005/hunch_net-2005-03-15-The_State_of_Tight_Bounds.html">41 hunch net-2005-03-15-The State of Tight Bounds</a></p>
<p>18 0.039502025 <a title="408-tfidf-18" href="../hunch_net-2008/hunch_net-2008-11-10-ICML_Reviewing_Criteria.html">325 hunch net-2008-11-10-ICML Reviewing Criteria</a></p>
<p>19 0.039182659 <a title="408-tfidf-19" href="../hunch_net-2007/hunch_net-2007-12-12-Workshop_Summary%26%238212%3BPrinciples_of_Learning_Problem_Design.html">277 hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</a></p>
<p>20 0.038750898 <a title="408-tfidf-20" href="../hunch_net-2007/hunch_net-2007-09-18-It%26%238217%3Bs_MDL_Jim%2C_but_not_as_we_know_it%26%238230%3B%28on_Bayes%2C_MDL_and_consistency%29.html">263 hunch net-2007-09-18-It&#8217;s MDL Jim, but not as we know it&#8230;(on Bayes, MDL and consistency)</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.032), (1, -0.023), (2, 0.033), (3, 0.003), (4, -0.034), (5, 0.03), (6, -0.062), (7, 0.012), (8, 0.024), (9, 0.009), (10, -0.071), (11, 0.09), (12, 0.003), (13, -0.011), (14, 0.029), (15, -0.084), (16, 0.032), (17, 0.019), (18, 0.074), (19, -0.043), (20, -0.016), (21, -0.013), (22, 0.036), (23, -0.056), (24, 0.032), (25, -0.004), (26, 0.049), (27, 0.058), (28, 0.09), (29, 0.057), (30, -0.02), (31, -0.015), (32, 0.128), (33, -0.012), (34, -0.064), (35, -0.095), (36, 0.018), (37, -0.009), (38, 0.001), (39, 0.012), (40, 0.032), (41, 0.033), (42, -0.028), (43, 0.055), (44, -0.01), (45, 0.022), (46, -0.036), (47, -0.065), (48, -0.043), (49, -0.04)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98174584 <a title="408-lsi-1" href="../hunch_net-2010/hunch_net-2010-08-24-Alex_Smola_starts_a_blog.html">408 hunch net-2010-08-24-Alex Smola starts a blog</a></p>
<p>Introduction: Adventures in Data Land.</p><p>2 0.64138436 <a title="408-lsi-2" href="../hunch_net-2012/hunch_net-2012-02-20-Berkeley_Streaming_Data_Workshop.html">455 hunch net-2012-02-20-Berkeley Streaming Data Workshop</a></p>
<p>Introduction: TheFrom Data to Knowledgeworkshop May 7-11 atBerkeleyshould be of interest to
the many people encountering streaming data in different disciplines. It's run
by a group of astronomers who encounter streaming data all the time. I metJosh
Bloomrecently and he is broadly interested in a workshop covering all aspects
of Machine Learning on streaming data. The hope here is that techniques
developed in one area turn out useful in another which seems quite plausible.
Particularly if you are in the bay area, consider checking it out.</p><p>3 0.63052469 <a title="408-lsi-3" href="../hunch_net-2007/hunch_net-2007-08-25-The_Privacy_Problem.html">260 hunch net-2007-08-25-The Privacy Problem</a></p>
<p>Introduction: Machine Learning is rising in importance because data is being collected for
all sorts of tasks where it either wasn't previously collected, or for tasks
that did not previously exist. While this is great for Machine Learning, it
has a downside--the massive data collection which is so useful can also lead
to substantial privacy problems.It's important to understand that this is a
much harder problem than many people appreciate. TheAOLdatareleaseis a good
example. To those doing machine learning, the following strategies might be
obvious:Just delete any names or other obviously personally identifiable
information. The logic here seems to be "if I can't easily find the person
then no one can". That doesn't work as demonstrated by the people who were
found circumstantially from the AOL data.â€¦ then just hash all the search
terms! The logic here is "if I can't read it, then no one can". It's also
trivially broken by a dictionary attack--just hash all the strings that might
be in the data an</p><p>4 0.61588001 <a title="408-lsi-4" href="../hunch_net-2005/hunch_net-2005-04-25-Embeddings%3A_what_are_they_good_for%3F.html">61 hunch net-2005-04-25-Embeddings: what are they good for?</a></p>
<p>Introduction: I've been looking at some recent embeddings work, and am struck by how
beautiful the theory and algorithms are. It also makes me wonder, what are
embeddings good for?A few things immediately come to mind:(1) For
visualization of high-dimensional data sets.In this case, one would like good
algorithms for embedding specifically into 2- and 3-dimensional Euclidean
spaces.(2) For nonparametric modeling.The usual nonparametric models
(histograms, nearest neighbor) often require resources which are exponential
in the dimension. So if the data actually lie close to some low-
dimensionalsurface, it might be a good idea to first identify this surface and
embed the data before applying the model.Incidentally, for applications like
these, it's important to have a functional mapping from high to low dimension,
which some techniques do not yield up.(3) As a prelude to classifier
learning.The hope here is presumably that learning will be easier in the low-
dimensional space, because of (i) better ge</p><p>5 0.5777396 <a title="408-lsi-5" href="../hunch_net-2010/hunch_net-2010-03-12-Netflix_Challenge_2_Canceled.html">390 hunch net-2010-03-12-Netflix Challenge 2 Canceled</a></p>
<p>Introduction: Thesecond Netflix prize is canceleddue toprivacy problems. I continue to
believe my original assessment of this paper, that the privacy break was
somewhat overstated. I still haven't seen any serious privacy failures on the
scale of theAOL search log release.I expect privacy concerns to continue to be
a big issue when dealing with data releases by companies or governments. The
theory of maintaining privacy while using data is improving, but it is not yet
in a state where the limits of what's possible are clear let alone how to
achieve these limits in a manner friendly to a prediction competition.</p><p>6 0.56616688 <a title="408-lsi-6" href="../hunch_net-2007/hunch_net-2007-09-18-It%26%238217%3Bs_MDL_Jim%2C_but_not_as_we_know_it%26%238230%3B%28on_Bayes%2C_MDL_and_consistency%29.html">263 hunch net-2007-09-18-It&#8217;s MDL Jim, but not as we know it&#8230;(on Bayes, MDL and consistency)</a></p>
<p>7 0.56495744 <a title="408-lsi-7" href="../hunch_net-2005/hunch_net-2005-12-07-Is_the_Google_way_the_way_for_machine_learning%3F.html">136 hunch net-2005-12-07-Is the Google way the way for machine learning?</a></p>
<p>8 0.55799705 <a title="408-lsi-8" href="../hunch_net-2006/hunch_net-2006-02-07-Pittsburgh_Mind_Reading_Competition.html">155 hunch net-2006-02-07-Pittsburgh Mind Reading Competition</a></p>
<p>9 0.54397815 <a title="408-lsi-9" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>10 0.54084861 <a title="408-lsi-10" href="../hunch_net-2012/hunch_net-2012-08-24-Patterns_for_research_in_machine_learning.html">471 hunch net-2012-08-24-Patterns for research in machine learning</a></p>
<p>11 0.4991321 <a title="408-lsi-11" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>12 0.45689073 <a title="408-lsi-12" href="../hunch_net-2012/hunch_net-2012-12-29-Simons_Institute_Big_Data_Program.html">476 hunch net-2012-12-29-Simons Institute Big Data Program</a></p>
<p>13 0.41373193 <a title="408-lsi-13" href="../hunch_net-2005/hunch_net-2005-02-03-Learning_Theory%2C_by_assumption.html">12 hunch net-2005-02-03-Learning Theory, by assumption</a></p>
<p>14 0.40025249 <a title="408-lsi-14" href="../hunch_net-2006/hunch_net-2006-02-27-The_Peekaboom_Dataset.html">159 hunch net-2006-02-27-The Peekaboom Dataset</a></p>
<p>15 0.39152539 <a title="408-lsi-15" href="../hunch_net-2006/hunch_net-2006-03-23-The_Approximation_Argument.html">165 hunch net-2006-03-23-The Approximation Argument</a></p>
<p>16 0.39077875 <a title="408-lsi-16" href="../hunch_net-2006/hunch_net-2006-11-06-Data_Linkage_Problems.html">217 hunch net-2006-11-06-Data Linkage Problems</a></p>
<p>17 0.38199031 <a title="408-lsi-17" href="../hunch_net-2005/hunch_net-2005-08-01-Peekaboom.html">99 hunch net-2005-08-01-Peekaboom</a></p>
<p>18 0.37448722 <a title="408-lsi-18" href="../hunch_net-2006/hunch_net-2006-12-12-Interesting_Papers_at_NIPS_2006.html">224 hunch net-2006-12-12-Interesting Papers at NIPS 2006</a></p>
<p>19 0.36916667 <a title="408-lsi-19" href="../hunch_net-2005/hunch_net-2005-01-31-Watchword%3A_Assumption.html">7 hunch net-2005-01-31-Watchword: Assumption</a></p>
<p>20 0.36499602 <a title="408-lsi-20" href="../hunch_net-2005/hunch_net-2005-01-24-The_Humanloop_Spectrum_of_Machine_Learning.html">3 hunch net-2005-01-24-The Humanloop Spectrum of Machine Learning</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(69, 0.563)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.9569847 <a title="408-lda-1" href="../hunch_net-2007/hunch_net-2007-10-15-NIPS_workshops_extended_to_3_days.html">266 hunch net-2007-10-15-NIPS workshops extended to 3 days</a></p>
<p>Introduction: (Unofficially, at least.) TheDeep Learning Workshopis being held the afternoon
before the rest of the workshops in Vancouver, BC. Separate registration is
needed, and open.What's happening fundamentally here is that there are too
many interesting workshops to fit into 2 days. Perhaps we can get it
officially expanded to 3 days next year.</p><p>2 0.92852813 <a title="408-lda-2" href="../hunch_net-2006/hunch_net-2006-03-24-NLPers.html">166 hunch net-2006-03-24-NLPers</a></p>
<p>Introduction: Hal Daumehas started theNLPersblog to discuss learning for language problems.</p><p>3 0.8923558 <a title="408-lda-3" href="../hunch_net-2005/hunch_net-2005-10-08-We_have_a_winner.html">119 hunch net-2005-10-08-We have a winner</a></p>
<p>Introduction: The DARPA grandchallenge is a big contest for autonomous robot vehicle
driving. It was run once in 2004 for the first time and all teams did badly.
This year was notably different with theStanfordandCMUteams succesfully
completing the course. A number of details arehereandwikipedia has continuing
coverage.A formal winner hasn't been declared yet although Stanford completed
the course quickest.The Stanford and CMU teams deserve a large round of
applause as they have strongly demonstrated the feasibility of autonomous
vehicles.The good news for machine learning is that the Stanford team (at
least) is using some machine learning techniques.</p><p>4 0.76477093 <a title="408-lda-4" href="../hunch_net-2007/hunch_net-2007-04-30-COLT_2007.html">242 hunch net-2007-04-30-COLT 2007</a></p>
<p>Introduction: Registration for COLT 2007 is now open.The conference will take place on 13-15
June, 2007, in San Diego, California, as part of the 2007 Federated Computing
Research Conference (FCRC), which includes STOC, Complexity, and EC.The
website for COLT: http://www.learningtheory.org/colt2007/index.htmlThe early
registration deadline is May 11, and the cutoff date for discounted hotel
rates is May 9.Before registering, take note that the fees are substantially
lower for members of ACM and/or SIGACT than for nonmembers. If you've been
contemplating joining either of these two societies (annual dues: $99 for ACM,
$18 for SIGACT), now would be a good time!</p><p>same-blog 5 0.74619472 <a title="408-lda-5" href="../hunch_net-2010/hunch_net-2010-08-24-Alex_Smola_starts_a_blog.html">408 hunch net-2010-08-24-Alex Smola starts a blog</a></p>
<p>Introduction: Adventures in Data Land.</p><p>6 0.58992934 <a title="408-lda-6" href="../hunch_net-2005/hunch_net-2005-07-10-Thinking_the_Unthought.html">91 hunch net-2005-07-10-Thinking the Unthought</a></p>
<p>7 0.51810455 <a title="408-lda-7" href="../hunch_net-2005/hunch_net-2005-11-16-The_Everything_Ensemble_Edge.html">131 hunch net-2005-11-16-The Everything Ensemble Edge</a></p>
<p>8 0.50807464 <a title="408-lda-8" href="../hunch_net-2011/hunch_net-2011-09-28-Somebody%26%238217%3Bs_Eating_Your_Lunch.html">445 hunch net-2011-09-28-Somebody&#8217;s Eating Your Lunch</a></p>
<p>9 0.44010821 <a title="408-lda-9" href="../hunch_net-2007/hunch_net-2007-07-20-Motivation_should_be_the_Responsibility_of_the_Reviewer.html">256 hunch net-2007-07-20-Motivation should be the Responsibility of the Reviewer</a></p>
<p>10 0.40143487 <a title="408-lda-10" href="../hunch_net-2012/hunch_net-2012-01-28-Why_COLT%3F.html">453 hunch net-2012-01-28-Why COLT?</a></p>
<p>11 0.26317286 <a title="408-lda-11" href="../hunch_net-2005/hunch_net-2005-04-23-Advantages_and_Disadvantages_of_Bayesian_Learning.html">60 hunch net-2005-04-23-Advantages and Disadvantages of Bayesian Learning</a></p>
<p>12 0.26243386 <a title="408-lda-12" href="../hunch_net-2005/hunch_net-2005-02-25-Solution%3A_Reinforcement_Learning_with_Classification.html">29 hunch net-2005-02-25-Solution: Reinforcement Learning with Classification</a></p>
<p>13 0.23454335 <a title="408-lda-13" href="../hunch_net-2005/hunch_net-2005-08-01-Peekaboom.html">99 hunch net-2005-08-01-Peekaboom</a></p>
<p>14 0.20784123 <a title="408-lda-14" href="../hunch_net-2005/hunch_net-2005-09-04-Science_in_the_Government.html">106 hunch net-2005-09-04-Science in the Government</a></p>
<p>15 0.17956129 <a title="408-lda-15" href="../hunch_net-2012/hunch_net-2012-08-24-Patterns_for_research_in_machine_learning.html">471 hunch net-2012-08-24-Patterns for research in machine learning</a></p>
<p>16 0.17712572 <a title="408-lda-16" href="../hunch_net-2005/hunch_net-2005-10-13-Site_tweak.html">122 hunch net-2005-10-13-Site tweak</a></p>
<p>17 0.15969047 <a title="408-lda-17" href="../hunch_net-2011/hunch_net-2011-11-26-Giving_Thanks.html">449 hunch net-2011-11-26-Giving Thanks</a></p>
<p>18 0.15707891 <a title="408-lda-18" href="../hunch_net-2006/hunch_net-2006-03-27-Gradients_everywhere.html">167 hunch net-2006-03-27-Gradients everywhere</a></p>
<p>19 0.13447934 <a title="408-lda-19" href="../hunch_net-2011/hunch_net-2011-08-20-The_Large_Scale_Learning_Survey_Tutorial.html">442 hunch net-2011-08-20-The Large Scale Learning Survey Tutorial</a></p>
<p>20 0.13226812 <a title="408-lda-20" href="../hunch_net-2010/hunch_net-2010-10-29-To_Vidoelecture_or_not.html">416 hunch net-2010-10-29-To Vidoelecture or not</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
