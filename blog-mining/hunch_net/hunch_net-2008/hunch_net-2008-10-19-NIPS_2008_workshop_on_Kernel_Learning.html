<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>321 hunch net-2008-10-19-NIPS 2008 workshop on Kernel Learning</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2008" href="../home/hunch_net-2008_home.html">hunch_net-2008</a> <a title="hunch_net-2008-321" href="#">hunch_net-2008-321</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>321 hunch net-2008-10-19-NIPS 2008 workshop on Kernel Learning</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2008-321-html" href="http://hunch.net/?p=446">html</a></p><p>Introduction: We'd like to invite hunch.net readers to participate in the NIPS 2008 workshop
on kernel learning. While the main focus is on automatically learning kernels
from data, we are also also looking at the broader questions of feature
selection, multi-task learning and multi-view learning. There are no
restrictions on the learning problem being addressed (regression,
classification, etc), and both theoretical and applied work will be
considered. The deadline for submissions isOctober 24.More detail can be
foundhere.Corinna Cortes, Arthur Gretton, Gert Lanckriet, Mehryar Mohri,
Afshin Rostamizadeh</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('restrictions', 0.314), ('kernels', 0.251), ('readers', 0.251), ('broader', 0.243), ('participate', 0.235), ('detail', 0.228), ('invite', 0.228), ('selection', 0.222), ('addressed', 0.212), ('submissions', 0.199), ('automatically', 0.192), ('main', 0.192), ('regression', 0.189), ('kernel', 0.175), ('deadline', 0.175), ('focus', 0.172), ('looking', 0.164), ('nips', 0.156), ('workshop', 0.146), ('theoretical', 0.145)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="321-tfidf-1" href="../hunch_net-2008/hunch_net-2008-10-19-NIPS_2008_workshop_on_Kernel_Learning.html">321 hunch net-2008-10-19-NIPS 2008 workshop on Kernel Learning</a></p>
<p>Introduction: We'd like to invite hunch.net readers to participate in the NIPS 2008 workshop
on kernel learning. While the main focus is on automatically learning kernels
from data, we are also also looking at the broader questions of feature
selection, multi-task learning and multi-view learning. There are no
restrictions on the learning problem being addressed (regression,
classification, etc), and both theoretical and applied work will be
considered. The deadline for submissions isOctober 24.More detail can be
foundhere.Corinna Cortes, Arthur Gretton, Gert Lanckriet, Mehryar Mohri,
Afshin Rostamizadeh</p><p>2 0.14139265 <a title="321-tfidf-2" href="../hunch_net-2006/hunch_net-2006-07-13-Regression_vs._Classification_as_a_Primitive.html">196 hunch net-2006-07-13-Regression vs. Classification as a Primitive</a></p>
<p>Introduction: Forlearning reductionswe have been concentrating on reducing various complex
learning problems to binary classification. This choice needs to be actively
questioned, because it was not carefully considered.Binary clasification is
learning a classifierc:X -> {0,1}so as to minimize the probability of being
wrong,Prx,y~D(c(x)y).The primary alternative candidate seems to be squared
error regression. In squared error regression, you learn a regressors:X ->
[0,1]so as to minimize squared error,Ex,y~D(s(x)-y)2.It is difficult to judge
one primitive against another. The judgement must at least partially be made
on nontheoretical grounds because (essentially) we are evaluating a choice
between two axioms/assumptions.These two primitives are significantly related.
Classification can be reduced to regression in the obvious way: you use the
regressor to predictD(y=1|x), then threshold at0.5. For this simple reduction
a squared error regret ofrimplies a classification regret of at mostr0.5.
Regress</p><p>3 0.11662918 <a title="321-tfidf-3" href="../hunch_net-2012/hunch_net-2012-02-24-ICML%2B50%25.html">456 hunch net-2012-02-24-ICML+50%</a></p>
<p>Introduction: TheICMLpaper deadline has passed.Joelleand I were surprised to see the number
of submissions jump from last year by about 50% to around 900 submissions. A
tiny portion of these are immediate rejects(*), so this is a much larger set
of papers than expected. The number of workshop submissions also doubled
compared to last year, so ICML may grow significantly this year, if we can
manage to handle the load well. The prospect of making 900 good decisions is
fundamentally daunting, and success will rely heavily on theprogram
committeeandarea chairsat this point.For those who want to rubberneck a bit
more, here's a breakdown of submissions by primary topic of submitted
papers:66 Reinforcement Learning 52 Supervised Learning 51 Clustering 46
Kernel Methods 40 Optimization Algorithms 39 Feature Selection and
Dimensionality Reduction 33 Learning Theory 33 Graphical Models 33
Applications 29 Probabilistic Models 29 NN & Deep Learning 26 Transfer and
Multi-Task Learning 25 Online Learning 25 Activ</p><p>4 0.11616418 <a title="321-tfidf-4" href="../hunch_net-2007/hunch_net-2007-02-22-Create_Your_Own_ICML_Workshop.html">234 hunch net-2007-02-22-Create Your Own ICML Workshop</a></p>
<p>Introduction: As usualICML 2007will be hosting aworkshop programto be held this year on June
24th. The success of the program depends on having researchers like you
propose interesting workshop topics and then organize the workshops. I'd like
to encourage all of you to consider sending a workshop proposal. The proposal
deadline has been extended to March 5. See the workshop web-site for
details.Organizing a workshop is a unique way to gather an international group
of researchers together to focus for an entire day on a topic of your
choosing. I've always found that the cost of organizing a workshop is not so
large, and very low compared to the benefits. The topic and format of a
workshop are limited only by your imagination (and the attractiveness to
potential participants) and need not follow the usual model of a mini-
conference on a particular ML sub-area. Hope to see some interesting proposals
rolling in.</p><p>5 0.10919261 <a title="321-tfidf-5" href="../hunch_net-2007/hunch_net-2007-10-14-NIPS_workshp%3A_Learning_Problem_Design.html">265 hunch net-2007-10-14-NIPS workshp: Learning Problem Design</a></p>
<p>Introduction: Alinaand I are organizing a workshop onLearning Problem DesignatNIPS.What is
learning problem design?It's about being clever in creating learning problems
from otherwise unlabeled data. Read the webpage above for examples.I want to
participate!Email us before Nov. 1 with a description of what you want to talk
about.</p><p>6 0.097438589 <a title="321-tfidf-6" href="../hunch_net-2007/hunch_net-2007-12-12-Workshop_Summary%26%238212%3BPrinciples_of_Learning_Problem_Design.html">277 hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</a></p>
<p>7 0.09570615 <a title="321-tfidf-7" href="../hunch_net-2005/hunch_net-2005-03-24-The_Role_of_Workshops.html">46 hunch net-2005-03-24-The Role of Workshops</a></p>
<p>8 0.094109051 <a title="321-tfidf-8" href="../hunch_net-2006/hunch_net-2006-06-25-Presentation_of_Proofs_is_Hard..html">187 hunch net-2006-06-25-Presentation of Proofs is Hard.</a></p>
<p>9 0.091558732 <a title="321-tfidf-9" href="../hunch_net-2005/hunch_net-2005-12-17-Workshops_as_Franchise_Conferences.html">141 hunch net-2005-12-17-Workshops as Franchise Conferences</a></p>
<p>10 0.090238616 <a title="321-tfidf-10" href="../hunch_net-2005/hunch_net-2005-12-14-More_NIPS_Papers_II.html">140 hunch net-2005-12-14-More NIPS Papers II</a></p>
<p>11 0.083411753 <a title="321-tfidf-11" href="../hunch_net-2005/hunch_net-2005-10-19-Workshop%3A_Atomic_Learning.html">124 hunch net-2005-10-19-Workshop: Atomic Learning</a></p>
<p>12 0.083362132 <a title="321-tfidf-12" href="../hunch_net-2005/hunch_net-2005-08-04-Why_Reinforcement_Learning_is_Important.html">100 hunch net-2005-08-04-Why Reinforcement Learning is Important</a></p>
<p>13 0.079329878 <a title="321-tfidf-13" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>14 0.077952683 <a title="321-tfidf-14" href="../hunch_net-2008/hunch_net-2008-10-01-NIPS_2008_workshop_on_%26%238216%3BLearning_over_Empirical_Hypothesis_Spaces%26%238217%3B.html">319 hunch net-2008-10-01-NIPS 2008 workshop on &#8216;Learning over Empirical Hypothesis Spaces&#8217;</a></p>
<p>15 0.074175499 <a title="321-tfidf-15" href="../hunch_net-2005/hunch_net-2005-01-24-The_Humanloop_Spectrum_of_Machine_Learning.html">3 hunch net-2005-01-24-The Humanloop Spectrum of Machine Learning</a></p>
<p>16 0.071930818 <a title="321-tfidf-16" href="../hunch_net-2008/hunch_net-2008-11-26-Efficient_Reinforcement_Learning_in_MDPs.html">328 hunch net-2008-11-26-Efficient Reinforcement Learning in MDPs</a></p>
<p>17 0.070692882 <a title="321-tfidf-17" href="../hunch_net-2010/hunch_net-2010-09-28-Machined_Learnings.html">412 hunch net-2010-09-28-Machined Learnings</a></p>
<p>18 0.070118576 <a title="321-tfidf-18" href="../hunch_net-2012/hunch_net-2012-06-29-ICML_survey_and_comments.html">468 hunch net-2012-06-29-ICML survey and comments</a></p>
<p>19 0.068125308 <a title="321-tfidf-19" href="../hunch_net-2008/hunch_net-2008-01-23-Why_Workshop%3F.html">285 hunch net-2008-01-23-Why Workshop?</a></p>
<p>20 0.06803222 <a title="321-tfidf-20" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.12), (1, 0.001), (2, 0.024), (3, 0.137), (4, -0.039), (5, -0.038), (6, -0.083), (7, -0.027), (8, -0.009), (9, 0.127), (10, 0.057), (11, 0.127), (12, 0.029), (13, 0.079), (14, -0.012), (15, -0.026), (16, -0.008), (17, -0.009), (18, 0.026), (19, -0.019), (20, 0.002), (21, -0.105), (22, 0.029), (23, 0.123), (24, -0.045), (25, -0.032), (26, 0.073), (27, -0.024), (28, -0.006), (29, 0.062), (30, -0.106), (31, -0.003), (32, -0.008), (33, -0.046), (34, 0.016), (35, 0.042), (36, 0.093), (37, 0.064), (38, 0.047), (39, 0.065), (40, -0.041), (41, 0.032), (42, -0.071), (43, 0.039), (44, -0.051), (45, -0.042), (46, 0.007), (47, 0.04), (48, 0.027), (49, 0.052)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94919276 <a title="321-lsi-1" href="../hunch_net-2008/hunch_net-2008-10-19-NIPS_2008_workshop_on_Kernel_Learning.html">321 hunch net-2008-10-19-NIPS 2008 workshop on Kernel Learning</a></p>
<p>Introduction: We'd like to invite hunch.net readers to participate in the NIPS 2008 workshop
on kernel learning. While the main focus is on automatically learning kernels
from data, we are also also looking at the broader questions of feature
selection, multi-task learning and multi-view learning. There are no
restrictions on the learning problem being addressed (regression,
classification, etc), and both theoretical and applied work will be
considered. The deadline for submissions isOctober 24.More detail can be
foundhere.Corinna Cortes, Arthur Gretton, Gert Lanckriet, Mehryar Mohri,
Afshin Rostamizadeh</p><p>2 0.72579759 <a title="321-lsi-2" href="../hunch_net-2008/hunch_net-2008-10-01-NIPS_2008_workshop_on_%26%238216%3BLearning_over_Empirical_Hypothesis_Spaces%26%238217%3B.html">319 hunch net-2008-10-01-NIPS 2008 workshop on &#8216;Learning over Empirical Hypothesis Spaces&#8217;</a></p>
<p>Introduction: This workshop asks for insights how far we may/can push the theoretical
boundary of using data in the design of learning machines. Can we express our
classification rule in terms of the sample, or do we have to stick to a core
assumption of classical statistical learning theory, namely that the
hypothesis space is to be defined independent from the sample? This workshop
is particularly interested in - but not restricted to - the 'luckiness
framework' and the recently introduced notion of 'compatibility functions' in
a semi-supervised learning context (more information can be found
athttp://www.kuleuven.be/wehys).</p><p>3 0.65787923 <a title="321-lsi-3" href="../hunch_net-2007/hunch_net-2007-02-22-Create_Your_Own_ICML_Workshop.html">234 hunch net-2007-02-22-Create Your Own ICML Workshop</a></p>
<p>Introduction: As usualICML 2007will be hosting aworkshop programto be held this year on June
24th. The success of the program depends on having researchers like you
propose interesting workshop topics and then organize the workshops. I'd like
to encourage all of you to consider sending a workshop proposal. The proposal
deadline has been extended to March 5. See the workshop web-site for
details.Organizing a workshop is a unique way to gather an international group
of researchers together to focus for an entire day on a topic of your
choosing. I've always found that the cost of organizing a workshop is not so
large, and very low compared to the benefits. The topic and format of a
workshop are limited only by your imagination (and the attractiveness to
potential participants) and need not follow the usual model of a mini-
conference on a particular ML sub-area. Hope to see some interesting proposals
rolling in.</p><p>4 0.62615979 <a title="321-lsi-4" href="../hunch_net-2005/hunch_net-2005-10-19-Workshop%3A_Atomic_Learning.html">124 hunch net-2005-10-19-Workshop: Atomic Learning</a></p>
<p>Introduction: We are planning to have a workshop on atomic learning Jan 7 & 8 at TTI-
Chicago.Details are here.The earlier request for interest ishere.The primary
deadline is abstracts due Nov. 20 to jl@tti-c.org.</p><p>5 0.61311316 <a title="321-lsi-5" href="../hunch_net-2006/hunch_net-2006-07-25-Upcoming_conference.html">198 hunch net-2006-07-25-Upcoming conference</a></p>
<p>Introduction: The Workshop for Women in Machine Learning will be held in San Diego on
October 4, 2006.For details see the workshop
website:http://www.seas.upenn.edu/~wiml/</p><p>6 0.59377187 <a title="321-lsi-6" href="../hunch_net-2005/hunch_net-2005-09-20-Workshop_Proposal%3A_Atomic_Learning.html">114 hunch net-2005-09-20-Workshop Proposal: Atomic Learning</a></p>
<p>7 0.55176818 <a title="321-lsi-7" href="../hunch_net-2007/hunch_net-2007-10-14-NIPS_workshp%3A_Learning_Problem_Design.html">265 hunch net-2007-10-14-NIPS workshp: Learning Problem Design</a></p>
<p>8 0.49578488 <a title="321-lsi-8" href="../hunch_net-2012/hunch_net-2012-02-24-ICML%2B50%25.html">456 hunch net-2012-02-24-ICML+50%</a></p>
<p>9 0.49142379 <a title="321-lsi-9" href="../hunch_net-2007/hunch_net-2007-12-12-Workshop_Summary%26%238212%3BPrinciples_of_Learning_Problem_Design.html">277 hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</a></p>
<p>10 0.47345814 <a title="321-lsi-10" href="../hunch_net-2005/hunch_net-2005-09-19-NIPS_Workshops.html">113 hunch net-2005-09-19-NIPS Workshops</a></p>
<p>11 0.46521753 <a title="321-lsi-11" href="../hunch_net-2006/hunch_net-2006-07-08-Some_recent_papers.html">192 hunch net-2006-07-08-Some recent papers</a></p>
<p>12 0.46039796 <a title="321-lsi-12" href="../hunch_net-2005/hunch_net-2005-03-24-The_Role_of_Workshops.html">46 hunch net-2005-03-24-The Role of Workshops</a></p>
<p>13 0.42792863 <a title="321-lsi-13" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>14 0.4213942 <a title="321-lsi-14" href="../hunch_net-2009/hunch_net-2009-08-27-New_York_Area_Machine_Learning_Events.html">369 hunch net-2009-08-27-New York Area Machine Learning Events</a></p>
<p>15 0.40278608 <a title="321-lsi-15" href="../hunch_net-2012/hunch_net-2012-02-20-Berkeley_Streaming_Data_Workshop.html">455 hunch net-2012-02-20-Berkeley Streaming Data Workshop</a></p>
<p>16 0.40223664 <a title="321-lsi-16" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>17 0.40217438 <a title="321-lsi-17" href="../hunch_net-2010/hunch_net-2010-12-26-NIPS_2010.html">420 hunch net-2010-12-26-NIPS 2010</a></p>
<p>18 0.39756978 <a title="321-lsi-18" href="../hunch_net-2008/hunch_net-2008-01-23-Why_Workshop%3F.html">285 hunch net-2008-01-23-Why Workshop?</a></p>
<p>19 0.3887935 <a title="321-lsi-19" href="../hunch_net-2009/hunch_net-2009-12-07-Vowpal_Wabbit_version_4.0%2C_and_a_NIPS_heresy.html">381 hunch net-2009-12-07-Vowpal Wabbit version 4.0, and a NIPS heresy</a></p>
<p>20 0.38517427 <a title="321-lsi-20" href="../hunch_net-2007/hunch_net-2007-06-19-How_is_Compressed_Sensing_going_to_change_Machine_Learning_%3F.html">248 hunch net-2007-06-19-How is Compressed Sensing going to change Machine Learning ?</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(42, 0.169), (44, 0.673)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.81620133 <a title="321-lda-1" href="../hunch_net-2008/hunch_net-2008-01-23-Why_Workshop%3F.html">285 hunch net-2008-01-23-Why Workshop?</a></p>
<p>Introduction: I second thecall for workshops at ICML/COLT/UAI.Severaltimesbefore, details of
why and how to run a workshop have been mentioned.There is a simple reason to
prefer workshops here: attendance. The Helsinki colocation has placed
workshopsdirectly between ICML and COLT/UAI, which is optimal for getting
attendees from any conference. In addition,last year ICML had relatively few
workshopsand NIPS workshops were overloaded. In addition tothose that
happeneda similar number were rejected. The overload has strange consequences
--for example,the best attended workshopwasn't an official NIPS workshop.
Aside from intrinsic interest, the Deep Learning workshop benefited greatly
from being off schedule.</p><p>same-blog 2 0.8159315 <a title="321-lda-2" href="../hunch_net-2008/hunch_net-2008-10-19-NIPS_2008_workshop_on_Kernel_Learning.html">321 hunch net-2008-10-19-NIPS 2008 workshop on Kernel Learning</a></p>
<p>Introduction: We'd like to invite hunch.net readers to participate in the NIPS 2008 workshop
on kernel learning. While the main focus is on automatically learning kernels
from data, we are also also looking at the broader questions of feature
selection, multi-task learning and multi-view learning. There are no
restrictions on the learning problem being addressed (regression,
classification, etc), and both theoretical and applied work will be
considered. The deadline for submissions isOctober 24.More detail can be
foundhere.Corinna Cortes, Arthur Gretton, Gert Lanckriet, Mehryar Mohri,
Afshin Rostamizadeh</p><p>3 0.59758663 <a title="321-lda-3" href="../hunch_net-2011/hunch_net-2011-03-27-Vowpal_Wabbit%2C_v5.1.html">428 hunch net-2011-03-27-Vowpal Wabbit, v5.1</a></p>
<p>Introduction: I just createdversion 5.1ofvowpal wabbit. This almost entirely a bugfix
release, so it's an easy upgrade from v5.0.In addition:There is now amailing
list, which I and several other developers are subscribed to.The main website
has shifted to the wiki on github. This means that anyone with a github
account can now edit it.I'm planning to give a tutorial tomorrow on it
ateHarmony/the LA machine learning meetupat 10am. Drop by if you're
interested.The status of VW amongst other open source projects has changed.
When VW first came out, it was relatively unique amongst existing projects in
terms of features. At this point, many other projects have started to
appreciate the value of the design choices here. This includes:Mahout, which
now has an SGD implementation.Shogun, whereSoerenis keen onincorporating
features.LibLinear, where they won the KDD best paper award forout-of-core
learning.This is expected--any open source approach which works well should be
widely adopted. None of these othe</p><p>4 0.49446899 <a title="321-lda-4" href="../hunch_net-2008/hunch_net-2008-09-12-How_do_we_get_weak_action_dependence_for_learning_with_partial_observations%3F.html">317 hunch net-2008-09-12-How do we get weak action dependence for learning with partial observations?</a></p>
<p>Introduction: This post is about contextual bandit problems where, repeatedly:The world
chooses featuresxand rewards for each actionr1,â&euro;Ś,rkthen announces the
featuresx(but not the rewards).A policy chooses an actiona.The world announces
the rewardraThe goal in these situations is to learn a policy which
maximizesrain expectation efficiently. I'm thinking about all situations which
fit the above setting, whether they are drawn IID or adversarially from round
to round and whether they involve past logged data or rapidly learning via
interaction.One common drawback of all algorithms for solving this setting, is
that they have a poor dependence on the number of actions. For example ifkis
the number of actions,EXP4 (page 66)has a dependence onk0.5,epoch-greedy(and
the simpler epsilon greedy) have a dependence onk1/3, and theoffset treehas a
dependence onk-1. These results aren't directly comparable because different
things are being analyzed. The fact thatallanalyses have poor dependence onkis
troublesom</p><p>5 0.45232013 <a title="321-lda-5" href="../hunch_net-2008/hunch_net-2008-04-30-Concerns_about_the_Large_Scale_Learning_Challenge.html">300 hunch net-2008-04-30-Concerns about the Large Scale Learning Challenge</a></p>
<p>Introduction: Thelarge scale learning challengefor ICML interests me a great deal, although
I have concerns about the way it is structured.From theinstructions page,
several issues come up:Large DefinitionMy personal definition of dataset size
is:smallA dataset is small if a human could look at the dataset and plausibly
find a good solution.mediumA dataset is mediumsize if it fits in the RAM of a
reasonably priced computer.largeA large dataset does not fit in the RAM of a
reasonably priced computer.By this definition, all of the datasets are medium
sized. This might sound like a pissing match over dataset size, but I believe
it is more than that.The fundamental reason for these definitions is that they
correspond to transitions in the sorts of approaches which are feasible. From
small to medium, the ability to use a human as the learning algorithm
degrades. From medium to large, it becomes essential to have learning
algorithms that don't require random access to examples.No Loading TimeThe
medium sc</p><p>6 0.42667204 <a title="321-lda-6" href="../hunch_net-2005/hunch_net-2005-01-24-The_Humanloop_Spectrum_of_Machine_Learning.html">3 hunch net-2005-01-24-The Humanloop Spectrum of Machine Learning</a></p>
<p>7 0.29146981 <a title="321-lda-7" href="../hunch_net-2007/hunch_net-2007-12-21-Vowpal_Wabbit_Code_Release.html">281 hunch net-2007-12-21-Vowpal Wabbit Code Release</a></p>
<p>8 0.25639474 <a title="321-lda-8" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>9 0.24673834 <a title="321-lda-9" href="../hunch_net-2013/hunch_net-2013-01-07-NYU_Large_Scale_Machine_Learning_Class.html">478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</a></p>
<p>10 0.24384032 <a title="321-lda-10" href="../hunch_net-2008/hunch_net-2008-04-27-Watchword%3A_Supervised_Learning.html">299 hunch net-2008-04-27-Watchword: Supervised Learning</a></p>
<p>11 0.24366428 <a title="321-lda-11" href="../hunch_net-2006/hunch_net-2006-01-23-On_Coding_via_Mutual_Information_%26%23038%3B_Bayes_Nets.html">150 hunch net-2006-01-23-On Coding via Mutual Information &#038; Bayes Nets</a></p>
<p>12 0.24364866 <a title="321-lda-12" href="../hunch_net-2012/hunch_net-2012-10-26-ML_Symposium_and_Strata-Hadoop_World.html">475 hunch net-2012-10-26-ML Symposium and Strata-Hadoop World</a></p>
<p>13 0.24358009 <a title="321-lda-13" href="../hunch_net-2005/hunch_net-2005-05-06-Don%26%238217%3Bt_mix_the_solution_into_the_problem.html">67 hunch net-2005-05-06-Don&#8217;t mix the solution into the problem</a></p>
<p>14 0.24340753 <a title="321-lda-14" href="../hunch_net-2005/hunch_net-2005-12-09-Some_NIPS_papers.html">138 hunch net-2005-12-09-Some NIPS papers</a></p>
<p>15 0.24338302 <a title="321-lda-15" href="../hunch_net-2006/hunch_net-2006-06-15-IJCAI_is_out_of_season.html">184 hunch net-2006-06-15-IJCAI is out of season</a></p>
<p>16 0.24324284 <a title="321-lda-16" href="../hunch_net-2005/hunch_net-2005-03-29-Academic_Mechanism_Design.html">48 hunch net-2005-03-29-Academic Mechanism Design</a></p>
<p>17 0.24281445 <a title="321-lda-17" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>18 0.24276318 <a title="321-lda-18" href="../hunch_net-2005/hunch_net-2005-07-07-The_Limits_of_Learning_Theory.html">90 hunch net-2005-07-07-The Limits of Learning Theory</a></p>
<p>19 0.24078842 <a title="321-lda-19" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>20 0.24047689 <a title="321-lda-20" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
