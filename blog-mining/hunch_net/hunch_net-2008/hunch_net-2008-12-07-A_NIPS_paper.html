<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>330 hunch net-2008-12-07-A NIPS paper</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2008" href="../home/hunch_net-2008_home.html">hunch_net-2008</a> <a title="hunch_net-2008-330" href="#">hunch_net-2008-330</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>330 hunch net-2008-12-07-A NIPS paper</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2008-330-html" href="http://hunch.net/?p=482">html</a></p><p>Introduction: I'm skipping NIPS this year in favor ofAda, but I wanted to point outthis
paperbyAndriy MnihandGeoff Hinton. The basic claim of the paper is that by
carefully but automatically constructing a binary tree over words, it's
possible to predict words well with huge computational resource savings over
unstructured approaches.I'm interested in this beyond the application to word
prediction because it is relevant to the general normalization problem: If you
want to predict the probability of one of a large number of events, often you
must compute a predicted score for all the events and then normalize, a
computationally inefficient operation. The problem comes up in many places
using probabilistic models, but I've run into it with high-dimensional
regression.There are a couple workarounds for this computational
bug:Approximate. There are many ways. Often the approximations are
uncontrolled (i.e. can be arbitrarily bad), and hence finicky in
application.Avoid. You don't really want a probabili</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('probability', 0.237), ('events', 0.226), ('words', 0.208), ('unstructured', 0.181), ('uncontrolled', 0.168), ('savings', 0.168), ('want', 0.159), ('finicky', 0.159), ('inefficient', 0.159), ('probable', 0.159), ('resource', 0.151), ('normalization', 0.151), ('approximations', 0.145), ('arbitrarily', 0.145), ('bug', 0.14), ('viable', 0.14), ('predict', 0.14), ('computational', 0.14), ('constructing', 0.136), ('favor', 0.125)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="330-tfidf-1" href="../hunch_net-2008/hunch_net-2008-12-07-A_NIPS_paper.html">330 hunch net-2008-12-07-A NIPS paper</a></p>
<p>Introduction: I'm skipping NIPS this year in favor ofAda, but I wanted to point outthis
paperbyAndriy MnihandGeoff Hinton. The basic claim of the paper is that by
carefully but automatically constructing a binary tree over words, it's
possible to predict words well with huge computational resource savings over
unstructured approaches.I'm interested in this beyond the application to word
prediction because it is relevant to the general normalization problem: If you
want to predict the probability of one of a large number of events, often you
must compute a predicted score for all the events and then normalize, a
computationally inefficient operation. The problem comes up in many places
using probabilistic models, but I've run into it with high-dimensional
regression.There are a couple workarounds for this computational
bug:Approximate. There are many ways. Often the approximations are
uncontrolled (i.e. can be arbitrarily bad), and hence finicky in
application.Avoid. You don't really want a probabili</p><p>2 0.16733071 <a title="330-tfidf-2" href="../hunch_net-2006/hunch_net-2006-11-20-Context_and_the_calculation_misperception.html">218 hunch net-2006-11-20-Context and the calculation misperception</a></p>
<p>Introduction: This post is really for peoplenotin machine learning (or related fields). It
is about a common misperception which affects people who have not thought
about the process of trying to predict somethinng. Hopefully, by precisely
stating it, we can remove it.Suppose we have a set of events, each described
by a vector of features.01011101011101000111110011000101110Suppose we want to
predict the value of the first feature given the others. One approach is to
bin the data byonefeature. For the above example, we might partition the data
according to feature 2, then observe that when feature 2 is 0 the label
(feature 1) is mostly 1. On the other hand, when feature 2 is 1, the label
(feature 1) is mostly 0. Using this simple rule we get an observed error rate
of 3/7.There are two issues here. The first is that this is really a training
error rate, and (hence) may be an overoptimistic prediction. This is not a
very serious issue as long as there are a reasonable number of representative
examples.</p><p>3 0.14803158 <a title="330-tfidf-3" href="../hunch_net-2005/hunch_net-2005-01-26-Watchword%3A_Probability.html">5 hunch net-2005-01-26-Watchword: Probability</a></p>
<p>Introduction: Probability is one of the most confusingly used words in machine learning.
There are at least 3 distinct ways the word is used.BayesianThe Bayesian
notion of probability is a 'degree of belief'. The degree of belief that some
event (i.e. "stock goes up" or "stock goes down") occurs can be measured by
asking a sequence of questions of the form "Would you bet the stock goes up or
down atYto 1 odds?" A consistent better will switch from 'for' to 'against' at
some single value ofY. The probability is thenY/(Y+1). Bayesian probabilities
express lack of knowledge rather than randomization. They are useful in
learning because we often lack knowledge and expressing that lack flexibly
makes the learning algorithms work better. Bayesian Learning uses
'probability' in this way exclusively.FrequentistThe Frequentist notion of
probability is a rate of occurence. A rate of occurrence can be measured by
doing an experiment many times. If an event occursktimes innexperiments then
it has probability ab</p><p>4 0.14367339 <a title="330-tfidf-4" href="../hunch_net-2011/hunch_net-2011-08-06-Interesting_thing_at_UAI_2011.html">440 hunch net-2011-08-06-Interesting thing at UAI 2011</a></p>
<p>Introduction: I had a chance to attendUAIthis year, where several papers interested me,
including:Hoifung PoonandPedro DomingosSum-Product Networks: A New Deep
Architecture. We'vealready discussed this one, but in a nutshell, they
identify a large class of efficiently normalizable distributions and do
learning with it.Yao-Liang YuandDale Schuurmans,Rank/norm regularization with
closed-form solutions: Application to subspace clustering. This paper is about
matrices, and in particular they prove that certain matrices are the solution
of matrix optimizations. I'm not matrix inclined enough to fully appreciate
this one, but I believe many others may be, and anytime closed form solutions
come into play, you get 2 order of magnitude speedups, as they show
experimentally.Laurent Charlin,Richard ZemelandCraig Boutilier,A Framework for
Optimizing Paper Matching. This is about what works in matching papers to
reviewers, as has been tested at several previous NIPS. We are looking into
using this system for ICM</p><p>5 0.12455264 <a title="330-tfidf-5" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">23 hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<p>Introduction: This is apaperby Yann LeCun and Fu Jie Huang published atAISTAT 2005. I found
this paper very difficult to read, but it does have some point about a
computational shortcut.This paper takes for granted that the method of solving
a problem is gradient descent on parameters. Given this assumption, the
question arises: Do you want to do gradient descent on a probabilistic model
or something else?All (conditional) probabilistic models have the formp(y|x) =
f(x,y)/Z(x)whereZ(x) = sumyf(x,y)(the paper calls- log f(x,y)an "energy").
Iffis parameterized by somew, the gradient has a term forZ(x), and hence for
every value ofy. The paper claims, that such models can be optimized for
classification purposes using only the correctyand the othery' not ywhich
maximizesf(x,y). This can even be done on unnormalizable models. The paper
further claims that this can be done with an approximate maximum. These claims
are plausible based on experimental results and intuition.It wouldn't surprise
me to learn</p><p>6 0.12130336 <a title="330-tfidf-6" href="../hunch_net-2009/hunch_net-2009-06-03-Functionally_defined_Nonlinear_Dynamic_Models.html">359 hunch net-2009-06-03-Functionally defined Nonlinear Dynamic Models</a></p>
<p>7 0.10643557 <a title="330-tfidf-7" href="../hunch_net-2011/hunch_net-2011-04-18-A_paper_not_at_Snowbird.html">431 hunch net-2011-04-18-A paper not at Snowbird</a></p>
<p>8 0.10455659 <a title="330-tfidf-8" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<p>9 0.10112137 <a title="330-tfidf-9" href="../hunch_net-2010/hunch_net-2010-12-04-Vowpal_Wabbit%2C_version_5.0%2C_and_the_second_heresy.html">419 hunch net-2010-12-04-Vowpal Wabbit, version 5.0, and the second heresy</a></p>
<p>10 0.096941218 <a title="330-tfidf-10" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>11 0.095023237 <a title="330-tfidf-11" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>12 0.092191204 <a title="330-tfidf-12" href="../hunch_net-2007/hunch_net-2007-12-19-Cool_and_interesting_things_seen_at_NIPS.html">279 hunch net-2007-12-19-Cool and interesting things seen at NIPS</a></p>
<p>13 0.090688735 <a title="330-tfidf-13" href="../hunch_net-2006/hunch_net-2006-03-02-Why_do_people_count_for_learning%3F.html">160 hunch net-2006-03-02-Why do people count for learning?</a></p>
<p>14 0.08985997 <a title="330-tfidf-14" href="../hunch_net-2005/hunch_net-2005-09-20-Workshop_Proposal%3A_Atomic_Learning.html">114 hunch net-2005-09-20-Workshop Proposal: Atomic Learning</a></p>
<p>15 0.087744027 <a title="330-tfidf-15" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>16 0.08768063 <a title="330-tfidf-16" href="../hunch_net-2008/hunch_net-2008-10-14-Who_is_Responsible_for_a_Bad_Review%3F.html">320 hunch net-2008-10-14-Who is Responsible for a Bad Review?</a></p>
<p>17 0.086520545 <a title="330-tfidf-17" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>18 0.086183429 <a title="330-tfidf-18" href="../hunch_net-2009/hunch_net-2009-12-27-Interesting_things_at_NIPS_2009.html">385 hunch net-2009-12-27-Interesting things at NIPS 2009</a></p>
<p>19 0.084936067 <a title="330-tfidf-19" href="../hunch_net-2005/hunch_net-2005-09-19-NIPS_Workshops.html">113 hunch net-2005-09-19-NIPS Workshops</a></p>
<p>20 0.083024502 <a title="330-tfidf-20" href="../hunch_net-2010/hunch_net-2010-03-15-The_Efficient_Robust_Conditional_Probability_Estimation_Problem.html">391 hunch net-2010-03-15-The Efficient Robust Conditional Probability Estimation Problem</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.219), (1, -0.069), (2, -0.034), (3, 0.019), (4, -0.023), (5, -0.01), (6, -0.045), (7, -0.047), (8, 0.037), (9, -0.025), (10, 0.035), (11, 0.083), (12, -0.022), (13, 0.009), (14, 0.018), (15, -0.03), (16, 0.059), (17, 0.067), (18, -0.1), (19, -0.027), (20, -0.039), (21, 0.069), (22, -0.079), (23, -0.002), (24, 0.122), (25, -0.027), (26, 0.0), (27, -0.049), (28, -0.082), (29, 0.006), (30, -0.085), (31, -0.009), (32, -0.126), (33, -0.065), (34, 0.12), (35, 0.029), (36, -0.006), (37, 0.024), (38, -0.007), (39, 0.022), (40, 0.012), (41, -0.029), (42, -0.073), (43, 0.038), (44, 0.123), (45, -0.104), (46, -0.084), (47, 0.002), (48, -0.076), (49, -0.05)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97542357 <a title="330-lsi-1" href="../hunch_net-2008/hunch_net-2008-12-07-A_NIPS_paper.html">330 hunch net-2008-12-07-A NIPS paper</a></p>
<p>Introduction: I'm skipping NIPS this year in favor ofAda, but I wanted to point outthis
paperbyAndriy MnihandGeoff Hinton. The basic claim of the paper is that by
carefully but automatically constructing a binary tree over words, it's
possible to predict words well with huge computational resource savings over
unstructured approaches.I'm interested in this beyond the application to word
prediction because it is relevant to the general normalization problem: If you
want to predict the probability of one of a large number of events, often you
must compute a predicted score for all the events and then normalize, a
computationally inefficient operation. The problem comes up in many places
using probabilistic models, but I've run into it with high-dimensional
regression.There are a couple workarounds for this computational
bug:Approximate. There are many ways. Often the approximations are
uncontrolled (i.e. can be arbitrarily bad), and hence finicky in
application.Avoid. You don't really want a probabili</p><p>2 0.68088692 <a title="330-lsi-2" href="../hunch_net-2005/hunch_net-2005-01-26-Watchword%3A_Probability.html">5 hunch net-2005-01-26-Watchword: Probability</a></p>
<p>Introduction: Probability is one of the most confusingly used words in machine learning.
There are at least 3 distinct ways the word is used.BayesianThe Bayesian
notion of probability is a 'degree of belief'. The degree of belief that some
event (i.e. "stock goes up" or "stock goes down") occurs can be measured by
asking a sequence of questions of the form "Would you bet the stock goes up or
down atYto 1 odds?" A consistent better will switch from 'for' to 'against' at
some single value ofY. The probability is thenY/(Y+1). Bayesian probabilities
express lack of knowledge rather than randomization. They are useful in
learning because we often lack knowledge and expressing that lack flexibly
makes the learning algorithms work better. Bayesian Learning uses
'probability' in this way exclusively.FrequentistThe Frequentist notion of
probability is a rate of occurence. A rate of occurrence can be measured by
doing an experiment many times. If an event occursktimes innexperiments then
it has probability ab</p><p>3 0.66298622 <a title="330-lsi-3" href="../hunch_net-2011/hunch_net-2011-08-06-Interesting_thing_at_UAI_2011.html">440 hunch net-2011-08-06-Interesting thing at UAI 2011</a></p>
<p>Introduction: I had a chance to attendUAIthis year, where several papers interested me,
including:Hoifung PoonandPedro DomingosSum-Product Networks: A New Deep
Architecture. We'vealready discussed this one, but in a nutshell, they
identify a large class of efficiently normalizable distributions and do
learning with it.Yao-Liang YuandDale Schuurmans,Rank/norm regularization with
closed-form solutions: Application to subspace clustering. This paper is about
matrices, and in particular they prove that certain matrices are the solution
of matrix optimizations. I'm not matrix inclined enough to fully appreciate
this one, but I believe many others may be, and anytime closed form solutions
come into play, you get 2 order of magnitude speedups, as they show
experimentally.Laurent Charlin,Richard ZemelandCraig Boutilier,A Framework for
Optimizing Paper Matching. This is about what works in matching papers to
reviewers, as has been tested at several previous NIPS. We are looking into
using this system for ICM</p><p>4 0.61233205 <a title="330-lsi-4" href="../hunch_net-2005/hunch_net-2005-04-26-To_calibrate_or_not%3F.html">62 hunch net-2005-04-26-To calibrate or not?</a></p>
<p>Introduction: A calibrated predictor is one which predicts the probability of a binary event
with the property: For all predictionsp, the proportion of the time that1is
observed isp.Since there are infinitely manyp, this definition must be
"softened" to make sense for any finite number of samples. The standard method
for "softening" is to consider all predictions in a small neighborhood about
each possiblep.A great deal of effort has been devoted to strategies for
achieving calibrated (such ashere) prediction. With statements like: (under
minimal conditions) you can always make calibrated predictions.Given the
strength of these statements, we might conclude we are done, but that would be
a "confusion of ends". A confusion of ends arises in the following way:We want
good probabilistic predictions.Good probabilistic predictions are
calibrated.Therefore, we want calibrated predictions.The "Therefore" step
misses the fact that calibration is a necessary but not
asufficientcharacterization of good probab</p><p>5 0.58662045 <a title="330-lsi-5" href="../hunch_net-2006/hunch_net-2006-11-20-Context_and_the_calculation_misperception.html">218 hunch net-2006-11-20-Context and the calculation misperception</a></p>
<p>Introduction: This post is really for peoplenotin machine learning (or related fields). It
is about a common misperception which affects people who have not thought
about the process of trying to predict somethinng. Hopefully, by precisely
stating it, we can remove it.Suppose we have a set of events, each described
by a vector of features.01011101011101000111110011000101110Suppose we want to
predict the value of the first feature given the others. One approach is to
bin the data byonefeature. For the above example, we might partition the data
according to feature 2, then observe that when feature 2 is 0 the label
(feature 1) is mostly 1. On the other hand, when feature 2 is 1, the label
(feature 1) is mostly 0. Using this simple rule we get an observed error rate
of 3/7.There are two issues here. The first is that this is really a training
error rate, and (hence) may be an overoptimistic prediction. This is not a
very serious issue as long as there are a reasonable number of representative
examples.</p><p>6 0.54953641 <a title="330-lsi-6" href="../hunch_net-2008/hunch_net-2008-04-27-Watchword%3A_Supervised_Learning.html">299 hunch net-2008-04-27-Watchword: Supervised Learning</a></p>
<p>7 0.53158039 <a title="330-lsi-7" href="../hunch_net-2005/hunch_net-2005-12-14-More_NIPS_Papers_II.html">140 hunch net-2005-12-14-More NIPS Papers II</a></p>
<p>8 0.52338499 <a title="330-lsi-8" href="../hunch_net-2005/hunch_net-2005-10-07-On-line_learning_of_regular_decision_rules.html">118 hunch net-2005-10-07-On-line learning of regular decision rules</a></p>
<p>9 0.51905978 <a title="330-lsi-9" href="../hunch_net-2009/hunch_net-2009-06-03-Functionally_defined_Nonlinear_Dynamic_Models.html">359 hunch net-2009-06-03-Functionally defined Nonlinear Dynamic Models</a></p>
<p>10 0.49718693 <a title="330-lsi-10" href="../hunch_net-2005/hunch_net-2005-05-29-Maximum_Margin_Mismatch%3F.html">77 hunch net-2005-05-29-Maximum Margin Mismatch?</a></p>
<p>11 0.49464715 <a title="330-lsi-11" href="../hunch_net-2008/hunch_net-2008-05-25-Inappropriate_Mathematics_for_Machine_Learning.html">302 hunch net-2008-05-25-Inappropriate Mathematics for Machine Learning</a></p>
<p>12 0.49194911 <a title="330-lsi-12" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>13 0.49079147 <a title="330-lsi-13" href="../hunch_net-2005/hunch_net-2005-02-28-Regularization.html">33 hunch net-2005-02-28-Regularization</a></p>
<p>14 0.48251 <a title="330-lsi-14" href="../hunch_net-2006/hunch_net-2006-11-06-Data_Linkage_Problems.html">217 hunch net-2006-11-06-Data Linkage Problems</a></p>
<p>15 0.46786299 <a title="330-lsi-15" href="../hunch_net-2005/hunch_net-2005-10-16-Complexity%3A_It%26%238217%3Bs_all_in_your_head.html">123 hunch net-2005-10-16-Complexity: It&#8217;s all in your head</a></p>
<p>16 0.46646327 <a title="330-lsi-16" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">23 hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<p>17 0.46608996 <a title="330-lsi-17" href="../hunch_net-2009/hunch_net-2009-04-02-Asymmophobia.html">348 hunch net-2009-04-02-Asymmophobia</a></p>
<p>18 0.46408725 <a title="330-lsi-18" href="../hunch_net-2009/hunch_net-2009-06-24-Interesting_papers_at_UAICMOLT_2009.html">361 hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</a></p>
<p>19 0.46057934 <a title="330-lsi-19" href="../hunch_net-2010/hunch_net-2010-03-15-The_Efficient_Robust_Conditional_Probability_Estimation_Problem.html">391 hunch net-2010-03-15-The Efficient Robust Conditional Probability Estimation Problem</a></p>
<p>20 0.45196196 <a title="330-lsi-20" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(29, 0.322), (35, 0.136), (42, 0.247), (45, 0.02), (74, 0.165), (88, 0.011)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.98154318 <a title="330-lda-1" href="../hunch_net-2006/hunch_net-2006-07-17-A_Winner.html">197 hunch net-2006-07-17-A Winner</a></p>
<p>Introduction: Ed Snelsonwon thePredictive Uncertainty in Environmental Modelling
Competitionin the temp(erature) category usingthis algorithm. Some
characteristics of the algorithm are:Gradient descent… on about 600
parameters… with local minima… to solve regression.This bears a strong
resemblance to a neural network. The two main differences seem to be:The
system has a probabilistic interpretation (which may aid design).There are
(perhaps) fewer parameters than a typical neural network might have for the
same problem (aiding speed).</p><p>same-blog 2 0.92183977 <a title="330-lda-2" href="../hunch_net-2008/hunch_net-2008-12-07-A_NIPS_paper.html">330 hunch net-2008-12-07-A NIPS paper</a></p>
<p>Introduction: I'm skipping NIPS this year in favor ofAda, but I wanted to point outthis
paperbyAndriy MnihandGeoff Hinton. The basic claim of the paper is that by
carefully but automatically constructing a binary tree over words, it's
possible to predict words well with huge computational resource savings over
unstructured approaches.I'm interested in this beyond the application to word
prediction because it is relevant to the general normalization problem: If you
want to predict the probability of one of a large number of events, often you
must compute a predicted score for all the events and then normalize, a
computationally inefficient operation. The problem comes up in many places
using probabilistic models, but I've run into it with high-dimensional
regression.There are a couple workarounds for this computational
bug:Approximate. There are many ways. Often the approximations are
uncontrolled (i.e. can be arbitrarily bad), and hence finicky in
application.Avoid. You don't really want a probabili</p><p>3 0.90392983 <a title="330-lda-3" href="../hunch_net-2007/hunch_net-2007-04-13-What_to_do_with_an_unreasonable_conditional_accept.html">238 hunch net-2007-04-13-What to do with an unreasonable conditional accept</a></p>
<p>Introduction: Last year about this time, we received a conditional accept for thesearn
paper, which asked us to reference a paper that was not reasonable to cite
because there was strictly more relevant work by the same authors that we
already cited. We wrote a response explaining this, and didn't cite it in the
final draft, giving the SPC an excuse toreject the paper, leading to
unhappiness for all.Later,Sanjoy Dasguptasuggested that an alternative was to
talk to the PC chair instead, as soon as you see that a conditional accept is
unreasonable.William Cohenand I spoke about this by email, the relevant bit of
which is:If an SPC asks for a revision that is inappropriate, the
correctaction is to contact the chairs as soon as the decision is made,clearly
explaining what the problem is, so we can decide whether ornot to over-rule
the SPC. As you say, this is extra work for uschairs, but that's part of the
job, and we're willing to do that sortof work to improve the overall quality
of the reviewing proc</p><p>4 0.89976001 <a title="330-lda-4" href="../hunch_net-2009/hunch_net-2009-04-21-Interesting_Presentations_at_Snowbird.html">349 hunch net-2009-04-21-Interesting Presentations at Snowbird</a></p>
<p>Introduction: Here are a few of presentations interesting me at thesnowbird learningworkshop
(which, amusingly, was in Florida withAIStat).Thomas Breueldescribed machine
learning problems within OCR and an open sourceOCR software/researchplatform
with modular learning components as well has a 60Million size dataset derived
fromGoogle's scanned books.Kristen GraumanandFei-Fei Lidiscussed using active
learning with different cost labels and large datasets forimage ontology. Both
of them usedMechanical Turkas alabeling system, which looks to become routine,
at least for vision problems.Russ Tedrakediscussed using machine learning for
control, with a basic claim that it was the way to go for problems involving a
mediumReynold's numbersuch as in bird flight, where simulation is extremely
intense.Yann LeCunpresented a poster on anFPGA for convolutional neural
networksyielding a factor of 100 speedup in processing. In addition to the
graphics processor approachRajathas worked on, this seems like an effecti</p><p>5 0.86583292 <a title="330-lda-5" href="../hunch_net-2006/hunch_net-2006-10-02-%241M_Netflix_prediction_contest.html">211 hunch net-2006-10-02-$1M Netflix prediction contest</a></p>
<p>Introduction: Netflix isrunning a contestto improve recommender prediction systems. A 10%
improvement over their current system yields a $1M prize. Failing that, the
best smaller improvement yields a smaller $50K prize. This contest looks quite
real, and the $50K prize money is almost certainly achievable with a bit of
thought. The contest also comes with a dataset which is apparently 2 orders of
magnitude larger than any other public recommendation system datasets.</p><p>6 0.7272917 <a title="330-lda-6" href="../hunch_net-2012/hunch_net-2012-06-05-ICML_acceptance_statistics.html">466 hunch net-2012-06-05-ICML acceptance statistics</a></p>
<p>7 0.69110775 <a title="330-lda-7" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>8 0.68973839 <a title="330-lda-8" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>9 0.67889911 <a title="330-lda-9" href="../hunch_net-2007/hunch_net-2007-01-15-The_Machine_Learning_Department.html">228 hunch net-2007-01-15-The Machine Learning Department</a></p>
<p>10 0.67757082 <a title="330-lda-10" href="../hunch_net-2005/hunch_net-2005-06-13-Wikis_for_Summer_Schools_and_Workshops.html">81 hunch net-2005-06-13-Wikis for Summer Schools and Workshops</a></p>
<p>11 0.67735326 <a title="330-lda-11" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">23 hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<p>12 0.67660904 <a title="330-lda-12" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>13 0.67659086 <a title="330-lda-13" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>14 0.67386109 <a title="330-lda-14" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>15 0.67233998 <a title="330-lda-15" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>16 0.67226124 <a title="330-lda-16" href="../hunch_net-2005/hunch_net-2005-02-17-Learning_Research_Programs.html">21 hunch net-2005-02-17-Learning Research Programs</a></p>
<p>17 0.66920668 <a title="330-lda-17" href="../hunch_net-2008/hunch_net-2008-05-23-Three_levels_of_addressing_the_Netflix_Prize.html">301 hunch net-2008-05-23-Three levels of addressing the Netflix Prize</a></p>
<p>18 0.66517276 <a title="330-lda-18" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>19 0.66420537 <a title="330-lda-19" href="../hunch_net-2009/hunch_net-2009-09-21-Netflix_finishes_%28and_starts%29.html">371 hunch net-2009-09-21-Netflix finishes (and starts)</a></p>
<p>20 0.66419798 <a title="330-lda-20" href="../hunch_net-2006/hunch_net-2006-05-05-An_ICML_reject.html">177 hunch net-2006-05-05-An ICML reject</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
