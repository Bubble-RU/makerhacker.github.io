<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>314 hunch net-2008-08-24-Mass Customized Medicine in the Future?</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2008" href="../home/hunch_net-2008_home.html">hunch_net-2008</a> <a title="hunch_net-2008-314" href="#">hunch_net-2008-314</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>314 hunch net-2008-08-24-Mass Customized Medicine in the Future?</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2008-314-html" href="http://hunch.net/?p=400">html</a></p><p>Introduction: This post is about a technology which could develop in the future.Right now, a
new drug might be tested by finding patients with some diagnosis and giving or
not giving them a drug according to a secret randomization. The outcome is
observed, and if the average outcome for those treated is measurably better
than the average outcome for those not treated, the drug might become a
standard treatment.Generalizing this, a filterFsorts people into two groups:
those for treatmentAand those not for treatmentBbased upon observationsx. To
measure the outcome, you randomize between treatment and nontreatment of
groupAand measure the relative performance of the treatment.A problem often
arises: in many cases the treated group does not do better than the nontreated
group. A basic question is: does this mean the treatment is bad? With respect
to the filterFit may mean that, but with respect to another filterF', the
treatment might be very effective. For example, a drug might work great for
people wh</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Right now, a new drug might be tested by finding patients with some diagnosis and giving or not giving them a drug according to a secret randomization. [sent-2, score-1.481]
</p><p>2 The outcome is observed, and if the average outcome for those treated is measurably better than the average outcome for those not treated, the drug might become a standard treatment. [sent-3, score-1.674]
</p><p>3 To measure the outcome, you randomize between treatment and nontreatment of groupAand measure the relative performance of the treatment. [sent-5, score-0.791]
</p><p>4 A problem often arises: in many cases the treated group does not do better than the nontreated group. [sent-6, score-0.309]
</p><p>5 A basic question is: does this mean the treatment is bad? [sent-7, score-0.459]
</p><p>6 With respect to the filterFit may mean that, but with respect to another filterF', the treatment might be very effective. [sent-8, score-0.711]
</p><p>7 For example, a drug might work great for people which have one blood type, but not so well for others. [sent-9, score-0.464]
</p><p>8 This can be done onpast recorded data, and if done properly we can even statistically prove thatF'works,withoutanother randomized trial. [sent-13, score-0.228]
</p><p>9 Here's what this future might look like:Doctors lose a bit of control. [sent-15, score-0.181]
</p><p>10 Right now, the filtersFare typically a diagnosis of one sort or another. [sent-16, score-0.285]
</p><p>11 Instead, a doctor might record many observations, and have many learned filtersF'applied to suggest treatments. [sent-18, score-0.201]
</p><p>12 The "not understanding the details" problem is sometimes severe, so we can expect a renewed push for understandable machine learning rules. [sent-19, score-0.232]
</p><p>13 Some tradeoff between understandability and predictive power seems to exist creating a tension: do you want a good treatment or do you want an understandable treatment? [sent-20, score-0.715]
</p><p>14 The more information fed into a learning algorithm, the greater it's performance can be. [sent-21, score-0.146]
</p><p>15 If we manage to reach a pointer in the future whereGattaca stylenear instantaneous genomic sequencing is available, feeding this into a learning algorithm is potentially very effective. [sent-22, score-0.227]
</p><p>16 In general a constant pressure to measure more should be expected. [sent-23, score-0.335]
</p><p>17 Given that we can learn frompastdata, going back and measuring additional characteristics of past patients may even be desirable. [sent-24, score-0.357]
</p><p>18 Since many treatments are commercial in the US, there will be a great deal of pressure to find a filterF'which appears good, and a company investing millions into the question is quite capable of overfitting so thatF'is better than it appears. [sent-25, score-0.762]
</p><p>19 Safe and sane ways to deal with this exist, as showcased by various machine learning challenges, such as theNetflix challenge. [sent-26, score-0.144]
</p><p>20 To gain trust in such approaches, a trustable and trusted third party capable of this sort of testing must exist. [sent-27, score-0.409]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('treatment', 0.364), ('drug', 0.35), ('treated', 0.243), ('outcome', 0.241), ('diagnosis', 0.197), ('patients', 0.197), ('pressure', 0.197), ('understandable', 0.162), ('measure', 0.138), ('exist', 0.125), ('might', 0.114), ('technology', 0.099), ('capable', 0.097), ('giving', 0.096), ('mean', 0.095), ('average', 0.089), ('sort', 0.088), ('measuring', 0.087), ('feeding', 0.087), ('statistically', 0.087), ('investing', 0.087), ('commercial', 0.087), ('doctor', 0.087), ('secret', 0.081), ('trusted', 0.081), ('randomize', 0.081), ('treatments', 0.081), ('trial', 0.076), ('tension', 0.076), ('fed', 0.076), ('millions', 0.076), ('safe', 0.076), ('filters', 0.076), ('pointer', 0.073), ('sane', 0.073), ('characteristics', 0.073), ('party', 0.073), ('recorded', 0.073), ('deal', 0.071), ('push', 0.07), ('trust', 0.07), ('described', 0.07), ('thenetflix', 0.07), ('performance', 0.07), ('respect', 0.069), ('randomized', 0.068), ('challenges', 0.068), ('future', 0.067), ('better', 0.066), ('tradeoff', 0.064)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="314-tfidf-1" href="../hunch_net-2008/hunch_net-2008-08-24-Mass_Customized_Medicine_in_the_Future%3F.html">314 hunch net-2008-08-24-Mass Customized Medicine in the Future?</a></p>
<p>Introduction: This post is about a technology which could develop in the future.Right now, a
new drug might be tested by finding patients with some diagnosis and giving or
not giving them a drug according to a secret randomization. The outcome is
observed, and if the average outcome for those treated is measurably better
than the average outcome for those not treated, the drug might become a
standard treatment.Generalizing this, a filterFsorts people into two groups:
those for treatmentAand those not for treatmentBbased upon observationsx. To
measure the outcome, you randomize between treatment and nontreatment of
groupAand measure the relative performance of the treatment.A problem often
arises: in many cases the treated group does not do better than the nontreated
group. A basic question is: does this mean the treatment is bad? With respect
to the filterFit may mean that, but with respect to another filterF', the
treatment might be very effective. For example, a drug might work great for
people wh</p><p>2 0.18920937 <a title="314-tfidf-2" href="../hunch_net-2010/hunch_net-2010-09-21-Regretting_the_dead.html">411 hunch net-2010-09-21-Regretting the dead</a></p>
<p>Introduction: Nikospointed out thisnew york timesarticle aboutpoor clinical design killing
people. For those of us who study learning from exploration information this
is a reminder that low regret algorithms are particularly important, as regret
in clinical trials is measured by patient deaths.Two obvious improvements on
the experimental design are:With reasonable record keeping of existing
outcomes for the standard treatments, there is no need to explicitly assign
people to a control group with the standard treatment, as that approach is
effectively explored with great certainty. Asserting otherwise would imply
that the nature of effective treatments for cancer has changed between now and
a year ago, which denies the value of any clinical trial.An optimal
experimental design will smoothly phase between exploration and exploitation
as evidence for a new treatment shows that it can be effective. This is old
tech, for example in theEXP3.P algorithm (page 12 aka 59)although I prefer the
generalized an</p><p>3 0.11850341 <a title="314-tfidf-3" href="../hunch_net-2005/hunch_net-2005-09-26-Prediction_Bounds_as_the_Mathematics_of_Science.html">115 hunch net-2005-09-26-Prediction Bounds as the Mathematics of Science</a></p>
<p>Introduction: "Science" has many meanings, but one common meaning is "thescientific method"
which is a principled method for investigating the world using the following
steps:Form a hypothesis about the world.Use the hypothesis to make
predictions.Run experiments to confirm or disprove the predictions.The
ordering of these steps is very important to the scientific method. In
particular, predictionsmustbe made before experiments are run.Given that we
all believe in the scientific method of investigation, it may be surprising to
learn that cheating is very common. This happens for many reasons, some
innocent and some not.Drug studies. Pharmaceutical companies make predictions
about the effects of their drugs and then conduct blind clinical studies to
determine their effect. Unfortunately, they have also been caught using some
of the more advanced techniques for cheatinghere: including "reprobleming",
"data set selection", and probably "overfitting by review". It isn't too
surprising to observe this: w</p><p>4 0.11681756 <a title="314-tfidf-4" href="../hunch_net-2006/hunch_net-2006-11-06-Data_Linkage_Problems.html">217 hunch net-2006-11-06-Data Linkage Problems</a></p>
<p>Introduction: Data linkage is a problem which seems to come up in various applied machine
learning problems. I have heard it mentioned in various data mining contexts,
but it seems relatively less studied for systemic reasons.A very simple
version of the data linkage problem is a cross hospital patient record merge.
Suppose a patient (John Doe) is admitted to a hospital (General Health),
treated, and released. Later, John Doe is admitted to a second hospital
(Health General), treated, and released. Given a large number of records of
this sort, it becomes very tempting to try and predict the outcomes of
treatments. This is reasonably straightforward as a machine learning problem
if there is a shared unique identifier for John Doe used by General Health and
Health General along with time stamps. We can merge the records and create
examples of the form "Given symptoms and treatment, did the patient come back
to a hospital within the next year?" These examples could be fed into a
learning algorithm, and</p><p>5 0.098270901 <a title="314-tfidf-5" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>Introduction: "Overfitting" is traditionally defined as training some flexible
representation so that it memorizes the data but fails to predict well in the
future. For this post, I will define overfitting more generally as over-
representing the performance of systems. There are two styles of general
overfitting: overrepresenting performance on particular datasets and
(implicitly) overrepresenting performance of a method on future datasets.We
should all be aware of these methods, avoid them where possible, and take them
into account otherwise. I have used "reproblem" and "old datasets", and may
have participated in "overfitting by review"--some of these are very difficult
to avoid.NameMethodExplanationRemedyTraditional overfittingTrain a complex
predictor on too-few examples.Hold out pristine examples for testing.Use a
simpler predictor.Get more training examples.Integrate over many
predictors.Reject papers which do this.Parameter tweak overfittingUse a
learning algorithm with many parameters. Choo</p><p>6 0.092743903 <a title="314-tfidf-6" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>7 0.092503101 <a title="314-tfidf-7" href="../hunch_net-2008/hunch_net-2008-01-06-Research_Political_Issues.html">282 hunch net-2008-01-06-Research Political Issues</a></p>
<p>8 0.092501879 <a title="314-tfidf-8" href="../hunch_net-2009/hunch_net-2009-01-08-Predictive_Analytics_World.html">335 hunch net-2009-01-08-Predictive Analytics World</a></p>
<p>9 0.084080555 <a title="314-tfidf-9" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>10 0.081756443 <a title="314-tfidf-10" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>11 0.081307858 <a title="314-tfidf-11" href="../hunch_net-2005/hunch_net-2005-10-20-Machine_Learning_in_the_News.html">125 hunch net-2005-10-20-Machine Learning in the News</a></p>
<p>12 0.078035705 <a title="314-tfidf-12" href="../hunch_net-2005/hunch_net-2005-03-08-Fast_Physics_for_Learning.html">37 hunch net-2005-03-08-Fast Physics for Learning</a></p>
<p>13 0.07776656 <a title="314-tfidf-13" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>14 0.077367984 <a title="314-tfidf-14" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>15 0.075662613 <a title="314-tfidf-15" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>16 0.075562119 <a title="314-tfidf-16" href="../hunch_net-2009/hunch_net-2009-04-02-Asymmophobia.html">348 hunch net-2009-04-02-Asymmophobia</a></p>
<p>17 0.075306579 <a title="314-tfidf-17" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>18 0.074961871 <a title="314-tfidf-18" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>19 0.073787361 <a title="314-tfidf-19" href="../hunch_net-2009/hunch_net-2009-09-21-Netflix_finishes_%28and_starts%29.html">371 hunch net-2009-09-21-Netflix finishes (and starts)</a></p>
<p>20 0.071868926 <a title="314-tfidf-20" href="../hunch_net-2006/hunch_net-2006-07-09-The_Stock_Prediction_Machine_Learning_Problem.html">193 hunch net-2006-07-09-The Stock Prediction Machine Learning Problem</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.191), (1, -0.033), (2, 0.042), (3, -0.052), (4, 0.009), (5, -0.03), (6, -0.038), (7, 0.033), (8, 0.041), (9, -0.03), (10, -0.073), (11, 0.021), (12, -0.025), (13, -0.013), (14, -0.019), (15, -0.03), (16, -0.058), (17, 0.024), (18, 0.006), (19, 0.016), (20, 0.05), (21, -0.011), (22, 0.009), (23, -0.02), (24, -0.055), (25, -0.005), (26, 0.029), (27, 0.049), (28, 0.015), (29, -0.007), (30, 0.01), (31, 0.023), (32, -0.055), (33, -0.022), (34, 0.037), (35, -0.013), (36, 0.05), (37, 0.046), (38, -0.017), (39, -0.021), (40, -0.035), (41, 0.025), (42, 0.032), (43, -0.013), (44, 0.067), (45, -0.056), (46, 0.025), (47, 0.017), (48, -0.115), (49, 0.087)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.91273242 <a title="314-lsi-1" href="../hunch_net-2008/hunch_net-2008-08-24-Mass_Customized_Medicine_in_the_Future%3F.html">314 hunch net-2008-08-24-Mass Customized Medicine in the Future?</a></p>
<p>Introduction: This post is about a technology which could develop in the future.Right now, a
new drug might be tested by finding patients with some diagnosis and giving or
not giving them a drug according to a secret randomization. The outcome is
observed, and if the average outcome for those treated is measurably better
than the average outcome for those not treated, the drug might become a
standard treatment.Generalizing this, a filterFsorts people into two groups:
those for treatmentAand those not for treatmentBbased upon observationsx. To
measure the outcome, you randomize between treatment and nontreatment of
groupAand measure the relative performance of the treatment.A problem often
arises: in many cases the treated group does not do better than the nontreated
group. A basic question is: does this mean the treatment is bad? With respect
to the filterFit may mean that, but with respect to another filterF', the
treatment might be very effective. For example, a drug might work great for
people wh</p><p>2 0.6837666 <a title="314-lsi-2" href="../hunch_net-2010/hunch_net-2010-09-21-Regretting_the_dead.html">411 hunch net-2010-09-21-Regretting the dead</a></p>
<p>Introduction: Nikospointed out thisnew york timesarticle aboutpoor clinical design killing
people. For those of us who study learning from exploration information this
is a reminder that low regret algorithms are particularly important, as regret
in clinical trials is measured by patient deaths.Two obvious improvements on
the experimental design are:With reasonable record keeping of existing
outcomes for the standard treatments, there is no need to explicitly assign
people to a control group with the standard treatment, as that approach is
effectively explored with great certainty. Asserting otherwise would imply
that the nature of effective treatments for cancer has changed between now and
a year ago, which denies the value of any clinical trial.An optimal
experimental design will smoothly phase between exploration and exploitation
as evidence for a new treatment shows that it can be effective. This is old
tech, for example in theEXP3.P algorithm (page 12 aka 59)although I prefer the
generalized an</p><p>3 0.66166717 <a title="314-lsi-3" href="../hunch_net-2006/hunch_net-2006-11-06-Data_Linkage_Problems.html">217 hunch net-2006-11-06-Data Linkage Problems</a></p>
<p>Introduction: Data linkage is a problem which seems to come up in various applied machine
learning problems. I have heard it mentioned in various data mining contexts,
but it seems relatively less studied for systemic reasons.A very simple
version of the data linkage problem is a cross hospital patient record merge.
Suppose a patient (John Doe) is admitted to a hospital (General Health),
treated, and released. Later, John Doe is admitted to a second hospital
(Health General), treated, and released. Given a large number of records of
this sort, it becomes very tempting to try and predict the outcomes of
treatments. This is reasonably straightforward as a machine learning problem
if there is a shared unique identifier for John Doe used by General Health and
Health General along with time stamps. We can merge the records and create
examples of the form "Given symptoms and treatment, did the patient come back
to a hospital within the next year?" These examples could be fed into a
learning algorithm, and</p><p>4 0.6168986 <a title="314-lsi-4" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>Introduction: "Overfitting" is traditionally defined as training some flexible
representation so that it memorizes the data but fails to predict well in the
future. For this post, I will define overfitting more generally as over-
representing the performance of systems. There are two styles of general
overfitting: overrepresenting performance on particular datasets and
(implicitly) overrepresenting performance of a method on future datasets.We
should all be aware of these methods, avoid them where possible, and take them
into account otherwise. I have used "reproblem" and "old datasets", and may
have participated in "overfitting by review"--some of these are very difficult
to avoid.NameMethodExplanationRemedyTraditional overfittingTrain a complex
predictor on too-few examples.Hold out pristine examples for testing.Use a
simpler predictor.Get more training examples.Integrate over many
predictors.Reject papers which do this.Parameter tweak overfittingUse a
learning algorithm with many parameters. Choo</p><p>5 0.61445713 <a title="314-lsi-5" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>Introduction: One view of machine learning is that it's about how to program computers to
predict well. This suggests a broader research program centered around the
more pervasive goal of simply predicting well.There are many distinct strands
of this broader research program which are only partially unified. Here are
the ones that I know of:Learning Theory. Learning theory focuses on several
topics related to the dynamics and process of prediction. Convergence bounds
like theVC boundgive an intellectual foundation to many learning algorithms.
Online learning algorithms likeWeighted Majorityprovide an alternate purely
game theoretic foundation for learning.Boosting algorithmsyield algorithms for
purifying prediction abiliity.Reduction algorithmsprovide means for changing
esoteric problems into well known ones.Machine Learning. A great deal of
experience has accumulated in practical algorithm design from a mixture of
paradigms, including bayesian, biological, optimization, and
theoretical.Mechanism De</p><p>6 0.58872283 <a title="314-lsi-6" href="../hunch_net-2006/hunch_net-2006-09-07-Objective_and_subjective_interpretations_of_probability.html">205 hunch net-2006-09-07-Objective and subjective interpretations of probability</a></p>
<p>7 0.57999325 <a title="314-lsi-7" href="../hunch_net-2006/hunch_net-2006-04-09-Progress_in_Machine_Translation.html">171 hunch net-2006-04-09-Progress in Machine Translation</a></p>
<p>8 0.57748526 <a title="314-lsi-8" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>9 0.57158726 <a title="314-lsi-9" href="../hunch_net-2005/hunch_net-2005-01-19-Why_I_decided_to_run_a_weblog..html">1 hunch net-2005-01-19-Why I decided to run a weblog.</a></p>
<p>10 0.5650875 <a title="314-lsi-10" href="../hunch_net-2005/hunch_net-2005-09-26-Prediction_Bounds_as_the_Mathematics_of_Science.html">115 hunch net-2005-09-26-Prediction Bounds as the Mathematics of Science</a></p>
<p>11 0.5637356 <a title="314-lsi-11" href="../hunch_net-2005/hunch_net-2005-04-27-DARPA_project%3A_LAGR.html">63 hunch net-2005-04-27-DARPA project: LAGR</a></p>
<p>12 0.56103718 <a title="314-lsi-12" href="../hunch_net-2005/hunch_net-2005-03-08-Fast_Physics_for_Learning.html">37 hunch net-2005-03-08-Fast Physics for Learning</a></p>
<p>13 0.55969763 <a title="314-lsi-13" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>14 0.55682701 <a title="314-lsi-14" href="../hunch_net-2013/hunch_net-2013-11-21-Ben_Taskar_is_gone.html">491 hunch net-2013-11-21-Ben Taskar is gone</a></p>
<p>15 0.55651557 <a title="314-lsi-15" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<p>16 0.55446404 <a title="314-lsi-16" href="../hunch_net-2005/hunch_net-2005-03-29-Academic_Mechanism_Design.html">48 hunch net-2005-03-29-Academic Mechanism Design</a></p>
<p>17 0.55229199 <a title="314-lsi-17" href="../hunch_net-2010/hunch_net-2010-03-26-A_Variance_only_Deviation_Bound.html">392 hunch net-2010-03-26-A Variance only Deviation Bound</a></p>
<p>18 0.55105418 <a title="314-lsi-18" href="../hunch_net-2008/hunch_net-2008-01-06-Research_Political_Issues.html">282 hunch net-2008-01-06-Research Political Issues</a></p>
<p>19 0.54409528 <a title="314-lsi-19" href="../hunch_net-2005/hunch_net-2005-08-22-Do_you_believe_in_induction%3F.html">104 hunch net-2005-08-22-Do you believe in induction?</a></p>
<p>20 0.54174286 <a title="314-lsi-20" href="../hunch_net-2005/hunch_net-2005-09-14-The_Predictionist_Viewpoint.html">112 hunch net-2005-09-14-The Predictionist Viewpoint</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(42, 0.258), (45, 0.026), (48, 0.012), (68, 0.034), (74, 0.067), (82, 0.458), (95, 0.054)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.96528369 <a title="314-lda-1" href="../hunch_net-2010/hunch_net-2010-09-13-AIStats.html">409 hunch net-2010-09-13-AIStats</a></p>
<p>Introduction: Geoff Gordonpoints outAIStats 2011in Ft. Lauderdale, Florida. Thecall for
papersis now out, due Nov. 1. The plan is toexperiment with the review
processto encourage quality in several ways. I expect to submit a paper and
would encourage others with good research to do likewise.</p><p>2 0.95293224 <a title="314-lda-2" href="../hunch_net-2005/hunch_net-2005-05-02-Reviewing_techniques_for_conferences.html">65 hunch net-2005-05-02-Reviewing techniques for conferences</a></p>
<p>Introduction: The many reviews following the many paper deadlines are just about over. AAAI
and ICML in particular were experimenting with several reviewing
techniques.Double Blind: AAAI and ICML were both double blind this year. It
seemed (overall) beneficial, but two problems arose.For theoretical papers,
with a lot to say, authors often leave out the proofs. This is very hard to
cope with under a double blind review because (1) you can not trust the
authors got the proof right but (2) a blanket "reject" hits many probably-good
papers. Perhaps authors should more strongly favor proof-complete papers sent
to double blind conferences.On the author side, double blind reviewing is
actually somewhat disruptive to research. In particular, it discourages the
author from talking about the subject, which is one of the mechanisms of
research. This is not a great drawback, but it is one not previously
appreciated.Author feedback: AAAI and ICML did author feedback this year. It
seemed helpful for several pape</p><p>3 0.93928981 <a title="314-lda-3" href="../hunch_net-2006/hunch_net-2006-07-09-The_Stock_Prediction_Machine_Learning_Problem.html">193 hunch net-2006-07-09-The Stock Prediction Machine Learning Problem</a></p>
<p>Introduction: …is discussed inthis nytimes article. I generally expect such approaches to
become more common since computers are getting faster, machine learning is
getting better, and data is becoming more plentiful. This is another example
where machine learning technology may have a huge economic impact. Some side
notes:We-in-research know almost nothing about how these things are done
(because it is typically a corporate secret).… but the limited discussion in
the article seem naive from a machine learning viewpoint.The learning process
used apparently often fails to take into account transaction costs.What little
of the approaches is discussed appears modeling based. It seems plausible that
more direct prediction methods can yield an edge.One difficulty with stock
picking as a research topic is that it is inherently a zero sum game (for
every winner, there is a loser). Much of the rest of research is positive sum
(basically, everyone wins).</p><p>4 0.91785407 <a title="314-lda-4" href="../hunch_net-2005/hunch_net-2005-03-13-Avoiding_Bad_Reviewing.html">40 hunch net-2005-03-13-Avoiding Bad Reviewing</a></p>
<p>Introduction: If we accept that bad reviewing often occurs and want to fix it, the question
is "how"?Reviewing is done by paper writers just like yourself, so a good
proxy for this question is asking "How can I be a better reviewer?" Here are a
few things I've learned by trial (and error), as a paper writer, and as a
reviewer.The secret ingredient is careful thought. There is no good
substitution for a deep and careful understanding.Avoid reviewing papers that
you feel competitive about. You almost certainly will be asked to review
papers that feel competitive if you work on subjects of common interest. But,
the feeling of competition can easily lead to bad judgement.If you feel biased
for some other reason, then you should avoid reviewing. For exampleâ&euro;ŚFeeling
angry or threatened by a paper is a form of bias. See above.Double blind
yourself (avoid looking at the name even in a single-blind situation). The
significant effect of a name you recognize is making you pay close attention
to a paper. Since</p><p>same-blog 5 0.86672986 <a title="314-lda-5" href="../hunch_net-2008/hunch_net-2008-08-24-Mass_Customized_Medicine_in_the_Future%3F.html">314 hunch net-2008-08-24-Mass Customized Medicine in the Future?</a></p>
<p>Introduction: This post is about a technology which could develop in the future.Right now, a
new drug might be tested by finding patients with some diagnosis and giving or
not giving them a drug according to a secret randomization. The outcome is
observed, and if the average outcome for those treated is measurably better
than the average outcome for those not treated, the drug might become a
standard treatment.Generalizing this, a filterFsorts people into two groups:
those for treatmentAand those not for treatmentBbased upon observationsx. To
measure the outcome, you randomize between treatment and nontreatment of
groupAand measure the relative performance of the treatment.A problem often
arises: in many cases the treated group does not do better than the nontreated
group. A basic question is: does this mean the treatment is bad? With respect
to the filterFit may mean that, but with respect to another filterF', the
treatment might be very effective. For example, a drug might work great for
people wh</p><p>6 0.74174911 <a title="314-lda-6" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>7 0.7328878 <a title="314-lda-7" href="../hunch_net-2010/hunch_net-2010-04-26-Compassionate_Reviewing.html">395 hunch net-2010-04-26-Compassionate Reviewing</a></p>
<p>8 0.68471384 <a title="314-lda-8" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>9 0.64030677 <a title="314-lda-9" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>10 0.63289762 <a title="314-lda-10" href="../hunch_net-2008/hunch_net-2008-12-12-Summer_Conferences.html">331 hunch net-2008-12-12-Summer Conferences</a></p>
<p>11 0.61770225 <a title="314-lda-11" href="../hunch_net-2012/hunch_net-2012-06-29-ICML_survey_and_comments.html">468 hunch net-2012-06-29-ICML survey and comments</a></p>
<p>12 0.61685866 <a title="314-lda-12" href="../hunch_net-2009/hunch_net-2009-12-09-Future_Publication_Models_%40_NIPS.html">382 hunch net-2009-12-09-Future Publication Models @ NIPS</a></p>
<p>13 0.61592007 <a title="314-lda-13" href="../hunch_net-2012/hunch_net-2012-06-05-ICML_acceptance_statistics.html">466 hunch net-2012-06-05-ICML acceptance statistics</a></p>
<p>14 0.61556762 <a title="314-lda-14" href="../hunch_net-2013/hunch_net-2013-06-29-The_Benefits_of_Double-Blind_Review.html">485 hunch net-2013-06-29-The Benefits of Double-Blind Review</a></p>
<p>15 0.61547071 <a title="314-lda-15" href="../hunch_net-2006/hunch_net-2006-09-12-Incentive_Compatible_Reviewing.html">207 hunch net-2006-09-12-Incentive Compatible Reviewing</a></p>
<p>16 0.60968256 <a title="314-lda-16" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>17 0.59274554 <a title="314-lda-17" href="../hunch_net-2006/hunch_net-2006-04-06-Bounds_greater_than_1.html">170 hunch net-2006-04-06-Bounds greater than 1</a></p>
<p>18 0.59135592 <a title="314-lda-18" href="../hunch_net-2013/hunch_net-2013-06-16-Representative_Reviewing.html">484 hunch net-2013-06-16-Representative Reviewing</a></p>
<p>19 0.59092462 <a title="314-lda-19" href="../hunch_net-2008/hunch_net-2008-09-26-The_SODA_Program_Committee.html">318 hunch net-2008-09-26-The SODA Program Committee</a></p>
<p>20 0.5803318 <a title="314-lda-20" href="../hunch_net-2009/hunch_net-2009-06-01-Multitask_Poisoning.html">358 hunch net-2009-06-01-Multitask Poisoning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
