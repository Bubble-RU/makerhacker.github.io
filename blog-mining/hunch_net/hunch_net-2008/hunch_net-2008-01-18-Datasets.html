<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>284 hunch net-2008-01-18-Datasets</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2008" href="../home/hunch_net-2008_home.html">hunch_net-2008</a> <a title="hunch_net-2008-284" href="#">hunch_net-2008-284</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>284 hunch net-2008-01-18-Datasets</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2008-284-html" href="http://hunch.net/?p=312">html</a></p><p>Introduction: David Pennock  notes the impressive  set of datasets  at  datawrangling .</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 David Pennock  notes the impressive  set of datasets  at  datawrangling . [sent-1, score-1.719]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('impressive', 0.549), ('notes', 0.537), ('david', 0.435), ('datasets', 0.419), ('set', 0.214)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="284-tfidf-1" href="../hunch_net-2008/hunch_net-2008-01-18-Datasets.html">284 hunch net-2008-01-18-Datasets</a></p>
<p>Introduction: David Pennock  notes the impressive  set of datasets  at  datawrangling .</p><p>2 0.17839091 <a title="284-tfidf-2" href="../hunch_net-2013/hunch_net-2013-07-10-Thoughts_on_Artificial_Intelligence.html">486 hunch net-2013-07-10-Thoughts on Artificial Intelligence</a></p>
<p>Introduction: David McAllester   starts a blog .</p><p>3 0.14785241 <a title="284-tfidf-3" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>Introduction: “Overfitting” is traditionally defined as training some flexible representation so that it memorizes the data but fails to predict well in the future. For this post, I will define overfitting more generally as over-representing the performance of systems.  There are two styles of general overfitting: overrepresenting performance on particular datasets and (implicitly) overrepresenting performance of a method on future datasets.  
 
We should all be aware of these methods, avoid them where possible, and take them into account otherwise.   I have used “reproblem” and “old datasets”, and may have participated in “overfitting by review”—some of these are very difficult to avoid.
  
 
 Name 
 Method 
 Explanation 
 Remedy 
 
 
 Traditional overfitting 
 Train a complex predictor on too-few examples. 
  
 
 
 Hold out pristine examples for testing. 
 Use a simpler predictor. 
 Get more training examples. 
 Integrate over many predictors. 
 Reject papers which do this. 
 
 
 
 
 Parameter twe</p><p>4 0.12191123 <a title="284-tfidf-4" href="../hunch_net-2006/hunch_net-2006-08-03-AOL%26%238217%3Bs_data_drop.html">200 hunch net-2006-08-03-AOL&#8217;s data drop</a></p>
<p>Introduction: AOL has  released  several large search engine related datasets.  This looks like a pretty impressive data release, and it is a big opportunity for people everywhere to worry about search engine related learning problems, if they want.</p><p>5 0.11149674 <a title="284-tfidf-5" href="../hunch_net-2007/hunch_net-2007-06-24-Interesting_Papers_at_ICML_2007.html">251 hunch net-2007-06-24-Interesting Papers at ICML 2007</a></p>
<p>Introduction: Here are a few of the papers I enjoyed at ICML.
  
  Steffen Bickel , Michael BrÃƒÂ¼eckner,  Tobias Scheffer ,  Discriminative Learning for Differing Training and Test Distributions   There is a nice trick in this paper: they predict the probability that an unlabeled sample is in the training set vs. the test set, and then use this prediction to importance weight labeled samples in the training set.  This paper uses a specific parametric model, but the approach is easily generalized. 
  Steve Hanneke   A Bound on the Label Complexity of Agnostic Active Learning   This paper bounds the number of labels required by the A 2  algorithm for active learning in the agnostic case.   Last year we figured out agnostic active learning was possible.  This year, it’s quantified.  Hopefull soon, it will be practical. 
  Sylvian Gelly ,  David Silver   Combining Online and Offline Knowledge in UCT .  This paper is about techniques for improving  MoGo  with various sorts of learning.  MoGo has a fair</p><p>6 0.071852222 <a title="284-tfidf-6" href="../hunch_net-2006/hunch_net-2006-07-09-The_Stock_Prediction_Machine_Learning_Problem.html">193 hunch net-2006-07-09-The Stock Prediction Machine Learning Problem</a></p>
<p>7 0.070244342 <a title="284-tfidf-7" href="../hunch_net-2011/hunch_net-2011-01-16-2011_Summer_Conference_Deadline_Season.html">422 hunch net-2011-01-16-2011 Summer Conference Deadline Season</a></p>
<p>8 0.069192402 <a title="284-tfidf-8" href="../hunch_net-2005/hunch_net-2005-11-07-Prediction_Competitions.html">129 hunch net-2005-11-07-Prediction Competitions</a></p>
<p>9 0.069126964 <a title="284-tfidf-9" href="../hunch_net-2008/hunch_net-2008-04-30-Concerns_about_the_Large_Scale_Learning_Challenge.html">300 hunch net-2008-04-30-Concerns about the Large Scale Learning Challenge</a></p>
<p>10 0.066005543 <a title="284-tfidf-10" href="../hunch_net-2007/hunch_net-2007-07-13-The_View_From_China.html">255 hunch net-2007-07-13-The View From China</a></p>
<p>11 0.061860919 <a title="284-tfidf-11" href="../hunch_net-2005/hunch_net-2005-11-16-The_Everything_Ensemble_Edge.html">131 hunch net-2005-11-16-The Everything Ensemble Edge</a></p>
<p>12 0.058820173 <a title="284-tfidf-12" href="../hunch_net-2010/hunch_net-2010-04-14-MLcomp%3A_a_website_for_objectively_comparing_ML_algorithms.html">393 hunch net-2010-04-14-MLcomp: a website for objectively comparing ML algorithms</a></p>
<p>13 0.056726277 <a title="284-tfidf-13" href="../hunch_net-2006/hunch_net-2006-06-30-ICML_papers.html">188 hunch net-2006-06-30-ICML papers</a></p>
<p>14 0.05618066 <a title="284-tfidf-14" href="../hunch_net-2009/hunch_net-2009-10-26-NIPS_workshops.html">375 hunch net-2009-10-26-NIPS workshops</a></p>
<p>15 0.055017285 <a title="284-tfidf-15" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>16 0.054667961 <a title="284-tfidf-16" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>17 0.051812291 <a title="284-tfidf-17" href="../hunch_net-2010/hunch_net-2010-09-17-New_York_Area_Machine_Learning_Events.html">410 hunch net-2010-09-17-New York Area Machine Learning Events</a></p>
<p>18 0.051544018 <a title="284-tfidf-18" href="../hunch_net-2006/hunch_net-2006-10-02-%241M_Netflix_prediction_contest.html">211 hunch net-2006-10-02-$1M Netflix prediction contest</a></p>
<p>19 0.051514853 <a title="284-tfidf-19" href="../hunch_net-2008/hunch_net-2008-07-15-Interesting_papers_at_COLT_%28and_a_bit_of_UAI_%26%23038%3B_workshops%29.html">310 hunch net-2008-07-15-Interesting papers at COLT (and a bit of UAI &#038; workshops)</a></p>
<p>20 0.05084743 <a title="284-tfidf-20" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.047), (1, 0.017), (2, -0.021), (3, -0.01), (4, 0.022), (5, -0.015), (6, -0.05), (7, -0.037), (8, 0.021), (9, -0.011), (10, -0.029), (11, 0.129), (12, 0.012), (13, -0.007), (14, -0.013), (15, 0.004), (16, 0.036), (17, 0.026), (18, -0.019), (19, 0.027), (20, 0.033), (21, -0.062), (22, -0.033), (23, -0.007), (24, 0.011), (25, 0.021), (26, 0.009), (27, -0.077), (28, -0.058), (29, -0.022), (30, -0.001), (31, 0.081), (32, 0.111), (33, 0.005), (34, 0.126), (35, 0.014), (36, -0.04), (37, -0.019), (38, 0.07), (39, 0.108), (40, 0.166), (41, -0.004), (42, -0.069), (43, -0.006), (44, 0.005), (45, -0.064), (46, -0.146), (47, -0.049), (48, -0.099), (49, 0.047)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99163461 <a title="284-lsi-1" href="../hunch_net-2008/hunch_net-2008-01-18-Datasets.html">284 hunch net-2008-01-18-Datasets</a></p>
<p>Introduction: David Pennock  notes the impressive  set of datasets  at  datawrangling .</p><p>2 0.63050938 <a title="284-lsi-2" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>Introduction: “Overfitting” is traditionally defined as training some flexible representation so that it memorizes the data but fails to predict well in the future. For this post, I will define overfitting more generally as over-representing the performance of systems.  There are two styles of general overfitting: overrepresenting performance on particular datasets and (implicitly) overrepresenting performance of a method on future datasets.  
 
We should all be aware of these methods, avoid them where possible, and take them into account otherwise.   I have used “reproblem” and “old datasets”, and may have participated in “overfitting by review”—some of these are very difficult to avoid.
  
 
 Name 
 Method 
 Explanation 
 Remedy 
 
 
 Traditional overfitting 
 Train a complex predictor on too-few examples. 
  
 
 
 Hold out pristine examples for testing. 
 Use a simpler predictor. 
 Get more training examples. 
 Integrate over many predictors. 
 Reject papers which do this. 
 
 
 
 
 Parameter twe</p><p>3 0.44778663 <a title="284-lsi-3" href="../hunch_net-2005/hunch_net-2005-11-16-The_Everything_Ensemble_Edge.html">131 hunch net-2005-11-16-The Everything Ensemble Edge</a></p>
<p>Introduction: Rich Caruana ,  Alexandru Niculescu , Geoff Crew, and Alex Ksikes have done  a lot of empirical testing  which shows that  using all methods to make a prediction  is more powerful than using any single method.  This is in rough agreement with the Bayesian way of solving problems, but based upon a different (essentially empirical) motivation.  A rough summary is:
  
 Take all of {decision trees, boosted decision trees, bagged decision trees, boosted decision stumps, K nearest neighbors, neural networks, SVM} with all reasonable parameter settings. 
 Run the methods on each problem of 8 problems with a large test set, calibrating margins using either  sigmoid fitting  or  isotonic regression . 
 For each loss of {accuracy, area under the ROC curve, cross entropy, squared error, etc…} evaluate the average performance of the method. 
  
A series of conclusions can be drawn from the observations.
  
 ( Calibrated ) boosted decision trees appear to perform best, in general although support v</p><p>4 0.4183442 <a title="284-lsi-4" href="../hunch_net-2009/hunch_net-2009-06-26-Netflix_nearly_done.html">362 hunch net-2009-06-26-Netflix nearly done</a></p>
<p>Introduction: A $1M qualifying result was achieved on the  public Netflix test set  by a  3-way ensemble team .  This is just in time for  Yehuda ‘s presentation at  KDD , which I’m sure will be one of the best attended ever.  
 
This isn’t quite over—there are a few days for another super-conglomerate team to come together and there is some small chance that the performance is nonrepresentative of the final test set, but I expect not.  
 
Regardless of the final outcome, the biggest lesson for ML from the Netflix contest has been the formidable performance edge of ensemble methods.</p><p>5 0.37951005 <a title="284-lsi-5" href="../hunch_net-2006/hunch_net-2006-10-02-%241M_Netflix_prediction_contest.html">211 hunch net-2006-10-02-$1M Netflix prediction contest</a></p>
<p>Introduction: Netflix is  running a contest  to improve recommender prediction systems.   A 10% improvement over their current system yields a $1M prize.  Failing that, the best smaller improvement yields a smaller $50K prize.  This contest looks quite real, and the $50K prize money is almost certainly achievable with a bit of thought.  The contest also comes with a dataset which is apparently 2 orders of magnitude larger than any other public recommendation system datasets.</p><p>6 0.37569514 <a title="284-lsi-6" href="../hunch_net-2005/hunch_net-2005-02-12-ROC_vs._Accuracy_vs._AROC.html">18 hunch net-2005-02-12-ROC vs. Accuracy vs. AROC</a></p>
<p>7 0.37141412 <a title="284-lsi-7" href="../hunch_net-2013/hunch_net-2013-07-10-Thoughts_on_Artificial_Intelligence.html">486 hunch net-2013-07-10-Thoughts on Artificial Intelligence</a></p>
<p>8 0.37067014 <a title="284-lsi-8" href="../hunch_net-2005/hunch_net-2005-11-07-Prediction_Competitions.html">129 hunch net-2005-11-07-Prediction Competitions</a></p>
<p>9 0.35985252 <a title="284-lsi-9" href="../hunch_net-2007/hunch_net-2007-06-24-Interesting_Papers_at_ICML_2007.html">251 hunch net-2007-06-24-Interesting Papers at ICML 2007</a></p>
<p>10 0.35390645 <a title="284-lsi-10" href="../hunch_net-2005/hunch_net-2005-04-14-Families_of_Learning_Theory_Statements.html">56 hunch net-2005-04-14-Families of Learning Theory Statements</a></p>
<p>11 0.35197836 <a title="284-lsi-11" href="../hunch_net-2007/hunch_net-2007-04-18-%2450K_Spock_Challenge.html">239 hunch net-2007-04-18-$50K Spock Challenge</a></p>
<p>12 0.34989786 <a title="284-lsi-12" href="../hunch_net-2008/hunch_net-2008-04-30-Concerns_about_the_Large_Scale_Learning_Challenge.html">300 hunch net-2008-04-30-Concerns about the Large Scale Learning Challenge</a></p>
<p>13 0.33495378 <a title="284-lsi-13" href="../hunch_net-2010/hunch_net-2010-04-14-MLcomp%3A_a_website_for_objectively_comparing_ML_algorithms.html">393 hunch net-2010-04-14-MLcomp: a website for objectively comparing ML algorithms</a></p>
<p>14 0.33208561 <a title="284-lsi-14" href="../hunch_net-2005/hunch_net-2005-02-21-Problem%3A_Cross_Validation.html">26 hunch net-2005-02-21-Problem: Cross Validation</a></p>
<p>15 0.33186367 <a title="284-lsi-15" href="../hunch_net-2010/hunch_net-2010-05-20-Google_Predict.html">399 hunch net-2010-05-20-Google Predict</a></p>
<p>16 0.32096979 <a title="284-lsi-16" href="../hunch_net-2009/hunch_net-2009-04-21-Interesting_Presentations_at_Snowbird.html">349 hunch net-2009-04-21-Interesting Presentations at Snowbird</a></p>
<p>17 0.30086595 <a title="284-lsi-17" href="../hunch_net-2009/hunch_net-2009-07-11-Interesting_papers_at_KDD.html">364 hunch net-2009-07-11-Interesting papers at KDD</a></p>
<p>18 0.30046475 <a title="284-lsi-18" href="../hunch_net-2006/hunch_net-2006-05-05-An_ICML_reject.html">177 hunch net-2006-05-05-An ICML reject</a></p>
<p>19 0.27420723 <a title="284-lsi-19" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>20 0.27038646 <a title="284-lsi-20" href="../hunch_net-2010/hunch_net-2010-12-02-Traffic_Prediction_Problem.html">418 hunch net-2010-12-02-Traffic Prediction Problem</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(90, 0.686)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.92294157 <a title="284-lda-1" href="../hunch_net-2008/hunch_net-2008-11-04-Rise_of_the_Machines.html">323 hunch net-2008-11-04-Rise of the Machines</a></p>
<p>Introduction: On the  enduring topic of how people deal with intelligent machines , we have this important  election bulletin .</p><p>2 0.89859974 <a title="284-lda-2" href="../hunch_net-2009/hunch_net-2009-01-28-Nielsen%26%238217%3Bs_talk.html">340 hunch net-2009-01-28-Nielsen&#8217;s talk</a></p>
<p>Introduction: I wanted to point to   Michael Nielsenâ&euro;&trade;s talk  about blogging science, which I found interesting.</p><p>same-blog 3 0.78501713 <a title="284-lda-3" href="../hunch_net-2008/hunch_net-2008-01-18-Datasets.html">284 hunch net-2008-01-18-Datasets</a></p>
<p>Introduction: David Pennock  notes the impressive  set of datasets  at  datawrangling .</p><p>4 0.7049492 <a title="284-lda-4" href="../hunch_net-2005/hunch_net-2005-02-27-Antilearning%3A_When_proximity_goes_bad.html">32 hunch net-2005-02-27-Antilearning: When proximity goes bad</a></p>
<p>Introduction: Joel Predd   mentioned  “ Antilearning ” by  Adam Kowalczyk , which is interesting from a foundational intuitions viewpoint.
 
There is a pervasive intuition that “nearby things tend to have the same label”.  This intuition is instantiated in SVMs, nearest neighbor classifiers, decision trees, and neural networks.  It turns out there are natural problems where this intuition is opposite of the truth.
 
One natural situation where this occurs is in competition.   For example, when  Intel  fails to meet its earnings estimate, is this evidence that  AMD  is doing badly also?  Or evidence that AMD is doing well?
 
This violation of the proximity intuition means that when the number of examples is few,  negating  a classifier which attempts to exploit proximity can provide predictive power (thus, the term “antilearning”).</p><p>5 0.68387479 <a title="284-lda-5" href="../hunch_net-2007/hunch_net-2007-04-18-%2450K_Spock_Challenge.html">239 hunch net-2007-04-18-$50K Spock Challenge</a></p>
<p>Introduction: Apparently, the company  Spock  is setting up a  $50k entity resolution challenge .  $50k is much less than the Netflix challenge, but it’s effectively the same as Netflix until  someone reaches 10% .  It’s also nice that the Spock challenge has a short duration.  The (visible) test set is of size 25k and the training set has size 75k.</p><p>6 0.52578551 <a title="284-lda-6" href="../hunch_net-2005/hunch_net-2005-12-11-More_NIPS_Papers.html">139 hunch net-2005-12-11-More NIPS Papers</a></p>
<p>7 0.48276007 <a title="284-lda-7" href="../hunch_net-2005/hunch_net-2005-12-28-Yet_more_nips_thoughts.html">144 hunch net-2005-12-28-Yet more nips thoughts</a></p>
<p>8 0.28467268 <a title="284-lda-8" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>9 0.037052784 <a title="284-lda-9" href="../hunch_net-2011/hunch_net-2011-01-16-2011_Summer_Conference_Deadline_Season.html">422 hunch net-2011-01-16-2011 Summer Conference Deadline Season</a></p>
<p>10 0.032866511 <a title="284-lda-10" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>11 0.024599608 <a title="284-lda-11" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>12 0.0 <a title="284-lda-12" href="../hunch_net-2005/hunch_net-2005-01-19-Why_I_decided_to_run_a_weblog..html">1 hunch net-2005-01-19-Why I decided to run a weblog.</a></p>
<p>13 0.0 <a title="284-lda-13" href="../hunch_net-2005/hunch_net-2005-01-24-Holy_grails_of_machine_learning%3F.html">2 hunch net-2005-01-24-Holy grails of machine learning?</a></p>
<p>14 0.0 <a title="284-lda-14" href="../hunch_net-2005/hunch_net-2005-01-24-The_Humanloop_Spectrum_of_Machine_Learning.html">3 hunch net-2005-01-24-The Humanloop Spectrum of Machine Learning</a></p>
<p>15 0.0 <a title="284-lda-15" href="../hunch_net-2005/hunch_net-2005-01-26-Summer_Schools.html">4 hunch net-2005-01-26-Summer Schools</a></p>
<p>16 0.0 <a title="284-lda-16" href="../hunch_net-2005/hunch_net-2005-01-26-Watchword%3A_Probability.html">5 hunch net-2005-01-26-Watchword: Probability</a></p>
<p>17 0.0 <a title="284-lda-17" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>18 0.0 <a title="284-lda-18" href="../hunch_net-2005/hunch_net-2005-01-31-Watchword%3A_Assumption.html">7 hunch net-2005-01-31-Watchword: Assumption</a></p>
<p>19 0.0 <a title="284-lda-19" href="../hunch_net-2005/hunch_net-2005-02-01-NIPS%3A_Online_Bayes.html">8 hunch net-2005-02-01-NIPS: Online Bayes</a></p>
<p>20 0.0 <a title="284-lda-20" href="../hunch_net-2005/hunch_net-2005-02-01-Watchword%3A_Loss.html">9 hunch net-2005-02-01-Watchword: Loss</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
