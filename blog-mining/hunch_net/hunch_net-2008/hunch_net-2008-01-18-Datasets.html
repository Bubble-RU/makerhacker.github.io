<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>284 hunch net-2008-01-18-Datasets</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2008" href="../home/hunch_net-2008_home.html">hunch_net-2008</a> <a title="hunch_net-2008-284" href="#">hunch_net-2008-284</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>284 hunch net-2008-01-18-Datasets</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2008-284-html" href="http://hunch.net/?p=312">html</a></p><p>Introduction: David Pennocknotes the impressiveset of datasetsatdatawrangling.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('david', 1.0)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="284-tfidf-1" href="../hunch_net-2008/hunch_net-2008-01-18-Datasets.html">284 hunch net-2008-01-18-Datasets</a></p>
<p>Introduction: David Pennocknotes the impressiveset of datasetsatdatawrangling.</p><p>2 0.77433622 <a title="284-tfidf-2" href="../hunch_net-2013/hunch_net-2013-07-10-Thoughts_on_Artificial_Intelligence.html">486 hunch net-2013-07-10-Thoughts on Artificial Intelligence</a></p>
<p>Introduction: David McAllesterstarts a blog.</p><p>3 0.14934608 <a title="284-tfidf-3" href="../hunch_net-2010/hunch_net-2010-09-17-New_York_Area_Machine_Learning_Events.html">410 hunch net-2010-09-17-New York Area Machine Learning Events</a></p>
<p>Introduction: On Sept 21, there is anothermachine learning meetupwhere I'll be speaking.
Although the topic is contextual bandits, I think of it as "the future of
machine learning". In particular, it's all about how to learn in an
interactive environment, such as for ad display, trading, news recommendation,
etcâ&euro;ŚOn Sept 24, abstracts for theNew York Machine Learning Symposiumare due.
This is the largest Machine Learning event in the area, so it's a great way to
have a conversation with other people.On Oct 22, the NY ML Symposium actually
happens. This year, we are expanding the spotlights, and trying to have more
time for posters. In addition, we have a strong set of invited speakers:David
Blei,Sanjoy Dasgupta,Tommi Jaakkola, andYann LeCun. After the meeting, a
latehackNYrelated event is planned where students and startups can meet.I'd
also like to point out the relatedCS/Econ symposiumas I have interests there
as well.</p><p>4 0.11953912 <a title="284-tfidf-4" href="../hunch_net-2009/hunch_net-2009-08-26-Another_10-year_paper_in_Machine_Learning.html">368 hunch net-2009-08-26-Another 10-year paper in Machine Learning</a></p>
<p>Introduction: When I was thinking about the best "10 year paper" forICML, I also took a look
at a few other conferences. Here is one from 10 years ago that interested
me:David McAllesterPAC-Bayesian Model Averaging,COLT1999.2001 Journal
Draft.Prior to this paper, the only mechanism known for controlling or
estimating the necessary sample complexity for learning over continuously
parameterized predictors was VC theory and variants, all of which suffered
from a basic problem: they were incredibly pessimistic in practice. This meant
that only very gross guidance could be provided for learning algorithm design.
The PAC-Bayes bound provided an alternative approach to sample complexity
bounds which was radically tighter, quantitatively. It also imported and
explained many of the motivations for Bayesian learning in a way that learning
theory and perhaps optimization people might appreciate. Since this paper came
out, there have been a number of moderately successful attempts to drive
algorithms directly b</p><p>5 0.085164338 <a title="284-tfidf-5" href="../hunch_net-2005/hunch_net-2005-04-28-Science_Fiction_and_Research.html">64 hunch net-2005-04-28-Science Fiction and Research</a></p>
<p>Introduction: A big part of doing research is imagining how things could be different, and
then trying to figure out how to get there.A big part of science fiction is
imagining how things could be different, and then working through the
implications.Because of the similarity here, reading science fiction can
sometimes be helpful in understanding and doing research. (And, hey, it's
fun.) Here's some list of science fiction books I enjoyed which seem
particularly relevant to computer science and (sometimes) learning
systems:Vernor Vinge, "True Names", "A Fire Upon the Deep"Marc Stiegler,
"David's Sling", "Earthweb"Charles Stross, "Singularity Sky"Greg Egan,
"Diaspora"Joe Haldeman, "Forever Peace"(There are surely many
others.)Incidentally, the nature of science fiction itself has changed.
Decades ago, science fiction projected great increases in the power humans
control (example: E.E. Smith Lensman series). That didn't really happen in the
last 50 years. Instead, we gradually refined the degree to whi</p><p>6 0.080994204 <a title="284-tfidf-6" href="../hunch_net-2006/hunch_net-2006-07-08-MaxEnt_contradicts_Bayes_Rule%3F.html">191 hunch net-2006-07-08-MaxEnt contradicts Bayes Rule?</a></p>
<p>7 0.073457666 <a title="284-tfidf-7" href="../hunch_net-2005/hunch_net-2005-04-21-Dynamic_Programming_Generalizations_and_Their_Use.html">58 hunch net-2005-04-21-Dynamic Programming Generalizations and Their Use</a></p>
<p>8 0.069058076 <a title="284-tfidf-8" href="../hunch_net-2007/hunch_net-2007-12-12-Workshop_Summary%26%238212%3BPrinciples_of_Learning_Problem_Design.html">277 hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</a></p>
<p>9 0.0 <a title="284-tfidf-9" href="../hunch_net-2005/hunch_net-2005-01-19-Why_I_decided_to_run_a_weblog..html">1 hunch net-2005-01-19-Why I decided to run a weblog.</a></p>
<p>10 0.0 <a title="284-tfidf-10" href="../hunch_net-2005/hunch_net-2005-01-24-Holy_grails_of_machine_learning%3F.html">2 hunch net-2005-01-24-Holy grails of machine learning?</a></p>
<p>11 0.0 <a title="284-tfidf-11" href="../hunch_net-2005/hunch_net-2005-01-24-The_Humanloop_Spectrum_of_Machine_Learning.html">3 hunch net-2005-01-24-The Humanloop Spectrum of Machine Learning</a></p>
<p>12 0.0 <a title="284-tfidf-12" href="../hunch_net-2005/hunch_net-2005-01-26-Summer_Schools.html">4 hunch net-2005-01-26-Summer Schools</a></p>
<p>13 0.0 <a title="284-tfidf-13" href="../hunch_net-2005/hunch_net-2005-01-26-Watchword%3A_Probability.html">5 hunch net-2005-01-26-Watchword: Probability</a></p>
<p>14 0.0 <a title="284-tfidf-14" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>15 0.0 <a title="284-tfidf-15" href="../hunch_net-2005/hunch_net-2005-01-31-Watchword%3A_Assumption.html">7 hunch net-2005-01-31-Watchword: Assumption</a></p>
<p>16 0.0 <a title="284-tfidf-16" href="../hunch_net-2005/hunch_net-2005-02-01-NIPS%3A_Online_Bayes.html">8 hunch net-2005-02-01-NIPS: Online Bayes</a></p>
<p>17 0.0 <a title="284-tfidf-17" href="../hunch_net-2005/hunch_net-2005-02-01-Watchword%3A_Loss.html">9 hunch net-2005-02-01-Watchword: Loss</a></p>
<p>18 0.0 <a title="284-tfidf-18" href="../hunch_net-2005/hunch_net-2005-02-02-Kolmogorov_Complexity_and_Googling.html">10 hunch net-2005-02-02-Kolmogorov Complexity and Googling</a></p>
<p>19 0.0 <a title="284-tfidf-19" href="../hunch_net-2005/hunch_net-2005-02-02-Paper_Deadlines.html">11 hunch net-2005-02-02-Paper Deadlines</a></p>
<p>20 0.0 <a title="284-tfidf-20" href="../hunch_net-2005/hunch_net-2005-02-03-Learning_Theory%2C_by_assumption.html">12 hunch net-2005-02-03-Learning Theory, by assumption</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.007), (1, 0.003), (2, 0.027), (3, -0.013), (4, 0.012), (5, -0.036), (6, 0.032), (7, -0.364), (8, 0.065), (9, 0.009), (10, 0.015), (11, -0.068), (12, 0.012), (13, -0.068), (14, 0.036), (15, 0.026), (16, -0.186), (17, 0.104), (18, 0.142), (19, -0.092), (20, -0.297), (21, -0.024), (22, 0.059), (23, -0.015), (24, 0.017), (25, -0.087), (26, -0.141), (27, 0.061), (28, -0.083), (29, 0.118), (30, 0.081), (31, 0.072), (32, -0.043), (33, 0.194), (34, -0.089), (35, 0.256), (36, -0.036), (37, 0.117), (38, 0.03), (39, 0.061), (40, -0.134), (41, 0.127), (42, 0.01), (43, 0.088), (44, -0.018), (45, -0.062), (46, -0.028), (47, -0.031), (48, 0.144), (49, -0.01)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000002 <a title="284-lsi-1" href="../hunch_net-2008/hunch_net-2008-01-18-Datasets.html">284 hunch net-2008-01-18-Datasets</a></p>
<p>Introduction: David Pennocknotes the impressiveset of datasetsatdatawrangling.</p><p>2 0.89968699 <a title="284-lsi-2" href="../hunch_net-2013/hunch_net-2013-07-10-Thoughts_on_Artificial_Intelligence.html">486 hunch net-2013-07-10-Thoughts on Artificial Intelligence</a></p>
<p>Introduction: David McAllesterstarts a blog.</p><p>3 0.37315461 <a title="284-lsi-3" href="../hunch_net-2006/hunch_net-2006-10-13-David_Pennock_starts_Oddhead.html">214 hunch net-2006-10-13-David Pennock starts Oddhead</a></p>
<p>Introduction: his blog on information markets and other research topics.</p><p>4 0.33021706 <a title="284-lsi-4" href="../hunch_net-2006/hunch_net-2006-07-08-MaxEnt_contradicts_Bayes_Rule%3F.html">191 hunch net-2006-07-08-MaxEnt contradicts Bayes Rule?</a></p>
<p>Introduction: A few weeks ago I readthis. David Blei and I spent some time thinking hard
about this a few years back (thanks to Kary Myers for pointing us to it):In
short I was thinking that Ã¢â‚¬Å“bayesian belief updatingÃ¢â‚¬Â and
Ã¢â‚¬Å“maximum entropyÃ¢â‚¬Â were two othogonal principles. But it appear
that they are not, and that they can even be in conflict !Example (from Kass
1996); consider a Die (6 sides), consider prior knowledge E[X]=3.5.Maximum
entropy leads to P(X)= (1/6, 1/6, 1/6, 1/6, 1/6, 1/6).Now consider a new piece
of evidence A=Ã¢â‚¬ÂX is an odd numberÃ¢â‚¬ÂBayesian posterior P(X|A)=
P(A|X) P(X) = (1/3, 0, 1/3, 0, 1/3, 0).But MaxEnt with the constraints
E[X]=3.5 and E[Indicator function of A]=1 leads to (.22, 0, .32, 0, .47, 0) !!
(note that E[Indicator function of A]=P(A))Indeed, for MaxEnt, because there
is no more Ã¢â‚¬Ëœ6Ã¢â‚¬Â², big numbers must be more probable to ensure an
average of 3.5. For bayesian updating, P(X|A) doesnÃ¢â‚¬â„¢t have to have a
3.5 expectation. P(X) a</p><p>5 0.31484622 <a title="284-lsi-5" href="../hunch_net-2010/hunch_net-2010-09-17-New_York_Area_Machine_Learning_Events.html">410 hunch net-2010-09-17-New York Area Machine Learning Events</a></p>
<p>Introduction: On Sept 21, there is anothermachine learning meetupwhere I'll be speaking.
Although the topic is contextual bandits, I think of it as "the future of
machine learning". In particular, it's all about how to learn in an
interactive environment, such as for ad display, trading, news recommendation,
etcâ&euro;ŚOn Sept 24, abstracts for theNew York Machine Learning Symposiumare due.
This is the largest Machine Learning event in the area, so it's a great way to
have a conversation with other people.On Oct 22, the NY ML Symposium actually
happens. This year, we are expanding the spotlights, and trying to have more
time for posters. In addition, we have a strong set of invited speakers:David
Blei,Sanjoy Dasgupta,Tommi Jaakkola, andYann LeCun. After the meeting, a
latehackNYrelated event is planned where students and startups can meet.I'd
also like to point out the relatedCS/Econ symposiumas I have interests there
as well.</p><p>6 0.27169195 <a title="284-lsi-6" href="../hunch_net-2009/hunch_net-2009-08-26-Another_10-year_paper_in_Machine_Learning.html">368 hunch net-2009-08-26-Another 10-year paper in Machine Learning</a></p>
<p>7 0.25166947 <a title="284-lsi-7" href="../hunch_net-2005/hunch_net-2005-04-21-Dynamic_Programming_Generalizations_and_Their_Use.html">58 hunch net-2005-04-21-Dynamic Programming Generalizations and Their Use</a></p>
<p>8 0.2220369 <a title="284-lsi-8" href="../hunch_net-2005/hunch_net-2005-07-21-Six_Months.html">96 hunch net-2005-07-21-Six Months</a></p>
<p>9 0.20350072 <a title="284-lsi-9" href="../hunch_net-2005/hunch_net-2005-04-28-Science_Fiction_and_Research.html">64 hunch net-2005-04-28-Science Fiction and Research</a></p>
<p>10 0.20223063 <a title="284-lsi-10" href="../hunch_net-2009/hunch_net-2009-12-09-Inherent_Uncertainty.html">383 hunch net-2009-12-09-Inherent Uncertainty</a></p>
<p>11 0.19368894 <a title="284-lsi-11" href="../hunch_net-2006/hunch_net-2006-06-05-Server_Shift%2C_Site_Tweaks%2C_Suggestions%3F.html">182 hunch net-2006-06-05-Server Shift, Site Tweaks, Suggestions?</a></p>
<p>12 0.17125525 <a title="284-lsi-12" href="../hunch_net-2007/hunch_net-2007-01-02-Retrospective.html">225 hunch net-2007-01-02-Retrospective</a></p>
<p>13 0.12207824 <a title="284-lsi-13" href="../hunch_net-2006/hunch_net-2006-06-30-ICML_papers.html">188 hunch net-2006-06-30-ICML papers</a></p>
<p>14 0.12206693 <a title="284-lsi-14" href="../hunch_net-2007/hunch_net-2007-12-12-Workshop_Summary%26%238212%3BPrinciples_of_Learning_Problem_Design.html">277 hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</a></p>
<p>15 0.12148142 <a title="284-lsi-15" href="../hunch_net-2009/hunch_net-2009-09-29-Machine_Learning_Protests_at_the_G20.html">372 hunch net-2009-09-29-Machine Learning Protests at the G20</a></p>
<p>16 0.11154497 <a title="284-lsi-16" href="../hunch_net-2013/hunch_net-2013-07-24-ICML_2012_videos_lost.html">487 hunch net-2013-07-24-ICML 2012 videos lost</a></p>
<p>17 0.10739981 <a title="284-lsi-17" href="../hunch_net-2009/hunch_net-2009-05-08-Computability_in_Artificial_Intelligence.html">353 hunch net-2009-05-08-Computability in Artificial Intelligence</a></p>
<p>18 0.10475723 <a title="284-lsi-18" href="../hunch_net-2007/hunch_net-2007-06-14-Interesting_Papers_at_COLT_2007.html">247 hunch net-2007-06-14-Interesting Papers at COLT 2007</a></p>
<p>19 0.10004697 <a title="284-lsi-19" href="../hunch_net-2008/hunch_net-2008-04-12-Blog_compromised.html">294 hunch net-2008-04-12-Blog compromised</a></p>
<p>20 0.096040145 <a title="284-lsi-20" href="../hunch_net-2007/hunch_net-2007-05-09-The_Missing_Bound.html">244 hunch net-2007-05-09-The Missing Bound</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(94, 0.505)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="284-lda-1" href="../hunch_net-2008/hunch_net-2008-01-18-Datasets.html">284 hunch net-2008-01-18-Datasets</a></p>
<p>Introduction: David Pennocknotes the impressiveset of datasetsatdatawrangling.</p><p>2 0.39276102 <a title="284-lda-2" href="../hunch_net-2009/hunch_net-2009-11-29-AI_Safety.html">380 hunch net-2009-11-29-AI Safety</a></p>
<p>Introduction: Dan Reevesintroduced me toMichael Vassarwho ran theSingularity Summitand
educated me a bit on the subject of AI safety which theSingularity
Institutehassmall grants for.I still believe thatinterstellar space travel is
necessary for long term civilization survival, and the AI is necessary for
interstellar space travel. On these grounds alone, we could judge that
developing AI is much more safe than not. Nevertheless, there is a basic
reasonable fear, as expressed by some commenters, that AI could go bad.A basic
scenario starts with someone inventing an AI and telling it to make as much
money as possible. The AI promptly starts trading in various markets to make
money. To improve, it crafts a virus that takes over most of the world's
computers using it as a surveillance network so that it can always make the
right decision. The AI also branches out into any form of distance work,
taking over the entire outsourcing process for all jobs that are entirely
digital. To further improve, the AI</p><p>3 0.33771992 <a title="284-lda-3" href="../hunch_net-2005/hunch_net-2005-02-20-At_One_Month.html">25 hunch net-2005-02-20-At One Month</a></p>
<p>Introduction: This is near the one month point, so it seems appropriate to consider meta-
issues for the moment.The number of posts is a bit over 20.The number of
people speaking up in discussions is about 10.The number of people viewing the
site is somewhat more than 100.I am (naturally) dissatisfied with many
things.Many of thepotential useshaven't been realized. This is partly a matter
of opportunity (no conferences in the last month), partly a matter of will (no
open problems because it's hard to give them up), and partly a matter of
tradition. In academia, there is a strong tradition of trying to get
everything perfectly right before presentation. This is somewhat contradictory
to the nature of making many posts, and it's definitely contradictory to the
idea of doing "public research". If that sort of idea is to pay off, it must
be significantly more succesful than previous methods. In an effort to
continue experimenting, I'm going to use the next week as "open problems
week".Spam is a problem.</p><p>4 0.2445545 <a title="284-lda-4" href="../hunch_net-2005/hunch_net-2005-02-28-Regularization.html">33 hunch net-2005-02-28-Regularization</a></p>
<p>Introduction: Yaroslav Bulatovsays that we should think about regularization a bit. It's a
complex topic which I only partially understand, so I'll try to explain from a
couple viewpoints.Functionally. Regularization is optimizing some
representation to fit the dataandminimize some notion of predictor complexity.
This notion of complexity is often the l1or l2norm on a set of parameters, but
the term can be used much more generally. Empirically, this often works much
better than simply fitting the data.Statistical Learning
ViewpointRegularization is about the failiure of statistical learning to
adequately predict generalization error. Lete(c,D)be the expected error rate
with respect toDof classifiercande(c,S)the observed error rate on a sampleS.
There are numerous bounds of the form: assuming i.i.d. samples, with high
probability over the drawn samplesS,e(c,D) less than e(c,S) +
f(complexity)wherecomplexityis some measure of the size of a set of functions.
Unfortunately, we have never convincingly na</p><p>5 0.0 <a title="284-lda-5" href="../hunch_net-2005/hunch_net-2005-01-19-Why_I_decided_to_run_a_weblog..html">1 hunch net-2005-01-19-Why I decided to run a weblog.</a></p>
<p>Introduction: I have decided to run a weblog on machine learning and learning theory
research. Here are some reasons:1) Weblogs enable new functionality:Public
comment on papers. No mechanism for this exists at conferences and most
journals. I have encountered it once for asciencepaper. Some communities have
mailing lists supporting this, but not machine learning or learning theory. I
have often read papers and found myself wishing there was some method to
consider other's questions and read the replies.Conference shortlists. One of
the most common conversations at a conference is "what did you find
interesting?" There is no explicit mechanism for sharing this information at
conferences, and it's easy to imagine that it would be handy to do
so.Evaluation and comment on research directions. Papers are almost
exclusively about new research, rather than evaluation (and consideration) of
research directions. This last role is satisfied by funding agencies to some
extent, but that is a private debate of</p><p>6 0.0 <a title="284-lda-6" href="../hunch_net-2005/hunch_net-2005-01-24-Holy_grails_of_machine_learning%3F.html">2 hunch net-2005-01-24-Holy grails of machine learning?</a></p>
<p>7 0.0 <a title="284-lda-7" href="../hunch_net-2005/hunch_net-2005-01-24-The_Humanloop_Spectrum_of_Machine_Learning.html">3 hunch net-2005-01-24-The Humanloop Spectrum of Machine Learning</a></p>
<p>8 0.0 <a title="284-lda-8" href="../hunch_net-2005/hunch_net-2005-01-26-Summer_Schools.html">4 hunch net-2005-01-26-Summer Schools</a></p>
<p>9 0.0 <a title="284-lda-9" href="../hunch_net-2005/hunch_net-2005-01-26-Watchword%3A_Probability.html">5 hunch net-2005-01-26-Watchword: Probability</a></p>
<p>10 0.0 <a title="284-lda-10" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>11 0.0 <a title="284-lda-11" href="../hunch_net-2005/hunch_net-2005-01-31-Watchword%3A_Assumption.html">7 hunch net-2005-01-31-Watchword: Assumption</a></p>
<p>12 0.0 <a title="284-lda-12" href="../hunch_net-2005/hunch_net-2005-02-01-NIPS%3A_Online_Bayes.html">8 hunch net-2005-02-01-NIPS: Online Bayes</a></p>
<p>13 0.0 <a title="284-lda-13" href="../hunch_net-2005/hunch_net-2005-02-01-Watchword%3A_Loss.html">9 hunch net-2005-02-01-Watchword: Loss</a></p>
<p>14 0.0 <a title="284-lda-14" href="../hunch_net-2005/hunch_net-2005-02-02-Kolmogorov_Complexity_and_Googling.html">10 hunch net-2005-02-02-Kolmogorov Complexity and Googling</a></p>
<p>15 0.0 <a title="284-lda-15" href="../hunch_net-2005/hunch_net-2005-02-02-Paper_Deadlines.html">11 hunch net-2005-02-02-Paper Deadlines</a></p>
<p>16 0.0 <a title="284-lda-16" href="../hunch_net-2005/hunch_net-2005-02-03-Learning_Theory%2C_by_assumption.html">12 hunch net-2005-02-03-Learning Theory, by assumption</a></p>
<p>17 0.0 <a title="284-lda-17" href="../hunch_net-2005/hunch_net-2005-02-04-JMLG.html">13 hunch net-2005-02-04-JMLG</a></p>
<p>18 0.0 <a title="284-lda-18" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>19 0.0 <a title="284-lda-19" href="../hunch_net-2005/hunch_net-2005-02-08-Some_Links.html">15 hunch net-2005-02-08-Some Links</a></p>
<p>20 0.0 <a title="284-lda-20" href="../hunch_net-2005/hunch_net-2005-02-09-Intuitions_from_applied_learning.html">16 hunch net-2005-02-09-Intuitions from applied learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
