<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>295 hunch net-2008-04-12-It Doesn&#8217;t Stop</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2008" href="../home/hunch_net-2008_home.html">hunch_net-2008</a> <a title="hunch_net-2008-295" href="#">hunch_net-2008-295</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>295 hunch net-2008-04-12-It Doesn&#8217;t Stop</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2008-295-html" href="http://hunch.net/?p=324">html</a></p><p>Introduction: I've enjoyed theTerminatormovies and show. Neglecting the whacky aspects (time
travel and associated paradoxes), there is an enduring topic of discussion:
how do people deal with intelligent machines (and vice versa)?In Terminator-
land, the primary method for dealing with intelligent machines is to prevent
them from being made. This approach works pretty badly, because a new angle on
building an intelligent machine keeps coming up. This is partly a ploy for
writer's to avoid writing themselves out of a job, but there is a fundamental
truth to it as well: preventing progress in research is hard.The United
States, has been experimenting with trying to stop research onstem cells. It
hasn't worked very well--the net effect has been retarding research programs a
bit, and exporting some research to other countries. Another less recent
example was encryption technology, for which the United States generally did
not encourage early public research and evendiscouraged as a munition. This
slowe</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('civilization', 0.269), ('star', 0.245), ('travel', 0.205), ('accident', 0.163), ('doomsday', 0.163), ('malevolence', 0.163), ('intelligent', 0.154), ('scenarios', 0.145), ('encryption', 0.145), ('ai', 0.135), ('stop', 0.134), ('preventing', 0.134), ('research', 0.131), ('land', 0.116), ('united', 0.112), ('society', 0.109), ('support', 0.103), ('manage', 0.096), ('question', 0.095), ('fundamental', 0.093)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999982 <a title="295-tfidf-1" href="../hunch_net-2008/hunch_net-2008-04-12-It_Doesn%26%238217%3Bt_Stop.html">295 hunch net-2008-04-12-It Doesn&#8217;t Stop</a></p>
<p>Introduction: I've enjoyed theTerminatormovies and show. Neglecting the whacky aspects (time
travel and associated paradoxes), there is an enduring topic of discussion:
how do people deal with intelligent machines (and vice versa)?In Terminator-
land, the primary method for dealing with intelligent machines is to prevent
them from being made. This approach works pretty badly, because a new angle on
building an intelligent machine keeps coming up. This is partly a ploy for
writer's to avoid writing themselves out of a job, but there is a fundamental
truth to it as well: preventing progress in research is hard.The United
States, has been experimenting with trying to stop research onstem cells. It
hasn't worked very well--the net effect has been retarding research programs a
bit, and exporting some research to other countries. Another less recent
example was encryption technology, for which the United States generally did
not encourage early public research and evendiscouraged as a munition. This
slowe</p><p>2 0.18111922 <a title="295-tfidf-2" href="../hunch_net-2009/hunch_net-2009-11-29-AI_Safety.html">380 hunch net-2009-11-29-AI Safety</a></p>
<p>Introduction: Dan Reevesintroduced me toMichael Vassarwho ran theSingularity Summitand
educated me a bit on the subject of AI safety which theSingularity
Institutehassmall grants for.I still believe thatinterstellar space travel is
necessary for long term civilization survival, and the AI is necessary for
interstellar space travel. On these grounds alone, we could judge that
developing AI is much more safe than not. Nevertheless, there is a basic
reasonable fear, as expressed by some commenters, that AI could go bad.A basic
scenario starts with someone inventing an AI and telling it to make as much
money as possible. The AI promptly starts trading in various markets to make
money. To improve, it crafts a virus that takes over most of the world's
computers using it as a surveillance network so that it can always make the
right decision. The AI also branches out into any form of distance work,
taking over the entire outsourcing process for all jobs that are entirely
digital. To further improve, the AI</p><p>3 0.15921064 <a title="295-tfidf-3" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<p>Introduction: I recently had fun discussions with bothVikash MansinghkaandThomas Breuelabout
approaching AI with machine learning. The general interest in taking a crack
at AI with machine learning seems to be rising on many fronts
includingDARPA.As a matter of history, there was a great deal of interest in
AI which died down before I began research. There remain many projects and
conferences spawned in this earlier AI wave, as well as a good bit of
experience about what did not work, or at least did not work yet. Here are a
few examples of failure modes that people seem to run into:Supply/Product
confusion. Sometimes we think "Intelligences use X, so I'll create X and have
an Intelligence." An example of this is theCyc Projectwhich inspires some
people as "intelligences use ontologies, so I'll create an ontology and a
system using it to have an Intelligence." The flaw here is that
Intelligencescreateontologies, which they use, and without the ability to
create ontologies you don't have an Intellige</p><p>4 0.14069322 <a title="295-tfidf-4" href="../hunch_net-2009/hunch_net-2009-05-08-Computability_in_Artificial_Intelligence.html">353 hunch net-2009-05-08-Computability in Artificial Intelligence</a></p>
<p>Introduction: Normally I do not blog, but John kindly invited me to do so. Since
computability issues play a major role in Artificial Intelligence and Machine
Learning, I would like to take the opportunity to comment on that and raise
some questions.The general attitude is that AI is about finding efficient
smart algorithms. For large parts of machine learning, the same attitude is
not too dangerous. If you want to concentrate on conceptual problems, simply
become a statistician. There is no analogous escape for modern research on AI
(as opposed toGOFAIrooted in logic).Let me show by analogy whylimiting
research to computational questions is bad for any field.Except in computer
science, computational aspects play little role in the development
offundamentaltheories: Consider e.g. set theory with axiom of choice,
foundations of logic, exact/full minimax for zero-sum games, quantum (field)
theory, string theory, â&euro;Ś Indeed, at least in physics, every new fundamental
theory seems to be less computable th</p><p>5 0.13360222 <a title="295-tfidf-5" href="../hunch_net-2009/hunch_net-2009-02-22-Effective_Research_Funding.html">344 hunch net-2009-02-22-Effective Research Funding</a></p>
<p>Introduction: With a worldwide recession on, my impression is that the carnage in research
has not been as severe as might be feared, at least in the United States. I
know of two notable negative impacts:It's quite difficult to get a job this
year, as many companies and universities simply aren't hiring. This is
particularly tough on graduating students.Perhaps 10% ofIBM researchwas
fired.In contrast, around the time of the dot com bust,ATnT
ResearchandLucenthad one or several 50% size firings wiping out much of the
remainder ofBell Labs, triggering a notable diaspora for the respected machine
learning group there. As the recession progresses, we may easily see more
firings as companies in particular reach a point where they can no longer
support research.There are a couple positives to the recession as well.Both
the implosion of Wall Street (which siphoned off smart people) and the general
difficulty of getting a job coming out of an undergraduate education suggest
that the quality of admitted phd</p><p>6 0.12971254 <a title="295-tfidf-6" href="../hunch_net-2008/hunch_net-2008-11-04-Rise_of_the_Machines.html">323 hunch net-2008-11-04-Rise of the Machines</a></p>
<p>7 0.12554316 <a title="295-tfidf-7" href="../hunch_net-2009/hunch_net-2009-08-03-Carbon_in_Computer_Science_Research.html">366 hunch net-2009-08-03-Carbon in Computer Science Research</a></p>
<p>8 0.11934866 <a title="295-tfidf-8" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>9 0.11523438 <a title="295-tfidf-9" href="../hunch_net-2011/hunch_net-2011-02-17-What_does_Watson_mean%3F.html">424 hunch net-2011-02-17-What does Watson mean?</a></p>
<p>10 0.11163892 <a title="295-tfidf-10" href="../hunch_net-2008/hunch_net-2008-01-06-Research_Political_Issues.html">282 hunch net-2008-01-06-Research Political Issues</a></p>
<p>11 0.11013076 <a title="295-tfidf-11" href="../hunch_net-2010/hunch_net-2010-08-24-Alex_Smola_starts_a_blog.html">408 hunch net-2010-08-24-Alex Smola starts a blog</a></p>
<p>12 0.10015851 <a title="295-tfidf-12" href="../hunch_net-2005/hunch_net-2005-03-05-Funding_Research.html">36 hunch net-2005-03-05-Funding Research</a></p>
<p>13 0.093667433 <a title="295-tfidf-13" href="../hunch_net-2009/hunch_net-2009-06-01-Multitask_Poisoning.html">358 hunch net-2009-06-01-Multitask Poisoning</a></p>
<p>14 0.092614308 <a title="295-tfidf-14" href="../hunch_net-2005/hunch_net-2005-09-10-%26%238220%3BFailure%26%238221%3B_is_an_option.html">110 hunch net-2005-09-10-&#8220;Failure&#8221; is an option</a></p>
<p>15 0.092103846 <a title="295-tfidf-15" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>16 0.091847271 <a title="295-tfidf-16" href="../hunch_net-2011/hunch_net-2011-11-26-Giving_Thanks.html">449 hunch net-2011-11-26-Giving Thanks</a></p>
<p>17 0.091800548 <a title="295-tfidf-17" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>18 0.089986667 <a title="295-tfidf-18" href="../hunch_net-2005/hunch_net-2005-04-28-Science_Fiction_and_Research.html">64 hunch net-2005-04-28-Science Fiction and Research</a></p>
<p>19 0.089880273 <a title="295-tfidf-19" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>20 0.087907426 <a title="295-tfidf-20" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.221), (1, 0.035), (2, 0.131), (3, -0.128), (4, 0.075), (5, 0.011), (6, -0.048), (7, 0.028), (8, -0.003), (9, 0.027), (10, 0.022), (11, 0.02), (12, 0.015), (13, -0.058), (14, 0.091), (15, 0.045), (16, 0.008), (17, 0.007), (18, 0.018), (19, -0.062), (20, -0.049), (21, 0.056), (22, -0.126), (23, 0.091), (24, 0.081), (25, -0.096), (26, 0.121), (27, -0.027), (28, 0.078), (29, 0.044), (30, 0.044), (31, 0.083), (32, 0.097), (33, 0.036), (34, 0.058), (35, 0.01), (36, 0.002), (37, 0.014), (38, -0.044), (39, 0.034), (40, -0.017), (41, 0.001), (42, 0.058), (43, -0.065), (44, -0.0), (45, -0.023), (46, 0.076), (47, -0.013), (48, -0.033), (49, -0.011)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95092231 <a title="295-lsi-1" href="../hunch_net-2008/hunch_net-2008-04-12-It_Doesn%26%238217%3Bt_Stop.html">295 hunch net-2008-04-12-It Doesn&#8217;t Stop</a></p>
<p>Introduction: I've enjoyed theTerminatormovies and show. Neglecting the whacky aspects (time
travel and associated paradoxes), there is an enduring topic of discussion:
how do people deal with intelligent machines (and vice versa)?In Terminator-
land, the primary method for dealing with intelligent machines is to prevent
them from being made. This approach works pretty badly, because a new angle on
building an intelligent machine keeps coming up. This is partly a ploy for
writer's to avoid writing themselves out of a job, but there is a fundamental
truth to it as well: preventing progress in research is hard.The United
States, has been experimenting with trying to stop research onstem cells. It
hasn't worked very well--the net effect has been retarding research programs a
bit, and exporting some research to other countries. Another less recent
example was encryption technology, for which the United States generally did
not encourage early public research and evendiscouraged as a munition. This
slowe</p><p>2 0.8490743 <a title="295-lsi-2" href="../hunch_net-2009/hunch_net-2009-11-29-AI_Safety.html">380 hunch net-2009-11-29-AI Safety</a></p>
<p>Introduction: Dan Reevesintroduced me toMichael Vassarwho ran theSingularity Summitand
educated me a bit on the subject of AI safety which theSingularity
Institutehassmall grants for.I still believe thatinterstellar space travel is
necessary for long term civilization survival, and the AI is necessary for
interstellar space travel. On these grounds alone, we could judge that
developing AI is much more safe than not. Nevertheless, there is a basic
reasonable fear, as expressed by some commenters, that AI could go bad.A basic
scenario starts with someone inventing an AI and telling it to make as much
money as possible. The AI promptly starts trading in various markets to make
money. To improve, it crafts a virus that takes over most of the world's
computers using it as a surveillance network so that it can always make the
right decision. The AI also branches out into any form of distance work,
taking over the entire outsourcing process for all jobs that are entirely
digital. To further improve, the AI</p><p>3 0.7861414 <a title="295-lsi-3" href="../hunch_net-2009/hunch_net-2009-05-08-Computability_in_Artificial_Intelligence.html">353 hunch net-2009-05-08-Computability in Artificial Intelligence</a></p>
<p>Introduction: Normally I do not blog, but John kindly invited me to do so. Since
computability issues play a major role in Artificial Intelligence and Machine
Learning, I would like to take the opportunity to comment on that and raise
some questions.The general attitude is that AI is about finding efficient
smart algorithms. For large parts of machine learning, the same attitude is
not too dangerous. If you want to concentrate on conceptual problems, simply
become a statistician. There is no analogous escape for modern research on AI
(as opposed toGOFAIrooted in logic).Let me show by analogy whylimiting
research to computational questions is bad for any field.Except in computer
science, computational aspects play little role in the development
offundamentaltheories: Consider e.g. set theory with axiom of choice,
foundations of logic, exact/full minimax for zero-sum games, quantum (field)
theory, string theory, â&euro;Ś Indeed, at least in physics, every new fundamental
theory seems to be less computable th</p><p>4 0.74779403 <a title="295-lsi-4" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<p>Introduction: I recently had fun discussions with bothVikash MansinghkaandThomas Breuelabout
approaching AI with machine learning. The general interest in taking a crack
at AI with machine learning seems to be rising on many fronts
includingDARPA.As a matter of history, there was a great deal of interest in
AI which died down before I began research. There remain many projects and
conferences spawned in this earlier AI wave, as well as a good bit of
experience about what did not work, or at least did not work yet. Here are a
few examples of failure modes that people seem to run into:Supply/Product
confusion. Sometimes we think "Intelligences use X, so I'll create X and have
an Intelligence." An example of this is theCyc Projectwhich inspires some
people as "intelligences use ontologies, so I'll create an ontology and a
system using it to have an Intelligence." The flaw here is that
Intelligencescreateontologies, which they use, and without the ability to
create ontologies you don't have an Intellige</p><p>5 0.72628057 <a title="295-lsi-5" href="../hunch_net-2008/hunch_net-2008-01-28-Sufficient_Computation.html">287 hunch net-2008-01-28-Sufficient Computation</a></p>
<p>Introduction: Do we have computer hardware sufficient for AI? This question is difficult to
answer, but here's a try:One way to achieve AI is by simulating a human brain.
A human brain has about 1015synapses which operate at about 102per second
implying about 1017bit ops per second.A modern computer runs at
109cycles/second and operates on 102bits per cycle implying 1011bits processed
per second.The gap here is only 6 orders of magnitude, which can be plausibly
surpassed via cluster machines. For example, theBlueGene/Loperates 105nodes
(one order of magnitude short). It's peak recorded performance is about
0.5*1015FLOPS which translates to about 1016bit ops per second, which is
nearly 1017.There are many criticisms (both positive and negative) for this
argument.Simulation of a human brain might require substantially more detail.
Perhaps an additional 102is required per neuron.We may not need to simulate a
human brain to achieve AI. There are certainly many examples where we have
been able to design</p><p>6 0.64328033 <a title="295-lsi-6" href="../hunch_net-2009/hunch_net-2009-08-03-Carbon_in_Computer_Science_Research.html">366 hunch net-2009-08-03-Carbon in Computer Science Research</a></p>
<p>7 0.60424769 <a title="295-lsi-7" href="../hunch_net-2006/hunch_net-2006-02-02-Introspectionism_as_a_Disease.html">153 hunch net-2006-02-02-Introspectionism as a Disease</a></p>
<p>8 0.60280389 <a title="295-lsi-8" href="../hunch_net-2007/hunch_net-2007-04-28-The_Coming_Patent_Apocalypse.html">241 hunch net-2007-04-28-The Coming Patent Apocalypse</a></p>
<p>9 0.57796413 <a title="295-lsi-9" href="../hunch_net-2011/hunch_net-2011-02-17-What_does_Watson_mean%3F.html">424 hunch net-2011-02-17-What does Watson mean?</a></p>
<p>10 0.57550186 <a title="295-lsi-10" href="../hunch_net-2008/hunch_net-2008-01-06-Research_Political_Issues.html">282 hunch net-2008-01-06-Research Political Issues</a></p>
<p>11 0.56968766 <a title="295-lsi-11" href="../hunch_net-2005/hunch_net-2005-05-29-Bad_ideas.html">76 hunch net-2005-05-29-Bad ideas</a></p>
<p>12 0.54793406 <a title="295-lsi-12" href="../hunch_net-2005/hunch_net-2005-03-10-Breaking_Abstractions.html">39 hunch net-2005-03-10-Breaking Abstractions</a></p>
<p>13 0.54391479 <a title="295-lsi-13" href="../hunch_net-2009/hunch_net-2009-02-22-Effective_Research_Funding.html">344 hunch net-2009-02-22-Effective Research Funding</a></p>
<p>14 0.53335679 <a title="295-lsi-14" href="../hunch_net-2008/hunch_net-2008-07-02-Proprietary_Data_in_Academic_Research%3F.html">306 hunch net-2008-07-02-Proprietary Data in Academic Research?</a></p>
<p>15 0.5328055 <a title="295-lsi-15" href="../hunch_net-2009/hunch_net-2009-06-01-Multitask_Poisoning.html">358 hunch net-2009-06-01-Multitask Poisoning</a></p>
<p>16 0.53130102 <a title="295-lsi-16" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>17 0.53095883 <a title="295-lsi-17" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>18 0.52998197 <a title="295-lsi-18" href="../hunch_net-2007/hunch_net-2007-07-13-The_View_From_China.html">255 hunch net-2007-07-13-The View From China</a></p>
<p>19 0.50089318 <a title="295-lsi-19" href="../hunch_net-2006/hunch_net-2006-12-05-Recruitment_Conferences.html">222 hunch net-2006-12-05-Recruitment Conferences</a></p>
<p>20 0.498914 <a title="295-lsi-20" href="../hunch_net-2005/hunch_net-2005-09-04-Science_in_the_Government.html">106 hunch net-2005-09-04-Science in the Government</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(35, 0.024), (42, 0.229), (45, 0.048), (48, 0.012), (68, 0.411), (69, 0.031), (74, 0.095), (76, 0.019), (83, 0.021), (95, 0.027)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.99161446 <a title="295-lda-1" href="../hunch_net-2005/hunch_net-2005-02-09-Intuitions_from_applied_learning.html">16 hunch net-2005-02-09-Intuitions from applied learning</a></p>
<p>Introduction: Since learning is far from an exact science, it's good to pay attention to
basic intuitions of applied learning. Here are a few I've
collected.IntegrationIn Bayesian learning, the posterior is computed by an
integral, and the optimal thing to do is to predict according to this
integral. This phenomena seems to be far more general. Bagging, Boosting,
SVMs, and Neural Networks all take advantage of this idea to some extent. The
phenomena is more general: you can average over many differentclassification
predictorsto improve performance.
Sources:Zoubin,CaruanaDifferentiationDifferent pieces of an average should
differentiate to achieve good performance by different methods. This is know
as the 'symmetry breaking' problem for neural networks, and it's why weights
are initialized randomly. Boosting explicitly attempts to achieve good
differentiation by creating new, different, learning problems. Sources:Yann
LeCun,Phil LongDeep RepresentationHaving a deep representation is necessary
for hav</p><p>2 0.96572798 <a title="295-lda-2" href="../hunch_net-2005/hunch_net-2005-06-10-Workshops_are_not_Conferences.html">80 hunch net-2005-06-10-Workshops are not Conferences</a></p>
<p>Introduction: â&euro;Ś and you should use that fact.A workshop differs from a conference in that it
is about a focused group of people worrying about a focused topic. It also
differs in that a workshop is typically a "one-time affair" rather than a
series. (TheSnowbird learning workshopcounts as a conference in this
respect.)A common failure mode of both organizers and speakers at a workshop
is to treat it as a conference. This is "ok", but it is not really taking
advantage of the situation. Here are some things I've learned:For speakers: A
smaller audience means it can be more interactive. Interactive means a better
chance to avoid losing your audience and a more interesting presentation
(because you can adapt to your audience). Greater focus amongst the
participants means you can get to the heart of the matter more easily, and
discuss tradeoffs more carefully. Unlike conferences, relevance is more valued
than newness.For organizers: Not everything needs to be in a conference style
presentation format (i.</p><p>3 0.96459275 <a title="295-lda-3" href="../hunch_net-2009/hunch_net-2009-02-04-Optimal_Proxy_Loss_for_Classification.html">341 hunch net-2009-02-04-Optimal Proxy Loss for Classification</a></p>
<p>Introduction: Many people in machine learning take advantage of the notion of a proxy loss:
A loss function which is much easier to optimize computationally than the loss
function imposed by the world. A canonical example is when we want to learn a
weight vectorwand predict according to a dot productfw(x)= sumiwixiwhere
optimizing squared loss(y-fw(x))2over many samples is much more tractable than
optimizing 0-1 lossI(y = Threshold(fw(x) - 0.5)).While the computational
advantages of optimizing a proxy loss are substantial, we are curious: which
proxy loss is best? The answer of course depends on what the real loss imposed
by the world is. For 0-1 loss classification, there are adherents to many
choices:Log loss. If we confine the prediction to[0,1], we can treat it as a
predicted probability that the label is1, and measure loss according tolog
1/p'(y|x)wherep'(y|x)is the predicted probability of the observed label. A
standard method for confining the prediction to[0,1]islogistic regressionwhich
expo</p><p>4 0.96332401 <a title="295-lda-4" href="../hunch_net-2009/hunch_net-2009-01-21-Nearly_all_natural_problems_require_nonlinearity.html">337 hunch net-2009-01-21-Nearly all natural problems require nonlinearity</a></p>
<p>Introduction: One conventional wisdom is that learning algorithms with linear
representations are sufficient to solve natural learning problems. This
conventional wisdom appears unsupported by empirical evidence as far as I can
tell. In nearly all vision, language, robotics, and speech applications I know
where machine learning is effectively applied, the approach involves either a
linear representation on hand crafted features capturing substantial
nonlinearities or learning directly on nonlinear representations.There are a
few exceptions to this--for example, if the problem of interest to you is
predicting the next word given previous words, n-gram methods have been shown
effective. Viewed the right way, n-gram methods are essentially linear
predictors on an enormous sparse feature space, learned from an enormous
number of examples. Hal's postheredescribes some of this in more detail.In
contrast, if you go to a machine learning conference, a large number of the
new algorithms are variations of lea</p><p>5 0.96292764 <a title="295-lda-5" href="../hunch_net-2012/hunch_net-2012-03-13-The_Submodularity_workshop_and_Lucca_Professorship.html">459 hunch net-2012-03-13-The Submodularity workshop and Lucca Professorship</a></p>
<p>Introduction: Ninapoints out theSubmodularity WorkshopMarch 19-20next week atGeorgia Tech.
Many people want to make Submodularity the new Convexity in machine learning,
and it certainly seems worth exploring.Sara Olsonalso points out atenured
faculty positionatIMT Luccawith a deadline ofMay 15th. Lucca happens to be the
ancestral home of 1/4 of my heritage</p><p>6 0.9619332 <a title="295-lda-6" href="../hunch_net-2006/hunch_net-2006-07-08-Some_recent_papers.html">192 hunch net-2006-07-08-Some recent papers</a></p>
<p>7 0.93828064 <a title="295-lda-7" href="../hunch_net-2012/hunch_net-2012-03-24-David_Waltz.html">460 hunch net-2012-03-24-David Waltz</a></p>
<p>same-blog 8 0.92361188 <a title="295-lda-8" href="../hunch_net-2008/hunch_net-2008-04-12-It_Doesn%26%238217%3Bt_Stop.html">295 hunch net-2008-04-12-It Doesn&#8217;t Stop</a></p>
<p>9 0.87845671 <a title="295-lda-9" href="../hunch_net-2005/hunch_net-2005-08-22-Do_you_believe_in_induction%3F.html">104 hunch net-2005-08-22-Do you believe in induction?</a></p>
<p>10 0.85288739 <a title="295-lda-10" href="../hunch_net-2007/hunch_net-2007-05-12-Loss_Function_Semantics.html">245 hunch net-2007-05-12-Loss Function Semantics</a></p>
<p>11 0.82105178 <a title="295-lda-11" href="../hunch_net-2005/hunch_net-2005-09-10-%26%238220%3BFailure%26%238221%3B_is_an_option.html">110 hunch net-2005-09-10-&#8220;Failure&#8221; is an option</a></p>
<p>12 0.81784958 <a title="295-lda-12" href="../hunch_net-2011/hunch_net-2011-07-11-Interesting_Neural_Network_Papers_at_ICML_2011.html">438 hunch net-2011-07-11-Interesting Neural Network Papers at ICML 2011</a></p>
<p>13 0.80796528 <a title="295-lda-13" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>14 0.80415994 <a title="295-lda-14" href="../hunch_net-2007/hunch_net-2007-08-12-Exponentiated_Gradient.html">258 hunch net-2007-08-12-Exponentiated Gradient</a></p>
<p>15 0.77812898 <a title="295-lda-15" href="../hunch_net-2007/hunch_net-2007-11-28-Computational_Consequences_of_Classification.html">274 hunch net-2007-11-28-Computational Consequences of Classification</a></p>
<p>16 0.77672285 <a title="295-lda-16" href="../hunch_net-2005/hunch_net-2005-02-01-Watchword%3A_Loss.html">9 hunch net-2005-02-01-Watchword: Loss</a></p>
<p>17 0.76557583 <a title="295-lda-17" href="../hunch_net-2005/hunch_net-2005-11-07-Prediction_Competitions.html">129 hunch net-2005-11-07-Prediction Competitions</a></p>
<p>18 0.75362235 <a title="295-lda-18" href="../hunch_net-2005/hunch_net-2005-06-08-Question%3A_%26%238220%3BWhen_is_the_right_time_to_insert_the_loss_function%3F%26%238221%3B.html">79 hunch net-2005-06-08-Question: &#8220;When is the right time to insert the loss function?&#8221;</a></p>
<p>19 0.71790975 <a title="295-lda-19" href="../hunch_net-2005/hunch_net-2005-08-11-Why_Manifold-Based_Dimension_Reduction_Techniques%3F.html">102 hunch net-2005-08-11-Why Manifold-Based Dimension Reduction Techniques?</a></p>
<p>20 0.71731073 <a title="295-lda-20" href="../hunch_net-2010/hunch_net-2010-08-23-Boosted_Decision_Trees_for_Deep_Learning.html">407 hunch net-2010-08-23-Boosted Decision Trees for Deep Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
