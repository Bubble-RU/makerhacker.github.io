<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>328 hunch net-2008-11-26-Efficient Reinforcement Learning in MDPs</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2008" href="../home/hunch_net-2008_home.html">hunch_net-2008</a> <a title="hunch_net-2008-328" href="#">hunch_net-2008-328</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>328 hunch net-2008-11-26-Efficient Reinforcement Learning in MDPs</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2008-328-html" href="http://hunch.net/?p=472">html</a></p><p>Introduction: Claude Sammut  is attempting to put together an  Encyclopedia of Machine Learning .  I volunteered to write one article on  Efficient RL in MDPs , which I would like to invite comment on.  Is something critical missing?</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Claude Sammut  is attempting to put together an  Encyclopedia of Machine Learning . [sent-1, score-0.76]
</p><p>2 I volunteered to write one article on  Efficient RL in MDPs , which I would like to invite comment on. [sent-2, score-1.617]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mdps', 0.398), ('volunteered', 0.398), ('attempting', 0.319), ('invite', 0.29), ('article', 0.263), ('rl', 0.258), ('write', 0.228), ('put', 0.225), ('missing', 0.222), ('comment', 0.216), ('together', 0.216), ('critical', 0.208), ('efficient', 0.191), ('something', 0.122), ('would', 0.099), ('like', 0.078), ('machine', 0.058), ('one', 0.045), ('learning', 0.024)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="328-tfidf-1" href="../hunch_net-2008/hunch_net-2008-11-26-Efficient_Reinforcement_Learning_in_MDPs.html">328 hunch net-2008-11-26-Efficient Reinforcement Learning in MDPs</a></p>
<p>Introduction: Claude Sammut  is attempting to put together an  Encyclopedia of Machine Learning .  I volunteered to write one article on  Efficient RL in MDPs , which I would like to invite comment on.  Is something critical missing?</p><p>2 0.10725865 <a title="328-tfidf-2" href="../hunch_net-2010/hunch_net-2010-09-28-Machined_Learnings.html">412 hunch net-2010-09-28-Machined Learnings</a></p>
<p>Introduction: Paul Mineiro  has started  Machined Learnings  where heâ&euro;&trade;s seriously attempting to do ML research in public.  I personally need to read through in greater detail, as much of it is learning reduction related, trying to deal with the sorts of complex source problems that come up in practice.</p><p>3 0.087474056 <a title="328-tfidf-3" href="../hunch_net-2005/hunch_net-2005-10-13-Site_tweak.html">122 hunch net-2005-10-13-Site tweak</a></p>
<p>Introduction: Several people have had difficulty with comments which seem to have an allowed language significantly poorer than posts.  The set of allowed html tags has been increased and the  markdown filter  has been put in place to try to make commenting easier.  Iâ&euro;&trade;ll put some examples into the comments of this post.</p><p>4 0.077635065 <a title="328-tfidf-4" href="../hunch_net-2006/hunch_net-2006-07-09-The_Stock_Prediction_Machine_Learning_Problem.html">193 hunch net-2006-07-09-The Stock Prediction Machine Learning Problem</a></p>
<p>Introduction: …is discussed in  this nytimes article .  I generally expect such approaches to become more common since computers are getting faster, machine learning is getting better, and data is becoming more plentiful.   This is another example where machine learning technology may have a huge economic impact.  Some side notes:
  
 We-in-research know almost nothing about how these things are done (because it is typically a corporate secret). 
 … but the limited discussion in the article seem naive from a machine learning viewpoint.
 
 The learning process used apparently often fails to take into account transaction costs. 
 What little of the approaches is discussed appears modeling based.  It seems plausible that more direct prediction methods can yield an edge. 
 
 
 One difficulty with stock picking as a research topic is that it is inherently a zero sum game (for every winner, there is a loser).  Much of the rest of research is positive sum (basically, everyone wins).</p><p>5 0.070921808 <a title="328-tfidf-5" href="../hunch_net-2011/hunch_net-2011-08-20-The_Large_Scale_Learning_Survey_Tutorial.html">442 hunch net-2011-08-20-The Large Scale Learning Survey Tutorial</a></p>
<p>Introduction: Ron Bekkerman  initiated an effort to create an  edited book on parallel machine learning  that  Misha  and I have been helping with.  The breadth of efforts to parallelize machine learning surprised me: I was only aware of a small fraction initially.
 
This put us in a unique position, with knowledge of a wide array of different efforts, so it is natural to put together a  survey tutorial on the subject of parallel learning  for  KDD , tomorrow.  This tutorial is  not  limited to the book itself however, as several interesting new algorithms have come out since we started inviting chapters.  
 
This tutorial should interest anyone trying to use machine learning on significant quantities of data, anyone interested in developing algorithms for such, and of course who has bragging rights to the fastest learning algorithm on planet earth   
 
(Also note the Modeling with Hadoop tutorial just before ours which deals with one way of trying to speed up learning algorithms.  We have almost no</p><p>6 0.069883853 <a title="328-tfidf-6" href="../hunch_net-2005/hunch_net-2005-08-04-Why_Reinforcement_Learning_is_Important.html">100 hunch net-2005-08-04-Why Reinforcement Learning is Important</a></p>
<p>7 0.064559743 <a title="328-tfidf-7" href="../hunch_net-2010/hunch_net-2010-12-26-NIPS_2010.html">420 hunch net-2010-12-26-NIPS 2010</a></p>
<p>8 0.064480945 <a title="328-tfidf-8" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>9 0.062432557 <a title="328-tfidf-9" href="../hunch_net-2008/hunch_net-2008-03-15-COLT_Open_Problems.html">292 hunch net-2008-03-15-COLT Open Problems</a></p>
<p>10 0.062051456 <a title="328-tfidf-10" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>11 0.061891533 <a title="328-tfidf-11" href="../hunch_net-2005/hunch_net-2005-04-16-Which_Assumptions_are_Reasonable%3F.html">57 hunch net-2005-04-16-Which Assumptions are Reasonable?</a></p>
<p>12 0.060340844 <a title="328-tfidf-12" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>13 0.05768754 <a title="328-tfidf-13" href="../hunch_net-2008/hunch_net-2008-10-19-NIPS_2008_workshop_on_Kernel_Learning.html">321 hunch net-2008-10-19-NIPS 2008 workshop on Kernel Learning</a></p>
<p>14 0.052980773 <a title="328-tfidf-14" href="../hunch_net-2007/hunch_net-2007-02-10-Best_Practices_for_Collaboration.html">231 hunch net-2007-02-10-Best Practices for Collaboration</a></p>
<p>15 0.052447625 <a title="328-tfidf-15" href="../hunch_net-2012/hunch_net-2012-04-09-ICML_author_feedback_is_open.html">461 hunch net-2012-04-09-ICML author feedback is open</a></p>
<p>16 0.0523801 <a title="328-tfidf-16" href="../hunch_net-2008/hunch_net-2008-04-21-The_Science_2.0_article.html">296 hunch net-2008-04-21-The Science 2.0 article</a></p>
<p>17 0.05185052 <a title="328-tfidf-17" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>18 0.050738838 <a title="328-tfidf-18" href="../hunch_net-2007/hunch_net-2007-09-16-Optimizing_Machine_Learning_Programs.html">262 hunch net-2007-09-16-Optimizing Machine Learning Programs</a></p>
<p>19 0.050268441 <a title="328-tfidf-19" href="../hunch_net-2005/hunch_net-2005-09-04-Science_in_the_Government.html">106 hunch net-2005-09-04-Science in the Government</a></p>
<p>20 0.049921282 <a title="328-tfidf-20" href="../hunch_net-2008/hunch_net-2008-07-04-More_Presentation_Preparation.html">307 hunch net-2008-07-04-More Presentation Preparation</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.069), (1, -0.008), (2, -0.04), (3, 0.035), (4, -0.008), (5, 0.012), (6, 0.013), (7, -0.003), (8, -0.003), (9, 0.016), (10, -0.004), (11, -0.056), (12, -0.014), (13, 0.065), (14, 0.009), (15, -0.004), (16, 0.0), (17, 0.019), (18, -0.003), (19, -0.028), (20, 0.0), (21, 0.04), (22, -0.095), (23, -0.019), (24, 0.015), (25, -0.035), (26, 0.041), (27, 0.017), (28, 0.005), (29, 0.018), (30, 0.003), (31, 0.017), (32, -0.023), (33, -0.004), (34, 0.023), (35, -0.017), (36, -0.043), (37, 0.036), (38, 0.049), (39, 0.044), (40, 0.028), (41, -0.025), (42, 0.004), (43, -0.064), (44, -0.109), (45, -0.003), (46, 0.01), (47, 0.123), (48, 0.046), (49, -0.079)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94519067 <a title="328-lsi-1" href="../hunch_net-2008/hunch_net-2008-11-26-Efficient_Reinforcement_Learning_in_MDPs.html">328 hunch net-2008-11-26-Efficient Reinforcement Learning in MDPs</a></p>
<p>Introduction: Claude Sammut  is attempting to put together an  Encyclopedia of Machine Learning .  I volunteered to write one article on  Efficient RL in MDPs , which I would like to invite comment on.  Is something critical missing?</p><p>2 0.43661466 <a title="328-lsi-2" href="../hunch_net-2005/hunch_net-2005-10-13-Site_tweak.html">122 hunch net-2005-10-13-Site tweak</a></p>
<p>Introduction: Several people have had difficulty with comments which seem to have an allowed language significantly poorer than posts.  The set of allowed html tags has been increased and the  markdown filter  has been put in place to try to make commenting easier.  Iâ&euro;&trade;ll put some examples into the comments of this post.</p><p>3 0.43592322 <a title="328-lsi-3" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>Introduction: This post is some combination of belaboring the obvious and speculating wildly about the future. The basic issue to be addressed is how to think about machine learning in terms given to us from Programming Language theory.
  Types and Reductions  
John’s research programme (I feel this should be in British spelling to reflect the grandiousness of the idea…) of machine learning reductions  StateOfReduction  is at some essential level type-theoretic in nature.  The fundamental elements are the classifier, a function f: alpha -> beta, and the corresponding classifier trainer g: List of (alpha,beta) -> (alpha -> beta). The research goal is to create *combinators* that produce new f’s and g’s given existing ones. John (probably quite rightly) seems unwilling at the moment to commit to any notion stronger than these combinators are correctly typed.  One way to see the result of a reduction is something typed like: (For those denied the joy of the Hindly-Milner type system, “simple” is probab</p><p>4 0.42152137 <a title="328-lsi-4" href="../hunch_net-2006/hunch_net-2006-09-18-What_is_missing_for_online_collaborative_research%3F.html">208 hunch net-2006-09-18-What is missing for online collaborative research?</a></p>
<p>Introduction: The internet has recently made the research process much smoother: papers are easy to obtain, citations are easy to follow, and unpublished “tutorials” are often available. Yet, new research fields can look very complicated to outsiders or newcomers. Every paper is like a small piece of an unfinished jigsaw puzzle: to understand just one publication, a researcher without experience in the field will typically have to follow several layers of citations, and many of the papers he encounters have a great deal of repeated information. Furthermore, from one publication to the next, notation and terminology may not be consistent which can further confuse the reader.
 
But the internet is now proving to be an extremely useful medium for collaboration and knowledge aggregation. Online forums allow users to ask and answer questions and to share ideas. The recent phenomenon of Wikipedia provides a proof-of-concept for the “anyone can edit” system. Can such models be used to facilitate research a</p><p>5 0.42073575 <a title="328-lsi-5" href="../hunch_net-2006/hunch_net-2006-08-10-Precision_is_not_accuracy.html">202 hunch net-2006-08-10-Precision is not accuracy</a></p>
<p>Introduction: In my experience, there are two different groups of people who believe the same thing: the mathematics encountered in typical machine learning conference papers is often of questionable value. 
The two groups who agree on this are applied machine learning people who have given up on math, and mature theoreticians who understand the limits of theory. 
 
Partly, this is just a statement about where we are with respect to machine learning.  In particular, we have no mechanism capable of generating a prescription for how to solve all learning problems.  In the absence of such certainty, people try to come up with formalisms that partially describe and motivate how and why they do things.  This is natural and healthy—we might hope that it will eventually lead to just such a mechanism.
 
But, part of this is simply an emphasis on complexity over clarity.  A very natural and simple theoretical statement is often obscured by complexifications.  Common sources of complexification include:</p><p>6 0.41504061 <a title="328-lsi-6" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<p>7 0.4133749 <a title="328-lsi-7" href="../hunch_net-2011/hunch_net-2011-08-20-The_Large_Scale_Learning_Survey_Tutorial.html">442 hunch net-2011-08-20-The Large Scale Learning Survey Tutorial</a></p>
<p>8 0.41329697 <a title="328-lsi-8" href="../hunch_net-2010/hunch_net-2010-10-17-Partha_Niyogi_has_died.html">414 hunch net-2010-10-17-Partha Niyogi has died</a></p>
<p>9 0.41072953 <a title="328-lsi-9" href="../hunch_net-2009/hunch_net-2009-11-29-AI_Safety.html">380 hunch net-2009-11-29-AI Safety</a></p>
<p>10 0.39722759 <a title="328-lsi-10" href="../hunch_net-2009/hunch_net-2009-05-08-Computability_in_Artificial_Intelligence.html">353 hunch net-2009-05-08-Computability in Artificial Intelligence</a></p>
<p>11 0.38467044 <a title="328-lsi-11" href="../hunch_net-2008/hunch_net-2008-11-04-Rise_of_the_Machines.html">323 hunch net-2008-11-04-Rise of the Machines</a></p>
<p>12 0.38317326 <a title="328-lsi-12" href="../hunch_net-2008/hunch_net-2008-07-04-More_Presentation_Preparation.html">307 hunch net-2008-07-04-More Presentation Preparation</a></p>
<p>13 0.38282454 <a title="328-lsi-13" href="../hunch_net-2008/hunch_net-2008-04-12-It_Doesn%26%238217%3Bt_Stop.html">295 hunch net-2008-04-12-It Doesn&#8217;t Stop</a></p>
<p>14 0.38197422 <a title="328-lsi-14" href="../hunch_net-2006/hunch_net-2006-06-25-Presentation_of_Proofs_is_Hard..html">187 hunch net-2006-06-25-Presentation of Proofs is Hard.</a></p>
<p>15 0.37511528 <a title="328-lsi-15" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>16 0.37120628 <a title="328-lsi-16" href="../hunch_net-2009/hunch_net-2009-08-03-Carbon_in_Computer_Science_Research.html">366 hunch net-2009-08-03-Carbon in Computer Science Research</a></p>
<p>17 0.37051752 <a title="328-lsi-17" href="../hunch_net-2008/hunch_net-2008-04-22-Taking_the_next_step.html">297 hunch net-2008-04-22-Taking the next step</a></p>
<p>18 0.36310408 <a title="328-lsi-18" href="../hunch_net-2005/hunch_net-2005-02-02-Kolmogorov_Complexity_and_Googling.html">10 hunch net-2005-02-02-Kolmogorov Complexity and Googling</a></p>
<p>19 0.36293519 <a title="328-lsi-19" href="../hunch_net-2006/hunch_net-2006-03-09-Use_of_Notation.html">162 hunch net-2006-03-09-Use of Notation</a></p>
<p>20 0.3611083 <a title="328-lsi-20" href="../hunch_net-2010/hunch_net-2010-09-28-Machined_Learnings.html">412 hunch net-2010-09-28-Machined Learnings</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(27, 0.055), (89, 0.744)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.90604568 <a title="328-lda-1" href="../hunch_net-2008/hunch_net-2008-11-26-Efficient_Reinforcement_Learning_in_MDPs.html">328 hunch net-2008-11-26-Efficient Reinforcement Learning in MDPs</a></p>
<p>Introduction: Claude Sammut  is attempting to put together an  Encyclopedia of Machine Learning .  I volunteered to write one article on  Efficient RL in MDPs , which I would like to invite comment on.  Is something critical missing?</p><p>2 0.6687277 <a title="328-lda-2" href="../hunch_net-2005/hunch_net-2005-04-01-Basic_computer_science_research_takes_a_hit.html">50 hunch net-2005-04-01-Basic computer science research takes a hit</a></p>
<p>Introduction: The New York Times has an interesting  article  about how DARPA has dropped funding for computer science to universities by about a factor of 2 over the last 5 years and become less directed towards basic research.  Partially in response, the number of grant submissions to NSF has grown by a factor of 3 (with the NSF budget staying approximately constant in the interim).
 
This is the sort of policy decision which may make sense for the defense department, but which means a large hit for basic research on information technology development in the US.  For example “darpa funded the invention of the internet” is reasonably correct.  This policy decision is particularly painful in the context of NSF budget cuts and the end of extensive phone monopoly funded research at Bell labs. 
 
The good news from a learning perspective is that (based on anecdotal evidence) much of the remaining funding is aimed at learning and learning-related fields.  Methods of making good automated predictions obv</p><p>3 0.65695828 <a title="328-lda-3" href="../hunch_net-2013/hunch_net-2013-03-22-I%26%238217%3Bm_a_bandit.html">480 hunch net-2013-03-22-I&#8217;m a bandit</a></p>
<p>Introduction: Sebastien Bubeck has a  new ML blog  focused on optimization and partial feedback which may interest people.</p><p>4 0.49148461 <a title="328-lda-4" href="../hunch_net-2005/hunch_net-2005-06-22-Languages__of_Learning.html">84 hunch net-2005-06-22-Languages  of Learning</a></p>
<p>Introduction: A language is a set of primitives which can be combined to succesfully create complex objects.  Languages arise in all sorts of situations: mechanical construction, martial arts, communication, etc…  Languages appear to be the key to succesfully creating complex objects—it is difficult to come up with any convincing example of a complex object which is not built using some language.  Since languages are so crucial to success, it is interesting to organize various machine learning research programs by language.
 
The most common language in machine learning are languages for representing the solution to machine learning.   This includes:
  
  Bayes Nets and Graphical Models  A language for representing probability distributions.  The key concept supporting modularity is conditional independence.   Michael Kearns  has been working on extending this to game theory. 
  Kernelized Linear Classifiers  A language for representing linear separators, possibly in a large space.  The key form of</p><p>5 0.4674955 <a title="328-lda-5" href="../hunch_net-2005/hunch_net-2005-10-07-On-line_learning_of_regular_decision_rules.html">118 hunch net-2005-10-07-On-line learning of regular decision rules</a></p>
<p>Introduction: Many decision problems can be represented in the form 
FOR  n =1,2,…: 
— Reality chooses a datum  x n  . 
— Decision Maker chooses his decision  d n  . 
— Reality chooses an observation  y n  . 
— Decision Maker suffers loss   L ( y n  , d n  ). 
END FOR. 
The observation  y n   can be, for example, tomorrow’s stock price and the decision  d n   the number of shares Decision Maker chooses to buy.  The datum  x n   ideally contains all information that might be relevant in making this decision.  We do not want to assume anything about the way Reality generates the observations and data.
 
Suppose there is a good and not too complex decision rule  D  mapping each datum  x  to a decision  D ( x ).  Can we perform as well, or almost as well, as  D , without knowing it?  This is essentially a special case of the problem of  on-line learning .
 
This is a simple result of this kind.  Suppose the data  x n   are taken from [0,1] and  L ( y , d )=| y – d |.  A norm || h || of a function  h  on</p><p>6 0.38242206 <a title="328-lda-6" href="../hunch_net-2008/hunch_net-2008-04-27-Watchword%3A_Supervised_Learning.html">299 hunch net-2008-04-27-Watchword: Supervised Learning</a></p>
<p>7 0.17639041 <a title="328-lda-7" href="../hunch_net-2005/hunch_net-2005-03-05-Funding_Research.html">36 hunch net-2005-03-05-Funding Research</a></p>
<p>8 0.15140846 <a title="328-lda-8" href="../hunch_net-2006/hunch_net-2006-02-04-Research_Budget_Changes.html">154 hunch net-2006-02-04-Research Budget Changes</a></p>
<p>9 0.11945958 <a title="328-lda-9" href="../hunch_net-2005/hunch_net-2005-02-17-Learning_Research_Programs.html">21 hunch net-2005-02-17-Learning Research Programs</a></p>
<p>10 0.11266527 <a title="328-lda-10" href="../hunch_net-2009/hunch_net-2009-05-30-Many_ways_to_Learn_this_summer.html">357 hunch net-2009-05-30-Many ways to Learn this summer</a></p>
<p>11 0.10918485 <a title="328-lda-11" href="../hunch_net-2010/hunch_net-2010-04-28-CI_Fellows_program_renewed.html">396 hunch net-2010-04-28-CI Fellows program renewed</a></p>
<p>12 0.09204495 <a title="328-lda-12" href="../hunch_net-2011/hunch_net-2011-11-26-Giving_Thanks.html">449 hunch net-2011-11-26-Giving Thanks</a></p>
<p>13 0.085642874 <a title="328-lda-13" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>14 0.081794411 <a title="328-lda-14" href="../hunch_net-2008/hunch_net-2008-09-03-Bidding_Problems.html">315 hunch net-2008-09-03-Bidding Problems</a></p>
<p>15 0.079810672 <a title="328-lda-15" href="../hunch_net-2008/hunch_net-2008-04-21-The_Science_2.0_article.html">296 hunch net-2008-04-21-The Science 2.0 article</a></p>
<p>16 0.07896401 <a title="328-lda-16" href="../hunch_net-2006/hunch_net-2006-01-08-Debugging_Your_Brain.html">147 hunch net-2006-01-08-Debugging Your Brain</a></p>
<p>17 0.078195028 <a title="328-lda-17" href="../hunch_net-2007/hunch_net-2007-04-28-The_Coming_Patent_Apocalypse.html">241 hunch net-2007-04-28-The Coming Patent Apocalypse</a></p>
<p>18 0.077142805 <a title="328-lda-18" href="../hunch_net-2005/hunch_net-2005-04-04-Grounds_for_Rejection.html">52 hunch net-2005-04-04-Grounds for Rejection</a></p>
<p>19 0.073337078 <a title="328-lda-19" href="../hunch_net-2006/hunch_net-2006-03-24-NLPers.html">166 hunch net-2006-03-24-NLPers</a></p>
<p>20 0.073337078 <a title="328-lda-20" href="../hunch_net-2007/hunch_net-2007-06-13-Not_Posting.html">246 hunch net-2007-06-13-Not Posting</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
