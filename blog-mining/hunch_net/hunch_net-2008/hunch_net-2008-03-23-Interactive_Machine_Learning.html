<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>293 hunch net-2008-03-23-Interactive Machine Learning</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2008" href="../home/hunch_net-2008_home.html">hunch_net-2008</a> <a title="hunch_net-2008-293" href="#">hunch_net-2008-293</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>293 hunch net-2008-03-23-Interactive Machine Learning</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2008-293-html" href="http://hunch.net/?p=322">html</a></p><p>Introduction: A new direction of research seems to be arising in machine learning: Interactive Machine Learning.  This isn’t a familiar term, although it does include some familiar subjects.
 
 What is Interactive Machine Learning?   The fundamental requirement is (a) learning algorithms which interact with the world and (b) learn.  
 
For our purposes, let’s define learning as efficiently competing with a large set of possible predictors.  Examples include:
  
 Online learning against an adversary ( Avrim’s Notes ).  The interaction is almost trivial: the learning algorithm makes a prediction and then receives feedback. The learning is choosing based upon the advice of many experts. 
   Active Learning  .  In active learning, the interaction is choosing which examples to label, and the learning is choosing from amongst a large set of hypotheses. 
   Contextual Bandits  .  The interaction is choosing one of several actions and learning only the value of the chosen action (weaker than active learning</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The interaction is almost trivial: the learning algorithm makes a prediction and then receives feedback. [sent-7, score-0.653]
</p><p>2 In active learning, the interaction is choosing which examples to label, and the learning is choosing from amongst a large set of hypotheses. [sent-10, score-1.207]
</p><p>3 The interaction is choosing one of several actions and learning only the value of the chosen action (weaker than active learning feedback). [sent-12, score-0.99]
</p><p>4 More forms of interaction will doubtless be noted and tackled as time progresses. [sent-13, score-0.516]
</p><p>5 I created a webpage for  my own research on interactive learning  which helps define the above subjects a bit more. [sent-14, score-0.718]
</p><p>6 There are several learning settings which fail either the interaction or the learning test. [sent-16, score-0.839]
</p><p>7 The basic paradigm in supervised learning is that you ask experts to label examples, and then you learn a predictor based upon the predictions of these experts. [sent-18, score-0.602]
</p><p>8 The interaction is there, but the set of policies learned over is still too limited—essentially the policies just memorize what to do in each state. [sent-26, score-0.764]
</p><p>9 All of these not-quite-interactive-learning topics are of course very useful background information for interactive machine learning. [sent-28, score-0.568]
</p><p>10 We know from other fields and various examples that interaction is very powerful. [sent-31, score-0.636]
</p><p>11 From online learning against an adversary, we know that independence of samples is unnecessary in an interactive setting—in fact you can even function against an adversary. [sent-32, score-0.898]
</p><p>12 From active learning, we know that interaction sometimes allows us to use exponentially fewer labeled samples than in supervised learning. [sent-33, score-0.951]
</p><p>13 From context bandits, we gain the ability to learn in settings where traditional supervised learning just doesn’t apply. [sent-34, score-0.538]
</p><p>14 From complexity theory we have “ IP = PSPACE ” roughly: interactive proofs are as powerful as polynomial space algorithms, which is a strong statement about the power of interaction. [sent-35, score-0.515]
</p><p>15 Several other variations of interactive settings have been proposed and analyzed. [sent-38, score-0.627]
</p><p>16 There are plenty of kinds of natural interaction which haven’t been formalized or analyzed. [sent-45, score-0.519]
</p><p>17 Many people doing machine learning want to reach AI, and it seems clear that any AI must engage in interactive learning. [sent-48, score-0.756]
</p><p>18 Some of the techniques for other methods of interactive learning may be helpful. [sent-51, score-0.646]
</p><p>19 How do we blend interactive and noninteractive learning? [sent-52, score-0.633]
</p><p>20 Are there general methods for reducing interactive learning problems to supervised learning problems (which we know better)? [sent-54, score-1.208]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('interactive', 0.515), ('interaction', 0.465), ('supervised', 0.215), ('choosing', 0.135), ('learning', 0.131), ('active', 0.128), ('contextual', 0.124), ('adversary', 0.121), ('bandit', 0.121), ('settings', 0.112), ('ad', 0.102), ('semisupervised', 0.098), ('bandits', 0.098), ('doesn', 0.097), ('mdp', 0.095), ('essentially', 0.093), ('examples', 0.089), ('policies', 0.087), ('know', 0.082), ('learn', 0.08), ('ai', 0.074), ('define', 0.072), ('problems', 0.067), ('familiar', 0.066), ('upon', 0.065), ('set', 0.064), ('except', 0.064), ('arising', 0.061), ('memorize', 0.061), ('blend', 0.061), ('mastering', 0.061), ('samples', 0.061), ('label', 0.06), ('amongst', 0.06), ('include', 0.059), ('online', 0.058), ('engage', 0.057), ('receives', 0.057), ('noninteractive', 0.057), ('formalized', 0.054), ('weaker', 0.054), ('ip', 0.054), ('throw', 0.054), ('machine', 0.053), ('unnecessary', 0.051), ('doubtless', 0.051), ('paradigm', 0.051), ('requirement', 0.051), ('closer', 0.051), ('exception', 0.051)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="293-tfidf-1" href="../hunch_net-2008/hunch_net-2008-03-23-Interactive_Machine_Learning.html">293 hunch net-2008-03-23-Interactive Machine Learning</a></p>
<p>Introduction: A new direction of research seems to be arising in machine learning: Interactive Machine Learning.  This isn’t a familiar term, although it does include some familiar subjects.
 
 What is Interactive Machine Learning?   The fundamental requirement is (a) learning algorithms which interact with the world and (b) learn.  
 
For our purposes, let’s define learning as efficiently competing with a large set of possible predictors.  Examples include:
  
 Online learning against an adversary ( Avrim’s Notes ).  The interaction is almost trivial: the learning algorithm makes a prediction and then receives feedback. The learning is choosing based upon the advice of many experts. 
   Active Learning  .  In active learning, the interaction is choosing which examples to label, and the learning is choosing from amongst a large set of hypotheses. 
   Contextual Bandits  .  The interaction is choosing one of several actions and learning only the value of the chosen action (weaker than active learning</p><p>2 0.23362526 <a title="293-tfidf-2" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>Introduction: Consider the contextual bandit setting where, repeatedly:
  
 A context  x  is observed. 
 An action  a  is taken given the context  x .  
 A reward  r  is observed, dependent on  x  and  a . 
  
Where the goal of a learning agent is to find a policy for step 2 achieving a large expected reward.  
 
This setting is of obvious importance, because in the real world we typically make decisions based on some set of information and then get feedback only about the single action taken.  It also fundamentally differs from supervised learning settings because knowing the value of one action is not equivalent to knowing the value of all actions.
 
A decade ago the best machine learning techniques for this setting where implausibly inefficient.   Dean Foster  once told me he thought the area was a research sinkhole with little progress to be expected.  Now we are on the verge of being able to routinely attack these problems, in almost exactly the same sense that we routinely attack bread and but</p><p>3 0.21743451 <a title="293-tfidf-3" href="../hunch_net-2007/hunch_net-2007-10-24-Contextual_Bandits.html">269 hunch net-2007-10-24-Contextual Bandits</a></p>
<p>Introduction: One of the fundamental underpinnings of the internet is advertising based content.  This has become much more effective due to targeted advertising where ads are specifically matched to interests.  Everyone is familiar with this, because everyone uses search engines and all search engines try to make money this way.
 
The problem of matching ads to interests is a natural machine learning problem in some ways since there is much information in who clicks on what.  A fundamental problem with this information is that it is not supervised—in particular a click-or-not on one ad doesn’t generally tell you if a different ad would have been clicked on.  This implies we have a fundamental exploration problem.
 
A standard mathematical setting for this situation is “ k -Armed Bandits”, often with various relevant embellishments.  The  k -Armed Bandit setting works on a round-by-round basis.  On each round:
  
 A policy chooses arm  a  from  1  of  k  arms (i.e. 1 of k ads). 
 The world reveals t</p><p>4 0.16934831 <a title="293-tfidf-4" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>Introduction: A little over 4 years ago,  Sanjoy   made a post  saying roughly “we should  study active learning theoretically, because not much is understood”.   
 
At the time, we did not understand basic things such as whether or not it was possible to PAC-learn with an active algorithm without making strong assumptions about the noise rate.  In other words, the fundamental question was “can we do it?”
 
The nature of the question has fundamentally changed in my mind.   The answer is to the previous question is “yes”, both information theoretically and computationally, most places where supervised learning could be applied.  
 
In many situation, the question has now changed to: “is it worth it?”  Is the programming and computational overhead low enough to make the label cost savings of active learning worthwhile?  Currently, there are situations where this question could go either way.  Much of the challenge for the future is in figuring out how to make active learning easier or more worthwhile.</p><p>5 0.166596 <a title="293-tfidf-5" href="../hunch_net-2007/hunch_net-2007-09-30-NIPS_workshops_are_out..html">264 hunch net-2007-09-30-NIPS workshops are out.</a></p>
<p>Introduction: Here .  Iâ&euro;&trade;m particularly interested in the  Web Search ,  Efficient ML , and (of course)  Learning Problem Design  workshops but there are many others to check out as well. Workshops are a great chance to make progress on or learn about a topic.  Relevance and interaction amongst diverse people can sometimes be magical.</p><p>6 0.15775268 <a title="293-tfidf-6" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<p>7 0.15009263 <a title="293-tfidf-7" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>8 0.14948905 <a title="293-tfidf-8" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>9 0.14328513 <a title="293-tfidf-9" href="../hunch_net-2010/hunch_net-2010-09-17-New_York_Area_Machine_Learning_Events.html">410 hunch net-2010-09-17-New York Area Machine Learning Events</a></p>
<p>10 0.14047515 <a title="293-tfidf-10" href="../hunch_net-2007/hunch_net-2007-12-19-Cool_and_interesting_things_seen_at_NIPS.html">279 hunch net-2007-12-19-Cool and interesting things seen at NIPS</a></p>
<p>11 0.13459204 <a title="293-tfidf-11" href="../hunch_net-2008/hunch_net-2008-04-27-Watchword%3A_Supervised_Learning.html">299 hunch net-2008-04-27-Watchword: Supervised Learning</a></p>
<p>12 0.1317035 <a title="293-tfidf-12" href="../hunch_net-2009/hunch_net-2009-11-06-Yisong_Yue_on_Self-improving_Systems.html">376 hunch net-2009-11-06-Yisong Yue on Self-improving Systems</a></p>
<p>13 0.13113771 <a title="293-tfidf-13" href="../hunch_net-2009/hunch_net-2009-10-26-NIPS_workshops.html">375 hunch net-2009-10-26-NIPS workshops</a></p>
<p>14 0.12521029 <a title="293-tfidf-14" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>15 0.11749483 <a title="293-tfidf-15" href="../hunch_net-2008/hunch_net-2008-09-12-How_do_we_get_weak_action_dependence_for_learning_with_partial_observations%3F.html">317 hunch net-2008-09-12-How do we get weak action dependence for learning with partial observations?</a></p>
<p>16 0.1169645 <a title="293-tfidf-16" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>17 0.11681201 <a title="293-tfidf-17" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>18 0.11434552 <a title="293-tfidf-18" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>19 0.10850855 <a title="293-tfidf-19" href="../hunch_net-2009/hunch_net-2009-05-08-Computability_in_Artificial_Intelligence.html">353 hunch net-2009-05-08-Computability in Artificial Intelligence</a></p>
<p>20 0.10617594 <a title="293-tfidf-20" href="../hunch_net-2005/hunch_net-2005-03-22-Active_learning.html">45 hunch net-2005-03-22-Active learning</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.245), (1, 0.099), (2, -0.088), (3, -0.008), (4, 0.106), (5, -0.025), (6, 0.014), (7, 0.004), (8, 0.022), (9, 0.062), (10, 0.177), (11, 0.008), (12, 0.042), (13, 0.19), (14, -0.189), (15, 0.084), (16, 0.025), (17, -0.046), (18, 0.037), (19, 0.136), (20, 0.038), (21, 0.004), (22, 0.043), (23, 0.001), (24, -0.008), (25, -0.02), (26, 0.032), (27, 0.009), (28, -0.057), (29, 0.038), (30, 0.016), (31, -0.008), (32, 0.066), (33, -0.042), (34, 0.06), (35, -0.03), (36, -0.048), (37, -0.042), (38, -0.042), (39, 0.065), (40, -0.004), (41, -0.077), (42, -0.027), (43, 0.071), (44, -0.121), (45, 0.013), (46, -0.097), (47, 0.033), (48, 0.086), (49, 0.053)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.92018777 <a title="293-lsi-1" href="../hunch_net-2008/hunch_net-2008-03-23-Interactive_Machine_Learning.html">293 hunch net-2008-03-23-Interactive Machine Learning</a></p>
<p>Introduction: A new direction of research seems to be arising in machine learning: Interactive Machine Learning.  This isn’t a familiar term, although it does include some familiar subjects.
 
 What is Interactive Machine Learning?   The fundamental requirement is (a) learning algorithms which interact with the world and (b) learn.  
 
For our purposes, let’s define learning as efficiently competing with a large set of possible predictors.  Examples include:
  
 Online learning against an adversary ( Avrim’s Notes ).  The interaction is almost trivial: the learning algorithm makes a prediction and then receives feedback. The learning is choosing based upon the advice of many experts. 
   Active Learning  .  In active learning, the interaction is choosing which examples to label, and the learning is choosing from amongst a large set of hypotheses. 
   Contextual Bandits  .  The interaction is choosing one of several actions and learning only the value of the chosen action (weaker than active learning</p><p>2 0.82844114 <a title="293-lsi-2" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>Introduction: Consider the contextual bandit setting where, repeatedly:
  
 A context  x  is observed. 
 An action  a  is taken given the context  x .  
 A reward  r  is observed, dependent on  x  and  a . 
  
Where the goal of a learning agent is to find a policy for step 2 achieving a large expected reward.  
 
This setting is of obvious importance, because in the real world we typically make decisions based on some set of information and then get feedback only about the single action taken.  It also fundamentally differs from supervised learning settings because knowing the value of one action is not equivalent to knowing the value of all actions.
 
A decade ago the best machine learning techniques for this setting where implausibly inefficient.   Dean Foster  once told me he thought the area was a research sinkhole with little progress to be expected.  Now we are on the verge of being able to routinely attack these problems, in almost exactly the same sense that we routinely attack bread and but</p><p>3 0.74840206 <a title="293-lsi-3" href="../hunch_net-2007/hunch_net-2007-10-24-Contextual_Bandits.html">269 hunch net-2007-10-24-Contextual Bandits</a></p>
<p>Introduction: One of the fundamental underpinnings of the internet is advertising based content.  This has become much more effective due to targeted advertising where ads are specifically matched to interests.  Everyone is familiar with this, because everyone uses search engines and all search engines try to make money this way.
 
The problem of matching ads to interests is a natural machine learning problem in some ways since there is much information in who clicks on what.  A fundamental problem with this information is that it is not supervised—in particular a click-or-not on one ad doesn’t generally tell you if a different ad would have been clicked on.  This implies we have a fundamental exploration problem.
 
A standard mathematical setting for this situation is “ k -Armed Bandits”, often with various relevant embellishments.  The  k -Armed Bandit setting works on a round-by-round basis.  On each round:
  
 A policy chooses arm  a  from  1  of  k  arms (i.e. 1 of k ads). 
 The world reveals t</p><p>4 0.70853329 <a title="293-lsi-4" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>Introduction: A little over 4 years ago,  Sanjoy   made a post  saying roughly “we should  study active learning theoretically, because not much is understood”.   
 
At the time, we did not understand basic things such as whether or not it was possible to PAC-learn with an active algorithm without making strong assumptions about the noise rate.  In other words, the fundamental question was “can we do it?”
 
The nature of the question has fundamentally changed in my mind.   The answer is to the previous question is “yes”, both information theoretically and computationally, most places where supervised learning could be applied.  
 
In many situation, the question has now changed to: “is it worth it?”  Is the programming and computational overhead low enough to make the label cost savings of active learning worthwhile?  Currently, there are situations where this question could go either way.  Much of the challenge for the future is in figuring out how to make active learning easier or more worthwhile.</p><p>5 0.68437636 <a title="293-lsi-5" href="../hunch_net-2008/hunch_net-2008-04-27-Watchword%3A_Supervised_Learning.html">299 hunch net-2008-04-27-Watchword: Supervised Learning</a></p>
<p>Introduction: I recently discovered that supervised learning is a controversial term.  The two definitions are:
  
  Known Loss  Supervised learning corresponds to the situation where you have unlabeled examples plus knowledge of the loss of each possible predicted choice.  This is the definition I’m familiar and comfortable with.  One reason to prefer this definition is that the analysis of sample complexity for this class of learning problems are all pretty similar. 
  Any kind of signal  Supervised learning corresponds to the situation where you have unlabeled examples plus any source of side information about what the right choice is.  This notion of supervised learning seems to subsume reinforcement learning, which makes me uncomfortable, because it means there are two words for the same class.  This also means there isn’t a convenient word to describe the first definition. 
  
Reviews suggest there are people who are dedicated to the second definition out there, so it can be important to discr</p><p>6 0.6767953 <a title="293-lsi-6" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<p>7 0.64861149 <a title="293-lsi-7" href="../hunch_net-2008/hunch_net-2008-09-12-How_do_we_get_weak_action_dependence_for_learning_with_partial_observations%3F.html">317 hunch net-2008-09-12-How do we get weak action dependence for learning with partial observations?</a></p>
<p>8 0.64803064 <a title="293-lsi-8" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>9 0.64683431 <a title="293-lsi-9" href="../hunch_net-2009/hunch_net-2009-11-06-Yisong_Yue_on_Self-improving_Systems.html">376 hunch net-2009-11-06-Yisong Yue on Self-improving Systems</a></p>
<p>10 0.61519182 <a title="293-lsi-10" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>11 0.58826149 <a title="293-lsi-11" href="../hunch_net-2007/hunch_net-2007-12-19-Cool_and_interesting_things_seen_at_NIPS.html">279 hunch net-2007-12-19-Cool and interesting things seen at NIPS</a></p>
<p>12 0.58386523 <a title="293-lsi-12" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>13 0.57350874 <a title="293-lsi-13" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<p>14 0.56788027 <a title="293-lsi-14" href="../hunch_net-2009/hunch_net-2009-12-24-Top_graduates_this_season.html">384 hunch net-2009-12-24-Top graduates this season</a></p>
<p>15 0.56144512 <a title="293-lsi-15" href="../hunch_net-2010/hunch_net-2010-01-13-Sam_Roweis_died.html">386 hunch net-2010-01-13-Sam Roweis died</a></p>
<p>16 0.54180992 <a title="293-lsi-16" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>17 0.53821111 <a title="293-lsi-17" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>18 0.53086513 <a title="293-lsi-18" href="../hunch_net-2008/hunch_net-2008-07-15-Interesting_papers_at_COLT_%28and_a_bit_of_UAI_%26%23038%3B_workshops%29.html">310 hunch net-2008-07-15-Interesting papers at COLT (and a bit of UAI &#038; workshops)</a></p>
<p>19 0.52895141 <a title="293-lsi-19" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>20 0.52870345 <a title="293-lsi-20" href="../hunch_net-2010/hunch_net-2010-03-26-A_Variance_only_Deviation_Bound.html">392 hunch net-2010-03-26-A Variance only Deviation Bound</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(3, 0.017), (10, 0.022), (23, 0.031), (27, 0.311), (38, 0.028), (53, 0.04), (55, 0.102), (77, 0.064), (92, 0.191), (94, 0.089), (95, 0.015)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.93838137 <a title="293-lda-1" href="../hunch_net-2007/hunch_net-2007-04-13-What_to_do_with_an_unreasonable_conditional_accept.html">238 hunch net-2007-04-13-What to do with an unreasonable conditional accept</a></p>
<p>Introduction: Last year about this time, we received a conditional accept for the  searn paper , which asked us to reference a paper that was not reasonable to cite because there was strictly more relevant work by the same authors that we already cited.  We wrote a response explaining this, and didn’t cite it in the final draft, giving the SPC an excuse to  reject the paper , leading to unhappiness for all.
 
Later,  Sanjoy Dasgupta  suggested that an alternative was to talk to the PC chair instead, as soon as you see that a conditional accept is unreasonable.   William Cohen  and I spoke about this by email, the relevant bit of which is:
  

If an SPC asks for a revision that is inappropriate, the correct 
action is to contact the chairs as soon as the decision is made, 
clearly explaining what the problem is, so we can decide whether or 
not to over-rule the SPC.  As you say, this is extra work for us 
chairs, but that’s part of the job, and we’re willing to do that sort 
of work to improve the ov</p><p>same-blog 2 0.93291974 <a title="293-lda-2" href="../hunch_net-2008/hunch_net-2008-03-23-Interactive_Machine_Learning.html">293 hunch net-2008-03-23-Interactive Machine Learning</a></p>
<p>Introduction: A new direction of research seems to be arising in machine learning: Interactive Machine Learning.  This isn’t a familiar term, although it does include some familiar subjects.
 
 What is Interactive Machine Learning?   The fundamental requirement is (a) learning algorithms which interact with the world and (b) learn.  
 
For our purposes, let’s define learning as efficiently competing with a large set of possible predictors.  Examples include:
  
 Online learning against an adversary ( Avrim’s Notes ).  The interaction is almost trivial: the learning algorithm makes a prediction and then receives feedback. The learning is choosing based upon the advice of many experts. 
   Active Learning  .  In active learning, the interaction is choosing which examples to label, and the learning is choosing from amongst a large set of hypotheses. 
   Contextual Bandits  .  The interaction is choosing one of several actions and learning only the value of the chosen action (weaker than active learning</p><p>3 0.91078389 <a title="293-lda-3" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>Introduction: The 2006 Machine Learning Summer School in Taipei, Taiwan ended on August 4, 2006. It has been a very exciting two weeks for a record crowd of 245 participants (including speakers and organizers) from 18 countries. We had a lineup of speakers that is hard to match up for other similar events (see our  WIKI  for more information). With this lineup, it is difficult for us as organizers to screw it up too bad.  Also, since we have pretty good infrastructure for international meetings and experienced staff at NTUST and Academia Sinica, plus the reputation established by previous MLSS series, it was relatively easy for us to attract registrations and simply enjoyed this two-week long party of machine learning.
 
In the end of MLSS we distributed a survey form for participants to fill in. I will report what we found from this survey, together with the registration data and word-of-mouth from participants.  
 
The first question is designed to find out how our participants learned about MLSS</p><p>4 0.88804871 <a title="293-lda-4" href="../hunch_net-2007/hunch_net-2007-11-14-BellKor_wins_Netflix.html">272 hunch net-2007-11-14-BellKor wins Netflix</a></p>
<p>Introduction: … but only the little prize.  The  BellKor team  focused on integrating predictions from many different methods.  The base methods consist of:
  
 Nearest Neighbor Methods 
 Matrix Factorization Methods (asymmetric and symmetric) 
 Linear Regression on various feature spaces 
 Restricted Boltzman Machines 
  
The final predictor was an ensemble (as was reasonable to expect), although it’s a little bit more complicated than just a weighted average—it’s essentially a customized learning algorithm.  Base approaches (1)-(3) seem like relatively well-known approaches (although I haven’t seen the asymmetric factorization variant before).  RBMs are the new approach.
 
The  writeup  is pretty clear for more details.
 
The contestants are close to reaching the big prize, but the last 1.5% is probably at least as hard as what’s been done.  A few new structurally different methods for making predictions may need to be discovered and added into the mixture.  In other words, research may be require</p><p>5 0.86607713 <a title="293-lda-5" href="../hunch_net-2011/hunch_net-2011-07-10-ICML_2011_and_the_future.html">437 hunch net-2011-07-10-ICML 2011 and the future</a></p>
<p>Introduction: Unfortunately, I ended up sick for much of this ICML.  I did manage to catch one interesting paper:
 
 Richard Socher ,  Cliff Lin ,  Andrew Y. Ng , and  Christopher D. Manning   Parsing Natural Scenes and Natural Language with Recursive Neural Networks .
 
I invited Richard to share his list of interesting papers, so hopefully we’ll hear from him soon.  In the meantime,  Paul  and  Hal  have posted some lists.
 
 the future 
 
 Joelle  and I are program chairs for ICML 2012 in  Edinburgh , which I previously enjoyed visiting in  2005 .  This is a huge responsibility, that we hope to accomplish well.  A part of this (perhaps the most fun part), is imagining how we can make ICML better.  A key and critical constraint is choosing things that can be accomplished.  So far we have:
  
  Colocation . The first thing we looked into was potential colocations.  We quickly discovered that many other conferences precomitted their location.  For the future, getting a colocation with  ACL  or  SIGI</p><p>6 0.8509109 <a title="293-lda-6" href="../hunch_net-2008/hunch_net-2008-09-12-How_do_we_get_weak_action_dependence_for_learning_with_partial_observations%3F.html">317 hunch net-2008-09-12-How do we get weak action dependence for learning with partial observations?</a></p>
<p>7 0.85023224 <a title="293-lda-7" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>8 0.84975624 <a title="293-lda-8" href="../hunch_net-2012/hunch_net-2012-05-02-ICML%3A_Behind_the_Scenes.html">463 hunch net-2012-05-02-ICML: Behind the Scenes</a></p>
<p>9 0.84804672 <a title="293-lda-9" href="../hunch_net-2007/hunch_net-2007-08-12-Exponentiated_Gradient.html">258 hunch net-2007-08-12-Exponentiated Gradient</a></p>
<p>10 0.84722239 <a title="293-lda-10" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>11 0.84711242 <a title="293-lda-11" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>12 0.84593016 <a title="293-lda-12" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>13 0.84576887 <a title="293-lda-13" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>14 0.84505206 <a title="293-lda-14" href="../hunch_net-2008/hunch_net-2008-10-14-Who_is_Responsible_for_a_Bad_Review%3F.html">320 hunch net-2008-10-14-Who is Responsible for a Bad Review?</a></p>
<p>15 0.84487045 <a title="293-lda-15" href="../hunch_net-2005/hunch_net-2005-06-08-Question%3A_%26%238220%3BWhen_is_the_right_time_to_insert_the_loss_function%3F%26%238221%3B.html">79 hunch net-2005-06-08-Question: &#8220;When is the right time to insert the loss function?&#8221;</a></p>
<p>16 0.84443361 <a title="293-lda-16" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>17 0.84421682 <a title="293-lda-17" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>18 0.84376651 <a title="293-lda-18" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<p>19 0.84270233 <a title="293-lda-19" href="../hunch_net-2007/hunch_net-2007-08-19-Choice_of_Metrics.html">259 hunch net-2007-08-19-Choice of Metrics</a></p>
<p>20 0.84134954 <a title="293-lda-20" href="../hunch_net-2009/hunch_net-2009-01-21-Nearly_all_natural_problems_require_nonlinearity.html">337 hunch net-2009-01-21-Nearly all natural problems require nonlinearity</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
