<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>293 hunch net-2008-03-23-Interactive Machine Learning</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2008" href="../home/hunch_net-2008_home.html">hunch_net-2008</a> <a title="hunch_net-2008-293" href="#">hunch_net-2008-293</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>293 hunch net-2008-03-23-Interactive Machine Learning</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2008-293-html" href="http://hunch.net/?p=322">html</a></p><p>Introduction: A new direction of research seems to be arising in machine learning:
Interactive Machine Learning. This isn't a familiar term, although it does
include some familiar subjects.What is Interactive Machine Learning?The
fundamental requirement is (a) learning algorithms which interact with the
world and (b) learn.For our purposes, let's define learning as efficiently
competing with a large set of possible predictors. Examples include:Online
learning against an adversary (Avrim's Notes). The interaction is almost
trivial: the learning algorithm makes a prediction and then receives feedback.
The learning is choosing based upon the advice of many experts.Active
Learning. In active learning, the interaction is choosing which examples to
label, and the learning is choosing from amongst a large set of
hypotheses.Contextual Bandits. The interaction is choosing one of several
actions and learning only the value of the chosen action (weaker than active
learning feedback).More forms of interaction w</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The interaction is almost trivial: the learning algorithm makes a prediction and then receives feedback. [sent-7, score-0.67]
</p><p>2 In active learning, the interaction is choosing which examples to label, and the learning is choosing from amongst a large set of hypotheses. [sent-10, score-1.196]
</p><p>3 The interaction is choosing one of several actions and learning only the value of the chosen action (weaker than active learning feedback). [sent-12, score-0.971]
</p><p>4 More forms of interaction will doubtless be noted and tackled as time progresses. [sent-13, score-0.528]
</p><p>5 I created a webpage formy own research on interactive learningwhich helps define the above subjects a bit more. [sent-14, score-0.623]
</p><p>6 There are several learning settings which fail either the interaction or the learning test. [sent-16, score-0.864]
</p><p>7 The basic paradigm in supervised learning is that you ask experts to label examples, and then you learn a predictor based upon the predictions of these experts. [sent-18, score-0.594]
</p><p>8 The interaction is there, but the set of policies learned over is still too limited--essentially the policies just memorize what to do in each state. [sent-26, score-0.785]
</p><p>9 All of these not-quite-interactive-learning topics are of course very useful background information for interactive machine learning. [sent-28, score-0.605]
</p><p>10 We know from other fields and various examples that interaction is very powerful. [sent-31, score-0.653]
</p><p>11 From online learning against an adversary, we know that independence of samples is unnecessary in an interactive setting--in fact you can even function against an adversary. [sent-32, score-0.892]
</p><p>12 From active learning, we know that interaction sometimes allows us to use exponentially fewer labeled samples than in supervised learning. [sent-33, score-0.894]
</p><p>13 From context bandits, we gain the ability to learn in settings where traditional supervised learning just doesn't apply. [sent-34, score-0.524]
</p><p>14 From complexity theory we have "IP=PSPACE" roughly: interactive proofs are as powerful as polynomial space algorithms, which is a strong statement about the power of interaction. [sent-35, score-0.655]
</p><p>15 Several other variations of interactive settings have been proposed and analyzed. [sent-38, score-0.661]
</p><p>16 There are plenty of kinds of natural interaction which haven't been formalized or analyzed. [sent-45, score-0.528]
</p><p>17 Many people doing machine learning want to reach AI, and it seems clear that any AI must engage in interactive learning. [sent-48, score-0.801]
</p><p>18 Basic QuestionsFor natural interaction form [insert yours here], how do you learn? [sent-50, score-0.474]
</p><p>19 Some of the techniques for other methods of interactive learning may be helpful. [sent-51, score-0.685]
</p><p>20 Are there general methods for reducing interactive learning problems to supervised learning problems (which we know better)? [sent-54, score-1.238]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('interactive', 0.547), ('interaction', 0.474), ('supervised', 0.19), ('learning', 0.138), ('choosing', 0.137), ('adversary', 0.126), ('settings', 0.114), ('ad', 0.104), ('mdp', 0.1), ('examples', 0.096), ('policies', 0.091), ('contextual', 0.086), ('bandit', 0.084), ('active', 0.084), ('know', 0.083), ('learn', 0.082), ('ai', 0.078), ('define', 0.076), ('essentially', 0.073), ('problems', 0.071), ('except', 0.068), ('familiar', 0.068), ('set', 0.067), ('upon', 0.066), ('label', 0.064), ('samples', 0.063), ('include', 0.063), ('amongst', 0.063), ('arising', 0.062), ('memorize', 0.062), ('avrim', 0.062), ('blend', 0.062), ('mastering', 0.062), ('online', 0.061), ('machine', 0.058), ('engage', 0.058), ('receives', 0.058), ('noninteractive', 0.058), ('semisupervised', 0.058), ('formalized', 0.054), ('noted', 0.054), ('paradigm', 0.054), ('requirement', 0.054), ('weaker', 0.054), ('ip', 0.054), ('proofs', 0.054), ('polynomial', 0.054), ('throw', 0.054), ('bandits', 0.054), ('closer', 0.054)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="293-tfidf-1" href="../hunch_net-2008/hunch_net-2008-03-23-Interactive_Machine_Learning.html">293 hunch net-2008-03-23-Interactive Machine Learning</a></p>
<p>Introduction: A new direction of research seems to be arising in machine learning:
Interactive Machine Learning. This isn't a familiar term, although it does
include some familiar subjects.What is Interactive Machine Learning?The
fundamental requirement is (a) learning algorithms which interact with the
world and (b) learn.For our purposes, let's define learning as efficiently
competing with a large set of possible predictors. Examples include:Online
learning against an adversary (Avrim's Notes). The interaction is almost
trivial: the learning algorithm makes a prediction and then receives feedback.
The learning is choosing based upon the advice of many experts.Active
Learning. In active learning, the interaction is choosing which examples to
label, and the learning is choosing from amongst a large set of
hypotheses.Contextual Bandits. The interaction is choosing one of several
actions and learning only the value of the chosen action (weaker than active
learning feedback).More forms of interaction w</p><p>2 0.21532929 <a title="293-tfidf-2" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>Introduction: Consider the contextual bandit setting where, repeatedly:A contextxis
observed.An actionais taken given the contextx.A rewardris observed, dependent
onxanda.Where the goal of a learning agent is to find a policy for step 2
achieving a large expected reward.This setting is of obvious importance,
because in the real world we typically make decisions based on some set of
information and then get feedback only about the single action taken. It also
fundamentally differs from supervised learning settings because knowing the
value of one action is not equivalent to knowing the value of all actions.A
decade ago the best machine learning techniques for this setting where
implausibly inefficient.Dean Fosteronce told me he thought the area was a
research sinkhole with little progress to be expected. Now we are on the verge
of being able to routinely attack these problems, in almost exactly the same
sense that we routinely attack bread and butter supervised learning problems.
Just as for supervis</p><p>3 0.19676545 <a title="293-tfidf-3" href="../hunch_net-2007/hunch_net-2007-09-30-NIPS_workshops_are_out..html">264 hunch net-2007-09-30-NIPS workshops are out.</a></p>
<p>Introduction: Here. I'm particularly interested in theWeb Search,Efficient ML, and (of
course)Learning Problem Designworkshops but there are many others to check out
as well. Workshops are a great chance to make progress on or learn about a
topic. Relevance and interaction amongst diverse people can sometimes be
magical.</p><p>4 0.18767452 <a title="293-tfidf-4" href="../hunch_net-2007/hunch_net-2007-10-24-Contextual_Bandits.html">269 hunch net-2007-10-24-Contextual Bandits</a></p>
<p>Introduction: One of the fundamental underpinnings of the internet is advertising based
content. This has become much more effective due to targeted advertising where
ads are specifically matched to interests. Everyone is familiar with this,
because everyone uses search engines and all search engines try to make money
this way.The problem of matching ads to interests is a natural machine
learning problem in some ways since there is much information in who clicks on
what. A fundamental problem with this information is that it is not supervised
--in particular a click-or-not on one ad doesn't generally tell you if a
different ad would have been clicked on. This implies we have a fundamental
exploration problem.A standard mathematical setting for this situation is
"k-Armed Bandits", often with various relevant embellishments. Thek-Armed
Bandit setting works on a round-by-round basis. On each round:A policy chooses
armafrom1ofkarms (i.e. 1 of k ads).The world reveals the rewardraof the chosen
arm (i.e.</p><p>5 0.15865964 <a title="293-tfidf-5" href="../hunch_net-2007/hunch_net-2007-12-19-Cool_and_interesting_things_seen_at_NIPS.html">279 hunch net-2007-12-19-Cool and interesting things seen at NIPS</a></p>
<p>Introduction: I learned a number of things atNIPS.The financial people were there in greater
force than previously.Two Sigmasponsored NIPS whileDRW Tradinghad a
booth.Theadversarial machine learning workshophad a number of talks about
interesting applications where an adversary really is out to try and mess up
your learning algorithm. This is very different from the situation we often
think of where the world is oblivious to our learning. This may present new
and convincing applications for the learning-against-an-adversary work common
atCOLT.There were several interesing papers.Sanjoy Dasgupta,Daniel Hsu,
andClaire Monteleonihad a paper onGeneral Agnostic Active Learning. The basic
idea is that active learning can be done via reduction to a form of supervised
learning problem. This is great, because we have many supervised learning
algorithms from which the benefits of active learning may be derived.Joseph
BradleyandRobert Schapirehad aPaper on Filterboost. Filterboost is an online
boosting algorit</p><p>6 0.15602273 <a title="293-tfidf-6" href="../hunch_net-2009/hunch_net-2009-11-06-Yisong_Yue_on_Self-improving_Systems.html">376 hunch net-2009-11-06-Yisong Yue on Self-improving Systems</a></p>
<p>7 0.15121907 <a title="293-tfidf-7" href="../hunch_net-2010/hunch_net-2010-09-17-New_York_Area_Machine_Learning_Events.html">410 hunch net-2010-09-17-New York Area Machine Learning Events</a></p>
<p>8 0.15001628 <a title="293-tfidf-8" href="../hunch_net-2009/hunch_net-2009-10-26-NIPS_workshops.html">375 hunch net-2009-10-26-NIPS workshops</a></p>
<p>9 0.14257137 <a title="293-tfidf-9" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>10 0.14033712 <a title="293-tfidf-10" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<p>11 0.13417883 <a title="293-tfidf-11" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>12 0.12066361 <a title="293-tfidf-12" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>13 0.11987065 <a title="293-tfidf-13" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>14 0.11974323 <a title="293-tfidf-14" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>15 0.11000274 <a title="293-tfidf-15" href="../hunch_net-2009/hunch_net-2009-12-24-Top_graduates_this_season.html">384 hunch net-2009-12-24-Top graduates this season</a></p>
<p>16 0.1082537 <a title="293-tfidf-16" href="../hunch_net-2008/hunch_net-2008-09-12-How_do_we_get_weak_action_dependence_for_learning_with_partial_observations%3F.html">317 hunch net-2008-09-12-How do we get weak action dependence for learning with partial observations?</a></p>
<p>17 0.10593294 <a title="293-tfidf-17" href="../hunch_net-2009/hunch_net-2009-05-08-Computability_in_Artificial_Intelligence.html">353 hunch net-2009-05-08-Computability in Artificial Intelligence</a></p>
<p>18 0.10577464 <a title="293-tfidf-18" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>19 0.10009694 <a title="293-tfidf-19" href="../hunch_net-2005/hunch_net-2005-09-08-Online_Learning_as_the_Mathematics_of_Accountability.html">109 hunch net-2005-09-08-Online Learning as the Mathematics of Accountability</a></p>
<p>20 0.099917054 <a title="293-tfidf-20" href="../hunch_net-2010/hunch_net-2010-01-13-Sam_Roweis_died.html">386 hunch net-2010-01-13-Sam Roweis died</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.24), (1, -0.102), (2, 0.08), (3, 0.016), (4, -0.106), (5, -0.016), (6, 0.041), (7, -0.021), (8, -0.146), (9, -0.108), (10, -0.043), (11, 0.023), (12, 0.19), (13, -0.006), (14, 0.067), (15, -0.029), (16, -0.149), (17, 0.04), (18, -0.032), (19, 0.007), (20, 0.072), (21, 0.039), (22, -0.03), (23, -0.034), (24, 0.022), (25, -0.017), (26, -0.002), (27, 0.001), (28, -0.028), (29, -0.116), (30, 0.044), (31, -0.072), (32, -0.056), (33, -0.0), (34, 0.063), (35, 0.091), (36, 0.014), (37, -0.036), (38, 0.04), (39, 0.031), (40, 0.019), (41, 0.115), (42, 0.083), (43, -0.043), (44, 0.153), (45, -0.095), (46, -0.011), (47, 0.036), (48, -0.012), (49, -0.074)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.92128938 <a title="293-lsi-1" href="../hunch_net-2008/hunch_net-2008-03-23-Interactive_Machine_Learning.html">293 hunch net-2008-03-23-Interactive Machine Learning</a></p>
<p>Introduction: A new direction of research seems to be arising in machine learning:
Interactive Machine Learning. This isn't a familiar term, although it does
include some familiar subjects.What is Interactive Machine Learning?The
fundamental requirement is (a) learning algorithms which interact with the
world and (b) learn.For our purposes, let's define learning as efficiently
competing with a large set of possible predictors. Examples include:Online
learning against an adversary (Avrim's Notes). The interaction is almost
trivial: the learning algorithm makes a prediction and then receives feedback.
The learning is choosing based upon the advice of many experts.Active
Learning. In active learning, the interaction is choosing which examples to
label, and the learning is choosing from amongst a large set of
hypotheses.Contextual Bandits. The interaction is choosing one of several
actions and learning only the value of the chosen action (weaker than active
learning feedback).More forms of interaction w</p><p>2 0.80951279 <a title="293-lsi-2" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>Introduction: Consider the contextual bandit setting where, repeatedly:A contextxis
observed.An actionais taken given the contextx.A rewardris observed, dependent
onxanda.Where the goal of a learning agent is to find a policy for step 2
achieving a large expected reward.This setting is of obvious importance,
because in the real world we typically make decisions based on some set of
information and then get feedback only about the single action taken. It also
fundamentally differs from supervised learning settings because knowing the
value of one action is not equivalent to knowing the value of all actions.A
decade ago the best machine learning techniques for this setting where
implausibly inefficient.Dean Fosteronce told me he thought the area was a
research sinkhole with little progress to be expected. Now we are on the verge
of being able to routinely attack these problems, in almost exactly the same
sense that we routinely attack bread and butter supervised learning problems.
Just as for supervis</p><p>3 0.70169371 <a title="293-lsi-3" href="../hunch_net-2007/hunch_net-2007-10-24-Contextual_Bandits.html">269 hunch net-2007-10-24-Contextual Bandits</a></p>
<p>Introduction: One of the fundamental underpinnings of the internet is advertising based
content. This has become much more effective due to targeted advertising where
ads are specifically matched to interests. Everyone is familiar with this,
because everyone uses search engines and all search engines try to make money
this way.The problem of matching ads to interests is a natural machine
learning problem in some ways since there is much information in who clicks on
what. A fundamental problem with this information is that it is not supervised
--in particular a click-or-not on one ad doesn't generally tell you if a
different ad would have been clicked on. This implies we have a fundamental
exploration problem.A standard mathematical setting for this situation is
"k-Armed Bandits", often with various relevant embellishments. Thek-Armed
Bandit setting works on a round-by-round basis. On each round:A policy chooses
armafrom1ofkarms (i.e. 1 of k ads).The world reveals the rewardraof the chosen
arm (i.e.</p><p>4 0.64756227 <a title="293-lsi-4" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>Introduction: A little over 4 years ago,Sanjoymade a postsaying roughly "we should study
active learning theoretically, because not much is understood".At the time, we
did not understand basic things such as whether or not it was possible to PAC-
learn with an active algorithm without making strong assumptions about the
noise rate. In other words, the fundamental question was "can we do it?"The
nature of the question has fundamentally changed in my mind. The answer is to
the previous question is "yes", both information theoretically and
computationally, most places where supervised learning could be applied.In
many situation, the question has now changed to: "is it worth it?" Is the
programming and computational overhead low enough to make the label cost
savings of active learning worthwhile? Currently, there are situations where
this question could go either way. Much of the challenge for the future is in
figuring out how to make active learning easier or more worthwhile.At
theactive learning tutor</p><p>5 0.64754385 <a title="293-lsi-5" href="../hunch_net-2009/hunch_net-2009-11-06-Yisong_Yue_on_Self-improving_Systems.html">376 hunch net-2009-11-06-Yisong Yue on Self-improving Systems</a></p>
<p>Introduction: I'd like to point outYisong Yue'spost on Self-improving systems, which is a
nicely readable description of the necessity and potential of interactive
learning to deal with the information overload problem that is endemic to the
modern internet.</p><p>6 0.64327031 <a title="293-lsi-6" href="../hunch_net-2009/hunch_net-2009-12-24-Top_graduates_this_season.html">384 hunch net-2009-12-24-Top graduates this season</a></p>
<p>7 0.6427412 <a title="293-lsi-7" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<p>8 0.63432258 <a title="293-lsi-8" href="../hunch_net-2008/hunch_net-2008-04-27-Watchword%3A_Supervised_Learning.html">299 hunch net-2008-04-27-Watchword: Supervised Learning</a></p>
<p>9 0.617235 <a title="293-lsi-9" href="../hunch_net-2007/hunch_net-2007-12-19-Cool_and_interesting_things_seen_at_NIPS.html">279 hunch net-2007-12-19-Cool and interesting things seen at NIPS</a></p>
<p>10 0.60362458 <a title="293-lsi-10" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>11 0.60284936 <a title="293-lsi-11" href="../hunch_net-2007/hunch_net-2007-07-01-Watchword%3A_Online_Learning.html">252 hunch net-2007-07-01-Watchword: Online Learning</a></p>
<p>12 0.602745 <a title="293-lsi-12" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>13 0.59937954 <a title="293-lsi-13" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>14 0.58871633 <a title="293-lsi-14" href="../hunch_net-2008/hunch_net-2008-07-26-Compositional_Machine_Learning_Algorithm_Design.html">311 hunch net-2008-07-26-Compositional Machine Learning Algorithm Design</a></p>
<p>15 0.58400124 <a title="293-lsi-15" href="../hunch_net-2007/hunch_net-2007-09-30-NIPS_workshops_are_out..html">264 hunch net-2007-09-30-NIPS workshops are out.</a></p>
<p>16 0.57940644 <a title="293-lsi-16" href="../hunch_net-2006/hunch_net-2006-04-02-Mad_%28Neuro%29science.html">168 hunch net-2006-04-02-Mad (Neuro)science</a></p>
<p>17 0.57727039 <a title="293-lsi-17" href="../hunch_net-2009/hunch_net-2009-10-26-NIPS_workshops.html">375 hunch net-2009-10-26-NIPS workshops</a></p>
<p>18 0.5728637 <a title="293-lsi-18" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>19 0.57222366 <a title="293-lsi-19" href="../hunch_net-2010/hunch_net-2010-01-13-Sam_Roweis_died.html">386 hunch net-2010-01-13-Sam Roweis died</a></p>
<p>20 0.56911701 <a title="293-lsi-20" href="../hunch_net-2008/hunch_net-2008-08-24-Mass_Customized_Medicine_in_the_Future%3F.html">314 hunch net-2008-08-24-Mass Customized Medicine in the Future?</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(2, 0.02), (6, 0.014), (7, 0.01), (35, 0.034), (38, 0.047), (39, 0.011), (42, 0.303), (45, 0.022), (68, 0.049), (74, 0.094), (82, 0.028), (95, 0.056), (98, 0.204)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.96991652 <a title="293-lda-1" href="../hunch_net-2008/hunch_net-2008-06-09-The_Minimum_Sample_Complexity_of_Importance_Weighting.html">303 hunch net-2008-06-09-The Minimum Sample Complexity of Importance Weighting</a></p>
<p>Introduction: This post is about a trick that I learned fromDale Schuurmanswhich has been
repeatedly useful for me over time.The basic trick has to do with importance
weighting for monte carlo integration. Consider the problem of finding:N = Ex
~ Df(x)given samples fromDand knowledge off.Often, we don't have samples
fromDavailable. Instead, we must make do with samples from some other
distributionQ. In that case, we can still often solve the problem, as long as
Q(x) isn't 0 when D(x) is nonzero, using the importance weighting formula:Ex ~
Qf(x) D(x)/Q(x)A basic question is: How many samples fromQare required in
order to estimateNto some precision? In general the convergence rate is not
bounded, becausef(x) D(x)/Q(x)is not bounded given the
assumptions.Nevertheless, there is one special valueQ(x) = f(x) D(x) / Nwhere
the sample complexity turns out to be1, which is typically substantially
better than the sample complexity of the original problem.This observation
underlies the motivation for voluntary</p><p>2 0.96407002 <a title="293-lda-2" href="../hunch_net-2005/hunch_net-2005-12-22-Yes_%2C_I_am_applying.html">142 hunch net-2005-12-22-Yes , I am applying</a></p>
<p>Introduction: Every year about now hundreds of applicants apply for a research/teaching job
with the timing governed by the university recruitment schedule. This time,
it's my turn--the hat's in the ring, I am a contender, etcâ&euro;Ś What I have heard
is that this year is good in both directions--both an increased supply and an
increased demand for machine learning expertise.I consider this post a bit of
an abuse as it is neither about general research nor machine learning. Please
forgive me this once.My hope is that I will learn about new places interested
in funding basic research--it's easy to imagine that I have overlooked
possibilities.I am not dogmatic about where I end up in any particular way.
Several earlier posts detail what I think of as a good research environment,
so I will avoid a repeat. A few more details seem important:Application. There
is often a tension between basic research and immediate application. This
tension is not as strong as might be expected in my case. As evidence, many of</p><p>3 0.93643743 <a title="293-lda-3" href="../hunch_net-2005/hunch_net-2005-11-28-A_question_of_quantification.html">133 hunch net-2005-11-28-A question of quantification</a></p>
<p>Introduction: This is about methods for phrasing and think about the scope of some theorems
in learning theory. The basic claim is that there are several different ways
of quantifying the scope which sound different yet are essentially the
same.For all sequences of examples. This is the standard quantification in
online learning analysis. Standard theorems would say something like "for all
sequences of predictions by experts, the algorithm A will perform almost as
well as the best expert."For all training sets. This is the standard
quantification for boosting analysis such asadaboostormulticlass
boosting.Standard theorems have the form "for all training sets the error rate
inequalities … hold".For all distributions over examples. This is the one that
we have been using for reductions analysis. Standard theorem statements have
the form "For all distributions over examples, the error rate inequalities …
hold".It is not quite true that each of these is equivalent. For example, in
the online learning se</p><p>same-blog 4 0.92931473 <a title="293-lda-4" href="../hunch_net-2008/hunch_net-2008-03-23-Interactive_Machine_Learning.html">293 hunch net-2008-03-23-Interactive Machine Learning</a></p>
<p>Introduction: A new direction of research seems to be arising in machine learning:
Interactive Machine Learning. This isn't a familiar term, although it does
include some familiar subjects.What is Interactive Machine Learning?The
fundamental requirement is (a) learning algorithms which interact with the
world and (b) learn.For our purposes, let's define learning as efficiently
competing with a large set of possible predictors. Examples include:Online
learning against an adversary (Avrim's Notes). The interaction is almost
trivial: the learning algorithm makes a prediction and then receives feedback.
The learning is choosing based upon the advice of many experts.Active
Learning. In active learning, the interaction is choosing which examples to
label, and the learning is choosing from amongst a large set of
hypotheses.Contextual Bandits. The interaction is choosing one of several
actions and learning only the value of the chosen action (weaker than active
learning feedback).More forms of interaction w</p><p>5 0.92532164 <a title="293-lda-5" href="../hunch_net-2005/hunch_net-2005-01-24-Holy_grails_of_machine_learning%3F.html">2 hunch net-2005-01-24-Holy grails of machine learning?</a></p>
<p>Introduction: Let me kick things off by posing this question to ML researchers:What do you
think are some important holy grails of machine learning?For example:- "A
classifier with SVM-level performance but much more scalable"- "Practical
confidence bounds (or learning bounds) for classification"- "A reinforcement
learning algorithm that can handle the ___ problem"- "Understanding
theoretically why ___ works so well in practice"etc.I pose this question
because I believe that when goals are stated explicitly and well (thus
providing clarity as well as opening up the problems to more people), rather
than left implicit, they are likely to be achieved much more quickly. I would
also like to know more about the internal goals of the various machine
learning sub-areas (theory, kernel methods, graphical models, reinforcement
learning, etc) as stated by people in these respective areas. This could help
people cross sub-areas.</p><p>6 0.86049426 <a title="293-lda-6" href="../hunch_net-2013/hunch_net-2013-11-21-Ben_Taskar_is_gone.html">491 hunch net-2013-11-21-Ben Taskar is gone</a></p>
<p>7 0.85287583 <a title="293-lda-7" href="../hunch_net-2007/hunch_net-2007-06-24-Interesting_Papers_at_ICML_2007.html">251 hunch net-2007-06-24-Interesting Papers at ICML 2007</a></p>
<p>8 0.84506959 <a title="293-lda-8" href="../hunch_net-2010/hunch_net-2010-06-13-The_Good_News_on_Exploration_and_Learning.html">400 hunch net-2010-06-13-The Good News on Exploration and Learning</a></p>
<p>9 0.84162402 <a title="293-lda-9" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>10 0.8410843 <a title="293-lda-10" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>11 0.84108204 <a title="293-lda-11" href="../hunch_net-2006/hunch_net-2006-04-05-What_is_state%3F.html">169 hunch net-2006-04-05-What is state?</a></p>
<p>12 0.8394587 <a title="293-lda-12" href="../hunch_net-2009/hunch_net-2009-12-24-Top_graduates_this_season.html">384 hunch net-2009-12-24-Top graduates this season</a></p>
<p>13 0.83931124 <a title="293-lda-13" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>14 0.83925086 <a title="293-lda-14" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>15 0.83387744 <a title="293-lda-15" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>16 0.83384502 <a title="293-lda-16" href="../hunch_net-2007/hunch_net-2007-01-10-A_Deep_Belief_Net_Learning_Problem.html">227 hunch net-2007-01-10-A Deep Belief Net Learning Problem</a></p>
<p>17 0.83040464 <a title="293-lda-17" href="../hunch_net-2010/hunch_net-2010-03-15-The_Efficient_Robust_Conditional_Probability_Estimation_Problem.html">391 hunch net-2010-03-15-The Efficient Robust Conditional Probability Estimation Problem</a></p>
<p>18 0.83002228 <a title="293-lda-18" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>19 0.8296752 <a title="293-lda-19" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>20 0.82927901 <a title="293-lda-20" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
