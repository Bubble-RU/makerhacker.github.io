<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>313 hunch net-2008-08-18-Radford Neal starts a blog</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2008" href="../home/hunch_net-2008_home.html">hunch_net-2008</a> <a title="hunch_net-2008-313" href="#">hunch_net-2008-313</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>313 hunch net-2008-08-18-Radford Neal starts a blog</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2008-313-html" href="http://hunch.net/?p=398">html</a></p><p>Introduction: here  on statistics, ML, CS, and other things he knows well.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 here  on statistics, ML, CS, and other things he knows well. [sent-1, score-0.791]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('knows', 0.586), ('cs', 0.54), ('statistics', 0.418), ('ml', 0.354), ('things', 0.205), ('well', 0.152)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="313-tfidf-1" href="../hunch_net-2008/hunch_net-2008-08-18-Radford_Neal_starts_a_blog.html">313 hunch net-2008-08-18-Radford Neal starts a blog</a></p>
<p>Introduction: here  on statistics, ML, CS, and other things he knows well.</p><p>2 0.27753794 <a title="313-tfidf-2" href="../hunch_net-2005/hunch_net-2005-04-22-New_Blog%3A_%5BLowerbounds%2CUpperbounds%5D.html">59 hunch net-2005-04-22-New Blog: [Lowerbounds,Upperbounds]</a></p>
<p>Introduction: Maverick Woo and the Aladdin group at CMU have started a CS theory-related blog  here .</p><p>3 0.13603322 <a title="313-tfidf-3" href="../hunch_net-2007/hunch_net-2007-01-15-The_Machine_Learning_Department.html">228 hunch net-2007-01-15-The Machine Learning Department</a></p>
<p>Introduction: Carnegie Mellon   School of Computer Science  has the first academic  Machine Learning department .  This department already existed as the  Center for Automated Learning and Discovery , but recently changed it’s name.  
 
The reason for changing the name is obvious: very few people think of themselves as “Automated Learner and Discoverers”, but there are number of people who think of themselves as “Machine Learners”.  Machine learning is both more succinct and recognizable—good properties for a name.
 
A more interesting question is “Should there be a Machine Learning Department?”.    Tom Mitchell  has a relevant  whitepaper  claiming that machine learning  is answering a different question than other fields or departments.  The fundamental debate here is “Is machine learning different from statistics?”  
 
At a cultural level, there is no real debate: they are different.  Machine learning is characterized by several very active large peer reviewed conferences, operating in a computer</p><p>4 0.12670222 <a title="313-tfidf-4" href="../hunch_net-2008/hunch_net-2008-11-28-A_Bumper_Crop_of_Machine_Learning_Graduates.html">329 hunch net-2008-11-28-A Bumper Crop of Machine Learning Graduates</a></p>
<p>Introduction: My impression is that this is a particularly strong year for machine learning graduates.  Here’s my short list of the strong graduates I know.  Analpha (for perversity’s sake) by last name:
  
  Jenn Wortmann . When Jenn visited us for the summer, she had  one ,  two ,  three ,  four  papers.  That is typical—she’s smart, capable, and follows up many directions of research.  I believe approximately all of her many papers are on different subjects. 
  Ruslan Salakhutdinov . A  Science paper on bijective dimensionality reduction , mastered and improved on deep belief nets which seems like an important flavor of nonlinear learning, and in my experience he’s very fast, capable and creative at problem solving. 
  Marc’Aurelio Ranzato .  I haven’t spoken with Marc very much, but he had a great visit at Yahoo! this summer, and has an impressive portfolio of applications and improvements on convolutional neural networks and other deep learning algorithms. 
  Lihong Li .  Lihong developed the</p><p>5 0.11645415 <a title="313-tfidf-5" href="../hunch_net-2009/hunch_net-2009-06-01-Multitask_Poisoning.html">358 hunch net-2009-06-01-Multitask Poisoning</a></p>
<p>Introduction: There are many ways that interesting research gets done.  For example it’s common at a conference for someone to discuss a problem with a partial solution, and for someone else to know how to solve a piece of it, resulting in a paper.  In some sense,  these are the easiest results we can achieve, so we should ask: Can all research be this easy?  
 
The answer is certainly no for fields where research inherently requires  experimentation to discover how the real world works.  However, mathematics, including parts of physics, computer science, statistics, etc… which are effectively mathematics don’t require experimentation. In effect, a paper can be simply a pure expression of thinking.  Can all mathematical-style research be this easy?
 
What’s going on here is research-by-communication.  Someone knows something, someone knows something else, and as soon as someone knows both things, a problem is solved.  The interesting thing about research-by-communication is that it is becoming radic</p><p>6 0.11569185 <a title="313-tfidf-6" href="../hunch_net-2009/hunch_net-2009-08-27-New_York_Area_Machine_Learning_Events.html">369 hunch net-2009-08-27-New York Area Machine Learning Events</a></p>
<p>7 0.10962685 <a title="313-tfidf-7" href="../hunch_net-2007/hunch_net-2007-06-19-How_is_Compressed_Sensing_going_to_change_Machine_Learning_%3F.html">248 hunch net-2007-06-19-How is Compressed Sensing going to change Machine Learning ?</a></p>
<p>8 0.10921868 <a title="313-tfidf-8" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>9 0.086072087 <a title="313-tfidf-9" href="../hunch_net-2006/hunch_net-2006-03-17-Multitask_learning_is_Black-Boxable.html">164 hunch net-2006-03-17-Multitask learning is Black-Boxable</a></p>
<p>10 0.084978893 <a title="313-tfidf-10" href="../hunch_net-2008/hunch_net-2008-02-27-The_Stats_Handicap.html">290 hunch net-2008-02-27-The Stats Handicap</a></p>
<p>11 0.080305427 <a title="313-tfidf-11" href="../hunch_net-2013/hunch_net-2013-03-22-I%26%238217%3Bm_a_bandit.html">480 hunch net-2013-03-22-I&#8217;m a bandit</a></p>
<p>12 0.074260876 <a title="313-tfidf-12" href="../hunch_net-2011/hunch_net-2011-10-24-2011_ML_symposium_and_the_bears.html">448 hunch net-2011-10-24-2011 ML symposium and the bears</a></p>
<p>13 0.071704373 <a title="313-tfidf-13" href="../hunch_net-2007/hunch_net-2007-11-02-The_Machine_Learning_Award_goes_to_%26%238230%3B.html">270 hunch net-2007-11-02-The Machine Learning Award goes to &#8230;</a></p>
<p>14 0.067268573 <a title="313-tfidf-14" href="../hunch_net-2005/hunch_net-2005-01-24-Holy_grails_of_machine_learning%3F.html">2 hunch net-2005-01-24-Holy grails of machine learning?</a></p>
<p>15 0.066739708 <a title="313-tfidf-15" href="../hunch_net-2010/hunch_net-2010-09-28-Machined_Learnings.html">412 hunch net-2010-09-28-Machined Learnings</a></p>
<p>16 0.063461214 <a title="313-tfidf-16" href="../hunch_net-2010/hunch_net-2010-07-02-MetaOptimize.html">402 hunch net-2010-07-02-MetaOptimize</a></p>
<p>17 0.06098336 <a title="313-tfidf-17" href="../hunch_net-2007/hunch_net-2007-09-30-NIPS_workshops_are_out..html">264 hunch net-2007-09-30-NIPS workshops are out.</a></p>
<p>18 0.060624253 <a title="313-tfidf-18" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>19 0.060399823 <a title="313-tfidf-19" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<p>20 0.057531793 <a title="313-tfidf-20" href="../hunch_net-2006/hunch_net-2006-02-11-Yahoo%26%238217%3Bs_Learning_Problems..html">156 hunch net-2006-02-11-Yahoo&#8217;s Learning Problems.</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.056), (1, -0.018), (2, -0.063), (3, 0.026), (4, -0.021), (5, 0.003), (6, -0.004), (7, -0.112), (8, -0.016), (9, -0.116), (10, 0.038), (11, 0.017), (12, 0.082), (13, -0.021), (14, -0.043), (15, 0.051), (16, 0.084), (17, 0.054), (18, 0.019), (19, 0.007), (20, 0.063), (21, 0.004), (22, 0.049), (23, 0.075), (24, 0.038), (25, -0.192), (26, -0.004), (27, -0.017), (28, -0.035), (29, 0.075), (30, -0.047), (31, -0.022), (32, 0.043), (33, -0.003), (34, -0.033), (35, 0.092), (36, -0.044), (37, -0.056), (38, 0.094), (39, -0.037), (40, -0.019), (41, 0.07), (42, 0.098), (43, 0.085), (44, 0.064), (45, -0.142), (46, 0.121), (47, 0.1), (48, 0.032), (49, 0.066)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99027383 <a title="313-lsi-1" href="../hunch_net-2008/hunch_net-2008-08-18-Radford_Neal_starts_a_blog.html">313 hunch net-2008-08-18-Radford Neal starts a blog</a></p>
<p>Introduction: here  on statistics, ML, CS, and other things he knows well.</p><p>2 0.5565719 <a title="313-lsi-2" href="../hunch_net-2005/hunch_net-2005-04-22-New_Blog%3A_%5BLowerbounds%2CUpperbounds%5D.html">59 hunch net-2005-04-22-New Blog: [Lowerbounds,Upperbounds]</a></p>
<p>Introduction: Maverick Woo and the Aladdin group at CMU have started a CS theory-related blog  here .</p><p>3 0.55610383 <a title="313-lsi-3" href="../hunch_net-2009/hunch_net-2009-09-29-Machine_Learning_Protests_at_the_G20.html">372 hunch net-2009-09-29-Machine Learning Protests at the G20</a></p>
<p>Introduction: The  machine learning department at CMU  turned out en masse to protest the G20 summit in Pittsburgh.    Arthur Gretton  uploaded some  great photos  covering the event</p><p>4 0.5487299 <a title="313-lsi-4" href="../hunch_net-2010/hunch_net-2010-07-02-MetaOptimize.html">402 hunch net-2010-07-02-MetaOptimize</a></p>
<p>Introduction: Joseph Turian  creates  MetaOptimize  for discussion of NLP and ML on big datasets.  This includes a  blog , but perhaps more importantly a  question and answer section .  Iâ&euro;&trade;m hopeful it will take off.</p><p>5 0.52350843 <a title="313-lsi-5" href="../hunch_net-2008/hunch_net-2008-02-27-The_Stats_Handicap.html">290 hunch net-2008-02-27-The Stats Handicap</a></p>
<p>Introduction: Graduating students in Statistics appear to be at a substantial handicap compared to graduating students in Machine Learning, despite being in substantially overlapping subjects.
 
The problem seems to be cultural.  Statistics comes from a mathematics background which emphasizes large publications slowly published under review at journals.  Machine Learning comes from a Computer Science background which emphasizes quick publishing at reviewed conferences.  This has a number of implications:
  
 Graduating statistics PhDs often have 0-2 publications while graduating machine learning PhDs might have 5-15. 
 Graduating ML students have had a chance for others to build on their work.  Stats students have had no such chance. 
 Graduating ML students have attended a number of conferences and presented their work, giving them a chance to meet people.  Stats students have had fewer chances of this sort. 
  
In short, Stats students have had relatively few chances to distinguish themselves and</p><p>6 0.49140489 <a title="313-lsi-6" href="../hunch_net-2010/hunch_net-2010-09-28-Machined_Learnings.html">412 hunch net-2010-09-28-Machined Learnings</a></p>
<p>7 0.47296709 <a title="313-lsi-7" href="../hunch_net-2010/hunch_net-2010-08-21-Rob_Schapire_at_NYC_ML_Meetup.html">405 hunch net-2010-08-21-Rob Schapire at NYC ML Meetup</a></p>
<p>8 0.46368733 <a title="313-lsi-8" href="../hunch_net-2007/hunch_net-2007-01-15-The_Machine_Learning_Department.html">228 hunch net-2007-01-15-The Machine Learning Department</a></p>
<p>9 0.42662594 <a title="313-lsi-9" href="../hunch_net-2011/hunch_net-2011-10-24-2011_ML_symposium_and_the_bears.html">448 hunch net-2011-10-24-2011 ML symposium and the bears</a></p>
<p>10 0.41620633 <a title="313-lsi-10" href="../hunch_net-2005/hunch_net-2005-10-08-We_have_a_winner.html">119 hunch net-2005-10-08-We have a winner</a></p>
<p>11 0.41186506 <a title="313-lsi-11" href="../hunch_net-2006/hunch_net-2006-03-17-Multitask_learning_is_Black-Boxable.html">164 hunch net-2006-03-17-Multitask learning is Black-Boxable</a></p>
<p>12 0.40008584 <a title="313-lsi-12" href="../hunch_net-2009/hunch_net-2009-08-27-New_York_Area_Machine_Learning_Events.html">369 hunch net-2009-08-27-New York Area Machine Learning Events</a></p>
<p>13 0.38730028 <a title="313-lsi-13" href="../hunch_net-2010/hunch_net-2010-09-17-New_York_Area_Machine_Learning_Events.html">410 hunch net-2010-09-17-New York Area Machine Learning Events</a></p>
<p>14 0.38347316 <a title="313-lsi-14" href="../hunch_net-2007/hunch_net-2007-06-19-How_is_Compressed_Sensing_going_to_change_Machine_Learning_%3F.html">248 hunch net-2007-06-19-How is Compressed Sensing going to change Machine Learning ?</a></p>
<p>15 0.36820871 <a title="313-lsi-15" href="../hunch_net-2013/hunch_net-2013-03-22-I%26%238217%3Bm_a_bandit.html">480 hunch net-2013-03-22-I&#8217;m a bandit</a></p>
<p>16 0.36343572 <a title="313-lsi-16" href="../hunch_net-2010/hunch_net-2010-10-17-Partha_Niyogi_has_died.html">414 hunch net-2010-10-17-Partha Niyogi has died</a></p>
<p>17 0.34593892 <a title="313-lsi-17" href="../hunch_net-2008/hunch_net-2008-09-04-Fall_ML_Conferences.html">316 hunch net-2008-09-04-Fall ML Conferences</a></p>
<p>18 0.33918536 <a title="313-lsi-18" href="../hunch_net-2007/hunch_net-2007-11-02-The_Machine_Learning_Award_goes_to_%26%238230%3B.html">270 hunch net-2007-11-02-The Machine Learning Award goes to &#8230;</a></p>
<p>19 0.33535337 <a title="313-lsi-19" href="../hunch_net-2012/hunch_net-2012-07-09-Videolectures.html">469 hunch net-2012-07-09-Videolectures</a></p>
<p>20 0.3142485 <a title="313-lsi-20" href="../hunch_net-2005/hunch_net-2005-02-04-JMLG.html">13 hunch net-2005-02-04-JMLG</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(17, 0.696)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.90275437 <a title="313-lda-1" href="../hunch_net-2008/hunch_net-2008-08-18-Radford_Neal_starts_a_blog.html">313 hunch net-2008-08-18-Radford Neal starts a blog</a></p>
<p>Introduction: here  on statistics, ML, CS, and other things he knows well.</p><p>2 0.53682739 <a title="313-lda-2" href="../hunch_net-2009/hunch_net-2009-11-09-NYAS_ML_Symposium_this_year..html">377 hunch net-2009-11-09-NYAS ML Symposium this year.</a></p>
<p>Introduction: The  NYAS ML symposium  grew again this year to 170 participants, despite the need to outsmart or otherwise tunnel through  a crowd .  
 
Perhaps the most distinct talk was by Bob Bell on various aspects of the  Netflix prize  competition.  I also enjoyed several student posters including  Matt Hoffman ‘s cool examples of blind source separation for music.
 
I’m somewhat surprised how much the workshop has grown, as it is now comparable in size to a small conference, although in style more similar to a workshop.  At some point as an event grows, it becomes owned by the community rather than the organizers, so if anyone has suggestions on improving it, speak up and be heard.</p><p>3 0.324352 <a title="313-lda-3" href="../hunch_net-2009/hunch_net-2009-08-03-Carbon_in_Computer_Science_Research.html">366 hunch net-2009-08-03-Carbon in Computer Science Research</a></p>
<p>Introduction: Al Gore ‘s  film  and gradually more assertive and thorough science has managed to mostly shift the debate on climate change from “Is it happening?” to “What should be done?”  In that context, it’s worthwhile to think a bit about what can be done within computer science research.
 
There are two things we can think about:
  
  Doing Research  At a cartoon level, computer science research consists of some combination of commuting to&from; work, writing programs, running them on computers, writing papers, and presenting them at conferences.  A typical computer has a power usage on the order of 100 Watts, which works out to 2.4 kiloWatt-hours/day.  Looking up  David MacKay ‘s  reference on power usage per person , it becomes clear that this is a relatively minor part of the lifestyle, although it could become substantial if many more computers are required.  Much larger costs are associated with commuting (which is in common with many people) and attending conferences.  Since local commuti</p><p>4 0.24740344 <a title="313-lda-4" href="../hunch_net-2007/hunch_net-2007-07-06-Idempotent-capable_Predictors.html">253 hunch net-2007-07-06-Idempotent-capable Predictors</a></p>
<p>Introduction: One way to distinguish different learning algorithms is by their ability or inability to easily use an input variable as the predicted output.  This is desirable for at least two reasons:
  
  Modularity   If we want to build complex learning systems via reuse of a subsystem, it’s important to have compatible I/O. 
  “Prior” knowledge   Machine learning is often applied in situations where we do have some knowledge of what the right solution is, often in the form of an existing system.  In such situations, it’s good to start with a learning algorithm that can be at least as good as any existing system. 
  
When doing classification, most learning algorithms can do this.  For example, a decision tree can split on a feature, and then classify.  The real differences come up when we attempt regression.  Many of the algorithms we know and commonly use are not idempotent predictors.
  
 Logistic regressors can not be idempotent, because all input features are mapped through a nonlinearity.</p><p>5 0.19448395 <a title="313-lda-5" href="../hunch_net-2005/hunch_net-2005-12-27-Automated_Labeling.html">143 hunch net-2005-12-27-Automated Labeling</a></p>
<p>Introduction: One of the common trends in machine learning has been an emphasis on the use of unlabeled data.  The argument goes something like “there aren’t many labeled web pages out there, but there are a  huge  number of web pages, so we must find a way to take advantage of them.”  There are several standard approaches for doing this:
  
  Unsupervised Learning .  You use only unlabeled data.  In a typical application, you cluster the data and hope that the clusters somehow correspond to what you care about. 
 Semisupervised Learning.  You use both unlabeled and labeled data to build a predictor.  The unlabeled data influences the learned predictor in some way. 
  Active Learning . You have unlabeled data and access to a labeling oracle.  You interactively choose which examples to label so as to optimize prediction accuracy. 
  
It seems there is a fourth approach worth serious investigation—automated labeling.  The approach goes as follows:
  
 Identify some subset of observed values to predict</p><p>6 0.063432902 <a title="313-lda-6" href="../hunch_net-2010/hunch_net-2010-10-28-NY_ML_Symposium_2010.html">415 hunch net-2010-10-28-NY ML Symposium 2010</a></p>
<p>7 0.041515503 <a title="313-lda-7" href="../hunch_net-2007/hunch_net-2007-03-15-Alternative_Machine_Learning_Reductions_Definitions.html">236 hunch net-2007-03-15-Alternative Machine Learning Reductions Definitions</a></p>
<p>8 0.036967706 <a title="313-lda-8" href="../hunch_net-2010/hunch_net-2010-12-04-Vowpal_Wabbit%2C_version_5.0%2C_and_the_second_heresy.html">419 hunch net-2010-12-04-Vowpal Wabbit, version 5.0, and the second heresy</a></p>
<p>9 0.033036854 <a title="313-lda-9" href="../hunch_net-2005/hunch_net-2005-12-09-Some_NIPS_papers.html">138 hunch net-2005-12-09-Some NIPS papers</a></p>
<p>10 0.030902753 <a title="313-lda-10" href="../hunch_net-2008/hunch_net-2008-11-28-A_Bumper_Crop_of_Machine_Learning_Graduates.html">329 hunch net-2008-11-28-A Bumper Crop of Machine Learning Graduates</a></p>
<p>11 0.027723921 <a title="313-lda-11" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>12 0.0 <a title="313-lda-12" href="../hunch_net-2005/hunch_net-2005-01-19-Why_I_decided_to_run_a_weblog..html">1 hunch net-2005-01-19-Why I decided to run a weblog.</a></p>
<p>13 0.0 <a title="313-lda-13" href="../hunch_net-2005/hunch_net-2005-01-24-Holy_grails_of_machine_learning%3F.html">2 hunch net-2005-01-24-Holy grails of machine learning?</a></p>
<p>14 0.0 <a title="313-lda-14" href="../hunch_net-2005/hunch_net-2005-01-24-The_Humanloop_Spectrum_of_Machine_Learning.html">3 hunch net-2005-01-24-The Humanloop Spectrum of Machine Learning</a></p>
<p>15 0.0 <a title="313-lda-15" href="../hunch_net-2005/hunch_net-2005-01-26-Summer_Schools.html">4 hunch net-2005-01-26-Summer Schools</a></p>
<p>16 0.0 <a title="313-lda-16" href="../hunch_net-2005/hunch_net-2005-01-26-Watchword%3A_Probability.html">5 hunch net-2005-01-26-Watchword: Probability</a></p>
<p>17 0.0 <a title="313-lda-17" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>18 0.0 <a title="313-lda-18" href="../hunch_net-2005/hunch_net-2005-01-31-Watchword%3A_Assumption.html">7 hunch net-2005-01-31-Watchword: Assumption</a></p>
<p>19 0.0 <a title="313-lda-19" href="../hunch_net-2005/hunch_net-2005-02-01-NIPS%3A_Online_Bayes.html">8 hunch net-2005-02-01-NIPS: Online Bayes</a></p>
<p>20 0.0 <a title="313-lda-20" href="../hunch_net-2005/hunch_net-2005-02-01-Watchword%3A_Loss.html">9 hunch net-2005-02-01-Watchword: Loss</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
