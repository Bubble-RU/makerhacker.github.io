<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>327 hunch net-2008-11-16-Observations on Linearity for Reductions to Regression</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2008" href="../home/hunch_net-2008_home.html">hunch_net-2008</a> <a title="hunch_net-2008-327" href="#">hunch_net-2008-327</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>327 hunch net-2008-11-16-Observations on Linearity for Reductions to Regression</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2008-327-html" href="http://hunch.net/?p=468">html</a></p><p>Introduction: Dean FosterandDaniel Hsuhad a couple observations about reductions to
regression that I wanted to share. This will make the most sense for people
familiar with error correcting output codes (see thetutorial, page 11).Many
people are comfortable using linear regression in a one-against-all style,
where you try to predict the probability of choiceivs other classes, yet they
are not comfortable with more complex error correcting codes because they fear
that they create harder problems. This fear turns out to be mathematically
incoherent under a linear representation: comfort in the linear case should
imply comfort with more complex codes.In particular, If there exists a set of
weight vectorswisuch thatP(i|x)=, then for any invertible error
correcting output codeC, there exists weight vectorswcwhich decode to
perfectly predict the probability of each class. The proof is simple and
constructive: the weight vectorwccan be constructed according to the linear
superposition ofwiimplied b</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Dean FosterandDaniel Hsuhad a couple observations about reductions to regression that I wanted to share. [sent-1, score-0.458]
</p><p>2 This will make the most sense for people familiar with error correcting output codes (see thetutorial, page 11). [sent-2, score-0.997]
</p><p>3 Many people are comfortable using linear regression in a one-against-all style, where you try to predict the probability of choiceivs other classes, yet they are not comfortable with more complex error correcting codes because they fear that they create harder problems. [sent-3, score-2.147]
</p><p>4 This fear turns out to be mathematically incoherent under a linear representation: comfort in the linear case should imply comfort with more complex codes. [sent-4, score-1.645]
</p><p>5 In particular, If there exists a set of weight vectorswisuch thatP(i|x)=, then for any invertible error correcting output codeC, there exists weight vectorswcwhich decode to perfectly predict the probability of each class. [sent-5, score-1.603]
</p><p>6 The proof is simple and constructive: the weight vectorwccan be constructed according to the linear superposition ofwiimplied by the code, and invertibility implies that a correct encoding implies a correct decoding. [sent-6, score-1.172]
</p><p>7 This observation extends to all- pairs like codes which compare subsets of choices to subsets of choices using "don't cares". [sent-7, score-1.437]
</p><p>8 Using this observation, Daniel created a very short proof of the PECOC regret transform theorem (here, and Daniel'supdated version). [sent-8, score-0.268]
</p><p>9 One further observation is that under ridge regression (a special case of linear regression), for any code, there exists a setting of parameters such that you might as well use one-against-all instead, because you get the same answer numerically. [sent-9, score-1.033]
</p><p>10 The implication is that the advantages of codes more complex than one-against-all is confined to other prediction methods. [sent-10, score-0.701]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('codes', 0.387), ('regression', 0.266), ('comfort', 0.249), ('linear', 0.238), ('correcting', 0.235), ('weight', 0.211), ('comfortable', 0.193), ('subsets', 0.193), ('complex', 0.161), ('fear', 0.161), ('observation', 0.158), ('exists', 0.155), ('error', 0.132), ('choices', 0.129), ('proof', 0.123), ('output', 0.12), ('correct', 0.118), ('code', 0.113), ('cares', 0.111), ('dean', 0.111), ('pecoc', 0.111), ('incoherent', 0.111), ('invertible', 0.111), ('encoding', 0.103), ('thetutorial', 0.103), ('mathematically', 0.097), ('extends', 0.097), ('daniel', 0.097), ('probability', 0.096), ('perfectly', 0.092), ('constructed', 0.089), ('implies', 0.086), ('predict', 0.085), ('transform', 0.083), ('implication', 0.083), ('case', 0.081), ('constructive', 0.08), ('pairs', 0.078), ('compare', 0.073), ('classes', 0.072), ('special', 0.072), ('advantages', 0.07), ('couple', 0.064), ('observations', 0.064), ('wanted', 0.064), ('parameters', 0.063), ('page', 0.063), ('created', 0.062), ('imply', 0.06), ('familiar', 0.06)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="327-tfidf-1" href="../hunch_net-2008/hunch_net-2008-11-16-Observations_on_Linearity_for_Reductions_to_Regression.html">327 hunch net-2008-11-16-Observations on Linearity for Reductions to Regression</a></p>
<p>Introduction: Dean FosterandDaniel Hsuhad a couple observations about reductions to
regression that I wanted to share. This will make the most sense for people
familiar with error correcting output codes (see thetutorial, page 11).Many
people are comfortable using linear regression in a one-against-all style,
where you try to predict the probability of choiceivs other classes, yet they
are not comfortable with more complex error correcting codes because they fear
that they create harder problems. This fear turns out to be mathematically
incoherent under a linear representation: comfort in the linear case should
imply comfort with more complex codes.In particular, If there exists a set of
weight vectorswisuch thatP(i|x)=, then for any invertible error
correcting output codeC, there exists weight vectorswcwhich decode to
perfectly predict the probability of each class. The proof is simple and
constructive: the weight vectorwccan be constructed according to the linear
superposition ofwiimplied b</p><p>2 0.19624203 <a title="327-tfidf-2" href="../hunch_net-2006/hunch_net-2006-07-13-Regression_vs._Classification_as_a_Primitive.html">196 hunch net-2006-07-13-Regression vs. Classification as a Primitive</a></p>
<p>Introduction: Forlearning reductionswe have been concentrating on reducing various complex
learning problems to binary classification. This choice needs to be actively
questioned, because it was not carefully considered.Binary clasification is
learning a classifierc:X -> {0,1}so as to minimize the probability of being
wrong,Prx,y~D(c(x)y).The primary alternative candidate seems to be squared
error regression. In squared error regression, you learn a regressors:X ->
[0,1]so as to minimize squared error,Ex,y~D(s(x)-y)2.It is difficult to judge
one primitive against another. The judgement must at least partially be made
on nontheoretical grounds because (essentially) we are evaluating a choice
between two axioms/assumptions.These two primitives are significantly related.
Classification can be reduced to regression in the obvious way: you use the
regressor to predictD(y=1|x), then threshold at0.5. For this simple reduction
a squared error regret ofrimplies a classification regret of at mostr0.5.
Regress</p><p>3 0.18086611 <a title="327-tfidf-3" href="../hunch_net-2005/hunch_net-2005-05-16-Regret_minimizing_vs_error_limiting_reductions.html">72 hunch net-2005-05-16-Regret minimizing vs error limiting reductions</a></p>
<p>Introduction: This post is about a reductions-related problem that I find mysterious. There
are two kinds of reductions analysis currently under consideration.Error
limiting reductions. Here, the goal is to bound the error rate of the created
classifier in terms of the error rate of the binary classifiers that you
reduce to. A very simple example of this is thaterror correcting output
codeswhere it is possible to prove that for certain codes, the multiclass
error rate is at most 4 * the binary classifier error rate.Regret minimizing
reductions. Here, the goal is to bound theregretof the created classifier in
terms of theregretof the binary classifiers reduced to. The regret is the
error rate minus the minimum error rate. When the learning problem is noisy
the minimum error rate may not be0. An analagous result for reget is that for
aprobabilistic error correcting output code, multiclass regret is at most 4 *
(binary regret)0.5.The use of "regret" is more desirable than the use of error
rates, becaus</p><p>4 0.15311921 <a title="327-tfidf-4" href="../hunch_net-2006/hunch_net-2006-05-23-What_is_the_best_regret_transform_reduction_from_multiclass_to_binary%3F.html">181 hunch net-2006-05-23-What is the best regret transform reduction from multiclass to binary?</a></p>
<p>Introduction: This post is about an open problem in learning reductions.BackgroundA
reduction might transform a a multiclass prediction problem where there
arekpossible labels into a binary learning problem where there are only 2
possible labels. On this induced binary problem we might learn a binary
classifier with some error ratee. After subtracting the minimum possible
(Bayes) error rateb, we get a regretr = e - b. ThePECOC(Probabilistic Error
Correcting Output Code) reduction has the property that binary regretrimplies
multiclass regret at most4r0.5.The problemThis is not the "rightest" answer.
Consider thek=2case, where we reduce binary to binary. There exists a
reduction (the identity) with the property that regretrimplies regretr. This
is substantially superior to the transform given by the PECOC reduction, which
suggests that a better reduction may exist for generalk. For example, we can
not rule out the possibility that a reductionRexists with regret transform
guaranteeing binary regretrimp</p><p>5 0.15274243 <a title="327-tfidf-5" href="../hunch_net-2009/hunch_net-2009-01-21-Nearly_all_natural_problems_require_nonlinearity.html">337 hunch net-2009-01-21-Nearly all natural problems require nonlinearity</a></p>
<p>Introduction: One conventional wisdom is that learning algorithms with linear
representations are sufficient to solve natural learning problems. This
conventional wisdom appears unsupported by empirical evidence as far as I can
tell. In nearly all vision, language, robotics, and speech applications I know
where machine learning is effectively applied, the approach involves either a
linear representation on hand crafted features capturing substantial
nonlinearities or learning directly on nonlinear representations.There are a
few exceptions to this--for example, if the problem of interest to you is
predicting the next word given previous words, n-gram methods have been shown
effective. Viewed the right way, n-gram methods are essentially linear
predictors on an enormous sparse feature space, learned from an enormous
number of examples. Hal's postheredescribes some of this in more detail.In
contrast, if you go to a machine learning conference, a large number of the
new algorithms are variations of lea</p><p>6 0.1457724 <a title="327-tfidf-6" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>7 0.11199825 <a title="327-tfidf-7" href="../hunch_net-2007/hunch_net-2007-01-10-A_Deep_Belief_Net_Learning_Problem.html">227 hunch net-2007-01-10-A Deep Belief Net Learning Problem</a></p>
<p>8 0.097716905 <a title="327-tfidf-8" href="../hunch_net-2007/hunch_net-2007-09-18-It%26%238217%3Bs_MDL_Jim%2C_but_not_as_we_know_it%26%238230%3B%28on_Bayes%2C_MDL_and_consistency%29.html">263 hunch net-2007-09-18-It&#8217;s MDL Jim, but not as we know it&#8230;(on Bayes, MDL and consistency)</a></p>
<p>9 0.097663455 <a title="327-tfidf-9" href="../hunch_net-2010/hunch_net-2010-03-15-The_Efficient_Robust_Conditional_Probability_Estimation_Problem.html">391 hunch net-2010-03-15-The Efficient Robust Conditional Probability Estimation Problem</a></p>
<p>10 0.096038036 <a title="327-tfidf-10" href="../hunch_net-2009/hunch_net-2009-06-03-Functionally_defined_Nonlinear_Dynamic_Models.html">359 hunch net-2009-06-03-Functionally defined Nonlinear Dynamic Models</a></p>
<p>11 0.092553735 <a title="327-tfidf-11" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>12 0.091294669 <a title="327-tfidf-12" href="../hunch_net-2005/hunch_net-2005-12-14-More_NIPS_Papers_II.html">140 hunch net-2005-12-14-More NIPS Papers II</a></p>
<p>13 0.089180067 <a title="327-tfidf-13" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>14 0.083965987 <a title="327-tfidf-14" href="../hunch_net-2008/hunch_net-2008-07-06-To_Dual_or_Not.html">308 hunch net-2008-07-06-To Dual or Not</a></p>
<p>15 0.081338122 <a title="327-tfidf-15" href="../hunch_net-2009/hunch_net-2009-06-24-Interesting_papers_at_UAICMOLT_2009.html">361 hunch net-2009-06-24-Interesting papers at UAICMOLT 2009</a></p>
<p>16 0.080966629 <a title="327-tfidf-16" href="../hunch_net-2010/hunch_net-2010-05-10-Aggregation_of_estimators%2C_sparsity_in_high_dimension_and_computational_feasibility.html">398 hunch net-2010-05-10-Aggregation of estimators, sparsity in high dimension and computational feasibility</a></p>
<p>17 0.07864017 <a title="327-tfidf-17" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>18 0.078304119 <a title="327-tfidf-18" href="../hunch_net-2011/hunch_net-2011-09-07-KDD_and_MUCMD_2011.html">444 hunch net-2011-09-07-KDD and MUCMD 2011</a></p>
<p>19 0.072993398 <a title="327-tfidf-19" href="../hunch_net-2006/hunch_net-2006-06-25-Presentation_of_Proofs_is_Hard..html">187 hunch net-2006-06-25-Presentation of Proofs is Hard.</a></p>
<p>20 0.071455404 <a title="327-tfidf-20" href="../hunch_net-2013/hunch_net-2013-12-01-NIPS_tutorials_and_Vowpal_Wabbit_7.4.html">492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.143), (1, -0.137), (2, -0.068), (3, 0.028), (4, -0.001), (5, -0.098), (6, 0.068), (7, 0.037), (8, 0.113), (9, 0.098), (10, 0.098), (11, 0.028), (12, -0.019), (13, -0.008), (14, -0.045), (15, -0.039), (16, 0.042), (17, 0.043), (18, -0.077), (19, -0.053), (20, -0.009), (21, -0.031), (22, 0.018), (23, 0.035), (24, 0.115), (25, 0.023), (26, -0.023), (27, 0.016), (28, -0.049), (29, -0.002), (30, 0.034), (31, 0.033), (32, -0.063), (33, -0.128), (34, -0.028), (35, -0.003), (36, 0.019), (37, 0.055), (38, 0.073), (39, 0.056), (40, 0.106), (41, 0.012), (42, 0.03), (43, 0.044), (44, 0.002), (45, 0.066), (46, 0.054), (47, -0.116), (48, 0.031), (49, -0.029)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98903835 <a title="327-lsi-1" href="../hunch_net-2008/hunch_net-2008-11-16-Observations_on_Linearity_for_Reductions_to_Regression.html">327 hunch net-2008-11-16-Observations on Linearity for Reductions to Regression</a></p>
<p>Introduction: Dean FosterandDaniel Hsuhad a couple observations about reductions to
regression that I wanted to share. This will make the most sense for people
familiar with error correcting output codes (see thetutorial, page 11).Many
people are comfortable using linear regression in a one-against-all style,
where you try to predict the probability of choiceivs other classes, yet they
are not comfortable with more complex error correcting codes because they fear
that they create harder problems. This fear turns out to be mathematically
incoherent under a linear representation: comfort in the linear case should
imply comfort with more complex codes.In particular, If there exists a set of
weight vectorswisuch thatP(i|x)=, then for any invertible error
correcting output codeC, there exists weight vectorswcwhich decode to
perfectly predict the probability of each class. The proof is simple and
constructive: the weight vectorwccan be constructed according to the linear
superposition ofwiimplied b</p><p>2 0.64451283 <a title="327-lsi-2" href="../hunch_net-2006/hunch_net-2006-05-23-What_is_the_best_regret_transform_reduction_from_multiclass_to_binary%3F.html">181 hunch net-2006-05-23-What is the best regret transform reduction from multiclass to binary?</a></p>
<p>Introduction: This post is about an open problem in learning reductions.BackgroundA
reduction might transform a a multiclass prediction problem where there
arekpossible labels into a binary learning problem where there are only 2
possible labels. On this induced binary problem we might learn a binary
classifier with some error ratee. After subtracting the minimum possible
(Bayes) error rateb, we get a regretr = e - b. ThePECOC(Probabilistic Error
Correcting Output Code) reduction has the property that binary regretrimplies
multiclass regret at most4r0.5.The problemThis is not the "rightest" answer.
Consider thek=2case, where we reduce binary to binary. There exists a
reduction (the identity) with the property that regretrimplies regretr. This
is substantially superior to the transform given by the PECOC reduction, which
suggests that a better reduction may exist for generalk. For example, we can
not rule out the possibility that a reductionRexists with regret transform
guaranteeing binary regretrimp</p><p>3 0.63050371 <a title="327-lsi-3" href="../hunch_net-2006/hunch_net-2006-07-13-Regression_vs._Classification_as_a_Primitive.html">196 hunch net-2006-07-13-Regression vs. Classification as a Primitive</a></p>
<p>Introduction: Forlearning reductionswe have been concentrating on reducing various complex
learning problems to binary classification. This choice needs to be actively
questioned, because it was not carefully considered.Binary clasification is
learning a classifierc:X -> {0,1}so as to minimize the probability of being
wrong,Prx,y~D(c(x)y).The primary alternative candidate seems to be squared
error regression. In squared error regression, you learn a regressors:X ->
[0,1]so as to minimize squared error,Ex,y~D(s(x)-y)2.It is difficult to judge
one primitive against another. The judgement must at least partially be made
on nontheoretical grounds because (essentially) we are evaluating a choice
between two axioms/assumptions.These two primitives are significantly related.
Classification can be reduced to regression in the obvious way: you use the
regressor to predictD(y=1|x), then threshold at0.5. For this simple reduction
a squared error regret ofrimplies a classification regret of at mostr0.5.
Regress</p><p>4 0.61794788 <a title="327-lsi-4" href="../hunch_net-2010/hunch_net-2010-03-15-The_Efficient_Robust_Conditional_Probability_Estimation_Problem.html">391 hunch net-2010-03-15-The Efficient Robust Conditional Probability Estimation Problem</a></p>
<p>Introduction: I'm offering a reward of $1000 for a solution to this problem. This joins
thecross validation problemwhich I'm offering a$500 rewardfor. I believe both
of these problems are hard but plausibly solvable, and plausibly with a
solution of substantial practical value. While it's unlikely these rewards are
worth your time on an hourly wage basis, the recognition for solving them
definitely should beThe ProblemThe problem is finding a general, robust, and
efficient mechanism for estimating a conditional probabilityP(y|x)where
robustness and efficiency are measured using techniques from learning
reductions.In particular, suppose we have access to a binary regression
oracleBwhich has two interfaces--one for specifying training information and
one for testing. Training information is specified asB(x',y')wherex'is a
feature vector andy'is a scalar in[0,1]with no value returned. Testing is done
according toB(x')with a value in[0,1]returned.A learning reduction consists of
two algorithmsRandR-1whi</p><p>5 0.61720783 <a title="327-lsi-5" href="../hunch_net-2005/hunch_net-2005-05-16-Regret_minimizing_vs_error_limiting_reductions.html">72 hunch net-2005-05-16-Regret minimizing vs error limiting reductions</a></p>
<p>Introduction: This post is about a reductions-related problem that I find mysterious. There
are two kinds of reductions analysis currently under consideration.Error
limiting reductions. Here, the goal is to bound the error rate of the created
classifier in terms of the error rate of the binary classifiers that you
reduce to. A very simple example of this is thaterror correcting output
codeswhere it is possible to prove that for certain codes, the multiclass
error rate is at most 4 * the binary classifier error rate.Regret minimizing
reductions. Here, the goal is to bound theregretof the created classifier in
terms of theregretof the binary classifiers reduced to. The regret is the
error rate minus the minimum error rate. When the learning problem is noisy
the minimum error rate may not be0. An analagous result for reget is that for
aprobabilistic error correcting output code, multiclass regret is at most 4 *
(binary regret)0.5.The use of "regret" is more desirable than the use of error
rates, becaus</p><p>6 0.59480137 <a title="327-lsi-6" href="../hunch_net-2007/hunch_net-2007-05-08-Conditional_Tournaments_for_Multiclass_to_Binary.html">243 hunch net-2007-05-08-Conditional Tournaments for Multiclass to Binary</a></p>
<p>7 0.51001793 <a title="327-lsi-7" href="../hunch_net-2008/hunch_net-2008-04-26-Eliminating_the_Birthday_Paradox_for_Universal_Features.html">298 hunch net-2008-04-26-Eliminating the Birthday Paradox for Universal Features</a></p>
<p>8 0.50787354 <a title="327-lsi-8" href="../hunch_net-2007/hunch_net-2007-01-10-A_Deep_Belief_Net_Learning_Problem.html">227 hunch net-2007-01-10-A Deep Belief Net Learning Problem</a></p>
<p>9 0.45952833 <a title="327-lsi-9" href="../hunch_net-2009/hunch_net-2009-01-21-Nearly_all_natural_problems_require_nonlinearity.html">337 hunch net-2009-01-21-Nearly all natural problems require nonlinearity</a></p>
<p>10 0.449305 <a title="327-lsi-10" href="../hunch_net-2008/hunch_net-2008-06-09-The_Minimum_Sample_Complexity_of_Importance_Weighting.html">303 hunch net-2008-06-09-The Minimum Sample Complexity of Importance Weighting</a></p>
<p>11 0.44514126 <a title="327-lsi-11" href="../hunch_net-2007/hunch_net-2007-03-15-Alternative_Machine_Learning_Reductions_Definitions.html">236 hunch net-2007-03-15-Alternative Machine Learning Reductions Definitions</a></p>
<p>12 0.44160461 <a title="327-lsi-12" href="../hunch_net-2005/hunch_net-2005-02-07-The_State_of_the_Reduction.html">14 hunch net-2005-02-07-The State of the Reduction</a></p>
<p>13 0.44125202 <a title="327-lsi-13" href="../hunch_net-2006/hunch_net-2006-09-09-How_to_solve_an_NP_hard_problem_in_quadratic_time.html">206 hunch net-2006-09-09-How to solve an NP hard problem in quadratic time</a></p>
<p>14 0.43420812 <a title="327-lsi-14" href="../hunch_net-2010/hunch_net-2010-10-08-An_easy_proof_of_the_Chernoff-Hoeffding_bound.html">413 hunch net-2010-10-08-An easy proof of the Chernoff-Hoeffding bound</a></p>
<p>15 0.42539051 <a title="327-lsi-15" href="../hunch_net-2005/hunch_net-2005-06-06-Exact_Online_Learning_for_Classification.html">78 hunch net-2005-06-06-Exact Online Learning for Classification</a></p>
<p>16 0.41438931 <a title="327-lsi-16" href="../hunch_net-2005/hunch_net-2005-05-12-Math_on_the_Web.html">70 hunch net-2005-05-12-Math on the Web</a></p>
<p>17 0.40905857 <a title="327-lsi-17" href="../hunch_net-2006/hunch_net-2006-01-18-Is_Multitask_Learning_Black-Boxable%3F.html">149 hunch net-2006-01-18-Is Multitask Learning Black-Boxable?</a></p>
<p>18 0.40475041 <a title="327-lsi-18" href="../hunch_net-2006/hunch_net-2006-01-23-On_Coding_via_Mutual_Information_%26%23038%3B_Bayes_Nets.html">150 hunch net-2006-01-23-On Coding via Mutual Information &#038; Bayes Nets</a></p>
<p>19 0.40342194 <a title="327-lsi-19" href="../hunch_net-2005/hunch_net-2005-03-04-The_Big_O_and_Constants_in_Learning.html">35 hunch net-2005-03-04-The Big O and Constants in Learning</a></p>
<p>20 0.40064234 <a title="327-lsi-20" href="../hunch_net-2006/hunch_net-2006-11-20-Context_and_the_calculation_misperception.html">218 hunch net-2006-11-20-Context and the calculation misperception</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(6, 0.039), (35, 0.062), (42, 0.259), (65, 0.34), (67, 0.033), (68, 0.108), (74, 0.05)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.93391985 <a title="327-lda-1" href="../hunch_net-2008/hunch_net-2008-11-16-Observations_on_Linearity_for_Reductions_to_Regression.html">327 hunch net-2008-11-16-Observations on Linearity for Reductions to Regression</a></p>
<p>Introduction: Dean FosterandDaniel Hsuhad a couple observations about reductions to
regression that I wanted to share. This will make the most sense for people
familiar with error correcting output codes (see thetutorial, page 11).Many
people are comfortable using linear regression in a one-against-all style,
where you try to predict the probability of choiceivs other classes, yet they
are not comfortable with more complex error correcting codes because they fear
that they create harder problems. This fear turns out to be mathematically
incoherent under a linear representation: comfort in the linear case should
imply comfort with more complex codes.In particular, If there exists a set of
weight vectorswisuch thatP(i|x)=, then for any invertible error
correcting output codeC, there exists weight vectorswcwhich decode to
perfectly predict the probability of each class. The proof is simple and
constructive: the weight vectorwccan be constructed according to the linear
superposition ofwiimplied b</p><p>2 0.91398764 <a title="327-lda-2" href="../hunch_net-2010/hunch_net-2010-07-02-MetaOptimize.html">402 hunch net-2010-07-02-MetaOptimize</a></p>
<p>Introduction: Joseph TuriancreatesMetaOptimizefor discussion of NLP and ML on big datasets.
This includes ablog, but perhaps more importantly aquestion and answer
section. I'm hopeful it will take off.</p><p>3 0.77214396 <a title="327-lda-3" href="../hunch_net-2005/hunch_net-2005-07-21-Six_Months.html">96 hunch net-2005-07-21-Six Months</a></p>
<p>Introduction: This is the6 month pointin the "run a research blog" experiment, so it seems
like a good point to take stock and assess.One fundamental question is: "Is it
worth it?" The idea of running a research blog will never become widely
popular and useful unless it actually aids research. On the negative side,
composing ideas for a post and maintaining a blog takes a significant amount
of time. On the positive side, the process might yield better research because
there is an opportunity for better, faster feedback implying better, faster
thinking.My answer at the moment is a provisional "yes". Running the blog has
been incidentally helpful in several ways:It is sometimes
educational.exampleMore often, the process of composing thoughts well enough
to post simply aids thinking. This has resulted in a couple solutions to
problems ofinterest(and perhaps more over time). If you really want to solve a
problem, letting the world know is helpful. This isn't necessarily because the
world will help you s</p><p>4 0.72851843 <a title="327-lda-4" href="../hunch_net-2009/hunch_net-2009-11-06-Yisong_Yue_on_Self-improving_Systems.html">376 hunch net-2009-11-06-Yisong Yue on Self-improving Systems</a></p>
<p>Introduction: I'd like to point outYisong Yue'spost on Self-improving systems, which is a
nicely readable description of the necessity and potential of interactive
learning to deal with the information overload problem that is endemic to the
modern internet.</p><p>5 0.64653093 <a title="327-lda-5" href="../hunch_net-2005/hunch_net-2005-05-16-Regret_minimizing_vs_error_limiting_reductions.html">72 hunch net-2005-05-16-Regret minimizing vs error limiting reductions</a></p>
<p>Introduction: This post is about a reductions-related problem that I find mysterious. There
are two kinds of reductions analysis currently under consideration.Error
limiting reductions. Here, the goal is to bound the error rate of the created
classifier in terms of the error rate of the binary classifiers that you
reduce to. A very simple example of this is thaterror correcting output
codeswhere it is possible to prove that for certain codes, the multiclass
error rate is at most 4 * the binary classifier error rate.Regret minimizing
reductions. Here, the goal is to bound theregretof the created classifier in
terms of theregretof the binary classifiers reduced to. The regret is the
error rate minus the minimum error rate. When the learning problem is noisy
the minimum error rate may not be0. An analagous result for reget is that for
aprobabilistic error correcting output code, multiclass regret is at most 4 *
(binary regret)0.5.The use of "regret" is more desirable than the use of error
rates, becaus</p><p>6 0.63927031 <a title="327-lda-6" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>7 0.63565767 <a title="327-lda-7" href="../hunch_net-2007/hunch_net-2007-08-12-Exponentiated_Gradient.html">258 hunch net-2007-08-12-Exponentiated Gradient</a></p>
<p>8 0.6349979 <a title="327-lda-8" href="../hunch_net-2005/hunch_net-2005-02-01-Watchword%3A_Loss.html">9 hunch net-2005-02-01-Watchword: Loss</a></p>
<p>9 0.63233006 <a title="327-lda-9" href="../hunch_net-2007/hunch_net-2007-01-10-A_Deep_Belief_Net_Learning_Problem.html">227 hunch net-2007-01-10-A Deep Belief Net Learning Problem</a></p>
<p>10 0.6312964 <a title="327-lda-10" href="../hunch_net-2006/hunch_net-2006-08-07-The_Call_of_the_Deep.html">201 hunch net-2006-08-07-The Call of the Deep</a></p>
<p>11 0.62559456 <a title="327-lda-11" href="../hunch_net-2005/hunch_net-2005-08-22-Do_you_believe_in_induction%3F.html">104 hunch net-2005-08-22-Do you believe in induction?</a></p>
<p>12 0.62489676 <a title="327-lda-12" href="../hunch_net-2007/hunch_net-2007-08-19-Choice_of_Metrics.html">259 hunch net-2007-08-19-Choice of Metrics</a></p>
<p>13 0.62438351 <a title="327-lda-13" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>14 0.62325442 <a title="327-lda-14" href="../hunch_net-2005/hunch_net-2005-09-10-%26%238220%3BFailure%26%238221%3B_is_an_option.html">110 hunch net-2005-09-10-&#8220;Failure&#8221; is an option</a></p>
<p>15 0.62229931 <a title="327-lda-15" href="../hunch_net-2005/hunch_net-2005-06-08-Question%3A_%26%238220%3BWhen_is_the_right_time_to_insert_the_loss_function%3F%26%238221%3B.html">79 hunch net-2005-06-08-Question: &#8220;When is the right time to insert the loss function?&#8221;</a></p>
<p>16 0.62002498 <a title="327-lda-16" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<p>17 0.61941814 <a title="327-lda-17" href="../hunch_net-2007/hunch_net-2007-11-28-Computational_Consequences_of_Classification.html">274 hunch net-2007-11-28-Computational Consequences of Classification</a></p>
<p>18 0.6183691 <a title="327-lda-18" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>19 0.6165458 <a title="327-lda-19" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>20 0.61559296 <a title="327-lda-20" href="../hunch_net-2006/hunch_net-2006-11-22-Explicit_Randomization_in_Learning_algorithms.html">219 hunch net-2006-11-22-Explicit Randomization in Learning algorithms</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
