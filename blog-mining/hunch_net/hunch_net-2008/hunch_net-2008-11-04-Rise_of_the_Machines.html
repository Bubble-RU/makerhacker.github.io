<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>323 hunch net-2008-11-04-Rise of the Machines</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2008" href="../home/hunch_net-2008_home.html">hunch_net-2008</a> <a title="hunch_net-2008-323" href="#">hunch_net-2008-323</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>323 hunch net-2008-11-04-Rise of the Machines</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2008-323-html" href="http://hunch.net/?p=454">html</a></p><p>Introduction: On the  enduring topic of how people deal with intelligent machines , we have this important  election bulletin .</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 On the  enduring topic of how people deal with intelligent machines , we have this important  election bulletin . [sent-1, score-2.72]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bulletin', 0.497), ('election', 0.461), ('enduring', 0.461), ('intelligent', 0.336), ('machines', 0.29), ('topic', 0.253), ('deal', 0.202), ('important', 0.143), ('people', 0.077)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="323-tfidf-1" href="../hunch_net-2008/hunch_net-2008-11-04-Rise_of_the_Machines.html">323 hunch net-2008-11-04-Rise of the Machines</a></p>
<p>Introduction: On the  enduring topic of how people deal with intelligent machines , we have this important  election bulletin .</p><p>2 0.1137107 <a title="323-tfidf-2" href="../hunch_net-2008/hunch_net-2008-04-12-It_Doesn%26%238217%3Bt_Stop.html">295 hunch net-2008-04-12-It Doesn&#8217;t Stop</a></p>
<p>Introduction: I’ve enjoyed the  Terminator  movies and show.  Neglecting the whacky aspects (time travel and associated paradoxes), there is an enduring topic of discussion: how do people deal with intelligent machines (and vice versa)?
 
In Terminator-land, the primary method for dealing with intelligent machines is to prevent them from being made.  This approach works pretty badly, because a new angle on building an intelligent machine keeps coming up.  This is partly a ploy for writer’s to avoid writing themselves out of a job, but there is a fundamental truth to it as well: preventing progress in research is hard.
 
The United States, has been experimenting with trying to stop research on  stem cells .  It hasn’t worked very well—the net effect has been retarding research programs a bit, and exporting some research to other countries.  Another less recent example was encryption technology, for which the United States generally did not encourage early public research and even  discouraged as a mu</p><p>3 0.06171684 <a title="323-tfidf-3" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>Introduction: The internet has significantly effected the way we do research but it’s capabilities have not yet been fully realized.
 
First, let’s acknowledge some known effects.
  
  Self-publishing  By default, all researchers in machine learning (and more generally computer science and physics) place their papers online for anyone to download.  The exact mechanism differs—physicists tend to use a central repository ( Arxiv ) while computer scientists tend to place the papers on their webpage.  Arxiv has been slowly growing in subject breadth so it now sometimes used by computer scientists. 
  Collaboration  Email has enabled working remotely with coauthors.  This has allowed collaborationis which would not otherwise have been possible and generally speeds research. 
  
Now, let’s look at attempts to go further.
  
  Blogs  (like this one) allow public discussion about topics which are not easily categorized as “a new idea in machine learning” (like this topic). 
  Organization  of some subfield</p><p>4 0.060114227 <a title="323-tfidf-4" href="../hunch_net-2008/hunch_net-2008-02-10-Complexity_Illness.html">288 hunch net-2008-02-10-Complexity Illness</a></p>
<p>Introduction: One of the enduring stereotypes of academia is that people spend a great deal of intelligence, time, and effort finding complexity rather than simplicity.  This is at least anecdotally true in my experience.
  
  Math++  Several people have found that adding useless math makes their paper more publishable as evidenced by a reject-add-accept sequence. 
  8 page minimum  Who submitted a paper to  ICML  violating the 8 page minimum?  Every author fears that the reviewers won’t take their work seriously unless the allowed length is fully used.  The best minimum violation I know is  Adam ‘s paper at SODA on  generating random factored numbers , but this is deeply exceptional.  It’s a fair bet that 90% of papers submitted are exactly at the page limit.  We could imagine that this is because papers naturally take more space, but few people seem to be clamoring for more space. 
  Journalong   Has anyone been asked to review a 100 page journal paper?  I have.  Journal papers can be nice, becaus</p><p>5 0.057870008 <a title="323-tfidf-5" href="../hunch_net-2006/hunch_net-2006-04-17-Rexa_is_live.html">173 hunch net-2006-04-17-Rexa is live</a></p>
<p>Introduction: Rexa  is now publicly available.  Anyone can create an account and login.  
 
Rexa is similar to  Citeseer  and  Google Scholar  in functionality with more emphasis on the use of machine learning for intelligent information extraction.   For example, Rexa can automatically display a picture on an authorâ&euro;&trade;s homepage when the author is searched for.</p><p>6 0.05684704 <a title="323-tfidf-6" href="../hunch_net-2011/hunch_net-2011-06-22-Ultra_LDA.html">436 hunch net-2011-06-22-Ultra LDA</a></p>
<p>7 0.049313232 <a title="323-tfidf-7" href="../hunch_net-2005/hunch_net-2005-01-26-Summer_Schools.html">4 hunch net-2005-01-26-Summer Schools</a></p>
<p>8 0.045524463 <a title="323-tfidf-8" href="../hunch_net-2007/hunch_net-2007-02-22-Create_Your_Own_ICML_Workshop.html">234 hunch net-2007-02-22-Create Your Own ICML Workshop</a></p>
<p>9 0.043466326 <a title="323-tfidf-9" href="../hunch_net-2005/hunch_net-2005-12-07-Is_the_Google_way_the_way_for_machine_learning%3F.html">136 hunch net-2005-12-07-Is the Google way the way for machine learning?</a></p>
<p>10 0.040677402 <a title="323-tfidf-10" href="../hunch_net-2012/hunch_net-2012-03-06-COLT-ICML_Open_Questions_and_ICML_Instructions.html">458 hunch net-2012-03-06-COLT-ICML Open Questions and ICML Instructions</a></p>
<p>11 0.039725058 <a title="323-tfidf-11" href="../hunch_net-2006/hunch_net-2006-10-04-Health_of_Conferences_Wiki.html">212 hunch net-2006-10-04-Health of Conferences Wiki</a></p>
<p>12 0.038538612 <a title="323-tfidf-12" href="../hunch_net-2005/hunch_net-2005-03-08-Fast_Physics_for_Learning.html">37 hunch net-2005-03-08-Fast Physics for Learning</a></p>
<p>13 0.036324911 <a title="323-tfidf-13" href="../hunch_net-2005/hunch_net-2005-09-04-Science_in_the_Government.html">106 hunch net-2005-09-04-Science in the Government</a></p>
<p>14 0.035963163 <a title="323-tfidf-14" href="../hunch_net-2009/hunch_net-2009-05-06-Machine_Learning_to_AI.html">352 hunch net-2009-05-06-Machine Learning to AI</a></p>
<p>15 0.034178387 <a title="323-tfidf-15" href="../hunch_net-2012/hunch_net-2012-07-09-Videolectures.html">469 hunch net-2012-07-09-Videolectures</a></p>
<p>16 0.034122992 <a title="323-tfidf-16" href="../hunch_net-2010/hunch_net-2010-09-28-Machined_Learnings.html">412 hunch net-2010-09-28-Machined Learnings</a></p>
<p>17 0.031893346 <a title="323-tfidf-17" href="../hunch_net-2011/hunch_net-2011-05-16-Research_Directions_for_Machine_Learning_and_Algorithms.html">435 hunch net-2011-05-16-Research Directions for Machine Learning and Algorithms</a></p>
<p>18 0.029823158 <a title="323-tfidf-18" href="../hunch_net-2009/hunch_net-2009-11-06-Yisong_Yue_on_Self-improving_Systems.html">376 hunch net-2009-11-06-Yisong Yue on Self-improving Systems</a></p>
<p>19 0.029160727 <a title="323-tfidf-19" href="../hunch_net-2005/hunch_net-2005-06-13-Wikis_for_Summer_Schools_and_Workshops.html">81 hunch net-2005-06-13-Wikis for Summer Schools and Workshops</a></p>
<p>20 0.029115023 <a title="323-tfidf-20" href="../hunch_net-2006/hunch_net-2006-11-22-Explicit_Randomization_in_Learning_algorithms.html">219 hunch net-2006-11-22-Explicit Randomization in Learning algorithms</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.035), (1, -0.008), (2, -0.017), (3, 0.031), (4, 0.002), (5, -0.004), (6, 0.004), (7, -0.006), (8, 0.016), (9, 0.002), (10, -0.031), (11, -0.018), (12, -0.002), (13, -0.012), (14, -0.002), (15, -0.014), (16, 0.008), (17, 0.025), (18, -0.004), (19, 0.001), (20, 0.01), (21, 0.031), (22, -0.056), (23, 0.04), (24, 0.024), (25, -0.034), (26, 0.02), (27, 0.032), (28, -0.023), (29, 0.026), (30, 0.008), (31, -0.044), (32, 0.068), (33, -0.06), (34, -0.01), (35, 0.028), (36, 0.017), (37, -0.014), (38, -0.009), (39, 0.033), (40, 0.019), (41, 0.044), (42, -0.057), (43, -0.071), (44, -0.046), (45, -0.008), (46, -0.028), (47, 0.009), (48, 0.001), (49, 0.009)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96231192 <a title="323-lsi-1" href="../hunch_net-2008/hunch_net-2008-11-04-Rise_of_the_Machines.html">323 hunch net-2008-11-04-Rise of the Machines</a></p>
<p>Introduction: On the  enduring topic of how people deal with intelligent machines , we have this important  election bulletin .</p><p>2 0.44913691 <a title="323-lsi-2" href="../hunch_net-2008/hunch_net-2008-04-12-It_Doesn%26%238217%3Bt_Stop.html">295 hunch net-2008-04-12-It Doesn&#8217;t Stop</a></p>
<p>Introduction: I’ve enjoyed the  Terminator  movies and show.  Neglecting the whacky aspects (time travel and associated paradoxes), there is an enduring topic of discussion: how do people deal with intelligent machines (and vice versa)?
 
In Terminator-land, the primary method for dealing with intelligent machines is to prevent them from being made.  This approach works pretty badly, because a new angle on building an intelligent machine keeps coming up.  This is partly a ploy for writer’s to avoid writing themselves out of a job, but there is a fundamental truth to it as well: preventing progress in research is hard.
 
The United States, has been experimenting with trying to stop research on  stem cells .  It hasn’t worked very well—the net effect has been retarding research programs a bit, and exporting some research to other countries.  Another less recent example was encryption technology, for which the United States generally did not encourage early public research and even  discouraged as a mu</p><p>3 0.44365627 <a title="323-lsi-3" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>Introduction: The internet has significantly effected the way we do research but it’s capabilities have not yet been fully realized.
 
First, let’s acknowledge some known effects.
  
  Self-publishing  By default, all researchers in machine learning (and more generally computer science and physics) place their papers online for anyone to download.  The exact mechanism differs—physicists tend to use a central repository ( Arxiv ) while computer scientists tend to place the papers on their webpage.  Arxiv has been slowly growing in subject breadth so it now sometimes used by computer scientists. 
  Collaboration  Email has enabled working remotely with coauthors.  This has allowed collaborationis which would not otherwise have been possible and generally speeds research. 
  
Now, let’s look at attempts to go further.
  
  Blogs  (like this one) allow public discussion about topics which are not easily categorized as “a new idea in machine learning” (like this topic). 
  Organization  of some subfield</p><p>4 0.42199686 <a title="323-lsi-4" href="../hunch_net-2006/hunch_net-2006-09-18-What_is_missing_for_online_collaborative_research%3F.html">208 hunch net-2006-09-18-What is missing for online collaborative research?</a></p>
<p>Introduction: The internet has recently made the research process much smoother: papers are easy to obtain, citations are easy to follow, and unpublished “tutorials” are often available. Yet, new research fields can look very complicated to outsiders or newcomers. Every paper is like a small piece of an unfinished jigsaw puzzle: to understand just one publication, a researcher without experience in the field will typically have to follow several layers of citations, and many of the papers he encounters have a great deal of repeated information. Furthermore, from one publication to the next, notation and terminology may not be consistent which can further confuse the reader.
 
But the internet is now proving to be an extremely useful medium for collaboration and knowledge aggregation. Online forums allow users to ask and answer questions and to share ideas. The recent phenomenon of Wikipedia provides a proof-of-concept for the “anyone can edit” system. Can such models be used to facilitate research a</p><p>5 0.40336493 <a title="323-lsi-5" href="../hunch_net-2006/hunch_net-2006-04-17-Rexa_is_live.html">173 hunch net-2006-04-17-Rexa is live</a></p>
<p>Introduction: Rexa  is now publicly available.  Anyone can create an account and login.  
 
Rexa is similar to  Citeseer  and  Google Scholar  in functionality with more emphasis on the use of machine learning for intelligent information extraction.   For example, Rexa can automatically display a picture on an authorâ&euro;&trade;s homepage when the author is searched for.</p><p>6 0.37299716 <a title="323-lsi-6" href="../hunch_net-2013/hunch_net-2013-12-01-NIPS_tutorials_and_Vowpal_Wabbit_7.4.html">492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</a></p>
<p>7 0.36661723 <a title="323-lsi-7" href="../hunch_net-2013/hunch_net-2013-11-21-Ben_Taskar_is_gone.html">491 hunch net-2013-11-21-Ben Taskar is gone</a></p>
<p>8 0.36445281 <a title="323-lsi-8" href="../hunch_net-2010/hunch_net-2010-01-13-Sam_Roweis_died.html">386 hunch net-2010-01-13-Sam Roweis died</a></p>
<p>9 0.35648474 <a title="323-lsi-9" href="../hunch_net-2009/hunch_net-2009-11-29-AI_Safety.html">380 hunch net-2009-11-29-AI Safety</a></p>
<p>10 0.34477943 <a title="323-lsi-10" href="../hunch_net-2007/hunch_net-2007-02-10-Best_Practices_for_Collaboration.html">231 hunch net-2007-02-10-Best Practices for Collaboration</a></p>
<p>11 0.34254947 <a title="323-lsi-11" href="../hunch_net-2010/hunch_net-2010-11-18-ICML_2011_%26%238211%3B_Call_for_Tutorials.html">417 hunch net-2010-11-18-ICML 2011 &#8211; Call for Tutorials</a></p>
<p>12 0.33986628 <a title="323-lsi-12" href="../hunch_net-2009/hunch_net-2009-12-07-Vowpal_Wabbit_version_4.0%2C_and_a_NIPS_heresy.html">381 hunch net-2009-12-07-Vowpal Wabbit version 4.0, and a NIPS heresy</a></p>
<p>13 0.33523062 <a title="323-lsi-13" href="../hunch_net-2009/hunch_net-2009-11-06-Yisong_Yue_on_Self-improving_Systems.html">376 hunch net-2009-11-06-Yisong Yue on Self-improving Systems</a></p>
<p>14 0.32493916 <a title="323-lsi-14" href="../hunch_net-2008/hunch_net-2008-11-26-Efficient_Reinforcement_Learning_in_MDPs.html">328 hunch net-2008-11-26-Efficient Reinforcement Learning in MDPs</a></p>
<p>15 0.31191692 <a title="323-lsi-15" href="../hunch_net-2006/hunch_net-2006-07-09-The_Stock_Prediction_Machine_Learning_Problem.html">193 hunch net-2006-07-09-The Stock Prediction Machine Learning Problem</a></p>
<p>16 0.31090775 <a title="323-lsi-16" href="../hunch_net-2005/hunch_net-2005-01-24-Holy_grails_of_machine_learning%3F.html">2 hunch net-2005-01-24-Holy grails of machine learning?</a></p>
<p>17 0.31033927 <a title="323-lsi-17" href="../hunch_net-2009/hunch_net-2009-05-08-Computability_in_Artificial_Intelligence.html">353 hunch net-2009-05-08-Computability in Artificial Intelligence</a></p>
<p>18 0.31029767 <a title="323-lsi-18" href="../hunch_net-2008/hunch_net-2008-02-10-Complexity_Illness.html">288 hunch net-2008-02-10-Complexity Illness</a></p>
<p>19 0.30905777 <a title="323-lsi-19" href="../hunch_net-2005/hunch_net-2005-12-11-More_NIPS_Papers.html">139 hunch net-2005-12-11-More NIPS Papers</a></p>
<p>20 0.30631351 <a title="323-lsi-20" href="../hunch_net-2005/hunch_net-2005-06-13-Wikis_for_Summer_Schools_and_Workshops.html">81 hunch net-2005-06-13-Wikis for Summer Schools and Workshops</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(90, 0.734)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.92294157 <a title="323-lda-1" href="../hunch_net-2008/hunch_net-2008-11-04-Rise_of_the_Machines.html">323 hunch net-2008-11-04-Rise of the Machines</a></p>
<p>Introduction: On the  enduring topic of how people deal with intelligent machines , we have this important  election bulletin .</p><p>2 0.89859974 <a title="323-lda-2" href="../hunch_net-2009/hunch_net-2009-01-28-Nielsen%26%238217%3Bs_talk.html">340 hunch net-2009-01-28-Nielsen&#8217;s talk</a></p>
<p>Introduction: I wanted to point to   Michael Nielsenâ&euro;&trade;s talk  about blogging science, which I found interesting.</p><p>3 0.78501713 <a title="323-lda-3" href="../hunch_net-2008/hunch_net-2008-01-18-Datasets.html">284 hunch net-2008-01-18-Datasets</a></p>
<p>Introduction: David Pennock  notes the impressive  set of datasets  at  datawrangling .</p><p>4 0.7049492 <a title="323-lda-4" href="../hunch_net-2005/hunch_net-2005-02-27-Antilearning%3A_When_proximity_goes_bad.html">32 hunch net-2005-02-27-Antilearning: When proximity goes bad</a></p>
<p>Introduction: Joel Predd   mentioned  “ Antilearning ” by  Adam Kowalczyk , which is interesting from a foundational intuitions viewpoint.
 
There is a pervasive intuition that “nearby things tend to have the same label”.  This intuition is instantiated in SVMs, nearest neighbor classifiers, decision trees, and neural networks.  It turns out there are natural problems where this intuition is opposite of the truth.
 
One natural situation where this occurs is in competition.   For example, when  Intel  fails to meet its earnings estimate, is this evidence that  AMD  is doing badly also?  Or evidence that AMD is doing well?
 
This violation of the proximity intuition means that when the number of examples is few,  negating  a classifier which attempts to exploit proximity can provide predictive power (thus, the term “antilearning”).</p><p>5 0.68387479 <a title="323-lda-5" href="../hunch_net-2007/hunch_net-2007-04-18-%2450K_Spock_Challenge.html">239 hunch net-2007-04-18-$50K Spock Challenge</a></p>
<p>Introduction: Apparently, the company  Spock  is setting up a  $50k entity resolution challenge .  $50k is much less than the Netflix challenge, but it’s effectively the same as Netflix until  someone reaches 10% .  It’s also nice that the Spock challenge has a short duration.  The (visible) test set is of size 25k and the training set has size 75k.</p><p>6 0.52578551 <a title="323-lda-6" href="../hunch_net-2005/hunch_net-2005-12-11-More_NIPS_Papers.html">139 hunch net-2005-12-11-More NIPS Papers</a></p>
<p>7 0.48276007 <a title="323-lda-7" href="../hunch_net-2005/hunch_net-2005-12-28-Yet_more_nips_thoughts.html">144 hunch net-2005-12-28-Yet more nips thoughts</a></p>
<p>8 0.28467268 <a title="323-lda-8" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>9 0.037052784 <a title="323-lda-9" href="../hunch_net-2011/hunch_net-2011-01-16-2011_Summer_Conference_Deadline_Season.html">422 hunch net-2011-01-16-2011 Summer Conference Deadline Season</a></p>
<p>10 0.032866511 <a title="323-lda-10" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>11 0.024599608 <a title="323-lda-11" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>12 0.0 <a title="323-lda-12" href="../hunch_net-2005/hunch_net-2005-01-19-Why_I_decided_to_run_a_weblog..html">1 hunch net-2005-01-19-Why I decided to run a weblog.</a></p>
<p>13 0.0 <a title="323-lda-13" href="../hunch_net-2005/hunch_net-2005-01-24-Holy_grails_of_machine_learning%3F.html">2 hunch net-2005-01-24-Holy grails of machine learning?</a></p>
<p>14 0.0 <a title="323-lda-14" href="../hunch_net-2005/hunch_net-2005-01-24-The_Humanloop_Spectrum_of_Machine_Learning.html">3 hunch net-2005-01-24-The Humanloop Spectrum of Machine Learning</a></p>
<p>15 0.0 <a title="323-lda-15" href="../hunch_net-2005/hunch_net-2005-01-26-Summer_Schools.html">4 hunch net-2005-01-26-Summer Schools</a></p>
<p>16 0.0 <a title="323-lda-16" href="../hunch_net-2005/hunch_net-2005-01-26-Watchword%3A_Probability.html">5 hunch net-2005-01-26-Watchword: Probability</a></p>
<p>17 0.0 <a title="323-lda-17" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>18 0.0 <a title="323-lda-18" href="../hunch_net-2005/hunch_net-2005-01-31-Watchword%3A_Assumption.html">7 hunch net-2005-01-31-Watchword: Assumption</a></p>
<p>19 0.0 <a title="323-lda-19" href="../hunch_net-2005/hunch_net-2005-02-01-NIPS%3A_Online_Bayes.html">8 hunch net-2005-02-01-NIPS: Online Bayes</a></p>
<p>20 0.0 <a title="323-lda-20" href="../hunch_net-2005/hunch_net-2005-02-01-Watchword%3A_Loss.html">9 hunch net-2005-02-01-Watchword: Loss</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
