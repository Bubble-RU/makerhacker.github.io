<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>312 hunch net-2008-08-04-Electoralmarkets.com</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2008" href="../home/hunch_net-2008_home.html">hunch_net-2008</a> <a title="hunch_net-2008-312" href="#">hunch_net-2008-312</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>312 hunch net-2008-08-04-Electoralmarkets.com</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2008-312-html" href="http://hunch.net/?p=396">html</a></p><p>Introduction: Lancereminded me aboutelectoralmarketstoday, which is cool enough that I want
to point it out explicitly here.Most people stilluse pollsto predict who wins,
while electoralmarkets uses people betting real money. They might use polling
information, but any other sources of information are implicitly also allowed.
A side-by-side comparison of how polls compare to prediction markets might be
fun in a few months.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Lancereminded me aboutelectoralmarketstoday, which is cool enough that I want to point it out explicitly here. [sent-1, score-0.788]
</p><p>2 Most people stilluse pollsto predict who wins, while electoralmarkets uses people betting real money. [sent-2, score-1.039]
</p><p>3 They might use polling information, but any other sources of information are implicitly also allowed. [sent-3, score-1.019]
</p><p>4 A side-by-side comparison of how polls compare to prediction markets might be fun in a few months. [sent-4, score-1.295]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('betting', 0.372), ('wins', 0.325), ('markets', 0.298), ('implicitly', 0.263), ('months', 0.257), ('compare', 0.246), ('fun', 0.236), ('comparison', 0.232), ('cool', 0.224), ('information', 0.223), ('sources', 0.22), ('explicitly', 0.199), ('uses', 0.175), ('might', 0.162), ('predict', 0.143), ('enough', 0.143), ('prediction', 0.121), ('people', 0.119), ('point', 0.113), ('real', 0.111), ('want', 0.109), ('use', 0.081), ('also', 0.07)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="312-tfidf-1" href="../hunch_net-2008/hunch_net-2008-08-04-Electoralmarkets.com.html">312 hunch net-2008-08-04-Electoralmarkets.com</a></p>
<p>Introduction: Lancereminded me aboutelectoralmarketstoday, which is cool enough that I want
to point it out explicitly here.Most people stilluse pollsto predict who wins,
while electoralmarkets uses people betting real money. They might use polling
information, but any other sources of information are implicitly also allowed.
A side-by-side comparison of how polls compare to prediction markets might be
fun in a few months.</p><p>2 0.24857748 <a title="312-tfidf-2" href="../hunch_net-2006/hunch_net-2006-10-13-David_Pennock_starts_Oddhead.html">214 hunch net-2006-10-13-David Pennock starts Oddhead</a></p>
<p>Introduction: his blog on information markets and other research topics.</p><p>3 0.12678984 <a title="312-tfidf-3" href="../hunch_net-2005/hunch_net-2005-07-11-AAAI_blog.html">92 hunch net-2005-07-11-AAAI blog</a></p>
<p>Introduction: TheAAAI conferenceis running astudent blogwhich looks like a fun experiment.</p><p>4 0.1219117 <a title="312-tfidf-4" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>Introduction: One view of machine learning is that it's about how to program computers to
predict well. This suggests a broader research program centered around the
more pervasive goal of simply predicting well.There are many distinct strands
of this broader research program which are only partially unified. Here are
the ones that I know of:Learning Theory. Learning theory focuses on several
topics related to the dynamics and process of prediction. Convergence bounds
like theVC boundgive an intellectual foundation to many learning algorithms.
Online learning algorithms likeWeighted Majorityprovide an alternate purely
game theoretic foundation for learning.Boosting algorithmsyield algorithms for
purifying prediction abiliity.Reduction algorithmsprovide means for changing
esoteric problems into well known ones.Machine Learning. A great deal of
experience has accumulated in practical algorithm design from a mixture of
paradigms, including bayesian, biological, optimization, and
theoretical.Mechanism De</p><p>5 0.097194634 <a title="312-tfidf-5" href="../hunch_net-2008/hunch_net-2008-01-07-2008_Summer_Machine_Learning_Conference_Schedule.html">283 hunch net-2008-01-07-2008 Summer Machine Learning Conference Schedule</a></p>
<p>Introduction: ConferencePaper due dateConference DateLocationAAAIJanuary 22/23/25/30July
13-17Chicago, IllinoisICMLFeb 8July 5-9Helsinki, FinlandCOLTFeb 20July
9-12Helsinki, FinlandKDDFeb 23/29August 24-27Las Vegas, NevadaUAIFeb 27/Feb
29July 9-12Helsinki, FinlandHelsinki is a fun place to visit.</p><p>6 0.092187509 <a title="312-tfidf-6" href="../hunch_net-2005/hunch_net-2005-02-09-Intuitions_from_applied_learning.html">16 hunch net-2005-02-09-Intuitions from applied learning</a></p>
<p>7 0.091384187 <a title="312-tfidf-7" href="../hunch_net-2011/hunch_net-2011-03-20-KDD_Cup_2011.html">427 hunch net-2011-03-20-KDD Cup 2011</a></p>
<p>8 0.091128163 <a title="312-tfidf-8" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>9 0.085740432 <a title="312-tfidf-9" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>10 0.085481927 <a title="312-tfidf-10" href="../hunch_net-2006/hunch_net-2006-11-06-Data_Linkage_Problems.html">217 hunch net-2006-11-06-Data Linkage Problems</a></p>
<p>11 0.079699166 <a title="312-tfidf-11" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>12 0.077156879 <a title="312-tfidf-12" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<p>13 0.073235095 <a title="312-tfidf-13" href="../hunch_net-2007/hunch_net-2007-12-12-Workshop_Summary%26%238212%3BPrinciples_of_Learning_Problem_Design.html">277 hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</a></p>
<p>14 0.068715274 <a title="312-tfidf-14" href="../hunch_net-2010/hunch_net-2010-02-26-Yahoo%21_ML_events.html">389 hunch net-2010-02-26-Yahoo! ML events</a></p>
<p>15 0.066922575 <a title="312-tfidf-15" href="../hunch_net-2006/hunch_net-2006-07-09-The_Stock_Prediction_Machine_Learning_Problem.html">193 hunch net-2006-07-09-The Stock Prediction Machine Learning Problem</a></p>
<p>16 0.062138721 <a title="312-tfidf-16" href="../hunch_net-2009/hunch_net-2009-06-03-Functionally_defined_Nonlinear_Dynamic_Models.html">359 hunch net-2009-06-03-Functionally defined Nonlinear Dynamic Models</a></p>
<p>17 0.06115143 <a title="312-tfidf-17" href="../hunch_net-2011/hunch_net-2011-04-06-COLT_open_questions.html">429 hunch net-2011-04-06-COLT open questions</a></p>
<p>18 0.055947646 <a title="312-tfidf-18" href="../hunch_net-2006/hunch_net-2006-09-09-How_to_solve_an_NP_hard_problem_in_quadratic_time.html">206 hunch net-2006-09-09-How to solve an NP hard problem in quadratic time</a></p>
<p>19 0.055778012 <a title="312-tfidf-19" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>20 0.054527689 <a title="312-tfidf-20" href="../hunch_net-2008/hunch_net-2008-09-04-Fall_ML_Conferences.html">316 hunch net-2008-09-04-Fall ML Conferences</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.112), (1, -0.02), (2, 0.023), (3, -0.019), (4, 0.025), (5, 0.038), (6, -0.025), (7, -0.074), (8, 0.079), (9, 0.019), (10, -0.082), (11, -0.032), (12, -0.039), (13, 0.007), (14, 0.054), (15, -0.072), (16, -0.055), (17, -0.005), (18, -0.105), (19, -0.111), (20, -0.034), (21, -0.139), (22, 0.062), (23, 0.022), (24, 0.061), (25, -0.02), (26, -0.075), (27, -0.114), (28, 0.046), (29, -0.166), (30, -0.006), (31, -0.073), (32, -0.13), (33, -0.008), (34, 0.11), (35, -0.0), (36, 0.123), (37, -0.018), (38, 0.061), (39, 0.014), (40, -0.117), (41, -0.014), (42, 0.039), (43, -0.083), (44, 0.021), (45, 0.114), (46, -0.04), (47, -0.119), (48, -0.085), (49, 0.105)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98167753 <a title="312-lsi-1" href="../hunch_net-2008/hunch_net-2008-08-04-Electoralmarkets.com.html">312 hunch net-2008-08-04-Electoralmarkets.com</a></p>
<p>Introduction: Lancereminded me aboutelectoralmarketstoday, which is cool enough that I want
to point it out explicitly here.Most people stilluse pollsto predict who wins,
while electoralmarkets uses people betting real money. They might use polling
information, but any other sources of information are implicitly also allowed.
A side-by-side comparison of how polls compare to prediction markets might be
fun in a few months.</p><p>2 0.53345704 <a title="312-lsi-2" href="../hunch_net-2005/hunch_net-2005-07-11-AAAI_blog.html">92 hunch net-2005-07-11-AAAI blog</a></p>
<p>Introduction: TheAAAI conferenceis running astudent blogwhich looks like a fun experiment.</p><p>3 0.53301352 <a title="312-lsi-3" href="../hunch_net-2006/hunch_net-2006-10-13-David_Pennock_starts_Oddhead.html">214 hunch net-2006-10-13-David Pennock starts Oddhead</a></p>
<p>Introduction: his blog on information markets and other research topics.</p><p>4 0.52209872 <a title="312-lsi-4" href="../hunch_net-2008/hunch_net-2008-01-07-2008_Summer_Machine_Learning_Conference_Schedule.html">283 hunch net-2008-01-07-2008 Summer Machine Learning Conference Schedule</a></p>
<p>Introduction: ConferencePaper due dateConference DateLocationAAAIJanuary 22/23/25/30July
13-17Chicago, IllinoisICMLFeb 8July 5-9Helsinki, FinlandCOLTFeb 20July
9-12Helsinki, FinlandKDDFeb 23/29August 24-27Las Vegas, NevadaUAIFeb 27/Feb
29July 9-12Helsinki, FinlandHelsinki is a fun place to visit.</p><p>5 0.47537526 <a title="312-lsi-5" href="../hunch_net-2009/hunch_net-2009-11-06-Yisong_Yue_on_Self-improving_Systems.html">376 hunch net-2009-11-06-Yisong Yue on Self-improving Systems</a></p>
<p>Introduction: I'd like to point outYisong Yue'spost on Self-improving systems, which is a
nicely readable description of the necessity and potential of interactive
learning to deal with the information overload problem that is endemic to the
modern internet.</p><p>6 0.47109178 <a title="312-lsi-6" href="../hunch_net-2006/hunch_net-2006-11-06-Data_Linkage_Problems.html">217 hunch net-2006-11-06-Data Linkage Problems</a></p>
<p>7 0.44832835 <a title="312-lsi-7" href="../hunch_net-2006/hunch_net-2006-09-07-Objective_and_subjective_interpretations_of_probability.html">205 hunch net-2006-09-07-Objective and subjective interpretations of probability</a></p>
<p>8 0.4388594 <a title="312-lsi-8" href="../hunch_net-2011/hunch_net-2011-03-20-KDD_Cup_2011.html">427 hunch net-2011-03-20-KDD Cup 2011</a></p>
<p>9 0.42730412 <a title="312-lsi-9" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>10 0.38968569 <a title="312-lsi-10" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<p>11 0.3886781 <a title="312-lsi-11" href="../hunch_net-2006/hunch_net-2006-01-23-On_Coding_via_Mutual_Information_%26%23038%3B_Bayes_Nets.html">150 hunch net-2006-01-23-On Coding via Mutual Information &#038; Bayes Nets</a></p>
<p>12 0.38380098 <a title="312-lsi-12" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>13 0.35997051 <a title="312-lsi-13" href="../hunch_net-2010/hunch_net-2010-12-02-Traffic_Prediction_Problem.html">418 hunch net-2010-12-02-Traffic Prediction Problem</a></p>
<p>14 0.35926327 <a title="312-lsi-14" href="../hunch_net-2006/hunch_net-2006-08-03-AOL%26%238217%3Bs_data_drop.html">200 hunch net-2006-08-03-AOL&#8217;s data drop</a></p>
<p>15 0.34139848 <a title="312-lsi-15" href="../hunch_net-2010/hunch_net-2010-09-21-Regretting_the_dead.html">411 hunch net-2010-09-21-Regretting the dead</a></p>
<p>16 0.33716014 <a title="312-lsi-16" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>17 0.33277309 <a title="312-lsi-17" href="../hunch_net-2009/hunch_net-2009-06-03-Functionally_defined_Nonlinear_Dynamic_Models.html">359 hunch net-2009-06-03-Functionally defined Nonlinear Dynamic Models</a></p>
<p>18 0.32606855 <a title="312-lsi-18" href="../hunch_net-2005/hunch_net-2005-10-03-Not_ICML.html">117 hunch net-2005-10-03-Not ICML</a></p>
<p>19 0.32099515 <a title="312-lsi-19" href="../hunch_net-2005/hunch_net-2005-04-14-Families_of_Learning_Theory_Statements.html">56 hunch net-2005-04-14-Families of Learning Theory Statements</a></p>
<p>20 0.32084388 <a title="312-lsi-20" href="../hunch_net-2007/hunch_net-2007-06-21-Presentation_Preparation.html">249 hunch net-2007-06-21-Presentation Preparation</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(35, 0.195), (42, 0.169), (74, 0.101), (85, 0.359)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.89013219 <a title="312-lda-1" href="../hunch_net-2008/hunch_net-2008-08-04-Electoralmarkets.com.html">312 hunch net-2008-08-04-Electoralmarkets.com</a></p>
<p>Introduction: Lancereminded me aboutelectoralmarketstoday, which is cool enough that I want
to point it out explicitly here.Most people stilluse pollsto predict who wins,
while electoralmarkets uses people betting real money. They might use polling
information, but any other sources of information are implicitly also allowed.
A side-by-side comparison of how polls compare to prediction markets might be
fun in a few months.</p><p>2 0.79018772 <a title="312-lda-2" href="../hunch_net-2012/hunch_net-2012-08-27-NYAS_ML_2012_and_ICML_2013.html">472 hunch net-2012-08-27-NYAS ML 2012 and ICML 2013</a></p>
<p>Introduction: TheNew York Machine Learning Symposiumis October 19 with a 2 page abstract
deadline due September 13 via email with subject "Machine Learning Poster
Submission" sent to physicalscience@nyas.org. Everyone is welcome to submit.
Last year's attendance was 246 and I expect more this year.The primary
experiment forICML 2013is multiple paper submission deadlines with rolling
review cycles. The key dates are October 1, December 15, and February 15. This
is an attempt to shift ICML further towards a journal style review process and
reduce peak load. The "not for proceedings" experiment from this year's ICML
is not continuing.Edit: Fixed second ICML deadline.</p><p>3 0.66829169 <a title="312-lda-3" href="../hunch_net-2005/hunch_net-2005-08-18-SVM_Adaptability.html">103 hunch net-2005-08-18-SVM Adaptability</a></p>
<p>Introduction: Several recent papers have shown that SVM-like optimizations can be used to
handle several large family loss functions.This is a good thing because it is
implausible that thelossfunction imposed by the world can not be taken into
account in the process of solving a prediction problem. Even people used to
the hard-coreBayesianapproach to learning often note that some approximations
are almost inevitable in specifying apriorand/or integrating to achieve a
posterior. Taking into account how the system will be evaluated can allow both
computational effort and design effort to be focused so as to improve
performance.A current laundry list of capabilities includes:2002multiclass SVM
including arbitrary cost matricesICML 2003Hidden Markov ModelsNIPS 2003Markov
Networks(see somediscussion)EMNLP 2004Context free grammarsICML 2004Any loss
(with much computation)ICML 2005Anyconstrained linear prediction model(that's
my own name).ICML 2005Any loss dependent on a contingency tableI am personally
in</p><p>4 0.60467356 <a title="312-lda-4" href="../hunch_net-2005/hunch_net-2005-06-13-Wikis_for_Summer_Schools_and_Workshops.html">81 hunch net-2005-06-13-Wikis for Summer Schools and Workshops</a></p>
<p>Introduction: Chicago '05ended a couple of weeks ago. This was the sixthMachine Learning
Summer School, and the second one that used awiki. (The first was Berder '04,
thanks to Gunnar Raetsch.) Wikis are relatively easy to set up, greatly aid
social interaction, and should be used a lot more at summer schools and
workshops. They can even be used as the meeting's webpage, as a permanent
record of its participants' collaborations -- see for example the wiki/website
for last year'sNVO Summer School.A basic wiki is a collection of editable
webpages, maintained by software called awiki engine. The engine used at both
Berder and Chicago wasTikiWiki-- it is well documented and gets you something
running fast. It uses PHP and MySQL, but doesn't require you to know either.
Tikiwiki has far more features than most wikis, as it is really a fullContent
Management System. (My thanks to Sebastian Stark for pointing this out.) Here
are the features we found most useful:Bulletin boards, or forums. The most-
used on</p><p>5 0.6024574 <a title="312-lda-5" href="../hunch_net-2007/hunch_net-2007-05-12-Loss_Function_Semantics.html">245 hunch net-2007-05-12-Loss Function Semantics</a></p>
<p>Introduction: Some loss functions have a meaning, which can be understood in a manner
independent of the loss function itself.Optimizing squared
losslsq(y,y')=(y-y')2means predicting the (conditional) mean ofy.Optimizing
absolute value losslav(y,y')=|y-y'|means predicting the (conditional) median
ofy. Variants canhandle other quantiles. 0/1 loss for classification is a
special case.Optimizing log lossllog(y,y')=log (1/Prz~y'(z=y))means minimizing
the description length ofy.The semantics (= meaning) of the loss are made
explicit by a theorem in each case. For squared loss, we can prove a theorem
of the form:For all distributionsDoverY, ify' = arg miny'Ey ~ Dlsq(y,y')theny'
= Ey~DySimilar theorems hold for the other examples above, and they can all be
extended to predictors ofy'for distributionsDover a contextXand a valueY.There
are 3 points to this post.Everyone doing general machine learning should be
aware of the laundry list above. They form a handy toolkit which can match
many of the problems nat</p><p>6 0.60045683 <a title="312-lda-6" href="../hunch_net-2010/hunch_net-2010-12-04-Vowpal_Wabbit%2C_version_5.0%2C_and_the_second_heresy.html">419 hunch net-2010-12-04-Vowpal Wabbit, version 5.0, and the second heresy</a></p>
<p>7 0.59298527 <a title="312-lda-7" href="../hunch_net-2005/hunch_net-2005-05-21-What_is_the_right_form_of_modularity_in_structured_prediction%3F.html">74 hunch net-2005-05-21-What is the right form of modularity in structured prediction?</a></p>
<p>8 0.59234118 <a title="312-lda-8" href="../hunch_net-2007/hunch_net-2007-01-15-The_Machine_Learning_Department.html">228 hunch net-2007-01-15-The Machine Learning Department</a></p>
<p>9 0.59202993 <a title="312-lda-9" href="../hunch_net-2005/hunch_net-2005-05-29-Maximum_Margin_Mismatch%3F.html">77 hunch net-2005-05-29-Maximum Margin Mismatch?</a></p>
<p>10 0.59070408 <a title="312-lda-10" href="../hunch_net-2006/hunch_net-2006-05-16-The_value_of_the_orthodox_view_of_Boosting.html">179 hunch net-2006-05-16-The value of the orthodox view of Boosting</a></p>
<p>11 0.57630742 <a title="312-lda-11" href="../hunch_net-2012/hunch_net-2012-05-03-Microsoft_Research%2C_New_York_City.html">464 hunch net-2012-05-03-Microsoft Research, New York City</a></p>
<p>12 0.57021803 <a title="312-lda-12" href="../hunch_net-2006/hunch_net-2006-08-18-Report_of_MLSS_2006_Taipei.html">203 hunch net-2006-08-18-Report of MLSS 2006 Taipei</a></p>
<p>13 0.56291461 <a title="312-lda-13" href="../hunch_net-2006/hunch_net-2006-06-24-Online_convex_optimization_at_COLT.html">186 hunch net-2006-06-24-Online convex optimization at COLT</a></p>
<p>14 0.52442443 <a title="312-lda-14" href="../hunch_net-2005/hunch_net-2005-02-19-Loss_Functions_for_Discriminative_Training_of_Energy-Based_Models.html">23 hunch net-2005-02-19-Loss Functions for Discriminative Training of Energy-Based Models</a></p>
<p>15 0.52150643 <a title="312-lda-15" href="../hunch_net-2012/hunch_net-2012-09-29-Vowpal_Wabbit%2C_version_7.0.html">473 hunch net-2012-09-29-Vowpal Wabbit, version 7.0</a></p>
<p>16 0.51468885 <a title="312-lda-16" href="../hunch_net-2012/hunch_net-2012-06-05-ICML_acceptance_statistics.html">466 hunch net-2012-06-05-ICML acceptance statistics</a></p>
<p>17 0.50854158 <a title="312-lda-17" href="../hunch_net-2006/hunch_net-2006-03-27-Gradients_everywhere.html">167 hunch net-2006-03-27-Gradients everywhere</a></p>
<p>18 0.50841975 <a title="312-lda-18" href="../hunch_net-2010/hunch_net-2010-08-23-Boosted_Decision_Trees_for_Deep_Learning.html">407 hunch net-2010-08-23-Boosted Decision Trees for Deep Learning</a></p>
<p>19 0.50575095 <a title="312-lda-19" href="../hunch_net-2005/hunch_net-2005-02-17-Learning_Research_Programs.html">21 hunch net-2005-02-17-Learning Research Programs</a></p>
<p>20 0.50456691 <a title="312-lda-20" href="../hunch_net-2008/hunch_net-2008-12-07-A_NIPS_paper.html">330 hunch net-2008-12-07-A NIPS paper</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
