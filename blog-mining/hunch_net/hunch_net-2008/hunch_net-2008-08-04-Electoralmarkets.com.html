<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>312 hunch net-2008-08-04-Electoralmarkets.com</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2008" href="../home/hunch_net-2008_home.html">hunch_net-2008</a> <a title="hunch_net-2008-312" href="#">hunch_net-2008-312</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>312 hunch net-2008-08-04-Electoralmarkets.com</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2008-312-html" href="http://hunch.net/?p=396">html</a></p><p>Introduction: Lance  reminded me about  electoralmarkets  today, which is cool enough that I want to point it out explicitly here.  
 
Most people still  use polls  to predict who wins, while electoralmarkets uses people betting real money.  They might use polling information, but any other sources of information are implicitly also allowed.  A side-by-side comparison of how polls compare to prediction markets might be fun in a few months.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Lance  reminded me about  electoralmarkets  today, which is cool enough that I want to point it out explicitly here. [sent-1, score-1.195]
</p><p>2 Most people still  use polls  to predict who wins, while electoralmarkets uses people betting real money. [sent-2, score-1.827]
</p><p>3 They might use polling information, but any other sources of information are implicitly also allowed. [sent-3, score-0.659]
</p><p>4 A side-by-side comparison of how polls compare to prediction markets might be fun in a few months. [sent-4, score-1.281]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('electoralmarkets', 0.51), ('polls', 0.51), ('betting', 0.227), ('reminded', 0.21), ('wins', 0.198), ('lance', 0.181), ('markets', 0.175), ('implicitly', 0.16), ('today', 0.16), ('months', 0.156), ('compare', 0.147), ('fun', 0.141), ('comparison', 0.139), ('cool', 0.136), ('information', 0.133), ('sources', 0.13), ('explicitly', 0.121), ('uses', 0.101), ('might', 0.099), ('use', 0.096), ('still', 0.093), ('enough', 0.085), ('predict', 0.084), ('people', 0.07), ('prediction', 0.07), ('point', 0.067), ('real', 0.066), ('want', 0.066), ('also', 0.041)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="312-tfidf-1" href="../hunch_net-2008/hunch_net-2008-08-04-Electoralmarkets.com.html">312 hunch net-2008-08-04-Electoralmarkets.com</a></p>
<p>Introduction: Lance  reminded me about  electoralmarkets  today, which is cool enough that I want to point it out explicitly here.  
 
Most people still  use polls  to predict who wins, while electoralmarkets uses people betting real money.  They might use polling information, but any other sources of information are implicitly also allowed.  A side-by-side comparison of how polls compare to prediction markets might be fun in a few months.</p><p>2 0.14867298 <a title="312-tfidf-2" href="../hunch_net-2006/hunch_net-2006-10-13-David_Pennock_starts_Oddhead.html">214 hunch net-2006-10-13-David Pennock starts Oddhead</a></p>
<p>Introduction: his blog on information markets and other research topics .</p><p>3 0.088206559 <a title="312-tfidf-3" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>Introduction: One view of machine learning is that it’s about how to program computers to predict well.  This suggests a broader research program centered around the more pervasive goal of simply predicting well. 
There are many distinct strands of this broader research program which are only partially unified.  Here are the ones that I know of:
  
  Learning Theory .  Learning theory focuses on several topics related to the dynamics and process of prediction.  Convergence bounds like the  VC bound   give an intellectual foundation to many learning algorithms.  Online learning algorithms like  Weighted Majority  provide an alternate purely game theoretic foundation for learning.   Boosting algorithms  yield algorithms for purifying prediction abiliity.   Reduction algorithms  provide means for changing esoteric problems into well known ones. 
  Machine Learning .  A great deal of experience has accumulated in practical algorithm design from a mixture of paradigms, including bayesian, biological, opt</p><p>4 0.067496613 <a title="312-tfidf-4" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>Introduction: Machine learning has a new kind of “scaling to larger problems” to worry about: scaling with the amount of contextual information.  The standard development path for a machine learning application in practice seems to be the following:
  
  Marginal . In the beginning, there was “majority vote”.  At this stage, it isn’t necessary to understand that you have a prediction problem.  People just realize that one answer is right sometimes and another answer other times.  In machine learning terms, this corresponds to making a prediction without side information. 
  First context . A clever person realizes that some bit of information  x 1   could be helpful.  If  x 1   is discrete, they condition on it and make a predictor  h(x 1 ) , typically by counting.  If they are clever, then they also do some smoothing.  If  x 1   is some real valued parameter, it’s very common to make a threshold cutoff.  Often, these tasks are simply done by hand. 
  Second . Another clever person (or perhaps the s</p><p>5 0.05581636 <a title="312-tfidf-5" href="../hunch_net-2005/hunch_net-2005-07-11-AAAI_blog.html">92 hunch net-2005-07-11-AAAI blog</a></p>
<p>Introduction: The  AAAI conference   is running a  student blog  which looks like a fun experiment.</p><p>6 0.053447019 <a title="312-tfidf-6" href="../hunch_net-2006/hunch_net-2006-11-06-Data_Linkage_Problems.html">217 hunch net-2006-11-06-Data Linkage Problems</a></p>
<p>7 0.052589662 <a title="312-tfidf-7" href="../hunch_net-2005/hunch_net-2005-01-19-Why_I_decided_to_run_a_weblog..html">1 hunch net-2005-01-19-Why I decided to run a weblog.</a></p>
<p>8 0.05150523 <a title="312-tfidf-8" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<p>9 0.04899076 <a title="312-tfidf-9" href="../hunch_net-2005/hunch_net-2005-02-09-Intuitions_from_applied_learning.html">16 hunch net-2005-02-09-Intuitions from applied learning</a></p>
<p>10 0.047825541 <a title="312-tfidf-10" href="../hunch_net-2005/hunch_net-2005-09-06-A_link.html">108 hunch net-2005-09-06-A link</a></p>
<p>11 0.047666863 <a title="312-tfidf-11" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>12 0.043122403 <a title="312-tfidf-12" href="../hunch_net-2007/hunch_net-2007-12-12-Workshop_Summary%26%238212%3BPrinciples_of_Learning_Problem_Design.html">277 hunch net-2007-12-12-Workshop Summary&#8212;Principles of Learning Problem Design</a></p>
<p>13 0.042018257 <a title="312-tfidf-13" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>14 0.040584333 <a title="312-tfidf-14" href="../hunch_net-2010/hunch_net-2010-02-26-Yahoo%21_ML_events.html">389 hunch net-2010-02-26-Yahoo! ML events</a></p>
<p>15 0.040576641 <a title="312-tfidf-15" href="../hunch_net-2011/hunch_net-2011-03-20-KDD_Cup_2011.html">427 hunch net-2011-03-20-KDD Cup 2011</a></p>
<p>16 0.03958961 <a title="312-tfidf-16" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>17 0.039538473 <a title="312-tfidf-17" href="../hunch_net-2013/hunch_net-2013-06-10-The_Large_Scale_Learning_class_notes.html">483 hunch net-2013-06-10-The Large Scale Learning class notes</a></p>
<p>18 0.039474078 <a title="312-tfidf-18" href="../hunch_net-2005/hunch_net-2005-09-14-The_Predictionist_Viewpoint.html">112 hunch net-2005-09-14-The Predictionist Viewpoint</a></p>
<p>19 0.039463017 <a title="312-tfidf-19" href="../hunch_net-2006/hunch_net-2006-07-09-The_Stock_Prediction_Machine_Learning_Problem.html">193 hunch net-2006-07-09-The Stock Prediction Machine Learning Problem</a></p>
<p>20 0.038871035 <a title="312-tfidf-20" href="../hunch_net-2011/hunch_net-2011-04-06-COLT_open_questions.html">429 hunch net-2011-04-06-COLT open questions</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.074), (1, 0.013), (2, -0.016), (3, 0.024), (4, -0.016), (5, -0.01), (6, -0.014), (7, -0.026), (8, 0.028), (9, 0.003), (10, -0.024), (11, 0.032), (12, 0.007), (13, -0.007), (14, -0.0), (15, -0.004), (16, -0.012), (17, -0.025), (18, 0.013), (19, -0.037), (20, 0.005), (21, -0.032), (22, 0.073), (23, 0.015), (24, -0.009), (25, 0.054), (26, 0.054), (27, 0.015), (28, -0.007), (29, -0.014), (30, 0.004), (31, 0.016), (32, 0.054), (33, -0.037), (34, -0.039), (35, -0.054), (36, -0.028), (37, 0.009), (38, -0.009), (39, 0.017), (40, 0.031), (41, 0.102), (42, -0.001), (43, -0.025), (44, 0.069), (45, -0.01), (46, -0.05), (47, 0.037), (48, 0.068), (49, -0.066)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95518136 <a title="312-lsi-1" href="../hunch_net-2008/hunch_net-2008-08-04-Electoralmarkets.com.html">312 hunch net-2008-08-04-Electoralmarkets.com</a></p>
<p>Introduction: Lance  reminded me about  electoralmarkets  today, which is cool enough that I want to point it out explicitly here.  
 
Most people still  use polls  to predict who wins, while electoralmarkets uses people betting real money.  They might use polling information, but any other sources of information are implicitly also allowed.  A side-by-side comparison of how polls compare to prediction markets might be fun in a few months.</p><p>2 0.55189484 <a title="312-lsi-2" href="../hunch_net-2006/hunch_net-2006-01-23-On_Coding_via_Mutual_Information_%26%23038%3B_Bayes_Nets.html">150 hunch net-2006-01-23-On Coding via Mutual Information &#038; Bayes Nets</a></p>
<p>Introduction: Say we have two random variables  X,Y  with mutual information  I(X,Y) . Let’s say we want to represent them with a bayes net of the form  X< -M->Y , such that the entropy of  M  equals the mutual information, i.e.  H(M)=I(X,Y) . Intuitively, we would like our hidden state to be as simple as possible (entropy wise). The data processing inequality means that  H(M)>=I(X,Y) , so the mutual information is a lower bound on how simple the  M  could be. Furthermore, if such a construction existed it would have a nice coding interpretation — one could jointly code  X  and  Y  by first coding the mutual information, then coding  X  with this mutual info (without  Y ) and coding  Y  with this mutual info (without  X ).
 
It turns out that such a construction does not exist in general (Thx  Alina Beygelzimer  for a counterexample! see below for the sketch).
 
What are the implications of this? Well, it’s hard for me to say, but it does suggest to me that the ‘generative’ model philosophy might be</p><p>3 0.51823539 <a title="312-lsi-3" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>Introduction: Machine learning has a new kind of “scaling to larger problems” to worry about: scaling with the amount of contextual information.  The standard development path for a machine learning application in practice seems to be the following:
  
  Marginal . In the beginning, there was “majority vote”.  At this stage, it isn’t necessary to understand that you have a prediction problem.  People just realize that one answer is right sometimes and another answer other times.  In machine learning terms, this corresponds to making a prediction without side information. 
  First context . A clever person realizes that some bit of information  x 1   could be helpful.  If  x 1   is discrete, they condition on it and make a predictor  h(x 1 ) , typically by counting.  If they are clever, then they also do some smoothing.  If  x 1   is some real valued parameter, it’s very common to make a threshold cutoff.  Often, these tasks are simply done by hand. 
  Second . Another clever person (or perhaps the s</p><p>4 0.51210058 <a title="312-lsi-4" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<p>Introduction: I want to comment on the “Bing copies Google” discussion  here ,  here , and  here , because there are data-related issues which the general public may not understand, and some of the framing seems substantially misleading to me.
 
As a not-distant-outsider, let me mention the sources of bias I may have.  I work at  Yahoo! , which has started using  Bing .  This might predispose me towards Bing, but on the other hand I’m still at Yahoo!, and have been using  Linux  exclusively as an OS for many years, including even a couple minor kernel patches.  And,  on the gripping hand , I’ve spent quite a bit of time thinking about the basic  principles of incorporating user feedback in machine learning .  Also note, this post is not  related to official Yahoo! policy, it’s just my personal view.
 
 The issue  Google engineers inserted synthetic responses to synthetic queries on google.com, then executed the synthetic searches on google.com using Internet Explorer with the Bing toolbar and later</p><p>5 0.47521549 <a title="312-lsi-5" href="../hunch_net-2006/hunch_net-2006-11-06-Data_Linkage_Problems.html">217 hunch net-2006-11-06-Data Linkage Problems</a></p>
<p>Introduction: Data linkage is a problem which seems to come up in various applied machine learning problems.  I have heard it mentioned in various data mining contexts, but it seems relatively less studied for systemic reasons.
 
A very simple version of the data linkage problem is a cross hospital patient record merge.  Suppose a patient (John Doe) is admitted to a hospital (General Health), treated, and released.  Later, John Doe is admitted to a second hospital (Health General), treated, and released.  Given a large number of records of this sort, it becomes very tempting to try and predict the outcomes of treatments.  This is reasonably straightforward as a machine learning problem if there is a shared unique identifier for John Doe used by General Health and Health General along with time stamps.  We can merge the records and create examples of the form “Given symptoms and treatment, did the patient come back to a hospital within the next year?”  These examples could be fed into a learning algo</p><p>6 0.46505982 <a title="312-lsi-6" href="../hunch_net-2006/hunch_net-2006-08-03-AOL%26%238217%3Bs_data_drop.html">200 hunch net-2006-08-03-AOL&#8217;s data drop</a></p>
<p>7 0.44962782 <a title="312-lsi-7" href="../hunch_net-2006/hunch_net-2006-10-13-David_Pennock_starts_Oddhead.html">214 hunch net-2006-10-13-David Pennock starts Oddhead</a></p>
<p>8 0.43846142 <a title="312-lsi-8" href="../hunch_net-2006/hunch_net-2006-09-07-Objective_and_subjective_interpretations_of_probability.html">205 hunch net-2006-09-07-Objective and subjective interpretations of probability</a></p>
<p>9 0.43046257 <a title="312-lsi-9" href="../hunch_net-2009/hunch_net-2009-03-08-Prediction_Science.html">345 hunch net-2009-03-08-Prediction Science</a></p>
<p>10 0.40837815 <a title="312-lsi-10" href="../hunch_net-2005/hunch_net-2005-10-10-Predictive_Search_is_Coming.html">120 hunch net-2005-10-10-Predictive Search is Coming</a></p>
<p>11 0.39316559 <a title="312-lsi-11" href="../hunch_net-2008/hunch_net-2008-04-26-Eliminating_the_Birthday_Paradox_for_Universal_Features.html">298 hunch net-2008-04-26-Eliminating the Birthday Paradox for Universal Features</a></p>
<p>12 0.38498783 <a title="312-lsi-12" href="../hunch_net-2006/hunch_net-2006-07-06-Branch_Prediction_Competition.html">190 hunch net-2006-07-06-Branch Prediction Competition</a></p>
<p>13 0.37728262 <a title="312-lsi-13" href="../hunch_net-2010/hunch_net-2010-09-21-Regretting_the_dead.html">411 hunch net-2010-09-21-Regretting the dead</a></p>
<p>14 0.37652707 <a title="312-lsi-14" href="../hunch_net-2008/hunch_net-2008-04-21-The_Science_2.0_article.html">296 hunch net-2008-04-21-The Science 2.0 article</a></p>
<p>15 0.37085649 <a title="312-lsi-15" href="../hunch_net-2005/hunch_net-2005-08-11-Why_Manifold-Based_Dimension_Reduction_Techniques%3F.html">102 hunch net-2005-08-11-Why Manifold-Based Dimension Reduction Techniques?</a></p>
<p>16 0.35915765 <a title="312-lsi-16" href="../hunch_net-2009/hunch_net-2009-06-03-Functionally_defined_Nonlinear_Dynamic_Models.html">359 hunch net-2009-06-03-Functionally defined Nonlinear Dynamic Models</a></p>
<p>17 0.35703972 <a title="312-lsi-17" href="../hunch_net-2011/hunch_net-2011-10-03-Monday_announcements.html">446 hunch net-2011-10-03-Monday announcements</a></p>
<p>18 0.35418084 <a title="312-lsi-18" href="../hunch_net-2005/hunch_net-2005-01-19-Why_I_decided_to_run_a_weblog..html">1 hunch net-2005-01-19-Why I decided to run a weblog.</a></p>
<p>19 0.35414612 <a title="312-lsi-19" href="../hunch_net-2012/hunch_net-2012-03-13-The_Submodularity_workshop_and_Lucca_Professorship.html">459 hunch net-2012-03-13-The Submodularity workshop and Lucca Professorship</a></p>
<p>20 0.34707773 <a title="312-lsi-20" href="../hunch_net-2006/hunch_net-2006-09-18-What_is_missing_for_online_collaborative_research%3F.html">208 hunch net-2006-09-18-What is missing for online collaborative research?</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(10, 0.028), (13, 0.112), (15, 0.353), (27, 0.175), (53, 0.058), (55, 0.071), (95, 0.031)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.87070435 <a title="312-lda-1" href="../hunch_net-2008/hunch_net-2008-08-04-Electoralmarkets.com.html">312 hunch net-2008-08-04-Electoralmarkets.com</a></p>
<p>Introduction: Lance  reminded me about  electoralmarkets  today, which is cool enough that I want to point it out explicitly here.  
 
Most people still  use polls  to predict who wins, while electoralmarkets uses people betting real money.  They might use polling information, but any other sources of information are implicitly also allowed.  A side-by-side comparison of how polls compare to prediction markets might be fun in a few months.</p><p>2 0.52341586 <a title="312-lda-2" href="../hunch_net-2010/hunch_net-2010-01-13-Sam_Roweis_died.html">386 hunch net-2010-01-13-Sam Roweis died</a></p>
<p>Introduction: and I can’t help but remember him.
 
I first met  Sam  as an undergraduate at  Caltech  where he was TA for  Hopfield ‘s class, and again when I visited  Gatsby , when he invited me to visit  Toronto , and at too many conferences to recount.  His personality was a combination of enthusiastic and thoughtful, with a great ability to phrase a problem so it’s solution must be understood.  With respect to my own work, Sam was the one who advised me to make  my first tutorial , leading to others, and to other things, all of which I’m grateful to him for.  In fact, my every interaction with Sam was positive, and that was his way.
 
His death is  being called a suicide  which is so incompatible with my understanding of Sam that it strains my credibility.  But we know that his many responsibilities were great, and it is well understood that basically all sane researchers have legions of inner doubts.  Having been depressed now and then myself, it’s helpful to understand at least intellectually</p><p>3 0.50641763 <a title="312-lda-3" href="../hunch_net-2005/hunch_net-2005-02-01-Watchword%3A_Loss.html">9 hunch net-2005-02-01-Watchword: Loss</a></p>
<p>Introduction: A loss function is some function which, for any example, takes a prediction and the correct prediction, and determines how much loss is incurred.  (People sometimes attempt to optimize functions of more than one example such as “area under the ROC curve” or “harmonic mean of precision and recall”.)  Typically we try to find predictors that minimize loss.  
 
There seems to be a strong dichotomy between two views of what “loss” means in learning.
  
  Loss is determined by the problem.  Loss is a part of the specification of the learning problem.  Examples of problems specified by the loss function include “binary classification”, “multiclass classification”, “importance weighted classification”, “l 2  regression”, etc…  This is the decision theory view of what loss means, and the view that I prefer. 
  Loss is determined by the solution.  To solve a problem, you optimize some particular loss function  not  given by the problem.  Examples of these loss functions are “hinge loss” (for SV</p><p>4 0.4980514 <a title="312-lda-4" href="../hunch_net-2010/hunch_net-2010-08-22-KDD_2010.html">406 hunch net-2010-08-22-KDD 2010</a></p>
<p>Introduction: There were several papers that seemed fairly interesting at  KDD this year .  The ones that caught my attention are:
  
  Xin Jin , Mingyang Zhang,  Nan Zhang , and  Gautam Das ,  Versatile Publishing For Privacy Preservation .  This paper provides a conservative method for safely determining which data is publishable from any complete source of information (for example, a hospital) such that it does not violate privacy rules in a natural language.  It is not differentially private, so no external sources of join information can exist.  However, it is a mechanism for  publishing  data rather than (say) the output of a learning algorithm. 
  Arik Friedman   Assaf Schuster ,  Data Mining with Differential Privacy .  This paper shows how to create effective differentially private decision trees.  Progress in differentially private datamining is pretty impressive, as it was  defined in 2006 . 
 David Chan, Rong Ge, Ori Gershony,  Tim Hesterberg ,  Diane Lambert ,  Evaluating Online Ad Camp</p><p>5 0.49200639 <a title="312-lda-5" href="../hunch_net-2013/hunch_net-2013-12-01-NIPS_tutorials_and_Vowpal_Wabbit_7.4.html">492 hunch net-2013-12-01-NIPS tutorials and Vowpal Wabbit 7.4</a></p>
<p>Introduction: At NIPS I’m giving a  tutorial on Learning to Interact .  In essence this is about dealing with causality in a contextual bandit framework.  Relative to  previous tutorials , I’ll be covering several new results that changed my understanding of the nature of the problem.  Note that  Judea Pearl  and  Elias Bareinboim  have a  tutorial on causality .  This might appear similar, but is quite different in practice.  Pearl and Bareinboim’s tutorial will be about the general concepts while mine will be about total mastery of the simplest nontrivial case, including code.  Luckily, they have the right order.  I recommend going to both   
 
I also just released version 7.4 of  Vowpal Wabbit .  When I was a frustrated learning theorist, I did not understand why people were not using learning reductions to solve problems.  I’ve been slowly discovering why with VW, and addressing the issues.  One of the issues is that machine learning itself was not automatic enough, while another is that creatin</p><p>6 0.48900762 <a title="312-lda-6" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>7 0.48769641 <a title="312-lda-7" href="../hunch_net-2005/hunch_net-2005-12-09-Machine_Learning_Thoughts.html">137 hunch net-2005-12-09-Machine Learning Thoughts</a></p>
<p>8 0.48593596 <a title="312-lda-8" href="../hunch_net-2006/hunch_net-2006-10-04-Health_of_Conferences_Wiki.html">212 hunch net-2006-10-04-Health of Conferences Wiki</a></p>
<p>9 0.47734937 <a title="312-lda-9" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>10 0.46298972 <a title="312-lda-10" href="../hunch_net-2007/hunch_net-2007-01-02-Retrospective.html">225 hunch net-2007-01-02-Retrospective</a></p>
<p>11 0.45573533 <a title="312-lda-11" href="../hunch_net-2005/hunch_net-2005-10-03-Not_ICML.html">117 hunch net-2005-10-03-Not ICML</a></p>
<p>12 0.45233241 <a title="312-lda-12" href="../hunch_net-2008/hunch_net-2008-06-27-Reviewing_Horror_Stories.html">304 hunch net-2008-06-27-Reviewing Horror Stories</a></p>
<p>13 0.45224991 <a title="312-lda-13" href="../hunch_net-2006/hunch_net-2006-07-11-New_Models.html">194 hunch net-2006-07-11-New Models</a></p>
<p>14 0.45213214 <a title="312-lda-14" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>15 0.45209771 <a title="312-lda-15" href="../hunch_net-2013/hunch_net-2013-01-07-NYU_Large_Scale_Machine_Learning_Class.html">478 hunch net-2013-01-07-NYU Large Scale Machine Learning Class</a></p>
<p>16 0.45126322 <a title="312-lda-16" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>17 0.45103803 <a title="312-lda-17" href="../hunch_net-2008/hunch_net-2008-07-10-Interesting_papers%2C_ICML_2008.html">309 hunch net-2008-07-10-Interesting papers, ICML 2008</a></p>
<p>18 0.44980088 <a title="312-lda-18" href="../hunch_net-2013/hunch_net-2013-06-10-The_Large_Scale_Learning_class_notes.html">483 hunch net-2013-06-10-The Large Scale Learning class notes</a></p>
<p>19 0.44871491 <a title="312-lda-19" href="../hunch_net-2007/hunch_net-2007-12-19-Cool_and_interesting_things_seen_at_NIPS.html">279 hunch net-2007-12-19-Cool and interesting things seen at NIPS</a></p>
<p>20 0.44836998 <a title="312-lda-20" href="../hunch_net-2006/hunch_net-2006-01-18-Is_Multitask_Learning_Black-Boxable%3F.html">149 hunch net-2006-01-18-Is Multitask Learning Black-Boxable?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
