<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>302 hunch net-2008-05-25-Inappropriate Mathematics for Machine Learning</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2008" href="../home/hunch_net-2008_home.html">hunch_net-2008</a> <a title="hunch_net-2008-302" href="#">hunch_net-2008-302</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>302 hunch net-2008-05-25-Inappropriate Mathematics for Machine Learning</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2008-302-html" href="http://hunch.net/?p=332">html</a></p><p>Introduction: Reviewers and students are sometimes greatly concerned by the distinction
between:Anopen setand aclosed set.ASupremumand aMaximum.An event which happens
with probability 1 and an event that always happens.I don't appreciate this
distinction in machine learning & learning theory. All machine learning takes
place (by definition) on a machine where every parameter has finite precision.
Consequently, every set is closed, a maximal element always exists, and
probability 1 events always happen.The fundamental issue here is that
substantial parts of mathematics don't appear well-matched to computation in
the physical world, because the mathematics has concerns which are unphysical.
This mismatched mathematics makes irrelevant distinctions. We can ask "what
mathematics is appropriate to computation?"Andrejhas convinced me that a
pretty good answer to this question isconstructive mathematics.So, here's a
basic challenge: Can anyone name a situation where any of the distinctions
above (or simila</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mathematics', 0.472), ('distinction', 0.292), ('distinctions', 0.292), ('event', 0.204), ('always', 0.192), ('computation', 0.176), ('setand', 0.175), ('maximal', 0.153), ('convinced', 0.153), ('mismatched', 0.153), ('probability', 0.152), ('concerned', 0.146), ('closed', 0.14), ('physical', 0.135), ('every', 0.13), ('parts', 0.121), ('element', 0.118), ('name', 0.118), ('parameter', 0.113), ('concerns', 0.113)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="302-tfidf-1" href="../hunch_net-2008/hunch_net-2008-05-25-Inappropriate_Mathematics_for_Machine_Learning.html">302 hunch net-2008-05-25-Inappropriate Mathematics for Machine Learning</a></p>
<p>Introduction: Reviewers and students are sometimes greatly concerned by the distinction
between:Anopen setand aclosed set.ASupremumand aMaximum.An event which happens
with probability 1 and an event that always happens.I don't appreciate this
distinction in machine learning & learning theory. All machine learning takes
place (by definition) on a machine where every parameter has finite precision.
Consequently, every set is closed, a maximal element always exists, and
probability 1 events always happen.The fundamental issue here is that
substantial parts of mathematics don't appear well-matched to computation in
the physical world, because the mathematics has concerns which are unphysical.
This mismatched mathematics makes irrelevant distinctions. We can ask "what
mathematics is appropriate to computation?"Andrejhas convinced me that a
pretty good answer to this question isconstructive mathematics.So, here's a
basic challenge: Can anyone name a situation where any of the distinctions
above (or simila</p><p>2 0.17437117 <a title="302-tfidf-2" href="../hunch_net-2005/hunch_net-2005-04-10-Is_the_Goal_Understanding_or_Prediction%3F.html">55 hunch net-2005-04-10-Is the Goal Understanding or Prediction?</a></p>
<p>Introduction: Steve Smaleand I have a debate about goals of learning theory.Steve likes
theorems with a dependence on unobservable quantities. For example, ifDis a
distribution over a spaceX x [0,1], you can state a theorem about the error
rate dependent on the variance,E(x,y)~D(y-Ey'~D|x[y'])2.I dislike this,
because I want to use the theorems to produce code solving learning problems.
Since I don't know (and can't measure) the variance, a theorem depending on
the variance does not help me--I would not know what variance to plug into the
learning algorithm.Recast more broadly, this is a debate between "declarative"
and "operative" mathematics. A strong example of "declarative" mathematics
is"a new kind of science". Roughly speaking, the goal of this kind of approach
seems to be finding a way to explain the observations we make. Examples
include "some things are unpredictable", "a phase transition exists",
etcâ&euro;Ś"Operative" mathematics helps you make predictions about the world. A
strong example of op</p><p>3 0.15954404 <a title="302-tfidf-3" href="../hunch_net-2005/hunch_net-2005-07-07-The_Limits_of_Learning_Theory.html">90 hunch net-2005-07-07-The Limits of Learning Theory</a></p>
<p>Introduction: Suppose we had an infinitely powerful mathematician sitting in a room and
proving theorems about learning. Could he solve machine learning?The answer is
"no". This answer is both obvious and sometimes underappreciated.There are
several ways to conclude that somebiasis necessary in order to succesfully
learn. For example, suppose we are trying to solve classification. At
prediction time, we observe some featuresXand want to make a prediction of
either0or1. Bias is what makes us prefer one answer over the other based on
past experience. In order to learn we must:Have a bias. Always predicting0is
as likely as1is useless.Have the "right" bias. Predicting1when the answer
is0is also not helpful.The implication of "have a bias" is that we can not
design effective learning algorithms with "a uniform prior over all
possibilities". The implication of "have the 'right' bias" is that our
mathematician fails since "right" is defined with respect to the solutions to
problems encountered in the real</p><p>4 0.12074279 <a title="302-tfidf-4" href="../hunch_net-2005/hunch_net-2005-01-26-Watchword%3A_Probability.html">5 hunch net-2005-01-26-Watchword: Probability</a></p>
<p>Introduction: Probability is one of the most confusingly used words in machine learning.
There are at least 3 distinct ways the word is used.BayesianThe Bayesian
notion of probability is a 'degree of belief'. The degree of belief that some
event (i.e. "stock goes up" or "stock goes down") occurs can be measured by
asking a sequence of questions of the form "Would you bet the stock goes up or
down atYto 1 odds?" A consistent better will switch from 'for' to 'against' at
some single value ofY. The probability is thenY/(Y+1). Bayesian probabilities
express lack of knowledge rather than randomization. They are useful in
learning because we often lack knowledge and expressing that lack flexibly
makes the learning algorithms work better. Bayesian Learning uses
'probability' in this way exclusively.FrequentistThe Frequentist notion of
probability is a rate of occurence. A rate of occurrence can be measured by
doing an experiment many times. If an event occursktimes innexperiments then
it has probability ab</p><p>5 0.1114585 <a title="302-tfidf-5" href="../hunch_net-2005/hunch_net-2005-05-12-Math_on_the_Web.html">70 hunch net-2005-05-12-Math on the Web</a></p>
<p>Introduction: Andrej Bauer has setup aMathematics and ComputationBlog. As a first step he
has tried to address the persistent and annoying problem of math on the web.
As a basic tool for precisely stating and transfering understanding of
technical subjects, mathematics is very necessary. Despite this necessity,
every mechanism for expressing mathematics on the web seems unnaturally
clumsy. Here are some of the methods and their drawbacks:MathMLThis was
supposed to be the answer, but it has two severe drawbacks: "Internet
Explorer" doesn't read it and the language is an example of push-XML-to-the-
limit which no one would ever consider writing in. (In contrast, html is easy
to write in.) It's also very annoying that math fonts must be installed
independent of the browser, even for mozilla based browsers.Create inline
images. This has several big drawbacks: font size is fixed for all viewers,
you can't cut & paste inside the images, and you can't hyperlink from (say)
symbol to definition.Math Worldis</p><p>6 0.096397862 <a title="302-tfidf-6" href="../hunch_net-2006/hunch_net-2006-12-05-Recruitment_Conferences.html">222 hunch net-2006-12-05-Recruitment Conferences</a></p>
<p>7 0.09568762 <a title="302-tfidf-7" href="../hunch_net-2006/hunch_net-2006-11-20-Context_and_the_calculation_misperception.html">218 hunch net-2006-11-20-Context and the calculation misperception</a></p>
<p>8 0.091607548 <a title="302-tfidf-8" href="../hunch_net-2008/hunch_net-2008-02-27-The_Stats_Handicap.html">290 hunch net-2008-02-27-The Stats Handicap</a></p>
<p>9 0.089996316 <a title="302-tfidf-9" href="../hunch_net-2009/hunch_net-2009-06-01-Multitask_Poisoning.html">358 hunch net-2009-06-01-Multitask Poisoning</a></p>
<p>10 0.087278962 <a title="302-tfidf-10" href="../hunch_net-2009/hunch_net-2009-09-29-Machine_Learning_Protests_at_the_G20.html">372 hunch net-2009-09-29-Machine Learning Protests at the G20</a></p>
<p>11 0.085224532 <a title="302-tfidf-11" href="../hunch_net-2005/hunch_net-2005-02-02-Kolmogorov_Complexity_and_Googling.html">10 hunch net-2005-02-02-Kolmogorov Complexity and Googling</a></p>
<p>12 0.072767355 <a title="302-tfidf-12" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>13 0.072410807 <a title="302-tfidf-13" href="../hunch_net-2009/hunch_net-2009-10-03-Static_vs._Dynamic_multiclass_prediction.html">373 hunch net-2009-10-03-Static vs. Dynamic multiclass prediction</a></p>
<p>14 0.072295681 <a title="302-tfidf-14" href="../hunch_net-2005/hunch_net-2005-03-18-Binomial_Weighting.html">43 hunch net-2005-03-18-Binomial Weighting</a></p>
<p>15 0.071593627 <a title="302-tfidf-15" href="../hunch_net-2010/hunch_net-2010-09-17-New_York_Area_Machine_Learning_Events.html">410 hunch net-2010-09-17-New York Area Machine Learning Events</a></p>
<p>16 0.066130698 <a title="302-tfidf-16" href="../hunch_net-2008/hunch_net-2008-12-07-A_NIPS_paper.html">330 hunch net-2008-12-07-A NIPS paper</a></p>
<p>17 0.064635195 <a title="302-tfidf-17" href="../hunch_net-2008/hunch_net-2008-04-27-Watchword%3A_Supervised_Learning.html">299 hunch net-2008-04-27-Watchword: Supervised Learning</a></p>
<p>18 0.063911505 <a title="302-tfidf-18" href="../hunch_net-2007/hunch_net-2007-01-15-The_Machine_Learning_Department.html">228 hunch net-2007-01-15-The Machine Learning Department</a></p>
<p>19 0.06297259 <a title="302-tfidf-19" href="../hunch_net-2006/hunch_net-2006-09-19-Luis_von_Ahn_is_awarded_a_MacArthur_fellowship..html">209 hunch net-2006-09-19-Luis von Ahn is awarded a MacArthur fellowship.</a></p>
<p>20 0.062051553 <a title="302-tfidf-20" href="../hunch_net-2008/hunch_net-2008-10-14-Who_is_Responsible_for_a_Bad_Review%3F.html">320 hunch net-2008-10-14-Who is Responsible for a Bad Review?</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.134), (1, -0.002), (2, 0.026), (3, -0.036), (4, 0.033), (5, -0.013), (6, -0.057), (7, 0.005), (8, 0.049), (9, -0.051), (10, 0.002), (11, 0.055), (12, 0.04), (13, -0.112), (14, -0.053), (15, -0.038), (16, -0.026), (17, 0.112), (18, -0.13), (19, 0.034), (20, -0.009), (21, 0.044), (22, -0.089), (23, -0.004), (24, 0.008), (25, -0.016), (26, -0.033), (27, 0.015), (28, -0.15), (29, -0.014), (30, -0.058), (31, 0.054), (32, -0.086), (33, -0.061), (34, -0.094), (35, 0.015), (36, -0.127), (37, 0.028), (38, -0.011), (39, 0.135), (40, 0.017), (41, -0.11), (42, 0.003), (43, 0.114), (44, 0.047), (45, 0.026), (46, -0.015), (47, 0.013), (48, -0.024), (49, -0.051)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94628352 <a title="302-lsi-1" href="../hunch_net-2008/hunch_net-2008-05-25-Inappropriate_Mathematics_for_Machine_Learning.html">302 hunch net-2008-05-25-Inappropriate Mathematics for Machine Learning</a></p>
<p>Introduction: Reviewers and students are sometimes greatly concerned by the distinction
between:Anopen setand aclosed set.ASupremumand aMaximum.An event which happens
with probability 1 and an event that always happens.I don't appreciate this
distinction in machine learning & learning theory. All machine learning takes
place (by definition) on a machine where every parameter has finite precision.
Consequently, every set is closed, a maximal element always exists, and
probability 1 events always happen.The fundamental issue here is that
substantial parts of mathematics don't appear well-matched to computation in
the physical world, because the mathematics has concerns which are unphysical.
This mismatched mathematics makes irrelevant distinctions. We can ask "what
mathematics is appropriate to computation?"Andrejhas convinced me that a
pretty good answer to this question isconstructive mathematics.So, here's a
basic challenge: Can anyone name a situation where any of the distinctions
above (or simila</p><p>2 0.64899951 <a title="302-lsi-2" href="../hunch_net-2010/hunch_net-2010-10-08-An_easy_proof_of_the_Chernoff-Hoeffding_bound.html">413 hunch net-2010-10-08-An easy proof of the Chernoff-Hoeffding bound</a></p>
<p>Introduction: Textbooks invariably seem to carry the proof that uses Markov's inequality,
moment-generating functions, and Taylor approximations. Here's an easier
way.For, letbe the KL divergence between a coin of biasand one of
bias:Theorem:Suppose you doindependent tosses of a coin of bias. The
probability of seeingheads or more, for, is at most. So is the probability of
seeingheads or less, for.Remark:By Pinsker's inequality,.ProofLet's do
thecase; the other is identical.Letbe the distribution overinduced by a coin
of bias, and likewisefor a coin of bias. Letbe the set of all sequences
oftosses which containheads or more. We'd like to show thatis unlikely
under.Pick any, with sayheads. Then:Sincefor every, we haveand we're done.</p><p>3 0.62628168 <a title="302-lsi-3" href="../hunch_net-2005/hunch_net-2005-04-10-Is_the_Goal_Understanding_or_Prediction%3F.html">55 hunch net-2005-04-10-Is the Goal Understanding or Prediction?</a></p>
<p>Introduction: Steve Smaleand I have a debate about goals of learning theory.Steve likes
theorems with a dependence on unobservable quantities. For example, ifDis a
distribution over a spaceX x [0,1], you can state a theorem about the error
rate dependent on the variance,E(x,y)~D(y-Ey'~D|x[y'])2.I dislike this,
because I want to use the theorems to produce code solving learning problems.
Since I don't know (and can't measure) the variance, a theorem depending on
the variance does not help me--I would not know what variance to plug into the
learning algorithm.Recast more broadly, this is a debate between "declarative"
and "operative" mathematics. A strong example of "declarative" mathematics
is"a new kind of science". Roughly speaking, the goal of this kind of approach
seems to be finding a way to explain the observations we make. Examples
include "some things are unpredictable", "a phase transition exists",
etcâ&euro;Ś"Operative" mathematics helps you make predictions about the world. A
strong example of op</p><p>4 0.58627063 <a title="302-lsi-4" href="../hunch_net-2005/hunch_net-2005-07-07-The_Limits_of_Learning_Theory.html">90 hunch net-2005-07-07-The Limits of Learning Theory</a></p>
<p>Introduction: Suppose we had an infinitely powerful mathematician sitting in a room and
proving theorems about learning. Could he solve machine learning?The answer is
"no". This answer is both obvious and sometimes underappreciated.There are
several ways to conclude that somebiasis necessary in order to succesfully
learn. For example, suppose we are trying to solve classification. At
prediction time, we observe some featuresXand want to make a prediction of
either0or1. Bias is what makes us prefer one answer over the other based on
past experience. In order to learn we must:Have a bias. Always predicting0is
as likely as1is useless.Have the "right" bias. Predicting1when the answer
is0is also not helpful.The implication of "have a bias" is that we can not
design effective learning algorithms with "a uniform prior over all
possibilities". The implication of "have the 'right' bias" is that our
mathematician fails since "right" is defined with respect to the solutions to
problems encountered in the real</p><p>5 0.56447095 <a title="302-lsi-5" href="../hunch_net-2005/hunch_net-2005-01-26-Watchword%3A_Probability.html">5 hunch net-2005-01-26-Watchword: Probability</a></p>
<p>Introduction: Probability is one of the most confusingly used words in machine learning.
There are at least 3 distinct ways the word is used.BayesianThe Bayesian
notion of probability is a 'degree of belief'. The degree of belief that some
event (i.e. "stock goes up" or "stock goes down") occurs can be measured by
asking a sequence of questions of the form "Would you bet the stock goes up or
down atYto 1 odds?" A consistent better will switch from 'for' to 'against' at
some single value ofY. The probability is thenY/(Y+1). Bayesian probabilities
express lack of knowledge rather than randomization. They are useful in
learning because we often lack knowledge and expressing that lack flexibly
makes the learning algorithms work better. Bayesian Learning uses
'probability' in this way exclusively.FrequentistThe Frequentist notion of
probability is a rate of occurence. A rate of occurrence can be measured by
doing an experiment many times. If an event occursktimes innexperiments then
it has probability ab</p><p>6 0.5032438 <a title="302-lsi-6" href="../hunch_net-2005/hunch_net-2005-02-02-Kolmogorov_Complexity_and_Googling.html">10 hunch net-2005-02-02-Kolmogorov Complexity and Googling</a></p>
<p>7 0.49871337 <a title="302-lsi-7" href="../hunch_net-2009/hunch_net-2009-09-29-Machine_Learning_Protests_at_the_G20.html">372 hunch net-2009-09-29-Machine Learning Protests at the G20</a></p>
<p>8 0.4855198 <a title="302-lsi-8" href="../hunch_net-2008/hunch_net-2008-02-27-The_Stats_Handicap.html">290 hunch net-2008-02-27-The Stats Handicap</a></p>
<p>9 0.46805242 <a title="302-lsi-9" href="../hunch_net-2008/hunch_net-2008-12-07-A_NIPS_paper.html">330 hunch net-2008-12-07-A NIPS paper</a></p>
<p>10 0.45804077 <a title="302-lsi-10" href="../hunch_net-2005/hunch_net-2005-05-12-Math_on_the_Web.html">70 hunch net-2005-05-12-Math on the Web</a></p>
<p>11 0.45494431 <a title="302-lsi-11" href="../hunch_net-2007/hunch_net-2007-01-15-The_Machine_Learning_Department.html">228 hunch net-2007-01-15-The Machine Learning Department</a></p>
<p>12 0.44222409 <a title="302-lsi-12" href="../hunch_net-2008/hunch_net-2008-04-27-Watchword%3A_Supervised_Learning.html">299 hunch net-2008-04-27-Watchword: Supervised Learning</a></p>
<p>13 0.43123204 <a title="302-lsi-13" href="../hunch_net-2005/hunch_net-2005-04-26-To_calibrate_or_not%3F.html">62 hunch net-2005-04-26-To calibrate or not?</a></p>
<p>14 0.4303194 <a title="302-lsi-14" href="../hunch_net-2006/hunch_net-2006-04-09-Progress_in_Machine_Translation.html">171 hunch net-2006-04-09-Progress in Machine Translation</a></p>
<p>15 0.41683623 <a title="302-lsi-15" href="../hunch_net-2006/hunch_net-2006-04-02-Mad_%28Neuro%29science.html">168 hunch net-2006-04-02-Mad (Neuro)science</a></p>
<p>16 0.41573316 <a title="302-lsi-16" href="../hunch_net-2006/hunch_net-2006-03-17-Multitask_learning_is_Black-Boxable.html">164 hunch net-2006-03-17-Multitask learning is Black-Boxable</a></p>
<p>17 0.40974736 <a title="302-lsi-17" href="../hunch_net-2006/hunch_net-2006-04-05-What_is_state%3F.html">169 hunch net-2006-04-05-What is state?</a></p>
<p>18 0.40473843 <a title="302-lsi-18" href="../hunch_net-2009/hunch_net-2009-04-23-Jonathan_Chang_at_Slycoder.html">350 hunch net-2009-04-23-Jonathan Chang at Slycoder</a></p>
<p>19 0.39757842 <a title="302-lsi-19" href="../hunch_net-2006/hunch_net-2006-11-20-Context_and_the_calculation_misperception.html">218 hunch net-2006-11-20-Context and the calculation misperception</a></p>
<p>20 0.39662042 <a title="302-lsi-20" href="../hunch_net-2005/hunch_net-2005-10-20-Machine_Learning_in_the_News.html">125 hunch net-2005-10-20-Machine Learning in the News</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(42, 0.32), (45, 0.044), (64, 0.4), (74, 0.104)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.91982937 <a title="302-lda-1" href="../hunch_net-2008/hunch_net-2008-11-04-Rise_of_the_Machines.html">323 hunch net-2008-11-04-Rise of the Machines</a></p>
<p>Introduction: On theenduring topic of how people deal with intelligent machines, we have
this importantelection bulletin.</p><p>same-blog 2 0.9108566 <a title="302-lda-2" href="../hunch_net-2008/hunch_net-2008-05-25-Inappropriate_Mathematics_for_Machine_Learning.html">302 hunch net-2008-05-25-Inappropriate Mathematics for Machine Learning</a></p>
<p>Introduction: Reviewers and students are sometimes greatly concerned by the distinction
between:Anopen setand aclosed set.ASupremumand aMaximum.An event which happens
with probability 1 and an event that always happens.I don't appreciate this
distinction in machine learning & learning theory. All machine learning takes
place (by definition) on a machine where every parameter has finite precision.
Consequently, every set is closed, a maximal element always exists, and
probability 1 events always happen.The fundamental issue here is that
substantial parts of mathematics don't appear well-matched to computation in
the physical world, because the mathematics has concerns which are unphysical.
This mismatched mathematics makes irrelevant distinctions. We can ask "what
mathematics is appropriate to computation?"Andrejhas convinced me that a
pretty good answer to this question isconstructive mathematics.So, here's a
basic challenge: Can anyone name a situation where any of the distinctions
above (or simila</p><p>3 0.89076251 <a title="302-lda-3" href="../hunch_net-2009/hunch_net-2009-04-23-Jonathan_Chang_at_Slycoder.html">350 hunch net-2009-04-23-Jonathan Chang at Slycoder</a></p>
<p>Introduction: Jonathan Changhas aresearch blogon aspects of machine learning.</p><p>4 0.84729326 <a title="302-lda-4" href="../hunch_net-2010/hunch_net-2010-02-26-Yahoo%21_ML_events.html">389 hunch net-2010-02-26-Yahoo! ML events</a></p>
<p>Introduction: Yahoo! is sponsoring two machine learning events that might interest
people.TheKey Scientific Challengesprogram (due March 5) forMachine
LearningandStatisticsoffers $5K (plus bonuses) for graduate students working
on a core problem of interest to Y! If you are already working on one of these
problems, there is no reason not to submit, and if you aren't you might want
to think about it for next year, as I am confident they all press the boundary
of the possible in Machine Learning. There are 7 days left.TheLearning to Rank
challenge(due May 31) offers an $8K first prize for the best ranking algorithm
on a real (and really used) dataset for search ranking, with presentations at
an ICML workshop. Unlike the Netflix competition, there are prizes for 2nd,
3rd, and 4th place, perhaps avoiding the heartbreakthe ensembleencountered. If
you think you know how to rank, you should give it a try, and we might all
learn something. There are 3 months left.</p><p>5 0.819803 <a title="302-lda-5" href="../hunch_net-2011/hunch_net-2011-08-15-Vowpal_Wabbit_6.0.html">441 hunch net-2011-08-15-Vowpal Wabbit 6.0</a></p>
<p>Introduction: I just releasedVowpal Wabbit 6.0. Since the last version:VW is now 2-3 orders
of magnitude faster at linear learning, primarily thanks toAlekh. Given the
baseline, this is loads of fun, allowing us to easily deal with terafeature
datasets, and dwarfing the scale of any other open source projects. The core
improvement here comes from effective parallelization over kilonode clusters
(eitherHadoopor not). This code is highly scalable, so it even helps with
clusters of size 2 (and doesn't hurt for clusters of size 1). The core
allreduce technique appears widely and easily reused--we've already used it to
parallelize Conjugate Gradient, LBFGS, and two variants of online learning.
We'll be documenting how to do this more thoroughly, but for now
"README_cluster" and associated scripts should provide a good starting
point.The newLBFGScode fromMiroseems to commonly dominate the existing
conjugate gradient code in time/quality tradeoffs.The new matrix factorization
code fromJakeadds a core algor</p><p>6 0.77644169 <a title="302-lda-6" href="../hunch_net-2010/hunch_net-2010-12-26-NIPS_2010.html">420 hunch net-2010-12-26-NIPS 2010</a></p>
<p>7 0.77321088 <a title="302-lda-7" href="../hunch_net-2007/hunch_net-2007-06-24-Interesting_Papers_at_ICML_2007.html">251 hunch net-2007-06-24-Interesting Papers at ICML 2007</a></p>
<p>8 0.65423071 <a title="302-lda-8" href="../hunch_net-2011/hunch_net-2011-03-19-The_Ideal_Large_Scale_Learning_Class.html">426 hunch net-2011-03-19-The Ideal Large Scale Learning Class</a></p>
<p>9 0.63535064 <a title="302-lda-9" href="../hunch_net-2009/hunch_net-2009-06-15-In_Active_Learning%2C_the_question_changes.html">360 hunch net-2009-06-15-In Active Learning, the question changes</a></p>
<p>10 0.63430142 <a title="302-lda-10" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>11 0.63368779 <a title="302-lda-11" href="../hunch_net-2009/hunch_net-2009-03-26-Machine_Learning_is_too_easy.html">347 hunch net-2009-03-26-Machine Learning is too easy</a></p>
<p>12 0.63281399 <a title="302-lda-12" href="../hunch_net-2006/hunch_net-2006-02-24-A_Fundamentalist_Organization_of_Machine_Learning.html">158 hunch net-2006-02-24-A Fundamentalist Organization of Machine Learning</a></p>
<p>13 0.63257456 <a title="302-lda-13" href="../hunch_net-2008/hunch_net-2008-12-23-Use_of_Learning_Theory.html">332 hunch net-2008-12-23-Use of Learning Theory</a></p>
<p>14 0.6309942 <a title="302-lda-14" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>15 0.6300689 <a title="302-lda-15" href="../hunch_net-2009/hunch_net-2009-07-31-Vowpal_Wabbit_Open_Source_Project.html">365 hunch net-2009-07-31-Vowpal Wabbit Open Source Project</a></p>
<p>16 0.62973821 <a title="302-lda-16" href="../hunch_net-2009/hunch_net-2009-11-15-The_Other_Online_Learning.html">378 hunch net-2009-11-15-The Other Online Learning</a></p>
<p>17 0.62927282 <a title="302-lda-17" href="../hunch_net-2005/hunch_net-2005-03-21-Research_Styles_in_Machine_Learning.html">44 hunch net-2005-03-21-Research Styles in Machine Learning</a></p>
<p>18 0.62870663 <a title="302-lda-18" href="../hunch_net-2005/hunch_net-2005-01-27-Learning_Complete_Problems.html">6 hunch net-2005-01-27-Learning Complete Problems</a></p>
<p>19 0.62843758 <a title="302-lda-19" href="../hunch_net-2006/hunch_net-2006-04-05-What_is_state%3F.html">169 hunch net-2006-04-05-What is state?</a></p>
<p>20 0.62826395 <a title="302-lda-20" href="../hunch_net-2011/hunch_net-2011-04-20-The_End_of_the_Beginning_of_Active_Learning.html">432 hunch net-2011-04-20-The End of the Beginning of Active Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
