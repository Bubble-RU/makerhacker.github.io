<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>306 hunch net-2008-07-02-Proprietary Data in Academic Research?</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2008" href="../home/hunch_net-2008_home.html">hunch_net-2008</a> <a title="hunch_net-2008-306" href="#">hunch_net-2008-306</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>306 hunch net-2008-07-02-Proprietary Data in Academic Research?</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2008-306-html" href="http://hunch.net/?p=336">html</a></p><p>Introduction: Should results of experiments on proprietary datasets be in the academic
research literature?The arguments I can imagine in the "against" column
are:Experiments are not repeatable. Repeatability in experiments is essential
to science because it allows others to compare new methods with old and
discover which is better.It's unfair. Academics who don't have insider access
to proprietary data are at a substantial disadvantage when competing with
others who do.I'm unsympathetic to argument (2). To me, it looks like their
are simply some resource constraints, and these should not prevent research
progress. For example, we wouldn't prevent publishing about particle
accelerator experiments by physicists atCERNbecause physicists atCMUcouldn't
run their own experiments.Argument (1) seems like a real issue.The argument
for is:Yes, they are another form of evidence that an algorithm is good. The
degree to which they are evidence is less than for publicly repeatable
experiments, but greater than n</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('proprietary', 0.502), ('experiments', 0.419), ('arguments', 0.26), ('physicists', 0.232), ('prevent', 0.209), ('others', 0.146), ('argument', 0.144), ('column', 0.125), ('research', 0.114), ('evidence', 0.112), ('trends', 0.11), ('grows', 0.11), ('resource', 0.105), ('academics', 0.1), ('literature', 0.097), ('society', 0.094), ('publicly', 0.091), ('disadvantage', 0.091), ('competing', 0.091), ('starts', 0.089)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="306-tfidf-1" href="../hunch_net-2008/hunch_net-2008-07-02-Proprietary_Data_in_Academic_Research%3F.html">306 hunch net-2008-07-02-Proprietary Data in Academic Research?</a></p>
<p>Introduction: Should results of experiments on proprietary datasets be in the academic
research literature?The arguments I can imagine in the "against" column
are:Experiments are not repeatable. Repeatability in experiments is essential
to science because it allows others to compare new methods with old and
discover which is better.It's unfair. Academics who don't have insider access
to proprietary data are at a substantial disadvantage when competing with
others who do.I'm unsympathetic to argument (2). To me, it looks like their
are simply some resource constraints, and these should not prevent research
progress. For example, we wouldn't prevent publishing about particle
accelerator experiments by physicists atCERNbecause physicists atCMUcouldn't
run their own experiments.Argument (1) seems like a real issue.The argument
for is:Yes, they are another form of evidence that an algorithm is good. The
degree to which they are evidence is less than for publicly repeatable
experiments, but greater than n</p><p>2 0.13361786 <a title="306-tfidf-2" href="../hunch_net-2005/hunch_net-2005-09-26-Prediction_Bounds_as_the_Mathematics_of_Science.html">115 hunch net-2005-09-26-Prediction Bounds as the Mathematics of Science</a></p>
<p>Introduction: "Science" has many meanings, but one common meaning is "thescientific method"
which is a principled method for investigating the world using the following
steps:Form a hypothesis about the world.Use the hypothesis to make
predictions.Run experiments to confirm or disprove the predictions.The
ordering of these steps is very important to the scientific method. In
particular, predictionsmustbe made before experiments are run.Given that we
all believe in the scientific method of investigation, it may be surprising to
learn that cheating is very common. This happens for many reasons, some
innocent and some not.Drug studies. Pharmaceutical companies make predictions
about the effects of their drugs and then conduct blind clinical studies to
determine their effect. Unfortunately, they have also been caught using some
of the more advanced techniques for cheatinghere: including "reprobleming",
"data set selection", and probably "overfitting by review". It isn't too
surprising to observe this: w</p><p>3 0.1084669 <a title="306-tfidf-3" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>Introduction: Researchers are typically confronted with big problems that they have no idea
how to solve. In trying to come up with a solution, a natural approach is to
decompose the big problem into a set of subproblems whose solution yields a
solution to the larger problem. This approach can go wrong in several
ways.Decomposition failure. The solution to the decomposition does not in fact
yield a solution to the overall problem.Artificial hardness. The subproblems
created are sufficient if solved to solve the overall problem, but they are
harder than necessary.As you can see, computational complexity forms a
relatively new (in research-history) razor by which to judge an approach
sufficient but not necessary.In my experience, the artificial hardness problem
is very common. Many researchers abdicate the responsibility of choosing a
problem to work on to other people. This process starts very naturally as a
graduate student, when an incoming student might have relatively little idea
about how to do</p><p>4 0.090009093 <a title="306-tfidf-4" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>Introduction: Normally, I don't indulge in posters forICML, but this year is naturally an
exception for me. If you want one, there are a small numberleft here, if you
sign up before February.It also seems worthwhile to give some sense of the
scope and reviewing criteria for ICML for authors considering submitting
papers. At ICML, the (very large) program committee does the reviewing which
informs final decisions by area chairs on most papers. Program chairs setup
the process, deal with exceptions or disagreements, and provide advice for the
reviewing process. Providing advice is tricky (and easily misleading) because
a conference is a community, and in the end the aggregate interests of the
community determine the conference. Nevertheless, as a program chair this year
it seems worthwhile to state the overall philosophy I have and what I plan to
encourage (and occasionally discourage).At the highest level, I believe ICML
exists to further research into machine learning, which I generally think of
as</p><p>5 0.0833534 <a title="306-tfidf-5" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>Introduction: This post is about a general technique for problem solving which I've never
seen taught (in full generality), but which I've found very useful.Many
problems in computer science turn out to be discretely difficult. The best
known version of such problems are NP-hard problems, but I mean 'discretely
difficult' in a much more general way, which I only know how to capture by
examples.ERMIn empirical risk minimization, you choose a minimum error rate
classifier from a set of classifiers. This is NP hard for common sets, but it
can be much harder, depending on the set.ExpertsIn the online learning with
experts setting, you try to predict well so as to compete with a set of
(adversarial) experts. Here the alternating quantifiers of you and an
adversary playing out a game can yield a dynamic programming problem that
grows exponentially.Policy IterationThe problem with policy iteration is that
you learn a new policy with respect to an old policy, which implies that
simply adopting the new polic</p><p>6 0.083075777 <a title="306-tfidf-6" href="../hunch_net-2005/hunch_net-2005-04-06-Structured_Regret_Minimization.html">53 hunch net-2005-04-06-Structured Regret Minimization</a></p>
<p>7 0.082657233 <a title="306-tfidf-7" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>8 0.082469098 <a title="306-tfidf-8" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>9 0.081942543 <a title="306-tfidf-9" href="../hunch_net-2008/hunch_net-2008-11-10-ICML_Reviewing_Criteria.html">325 hunch net-2008-11-10-ICML Reviewing Criteria</a></p>
<p>10 0.079028256 <a title="306-tfidf-10" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>11 0.077804729 <a title="306-tfidf-11" href="../hunch_net-2008/hunch_net-2008-04-12-It_Doesn%26%238217%3Bt_Stop.html">295 hunch net-2008-04-12-It Doesn&#8217;t Stop</a></p>
<p>12 0.077112511 <a title="306-tfidf-12" href="../hunch_net-2008/hunch_net-2008-09-03-Bidding_Problems.html">315 hunch net-2008-09-03-Bidding Problems</a></p>
<p>13 0.075296745 <a title="306-tfidf-13" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>14 0.07471747 <a title="306-tfidf-14" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>15 0.073360845 <a title="306-tfidf-15" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>16 0.072823614 <a title="306-tfidf-16" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>17 0.069468081 <a title="306-tfidf-17" href="../hunch_net-2005/hunch_net-2005-09-10-%26%238220%3BFailure%26%238221%3B_is_an_option.html">110 hunch net-2005-09-10-&#8220;Failure&#8221; is an option</a></p>
<p>18 0.069394588 <a title="306-tfidf-18" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>19 0.068756051 <a title="306-tfidf-19" href="../hunch_net-2005/hunch_net-2005-03-17-Going_all_the_Way%2C_Sometimes.html">42 hunch net-2005-03-17-Going all the Way, Sometimes</a></p>
<p>20 0.068464912 <a title="306-tfidf-20" href="../hunch_net-2006/hunch_net-2006-05-05-An_ICML_reject.html">177 hunch net-2006-05-05-An ICML reject</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.165), (1, 0.025), (2, 0.036), (3, -0.078), (4, 0.022), (5, -0.024), (6, 0.009), (7, 0.019), (8, -0.05), (9, -0.057), (10, -0.013), (11, 0.011), (12, -0.052), (13, 0.013), (14, 0.067), (15, 0.0), (16, -0.052), (17, -0.02), (18, 0.052), (19, -0.015), (20, 0.037), (21, -0.047), (22, 0.013), (23, -0.022), (24, -0.002), (25, 0.016), (26, 0.055), (27, -0.026), (28, 0.023), (29, -0.047), (30, 0.015), (31, 0.076), (32, 0.033), (33, 0.048), (34, 0.033), (35, -0.028), (36, 0.068), (37, 0.053), (38, 0.01), (39, 0.024), (40, -0.014), (41, -0.033), (42, -0.013), (43, -0.051), (44, -0.043), (45, 0.025), (46, 0.004), (47, -0.022), (48, -0.062), (49, -0.025)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95857823 <a title="306-lsi-1" href="../hunch_net-2008/hunch_net-2008-07-02-Proprietary_Data_in_Academic_Research%3F.html">306 hunch net-2008-07-02-Proprietary Data in Academic Research?</a></p>
<p>Introduction: Should results of experiments on proprietary datasets be in the academic
research literature?The arguments I can imagine in the "against" column
are:Experiments are not repeatable. Repeatability in experiments is essential
to science because it allows others to compare new methods with old and
discover which is better.It's unfair. Academics who don't have insider access
to proprietary data are at a substantial disadvantage when competing with
others who do.I'm unsympathetic to argument (2). To me, it looks like their
are simply some resource constraints, and these should not prevent research
progress. For example, we wouldn't prevent publishing about particle
accelerator experiments by physicists atCERNbecause physicists atCMUcouldn't
run their own experiments.Argument (1) seems like a real issue.The argument
for is:Yes, they are another form of evidence that an algorithm is good. The
degree to which they are evidence is less than for publicly repeatable
experiments, but greater than n</p><p>2 0.69171709 <a title="306-lsi-2" href="../hunch_net-2005/hunch_net-2005-09-04-Science_in_the_Government.html">106 hunch net-2005-09-04-Science in the Government</a></p>
<p>Introduction: I found the article on "Political Science" at theNew York Timesinteresting.
Essentially the article is about allegations that the US government has been
systematically distorting scientific views. With apetitionby some7000+
scientistsalleging such behavior this is clearly a significant concern.One
thing not mentioned explicitly in this discussion is that there are
fundamental cultural differences between academic research and the rest of the
world. In academic research, careful, clear thought is valued. This value is
achieved by both formal and informal mechanisms. One example of a formal
mechanism is peer review.In contrast, in the land of politics, the basic value
is agreement. It is only with some amount of agreement that a new law can be
passed or other actions can be taken. Since Science (with a capitol 'S') has
accomplished many things, it can be a significant tool in persuading people.
This makes it compelling for a politician to use science as a mechanism for
pushing agreement</p><p>3 0.65804106 <a title="306-lsi-3" href="../hunch_net-2007/hunch_net-2007-04-28-The_Coming_Patent_Apocalypse.html">241 hunch net-2007-04-28-The Coming Patent Apocalypse</a></p>
<p>Introduction: Many people in computer science believe that patents are problematic. The
truth is even worse--the patent system in the US is fundamentally broken in
ways that will require much more significant reform thanis being considered
now.The myth of the patent is the following: Patents are a mechanism for
inventors to be compensated according to the value of their inventions while
making the invention available to all. This myth sounds pretty desirable, but
the reality is a strange distortion slowly leading towards collapse.There are
many problems associated with patents, but I would like to focus on just two
of them:Patent TrollsThe way that patents have generally worked over the last
several decades is that they were a tool of large companies. Large companies
would amass a large number of patents and then cross-license each other's
patents--in effect saying "we agree to owe each other nothing". Smaller
companies would sometimes lose in this game, essentially because they didn't
have enough p</p><p>4 0.62952656 <a title="306-lsi-4" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>Introduction: The internet has significantly effected the way we do research but it's
capabilities have not yet been fully realized.First, let's acknowledge some
known effects.Self-publishingBy default, all researchers in machine learning
(and more generally computer science and physics) place their papers online
for anyone to download. The exact mechanism differs--physicists tend to use a
central repository (Arxiv) while computer scientists tend to place the papers
on their webpage. Arxiv has been slowly growing in subject breadth so it now
sometimes used by computer scientists.CollaborationEmail has enabled working
remotely with coauthors. This has allowed collaborationis which would not
otherwise have been possible and generally speeds research.Now, let's look at
attempts to go further.Blogs(like this one) allow public discussion about
topics which are not easily categorized as "a new idea in machine learning"
(like this topic).Organizationof some subfield of research. This
includesSatinder Singh</p><p>5 0.6282059 <a title="306-lsi-5" href="../hunch_net-2005/hunch_net-2005-01-19-Why_I_decided_to_run_a_weblog..html">1 hunch net-2005-01-19-Why I decided to run a weblog.</a></p>
<p>Introduction: I have decided to run a weblog on machine learning and learning theory
research. Here are some reasons:1) Weblogs enable new functionality:Public
comment on papers. No mechanism for this exists at conferences and most
journals. I have encountered it once for asciencepaper. Some communities have
mailing lists supporting this, but not machine learning or learning theory. I
have often read papers and found myself wishing there was some method to
consider other's questions and read the replies.Conference shortlists. One of
the most common conversations at a conference is "what did you find
interesting?" There is no explicit mechanism for sharing this information at
conferences, and it's easy to imagine that it would be handy to do
so.Evaluation and comment on research directions. Papers are almost
exclusively about new research, rather than evaluation (and consideration) of
research directions. This last role is satisfied by funding agencies to some
extent, but that is a private debate of</p><p>6 0.61765122 <a title="306-lsi-6" href="../hunch_net-2005/hunch_net-2005-03-29-Academic_Mechanism_Design.html">48 hunch net-2005-03-29-Academic Mechanism Design</a></p>
<p>7 0.61360168 <a title="306-lsi-7" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>8 0.60358727 <a title="306-lsi-8" href="../hunch_net-2008/hunch_net-2008-04-12-It_Doesn%26%238217%3Bt_Stop.html">295 hunch net-2008-04-12-It Doesn&#8217;t Stop</a></p>
<p>9 0.59955841 <a title="306-lsi-9" href="../hunch_net-2005/hunch_net-2005-04-01-The_Producer-Consumer_Model_of_Research.html">51 hunch net-2005-04-01-The Producer-Consumer Model of Research</a></p>
<p>10 0.58771294 <a title="306-lsi-10" href="../hunch_net-2006/hunch_net-2006-09-18-What_is_missing_for_online_collaborative_research%3F.html">208 hunch net-2006-09-18-What is missing for online collaborative research?</a></p>
<p>11 0.58048958 <a title="306-lsi-11" href="../hunch_net-2005/hunch_net-2005-09-26-Prediction_Bounds_as_the_Mathematics_of_Science.html">115 hunch net-2005-09-26-Prediction Bounds as the Mathematics of Science</a></p>
<p>12 0.57844216 <a title="306-lsi-12" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>13 0.57413137 <a title="306-lsi-13" href="../hunch_net-2008/hunch_net-2008-01-06-Research_Political_Issues.html">282 hunch net-2008-01-06-Research Political Issues</a></p>
<p>14 0.56754351 <a title="306-lsi-14" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>15 0.56604618 <a title="306-lsi-15" href="../hunch_net-2013/hunch_net-2013-06-29-The_Benefits_of_Double-Blind_Review.html">485 hunch net-2013-06-29-The Benefits of Double-Blind Review</a></p>
<p>16 0.55382359 <a title="306-lsi-16" href="../hunch_net-2005/hunch_net-2005-02-18-What_it_means_to_do_research..html">22 hunch net-2005-02-18-What it means to do research.</a></p>
<p>17 0.55318874 <a title="306-lsi-17" href="../hunch_net-2010/hunch_net-2010-10-17-Partha_Niyogi_has_died.html">414 hunch net-2010-10-17-Partha Niyogi has died</a></p>
<p>18 0.55190319 <a title="306-lsi-18" href="../hunch_net-2005/hunch_net-2005-05-29-Bad_ideas.html">76 hunch net-2005-05-29-Bad ideas</a></p>
<p>19 0.54929435 <a title="306-lsi-19" href="../hunch_net-2007/hunch_net-2007-07-13-The_View_From_China.html">255 hunch net-2007-07-13-The View From China</a></p>
<p>20 0.54737204 <a title="306-lsi-20" href="../hunch_net-2008/hunch_net-2008-04-21-The_Science_2.0_article.html">296 hunch net-2008-04-21-The Science 2.0 article</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(35, 0.034), (42, 0.198), (45, 0.027), (68, 0.076), (74, 0.154), (76, 0.353), (88, 0.011), (95, 0.031)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.99231422 <a title="306-lda-1" href="../hunch_net-2010/hunch_net-2010-10-28-NY_ML_Symposium_2010.html">415 hunch net-2010-10-28-NY ML Symposium 2010</a></p>
<p>Introduction: About 200 people attended the2010 NYAS ML Symposiumthis year. (It wasabout 170
last year.) I particularly enjoyed several talks.Yannhas a new live demo of
(limited) real-time object recognition learning.Sanjoygave a fairly convincing
and comprehensible explanation of why amodified form of single-linkage
clusteringis consistent in higher dimensions, and why consistency is a
critical feature for clustering algorithms. I'm curious how well this
algorithm works in practice.Matt Hoffman's poster covering online LDA seemed
pretty convincing to me as an algorithmic improvement.This year, we allocated
more time towards posters & poster spotlights.For next year, we are
considering some further changes. The format has traditionally been 4 invited
Professor speakers, with posters and poster spotlight for students. Demand
from other parties to participate is growing, for example from postdocs and
startups in the area. Another growing concern is the facility--the location is
exceptional, but fittin</p><p>2 0.90141296 <a title="306-lda-2" href="../hunch_net-2006/hunch_net-2006-02-18-Multiplication_of_Learned_Probabilities_is_Dangerous.html">157 hunch net-2006-02-18-Multiplication of Learned Probabilities is Dangerous</a></p>
<p>Introduction: This is about a design flaw in several learning algorithms such as the Naive
Bayes classifier and Hidden Markov Models. A number of people are aware of it,
but it seems that not everyone is.Several learning systems have the property
that they estimate some conditional probabilitiesP(event | other events)either
explicitly or implicitly. Then, at prediction time, these learned
probabilities are multiplied together according to some formula to produce a
final prediction. The Naive Bayes classifier for binary data is the simplest
of these, so it seems like a good example.When Naive Bayes is used, a set of
probabilities of the formPr'(feature i | label)are estimated via counting
statistics and some prior. Predictions are made according to the label
maximizing:Pr'(label) * Productfeatures iPr'(feature i | label)(ThePr'notation
indicates these are estimated values.)There is nothing wrong with this method
as long as (a) the prior for the sample counts is very strong and (b) the
prior (on the c</p><p>same-blog 3 0.90018702 <a title="306-lda-3" href="../hunch_net-2008/hunch_net-2008-07-02-Proprietary_Data_in_Academic_Research%3F.html">306 hunch net-2008-07-02-Proprietary Data in Academic Research?</a></p>
<p>Introduction: Should results of experiments on proprietary datasets be in the academic
research literature?The arguments I can imagine in the "against" column
are:Experiments are not repeatable. Repeatability in experiments is essential
to science because it allows others to compare new methods with old and
discover which is better.It's unfair. Academics who don't have insider access
to proprietary data are at a substantial disadvantage when competing with
others who do.I'm unsympathetic to argument (2). To me, it looks like their
are simply some resource constraints, and these should not prevent research
progress. For example, we wouldn't prevent publishing about particle
accelerator experiments by physicists atCERNbecause physicists atCMUcouldn't
run their own experiments.Argument (1) seems like a real issue.The argument
for is:Yes, they are another form of evidence that an algorithm is good. The
degree to which they are evidence is less than for publicly repeatable
experiments, but greater than n</p><p>4 0.85825098 <a title="306-lda-4" href="../hunch_net-2006/hunch_net-2006-03-23-The_Approximation_Argument.html">165 hunch net-2006-03-23-The Approximation Argument</a></p>
<p>Introduction: An argument is sometimes made that the Bayesian way is the "right" way to do
machine learning. This is a serious argument which deserves a serious reply.
The approximation argument is a serious reply for which I have not yet seen a
reply2.The idea for the Bayesian approach is quite simple, elegant, and
general. Essentially, you first specify a priorP(D)over possible
processesDproducing the data, observe the data, then condition on the data
according to Bayes law to construct a posterior:P(D|x) = P(x|D)P(D)/P(x)After
this, hard decisions are made (such as "turn left" or "turn right") by
choosing the one which minimizes the expected (with respect to the posterior)
loss.This basic idea is reused thousands of times with various choices
ofP(D)and loss functions which is unsurprising given the many nice
properties:There is an extremely strong associated guarantee: If the actual
distribution generating the data is drawn fromP(D)there is no better method.
One way to think about this is that in</p><p>5 0.83033276 <a title="306-lda-5" href="../hunch_net-2009/hunch_net-2009-11-06-Yisong_Yue_on_Self-improving_Systems.html">376 hunch net-2009-11-06-Yisong Yue on Self-improving Systems</a></p>
<p>Introduction: I'd like to point outYisong Yue'spost on Self-improving systems, which is a
nicely readable description of the necessity and potential of interactive
learning to deal with the information overload problem that is endemic to the
modern internet.</p><p>6 0.80916578 <a title="306-lda-6" href="../hunch_net-2009/hunch_net-2009-06-03-Functionally_defined_Nonlinear_Dynamic_Models.html">359 hunch net-2009-06-03-Functionally defined Nonlinear Dynamic Models</a></p>
<p>7 0.72801971 <a title="306-lda-7" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>8 0.65781134 <a title="306-lda-8" href="../hunch_net-2005/hunch_net-2005-08-11-Why_Manifold-Based_Dimension_Reduction_Techniques%3F.html">102 hunch net-2005-08-11-Why Manifold-Based Dimension Reduction Techniques?</a></p>
<p>9 0.65602237 <a title="306-lda-9" href="../hunch_net-2008/hunch_net-2008-05-23-Three_levels_of_addressing_the_Netflix_Prize.html">301 hunch net-2008-05-23-Three levels of addressing the Netflix Prize</a></p>
<p>10 0.64247203 <a title="306-lda-10" href="../hunch_net-2006/hunch_net-2006-08-10-Precision_is_not_accuracy.html">202 hunch net-2006-08-10-Precision is not accuracy</a></p>
<p>11 0.64043838 <a title="306-lda-11" href="../hunch_net-2005/hunch_net-2005-09-30-Research_in_conferences.html">116 hunch net-2005-09-30-Research in conferences</a></p>
<p>12 0.63954937 <a title="306-lda-12" href="../hunch_net-2006/hunch_net-2006-11-06-Data_Linkage_Problems.html">217 hunch net-2006-11-06-Data Linkage Problems</a></p>
<p>13 0.63942927 <a title="306-lda-13" href="../hunch_net-2006/hunch_net-2006-06-14-Explorations_of_Exploration.html">183 hunch net-2006-06-14-Explorations of Exploration</a></p>
<p>14 0.63848466 <a title="306-lda-14" href="../hunch_net-2009/hunch_net-2009-02-22-Effective_Research_Funding.html">344 hunch net-2009-02-22-Effective Research Funding</a></p>
<p>15 0.63837242 <a title="306-lda-15" href="../hunch_net-2007/hunch_net-2007-03-03-All_Models_of_Learning_have_Flaws.html">235 hunch net-2007-03-03-All Models of Learning have Flaws</a></p>
<p>16 0.63815361 <a title="306-lda-16" href="../hunch_net-2005/hunch_net-2005-01-24-The_Humanloop_Spectrum_of_Machine_Learning.html">3 hunch net-2005-01-24-The Humanloop Spectrum of Machine Learning</a></p>
<p>17 0.63805705 <a title="306-lda-17" href="../hunch_net-2006/hunch_net-2006-03-02-Why_do_people_count_for_learning%3F.html">160 hunch net-2006-03-02-Why do people count for learning?</a></p>
<p>18 0.63666713 <a title="306-lda-18" href="../hunch_net-2006/hunch_net-2006-07-08-MaxEnt_contradicts_Bayes_Rule%3F.html">191 hunch net-2006-07-08-MaxEnt contradicts Bayes Rule?</a></p>
<p>19 0.63648307 <a title="306-lda-19" href="../hunch_net-2005/hunch_net-2005-01-19-Why_I_decided_to_run_a_weblog..html">1 hunch net-2005-01-19-Why I decided to run a weblog.</a></p>
<p>20 0.63327414 <a title="306-lda-20" href="../hunch_net-2005/hunch_net-2005-02-12-ROC_vs._Accuracy_vs._AROC.html">18 hunch net-2005-02-12-ROC vs. Accuracy vs. AROC</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
