<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>306 hunch net-2008-07-02-Proprietary Data in Academic Research?</title>
</head>

<body>
<p><a title="hunch_net" href="../hunch_net_home.html">hunch_net</a> <a title="hunch_net-2008" href="../home/hunch_net-2008_home.html">hunch_net-2008</a> <a title="hunch_net-2008-306" href="#">hunch_net-2008-306</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>306 hunch net-2008-07-02-Proprietary Data in Academic Research?</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="hunch_net-2008-306-html" href="http://hunch.net/?p=336">html</a></p><p>Introduction: Should results of experiments on proprietary datasets be in the academic research literature?
 
The arguments I can imagine in the “against” column are: 
  
  Experiments are not repeatable.  Repeatability in experiments is essential to science because it allows others to compare new methods with old and discover which is better. 
  It’s unfair.  Academics who don’t have insider access to proprietary data are at a substantial disadvantage when competing with others who do. 
  
I’m unsympathetic to argument (2).  To me, it looks like their are simply some resource constraints, and these should not prevent research progress.  For example, we wouldn’t prevent publishing about particle accelerator experiments by physicists at  CERN  because physicists at  CMU  couldn’t run their own experiments.  
 
Argument (1) seems like a real issue.
 
The argument for is: 
  
 Yes, they are another form of evidence that an algorithm is good. The degree to which they are evidence is less than for public</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Should results of experiments on proprietary datasets be in the academic research literature? [sent-1, score-1.029]
</p><p>2 The arguments I can imagine in the “against” column are:       Experiments are not repeatable. [sent-2, score-0.359]
</p><p>3 Repeatability in experiments is essential to science because it allows others to compare new methods with old and discover which is better. [sent-3, score-0.802]
</p><p>4 Academics who don’t have insider access to proprietary data are at a substantial disadvantage when competing with others who do. [sent-5, score-0.779]
</p><p>5 To me, it looks like their are simply some resource constraints, and these should not prevent research progress. [sent-7, score-0.46]
</p><p>6 For example, we wouldn’t prevent publishing about particle accelerator experiments by physicists at  CERN  because physicists at  CMU  couldn’t run their own experiments. [sent-8, score-1.171]
</p><p>7 The argument for is:      Yes, they are another form of evidence that an algorithm is good. [sent-10, score-0.307]
</p><p>8 The degree to which they are evidence is less than for publicly repeatable experiments, but greater than nothing. [sent-11, score-0.331]
</p><p>9 What if research can only be done in a proprietary setting? [sent-12, score-0.578]
</p><p>10 It has to be good for society at large to know what works. [sent-13, score-0.089]
</p><p>11 For example, suppose  ICML  decides to reject all papers with experiments on proprietary datasets. [sent-15, score-1.289]
</p><p>12 And suppose  KDD  decides to consider them as weak evidence. [sent-16, score-0.545]
</p><p>13 The long term result may be that beginning research on new topics which is only really doable in companies starts and then grows at KDD. [sent-17, score-0.508]
</p><p>14 I consider the arguments for to be stronger than the arguments against, but I’m aware others have other beliefs. [sent-18, score-0.871]
</p><p>15 I think it would be good to have a policy statement from machine learning conferences in their call for papers, as trends suggest this becoming a more serious problem in the mid-term future. [sent-19, score-0.376]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('proprietary', 0.474), ('experiments', 0.385), ('arguments', 0.24), ('decides', 0.22), ('physicists', 0.207), ('argument', 0.201), ('prevent', 0.19), ('suppose', 0.138), ('others', 0.133), ('consider', 0.123), ('column', 0.119), ('couldn', 0.11), ('particle', 0.11), ('evidence', 0.106), ('research', 0.104), ('trends', 0.099), ('grows', 0.099), ('academics', 0.095), ('resource', 0.095), ('wouldn', 0.092), ('literature', 0.092), ('society', 0.089), ('publicly', 0.086), ('disadvantage', 0.086), ('competing', 0.086), ('cmu', 0.084), ('starts', 0.082), ('beginning', 0.078), ('compare', 0.077), ('companies', 0.075), ('discover', 0.074), ('game', 0.074), ('becoming', 0.074), ('publishing', 0.072), ('reject', 0.072), ('degree', 0.071), ('stronger', 0.071), ('looks', 0.071), ('call', 0.071), ('allows', 0.07), ('topics', 0.07), ('suggest', 0.069), ('greater', 0.068), ('constraints', 0.067), ('academic', 0.066), ('weak', 0.064), ('aware', 0.064), ('yes', 0.063), ('statement', 0.063), ('old', 0.063)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000002 <a title="306-tfidf-1" href="../hunch_net-2008/hunch_net-2008-07-02-Proprietary_Data_in_Academic_Research%3F.html">306 hunch net-2008-07-02-Proprietary Data in Academic Research?</a></p>
<p>Introduction: Should results of experiments on proprietary datasets be in the academic research literature?
 
The arguments I can imagine in the “against” column are: 
  
  Experiments are not repeatable.  Repeatability in experiments is essential to science because it allows others to compare new methods with old and discover which is better. 
  It’s unfair.  Academics who don’t have insider access to proprietary data are at a substantial disadvantage when competing with others who do. 
  
I’m unsympathetic to argument (2).  To me, it looks like their are simply some resource constraints, and these should not prevent research progress.  For example, we wouldn’t prevent publishing about particle accelerator experiments by physicists at  CERN  because physicists at  CMU  couldn’t run their own experiments.  
 
Argument (1) seems like a real issue.
 
The argument for is: 
  
 Yes, they are another form of evidence that an algorithm is good. The degree to which they are evidence is less than for public</p><p>2 0.12121099 <a title="306-tfidf-2" href="../hunch_net-2005/hunch_net-2005-04-06-Structured_Regret_Minimization.html">53 hunch net-2005-04-06-Structured Regret Minimization</a></p>
<p>Introduction: Geoff Gordon made an interesting presentation at the  snowbird learning workshop  discussing the use of no-regret algorithms for the use of several robot-related learning problems.    There seems to be a draft  here .  This seems interesting in two ways:
  
  Drawback Removal  One of the significant problems with these online algorithms is that they can’t cope with structure very easily.  This drawback is addressed for certain structures. 
  Experiments  One criticism of such algorithms is that they are too “worst case”.   Several experiments suggest that protecting yourself against this worst case does not necessarily incur a great loss.</p><p>3 0.11425486 <a title="306-tfidf-3" href="../hunch_net-2005/hunch_net-2005-09-26-Prediction_Bounds_as_the_Mathematics_of_Science.html">115 hunch net-2005-09-26-Prediction Bounds as the Mathematics of Science</a></p>
<p>Introduction: “Science” has many meanings, but one common meaning is “the  scientific method ” which is a principled method for investigating the world using the following steps:
  
 Form a hypothesis about the world. 
 Use the hypothesis to make predictions. 
 Run experiments to confirm or disprove the predictions. 
  
The ordering of these steps is very important to the scientific method.  In particular, predictions  must  be made before experiments are run.  
 
Given that we all believe in the scientific method of investigation, it may be surprising to learn that cheating is very common.  This happens for many reasons, some innocent and some not.    
  
 Drug studies.  Pharmaceutical companies make predictions about the effects of their drugs and then conduct blind clinical studies to determine their effect.  Unfortunately, they have also been caught using some of the more advanced techniques for cheating  here : including “reprobleming”, “data set selection”, and probably “overfitting by review”</p><p>4 0.10162358 <a title="306-tfidf-4" href="../hunch_net-2008/hunch_net-2008-11-10-ICML_Reviewing_Criteria.html">325 hunch net-2008-11-10-ICML Reviewing Criteria</a></p>
<p>Introduction: Michael Littman  and  Leon Bottou  have decided to use a franchise program chair approach to  reviewing at ICML  this year.  I’ll be one of the area chairs, so I wanted to mention a few things if you are thinking about naming me.
  
 I take reviewing seriously.  That means papers to be reviewed are read, the implications are considered, and decisions are only made after that.  I do my best to be fair, and there are zero subjects that I consider categorical rejects.  I don’t consider several  arguments for rejection-not-on-the-merits reasonable . 
 I am generally interested in papers that (a) analyze new models of machine learning, (b) provide new algorithms, and (c) show that they work empirically on plausibly real problems.  If a paper has the trifecta, I’m particularly interested. With 2 out of 3, I might be interested.  I often find papers with only one element harder to accept, including papers with just (a).  
 I’m a bit tough.  I rarely jump-up-and-down about a paper, because I b</p><p>5 0.095845148 <a title="306-tfidf-5" href="../hunch_net-2009/hunch_net-2009-09-18-Necessary_and_Sufficient_Research.html">370 hunch net-2009-09-18-Necessary and Sufficient Research</a></p>
<p>Introduction: Researchers are typically confronted with big problems that they have no idea how to solve.  In trying to come up with a solution, a natural approach is to decompose the big problem into a set of subproblems whose solution yields a solution to the larger problem.  This approach can go wrong in several ways. 
  
  Decomposition failure .  The solution to the decomposition does not in fact yield a solution to the overall problem. 
  Artificial hardness .  The subproblems created are sufficient if solved to solve the overall problem, but they are harder than necessary. 
  
As you can see, computational complexity forms a relatively new (in research-history) razor by which to judge an approach sufficient but not necessary.
 
In my experience, the artificial hardness problem is very common.  Many researchers abdicate the responsibility of choosing a problem to work on to other people.  This process starts very naturally as a graduate student, when an incoming student might have relatively l</p><p>6 0.090131283 <a title="306-tfidf-6" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>7 0.083308324 <a title="306-tfidf-7" href="../hunch_net-2009/hunch_net-2009-02-18-Decision_by_Vetocracy.html">343 hunch net-2009-02-18-Decision by Vetocracy</a></p>
<p>8 0.081044018 <a title="306-tfidf-8" href="../hunch_net-2012/hunch_net-2012-01-04-Why_ICML%3F_and_the_summer_conferences.html">452 hunch net-2012-01-04-Why ICML? and the summer conferences</a></p>
<p>9 0.078617692 <a title="306-tfidf-9" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>10 0.076276034 <a title="306-tfidf-10" href="../hunch_net-2005/hunch_net-2005-02-14-Clever_Methods_of_Overfitting.html">19 hunch net-2005-02-14-Clever Methods of Overfitting</a></p>
<p>11 0.07600411 <a title="306-tfidf-11" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>12 0.072749227 <a title="306-tfidf-12" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>13 0.072065942 <a title="306-tfidf-13" href="../hunch_net-2010/hunch_net-2010-01-24-Specializations_of_the_Master_Problem.html">388 hunch net-2010-01-24-Specializations of the Master Problem</a></p>
<p>14 0.069979936 <a title="306-tfidf-14" href="../hunch_net-2008/hunch_net-2008-09-03-Bidding_Problems.html">315 hunch net-2008-09-03-Bidding Problems</a></p>
<p>15 0.069800451 <a title="306-tfidf-15" href="../hunch_net-2006/hunch_net-2006-11-27-Continuizing_Solutions.html">220 hunch net-2006-11-27-Continuizing Solutions</a></p>
<p>16 0.068288185 <a title="306-tfidf-16" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<p>17 0.067729712 <a title="306-tfidf-17" href="../hunch_net-2005/hunch_net-2005-10-12-The_unrealized_potential_of_the_research_lab.html">121 hunch net-2005-10-12-The unrealized potential of the research lab</a></p>
<p>18 0.067594767 <a title="306-tfidf-18" href="../hunch_net-2006/hunch_net-2006-05-05-An_ICML_reject.html">177 hunch net-2006-05-05-An ICML reject</a></p>
<p>19 0.06633725 <a title="306-tfidf-19" href="../hunch_net-2012/hunch_net-2012-06-05-ICML_acceptance_statistics.html">466 hunch net-2012-06-05-ICML acceptance statistics</a></p>
<p>20 0.065640733 <a title="306-tfidf-20" href="../hunch_net-2010/hunch_net-2010-04-26-Compassionate_Reviewing.html">395 hunch net-2010-04-26-Compassionate Reviewing</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/hunch_net_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.161), (1, -0.028), (2, -0.014), (3, 0.054), (4, -0.03), (5, -0.041), (6, 0.02), (7, -0.0), (8, 0.017), (9, 0.015), (10, 0.03), (11, 0.052), (12, 0.002), (13, -0.031), (14, -0.004), (15, 0.027), (16, -0.003), (17, -0.048), (18, -0.03), (19, -0.04), (20, 0.001), (21, -0.035), (22, -0.064), (23, 0.022), (24, 0.025), (25, -0.04), (26, 0.03), (27, -0.026), (28, -0.013), (29, -0.043), (30, -0.039), (31, 0.017), (32, 0.007), (33, -0.003), (34, 0.054), (35, -0.002), (36, -0.024), (37, 0.003), (38, 0.028), (39, 0.061), (40, 0.015), (41, 0.106), (42, 0.016), (43, 0.114), (44, -0.004), (45, -0.033), (46, 0.088), (47, -0.002), (48, -0.011), (49, 0.064)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95624447 <a title="306-lsi-1" href="../hunch_net-2008/hunch_net-2008-07-02-Proprietary_Data_in_Academic_Research%3F.html">306 hunch net-2008-07-02-Proprietary Data in Academic Research?</a></p>
<p>Introduction: Should results of experiments on proprietary datasets be in the academic research literature?
 
The arguments I can imagine in the “against” column are: 
  
  Experiments are not repeatable.  Repeatability in experiments is essential to science because it allows others to compare new methods with old and discover which is better. 
  It’s unfair.  Academics who don’t have insider access to proprietary data are at a substantial disadvantage when competing with others who do. 
  
I’m unsympathetic to argument (2).  To me, it looks like their are simply some resource constraints, and these should not prevent research progress.  For example, we wouldn’t prevent publishing about particle accelerator experiments by physicists at  CERN  because physicists at  CMU  couldn’t run their own experiments.  
 
Argument (1) seems like a real issue.
 
The argument for is: 
  
 Yes, they are another form of evidence that an algorithm is good. The degree to which they are evidence is less than for public</p><p>2 0.5831666 <a title="306-lsi-2" href="../hunch_net-2012/hunch_net-2012-01-30-ICML_Posters_and_Scope.html">454 hunch net-2012-01-30-ICML Posters and Scope</a></p>
<p>Introduction: Normally, I don’t indulge in posters for  ICML , but this year is naturally an exception for me.   If you want one, there are a small number  left here , if you sign up before February.
 
It also seems worthwhile to give some sense of the scope and reviewing criteria for ICML for authors considering submitting papers.  At ICML, the (very large) program committee does the reviewing which informs final decisions by area chairs on most papers.  Program chairs setup the process, deal with exceptions or disagreements, and provide advice for the reviewing process.  Providing advice is tricky (and easily misleading) because a conference is a community, and in the end the aggregate interests of the community determine the conference.  Nevertheless, as a program chair this year it seems worthwhile to state the overall philosophy I have and what I plan to encourage (and occasionally discourage).
 
At the highest level, I believe ICML exists to further research into machine learning, which I gene</p><p>3 0.570494 <a title="306-lsi-3" href="../hunch_net-2008/hunch_net-2008-11-10-ICML_Reviewing_Criteria.html">325 hunch net-2008-11-10-ICML Reviewing Criteria</a></p>
<p>Introduction: Michael Littman  and  Leon Bottou  have decided to use a franchise program chair approach to  reviewing at ICML  this year.  I’ll be one of the area chairs, so I wanted to mention a few things if you are thinking about naming me.
  
 I take reviewing seriously.  That means papers to be reviewed are read, the implications are considered, and decisions are only made after that.  I do my best to be fair, and there are zero subjects that I consider categorical rejects.  I don’t consider several  arguments for rejection-not-on-the-merits reasonable . 
 I am generally interested in papers that (a) analyze new models of machine learning, (b) provide new algorithms, and (c) show that they work empirically on plausibly real problems.  If a paper has the trifecta, I’m particularly interested. With 2 out of 3, I might be interested.  I often find papers with only one element harder to accept, including papers with just (a).  
 I’m a bit tough.  I rarely jump-up-and-down about a paper, because I b</p><p>4 0.56486005 <a title="306-lsi-4" href="../hunch_net-2005/hunch_net-2005-09-06-A_link.html">108 hunch net-2005-09-06-A link</a></p>
<p>Introduction: I read through some of the essays of  Michael Nielsen  today, and recommend them.   Principles of Effective Research  and  Extreme Thinking  are both relevant to several discussions here.</p><p>5 0.56043559 <a title="306-lsi-5" href="../hunch_net-2005/hunch_net-2005-12-01-The_Webscience_Future.html">134 hunch net-2005-12-01-The Webscience Future</a></p>
<p>Introduction: The internet has significantly effected the way we do research but it’s capabilities have not yet been fully realized.
 
First, let’s acknowledge some known effects.
  
  Self-publishing  By default, all researchers in machine learning (and more generally computer science and physics) place their papers online for anyone to download.  The exact mechanism differs—physicists tend to use a central repository ( Arxiv ) while computer scientists tend to place the papers on their webpage.  Arxiv has been slowly growing in subject breadth so it now sometimes used by computer scientists. 
  Collaboration  Email has enabled working remotely with coauthors.  This has allowed collaborationis which would not otherwise have been possible and generally speeds research. 
  
Now, let’s look at attempts to go further.
  
  Blogs  (like this one) allow public discussion about topics which are not easily categorized as “a new idea in machine learning” (like this topic). 
  Organization  of some subfield</p><p>6 0.55327392 <a title="306-lsi-6" href="../hunch_net-2008/hunch_net-2008-04-21-The_Science_2.0_article.html">296 hunch net-2008-04-21-The Science 2.0 article</a></p>
<p>7 0.54901546 <a title="306-lsi-7" href="../hunch_net-2010/hunch_net-2010-04-14-MLcomp%3A_a_website_for_objectively_comparing_ML_algorithms.html">393 hunch net-2010-04-14-MLcomp: a website for objectively comparing ML algorithms</a></p>
<p>8 0.5360918 <a title="306-lsi-8" href="../hunch_net-2010/hunch_net-2010-10-17-Partha_Niyogi_has_died.html">414 hunch net-2010-10-17-Partha Niyogi has died</a></p>
<p>9 0.52107221 <a title="306-lsi-9" href="../hunch_net-2009/hunch_net-2009-01-07-Interesting_Papers_at_SODA_2009.html">334 hunch net-2009-01-07-Interesting Papers at SODA 2009</a></p>
<p>10 0.51916271 <a title="306-lsi-10" href="../hunch_net-2005/hunch_net-2005-01-19-Why_I_decided_to_run_a_weblog..html">1 hunch net-2005-01-19-Why I decided to run a weblog.</a></p>
<p>11 0.51573449 <a title="306-lsi-11" href="../hunch_net-2005/hunch_net-2005-12-22-Yes_%2C_I_am_applying.html">142 hunch net-2005-12-22-Yes , I am applying</a></p>
<p>12 0.5148527 <a title="306-lsi-12" href="../hunch_net-2005/hunch_net-2005-09-04-Science_in_the_Government.html">106 hunch net-2005-09-04-Science in the Government</a></p>
<p>13 0.51285791 <a title="306-lsi-13" href="../hunch_net-2009/hunch_net-2009-08-03-Carbon_in_Computer_Science_Research.html">366 hunch net-2009-08-03-Carbon in Computer Science Research</a></p>
<p>14 0.51048309 <a title="306-lsi-14" href="../hunch_net-2008/hunch_net-2008-12-27-Adversarial_Academia.html">333 hunch net-2008-12-27-Adversarial Academia</a></p>
<p>15 0.50866216 <a title="306-lsi-15" href="../hunch_net-2010/hunch_net-2010-09-13-AIStats.html">409 hunch net-2010-09-13-AIStats</a></p>
<p>16 0.50604242 <a title="306-lsi-16" href="../hunch_net-2010/hunch_net-2010-05-20-Google_Predict.html">399 hunch net-2010-05-20-Google Predict</a></p>
<p>17 0.5026558 <a title="306-lsi-17" href="../hunch_net-2008/hunch_net-2008-01-06-Research_Political_Issues.html">282 hunch net-2008-01-06-Research Political Issues</a></p>
<p>18 0.5024482 <a title="306-lsi-18" href="../hunch_net-2007/hunch_net-2007-07-13-The_View_From_China.html">255 hunch net-2007-07-13-The View From China</a></p>
<p>19 0.4955681 <a title="306-lsi-19" href="../hunch_net-2005/hunch_net-2005-04-06-Structured_Regret_Minimization.html">53 hunch net-2005-04-06-Structured Regret Minimization</a></p>
<p>20 0.49424294 <a title="306-lsi-20" href="../hunch_net-2010/hunch_net-2010-05-02-What%26%238217%3Bs_the_difference_between_gambling_and_rewarding_good_prediction%3F.html">397 hunch net-2010-05-02-What&#8217;s the difference between gambling and rewarding good prediction?</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/hunch_net_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(3, 0.024), (19, 0.238), (27, 0.193), (38, 0.096), (53, 0.04), (55, 0.096), (79, 0.013), (94, 0.15), (95, 0.04)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.91079062 <a title="306-lda-1" href="../hunch_net-2008/hunch_net-2008-07-02-Proprietary_Data_in_Academic_Research%3F.html">306 hunch net-2008-07-02-Proprietary Data in Academic Research?</a></p>
<p>Introduction: Should results of experiments on proprietary datasets be in the academic research literature?
 
The arguments I can imagine in the “against” column are: 
  
  Experiments are not repeatable.  Repeatability in experiments is essential to science because it allows others to compare new methods with old and discover which is better. 
  It’s unfair.  Academics who don’t have insider access to proprietary data are at a substantial disadvantage when competing with others who do. 
  
I’m unsympathetic to argument (2).  To me, it looks like their are simply some resource constraints, and these should not prevent research progress.  For example, we wouldn’t prevent publishing about particle accelerator experiments by physicists at  CERN  because physicists at  CMU  couldn’t run their own experiments.  
 
Argument (1) seems like a real issue.
 
The argument for is: 
  
 Yes, they are another form of evidence that an algorithm is good. The degree to which they are evidence is less than for public</p><p>2 0.88027149 <a title="306-lda-2" href="../hunch_net-2010/hunch_net-2010-12-04-Vowpal_Wabbit%2C_version_5.0%2C_and_the_second_heresy.html">419 hunch net-2010-12-04-Vowpal Wabbit, version 5.0, and the second heresy</a></p>
<p>Introduction: I’ve released  version 5.0  of the  Vowpal Wabbit  online learning software.  The major number has changed since the  last release  because I regard all earlier versions as obsolete—there are several new algorithms & features including substantial changes and upgrades to the default learning algorithm.  
 
The biggest changes are new algorithms:
  
  Nikos  and I improved the default algorithm.  The basic update rule still uses gradient descent, but the size of the update is carefully controlled so that it’s impossible to overrun the label.  In addition, the normalization has changed.  Computationally, these changes are virtually free and yield better results, sometimes much better.  Less careful updates can be reenabled with –loss_function classic, although results are still not identical to previous due to normalization changes. 
 Nikos also implemented the per-feature learning rates as per these  two   papers .  Often, this works better than the default algorithm.  It isn’t the defa</p><p>3 0.87084377 <a title="306-lda-3" href="../hunch_net-2013/hunch_net-2013-11-21-Ben_Taskar_is_gone.html">491 hunch net-2013-11-21-Ben Taskar is gone</a></p>
<p>Introduction: I was not as personally close to  Ben  as  Sam , but the level of tragedy is similar and I canâ&euro;&trade;t help but be greatly saddened by the loss.
 
Various  news   stories  have coverage, but the synopsis is that he had a heart attack on Sunday and is survived by his wife Anat and daughter Aviv.  There is discussion of creating a memorial fund for them, which I hope comes to fruition, and plan to contribute to.  
 
I will remember Ben as someone who thought carefully and comprehensively about new ways to do things, then fought hard and successfully for what he believed in.  It is an ideal we strive for, that Ben accomplished.  
 
Edit: donations  go here , and more information is  here .</p><p>4 0.73901755 <a title="306-lda-4" href="../hunch_net-2006/hunch_net-2006-12-04-Structural_Problems_in_NIPS_Decision_Making.html">221 hunch net-2006-12-04-Structural Problems in NIPS Decision Making</a></p>
<p>Introduction: This is a very difficult post to write, because it is about a perenially touchy subject.  Nevertheless, it is an important one which needs to be thought about carefully.
 
There are a few things which should be understood:
  
 The system is changing and responsive.  We-the-authors are we-the-reviewers, we-the-PC, and even we-the-NIPS-board.  NIPS has implemented ‘secondary program chairs’, ‘author response’, and ‘double blind reviewing’ in the last few years to help with the decision process, and more changes may happen in the future. 
 Agreement creates a perception of correctness.  When any PC meets and makes a group decision about a paper, there is a strong tendency for the reinforcement inherent in a group decision to create the perception of correctness.  For the many people who have been on the NIPS PC it’s reasonable to entertain a healthy skepticism in the face of this reinforcing certainty. 
 This post is about structural problems.  What problems arise because of the structure</p><p>5 0.7336728 <a title="306-lda-5" href="../hunch_net-2008/hunch_net-2008-01-25-Turing%26%238217%3Bs_Club_for_Machine_Learning.html">286 hunch net-2008-01-25-Turing&#8217;s Club for Machine Learning</a></p>
<p>Introduction: Many people in Machine Learning don’t fully understand the impact of computation, as demonstrated by a lack of  big-O  analysis of new learning algorithms.  This is important—some current active research programs are fundamentally flawed w.r.t. computation, and other research programs are directly motivated by it.  When considering a learning algorithm, I think about the following questions:
  
 How does the learning algorithm scale with the number of examples  m ?  Any algorithm using all of the data is at least  O(m) , but in many cases this is  O(m 2 )  (naive nearest neighbor for self-prediction) or unknown (k-means or many other optimization algorithms).  The unknown case is very common, and it can mean (for example) that the algorithm isn’t convergent or simply that the amount of computation isn’t controlled. 
 The above question can also be asked for test cases.  In some applications, test-time performance is of great importance. 
 How does the algorithm scale with the number of</p><p>6 0.72858828 <a title="306-lda-6" href="../hunch_net-2011/hunch_net-2011-02-02-User_preferences_for_search_engines.html">423 hunch net-2011-02-02-User preferences for search engines</a></p>
<p>7 0.72691703 <a title="306-lda-7" href="../hunch_net-2005/hunch_net-2005-07-14-What_Learning_Theory_might_do.html">95 hunch net-2005-07-14-What Learning Theory might do</a></p>
<p>8 0.7242347 <a title="306-lda-8" href="../hunch_net-2005/hunch_net-2005-12-07-Is_the_Google_way_the_way_for_machine_learning%3F.html">136 hunch net-2005-12-07-Is the Google way the way for machine learning?</a></p>
<p>9 0.71979153 <a title="306-lda-9" href="../hunch_net-2006/hunch_net-2006-08-28-Learning_Theory_standards_for_NIPS_2006.html">204 hunch net-2006-08-28-Learning Theory standards for NIPS 2006</a></p>
<p>10 0.71873569 <a title="306-lda-10" href="../hunch_net-2007/hunch_net-2007-12-10-Learning_Track_of_International_Planning_Competition.html">276 hunch net-2007-12-10-Learning Track of International Planning Competition</a></p>
<p>11 0.71658325 <a title="306-lda-11" href="../hunch_net-2009/hunch_net-2009-06-03-Functionally_defined_Nonlinear_Dynamic_Models.html">359 hunch net-2009-06-03-Functionally defined Nonlinear Dynamic Models</a></p>
<p>12 0.71512425 <a title="306-lda-12" href="../hunch_net-2005/hunch_net-2005-03-30-What_can_Type_Theory_teach_us_about_Machine_Learning%3F.html">49 hunch net-2005-03-30-What can Type Theory teach us about Machine Learning?</a></p>
<p>13 0.71495181 <a title="306-lda-13" href="../hunch_net-2007/hunch_net-2007-01-26-Parallel_Machine_Learning_Problems.html">229 hunch net-2007-01-26-Parallel Machine Learning Problems</a></p>
<p>14 0.71404231 <a title="306-lda-14" href="../hunch_net-2007/hunch_net-2007-04-02-Contextual_Scaling.html">237 hunch net-2007-04-02-Contextual Scaling</a></p>
<p>15 0.71057272 <a title="306-lda-15" href="../hunch_net-2006/hunch_net-2006-03-09-Use_of_Notation.html">162 hunch net-2006-03-09-Use of Notation</a></p>
<p>16 0.71025252 <a title="306-lda-16" href="../hunch_net-2005/hunch_net-2005-07-27-Not_goal_metrics.html">98 hunch net-2005-07-27-Not goal metrics</a></p>
<p>17 0.70948267 <a title="306-lda-17" href="../hunch_net-2009/hunch_net-2009-09-21-Netflix_finishes_%28and_starts%29.html">371 hunch net-2009-09-21-Netflix finishes (and starts)</a></p>
<p>18 0.70813829 <a title="306-lda-18" href="../hunch_net-2011/hunch_net-2011-12-02-Hadoop_AllReduce_and_Terascale_Learning.html">450 hunch net-2011-12-02-Hadoop AllReduce and Terascale Learning</a></p>
<p>19 0.70695859 <a title="306-lda-19" href="../hunch_net-2009/hunch_net-2009-05-02-Wielding_a_New_Abstraction.html">351 hunch net-2009-05-02-Wielding a New Abstraction</a></p>
<p>20 0.70620286 <a title="306-lda-20" href="../hunch_net-2005/hunch_net-2005-11-26-The_Design_of_an_Optimal_Research_Environment.html">132 hunch net-2005-11-26-The Design of an Optimal Research Environment</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
