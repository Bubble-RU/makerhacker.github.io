<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>31 nathan marz storm-2011-10-13-How to beat the CAP theorem</title>
</head>

<body>
<p><a title="nathan_marz_storm" href="../nathan_marz_storm_home.html">nathan_marz_storm</a> <a title="nathan_marz_storm-2011" href="../home/nathan_marz_storm-2011_home.html">nathan_marz_storm-2011</a> <a title="nathan_marz_storm-2011-31" href="#">nathan_marz_storm-2011-31</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>31 nathan marz storm-2011-10-13-How to beat the CAP theorem</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="nathan_marz_storm-2011-31-html" href="http://nathanmarz.com//blog/how-to-beat-the-cap-theorem.html">html</a></p><p>Introduction: The CAP theorem states a database cannot guarantee consistency, availability, and partition-tolerance at the same time. But you can't sacrifice partition-tolerance (see  here  and  here ), so you must make a tradeoff between availability and consistency. Managing this tradeoff is a central focus of the NoSQL movement.
 
Consistency means that after you do a successful write, future reads will always take that write into account. Availability means that you can always read and write to the system. During a partition, you can only have one of these properties.
 
Systems that choose consistency over availability have to deal with some awkward issues. What do you do when the database isn't available? You can try buffering writes for later, but you risk losing those writes if you lose the machine with the buffer. Also, buffering writes can be a form of inconsistency because a client thinks a write has succeeded but the write isn't in the database yet. Alternatively, you can return errors ba</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Second, the immutability of data is the key property we're going to exploit in designing a human fault-tolerant data system that beats the CAP theorem. [sent-88, score-0.832]
</p><p>2 The complexity the CAP theorem normally causes is avoided by using immutable data and computing queries from scratch. [sent-109, score-0.987]
</p><p>3 Eventually that data will be consistent and queries will incorporate that data into their computations. [sent-117, score-0.722]
</p><p>4 Databases, indexing, ETL, batch computation, stream processing -- these are all techniques for optimizing query functions and bringing the latency down to an acceptable level. [sent-132, score-0.722]
</p><p>5 Since data is immutable and the master dataset is append-only, writing bad data does not override or otherwise destroy good data. [sent-194, score-0.746]
</p><p>6 Realtime layer   Believe it or not, the batch solution almost solves the complete problem of computing arbitrary functions on arbitrary data in realtime. [sent-199, score-1.079]
</p><p>7 Any data older than a few hours has already been incorporated into the batch views, so all that's left to do is compensate for the last few hours of data. [sent-200, score-0.72]
</p><p>8 To compensate for those few hours of data, you need a realtime system that runs in parallel with the batch system. [sent-203, score-0.736]
</p><p>9 To resolve a query function, you query the batch view and the realtime view and merge the results together to get the final answer. [sent-205, score-0.851]
</p><p>10 The realtime layer is where you use read/write databases like Riak or Cassandra, and the realtime layer relies on incremental algorithms to update the state in those databases. [sent-206, score-1.214]
</p><p>11 The batch system is the same as before: a batch workflow based on Hadoop and ElephantDB precomputes the query for everything but the last few hours of data. [sent-211, score-1.077]
</p><p>12 Since the realtime layer only compensates for the last few hours of data, everything the realtime layer computes is eventually overridden by the batch layer. [sent-220, score-1.278]
</p><p>13 So if you make a mistake or something goes wrong in the realtime layer, the batch layer will correct it. [sent-221, score-0.792]
</p><p>14 The append-only immutable dataset in the batch layer is still the core of the system, so any mistake can be recovered from just like before. [sent-232, score-0.852]
</p><p>15 Since garbage collection is run as an offline batch processing task, it doesn't affect how the system interacts with the CAP theorem. [sent-259, score-0.748]
</p><p>16 The batch/realtime split gives you the flexibility to use the exact algorithm on the batch layer and an approximate algorithm on the realtime layer. [sent-270, score-0.792]
</p><p>17 The batch layer constantly overrides the realtime layer, so the approximation gets corrected and your system exhibits the property of "eventual accuracy". [sent-271, score-0.94]
</p><p>18 Easy ad-hoc analysis:  The arbitrariness of the batch layer means you can run any query you like on your data. [sent-275, score-0.82]
</p><p>19 Better batch processing primitives : Hadoop is not the end-all-be-all of batch computation. [sent-286, score-0.784]
</p><p>20 Big data and the NoSQL movement seemed to make data management more complex than it was with the RDBMS, but that's only because we were trying to treat "Big Data" the same way we treated data with an RDBMS: by conflating data and views and relying on incremental algorithms. [sent-294, score-1.228]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('batch', 0.359), ('cap', 0.328), ('layer', 0.257), ('data', 0.255), ('theorem', 0.227), ('realtime', 0.176), ('consistency', 0.164), ('databases', 0.16), ('query', 0.158), ('complexity', 0.154), ('immutable', 0.151), ('queries', 0.15), ('system', 0.148), ('incremental', 0.126), ('updates', 0.126), ('eventual', 0.114), ('function', 0.103), ('human', 0.103), ('availability', 0.101), ('functions', 0.095), ('dataset', 0.085), ('views', 0.082), ('scalable', 0.082), ('elephantdb', 0.076), ('garbage', 0.076), ('nosql', 0.076), ('writes', 0.072), ('key', 0.071), ('page', 0.071), ('systems', 0.067), ('processing', 0.066), ('equation', 0.063), ('complete', 0.063), ('compute', 0.063), ('sally', 0.063), ('mapreduce', 0.062), ('location', 0.062), ('consistent', 0.062), ('algorithms', 0.062), ('collection', 0.053), ('hours', 0.053), ('database', 0.052), ('definition', 0.051), ('computing', 0.05), ('moment', 0.05), ('mutable', 0.05), ('replicas', 0.05), ('piece', 0.05), ('run', 0.046), ('stream', 0.044)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000032 <a title="31-tfidf-1" href="../nathan_marz_storm-2011/nathan_marz_storm-2011-10-13-How_to_beat_the_CAP_theorem.html">31 nathan marz storm-2011-10-13-How to beat the CAP theorem</a></p>
<p>Introduction: The CAP theorem states a database cannot guarantee consistency, availability, and partition-tolerance at the same time. But you can't sacrifice partition-tolerance (see  here  and  here ), so you must make a tradeoff between availability and consistency. Managing this tradeoff is a central focus of the NoSQL movement.
 
Consistency means that after you do a successful write, future reads will always take that write into account. Availability means that you can always read and write to the system. During a partition, you can only have one of these properties.
 
Systems that choose consistency over availability have to deal with some awkward issues. What do you do when the database isn't available? You can try buffering writes for later, but you risk losing those writes if you lose the machine with the buffer. Also, buffering writes can be a form of inconsistency because a client thinks a write has succeeded but the write isn't in the database yet. Alternatively, you can return errors ba</p><p>2 0.18491732 <a title="31-tfidf-2" href="../nathan_marz_storm-2012/nathan_marz_storm-2012-02-06-Suffering-oriented_programming.html">33 nathan marz storm-2012-02-06-Suffering-oriented programming</a></p>
<p>Introduction: Someone asked me an interesting question the other day: "How did you justify taking such a huge risk on building  Storm  while working on a  startup ?" (Storm is a realtime computation system). I can see how from an outsider's perspective investing in such a massive project seems extremely risky for a startup. From my perspective, though, building Storm wasn't risky at all. It was challenging, but not risky.
 
I follow a style of development that greatly reduces the risk of big projects like Storm. I call this style "suffering-oriented programming." Suffering-oriented programming can be summarized like so: don't build technology unless you feel the pain of not having it. It applies to the big, architectural decisions as well as the smaller everyday programming decisions. Suffering-oriented programming greatly reduces risk by ensuring that you're always working on something important, and it ensures that you are well-versed in a problem space before attempting a large investment.
 
I ha</p><p>3 0.16934976 <a title="31-tfidf-3" href="../nathan_marz_storm-2014/nathan_marz_storm-2014-02-12-Interview_with_%22Programmer_Magazine%22.html">39 nathan marz storm-2014-02-12-Interview with "Programmer Magazine"</a></p>
<p>Introduction: I was  recently interviewed  for "Programmer Magazine", a Chinese magazine. The interview was published in Chinese, but a lot of people told me they'd like to see the English version of the interview. Due to the Google translation being, ahem, a little iffy, I decided to just publish the original English version on my blog. Hope you enjoy! 
  What drew you to programming and what was the first interesting program you wrote?  
I started programming when I was 10 years old on my TI-82 graphing calculator. Initially I started programming because I wanted to make games on my calculator – and also because I was bored in math class :D. The first interesting game I made on my calculator was an archery game where you'd shoot arrows at moving targets. You'd get points for hitting more targets or completing all the targets faster. A couple years later I graduated to programming the TI-89 which was a huge upgrade in power. I remember how the TI-82 only let you have 26 variables (for the character</p><p>4 0.14067107 <a title="31-tfidf-4" href="../nathan_marz_storm-2010/nathan_marz_storm-2010-04-14-Introducing_Cascalog%3A_a_Clojure-based_query_language_for_Hadoop.html">13 nathan marz storm-2010-04-14-Introducing Cascalog: a Clojure-based query language for Hadoop</a></p>
<p>Introduction: I'm very excited to be releasing  Cascalog  as open-source today. Cascalog is a Clojure-based query language for Hadoop inspired by  Datalog .
  Highlights   
  Simple  - Functions, filters, and aggregators all use the same syntax. Joins are implicit and natural. 
  Expressive  - Logical composition is very powerful, and you can run arbitrary Clojure code in your query with little effort. 
  Interactive  - Run queries from the Clojure REPL. 
  Scalable  - Cascalog queries run as a series of MapReduce jobs. 
  Query anything  - Query HDFS data, database data, and/or local data by making use of Cascading's "Tap" abstraction 
  Careful handling of null values  - Null values can make life difficult. Cascalog has a feature called "non-nullable variables" that makes dealing with nulls painless. 
  First class interoperability with Cascading  - Operations defined for Cascalog can be used in a Cascading flow and vice-versa 
  First class interoperability with Clojure  - Can use regular Clojure</p><p>5 0.13088641 <a title="31-tfidf-5" href="../nathan_marz_storm-2011/nathan_marz_storm-2011-03-29-My_talks_at_POSSCON.html">30 nathan marz storm-2011-03-29-My talks at POSSCON</a></p>
<p>Introduction: Last week I went to POSSCON in Columbia, South Carolina. It was an interesting experience and a good reminder that not everyone in the world thinks like we do in Silicon Valley.
 
I gave two talks at the conference. One was a technical talk about how to build realtime Big Data systems, and the other was a non-technical talk about the things we do at BackType to be a super-productive team. Both slide decks are embedded below.
    The Secrets of Building Realtime Big Data Systems                  
   Become Efficient or Die: The Story of BackType</p><p>6 0.10891978 <a title="31-tfidf-6" href="../nathan_marz_storm-2010/nathan_marz_storm-2010-03-10-Thrift_%2B_Graphs_%3D_Strong%2C_flexible_schemas_on_Hadoop.html">9 nathan marz storm-2010-03-10-Thrift + Graphs = Strong, flexible schemas on Hadoop</a></p>
<p>7 0.10443252 <a title="31-tfidf-7" href="../nathan_marz_storm-2010/nathan_marz_storm-2010-01-03-Tips_for_Optimizing_Cascading_Flows.html">2 nathan marz storm-2010-01-03-Tips for Optimizing Cascading Flows</a></p>
<p>8 0.10316534 <a title="31-tfidf-8" href="../nathan_marz_storm-2010/nathan_marz_storm-2010-05-08-News_Feed_in_38_lines_of_code_using_Cascalog.html">16 nathan marz storm-2010-05-08-News Feed in 38 lines of code using Cascalog</a></p>
<p>9 0.098905072 <a title="31-tfidf-9" href="../nathan_marz_storm-2009/nathan_marz_storm-2009-12-28-The_mathematics_behind_Hadoop-based_systems.html">1 nathan marz storm-2009-12-28-The mathematics behind Hadoop-based systems</a></p>
<p>10 0.098618306 <a title="31-tfidf-10" href="../nathan_marz_storm-2012/nathan_marz_storm-2012-01-09-Early_access_edition_of_my_book_is_available.html">32 nathan marz storm-2012-01-09-Early access edition of my book is available</a></p>
<p>11 0.079760842 <a title="31-tfidf-11" href="../nathan_marz_storm-2012/nathan_marz_storm-2012-09-19-Storm%27s_1st_birthday.html">34 nathan marz storm-2012-09-19-Storm's 1st birthday</a></p>
<p>12 0.078183465 <a title="31-tfidf-12" href="../nathan_marz_storm-2013/nathan_marz_storm-2013-04-02-Principles_of_Software_Engineering%2C_Part_1.html">37 nathan marz storm-2013-04-02-Principles of Software Engineering, Part 1</a></p>
<p>13 0.077313505 <a title="31-tfidf-13" href="../nathan_marz_storm-2010/nathan_marz_storm-2010-03-08-Follow-up_to_%22The_mathematics_behind_Hadoop-based_systems%22.html">8 nathan marz storm-2010-03-08-Follow-up to "The mathematics behind Hadoop-based systems"</a></p>
<p>14 0.074904457 <a title="31-tfidf-14" href="../nathan_marz_storm-2010/nathan_marz_storm-2010-07-12-My_experience_as_the_first_employee_of_a_Y_Combinator_startup.html">19 nathan marz storm-2010-07-12-My experience as the first employee of a Y Combinator startup</a></p>
<p>15 0.073464558 <a title="31-tfidf-15" href="../nathan_marz_storm-2011/nathan_marz_storm-2011-01-07-Analysis_of_the_%23LessAmbitiousMovies_Twitter_Meme.html">27 nathan marz storm-2011-01-07-Analysis of the #LessAmbitiousMovies Twitter Meme</a></p>
<p>16 0.068816535 <a title="31-tfidf-16" href="../nathan_marz_storm-2010/nathan_marz_storm-2010-04-27-New_Cascalog_features%3A_outer_joins%2C_combiners%2C_sorting%2C_and_more.html">14 nathan marz storm-2010-04-27-New Cascalog features: outer joins, combiners, sorting, and more</a></p>
<p>17 0.058561366 <a title="31-tfidf-17" href="../nathan_marz_storm-2013/nathan_marz_storm-2013-04-12-Break_into_Silicon_Valley_with_a_blog.html">38 nathan marz storm-2013-04-12-Break into Silicon Valley with a blog</a></p>
<p>18 0.056452334 <a title="31-tfidf-18" href="../nathan_marz_storm-2010/nathan_marz_storm-2010-03-23-Migrating_data_from_a_SQL_database_to_Hadoop.html">11 nathan marz storm-2010-03-23-Migrating data from a SQL database to Hadoop</a></p>
<p>19 0.053357124 <a title="31-tfidf-19" href="../nathan_marz_storm-2010/nathan_marz_storm-2010-07-30-You_should_blog_even_if_you_have_no_readers.html">20 nathan marz storm-2010-07-30-You should blog even if you have no readers</a></p>
<p>20 0.043636717 <a title="31-tfidf-20" href="../nathan_marz_storm-2014/nathan_marz_storm-2014-05-10-Why_we_in_tech_must_support_Lawrence_Lessig.html">41 nathan marz storm-2014-05-10-Why we in tech must support Lawrence Lessig</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/nathan_marz_storm_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.373), (1, -0.094), (2, 0.102), (3, 0.296), (4, 0.079), (5, 0.191), (6, -0.218), (7, 0.111), (8, 0.1), (9, -0.034), (10, -0.019), (11, -0.148), (12, 0.09), (13, -0.036), (14, -0.026), (15, 0.038), (16, -0.095), (17, -0.02), (18, -0.228), (19, -0.045), (20, 0.134), (21, -0.004), (22, 0.244), (23, 0.018), (24, 0.194), (25, -0.083), (26, 0.023), (27, -0.1), (28, 0.077), (29, 0.263), (30, 0.151), (31, 0.061), (32, 0.071), (33, -0.44), (34, -0.306), (35, 0.037), (36, -0.11), (37, -0.03), (38, -0.018), (39, 0.008), (40, -0.049)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97391385 <a title="31-lsi-1" href="../nathan_marz_storm-2011/nathan_marz_storm-2011-10-13-How_to_beat_the_CAP_theorem.html">31 nathan marz storm-2011-10-13-How to beat the CAP theorem</a></p>
<p>Introduction: The CAP theorem states a database cannot guarantee consistency, availability, and partition-tolerance at the same time. But you can't sacrifice partition-tolerance (see  here  and  here ), so you must make a tradeoff between availability and consistency. Managing this tradeoff is a central focus of the NoSQL movement.
 
Consistency means that after you do a successful write, future reads will always take that write into account. Availability means that you can always read and write to the system. During a partition, you can only have one of these properties.
 
Systems that choose consistency over availability have to deal with some awkward issues. What do you do when the database isn't available? You can try buffering writes for later, but you risk losing those writes if you lose the machine with the buffer. Also, buffering writes can be a form of inconsistency because a client thinks a write has succeeded but the write isn't in the database yet. Alternatively, you can return errors ba</p><p>2 0.23513126 <a title="31-lsi-2" href="../nathan_marz_storm-2010/nathan_marz_storm-2010-03-10-Thrift_%2B_Graphs_%3D_Strong%2C_flexible_schemas_on_Hadoop.html">9 nathan marz storm-2010-03-10-Thrift + Graphs = Strong, flexible schemas on Hadoop</a></p>
<p>Introduction: There are a lot of misconceptions about what Hadoop is useful for and what kind of data you can put in it. A lot of people think that Hadoop is meant for unstructured data like log files. While Hadoop is great for log files, it's also  fantastic  for strongly typed, structured data.
 
In this post I'll discuss how you can use a tool like  Thrift  to store strongly typed data in Hadoop while retaining the flexibility to evolve your schema. We'll look at graph-based schemas and see why they are an ideal fit for many Hadoop-based applications.
 
 OK, so what kind of "structured" data can you put in Hadoop? 
 
Anything! At  BackType  we put data about news, conversations, and people into Hadoop as structured objects. You can easily push structured information about social graphs, financial information, or anything you want into Hadoop. Â  
 
 That sounds all well and good, but why not just use JSON as the data format? 
 
JSON doesn't give you a real schema and doesn't protect against data i</p><p>3 0.22410528 <a title="31-lsi-3" href="../nathan_marz_storm-2014/nathan_marz_storm-2014-02-12-Interview_with_%22Programmer_Magazine%22.html">39 nathan marz storm-2014-02-12-Interview with "Programmer Magazine"</a></p>
<p>Introduction: I was  recently interviewed  for "Programmer Magazine", a Chinese magazine. The interview was published in Chinese, but a lot of people told me they'd like to see the English version of the interview. Due to the Google translation being, ahem, a little iffy, I decided to just publish the original English version on my blog. Hope you enjoy! 
  What drew you to programming and what was the first interesting program you wrote?  
I started programming when I was 10 years old on my TI-82 graphing calculator. Initially I started programming because I wanted to make games on my calculator – and also because I was bored in math class :D. The first interesting game I made on my calculator was an archery game where you'd shoot arrows at moving targets. You'd get points for hitting more targets or completing all the targets faster. A couple years later I graduated to programming the TI-89 which was a huge upgrade in power. I remember how the TI-82 only let you have 26 variables (for the character</p><p>4 0.22181657 <a title="31-lsi-4" href="../nathan_marz_storm-2012/nathan_marz_storm-2012-02-06-Suffering-oriented_programming.html">33 nathan marz storm-2012-02-06-Suffering-oriented programming</a></p>
<p>Introduction: Someone asked me an interesting question the other day: "How did you justify taking such a huge risk on building  Storm  while working on a  startup ?" (Storm is a realtime computation system). I can see how from an outsider's perspective investing in such a massive project seems extremely risky for a startup. From my perspective, though, building Storm wasn't risky at all. It was challenging, but not risky.
 
I follow a style of development that greatly reduces the risk of big projects like Storm. I call this style "suffering-oriented programming." Suffering-oriented programming can be summarized like so: don't build technology unless you feel the pain of not having it. It applies to the big, architectural decisions as well as the smaller everyday programming decisions. Suffering-oriented programming greatly reduces risk by ensuring that you're always working on something important, and it ensures that you are well-versed in a problem space before attempting a large investment.
 
I ha</p><p>5 0.21433872 <a title="31-lsi-5" href="../nathan_marz_storm-2009/nathan_marz_storm-2009-12-28-The_mathematics_behind_Hadoop-based_systems.html">1 nathan marz storm-2009-12-28-The mathematics behind Hadoop-based systems</a></p>
<p>Introduction: I wish I had known this a year ago. Now, with some simple mathematics I can finally answer:
  
 Why doesn't the speed of my workflow double when I double the amount of processing power? 
 Why does a 10% failure rate cause my runtime to go up by 300%? 
 How does optimizing out 30% of my workflow runtime cause the runtime to decrease by 80%? 
 How many machines should I have in my cluster to be adequately performant and fault-tolerant? 
  
All of these questions are neatly answered by one simple equation:
 
Runtime = Overhead / (1 - {Time to process one hour of data})
 
We will derive this equation in a moment. First, let's briefly discuss what I mean by "Hadoop-based system."  1   A common use-case of Hadoop is running a workflow that processes a continuous stream of incoming data. The workflow runs in a "while(true)" loop, and each iteration of the workflow processes the data that accumulated since last iteration.
 
The inspiration for the following analysis can be summarized in a simp</p><p>6 0.19262867 <a title="31-lsi-6" href="../nathan_marz_storm-2011/nathan_marz_storm-2011-03-29-My_talks_at_POSSCON.html">30 nathan marz storm-2011-03-29-My talks at POSSCON</a></p>
<p>7 0.17866531 <a title="31-lsi-7" href="../nathan_marz_storm-2010/nathan_marz_storm-2010-05-08-News_Feed_in_38_lines_of_code_using_Cascalog.html">16 nathan marz storm-2010-05-08-News Feed in 38 lines of code using Cascalog</a></p>
<p>8 0.17699991 <a title="31-lsi-8" href="../nathan_marz_storm-2012/nathan_marz_storm-2012-01-09-Early_access_edition_of_my_book_is_available.html">32 nathan marz storm-2012-01-09-Early access edition of my book is available</a></p>
<p>9 0.17219137 <a title="31-lsi-9" href="../nathan_marz_storm-2010/nathan_marz_storm-2010-01-03-Tips_for_Optimizing_Cascading_Flows.html">2 nathan marz storm-2010-01-03-Tips for Optimizing Cascading Flows</a></p>
<p>10 0.17057815 <a title="31-lsi-10" href="../nathan_marz_storm-2011/nathan_marz_storm-2011-01-07-Analysis_of_the_%23LessAmbitiousMovies_Twitter_Meme.html">27 nathan marz storm-2011-01-07-Analysis of the #LessAmbitiousMovies Twitter Meme</a></p>
<p>11 0.16069713 <a title="31-lsi-11" href="../nathan_marz_storm-2010/nathan_marz_storm-2010-04-14-Introducing_Cascalog%3A_a_Clojure-based_query_language_for_Hadoop.html">13 nathan marz storm-2010-04-14-Introducing Cascalog: a Clojure-based query language for Hadoop</a></p>
<p>12 0.14199336 <a title="31-lsi-12" href="../nathan_marz_storm-2010/nathan_marz_storm-2010-03-08-Follow-up_to_%22The_mathematics_behind_Hadoop-based_systems%22.html">8 nathan marz storm-2010-03-08-Follow-up to "The mathematics behind Hadoop-based systems"</a></p>
<p>13 0.13499749 <a title="31-lsi-13" href="../nathan_marz_storm-2010/nathan_marz_storm-2010-07-12-My_experience_as_the_first_employee_of_a_Y_Combinator_startup.html">19 nathan marz storm-2010-07-12-My experience as the first employee of a Y Combinator startup</a></p>
<p>14 0.11950497 <a title="31-lsi-14" href="../nathan_marz_storm-2013/nathan_marz_storm-2013-04-02-Principles_of_Software_Engineering%2C_Part_1.html">37 nathan marz storm-2013-04-02-Principles of Software Engineering, Part 1</a></p>
<p>15 0.11667296 <a title="31-lsi-15" href="../nathan_marz_storm-2010/nathan_marz_storm-2010-03-23-Migrating_data_from_a_SQL_database_to_Hadoop.html">11 nathan marz storm-2010-03-23-Migrating data from a SQL database to Hadoop</a></p>
<p>16 0.10157881 <a title="31-lsi-16" href="../nathan_marz_storm-2012/nathan_marz_storm-2012-09-19-Storm%27s_1st_birthday.html">34 nathan marz storm-2012-09-19-Storm's 1st birthday</a></p>
<p>17 0.094103754 <a title="31-lsi-17" href="../nathan_marz_storm-2010/nathan_marz_storm-2010-10-27-Fastest_Viable_Product%3A_Investing_in_Speed_at_a_Startup.html">23 nathan marz storm-2010-10-27-Fastest Viable Product: Investing in Speed at a Startup</a></p>
<p>18 0.089832082 <a title="31-lsi-18" href="../nathan_marz_storm-2010/nathan_marz_storm-2010-04-27-New_Cascalog_features%3A_outer_joins%2C_combiners%2C_sorting%2C_and_more.html">14 nathan marz storm-2010-04-27-New Cascalog features: outer joins, combiners, sorting, and more</a></p>
<p>19 0.087985903 <a title="31-lsi-19" href="../nathan_marz_storm-2013/nathan_marz_storm-2013-04-12-Break_into_Silicon_Valley_with_a_blog.html">38 nathan marz storm-2013-04-12-Break into Silicon Valley with a blog</a></p>
<p>20 0.084104471 <a title="31-lsi-20" href="../nathan_marz_storm-2010/nathan_marz_storm-2010-01-13-Mimi_Silbert%3A_the_greatest_hacker_in_the_world.html">3 nathan marz storm-2010-01-13-Mimi Silbert: the greatest hacker in the world</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/nathan_marz_storm_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.026), (5, 0.03), (10, 0.025), (21, 0.027), (26, 0.017), (28, 0.017), (33, 0.035), (41, 0.047), (48, 0.024), (59, 0.048), (61, 0.011), (68, 0.079), (76, 0.016), (78, 0.016), (82, 0.038), (84, 0.379), (88, 0.054), (99, 0.028)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97499114 <a title="31-lda-1" href="../nathan_marz_storm-2011/nathan_marz_storm-2011-10-13-How_to_beat_the_CAP_theorem.html">31 nathan marz storm-2011-10-13-How to beat the CAP theorem</a></p>
<p>Introduction: The CAP theorem states a database cannot guarantee consistency, availability, and partition-tolerance at the same time. But you can't sacrifice partition-tolerance (see  here  and  here ), so you must make a tradeoff between availability and consistency. Managing this tradeoff is a central focus of the NoSQL movement.
 
Consistency means that after you do a successful write, future reads will always take that write into account. Availability means that you can always read and write to the system. During a partition, you can only have one of these properties.
 
Systems that choose consistency over availability have to deal with some awkward issues. What do you do when the database isn't available? You can try buffering writes for later, but you risk losing those writes if you lose the machine with the buffer. Also, buffering writes can be a form of inconsistency because a client thinks a write has succeeded but the write isn't in the database yet. Alternatively, you can return errors ba</p><p>2 0.33951539 <a title="31-lda-2" href="../nathan_marz_storm-2010/nathan_marz_storm-2010-03-10-Thrift_%2B_Graphs_%3D_Strong%2C_flexible_schemas_on_Hadoop.html">9 nathan marz storm-2010-03-10-Thrift + Graphs = Strong, flexible schemas on Hadoop</a></p>
<p>Introduction: There are a lot of misconceptions about what Hadoop is useful for and what kind of data you can put in it. A lot of people think that Hadoop is meant for unstructured data like log files. While Hadoop is great for log files, it's also  fantastic  for strongly typed, structured data.
 
In this post I'll discuss how you can use a tool like  Thrift  to store strongly typed data in Hadoop while retaining the flexibility to evolve your schema. We'll look at graph-based schemas and see why they are an ideal fit for many Hadoop-based applications.
 
 OK, so what kind of "structured" data can you put in Hadoop? 
 
Anything! At  BackType  we put data about news, conversations, and people into Hadoop as structured objects. You can easily push structured information about social graphs, financial information, or anything you want into Hadoop. Â  
 
 That sounds all well and good, but why not just use JSON as the data format? 
 
JSON doesn't give you a real schema and doesn't protect against data i</p><p>3 0.29566988 <a title="31-lda-3" href="../nathan_marz_storm-2014/nathan_marz_storm-2014-02-12-Interview_with_%22Programmer_Magazine%22.html">39 nathan marz storm-2014-02-12-Interview with "Programmer Magazine"</a></p>
<p>Introduction: I was  recently interviewed  for "Programmer Magazine", a Chinese magazine. The interview was published in Chinese, but a lot of people told me they'd like to see the English version of the interview. Due to the Google translation being, ahem, a little iffy, I decided to just publish the original English version on my blog. Hope you enjoy! 
  What drew you to programming and what was the first interesting program you wrote?  
I started programming when I was 10 years old on my TI-82 graphing calculator. Initially I started programming because I wanted to make games on my calculator – and also because I was bored in math class :D. The first interesting game I made on my calculator was an archery game where you'd shoot arrows at moving targets. You'd get points for hitting more targets or completing all the targets faster. A couple years later I graduated to programming the TI-89 which was a huge upgrade in power. I remember how the TI-82 only let you have 26 variables (for the character</p><p>4 0.27393293 <a title="31-lda-4" href="../nathan_marz_storm-2012/nathan_marz_storm-2012-02-06-Suffering-oriented_programming.html">33 nathan marz storm-2012-02-06-Suffering-oriented programming</a></p>
<p>Introduction: Someone asked me an interesting question the other day: "How did you justify taking such a huge risk on building  Storm  while working on a  startup ?" (Storm is a realtime computation system). I can see how from an outsider's perspective investing in such a massive project seems extremely risky for a startup. From my perspective, though, building Storm wasn't risky at all. It was challenging, but not risky.
 
I follow a style of development that greatly reduces the risk of big projects like Storm. I call this style "suffering-oriented programming." Suffering-oriented programming can be summarized like so: don't build technology unless you feel the pain of not having it. It applies to the big, architectural decisions as well as the smaller everyday programming decisions. Suffering-oriented programming greatly reduces risk by ensuring that you're always working on something important, and it ensures that you are well-versed in a problem space before attempting a large investment.
 
I ha</p><p>5 0.22883447 <a title="31-lda-5" href="../nathan_marz_storm-2010/nathan_marz_storm-2010-04-14-Introducing_Cascalog%3A_a_Clojure-based_query_language_for_Hadoop.html">13 nathan marz storm-2010-04-14-Introducing Cascalog: a Clojure-based query language for Hadoop</a></p>
<p>Introduction: I'm very excited to be releasing  Cascalog  as open-source today. Cascalog is a Clojure-based query language for Hadoop inspired by  Datalog .
  Highlights   
  Simple  - Functions, filters, and aggregators all use the same syntax. Joins are implicit and natural. 
  Expressive  - Logical composition is very powerful, and you can run arbitrary Clojure code in your query with little effort. 
  Interactive  - Run queries from the Clojure REPL. 
  Scalable  - Cascalog queries run as a series of MapReduce jobs. 
  Query anything  - Query HDFS data, database data, and/or local data by making use of Cascading's "Tap" abstraction 
  Careful handling of null values  - Null values can make life difficult. Cascalog has a feature called "non-nullable variables" that makes dealing with nulls painless. 
  First class interoperability with Cascading  - Operations defined for Cascalog can be used in a Cascading flow and vice-versa 
  First class interoperability with Clojure  - Can use regular Clojure</p><p>6 0.19411869 <a title="31-lda-6" href="../nathan_marz_storm-2013/nathan_marz_storm-2013-04-02-Principles_of_Software_Engineering%2C_Part_1.html">37 nathan marz storm-2013-04-02-Principles of Software Engineering, Part 1</a></p>
<p>7 0.1889465 <a title="31-lda-7" href="../nathan_marz_storm-2010/nathan_marz_storm-2010-07-12-My_experience_as_the_first_employee_of_a_Y_Combinator_startup.html">19 nathan marz storm-2010-07-12-My experience as the first employee of a Y Combinator startup</a></p>
<p>8 0.18488115 <a title="31-lda-8" href="../nathan_marz_storm-2010/nathan_marz_storm-2010-03-08-Follow-up_to_%22The_mathematics_behind_Hadoop-based_systems%22.html">8 nathan marz storm-2010-03-08-Follow-up to "The mathematics behind Hadoop-based systems"</a></p>
<p>9 0.18043879 <a title="31-lda-9" href="../nathan_marz_storm-2010/nathan_marz_storm-2010-01-03-Tips_for_Optimizing_Cascading_Flows.html">2 nathan marz storm-2010-01-03-Tips for Optimizing Cascading Flows</a></p>
<p>10 0.17504404 <a title="31-lda-10" href="../nathan_marz_storm-2009/nathan_marz_storm-2009-12-28-The_mathematics_behind_Hadoop-based_systems.html">1 nathan marz storm-2009-12-28-The mathematics behind Hadoop-based systems</a></p>
<p>11 0.16753459 <a title="31-lda-11" href="../nathan_marz_storm-2014/nathan_marz_storm-2014-05-10-Why_we_in_tech_must_support_Lawrence_Lessig.html">41 nathan marz storm-2014-05-10-Why we in tech must support Lawrence Lessig</a></p>
<p>12 0.16741152 <a title="31-lda-12" href="../nathan_marz_storm-2013/nathan_marz_storm-2013-04-12-Break_into_Silicon_Valley_with_a_blog.html">38 nathan marz storm-2013-04-12-Break into Silicon Valley with a blog</a></p>
<p>13 0.16622187 <a title="31-lda-13" href="../nathan_marz_storm-2010/nathan_marz_storm-2010-10-05-How_to_get_a_job_at_a_kick-ass_startup_%28for_programmers%29.html">22 nathan marz storm-2010-10-05-How to get a job at a kick-ass startup (for programmers)</a></p>
<p>14 0.1644416 <a title="31-lda-14" href="../nathan_marz_storm-2010/nathan_marz_storm-2010-10-27-Fastest_Viable_Product%3A_Investing_in_Speed_at_a_Startup.html">23 nathan marz storm-2010-10-27-Fastest Viable Product: Investing in Speed at a Startup</a></p>
<p>15 0.16428638 <a title="31-lda-15" href="../nathan_marz_storm-2010/nathan_marz_storm-2010-08-20-5_Tips_for_Thinking_Under_Uncertainty.html">21 nathan marz storm-2010-08-20-5 Tips for Thinking Under Uncertainty</a></p>
<p>16 0.16242841 <a title="31-lda-16" href="../nathan_marz_storm-2010/nathan_marz_storm-2010-06-16-Your_company_has_a_knowledge_debt_problem.html">18 nathan marz storm-2010-06-16-Your company has a knowledge debt problem</a></p>
<p>17 0.16087805 <a title="31-lda-17" href="../nathan_marz_storm-2014/nathan_marz_storm-2014-02-24-The_inexplicable_rise_of_open_floor_plans_in_tech_companies.html">40 nathan marz storm-2014-02-24-The inexplicable rise of open floor plans in tech companies</a></p>
<p>18 0.15625679 <a title="31-lda-18" href="../nathan_marz_storm-2010/nathan_marz_storm-2010-05-08-News_Feed_in_38_lines_of_code_using_Cascalog.html">16 nathan marz storm-2010-05-08-News Feed in 38 lines of code using Cascalog</a></p>
<p>19 0.15579261 <a title="31-lda-19" href="../nathan_marz_storm-2010/nathan_marz_storm-2010-03-04-Introducing_%22Nanny%22_-_a_really_simple_dependency_management_tool.html">7 nathan marz storm-2010-03-04-Introducing "Nanny" - a really simple dependency management tool</a></p>
<p>20 0.14784348 <a title="31-lda-20" href="../nathan_marz_storm-2010/nathan_marz_storm-2010-12-06-You_Are_a_Product.html">25 nathan marz storm-2010-12-06-You Are a Product</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
