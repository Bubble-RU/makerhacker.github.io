<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1445 high scalability-2013-04-24-Strategy: Using Lots of RAM Often Cheaper than Using a Hadoop Cluster</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2013" href="../home/high_scalability-2013_home.html">high_scalability-2013</a> <a title="high_scalability-2013-1445" href="#">high_scalability-2013-1445</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1445 high scalability-2013-04-24-Strategy: Using Lots of RAM Often Cheaper than Using a Hadoop Cluster</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2013-1445-html" href="http://highscalability.com//blog/2013/4/24/strategy-using-lots-of-ram-often-cheaper-than-using-a-hadoop.html">html</a></p><p>Introduction: Solving problems while saving money is always a problem. In Nobody ever got
ďŹ red for using Hadoop on a cluster they give some counter-intuitive advice by
showing a big-memory server may  provide better performance per dollar than a
cluster:For jobs where the input data is multi-terabyte or larger a Hadoop
cluster is the right solution.For smaller problems memory has reached a GB/$
ratio where it is technically and financially feasible to use a single server
with 100s of GB of DRAM rather than a cluster. Given the majority of analytics
jobs do not process huge data sets, a cluster doesn't need to be your first
option. Scaling up RAM saves on programmer time, reduces programmer effort,
improved accuracy, and reduces hardware costs.</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cluster', 0.281), ('financially', 0.252), ('reduces', 0.24), ('programmer', 0.233), ('feasible', 0.231), ('jobs', 0.217), ('accuracy', 0.201), ('hadoop', 0.195), ('dollar', 0.189), ('technically', 0.179), ('dram', 0.179), ('ratio', 0.17), ('saves', 0.165), ('red', 0.165), ('reached', 0.159), ('saving', 0.155), ('majority', 0.151), ('input', 0.145), ('showing', 0.142), ('advice', 0.136)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="1445-tfidf-1" href="../high_scalability-2013/high_scalability-2013-04-24-Strategy%3A_Using_Lots_of_RAM_Often_Cheaper_than_Using_a_Hadoop_Cluster.html">1445 high scalability-2013-04-24-Strategy: Using Lots of RAM Often Cheaper than Using a Hadoop Cluster</a></p>
<p>Introduction: Solving problems while saving money is always a problem. In Nobody ever got
ďŹ red for using Hadoop on a cluster they give some counter-intuitive advice by
showing a big-memory server may  provide better performance per dollar than a
cluster:For jobs where the input data is multi-terabyte or larger a Hadoop
cluster is the right solution.For smaller problems memory has reached a GB/$
ratio where it is technically and financially feasible to use a single server
with 100s of GB of DRAM rather than a cluster. Given the majority of analytics
jobs do not process huge data sets, a cluster doesn't need to be your first
option. Scaling up RAM saves on programmer time, reduces programmer effort,
improved accuracy, and reduces hardware costs.</p><p>2 0.18043467 <a title="1445-tfidf-2" href="../high_scalability-2008/high_scalability-2008-02-19-Hadoop_Getting_Closer_to_1.0_Release.html">254 high scalability-2008-02-19-Hadoop Getting Closer to 1.0 Release</a></p>
<p>Introduction: Update:Yahoo! Launches World's Largest Hadoop Production Application. A 10,000
core Hadoop cluster produces data used in every Yahoo! Web search query. Raw
disk is at 5 Petabytes. Their previous 1 petabyte database couldn't handle the
load and couldn't grow larger.Greg Lindenthinks the Google cluster has way
over 133,000 machines.From an InfoQinterviewwith project lead Doug Cutting, it
appearsHadoop, an open source distributed computing platform, is making good
progress towards their 1.0 release. They've successfully reached a 1000 node
cluster size, improved file system integrity, and jacked performance by 20x in
the last year.How they are making progress could be a good model for
anyone:breakThe speedup has been an aggregation of our work in the past few
years, and has been accomplished mostly by trial-and-error. We get things
running smoothly on a cluster of a given size, then double the size of the
cluster and see what breaks. We aim for performance to scale linearly as you
increas</p><p>3 0.12486821 <a title="1445-tfidf-3" href="../high_scalability-2008/high_scalability-2008-09-28-Product%3A_Happy_%3D_Hadoop_%2B_Python.html">397 high scalability-2008-09-28-Product: Happy = Hadoop + Python</a></p>
<p>Introduction: Has a Java only Hadoop been getting you down? Now you can beHappy. Happy is
aframework for writing map-reduce programs for Hadoop using Jython. It files
off the sharp edges on Hadoop and makes writing map-reduce programs a breeze.
There's really no history yet on Happy, but I'm delighted at the idea of being
able to map-reduce inother languages. The more ways the better.From the
website:Happy is a framework that allows Hadoop jobs to be written and run in
Python 2.2 using Jython. It is aneasy way to write map-reduce programs for
Hadoop, and includes some new useful features as well.The current release
supports Hadoop 0.17.2.Map-reduce jobs in Happy are defined by sub-classing
happy.HappyJob and implementing amap(records, task) and reduce(key, values,
task) function. Then you create an instance of theclass, set the job
parameters (such as inputs and outputs) and call run().When you call run(),
Happy serializes your job instance and copies it and all accompanyinglibraries
out to the Hado</p><p>4 0.12129965 <a title="1445-tfidf-4" href="../high_scalability-2011/high_scalability-2011-01-04-Map-Reduce_With_Ruby_Using_Hadoop.html">968 high scalability-2011-01-04-Map-Reduce With Ruby Using Hadoop</a></p>
<p>Introduction: A demonstration, with repeatable steps, of how to quickly fire-up a Hadoop
cluster on Amazon EC2, load data onto the HDFS (Hadoop Distributed File-
System), write map-reduce scripts in Ruby and use them to run a map-reduce job
on your Hadoop cluster. You willnotneed to ssh into the cluster, as all tasks
are run from your local machine. Below I am using my MacBook Pro as my local
machine, but the steps I have provided should be reproducible on other
platforms running bash and Java.Fire-Up Your Hadoop ClusterI choose
theCloudera distribution of Hadoopwhich is still 100% Apache licensed, but has
some additional benefits. One of these benefits is that it is released byDoug
Cutting, who started Hadoop and drove it’s development at Yahoo! He also
startedLucene, which is another of my favourite Apache Projects, so I have
good faith that he knows what he is doing. Another benefit, as you will see,
is that it is simple to fire-up a Hadoop cluster.I am going to use
Cloudera’sWhirr script, which</p><p>5 0.10939138 <a title="1445-tfidf-5" href="../high_scalability-2007/high_scalability-2007-12-28-Amazon%27s_EC2%3A_Pay_as_You_Grow_Could_Cut_Your_Costs_in_Half.html">195 high scalability-2007-12-28-Amazon's EC2: Pay as You Grow Could Cut Your Costs in Half</a></p>
<p>Introduction: Update 2: SummizeComputes Computing Resources for a Startup. Lots of nice
graphs showing Amazon is hard to beat for small machines and become less cost
efficient for well used larger machines. Long term storage costs may eat your
saving away. And out of cloud bandwidth costs are high.Update:
viaProductionScale, a nice Digital Web article onhow to setup S3 to store
media filesand how Blue Origin was able to handle 3.5 million requests and 758
GBs in bandwidth in a single day for very little $$$. Also a Right Scale
article onNetwork performance within Amazon EC2 and to Amazon S3. 75MB/s
between EC2 instances, 10.2MB/s between EC2 and S3 for download, 6.9MB/s
upload.Now that Amazon's S3 (storage service) isout of betaand EC2 (elastic
compute cloud) has added newinstance types(the class of machine you can rent)
with more CPU and more RAM, I thought it would be interesting to take a look
out how their pricing stacks up.The quick conclusion:the more you scale the
more you save. A six node co</p><p>6 0.10487552 <a title="1445-tfidf-6" href="../high_scalability-2010/high_scalability-2010-07-20-Strategy%3A_Consider_When_a_Service_Starts_Billing_in_Your_Algorithm_Cost.html">862 high scalability-2010-07-20-Strategy: Consider When a Service Starts Billing in Your Algorithm Cost</a></p>
<p>7 0.10451291 <a title="1445-tfidf-7" href="../high_scalability-2009/high_scalability-2009-06-11-Yahoo%21_Distribution_of_Hadoop.html">627 high scalability-2009-06-11-Yahoo! Distribution of Hadoop</a></p>
<p>8 0.10377393 <a title="1445-tfidf-8" href="../high_scalability-2009/high_scalability-2009-07-30-Learn_How_to_Think_at_Scale.html">666 high scalability-2009-07-30-Learn How to Think at Scale</a></p>
<p>9 0.10327198 <a title="1445-tfidf-9" href="../high_scalability-2011/high_scalability-2011-07-27-Making_Hadoop_1000x_Faster_for_Graph_Problems.html">1088 high scalability-2011-07-27-Making Hadoop 1000x Faster for Graph Problems</a></p>
<p>10 0.1009643 <a title="1445-tfidf-10" href="../high_scalability-2009/high_scalability-2009-05-17-Product%3A_Hadoop.html">601 high scalability-2009-05-17-Product: Hadoop</a></p>
<p>11 0.10017792 <a title="1445-tfidf-11" href="../high_scalability-2013/high_scalability-2013-06-26-Leveraging_Cloud_Computing_at_Yelp_-_102_Million_Monthly_Vistors_and_39_Million_Reviews.html">1482 high scalability-2013-06-26-Leveraging Cloud Computing at Yelp - 102 Million Monthly Vistors and 39 Million Reviews</a></p>
<p>12 0.099241585 <a title="1445-tfidf-12" href="../high_scalability-2013/high_scalability-2013-05-06-7_Not_So_Sexy_Tips_for_Saving_Money_On_Amazon.html">1452 high scalability-2013-05-06-7 Not So Sexy Tips for Saving Money On Amazon</a></p>
<p>13 0.089836866 <a title="1445-tfidf-13" href="../high_scalability-2014/high_scalability-2014-03-24-Big%2C_Small%2C_Hot_or_Cold_-_Examples_of_Robust_Data_Pipelines_from_Stripe%2C_Tapad%2C_Etsy_and_Square.html">1618 high scalability-2014-03-24-Big, Small, Hot or Cold - Examples of Robust Data Pipelines from Stripe, Tapad, Etsy and Square</a></p>
<p>14 0.088746555 <a title="1445-tfidf-14" href="../high_scalability-2011/high_scalability-2011-03-01-Sponsored_Post%3A__ScaleOut%2C_aiCache%2C_WAPT%2C_Karmasphere%2C_Kabam%2C_Opera_Solutions%2C_Newrelic%2C_Cloudkick%2C_Membase%2C_Joyent%2C_CloudSigma%2C_ManageEngine%2C_Site24x7.html">997 high scalability-2011-03-01-Sponsored Post:  ScaleOut, aiCache, WAPT, Karmasphere, Kabam, Opera Solutions, Newrelic, Cloudkick, Membase, Joyent, CloudSigma, ManageEngine, Site24x7</a></p>
<p>15 0.088034421 <a title="1445-tfidf-15" href="../high_scalability-2008/high_scalability-2008-01-13-Google_Reveals_New_MapReduce_Stats.html">211 high scalability-2008-01-13-Google Reveals New MapReduce Stats</a></p>
<p>16 0.086765103 <a title="1445-tfidf-16" href="../high_scalability-2013/high_scalability-2013-02-11-At_Scale_Even_Little_Wins_Pay_Off_Big_-_Google_and_Facebook_Examples.html">1404 high scalability-2013-02-11-At Scale Even Little Wins Pay Off Big - Google and Facebook Examples</a></p>
<p>17 0.086011373 <a title="1445-tfidf-17" href="../high_scalability-2012/high_scalability-2012-08-28-Making_Hadoop_Run_Faster.html">1313 high scalability-2012-08-28-Making Hadoop Run Faster</a></p>
<p>18 0.085893258 <a title="1445-tfidf-18" href="../high_scalability-2013/high_scalability-2013-07-01-PRISM%3A_The_Amazingly_Low_Cost_of_%C2%ADUsing_BigData_to_Know_More_About_You_in_Under_a_Minute.html">1485 high scalability-2013-07-01-PRISM: The Amazingly Low Cost of ­Using BigData to Know More About You in Under a Minute</a></p>
<p>19 0.082830444 <a title="1445-tfidf-19" href="../high_scalability-2009/high_scalability-2009-03-16-Are_Cloud_Based_Memory_Architectures_the_Next_Big_Thing%3F.html">538 high scalability-2009-03-16-Are Cloud Based Memory Architectures the Next Big Thing?</a></p>
<p>20 0.08223746 <a title="1445-tfidf-20" href="../high_scalability-2010/high_scalability-2010-04-06-Strategy%3A_Make_it_Really_Fast_vs_Do_the_Work_Up_Front.html">805 high scalability-2010-04-06-Strategy: Make it Really Fast vs Do the Work Up Front</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.129), (1, 0.061), (2, 0.013), (3, 0.0), (4, -0.004), (5, 0.036), (6, 0.086), (7, 0.034), (8, 0.076), (9, 0.037), (10, 0.058), (11, -0.063), (12, 0.081), (13, -0.046), (14, 0.036), (15, -0.018), (16, -0.011), (17, -0.012), (18, -0.036), (19, 0.078), (20, -0.022), (21, 0.059), (22, 0.041), (23, -0.026), (24, -0.019), (25, 0.02), (26, 0.044), (27, -0.025), (28, -0.0), (29, 0.026), (30, 0.088), (31, 0.084), (32, 0.002), (33, 0.02), (34, -0.012), (35, 0.045), (36, -0.032), (37, 0.022), (38, -0.01), (39, -0.036), (40, 0.066), (41, 0.037), (42, -0.037), (43, 0.002), (44, 0.029), (45, 0.028), (46, 0.021), (47, -0.015), (48, -0.002), (49, -0.081)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96362561 <a title="1445-lsi-1" href="../high_scalability-2013/high_scalability-2013-04-24-Strategy%3A_Using_Lots_of_RAM_Often_Cheaper_than_Using_a_Hadoop_Cluster.html">1445 high scalability-2013-04-24-Strategy: Using Lots of RAM Often Cheaper than Using a Hadoop Cluster</a></p>
<p>Introduction: Solving problems while saving money is always a problem. In Nobody ever got
ďŹ red for using Hadoop on a cluster they give some counter-intuitive advice by
showing a big-memory server may  provide better performance per dollar than a
cluster:For jobs where the input data is multi-terabyte or larger a Hadoop
cluster is the right solution.For smaller problems memory has reached a GB/$
ratio where it is technically and financially feasible to use a single server
with 100s of GB of DRAM rather than a cluster. Given the majority of analytics
jobs do not process huge data sets, a cluster doesn't need to be your first
option. Scaling up RAM saves on programmer time, reduces programmer effort,
improved accuracy, and reduces hardware costs.</p><p>2 0.79762214 <a title="1445-lsi-2" href="../high_scalability-2010/high_scalability-2010-07-20-Strategy%3A_Consider_When_a_Service_Starts_Billing_in_Your_Algorithm_Cost.html">862 high scalability-2010-07-20-Strategy: Consider When a Service Starts Billing in Your Algorithm Cost</a></p>
<p>Introduction: At Monday'sCloud Computing Meetup,Paco Nathangave an excellentGetting Started
on Hadooptalk (slides). I found one of Paco's strategies particularly
interesting: consider when a service starts charging in cost calculations.
Depending on your use case it may be cheaper to go with a more expensive
service that charges only for work accomplished rather than charging for both
work + startup time.The example is comparing the cost of running Hadoop on AWS
yourself versus using Amazon's prepackaged Hadoop service,Elastic
MapReduce(EMR). The thought may have gone through your mind as it did mine
that it doesn't necessarily make sense to use Amazon's Hadoop service. Why pay
a premium for EMR when Hadoop will run directly on AWS?One reason is that
Amazon has made significant changes to Hadoop to make it run more efficiently
and easily on AWS. The other more surprising reason is cost.When starting a
500 node Hadoop cluster, for example, you have to wait for all the nodes to
start and join the clus</p><p>3 0.77801239 <a title="1445-lsi-3" href="../high_scalability-2011/high_scalability-2011-01-04-Map-Reduce_With_Ruby_Using_Hadoop.html">968 high scalability-2011-01-04-Map-Reduce With Ruby Using Hadoop</a></p>
<p>Introduction: A demonstration, with repeatable steps, of how to quickly fire-up a Hadoop
cluster on Amazon EC2, load data onto the HDFS (Hadoop Distributed File-
System), write map-reduce scripts in Ruby and use them to run a map-reduce job
on your Hadoop cluster. You willnotneed to ssh into the cluster, as all tasks
are run from your local machine. Below I am using my MacBook Pro as my local
machine, but the steps I have provided should be reproducible on other
platforms running bash and Java.Fire-Up Your Hadoop ClusterI choose
theCloudera distribution of Hadoopwhich is still 100% Apache licensed, but has
some additional benefits. One of these benefits is that it is released byDoug
Cutting, who started Hadoop and drove it’s development at Yahoo! He also
startedLucene, which is another of my favourite Apache Projects, so I have
good faith that he knows what he is doing. Another benefit, as you will see,
is that it is simple to fire-up a Hadoop cluster.I am going to use
Cloudera’sWhirr script, which</p><p>4 0.77406633 <a title="1445-lsi-4" href="../high_scalability-2008/high_scalability-2008-09-28-Product%3A_Happy_%3D_Hadoop_%2B_Python.html">397 high scalability-2008-09-28-Product: Happy = Hadoop + Python</a></p>
<p>Introduction: Has a Java only Hadoop been getting you down? Now you can beHappy. Happy is
aframework for writing map-reduce programs for Hadoop using Jython. It files
off the sharp edges on Hadoop and makes writing map-reduce programs a breeze.
There's really no history yet on Happy, but I'm delighted at the idea of being
able to map-reduce inother languages. The more ways the better.From the
website:Happy is a framework that allows Hadoop jobs to be written and run in
Python 2.2 using Jython. It is aneasy way to write map-reduce programs for
Hadoop, and includes some new useful features as well.The current release
supports Hadoop 0.17.2.Map-reduce jobs in Happy are defined by sub-classing
happy.HappyJob and implementing amap(records, task) and reduce(key, values,
task) function. Then you create an instance of theclass, set the job
parameters (such as inputs and outputs) and call run().When you call run(),
Happy serializes your job instance and copies it and all accompanyinglibraries
out to the Hado</p><p>5 0.76512462 <a title="1445-lsi-5" href="../high_scalability-2012/high_scalability-2012-06-15-Stuff_The_Internet_Says_On_Scalability_For_June_15%2C_2012.html">1265 high scalability-2012-06-15-Stuff The Internet Says On Scalability For June 15, 2012</a></p>
<p>Introduction: It's HighScalability Time:100PB: Facebook HDFS Cluster;One Trillion: Objects
in S3Quotable quotes:@mwinkle: Listening to NASA big data challenges at
‪#hadoopSummit‬, the square kilometer array project will produce 700tb per
second. TB. Per second.@imrantech: #hadoopsummit‬ @twitter - 400M tweets,
80-100TB per day@r39132: At Netflix talk at ‪#hadoopsummit‬ : 2 B hours
streamed in Q4 2011, 75% of the 30M daily movie starts are sourced from
recommendations@nattybnatkins: Run job. Identify bottleneck. Address
bottleneck. Repeat. Sage wisdom from @tlipcon on optimizing MR jobs ‬
‪#HadoopSummit‬@chiradeep:  mainframe cost of operation - $5k per MIP per year
‪#hadoopsummit‬@MCanalytics: #hadoopsummit‬ Yahoo metrics - 140pb on 42k nodes
with 500 users on 360k Hadoop jobs for 100b events/day Holy smokes!@M_Wein:
Domain expertise is the wave of the future: it's more about "Hadoop and
Healthcare" than "Using Bayesian counters with Hadoop"
‪#hadoopsummit‬@JohnM_Haddad: Netflix at ‪#Hadoopsummit‬ l</p><p>6 0.75875813 <a title="1445-lsi-6" href="../high_scalability-2009/high_scalability-2009-05-17-Product%3A_Hadoop.html">601 high scalability-2009-05-17-Product: Hadoop</a></p>
<p>7 0.75720692 <a title="1445-lsi-7" href="../high_scalability-2012/high_scalability-2012-01-12-Peregrine_-_A_Map_Reduce_Framework_for_Iterative_and_Pipelined_Jobs.html">1173 high scalability-2012-01-12-Peregrine - A Map Reduce Framework for Iterative and Pipelined Jobs</a></p>
<p>8 0.7486341 <a title="1445-lsi-8" href="../high_scalability-2012/high_scalability-2012-08-28-Making_Hadoop_Run_Faster.html">1313 high scalability-2012-08-28-Making Hadoop Run Faster</a></p>
<p>9 0.73394614 <a title="1445-lsi-9" href="../high_scalability-2009/high_scalability-2009-08-03-Building_a_Data_Intensive_Web_Application_with_Cloudera%2C_Hadoop%2C_Hive%2C_Pig%2C_and_EC2.html">669 high scalability-2009-08-03-Building a Data Intensive Web Application with Cloudera, Hadoop, Hive, Pig, and EC2</a></p>
<p>10 0.72816366 <a title="1445-lsi-10" href="../high_scalability-2008/high_scalability-2008-02-19-Hadoop_Getting_Closer_to_1.0_Release.html">254 high scalability-2008-02-19-Hadoop Getting Closer to 1.0 Release</a></p>
<p>11 0.71957886 <a title="1445-lsi-11" href="../high_scalability-2008/high_scalability-2008-11-14-Paper%3A_Pig_Latin%3A_A_Not-So-Foreign_Language_for_Data_Processing.html">443 high scalability-2008-11-14-Paper: Pig Latin: A Not-So-Foreign Language for Data Processing</a></p>
<p>12 0.68000478 <a title="1445-lsi-12" href="../high_scalability-2013/high_scalability-2013-01-07-Analyzing_billions_of_credit_card_transactions_and_serving_low-latency_insights_in_the_cloud.html">1382 high scalability-2013-01-07-Analyzing billions of credit card transactions and serving low-latency insights in the cloud</a></p>
<p>13 0.6726383 <a title="1445-lsi-13" href="../high_scalability-2009/high_scalability-2009-06-11-Yahoo%21_Distribution_of_Hadoop.html">627 high scalability-2009-06-11-Yahoo! Distribution of Hadoop</a></p>
<p>14 0.65442187 <a title="1445-lsi-14" href="../high_scalability-2008/high_scalability-2008-10-15-Hadoop_-_A_Primer.html">414 high scalability-2008-10-15-Hadoop - A Primer</a></p>
<p>15 0.65172678 <a title="1445-lsi-15" href="../high_scalability-2009/high_scalability-2009-07-30-Learn_How_to_Think_at_Scale.html">666 high scalability-2009-07-30-Learn How to Think at Scale</a></p>
<p>16 0.64027417 <a title="1445-lsi-16" href="../high_scalability-2010/high_scalability-2010-07-02-Hot_Scalability_Links_for_July_2%2C_2010.html">851 high scalability-2010-07-02-Hot Scalability Links for July 2, 2010</a></p>
<p>17 0.62452406 <a title="1445-lsi-17" href="../high_scalability-2011/high_scalability-2011-07-27-Making_Hadoop_1000x_Faster_for_Graph_Problems.html">1088 high scalability-2011-07-27-Making Hadoop 1000x Faster for Graph Problems</a></p>
<p>18 0.62398517 <a title="1445-lsi-18" href="../high_scalability-2013/high_scalability-2013-06-26-Leveraging_Cloud_Computing_at_Yelp_-_102_Million_Monthly_Vistors_and_39_Million_Reviews.html">1482 high scalability-2013-06-26-Leveraging Cloud Computing at Yelp - 102 Million Monthly Vistors and 39 Million Reviews</a></p>
<p>19 0.62316102 <a title="1445-lsi-19" href="../high_scalability-2011/high_scalability-2011-07-07-Myth%3A_Google_Uses_Server_Farms_So_You_Should_Too_-_Resurrection_of_the_Big-Ass_Machines.html">1075 high scalability-2011-07-07-Myth: Google Uses Server Farms So You Should Too - Resurrection of the Big-Ass Machines</a></p>
<p>20 0.61686993 <a title="1445-lsi-20" href="../high_scalability-2009/high_scalability-2009-07-02-Product%3A_Hbase.html">650 high scalability-2009-07-02-Product: Hbase</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.086), (2, 0.301), (61, 0.102), (70, 0.204), (79, 0.076), (94, 0.108)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.89005643 <a title="1445-lda-1" href="../high_scalability-2008/high_scalability-2008-04-02-Product%3A_Supervisor_-__Monitor_and_Control_Your_Processes.html">295 high scalability-2008-04-02-Product: Supervisor -  Monitor and Control Your Processes</a></p>
<p>Introduction: It's a sad fact of life, but processes die. I know, it's horrible. You start
them, send them out into process space, and hope for the best. Yet sometimes,
despite your best coding, they core dump, seg fault, or some other calamity
befalls them. Unlike our messy biological world so cruelly ruled by entropy,
in the digital world processes can be given another chance. They can be
restarted. A greater destiny awaits. And hopefully this time the random
lottery of unforeseen killing factors will be avoided and a long productive
life will be had by all.This is fun code to write because it's a lot more
complicated than you might think. And restarting processes is a highly
effective high availability strategy. Most faults are transient, caused by an
unexpected series of events. Rather than taking drastic action, like taking a
node out of production or failing over, transients can be effectively masked
by simply restarting failed processes. Though complexity makes it a fun
problem, it's also why</p><p>same-blog 2 0.88819039 <a title="1445-lda-2" href="../high_scalability-2013/high_scalability-2013-04-24-Strategy%3A_Using_Lots_of_RAM_Often_Cheaper_than_Using_a_Hadoop_Cluster.html">1445 high scalability-2013-04-24-Strategy: Using Lots of RAM Often Cheaper than Using a Hadoop Cluster</a></p>
<p>Introduction: Solving problems while saving money is always a problem. In Nobody ever got
ďŹ red for using Hadoop on a cluster they give some counter-intuitive advice by
showing a big-memory server may  provide better performance per dollar than a
cluster:For jobs where the input data is multi-terabyte or larger a Hadoop
cluster is the right solution.For smaller problems memory has reached a GB/$
ratio where it is technically and financially feasible to use a single server
with 100s of GB of DRAM rather than a cluster. Given the majority of analytics
jobs do not process huge data sets, a cluster doesn't need to be your first
option. Scaling up RAM saves on programmer time, reduces programmer effort,
improved accuracy, and reduces hardware costs.</p><p>3 0.87994158 <a title="1445-lda-3" href="../high_scalability-2012/high_scalability-2012-06-04-OpenFlow-SDN_is_Not_a_Silver_Bullet_for_Network_Scalability.html">1256 high scalability-2012-06-04-OpenFlow-SDN is Not a Silver Bullet for Network Scalability</a></p>
<p>Introduction: Ivan Pepelnjak(CCIE#1354 Emeritus) is Chief Technology Advisor atNIL Data
Communications,author of numerous webinarsandadvanced networking books, and
aprolific blogger. He's focusing on data center and cloud networking, network
virtualization, and scalable application design.OpenFlow is an interesting
emerging networking technology appearingseemingly out of nowhere with much
hype and fanfarein March 2011. More than a year later, there are two
commercial products based on OpenFlow (NEC's Programmable FlowandNicira's
Network Virtualization Platform) and probably less than a dozen production-
grade implementations (includingGoogle's G-Scale networkandIndiana
University's campus network). Is this an expected result for an emerging
technology or another case of overhyped technology hitting limits imposed by
reality?OpenFlow-based solutions have to overcome numerous problems every
emerging technology is facing, in OpenFlow's case ranging from compatibility
with existing chipsets to incomplet</p><p>4 0.85204875 <a title="1445-lda-4" href="../high_scalability-2008/high_scalability-2008-02-02-The_case_against_ORM_Frameworks_in_High_Scalability_Architectures.html">235 high scalability-2008-02-02-The case against ORM Frameworks in High Scalability Architectures</a></p>
<p>Introduction: Let me begin by saying that I have used and continue to use various ORM
frameworks such as hibernate, ibatis, propel and activerecord in applications
and websites that have a user base ranging from a couple hundred to 500k
users. Especially for projects that have to be up and running in a short
duration of time, ORM frameworks significantly reduce the effort required to
manipulate and persist OOP objects by providing time saving facilities such as
automatically generated model objects, integrated unit testing, secure
variable substitution, etc. Hibernate even supports horizontal data
partitioning via Hibernate Shards.However, the lay of the land is
significantly different in the rarefied space occupied by applications needing
to support millions of users. Profiling an application at this level and
paying particular attention to the operations needed to move data to and from
the database, it becomes evident that a significant portion of the operations
are API related, whereby the ORM fr</p><p>5 0.84484899 <a title="1445-lda-5" href="../high_scalability-2009/high_scalability-2009-03-19-Product%3A_Redis_-_Not_Just_Another_Key-Value_Store.html">545 high scalability-2009-03-19-Product: Redis - Not Just Another Key-Value Store</a></p>
<p>Introduction: With the introduction ofRedisyour options in the key-value space just grew and
your choice of which to pick just got a lot harder. But when you think about
it, that's not a bad position to be in at all.Redis (REmote DIctionary Server)
-a key-value database. It's similar to memcached but the dataset is not
volatile, and values can be strings, exactly like in memcached, but also lists
and sets with atomic operations to push/pop elements.The key points are: open
source; speed (benchmarked performing 110,000 SET operations, and 81,000 GETs,
per second); persistence, but in an asynchronous way taking everything in
memory; support for higher level data structures and atomic operations.The
home page is well organized so I'll spare the excessive-copying-to-make-this-
post-longer. For a good overview of Redis take a look at Antonio Cangiano's
article:Introducing Redis: a fast key-value database.If you are looking at a
way to understand how Redis is different than something like Tokyo
Cabinet/Ty</p><p>6 0.84399176 <a title="1445-lda-6" href="../high_scalability-2011/high_scalability-2011-09-28-Pursue_robust_indefinite_scalability_with_the_Movable_Feast_Machine.html">1127 high scalability-2011-09-28-Pursue robust indefinite scalability with the Movable Feast Machine</a></p>
<p>7 0.84088892 <a title="1445-lda-7" href="../high_scalability-2007/high_scalability-2007-07-16-Paper%3A_Guide_to_Cost-effective_Database_Scale-Out_using_MySQL.html">17 high scalability-2007-07-16-Paper: Guide to Cost-effective Database Scale-Out using MySQL</a></p>
<p>8 0.83956492 <a title="1445-lda-8" href="../high_scalability-2008/high_scalability-2008-08-16-Strategy%3A_Serve_Pre-generated_Static_Files_Instead_Of_Dynamic_Pages.html">365 high scalability-2008-08-16-Strategy: Serve Pre-generated Static Files Instead Of Dynamic Pages</a></p>
<p>9 0.83945811 <a title="1445-lda-9" href="../high_scalability-2009/high_scalability-2009-07-31-NSFW%3A_Hilarious_Fault-Tolerance_Cartoon_.html">667 high scalability-2009-07-31-NSFW: Hilarious Fault-Tolerance Cartoon </a></p>
<p>10 0.83841878 <a title="1445-lda-10" href="../high_scalability-2008/high_scalability-2008-09-03-Some_Facebook_Secrets_to_Better_Operations.html">378 high scalability-2008-09-03-Some Facebook Secrets to Better Operations</a></p>
<p>11 0.83816248 <a title="1445-lda-11" href="../high_scalability-2009/high_scalability-2009-04-05-At_Some_Point_the_Cost_of_Servers_Outweighs_the_Cost_of_Programmers.html">556 high scalability-2009-04-05-At Some Point the Cost of Servers Outweighs the Cost of Programmers</a></p>
<p>12 0.83815885 <a title="1445-lda-12" href="../high_scalability-2008/high_scalability-2008-09-03-MapReduce_framework_Disco.html">376 high scalability-2008-09-03-MapReduce framework Disco</a></p>
<p>13 0.8356418 <a title="1445-lda-13" href="../high_scalability-2011/high_scalability-2011-10-31-15_Ways_to_Make_Your_Application_Feel_More_Responsive_under_Google_App_Engine.html">1135 high scalability-2011-10-31-15 Ways to Make Your Application Feel More Responsive under Google App Engine</a></p>
<p>14 0.83509511 <a title="1445-lda-14" href="../high_scalability-2012/high_scalability-2012-09-19-The_4_Building_Blocks_of_Architecting_Systems_for_Scale.html">1325 high scalability-2012-09-19-The 4 Building Blocks of Architecting Systems for Scale</a></p>
<p>15 0.83467025 <a title="1445-lda-15" href="../high_scalability-2013/high_scalability-2013-12-09-Site_Moves_from_PHP_to_Facebook%27s_HipHop%2C_Now_Pages_Load_in_.6_Seconds_Instead_of_Five.html">1561 high scalability-2013-12-09-Site Moves from PHP to Facebook's HipHop, Now Pages Load in .6 Seconds Instead of Five</a></p>
<p>16 0.83459932 <a title="1445-lda-16" href="../high_scalability-2010/high_scalability-2010-11-16-Facebook%27s_New_Real-time_Messaging_System%3A_HBase_to_Store_135%2B_Billion_Messages_a_Month.html">943 high scalability-2010-11-16-Facebook's New Real-time Messaging System: HBase to Store 135+ Billion Messages a Month</a></p>
<p>17 0.83443385 <a title="1445-lda-17" href="../high_scalability-2008/high_scalability-2008-01-24-Mailinator_Architecture.html">221 high scalability-2008-01-24-Mailinator Architecture</a></p>
<p>18 0.83419979 <a title="1445-lda-18" href="../high_scalability-2007/high_scalability-2007-11-08-scaling_drupal_-_an_open-source_infrastructure_for_high-traffic_drupal_sites.html">146 high scalability-2007-11-08-scaling drupal - an open-source infrastructure for high-traffic drupal sites</a></p>
<p>19 0.83344042 <a title="1445-lda-19" href="../high_scalability-2010/high_scalability-2010-12-20-Netflix%3A_Use_Less_Chatty_Protocols_in_the_Cloud_-_Plus_26_Fixes.html">960 high scalability-2010-12-20-Netflix: Use Less Chatty Protocols in the Cloud - Plus 26 Fixes</a></p>
<p>20 0.83280027 <a title="1445-lda-20" href="../high_scalability-2010/high_scalability-2010-11-01-Hot_Trend%3A__Move_Behavior_to_Data_for_a_New_Interactive_Application_Architecture.html">933 high scalability-2010-11-01-Hot Trend:  Move Behavior to Data for a New Interactive Application Architecture</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
