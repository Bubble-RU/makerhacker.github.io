<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1498 high scalability-2013-08-07-RAFT - In Search of an Understandable Consensus Algorithm</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2013" href="../home/high_scalability-2013_home.html">high_scalability-2013</a> <a title="high_scalability-2013-1498" href="#">high_scalability-2013-1498</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1498 high scalability-2013-08-07-RAFT - In Search of an Understandable Consensus Algorithm</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2013-1498-html" href="http://highscalability.com//blog/2013/8/7/raft-in-search-of-an-understandable-consensus-algorithm.html">html</a></p><p>Introduction: If like many humans you've found even  Paxos Made Simple  a bit difficult to understand, you might enjoy RAFT as described in  In Search of an Understandable Consensus Algorithm  by Stanford's  Diego Ongaro  and John Ousterhout. The video presentation of the paper is given by  John Ousterhout . Both the paper and the video are delightfully accessible.
 
 mcherm  has a good summary of the paper:
  
 A consensus algorithm is: a cluster of servers should record a series of records ("log entries") in response to requests from clients of the cluster. (It may also take action based on those entries.) It does so in a way that guarantees that the responses seen by clients of the cluster will be consistent EVEN in the face of servers crashing in unpredictable ways (but not loosing data that was synched to disk), and networks introducing unpredictable delays or communication blockages. 
  
 Here's what Raft does. First, it elects a leader, then the leader records the master version of the log, t</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The video presentation of the paper is given by  John Ousterhout . [sent-2, score-0.125]
</p><p>2 Both the paper and the video are delightfully accessible. [sent-3, score-0.197]
</p><p>3 mcherm  has a good summary of the paper:     A consensus algorithm is: a cluster of servers should record a series of records ("log entries") in response to requests from clients of the cluster. [sent-4, score-0.515]
</p><p>4 ) It does so in a way that guarantees that the responses seen by clients of the cluster will be consistent EVEN in the face of servers crashing in unpredictable ways (but not loosing data that was synched to disk), and networks introducing unpredictable delays or communication blockages. [sent-6, score-0.571]
</p><p>5 That works unless the leader crashes or loses communication with too many others; in such a case Raft elects a new leader. [sent-9, score-0.531]
</p><p>6 The election process is designed to guarantee that any newly elected leader will have (at least) all of the already-committed entries. [sent-10, score-0.435]
</p><p>7 We also have a treat in the form of a great roundtable discussion of the topic via a  Think Distributed  hangout, featuring several folks from Basho, Peter Bailis, and Diego Ongaro. [sent-11, score-0.282]
</p><p>8 Perhaps the most interesting part of the talk came late in the discussion when Peter commented that he was astounded that an academic paper already has so many open source implementations. [sent-12, score-0.62]
</p><p>9 The key that others can learn from is:  understandability . [sent-14, score-0.178]
</p><p>10 Most academic papers are opaque, to put it generously. [sent-15, score-0.222]
</p><p>11 Diego talks about this saying:     Understandability came from the perspective of building the best system possible. [sent-16, score-0.131]
</p><p>12 RAFT made simplifying assumptions in having a leader, having a log, and how leader election occurs. [sent-18, score-0.45]
</p><p>13 These simplifications are fine most of the time during operation, but by simplifying they can write down a concrete explanation that people can understand. [sent-19, score-0.16]
</p><p>14 Academically this paper has been hard to publish. [sent-22, score-0.125]
</p><p>15 It lacks academic novelty in that it solves the same problem as Paxos. [sent-23, score-0.284]
</p><p>16 The key difference is if you look at the code they took a systems building approach, which obviously appeals to programmers more than academics. [sent-24, score-0.131]
</p><p>17 There was a discussion of a lack of clarity of using an AppendEntry to mean both HeartBeat and AppendEntry. [sent-28, score-0.137]
</p><p>18 While non-heartbeat messages can indicate a heartbeat, what do you do when there are no messages being sent? [sent-30, score-0.118]
</p><p>19 There's an interesting discussion if using RAFT could be used for all  RAMCloud  operations. [sent-34, score-0.137]
</p><p>20 Should your availability be dependendent on core dataservers or consensus groups or some other method? [sent-35, score-0.152]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('raft', 0.563), ('leader', 0.23), ('academic', 0.222), ('heartbeat', 0.192), ('appendentry', 0.178), ('elects', 0.178), ('understandability', 0.178), ('consensus', 0.152), ('discussion', 0.137), ('election', 0.125), ('paper', 0.125), ('bailis', 0.122), ('diego', 0.111), ('log', 0.107), ('unpredictable', 0.102), ('cluster', 0.097), ('simplifying', 0.095), ('peter', 0.089), ('entry', 0.087), ('elected', 0.08), ('roundtable', 0.08), ('ramcloud', 0.076), ('loosing', 0.076), ('acknowledge', 0.076), ('hangout', 0.076), ('crashing', 0.072), ('appended', 0.072), ('delightfully', 0.072), ('records', 0.071), ('unclear', 0.069), ('opaque', 0.069), ('commented', 0.069), ('record', 0.068), ('appeals', 0.067), ('clarity', 0.067), ('algorithm', 0.067), ('came', 0.067), ('featuring', 0.065), ('committing', 0.065), ('concrete', 0.065), ('building', 0.064), ('sad', 0.062), ('lacks', 0.062), ('communication', 0.062), ('loses', 0.061), ('basho', 0.061), ('clients', 0.06), ('stone', 0.06), ('messages', 0.059), ('master', 0.058)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="1498-tfidf-1" href="../high_scalability-2013/high_scalability-2013-08-07-RAFT_-_In_Search_of_an_Understandable_Consensus_Algorithm.html">1498 high scalability-2013-08-07-RAFT - In Search of an Understandable Consensus Algorithm</a></p>
<p>Introduction: If like many humans you've found even  Paxos Made Simple  a bit difficult to understand, you might enjoy RAFT as described in  In Search of an Understandable Consensus Algorithm  by Stanford's  Diego Ongaro  and John Ousterhout. The video presentation of the paper is given by  John Ousterhout . Both the paper and the video are delightfully accessible.
 
 mcherm  has a good summary of the paper:
  
 A consensus algorithm is: a cluster of servers should record a series of records ("log entries") in response to requests from clients of the cluster. (It may also take action based on those entries.) It does so in a way that guarantees that the responses seen by clients of the cluster will be consistent EVEN in the face of servers crashing in unpredictable ways (but not loosing data that was synched to disk), and networks introducing unpredictable delays or communication blockages. 
  
 Here's what Raft does. First, it elects a leader, then the leader records the master version of the log, t</p><p>2 0.13175128 <a title="1498-tfidf-2" href="../high_scalability-2013/high_scalability-2013-05-03-Stuff_The_Internet_Says_On_Scalability_For_May_3%2C_2013.html">1451 high scalability-2013-05-03-Stuff The Internet Says On Scalability For May 3, 2013</a></p>
<p>Introduction: Hey, it's HighScalability time:
    ( Giant Hurricane on Saturn , here's one  in New Orleans )  
 
  
  1,966,080 cores : Time Warp synchronization protocol using up to 7.8M MPI tasks on 1,966,080 cores of the {Sequoia} Blue Gene/Q supercomputer system. 33 trillion events processed in 65 seconds yielding a peak event-rate in excess of 504 billion events/second using 120 racks of Sequoia. 
 Quotable Quotes:                             
 
  Thad Starner : the longer accessing a device exceeds 2s, the more its actually usage would decrease exponentially. Thus, he made a claim that wrist watch interface always sitting on one's wrist ready to use should be more successful than mobile phones which have to pulled out of the pocket.  
  @joedevon : We came for scalability but we stayed for agility #NoSQL 
  @jahmailay : "Our user base is exploding. I really wish we spent more time on scalability instead of features customers don't use." - Everybody, always. 
  @bsletten : I don’t think it is a</p><p>3 0.12300745 <a title="1498-tfidf-3" href="../high_scalability-2009/high_scalability-2009-02-09-Paper%3A_Consensus_Protocols%3A_Two-Phase_Commit__.html">510 high scalability-2009-02-09-Paper: Consensus Protocols: Two-Phase Commit  </a></p>
<p>Introduction: Henry Robinson has created an excellent series of articles on consensus protocols. Henry starts with a very useful discussion of what all this talk about consensus really means:  The consensus problem is the problem of getting a set of nodes in a distributed system to agree on something - it might be a value, a course of action or a decision. Achieving consensus allows a distributed system to act as a single entity, with every individual node aware of and in agreement with the actions of the whole of the network.   In this article Henry tackles Two-Phase Commit, the protocol most databases use to arrive at a consensus for database writes. The article is very well written with lots of pretty and informative pictures. He did a really good job.  In conclusion we learn 2PC is very efficient, a minimal number of messages are exchanged and latency is low. The problem is when a co-ordinator fails availability is dramatically reduced. This is why 2PC isn't generally used on highly distributed</p><p>4 0.10586502 <a title="1498-tfidf-4" href="../high_scalability-2008/high_scalability-2008-07-26-Google%27s_Paxos_Made_Live_%E2%80%93_An_Engineering_Perspective.html">357 high scalability-2008-07-26-Google's Paxos Made Live – An Engineering Perspective</a></p>
<p>Introduction: This is an unusually well written and  useful paper . It talks in detail about experiences implementing a complex project, something we don't see very often. They shockingly even admit that creating a working implementation of Paxos was more difficult than just translating the pseudo code. Imagine that, programmers aren't merely typists! I particularly like the explanation of the Paxos algorithm and why anyone would care about it, working with disk corruption, using leases to support simultaneous reads, using epoch numbers to indicate a new master election, using snapshots to prevent unbounded logs, using MultiOp to implement database transactions, how they tested the system, and their openness with the various problems they had. A lot to learn here.  From the paper:  We describe our experience building a fault-tolerant data-base using the Paxos consensus algorithm. Despite the existing literature in the field, building such a database proved to be non-trivial. We describe selected alg</p><p>5 0.075306743 <a title="1498-tfidf-5" href="../high_scalability-2009/high_scalability-2009-08-24-How_Google_Serves_Data_from_Multiple_Datacenters.html">687 high scalability-2009-08-24-How Google Serves Data from Multiple Datacenters</a></p>
<p>Introduction: Update:   Streamy Explains CAP and HBase's Approach to CAP .  We plan to employ inter-cluster replication, with each cluster located in a single DC.  Remote replication will introduce some eventual consistency into the system, but each cluster will continue to be strongly consistent.   Ryan Barrett, Google App Engine datastore lead, gave this talk   Transactions Across Datacenters (and Other Weekend Projects)   at the Google I/O 2009 conference.   While the talk doesn't necessarily break new technical ground, Ryan does an excellent job explaining and evaluating the different options you have when architecting a system to work across multiple datacenters. This is called  multihoming ,  operating from multiple datacenters simultaneously.  As multihoming is one of the most challenging tasks in all computing, Ryan's clear and thoughtful style comfortably leads you through the various options. On the trip you learn:
   The different  multi-homing options  are: Backups, Master-Slave, Multi-M</p><p>6 0.073078319 <a title="1498-tfidf-6" href="../high_scalability-2008/high_scalability-2008-01-30-How_Rackspace_Now_Uses_MapReduce_and_Hadoop_to_Query_Terabytes_of_Data.html">233 high scalability-2008-01-30-How Rackspace Now Uses MapReduce and Hadoop to Query Terabytes of Data</a></p>
<p>7 0.070289962 <a title="1498-tfidf-7" href="../high_scalability-2009/high_scalability-2009-09-16-Paper%3A_A_practical_scalable_distributed_B-tree.html">705 high scalability-2009-09-16-Paper: A practical scalable distributed B-tree</a></p>
<p>8 0.069910564 <a title="1498-tfidf-8" href="../high_scalability-2013/high_scalability-2013-01-21-Processing_100_Million_Pixels_a_Day_-_Small_Amounts_of_Contention_Cause_Big_Problems_at_Scale.html">1390 high scalability-2013-01-21-Processing 100 Million Pixels a Day - Small Amounts of Contention Cause Big Problems at Scale</a></p>
<p>9 0.068012096 <a title="1498-tfidf-9" href="../high_scalability-2007/high_scalability-2007-08-30-Log_Everything_All_the_Time.html">77 high scalability-2007-08-30-Log Everything All the Time</a></p>
<p>10 0.067775391 <a title="1498-tfidf-10" href="../high_scalability-2012/high_scalability-2012-09-21-Stuff_The_Internet_Says_On_Scalability_For_September_21%2C_2012.html">1327 high scalability-2012-09-21-Stuff The Internet Says On Scalability For September 21, 2012</a></p>
<p>11 0.067503311 <a title="1498-tfidf-11" href="../high_scalability-2009/high_scalability-2009-03-10-Paper%3A_Consensus_Protocols%3A_Paxos___.html">529 high scalability-2009-03-10-Paper: Consensus Protocols: Paxos   </a></p>
<p>12 0.066874146 <a title="1498-tfidf-12" href="../high_scalability-2008/high_scalability-2008-09-03-SMACKDOWN_%3A%3A_Who_are_the_Open_Source_Content_Management_System_%28CMS%29_market_leaders_in_2008%3F.html">377 high scalability-2008-09-03-SMACKDOWN :: Who are the Open Source Content Management System (CMS) market leaders in 2008?</a></p>
<p>13 0.066689432 <a title="1498-tfidf-13" href="../high_scalability-2010/high_scalability-2010-05-14-Hot_Scalability_Links_for_May_14%2C_2010.html">827 high scalability-2010-05-14-Hot Scalability Links for May 14, 2010</a></p>
<p>14 0.066553161 <a title="1498-tfidf-14" href="../high_scalability-2012/high_scalability-2012-11-15-Gone_Fishin%27%3A_Justin.Tv%27s_Live_Video_Broadcasting_Architecture.html">1359 high scalability-2012-11-15-Gone Fishin': Justin.Tv's Live Video Broadcasting Architecture</a></p>
<p>15 0.066465773 <a title="1498-tfidf-15" href="../high_scalability-2010/high_scalability-2010-10-28-Notes_from_A_NOSQL_Evening_in_Palo_Alto_.html">931 high scalability-2010-10-28-Notes from A NOSQL Evening in Palo Alto </a></p>
<p>16 0.066428013 <a title="1498-tfidf-16" href="../high_scalability-2010/high_scalability-2010-03-16-Justin.tv%27s_Live_Video_Broadcasting_Architecture.html">796 high scalability-2010-03-16-Justin.tv's Live Video Broadcasting Architecture</a></p>
<p>17 0.066344239 <a title="1498-tfidf-17" href="../high_scalability-2007/high_scalability-2007-07-26-Product%3A_AWStats_a_Log_Analyzer.html">30 high scalability-2007-07-26-Product: AWStats a Log Analyzer</a></p>
<p>18 0.064836882 <a title="1498-tfidf-18" href="../high_scalability-2011/high_scalability-2011-05-17-Facebook%3A_An_Example_Canonical_Architecture_for_Scaling_Billions_of_Messages.html">1042 high scalability-2011-05-17-Facebook: An Example Canonical Architecture for Scaling Billions of Messages</a></p>
<p>19 0.064534098 <a title="1498-tfidf-19" href="../high_scalability-2008/high_scalability-2008-07-15-ZooKeeper_-_A_Reliable%2C_Scalable_Distributed_Coordination_System_.html">350 high scalability-2008-07-15-ZooKeeper - A Reliable, Scalable Distributed Coordination System </a></p>
<p>20 0.063756481 <a title="1498-tfidf-20" href="../high_scalability-2010/high_scalability-2010-10-26-Scaling_DISQUS_to_75_Million_Comments_and_17%2C000_RPS.html">928 high scalability-2010-10-26-Scaling DISQUS to 75 Million Comments and 17,000 RPS</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.111), (1, 0.07), (2, 0.005), (3, 0.019), (4, 0.015), (5, 0.023), (6, 0.01), (7, 0.023), (8, -0.011), (9, 0.014), (10, -0.004), (11, 0.003), (12, -0.012), (13, -0.032), (14, 0.017), (15, 0.023), (16, 0.048), (17, 0.005), (18, -0.012), (19, -0.027), (20, 0.046), (21, -0.013), (22, -0.05), (23, 0.07), (24, -0.026), (25, 0.01), (26, 0.015), (27, 0.048), (28, -0.005), (29, -0.014), (30, 0.014), (31, -0.037), (32, -0.003), (33, -0.02), (34, -0.011), (35, -0.008), (36, -0.008), (37, -0.037), (38, 0.05), (39, 0.008), (40, 0.005), (41, 0.032), (42, -0.011), (43, 0.007), (44, -0.011), (45, 0.001), (46, -0.012), (47, 0.035), (48, 0.034), (49, -0.016)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94759876 <a title="1498-lsi-1" href="../high_scalability-2013/high_scalability-2013-08-07-RAFT_-_In_Search_of_an_Understandable_Consensus_Algorithm.html">1498 high scalability-2013-08-07-RAFT - In Search of an Understandable Consensus Algorithm</a></p>
<p>Introduction: If like many humans you've found even  Paxos Made Simple  a bit difficult to understand, you might enjoy RAFT as described in  In Search of an Understandable Consensus Algorithm  by Stanford's  Diego Ongaro  and John Ousterhout. The video presentation of the paper is given by  John Ousterhout . Both the paper and the video are delightfully accessible.
 
 mcherm  has a good summary of the paper:
  
 A consensus algorithm is: a cluster of servers should record a series of records ("log entries") in response to requests from clients of the cluster. (It may also take action based on those entries.) It does so in a way that guarantees that the responses seen by clients of the cluster will be consistent EVEN in the face of servers crashing in unpredictable ways (but not loosing data that was synched to disk), and networks introducing unpredictable delays or communication blockages. 
  
 Here's what Raft does. First, it elects a leader, then the leader records the master version of the log, t</p><p>2 0.69889367 <a title="1498-lsi-2" href="../high_scalability-2008/high_scalability-2008-07-26-Google%27s_Paxos_Made_Live_%E2%80%93_An_Engineering_Perspective.html">357 high scalability-2008-07-26-Google's Paxos Made Live – An Engineering Perspective</a></p>
<p>Introduction: This is an unusually well written and  useful paper . It talks in detail about experiences implementing a complex project, something we don't see very often. They shockingly even admit that creating a working implementation of Paxos was more difficult than just translating the pseudo code. Imagine that, programmers aren't merely typists! I particularly like the explanation of the Paxos algorithm and why anyone would care about it, working with disk corruption, using leases to support simultaneous reads, using epoch numbers to indicate a new master election, using snapshots to prevent unbounded logs, using MultiOp to implement database transactions, how they tested the system, and their openness with the various problems they had. A lot to learn here.  From the paper:  We describe our experience building a fault-tolerant data-base using the Paxos consensus algorithm. Despite the existing literature in the field, building such a database proved to be non-trivial. We describe selected alg</p><p>3 0.68908346 <a title="1498-lsi-3" href="../high_scalability-2009/high_scalability-2009-02-09-Paper%3A_Consensus_Protocols%3A_Two-Phase_Commit__.html">510 high scalability-2009-02-09-Paper: Consensus Protocols: Two-Phase Commit  </a></p>
<p>Introduction: Henry Robinson has created an excellent series of articles on consensus protocols. Henry starts with a very useful discussion of what all this talk about consensus really means:  The consensus problem is the problem of getting a set of nodes in a distributed system to agree on something - it might be a value, a course of action or a decision. Achieving consensus allows a distributed system to act as a single entity, with every individual node aware of and in agreement with the actions of the whole of the network.   In this article Henry tackles Two-Phase Commit, the protocol most databases use to arrive at a consensus for database writes. The article is very well written with lots of pretty and informative pictures. He did a really good job.  In conclusion we learn 2PC is very efficient, a minimal number of messages are exchanged and latency is low. The problem is when a co-ordinator fails availability is dramatically reduced. This is why 2PC isn't generally used on highly distributed</p><p>4 0.68533099 <a title="1498-lsi-4" href="../high_scalability-2012/high_scalability-2012-08-03-Stuff_The_Internet_Says_On_Scalability_For_August_3%2C_2012.html">1297 high scalability-2012-08-03-Stuff The Internet Says On Scalability For August 3, 2012</a></p>
<p>Introduction: It's HighScalability Time:
  
 Quotable Quotes:                       
 
  Ross Tur : the tricks you learned to make things big are not the same tricks you can apply to make things infinite.  
  @gclaramunt : Son, I'm getting old, but let me tell you a secret: programming is hard, and high scalability and concurrent programming... frigging hard! 
 
 
  @Carnage4Life : At Apple the iOS team didn't see iPhone hardware or hardware team see OS until it shipped 
  @adrianco :  #ebspio caps iops but latency variance is much lower than EBS 
  @bernardgolden : RT @peakscale: A culture of automation is 10x more important than deployment/test/monkey thing you'd like to discuss < devops calling 
  @JayCollier : 50 years ago, school standardization was needed for scale. Now, scalability and flexibility (variability) can coexist. #FOL2012 
  @adrianco : Compared to vanilla EBS many times better for random reads. Bandwidth limits both for sequential and writes.  #ebspio  
  @SQLPerfTips : More hardw</p><p>5 0.6810717 <a title="1498-lsi-5" href="../high_scalability-2010/high_scalability-2010-06-18-Paper%3A_The_Declarative_Imperative%3A_Experiences_and_Conjectures_in_Distributed_Logic.html">844 high scalability-2010-06-18-Paper: The Declarative Imperative: Experiences and Conjectures in Distributed Logic</a></p>
<p>Introduction: The Declarative Imperative: Experiences and Conjectures in Distributed Logic  is written by UC Berkeley's  Joseph Hellerstein  for a keynote speech he gave at  PODS . The video version of the talk is  here . You may have heard about Mr. Hellerstein through the  Berkeley Orders Of Magnitude  project ( BOOM ), whose purpose is to help  people build systems that are OOM (orders of magnitude) bigger than are building today, with OOM less effort than traditional programming methodologies . A noble goal which may be why BOOM was rated as a top 10 emerging technology for 2010 by  MIT Technology Review . Quite an honor.
 
The motivation for the talk is a familiar one: it's a dark period for computer programming and if we don't learn how to write parallel programs the children of Moore's law will destroy us all. We have more and more processors, yet we are stuck on figuring out how the average programmer can exploit them. The BOOM solution is the Bloom language which is based on  Dedalus:</p><p>6 0.66280061 <a title="1498-lsi-6" href="../high_scalability-2009/high_scalability-2009-02-03-Paper%3A_Optimistic_Replication.html">507 high scalability-2009-02-03-Paper: Optimistic Replication</a></p>
<p>7 0.65976912 <a title="1498-lsi-7" href="../high_scalability-2013/high_scalability-2013-09-06-Stuff_The_Internet_Says_On_Scalability_For_September_6%2C_2013.html">1513 high scalability-2013-09-06-Stuff The Internet Says On Scalability For September 6, 2013</a></p>
<p>8 0.65714651 <a title="1498-lsi-8" href="../high_scalability-2014/high_scalability-2014-03-12-Paper%3A_Scalable_Eventually_Consistent_Counters_over_Unreliable_Networks.html">1611 high scalability-2014-03-12-Paper: Scalable Eventually Consistent Counters over Unreliable Networks</a></p>
<p>9 0.65332079 <a title="1498-lsi-9" href="../high_scalability-2009/high_scalability-2009-09-16-Paper%3A_A_practical_scalable_distributed_B-tree.html">705 high scalability-2009-09-16-Paper: A practical scalable distributed B-tree</a></p>
<p>10 0.65251434 <a title="1498-lsi-10" href="../high_scalability-2013/high_scalability-2013-07-19-Stuff_The_Internet_Says_On_Scalability_For_July_19%2C_2013.html">1494 high scalability-2013-07-19-Stuff The Internet Says On Scalability For July 19, 2013</a></p>
<p>11 0.64915496 <a title="1498-lsi-11" href="../high_scalability-2013/high_scalability-2013-05-31-Stuff_The_Internet_Says_On_Scalability_For_May_31%2C_2013.html">1468 high scalability-2013-05-31-Stuff The Internet Says On Scalability For May 31, 2013</a></p>
<p>12 0.64291906 <a title="1498-lsi-12" href="../high_scalability-2013/high_scalability-2013-10-25-Stuff_The_Internet_Says_On_Scalability_For_October_25th%2C_2013.html">1537 high scalability-2013-10-25-Stuff The Internet Says On Scalability For October 25th, 2013</a></p>
<p>13 0.64146572 <a title="1498-lsi-13" href="../high_scalability-2012/high_scalability-2012-05-10-Paper%3A_Paxos_Made_Moderately_Complex.html">1243 high scalability-2012-05-10-Paper: Paxos Made Moderately Complex</a></p>
<p>14 0.63762635 <a title="1498-lsi-14" href="../high_scalability-2013/high_scalability-2013-04-26-Stuff_The_Internet_Says_On_Scalability_For_April_26%2C_2013.html">1447 high scalability-2013-04-26-Stuff The Internet Says On Scalability For April 26, 2013</a></p>
<p>15 0.63606071 <a title="1498-lsi-15" href="../high_scalability-2008/high_scalability-2008-03-17-Paper%3A_Consistent_Hashing_and_Random_Trees%3A_Distributed_Caching_Protocols_for_Relieving_Hot_Spots_on_the_World_Wide_Web.html">280 high scalability-2008-03-17-Paper: Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web</a></p>
<p>16 0.62604225 <a title="1498-lsi-16" href="../high_scalability-2012/high_scalability-2012-05-04-Stuff_The_Internet_Says_On_Scalability_For_May_4%2C_2012.html">1239 high scalability-2012-05-04-Stuff The Internet Says On Scalability For May 4, 2012</a></p>
<p>17 0.62556577 <a title="1498-lsi-17" href="../high_scalability-2007/high_scalability-2007-08-30-Log_Everything_All_the_Time.html">77 high scalability-2007-08-30-Log Everything All the Time</a></p>
<p>18 0.62512982 <a title="1498-lsi-18" href="../high_scalability-2011/high_scalability-2011-11-14-Using_Gossip_Protocols_for_Failure_Detection%2C_Monitoring%2C_Messaging_and_Other_Good_Things.html">1142 high scalability-2011-11-14-Using Gossip Protocols for Failure Detection, Monitoring, Messaging and Other Good Things</a></p>
<p>19 0.62147617 <a title="1498-lsi-19" href="../high_scalability-2008/high_scalability-2008-12-29-Paper%3A_Spamalytics%3A_An_Empirical_Analysisof_Spam_Marketing_Conversion.html">478 high scalability-2008-12-29-Paper: Spamalytics: An Empirical Analysisof Spam Marketing Conversion</a></p>
<p>20 0.62014204 <a title="1498-lsi-20" href="../high_scalability-2009/high_scalability-2009-03-10-Paper%3A_Consensus_Protocols%3A_Paxos___.html">529 high scalability-2009-03-10-Paper: Consensus Protocols: Paxos   </a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.112), (2, 0.192), (10, 0.038), (30, 0.012), (39, 0.236), (40, 0.036), (51, 0.022), (61, 0.064), (79, 0.076), (85, 0.062), (94, 0.066)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.85196078 <a title="1498-lda-1" href="../high_scalability-2013/high_scalability-2013-08-07-RAFT_-_In_Search_of_an_Understandable_Consensus_Algorithm.html">1498 high scalability-2013-08-07-RAFT - In Search of an Understandable Consensus Algorithm</a></p>
<p>Introduction: If like many humans you've found even  Paxos Made Simple  a bit difficult to understand, you might enjoy RAFT as described in  In Search of an Understandable Consensus Algorithm  by Stanford's  Diego Ongaro  and John Ousterhout. The video presentation of the paper is given by  John Ousterhout . Both the paper and the video are delightfully accessible.
 
 mcherm  has a good summary of the paper:
  
 A consensus algorithm is: a cluster of servers should record a series of records ("log entries") in response to requests from clients of the cluster. (It may also take action based on those entries.) It does so in a way that guarantees that the responses seen by clients of the cluster will be consistent EVEN in the face of servers crashing in unpredictable ways (but not loosing data that was synched to disk), and networks introducing unpredictable delays or communication blockages. 
  
 Here's what Raft does. First, it elects a leader, then the leader records the master version of the log, t</p><p>2 0.84739727 <a title="1498-lda-2" href="../high_scalability-2010/high_scalability-2010-09-16-How_Can_the_Large_Hadron_Collider_Withstand_One_Petabyte_of_Data_a_Second%3F.html">901 high scalability-2010-09-16-How Can the Large Hadron Collider Withstand One Petabyte of Data a Second?</a></p>
<p>Introduction: Why is there something rather than nothing? That's the kind of question the  Large Hadron Collider  in CERN is hopefully poised to answer. And what is the output of this beautiful 17-mile long,  6 billion  dollar  wabi-sabish  proton smashing machine? Data. Great heaping torrents of Grand Canyon sized data. 15 million gigabytes every year. That's 1000 times the information printed in books every year. It's so much data 10,000 scientists will use a  grid  of  80,000+ computers , in 300 computer centers , in 50 different countries just to help make sense of it all.
 
How will all this data be collected, transported, stored, and analyzed? It turns out, using what amounts to sort of Internet of Particles instead of an Internet of Things.
 
Two good articles have recently shed some electro-magnetic energy in the human visible spectrum on the IT aspects of the collider:  LHC computing grid pushes petabytes of data, beats expectations  by John Timmer on Ars Technica and an overview of the  Br</p><p>3 0.82972324 <a title="1498-lda-3" href="../high_scalability-2010/high_scalability-2010-06-16-Hot_Scalability_Links_for_June_16%2C_2010.html">842 high scalability-2010-06-16-Hot Scalability Links for June 16, 2010</a></p>
<p>Introduction: You're Doing it Wrong  by Poul-Henning Kamp. Don't look so guilty, he's not talking about you know what, he's talking about writing high-performance server programs:  Not just wrong as in not perfect, but wrong as in wasting half, or more, of your performance. What good is an  O(log2(n))  algorithm if those operations cause page faults and slow disk operations? For most relevant datasets an  O(n)  or even an  O(n^2)  algorithm, which avoids page faults, will run circles around it.   
  A Microsoft Windows Azure primer: the basics  by Peter Bright. Nice article explaining the basics of Azure and how it compares to Google and Amazon. 
 A call to change the name from  NoSQL to Postmodern Databases . Interesting idea, but the problem is the same one I have for Postmodern Art, when is it? I always feel like I'm in the post-post modern period, yet for art it's really in the early 1900s. Let's save future developers from this existential time crisis. 
  Constructions from Dots and Lines  by M</p><p>4 0.82861882 <a title="1498-lda-4" href="../high_scalability-2009/high_scalability-2009-04-29-How_to_choice_and_build_perfect_server.html">585 high scalability-2009-04-29-How to choice and build perfect server</a></p>
<p>Introduction: There are a lot of questions about the server components, and how to choice and/or build perfect server with consider the power consumption. So I decide to  write about this topic .  Key Points:
  
 What kind of components the servers needs 
 
 The Green Computing and the Servers components. 
 
 How much power the server consume. 
 
 Choice the right components: Processors, HDD, RAID, Memory   
  Build Server, or buy?</p><p>5 0.81556666 <a title="1498-lda-5" href="../high_scalability-2009/high_scalability-2009-04-15-Using_HTTP_cache_headers_effectively.html">571 high scalability-2009-04-15-Using HTTP cache headers effectively</a></p>
<p>Introduction: Hi,     Some time ago , martin fowler bloged about how HTTP cache headers can be very effectively used in web site design.   http://www.martinfowler.com/bliki/SegmentationByFreshness.html     How actively HTTP cache headers are considered in web site design? I think it is a great tool to reduce lot of load on server and should be considered before designing any complex caching strategy. Thoughts?     Thanks,   Unmesh</p><p>6 0.81095719 <a title="1498-lda-6" href="../high_scalability-2011/high_scalability-2011-11-18-Stuff_The_Internet_Says_On_Scalability_For_November_18%2C_2011.html">1145 high scalability-2011-11-18-Stuff The Internet Says On Scalability For November 18, 2011</a></p>
<p>7 0.80825776 <a title="1498-lda-7" href="../high_scalability-2009/high_scalability-2009-07-08-Servers_Component_-_How_to_choice_and_build_perfect_server.html">653 high scalability-2009-07-08-Servers Component - How to choice and build perfect server</a></p>
<p>8 0.77847719 <a title="1498-lda-8" href="../high_scalability-2011/high_scalability-2011-11-29-DataSift_Architecture%3A_Realtime_Datamining_at_120%2C000_Tweets_Per_Second.html">1148 high scalability-2011-11-29-DataSift Architecture: Realtime Datamining at 120,000 Tweets Per Second</a></p>
<p>9 0.77245426 <a title="1498-lda-9" href="../high_scalability-2013/high_scalability-2013-12-11-Using_Node.js_PayPal_Doubles_RPS%2C_Lowers_Latency%2C_with_Fewer_Developers%2C_but_Where_Do_the_Improvements_Really_Come_From%3F.html">1563 high scalability-2013-12-11-Using Node.js PayPal Doubles RPS, Lowers Latency, with Fewer Developers, but Where Do the Improvements Really Come From?</a></p>
<p>10 0.7704795 <a title="1498-lda-10" href="../high_scalability-2009/high_scalability-2009-08-31-Scaling_MySQL_on_Amazon_Web_Services.html">690 high scalability-2009-08-31-Scaling MySQL on Amazon Web Services</a></p>
<p>11 0.76658577 <a title="1498-lda-11" href="../high_scalability-2014/high_scalability-2014-04-28-How_Disqus_Went_Realtime_with_165K_Messages_Per_Second_and_Less_than_.2_Seconds_Latency.html">1638 high scalability-2014-04-28-How Disqus Went Realtime with 165K Messages Per Second and Less than .2 Seconds Latency</a></p>
<p>12 0.75944525 <a title="1498-lda-12" href="../high_scalability-2011/high_scalability-2011-08-22-Strategy%3A_Run_a_Scalable%2C_Available%2C_and_Cheap_Static_Site_on_S3_or_GitHub.html">1102 high scalability-2011-08-22-Strategy: Run a Scalable, Available, and Cheap Static Site on S3 or GitHub</a></p>
<p>13 0.75648671 <a title="1498-lda-13" href="../high_scalability-2012/high_scalability-2012-02-07-Hypertable_Routs_HBase_in_Performance_Test_--_HBase_Overwhelmed_by_Garbage_Collection.html">1189 high scalability-2012-02-07-Hypertable Routs HBase in Performance Test -- HBase Overwhelmed by Garbage Collection</a></p>
<p>14 0.75487822 <a title="1498-lda-14" href="../high_scalability-2008/high_scalability-2008-08-18-Code_deployment_tools.html">369 high scalability-2008-08-18-Code deployment tools</a></p>
<p>15 0.75115436 <a title="1498-lda-15" href="../high_scalability-2014/high_scalability-2014-01-28-How_Next_Big_Sound_Tracks_Over_a_Trillion_Song_Plays%2C_Likes%2C_and_More_Using_a_Version_Control_System_for_Hadoop_Data.html">1586 high scalability-2014-01-28-How Next Big Sound Tracks Over a Trillion Song Plays, Likes, and More Using a Version Control System for Hadoop Data</a></p>
<p>16 0.72758818 <a title="1498-lda-16" href="../high_scalability-2013/high_scalability-2013-02-15-Stuff_The_Internet_Says_On_Scalability_For_February_15%2C_2013.html">1407 high scalability-2013-02-15-Stuff The Internet Says On Scalability For February 15, 2013</a></p>
<p>17 0.7261759 <a title="1498-lda-17" href="../high_scalability-2008/high_scalability-2008-07-26-Google%27s_Paxos_Made_Live_%E2%80%93_An_Engineering_Perspective.html">357 high scalability-2008-07-26-Google's Paxos Made Live – An Engineering Perspective</a></p>
<p>18 0.72480214 <a title="1498-lda-18" href="../high_scalability-2013/high_scalability-2013-01-18-Stuff_The_Internet_Says_On_Scalability_For_January_18%2C_2013.html">1389 high scalability-2013-01-18-Stuff The Internet Says On Scalability For January 18, 2013</a></p>
<p>19 0.72452825 <a title="1498-lda-19" href="../high_scalability-2007/high_scalability-2007-08-22-Wikimedia_architecture.html">72 high scalability-2007-08-22-Wikimedia architecture</a></p>
<p>20 0.72442377 <a title="1498-lda-20" href="../high_scalability-2009/high_scalability-2009-02-18-Numbers_Everyone_Should_Know.html">514 high scalability-2009-02-18-Numbers Everyone Should Know</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
