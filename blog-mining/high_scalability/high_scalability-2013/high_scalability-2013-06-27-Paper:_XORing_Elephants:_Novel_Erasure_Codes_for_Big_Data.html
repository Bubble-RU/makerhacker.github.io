<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1483 high scalability-2013-06-27-Paper: XORing Elephants: Novel Erasure Codes for Big Data</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2013" href="../home/high_scalability-2013_home.html">high_scalability-2013</a> <a title="high_scalability-2013-1483" href="#">high_scalability-2013-1483</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1483 high scalability-2013-06-27-Paper: XORing Elephants: Novel Erasure Codes for Big Data</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2013-1483-html" href="http://highscalability.com//blog/2013/6/27/paper-xoring-elephants-novel-erasure-codes-for-big-data.html">html</a></p><p>Introduction: Erasure codesare one of those seemingly magical mathematical creations that
with the developments described in the paper XORing Elephants: Novel Erasure
Codes for Big Data, are set to replace triple replication as the data storage
protection mechanism of choice.The result says Robin Harris (StorageMojo) in
an excellent article, Facebook's advanced erasure codes: "WebCos will be able
to store massive amounts of data more efficiently than ever before. Bad news:
so will anyone else."Robin says with cheap disks triple replication made sense
and was economical. With ever bigger BigData the overhead has become costly.
But erasure codes have always suffered from unacceptably long time to repair
times. This paper describes new Locally Repairable Codes (LRCs) that are
efficiently repairable in disk I/O and bandwidth requirements:These systems
are now designed to survive the loss of up to four storage elements - disks,
servers, nodes or even entire data centers - without losing any data. What is</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('codes', 0.396), ('rs', 0.385), ('erasure', 0.347), ('repairable', 0.257), ('triple', 0.168), ('robin', 0.148), ('repair', 0.145), ('creations', 0.116), ('xoring', 0.116), ('elephants', 0.109), ('reduced', 0.109), ('thanks', 0.109), ('paper', 0.108), ('replication', 0.102), ('obtained', 0.1), ('disks', 0.1), ('half', 0.099), ('remarkable', 0.097), ('nodes', 0.097), ('overhead', 0.095)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="1483-tfidf-1" href="../high_scalability-2013/high_scalability-2013-06-27-Paper%3A_XORing_Elephants%3A_Novel_Erasure_Codes_for_Big_Data.html">1483 high scalability-2013-06-27-Paper: XORing Elephants: Novel Erasure Codes for Big Data</a></p>
<p>Introduction: Erasure codesare one of those seemingly magical mathematical creations that
with the developments described in the paper XORing Elephants: Novel Erasure
Codes for Big Data, are set to replace triple replication as the data storage
protection mechanism of choice.The result says Robin Harris (StorageMojo) in
an excellent article, Facebook's advanced erasure codes: "WebCos will be able
to store massive amounts of data more efficiently than ever before. Bad news:
so will anyone else."Robin says with cheap disks triple replication made sense
and was economical. With ever bigger BigData the overhead has become costly.
But erasure codes have always suffered from unacceptably long time to repair
times. This paper describes new Locally Repairable Codes (LRCs) that are
efficiently repairable in disk I/O and bandwidth requirements:These systems
are now designed to survive the loss of up to four storage elements - disks,
servers, nodes or even entire data centers - without losing any data. What is</p><p>2 0.41954526 <a title="1483-tfidf-2" href="../high_scalability-2007/high_scalability-2007-10-18-another_approach_to_replication.html">125 high scalability-2007-10-18-another approach to replication</a></p>
<p>Introduction: File replication based on erasure codes can reduce total replicas size 2 times
and more.</p><p>3 0.1156246 <a title="1483-tfidf-3" href="../high_scalability-2012/high_scalability-2012-09-28-Stuff_The_Internet_Says_On_Scalability_For_September_28%2C_2012.html">1330 high scalability-2012-09-28-Stuff The Internet Says On Scalability For September 28, 2012</a></p>
<p>Introduction: It's HighScalability Time:Quotable Quotes:@dbasch: The world is full of
"scalability engineers" who would die from an orgasm if their software ever
saw 10,000 requests in a day.@mtnygard: "Scaling issues are always expressed
as a queue backing up somewhere." --@moonpolysoft #strangeloop@rbranson: If
your data fits in main memory, you're doing it wrong. #strangeloop@peakscale:
Using schemaless DBs an "overreaction" & "confuses the poor impl. of schemas
with the value that schemas provide"@adrianco: GM: Performance analysis is
complicated by your brain thinking LINEARLY about a computer system that is
NONLINEAR. @littleidea: it's better to have infinite scalability and not need
it, than to need infinite scalability and not have itLooks like Google is on
the right track with their language understanding efforts. How hierarchical is
language use: In this paper, we review evidence from the recent literature
supporting the hypothesis that sequential structure may be fundamental to the
compre</p><p>4 0.10504402 <a title="1483-tfidf-4" href="../high_scalability-2012/high_scalability-2012-07-09-Data_Replication_in_NoSQL_Databases.html">1279 high scalability-2012-07-09-Data Replication in NoSQL Databases</a></p>
<p>Introduction: This is the third guest post (part 1,part 2) of a series by Greg Lindahl, CTO
of blekko, the spam free search engine. Previously, Greg was Founder and
Distinguished Engineer at PathScale, at which he was the architect of the
InfiniPath low-latency InfiniBand HCA, used to build tightly-coupled
supercomputing clusters.blekko's home-grown NoSQL database was designed from
the start to support a web-scale search engine, with 1,000s of servers and
petabytes of disk. Data replication is a very important part of keeping the
database up and serving queries. Like many NoSQL database authors, we decided
to keep R=3 copies of each piece of data in the database, and not use RAID to
improve reliability. The key goal we were shooting for was a database which
degrades gracefully when there are many small failures over time, without
needing human intervention.Why don't we like RAID for big NoSQL databases?Most
big storage systems use RAID levels like 3, 4, 5, or 10 to improve
reliability. A conservativ</p><p>5 0.091568291 <a title="1483-tfidf-5" href="../high_scalability-2013/high_scalability-2013-02-15-Stuff_The_Internet_Says_On_Scalability_For_February_15%2C_2013.html">1407 high scalability-2013-02-15-Stuff The Internet Says On Scalability For February 15, 2013</a></p>
<p>Introduction: Hey, it's HighScalability time: The Herokulypse. A cautionary tale of what can
happen when scalability is left for later. Rap Genius created quite a stir
(reddit,Hacker News) when they documentedhigh costs ($20K/month for 15 million
monthly uniques) and poor performance (6 second average response times) using
Heroku'srandom routing mesh. The cause was tracked to queuing at the dyno
level when the expectation was requests are routed to free dynos. Heroku
admitsthis is a problem. So poor load balancing combined with RoR single
threading = poor performance, one that adding more dynos and spending more
money won't necessarily help. While it seems clear Heroku didn't make this
aspect of their system crystal clear, the incident has generated a lot of
teaching moments, if you slog through it all. This is a developing story.You
need money to feed the beast. Fred Wilson has somerevenue ideas for you: Paid
App Downloads - ex. WhatsApp; In-app purchases - ex. Zynga Poker; In-app
subscriptions - e</p><p>6 0.08911287 <a title="1483-tfidf-6" href="../high_scalability-2010/high_scalability-2010-03-02-Using_the_Ambient_Cloud_as_an_Application_Runtime.html">786 high scalability-2010-03-02-Using the Ambient Cloud as an Application Runtime</a></p>
<p>7 0.085454926 <a title="1483-tfidf-7" href="../high_scalability-2012/high_scalability-2012-07-06-Stuff_The_Internet_Says_On_Scalability_For_July_6%2C_2012.html">1278 high scalability-2012-07-06-Stuff The Internet Says On Scalability For July 6, 2012</a></p>
<p>8 0.083516255 <a title="1483-tfidf-8" href="../high_scalability-2012/high_scalability-2012-02-02-The_Data-Scope_Project_-_6PB_storage%2C_500GBytes-sec_sequential_IO%2C_20M_IOPS%2C_130TFlops.html">1186 high scalability-2012-02-02-The Data-Scope Project - 6PB storage, 500GBytes-sec sequential IO, 20M IOPS, 130TFlops</a></p>
<p>9 0.0804113 <a title="1483-tfidf-9" href="../high_scalability-2007/high_scalability-2007-10-04-You_Can_Now_Store_All_Your_Stuff_on_Your_Own_Google_Like_File_System.html">112 high scalability-2007-10-04-You Can Now Store All Your Stuff on Your Own Google Like File System</a></p>
<p>10 0.07946942 <a title="1483-tfidf-10" href="../high_scalability-2009/high_scalability-2009-12-16-Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_in_the_Ambient_Cloud.html">750 high scalability-2009-12-16-Building Super Scalable Systems: Blade Runner Meets Autonomic Computing in the Ambient Cloud</a></p>
<p>11 0.079406552 <a title="1483-tfidf-11" href="../high_scalability-2007/high_scalability-2007-07-16-Paper%3A_Replication_Under_Scalable_Hashing.html">19 high scalability-2007-07-16-Paper: Replication Under Scalable Hashing</a></p>
<p>12 0.079370171 <a title="1483-tfidf-12" href="../high_scalability-2012/high_scalability-2012-11-05-Gone_Fishin%27%3A_Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_In_The_Ambient_Cloud.html">1355 high scalability-2012-11-05-Gone Fishin': Building Super Scalable Systems: Blade Runner Meets Autonomic Computing In The Ambient Cloud</a></p>
<p>13 0.078911744 <a title="1483-tfidf-13" href="../high_scalability-2009/high_scalability-2009-05-05-Drop_ACID_and_Think_About_Data.html">589 high scalability-2009-05-05-Drop ACID and Think About Data</a></p>
<p>14 0.0776738 <a title="1483-tfidf-14" href="../high_scalability-2008/high_scalability-2008-08-17-Wuala_-_P2P_Online_Storage_Cloud.html">368 high scalability-2008-08-17-Wuala - P2P Online Storage Cloud</a></p>
<p>15 0.073561646 <a title="1483-tfidf-15" href="../high_scalability-2008/high_scalability-2008-12-09-Rules_of_Thumb_in_Data_Engineering.html">463 high scalability-2008-12-09-Rules of Thumb in Data Engineering</a></p>
<p>16 0.073168099 <a title="1483-tfidf-16" href="../high_scalability-2012/high_scalability-2012-09-04-Changing_Architectures%3A_New_Datacenter_Networks_Will_Set_Your_Code_and_Data_Free___.html">1316 high scalability-2012-09-04-Changing Architectures: New Datacenter Networks Will Set Your Code and Data Free   </a></p>
<p>17 0.068389244 <a title="1483-tfidf-17" href="../high_scalability-2014/high_scalability-2014-02-17-How_the_AOL.com_Architecture_Evolved_to_99.999%25_Availability%2C_8_Million_Visitors_Per_Day%2C_and_200%2C000_Requests_Per_Second.html">1597 high scalability-2014-02-17-How the AOL.com Architecture Evolved to 99.999% Availability, 8 Million Visitors Per Day, and 200,000 Requests Per Second</a></p>
<p>18 0.066836849 <a title="1483-tfidf-18" href="../high_scalability-2012/high_scalability-2012-09-07-Stuff_The_Internet_Says_On_Scalability_For_September_7%2C_2012.html">1318 high scalability-2012-09-07-Stuff The Internet Says On Scalability For September 7, 2012</a></p>
<p>19 0.066813223 <a title="1483-tfidf-19" href="../high_scalability-2013/high_scalability-2013-06-07-Stuff_The_Internet_Says_On_Scalability_For_June_7%2C_2013.html">1472 high scalability-2013-06-07-Stuff The Internet Says On Scalability For June 7, 2013</a></p>
<p>20 0.066192724 <a title="1483-tfidf-20" href="../high_scalability-2009/high_scalability-2009-06-10-Hive_-_A_Petabyte_Scale_Data_Warehouse_using_Hadoop.html">624 high scalability-2009-06-10-Hive - A Petabyte Scale Data Warehouse using Hadoop</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.117), (1, 0.07), (2, 0.005), (3, 0.032), (4, -0.022), (5, 0.052), (6, 0.019), (7, 0.001), (8, 0.015), (9, 0.035), (10, 0.033), (11, -0.035), (12, 0.008), (13, -0.023), (14, 0.009), (15, 0.07), (16, 0.035), (17, 0.01), (18, -0.048), (19, 0.013), (20, 0.021), (21, 0.084), (22, 0.021), (23, 0.027), (24, -0.037), (25, -0.017), (26, 0.006), (27, -0.022), (28, -0.017), (29, -0.024), (30, -0.047), (31, -0.004), (32, -0.011), (33, 0.033), (34, -0.03), (35, 0.038), (36, 0.065), (37, -0.005), (38, -0.059), (39, -0.026), (40, 0.021), (41, -0.035), (42, 0.006), (43, 0.013), (44, -0.029), (45, 0.043), (46, -0.02), (47, 0.02), (48, -0.005), (49, -0.015)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95978343 <a title="1483-lsi-1" href="../high_scalability-2013/high_scalability-2013-06-27-Paper%3A_XORing_Elephants%3A_Novel_Erasure_Codes_for_Big_Data.html">1483 high scalability-2013-06-27-Paper: XORing Elephants: Novel Erasure Codes for Big Data</a></p>
<p>Introduction: Erasure codesare one of those seemingly magical mathematical creations that
with the developments described in the paper XORing Elephants: Novel Erasure
Codes for Big Data, are set to replace triple replication as the data storage
protection mechanism of choice.The result says Robin Harris (StorageMojo) in
an excellent article, Facebook's advanced erasure codes: "WebCos will be able
to store massive amounts of data more efficiently than ever before. Bad news:
so will anyone else."Robin says with cheap disks triple replication made sense
and was economical. With ever bigger BigData the overhead has become costly.
But erasure codes have always suffered from unacceptably long time to repair
times. This paper describes new Locally Repairable Codes (LRCs) that are
efficiently repairable in disk I/O and bandwidth requirements:These systems
are now designed to survive the loss of up to four storage elements - disks,
servers, nodes or even entire data centers - without losing any data. What is</p><p>2 0.74437624 <a title="1483-lsi-2" href="../high_scalability-2007/high_scalability-2007-10-18-another_approach_to_replication.html">125 high scalability-2007-10-18-another approach to replication</a></p>
<p>Introduction: File replication based on erasure codes can reduce total replicas size 2 times
and more.</p><p>3 0.68002945 <a title="1483-lsi-3" href="../high_scalability-2011/high_scalability-2011-05-05-Paper%3A_A_Study_of_Practical_Deduplication.html">1035 high scalability-2011-05-05-Paper: A Study of Practical Deduplication</a></p>
<p>Introduction: With BigData comes BigStorage costs. One way to store less is simplynot to
store the same data twice. That's the radically simple and powerful notion
behind data deduplication. If you are one of those who got a good laugh out of
the idea of eliminating SQL queries as a rather obvious scalability strategy,
you'll love this one, but it is a powerful feature and one I don't hear talked
about outside the enterprise. A parallel idea in programming is the once-and-
only-once principle of never duplicating code.Using deduplication technology,
for some upfront CPU usage, which is a plentiful resource in many systems that
are IO bound anyway, it's possible to reduce storage requirements by upto
20:1, depending on your data, which saves both money and disk write overhead.
This comes up because of really good article Robin Harris of StorageMojo
wrote, All de-dup works, on a paper,  A Study of Practical Deduplication by
Dutch Meyer and William Bolosky, For a great explanation of deduplication we
t</p><p>4 0.67864883 <a title="1483-lsi-4" href="../high_scalability-2012/high_scalability-2012-09-04-Changing_Architectures%3A_New_Datacenter_Networks_Will_Set_Your_Code_and_Data_Free___.html">1316 high scalability-2012-09-04-Changing Architectures: New Datacenter Networks Will Set Your Code and Data Free   </a></p>
<p>Introduction: One consequence of ITstandardization and commodificationhas been
Google'sdatacenter is the computerview of the world. In that view all compute
resources (memory, CPU, storage) are fungible. They are interchangeable and
location independent, individual computers lose identity and become just a
part of a service.Thwarting that nirvana has been the abysmal performance of
commodity datacenter networks which have caused the preference of
architectures that favor the collocation of state and behaviour on the same
box. MapReduce famouslyships code over to storage nodesfor just this
reason.Change the network and you change the fundamental assumption driving
collocation based software architectures. You are then free to store data
anywhere and move compute anywhere you wish. The datacenter becomes the
computer.On the host side with an x8 slot running atPCI-Express 3.0speeds able
to push 8GB/sec (that's bytes) of bandwidth in both directions, we have enough
IO to feed Moore's progeny, wild packs</p><p>5 0.67313075 <a title="1483-lsi-5" href="../high_scalability-2008/high_scalability-2008-08-17-Wuala_-_P2P_Online_Storage_Cloud.html">368 high scalability-2008-08-17-Wuala - P2P Online Storage Cloud</a></p>
<p>Introduction: How do you design a reliable distributed file system when the expected
availability of the individual nodes are only ~1/5? That is the case for P2P
systems. Dominik Grolimund, the founder of a Swiss startup Caleido will show
you how! They havelaunched Wuala, the social online storage service which
scales as new nodes join the P2P network.The goal ofWua.lais to provide
distributed online storage that is:largescalablereliablesecureby harnessing
the idle resources of participating computers.This challenge is an old dream
of computer science. In fact asAndrew Tanenbaumwrote in 1995:"The design of a
world-wide, fully transparent distributed filesystem fot simultaneous use by
millions of mobile and frequently disconnected users is left as an exercise
for the reader"After three years of research and development at at ETH Zurich,
the Swiss Federal Institute of Technology on a distributed storage system,
Caleido is ready to unveil the result: Wuala. Wuala is a new way of storing,
sharing, and p</p><p>6 0.66969168 <a title="1483-lsi-6" href="../high_scalability-2012/high_scalability-2012-02-02-The_Data-Scope_Project_-_6PB_storage%2C_500GBytes-sec_sequential_IO%2C_20M_IOPS%2C_130TFlops.html">1186 high scalability-2012-02-02-The Data-Scope Project - 6PB storage, 500GBytes-sec sequential IO, 20M IOPS, 130TFlops</a></p>
<p>7 0.64397824 <a title="1483-lsi-7" href="../high_scalability-2013/high_scalability-2013-11-08-Stuff_The_Internet_Says_On_Scalability_For_November_8th%2C_2013.html">1545 high scalability-2013-11-08-Stuff The Internet Says On Scalability For November 8th, 2013</a></p>
<p>8 0.63962746 <a title="1483-lsi-8" href="../high_scalability-2012/high_scalability-2012-03-23-Stuff_The_Internet_Says_On_Scalability_For_March_23%2C_2012.html">1214 high scalability-2012-03-23-Stuff The Internet Says On Scalability For March 23, 2012</a></p>
<p>9 0.62776053 <a title="1483-lsi-9" href="../high_scalability-2007/high_scalability-2007-09-27-Product%3A_Ganglia_Monitoring_System.html">101 high scalability-2007-09-27-Product: Ganglia Monitoring System</a></p>
<p>10 0.62579077 <a title="1483-lsi-10" href="../high_scalability-2013/high_scalability-2013-01-18-Stuff_The_Internet_Says_On_Scalability_For_January_18%2C_2013.html">1389 high scalability-2013-01-18-Stuff The Internet Says On Scalability For January 18, 2013</a></p>
<p>11 0.62533247 <a title="1483-lsi-11" href="../high_scalability-2012/high_scalability-2012-10-11-RAMCube%3A_Exploiting_Network_Proximity_for_RAM-Based_Key-Value_Store.html">1338 high scalability-2012-10-11-RAMCube: Exploiting Network Proximity for RAM-Based Key-Value Store</a></p>
<p>12 0.6194458 <a title="1483-lsi-12" href="../high_scalability-2007/high_scalability-2007-07-31-BerkeleyDB_%26_other_distributed_high_performance_key-value_databases.html">50 high scalability-2007-07-31-BerkeleyDB & other distributed high performance key-value databases</a></p>
<p>13 0.61763418 <a title="1483-lsi-13" href="../high_scalability-2012/high_scalability-2012-08-20-The_Performance_of_Distributed_Data-Structures_Running_on_a_%22Cache-Coherent%22_In-Memory_Data_Grid.html">1307 high scalability-2012-08-20-The Performance of Distributed Data-Structures Running on a "Cache-Coherent" In-Memory Data Grid</a></p>
<p>14 0.6117062 <a title="1483-lsi-14" href="../high_scalability-2012/high_scalability-2012-09-21-Stuff_The_Internet_Says_On_Scalability_For_September_21%2C_2012.html">1327 high scalability-2012-09-21-Stuff The Internet Says On Scalability For September 21, 2012</a></p>
<p>15 0.61116201 <a title="1483-lsi-15" href="../high_scalability-2012/high_scalability-2012-07-09-Data_Replication_in_NoSQL_Databases.html">1279 high scalability-2012-07-09-Data Replication in NoSQL Databases</a></p>
<p>16 0.61016625 <a title="1483-lsi-16" href="../high_scalability-2013/high_scalability-2013-11-15-Stuff_The_Internet_Says_On_Scalability_For_November_15th%2C_2013.html">1549 high scalability-2013-11-15-Stuff The Internet Says On Scalability For November 15th, 2013</a></p>
<p>17 0.60941243 <a title="1483-lsi-17" href="../high_scalability-2009/high_scalability-2009-12-30-Terrastore_-_Scalable%2C_elastic%2C_consistent_document_store..html">756 high scalability-2009-12-30-Terrastore - Scalable, elastic, consistent document store.</a></p>
<p>18 0.60598415 <a title="1483-lsi-18" href="../high_scalability-2013/high_scalability-2013-10-04-Stuff_The_Internet_Says_On_Scalability_For_October_4th%2C_2013.html">1527 high scalability-2013-10-04-Stuff The Internet Says On Scalability For October 4th, 2013</a></p>
<p>19 0.60132492 <a title="1483-lsi-19" href="../high_scalability-2010/high_scalability-2010-03-16-1_Billion_Reasons_Why_Adobe_Chose_HBase_.html">795 high scalability-2010-03-16-1 Billion Reasons Why Adobe Chose HBase </a></p>
<p>20 0.60114127 <a title="1483-lsi-20" href="../high_scalability-2012/high_scalability-2012-09-07-Stuff_The_Internet_Says_On_Scalability_For_September_7%2C_2012.html">1318 high scalability-2012-09-07-Stuff The Internet Says On Scalability For September 7, 2012</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.069), (2, 0.21), (10, 0.032), (27, 0.293), (61, 0.122), (79, 0.109), (94, 0.067)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.93661159 <a title="1483-lda-1" href="../high_scalability-2009/high_scalability-2009-04-04-Performance_Anti-Pattern.html">555 high scalability-2009-04-04-Performance Anti-Pattern</a></p>
<p>Introduction: Want your apps to run faster? Here’s what not to do. By: Bart Smaalders, Sun
Microsystems.Performance Anti-Patterns:- Fixing Performance at the End of the
Project- Measuring and Comparing the Wrong Things- Algorithmic Antipathy-
Reusing Software- Iterating Because That’s What Computers Do Well- Premature
Optimization- Focusing on What You Can See Rather Than on the Problem-
Software Layering- Excessive Numbers of Threads- Asymmetric Hardware
Utilization- Not Optimizing for the Common Case- Needless Swapping of Cache
Lines Between CPUsFor more detailgo there</p><p>same-blog 2 0.88775599 <a title="1483-lda-2" href="../high_scalability-2013/high_scalability-2013-06-27-Paper%3A_XORing_Elephants%3A_Novel_Erasure_Codes_for_Big_Data.html">1483 high scalability-2013-06-27-Paper: XORing Elephants: Novel Erasure Codes for Big Data</a></p>
<p>Introduction: Erasure codesare one of those seemingly magical mathematical creations that
with the developments described in the paper XORing Elephants: Novel Erasure
Codes for Big Data, are set to replace triple replication as the data storage
protection mechanism of choice.The result says Robin Harris (StorageMojo) in
an excellent article, Facebook's advanced erasure codes: "WebCos will be able
to store massive amounts of data more efficiently than ever before. Bad news:
so will anyone else."Robin says with cheap disks triple replication made sense
and was economical. With ever bigger BigData the overhead has become costly.
But erasure codes have always suffered from unacceptably long time to repair
times. This paper describes new Locally Repairable Codes (LRCs) that are
efficiently repairable in disk I/O and bandwidth requirements:These systems
are now designed to survive the loss of up to four storage elements - disks,
servers, nodes or even entire data centers - without losing any data. What is</p><p>3 0.88645911 <a title="1483-lda-3" href="../high_scalability-2007/high_scalability-2007-10-03-Paper%3A_Brewer%27s_Conjecture_and_the_Feasibility_of_Consistent_Available_Partition-Tolerant_Web_Services.html">108 high scalability-2007-10-03-Paper: Brewer's Conjecture and the Feasibility of Consistent Available Partition-Tolerant Web Services</a></p>
<p>Introduction: Abstract: When designing distributed web services, there are three properties
that are commonly desired: consistency, availability, and partition tolerance.
It is impossible to achieve all three. In this note, we prove this conjecture
in the asynchronous network model, and then discuss solutions to this dilemma
in the partially synchronous model.</p><p>4 0.86748463 <a title="1483-lda-4" href="../high_scalability-2007/high_scalability-2007-07-25-Product%3A_NetApp_MetroCluster_Software.html">28 high scalability-2007-07-25-Product: NetApp MetroCluster Software</a></p>
<p>Introduction: NetApp MetroCluster SoftwareCost-effective is an integrated high-availability
storage cluster and site failover capability.NetApp MetroCluster is an
integrated high-availability and disaster recovery solution that can reduce
system complexity and simplify management while ensuring greater return on
investment. MetroCluster uses clustered server technology to replicate data
synchronously between sites located miles apart, eliminating data loss in case
of a disruption. Simple and powerful recovery process minimizes downtime, with
little or no user action required.At one company I worked at they used the
NetApp snap mirror feature to replicate data across long distances to multiple
datacenters. They had a very fast backbone and it worked well. The issue with
NetApp is always one of cost, but if you can afford it, it's a good option.</p><p>5 0.85259557 <a title="1483-lda-5" href="../high_scalability-2011/high_scalability-2011-08-12-Stuff_The_Internet_Says_On_Scalability_For_August_12%2C_2011.html">1097 high scalability-2011-08-12-Stuff The Internet Says On Scalability For August 12, 2011</a></p>
<p>Introduction: Submitted for your scaling pleasure, you may not  scale often, but when you
scale, please drink us:Quotably quotable quotes:@mardix: There is no single
point of truth in #NoSQL . #Consistency is no longer global, it's relative to
the one accessing it. #Scalability@kekline: RT @CurtMonash: "...from industry
figures, Basho/Riak is our third-biggest competitor." How often do you
encounter them? "Never have" #nosql@dave_jacobs: Love being in a city where I
can overhear a convo about Heroku scalability while doing deadlifts.
#ahsanfrancisco@satheeshilu: Doctor at #hospital in india says #ge #healthcare
software is slow to handle 100K X-rays an year.Scalability is critical 4
Indian #software@sufw: How can it be possible that Tagged has 80m users and I
have *never* heard of it!?!@EventCloudPro: One of my vacation realizations?
Whole #bigdata thing has turned into a lotta #bighype - many distinct issues &
nothing to do w/ #bigdataNoSQL as dynamic duos. NoSQL combinations - what
works best? A c</p><p>6 0.79589301 <a title="1483-lda-6" href="../high_scalability-2009/high_scalability-2009-03-18-QCon_London_2009%3A_Upgrading_Twitter_without_service_disruptions.html">544 high scalability-2009-03-18-QCon London 2009: Upgrading Twitter without service disruptions</a></p>
<p>7 0.79269522 <a title="1483-lda-7" href="../high_scalability-2011/high_scalability-2011-11-11-Stuff_The_Internet_Says_On_Scalability_For_November_11%2C_2011.html">1141 high scalability-2011-11-11-Stuff The Internet Says On Scalability For November 11, 2011</a></p>
<p>8 0.77833605 <a title="1483-lda-8" href="../high_scalability-2010/high_scalability-2010-08-20-Hot_Scalability_Links_For_Aug_20%2C_2010.html">883 high scalability-2010-08-20-Hot Scalability Links For Aug 20, 2010</a></p>
<p>9 0.77821386 <a title="1483-lda-9" href="../high_scalability-2009/high_scalability-2009-09-16-Paper%3A_A_practical_scalable_distributed_B-tree.html">705 high scalability-2009-09-16-Paper: A practical scalable distributed B-tree</a></p>
<p>10 0.76545596 <a title="1483-lda-10" href="../high_scalability-2012/high_scalability-2012-04-18-Ansible_-__A_Simple_Model-Driven_Configuration_Management_and_Command_Execution_Framework.html">1230 high scalability-2012-04-18-Ansible -  A Simple Model-Driven Configuration Management and Command Execution Framework</a></p>
<p>11 0.74531883 <a title="1483-lda-11" href="../high_scalability-2010/high_scalability-2010-09-11-Google%27s_Colossus_Makes_Search_Real-time_by_Dumping_MapReduce.html">900 high scalability-2010-09-11-Google's Colossus Makes Search Real-time by Dumping MapReduce</a></p>
<p>12 0.74215865 <a title="1483-lda-12" href="../high_scalability-2009/high_scalability-2009-10-07-How_to_Avoid_the_Top_5_Scale-Out_Pitfalls.html">717 high scalability-2009-10-07-How to Avoid the Top 5 Scale-Out Pitfalls</a></p>
<p>13 0.73808074 <a title="1483-lda-13" href="../high_scalability-2009/high_scalability-2009-07-30-Learn_How_to_Think_at_Scale.html">666 high scalability-2009-07-30-Learn How to Think at Scale</a></p>
<p>14 0.73262113 <a title="1483-lda-14" href="../high_scalability-2010/high_scalability-2010-06-03-Hot_Scalability_Links_for_June_3%2C_2010.html">835 high scalability-2010-06-03-Hot Scalability Links for June 3, 2010</a></p>
<p>15 0.72104537 <a title="1483-lda-15" href="../high_scalability-2014/high_scalability-2014-03-31-How_WhatsApp_Grew_to_Nearly_500_Million_Users%2C_11%2C000_cores%2C_and_70_Million_Messages_a_Second.html">1622 high scalability-2014-03-31-How WhatsApp Grew to Nearly 500 Million Users, 11,000 cores, and 70 Million Messages a Second</a></p>
<p>16 0.7201398 <a title="1483-lda-16" href="../high_scalability-2008/high_scalability-2008-03-03-Two_data_streams_for_a_happy_website.html">265 high scalability-2008-03-03-Two data streams for a happy website</a></p>
<p>17 0.70363742 <a title="1483-lda-17" href="../high_scalability-2012/high_scalability-2012-05-09-Cell_Architectures.html">1242 high scalability-2012-05-09-Cell Architectures</a></p>
<p>18 0.69004321 <a title="1483-lda-18" href="../high_scalability-2009/high_scalability-2009-03-12-QCon_London_2009%3A_Database_projects_to_watch_closely.html">537 high scalability-2009-03-12-QCon London 2009: Database projects to watch closely</a></p>
<p>19 0.68286854 <a title="1483-lda-19" href="../high_scalability-2011/high_scalability-2011-02-04-Stuff_The_Internet_Says_On_Scalability_For_February_4%2C_2011.html">984 high scalability-2011-02-04-Stuff The Internet Says On Scalability For February 4, 2011</a></p>
<p>20 0.68151718 <a title="1483-lda-20" href="../high_scalability-2009/high_scalability-2009-07-21-Paper%3A_Parallelizing_the_Web_Browser.html">660 high scalability-2009-07-21-Paper: Parallelizing the Web Browser</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
