<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1521 high scalability-2013-09-23-Salesforce Architecture - How they Handle 1.3 Billion Transactions a Day</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2013" href="../home/high_scalability-2013_home.html">high_scalability-2013</a> <a title="high_scalability-2013-1521" href="#">high_scalability-2013-1521</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1521 high scalability-2013-09-23-Salesforce Architecture - How they Handle 1.3 Billion Transactions a Day</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2013-1521-html" href="http://highscalability.com//blog/2013/9/23/salesforce-architecture-how-they-handle-13-billion-transacti.html">html</a></p><p>Introduction: This is a guest post written byClaude Johnson, a Lead Site Reliability
Engineer atsalesforce.com.The following is an architectural overview of
salesforce.com's core platform and applications. Other systems such as
Heroku's Dyno architecture or the subsystems of other products such as
work.com and do.com are specifically not covered by this material, although
database.com is. The idea is to share with the technology community some
insight about how salesforce.com does what it does. Any mistakes or omissions
are mine.This is by no means comprehensive but if there is interest, the
author would be happy to tackle other areas of how salesforce.com works.
Salesforce.com is interested in being more open with the technology
communities that we have not previously interacted with. Here's to the start
of "Opening the Kimono" about how we work.Since 1999, salesforce.com has been
singularly focused on building technologies for business that are delivered
over the Internet, displacing traditional e</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Once the traffic enters our network in that datacenter, it is directed to the load balancer pair on which that IP lives. [sent-55, score-0.318]
</p><p>2 Inside the instanceThe load balancer directs the traffic to the application tier of the given instance. [sent-57, score-0.514]
</p><p>3 API traffic makes up over 60% of the traffic serviced by our application tier overall. [sent-59, score-0.456]
</p><p>4 Core appThe core app tier contains anywhere from ten to 40 app servers, depending on the instance. [sent-61, score-0.693]
</p><p>5 We have a content search server and a content batch server for managing asynchronous processes on the content application tier. [sent-67, score-0.465]
</p><p>6 The content batch servers schedule processing of content types, including functions such as rendering previews of certain file types and file type conversion. [sent-68, score-0.57]
</p><p>7 DatabaseThe primary data flow occurs between the core app server tier and the database tier. [sent-69, score-0.643]
</p><p>8 ACS is a cursor cache running on a pair of servers, providing a method to offload cursor processing from the database tier. [sent-84, score-0.511]
</p><p>9 SearchOur search tier runs on commodity Linux hosts, each of which is augmented with a 640 GiB PCI-E flash drive which serves as a caching layer for search requests. [sent-85, score-0.65]
</p><p>10 These hosts get their data from a shared SAN array via an NFS file system. [sent-86, score-0.373]
</p><p>11 Search indexing currently occurs on translation servers which mount LUNs from storage arrays via Fibre Channel SANs. [sent-88, score-0.437]
</p><p>12 The translation occurs when these same LUNs are mounted read-only from a group of four NFS servers running Solaris 10 on SPARC. [sent-92, score-0.365]
</p><p>13 These SAN mounted file systems then are shared via NFS to the search tier previously described. [sent-93, score-0.82]
</p><p>14 FileforceWe maintain a tier of servers that provide object storage, similar in concept to Amazon's S3 or OpenStacks' Swift project. [sent-94, score-0.411]
</p><p>15 All BLOBs in Fileforce have a reference in the database so in order to restore Fileforce data from backups, we have to start a database instance based on a database backup from the same restore point. [sent-99, score-0.653]
</p><p>16 If 100+ objects smaller than 32 KB are stored in the database, a process runs on the app servers to bundle those objects into a single file. [sent-101, score-0.469]
</p><p>17 SupportEach instance contains various other servers for support roles such as debugging application servers and "Hammer testing" app servers in the app tier, hub servers which monitor each instance for health and monitor servers running Nagios. [sent-104, score-1.085]
</p><p>18 Outside of the instance itself reside supporting servers like storage management, database management, log aggregation, production access authentication and other functions. [sent-105, score-0.471]
</p><p>19 The SAN, NFS servers and SPARC based indexer hosts will all go away which will lead to a dramatic reduction in complexity of the entire search tier. [sent-111, score-0.423]
</p><p>20 We're also in the midst of a hardware refresh of our application tier hardware which will increase RAM sizes anywhere from 30% - 266% and introduce Sandy Bridge processors into our stack. [sent-113, score-0.391]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('fileforce', 0.347), ('tier', 0.276), ('luns', 0.157), ('search', 0.153), ('blobs', 0.151), ('cursor', 0.141), ('hosts', 0.135), ('servers', 0.135), ('database', 0.134), ('san', 0.13), ('nfs', 0.126), ('instance', 0.125), ('acs', 0.116), ('qfs', 0.116), ('anywhere', 0.115), ('file', 0.103), ('sandbox', 0.098), ('gib', 0.098), ('kib', 0.098), ('pair', 0.095), ('cursors', 0.094), ('objects', 0.093), ('traffic', 0.09), ('org', 0.088), ('customer', 0.087), ('instances', 0.083), ('content', 0.083), ('occurs', 0.083), ('app', 0.08), ('directs', 0.08), ('storage', 0.077), ('previously', 0.077), ('mounted', 0.076), ('export', 0.072), ('depending', 0.072), ('interconnects', 0.071), ('translation', 0.071), ('via', 0.071), ('core', 0.07), ('settings', 0.068), ('load', 0.068), ('runs', 0.068), ('directed', 0.065), ('login', 0.065), ('shared', 0.064), ('batch', 0.063), ('solr', 0.063), ('restore', 0.063), ('seek', 0.062), ('clustered', 0.059)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000005 <a title="1521-tfidf-1" href="../high_scalability-2013/high_scalability-2013-09-23-Salesforce_Architecture_-_How_they_Handle_1.3_Billion_Transactions_a_Day.html">1521 high scalability-2013-09-23-Salesforce Architecture - How they Handle 1.3 Billion Transactions a Day</a></p>
<p>Introduction: This is a guest post written byClaude Johnson, a Lead Site Reliability
Engineer atsalesforce.com.The following is an architectural overview of
salesforce.com's core platform and applications. Other systems such as
Heroku's Dyno architecture or the subsystems of other products such as
work.com and do.com are specifically not covered by this material, although
database.com is. The idea is to share with the technology community some
insight about how salesforce.com does what it does. Any mistakes or omissions
are mine.This is by no means comprehensive but if there is interest, the
author would be happy to tackle other areas of how salesforce.com works.
Salesforce.com is interested in being more open with the technology
communities that we have not previously interacted with. Here's to the start
of "Opening the Kimono" about how we work.Since 1999, salesforce.com has been
singularly focused on building technologies for business that are delivered
over the Internet, displacing traditional e</p><p>2 0.19613755 <a title="1521-tfidf-2" href="../high_scalability-2013/high_scalability-2013-08-13-In_Memoriam%3A_Lavabit_Architecture_-_Creating_a_Scalable_Email_Service.html">1501 high scalability-2013-08-13-In Memoriam: Lavabit Architecture - Creating a Scalable Email Service</a></p>
<p>Introduction: With Lavabitshutting down undermurky circumstances, it seems fitting torepost
an old(2009), yet still very good post byLadar Levisonon Lavabit's
architecture. I don't know how much of this information is still current, but
it should give you a general idea what Lavabit was all about.Getting to Know
YouWhat is the name of your system and where can we find out more about
it?Note: these links are no longer valid...Lavabithttp://lavabit.comhttp://lav
abit.com/network.htmlhttp://lavabit.com/about.htmlWhat is your system
for?Lavabit is a mid-sized email service provider. We currently have about
140,000 registered users with more than 260,000 email addresses. While most of
our accounts belong to individual users, we also provide corporate email
services to approximately 70 companies.Why did you decide to build this
system?We built the system to compete against the other large free email
providers, with an emphasis on serving the privacy conscious and technically
savvy user. Lavabit was one of</p><p>3 0.16924444 <a title="1521-tfidf-3" href="../high_scalability-2012/high_scalability-2012-10-02-An_Epic_TripAdvisor_Update%3A_Why_Not_Run_on_the_Cloud%3F_The_Grand_Experiment..html">1331 high scalability-2012-10-02-An Epic TripAdvisor Update: Why Not Run on the Cloud? The Grand Experiment.</a></p>
<p>Introduction: This is a guest post byShawn Hsiao,Luke Massa, andVictor Luu. Shawn
runsTripAdvisor's Technical Operations team, Luke and Victor interned on his
team this past summer. This post is introduced byAndy Gelfond, TripAdvisor's
head of engineering.It's been a little over a year since our last post about
theTripAdvisor architecture. It has been an exciting year. Our business and
team continues to grow, we are now an independent public company, and we have
continued to keep/scale our development process and culture as we have grown -
we still run dozens of independent teams, and each team continues to work
across the entire stack. All that has changed are the numbers:56M visitors per
month350M+ pages requests a day120TB+ of warehouse data running on a large
Hadoop cluster, and quickly growingWe also had a very successful college
intern program that brought on over 60 interns this past summer, all who were
quickly on boarded and doing the same kind of work as our full time
engineers.One recurri</p><p>4 0.15695591 <a title="1521-tfidf-4" href="../high_scalability-2014/high_scalability-2014-02-17-How_the_AOL.com_Architecture_Evolved_to_99.999%25_Availability%2C_8_Million_Visitors_Per_Day%2C_and_200%2C000_Requests_Per_Second.html">1597 high scalability-2014-02-17-How the AOL.com Architecture Evolved to 99.999% Availability, 8 Million Visitors Per Day, and 200,000 Requests Per Second</a></p>
<p>Introduction: This is a guest post byDave HaglerSystems Architect at AOL.The AOL homepages
receive more than8 million visitors per day.  That's more daily viewers than
Good Morning America or the Today Show on television.  Over a billion page
views are served each month.  AOL.com has been a major internet destination
since 1996, and still has a strong following of loyal users.The architecture
for AOL.com is in it's 5th generation.  It has essentially been rebuilt from
scratch 5 times over two decades.  The current architecture was designed 6
years ago.  Pieces have been upgraded and new components have been added along
the way, but the overall design remains largely intact.  The code, tools,
development and deployment processes are highly tuned over 6 years of
continual improvement, making the AOL.com architecture battle tested and very
stable.The engineering team is made up of developers, testers, and operations
andtotals around 25 people.  The majority are in Dulles, Virginia with a
smaller team i</p><p>5 0.15524779 <a title="1521-tfidf-5" href="../high_scalability-2009/high_scalability-2009-02-12-MySpace_Architecture.html">511 high scalability-2009-02-12-MySpace Architecture</a></p>
<p>Introduction: Update:Presentation: Behind the Scenes at MySpace.com. Dan Farino, Chief
Systems Architect at MySpace shares details of some of MySpace's cool internal
operations tools.MySpace.com is one of the fastest growing site on the
Internet with 65 million subscribers and 260,000 new users registering each
day. Often criticized for poor performance, MySpace has had to tackle
scalability issues few other sites have faced. How did they do it?Site:
http://myspace.comInformation SourcesPresentation: Behind the Scenes at
MySpace.comInside MySpace.comPlatformASP.NET 2.0WindowsIISSQL ServerWhat's
Inside?300 million users.Pushes 100 gigabits/second to the internet. 10Gb/sec
is HTML content.4,500+ web servers windows 2003/IIS 6.0/APS.NET.1,200+ cache
servers running 64-bit Windows 2003. 16GB of objects cached in RAM.500+
database servers running 64-bit Windows and SQL Server 2005.MySpace processes
1.5 Billion page views per day and handles 2.3 million concurrent users during
the dayMembership Milestones</p><p>6 0.14740832 <a title="1521-tfidf-6" href="../high_scalability-2014/high_scalability-2014-01-07-Sponsored_Post%3A_Netflix%2C_Logentries%2C_Host_Color%2C_Booking%2C_Apple%2C_ScaleOut%2C_MongoDB%2C_BlueStripe%2C_AiScaler%2C_Aerospike%2C_LogicMonitor%2C_AppDynamics%2C_ManageEngine%2C_Site24x7.html">1574 high scalability-2014-01-07-Sponsored Post: Netflix, Logentries, Host Color, Booking, Apple, ScaleOut, MongoDB, BlueStripe, AiScaler, Aerospike, LogicMonitor, AppDynamics, ManageEngine, Site24x7</a></p>
<p>7 0.14621392 <a title="1521-tfidf-7" href="../high_scalability-2012/high_scalability-2012-06-26-Sponsored_Post%3A_New_Relic%2C_Digital_Ocean%2C_NetDNA%2C_Torbit%2C_Reality_Check_Network%2C_Gigaspaces%2C_AiCache%2C_Logic_Monitor%2C_AppDynamics%2C_CloudSigma%2C_ManageEnine%2C_Site24x7.html">1272 high scalability-2012-06-26-Sponsored Post: New Relic, Digital Ocean, NetDNA, Torbit, Reality Check Network, Gigaspaces, AiCache, Logic Monitor, AppDynamics, CloudSigma, ManageEnine, Site24x7</a></p>
<p>8 0.14605494 <a title="1521-tfidf-8" href="../high_scalability-2014/high_scalability-2014-02-04-Sponsored_Post%3A_Logentries%2C_Booking%2C_Apple%2C_MongoDB%2C_BlueStripe%2C_AiScaler%2C_Aerospike%2C_LogicMonitor%2C_AppDynamics%2C_ManageEngine%2C_Site24x7__.html">1590 high scalability-2014-02-04-Sponsored Post: Logentries, Booking, Apple, MongoDB, BlueStripe, AiScaler, Aerospike, LogicMonitor, AppDynamics, ManageEngine, Site24x7  </a></p>
<p>9 0.14593615 <a title="1521-tfidf-9" href="../high_scalability-2014/high_scalability-2014-01-21-Sponsored_Post%3A_Netflix%2C_Logentries%2C_Host_Color%2C_Booking%2C_Apple%2C_MongoDB%2C_BlueStripe%2C_AiScaler%2C_Aerospike%2C_LogicMonitor%2C_AppDynamics%2C_ManageEngine%2C_Site24x7.html">1583 high scalability-2014-01-21-Sponsored Post: Netflix, Logentries, Host Color, Booking, Apple, MongoDB, BlueStripe, AiScaler, Aerospike, LogicMonitor, AppDynamics, ManageEngine, Site24x7</a></p>
<p>10 0.14555791 <a title="1521-tfidf-10" href="../high_scalability-2008/high_scalability-2008-04-29-High_performance_file_server.html">310 high scalability-2008-04-29-High performance file server</a></p>
<p>11 0.14544193 <a title="1521-tfidf-11" href="../high_scalability-2013/high_scalability-2013-12-24-Sponsored_Post%3A_Netflix%2C_Logentries%2C_Host_Color%2C_Booking%2C_Spokeo%2C_Apple%2C_ScaleOut%2C_MongoDB%2C_BlueStripe%2C_AiScaler%2C_Aerospike%2C_New_Relic%2C_LogicMonitor%2C_AppDynamics%2C_ManageEngine%2C_Site24x7.html">1569 high scalability-2013-12-24-Sponsored Post: Netflix, Logentries, Host Color, Booking, Spokeo, Apple, ScaleOut, MongoDB, BlueStripe, AiScaler, Aerospike, New Relic, LogicMonitor, AppDynamics, ManageEngine, Site24x7</a></p>
<p>12 0.14317399 <a title="1521-tfidf-12" href="../high_scalability-2008/high_scalability-2008-01-30-How_Rackspace_Now_Uses_MapReduce_and_Hadoop_to_Query_Terabytes_of_Data.html">233 high scalability-2008-01-30-How Rackspace Now Uses MapReduce and Hadoop to Query Terabytes of Data</a></p>
<p>13 0.14012559 <a title="1521-tfidf-13" href="../high_scalability-2010/high_scalability-2010-05-03-MocoSpace_Architecture_-_3_Billion_Mobile_Page_Views_a_Month.html">821 high scalability-2010-05-03-MocoSpace Architecture - 3 Billion Mobile Page Views a Month</a></p>
<p>14 0.13781595 <a title="1521-tfidf-14" href="../high_scalability-2012/high_scalability-2012-06-05-Sponsored_Post%3A_Digital_Ocean%2C_NetDNA%2C_Torbit%2C_Velocity%2C_Reality_Check_Network%2C_Gigaspaces%2C_AiCache%2C_Logic_Monitor%2C_Attribution_Modeling%2C_AppDynamics%2C_CloudSigma%2C_ManageEnine%2C_Site24x7.html">1257 high scalability-2012-06-05-Sponsored Post: Digital Ocean, NetDNA, Torbit, Velocity, Reality Check Network, Gigaspaces, AiCache, Logic Monitor, Attribution Modeling, AppDynamics, CloudSigma, ManageEnine, Site24x7</a></p>
<p>15 0.13743518 <a title="1521-tfidf-15" href="../high_scalability-2014/high_scalability-2014-02-18-Sponsored_Post%3A_Couchbase%2C_Tokutek%2C_Logentries%2C_Booking%2C_Apple%2C_MongoDB%2C_BlueStripe%2C_AiScaler%2C_Aerospike%2C_LogicMonitor%2C_AppDynamics%2C_ManageEngine%2C_Site24x7__.html">1598 high scalability-2014-02-18-Sponsored Post: Couchbase, Tokutek, Logentries, Booking, Apple, MongoDB, BlueStripe, AiScaler, Aerospike, LogicMonitor, AppDynamics, ManageEngine, Site24x7  </a></p>
<p>16 0.13685483 <a title="1521-tfidf-16" href="../high_scalability-2009/high_scalability-2009-08-16-ThePort__Network__Architecture.html">682 high scalability-2009-08-16-ThePort  Network  Architecture</a></p>
<p>17 0.13317935 <a title="1521-tfidf-17" href="../high_scalability-2013/high_scalability-2013-10-29-Sponsored_Post%3A_Apple%2C_NuoDB%2C_ScaleOut%2C_FreeAgent%2C_CloudStats.me%2C_Intechnica%2C_MongoDB%2C_Stackdriver%2C_BlueStripe%2C_Booking%2C_Rackspace%2C_AiCache%2C_Aerospike%2C_New_Relic%2C_LogicMonitor%2C_AppDynamics%2C_ManageEngine%2C_Site24x7.html">1539 high scalability-2013-10-29-Sponsored Post: Apple, NuoDB, ScaleOut, FreeAgent, CloudStats.me, Intechnica, MongoDB, Stackdriver, BlueStripe, Booking, Rackspace, AiCache, Aerospike, New Relic, LogicMonitor, AppDynamics, ManageEngine, Site24x7</a></p>
<p>18 0.13314447 <a title="1521-tfidf-18" href="../high_scalability-2011/high_scalability-2011-06-27-TripAdvisor_Architecture_-_40M_Visitors%2C_200M_Dynamic_Page_Views%2C_30TB_Data.html">1068 high scalability-2011-06-27-TripAdvisor Architecture - 40M Visitors, 200M Dynamic Page Views, 30TB Data</a></p>
<p>19 0.1329944 <a title="1521-tfidf-19" href="../high_scalability-2009/high_scalability-2009-06-05-HotPads_Shows_the_True_Cost_of_Hosting_on_Amazon.html">619 high scalability-2009-06-05-HotPads Shows the True Cost of Hosting on Amazon</a></p>
<p>20 0.13149422 <a title="1521-tfidf-20" href="../high_scalability-2009/high_scalability-2009-03-16-Are_Cloud_Based_Memory_Architectures_the_Next_Big_Thing%3F.html">538 high scalability-2009-03-16-Are Cloud Based Memory Architectures the Next Big Thing?</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.27), (1, 0.06), (2, -0.021), (3, -0.12), (4, -0.027), (5, -0.01), (6, 0.149), (7, -0.106), (8, 0.001), (9, 0.04), (10, -0.007), (11, -0.018), (12, -0.004), (13, -0.029), (14, -0.013), (15, 0.072), (16, -0.008), (17, 0.043), (18, 0.015), (19, 0.003), (20, 0.018), (21, -0.003), (22, -0.002), (23, -0.022), (24, 0.02), (25, -0.013), (26, -0.056), (27, -0.07), (28, -0.016), (29, 0.023), (30, -0.033), (31, 0.011), (32, -0.02), (33, 0.005), (34, -0.02), (35, -0.007), (36, -0.026), (37, -0.033), (38, 0.05), (39, 0.011), (40, 0.098), (41, -0.068), (42, -0.013), (43, 0.057), (44, -0.075), (45, 0.075), (46, 0.013), (47, 0.039), (48, -0.017), (49, 0.048)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97529846 <a title="1521-lsi-1" href="../high_scalability-2013/high_scalability-2013-09-23-Salesforce_Architecture_-_How_they_Handle_1.3_Billion_Transactions_a_Day.html">1521 high scalability-2013-09-23-Salesforce Architecture - How they Handle 1.3 Billion Transactions a Day</a></p>
<p>Introduction: This is a guest post written byClaude Johnson, a Lead Site Reliability
Engineer atsalesforce.com.The following is an architectural overview of
salesforce.com's core platform and applications. Other systems such as
Heroku's Dyno architecture or the subsystems of other products such as
work.com and do.com are specifically not covered by this material, although
database.com is. The idea is to share with the technology community some
insight about how salesforce.com does what it does. Any mistakes or omissions
are mine.This is by no means comprehensive but if there is interest, the
author would be happy to tackle other areas of how salesforce.com works.
Salesforce.com is interested in being more open with the technology
communities that we have not previously interacted with. Here's to the start
of "Opening the Kimono" about how we work.Since 1999, salesforce.com has been
singularly focused on building technologies for business that are delivered
over the Internet, displacing traditional e</p><p>2 0.771667 <a title="1521-lsi-2" href="../high_scalability-2013/high_scalability-2013-08-13-In_Memoriam%3A_Lavabit_Architecture_-_Creating_a_Scalable_Email_Service.html">1501 high scalability-2013-08-13-In Memoriam: Lavabit Architecture - Creating a Scalable Email Service</a></p>
<p>Introduction: With Lavabitshutting down undermurky circumstances, it seems fitting torepost
an old(2009), yet still very good post byLadar Levisonon Lavabit's
architecture. I don't know how much of this information is still current, but
it should give you a general idea what Lavabit was all about.Getting to Know
YouWhat is the name of your system and where can we find out more about
it?Note: these links are no longer valid...Lavabithttp://lavabit.comhttp://lav
abit.com/network.htmlhttp://lavabit.com/about.htmlWhat is your system
for?Lavabit is a mid-sized email service provider. We currently have about
140,000 registered users with more than 260,000 email addresses. While most of
our accounts belong to individual users, we also provide corporate email
services to approximately 70 companies.Why did you decide to build this
system?We built the system to compete against the other large free email
providers, with an emphasis on serving the privacy conscious and technically
savvy user. Lavabit was one of</p><p>3 0.76900953 <a title="1521-lsi-3" href="../high_scalability-2010/high_scalability-2010-05-03-MocoSpace_Architecture_-_3_Billion_Mobile_Page_Views_a_Month.html">821 high scalability-2010-05-03-MocoSpace Architecture - 3 Billion Mobile Page Views a Month</a></p>
<p>Introduction: This is a guest post by Jamie Hall, Co-founder & CTO ofMocoSpace, describing
the architecture for their mobile social network. This is a timely
architecture to learn from as it combines several hot trends: it is very
large, mobile, and social. What they think is especially cool about their
system is: how it optimizes for device/browser fragmentation on the mobile
Web; their multi-tiered, read/write, local/distributed caching system;
selecting PostgreSQL over MySQL as a relational DB that can scale.MocoSpace is
a mobile social network, with 12 million members and 3 billion page views a
month, which makes it one of the most highly trafficked mobile Websites in the
US. Members access the site mainly from their mobile phone Web browser,
ranging from high end smartphones to lower end devices, as well as the Web.
Activities on the site include customizing profiles, chat, instant messaging,
music, sharing photos & videos, games, eCards and blogs. The monetization
strategy is focused on advert</p><p>4 0.75723249 <a title="1521-lsi-4" href="../high_scalability-2014/high_scalability-2014-02-17-How_the_AOL.com_Architecture_Evolved_to_99.999%25_Availability%2C_8_Million_Visitors_Per_Day%2C_and_200%2C000_Requests_Per_Second.html">1597 high scalability-2014-02-17-How the AOL.com Architecture Evolved to 99.999% Availability, 8 Million Visitors Per Day, and 200,000 Requests Per Second</a></p>
<p>Introduction: This is a guest post byDave HaglerSystems Architect at AOL.The AOL homepages
receive more than8 million visitors per day.  That's more daily viewers than
Good Morning America or the Today Show on television.  Over a billion page
views are served each month.  AOL.com has been a major internet destination
since 1996, and still has a strong following of loyal users.The architecture
for AOL.com is in it's 5th generation.  It has essentially been rebuilt from
scratch 5 times over two decades.  The current architecture was designed 6
years ago.  Pieces have been upgraded and new components have been added along
the way, but the overall design remains largely intact.  The code, tools,
development and deployment processes are highly tuned over 6 years of
continual improvement, making the AOL.com architecture battle tested and very
stable.The engineering team is made up of developers, testers, and operations
andtotals around 25 people.  The majority are in Dulles, Virginia with a
smaller team i</p><p>5 0.75526953 <a title="1521-lsi-5" href="../high_scalability-2009/high_scalability-2009-06-05-HotPads_Shows_the_True_Cost_of_Hosting_on_Amazon.html">619 high scalability-2009-06-05-HotPads Shows the True Cost of Hosting on Amazon</a></p>
<p>Introduction: Mather Corgan, president of HotPads,gave a great talkon how HotPads uses AWS
to run their real estate search engine. I loved the presentation for a few
reasons:It gives real costs on on their servers, how many servers they have,
what they are used for, and exactly how they use S2, EBS, CloudFront and other
AWS services. This is great information for anybody trying to architect a
system and wondering where to run it.HotPads is a "real" application. It's a
small company and at 4.5 million page-views/month it's large but not super
large. It has custom server side components like indexing engines, image
processing, and background database update engines for syncing new real estate
data. And it also stores a lot of images and has low latency requirements.This
a really good example mix of where many companies are or would like to be with
their applications.Their total costs are about $11K/month, which is about what
they were paying at their previous provider. I found this is a little
surpris</p><p>6 0.74958718 <a title="1521-lsi-6" href="../high_scalability-2014/high_scalability-2014-02-10-13_Simple_Tricks_for_Scaling_Python_and_Django_with_Apache_from_HackerEarth.html">1593 high scalability-2014-02-10-13 Simple Tricks for Scaling Python and Django with Apache from HackerEarth</a></p>
<p>7 0.74645501 <a title="1521-lsi-7" href="../high_scalability-2007/high_scalability-2007-12-07-Synchronizing_databases_in_different_geographic_locations.html">176 high scalability-2007-12-07-Synchronizing databases in different geographic locations</a></p>
<p>8 0.74077368 <a title="1521-lsi-8" href="../high_scalability-2011/high_scalability-2011-07-11-ATMCash_Exploits_Virtualization_for_Security_-_Immutability_and_Reversion.html">1077 high scalability-2011-07-11-ATMCash Exploits Virtualization for Security - Immutability and Reversion</a></p>
<p>9 0.73421371 <a title="1521-lsi-9" href="../high_scalability-2013/high_scalability-2013-12-16-22_Recommendations_for_Building_Effective_High_Traffic_Web_Software.html">1565 high scalability-2013-12-16-22 Recommendations for Building Effective High Traffic Web Software</a></p>
<p>10 0.72003829 <a title="1521-lsi-10" href="../high_scalability-2008/high_scalability-2008-09-23-The_7_Stages_of_Scaling_Web_Apps.html">391 high scalability-2008-09-23-The 7 Stages of Scaling Web Apps</a></p>
<p>11 0.7192573 <a title="1521-lsi-11" href="../high_scalability-2012/high_scalability-2012-07-16-Cinchcast_Architecture_-_Producing_1%2C500_Hours_of_Audio_Every_Day.html">1284 high scalability-2012-07-16-Cinchcast Architecture - Producing 1,500 Hours of Audio Every Day</a></p>
<p>12 0.71713799 <a title="1521-lsi-12" href="../high_scalability-2010/high_scalability-2010-08-16-Scaling_an_AWS_infrastructure_-_Tools_and_Patterns.html">881 high scalability-2010-08-16-Scaling an AWS infrastructure - Tools and Patterns</a></p>
<p>13 0.71492469 <a title="1521-lsi-13" href="../high_scalability-2012/high_scalability-2012-08-27-Zoosk_-_The_Engineering_behind_Real_Time_Communications.html">1312 high scalability-2012-08-27-Zoosk - The Engineering behind Real Time Communications</a></p>
<p>14 0.71440649 <a title="1521-lsi-14" href="../high_scalability-2009/high_scalability-2009-07-28-37signals_Architecture.html">663 high scalability-2009-07-28-37signals Architecture</a></p>
<p>15 0.70952225 <a title="1521-lsi-15" href="../high_scalability-2011/high_scalability-2011-08-08-Tagged_Architecture_-_Scaling_to_100_Million_Users%2C_1000_Servers%2C_and_5_Billion_Page_Views.html">1094 high scalability-2011-08-08-Tagged Architecture - Scaling to 100 Million Users, 1000 Servers, and 5 Billion Page Views</a></p>
<p>16 0.7074213 <a title="1521-lsi-16" href="../high_scalability-2007/high_scalability-2007-08-22-Wikimedia_architecture.html">72 high scalability-2007-08-22-Wikimedia architecture</a></p>
<p>17 0.70664805 <a title="1521-lsi-17" href="../high_scalability-2009/high_scalability-2009-08-16-ThePort__Network__Architecture.html">682 high scalability-2009-08-16-ThePort  Network  Architecture</a></p>
<p>18 0.70335495 <a title="1521-lsi-18" href="../high_scalability-2009/high_scalability-2009-04-16-Serving_250M_quotes-day_at_CNBC.com_with_aiCache.html">573 high scalability-2009-04-16-Serving 250M quotes-day at CNBC.com with aiCache</a></p>
<p>19 0.70310539 <a title="1521-lsi-19" href="../high_scalability-2009/high_scalability-2009-02-12-MySpace_Architecture.html">511 high scalability-2009-02-12-MySpace Architecture</a></p>
<p>20 0.70215809 <a title="1521-lsi-20" href="../high_scalability-2008/high_scalability-2008-04-22-Simple_NFS_failover_solution_with_symbolic_link%3F.html">308 high scalability-2008-04-22-Simple NFS failover solution with symbolic link?</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.152), (2, 0.135), (10, 0.311), (30, 0.031), (40, 0.031), (47, 0.017), (49, 0.014), (61, 0.068), (79, 0.064), (85, 0.034), (94, 0.07)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.9659934 <a title="1521-lda-1" href="../high_scalability-2013/high_scalability-2013-06-24-Update_on_How_29_Cloud_Price_Drops_Changed_the_Bottom_Line_of_TripAdvisor_and_Pinterest_-_Results_Mixed.html">1480 high scalability-2013-06-24-Update on How 29 Cloud Price Drops Changed the Bottom Line of TripAdvisor and Pinterest - Results Mixed</a></p>
<p>Introduction: This is a guest post byAli Khajeh-Hosseini, Technical Lead atPlanForCloud. The
original article was publishedon their site. With 29 cloud price reductions I
thought it would be interesting to see how the bottom line would change
compared to an articlewe published last year. The result is surprisingly
little for TripAdvisor because prices for On Demand instances havenot dropped
as fastas for other other instances types.Over the last year and a half,we
counted 29 price reductionsin cloud services provided by AWS, Google Compute
Engine, Windows Azure, and Rackspace Cloud. Price reductions have a direct
effect on cloud users, but given the usual tiny reductions, how significant is
that effect on the bottom line?Last year I wrote aboutcloud cost forecasts for
TripAdvisor and Pinterest.TripAdvisor was experimenting with AWSand attempted
to process 700K HTTP requests per minute on a replica of its live site,
andPinterest was growing massively on AWS. In the wake of the cloud providers'
price</p><p>2 0.96545839 <a title="1521-lda-2" href="../high_scalability-2014/high_scalability-2014-04-21-This_is_why_Microsoft_won._And_why_they_lost..html">1635 high scalability-2014-04-21-This is why Microsoft won. And why they lost.</a></p>
<p>Introduction: My favorite kind of histories are those told from an insider's perspective.
The story of Richard the Lionheart is full of great battles and dynastic
intrigue. The story of one of his soldiers, not so much. Yet the soldiers'
story, as someone who has experienced the real consequences of decisions made
and actions taken, is more revealing.We get such a history in Chat Wars, a
wonderful article written by David Auerbach, who in 1998 worked at Microsoft
on MSN Messenger Service, Microsoft's instant messaging app (for a related
story see The Rise and Fall of AIM, the Breakthrough AOL Never Wanted).It's as
if Herodotus visited Microsoft and wrote down his experiences. It has that
same sort of conversational tone, insightful on-the-ground observations, and
facts no outsider might ever believe.Much of the article is a play-by-play
account of the cat and mouse game David plays changing Messenger to track
AOL's Instant Messenger protocol changes. AOL repeatedly tried to make it so
Messenger coul</p><p>3 0.95867974 <a title="1521-lda-3" href="../high_scalability-2008/high_scalability-2008-10-26-Should_you_use_a_SAN_to_scale_your_architecture%3F_.html">430 high scalability-2008-10-26-Should you use a SAN to scale your architecture? </a></p>
<p>Introduction: This is a question everyone must struggle with when building out their
datacenter. Storage choices are always the ones I have the least confidence
in. David Marks in his blogYou Can Change It Later!asks the questionShould I
get a SAN to scale my site architecture?and answers no. A better solution is
to use commodity hardware, directly attach storage on servers, and partition
across servers to scale and for greater availability.David's reasoning is
interesting:A SAN creates a SPOF (single point of failure) that is dependent
on a vendor to fly and fix when there's a problem. This can lead to long down
times during this outage you have no access to your data at all.Using easily
available commodity hardware minimizes risks to your company, it's not just
about saving money. Zooming over to Fry's to buy emergency equipment provides
the kind of agility startups need in order to respond quickly to ever changing
situations.It's hard to beat the power and flexibility (backups, easy to add
storag</p><p>4 0.93837464 <a title="1521-lda-4" href="../high_scalability-2011/high_scalability-2011-06-22-It%27s_the_Fraking_IOPS_-_1_SSD_is_44%2C000_IOPS%2C_Hard_Drive_is_180.html">1066 high scalability-2011-06-22-It's the Fraking IOPS - 1 SSD is 44,000 IOPS, Hard Drive is 180</a></p>
<p>Introduction: Planning your next buildout and thinking SSDs are still far in the future?
Still too expensive, too low density. Hard disks are cheap, familiar, and
store lots of stuff. In this short and entertaining video Wikia's Artur
Bergmanwants to change your mind about SSDs. SSDs are for today, get with the
math already.Here's Artur's logic:Wikia is all SSD in production. The new
Wikia file servers have a theoretical read rate of ~10GB/sec sequential,
6GB/sec random and 1.2 million IOPs. If you can't do math or love the past,
you love spinning rust. If you are awesome you love SSDs.SSDs are cheaper than
drives using the most relevant metric: $/GB/IOPS. 1 SSD is 44,000 IOPS and one
hard drive is 180 IOPS. Need 1 SSD instead of 50 hard drives.With 8 million
files there's a 9 minute fsck. Full backup in 12 minutes (X-25M based).4
GB/sec random read average latency 1 msec.2.2 GB/sec random write average
latency 1 msec.50TBs of SSDs in one machine for $80,000. With the densities
most products can ski</p><p>5 0.93442404 <a title="1521-lda-5" href="../high_scalability-2010/high_scalability-2010-08-07-ArchCamp%3A_Scalable_Databases_%28NoSQL%29.html">874 high scalability-2010-08-07-ArchCamp: Scalable Databases (NoSQL)</a></p>
<p>Introduction: ArchCamp: Scalable Databasess (NoSQL)The ArchCamp unconference was held this
past Friday at HackerDojo in Mountain View, CA.  There was plenty of pizza,
beer, and great conversation.  This session started out free-form, but shaped
up pretty quickly into a discussion of the popular open source scalable NoSQL
databases and the architectural categories in which they belong.</p><p>6 0.93331015 <a title="1521-lda-6" href="../high_scalability-2010/high_scalability-2010-01-27-Hot_Scalability_Links_for_January_28_2010.html">767 high scalability-2010-01-27-Hot Scalability Links for January 28 2010</a></p>
<p>7 0.93316954 <a title="1521-lda-7" href="../high_scalability-2011/high_scalability-2011-05-23-Evernote_Architecture_-_9_Million_Users_and_150_Million_Requests_a_Day.html">1046 high scalability-2011-05-23-Evernote Architecture - 9 Million Users and 150 Million Requests a Day</a></p>
<p>8 0.92634571 <a title="1521-lda-8" href="../high_scalability-2009/high_scalability-2009-04-27-Some_Questions_from_a_newbie.html">584 high scalability-2009-04-27-Some Questions from a newbie</a></p>
<p>9 0.92575365 <a title="1521-lda-9" href="../high_scalability-2012/high_scalability-2012-08-06-Paper%3A_High-Performance_Concurrency_Control_Mechanisms_for_Main-Memory_Databases.html">1299 high scalability-2012-08-06-Paper: High-Performance Concurrency Control Mechanisms for Main-Memory Databases</a></p>
<p>10 0.92537236 <a title="1521-lda-10" href="../high_scalability-2007/high_scalability-2007-12-02-a8cjdbc_-_update_verision_1.3.html">171 high scalability-2007-12-02-a8cjdbc - update verision 1.3</a></p>
<p>11 0.92536503 <a title="1521-lda-11" href="../high_scalability-2007/high_scalability-2007-12-02-Database-Clustering%3A_a8cjdbc_-_update%3A_version_1.3.html">170 high scalability-2007-12-02-Database-Clustering: a8cjdbc - update: version 1.3</a></p>
<p>12 0.91745025 <a title="1521-lda-12" href="../high_scalability-2010/high_scalability-2010-03-10-How_FarmVille_Scales_-_The_Follow-up.html">792 high scalability-2010-03-10-How FarmVille Scales - The Follow-up</a></p>
<p>13 0.91438752 <a title="1521-lda-13" href="../high_scalability-2014/high_scalability-2014-01-24-Stuff_The_Internet_Says_On_Scalability_For_January_24th%2C_2014.html">1585 high scalability-2014-01-24-Stuff The Internet Says On Scalability For January 24th, 2014</a></p>
<p>same-blog 14 0.88873714 <a title="1521-lda-14" href="../high_scalability-2013/high_scalability-2013-09-23-Salesforce_Architecture_-_How_they_Handle_1.3_Billion_Transactions_a_Day.html">1521 high scalability-2013-09-23-Salesforce Architecture - How they Handle 1.3 Billion Transactions a Day</a></p>
<p>15 0.88615412 <a title="1521-lda-15" href="../high_scalability-2014/high_scalability-2014-04-14-How_do_you_even_do_anything_without_using_EBS%3F.html">1631 high scalability-2014-04-14-How do you even do anything without using EBS?</a></p>
<p>16 0.87731242 <a title="1521-lda-16" href="../high_scalability-2009/high_scalability-2009-08-28-Strategy%3A_Solve_Only_80_Percent_of_the_Problem.html">689 high scalability-2009-08-28-Strategy: Solve Only 80 Percent of the Problem</a></p>
<p>17 0.8727451 <a title="1521-lda-17" href="../high_scalability-2007/high_scalability-2007-12-10-1_Master%2C_N_Slaves.html">178 high scalability-2007-12-10-1 Master, N Slaves</a></p>
<p>18 0.87263268 <a title="1521-lda-18" href="../high_scalability-2007/high_scalability-2007-11-05-Strategy%3A_Diagonal_Scaling_-_Don%27t_Forget_to_Scale_Out_AND_Up.html">142 high scalability-2007-11-05-Strategy: Diagonal Scaling - Don't Forget to Scale Out AND Up</a></p>
<p>19 0.86251968 <a title="1521-lda-19" href="../high_scalability-2012/high_scalability-2012-10-02-An_Epic_TripAdvisor_Update%3A_Why_Not_Run_on_the_Cloud%3F_The_Grand_Experiment..html">1331 high scalability-2012-10-02-An Epic TripAdvisor Update: Why Not Run on the Cloud? The Grand Experiment.</a></p>
<p>20 0.85163063 <a title="1521-lda-20" href="../high_scalability-2008/high_scalability-2008-02-22-Kevin%27s_Great_Adventures_in_SSDland.html">257 high scalability-2008-02-22-Kevin's Great Adventures in SSDland</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
