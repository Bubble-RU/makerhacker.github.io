<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1382 high scalability-2013-01-07-Analyzing billions of credit card transactions and serving low-latency insights in the cloud</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2013" href="../home/high_scalability-2013_home.html">high_scalability-2013</a> <a title="high_scalability-2013-1382" href="#">high_scalability-2013-1382</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1382 high scalability-2013-01-07-Analyzing billions of credit card transactions and serving low-latency insights in the cloud</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2013-1382-html" href="http://highscalability.com//blog/2013/1/7/analyzing-billions-of-credit-card-transactions-and-serving-l.html">html</a></p><p>Introduction: This is a guest post byIvan de PradoandPere Ferrera, founders ofDatasalt, the
company behindPangoolandSplout SQLBig Data open-source projects.The amount of
payments performed using credit cards is huge. It is clear that there is
inherent value in the data that can be derived from analyzing all the
transactions. Client fidelity, demographics, heat maps of activity, shop
recommendations, and many other statistics are useful to both clients and
shops for improving their relationship with the market. AtDatasaltwe have
developed a system in collaboration with theBBVA bankthat is able to analyze
years of data and serve insights and statistics to different low-latency web
and mobile applications.The main challenge we faced besides processing Big
Data input is thatthe output was also Big Data, and even bigger than the
input. And this output needed to be served quickly, under high load.The
solution we developed has an infrastructure cost of just a few thousands of
dollars per month thanks to th</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The amount of payments performed using credit cards is huge. [sent-2, score-0.352]
</p><p>2 Client fidelity, demographics, heat maps of activity, shop recommendations, and many other statistics are useful to both clients and shops for improving their relationship with the market. [sent-4, score-0.653]
</p><p>3 AtDatasaltwe have developed a system in collaboration with theBBVA bankthat is able to analyze years of data and serve insights and statistics to different low-latency web and mobile applications. [sent-5, score-0.476]
</p><p>4 Data, goals and first decisionsThe system uses BBVA's credit card transactions performed in shops all around the world as input source for the analysis. [sent-10, score-0.89]
</p><p>5 We calculate many statistics and data per each shop and per different periods of time. [sent-14, score-0.541]
</p><p>6 The architectureThe architecture has three main parts:Data storage: Used to maintain raw data (credit card transactions) and the resulting Voldemort stores. [sent-28, score-0.434]
</p><p>7 Data serving: A Voldemort cluster that serves the precomputed data from the data processing layer. [sent-30, score-0.358]
</p><p>8 This allows us to keep all historical data - all credit card transactions performed every single day. [sent-32, score-0.683]
</p><p>9 In order to implement them efficiently in Hadoop we created a custom histogram that can be computed in only one pass. [sent-63, score-0.352]
</p><p>10 Moreover, we can compute all simple statistics for each commerce together with the associated histograms in just one MapReduce step, and for an arbitrary number of time periods. [sent-64, score-0.367]
</p><p>11 In order to reduce the amount of storage used by histograms and to improve its visualization, the original computed histograms formed by many bins are transformed into variable-width bins histograms. [sent-65, score-0.776]
</p><p>12 The following diagram shows the 3-bins optimal histogram for a particular histogram:The optimal histogram is computed using arandom-restart hill climbingapproximated algorithm. [sent-66, score-0.668]
</p><p>13 The top co-ocurring shops for a given shop are recommendations for that shop. [sent-71, score-0.634]
</p><p>14 First, the most popular shops are filtered out using a simple frequency cut because almost everybody buys in them. [sent-73, score-0.419]
</p><p>15 Filtering recommendations by location (shops close to each other), by shop category or by both also improves the recommendations. [sent-75, score-0.414]
</p><p>16 Limiting the time where a co-ocurrence can happen results in recommendations of shops where people bought right after buying in the first one. [sent-77, score-0.552]
</p><p>17 Particularly, if one buyer is paying in many shops, the number of co-ocurrences for this credit call will show a quadratic growth, making the analysis not scale linearly. [sent-79, score-0.39]
</p><p>18 The cost & some numbersThe amount of information to serve in Voldemort for one year of BBVA's credit card transactions on Spain is 270 GB. [sent-81, score-0.602]
</p><p>19 The whole infrastructure, including the EC2 instances needed to serve the resulting data would cost approximately $3500/month. [sent-84, score-0.413]
</p><p>20 It would reduce the analysis time and the amount of data to serve as many aggregations could be performed "on the fly". [sent-90, score-0.429]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('voldemort', 0.405), ('shops', 0.28), ('credit', 0.197), ('shop', 0.187), ('histogram', 0.186), ('histograms', 0.168), ('recommendations', 0.167), ('card', 0.166), ('computed', 0.166), ('hadoop', 0.141), ('statistics', 0.133), ('buyer', 0.132), ('data', 0.109), ('flow', 0.109), ('bought', 0.105), ('serve', 0.104), ('bbva', 0.103), ('bins', 0.103), ('flexbility', 0.103), ('pangool', 0.103), ('input', 0.093), ('performed', 0.087), ('processing', 0.084), ('main', 0.08), ('resulting', 0.079), ('insights', 0.077), ('hill', 0.074), ('filtered', 0.073), ('emr', 0.073), ('serving', 0.072), ('stores', 0.069), ('mapreduce', 0.069), ('amount', 0.068), ('transactions', 0.067), ('simple', 0.066), ('needed', 0.065), ('analysis', 0.061), ('category', 0.06), ('derived', 0.06), ('project', 0.059), ('chunk', 0.058), ('us', 0.057), ('periods', 0.057), ('cluster', 0.056), ('diagram', 0.056), ('approximately', 0.056), ('calculate', 0.055), ('implemented', 0.055), ('developed', 0.053), ('clients', 0.053)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000005 <a title="1382-tfidf-1" href="../high_scalability-2013/high_scalability-2013-01-07-Analyzing_billions_of_credit_card_transactions_and_serving_low-latency_insights_in_the_cloud.html">1382 high scalability-2013-01-07-Analyzing billions of credit card transactions and serving low-latency insights in the cloud</a></p>
<p>Introduction: This is a guest post byIvan de PradoandPere Ferrera, founders ofDatasalt, the
company behindPangoolandSplout SQLBig Data open-source projects.The amount of
payments performed using credit cards is huge. It is clear that there is
inherent value in the data that can be derived from analyzing all the
transactions. Client fidelity, demographics, heat maps of activity, shop
recommendations, and many other statistics are useful to both clients and
shops for improving their relationship with the market. AtDatasaltwe have
developed a system in collaboration with theBBVA bankthat is able to analyze
years of data and serve insights and statistics to different low-latency web
and mobile applications.The main challenge we faced besides processing Big
Data input is thatthe output was also Big Data, and even bigger than the
input. And this output needed to be served quickly, under high load.The
solution we developed has an infrastructure cost of just a few thousands of
dollars per month thanks to th</p><p>2 0.35192382 <a title="1382-tfidf-2" href="../high_scalability-2009/high_scalability-2009-06-20-Building_a_data_cycle_at_LinkedIn_with_Hadoop_and_Project_Voldemort.html">634 high scalability-2009-06-20-Building a data cycle at LinkedIn with Hadoop and Project Voldemort</a></p>
<p>Introduction: Update:Building Voldemort read-only stores with Hadoop.Awrite upon
whatLinkedInis doing to integrate large offlineHadoopdata processing jobs with
a fast, distributed online key-value storage system,Project Voldemort.</p><p>3 0.17095543 <a title="1382-tfidf-3" href="../high_scalability-2009/high_scalability-2009-07-28-37signals_Architecture.html">663 high scalability-2009-07-28-37signals Architecture</a></p>
<p>Introduction: Update 7:Basecamp, now with more vroom. Basecamp application servers running
Ruby code were upgraded and virtualization was removed. The result:A 66 %
reduction in the response time while handling multiples of the traffic is
beyond what I expected. They still use virtualization (Linux KVM), just less
of it now.Update 6:Things We've Learned at 37Signals. Themes: less is more;
don't worry be happy.Update 5:Nuts & Bolts: HAproxy. Nice explanation (post,
screencast) by Mark Imbriaco of why HAProxy (load balancing proxy server) is
their favorite (fast, efficient, graceful configuration, queues requests when
Mongrels are busy) for spreading dynamic content between Apache web servers
and Mongrel application servers.Update 4:O'Rielly's Tim O'Brien
interviewsDavid Hansson, Rails creator and 37signals partner. Says BaseCamp
scales horizontally on the application and web tier. Scales up for the
database, using one "big ass" 128GB machine. Says:As technology moves on,
hardware gets cheaper and che</p><p>4 0.1574185 <a title="1382-tfidf-4" href="../high_scalability-2012/high_scalability-2012-08-28-Making_Hadoop_Run_Faster.html">1313 high scalability-2012-08-28-Making Hadoop Run Faster</a></p>
<p>Introduction: Making Hadoop Run FasterOne of the challenges in processing data is that the
speed at which we can input data is quite often much faster than the speed at
which we can process it. This problem becomes even more pronounced in the
context of Big Data, where the volume of data keeps on growing, along with a
corresponding need for more insights, and thus the need for more complex
processing also increases.Batch Processing to the RescueHadoop was designed to
deal with this challenge in the following ways:1. Use a distributed file
system: This enables us to spread the load and grow our system as needed.2.
Optimize for write speed: To enable fast writes the Hadoop architecture was
designed so that writes are first logged, and then processed. This enables
fairly fast write speeds.3. Use batch processing (Map/Reduce) to balance the
speed for the data feeds with the processing speed.Batch Processing
ChallengesThe challenge with batch-processing is that it assumes that the
feeds come in bursts. I</p><p>5 0.15214375 <a title="1382-tfidf-5" href="../high_scalability-2009/high_scalability-2009-07-02-Product%3A_Project_Voldemort_-_A_Distributed_Database.html">651 high scalability-2009-07-02-Product: Project Voldemort - A Distributed Database</a></p>
<p>Introduction: Update:Presentation from theNoSQL conference:slides,video 1,video 2.Project
Voldemort is an open source implementation of the basic parts ofDynamo
(Amazon's Highly Available Key-value Store)distributed key-value storage
system. LinkedIn is using it in their production environment for "certain
high-scalability storage problems where simple functional partitioning is not
sufficient."From their website:Data is automatically replicated over multiple
servers.Data is automatically partitioned so each server contains only a
subset of the total dataServer failure is handled transparentlyPluggable
serialization is supported to allow rich keys and values including lists and
tuples with named fields, as well as to integrate with common serialization
frameworks like Protocol Buffers, Thrift, and Java SerializationData items are
versioned to maximize data integrity in failure scenarios without compromising
availability of the systemEach node is independent of other nodes with no
central point of fa</p><p>6 0.13768111 <a title="1382-tfidf-6" href="../high_scalability-2009/high_scalability-2009-08-08-1dbase_vs._many_and_cloud_hosting_vs._dedicated_server%28s%29%3F.html">675 high scalability-2009-08-08-1dbase vs. many and cloud hosting vs. dedicated server(s)?</a></p>
<p>7 0.1363001 <a title="1382-tfidf-7" href="../high_scalability-2009/high_scalability-2009-05-17-Product%3A_Hadoop.html">601 high scalability-2009-05-17-Product: Hadoop</a></p>
<p>8 0.12919679 <a title="1382-tfidf-8" href="../high_scalability-2008/high_scalability-2008-11-22-Google_Architecture.html">448 high scalability-2008-11-22-Google Architecture</a></p>
<p>9 0.12664099 <a title="1382-tfidf-9" href="../high_scalability-2011/high_scalability-2011-03-08-Medialets_Architecture_-__Defeating_the_Daunting_Mobile_Device_Data_Deluge.html">1000 high scalability-2011-03-08-Medialets Architecture -  Defeating the Daunting Mobile Device Data Deluge</a></p>
<p>10 0.12462206 <a title="1382-tfidf-10" href="../high_scalability-2012/high_scalability-2012-01-24-The_State_of_NoSQL_in_2012.html">1180 high scalability-2012-01-24-The State of NoSQL in 2012</a></p>
<p>11 0.11976704 <a title="1382-tfidf-11" href="../high_scalability-2010/high_scalability-2010-12-06-What_the_heck_are_you_actually_using_NoSQL_for%3F.html">954 high scalability-2010-12-06-What the heck are you actually using NoSQL for?</a></p>
<p>12 0.11846381 <a title="1382-tfidf-12" href="../high_scalability-2010/high_scalability-2010-07-20-Strategy%3A_Consider_When_a_Service_Starts_Billing_in_Your_Algorithm_Cost.html">862 high scalability-2010-07-20-Strategy: Consider When a Service Starts Billing in Your Algorithm Cost</a></p>
<p>13 0.11548519 <a title="1382-tfidf-13" href="../high_scalability-2010/high_scalability-2010-08-18-Misco%3A_A_MapReduce_Framework_for_Mobile_Systems_-_Start_of_the_Ambient_Cloud%3F.html">882 high scalability-2010-08-18-Misco: A MapReduce Framework for Mobile Systems - Start of the Ambient Cloud?</a></p>
<p>14 0.11440556 <a title="1382-tfidf-14" href="../high_scalability-2007/high_scalability-2007-09-18-Amazon_Architecture.html">96 high scalability-2007-09-18-Amazon Architecture</a></p>
<p>15 0.110011 <a title="1382-tfidf-15" href="../high_scalability-2012/high_scalability-2012-05-07-Startups_are_Creating_a_New_System_of_the_World_for_IT.html">1240 high scalability-2012-05-07-Startups are Creating a New System of the World for IT</a></p>
<p>16 0.10663871 <a title="1382-tfidf-16" href="../high_scalability-2013/high_scalability-2013-08-13-In_Memoriam%3A_Lavabit_Architecture_-_Creating_a_Scalable_Email_Service.html">1501 high scalability-2013-08-13-In Memoriam: Lavabit Architecture - Creating a Scalable Email Service</a></p>
<p>17 0.10469739 <a title="1382-tfidf-17" href="../high_scalability-2009/high_scalability-2009-05-05-Drop_ACID_and_Think_About_Data.html">589 high scalability-2009-05-05-Drop ACID and Think About Data</a></p>
<p>18 0.10333045 <a title="1382-tfidf-18" href="../high_scalability-2014/high_scalability-2014-03-24-Big%2C_Small%2C_Hot_or_Cold_-_Examples_of_Robust_Data_Pipelines_from_Stripe%2C_Tapad%2C_Etsy_and_Square.html">1618 high scalability-2014-03-24-Big, Small, Hot or Cold - Examples of Robust Data Pipelines from Stripe, Tapad, Etsy and Square</a></p>
<p>19 0.10295285 <a title="1382-tfidf-19" href="../high_scalability-2012/high_scalability-2012-11-05-Gone_Fishin%27%3A_Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_In_The_Ambient_Cloud.html">1355 high scalability-2012-11-05-Gone Fishin': Building Super Scalable Systems: Blade Runner Meets Autonomic Computing In The Ambient Cloud</a></p>
<p>20 0.10281229 <a title="1382-tfidf-20" href="../high_scalability-2009/high_scalability-2009-12-16-Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_in_the_Ambient_Cloud.html">750 high scalability-2009-12-16-Building Super Scalable Systems: Blade Runner Meets Autonomic Computing in the Ambient Cloud</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.202), (1, 0.097), (2, 0.006), (3, 0.022), (4, 0.001), (5, 0.053), (6, 0.063), (7, -0.015), (8, 0.047), (9, 0.062), (10, 0.048), (11, -0.012), (12, 0.062), (13, -0.081), (14, 0.074), (15, 0.003), (16, -0.015), (17, -0.013), (18, -0.019), (19, 0.001), (20, 0.001), (21, 0.016), (22, 0.066), (23, 0.05), (24, 0.04), (25, -0.033), (26, -0.018), (27, -0.006), (28, -0.028), (29, 0.044), (30, 0.032), (31, 0.08), (32, -0.028), (33, -0.005), (34, -0.025), (35, 0.015), (36, -0.019), (37, -0.023), (38, -0.012), (39, -0.026), (40, 0.034), (41, 0.05), (42, -0.029), (43, -0.008), (44, 0.05), (45, -0.0), (46, -0.06), (47, -0.048), (48, -0.016), (49, -0.047)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95710677 <a title="1382-lsi-1" href="../high_scalability-2013/high_scalability-2013-01-07-Analyzing_billions_of_credit_card_transactions_and_serving_low-latency_insights_in_the_cloud.html">1382 high scalability-2013-01-07-Analyzing billions of credit card transactions and serving low-latency insights in the cloud</a></p>
<p>Introduction: This is a guest post byIvan de PradoandPere Ferrera, founders ofDatasalt, the
company behindPangoolandSplout SQLBig Data open-source projects.The amount of
payments performed using credit cards is huge. It is clear that there is
inherent value in the data that can be derived from analyzing all the
transactions. Client fidelity, demographics, heat maps of activity, shop
recommendations, and many other statistics are useful to both clients and
shops for improving their relationship with the market. AtDatasaltwe have
developed a system in collaboration with theBBVA bankthat is able to analyze
years of data and serve insights and statistics to different low-latency web
and mobile applications.The main challenge we faced besides processing Big
Data input is thatthe output was also Big Data, and even bigger than the
input. And this output needed to be served quickly, under high load.The
solution we developed has an infrastructure cost of just a few thousands of
dollars per month thanks to th</p><p>2 0.88698256 <a title="1382-lsi-2" href="../high_scalability-2012/high_scalability-2012-08-28-Making_Hadoop_Run_Faster.html">1313 high scalability-2012-08-28-Making Hadoop Run Faster</a></p>
<p>Introduction: Making Hadoop Run FasterOne of the challenges in processing data is that the
speed at which we can input data is quite often much faster than the speed at
which we can process it. This problem becomes even more pronounced in the
context of Big Data, where the volume of data keeps on growing, along with a
corresponding need for more insights, and thus the need for more complex
processing also increases.Batch Processing to the RescueHadoop was designed to
deal with this challenge in the following ways:1. Use a distributed file
system: This enables us to spread the load and grow our system as needed.2.
Optimize for write speed: To enable fast writes the Hadoop architecture was
designed so that writes are first logged, and then processed. This enables
fairly fast write speeds.3. Use batch processing (Map/Reduce) to balance the
speed for the data feeds with the processing speed.Batch Processing
ChallengesThe challenge with batch-processing is that it assumes that the
feeds come in bursts. I</p><p>3 0.85847402 <a title="1382-lsi-3" href="../high_scalability-2012/high_scalability-2012-01-12-Peregrine_-_A_Map_Reduce_Framework_for_Iterative_and_Pipelined_Jobs.html">1173 high scalability-2012-01-12-Peregrine - A Map Reduce Framework for Iterative and Pipelined Jobs</a></p>
<p>Introduction: The Peregrine falcon is a bird of prey, famous for itshigh speed
divingattacks, feeding primarily on much slower Hadoops. Wait, sorry, it is
Kevin Burton of Spinn3r's newPeregrine project--anew FAST modern map reduce
framework optimized for iterative and pipelined map reduce jobs--that feeds on
Hadoops.If you don't know Kevin, he does a lot of excellent technical work
that he's kind enough to share it onhis blog. Only he hasn't been blogging
much lately, he's been heads down working on Peregrine. Now that Peregrine has
been released, here's a short email interview with Kevin on why you might want
to take upfalconry, the ancient sport of MapReduce.What does Spinn3r do that
Peregrine is important to you?Ideally it was designed to execute pagerank but
many iterative applications that we deploy and WANT to deploy (k-means) would
be horribly inefficient under Hadoop as it doesn't have any support for
merging and joining IO between tasks.  It also doesn't support pipeline jobs
from one MR ta</p><p>4 0.83615851 <a title="1382-lsi-4" href="../high_scalability-2009/high_scalability-2009-05-17-Product%3A_Hadoop.html">601 high scalability-2009-05-17-Product: Hadoop</a></p>
<p>Introduction: Update 5:Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62
Secondsand has itsgreen cred questionedbecause it took 40 times the number of
machines Greenplum used to do the same work.Update 4:Introduction to Pig. Pig
allows you to skip programming Hadoop at the low map-reduce level. You don't
have to know Java. Using the Pig Latin language, which is a scripting data
flow language, you can think about your problem as a data flow program. 10
lines of Pig Latin = 200 lines of Java.Update 3: Scaling Hadoop to4000 nodes
at Yahoo!. 30,000 cores with nearly 16PB of raw disk; sorted 6TB of data
completed in 37 minutes; 14,000 map tasks writes (reads) 360 MB (about 3
blocks) of data into a single file with a total of 5.04 TB for the whole
job.Update 2: HadoopSummit and Data-Intensive Computing Symposium Videos and
Slides. Topics include: Pig, JAQL, Hbase, Hive, Data-Intensive Scalable
Computing, Clouds and ManyCore: The Revolution, Simplicity and Complexity in
Data Systems at Scale, Han</p><p>5 0.82381231 <a title="1382-lsi-5" href="../high_scalability-2009/high_scalability-2009-08-03-Building_a_Data_Intensive_Web_Application_with_Cloudera%2C_Hadoop%2C_Hive%2C_Pig%2C_and_EC2.html">669 high scalability-2009-08-03-Building a Data Intensive Web Application with Cloudera, Hadoop, Hive, Pig, and EC2</a></p>
<p>Introduction: This tutorial will show you how to use Amazon EC2 and Cloudera's Distribution
for Hadoop to run batch jobs for a data intensive web application.During the
tutorial, we will perform the following data processing steps....read more on
Cloudera website</p><p>6 0.82341123 <a title="1382-lsi-6" href="../high_scalability-2014/high_scalability-2014-03-24-Big%2C_Small%2C_Hot_or_Cold_-_Examples_of_Robust_Data_Pipelines_from_Stripe%2C_Tapad%2C_Etsy_and_Square.html">1618 high scalability-2014-03-24-Big, Small, Hot or Cold - Examples of Robust Data Pipelines from Stripe, Tapad, Etsy and Square</a></p>
<p>7 0.81938881 <a title="1382-lsi-7" href="../high_scalability-2014/high_scalability-2014-01-28-How_Next_Big_Sound_Tracks_Over_a_Trillion_Song_Plays%2C_Likes%2C_and_More_Using_a_Version_Control_System_for_Hadoop_Data.html">1586 high scalability-2014-01-28-How Next Big Sound Tracks Over a Trillion Song Plays, Likes, and More Using a Version Control System for Hadoop Data</a></p>
<p>8 0.80853212 <a title="1382-lsi-8" href="../high_scalability-2011/high_scalability-2011-03-08-Medialets_Architecture_-__Defeating_the_Daunting_Mobile_Device_Data_Deluge.html">1000 high scalability-2011-03-08-Medialets Architecture -  Defeating the Daunting Mobile Device Data Deluge</a></p>
<p>9 0.79288161 <a title="1382-lsi-9" href="../high_scalability-2009/high_scalability-2009-07-30-Learn_How_to_Think_at_Scale.html">666 high scalability-2009-07-30-Learn How to Think at Scale</a></p>
<p>10 0.78047836 <a title="1382-lsi-10" href="../high_scalability-2010/high_scalability-2010-12-08-How_To_Get_Experience_Working_With_Large_Datasets.html">956 high scalability-2010-12-08-How To Get Experience Working With Large Datasets</a></p>
<p>11 0.77112561 <a title="1382-lsi-11" href="../high_scalability-2013/high_scalability-2013-04-24-Strategy%3A_Using_Lots_of_RAM_Often_Cheaper_than_Using_a_Hadoop_Cluster.html">1445 high scalability-2013-04-24-Strategy: Using Lots of RAM Often Cheaper than Using a Hadoop Cluster</a></p>
<p>12 0.76786029 <a title="1382-lsi-12" href="../high_scalability-2012/high_scalability-2012-06-15-Stuff_The_Internet_Says_On_Scalability_For_June_15%2C_2012.html">1265 high scalability-2012-06-15-Stuff The Internet Says On Scalability For June 15, 2012</a></p>
<p>13 0.76628798 <a title="1382-lsi-13" href="../high_scalability-2008/high_scalability-2008-11-22-Google_Architecture.html">448 high scalability-2008-11-22-Google Architecture</a></p>
<p>14 0.74199665 <a title="1382-lsi-14" href="../high_scalability-2014/high_scalability-2014-01-14-Ask_HS%3A_Design_and_Implementation_of_scalable_services%3F.html">1578 high scalability-2014-01-14-Ask HS: Design and Implementation of scalable services?</a></p>
<p>15 0.7385878 <a title="1382-lsi-15" href="../high_scalability-2011/high_scalability-2011-12-22-Architecting_Massively-Scalable_Near-Real-Time_Risk_Analysis_Solutions.html">1161 high scalability-2011-12-22-Architecting Massively-Scalable Near-Real-Time Risk Analysis Solutions</a></p>
<p>16 0.73725533 <a title="1382-lsi-16" href="../high_scalability-2012/high_scalability-2012-11-26-BigData_using_Erlang%2C_C_and_Lisp_to_Fight_the_Tsunami_of_Mobile_Data.html">1362 high scalability-2012-11-26-BigData using Erlang, C and Lisp to Fight the Tsunami of Mobile Data</a></p>
<p>17 0.73608613 <a title="1382-lsi-17" href="../high_scalability-2011/high_scalability-2011-04-12-Caching_and_Processing_2TB_Mozilla_Crash_Reports_in_memory_with_Hazelcast.html">1020 high scalability-2011-04-12-Caching and Processing 2TB Mozilla Crash Reports in memory with Hazelcast</a></p>
<p>18 0.73583454 <a title="1382-lsi-18" href="../high_scalability-2010/high_scalability-2010-08-18-Misco%3A_A_MapReduce_Framework_for_Mobile_Systems_-_Start_of_the_Ambient_Cloud%3F.html">882 high scalability-2010-08-18-Misco: A MapReduce Framework for Mobile Systems - Start of the Ambient Cloud?</a></p>
<p>19 0.73244369 <a title="1382-lsi-19" href="../high_scalability-2012/high_scalability-2012-07-20-Stuff_The_Internet_Says_On_Scalability_For_July_20%2C_2012.html">1287 high scalability-2012-07-20-Stuff The Internet Says On Scalability For July 20, 2012</a></p>
<p>20 0.72788155 <a title="1382-lsi-20" href="../high_scalability-2012/high_scalability-2012-07-27-Stuff_The_Internet_Says_On_Scalability_For_July_27%2C_2012.html">1292 high scalability-2012-07-27-Stuff The Internet Says On Scalability For July 27, 2012</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.097), (2, 0.146), (10, 0.061), (30, 0.025), (40, 0.021), (43, 0.013), (49, 0.015), (52, 0.144), (61, 0.107), (77, 0.021), (79, 0.118), (85, 0.069), (94, 0.071)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9100346 <a title="1382-lda-1" href="../high_scalability-2013/high_scalability-2013-01-07-Analyzing_billions_of_credit_card_transactions_and_serving_low-latency_insights_in_the_cloud.html">1382 high scalability-2013-01-07-Analyzing billions of credit card transactions and serving low-latency insights in the cloud</a></p>
<p>Introduction: This is a guest post byIvan de PradoandPere Ferrera, founders ofDatasalt, the
company behindPangoolandSplout SQLBig Data open-source projects.The amount of
payments performed using credit cards is huge. It is clear that there is
inherent value in the data that can be derived from analyzing all the
transactions. Client fidelity, demographics, heat maps of activity, shop
recommendations, and many other statistics are useful to both clients and
shops for improving their relationship with the market. AtDatasaltwe have
developed a system in collaboration with theBBVA bankthat is able to analyze
years of data and serve insights and statistics to different low-latency web
and mobile applications.The main challenge we faced besides processing Big
Data input is thatthe output was also Big Data, and even bigger than the
input. And this output needed to be served quickly, under high load.The
solution we developed has an infrastructure cost of just a few thousands of
dollars per month thanks to th</p><p>2 0.90218008 <a title="1382-lda-2" href="../high_scalability-2010/high_scalability-2010-08-05-Pairing_NoSQL_and_Relational_Data_Storage%3A_MySQL_with_MongoDB.html">872 high scalability-2010-08-05-Pairing NoSQL and Relational Data Storage: MySQL with MongoDB</a></p>
<p>Introduction: I've largely steered clear of publicly commenting on the "NoSQL vs.
Relational" conflict. Keeping in mind that this argument is more about
currently available solutions and the features their developers have chosen to
build in, I'd like to dig into this and provide a decidedly neutral viewpoint.
In fact, by erring on the side of caution, I've inadvertently given myself
plenty of time to consider the pros and cons of both data storage approaches,
and although my mind was initially swaying toward the NoSQL camp, I can say
with a fair amount of certainty, that I've found a good compromise. You can
read the full storehere.</p><p>3 0.8876363 <a title="1382-lda-3" href="../high_scalability-2007/high_scalability-2007-07-30-Product%3A_Yslow_to_speed_up_your_web_pages.html">47 high scalability-2007-07-30-Product: Yslow to speed up your web pages</a></p>
<p>Introduction: Update:Speed up Apache - how I went from F to A in YSlow.Good example of using
YSlow to speed up a website with solid code examples.Every layer in the multi-
layer cake that is your website contributes to how long a page takes to
display.YSlow, from Yahoo, is a cool tool for discovering how the ingredients
of your site's top layer contribute to performance.YSlow analyzes web pages
and tells you why they're slow based on the rules for high performance web
sites. YSlow is a Firefox add-on integrated with the popular Firebug web
development tool.YSlow gives you:Performance report cardHTTP/HTML summaryList
of components in the pageTools including JSLint</p><p>4 0.88691151 <a title="1382-lda-4" href="../high_scalability-2008/high_scalability-2008-02-11-Yahoo_Live%27s_Scaling_Problems_Prove%3A_Release_Early_and_Often_-_Just_Don%27t_Screw_Up.html">244 high scalability-2008-02-11-Yahoo Live's Scaling Problems Prove: Release Early and Often - Just Don't Screw Up</a></p>
<p>Introduction: Tech Crunch chomped down on some initial scaling problems with Yahoo's new
live video streaming serviceYahoo Live. After a bit of chewing on Yahoo's old
bones, TC spat out:If Yahoo cant scale something like this (no matter how much
they claim it’s an experiment, it’s still a live service), it shows how far
the once brightest star of the online world has fallen.This kind of thinking
kills innovation. When there's no room for a few hiccups or a little failure
you have to cover your ass so completely nothing new will ever see the light
of day.breakI thought we were supposed to beagile. We are supposed to release
early and often. Not every 'i' has to be dotted and not every last router has
to be installed before we take the first step of a grand new journey.Get it
out there. Let users help you make it better. Listen to customers, make
changes, push the new code out, listen some more, and fix problems as they
come up. Following this process we'll make something the customer wants and
needs</p><p>5 0.86575574 <a title="1382-lda-5" href="../high_scalability-2011/high_scalability-2011-09-16-Stuff_The_Internet_Says_On_Scalability_For_September_16%2C_2011.html">1117 high scalability-2011-09-16-Stuff The Internet Says On Scalability For September 16, 2011</a></p>
<p>Introduction: Between love and madness lies HighScalability:Google now 10x better: MapReduce
sorts 1 petabyte of data using 8000 computers in 33 minutes; 1 Billion on
Social Networks;Tumblr at 10 Billion Posts;Twitter at 100 Million
Users;Testing at Google Scale: 1800 builds, 120 million test suites, 60
million tests run daily.From theDash Memoon Google's Plan:Go is a very
promising systems-programming language in the vein of C++. We fully hope and
expect that Go becomes the standard back-end language at Google over the next
few years.On GAE Go can load from a cold start in 100ms and the typical
instance size is 4MB. Is it any wonder Go is a go? Should we expect to see
Java and Python deprecated because Go is so much cheaper to run at
scale?Potent Quotables:@caciufo: 30x more scalability w/ many-core. So perf
doesn't have to level out or vex programmers. #IDF2011@joerglew: Evaluating
divide&conquer; vs. master-slave architecture for workflows scalability in
#vCO. I wonder what fits better to cust. ne</p><p>6 0.86305517 <a title="1382-lda-6" href="../high_scalability-2007/high_scalability-2007-11-16-Mogulus_Doesn%27t_Own_a_Single_Server_and_has_%241.2_million_in_funding%2C_15%2C000_People_Creating_Channels.html">156 high scalability-2007-11-16-Mogulus Doesn't Own a Single Server and has $1.2 million in funding, 15,000 People Creating Channels</a></p>
<p>7 0.8611334 <a title="1382-lda-7" href="../high_scalability-2013/high_scalability-2013-05-03-Stuff_The_Internet_Says_On_Scalability_For_May_3%2C_2013.html">1451 high scalability-2013-05-03-Stuff The Internet Says On Scalability For May 3, 2013</a></p>
<p>8 0.85142893 <a title="1382-lda-8" href="../high_scalability-2010/high_scalability-2010-08-18-Misco%3A_A_MapReduce_Framework_for_Mobile_Systems_-_Start_of_the_Ambient_Cloud%3F.html">882 high scalability-2010-08-18-Misco: A MapReduce Framework for Mobile Systems - Start of the Ambient Cloud?</a></p>
<p>9 0.84403723 <a title="1382-lda-9" href="../high_scalability-2013/high_scalability-2013-04-01-Khan_Academy_Checkbook_Scaling_to_6_Million_Users_a_Month_on_GAE.html">1432 high scalability-2013-04-01-Khan Academy Checkbook Scaling to 6 Million Users a Month on GAE</a></p>
<p>10 0.84343976 <a title="1382-lda-10" href="../high_scalability-2014/high_scalability-2014-01-06-How_HipChat_Stores_and_Indexes_Billions_of_Messages_Using_ElasticSearch_and_Redis.html">1573 high scalability-2014-01-06-How HipChat Stores and Indexes Billions of Messages Using ElasticSearch and Redis</a></p>
<p>11 0.84227681 <a title="1382-lda-11" href="../high_scalability-2013/high_scalability-2013-01-18-Stuff_The_Internet_Says_On_Scalability_For_January_18%2C_2013.html">1389 high scalability-2013-01-18-Stuff The Internet Says On Scalability For January 18, 2013</a></p>
<p>12 0.83870506 <a title="1382-lda-12" href="../high_scalability-2010/high_scalability-2010-07-02-Hot_Scalability_Links_for_July_2%2C_2010.html">851 high scalability-2010-07-02-Hot Scalability Links for July 2, 2010</a></p>
<p>13 0.83704919 <a title="1382-lda-13" href="../high_scalability-2011/high_scalability-2011-07-25-Is_NoSQL_a_Premature_Optimization_that%27s_Worse_than_Death%3F_Or_the_Lady_Gaga_of_the_Database_World%3F.html">1085 high scalability-2011-07-25-Is NoSQL a Premature Optimization that's Worse than Death? Or the Lady Gaga of the Database World?</a></p>
<p>14 0.83402634 <a title="1382-lda-14" href="../high_scalability-2013/high_scalability-2013-12-06-Stuff_The_Internet_Says_On_Scalability_For_December_6th%2C_2013.html">1559 high scalability-2013-12-06-Stuff The Internet Says On Scalability For December 6th, 2013</a></p>
<p>15 0.8322528 <a title="1382-lda-15" href="../high_scalability-2009/high_scalability-2009-10-06-Building_a_Unique_Data_Warehouse.html">716 high scalability-2009-10-06-Building a Unique Data Warehouse</a></p>
<p>16 0.83202761 <a title="1382-lda-16" href="../high_scalability-2011/high_scalability-2011-02-04-Stuff_The_Internet_Says_On_Scalability_For_February_4%2C_2011.html">984 high scalability-2011-02-04-Stuff The Internet Says On Scalability For February 4, 2011</a></p>
<p>17 0.83146524 <a title="1382-lda-17" href="../high_scalability-2014/high_scalability-2014-05-16-Stuff_The_Internet_Says_On_Scalability_For_May_16th%2C_2014.html">1649 high scalability-2014-05-16-Stuff The Internet Says On Scalability For May 16th, 2014</a></p>
<p>18 0.83043623 <a title="1382-lda-18" href="../high_scalability-2011/high_scalability-2011-09-02-Stuff_The_Internet_Says_On_Scalability_For_September_2%2C_2011.html">1109 high scalability-2011-09-02-Stuff The Internet Says On Scalability For September 2, 2011</a></p>
<p>19 0.83021325 <a title="1382-lda-19" href="../high_scalability-2012/high_scalability-2012-01-24-The_State_of_NoSQL_in_2012.html">1180 high scalability-2012-01-24-The State of NoSQL in 2012</a></p>
<p>20 0.82970917 <a title="1382-lda-20" href="../high_scalability-2014/high_scalability-2014-03-03-The_%E2%80%9CFour_Hamiltons%E2%80%9D_Framework_for_Mitigating_Faults_in_the_Cloud%3A_Avoid_it%2C_Mask_it%2C_Bound_it%2C_Fix_it_Fast.html">1604 high scalability-2014-03-03-The “Four Hamiltons” Framework for Mitigating Faults in the Cloud: Avoid it, Mask it, Bound it, Fix it Fast</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
