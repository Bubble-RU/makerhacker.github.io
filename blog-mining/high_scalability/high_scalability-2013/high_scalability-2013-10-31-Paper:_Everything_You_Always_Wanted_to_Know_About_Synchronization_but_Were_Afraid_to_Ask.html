<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1541 high scalability-2013-10-31-Paper: Everything You Always Wanted to Know About Synchronization but Were Afraid to Ask</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2013" href="../home/high_scalability-2013_home.html">high_scalability-2013</a> <a title="high_scalability-2013-1541" href="#">high_scalability-2013-1541</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1541 high scalability-2013-10-31-Paper: Everything You Always Wanted to Know About Synchronization but Were Afraid to Ask</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2013-1541-html" href="http://highscalability.com//blog/2013/10/31/paper-everything-you-always-wanted-to-know-about-synchroniza.html">html</a></p><p>Introduction: Awesome paper on how particular synchronization mechanisms scale on multi-core architectures:  Everything You Always Wanted to Know About Synchronization but Were Afraid to Ask .
 
The goal is to pick a locking approach that doesn't degrade as the number of cores increase. Like everything else in life, that doesn't appear to be generically possible:
  
 None of the nine locking schemes we consider consistently outperforms any other one, on all target architectures or workloads. Strictly speaking, to seek optimality,  a lock algorithm should thus be selected based on the hardware platform and the expected workload .  
  
Abstract:
  

This paper presents the most exhaustive study of synchronization to date. We span multiple layers, from hardware cache-coherence protocols up to high-level concurrent software. We do so on different types architectures, from single-socket – uniform and nonuniform – to multi-socket – directory and broadcastbased – many-cores. We draw a set of observations t</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Awesome paper on how particular synchronization mechanisms scale on multi-core architectures:  Everything You Always Wanted to Know About Synchronization but Were Afraid to Ask . [sent-1, score-0.067]
</p><p>2 The goal is to pick a locking approach that doesn't degrade as the number of cores increase. [sent-2, score-0.191]
</p><p>3 Like everything else in life, that doesn't appear to be generically possible:     None of the nine locking schemes we consider consistently outperforms any other one, on all target architectures or workloads. [sent-3, score-0.727]
</p><p>4 Strictly speaking, to seek optimality,  a lock algorithm should thus be selected based on the hardware platform and the expected workload . [sent-4, score-0.142]
</p><p>5 Abstract:     This paper presents the most exhaustive study of synchronization to date. [sent-5, score-0.823]
</p><p>6 We span multiple layers, from hardware cache-coherence protocols up to high-level concurrent software. [sent-6, score-0.067]
</p><p>7 We do so on different types architectures, from single-socket – uniform and nonuniform – to multi-socket – directory and broadcastbased – many-cores. [sent-7, score-0.482]
</p><p>8 We draw a set of observations that, roughly speaking, imply that  scalability of synchronization is mainly a property of the hardware . [sent-8, score-0.882]
</p><p>9 Some Findings:      Synchronization scales much better within a single socket, irrespective of the contention level crossing sockets significantly impacts synchronization, regardless of the layer, e. [sent-9, score-0.25]
</p><p>10 In order to be able to scale, synchronization should better be conﬁned to a single socket, ideally a uniform one. [sent-12, score-0.816]
</p><p>11 Even on a singlesocket many-core such as the TILE-Gx36, a system should reduce the amount of highly contended data to avoid performance degradation (due to the hardware). [sent-13, score-0.24]
</p><p>12 Each of the nine state-of-the-art lock algorithms we evaluate performs the best on at least one workload/platform combination. [sent-14, score-0.255]
</p><p>13 Nevertheless, if we reduce the context of synchronization to a single socket (either one socket of a multi-socket, or a single-socket many-core), then our results indicate that  spin locks should be preferred over more complex locks . [sent-15, score-0.985]
</p><p>14 Implementing multi-socket coherence using broadcast or an incomplete directory (as on the Opteron) is not favorable to synchronization. [sent-16, score-0.505]
</p><p>15 The behavior of the TPC-H benchmarks on MonetDB is similar to the get workload of Memcache: synchronization is not a bottleneck. [sent-17, score-0.594]
</p><p>16 Related Articles       SSYNC  is a cross-platform synchronization suite. [sent-18, score-0.594]
</p><p>17 Contains libslock, a library that abstracts lock algorithms behind a common interface and libssmp, a library with fine-tuned implementations of message passing for each of the supported platforms. [sent-19, score-0.48]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('synchronization', 0.594), ('socket', 0.252), ('nine', 0.181), ('uniform', 0.147), ('lock', 0.142), ('speaking', 0.136), ('coherence', 0.127), ('directory', 0.126), ('architectures', 0.109), ('irrespective', 0.108), ('broadcastbased', 0.108), ('libslock', 0.108), ('libssmp', 0.108), ('locking', 0.107), ('favorable', 0.101), ('nonuniform', 0.101), ('monetdb', 0.101), ('generically', 0.101), ('contended', 0.101), ('exhaustive', 0.097), ('ned', 0.09), ('library', 0.089), ('nevertheless', 0.086), ('con', 0.086), ('abstracts', 0.086), ('opteron', 0.084), ('incomplete', 0.084), ('outperforms', 0.084), ('degrade', 0.084), ('strictly', 0.082), ('findings', 0.081), ('crossing', 0.081), ('imply', 0.078), ('ideally', 0.075), ('consistently', 0.075), ('algorithms', 0.074), ('impacts', 0.073), ('degradation', 0.073), ('preferred', 0.072), ('observations', 0.071), ('hardware', 0.07), ('schemes', 0.07), ('draw', 0.069), ('sockets', 0.069), ('broadcast', 0.067), ('paper', 0.067), ('span', 0.067), ('indicate', 0.067), ('reduce', 0.066), ('presents', 0.065)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="1541-tfidf-1" href="../high_scalability-2013/high_scalability-2013-10-31-Paper%3A_Everything_You_Always_Wanted_to_Know_About_Synchronization_but_Were_Afraid_to_Ask.html">1541 high scalability-2013-10-31-Paper: Everything You Always Wanted to Know About Synchronization but Were Afraid to Ask</a></p>
<p>Introduction: Awesome paper on how particular synchronization mechanisms scale on multi-core architectures:  Everything You Always Wanted to Know About Synchronization but Were Afraid to Ask .
 
The goal is to pick a locking approach that doesn't degrade as the number of cores increase. Like everything else in life, that doesn't appear to be generically possible:
  
 None of the nine locking schemes we consider consistently outperforms any other one, on all target architectures or workloads. Strictly speaking, to seek optimality,  a lock algorithm should thus be selected based on the hardware platform and the expected workload .  
  
Abstract:
  

This paper presents the most exhaustive study of synchronization to date. We span multiple layers, from hardware cache-coherence protocols up to high-level concurrent software. We do so on different types architectures, from single-socket – uniform and nonuniform – to multi-socket – directory and broadcastbased – many-cores. We draw a set of observations t</p><p>2 0.34726974 <a title="1541-tfidf-2" href="../high_scalability-2009/high_scalability-2009-11-01-Squeeze_more_performance_from_Parallelism.html">735 high scalability-2009-11-01-Squeeze more performance from Parallelism</a></p>
<p>Introduction: In many posts, such as:  The Future of the Parallelism and its Challenges  I mentioned that synchronization the access to the shared resource is the major challenge to write parallel code.
 
The synchronization and coordination take long time from the overall execution time, which reduce the benefits of the parallelism; the synchronization and coordination also reduce the scalability.
 
There are many forms of synchronization and coordination, such as:
  
 Create Task object in frameworks such as: Microsoft TPL, Intel TDD, and Parallel Runtime Library. Create and enqueue task objects require synchronization that it’s takes long time especially if we create it into recursive work such as: Quick Sort algorithm. 
 Synchronization the access to shared data. 
  
But there are a few techniques to avoid these issues, such as: Shared-Nothing, Actor Model, and Hyper Object (A.K.A. Combinable Object). Simply if we reduce the shared data by re-architect our code this will gives us a huge benefits</p><p>3 0.25520524 <a title="1541-tfidf-3" href="../high_scalability-2013/high_scalability-2013-10-25-Stuff_The_Internet_Says_On_Scalability_For_October_25th%2C_2013.html">1537 high scalability-2013-10-25-Stuff The Internet Says On Scalability For October 25th, 2013</a></p>
<p>Introduction: Hey, it's HighScalability time:
    Test your sense of scale. Is this image of something microscopic or macroscopic?  Find out .   
  $465m : Amount lost in 45 minutes due to a  software bug . Where? Where else...the finance industry. 
 Quotable Quotes:                                                          
 
  FCC : Fiber-to-the-home, on average, has the best performance in terms of latency, with 18 ms average during the peak period, with cable having 26 ms latency and DSL 44 ms latency. 
  @CompSciFact : "About 1,000 instructions is a reasonable upper limit for the complexity of problems now envisioned." -- John von Neumann, 1946  
  @anildash : healthcare.gov got 20M unique visitors in 20 days, faster than Google+ launch. Took Pinterest 2 years & BuzzFeed 4 years to hit 20M. 
 Thomas A. Edison: I start where the last man left off. 
  @brycebaril : I've never had a tech conference toy with my emotions like this year's #realtimeconf 
 
 
 
 Great explanation of the Netflix people d</p><p>4 0.16338705 <a title="1541-tfidf-4" href="../high_scalability-2009/high_scalability-2009-09-10-How_to_handle_so_many_socket_connection.html">699 high scalability-2009-09-10-How to handle so many socket connection</a></p>
<p>Introduction: In my application, we receive request from many clients through socket.Client can connect to this socket and send data. this connection has to be maintained for indefinite hours. Client are continously sending data . There are so many clients simultaneously to server.  I am using java to make application which listen on port and do processing on data. How can i scale this socket overhead. Is there any product which helps in maintening socket .</p><p>5 0.13387986 <a title="1541-tfidf-5" href="../high_scalability-2012/high_scalability-2012-03-06-Ask_For_Forgiveness_Programming_-_Or_How_We%27ll_Program_1000_Cores.html">1204 high scalability-2012-03-06-Ask For Forgiveness Programming - Or How We'll Program 1000 Cores</a></p>
<p>Introduction: The argument for a massively multicore future is now familiar: while clock speeds have leveled off, device density is increasing, so the future is cheap chips with hundreds and thousands of cores. That’s the inexorable logic behind our multicore future.
 
The unsolved question that lurks deep in the dark part of a programmer’s mind is: how on earth are we to program these things? For problems that aren’t   embarrassingly parallel   , we really have no idea. IBM Research’s    David Ungar    has an idea. And it’s radical in the extreme...     Grace Hopper    once advised “It's easier to ask for forgiveness than it is to get permission.” I wonder if she had any idea that her strategy for dealing with human bureaucracy would the same strategy David Ungar thinks will help us tame  the technological bureaucracy of 1000+ core systems?    You may recognize David as the co-creator of the    Self programming    language, inspiration for the HotSpot technology in the JVM and the prototype model u</p><p>6 0.11316678 <a title="1541-tfidf-6" href="../high_scalability-2012/high_scalability-2012-08-16-Paper%3A_A_Provably_Correct_Scalable_Concurrent_Skip_List.html">1305 high scalability-2012-08-16-Paper: A Provably Correct Scalable Concurrent Skip List</a></p>
<p>7 0.095169365 <a title="1541-tfidf-7" href="../high_scalability-2007/high_scalability-2007-12-07-Synchronizing_databases_in_different_geographic_locations.html">176 high scalability-2007-12-07-Synchronizing databases in different geographic locations</a></p>
<p>8 0.094077617 <a title="1541-tfidf-8" href="../high_scalability-2008/high_scalability-2008-12-13-Strategy%3A_Facebook_Tweaks_to_Handle_6_Time_as_Many_Memcached_Requests.html">464 high scalability-2008-12-13-Strategy: Facebook Tweaks to Handle 6 Time as Many Memcached Requests</a></p>
<p>9 0.090886623 <a title="1541-tfidf-9" href="../high_scalability-2008/high_scalability-2008-12-06-Paper%3A_Real-world_Concurrency.html">462 high scalability-2008-12-06-Paper: Real-world Concurrency</a></p>
<p>10 0.084409587 <a title="1541-tfidf-10" href="../high_scalability-2013/high_scalability-2013-05-16-Paper%3A_Warp%3A_Multi-Key_Transactions_for_Key-Value_Stores.html">1459 high scalability-2013-05-16-Paper: Warp: Multi-Key Transactions for Key-Value Stores</a></p>
<p>11 0.083750121 <a title="1541-tfidf-11" href="../high_scalability-2013/high_scalability-2013-05-13-The_Secret_to_10_Million_Concurrent_Connections_-The_Kernel_is_the_Problem%2C_Not_the_Solution.html">1456 high scalability-2013-05-13-The Secret to 10 Million Concurrent Connections -The Kernel is the Problem, Not the Solution</a></p>
<p>12 0.08301539 <a title="1541-tfidf-12" href="../high_scalability-2013/high_scalability-2013-02-27-42_Monster_Problems_that_Attack_as_Loads_Increase.html">1413 high scalability-2013-02-27-42 Monster Problems that Attack as Loads Increase</a></p>
<p>13 0.081973948 <a title="1541-tfidf-13" href="../high_scalability-2014/high_scalability-2014-04-10-Paper%3A_Scalable_Atomic_Visibility_with_RAMP_Transactions_-_Scale_Linearly_to_100_Servers.html">1629 high scalability-2014-04-10-Paper: Scalable Atomic Visibility with RAMP Transactions - Scale Linearly to 100 Servers</a></p>
<p>14 0.081450179 <a title="1541-tfidf-14" href="../high_scalability-2013/high_scalability-2013-06-19-Paper%3A_MegaPipe%3A_A_New_Programming_Interface_for_Scalable_Network_I-O.html">1478 high scalability-2013-06-19-Paper: MegaPipe: A New Programming Interface for Scalable Network I-O</a></p>
<p>15 0.080798857 <a title="1541-tfidf-15" href="../high_scalability-2008/high_scalability-2008-10-15-Oracle_opens_Coherence_Incubator.html">416 high scalability-2008-10-15-Oracle opens Coherence Incubator</a></p>
<p>16 0.072294839 <a title="1541-tfidf-16" href="../high_scalability-2014/high_scalability-2014-05-06-The_Quest_for_Database_Scale%3A_the_1_M_TPS_challenge_-_Three_Design_Points_and_Five_common_Bottlenecks_to_avoid.html">1643 high scalability-2014-05-06-The Quest for Database Scale: the 1 M TPS challenge - Three Design Points and Five common Bottlenecks to avoid</a></p>
<p>17 0.072133176 <a title="1541-tfidf-17" href="../high_scalability-2008/high_scalability-2008-08-29-Product%3A_ScaleOut_StateServer_is_Memcached_on_Steroids.html">373 high scalability-2008-08-29-Product: ScaleOut StateServer is Memcached on Steroids</a></p>
<p>18 0.071535997 <a title="1541-tfidf-18" href="../high_scalability-2012/high_scalability-2012-08-10-Stuff_The_Internet_Says_On_Scalability_For_August_10%2C_2012.html">1302 high scalability-2012-08-10-Stuff The Internet Says On Scalability For August 10, 2012</a></p>
<p>19 0.071367562 <a title="1541-tfidf-19" href="../high_scalability-2011/high_scalability-2011-06-06-NoSQL_Pain%3F_Learn_How_to_Read-write_Scale_Without_a_Complete_Re-write.html">1054 high scalability-2011-06-06-NoSQL Pain? Learn How to Read-write Scale Without a Complete Re-write</a></p>
<p>20 0.067784764 <a title="1541-tfidf-20" href="../high_scalability-2012/high_scalability-2012-08-06-Paper%3A_High-Performance_Concurrency_Control_Mechanisms_for_Main-Memory_Databases.html">1299 high scalability-2012-08-06-Paper: High-Performance Concurrency Control Mechanisms for Main-Memory Databases</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.087), (1, 0.068), (2, -0.016), (3, 0.024), (4, -0.011), (5, 0.048), (6, 0.04), (7, 0.028), (8, -0.084), (9, -0.002), (10, 0.002), (11, 0.01), (12, -0.004), (13, -0.002), (14, -0.015), (15, -0.036), (16, 0.041), (17, -0.003), (18, 0.039), (19, -0.036), (20, 0.009), (21, -0.003), (22, -0.048), (23, 0.011), (24, -0.011), (25, -0.034), (26, 0.018), (27, -0.002), (28, 0.064), (29, 0.008), (30, 0.016), (31, 0.025), (32, -0.037), (33, -0.002), (34, -0.036), (35, 0.031), (36, 0.043), (37, -0.022), (38, 0.071), (39, 0.06), (40, -0.013), (41, 0.054), (42, -0.024), (43, -0.047), (44, -0.092), (45, 0.055), (46, -0.024), (47, 0.017), (48, 0.029), (49, 0.056)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9648729 <a title="1541-lsi-1" href="../high_scalability-2013/high_scalability-2013-10-31-Paper%3A_Everything_You_Always_Wanted_to_Know_About_Synchronization_but_Were_Afraid_to_Ask.html">1541 high scalability-2013-10-31-Paper: Everything You Always Wanted to Know About Synchronization but Were Afraid to Ask</a></p>
<p>Introduction: Awesome paper on how particular synchronization mechanisms scale on multi-core architectures:  Everything You Always Wanted to Know About Synchronization but Were Afraid to Ask .
 
The goal is to pick a locking approach that doesn't degrade as the number of cores increase. Like everything else in life, that doesn't appear to be generically possible:
  
 None of the nine locking schemes we consider consistently outperforms any other one, on all target architectures or workloads. Strictly speaking, to seek optimality,  a lock algorithm should thus be selected based on the hardware platform and the expected workload .  
  
Abstract:
  

This paper presents the most exhaustive study of synchronization to date. We span multiple layers, from hardware cache-coherence protocols up to high-level concurrent software. We do so on different types architectures, from single-socket – uniform and nonuniform – to multi-socket – directory and broadcastbased – many-cores. We draw a set of observations t</p><p>2 0.78804564 <a title="1541-lsi-2" href="../high_scalability-2009/high_scalability-2009-11-01-Squeeze_more_performance_from_Parallelism.html">735 high scalability-2009-11-01-Squeeze more performance from Parallelism</a></p>
<p>Introduction: In many posts, such as:  The Future of the Parallelism and its Challenges  I mentioned that synchronization the access to the shared resource is the major challenge to write parallel code.
 
The synchronization and coordination take long time from the overall execution time, which reduce the benefits of the parallelism; the synchronization and coordination also reduce the scalability.
 
There are many forms of synchronization and coordination, such as:
  
 Create Task object in frameworks such as: Microsoft TPL, Intel TDD, and Parallel Runtime Library. Create and enqueue task objects require synchronization that it’s takes long time especially if we create it into recursive work such as: Quick Sort algorithm. 
 Synchronization the access to shared data. 
  
But there are a few techniques to avoid these issues, such as: Shared-Nothing, Actor Model, and Hyper Object (A.K.A. Combinable Object). Simply if we reduce the shared data by re-architect our code this will gives us a huge benefits</p><p>3 0.68474185 <a title="1541-lsi-3" href="../high_scalability-2012/high_scalability-2012-08-06-Paper%3A_High-Performance_Concurrency_Control_Mechanisms_for_Main-Memory_Databases.html">1299 high scalability-2012-08-06-Paper: High-Performance Concurrency Control Mechanisms for Main-Memory Databases</a></p>
<p>Introduction: If you stayed up all night watching the life reaffirming  Curiosity landing on Mars , then this paper,  High-Performance Concurrency Control Mechanisms for Main-Memory Databases , has nothing to do with that at all, but it is an excellent look at how to use optimistic MVCC schemes to reduce lock overhead on in-memory datastructures:
  A database system optimized for in-memory storage can support  much higher transaction rates than current systems.  However,  standard concurrency control methods used today do not scale to  the high transaction rates achievable by such systems. In this paper we introduce two efficient concurrency control methods specifically designed for main-memory databases. Both use multiversioning to isolate read-only transactions from updates but differ in  how atomicity is ensured: one is optimistic and one is pessimistic. To avoid expensive context switching, transactions  never block  during normal processing but they may have to wait before commit to ensure corr</p><p>4 0.62649912 <a title="1541-lsi-4" href="../high_scalability-2012/high_scalability-2012-08-16-Paper%3A_A_Provably_Correct_Scalable_Concurrent_Skip_List.html">1305 high scalability-2012-08-16-Paper: A Provably Correct Scalable Concurrent Skip List</a></p>
<p>Introduction: In  MemSQL Architecture  we learned one of the core strategies MemSQL uses to achieve their need for speed is lock-free skip lists. Skip lists are used to efficiently handle range queries. Making the skip-lists lock-free helps eliminate contention and make writes fast. 
 
If this all sounds a little pie-in-the-sky then here's a very good paper on the subject that might help make it clearer:  A Provably Correct Scalable Concurrent Skip List .
 
From the abstract:
  We propose a new concurrent skip list algorithm distinguished by a combination of simplicity and scalability. The algorithm employs optimistic synchronization, searching without acquiring locks, followed by short lock-based validation before adding or removing nodes. It also logically removes an item before physically unlinking it. Unlike some other concurrent skip list algorithms, this algorithm preserves the skiplist properties at all times, which facilitates reasoning about its correctness. Experimental evidence shows that</p><p>5 0.62595284 <a title="1541-lsi-5" href="../high_scalability-2013/high_scalability-2013-06-19-Paper%3A_MegaPipe%3A_A_New_Programming_Interface_for_Scalable_Network_I-O.html">1478 high scalability-2013-06-19-Paper: MegaPipe: A New Programming Interface for Scalable Network I-O</a></p>
<p>Introduction: The paper  MegaPipe: A New Programming Interface for Scalable Network I/O  ( video ,  slides ) hits the common theme that if you want to go faster you need a better car design, not just a better driver. So that's why the authors started with a clean-slate and designed a network API from the ground up with support for concurrent I/O, a requirement for achieving high performance while scaling to large numbers of connections per thread, multiple cores, etc.  What they created is MegaPipe, "a new network programming API for message-oriented workloads to avoid the performance issues of BSD Socket API."
 
The result: MegaPipe outperforms baseline Linux between  29% (for long connections)  and  582% (for short connections) . MegaPipe improves the performance of a modiﬁed version of  memcached between 15% and 320% . For a workload based on real-world HTTP traces, MegaPipe boosts the throughput of  nginx by 75% .
 
What's this most excellent and interesting paper about?
  Message-oriented netwo</p><p>6 0.6127221 <a title="1541-lsi-6" href="../high_scalability-2010/high_scalability-2010-10-04-Paper%3A_An_Analysis_of_Linux_Scalability_to_Many_Cores__.html">914 high scalability-2010-10-04-Paper: An Analysis of Linux Scalability to Many Cores  </a></p>
<p>7 0.59278613 <a title="1541-lsi-7" href="../high_scalability-2011/high_scalability-2011-09-28-Pursue_robust_indefinite_scalability_with_the_Movable_Feast_Machine.html">1127 high scalability-2011-09-28-Pursue robust indefinite scalability with the Movable Feast Machine</a></p>
<p>8 0.5927285 <a title="1541-lsi-8" href="../high_scalability-2009/high_scalability-2009-04-26-Map-Reduce_for_Machine_Learning_on_Multicore.html">581 high scalability-2009-04-26-Map-Reduce for Machine Learning on Multicore</a></p>
<p>9 0.59132117 <a title="1541-lsi-9" href="../high_scalability-2011/high_scalability-2011-02-02-Piccolo_-_Building_Distributed_Programs_that_are_11x_Faster_than_Hadoop.html">983 high scalability-2011-02-02-Piccolo - Building Distributed Programs that are 11x Faster than Hadoop</a></p>
<p>10 0.58760238 <a title="1541-lsi-10" href="../high_scalability-2010/high_scalability-2010-06-18-Paper%3A_The_Declarative_Imperative%3A_Experiences_and_Conjectures_in_Distributed_Logic.html">844 high scalability-2010-06-18-Paper: The Declarative Imperative: Experiences and Conjectures in Distributed Logic</a></p>
<p>11 0.57003647 <a title="1541-lsi-11" href="../high_scalability-2013/high_scalability-2013-10-25-Stuff_The_Internet_Says_On_Scalability_For_October_25th%2C_2013.html">1537 high scalability-2013-10-25-Stuff The Internet Says On Scalability For October 25th, 2013</a></p>
<p>12 0.56388205 <a title="1541-lsi-12" href="../high_scalability-2009/high_scalability-2009-06-23-Learn_How_to_Exploit_Multiple_Cores_for_Better_Performance_and_Scalability.html">636 high scalability-2009-06-23-Learn How to Exploit Multiple Cores for Better Performance and Scalability</a></p>
<p>13 0.56094974 <a title="1541-lsi-13" href="../high_scalability-2013/high_scalability-2013-05-08-Typesafe_Interview%3A_Scala_%2B_Akka_is_an_IaaS_for_Your_Process_Architecture.html">1454 high scalability-2013-05-08-Typesafe Interview: Scala + Akka is an IaaS for Your Process Architecture</a></p>
<p>14 0.56053752 <a title="1541-lsi-14" href="../high_scalability-2012/high_scalability-2012-04-30-Masstree_-_Much_Faster_than_MongoDB%2C_VoltDB%2C_Redis%2C_and_Competitive_with_Memcached.html">1236 high scalability-2012-04-30-Masstree - Much Faster than MongoDB, VoltDB, Redis, and Competitive with Memcached</a></p>
<p>15 0.5553509 <a title="1541-lsi-15" href="../high_scalability-2008/high_scalability-2008-02-03-Ideas_on_how_to_scale_a_shared_inventory_database%3F%3F%3F.html">236 high scalability-2008-02-03-Ideas on how to scale a shared inventory database???</a></p>
<p>16 0.5478965 <a title="1541-lsi-16" href="../high_scalability-2012/high_scalability-2012-03-06-Ask_For_Forgiveness_Programming_-_Or_How_We%27ll_Program_1000_Cores.html">1204 high scalability-2012-03-06-Ask For Forgiveness Programming - Or How We'll Program 1000 Cores</a></p>
<p>17 0.5384472 <a title="1541-lsi-17" href="../high_scalability-2014/high_scalability-2014-04-10-Paper%3A_Scalable_Atomic_Visibility_with_RAMP_Transactions_-_Scale_Linearly_to_100_Servers.html">1629 high scalability-2014-04-10-Paper: Scalable Atomic Visibility with RAMP Transactions - Scale Linearly to 100 Servers</a></p>
<p>18 0.53606677 <a title="1541-lsi-18" href="../high_scalability-2009/high_scalability-2009-09-16-Paper%3A_A_practical_scalable_distributed_B-tree.html">705 high scalability-2009-09-16-Paper: A practical scalable distributed B-tree</a></p>
<p>19 0.53088474 <a title="1541-lsi-19" href="../high_scalability-2013/high_scalability-2013-05-30-Google_Finds_NUMA_Up_to_20%25_Slower_for_Gmail_and_Websearch.html">1467 high scalability-2013-05-30-Google Finds NUMA Up to 20% Slower for Gmail and Websearch</a></p>
<p>20 0.52823788 <a title="1541-lsi-20" href="../high_scalability-2013/high_scalability-2013-10-23-Strategy%3A_Use_Linux_Taskset_to_Pin_Processes_or_Let_the_OS_Schedule_It%3F.html">1536 high scalability-2013-10-23-Strategy: Use Linux Taskset to Pin Processes or Let the OS Schedule It?</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.055), (2, 0.122), (3, 0.302), (10, 0.101), (30, 0.083), (61, 0.082), (77, 0.049), (79, 0.08), (85, 0.033)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.8919608 <a title="1541-lda-1" href="../high_scalability-2013/high_scalability-2013-10-31-Paper%3A_Everything_You_Always_Wanted_to_Know_About_Synchronization_but_Were_Afraid_to_Ask.html">1541 high scalability-2013-10-31-Paper: Everything You Always Wanted to Know About Synchronization but Were Afraid to Ask</a></p>
<p>Introduction: Awesome paper on how particular synchronization mechanisms scale on multi-core architectures:  Everything You Always Wanted to Know About Synchronization but Were Afraid to Ask .
 
The goal is to pick a locking approach that doesn't degrade as the number of cores increase. Like everything else in life, that doesn't appear to be generically possible:
  
 None of the nine locking schemes we consider consistently outperforms any other one, on all target architectures or workloads. Strictly speaking, to seek optimality,  a lock algorithm should thus be selected based on the hardware platform and the expected workload .  
  
Abstract:
  

This paper presents the most exhaustive study of synchronization to date. We span multiple layers, from hardware cache-coherence protocols up to high-level concurrent software. We do so on different types architectures, from single-socket – uniform and nonuniform – to multi-socket – directory and broadcastbased – many-cores. We draw a set of observations t</p><p>2 0.72941548 <a title="1541-lda-2" href="../high_scalability-2007/high_scalability-2007-09-08-MP3.com_Web_Templating_Architecture_%28March%2C_2000%29.html">84 high scalability-2007-09-08-MP3.com Web Templating Architecture (March, 2000)</a></p>
<p>Introduction: In March, 2000, I  did a talk  about how we scaled with semi-static files while splitting data from presentation. For dynamic pages we used mod_perl doing an internal redirect with the XML on the style templates. Since then Apache 2.0 contains the concept of filters to allow for similar functionality.</p><p>3 0.72033101 <a title="1541-lda-3" href="../high_scalability-2010/high_scalability-2010-10-24-Hot_Scalability_Links_For_Oct_24%2C_2010.html">926 high scalability-2010-10-24-Hot Scalability Links For Oct 24, 2010</a></p>
<p>Introduction: On a cold and rainy Fall day, a day stolen from winter rather than our usual gorgeous  Indian Summers , a day not even the SF Giants winning the pennant can help warm, here are some hot links to read by a digital flame: 
  
  Using MySQL as a NoSQL - A story for exceeding 750,000 qps on a commodity server  by Yoshinori Matsunobu. Wonderfully detailed post on how you can lookup a row by ID really fast if you bypass all the typical MySQL query parsing overhead. 
  Minecraftwiki.net and minecraftforum.net now serve more traffic than Slashdot and Stackoverflow!  1 million pageviews and 100k uniques  per day, per site; 10TB of bandwidth a month; 4+ machines running Varnish, HAProxy, PHP, MySQL, Nginx.  
 Stuff the Internet Says:            
 
 @ old_sound : Somebody make me a t-shirt that says "I've read the CAP theorem and I liked it"  
  @dscape : How relevant do I think the CAP theorem is? Not at all. I honestly hate conversations where anyone talks about crap.. cap, sorry.  
  @humidbei</p><p>4 0.70937467 <a title="1541-lda-4" href="../high_scalability-2010/high_scalability-2010-01-25-Designing_applications_for_cloud_deployment.html">764 high scalability-2010-01-25-Designing applications for cloud deployment</a></p>
<p>Introduction: During the last two years, I was involved in several projects deployed on the Amazon cloud. Being a relatively early adopter was a fantastic experience that provided lots of opportunities to burn my fingers and learn from mistakes. It also seriously challenged my view of scalable software architectures. I spoke about key lessons learned at CloudCamp London last week – here is the summary of that presentation.﻿
 
 http://gojko.net/2010/01/25/designing-applications-for-cloud-deployment/</p><p>5 0.64911264 <a title="1541-lda-5" href="../high_scalability-2012/high_scalability-2012-01-20-Stuff_The_Internet_Says_On_Scalability_For_January_20%2C_2012.html">1178 high scalability-2012-01-20-Stuff The Internet Says On Scalability For January 20, 2012</a></p>
<p>Introduction: If you’ve got the time, we’ve got the HighScalability:
  
 Google+:  90 million users ;  Internet 2011 : 2.1 billion Internet users, 1 trillion YouTube views, 5.9 billion mobile subscriptions; Fusion-io:  One Billion IOPS ; 12 atoms:  size of IBM's new memory bit ; 32 Million:  Stack monthly visitors ; Gmail:  350 Million Users ; TimTebow:  1.5 million Tweets  
 Quotable Quotes:           
 
  Similarity  : There is no canonical schema anymore. Instead you should ask: What high-volume queries will I need to serve with my data? Then work backwards from there. 
  @kvirjee  : Dis/Agree? -- "there is no problem but scalability, and architecture is its solution" 
  @robpegoraro  : Eternal vigilance can be crowdsourced. 
 
 
 Didn't Bill Gates say once that 48 bits would always be enough for an ID? Well, Oracle ran out of bits:  Fundamental Oracle flaw revealed . 64 bits, that's the ticket, ipv6 went 128 bits.  
  The day Kodak died : We developed the world's first consumer digital camera bu</p><p>6 0.60910773 <a title="1541-lda-6" href="../high_scalability-2008/high_scalability-2008-07-20-Strategy%3A_Front_S3_with_a_Caching_Proxy.html">353 high scalability-2008-07-20-Strategy: Front S3 with a Caching Proxy</a></p>
<p>7 0.59112418 <a title="1541-lda-7" href="../high_scalability-2007/high_scalability-2007-12-21-Strategy%3A_Limit_Result_Sets.html">189 high scalability-2007-12-21-Strategy: Limit Result Sets</a></p>
<p>8 0.56781983 <a title="1541-lda-8" href="../high_scalability-2008/high_scalability-2008-10-24-11_Secrets_of_a_Cloud_Scale_Consultant_That_They_Dont%27_Want_You_to_Know.html">428 high scalability-2008-10-24-11 Secrets of a Cloud Scale Consultant That They Dont' Want You to Know</a></p>
<p>9 0.56530195 <a title="1541-lda-9" href="../high_scalability-2010/high_scalability-2010-03-09-Applications_as_Virtual_States.html">790 high scalability-2010-03-09-Applications as Virtual States</a></p>
<p>10 0.56350285 <a title="1541-lda-10" href="../high_scalability-2008/high_scalability-2008-12-28-How_to_Organize_a_Database_Table%E2%80%99s_Keys_for_Scalability.html">476 high scalability-2008-12-28-How to Organize a Database Table’s Keys for Scalability</a></p>
<p>11 0.56237018 <a title="1541-lda-11" href="../high_scalability-2007/high_scalability-2007-11-12-Scaling_Using_Cache_Farms_and_Read_Pooling_.html">149 high scalability-2007-11-12-Scaling Using Cache Farms and Read Pooling </a></p>
<p>12 0.55639118 <a title="1541-lda-12" href="../high_scalability-2012/high_scalability-2012-02-16-A_Super_Short_on_the_Youporn_Stack_-_300K_QPS_and_100_Million_Page_Views_Per_Day.html">1194 high scalability-2012-02-16-A Super Short on the Youporn Stack - 300K QPS and 100 Million Page Views Per Day</a></p>
<p>13 0.54960269 <a title="1541-lda-13" href="../high_scalability-2008/high_scalability-2008-08-08-Separation_into_read-write_only_databases.html">361 high scalability-2008-08-08-Separation into read-write only databases</a></p>
<p>14 0.54818416 <a title="1541-lda-14" href="../high_scalability-2008/high_scalability-2008-04-30-Rather_small_site_architecture..html">312 high scalability-2008-04-30-Rather small site architecture.</a></p>
<p>15 0.54771221 <a title="1541-lda-15" href="../high_scalability-2010/high_scalability-2010-05-26-End-To-End_Performance_Study_of_Cloud_Services.html">831 high scalability-2010-05-26-End-To-End Performance Study of Cloud Services</a></p>
<p>16 0.54597193 <a title="1541-lda-16" href="../high_scalability-2008/high_scalability-2008-08-16-Strategy%3A_Serve_Pre-generated_Static_Files_Instead_Of_Dynamic_Pages.html">365 high scalability-2008-08-16-Strategy: Serve Pre-generated Static Files Instead Of Dynamic Pages</a></p>
<p>17 0.54447907 <a title="1541-lda-17" href="../high_scalability-2012/high_scalability-2012-07-25-Vertical_Scaling_Ascendant_-_How_are_SSDs_Changing__Architectures%3F.html">1291 high scalability-2012-07-25-Vertical Scaling Ascendant - How are SSDs Changing  Architectures?</a></p>
<p>18 0.54241663 <a title="1541-lda-18" href="../high_scalability-2010/high_scalability-2010-04-16-Hot_Scalability_Links_for_April_16%2C_2010.html">811 high scalability-2010-04-16-Hot Scalability Links for April 16, 2010</a></p>
<p>19 0.5411244 <a title="1541-lda-19" href="../high_scalability-2012/high_scalability-2012-05-21-Pinterest_Architecture_Update_-_18_Million_Visitors%2C_10x_Growth%2C12_Employees%2C_410_TB_of_Data.html">1248 high scalability-2012-05-21-Pinterest Architecture Update - 18 Million Visitors, 10x Growth,12 Employees, 410 TB of Data</a></p>
<p>20 0.53455734 <a title="1541-lda-20" href="../high_scalability-2010/high_scalability-2010-10-08-4_Scalability_Themes_from_Surgecon.html">917 high scalability-2010-10-08-4 Scalability Themes from Surgecon</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
