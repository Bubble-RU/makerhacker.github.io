<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1456 high scalability-2013-05-13-The Secret to 10 Million Concurrent Connections -The Kernel is the Problem, Not the Solution</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2013" href="../home/high_scalability-2013_home.html">high_scalability-2013</a> <a title="high_scalability-2013-1456" href="#">high_scalability-2013-1456</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1456 high scalability-2013-05-13-The Secret to 10 Million Concurrent Connections -The Kernel is the Problem, Not the Solution</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2013-1456-html" href="http://highscalability.com//blog/2013/5/13/the-secret-to-10-million-concurrent-connections-the-kernel-i.html">html</a></p><p>Introduction: Now that we have the    C10K concurrent connection problem    licked, how do we level up and support 10 million concurrent connections? Impossible you say. Nope, systems right now are delivering 10 million concurrent connections using techniques that are as radical as they may be unfamiliar. 
   To learn how it’s done we turn to    Robert Graham   , CEO of Errata Security, and his absolutely fantastic talk at    Shmoocon 2013    called    C10M Defending The Internet At Scale   . 
  Robert has a brilliant way of framing the problem that I’ve never heard of before. He starts with a little bit of history, relating how Unix wasn’t originally designed to be a general server OS, it was designed to be a control system for a telephone network. It was the telephone network that actually transported the data so there was a clean separation between the control plane and the data plane. The  problem is we now use Unix servers as part of the data plane , which we shouldn’t do at all. If we were des</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Now that we have the    C10K concurrent connection problem    licked, how do we level up and support 10 million concurrent connections? [sent-1, score-0.655]
</p><p>2 Take packet handling, memory management, and processor scheduling out of the kernel and put it into the application, where it can be done efficiently. [sent-13, score-0.785]
</p><p>3 Let Linux handle the control plane and let the the application handle the data plane. [sent-14, score-0.744]
</p><p>4 The result will be a system that can handle 10 million concurrent connections with 200 clock cycles for packet handling and 1400 hundred clock cycles for application logic. [sent-15, score-1.639]
</p><p>5 With a    data plane oriented system    you can process 10 million packets per second. [sent-17, score-0.785]
</p><p>6 With a control plane oriented system you only get 1 million packets per second. [sent-18, score-0.789]
</p><p>7 With short term connections that last a few seconds, say a quick transaction, if you are executing a 1000 TPS then you’ll only have about a 1000 concurrent connections to the server. [sent-34, score-0.562]
</p><p>8 If you are handling 5,000 connections per second and you want to handle 10K, what do you do? [sent-38, score-0.577]
</p><p>9 As a packet came in it would walk down all 10K processes in the kernel to figure out which thread should handle the packet         Connections = select/poll (single thread). [sent-51, score-1.143]
</p><p>10 What the 10M Concurrent Connection Challenge means:         10 million concurrent connections         1 million connections/second - sustained rate at about 10 seconds a connections         10 gigabits/second connection - fast connections to the Internet. [sent-72, score-1.081]
</p><p>11 10 million packets/second - expect current servers to handle 50K packets per second, this is going to a higher level. [sent-73, score-0.636]
</p><p>12 Servers used to be able to handle 100K interrupts per second and every packet caused interrupts. [sent-74, score-0.697]
</p><p>13 You are using the thread scheduling system as the packet scheduling system   . [sent-87, score-0.743]
</p><p>14 What Nginx says it don’t use thread scheduling as the packet scheduler. [sent-89, score-0.611]
</p><p>15 To go to the next level the problems we need to solve are:      packet scalability   multi-core scalability   memory scalability       Packet Scaling - Write Your Own Custom Driver to Bypass the Stack         The problem with packets is they go through the Unix kernel. [sent-97, score-0.678]
</p><p>16 Intel has a benchmark where the process 80 million packets per second (200 clock cycles per packet) on a fairly lightweight server. [sent-106, score-0.96]
</p><p>17 Linux doesn’t do more than a million packets per second when getting UDP packets up to user mode and out again. [sent-109, score-0.785]
</p><p>18 For the 10 million packets per second goal if 200 clock cycles are used in getting the packet that leaves 1400 clocks cycles to implement functionally like a DNS/IDS. [sent-111, score-1.263]
</p><p>19 We only have 4 cache misses per packet and that's a problem. [sent-150, score-0.611]
</p><p>20 Allocate on a per object, per thread, and per socket basis. [sent-163, score-0.574]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('packet', 0.304), ('plane', 0.278), ('kernel', 0.251), ('unix', 0.214), ('packets', 0.2), ('connections', 0.191), ('concurrent', 0.18), ('thread', 0.175), ('clock', 0.16), ('per', 0.146), ('cycles', 0.145), ('socket', 0.136), ('scheduling', 0.132), ('intel', 0.115), ('connection', 0.11), ('handle', 0.109), ('million', 0.109), ('apache', 0.106), ('threads', 0.106), ('memory', 0.098), ('decade', 0.095), ('misses', 0.09), ('driver', 0.084), ('interrupts', 0.084), ('linux', 0.083), ('paging', 0.083), ('let', 0.081), ('os', 0.08), ('dpdk', 0.078), ('double', 0.077), ('handling', 0.077), ('mode', 0.076), ('problem', 0.076), ('cores', 0.074), ('servers', 0.072), ('cache', 0.071), ('epoll', 0.07), ('kit', 0.07), ('scale', 0.065), ('performance', 0.063), ('microsecond', 0.062), ('lifting', 0.059), ('telephone', 0.059), ('hardware', 0.059), ('application', 0.059), ('software', 0.057), ('control', 0.056), ('second', 0.054), ('pointer', 0.054), ('data', 0.052)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000005 <a title="1456-tfidf-1" href="../high_scalability-2013/high_scalability-2013-05-13-The_Secret_to_10_Million_Concurrent_Connections_-The_Kernel_is_the_Problem%2C_Not_the_Solution.html">1456 high scalability-2013-05-13-The Secret to 10 Million Concurrent Connections -The Kernel is the Problem, Not the Solution</a></p>
<p>Introduction: Now that we have the    C10K concurrent connection problem    licked, how do we level up and support 10 million concurrent connections? Impossible you say. Nope, systems right now are delivering 10 million concurrent connections using techniques that are as radical as they may be unfamiliar. 
   To learn how it’s done we turn to    Robert Graham   , CEO of Errata Security, and his absolutely fantastic talk at    Shmoocon 2013    called    C10M Defending The Internet At Scale   . 
  Robert has a brilliant way of framing the problem that I’ve never heard of before. He starts with a little bit of history, relating how Unix wasn’t originally designed to be a general server OS, it was designed to be a control system for a telephone network. It was the telephone network that actually transported the data so there was a clean separation between the control plane and the data plane. The  problem is we now use Unix servers as part of the data plane , which we shouldn’t do at all. If we were des</p><p>2 0.25590947 <a title="1456-tfidf-2" href="../high_scalability-2012/high_scalability-2012-09-10-Russ%E2%80%99_10_Ingredient_Recipe_for_Making_1_Million_TPS_on_%245K_Hardware.html">1319 high scalability-2012-09-10-Russ’ 10 Ingredient Recipe for Making 1 Million TPS on $5K Hardware</a></p>
<p>Introduction: My name is  Russell Sullivan , I am the author of AlchemyDB: a highly flexible NoSQL/SQL/DocumentStore/GraphDB-datastore built on top of redis. I have spent the last several years trying to find a way to sanely house multiple datastore-genres under one roof while (almost paradoxically) pushing performance to its limits.    I recently joined the NoSQL company    Aerospike    (formerly Citrusleaf) with the goal of incrementally grafting AlchemyDB’s flexible data-modeling capabilities onto Aerospike’s high-velocity horizontally-scalable key-value data-fabric. We recently completed a peak-performance    TPS optimization project   : starting at 200K TPS, pushing to the recent community edition launch at 500K TPS, and finally arriving at our 2012 goal:    1M TPS on $5K hardware   .    Getting to one million over-the-wire client-server database-requests per-second on a single machine costing $5K is a balance between trimming overhead on many axes and using a shared nothing architecture to   i</p><p>3 0.23169738 <a title="1456-tfidf-3" href="../high_scalability-2012/high_scalability-2012-03-22-Paper%3A_Revisiting_Network_I-O_APIs%3A_The_netmap_Framework.html">1213 high scalability-2012-03-22-Paper: Revisiting Network I-O APIs: The netmap Framework</a></p>
<p>Introduction: Here's a really good article in the Communications of the ACM on reducing network packet processing overhead by redesigning the network stack:  Revisiting Network I/O APIs: The Netmap Framework  by  Luigi Rizzo . As commodity networking performance increases operating systems need to keep up or all those CPUs will go to waste. How do they make this happen?

 


Abstract:

 

Today 10-gigabit interfaces are used more and more in datacenters and servers. On these links, packets flow as fast as one every 67.2 nanoseconds, yet modern operating systems can take 10-20 times longer just to move one packet between the wire and the application. We can do much better, not with more powerful hardware but by revising architectural decisions made long ago regarding the design of device drivers and network stacks.


The netmap framework is a promising step in this direction. Thanks to a careful design and the engineering of a new packet I/O API, netmap eliminates much unnecessary overhead and moves</p><p>4 0.2219374 <a title="1456-tfidf-4" href="../high_scalability-2014/high_scalability-2014-05-06-The_Quest_for_Database_Scale%3A_the_1_M_TPS_challenge_-_Three_Design_Points_and_Five_common_Bottlenecks_to_avoid.html">1643 high scalability-2014-05-06-The Quest for Database Scale: the 1 M TPS challenge - Three Design Points and Five common Bottlenecks to avoid</a></p>
<p>Introduction: This a guest post by  Rajkumar Iyer , a Member of Technical Staff at Aerospike. 
 
About a year ago, Aerospike embarked upon a quest to increase in-memory database performance - 1 Million TPS on a single inexpensive commodity server. NoSQL has the reputation of speed, and we saw great benefit from improving latency and throughput of cacheless architectures. At that time, we took a version of Aerospike delivering about 200K TPS, improved a few things - performance went to 500k TPS - and published the Aerospike 2.0 Community Edition. We then used kernel tuning techniques and published the  recipe  for how we achieved 1 M TPS on $5k of hardware.
  This year we continued the quest. Our goal was to achieve 1 Million database transactions per second per server; more than doubling previous performance. This compares to Cassandra’s boast of 1M TPS on over 300 servers in Google Compute Engine - at a cost of $2 million dollars per year. We  achieved this  without kernel tuning. 
  This article d</p><p>5 0.19440518 <a title="1456-tfidf-5" href="../high_scalability-2013/high_scalability-2013-05-17-Stuff_The_Internet_Says_On_Scalability_For_May_17%2C_2013.html">1460 high scalability-2013-05-17-Stuff The Internet Says On Scalability For May 17, 2013</a></p>
<p>Introduction: Hey, it's HighScalability time:
    ( Earth sized solar flare ,  some more flair )  
 
  
 Google I/O to world: Just try to keep up with us. You can't. But go ahead and try. Nah na na na nah... 
 
  17 billion : Google Cloud Messaging messages per day with 60ms latency;  1B page views : 500px;  121 billion :  edge graph using Titan;  4 billion hours : hours watched on Netflix per quarter;  4.5 trillion : BigTable transactions per month 
 
 Quotable Quotes:                     
 
  to3m : As with any time you make plans for the future, sometimes you get it wrong. Ars longa vita brevis, and all that. 
  Callaghan’s law : a given row can’t be modified more than once per RTT 
  Josh Haberman : I had an epiphany one day when I realized that the kernel is nothing but a library with an expensive calling convention. 
  fread2281 : Insane speed calls for insane measures. 
  Luke Gorrie : hardware really wants to run fast and you only need to avoid getting in the way --  not too hard if you writ</p><p>6 0.18386333 <a title="1456-tfidf-6" href="../high_scalability-2009/high_scalability-2009-07-25-Latency_is_Everywhere_and_it_Costs_You_Sales_-_How_to_Crush_it.html">661 high scalability-2009-07-25-Latency is Everywhere and it Costs You Sales - How to Crush it</a></p>
<p>7 0.18076377 <a title="1456-tfidf-7" href="../high_scalability-2014/high_scalability-2014-02-05-Little%E2%80%99s_Law%2C_Scalability_and_Fault_Tolerance%3A_The_OS_is_your_bottleneck._What_you_can_do%3F.html">1591 high scalability-2014-02-05-Little’s Law, Scalability and Fault Tolerance: The OS is your bottleneck. What you can do?</a></p>
<p>8 0.17416173 <a title="1456-tfidf-8" href="../high_scalability-2011/high_scalability-2011-05-27-Stuff_The_Internet_Says_On_Scalability_For_May_27%2C_2011.html">1048 high scalability-2011-05-27-Stuff The Internet Says On Scalability For May 27, 2011</a></p>
<p>9 0.17311577 <a title="1456-tfidf-9" href="../high_scalability-2012/high_scalability-2012-11-26-BigData_using_Erlang%2C_C_and_Lisp_to_Fight_the_Tsunami_of_Mobile_Data.html">1362 high scalability-2012-11-26-BigData using Erlang, C and Lisp to Fight the Tsunami of Mobile Data</a></p>
<p>10 0.17210925 <a title="1456-tfidf-10" href="../high_scalability-2013/high_scalability-2013-03-18-Beyond_Threads_and_Callbacks_-_Application_Architecture_Pros_and_Cons.html">1425 high scalability-2013-03-18-Beyond Threads and Callbacks - Application Architecture Pros and Cons</a></p>
<p>11 0.17128894 <a title="1456-tfidf-11" href="../high_scalability-2008/high_scalability-2008-12-13-Strategy%3A_Facebook_Tweaks_to_Handle_6_Time_as_Many_Memcached_Requests.html">464 high scalability-2008-12-13-Strategy: Facebook Tweaks to Handle 6 Time as Many Memcached Requests</a></p>
<p>12 0.16749999 <a title="1456-tfidf-12" href="../high_scalability-2009/high_scalability-2009-06-23-Learn_How_to_Exploit_Multiple_Cores_for_Better_Performance_and_Scalability.html">636 high scalability-2009-06-23-Learn How to Exploit Multiple Cores for Better Performance and Scalability</a></p>
<p>13 0.16730869 <a title="1456-tfidf-13" href="../high_scalability-2013/high_scalability-2013-10-23-Strategy%3A_Use_Linux_Taskset_to_Pin_Processes_or_Let_the_OS_Schedule_It%3F.html">1536 high scalability-2013-10-23-Strategy: Use Linux Taskset to Pin Processes or Let the OS Schedule It?</a></p>
<p>14 0.15911457 <a title="1456-tfidf-14" href="../high_scalability-2013/high_scalability-2013-11-19-We_Finally_Cracked_the_10K_Problem_-_This_Time_for_Managing_Servers_with_2000x_Servers_Managed_Per_Sysadmin.html">1550 high scalability-2013-11-19-We Finally Cracked the 10K Problem - This Time for Managing Servers with 2000x Servers Managed Per Sysadmin</a></p>
<p>15 0.15711553 <a title="1456-tfidf-15" href="../high_scalability-2010/high_scalability-2010-04-12-Poppen.de_Architecture.html">808 high scalability-2010-04-12-Poppen.de Architecture</a></p>
<p>16 0.156956 <a title="1456-tfidf-16" href="../high_scalability-2013/high_scalability-2013-06-13-Busting_4_Modern_Hardware_Myths_-_Are_Memory%2C_HDDs%2C_and_SSDs_Really_Random_Access%3F.html">1475 high scalability-2013-06-13-Busting 4 Modern Hardware Myths - Are Memory, HDDs, and SSDs Really Random Access?</a></p>
<p>17 0.15331006 <a title="1456-tfidf-17" href="../high_scalability-2009/high_scalability-2009-03-16-Are_Cloud_Based_Memory_Architectures_the_Next_Big_Thing%3F.html">538 high scalability-2009-03-16-Are Cloud Based Memory Architectures the Next Big Thing?</a></p>
<p>18 0.15257204 <a title="1456-tfidf-18" href="../high_scalability-2009/high_scalability-2009-06-30-Hot_New_Trend%3A_Linking_Clouds_Through_Cheap_IP_VPNs_Instead_of_Private_Lines_.html">645 high scalability-2009-06-30-Hot New Trend: Linking Clouds Through Cheap IP VPNs Instead of Private Lines </a></p>
<p>19 0.15256235 <a title="1456-tfidf-19" href="../high_scalability-2013/high_scalability-2013-08-13-In_Memoriam%3A_Lavabit_Architecture_-_Creating_a_Scalable_Email_Service.html">1501 high scalability-2013-08-13-In Memoriam: Lavabit Architecture - Creating a Scalable Email Service</a></p>
<p>20 0.15114215 <a title="1456-tfidf-20" href="../high_scalability-2013/high_scalability-2013-02-27-42_Monster_Problems_that_Attack_as_Loads_Increase.html">1413 high scalability-2013-02-27-42 Monster Problems that Attack as Loads Increase</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.28), (1, 0.147), (2, -0.022), (3, -0.054), (4, -0.038), (5, 0.013), (6, 0.089), (7, 0.141), (8, -0.145), (9, -0.018), (10, -0.02), (11, -0.051), (12, 0.05), (13, 0.037), (14, -0.097), (15, -0.018), (16, 0.039), (17, 0.021), (18, -0.069), (19, 0.021), (20, 0.036), (21, -0.01), (22, -0.052), (23, -0.026), (24, 0.092), (25, -0.011), (26, 0.022), (27, 0.014), (28, 0.082), (29, 0.043), (30, 0.039), (31, -0.013), (32, -0.008), (33, -0.037), (34, 0.063), (35, 0.039), (36, 0.01), (37, 0.096), (38, 0.037), (39, 0.097), (40, -0.013), (41, 0.019), (42, -0.002), (43, -0.038), (44, 0.014), (45, 0.048), (46, -0.059), (47, 0.01), (48, -0.017), (49, 0.041)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95228791 <a title="1456-lsi-1" href="../high_scalability-2013/high_scalability-2013-05-13-The_Secret_to_10_Million_Concurrent_Connections_-The_Kernel_is_the_Problem%2C_Not_the_Solution.html">1456 high scalability-2013-05-13-The Secret to 10 Million Concurrent Connections -The Kernel is the Problem, Not the Solution</a></p>
<p>Introduction: Now that we have the    C10K concurrent connection problem    licked, how do we level up and support 10 million concurrent connections? Impossible you say. Nope, systems right now are delivering 10 million concurrent connections using techniques that are as radical as they may be unfamiliar. 
   To learn how it’s done we turn to    Robert Graham   , CEO of Errata Security, and his absolutely fantastic talk at    Shmoocon 2013    called    C10M Defending The Internet At Scale   . 
  Robert has a brilliant way of framing the problem that I’ve never heard of before. He starts with a little bit of history, relating how Unix wasn’t originally designed to be a general server OS, it was designed to be a control system for a telephone network. It was the telephone network that actually transported the data so there was a clean separation between the control plane and the data plane. The  problem is we now use Unix servers as part of the data plane , which we shouldn’t do at all. If we were des</p><p>2 0.91699541 <a title="1456-lsi-2" href="../high_scalability-2012/high_scalability-2012-09-10-Russ%E2%80%99_10_Ingredient_Recipe_for_Making_1_Million_TPS_on_%245K_Hardware.html">1319 high scalability-2012-09-10-Russ’ 10 Ingredient Recipe for Making 1 Million TPS on $5K Hardware</a></p>
<p>Introduction: My name is  Russell Sullivan , I am the author of AlchemyDB: a highly flexible NoSQL/SQL/DocumentStore/GraphDB-datastore built on top of redis. I have spent the last several years trying to find a way to sanely house multiple datastore-genres under one roof while (almost paradoxically) pushing performance to its limits.    I recently joined the NoSQL company    Aerospike    (formerly Citrusleaf) with the goal of incrementally grafting AlchemyDB’s flexible data-modeling capabilities onto Aerospike’s high-velocity horizontally-scalable key-value data-fabric. We recently completed a peak-performance    TPS optimization project   : starting at 200K TPS, pushing to the recent community edition launch at 500K TPS, and finally arriving at our 2012 goal:    1M TPS on $5K hardware   .    Getting to one million over-the-wire client-server database-requests per-second on a single machine costing $5K is a balance between trimming overhead on many axes and using a shared nothing architecture to   i</p><p>3 0.86286217 <a title="1456-lsi-3" href="../high_scalability-2013/high_scalability-2013-10-23-Strategy%3A_Use_Linux_Taskset_to_Pin_Processes_or_Let_the_OS_Schedule_It%3F.html">1536 high scalability-2013-10-23-Strategy: Use Linux Taskset to Pin Processes or Let the OS Schedule It?</a></p>
<p>Introduction: This question comes from Ulysses on an  interesting thread  from the Mechanical Sympathy news group, especially given how multiple processors are now the norm:
 
Ulysses:
   
 On an 8xCPU Linux instance,  is it at all advantageous to use the Linux taskset command to pin an 8xJVM process set (co-ordinated as a www.infinispan.org distributed cache/data grid) to a specific CPU affinity set  (i.e. pin JVM0 process to CPU 0, JVM1 process to CPU1, ...., JVM7process to CPU 7) vs. just letting the Linux OS use its default mechanism for provisioning the 8xJVM process set to the available CPUs? 
 In effrort to seek an optimal point (in the full event space), what are the conceptual trade-offs in considering "searching" each permutation of provisioning an 8xJVM process set to an 8xCPU set via taskset? 
   
Given  taskset  is they key to the question, it would help to have a definition:
  

Used to set or retrieve the CPU affinity of a running process given its PID or to launch a new COMMAND with</p><p>4 0.8621189 <a title="1456-lsi-4" href="../high_scalability-2013/high_scalability-2013-06-19-Paper%3A_MegaPipe%3A_A_New_Programming_Interface_for_Scalable_Network_I-O.html">1478 high scalability-2013-06-19-Paper: MegaPipe: A New Programming Interface for Scalable Network I-O</a></p>
<p>Introduction: The paper  MegaPipe: A New Programming Interface for Scalable Network I/O  ( video ,  slides ) hits the common theme that if you want to go faster you need a better car design, not just a better driver. So that's why the authors started with a clean-slate and designed a network API from the ground up with support for concurrent I/O, a requirement for achieving high performance while scaling to large numbers of connections per thread, multiple cores, etc.  What they created is MegaPipe, "a new network programming API for message-oriented workloads to avoid the performance issues of BSD Socket API."
 
The result: MegaPipe outperforms baseline Linux between  29% (for long connections)  and  582% (for short connections) . MegaPipe improves the performance of a modiﬁed version of  memcached between 15% and 320% . For a workload based on real-world HTTP traces, MegaPipe boosts the throughput of  nginx by 75% .
 
What's this most excellent and interesting paper about?
  Message-oriented netwo</p><p>5 0.84757769 <a title="1456-lsi-5" href="../high_scalability-2014/high_scalability-2014-05-07-Update_on_Disqus%3A_It%27s_Still_About_Realtime%2C_But_Go_Demolishes_Python.html">1644 high scalability-2014-05-07-Update on Disqus: It's Still About Realtime, But Go Demolishes Python</a></p>
<p>Introduction: Our last article on Disqus:  How Disqus Went Realtime With 165K Messages Per Second And Less Than .2 Seconds Latency , was a little out of date, but the folks at Disqus have been busy implementing, not talking, so we don't know a lot about what they are doing now, but we do have a short update in  C1MM and NGINX  by John Watson and an article  Trying out this Go thing .
 
So Disqus has grown a bit:
  
 1.3 billion unique visitors 
 10 billion page views 
 500 million users engaged in discussions 
 3 million communities 
 25 million comments 
  
They are still all about realtime, but Go replaced Python in their Realtime system:
  
 Original Realtime backend was written in a pretty lightweight Python + gevent. 
 The realtime service is a hybrid of CPU intensive tasks + lots of network IO. Gevent was handling the network IO without an issue, but at higher contention, the CPU was choking everything. Switching over to Go removed that contention, which was the primary issue that was being se</p><p>6 0.79683656 <a title="1456-lsi-6" href="../high_scalability-2012/high_scalability-2012-08-30-Dramatically_Improving_Performance_by_Debugging_Brutally_Complex_Prolems.html">1314 high scalability-2012-08-30-Dramatically Improving Performance by Debugging Brutally Complex Prolems</a></p>
<p>7 0.77716476 <a title="1456-lsi-7" href="../high_scalability-2012/high_scalability-2012-03-29-Strategy%3A_Exploit_Processor_Affinity_for_High_and_Predictable_Performance.html">1218 high scalability-2012-03-29-Strategy: Exploit Processor Affinity for High and Predictable Performance</a></p>
<p>8 0.76865321 <a title="1456-lsi-8" href="../high_scalability-2010/high_scalability-2010-10-04-Paper%3A_An_Analysis_of_Linux_Scalability_to_Many_Cores__.html">914 high scalability-2010-10-04-Paper: An Analysis of Linux Scalability to Many Cores  </a></p>
<p>9 0.76348597 <a title="1456-lsi-9" href="../high_scalability-2012/high_scalability-2012-05-02-12_Ways_to_Increase_Throughput_by_32X_and_Reduce_Latency_by__20X.html">1237 high scalability-2012-05-02-12 Ways to Increase Throughput by 32X and Reduce Latency by  20X</a></p>
<p>10 0.75611669 <a title="1456-lsi-10" href="../high_scalability-2009/high_scalability-2009-06-23-Learn_How_to_Exploit_Multiple_Cores_for_Better_Performance_and_Scalability.html">636 high scalability-2009-06-23-Learn How to Exploit Multiple Cores for Better Performance and Scalability</a></p>
<p>11 0.75467402 <a title="1456-lsi-11" href="../high_scalability-2014/high_scalability-2014-05-06-The_Quest_for_Database_Scale%3A_the_1_M_TPS_challenge_-_Three_Design_Points_and_Five_common_Bottlenecks_to_avoid.html">1643 high scalability-2014-05-06-The Quest for Database Scale: the 1 M TPS challenge - Three Design Points and Five common Bottlenecks to avoid</a></p>
<p>12 0.75108683 <a title="1456-lsi-12" href="../high_scalability-2012/high_scalability-2012-02-17-Stuff_The_Internet_Says_On_Scalability_For_February_17%2C_2012.html">1195 high scalability-2012-02-17-Stuff The Internet Says On Scalability For February 17, 2012</a></p>
<p>13 0.7421369 <a title="1456-lsi-13" href="../high_scalability-2009/high_scalability-2009-02-01-More_Chips_Means_Less_Salsa.html">505 high scalability-2009-02-01-More Chips Means Less Salsa</a></p>
<p>14 0.73900944 <a title="1456-lsi-14" href="../high_scalability-2014/high_scalability-2014-02-05-Little%E2%80%99s_Law%2C_Scalability_and_Fault_Tolerance%3A_The_OS_is_your_bottleneck._What_you_can_do%3F.html">1591 high scalability-2014-02-05-Little’s Law, Scalability and Fault Tolerance: The OS is your bottleneck. What you can do?</a></p>
<p>15 0.72461742 <a title="1456-lsi-15" href="../high_scalability-2011/high_scalability-2011-05-27-Stuff_The_Internet_Says_On_Scalability_For_May_27%2C_2011.html">1048 high scalability-2011-05-27-Stuff The Internet Says On Scalability For May 27, 2011</a></p>
<p>16 0.72458798 <a title="1456-lsi-16" href="../high_scalability-2008/high_scalability-2008-12-13-Strategy%3A_Facebook_Tweaks_to_Handle_6_Time_as_Many_Memcached_Requests.html">464 high scalability-2008-12-13-Strategy: Facebook Tweaks to Handle 6 Time as Many Memcached Requests</a></p>
<p>17 0.7236954 <a title="1456-lsi-17" href="../high_scalability-2013/high_scalability-2013-02-27-42_Monster_Problems_that_Attack_as_Loads_Increase.html">1413 high scalability-2013-02-27-42 Monster Problems that Attack as Loads Increase</a></p>
<p>18 0.72281772 <a title="1456-lsi-18" href="../high_scalability-2014/high_scalability-2014-01-03-Stuff_The_Internet_Says_On_Scalability_For_January_3rd%2C_2014.html">1572 high scalability-2014-01-03-Stuff The Internet Says On Scalability For January 3rd, 2014</a></p>
<p>19 0.71671462 <a title="1456-lsi-19" href="../high_scalability-2014/high_scalability-2014-03-31-How_WhatsApp_Grew_to_Nearly_500_Million_Users%2C_11%2C000_cores%2C_and_70_Million_Messages_a_Second.html">1622 high scalability-2014-03-31-How WhatsApp Grew to Nearly 500 Million Users, 11,000 cores, and 70 Million Messages a Second</a></p>
<p>20 0.716178 <a title="1456-lsi-20" href="../high_scalability-2009/high_scalability-2009-05-19-Scaling_Memcached%3A_500%2C000%2B_Operations-Second_with_a_Single-Socket_UltraSPARC_T2.html">603 high scalability-2009-05-19-Scaling Memcached: 500,000+ Operations-Second with a Single-Socket UltraSPARC T2</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.087), (2, 0.311), (10, 0.084), (30, 0.044), (40, 0.017), (43, 0.011), (47, 0.015), (61, 0.068), (77, 0.035), (79, 0.133), (85, 0.018), (94, 0.047)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.99023575 <a title="1456-lda-1" href="../high_scalability-2012/high_scalability-2012-03-12-Google%3A_Taming_the_Long_Latency_Tail_-_When_More_Machines_Equals_Worse_Results.html">1207 high scalability-2012-03-12-Google: Taming the Long Latency Tail - When More Machines Equals Worse Results</a></p>
<p>Introduction: Likewise the current belief that, in the case of artificial machines the very large and the very small are equally feasible and lasting is a manifest error. Thus, for example, a small obelisk or column or other solid figure can certainly be laid down or set up without danger of breaking, while the large ones will go to pieces under the slightest provocation, and that purely on account of their own weight. -- Galileo  
Galileo observed how things broke if they were naively scaled up. Interestingly, Google noticed a similar pattern when building larger software systems using the same techniques used to build smaller systems. 
 
 Luiz André Barroso , Distinguished Engineer at Google, talks about this fundamental property of scaling systems in his fascinating talk,  Warehouse-Scale Computing: Entering the Teenage Decade . Google found the larger the scale the greater the impact of latency variability. When a request is implemented by work done in parallel, as is common with today's service</p><p>2 0.98698074 <a title="1456-lda-2" href="../high_scalability-2013/high_scalability-2013-06-13-Busting_4_Modern_Hardware_Myths_-_Are_Memory%2C_HDDs%2C_and_SSDs_Really_Random_Access%3F.html">1475 high scalability-2013-06-13-Busting 4 Modern Hardware Myths - Are Memory, HDDs, and SSDs Really Random Access?</a></p>
<p>Introduction: "It’s all a numbers game – the dirty little secret of scalable systems" 
      
 Martin Thompson  is a High Performance Computing Specialist with a real mission to teach programmers how to understand the innards of modern computing systems. He has many talks and classes (listed below) on caches, buffers, memory controllers, processor architectures, cache lines, etc.
 
His thought is programmers do not put a proper value on understanding how the underpinnings of our systems work. We gravitate to the shiny and trendy. His approach is not to teach people specific programming strategies, but to teach programmers to fish so they can feed themselves. Without a real understanding strategies are easy to apply wrongly.  It's strange how programmers will put a lot of effort into understanding complicated frameworks like Hibernate, but little effort into understanding the underlying hardware on which their programs run.
 
A major tenant of Martin's approach is to "lead by experimental observation</p><p>3 0.98650539 <a title="1456-lda-3" href="../high_scalability-2012/high_scalability-2012-06-18-Google_on_Latency_Tolerant_Systems%3A_Making_a_Predictable_Whole_Out_of_Unpredictable_Parts___.html">1266 high scalability-2012-06-18-Google on Latency Tolerant Systems: Making a Predictable Whole Out of Unpredictable Parts   </a></p>
<p>Introduction: In   Taming The Long Latency Tail   we covered   Luiz Barroso  ’s exploration of the long tail latency (some operations are really slow) problems generated by large fanout architectures (a request is composed of potentially thousands of other requests). You may have noticed there weren’t a lot of solutions. That’s where a talk I attended,   Achieving Rapid Response Times in Large Online Services   (  slide deck  ), by  Jeff Dean , also of Google, comes in:
  
  In this talk, I’ll describe a collection of techniques and practices lowering response times in large distributed systems whose components run on shared clusters of machines, where pieces of these systems are subject to interference by other tasks, and where unpredictable latency hiccups are the norm, not the exception. 

  
 The goal is to use software techniques to reduce variability given the increasing variability in underlying hardware, the need to handle dynamic workloads on a shared infrastructure, and the need to use lar</p><p>4 0.98623234 <a title="1456-lda-4" href="../high_scalability-2013/high_scalability-2013-03-06-Low_Level_Scalability_Solutions_-_The_Aggregation_Collection.html">1418 high scalability-2013-03-06-Low Level Scalability Solutions - The Aggregation Collection</a></p>
<p>Introduction: What good are problems without solutions? In  42 Monster Problems That Attack As Loads Increase  we talked about problems. In this first post (OK, there was an earlier post, but I'm doing some reorganizing), we'll cover what I call  aggregation  strategies.
 
Keep in mind these are low level architecture type suggestions of how to structure the components of your code and how they interact. We're not talking about massive scale-out clusters here, but of what your applications might like like internally, way below the service level interface level. There's a lot more to the world than evented architectures.
 
Aggregation simply means we aren't using stupid queues. Our queues will be smart. We are deeply aware of queues as containers of work that eventually dictate how the entire system performs. As work containers we know intimately what requests and data sit in our queues and we can use that intelligence to our great advantage.
  Prioritize Work  
The key idea to it all is an almost mi</p><p>5 0.98419726 <a title="1456-lda-5" href="../high_scalability-2009/high_scalability-2009-03-11-The_Implications_of_Punctuated_Scalabilium_for_Website_Architecture.html">533 high scalability-2009-03-11-The Implications of Punctuated Scalabilium for Website Architecture</a></p>
<p>Introduction: Update:     How do you design and handle peak load on the Cloud?   by Cloudiquity. Gives a formula to try and predict and plan for peak load and talks about how GigaSpaces XAP, Scalr, RightScale and FreedomOSS can be used to handle peak load within EC2.     Theo Schlossnagle, with his usual insight, talks about in   Dissecting today's surges   how the nature of internet traffic has evolved over time. Traffic now spikes like a heart attack, larger and more quickly than ever from traffic inflow sources like Digg and The New York Times. Theo relates how   At least eight times in the past month, we've experienced from 100% to 1000% sudden increases in traffic across many of our clients   and those spike can happen as quickly as 60 seconds. To me this sounds a lot like   Punctuated equilibrium   in evolution, a force that accounts for much creative growth in species...       VMs don't spin up in less than 60 seconds so your ability to respond to such massive quick spikes is limited. This as</p><p>same-blog 6 0.98388815 <a title="1456-lda-6" href="../high_scalability-2013/high_scalability-2013-05-13-The_Secret_to_10_Million_Concurrent_Connections_-The_Kernel_is_the_Problem%2C_Not_the_Solution.html">1456 high scalability-2013-05-13-The Secret to 10 Million Concurrent Connections -The Kernel is the Problem, Not the Solution</a></p>
<p>7 0.98332614 <a title="1456-lda-7" href="../high_scalability-2011/high_scalability-2011-03-24-Strategy%3A_Disk_Backup_for_Speed%2C_Tape_Backup_to_Save_Your_Bacon%2C_Just_Ask_Google.html">1010 high scalability-2011-03-24-Strategy: Disk Backup for Speed, Tape Backup to Save Your Bacon, Just Ask Google</a></p>
<p>8 0.9827314 <a title="1456-lda-8" href="../high_scalability-2012/high_scalability-2012-03-06-Ask_For_Forgiveness_Programming_-_Or_How_We%27ll_Program_1000_Cores.html">1204 high scalability-2012-03-06-Ask For Forgiveness Programming - Or How We'll Program 1000 Cores</a></p>
<p>9 0.98226231 <a title="1456-lda-9" href="../high_scalability-2013/high_scalability-2013-03-11-Low_Level_Scalability_Solutions_-_The_Conditioning_Collection.html">1421 high scalability-2013-03-11-Low Level Scalability Solutions - The Conditioning Collection</a></p>
<p>10 0.98181558 <a title="1456-lda-10" href="../high_scalability-2009/high_scalability-2009-07-25-Latency_is_Everywhere_and_it_Costs_You_Sales_-_How_to_Crush_it.html">661 high scalability-2009-07-25-Latency is Everywhere and it Costs You Sales - How to Crush it</a></p>
<p>11 0.9812178 <a title="1456-lda-11" href="../high_scalability-2007/high_scalability-2007-08-29-Skype_Failed_the_Boot_Scalability_Test%3A_Is_P2P_fundamentally_flawed%3F.html">76 high scalability-2007-08-29-Skype Failed the Boot Scalability Test: Is P2P fundamentally flawed?</a></p>
<p>12 0.98117143 <a title="1456-lda-12" href="../high_scalability-2013/high_scalability-2013-02-27-42_Monster_Problems_that_Attack_as_Loads_Increase.html">1413 high scalability-2013-02-27-42 Monster Problems that Attack as Loads Increase</a></p>
<p>13 0.97898346 <a title="1456-lda-13" href="../high_scalability-2010/high_scalability-2010-12-20-Netflix%3A_Use_Less_Chatty_Protocols_in_the_Cloud_-_Plus_26_Fixes.html">960 high scalability-2010-12-20-Netflix: Use Less Chatty Protocols in the Cloud - Plus 26 Fixes</a></p>
<p>14 0.97865707 <a title="1456-lda-14" href="../high_scalability-2013/high_scalability-2013-03-18-Beyond_Threads_and_Callbacks_-_Application_Architecture_Pros_and_Cons.html">1425 high scalability-2013-03-18-Beyond Threads and Callbacks - Application Architecture Pros and Cons</a></p>
<p>15 0.97689241 <a title="1456-lda-15" href="../high_scalability-2008/high_scalability-2008-01-24-Mailinator_Architecture.html">221 high scalability-2008-01-24-Mailinator Architecture</a></p>
<p>16 0.97685158 <a title="1456-lda-16" href="../high_scalability-2013/high_scalability-2013-01-15-More_Numbers_Every_Awesome_Programmer_Must_Know.html">1387 high scalability-2013-01-15-More Numbers Every Awesome Programmer Must Know</a></p>
<p>17 0.9764241 <a title="1456-lda-17" href="../high_scalability-2013/high_scalability-2013-04-10-Check_Yourself_Before_You_Wreck_Yourself_-_Avocado%27s_5_Early_Stages_of_Architecture_Evolution.html">1438 high scalability-2013-04-10-Check Yourself Before You Wreck Yourself - Avocado's 5 Early Stages of Architecture Evolution</a></p>
<p>18 0.97640419 <a title="1456-lda-18" href="../high_scalability-2013/high_scalability-2013-07-15-Ask_HS%3A_What%27s_Wrong_with_Twitter%2C_Why_Isn%27t_One_Machine_Enough%3F.html">1491 high scalability-2013-07-15-Ask HS: What's Wrong with Twitter, Why Isn't One Machine Enough?</a></p>
<p>19 0.97588122 <a title="1456-lda-19" href="../high_scalability-2013/high_scalability-2013-12-13-Stuff_The_Internet_Says_On_Scalability_For_December_13th%2C_2013.html">1564 high scalability-2013-12-13-Stuff The Internet Says On Scalability For December 13th, 2013</a></p>
<p>20 0.97574055 <a title="1456-lda-20" href="../high_scalability-2014/high_scalability-2014-02-26-The_WhatsApp_Architecture_Facebook_Bought_For_%2419_Billion.html">1602 high scalability-2014-02-26-The WhatsApp Architecture Facebook Bought For $19 Billion</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
