<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1548 high scalability-2013-11-13-Google: Multiplex Multiple Works Loads on Computers to Increase Machine Utilization and Save Money</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2013" href="../home/high_scalability-2013_home.html">high_scalability-2013</a> <a title="high_scalability-2013-1548" href="#">high_scalability-2013-1548</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1548 high scalability-2013-11-13-Google: Multiplex Multiple Works Loads on Computers to Increase Machine Utilization and Save Money</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2013-1548-html" href="http://highscalability.com//blog/2013/11/13/google-multiplex-multiple-works-loads-on-computers-to-increa.html">html</a></p><p>Introduction: Jeff Dean  gave a talk at   SFBay ACM   and at about 3 minutes in he goes over how Google runs jobs on computers, which is different than how most shops distribute workloads.
   It’s common for machines to be dedicated to one service, say run a database, run a cache, run this, or run that. The logic is: 
  
 
  Better control over responsiveness as you generally know the traffic loads a machine will experience and you can over provision a box to be safe. 

 
 
  Easier to manage, load balance, configure, upgrade, create and make highly available. Since you know what a machine does another machine can be provisioned to do the same work. 

 
    The problem is monocropping hardware though conceptually clean for humans and safe for applications, is hugely wasteful. Machines are woefully underutilized, even in a virtualized world. 
  What Google does is use a  shared environment  in a datacenter where all kinds of stuff run on each computer. Batch computation and interactive computations a</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Jeff Dean  gave a talk at   SFBay ACM   and at about 3 minutes in he goes over how Google runs jobs on computers, which is different than how most shops distribute workloads. [sent-1, score-0.271]
</p><p>2 It’s common for machines to be dedicated to one service, say run a database, run a cache, run this, or run that. [sent-2, score-0.681]
</p><p>3 The logic is:         Better control over responsiveness as you generally know the traffic loads a machine will experience and you can over provision a box to be safe. [sent-3, score-0.339]
</p><p>4 Since you know what a machine does another machine can be provisioned to do the same work. [sent-5, score-0.593]
</p><p>5 The problem is monocropping hardware though conceptually clean for humans and safe for applications, is hugely wasteful. [sent-6, score-0.701]
</p><p>6 Machines are woefully underutilized, even in a virtualized world. [sent-7, score-0.114]
</p><p>7 What Google does is use a  shared environment  in a datacenter where all kinds of stuff run on each computer. [sent-8, score-0.146]
</p><p>8 Batch computation and interactive computations all run together on the same machine. [sent-9, score-0.327]
</p><p>9 Each machine has (or may have): Linux, file system chunkserver, scheduling system, other system services, random application and higher level system services like Bigtable tablet server, a CPU intensive job, one ore more MapReduce jobs, and more random applications. [sent-10, score-1.067]
</p><p>10 The benefit giving a machine lots to do is greatly  increased utilization . [sent-11, score-0.446]
</p><p>11 Machines these days are like race horses pulling a plow. [sent-12, score-0.277]
</p><p>12 Machine monocropping like big agriculture monocropping is a relic from a past era. [sent-14, score-0.898]
</p><p>13 So we see here a parallel in the computer world with the permaculture revolution taking over our food production system. [sent-15, score-0.121]
</p><p>14 The downside to the increased variability of running multiple workloads on a machine is it’s constant change which to humans  appears as chaos  and we humans do not like chaos. [sent-17, score-0.721]
</p><p>15 You have jobs running in the foreground and background, constantly changing, bursting in CPU, memory, disk, and network usage, so nobody has guarantees which means you have unpredictable performance. [sent-18, score-0.557]
</p><p>16 For interactive jobs, those where a user wants a response fast, it's especially troublesome. [sent-19, score-0.205]
</p><p>17 With large fanout systems, which means architectures that encourage hundreds or thousands of different services to be contacted to service a request, the result can be a  high variability of latency . [sent-20, score-0.495]
</p><p>18 Google wants high machine utilization for its higher power efficiency and less money spent on expensive servers. [sent-21, score-0.58]
</p><p>19 So Google needed to go about  solving all the problems  generated by higher machine utilization, which as you might imagine involves lots of cool solutions. [sent-22, score-0.436]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('monocropping', 0.401), ('machine', 0.259), ('jobs', 0.181), ('variability', 0.165), ('groups', 0.161), ('unpredictable', 0.154), ('run', 0.146), ('chunkserver', 0.121), ('horses', 0.121), ('deangave', 0.121), ('ore', 0.121), ('permaculture', 0.121), ('cgroups', 0.121), ('foreground', 0.121), ('utilization', 0.119), ('woefully', 0.114), ('environmentin', 0.114), ('humans', 0.114), ('interactive', 0.112), ('higher', 0.109), ('ahigh', 0.109), ('ingoogle', 0.101), ('bursting', 0.101), ('machines', 0.097), ('random', 0.096), ('agriculture', 0.096), ('contacted', 0.094), ('hugely', 0.094), ('wants', 0.093), ('conceptually', 0.092), ('tablet', 0.092), ('underutilized', 0.092), ('fanout', 0.092), ('shops', 0.09), ('assigning', 0.089), ('pulling', 0.085), ('cpu', 0.084), ('responsiveness', 0.08), ('google', 0.077), ('provisioned', 0.075), ('linux', 0.074), ('system', 0.074), ('articleson', 0.074), ('encourage', 0.072), ('services', 0.072), ('race', 0.071), ('tolerant', 0.07), ('computations', 0.069), ('downside', 0.069), ('lots', 0.068)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999982 <a title="1548-tfidf-1" href="../high_scalability-2013/high_scalability-2013-11-13-Google%3A_Multiplex_Multiple_Works_Loads_on_Computers_to_Increase_Machine_Utilization_and_Save_Money.html">1548 high scalability-2013-11-13-Google: Multiplex Multiple Works Loads on Computers to Increase Machine Utilization and Save Money</a></p>
<p>Introduction: Jeff Dean  gave a talk at   SFBay ACM   and at about 3 minutes in he goes over how Google runs jobs on computers, which is different than how most shops distribute workloads.
   It’s common for machines to be dedicated to one service, say run a database, run a cache, run this, or run that. The logic is: 
  
 
  Better control over responsiveness as you generally know the traffic loads a machine will experience and you can over provision a box to be safe. 

 
 
  Easier to manage, load balance, configure, upgrade, create and make highly available. Since you know what a machine does another machine can be provisioned to do the same work. 

 
    The problem is monocropping hardware though conceptually clean for humans and safe for applications, is hugely wasteful. Machines are woefully underutilized, even in a virtualized world. 
  What Google does is use a  shared environment  in a datacenter where all kinds of stuff run on each computer. Batch computation and interactive computations a</p><p>2 0.23920378 <a title="1548-tfidf-2" href="../high_scalability-2012/high_scalability-2012-06-18-Google_on_Latency_Tolerant_Systems%3A_Making_a_Predictable_Whole_Out_of_Unpredictable_Parts___.html">1266 high scalability-2012-06-18-Google on Latency Tolerant Systems: Making a Predictable Whole Out of Unpredictable Parts   </a></p>
<p>Introduction: In   Taming The Long Latency Tail   we covered   Luiz Barroso  ’s exploration of the long tail latency (some operations are really slow) problems generated by large fanout architectures (a request is composed of potentially thousands of other requests). You may have noticed there weren’t a lot of solutions. That’s where a talk I attended,   Achieving Rapid Response Times in Large Online Services   (  slide deck  ), by  Jeff Dean , also of Google, comes in:
  
  In this talk, I’ll describe a collection of techniques and practices lowering response times in large distributed systems whose components run on shared clusters of machines, where pieces of these systems are subject to interference by other tasks, and where unpredictable latency hiccups are the norm, not the exception. 

  
 The goal is to use software techniques to reduce variability given the increasing variability in underlying hardware, the need to handle dynamic workloads on a shared infrastructure, and the need to use lar</p><p>3 0.14084764 <a title="1548-tfidf-3" href="../high_scalability-2008/high_scalability-2008-11-22-Google_Architecture.html">448 high scalability-2008-11-22-Google Architecture</a></p>
<p>Introduction: Update 2:   Sorting 1 PB with MapReduce . PB is not peanut-butter-and-jelly misspelled. It's 1 petabyte or 1000 terabytes or 1,000,000 gigabytes.  It took six hours and two minutes to sort 1PB (10 trillion 100-byte records) on 4,000 computers  and the results were replicated thrice on 48,000 disks.  Update:   Greg Linden  points to a new Google article  MapReduce: simplified data processing on large clusters . Some interesting stats: 100k MapReduce jobs are executed each day; more than 20 petabytes of data are processed per day; more than 10k MapReduce programs have been implemented; machines are dual processor with gigabit ethernet and 4-8 GB of memory.  Google is the King of scalability.  Everyone knows Google for their large,  sophisticated, and fast searching, but they don't just shine in search. Their platform approach to building scalable applications allows them to roll out internet scale applications at an alarmingly high competition crushing rate. Their goal is always to build</p><p>4 0.12610279 <a title="1548-tfidf-4" href="../high_scalability-2009/high_scalability-2009-03-16-Are_Cloud_Based_Memory_Architectures_the_Next_Big_Thing%3F.html">538 high scalability-2009-03-16-Are Cloud Based Memory Architectures the Next Big Thing?</a></p>
<p>Introduction: We are on the edge of two potent technological changes: Clouds and Memory Based Architectures. This evolution will rip open a chasm where new players can enter and prosper. Google is the master of disk. You can't beat them at a game they perfected. Disk based databases like SimpleDB and BigTable are complicated beasts, typical last gasp products of any aging technology before a change. The next era is the age of Memory and Cloud which will allow for new players to succeed. The tipping point will be soon.   Let's take a short trip down web architecture lane:
  It's 1993: Yahoo runs on FreeBSD, Apache, Perl scripts and a SQL database   It's 1995: Scale-up the database.   It's 1998: LAMP   It's 1999: Stateless + Load Balanced + Database + SAN   It's 2001: In-memory data-grid.   It's 2003: Add a caching layer.   It's 2004: Add scale-out and partitioning.   It's 2005: Add asynchronous job scheduling and maybe a distributed file system.   It's 2007: Move it all into the cloud.   It's 2008: C</p><p>5 0.12246394 <a title="1548-tfidf-5" href="../high_scalability-2013/high_scalability-2013-02-11-At_Scale_Even_Little_Wins_Pay_Off_Big_-_Google_and_Facebook_Examples.html">1404 high scalability-2013-02-11-At Scale Even Little Wins Pay Off Big - Google and Facebook Examples</a></p>
<p>Introduction: There's a popular line of thought that says don't waste time on optimization because developing features is more important than saving money. True, you can always add resources, but at some point, especially in a more mature part of a product lifecycle: performance equals $$$.   Two great examples of this evolution come from Facebook and Google. The upshot is that when you spend time and money on optimizing your tool chain you can get huge wins in performance, control, and costs. Certainly, don’t bother if you are just starting, but at some point you may want to switch to big development efforts in improving efficiency. 
   Facebook and HipHop   
 The Facebook example is quite well known:    HipHop   , a static PHP compiler released in 2010, after two years of development. PHP because Facebook implements their web tier    in PHP   . They've now developed a dynamic compiler,    HipHop VM   , using techniques like JIT, side exits, HipHop bytecode, type prediction, and parallel tracelet l</p><p>6 0.12202646 <a title="1548-tfidf-6" href="../high_scalability-2008/high_scalability-2008-01-25-Application_Database_and_DAL_Architecture.html">222 high scalability-2008-01-25-Application Database and DAL Architecture</a></p>
<p>7 0.1212903 <a title="1548-tfidf-7" href="../high_scalability-2012/high_scalability-2012-03-12-Google%3A_Taming_the_Long_Latency_Tail_-_When_More_Machines_Equals_Worse_Results.html">1207 high scalability-2012-03-12-Google: Taming the Long Latency Tail - When More Machines Equals Worse Results</a></p>
<p>8 0.11663375 <a title="1548-tfidf-8" href="../high_scalability-2011/high_scalability-2011-05-11-Troubleshooting_response_time_problems_%E2%80%93_why_you_cannot_trust_your_system_metrics.html">1038 high scalability-2011-05-11-Troubleshooting response time problems – why you cannot trust your system metrics</a></p>
<p>9 0.11343125 <a title="1548-tfidf-9" href="../high_scalability-2012/high_scalability-2012-07-02-C_is_for_Compute_-_Google_Compute_Engine_%28GCE%29.html">1275 high scalability-2012-07-02-C is for Compute - Google Compute Engine (GCE)</a></p>
<p>10 0.11237492 <a title="1548-tfidf-10" href="../high_scalability-2009/high_scalability-2009-12-16-Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_in_the_Ambient_Cloud.html">750 high scalability-2009-12-16-Building Super Scalable Systems: Blade Runner Meets Autonomic Computing in the Ambient Cloud</a></p>
<p>11 0.11225718 <a title="1548-tfidf-11" href="../high_scalability-2012/high_scalability-2012-11-05-Gone_Fishin%27%3A_Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_In_The_Ambient_Cloud.html">1355 high scalability-2012-11-05-Gone Fishin': Building Super Scalable Systems: Blade Runner Meets Autonomic Computing In The Ambient Cloud</a></p>
<p>12 0.11125862 <a title="1548-tfidf-12" href="../high_scalability-2008/high_scalability-2008-03-12-YouTube_Architecture.html">274 high scalability-2008-03-12-YouTube Architecture</a></p>
<p>13 0.11108348 <a title="1548-tfidf-13" href="../high_scalability-2009/high_scalability-2009-07-25-Latency_is_Everywhere_and_it_Costs_You_Sales_-_How_to_Crush_it.html">661 high scalability-2009-07-25-Latency is Everywhere and it Costs You Sales - How to Crush it</a></p>
<p>14 0.10470684 <a title="1548-tfidf-14" href="../high_scalability-2009/high_scalability-2009-02-21-Google_AppEngine_-_A_Second_Look.html">517 high scalability-2009-02-21-Google AppEngine - A Second Look</a></p>
<p>15 0.10442292 <a title="1548-tfidf-15" href="../high_scalability-2012/high_scalability-2012-07-30-Prismatic_Architecture_-_Using_Machine_Learning_on_Social_Networks_to_Figure_Out_What_You_Should_Read_on_the_Web_.html">1293 high scalability-2012-07-30-Prismatic Architecture - Using Machine Learning on Social Networks to Figure Out What You Should Read on the Web </a></p>
<p>16 0.10426426 <a title="1548-tfidf-16" href="../high_scalability-2012/high_scalability-2012-04-17-YouTube_Strategy%3A_Adding_Jitter_isn%27t_a_Bug.html">1229 high scalability-2012-04-17-YouTube Strategy: Adding Jitter isn't a Bug</a></p>
<p>17 0.098817162 <a title="1548-tfidf-17" href="../high_scalability-2010/high_scalability-2010-11-22-Strategy%3A_Google_Sends_Canary_Requests_into_the_Data_Mine.html">946 high scalability-2010-11-22-Strategy: Google Sends Canary Requests into the Data Mine</a></p>
<p>18 0.098796546 <a title="1548-tfidf-18" href="../high_scalability-2009/high_scalability-2009-11-06-Product%3A_Resque_-_GitHub%27s_Distrubuted_Job_Queue.html">738 high scalability-2009-11-06-Product: Resque - GitHub's Distrubuted Job Queue</a></p>
<p>19 0.098391496 <a title="1548-tfidf-19" href="../high_scalability-2013/high_scalability-2013-06-26-Leveraging_Cloud_Computing_at_Yelp_-_102_Million_Monthly_Vistors_and_39_Million_Reviews.html">1482 high scalability-2013-06-26-Leveraging Cloud Computing at Yelp - 102 Million Monthly Vistors and 39 Million Reviews</a></p>
<p>20 0.0967279 <a title="1548-tfidf-20" href="../high_scalability-2013/high_scalability-2013-02-27-42_Monster_Problems_that_Attack_as_Loads_Increase.html">1413 high scalability-2013-02-27-42 Monster Problems that Attack as Loads Increase</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.189), (1, 0.083), (2, 0.003), (3, 0.028), (4, -0.064), (5, -0.001), (6, 0.091), (7, 0.074), (8, -0.031), (9, -0.019), (10, -0.014), (11, -0.032), (12, 0.05), (13, -0.015), (14, 0.039), (15, -0.036), (16, -0.048), (17, -0.014), (18, 0.027), (19, 0.033), (20, 0.021), (21, 0.011), (22, 0.022), (23, -0.036), (24, 0.008), (25, 0.063), (26, 0.025), (27, 0.009), (28, -0.071), (29, -0.007), (30, 0.06), (31, -0.027), (32, 0.05), (33, -0.017), (34, 0.002), (35, -0.011), (36, -0.005), (37, -0.042), (38, -0.019), (39, 0.039), (40, 0.004), (41, -0.037), (42, -0.023), (43, -0.06), (44, -0.047), (45, -0.029), (46, 0.006), (47, -0.071), (48, 0.033), (49, -0.046)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.975586 <a title="1548-lsi-1" href="../high_scalability-2013/high_scalability-2013-11-13-Google%3A_Multiplex_Multiple_Works_Loads_on_Computers_to_Increase_Machine_Utilization_and_Save_Money.html">1548 high scalability-2013-11-13-Google: Multiplex Multiple Works Loads on Computers to Increase Machine Utilization and Save Money</a></p>
<p>Introduction: Jeff Dean  gave a talk at   SFBay ACM   and at about 3 minutes in he goes over how Google runs jobs on computers, which is different than how most shops distribute workloads.
   It’s common for machines to be dedicated to one service, say run a database, run a cache, run this, or run that. The logic is: 
  
 
  Better control over responsiveness as you generally know the traffic loads a machine will experience and you can over provision a box to be safe. 

 
 
  Easier to manage, load balance, configure, upgrade, create and make highly available. Since you know what a machine does another machine can be provisioned to do the same work. 

 
    The problem is monocropping hardware though conceptually clean for humans and safe for applications, is hugely wasteful. Machines are woefully underutilized, even in a virtualized world. 
  What Google does is use a  shared environment  in a datacenter where all kinds of stuff run on each computer. Batch computation and interactive computations a</p><p>2 0.85374576 <a title="1548-lsi-2" href="../high_scalability-2012/high_scalability-2012-06-18-Google_on_Latency_Tolerant_Systems%3A_Making_a_Predictable_Whole_Out_of_Unpredictable_Parts___.html">1266 high scalability-2012-06-18-Google on Latency Tolerant Systems: Making a Predictable Whole Out of Unpredictable Parts   </a></p>
<p>Introduction: In   Taming The Long Latency Tail   we covered   Luiz Barroso  ’s exploration of the long tail latency (some operations are really slow) problems generated by large fanout architectures (a request is composed of potentially thousands of other requests). You may have noticed there weren’t a lot of solutions. That’s where a talk I attended,   Achieving Rapid Response Times in Large Online Services   (  slide deck  ), by  Jeff Dean , also of Google, comes in:
  
  In this talk, I’ll describe a collection of techniques and practices lowering response times in large distributed systems whose components run on shared clusters of machines, where pieces of these systems are subject to interference by other tasks, and where unpredictable latency hiccups are the norm, not the exception. 

  
 The goal is to use software techniques to reduce variability given the increasing variability in underlying hardware, the need to handle dynamic workloads on a shared infrastructure, and the need to use lar</p><p>3 0.78718621 <a title="1548-lsi-3" href="../high_scalability-2012/high_scalability-2012-03-12-Google%3A_Taming_the_Long_Latency_Tail_-_When_More_Machines_Equals_Worse_Results.html">1207 high scalability-2012-03-12-Google: Taming the Long Latency Tail - When More Machines Equals Worse Results</a></p>
<p>Introduction: Likewise the current belief that, in the case of artificial machines the very large and the very small are equally feasible and lasting is a manifest error. Thus, for example, a small obelisk or column or other solid figure can certainly be laid down or set up without danger of breaking, while the large ones will go to pieces under the slightest provocation, and that purely on account of their own weight. -- Galileo  
Galileo observed how things broke if they were naively scaled up. Interestingly, Google noticed a similar pattern when building larger software systems using the same techniques used to build smaller systems. 
 
 Luiz André Barroso , Distinguished Engineer at Google, talks about this fundamental property of scaling systems in his fascinating talk,  Warehouse-Scale Computing: Entering the Teenage Decade . Google found the larger the scale the greater the impact of latency variability. When a request is implemented by work done in parallel, as is common with today's service</p><p>4 0.77706391 <a title="1548-lsi-4" href="../high_scalability-2012/high_scalability-2012-07-02-C_is_for_Compute_-_Google_Compute_Engine_%28GCE%29.html">1275 high scalability-2012-07-02-C is for Compute - Google Compute Engine (GCE)</a></p>
<p>Introduction: After poking around the   Google Compute Engine    (GCE) documentation I had some trouble creating a mental model of how GCE works. Is it like AWS, GAE, Rackspace, just what is it? After watching    Google I/O 2012 - Introducing Google Compute Engine    and    Google Compute Engine -- Technical Details   , it turns out my initial impression, that GCE is disarmingly straightforward, turns out to be the point.     The focus of GCE is on the C, which stands for Compute, and that’s what GCE is all about: deploying lots of servers to solve computationally hard problems. What you get with GCE is a Super Datacenter on Google Steroids.     If you are wondering how you will run the next Instagram on GCE then that would be missing the point. GAE is targeted at applications. GCE is targeted at: 
  
  Delivering a proven, pure, high performance, high scale compute infrastructure using a utility pricing model, on top of an open, secure, extensible Infrastructure-as-a-Service.  
  Delivering an expe</p><p>5 0.76297307 <a title="1548-lsi-5" href="../high_scalability-2011/high_scalability-2011-07-07-Myth%3A_Google_Uses_Server_Farms_So_You_Should_Too_-_Resurrection_of_the_Big-Ass_Machines.html">1075 high scalability-2011-07-07-Myth: Google Uses Server Farms So You Should Too - Resurrection of the Big-Ass Machines</a></p>
<p>Introduction: For a long epoch there was a strategy of scaling up by making ever bigger super computers. I had the pleasure of programming on a few large massively multi-processor machines from SGI and DEC. Beautiful, highly specialized machines that were very expensive. These met the double-tap extinction event of Moore's law and a Google inspired era of commodity machine based clusters and extreme software parallelism. Has the tide turned? Does it now make more sense to use big machines instead of clusters?
 
In  Big-Ass Servers™ and the myths of clusters in bioinformatics , Jerm makes the case that for bionformatics, it's more cost effective to buy a Big-Ass Server instead of using a cluster of machines and a lot of specialized parallel programming techniques. It's a classic scale-up argument that has been made more attractive by the recent development of relatively inexpensive large machines.  SeaMicro  has developed a 512 core machine. Dell has a new  96 core server . Supermicro has  48 core  m</p><p>6 0.75798076 <a title="1548-lsi-6" href="../high_scalability-2013/high_scalability-2013-02-11-At_Scale_Even_Little_Wins_Pay_Off_Big_-_Google_and_Facebook_Examples.html">1404 high scalability-2013-02-11-At Scale Even Little Wins Pay Off Big - Google and Facebook Examples</a></p>
<p>7 0.75478405 <a title="1548-lsi-7" href="../high_scalability-2010/high_scalability-2010-11-22-Strategy%3A_Google_Sends_Canary_Requests_into_the_Data_Mine.html">946 high scalability-2010-11-22-Strategy: Google Sends Canary Requests into the Data Mine</a></p>
<p>8 0.73648369 <a title="1548-lsi-8" href="../high_scalability-2010/high_scalability-2010-04-27-Paper%3A__Dapper%2C_Google%27s_Large-Scale_Distributed_Systems_Tracing_Infrastructure.html">815 high scalability-2010-04-27-Paper:  Dapper, Google's Large-Scale Distributed Systems Tracing Infrastructure</a></p>
<p>9 0.7182129 <a title="1548-lsi-9" href="../high_scalability-2009/high_scalability-2009-07-25-Latency_is_Everywhere_and_it_Costs_You_Sales_-_How_to_Crush_it.html">661 high scalability-2009-07-25-Latency is Everywhere and it Costs You Sales - How to Crush it</a></p>
<p>10 0.70942318 <a title="1548-lsi-10" href="../high_scalability-2012/high_scalability-2012-04-17-YouTube_Strategy%3A_Adding_Jitter_isn%27t_a_Bug.html">1229 high scalability-2012-04-17-YouTube Strategy: Adding Jitter isn't a Bug</a></p>
<p>11 0.7085529 <a title="1548-lsi-11" href="../high_scalability-2011/high_scalability-2011-08-29-The_Three_Ages_of_Google_-_Batch%2C_Warehouse%2C_Instant.html">1107 high scalability-2011-08-29-The Three Ages of Google - Batch, Warehouse, Instant</a></p>
<p>12 0.70307612 <a title="1548-lsi-12" href="../high_scalability-2011/high_scalability-2011-03-24-Strategy%3A_Disk_Backup_for_Speed%2C_Tape_Backup_to_Save_Your_Bacon%2C_Just_Ask_Google.html">1010 high scalability-2011-03-24-Strategy: Disk Backup for Speed, Tape Backup to Save Your Bacon, Just Ask Google</a></p>
<p>13 0.70114708 <a title="1548-lsi-13" href="../high_scalability-2012/high_scalability-2012-08-23-Economies_of_Scale_in_the_Datacenter%3A_Gmail_is_100x_Cheaper_to_Run_than_Your_Own_Server.html">1310 high scalability-2012-08-23-Economies of Scale in the Datacenter: Gmail is 100x Cheaper to Run than Your Own Server</a></p>
<p>14 0.70047486 <a title="1548-lsi-14" href="../high_scalability-2011/high_scalability-2011-02-01-Google_Strategy%3A_Tree_Distribution_of_Requests_and_Responses.html">981 high scalability-2011-02-01-Google Strategy: Tree Distribution of Requests and Responses</a></p>
<p>15 0.68958914 <a title="1548-lsi-15" href="../high_scalability-2008/high_scalability-2008-05-25-Product%3A_Condor__-_Compute_Intensive_Workload_Management.html">326 high scalability-2008-05-25-Product: Condor  - Compute Intensive Workload Management</a></p>
<p>16 0.68364406 <a title="1548-lsi-16" href="../high_scalability-2008/high_scalability-2008-01-13-Google_Reveals_New_MapReduce_Stats.html">211 high scalability-2008-01-13-Google Reveals New MapReduce Stats</a></p>
<p>17 0.67926872 <a title="1548-lsi-17" href="../high_scalability-2012/high_scalability-2012-08-30-Stuff_The_Internet_Says_On_Scalability_For_August_31%2C_2012.html">1315 high scalability-2012-08-30-Stuff The Internet Says On Scalability For August 31, 2012</a></p>
<p>18 0.67921305 <a title="1548-lsi-18" href="../high_scalability-2013/high_scalability-2013-02-27-42_Monster_Problems_that_Attack_as_Loads_Increase.html">1413 high scalability-2013-02-27-42 Monster Problems that Attack as Loads Increase</a></p>
<p>19 0.67335343 <a title="1548-lsi-19" href="../high_scalability-2013/high_scalability-2013-10-21-Google%27s_Sanjay_Ghemawat_on_What_Made_Google_Google_and_Great_Big_Data_Career_Advice.html">1535 high scalability-2013-10-21-Google's Sanjay Ghemawat on What Made Google Google and Great Big Data Career Advice</a></p>
<p>20 0.67204487 <a title="1548-lsi-20" href="../high_scalability-2009/high_scalability-2009-01-13-Product%3A_Gearman_-_Open_Source_Message_Queuing_System.html">491 high scalability-2009-01-13-Product: Gearman - Open Source Message Queuing System</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.075), (2, 0.211), (10, 0.041), (27, 0.021), (30, 0.031), (40, 0.031), (61, 0.104), (79, 0.185), (87, 0.197), (94, 0.02)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.92876434 <a title="1548-lda-1" href="../high_scalability-2013/high_scalability-2013-10-02-RFC_1925_-_The_Twelve_%28Timeless%29_Networking_Truths.html">1526 high scalability-2013-10-02-RFC 1925 - The Twelve (Timeless) Networking Truths</a></p>
<p>Introduction: The Twelve Networking Truths  is one of a long series of timeless truths documented in sacred   April Fools'  RFC   s. Though issued in 1996, it's no less true today.
 
It's hard to pick a favorite because they are all good. But if I had to pick, it would be:
  

Some things in life can never be fully appreciated nor understood unless experienced firsthand.

  
As we grow comfortable behind garden walls, clutching gadgets like lifelines and ideologies like shields, empathy is the true social network.
 
 
    Network Working Group                                  R.  Callon , Editor
Request for Comments: 1925                                           IOOF 
Category: Informational                                     1 April 1996


                      The Twelve Networking Truths

Status of this Memo

   This memo provides information for the Internet community.  This memo
   does not specify an Internet standard of any kind.  Distribution of
   this memo is unlimited.

Abstract

   Thi</p><p>2 0.91040719 <a title="1548-lda-2" href="../high_scalability-2008/high_scalability-2008-09-25-HighScalability.com_Rated_16th_Best_Blog_for_Development_Managers.html">394 high scalability-2008-09-25-HighScalability.com Rated 16th Best Blog for Development Managers</a></p>
<p>Introduction: Jurgen Appelo of Noop.nl asked for nominations for top blogs and then performed a sophisticated weighting of their popularity based on page range, trust authority, Alexa rank, Google hits, and number of comments. When all the results poured out of the blender HighScalability was ranked 16th. Not bad! Joel on Software was number one, of course. The next few were: 2) Coding Horror by Jeff Atwood 3) Seth's Blog by Seth Godin and 4) Paul Graham: Essays Paul Graham. All excellent blogs. Very cool.</p><p>same-blog 3 0.90096748 <a title="1548-lda-3" href="../high_scalability-2013/high_scalability-2013-11-13-Google%3A_Multiplex_Multiple_Works_Loads_on_Computers_to_Increase_Machine_Utilization_and_Save_Money.html">1548 high scalability-2013-11-13-Google: Multiplex Multiple Works Loads on Computers to Increase Machine Utilization and Save Money</a></p>
<p>Introduction: Jeff Dean  gave a talk at   SFBay ACM   and at about 3 minutes in he goes over how Google runs jobs on computers, which is different than how most shops distribute workloads.
   It’s common for machines to be dedicated to one service, say run a database, run a cache, run this, or run that. The logic is: 
  
 
  Better control over responsiveness as you generally know the traffic loads a machine will experience and you can over provision a box to be safe. 

 
 
  Easier to manage, load balance, configure, upgrade, create and make highly available. Since you know what a machine does another machine can be provisioned to do the same work. 

 
    The problem is monocropping hardware though conceptually clean for humans and safe for applications, is hugely wasteful. Machines are woefully underutilized, even in a virtualized world. 
  What Google does is use a  shared environment  in a datacenter where all kinds of stuff run on each computer. Batch computation and interactive computations a</p><p>4 0.88338089 <a title="1548-lda-4" href="../high_scalability-2013/high_scalability-2013-06-19-Paper%3A_MegaPipe%3A_A_New_Programming_Interface_for_Scalable_Network_I-O.html">1478 high scalability-2013-06-19-Paper: MegaPipe: A New Programming Interface for Scalable Network I-O</a></p>
<p>Introduction: The paper  MegaPipe: A New Programming Interface for Scalable Network I/O  ( video ,  slides ) hits the common theme that if you want to go faster you need a better car design, not just a better driver. So that's why the authors started with a clean-slate and designed a network API from the ground up with support for concurrent I/O, a requirement for achieving high performance while scaling to large numbers of connections per thread, multiple cores, etc.  What they created is MegaPipe, "a new network programming API for message-oriented workloads to avoid the performance issues of BSD Socket API."
 
The result: MegaPipe outperforms baseline Linux between  29% (for long connections)  and  582% (for short connections) . MegaPipe improves the performance of a modiﬁed version of  memcached between 15% and 320% . For a workload based on real-world HTTP traces, MegaPipe boosts the throughput of  nginx by 75% .
 
What's this most excellent and interesting paper about?
  Message-oriented netwo</p><p>5 0.87910163 <a title="1548-lda-5" href="../high_scalability-2008/high_scalability-2008-12-06-Paper%3A_Real-world_Concurrency.html">462 high scalability-2008-12-06-Paper: Real-world Concurrency</a></p>
<p>Introduction: An excellent article by Bryan Cantrill and Jeff Bonwick on how to write multi-threaded code. With more processors and no magic bullet solution for how to use them, knowing how to write multiprocessor code that doesn't screw up your system is still a valuable skill. Some topics:
   Know your cold paths from your hot paths.    Intuition is frequently wrong—be data intensive.    Know when—and when not—to break up a lock.     Be wary of readers/writer locks.   Consider per-CPU locking.   Know when to broadcast—and when to signal.    Learn to debug postmortem.   Design your systems to be composable.    Don't use a semaphore where a mutex would suffice.   Consider memory retiring to implement per-chain hash-table locks.    Be aware of false sharing.   Consider using nonblocking synchronization routines to monitor contention.   When reacquiring locks, consider using generation counts to detect state change.   Use wait- and lock-free structures only if you absolutely must.   Prepare for the th</p><p>6 0.83297122 <a title="1548-lda-6" href="../high_scalability-2009/high_scalability-2009-08-24-How_Google_Serves_Data_from_Multiple_Datacenters.html">687 high scalability-2009-08-24-How Google Serves Data from Multiple Datacenters</a></p>
<p>7 0.82801032 <a title="1548-lda-7" href="../high_scalability-2010/high_scalability-2010-07-27-YeSQL%3A_An_Overview_of_the_Various_Query_Semantics_in_the_Post_Only-SQL_World.html">867 high scalability-2010-07-27-YeSQL: An Overview of the Various Query Semantics in the Post Only-SQL World</a></p>
<p>8 0.82798845 <a title="1548-lda-8" href="../high_scalability-2012/high_scalability-2012-05-09-Cell_Architectures.html">1242 high scalability-2012-05-09-Cell Architectures</a></p>
<p>9 0.82746786 <a title="1548-lda-9" href="../high_scalability-2010/high_scalability-2010-02-19-Twitter%E2%80%99s_Plan_to_Analyze_100_Billion_Tweets.html">780 high scalability-2010-02-19-Twitter’s Plan to Analyze 100 Billion Tweets</a></p>
<p>10 0.82719475 <a title="1548-lda-10" href="../high_scalability-2009/high_scalability-2009-03-05-Strategy%3A__In_Cloud_Computing_Systematically_Drive_Load_to_the_CPU.html">526 high scalability-2009-03-05-Strategy:  In Cloud Computing Systematically Drive Load to the CPU</a></p>
<p>11 0.82631165 <a title="1548-lda-11" href="../high_scalability-2011/high_scalability-2011-05-27-Stuff_The_Internet_Says_On_Scalability_For_May_27%2C_2011.html">1048 high scalability-2011-05-27-Stuff The Internet Says On Scalability For May 27, 2011</a></p>
<p>12 0.82495111 <a title="1548-lda-12" href="../high_scalability-2009/high_scalability-2009-05-17-Product%3A_Hadoop.html">601 high scalability-2009-05-17-Product: Hadoop</a></p>
<p>13 0.82017493 <a title="1548-lda-13" href="../high_scalability-2013/high_scalability-2013-07-01-PRISM%3A_The_Amazingly_Low_Cost_of_%C2%ADUsing_BigData_to_Know_More_About_You_in_Under_a_Minute.html">1485 high scalability-2013-07-01-PRISM: The Amazingly Low Cost of ­Using BigData to Know More About You in Under a Minute</a></p>
<p>14 0.82000524 <a title="1548-lda-14" href="../high_scalability-2009/high_scalability-2009-09-19-Space_Based_Programming_in_.NET.html">709 high scalability-2009-09-19-Space Based Programming in .NET</a></p>
<p>15 0.81770849 <a title="1548-lda-15" href="../high_scalability-2011/high_scalability-2011-08-15-Should_any_cloud_be_considered_one_availability_zone%3F_The_Amazon_experience_says_yes..html">1098 high scalability-2011-08-15-Should any cloud be considered one availability zone? The Amazon experience says yes.</a></p>
<p>16 0.81716281 <a title="1548-lda-16" href="../high_scalability-2013/high_scalability-2013-01-04-Stuff_The_Internet_Says_On_Scalability_For_January_4%2C_2013.html">1381 high scalability-2013-01-04-Stuff The Internet Says On Scalability For January 4, 2013</a></p>
<p>17 0.81659681 <a title="1548-lda-17" href="../high_scalability-2014/high_scalability-2014-02-03-How_Google_Backs_Up_the_Internet_Along_With_Exabytes_of_Other_Data.html">1589 high scalability-2014-02-03-How Google Backs Up the Internet Along With Exabytes of Other Data</a></p>
<p>18 0.8160879 <a title="1548-lda-18" href="../high_scalability-2012/high_scalability-2012-08-20-The_Performance_of_Distributed_Data-Structures_Running_on_a_%22Cache-Coherent%22_In-Memory_Data_Grid.html">1307 high scalability-2012-08-20-The Performance of Distributed Data-Structures Running on a "Cache-Coherent" In-Memory Data Grid</a></p>
<p>19 0.81599998 <a title="1548-lda-19" href="../high_scalability-2009/high_scalability-2009-07-30-Learn_How_to_Think_at_Scale.html">666 high scalability-2009-07-30-Learn How to Think at Scale</a></p>
<p>20 0.81538516 <a title="1548-lda-20" href="../high_scalability-2012/high_scalability-2012-09-04-Changing_Architectures%3A_New_Datacenter_Networks_Will_Set_Your_Code_and_Data_Free___.html">1316 high scalability-2012-09-04-Changing Architectures: New Datacenter Networks Will Set Your Code and Data Free   </a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
