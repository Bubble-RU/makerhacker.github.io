<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1535 high scalability-2013-10-21-Google's Sanjay Ghemawat on What Made Google Google and Great Big Data Career Advice</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2013" href="../home/high_scalability-2013_home.html">high_scalability-2013</a> <a title="high_scalability-2013-1535" href="#">high_scalability-2013-1535</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1535 high scalability-2013-10-21-Google's Sanjay Ghemawat on What Made Google Google and Great Big Data Career Advice</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2013-1535-html" href="http://highscalability.com//blog/2013/10/21/googles-sanjay-ghemawat-on-what-made-google-google-and-great.html">html</a></p><p>Introduction: In a  People of ACM  interview with  Sanjay Ghemawat , a Google Fellow in the Systems Infrastructure Group (MapReduce, BigTable, Spanner, GFS, etc), talks about a few interesting aspects of Google's culture.
  What Made Google Google  
Progress is a modern idea. The conviction that future can be changed for the better through individual advancement and action has over hundreds of years driven an exponential growth in the technome.
 
What drives progress? Challenges. Individuals finding and defeating a challenge. There's usually something someone wants to do so badly that they put in the effort, the thought, and the money into solving all the problems. The results are often something new and amazing.
 
And so it was for Google:
  

The main motivation behind the development of much of Google's infrastructure was the challenge of keeping up with ever-growing data sets. For example, at the same time Google's web search was gaining usage very quickly, we were also scaling up the size of ou</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The conviction that future can be changed for the better through individual advancement and action has over hundreds of years driven an exponential growth in the technome. [sent-3, score-0.448]
</p><p>2 There's usually something someone wants to do so badly that they put in the effort, the thought, and the money into solving all the problems. [sent-7, score-0.101]
</p><p>3 And so it was for Google:     The main motivation behind the development of much of Google's infrastructure was the challenge of keeping up with ever-growing data sets. [sent-9, score-0.261]
</p><p>4 For example, at the same time Google's web search was gaining usage very quickly, we were also scaling up the size of our index dramatically and rebuilding it more often. [sent-10, score-0.198]
</p><p>5 This implied that we had to be able to process a larger amount of data efficiently in a smaller amount of time. [sent-11, score-0.255]
</p><p>6 This need directly led to the development of many of our infrastructure systems. [sent-12, score-0.081]
</p><p>7 And about MapReduce:     The solution was motivated by practical issues we had been running into when trying to solve such problems at Google. [sent-13, score-0.196]
</p><p>8 Perhaps it's because in tight focussed groups there's an interdependence that generates positive feedback loops that amplify innovations. [sent-16, score-1.204]
</p><p>9 Your typical cube farm will follow a Gaussian distribution where people are independent of each other thus cutting off any chance of positive feedback loops developing. [sent-17, score-0.88]
</p><p>10 One also wonders if particular organizational structures like stacked ranking also strangle positive feedback loops at inception. [sent-18, score-1.19]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('loops', 0.253), ('ghemawat', 0.249), ('positive', 0.241), ('google', 0.224), ('feedback', 0.207), ('progress', 0.147), ('advancement', 0.132), ('gaussian', 0.132), ('conviction', 0.132), ('interdependence', 0.132), ('sanjay', 0.124), ('byzantine', 0.124), ('amplify', 0.124), ('advice', 0.124), ('awesome', 0.116), ('wonders', 0.114), ('practical', 0.113), ('defeating', 0.111), ('leading', 0.11), ('stacked', 0.108), ('tip', 0.103), ('grounded', 0.103), ('envelope', 0.103), ('badly', 0.101), ('cube', 0.101), ('implied', 0.101), ('rebuilding', 0.101), ('driven', 0.101), ('challenge', 0.1), ('fellow', 0.099), ('mapreduce', 0.098), ('gaining', 0.097), ('acm', 0.097), ('gfs', 0.093), ('particular', 0.09), ('organizational', 0.089), ('focussed', 0.089), ('ranking', 0.088), ('spanner', 0.088), ('tight', 0.086), ('exponential', 0.083), ('motivated', 0.083), ('infrastructure', 0.081), ('individuals', 0.08), ('motivation', 0.08), ('articleson', 0.08), ('cutting', 0.078), ('amount', 0.077), ('calculations', 0.072), ('generates', 0.072)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="1535-tfidf-1" href="../high_scalability-2013/high_scalability-2013-10-21-Google%27s_Sanjay_Ghemawat_on_What_Made_Google_Google_and_Great_Big_Data_Career_Advice.html">1535 high scalability-2013-10-21-Google's Sanjay Ghemawat on What Made Google Google and Great Big Data Career Advice</a></p>
<p>Introduction: In a  People of ACM  interview with  Sanjay Ghemawat , a Google Fellow in the Systems Infrastructure Group (MapReduce, BigTable, Spanner, GFS, etc), talks about a few interesting aspects of Google's culture.
  What Made Google Google  
Progress is a modern idea. The conviction that future can be changed for the better through individual advancement and action has over hundreds of years driven an exponential growth in the technome.
 
What drives progress? Challenges. Individuals finding and defeating a challenge. There's usually something someone wants to do so badly that they put in the effort, the thought, and the money into solving all the problems. The results are often something new and amazing.
 
And so it was for Google:
  

The main motivation behind the development of much of Google's infrastructure was the challenge of keeping up with ever-growing data sets. For example, at the same time Google's web search was gaining usage very quickly, we were also scaling up the size of ou</p><p>2 0.15889747 <a title="1535-tfidf-2" href="../high_scalability-2012/high_scalability-2012-09-24-Google_Spanner%27s_Most_Surprising_Revelation%3A_NoSQL_is_Out_and_NewSQL_is_In.html">1328 high scalability-2012-09-24-Google Spanner's Most Surprising Revelation: NoSQL is Out and NewSQL is In</a></p>
<p>Introduction: Google recently released a   paper on Spanner   , their planet enveloping tool for organizing the world’s monetizable information. Reading the Spanner paper I felt it had that chiseled in stone feel that all of Google’s best papers have. An instant classic. Jeff Dean foreshadowed Spanner’s humungousness as early as    2009   .  Now Spanner seems fully online, just waiting to handle “millions of machines across hundreds of datacenters and trillions of database rows.” Wow.    The Wise have yet to weigh in on Spanner en masse. I look forward to more insightful commentary. There’s a lot to make sense of. What struck me most in the paper was a deeply buried section essentially describing Google’s motivation for shifting away from NoSQL and to    NewSQL   . The money quote: 
  
   We believe it is better to have application programmers deal with performance problems due to overuse of transactions as bottlenecks arise, rather than always coding around the lack of transactions.  

  
 This rea</p><p>3 0.13895014 <a title="1535-tfidf-3" href="../high_scalability-2009/high_scalability-2009-01-04-Paper%3A_MapReduce%3A_Simplified_Data_Processing_on_Large_Clusters.html">483 high scalability-2009-01-04-Paper: MapReduce: Simplified Data Processing on Large Clusters</a></p>
<p>Introduction: Update:   MapReduce and PageRank Notes from Remzi Arpaci-Dusseau's Fall 2008 class  . Collects interesting facts about MapReduce and PageRank. For example, the history of the solution to searching for the term "flu" is traced through multiple generations of technology.   With Google entering the cloud space with  Google AppEngine  and a maturing  Hadoop  product, the MapReduce scaling approach might finally become a standard programmer practice. This is the best paper on the subject and is an excellent primer on a content-addressable memory future.  Some interesting stats from the paper: Google executes 100k MapReduce jobs each day; more than 20 petabytes of data are processed per day; more than 10k MapReduce programs have been implemented; machines are dual processor with gigabit ethernet and 4-8 GB of memory.  One common criticism ex-Googlers have is that it takes months to get up and be productive in the Google environment. Hopefully a way will be found to lower the learning curve a</p><p>4 0.13580884 <a title="1535-tfidf-4" href="../high_scalability-2008/high_scalability-2008-04-23-Behind_The_Scenes_of_Google_Scalability.html">309 high scalability-2008-04-23-Behind The Scenes of Google Scalability</a></p>
<p>Introduction: The recent Data-Intensive Computing Symposium brought together experts in system design, programming, parallel algorithms, data management, scientific applications, and information-based applications to better understand existing capabilities in the development and application of large-scale computing systems, and to explore future opportunities.     Google Fellow Jeff Dean had a very interesting presentation on Handling Large Datasets at Google: Current Systems and Future Directions. He discussed:     • Hardware infrastructure    • Distributed systems infrastructure:    –Scheduling system    –GFS    –BigTable    –MapReduce    • Challenges and Future Directions    –Infrastructure that spans all datacenters    –More automation     It is really like a "How does Google work" presentation in ~60 slides?     Check out the   slides   and the   video  !</p><p>5 0.12402611 <a title="1535-tfidf-5" href="../high_scalability-2008/high_scalability-2008-11-22-Google_Architecture.html">448 high scalability-2008-11-22-Google Architecture</a></p>
<p>Introduction: Update 2:   Sorting 1 PB with MapReduce . PB is not peanut-butter-and-jelly misspelled. It's 1 petabyte or 1000 terabytes or 1,000,000 gigabytes.  It took six hours and two minutes to sort 1PB (10 trillion 100-byte records) on 4,000 computers  and the results were replicated thrice on 48,000 disks.  Update:   Greg Linden  points to a new Google article  MapReduce: simplified data processing on large clusters . Some interesting stats: 100k MapReduce jobs are executed each day; more than 20 petabytes of data are processed per day; more than 10k MapReduce programs have been implemented; machines are dual processor with gigabit ethernet and 4-8 GB of memory.  Google is the King of scalability.  Everyone knows Google for their large,  sophisticated, and fast searching, but they don't just shine in search. Their platform approach to building scalable applications allows them to roll out internet scale applications at an alarmingly high competition crushing rate. Their goal is always to build</p><p>6 0.11505577 <a title="1535-tfidf-6" href="../high_scalability-2009/high_scalability-2009-10-30-Hot_Scalabilty_Links_for_October_30_2009.html">734 high scalability-2009-10-30-Hot Scalabilty Links for October 30 2009</a></p>
<p>7 0.11471123 <a title="1535-tfidf-7" href="../high_scalability-2009/high_scalability-2009-02-21-Google_AppEngine_-_A_Second_Look.html">517 high scalability-2009-02-21-Google AppEngine - A Second Look</a></p>
<p>8 0.10982125 <a title="1535-tfidf-8" href="../high_scalability-2008/high_scalability-2008-04-07-Rumors_of_Signs_and_Portents_Concerning_Freeish_Google_Cloud.html">299 high scalability-2008-04-07-Rumors of Signs and Portents Concerning Freeish Google Cloud</a></p>
<p>9 0.10337473 <a title="1535-tfidf-9" href="../high_scalability-2008/high_scalability-2008-01-13-Google_Reveals_New_MapReduce_Stats.html">211 high scalability-2008-01-13-Google Reveals New MapReduce Stats</a></p>
<p>10 0.10255733 <a title="1535-tfidf-10" href="../high_scalability-2009/high_scalability-2009-07-20-A_Scalability_Lament.html">659 high scalability-2009-07-20-A Scalability Lament</a></p>
<p>11 0.10215903 <a title="1535-tfidf-11" href="../high_scalability-2009/high_scalability-2009-07-02-Product%3A_Hbase.html">650 high scalability-2009-07-02-Product: Hbase</a></p>
<p>12 0.10148077 <a title="1535-tfidf-12" href="../high_scalability-2012/high_scalability-2012-10-22-Spanner_-_It%27s_About_Programmers_Building_Apps_Using_SQL_Semantics_at_NoSQL_Scale.html">1345 high scalability-2012-10-22-Spanner - It's About Programmers Building Apps Using SQL Semantics at NoSQL Scale</a></p>
<p>13 0.10053626 <a title="1535-tfidf-13" href="../high_scalability-2012/high_scalability-2012-05-07-Startups_are_Creating_a_New_System_of_the_World_for_IT.html">1240 high scalability-2012-05-07-Startups are Creating a New System of the World for IT</a></p>
<p>14 0.099611431 <a title="1535-tfidf-14" href="../high_scalability-2011/high_scalability-2011-09-14-Big_List_of_Scalabilty_Conferences.html">1115 high scalability-2011-09-14-Big List of Scalabilty Conferences</a></p>
<p>15 0.099213891 <a title="1535-tfidf-15" href="../high_scalability-2013/high_scalability-2013-05-24-Stuff_The_Internet_Says_On_Scalability_For_May_24%2C_2013.html">1464 high scalability-2013-05-24-Stuff The Internet Says On Scalability For May 24, 2013</a></p>
<p>16 0.097967483 <a title="1535-tfidf-16" href="../high_scalability-2009/high_scalability-2009-12-16-Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_in_the_Ambient_Cloud.html">750 high scalability-2009-12-16-Building Super Scalable Systems: Blade Runner Meets Autonomic Computing in the Ambient Cloud</a></p>
<p>17 0.09793108 <a title="1535-tfidf-17" href="../high_scalability-2012/high_scalability-2012-11-05-Gone_Fishin%27%3A_Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_In_The_Ambient_Cloud.html">1355 high scalability-2012-11-05-Gone Fishin': Building Super Scalable Systems: Blade Runner Meets Autonomic Computing In The Ambient Cloud</a></p>
<p>18 0.09785933 <a title="1535-tfidf-18" href="../high_scalability-2011/high_scalability-2011-01-26-Google_Pro_Tip%3A_Use_Back-of-the-envelope-calculations_to_Choose_the_Best_Design.html">978 high scalability-2011-01-26-Google Pro Tip: Use Back-of-the-envelope-calculations to Choose the Best Design</a></p>
<p>19 0.096810065 <a title="1535-tfidf-19" href="../high_scalability-2011/high_scalability-2011-07-12-Google%2B_is_Built_Using_Tools_You_Can_Use_Too%3A_Closure%2C_Java_Servlets%2C_JavaScript%2C_BigTable%2C_Colossus%2C_Quick_Turnaround.html">1078 high scalability-2011-07-12-Google+ is Built Using Tools You Can Use Too: Closure, Java Servlets, JavaScript, BigTable, Colossus, Quick Turnaround</a></p>
<p>20 0.096267797 <a title="1535-tfidf-20" href="../high_scalability-2007/high_scalability-2007-09-18-Amazon_Architecture.html">96 high scalability-2007-09-18-Amazon Architecture</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.158), (1, 0.071), (2, 0.015), (3, 0.083), (4, 0.032), (5, -0.021), (6, -0.012), (7, 0.07), (8, 0.045), (9, 0.041), (10, -0.033), (11, -0.038), (12, -0.022), (13, -0.02), (14, 0.065), (15, -0.041), (16, -0.078), (17, -0.055), (18, 0.082), (19, -0.003), (20, 0.069), (21, -0.014), (22, 0.028), (23, -0.071), (24, 0.002), (25, 0.058), (26, -0.016), (27, 0.039), (28, -0.094), (29, 0.046), (30, 0.017), (31, -0.008), (32, 0.01), (33, -0.006), (34, -0.048), (35, 0.025), (36, 0.049), (37, -0.047), (38, 0.066), (39, 0.065), (40, 0.016), (41, 0.017), (42, 0.043), (43, -0.01), (44, -0.048), (45, -0.031), (46, 0.003), (47, -0.059), (48, 0.026), (49, -0.036)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98532802 <a title="1535-lsi-1" href="../high_scalability-2013/high_scalability-2013-10-21-Google%27s_Sanjay_Ghemawat_on_What_Made_Google_Google_and_Great_Big_Data_Career_Advice.html">1535 high scalability-2013-10-21-Google's Sanjay Ghemawat on What Made Google Google and Great Big Data Career Advice</a></p>
<p>Introduction: In a  People of ACM  interview with  Sanjay Ghemawat , a Google Fellow in the Systems Infrastructure Group (MapReduce, BigTable, Spanner, GFS, etc), talks about a few interesting aspects of Google's culture.
  What Made Google Google  
Progress is a modern idea. The conviction that future can be changed for the better through individual advancement and action has over hundreds of years driven an exponential growth in the technome.
 
What drives progress? Challenges. Individuals finding and defeating a challenge. There's usually something someone wants to do so badly that they put in the effort, the thought, and the money into solving all the problems. The results are often something new and amazing.
 
And so it was for Google:
  

The main motivation behind the development of much of Google's infrastructure was the challenge of keeping up with ever-growing data sets. For example, at the same time Google's web search was gaining usage very quickly, we were also scaling up the size of ou</p><p>2 0.85328823 <a title="1535-lsi-2" href="../high_scalability-2011/high_scalability-2011-07-12-Google%2B_is_Built_Using_Tools_You_Can_Use_Too%3A_Closure%2C_Java_Servlets%2C_JavaScript%2C_BigTable%2C_Colossus%2C_Quick_Turnaround.html">1078 high scalability-2011-07-12-Google+ is Built Using Tools You Can Use Too: Closure, Java Servlets, JavaScript, BigTable, Colossus, Quick Turnaround</a></p>
<p>Introduction: Joseph Smarr, former CTO of Plaxo (which explains why I recognized his picture), in  I'm a technical lead on the Google+ team. Ask me anything , reveals the stack used for building Google+:
  Our stack is pretty standard fare for Google apps these days: we use Java servlets for our server code and JavaScript for the browser-side of the UI, largely built with the (open-source) Closure framework, including Closure's JavaScript compiler and template system. A couple nifty tricks we do: we use the HTML5 History API to maintain pretty-looking URLs even though it's an AJAX app (falling back on hash-fragments for older browsers); and we often render our Closure templates server-side so the page renders before any JavaScript is loaded, then the JavaScript finds the right DOM nodes and hooks up event handlers, etc. to make it responsive (as a result, if you're on a slow connection and you click on stuff really fast, you may notice a lag before it does anything, but luckily most people don't run</p><p>3 0.82681239 <a title="1535-lsi-3" href="../high_scalability-2012/high_scalability-2012-09-24-Google_Spanner%27s_Most_Surprising_Revelation%3A_NoSQL_is_Out_and_NewSQL_is_In.html">1328 high scalability-2012-09-24-Google Spanner's Most Surprising Revelation: NoSQL is Out and NewSQL is In</a></p>
<p>Introduction: Google recently released a   paper on Spanner   , their planet enveloping tool for organizing the world’s monetizable information. Reading the Spanner paper I felt it had that chiseled in stone feel that all of Google’s best papers have. An instant classic. Jeff Dean foreshadowed Spanner’s humungousness as early as    2009   .  Now Spanner seems fully online, just waiting to handle “millions of machines across hundreds of datacenters and trillions of database rows.” Wow.    The Wise have yet to weigh in on Spanner en masse. I look forward to more insightful commentary. There’s a lot to make sense of. What struck me most in the paper was a deeply buried section essentially describing Google’s motivation for shifting away from NoSQL and to    NewSQL   . The money quote: 
  
   We believe it is better to have application programmers deal with performance problems due to overuse of transactions as bottlenecks arise, rather than always coding around the lack of transactions.  

  
 This rea</p><p>4 0.81570369 <a title="1535-lsi-4" href="../high_scalability-2008/high_scalability-2008-10-13-Challenges_from_large_scale_computing_at_Google.html">409 high scalability-2008-10-13-Challenges from large scale computing at Google</a></p>
<p>Introduction: From Greg Linden on a talk Google Fellow Jeff Dean gave last week at University of Washington Computer Science titled "Research Challenges Inspired by Large-Scale Computing at Google" :   Coming away from the talk, the biggest points for me were the considerable interest in reducing costs (especially reducing power costs), the suggestion that the Google cluster may eventually contain 10M machines at 1k locations, and the call to action for researchers on distributed systems and databases to think orders of magnitude bigger than they often are, not about running on hundreds of machines in one location, but hundreds of thousands of machines across many locations.</p><p>5 0.81310725 <a title="1535-lsi-5" href="../high_scalability-2011/high_scalability-2011-03-24-Strategy%3A_Disk_Backup_for_Speed%2C_Tape_Backup_to_Save_Your_Bacon%2C_Just_Ask_Google.html">1010 high scalability-2011-03-24-Strategy: Disk Backup for Speed, Tape Backup to Save Your Bacon, Just Ask Google</a></p>
<p>Introduction: In  Stack Overflow Architecture Update - Now At 95 Million Page Views A Month , a commenter expressed surprise about Stack Overflow's backup strategy: 
  

Backup is to disk for fast retrieval and to tape for historical archiving.

  
The comment was:
  

Really? People still do this? I know some organizations invested a tremendous amount in automated, robotic tape backup, but seriously, a site founded in 2008 is backing up to tape?

   The Case of the Missing Gmail Accounts  
I admit that I was surprised at this strategy too. In this age of copying data to disk three times for safety, I also wondered if tape backups were still necessary? Then, like in a movie, an event happened that made sense of everything, Google suffered the quintessential  #firstworldproblem , gmail accounts went missing! Queue emphatic music. And what's more they were taking a long time to come back. There was a palpable fear in the land that email accounts might never be restored. Think about that. They might ne</p><p>6 0.81256884 <a title="1535-lsi-6" href="../high_scalability-2011/high_scalability-2011-08-29-The_Three_Ages_of_Google_-_Batch%2C_Warehouse%2C_Instant.html">1107 high scalability-2011-08-29-The Three Ages of Google - Batch, Warehouse, Instant</a></p>
<p>7 0.80915844 <a title="1535-lsi-7" href="../high_scalability-2010/high_scalability-2010-08-04-Dremel%3A_Interactive_Analysis_of_Web-Scale_Datasets_-_Data_as_a_Programming_Paradigm.html">871 high scalability-2010-08-04-Dremel: Interactive Analysis of Web-Scale Datasets - Data as a Programming Paradigm</a></p>
<p>8 0.79828882 <a title="1535-lsi-8" href="../high_scalability-2011/high_scalability-2011-11-16-Google%2B_Infrastructure_Update_-_the_JavaScript_Story.html">1143 high scalability-2011-11-16-Google+ Infrastructure Update - the JavaScript Story</a></p>
<p>9 0.79217988 <a title="1535-lsi-9" href="../high_scalability-2008/high_scalability-2008-01-13-Google_Reveals_New_MapReduce_Stats.html">211 high scalability-2008-01-13-Google Reveals New MapReduce Stats</a></p>
<p>10 0.78260976 <a title="1535-lsi-10" href="../high_scalability-2009/high_scalability-2009-06-05-Google_Wave_Architecture.html">618 high scalability-2009-06-05-Google Wave Architecture</a></p>
<p>11 0.78253675 <a title="1535-lsi-11" href="../high_scalability-2007/high_scalability-2007-08-28-Google_Utilities_%3A_An_online_google_guide%2Ctools_and_Utilities..html">75 high scalability-2007-08-28-Google Utilities : An online google guide,tools and Utilities.</a></p>
<p>12 0.77802426 <a title="1535-lsi-12" href="../high_scalability-2013/high_scalability-2013-10-30-Strategy%3A_Use_Your_Quantum_Computer_Lab_to_Tell_Intentional_Blinks_from_Involuntary_Blinks.html">1540 high scalability-2013-10-30-Strategy: Use Your Quantum Computer Lab to Tell Intentional Blinks from Involuntary Blinks</a></p>
<p>13 0.76578748 <a title="1535-lsi-13" href="../high_scalability-2009/high_scalability-2009-01-04-Paper%3A_MapReduce%3A_Simplified_Data_Processing_on_Large_Clusters.html">483 high scalability-2009-01-04-Paper: MapReduce: Simplified Data Processing on Large Clusters</a></p>
<p>14 0.75508422 <a title="1535-lsi-14" href="../high_scalability-2009/high_scalability-2009-10-30-Hot_Scalabilty_Links_for_October_30_2009.html">734 high scalability-2009-10-30-Hot Scalabilty Links for October 30 2009</a></p>
<p>15 0.74320132 <a title="1535-lsi-15" href="../high_scalability-2009/high_scalability-2009-06-29-HighScalability_Rated_%233_Blog_for_Developers.html">642 high scalability-2009-06-29-HighScalability Rated #3 Blog for Developers</a></p>
<p>16 0.74096227 <a title="1535-lsi-16" href="../high_scalability-2009/high_scalability-2009-06-28-Google_Voice_Architecture.html">640 high scalability-2009-06-28-Google Voice Architecture</a></p>
<p>17 0.73617375 <a title="1535-lsi-17" href="../high_scalability-2008/high_scalability-2008-01-04-For_%245_Million_You_Can_Buy_Enough_Storage_to_Compete_with_Google.html">201 high scalability-2008-01-04-For $5 Million You Can Buy Enough Storage to Compete with Google</a></p>
<p>18 0.71445179 <a title="1535-lsi-18" href="../high_scalability-2008/high_scalability-2008-02-07-Looking_for_good_business_examples_of_compaines_using_Hadoop.html">242 high scalability-2008-02-07-Looking for good business examples of compaines using Hadoop</a></p>
<p>19 0.71095949 <a title="1535-lsi-19" href="../high_scalability-2010/high_scalability-2010-04-27-Paper%3A__Dapper%2C_Google%27s_Large-Scale_Distributed_Systems_Tracing_Infrastructure.html">815 high scalability-2010-04-27-Paper:  Dapper, Google's Large-Scale Distributed Systems Tracing Infrastructure</a></p>
<p>20 0.71033609 <a title="1535-lsi-20" href="../high_scalability-2008/high_scalability-2008-04-23-Behind_The_Scenes_of_Google_Scalability.html">309 high scalability-2008-04-23-Behind The Scenes of Google Scalability</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.092), (2, 0.19), (5, 0.035), (10, 0.019), (21, 0.169), (26, 0.02), (61, 0.145), (76, 0.026), (79, 0.169), (85, 0.053)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.92061049 <a title="1535-lda-1" href="../high_scalability-2013/high_scalability-2013-10-21-Google%27s_Sanjay_Ghemawat_on_What_Made_Google_Google_and_Great_Big_Data_Career_Advice.html">1535 high scalability-2013-10-21-Google's Sanjay Ghemawat on What Made Google Google and Great Big Data Career Advice</a></p>
<p>Introduction: In a  People of ACM  interview with  Sanjay Ghemawat , a Google Fellow in the Systems Infrastructure Group (MapReduce, BigTable, Spanner, GFS, etc), talks about a few interesting aspects of Google's culture.
  What Made Google Google  
Progress is a modern idea. The conviction that future can be changed for the better through individual advancement and action has over hundreds of years driven an exponential growth in the technome.
 
What drives progress? Challenges. Individuals finding and defeating a challenge. There's usually something someone wants to do so badly that they put in the effort, the thought, and the money into solving all the problems. The results are often something new and amazing.
 
And so it was for Google:
  

The main motivation behind the development of much of Google's infrastructure was the challenge of keeping up with ever-growing data sets. For example, at the same time Google's web search was gaining usage very quickly, we were also scaling up the size of ou</p><p>2 0.90459543 <a title="1535-lda-2" href="../high_scalability-2010/high_scalability-2010-12-01-8_Commonly_Used_Scalable_System_Design_Patterns.html">951 high scalability-2010-12-01-8 Commonly Used Scalable System Design Patterns</a></p>
<p>Introduction: Ricky Ho  in  Scalable System Design Patterns  has created a great list of scalability patterns along with very well done explanatory graphics. A summary of the patterns are:
  
  Load Balancer  - a dispatcher determines which worker instance will handle a request based on different policies. 
  Scatter and Gather  - a dispatcher multicasts requests to all workers in a pool. Each worker will compute a local result and send it back to the dispatcher, who will consolidate them into a single response and then send back to the client. 
  Result Cache  - a dispatcher will first lookup if the request has been made before and try to find the previous result to return, in order to save the actual execution. 
  Shared Space  - all workers monitors information from the shared space and contributes partial knowledge back to the blackboard. The information is continuously enriched until a solution is reached. 
  Pipe and Filter  - all workers connected by pipes across which data flows. 
  MapReduc</p><p>3 0.89329576 <a title="1535-lda-3" href="../high_scalability-2008/high_scalability-2008-03-16-Do_you_have_any_questions_for_the_Elastra_CEO%3F.html">277 high scalability-2008-03-16-Do you have any questions for the Elastra CEO?</a></p>
<p>Introduction: It looks like in the near future I'll have a chance to interview the Elastra CEO. Elastra provides standard   databases--MySQL, EnterpriseDB and PostgreSQL-- on top of EC2 and S3. They are selling aggressive pricing, expandable and contactable database resource usage in response to demand, and a simple management and operations interface to well known databases deployed in a cloud. Elastra could be an important option for developers looking for a more traditional cloudy database.      I was wondering if you guys had any suggestions for questions you would like answered? What would you like to know about their service? What are you looking for in a cloudy database? What would stop you from adopting it or what would make you decide to adopt it? Any ideas you have would help a lot and will probably be better than anything I have.</p><p>4 0.8901881 <a title="1535-lda-4" href="../high_scalability-2012/high_scalability-2012-06-20-Ask_HighScalability%3A_How_do_I_organize_millions_of_images%3F.html">1268 high scalability-2012-06-20-Ask HighScalability: How do I organize millions of images?</a></p>
<p>Introduction: Does anyone have any advice or suggestions on how to store millions of images? Currently images are stored in a MS SQL database which performance wise isn't ideal. We'd like to migrate the images over to a file system structure but I'd assume we don't just want to dump millions of images into a single directory. Besides having to contend with naming collisions, the windows filesystem might not perform optimally with that many files.
 
I'm assuming one approach may be to assign each user a unique CSLID, create a folder based on the CSLID and then place one users files in that particular folder. Even so, this could result in hundreds of thousands of folders. Whats the best organizational scheme/heirachy for doing this?</p><p>5 0.85773069 <a title="1535-lda-5" href="../high_scalability-2010/high_scalability-2010-10-01-Google_Paper%3A_Large-scale_Incremental_Processing_Using_Distributed_Transactions_and_Notifications.html">912 high scalability-2010-10-01-Google Paper: Large-scale Incremental Processing Using Distributed Transactions and Notifications</a></p>
<p>Introduction: This paper,  Large-scale Incremental Processing Using Distributed Transactions and Notifications  by Daniel Peng and Frank Dabek, is Google's much anticipated description of Percolator, their new  real-time indexing  system.
 
The abstract:
  
 Updating an index of the web as documents are crawled requires continuously transforming a large repository of existing documents as new documents arrive. This task is one example of a class of data processing tasks that transform a large repository of data via small, independent mutations. These tasks lie in a gap between the capabilities of existing infrastructure. Databases do not meet the storage or throughput requirements of these tasks: Google’s indexing system stores tens of petabytes of data and processes billions of updates per day on thousands of machines. MapReduce and other batch-processing systems cannot process small updates individually as they rely on creating large batches for efﬁciency.

 

 We have built Percolator, a system f</p><p>6 0.85425621 <a title="1535-lda-6" href="../high_scalability-2012/high_scalability-2012-05-09-Cell_Architectures.html">1242 high scalability-2012-05-09-Cell Architectures</a></p>
<p>7 0.84716892 <a title="1535-lda-7" href="../high_scalability-2008/high_scalability-2008-04-21-Using_Google_AppEngine_for_a_Little_Micro-Scalability.html">307 high scalability-2008-04-21-Using Google AppEngine for a Little Micro-Scalability</a></p>
<p>8 0.84509563 <a title="1535-lda-8" href="../high_scalability-2012/high_scalability-2012-05-25-Stuff_The_Internet_Says_On_Scalability_For_May_25%2C_2012.html">1252 high scalability-2012-05-25-Stuff The Internet Says On Scalability For May 25, 2012</a></p>
<p>9 0.84288085 <a title="1535-lda-9" href="../high_scalability-2013/high_scalability-2013-11-04-ESPN%27s_Architecture_at_Scale_-_Operating_at_100%2C000_Duh_Nuh_Nuhs_Per_Second.html">1542 high scalability-2013-11-04-ESPN's Architecture at Scale - Operating at 100,000 Duh Nuh Nuhs Per Second</a></p>
<p>10 0.84224612 <a title="1535-lda-10" href="../high_scalability-2011/high_scalability-2011-12-08-Update_on_Scalable_Causal_Consistency_For_Wide-Area_Storage_With_COPS.html">1153 high scalability-2011-12-08-Update on Scalable Causal Consistency For Wide-Area Storage With COPS</a></p>
<p>11 0.84048319 <a title="1535-lda-11" href="../high_scalability-2009/high_scalability-2009-08-24-How_Google_Serves_Data_from_Multiple_Datacenters.html">687 high scalability-2009-08-24-How Google Serves Data from Multiple Datacenters</a></p>
<p>12 0.83567566 <a title="1535-lda-12" href="../high_scalability-2008/high_scalability-2008-03-18-Shared_filesystem_on_EC2.html">283 high scalability-2008-03-18-Shared filesystem on EC2</a></p>
<p>13 0.8335138 <a title="1535-lda-13" href="../high_scalability-2011/high_scalability-2011-04-12-Caching_and_Processing_2TB_Mozilla_Crash_Reports_in_memory_with_Hazelcast.html">1020 high scalability-2011-04-12-Caching and Processing 2TB Mozilla Crash Reports in memory with Hazelcast</a></p>
<p>14 0.83297902 <a title="1535-lda-14" href="../high_scalability-2010/high_scalability-2010-02-19-Twitter%E2%80%99s_Plan_to_Analyze_100_Billion_Tweets.html">780 high scalability-2010-02-19-Twitter’s Plan to Analyze 100 Billion Tweets</a></p>
<p>15 0.83287257 <a title="1535-lda-15" href="../high_scalability-2012/high_scalability-2012-10-10-Antirez%3A_You_Need_to_Think_in_Terms_of_Organizing_Your_Data_for_Fetching.html">1337 high scalability-2012-10-10-Antirez: You Need to Think in Terms of Organizing Your Data for Fetching</a></p>
<p>16 0.83257872 <a title="1535-lda-16" href="../high_scalability-2013/high_scalability-2013-01-28-DuckDuckGo_Architecture_-_1_Million_Deep_Searches_a_Day_and_Growing.html">1395 high scalability-2013-01-28-DuckDuckGo Architecture - 1 Million Deep Searches a Day and Growing</a></p>
<p>17 0.83074558 <a title="1535-lda-17" href="../high_scalability-2012/high_scalability-2012-11-26-BigData_using_Erlang%2C_C_and_Lisp_to_Fight_the_Tsunami_of_Mobile_Data.html">1362 high scalability-2012-11-26-BigData using Erlang, C and Lisp to Fight the Tsunami of Mobile Data</a></p>
<p>18 0.83051372 <a title="1535-lda-18" href="../high_scalability-2009/high_scalability-2009-10-29-Paper%3A_No_Relation%3A_The_Mixed_Blessings_of_Non-Relational_Databases.html">733 high scalability-2009-10-29-Paper: No Relation: The Mixed Blessings of Non-Relational Databases</a></p>
<p>19 0.82871413 <a title="1535-lda-19" href="../high_scalability-2011/high_scalability-2011-03-18-Stuff_The_Internet_Says_On_Scalability_For_March_18%2C_2011.html">1007 high scalability-2011-03-18-Stuff The Internet Says On Scalability For March 18, 2011</a></p>
<p>20 0.82834065 <a title="1535-lda-20" href="../high_scalability-2007/high_scalability-2007-10-30-Paper%3A_Dynamo%3A_Amazon%E2%80%99s_Highly_Available_Key-value_Store.html">139 high scalability-2007-10-30-Paper: Dynamo: Amazon’s Highly Available Key-value Store</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
