<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>910 high scalability-2010-09-30-Facebook and Site Failures Caused by Complex, Weakly Interacting, Layered Systems</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2010" href="../home/high_scalability-2010_home.html">high_scalability-2010</a> <a title="high_scalability-2010-910" href="#">high_scalability-2010-910</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>910 high scalability-2010-09-30-Facebook and Site Failures Caused by Complex, Weakly Interacting, Layered Systems</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2010-910-html" href="http://highscalability.com//blog/2010/9/30/facebook-and-site-failures-caused-by-complex-weakly-interact.html">html</a></p><p>Introduction: Facebook has been so reliable that when a site outage does occur it's a
definite learning opportunity. Fortunately for us we can learn something
because in More Details on Today's Outage, Facebook'sRobert Johnson gave a
pretty candid explanation of what caused a rare 2.5 hour period of down time
for Facebook. It wasn't a simple problem. The root causes were feedback loops
and transient spikes caused ultimately by the complexity of weakly interacting
layers in modern systems. You know, the kind everyone is building these days.
Problems like this are notoriously hard to fix and finding a real solution may
send Facebook back to the whiteboard. There's a technical debt that must be
paid. The outline and my interpretation (reading between the lines) of what
happened is:Remember that Facebookcaches everything. They
have28terabytesofmemcacheddata on 800 servers. The database is the system of
record, but memory is where the action is. So when a problem happens that
involves the caching layer,</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('valid', 0.347), ('invalid', 0.228), ('value', 0.222), ('code', 0.148), ('checks', 0.125), ('database', 0.123), ('cache', 0.115), ('would', 0.111), ('centralized', 0.104), ('pile', 0.101), ('entered', 0.101), ('configuration', 0.098), ('caused', 0.097), ('facebook', 0.097), ('caches', 0.097), ('transient', 0.096), ('reply', 0.094), ('layer', 0.091), ('issue', 0.089), ('ddos', 0.087)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000005 <a title="910-tfidf-1" href="../high_scalability-2010/high_scalability-2010-09-30-Facebook_and_Site_Failures_Caused_by_Complex%2C_Weakly_Interacting%2C_Layered_Systems.html">910 high scalability-2010-09-30-Facebook and Site Failures Caused by Complex, Weakly Interacting, Layered Systems</a></p>
<p>Introduction: Facebook has been so reliable that when a site outage does occur it's a
definite learning opportunity. Fortunately for us we can learn something
because in More Details on Today's Outage, Facebook'sRobert Johnson gave a
pretty candid explanation of what caused a rare 2.5 hour period of down time
for Facebook. It wasn't a simple problem. The root causes were feedback loops
and transient spikes caused ultimately by the complexity of weakly interacting
layers in modern systems. You know, the kind everyone is building these days.
Problems like this are notoriously hard to fix and finding a real solution may
send Facebook back to the whiteboard. There's a technical debt that must be
paid. The outline and my interpretation (reading between the lines) of what
happened is:Remember that Facebookcaches everything. They
have28terabytesofmemcacheddata on 800 servers. The database is the system of
record, but memory is where the action is. So when a problem happens that
involves the caching layer,</p><p>2 0.17417185 <a title="910-tfidf-2" href="../high_scalability-2013/high_scalability-2013-08-28-Sean_Hull%27s_20_Biggest_Bottlenecks_that_Reduce_and_Slow_Down_Scalability.html">1508 high scalability-2013-08-28-Sean Hull's 20 Biggest Bottlenecks that Reduce and Slow Down Scalability</a></p>
<p>Introduction: This article is a lightly edited version of 20 Obstacles to Scalability bySean
Hull (with permission) from the always excellent and thought provoking ACM
Queue.1. TWO-PHASE COMMITNormally when data is changed in a database, it is
written both to memory and to disk. When a commit happens, a relational
database makes a commitment to freeze the data somewhere on real storage
media. Remember, memory doesn't survive a crash or reboot. Even if the data is
cached in memory, the database still has to write it to disk. MySQL binary
logs or Oracle redo logs fit the bill.With a MySQL cluster or distributed file
system such as DRBD (Distributed Replicated Block Device) or Amazon Multi-AZ
(Multi-Availability Zone), a commit occurs not only locally, but also at the
remote end. A two-phase commit means waiting for an acknowledgment from the
far end. Because of network and other latency, those commits can be slowed
down by milliseconds, as though all the cars on a highway were slowed down by
heavy loa</p><p>3 0.17113054 <a title="910-tfidf-3" href="../high_scalability-2010/high_scalability-2010-10-14-I%2C_Cloud.html">919 high scalability-2010-10-14-I, Cloud</a></p>
<p>Introduction: Every time a technological innovation has spurred automation - since the time
of Henry Ford right up to a minute ago - someone has claimed that machines
will displace human beings. But the rainbow and unicorn dream attributed to
business stakeholders everywhere, i.e. the elimination of IT, is just that - a
dream. It isn't realistic and in fact it's downright silly to think that
systems that only a few years ago were unable to automatically scale up and
scale down will suddenly be able to perform the complex analysis required of
IT to keep the business running.The rare reports of the elimination of IT
staff due tocloud computingand automation are highlighted in the news because
they evoke visceral reactions in technologists everywhere and, to be honest,
they get the click counts rising. But thejury remains out on this oneand in
fact many postulate that it is not a reduction in staff that will occur, but
atransformationof staff, which may eliminate some old timey positions (think
sysadmi</p><p>4 0.16801146 <a title="910-tfidf-4" href="../high_scalability-2010/high_scalability-2010-10-15-Troubles_with_Sharding_-_What_can_we_learn_from_the_Foursquare_Incident%3F.html">920 high scalability-2010-10-15-Troubles with Sharding - What can we learn from the Foursquare Incident?</a></p>
<p>Introduction: For everything given something seems to be taken. Caching is a great
scalability solution, but caching alsocomes with problems.Shardingis a great
scalability solution, but as Foursquare recently revealed in apost-mortemabout
their 17 hours of downtime, sharding also has problems. MongoDB, the database
Foursquare uses, also contributed theirpost-mortemof what went wrong too.Now
that everyone has shared and resharded, what can we learn to help us skip
these mistakes and quickly move on to a different set of mistakes?First, like
forFacebook, huge props to Foursquare and MongoDB for being upfront and honest
about their problems. This helps everyone get better and is a sign we work in
a pretty cool industry.Second, overall, the fault didn't flow from evil hearts
or gross negligence. As usual the cause was more mundane: a key system, that
could be a little more robust, combined with a very popular application built
by a small group of people, under immense pressure, trying to get a lot of
wo</p><p>5 0.16257006 <a title="910-tfidf-5" href="../high_scalability-2009/high_scalability-2009-03-16-Are_Cloud_Based_Memory_Architectures_the_Next_Big_Thing%3F.html">538 high scalability-2009-03-16-Are Cloud Based Memory Architectures the Next Big Thing?</a></p>
<p>Introduction: We are on the edge of two potent technological changes: Clouds and Memory
Based Architectures. This evolution will rip open a chasm where new players
can enter and prosper. Google is the master of disk. You can't beat them at a
game they perfected. Disk based databases like SimpleDB and BigTable are
complicated beasts, typical last gasp products of any aging technology before
a change. The next era is the age of Memory and Cloud which will allow for new
players to succeed. The tipping point will be soon.Let's take a short trip
down web architecture lane:It's 1993: Yahoo runs on FreeBSD, Apache, Perl
scripts and a SQL databaseIt's 1995: Scale-up the database.It's 1998: LAMPIt's
1999: Stateless + Load Balanced + Database + SANIt's 2001: In-memory data-
grid.It's 2003: Add a caching layer.It's 2004: Add scale-out and
partitioning.It's 2005: Add asynchronous job scheduling and maybe a
distributed file system.It's 2007: Move it all into the cloud.It's 2008: Cloud
+ web scalable database.It'</p><p>6 0.1572963 <a title="910-tfidf-6" href="../high_scalability-2010/high_scalability-2010-11-09-Facebook_Uses_Non-Stored_Procedures_to_Update_Social_Graphs.html">936 high scalability-2010-11-09-Facebook Uses Non-Stored Procedures to Update Social Graphs</a></p>
<p>7 0.15627344 <a title="910-tfidf-7" href="../high_scalability-2012/high_scalability-2012-11-05-Gone_Fishin%27%3A_Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_In_The_Ambient_Cloud.html">1355 high scalability-2012-11-05-Gone Fishin': Building Super Scalable Systems: Blade Runner Meets Autonomic Computing In The Ambient Cloud</a></p>
<p>8 0.15625037 <a title="910-tfidf-8" href="../high_scalability-2009/high_scalability-2009-12-16-Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_in_the_Ambient_Cloud.html">750 high scalability-2009-12-16-Building Super Scalable Systems: Blade Runner Meets Autonomic Computing in the Ambient Cloud</a></p>
<p>9 0.15272412 <a title="910-tfidf-9" href="../high_scalability-2008/high_scalability-2008-08-04-A_Bunch_of_Great_Strategies_for_Using_Memcached_and_MySQL_Better_Together.html">360 high scalability-2008-08-04-A Bunch of Great Strategies for Using Memcached and MySQL Better Together</a></p>
<p>10 0.14932698 <a title="910-tfidf-10" href="../high_scalability-2010/high_scalability-2010-09-11-Google%27s_Colossus_Makes_Search_Real-time_by_Dumping_MapReduce.html">900 high scalability-2010-09-11-Google's Colossus Makes Search Real-time by Dumping MapReduce</a></p>
<p>11 0.14566241 <a title="910-tfidf-11" href="../high_scalability-2012/high_scalability-2012-03-06-Ask_For_Forgiveness_Programming_-_Or_How_We%27ll_Program_1000_Cores.html">1204 high scalability-2012-03-06-Ask For Forgiveness Programming - Or How We'll Program 1000 Cores</a></p>
<p>12 0.14512189 <a title="910-tfidf-12" href="../high_scalability-2009/high_scalability-2009-10-13-Why_are_Facebook%2C_Digg%2C_and_Twitter_so_hard_to_scale%3F.html">721 high scalability-2009-10-13-Why are Facebook, Digg, and Twitter so hard to scale?</a></p>
<p>13 0.13762665 <a title="910-tfidf-13" href="../high_scalability-2007/high_scalability-2007-08-16-Scaling_Secret_%232%3A_Denormalizing_Your_Way_to_Speed_and_Profit.html">65 high scalability-2007-08-16-Scaling Secret #2: Denormalizing Your Way to Speed and Profit</a></p>
<p>14 0.13762471 <a title="910-tfidf-14" href="../high_scalability-2007/high_scalability-2007-09-18-Amazon_Architecture.html">96 high scalability-2007-09-18-Amazon Architecture</a></p>
<p>15 0.13545394 <a title="910-tfidf-15" href="../high_scalability-2010/high_scalability-2010-12-06-What_the_heck_are_you_actually_using_NoSQL_for%3F.html">954 high scalability-2010-12-06-What the heck are you actually using NoSQL for?</a></p>
<p>16 0.13453002 <a title="910-tfidf-16" href="../high_scalability-2012/high_scalability-2012-05-07-Startups_are_Creating_a_New_System_of_the_World_for_IT.html">1240 high scalability-2012-05-07-Startups are Creating a New System of the World for IT</a></p>
<p>17 0.13434921 <a title="910-tfidf-17" href="../high_scalability-2011/high_scalability-2011-04-06-Netflix%3A_Run_Consistency_Checkers_All_the_time_to_Fixup_Transactions.html">1017 high scalability-2011-04-06-Netflix: Run Consistency Checkers All the time to Fixup Transactions</a></p>
<p>18 0.13356875 <a title="910-tfidf-18" href="../high_scalability-2013/high_scalability-2013-03-11-Low_Level_Scalability_Solutions_-_The_Conditioning_Collection.html">1421 high scalability-2013-03-11-Low Level Scalability Solutions - The Conditioning Collection</a></p>
<p>19 0.13328467 <a title="910-tfidf-19" href="../high_scalability-2009/high_scalability-2009-06-27-Scaling_Twitter%3A_Making_Twitter_10000_Percent_Faster.html">639 high scalability-2009-06-27-Scaling Twitter: Making Twitter 10000 Percent Faster</a></p>
<p>20 0.13232878 <a title="910-tfidf-20" href="../high_scalability-2014/high_scalability-2014-04-16-Six_Lessons_Learned_the_Hard_Way_About_Scaling_a_Million_User_System_.html">1633 high scalability-2014-04-16-Six Lessons Learned the Hard Way About Scaling a Million User System </a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.269), (1, 0.158), (2, -0.048), (3, -0.045), (4, 0.043), (5, 0.003), (6, 0.019), (7, 0.029), (8, -0.057), (9, -0.076), (10, -0.035), (11, 0.096), (12, -0.017), (13, 0.03), (14, 0.012), (15, -0.063), (16, 0.009), (17, -0.001), (18, 0.007), (19, 0.044), (20, -0.022), (21, 0.037), (22, 0.117), (23, 0.075), (24, -0.004), (25, 0.021), (26, 0.03), (27, 0.043), (28, -0.001), (29, 0.036), (30, -0.129), (31, 0.011), (32, 0.001), (33, 0.033), (34, -0.004), (35, 0.054), (36, 0.005), (37, -0.005), (38, 0.05), (39, 0.042), (40, -0.028), (41, -0.076), (42, -0.039), (43, 0.011), (44, -0.01), (45, 0.012), (46, -0.0), (47, 0.027), (48, -0.014), (49, 0.01)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97644722 <a title="910-lsi-1" href="../high_scalability-2010/high_scalability-2010-09-30-Facebook_and_Site_Failures_Caused_by_Complex%2C_Weakly_Interacting%2C_Layered_Systems.html">910 high scalability-2010-09-30-Facebook and Site Failures Caused by Complex, Weakly Interacting, Layered Systems</a></p>
<p>Introduction: Facebook has been so reliable that when a site outage does occur it's a
definite learning opportunity. Fortunately for us we can learn something
because in More Details on Today's Outage, Facebook'sRobert Johnson gave a
pretty candid explanation of what caused a rare 2.5 hour period of down time
for Facebook. It wasn't a simple problem. The root causes were feedback loops
and transient spikes caused ultimately by the complexity of weakly interacting
layers in modern systems. You know, the kind everyone is building these days.
Problems like this are notoriously hard to fix and finding a real solution may
send Facebook back to the whiteboard. There's a technical debt that must be
paid. The outline and my interpretation (reading between the lines) of what
happened is:Remember that Facebookcaches everything. They
have28terabytesofmemcacheddata on 800 servers. The database is the system of
record, but memory is where the action is. So when a problem happens that
involves the caching layer,</p><p>2 0.82082915 <a title="910-lsi-2" href="../high_scalability-2014/high_scalability-2014-04-16-Six_Lessons_Learned_the_Hard_Way_About_Scaling_a_Million_User_System_.html">1633 high scalability-2014-04-16-Six Lessons Learned the Hard Way About Scaling a Million User System </a></p>
<p>Introduction: Ever come to a point where you feel you've learned enough to share your
experiences in the hopes of helping others traveling the same road? That's
whatMartin Kleppmannhas done in an lovingly written Six things I wish we had
known about scaling, an article well worth your time.It's not advice about
scaling a Twitter, but of building a million user system, which is the sweet
spot for a lot of projects. His conclusion rings true:Building scalable
systems is not all sexy roflscale fun. It's a lot of plumbing and yak shaving.
A lot of hacking together tools that really ought to exist already, but all
the open source solutions out there are too bad (and yours ends up bad too,
but at least it solves your particular problem).Here's a gloss on the six
lessons (plus a bonus lesson):Realistic load testing is hard. Testing a large
distributed system is not like a scientific experiment that can be conducted
under ideal conditions. This is hard for the scientific minded to accept.
Knowing your actua</p><p>3 0.80003226 <a title="910-lsi-3" href="../high_scalability-2010/high_scalability-2010-10-14-I%2C_Cloud.html">919 high scalability-2010-10-14-I, Cloud</a></p>
<p>Introduction: Every time a technological innovation has spurred automation - since the time
of Henry Ford right up to a minute ago - someone has claimed that machines
will displace human beings. But the rainbow and unicorn dream attributed to
business stakeholders everywhere, i.e. the elimination of IT, is just that - a
dream. It isn't realistic and in fact it's downright silly to think that
systems that only a few years ago were unable to automatically scale up and
scale down will suddenly be able to perform the complex analysis required of
IT to keep the business running.The rare reports of the elimination of IT
staff due tocloud computingand automation are highlighted in the news because
they evoke visceral reactions in technologists everywhere and, to be honest,
they get the click counts rising. But thejury remains out on this oneand in
fact many postulate that it is not a reduction in staff that will occur, but
atransformationof staff, which may eliminate some old timey positions (think
sysadmi</p><p>4 0.78548163 <a title="910-lsi-4" href="../high_scalability-2009/high_scalability-2009-11-16-Building_Scalable_Systems_Using_Data_as_a_Composite_Material.html">741 high scalability-2009-11-16-Building Scalable Systems Using Data as a Composite Material</a></p>
<p>Introduction: Think of building websites as engineeringcomposite materials. A composite
material is when two or more materials are combined to create a third material
that does something useful that the components couldn't do on their own.
Composites like reinforced concrete have revolutionized design and
construction. When building websites we usually bring different component
materials together, like creating a composite, to get the features we need
rather than building a completely new thing from scratch that does everything
we want.This approach has been seen as a hack because it leads to inelegancies
like data duplication; great gobs of component glue; consistency issues; and
messy operations. But what if the the composite approach is really a strength,
not a hack, but a messy part of the world that needs to be embraced rather
than belittled?They key is tosee data as a material. Right now we are arguing
which is the best single material to build with. Is itNoSQL, relational,
massively parallel,</p><p>5 0.77943236 <a title="910-lsi-5" href="../high_scalability-2008/high_scalability-2008-07-16-The_Mother_of_All_Database_Normalization_Debates_on_Coding_Horror.html">351 high scalability-2008-07-16-The Mother of All Database Normalization Debates on Coding Horror</a></p>
<p>Introduction: Jeff Atwood started a barn burner of a conversation inMaybe Normalizing Isn't
Normalon how to create a fast scalable tagging system. Jeff eventually asks
that terrible question:which is better -- a normalized database, or a
denormalized database?And all hell breaks loose. I know, it's hard to imagine
database debates becoming contentious, but it does happen :-) It's lucky
developers don't have temporal power or rivers of blood would flow.Here are a
few of the pithier points (summarized):breakNormalization is not magical fairy
dust you sprinkle over your database to cure all ills; it often creates as
many problems as it solves. (Jeff)Normalize until it hurts, denormalize until
it works. (Jeff)Use materialized views which are tables created and maintained
by your RDBMS. So a materialized view will act exactly like a de-normalized
table would - except you keep you original normalized structure and any change
to original data will propagate to the view automatically. (Goran)According to
Co</p><p>6 0.77611113 <a title="910-lsi-6" href="../high_scalability-2007/high_scalability-2007-12-21-Strategy%3A_Limit_Result_Sets.html">189 high scalability-2007-12-21-Strategy: Limit Result Sets</a></p>
<p>7 0.77475858 <a title="910-lsi-7" href="../high_scalability-2010/high_scalability-2010-05-17-7_Lessons_Learned_While_Building_Reddit_to_270_Million_Page_Views_a_Month.html">828 high scalability-2010-05-17-7 Lessons Learned While Building Reddit to 270 Million Page Views a Month</a></p>
<p>8 0.77164555 <a title="910-lsi-8" href="../high_scalability-2012/high_scalability-2012-04-17-YouTube_Strategy%3A_Adding_Jitter_isn%27t_a_Bug.html">1229 high scalability-2012-04-17-YouTube Strategy: Adding Jitter isn't a Bug</a></p>
<p>9 0.76817793 <a title="910-lsi-9" href="../high_scalability-2012/high_scalability-2012-03-14-The_Azure_Outage%3A_Time_Is_a_SPOF%2C_Leap_Day_Doubly_So.html">1209 high scalability-2012-03-14-The Azure Outage: Time Is a SPOF, Leap Day Doubly So</a></p>
<p>10 0.767555 <a title="910-lsi-10" href="../high_scalability-2008/high_scalability-2008-09-30-Scalability_Worst_Practices.html">398 high scalability-2008-09-30-Scalability Worst Practices</a></p>
<p>11 0.75523603 <a title="910-lsi-11" href="../high_scalability-2008/high_scalability-2008-02-12-We_want_to_cache_a_lot_%3A%29_How_do_we_go_about_it_%3F.html">247 high scalability-2008-02-12-We want to cache a lot :) How do we go about it ?</a></p>
<p>12 0.75317538 <a title="910-lsi-12" href="../high_scalability-2010/high_scalability-2010-05-10-Sify.com_Architecture_-_A_Portal_at_3900_Requests_Per_Second.html">825 high scalability-2010-05-10-Sify.com Architecture - A Portal at 3900 Requests Per Second</a></p>
<p>13 0.74880153 <a title="910-lsi-13" href="../high_scalability-2008/high_scalability-2008-04-02-Product%3A_Supervisor_-__Monitor_and_Control_Your_Processes.html">295 high scalability-2008-04-02-Product: Supervisor -  Monitor and Control Your Processes</a></p>
<p>14 0.74865592 <a title="910-lsi-14" href="../high_scalability-2012/high_scalability-2012-02-27-Zen_and_the_Art_of_Scaling_-_A_Koan_and_Epigram_Approach.html">1199 high scalability-2012-02-27-Zen and the Art of Scaling - A Koan and Epigram Approach</a></p>
<p>15 0.7443285 <a title="910-lsi-15" href="../high_scalability-2012/high_scalability-2012-03-26-7_Years_of_YouTube_Scalability_Lessons_in_30_Minutes.html">1215 high scalability-2012-03-26-7 Years of YouTube Scalability Lessons in 30 Minutes</a></p>
<p>16 0.74406499 <a title="910-lsi-16" href="../high_scalability-2009/high_scalability-2009-05-28-Scaling_PostgreSQL_using_CUDA.html">609 high scalability-2009-05-28-Scaling PostgreSQL using CUDA</a></p>
<p>17 0.7436831 <a title="910-lsi-17" href="../high_scalability-2009/high_scalability-2009-08-07-Strategy%3A_Break_Up_the_Memcache_Dog_Pile_.html">673 high scalability-2009-08-07-Strategy: Break Up the Memcache Dog Pile </a></p>
<p>18 0.73999286 <a title="910-lsi-18" href="../high_scalability-2008/high_scalability-2008-07-15-ZooKeeper_-_A_Reliable%2C_Scalable_Distributed_Coordination_System_.html">350 high scalability-2008-07-15-ZooKeeper - A Reliable, Scalable Distributed Coordination System </a></p>
<p>19 0.7350924 <a title="910-lsi-19" href="../high_scalability-2013/high_scalability-2013-08-19-What_can_the_Amazing_Race_to_the_South_Pole_Teach_us_About_Startups%3F.html">1503 high scalability-2013-08-19-What can the Amazing Race to the South Pole Teach us About Startups?</a></p>
<p>20 0.73504281 <a title="910-lsi-20" href="../high_scalability-2012/high_scalability-2012-06-25-StubHub_Architecture%3A_The_Surprising_Complexity_Behind_the_World%E2%80%99s_Largest_Ticket_Marketplace.html">1271 high scalability-2012-06-25-StubHub Architecture: The Surprising Complexity Behind the World’s Largest Ticket Marketplace</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.086), (2, 0.362), (10, 0.037), (30, 0.032), (40, 0.018), (41, 0.065), (47, 0.012), (49, 0.025), (61, 0.06), (77, 0.019), (79, 0.109), (85, 0.064), (94, 0.032)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98373902 <a title="910-lda-1" href="../high_scalability-2010/high_scalability-2010-09-30-Facebook_and_Site_Failures_Caused_by_Complex%2C_Weakly_Interacting%2C_Layered_Systems.html">910 high scalability-2010-09-30-Facebook and Site Failures Caused by Complex, Weakly Interacting, Layered Systems</a></p>
<p>Introduction: Facebook has been so reliable that when a site outage does occur it's a
definite learning opportunity. Fortunately for us we can learn something
because in More Details on Today's Outage, Facebook'sRobert Johnson gave a
pretty candid explanation of what caused a rare 2.5 hour period of down time
for Facebook. It wasn't a simple problem. The root causes were feedback loops
and transient spikes caused ultimately by the complexity of weakly interacting
layers in modern systems. You know, the kind everyone is building these days.
Problems like this are notoriously hard to fix and finding a real solution may
send Facebook back to the whiteboard. There's a technical debt that must be
paid. The outline and my interpretation (reading between the lines) of what
happened is:Remember that Facebookcaches everything. They
have28terabytesofmemcacheddata on 800 servers. The database is the system of
record, but memory is where the action is. So when a problem happens that
involves the caching layer,</p><p>2 0.97749639 <a title="910-lda-2" href="../high_scalability-2013/high_scalability-2013-01-15-More_Numbers_Every_Awesome_Programmer_Must_Know.html">1387 high scalability-2013-01-15-More Numbers Every Awesome Programmer Must Know</a></p>
<p>Introduction: Colin Scott, a Berkeley researcher, updated Jeff Dean's famousNumbers Everyone
Should Knowwith hisLatency Numbers Every Programmer Should Knowinteractive
graphic. The interactive aspect is cool because it has a slider that let's you
see numbers back from as early as 1990 to the far far future of 2020. Colin
explained hismotivation for updating the numbers:The other day, a friend
mentioned a latency number to me, and I realized that it was an order of
magnitude smaller than what I had memorized from Jeff's talk. The problem, of
course, is that hardware performance increases exponentially! After some
digging, I actually found that the numbers Jeff quotes are over a decade
oldSince numbers without interpretation are simply data, take a look atGoogle
Pro Tip: Use Back-Of-The-Envelope-Calculations To Choose The Best Design. The
idea is back-of-the-envelope calculations are estimates you create using a
combination of thought experiments and common performance numbers to a get a
good feel for</p><p>3 0.97652471 <a title="910-lda-3" href="../high_scalability-2011/high_scalability-2011-03-09-Google_and_Netflix_Strategy%3A_Use_Partial_Responses_to_Reduce_Request_Sizes.html">1001 high scalability-2011-03-09-Google and Netflix Strategy: Use Partial Responses to Reduce Request Sizes</a></p>
<p>Introduction: This strategy targets reducing the amount of protocol data in packets by
sending only the attributes that are needed. Google calls this Partial
Response and Partial Update.Netflix posted about adopting this strategy in
their recent Netflix API redesign. We've seen previously how Netflix improved
performance by creatingless chatty protocols.As a consequence packet sizes
rise as more data is being stuffed into each packet in order to reduce the
number of round trips. But we don't like large packets either (memory usage
and packet processing overhead), so we have to think of creative ways to
shrink them back down.The change Netflx is making is to conceptualize their
API as a database. What does this mean?Apartial responseis like a SQL select
statement where you can specify only the fields you want back. Only the
attributes of interest are requested.  Previously all fields for objects were
returned, even if the client didn't need them. So the goal is reduce payload
sizes by being more sele</p><p>4 0.97514153 <a title="910-lda-4" href="../high_scalability-2013/high_scalability-2013-03-06-Low_Level_Scalability_Solutions_-_The_Aggregation_Collection.html">1418 high scalability-2013-03-06-Low Level Scalability Solutions - The Aggregation Collection</a></p>
<p>Introduction: What good are problems without solutions? In 42 Monster Problems That Attack
As Loads Increase we talked about problems. In this first post (OK, there was
an earlier post, but I'm doing some reorganizing), we'll cover what I
callaggregation strategies.Keep in mind these are low level architecture type
suggestions of how to structure the components of your code and how they
interact. We're not talking about massive scale-out clusters here, but of what
your applications might like like internally, way below the service level
interface level. There's a lot more to the world than evented
architectures.Aggregation simply means we aren't using stupid queues. Our
queues will be smart. We are deeply aware of queues as containers of work that
eventually dictate how the entire system performs. As work containers we know
intimately what requests and data sit in our queues and we can use that
intelligence to our great advantage.Prioritize WorkThe key idea to it all is
an almost mindful approach to</p><p>5 0.9714905 <a title="910-lda-5" href="../high_scalability-2013/high_scalability-2013-02-27-42_Monster_Problems_that_Attack_as_Loads_Increase.html">1413 high scalability-2013-02-27-42 Monster Problems that Attack as Loads Increase</a></p>
<p>Introduction: For solutions take a look at:7 Life Saving Scalability Defenses Against Load
Monster Attacks.This is a look at all the bad things that can happen to your
carefully crafted program as loads increase: all hell breaks lose. Sure, you
can scale out or scale up, but you can also choose to program better. Make
your system handle larger loads. This saves money because fewer boxes are
needed and it will make the entire application more reliable and have better
response times. And it can be quite satisfying as a programmer.Large Number Of
ObjectsWe usually get into scaling problems when the number of objects gets
larger. Clearly resource usage of all types is stressed as the number of
objects grow.Continuous Failures Makes An Infinite Event StreamDuring large
network failure scenarios there is never time for the system recover. We are
in a continual state of stress.Lots of High Priority WorkFor example,
rerouting is a high priority activity. If there is a large amount of rerouting
work that can</p><p>6 0.97065157 <a title="910-lda-6" href="../high_scalability-2013/high_scalability-2013-05-13-The_Secret_to_10_Million_Concurrent_Connections_-The_Kernel_is_the_Problem%2C_Not_the_Solution.html">1456 high scalability-2013-05-13-The Secret to 10 Million Concurrent Connections -The Kernel is the Problem, Not the Solution</a></p>
<p>7 0.9701032 <a title="910-lda-7" href="../high_scalability-2013/high_scalability-2013-03-25-AppBackplane_-_A_Framework_for_Supporting_Multiple_Application_Architectures.html">1429 high scalability-2013-03-25-AppBackplane - A Framework for Supporting Multiple Application Architectures</a></p>
<p>8 0.96991229 <a title="910-lda-8" href="../high_scalability-2008/high_scalability-2008-01-24-Mailinator_Architecture.html">221 high scalability-2008-01-24-Mailinator Architecture</a></p>
<p>9 0.96958417 <a title="910-lda-9" href="../high_scalability-2008/high_scalability-2008-07-29-Ehcache_-_A_Java_Distributed_Cache_.html">359 high scalability-2008-07-29-Ehcache - A Java Distributed Cache </a></p>
<p>10 0.96910077 <a title="910-lda-10" href="../high_scalability-2013/high_scalability-2013-12-23-What_Happens_While_Your_Brain_Sleeps_is_Surprisingly_Like_How_Computers_Stay_Sane.html">1568 high scalability-2013-12-23-What Happens While Your Brain Sleeps is Surprisingly Like How Computers Stay Sane</a></p>
<p>11 0.96882951 <a title="910-lda-11" href="../high_scalability-2013/high_scalability-2013-03-11-Low_Level_Scalability_Solutions_-_The_Conditioning_Collection.html">1421 high scalability-2013-03-11-Low Level Scalability Solutions - The Conditioning Collection</a></p>
<p>12 0.96870345 <a title="910-lda-12" href="../high_scalability-2012/high_scalability-2012-03-06-Ask_For_Forgiveness_Programming_-_Or_How_We%27ll_Program_1000_Cores.html">1204 high scalability-2012-03-06-Ask For Forgiveness Programming - Or How We'll Program 1000 Cores</a></p>
<p>13 0.96784306 <a title="910-lda-13" href="../high_scalability-2013/high_scalability-2013-06-13-Busting_4_Modern_Hardware_Myths_-_Are_Memory%2C_HDDs%2C_and_SSDs_Really_Random_Access%3F.html">1475 high scalability-2013-06-13-Busting 4 Modern Hardware Myths - Are Memory, HDDs, and SSDs Really Random Access?</a></p>
<p>14 0.96730536 <a title="910-lda-14" href="../high_scalability-2012/high_scalability-2012-06-18-Google_on_Latency_Tolerant_Systems%3A_Making_a_Predictable_Whole_Out_of_Unpredictable_Parts___.html">1266 high scalability-2012-06-18-Google on Latency Tolerant Systems: Making a Predictable Whole Out of Unpredictable Parts   </a></p>
<p>15 0.96673596 <a title="910-lda-15" href="../high_scalability-2007/high_scalability-2007-08-29-Skype_Failed_the_Boot_Scalability_Test%3A_Is_P2P_fundamentally_flawed%3F.html">76 high scalability-2007-08-29-Skype Failed the Boot Scalability Test: Is P2P fundamentally flawed?</a></p>
<p>16 0.96635115 <a title="910-lda-16" href="../high_scalability-2014/high_scalability-2014-02-05-Little%E2%80%99s_Law%2C_Scalability_and_Fault_Tolerance%3A_The_OS_is_your_bottleneck._What_you_can_do%3F.html">1591 high scalability-2014-02-05-Little’s Law, Scalability and Fault Tolerance: The OS is your bottleneck. What you can do?</a></p>
<p>17 0.96606272 <a title="910-lda-17" href="../high_scalability-2008/high_scalability-2008-05-14-New_Facebook_Chat_Feature_Scales_to_70_Million_Users_Using_Erlang.html">318 high scalability-2008-05-14-New Facebook Chat Feature Scales to 70 Million Users Using Erlang</a></p>
<p>18 0.96596229 <a title="910-lda-18" href="../high_scalability-2008/high_scalability-2008-10-08-Strategy%3A_Flickr_-_Do_the_Essential_Work_Up-front_and_Queue_the_Rest_.html">406 high scalability-2008-10-08-Strategy: Flickr - Do the Essential Work Up-front and Queue the Rest </a></p>
<p>19 0.96563798 <a title="910-lda-19" href="../high_scalability-2011/high_scalability-2011-03-17-Are_long_VM_instance_spin-up_times_in_the_cloud_costing_you_money%3F.html">1006 high scalability-2011-03-17-Are long VM instance spin-up times in the cloud costing you money?</a></p>
<p>20 0.96501148 <a title="910-lda-20" href="../high_scalability-2011/high_scalability-2011-04-06-Netflix%3A_Run_Consistency_Checkers_All_the_time_to_Fixup_Transactions.html">1017 high scalability-2011-04-06-Netflix: Run Consistency Checkers All the time to Fixup Transactions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
