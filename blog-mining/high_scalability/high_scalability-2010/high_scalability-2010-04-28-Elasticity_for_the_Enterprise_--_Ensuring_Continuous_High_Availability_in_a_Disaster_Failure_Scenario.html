<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>816 high scalability-2010-04-28-Elasticity for the Enterprise -- Ensuring Continuous High Availability in a Disaster Failure Scenario</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2010" href="../home/high_scalability-2010_home.html">high_scalability-2010</a> <a title="high_scalability-2010-816" href="#">high_scalability-2010-816</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>816 high scalability-2010-04-28-Elasticity for the Enterprise -- Ensuring Continuous High Availability in a Disaster Failure Scenario</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2010-816-html" href="http://highscalability.com//blog/2010/4/28/elasticity-for-the-enterprise-ensuring-continuous-high-avail.html">html</a></p><p>Introduction: Many enterprises' high-availability architecture is based on the assumption
that you can prevent failure from happening by putting all your critical data
in a centralized database, back it up with expensive storage, and replicate it
somehow between the sites. As I argued in one of my previous posts (Why
Existing Databases (RAC) are So Breakable!) many of those assumptions are
broken at their core, as storage is doomed to failure just like any other
device, expensive hardware doesn't make things any better and database
replication is often not enough.One of the main lessons that we can take from
the likes of Amazon and Google is that the right way to ensure continuous high
availability is by designing our system to cope with failure. We need to
assume that what we tend to think of as unthinkable will probably happen, as
that's the nature of failure. So rather than trying to prevent failures, we
need to build a system that will tolerate them.As we can learn from a recent
outage event in</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Many enterprises' high-availability architecture is based on the assumption that you can prevent failure from happening by putting all your critical data in a centralized database, back it up with expensive storage, and replicate it somehow between the sites. [sent-1, score-1.273]
</p><p>2 As I argued in one of my previous posts (Why Existing Databases (RAC) are So Breakable! [sent-2, score-0.33]
</p><p>3 ) many of those assumptions are broken at their core, as storage is doomed to failure just like any other device, expensive hardware doesn't make things any better and database replication is often not enough. [sent-3, score-0.837]
</p><p>4 One of the main lessons that we can take from the likes of Amazon and Google is that the right way to ensure continuous high availability is by designing our system to cope with failure. [sent-4, score-0.47]
</p><p>5 We need to assume that what we tend to think of as unthinkable will probably happen, as that's the nature of failure. [sent-5, score-0.457]
</p><p>6 So rather than trying to prevent failures, we need to build a system that will tolerate them. [sent-6, score-0.333]
</p><p>7 As we can learn from a recent outage event in one of Amazon's cloud data centers, we can't rely on the data center alone to solve this type of failure. [sent-7, score-0.313]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('failure', 0.312), ('breakable', 0.201), ('prevent', 0.198), ('unthinkable', 0.18), ('doomed', 0.163), ('rac', 0.159), ('faq', 0.159), ('argued', 0.156), ('street', 0.15), ('availability', 0.148), ('somehow', 0.143), ('firm', 0.141), ('cope', 0.135), ('tolerate', 0.135), ('assumptions', 0.131), ('amazon', 0.13), ('expensive', 0.129), ('ensuring', 0.127), ('launching', 0.123), ('assumption', 0.122), ('enterprises', 0.12), ('wall', 0.119), ('discussed', 0.113), ('notes', 0.113), ('alone', 0.111), ('zones', 0.109), ('protect', 0.108), ('outage', 0.107), ('continues', 0.105), ('broken', 0.102), ('replicate', 0.102), ('tend', 0.101), ('scenario', 0.101), ('likes', 0.1), ('disaster', 0.1), ('assume', 0.097), ('centralized', 0.096), ('rely', 0.095), ('principles', 0.095), ('posts', 0.087), ('continuous', 0.087), ('happening', 0.087), ('previous', 0.087), ('elastic', 0.086), ('location', 0.086), ('device', 0.085), ('putting', 0.084), ('leading', 0.084), ('centers', 0.081), ('nature', 0.079)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="816-tfidf-1" href="../high_scalability-2010/high_scalability-2010-04-28-Elasticity_for_the_Enterprise_--_Ensuring_Continuous_High_Availability_in_a_Disaster_Failure_Scenario.html">816 high scalability-2010-04-28-Elasticity for the Enterprise -- Ensuring Continuous High Availability in a Disaster Failure Scenario</a></p>
<p>Introduction: Many enterprises' high-availability architecture is based on the assumption
that you can prevent failure from happening by putting all your critical data
in a centralized database, back it up with expensive storage, and replicate it
somehow between the sites. As I argued in one of my previous posts (Why
Existing Databases (RAC) are So Breakable!) many of those assumptions are
broken at their core, as storage is doomed to failure just like any other
device, expensive hardware doesn't make things any better and database
replication is often not enough.One of the main lessons that we can take from
the likes of Amazon and Google is that the right way to ensure continuous high
availability is by designing our system to cope with failure. We need to
assume that what we tend to think of as unthinkable will probably happen, as
that's the nature of failure. So rather than trying to prevent failures, we
need to build a system that will tolerate them.As we can learn from a recent
outage event in</p><p>2 0.16586497 <a title="816-tfidf-2" href="../high_scalability-2011/high_scalability-2011-08-15-Should_any_cloud_be_considered_one_availability_zone%3F_The_Amazon_experience_says_yes..html">1098 high scalability-2011-08-15-Should any cloud be considered one availability zone? The Amazon experience says yes.</a></p>
<p>Introduction: Amazon has a very will written account of their 8/8/2011 downtime:Summary of
the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West
Region. Power failed, backup generators failed to kick in, there weren't
enough resources for EBS volumes to recover, API servers where overwhelmed, a
DNS failure caused failovers to alternate availability zones to fail, a double
fault occurred as the power event interrupted the repair of a different bug.
All kind of typical stuff that just seems to happen.Considering the previous
outage, the big question for programmers is: what does this mean? What does it
mean for how systems should be structured? Have we learned something that
can't be unlearned?The Amazon post has lots of good insights into how EBS and
RDS work, plus lessons learned. The short of the problem is large + complex =
high probability of failure. The immediate fixes are adding more resources,
more redundancy, more isolation between components, more automation, reduce
recove</p><p>3 0.16272381 <a title="816-tfidf-3" href="../high_scalability-2011/high_scalability-2011-06-09-Retrospect_on_recent_AWS_outage_and_Resilient_Cloud-Based_Architecture.html">1056 high scalability-2011-06-09-Retrospect on recent AWS outage and Resilient Cloud-Based Architecture</a></p>
<p>Introduction: A bit over a month ago Amazon experienced its infamous AWS outage in the US
East Region. As a cloud evangelist, I was intrigued by the history of the
outage as it occurred. There were great posts during and after the outage from
those who went down. But more interestingly for me as architect were the
detailed posts of those who managed to survive the outage relatively unharmed,
such as SimpleGeo, Netflix,SmugMug, SmugMug's CTO, Twilio, Bizo and
others.Reading through the experience of others, I tried to summarize
thepatterns, principles and best practicesthat emerged from these posts, as I
believe we can learn a lot from them on how to design our business
applications to truly leverage on the benefits that the cloud offers in high
availability and scalability.The main principles, patterns and best practices
are:Design for failureStateless and autonomous servicesRedundant hot copies
spread across zonesSpread across several public cloud vendors and/or private
cloudAutomation and monitori</p><p>4 0.14079243 <a title="816-tfidf-4" href="../high_scalability-2011/high_scalability-2011-12-28-Strategy%3A_Guaranteed_Availability_Requires_Reserving_Instances_in_Specific_Zones.html">1165 high scalability-2011-12-28-Strategy: Guaranteed Availability Requires Reserving Instances in Specific Zones</a></p>
<p>Introduction: When EC2 first started the mental model was of a magic Pez dispenser supplying
an infinite stream of instances in any desired flavor. If you needed an
instance, because of a either a failure or traffic spike, it would be there.
As amazing as EC2 is, this model turned out to be optimistic.  From athread on
the Amazon discussion forum we learn any dispenser has limits:As Availability
Zones grow over time, our ability to continue to expand them can become
constrained. In these scenarios, we will prevent customers from launching in
the constrained zone if they do not yet have existing resources in that zone.
We also might remove the constrained zone entirely from the list of options
for new customers. This means that occasionally, different customers will see
a different number of Availability Zones in a particular Region. Both
approaches aim to help customers avoid accidentally starting to build up their
infrastructure in an Availability Zone where they might have less ability to
expand.T</p><p>5 0.13721874 <a title="816-tfidf-5" href="../high_scalability-2008/high_scalability-2008-03-27-Amazon_Announces_Static_IP_Addresses_and_Multiple_Datacenter_Operation.html">289 high scalability-2008-03-27-Amazon Announces Static IP Addresses and Multiple Datacenter Operation</a></p>
<p>Introduction: Amazon is fixing two of their major problems: no static IP addresses and
single datacenter operation. By adding these two new features developers can
finally build a no apology system on Amazon. Before you always had to throw in
an apology or two. No, we don't have low failover times because of the silly
DNS games and unexceptionable DNS update and propagation times and no, we
don't operate in more than one datacenter. No more. Now Amazon is
addingElastic IP AddressesandAvailability Zones.Elastic IP addresses are far
better than normal IP addresses because they are both in tight withJessica
Albaand they are:breakStatic IP addresses designed for dynamic cloud
computing. An Elastic IP address is associated with your account, not a
particular instance, and you control that address until you choose to
explicitly release it. Unlike traditional static IP addresses, however,
Elastic IP addresses allow you to mask instance or availability zone failures
by programmatically remapping your public</p><p>6 0.1317061 <a title="816-tfidf-6" href="../high_scalability-2011/high_scalability-2011-05-02-The_Updated_Big_List_of_Articles_on_the_Amazon_Outage.html">1033 high scalability-2011-05-02-The Updated Big List of Articles on the Amazon Outage</a></p>
<p>7 0.1302765 <a title="816-tfidf-7" href="../high_scalability-2014/high_scalability-2014-03-03-The_%E2%80%9CFour_Hamiltons%E2%80%9D_Framework_for_Mitigating_Faults_in_the_Cloud%3A_Avoid_it%2C_Mask_it%2C_Bound_it%2C_Fix_it_Fast.html">1604 high scalability-2014-03-03-The “Four Hamiltons” Framework for Mitigating Faults in the Cloud: Avoid it, Mask it, Bound it, Fix it Fast</a></p>
<p>8 0.11734532 <a title="816-tfidf-8" href="../high_scalability-2009/high_scalability-2009-11-30-Why_Existing_Databases_%28RAC%29_are_So_Breakable%21.html">748 high scalability-2009-11-30-Why Existing Databases (RAC) are So Breakable!</a></p>
<p>9 0.11671998 <a title="816-tfidf-9" href="../high_scalability-2007/high_scalability-2007-07-30-Build_an_Infinitely_Scalable_Infrastructure_for_%24100_Using_Amazon_Services.html">38 high scalability-2007-07-30-Build an Infinitely Scalable Infrastructure for $100 Using Amazon Services</a></p>
<p>10 0.11610215 <a title="816-tfidf-10" href="../high_scalability-2011/high_scalability-2011-04-25-The_Big_List_of_Articles_on_the_Amazon_Outage.html">1029 high scalability-2011-04-25-The Big List of Articles on the Amazon Outage</a></p>
<p>11 0.11537538 <a title="816-tfidf-11" href="../high_scalability-2010/high_scalability-2010-03-05-Strategy%3A_Planning_for_a_Power_Outage_Google_Style.html">789 high scalability-2010-03-05-Strategy: Planning for a Power Outage Google Style</a></p>
<p>12 0.11426295 <a title="816-tfidf-12" href="../high_scalability-2010/high_scalability-2010-01-04-11_Strategies_to_Rock_Your_Startup%E2%80%99s_Scalability_in_2010.html">757 high scalability-2010-01-04-11 Strategies to Rock Your Startup’s Scalability in 2010</a></p>
<p>13 0.11314247 <a title="816-tfidf-13" href="../high_scalability-2010/high_scalability-2010-05-04-Business_continuity_with_real-time_data_integration.html">822 high scalability-2010-05-04-Business continuity with real-time data integration</a></p>
<p>14 0.11016355 <a title="816-tfidf-14" href="../high_scalability-2012/high_scalability-2012-10-02-An_Epic_TripAdvisor_Update%3A_Why_Not_Run_on_the_Cloud%3F_The_Grand_Experiment..html">1331 high scalability-2012-10-02-An Epic TripAdvisor Update: Why Not Run on the Cloud? The Grand Experiment.</a></p>
<p>15 0.10832338 <a title="816-tfidf-15" href="../high_scalability-2007/high_scalability-2007-10-30-Paper%3A_Dynamo%3A_Amazon%E2%80%99s_Highly_Available_Key-value_Store.html">139 high scalability-2007-10-30-Paper: Dynamo: Amazon’s Highly Available Key-value Store</a></p>
<p>16 0.10666225 <a title="816-tfidf-16" href="../high_scalability-2008/high_scalability-2008-04-21-Google_App_Engine_-_what_about_existing_applications%3F.html">305 high scalability-2008-04-21-Google App Engine - what about existing applications?</a></p>
<p>17 0.10658719 <a title="816-tfidf-17" href="../high_scalability-2007/high_scalability-2007-07-30-Product%3A_Amazon_Elastic_Compute_Cloud.html">40 high scalability-2007-07-30-Product: Amazon Elastic Compute Cloud</a></p>
<p>18 0.10494471 <a title="816-tfidf-18" href="../high_scalability-2007/high_scalability-2007-09-18-Amazon_Architecture.html">96 high scalability-2007-09-18-Amazon Architecture</a></p>
<p>19 0.10179913 <a title="816-tfidf-19" href="../high_scalability-2010/high_scalability-2010-07-08-Cloud_AWS_Infrastructure_vs._Physical_Infrastructure.html">853 high scalability-2010-07-08-Cloud AWS Infrastructure vs. Physical Infrastructure</a></p>
<p>20 0.10164006 <a title="816-tfidf-20" href="../high_scalability-2012/high_scalability-2012-11-05-Are_we_seeing_the_renaissance_of_enterprises_in_the_cloud%3F.html">1354 high scalability-2012-11-05-Are we seeing the renaissance of enterprises in the cloud?</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.179), (1, 0.061), (2, 0.024), (3, 0.06), (4, -0.071), (5, -0.031), (6, 0.018), (7, -0.124), (8, 0.022), (9, -0.087), (10, -0.031), (11, 0.022), (12, -0.026), (13, -0.046), (14, 0.003), (15, 0.022), (16, 0.023), (17, -0.038), (18, 0.03), (19, 0.046), (20, 0.039), (21, 0.031), (22, -0.009), (23, 0.015), (24, -0.067), (25, -0.025), (26, -0.001), (27, 0.033), (28, 0.011), (29, 0.083), (30, -0.004), (31, -0.017), (32, 0.075), (33, -0.05), (34, 0.004), (35, 0.057), (36, -0.015), (37, 0.036), (38, 0.037), (39, 0.046), (40, 0.02), (41, -0.073), (42, 0.001), (43, -0.013), (44, 0.024), (45, -0.014), (46, -0.009), (47, -0.049), (48, -0.062), (49, -0.029)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96859097 <a title="816-lsi-1" href="../high_scalability-2010/high_scalability-2010-04-28-Elasticity_for_the_Enterprise_--_Ensuring_Continuous_High_Availability_in_a_Disaster_Failure_Scenario.html">816 high scalability-2010-04-28-Elasticity for the Enterprise -- Ensuring Continuous High Availability in a Disaster Failure Scenario</a></p>
<p>Introduction: Many enterprises' high-availability architecture is based on the assumption
that you can prevent failure from happening by putting all your critical data
in a centralized database, back it up with expensive storage, and replicate it
somehow between the sites. As I argued in one of my previous posts (Why
Existing Databases (RAC) are So Breakable!) many of those assumptions are
broken at their core, as storage is doomed to failure just like any other
device, expensive hardware doesn't make things any better and database
replication is often not enough.One of the main lessons that we can take from
the likes of Amazon and Google is that the right way to ensure continuous high
availability is by designing our system to cope with failure. We need to
assume that what we tend to think of as unthinkable will probably happen, as
that's the nature of failure. So rather than trying to prevent failures, we
need to build a system that will tolerate them.As we can learn from a recent
outage event in</p><p>2 0.78602076 <a title="816-lsi-2" href="../high_scalability-2011/high_scalability-2011-08-15-Should_any_cloud_be_considered_one_availability_zone%3F_The_Amazon_experience_says_yes..html">1098 high scalability-2011-08-15-Should any cloud be considered one availability zone? The Amazon experience says yes.</a></p>
<p>Introduction: Amazon has a very will written account of their 8/8/2011 downtime:Summary of
the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West
Region. Power failed, backup generators failed to kick in, there weren't
enough resources for EBS volumes to recover, API servers where overwhelmed, a
DNS failure caused failovers to alternate availability zones to fail, a double
fault occurred as the power event interrupted the repair of a different bug.
All kind of typical stuff that just seems to happen.Considering the previous
outage, the big question for programmers is: what does this mean? What does it
mean for how systems should be structured? Have we learned something that
can't be unlearned?The Amazon post has lots of good insights into how EBS and
RDS work, plus lessons learned. The short of the problem is large + complex =
high probability of failure. The immediate fixes are adding more resources,
more redundancy, more isolation between components, more automation, reduce
recove</p><p>3 0.7836495 <a title="816-lsi-3" href="../high_scalability-2011/high_scalability-2011-04-25-The_Big_List_of_Articles_on_the_Amazon_Outage.html">1029 high scalability-2011-04-25-The Big List of Articles on the Amazon Outage</a></p>
<p>Introduction: Please seeThe Updated Big List Of Articles On The Amazon Outage for a new
improved list.So many great articles have been written on the Amazon Outage.
Some aim at being helpful, some chastise developers for being so stupid, some
chastise Amazon for being so incompetent, some talk about the pain they and
their companies have experienced, and some even predict the downfall of the
cloud. Still others say we have seen a sea change in future of the cloud, a
prediction that's hard to disagree with, though the shape of the change
remains...cloudy.I'll try to keep this list update as more information comes
out. There will be a lot for developers to consider going forward. If there's
a resource you think should be added, just let me know.Amazon's Explanation of
What HappenedSummary of the Amazon EC2 and Amazon RDS Service Disruption in
the US East RegionHackers News thread on AWS Service Disruption Post Mortem
Quite Funny Commentary on the SummaryExperiences from Specific Companies, Both
Good a</p><p>4 0.76016724 <a title="816-lsi-4" href="../high_scalability-2011/high_scalability-2011-05-02-The_Updated_Big_List_of_Articles_on_the_Amazon_Outage.html">1033 high scalability-2011-05-02-The Updated Big List of Articles on the Amazon Outage</a></p>
<p>Introduction: SinceThe Big List Of Articles On The Amazon Outage was published we've a had
few updates that people might not have seen. Amazon of course released their
Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East
Region. Netlix shared theirLessons Learned from the AWS Outage as did Heroku
(How Heroku Survived the Amazon Outage), Smug Mug (How SmugMug survived the
Amazonpocalypse), and SimpleGeo (How SimpleGeo Stayed Up During the AWS
Downtime). The curious thing from my perspective is the general lack of
response to Amazon's explanation. I expected more discussion. There's been
almost none that I've seen. My guess is very few people understand what Amazon
was talking about enough to comment whereas almost everyone feels qualified to
talk about the event itself.Lesson for crisis handlers: deep dive post-mortems
that are timely, long, honestish, and highly technical are the most effective
means of staunching the downward spiral of media attention. Amazon's
Explanation of</p><p>5 0.75289899 <a title="816-lsi-5" href="../high_scalability-2014/high_scalability-2014-03-03-The_%E2%80%9CFour_Hamiltons%E2%80%9D_Framework_for_Mitigating_Faults_in_the_Cloud%3A_Avoid_it%2C_Mask_it%2C_Bound_it%2C_Fix_it_Fast.html">1604 high scalability-2014-03-03-The “Four Hamiltons” Framework for Mitigating Faults in the Cloud: Avoid it, Mask it, Bound it, Fix it Fast</a></p>
<p>Introduction: This is a guest post byPatrick Eaton, Software Engineer and Distributed
Systems Architect at Stackdriver.Stackdriver provides intelligent monitoring-
as-a-service for cloud hosted applications.  Behind this easy-to-use service
is a large distributed system for collecting and storing metrics and events,
monitoring and alerting on them, analyzing them, and serving up all the
results in a web UI.  Because we ourselves run in the cloud (mostly on AWS),
we spend a lot of time thinking about how to deal with faults in the cloud.
We have developed a framework for thinking about fault mitigation for large,
cloud-hosted systems.  We endearingly call this framework the "Four Hamiltons"
because it is inspired by an article from James Hamilton, the Vice President
and Distinguished Engineer at Amazon Web Services.The article that led to this
framework is called "The Power Failure Seen Around the World".  Hamilton
analyzes the causes of the power outage that affected Super Bowl XLVII in
early 2013.</p><p>6 0.74105126 <a title="816-lsi-6" href="../high_scalability-2007/high_scalability-2007-10-30-Paper%3A_Dynamo%3A_Amazon%E2%80%99s_Highly_Available_Key-value_Store.html">139 high scalability-2007-10-30-Paper: Dynamo: Amazon’s Highly Available Key-value Store</a></p>
<p>7 0.72947592 <a title="816-lsi-7" href="../high_scalability-2011/high_scalability-2011-06-09-Retrospect_on_recent_AWS_outage_and_Resilient_Cloud-Based_Architecture.html">1056 high scalability-2011-06-09-Retrospect on recent AWS outage and Resilient Cloud-Based Architecture</a></p>
<p>8 0.7195248 <a title="816-lsi-8" href="../high_scalability-2011/high_scalability-2011-07-20-Netflix%3A_Harden_Systems_Using_a_Barrel_of_Problem_Causing_Monkeys_-_Latency%2C_Conformity%2C_Doctor%2C_Janitor%2C_Security%2C_Internationalization%2C_Chaos.html">1083 high scalability-2011-07-20-Netflix: Harden Systems Using a Barrel of Problem Causing Monkeys - Latency, Conformity, Doctor, Janitor, Security, Internationalization, Chaos</a></p>
<p>9 0.7184934 <a title="816-lsi-9" href="../high_scalability-2007/high_scalability-2007-09-18-Amazon_Architecture.html">96 high scalability-2007-09-18-Amazon Architecture</a></p>
<p>10 0.71357846 <a title="816-lsi-10" href="../high_scalability-2007/high_scalability-2007-07-24-Major_Websites_Down%3A_Or_Why_You_Want_to_Run_in_Two_or_More_Data_Centers..html">23 high scalability-2007-07-24-Major Websites Down: Or Why You Want to Run in Two or More Data Centers.</a></p>
<p>11 0.70312887 <a title="816-lsi-11" href="../high_scalability-2009/high_scalability-2009-04-07-Six_Lessons_Learned_Deploying_a_Large-scale_Infrastructure_in_Amazon_EC2_.html">559 high scalability-2009-04-07-Six Lessons Learned Deploying a Large-scale Infrastructure in Amazon EC2 </a></p>
<p>12 0.70023775 <a title="816-lsi-12" href="../high_scalability-2011/high_scalability-2011-08-26-Stuff_The_Internet_Says_On_Scalability_For_August_26%2C_2011.html">1106 high scalability-2011-08-26-Stuff The Internet Says On Scalability For August 26, 2011</a></p>
<p>13 0.69747275 <a title="816-lsi-13" href="../high_scalability-2010/high_scalability-2010-04-19-The_cost_of_High_Availability_%28HA%29_with_Oracle_.html">813 high scalability-2010-04-19-The cost of High Availability (HA) with Oracle </a></p>
<p>14 0.69105721 <a title="816-lsi-14" href="../high_scalability-2010/high_scalability-2010-12-28-Netflix%3A_Continually_Test_by_Failing_Servers_with_Chaos_Monkey.html">964 high scalability-2010-12-28-Netflix: Continually Test by Failing Servers with Chaos Monkey</a></p>
<p>15 0.68467438 <a title="816-lsi-15" href="../high_scalability-2011/high_scalability-2011-04-22-Stuff_The_Internet_Says_On_Scalability_For_April_22%2C_2011.html">1028 high scalability-2011-04-22-Stuff The Internet Says On Scalability For April 22, 2011</a></p>
<p>16 0.68461299 <a title="816-lsi-16" href="../high_scalability-2010/high_scalability-2010-07-08-Cloud_AWS_Infrastructure_vs._Physical_Infrastructure.html">853 high scalability-2010-07-08-Cloud AWS Infrastructure vs. Physical Infrastructure</a></p>
<p>17 0.6834498 <a title="816-lsi-17" href="../high_scalability-2010/high_scalability-2010-10-22-Paper%3A_Netflix%E2%80%99s_Transition_to_High-Availability_Storage_Systems_.html">925 high scalability-2010-10-22-Paper: Netflix’s Transition to High-Availability Storage Systems </a></p>
<p>18 0.67953658 <a title="816-lsi-18" href="../high_scalability-2011/high_scalability-2011-10-27-Strategy%3A_Survive_a_Comet_Strike_in_the_East_With_Reserved_Instances_in_the_West.html">1133 high scalability-2011-10-27-Strategy: Survive a Comet Strike in the East With Reserved Instances in the West</a></p>
<p>19 0.67696631 <a title="816-lsi-19" href="../high_scalability-2012/high_scalability-2012-12-05-5_Ways_to_Make_Cloud_Failure_Not_an_Option.html">1367 high scalability-2012-12-05-5 Ways to Make Cloud Failure Not an Option</a></p>
<p>20 0.67218047 <a title="816-lsi-20" href="../high_scalability-2010/high_scalability-2010-01-27-Hot_Scalability_Links_for_January_28_2010.html">767 high scalability-2010-01-27-Hot Scalability Links for January 28 2010</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.145), (2, 0.224), (10, 0.077), (30, 0.045), (35, 0.106), (47, 0.036), (79, 0.213), (85, 0.032), (94, 0.033)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97467726 <a title="816-lda-1" href="../high_scalability-2010/high_scalability-2010-04-28-Elasticity_for_the_Enterprise_--_Ensuring_Continuous_High_Availability_in_a_Disaster_Failure_Scenario.html">816 high scalability-2010-04-28-Elasticity for the Enterprise -- Ensuring Continuous High Availability in a Disaster Failure Scenario</a></p>
<p>Introduction: Many enterprises' high-availability architecture is based on the assumption
that you can prevent failure from happening by putting all your critical data
in a centralized database, back it up with expensive storage, and replicate it
somehow between the sites. As I argued in one of my previous posts (Why
Existing Databases (RAC) are So Breakable!) many of those assumptions are
broken at their core, as storage is doomed to failure just like any other
device, expensive hardware doesn't make things any better and database
replication is often not enough.One of the main lessons that we can take from
the likes of Amazon and Google is that the right way to ensure continuous high
availability is by designing our system to cope with failure. We need to
assume that what we tend to think of as unthinkable will probably happen, as
that's the nature of failure. So rather than trying to prevent failures, we
need to build a system that will tolerate them.As we can learn from a recent
outage event in</p><p>2 0.94885933 <a title="816-lda-2" href="../high_scalability-2014/high_scalability-2014-05-14-Google_Says_Cloud_Prices_Will_Follow_Moore%E2%80%99s_Law%3A_Are_We_All_Renters_Now%3F.html">1647 high scalability-2014-05-14-Google Says Cloud Prices Will Follow Moore’s Law: Are We All Renters Now?</a></p>
<p>Introduction: After Googlecut prices on their Google Cloud Platform Amazon quickly followed
with their ownprice cuts. Even more interesting is what the future holds for
pricing. The near future looks great. After that? We'll see.Adrian Cockcroft
highlights that Google thinksprices should followMoore's law, which means we
should expect prices to halve every 18-24 months.That's good news. Greater
cost certainty means you can make much more aggressive build out plans. With
the savings you can hire more people, handle more customers, and add those
media rich features you thought you couldn't afford. Design is directly
related to costs.Without Google competing with Amazon there's little doubt the
price reduction curve would be much less favorable.As a late cloud entrant
Google is now in a customer acquisition phase, so they are willing to pay for
customers, which means lower prices are an acceptable cost of doing business.
Profit and high margins are not the objective. Getting market share is what is
imp</p><p>3 0.9423039 <a title="816-lda-3" href="../high_scalability-2007/high_scalability-2007-09-27-Product%3A_Ganglia_Monitoring_System.html">101 high scalability-2007-09-27-Product: Ganglia Monitoring System</a></p>
<p>Introduction: Gangliais a scalable distributed monitoring system for high-performance
computing systems such as clusters and Grids. It is based on a hierarchical
design targeted at federations of clusters. It leverages widely used
technologies such as XML for data representation, XDR for compact, portable
data transport, and RRDtool for data storage and visualization. It uses
carefully engineered data structures and algorithms to achieve very low per-
node overheads and high concurrency. The implementation is robust, has been
ported to an extensive set of operating systems and processor architectures,
and is currently in use on thousands of clusters around the world. It has been
used to link clusters across university campuses and around the world and can
scale to handle clusters with 2000 nodes.</p><p>4 0.931234 <a title="816-lda-4" href="../high_scalability-2012/high_scalability-2012-09-24-Google_Spanner%27s_Most_Surprising_Revelation%3A_NoSQL_is_Out_and_NewSQL_is_In.html">1328 high scalability-2012-09-24-Google Spanner's Most Surprising Revelation: NoSQL is Out and NewSQL is In</a></p>
<p>Introduction: Google recently released apaper on Spanner, their planet enveloping tool for
organizing the world's monetizable information. Reading the Spanner paper I
felt it had that chiseled in stone feel that all of Google's best papers have.
An instant classic. Jeff Dean foreshadowed Spanner's humungousness as early
as2009.  Now Spanner seems fully online, just waiting to handle "millions of
machines across hundreds of datacenters and trillions of database rows."
Wow.The Wise have yet to weigh in on Spanner en masse. I look forward to more
insightful commentary. There's a lot to make sense of. What struck me most in
the paper was a deeply buried section essentially describing Google's
motivation for shifting away from NoSQL and toNewSQL. The money quote:We
believe it is better to have application programmers deal with performance
problems due to overuse of transactions as bottlenecks arise, rather than
always coding around the lack of transactions.This reads as ironic given
Bigtable helped kicks</p><p>5 0.92551339 <a title="816-lda-5" href="../high_scalability-2012/high_scalability-2012-07-18-Strategy%3A_Kill_Off_Multi-tenant_Instances_with_High_CPU_Stolen_Time.html">1286 high scalability-2012-07-18-Strategy: Kill Off Multi-tenant Instances with High CPU Stolen Time</a></p>
<p>Introduction: Are all instances created equal? Perhaps because under multi-tenancy multiple
virtual machines run on the same physical host, not all applications will run
equally well on every instance. In that case it makes sense to measure and
move to a better performing instance. That's the interesting idea
from@botchagalupe:Imagine something like a "performance monkey" where an
infrastructure is so bound that it can kill lower performing instances
automatically.@adriancosays Netflix has throught of doing the same: We've
looked at killing off multi-tenant instances that have high CPU stolen
time...Related ArticlesHost server CPU utilization in Amazon EC2 cloud</p><p>6 0.92453516 <a title="816-lda-6" href="../high_scalability-2007/high_scalability-2007-07-30-Build_an_Infinitely_Scalable_Infrastructure_for_%24100_Using_Amazon_Services.html">38 high scalability-2007-07-30-Build an Infinitely Scalable Infrastructure for $100 Using Amazon Services</a></p>
<p>7 0.92007208 <a title="816-lda-7" href="../high_scalability-2013/high_scalability-2013-07-01-PRISM%3A_The_Amazingly_Low_Cost_of_%C2%ADUsing_BigData_to_Know_More_About_You_in_Under_a_Minute.html">1485 high scalability-2013-07-01-PRISM: The Amazingly Low Cost of ­Using BigData to Know More About You in Under a Minute</a></p>
<p>8 0.91885126 <a title="816-lda-8" href="../high_scalability-2008/high_scalability-2008-11-22-Google_Architecture.html">448 high scalability-2008-11-22-Google Architecture</a></p>
<p>9 0.91856736 <a title="816-lda-9" href="../high_scalability-2010/high_scalability-2010-01-17-Applications_Become_Black_Boxes_Using_Markets_to_Scale_and_Control_Costs.html">761 high scalability-2010-01-17-Applications Become Black Boxes Using Markets to Scale and Control Costs</a></p>
<p>10 0.91684854 <a title="816-lda-10" href="../high_scalability-2010/high_scalability-2010-03-02-Using_the_Ambient_Cloud_as_an_Application_Runtime.html">786 high scalability-2010-03-02-Using the Ambient Cloud as an Application Runtime</a></p>
<p>11 0.91532695 <a title="816-lda-11" href="../high_scalability-2012/high_scalability-2012-05-07-Startups_are_Creating_a_New_System_of_the_World_for_IT.html">1240 high scalability-2012-05-07-Startups are Creating a New System of the World for IT</a></p>
<p>12 0.91501957 <a title="816-lda-12" href="../high_scalability-2009/high_scalability-2009-05-17-Product%3A_Hadoop.html">601 high scalability-2009-05-17-Product: Hadoop</a></p>
<p>13 0.91460633 <a title="816-lda-13" href="../high_scalability-2014/high_scalability-2014-06-05-Cloud_Architecture_Revolution.html">1654 high scalability-2014-06-05-Cloud Architecture Revolution</a></p>
<p>14 0.91423774 <a title="816-lda-14" href="../high_scalability-2008/high_scalability-2008-03-27-Amazon_Announces_Static_IP_Addresses_and_Multiple_Datacenter_Operation.html">289 high scalability-2008-03-27-Amazon Announces Static IP Addresses and Multiple Datacenter Operation</a></p>
<p>15 0.91419071 <a title="816-lda-15" href="../high_scalability-2010/high_scalability-2010-12-28-Netflix%3A_Continually_Test_by_Failing_Servers_with_Chaos_Monkey.html">964 high scalability-2010-12-28-Netflix: Continually Test by Failing Servers with Chaos Monkey</a></p>
<p>16 0.91336441 <a title="816-lda-16" href="../high_scalability-2012/high_scalability-2012-11-05-Gone_Fishin%27%3A_Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_In_The_Ambient_Cloud.html">1355 high scalability-2012-11-05-Gone Fishin': Building Super Scalable Systems: Blade Runner Meets Autonomic Computing In The Ambient Cloud</a></p>
<p>17 0.91326576 <a title="816-lda-17" href="../high_scalability-2009/high_scalability-2009-12-16-Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_in_the_Ambient_Cloud.html">750 high scalability-2009-12-16-Building Super Scalable Systems: Blade Runner Meets Autonomic Computing in the Ambient Cloud</a></p>
<p>18 0.91309702 <a title="816-lda-18" href="../high_scalability-2013/high_scalability-2013-07-19-Stuff_The_Internet_Says_On_Scalability_For_July_19%2C_2013.html">1494 high scalability-2013-07-19-Stuff The Internet Says On Scalability For July 19, 2013</a></p>
<p>19 0.91276675 <a title="816-lda-19" href="../high_scalability-2014/high_scalability-2014-02-03-How_Google_Backs_Up_the_Internet_Along_With_Exabytes_of_Other_Data.html">1589 high scalability-2014-02-03-How Google Backs Up the Internet Along With Exabytes of Other Data</a></p>
<p>20 0.9116379 <a title="816-lda-20" href="../high_scalability-2012/high_scalability-2012-09-04-Changing_Architectures%3A_New_Datacenter_Networks_Will_Set_Your_Code_and_Data_Free___.html">1316 high scalability-2012-09-04-Changing Architectures: New Datacenter Networks Will Set Your Code and Data Free   </a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
