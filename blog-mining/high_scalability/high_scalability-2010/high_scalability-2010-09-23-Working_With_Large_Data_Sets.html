<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>907 high scalability-2010-09-23-Working With Large Data Sets</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2010" href="../home/high_scalability-2010_home.html">high_scalability-2010</a> <a title="high_scalability-2010-907" href="#">high_scalability-2010-907</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>907 high scalability-2010-09-23-Working With Large Data Sets</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2010-907-html" href="http://highscalability.com//blog/2010/9/23/working-with-large-data-sets.html">html</a></p><p>Introduction: This is an excerpt from my blogpost  Working With Large Data Sets ...
 
For the past 18 months I’ve moved from working on the SMTP proxy to working on our other systems, all of which make use of the data we collect from each connection. It’s a fair amount of data and it can be up to 2Kb in size for each connection. Our servers receive approximately 1000 of these pieces of data per second, which is fairly sustained due to our global distribution of customers. If you compare that to  Twitter’s peak of 3,283 tweets per second  (maximum of 140 characters), you can see it’s not a small amount of data that we are dealing with here.
 
I recently set out to scientifically prove the benefits of throttling, which is our technology for slowing down connections in order to detect spambots, who are kind enough to disconnect quite quickly when they see a slow connection. Due to the nature of the data we had, I needed to work with a long range of data to show evidence that an IP that appeared on Spam</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 This is an excerpt from my blogpost  Working With Large Data Sets . [sent-1, score-0.144]
</p><p>2 For the past 18 months I’ve moved from working on the SMTP proxy to working on our other systems, all of which make use of the data we collect from each connection. [sent-4, score-0.86]
</p><p>3 It’s a fair amount of data and it can be up to 2Kb in size for each connection. [sent-5, score-0.42]
</p><p>4 Our servers receive approximately 1000 of these pieces of data per second, which is fairly sustained due to our global distribution of customers. [sent-6, score-0.803]
</p><p>5 If you compare that to  Twitter’s peak of 3,283 tweets per second  (maximum of 140 characters), you can see it’s not a small amount of data that we are dealing with here. [sent-7, score-0.766]
</p><p>6 I recently set out to scientifically prove the benefits of throttling, which is our technology for slowing down connections in order to detect spambots, who are kind enough to disconnect quite quickly when they see a slow connection. [sent-8, score-0.772]
</p><p>7 Due to the nature of the data we had, I needed to work with a long range of data to show evidence that an IP that appeared on Spamhaus had previously been throttled and disconnected, and then measure the duration until it appeared on Spamhaus. [sent-9, score-1.513]
</p><p>8 I set a job to pre-process a selected set of customers data and arbitrarily decided 66 days would be a good amount to process, as this was 2 months plus a little breathing room. [sent-10, score-1.381]
</p><p>9 I knew from my experience it was possible that it might take 2 months for a bad IP to be picked up by Spamhaus. [sent-11, score-0.488]
</p><p>10 I extracted 28,204,693 distinct IPs, some of which were seen over million times in this data set. [sent-12, score-0.392]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('appeared', 0.312), ('months', 0.245), ('spambots', 0.185), ('throttled', 0.176), ('amount', 0.172), ('breathing', 0.17), ('ip', 0.168), ('disconnect', 0.164), ('throttling', 0.16), ('disconnected', 0.149), ('characters', 0.149), ('due', 0.146), ('duration', 0.144), ('extracted', 0.144), ('excerpt', 0.144), ('arbitrarily', 0.139), ('smtp', 0.137), ('sustained', 0.137), ('picked', 0.136), ('data', 0.132), ('evidence', 0.131), ('ips', 0.125), ('prove', 0.119), ('approximately', 0.117), ('slowing', 0.117), ('distinct', 0.116), ('fair', 0.116), ('set', 0.116), ('selected', 0.113), ('collect', 0.108), ('knew', 0.107), ('detect', 0.102), ('tweets', 0.101), ('working', 0.1), ('previously', 0.096), ('proxy', 0.095), ('compare', 0.095), ('pieces', 0.095), ('decided', 0.093), ('receive', 0.092), ('second', 0.091), ('maximum', 0.09), ('dealing', 0.09), ('plus', 0.085), ('peak', 0.085), ('fairly', 0.084), ('moved', 0.08), ('benefits', 0.079), ('nature', 0.078), ('recently', 0.075)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="907-tfidf-1" href="../high_scalability-2010/high_scalability-2010-09-23-Working_With_Large_Data_Sets.html">907 high scalability-2010-09-23-Working With Large Data Sets</a></p>
<p>Introduction: This is an excerpt from my blogpost  Working With Large Data Sets ...
 
For the past 18 months I’ve moved from working on the SMTP proxy to working on our other systems, all of which make use of the data we collect from each connection. It’s a fair amount of data and it can be up to 2Kb in size for each connection. Our servers receive approximately 1000 of these pieces of data per second, which is fairly sustained due to our global distribution of customers. If you compare that to  Twitter’s peak of 3,283 tweets per second  (maximum of 140 characters), you can see it’s not a small amount of data that we are dealing with here.
 
I recently set out to scientifically prove the benefits of throttling, which is our technology for slowing down connections in order to detect spambots, who are kind enough to disconnect quite quickly when they see a slow connection. Due to the nature of the data we had, I needed to work with a long range of data to show evidence that an IP that appeared on Spam</p><p>2 0.13090299 <a title="907-tfidf-2" href="../high_scalability-2008/high_scalability-2008-03-27-Amazon_Announces_Static_IP_Addresses_and_Multiple_Datacenter_Operation.html">289 high scalability-2008-03-27-Amazon Announces Static IP Addresses and Multiple Datacenter Operation</a></p>
<p>Introduction: Amazon is fixing two of their major problems: no static IP addresses and single datacenter operation. By adding these two new features developers can finally build a no apology system on Amazon. Before you always had to throw in an apology or two. No, we don't have low failover times because of the silly DNS games and unexceptionable DNS update and propagation times and no, we don't operate in more than one datacenter. No more. Now Amazon is adding   Elastic IP Addresses   and   Availability Zones  .     Elastic IP addresses are far better than normal IP addresses because they are both in tight with   Jessica Alba   and they are:        Static IP addresses designed for dynamic cloud computing. An Elastic IP address is associated with your account, not a particular instance, and you control that address until you choose to explicitly release it. Unlike traditional static IP addresses, however, Elastic IP addresses allow you to mask instance or availability zone failures by programmatica</p><p>3 0.12078713 <a title="907-tfidf-3" href="../high_scalability-2007/high_scalability-2007-11-30-Strategy%3A_Efficiently_Geo-referencing_IPs.html">168 high scalability-2007-11-30-Strategy: Efficiently Geo-referencing IPs</a></p>
<p>Introduction: A lot of apps need to map IP addresses to locations.  Jeremy Cole in   On efficiently geo-referencing IPs with MaxMind GeoIP and MySQL GIS   succinctly explains the many uses for such a feature:     Geo-referencing IPs is, in a nutshell, converting an IP address, perhaps from an incoming web visitor, a log file, a data file, or some other place, into the name of some entity owning that IP address. There are a lot of reasons you may want to geo-reference IP addresses to country, city, etc., such as in simple ad targeting systems, geographic load balancing, web analytics, and many more applications.       This is difficult to do efficiently, at least it gives me a bit of brain freeze. In the same post Jeremy nicely explains where to get the geo-rereferncing data, how to load data,  and the performance of different approaches for IP address searching. It's a great practical introduction to the subject.</p><p>4 0.082402393 <a title="907-tfidf-4" href="../high_scalability-2010/high_scalability-2010-10-15-Troubles_with_Sharding_-_What_can_we_learn_from_the_Foursquare_Incident%3F.html">920 high scalability-2010-10-15-Troubles with Sharding - What can we learn from the Foursquare Incident?</a></p>
<p>Introduction: For everything given something seems to be taken. Caching is a great scalability solution, but caching also  comes with problems .  Sharding  is a great scalability solution, but as Foursquare recently revealed in a  post-mortem  about their 17 hours of downtime, sharding also has problems. MongoDB, the database Foursquare uses, also contributed their  post-mortem  of what went wrong too.
 
Now that everyone has shared and resharded, what can we learn to help us skip these mistakes and quickly move on to a different set of mistakes?
 
First, like for  Facebook , huge props to Foursquare and MongoDB for being upfront and honest about their problems. This helps everyone get better and is a sign we work in a pretty cool industry.
 
Second, overall, the fault didn't flow from evil hearts or gross negligence. As usual the cause was more mundane: a key system, that could be a little more robust, combined with a very popular application built by a small group of people, under immense pressure</p><p>5 0.081279784 <a title="907-tfidf-5" href="../high_scalability-2013/high_scalability-2013-08-13-In_Memoriam%3A_Lavabit_Architecture_-_Creating_a_Scalable_Email_Service.html">1501 high scalability-2013-08-13-In Memoriam: Lavabit Architecture - Creating a Scalable Email Service</a></p>
<p>Introduction: With Lavabit  shutting down  under  murky circumstances , it seems fitting to  repost an old  (2009), yet still very good post by  Ladar Levison  on Lavabit's architecture. I don't know how much of this information is still current, but it should give you a general idea what Lavabit was all about.
  
 Getting to Know You 
 What is the name of your system and where can we find out more about it? 

 Note: these links are no longer valid... 


Lavabit   http://lavabit.com      http://lavabit.com/network.html    http://lavabit.com/about.html 

 What is your system for? 

Lavabit is a mid-sized email service provider. We currently have about 140,000 registered users with more than 260,000 email addresses. While most of our accounts belong to individual users, we also provide corporate email services to approximately 70 companies.

 Why did you decide to build this system? 

We built the system to compete against the other large free email providers, with an emphasis on serving the privacy c</p><p>6 0.07959684 <a title="907-tfidf-6" href="../high_scalability-2011/high_scalability-2011-03-14-Twitter_by_the_Numbers_-_460%2C000_New_Accounts_and_140_Million_Tweets_Per_Day.html">1004 high scalability-2011-03-14-Twitter by the Numbers - 460,000 New Accounts and 140 Million Tweets Per Day</a></p>
<p>7 0.078330941 <a title="907-tfidf-7" href="../high_scalability-2008/high_scalability-2008-01-30-How_Rackspace_Now_Uses_MapReduce_and_Hadoop_to_Query_Terabytes_of_Data.html">233 high scalability-2008-01-30-How Rackspace Now Uses MapReduce and Hadoop to Query Terabytes of Data</a></p>
<p>8 0.075628072 <a title="907-tfidf-8" href="../high_scalability-2007/high_scalability-2007-08-22-Wikimedia_architecture.html">72 high scalability-2007-08-22-Wikimedia architecture</a></p>
<p>9 0.073112428 <a title="907-tfidf-9" href="../high_scalability-2008/high_scalability-2008-04-19-How_to_build_a_real-time_analytics_system%3F.html">304 high scalability-2008-04-19-How to build a real-time analytics system?</a></p>
<p>10 0.071840852 <a title="907-tfidf-10" href="../high_scalability-2009/high_scalability-2009-08-31-Squarespace_Architecture_-_A_Grid_Handles_Hundreds_of_Millions_of_Requests_a_Month_.html">691 high scalability-2009-08-31-Squarespace Architecture - A Grid Handles Hundreds of Millions of Requests a Month </a></p>
<p>11 0.071047477 <a title="907-tfidf-11" href="../high_scalability-2010/high_scalability-2010-03-16-Justin.tv%27s_Live_Video_Broadcasting_Architecture.html">796 high scalability-2010-03-16-Justin.tv's Live Video Broadcasting Architecture</a></p>
<p>12 0.070951179 <a title="907-tfidf-12" href="../high_scalability-2008/high_scalability-2008-12-29-100%25_on_Amazon_Web_Services%3A_Soocial.com_-_a_lesson_of_porting_your_service_to_Amazon.html">477 high scalability-2008-12-29-100% on Amazon Web Services: Soocial.com - a lesson of porting your service to Amazon</a></p>
<p>13 0.070839837 <a title="907-tfidf-13" href="../high_scalability-2012/high_scalability-2012-11-15-Gone_Fishin%27%3A_Justin.Tv%27s_Live_Video_Broadcasting_Architecture.html">1359 high scalability-2012-11-15-Gone Fishin': Justin.Tv's Live Video Broadcasting Architecture</a></p>
<p>14 0.069315612 <a title="907-tfidf-14" href="../high_scalability-2010/high_scalability-2010-11-04-Facebook_at_13_Million_Queries_Per_Second_Recommends%3A_Minimize_Request_Variance.html">934 high scalability-2010-11-04-Facebook at 13 Million Queries Per Second Recommends: Minimize Request Variance</a></p>
<p>15 0.068485349 <a title="907-tfidf-15" href="../high_scalability-2010/high_scalability-2010-08-02-7_Scaling_Strategies_Facebook_Used_to_Grow_to_500_Million_Users.html">870 high scalability-2010-08-02-7 Scaling Strategies Facebook Used to Grow to 500 Million Users</a></p>
<p>16 0.068033583 <a title="907-tfidf-16" href="../high_scalability-2008/high_scalability-2008-04-05-Skype_Plans_for_PostgreSQL_to_Scale_to_1_Billion_Users.html">297 high scalability-2008-04-05-Skype Plans for PostgreSQL to Scale to 1 Billion Users</a></p>
<p>17 0.067730322 <a title="907-tfidf-17" href="../high_scalability-2014/high_scalability-2014-01-28-How_Next_Big_Sound_Tracks_Over_a_Trillion_Song_Plays%2C_Likes%2C_and_More_Using_a_Version_Control_System_for_Hadoop_Data.html">1586 high scalability-2014-01-28-How Next Big Sound Tracks Over a Trillion Song Plays, Likes, and More Using a Version Control System for Hadoop Data</a></p>
<p>18 0.067320809 <a title="907-tfidf-18" href="../high_scalability-2012/high_scalability-2012-12-14-Stuff_The_Internet_Says_On_Scalability_For_December_14%2C_2012.html">1372 high scalability-2012-12-14-Stuff The Internet Says On Scalability For December 14, 2012</a></p>
<p>19 0.066861153 <a title="907-tfidf-19" href="../high_scalability-2014/high_scalability-2014-03-24-Big%2C_Small%2C_Hot_or_Cold_-_Examples_of_Robust_Data_Pipelines_from_Stripe%2C_Tapad%2C_Etsy_and_Square.html">1618 high scalability-2014-03-24-Big, Small, Hot or Cold - Examples of Robust Data Pipelines from Stripe, Tapad, Etsy and Square</a></p>
<p>20 0.066779368 <a title="907-tfidf-20" href="../high_scalability-2008/high_scalability-2008-01-24-Mailinator_Architecture.html">221 high scalability-2008-01-24-Mailinator Architecture</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.124), (1, 0.067), (2, -0.019), (3, -0.026), (4, -0.01), (5, -0.002), (6, 0.019), (7, 0.019), (8, 0.018), (9, -0.009), (10, 0.015), (11, 0.021), (12, 0.029), (13, 0.012), (14, 0.041), (15, 0.051), (16, -0.005), (17, 0.013), (18, -0.031), (19, -0.001), (20, 0.002), (21, 0.017), (22, 0.021), (23, 0.009), (24, 0.004), (25, -0.006), (26, -0.041), (27, 0.022), (28, 0.01), (29, 0.003), (30, 0.016), (31, -0.011), (32, -0.032), (33, 0.048), (34, -0.003), (35, 0.055), (36, 0.018), (37, 0.06), (38, -0.004), (39, 0.014), (40, -0.004), (41, 0.026), (42, 0.067), (43, -0.032), (44, 0.022), (45, 0.052), (46, 0.013), (47, -0.023), (48, 0.006), (49, 0.0)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.86439419 <a title="907-lsi-1" href="../high_scalability-2010/high_scalability-2010-09-23-Working_With_Large_Data_Sets.html">907 high scalability-2010-09-23-Working With Large Data Sets</a></p>
<p>Introduction: This is an excerpt from my blogpost  Working With Large Data Sets ...
 
For the past 18 months I’ve moved from working on the SMTP proxy to working on our other systems, all of which make use of the data we collect from each connection. It’s a fair amount of data and it can be up to 2Kb in size for each connection. Our servers receive approximately 1000 of these pieces of data per second, which is fairly sustained due to our global distribution of customers. If you compare that to  Twitter’s peak of 3,283 tweets per second  (maximum of 140 characters), you can see it’s not a small amount of data that we are dealing with here.
 
I recently set out to scientifically prove the benefits of throttling, which is our technology for slowing down connections in order to detect spambots, who are kind enough to disconnect quite quickly when they see a slow connection. Due to the nature of the data we had, I needed to work with a long range of data to show evidence that an IP that appeared on Spam</p><p>2 0.72191697 <a title="907-lsi-2" href="../high_scalability-2012/high_scalability-2012-05-14-DynamoDB_Talk_Notes_and_the_SSD_Hot_S3_Cold_Pattern.html">1245 high scalability-2012-05-14-DynamoDB Talk Notes and the SSD Hot S3 Cold Pattern</a></p>
<p>Introduction: My impression of DynamoDB before attending a   Amazon DynamoDB for Developers    talk is that it’s the usual quality service produced by Amazon: simple, fast, scalable, geographically redundant, expensive enough to make you think twice about using it, and delightfully NoOp.     After the talk my impression has become more nuanced. The quality impression still stands. Look at the    forums    and you’ll see the typical issues every product has, but no real surprises. And as a SimpleDB++, DynamoDB seems to have avoided second system syndrome and produced a more elegant design.     What was surprising is how un-cloudy DynamoDB appears to be. The cloud pillars of pay for what you use and quick elastic response to bursty traffic have been abandoned, for some understandable reasons, but the result is you really have to consider your use cases before making DynamoDB the default choice.    Here are some of my impressions from the talk... 
  
   DynamoDB is a clean well lighted place for key-va</p><p>3 0.71560788 <a title="907-lsi-3" href="../high_scalability-2007/high_scalability-2007-10-10-WAN_Accelerate_Your_Way_to_Lightening_Fast_Transfers_Between_Data_Centers.html">119 high scalability-2007-10-10-WAN Accelerate Your Way to Lightening Fast Transfers Between Data Centers</a></p>
<p>Introduction: How do you keep in sync a crescendo of data between data centers over a slow WAN? That's the question   Alberto   posted a few weeks ago. Normally I'm not into all boy bands, but I was frustrated there wasn't a really good answer for his problem. It occurred to me later a WAN accelerator might help turn his slow WAN link into more of a LAN, so the overhead of copying files across the WAN wouldn't be so limiting. Many might not consider a WAN accelerator in this situation, but since my friend Damon Ennis works at the WAN accelerator vendor   Silver Peak  , I thought I would ask him if their product would help. Not surprisingly his answer is yes! Potentially a lot, depending on the nature of your data. Here's a no BS overview of their product:           What is it? - Scalable WAN Accelerator from Silver Peak (http://www.silver-peak.com)   What does it do? - You can send 5x-100x times more data across your expensive, low-bandwidth WAN link.   Why should you care? - Your data centers becom</p><p>4 0.70871508 <a title="907-lsi-4" href="../high_scalability-2012/high_scalability-2012-09-11-How_big_is_a_Petabyte%2C_Exabyte%2C_Zettabyte%2C_or_a_Yottabyte%3F.html">1320 high scalability-2012-09-11-How big is a Petabyte, Exabyte, Zettabyte, or a Yottabyte?</a></p>
<p>Introduction: This is an intuitive look at large data sizes By Julian Bunn in  Globally Interconnected Object Databases .
  Bytes(8 bits)   
 0.1 bytes:  A binary decision  
 1 byte:  A single character  
 10 bytes:  A single word  
 100 bytes:  A telegram  OR  A punched card  
   Kilobyte (1000 bytes)   
 1 Kilobyte:  A very short story  
 2 Kilobytes: A Typewritten page 
 10 Kilobytes:  An encyclopaedic page  OR  A deck of punched cards  
 50 Kilobytes: A compressed document image page 
 100 Kilobytes:  A low-resolution photograph  
 200 Kilobytes: A box of punched cards 
 500 Kilobytes: A very heavy box of punched cards 
   Megabyte (1 000 000 bytes)   
 1 Megabyte:  A small novel  OR  A 3.5 inch floppy disk  
 2 Megabytes: A high resolution photograph 
 5 Megabytes:  The complete works of Shakespeare  OR 30 seconds of TV-quality video 
 10 Megabytes: A minute of high-fidelity sound OR A digital chest X-ray 
 20 Megabytes:  A box of floppy disks  
 50 Megabytes: A digital mammogram 
 100 Megabyte</p><p>5 0.68668765 <a title="907-lsi-5" href="../high_scalability-2012/high_scalability-2012-10-08-How_UltraDNS_Handles_Hundreds_of_Thousands_of_Zones_and_Tens_of_Millions_of_Records.html">1335 high scalability-2012-10-08-How UltraDNS Handles Hundreds of Thousands of Zones and Tens of Millions of Records</a></p>
<p>Introduction: This is a guest post by  Jeffrey Damick , Principal Software Engineer for  Neustar .   Jeffrey has overseen the software architecture for  UltraDNS  for last two and half years as it went through substantial revitalization.  
 
UltraDNS is one the top the DNS providers, serving many  top-level domains (TLDs)  as well as  second-level domains (SLDs) .  This requires handling of several hundreds of thousands of zones with many containing millions of records each.  Even with all of its success UltraDNS had fallen into a rut several years ago, its release schedule had become haphazard at best and the team was struggling to keep up with feature requests in a waterfall development style.
  Development  
Realizing that something had to be done the team came together and identified the most important areas to attack first.  We began with the code base, stabilizing our flagship proprietary C++ DNS server, instituting common best practices and automation.   Testing was very manual and not easily</p><p>6 0.66891664 <a title="907-lsi-6" href="../high_scalability-2010/high_scalability-2010-11-04-Facebook_at_13_Million_Queries_Per_Second_Recommends%3A_Minimize_Request_Variance.html">934 high scalability-2010-11-04-Facebook at 13 Million Queries Per Second Recommends: Minimize Request Variance</a></p>
<p>7 0.66425169 <a title="907-lsi-7" href="../high_scalability-2012/high_scalability-2012-03-22-Paper%3A_Revisiting_Network_I-O_APIs%3A_The_netmap_Framework.html">1213 high scalability-2012-03-22-Paper: Revisiting Network I-O APIs: The netmap Framework</a></p>
<p>8 0.66323513 <a title="907-lsi-8" href="../high_scalability-2009/high_scalability-2009-06-30-Hot_New_Trend%3A_Linking_Clouds_Through_Cheap_IP_VPNs_Instead_of_Private_Lines_.html">645 high scalability-2009-06-30-Hot New Trend: Linking Clouds Through Cheap IP VPNs Instead of Private Lines </a></p>
<p>9 0.66291374 <a title="907-lsi-9" href="../high_scalability-2012/high_scalability-2012-11-26-BigData_using_Erlang%2C_C_and_Lisp_to_Fight_the_Tsunami_of_Mobile_Data.html">1362 high scalability-2012-11-26-BigData using Erlang, C and Lisp to Fight the Tsunami of Mobile Data</a></p>
<p>10 0.65763736 <a title="907-lsi-10" href="../high_scalability-2008/high_scalability-2008-02-16-S3_Failed_Because_of_Authentication_Overload.html">249 high scalability-2008-02-16-S3 Failed Because of Authentication Overload</a></p>
<p>11 0.65530598 <a title="907-lsi-11" href="../high_scalability-2013/high_scalability-2013-12-18-How_to_get_started_with_sizing_and_capacity_planning%2C_assuming_you_don%27t_know_the_software_behavior%3F.html">1566 high scalability-2013-12-18-How to get started with sizing and capacity planning, assuming you don't know the software behavior?</a></p>
<p>12 0.65481907 <a title="907-lsi-12" href="../high_scalability-2012/high_scalability-2012-04-05-Big_Data_Counting%3A_How_to_count_a_billion_distinct_objects_using_only_1.5KB_of_Memory.html">1222 high scalability-2012-04-05-Big Data Counting: How to count a billion distinct objects using only 1.5KB of Memory</a></p>
<p>13 0.65233094 <a title="907-lsi-13" href="../high_scalability-2013/high_scalability-2013-11-22-Stuff_The_Internet_Says_On_Scalability_For_November_22th%2C_2013.html">1552 high scalability-2013-11-22-Stuff The Internet Says On Scalability For November 22th, 2013</a></p>
<p>14 0.65070057 <a title="907-lsi-14" href="../high_scalability-2009/high_scalability-2009-04-06-How_do_you_monitor_the_performance_of_your_cluster%3F.html">558 high scalability-2009-04-06-How do you monitor the performance of your cluster?</a></p>
<p>15 0.65065801 <a title="907-lsi-15" href="../high_scalability-2009/high_scalability-2009-10-09-Have_you_collectl%27d_yet%3F__If_not%2C_maybe_collectl-utils_will_make_it_easier_to_do_so.html">719 high scalability-2009-10-09-Have you collectl'd yet?  If not, maybe collectl-utils will make it easier to do so</a></p>
<p>16 0.65026301 <a title="907-lsi-16" href="../high_scalability-2012/high_scalability-2012-03-07-Scale_Indefinitely_on_S3_With_These_Secrets_of_the_S3_Masters.html">1205 high scalability-2012-03-07-Scale Indefinitely on S3 With These Secrets of the S3 Masters</a></p>
<p>17 0.64964557 <a title="907-lsi-17" href="../high_scalability-2014/high_scalability-2014-01-24-Stuff_The_Internet_Says_On_Scalability_For_January_24th%2C_2014.html">1585 high scalability-2014-01-24-Stuff The Internet Says On Scalability For January 24th, 2014</a></p>
<p>18 0.64477259 <a title="907-lsi-18" href="../high_scalability-2011/high_scalability-2011-11-29-DataSift_Architecture%3A_Realtime_Datamining_at_120%2C000_Tweets_Per_Second.html">1148 high scalability-2011-11-29-DataSift Architecture: Realtime Datamining at 120,000 Tweets Per Second</a></p>
<p>19 0.644135 <a title="907-lsi-19" href="../high_scalability-2013/high_scalability-2013-12-13-Stuff_The_Internet_Says_On_Scalability_For_December_13th%2C_2013.html">1564 high scalability-2013-12-13-Stuff The Internet Says On Scalability For December 13th, 2013</a></p>
<p>20 0.64364344 <a title="907-lsi-20" href="../high_scalability-2009/high_scalability-2009-06-08-Distribution_of_queries_per_second.html">622 high scalability-2009-06-08-Distribution of queries per second</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.089), (2, 0.196), (10, 0.032), (61, 0.144), (77, 0.018), (94, 0.108), (99, 0.313)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.83479607 <a title="907-lda-1" href="../high_scalability-2008/high_scalability-2008-08-30-Paper%3A_GargantuanComputing%E2%80%94GRIDs_and_P2P.html">374 high scalability-2008-08-30-Paper: GargantuanComputing—GRIDs and P2P</a></p>
<p>Introduction: I found the discussion of the available bandwidth of tree vs higher dimensional virtual networks topologies quite, to quote Spock,  fascinating :   A mathematical analysis by Ritter (2002) (one of the original developers of Napster) presented a detailed numerical argument demonstrating that the Gnutella network could not scale to the capacity of its competitor,  the Napster network. Essentially, that model showed that the Gnutella network is severely bandwidth-limited long before the P2P population reaches a million peers.  In each of these previous studies, the conclusions have overlooked the intrinsic bandwidth limits of the underlying topology in the Gnutella network: a Cayley tree (Rains and Sloane 1999) (see Sect. 9.4 for the definition).  Trees are known to have lower aggregate bandwidth than higher dimensional topologies, e.g., hypercubes and hypertori. Studies of interconnection topologies in the literature have tended to focus on hardware implementations (see, e.g., Culler et</p><p>2 0.82765925 <a title="907-lda-2" href="../high_scalability-2011/high_scalability-2011-09-30-Gone_Fishin%27.html">1128 high scalability-2011-09-30-Gone Fishin'</a></p>
<p>Introduction: Well, not exactly Fishin', I'll be on vacation starting today and I'll be back in mid October. I won't be posting, so we'll all have a break. Disappointing, I know. If you've ever wanted to write an article for HighScalability, this would be a great time :-) I especially need help on writing  Stuff the Internet Says on Scalability  as I won't even be reading the Interwebs. Shock! Horror!   So if the spirit moves you, please write something. My connectivity in South Africa is unknown, but I will check in and approve articles when I can. See you on down the road...</p><p>3 0.80958152 <a title="907-lda-3" href="../high_scalability-2012/high_scalability-2012-10-29-Gone_Fishin%27_Two.html">1350 high scalability-2012-10-29-Gone Fishin' Two</a></p>
<p>Introduction: Well, not exactly Fishin', I'll be on vacation starting today and I'll be back late November. I won't be posting anything new, so we'll all have a break. Disappointing, I know, but fear not, I will be posting some oldies for your re-enjoyment.
 
And If you've ever wanted to write an article for HighScalability, this would be a great time :-) I especially need help on writing  Stuff the Internet Says on Scalability  as I will be reading the Interwebs on a much reduced schedule. Shock! Horror! Â  So if the spirit moves you, please write something.
 
My connectivity in Italy will probably be good, so I will check in and approve articles on a regular basis. Ciao...</p><p>4 0.80030882 <a title="907-lda-4" href="../high_scalability-2008/high_scalability-2008-12-29-Paper%3A_Spamalytics%3A_An_Empirical_Analysisof_Spam_Marketing_Conversion.html">478 high scalability-2008-12-29-Paper: Spamalytics: An Empirical Analysisof Spam Marketing Conversion</a></p>
<p>Introduction: Under the philosophy that  the best method to analyse spam is to become a spammer , this absolutely fascinating paper recounts how a team of UC Berkely researchers went under cover to infiltrate a spam network. Part CSI, part Mission Impossible, and part MacGyver, the team hijacked the botnet so that their code was actually part of the dark network itself. Once inside they figured out the architecture and protocols of the botnet and how many sales they were able to tally. Truly elegant work.   Two different spam campaigns were run on a  Storm  botnet network of 75,800 zombie computers. Storm is a peer-to-peer botnet that uses spam to creep its tentacles through the world wide computer network. One of the campains distributed viruses in order to recruit new bots into the network. This is normally accomplished by enticing people to download email attachments. An astonishing one in ten people downloaded the executable and ran it, which means we won't run out of zombies soon. The downloade</p><p>same-blog 5 0.8001191 <a title="907-lda-5" href="../high_scalability-2010/high_scalability-2010-09-23-Working_With_Large_Data_Sets.html">907 high scalability-2010-09-23-Working With Large Data Sets</a></p>
<p>Introduction: This is an excerpt from my blogpost  Working With Large Data Sets ...
 
For the past 18 months I’ve moved from working on the SMTP proxy to working on our other systems, all of which make use of the data we collect from each connection. It’s a fair amount of data and it can be up to 2Kb in size for each connection. Our servers receive approximately 1000 of these pieces of data per second, which is fairly sustained due to our global distribution of customers. If you compare that to  Twitter’s peak of 3,283 tweets per second  (maximum of 140 characters), you can see it’s not a small amount of data that we are dealing with here.
 
I recently set out to scientifically prove the benefits of throttling, which is our technology for slowing down connections in order to detect spambots, who are kind enough to disconnect quite quickly when they see a slow connection. Due to the nature of the data we had, I needed to work with a long range of data to show evidence that an IP that appeared on Spam</p><p>6 0.73757982 <a title="907-lda-6" href="../high_scalability-2011/high_scalability-2011-12-23-Stuff_The_Internet_Says_On_Scalability_For_December_23%2C_2011.html">1163 high scalability-2011-12-23-Stuff The Internet Says On Scalability For December 23, 2011</a></p>
<p>7 0.70489705 <a title="907-lda-7" href="../high_scalability-2012/high_scalability-2012-12-05-5_Ways_to_Make_Cloud_Failure_Not_an_Option.html">1367 high scalability-2012-12-05-5 Ways to Make Cloud Failure Not an Option</a></p>
<p>8 0.70261186 <a title="907-lda-8" href="../high_scalability-2014/high_scalability-2014-05-23-Gone_Fishin%27_2014.html">1653 high scalability-2014-05-23-Gone Fishin' 2014</a></p>
<p>9 0.69239712 <a title="907-lda-9" href="../high_scalability-2008/high_scalability-2008-05-05-HSCALE_-__Handling_200_Million_Transactions_Per_Month_Using_Transparent_Partitioning_With_MySQL_Proxy.html">315 high scalability-2008-05-05-HSCALE -  Handling 200 Million Transactions Per Month Using Transparent Partitioning With MySQL Proxy</a></p>
<p>10 0.68134302 <a title="907-lda-10" href="../high_scalability-2008/high_scalability-2008-09-16-EE-Appserver_Clustering_OR_Terracota_OR_Coherence_OR_something_else%3F.html">384 high scalability-2008-09-16-EE-Appserver Clustering OR Terracota OR Coherence OR something else?</a></p>
<p>11 0.67698056 <a title="907-lda-11" href="../high_scalability-2011/high_scalability-2011-11-04-Stuff_The_Internet_Says_On_Scalability_For_November_4%2C_2011.html">1137 high scalability-2011-11-04-Stuff The Internet Says On Scalability For November 4, 2011</a></p>
<p>12 0.67461836 <a title="907-lda-12" href="../high_scalability-2010/high_scalability-2010-04-14-Parallel_Information_Retrieval_and_Other_Search_Engine_Goodness.html">810 high scalability-2010-04-14-Parallel Information Retrieval and Other Search Engine Goodness</a></p>
<p>13 0.67332631 <a title="907-lda-13" href="../high_scalability-2012/high_scalability-2012-08-08-3_Tips_and_Tools_for_Creating_Reliable_Billion_Page_View_Web_Services.html">1301 high scalability-2012-08-08-3 Tips and Tools for Creating Reliable Billion Page View Web Services</a></p>
<p>14 0.65713656 <a title="907-lda-14" href="../high_scalability-2012/high_scalability-2012-09-12-Using_Varnish_for_Paywalls%3A_Moving_Logic_to_the_Edge.html">1321 high scalability-2012-09-12-Using Varnish for Paywalls: Moving Logic to the Edge</a></p>
<p>15 0.63086259 <a title="907-lda-15" href="../high_scalability-2007/high_scalability-2007-08-22-Wikimedia_architecture.html">72 high scalability-2007-08-22-Wikimedia architecture</a></p>
<p>16 0.62573606 <a title="907-lda-16" href="../high_scalability-2007/high_scalability-2007-10-11-How_Flickr_Handles_Moving_You_to_Another_Shard.html">120 high scalability-2007-10-11-How Flickr Handles Moving You to Another Shard</a></p>
<p>17 0.62362349 <a title="907-lda-17" href="../high_scalability-2012/high_scalability-2012-01-10-A_Perfect_Fifth_of_Notes_on_Scalability.html">1172 high scalability-2012-01-10-A Perfect Fifth of Notes on Scalability</a></p>
<p>18 0.62333107 <a title="907-lda-18" href="../high_scalability-2009/high_scalability-2009-03-19-Product%3A_Redis_-_Not_Just_Another_Key-Value_Store.html">545 high scalability-2009-03-19-Product: Redis - Not Just Another Key-Value Store</a></p>
<p>19 0.6218673 <a title="907-lda-19" href="../high_scalability-2008/high_scalability-2008-11-03-How_Sites_are_Scaling_Up_for_the_Election_Night_Crush.html">437 high scalability-2008-11-03-How Sites are Scaling Up for the Election Night Crush</a></p>
<p>20 0.62154877 <a title="907-lda-20" href="../high_scalability-2013/high_scalability-2013-10-28-Design_Decisions_for_Scaling_Your_High_Traffic_Feeds.html">1538 high scalability-2013-10-28-Design Decisions for Scaling Your High Traffic Feeds</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
