<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>830 high scalability-2010-05-25-Strategy: Rule of 3 Admins to Save Your Sanity</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2010" href="../home/high_scalability-2010_home.html">high_scalability-2010</a> <a title="high_scalability-2010-830" href="#">high_scalability-2010-830</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>830 high scalability-2010-05-25-Strategy: Rule of 3 Admins to Save Your Sanity</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2010-830-html" href="http://highscalability.com//blog/2010/5/25/strategy-rule-of-3-admins-to-save-your-sanity.html">html</a></p><p>Introduction: The idea came up in this Hacker News  thread , commenting on a 37signals interview, that having three system administrators is the minimum optimal number of admins. Everyone wants to lower their costs by having each admin administer a lot of machines. The problem is when you have fewer than three admins you can never get a break from the constant corrosive pressure of always being on call. When every moment of your life you are dreading the next emergency, it eats at you. Having three admins solves that problem. With three admins you can:
  
 Go on a real vacation. The two remaining admins can switch off being on call. 
 Not be on call all the time. 
  
A larger shop will naturally have more admins so it's not as big an issue, but at smaller shops trying to minimize head count, carrying three admins (or people in those roles) might be something to consider.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The idea came up in this Hacker News  thread , commenting on a 37signals interview, that having three system administrators is the minimum optimal number of admins. [sent-1, score-0.443]
</p><p>2 Everyone wants to lower their costs by having each admin administer a lot of machines. [sent-2, score-0.446]
</p><p>3 The problem is when you have fewer than three admins you can never get a break from the constant corrosive pressure of always being on call. [sent-3, score-1.538]
</p><p>4 When every moment of your life you are dreading the next emergency, it eats at you. [sent-4, score-0.511]
</p><p>5 With three admins you can:     Go on a real vacation. [sent-6, score-1.003]
</p><p>6 The two remaining admins can switch off being on call. [sent-7, score-0.899]
</p><p>7 A larger shop will naturally have more admins so it's not as big an issue, but at smaller shops trying to minimize head count, carrying three admins (or people in those roles) might be something to consider. [sent-9, score-2.563]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('admins', 0.728), ('three', 0.24), ('dreading', 0.171), ('administer', 0.171), ('corrosive', 0.171), ('eats', 0.148), ('newsthread', 0.148), ('commenting', 0.139), ('shops', 0.127), ('carrying', 0.121), ('emergency', 0.116), ('shop', 0.114), ('admin', 0.105), ('solves', 0.105), ('roles', 0.102), ('remaining', 0.101), ('naturally', 0.099), ('minimum', 0.097), ('moment', 0.094), ('administrators', 0.093), ('head', 0.089), ('pressure', 0.088), ('count', 0.083), ('fewer', 0.08), ('break', 0.077), ('constant', 0.075), ('minimize', 0.073), ('interview', 0.073), ('came', 0.071), ('issue', 0.07), ('switch', 0.07), ('hacker', 0.069), ('optimal', 0.069), ('wants', 0.066), ('smaller', 0.064), ('lower', 0.061), ('life', 0.06), ('larger', 0.057), ('consider', 0.056), ('everyone', 0.055), ('call', 0.055), ('trying', 0.053), ('never', 0.043), ('costs', 0.043), ('idea', 0.039), ('next', 0.038), ('might', 0.037), ('always', 0.036), ('real', 0.035), ('something', 0.033)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="830-tfidf-1" href="../high_scalability-2010/high_scalability-2010-05-25-Strategy%3A_Rule_of_3_Admins_to_Save_Your_Sanity.html">830 high scalability-2010-05-25-Strategy: Rule of 3 Admins to Save Your Sanity</a></p>
<p>Introduction: The idea came up in this Hacker News  thread , commenting on a 37signals interview, that having three system administrators is the minimum optimal number of admins. Everyone wants to lower their costs by having each admin administer a lot of machines. The problem is when you have fewer than three admins you can never get a break from the constant corrosive pressure of always being on call. When every moment of your life you are dreading the next emergency, it eats at you. Having three admins solves that problem. With three admins you can:
  
 Go on a real vacation. The two remaining admins can switch off being on call. 
 Not be on call all the time. 
  
A larger shop will naturally have more admins so it's not as big an issue, but at smaller shops trying to minimize head count, carrying three admins (or people in those roles) might be something to consider.</p><p>2 0.10239194 <a title="830-tfidf-2" href="../high_scalability-2010/high_scalability-2010-07-27-A_Metric_A%24%24-Ton_of_Joe_Stump%3A_The_Cloud_is_Cheaper_than_Bare_Metal.html">865 high scalability-2010-07-27-A Metric A$$-Ton of Joe Stump: The Cloud is Cheaper than Bare Metal</a></p>
<p>Introduction: Should you pay more in the cloud or pay less for bare metal in the datacenter? This is a crucial decision point facing startups today. Which way should you go? In this  Webpulp.tv interview   , Joe Stump, always a go-to guy when you need a metric ass-ton (a favorite expression of Joe’s) of good advice on cutting edge practices for the modern startup, laughs at conventional wisdom by saying  the cloud is really not more expensive than bare metal. 
 
The argument for a cheaper cloud has a three main points:
  
   Raw Hardware Costs are Close . Buying hardware is cheaper than renting hardware from Amazon, but it’s close when you consider  reserved instances . A reserved instance costs about $250 a year and a new server costs about $1500.  Let’s say Amazon is 30% slower. That means for a cluster of 20 machines you would need a another 7 servers. This takes 10 minutes and 4 button clicks to provision on the cloud (the advantage of a scale-out architecture + automation + cloud aware tools).</p><p>3 0.099386156 <a title="830-tfidf-3" href="../high_scalability-2007/high_scalability-2007-09-06-Scaling_IMAP_and_POP3.html">81 high scalability-2007-09-06-Scaling IMAP and POP3</a></p>
<p>Introduction: Another scalability strategy brought to you by Erik Osterman:     Just thought I'd drop a brief suggestion to anyone building a large mail system. Our solution for scaling mail pickup was to develop a sharded architecture whereby accounts are spread across a cluster of servers, each with imap/pop3 capability. Then we use a cluster of reverse proxies (Perdition) speaking to the backend imap/pop3 servers .      The benefit of this approach is you can use simply use round-robin or HA load balancing on the perdition servers that end users connect to (e.g. admins can easily move accounts around on the backend storage servers without affecting end users). Perdition manages routing users to the appropriate backend servers and has MySQL support.      What we also liked about this approach was that it had no dependency on a distributed or networked file system, so less chance of corruption or data consistency issues. When an individual server reaches capacity, we just off load users to a less u</p><p>4 0.09921027 <a title="830-tfidf-4" href="../high_scalability-2007/high_scalability-2007-08-03-Scaling_IMAP_and_POP3.html">57 high scalability-2007-08-03-Scaling IMAP and POP3</a></p>
<p>Introduction: Just thought I'd drop a brief suggestion to anyone building a large mail system. Our solution for scaling mail pickup was to develop a sharded architecture whereby accounts are spread across a cluster of servers, each with imap/pop3 capability. Then we use a cluster of reverse proxies (Perdition) speaking to the backend imap/pop3 servers . The benefit of this approach is you can use simply use round-robin or HA loadbalancing on the perdition servers that end users connect to (e.g. admins can easily move accounts around on the backend storage servers without affecting end users). Perdition manages routing users to the appropriate backend servers and has MySQL support. What we also liked about this approach was that it had no dependency on a distributed or networked filesystem, so less chance of corruption or data consistency issues. When an individual server reaches capacity, we just off load users to a less used server. If any server goes offline, it only affects the fraction of users</p><p>5 0.07263682 <a title="830-tfidf-5" href="../high_scalability-2013/high_scalability-2013-01-22-Sponsored_Post%3A_Amazon%2C_Zoosk%2C_Booking%2C_aiCache%2C_Teradata_Aster%2C_Aerospike%2C_Percona%2C_ScaleOut%2C_New_Relic%2C_NetDNA%2C_Logic_Monitor%2C_AppDynamics%2C_ManageEngine%2C_Site24x7.html">1391 high scalability-2013-01-22-Sponsored Post: Amazon, Zoosk, Booking, aiCache, Teradata Aster, Aerospike, Percona, ScaleOut, New Relic, NetDNA, Logic Monitor, AppDynamics, ManageEngine, Site24x7</a></p>
<p>Introduction: Who's Hiring?   
 The  AWS  Relational Database Service (RDS) automates management of relational databases in the cloud. We have a wide variety of customers and are part of many mission-critical applications, like the ones built by the 2012  Obama re-election campaign . If you're interested in joining a fast-growing service and team, please send your resume to  rds-jobs@amazon.com . 
  Hiring! Director of Site Operations  at  Zoosk.   We’re looking for an innovator. Someone who wants to take site operations along with a smart team of Sys Admins to the next level. This is a very hands-on leadership role in a high-availability production environment. Full details  here .    
 We need awesome people @  Booking.com  - We want YOU! Come design next
 generation interfaces, solve critical scalability problems, and hack on one of the largest Perl codebases. Apply:  http://www.booking.com/jobs.en-us.html  
 
  Teradata Aster  is looking for  Distributed Systems ,  Analytic Applications ,  and</p><p>6 0.068335414 <a title="830-tfidf-6" href="../high_scalability-2013/high_scalability-2013-01-07-Analyzing_billions_of_credit_card_transactions_and_serving_low-latency_insights_in_the_cloud.html">1382 high scalability-2013-01-07-Analyzing billions of credit card transactions and serving low-latency insights in the cloud</a></p>
<p>7 0.068220064 <a title="830-tfidf-7" href="../high_scalability-2013/high_scalability-2013-02-05-Sponsored_Post%3A_Amazon%2C_Zoosk%2C_aiCache%2C_Teradata_Aster%2C_Aerospike%2C_Percona%2C_ScaleOut%2C_New_Relic%2C_NetDNA%2C_Logic_Monitor%2C_AppDynamics%2C_ManageEngine%2C_Site24x7.html">1400 high scalability-2013-02-05-Sponsored Post: Amazon, Zoosk, aiCache, Teradata Aster, Aerospike, Percona, ScaleOut, New Relic, NetDNA, Logic Monitor, AppDynamics, ManageEngine, Site24x7</a></p>
<p>8 0.068071045 <a title="830-tfidf-8" href="../high_scalability-2013/high_scalability-2013-02-19-Sponsored_Post%3A_OLO%2C_Amazon%2C_Zoosk%2C_aiCache%2C_Teradata_Aster%2C_Aerospike%2C_Percona%2C_ScaleOut%2C_New_Relic%2C_Logic_Monitor%2C_AppDynamics%2C_ManageEngine%2C_Site24x7.html">1409 high scalability-2013-02-19-Sponsored Post: OLO, Amazon, Zoosk, aiCache, Teradata Aster, Aerospike, Percona, ScaleOut, New Relic, Logic Monitor, AppDynamics, ManageEngine, Site24x7</a></p>
<p>9 0.062266685 <a title="830-tfidf-9" href="../high_scalability-2011/high_scalability-2011-04-27-Heroku_Emergency_Strategy%3A_Incident_Command_System_and_8_Hour_Ops_Rotations_for_Fresh_Minds.html">1030 high scalability-2011-04-27-Heroku Emergency Strategy: Incident Command System and 8 Hour Ops Rotations for Fresh Minds</a></p>
<p>10 0.06048546 <a title="830-tfidf-10" href="../high_scalability-2011/high_scalability-2011-08-26-Stuff_The_Internet_Says_On_Scalability_For_August_26%2C_2011.html">1106 high scalability-2011-08-26-Stuff The Internet Says On Scalability For August 26, 2011</a></p>
<p>11 0.057675011 <a title="830-tfidf-11" href="../high_scalability-2012/high_scalability-2012-05-25-Stuff_The_Internet_Says_On_Scalability_For_May_25%2C_2012.html">1252 high scalability-2012-05-25-Stuff The Internet Says On Scalability For May 25, 2012</a></p>
<p>12 0.057348311 <a title="830-tfidf-12" href="../high_scalability-2009/high_scalability-2009-08-08-1dbase_vs._many_and_cloud_hosting_vs._dedicated_server%28s%29%3F.html">675 high scalability-2009-08-08-1dbase vs. many and cloud hosting vs. dedicated server(s)?</a></p>
<p>13 0.054427415 <a title="830-tfidf-13" href="../high_scalability-2009/high_scalability-2009-08-09-Writing_about_cisco_loadbalancer%3F.html">678 high scalability-2009-08-09-Writing about cisco loadbalancer?</a></p>
<p>14 0.053826842 <a title="830-tfidf-14" href="../high_scalability-2009/high_scalability-2009-02-14-Scaling_Digg_and_Other_Web_Applications.html">512 high scalability-2009-02-14-Scaling Digg and Other Web Applications</a></p>
<p>15 0.04955025 <a title="830-tfidf-15" href="../high_scalability-2009/high_scalability-2009-04-06-A_picture_is_realy_worth_a_thousand_word%2C_and_also_a_window_in_time....html">557 high scalability-2009-04-06-A picture is realy worth a thousand word, and also a window in time...</a></p>
<p>16 0.048082527 <a title="830-tfidf-16" href="../high_scalability-2009/high_scalability-2009-09-09-GridwiseTech_revolutionizes_data_management.html">697 high scalability-2009-09-09-GridwiseTech revolutionizes data management</a></p>
<p>17 0.045170862 <a title="830-tfidf-17" href="../high_scalability-2014/high_scalability-2014-03-27-Strategy%3A_Cache_Stored_Procedure_Results.html">1620 high scalability-2014-03-27-Strategy: Cache Stored Procedure Results</a></p>
<p>18 0.044612784 <a title="830-tfidf-18" href="../high_scalability-2007/high_scalability-2007-09-23-HA_for_switches.html">99 high scalability-2007-09-23-HA for switches</a></p>
<p>19 0.04307257 <a title="830-tfidf-19" href="../high_scalability-2007/high_scalability-2007-09-18-Amazon_Architecture.html">96 high scalability-2007-09-18-Amazon Architecture</a></p>
<p>20 0.040982783 <a title="830-tfidf-20" href="../high_scalability-2014/high_scalability-2014-04-16-Six_Lessons_Learned_the_Hard_Way_About_Scaling_a_Million_User_System_.html">1633 high scalability-2014-04-16-Six Lessons Learned the Hard Way About Scaling a Million User System </a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.055), (1, 0.032), (2, -0.018), (3, 0.011), (4, -0.001), (5, -0.02), (6, -0.016), (7, 0.018), (8, -0.007), (9, -0.035), (10, 0.001), (11, -0.01), (12, 0.007), (13, 0.005), (14, 0.028), (15, 0.0), (16, 0.024), (17, 0.004), (18, -0.028), (19, 0.029), (20, 0.028), (21, -0.001), (22, 0.003), (23, -0.009), (24, -0.013), (25, 0.001), (26, 0.014), (27, 0.001), (28, -0.025), (29, 0.011), (30, -0.004), (31, -0.016), (32, -0.027), (33, -0.001), (34, -0.036), (35, 0.016), (36, -0.001), (37, 0.012), (38, 0.018), (39, 0.008), (40, 0.016), (41, -0.005), (42, -0.024), (43, -0.013), (44, 0.011), (45, -0.028), (46, -0.003), (47, 0.005), (48, -0.011), (49, 0.003)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94927645 <a title="830-lsi-1" href="../high_scalability-2010/high_scalability-2010-05-25-Strategy%3A_Rule_of_3_Admins_to_Save_Your_Sanity.html">830 high scalability-2010-05-25-Strategy: Rule of 3 Admins to Save Your Sanity</a></p>
<p>Introduction: The idea came up in this Hacker News  thread , commenting on a 37signals interview, that having three system administrators is the minimum optimal number of admins. Everyone wants to lower their costs by having each admin administer a lot of machines. The problem is when you have fewer than three admins you can never get a break from the constant corrosive pressure of always being on call. When every moment of your life you are dreading the next emergency, it eats at you. Having three admins solves that problem. With three admins you can:
  
 Go on a real vacation. The two remaining admins can switch off being on call. 
 Not be on call all the time. 
  
A larger shop will naturally have more admins so it's not as big an issue, but at smaller shops trying to minimize head count, carrying three admins (or people in those roles) might be something to consider.</p><p>2 0.66398919 <a title="830-lsi-2" href="../high_scalability-2011/high_scalability-2011-04-20-Packet_Pushers%3A_How_to_Build_a_Low_Cost_Data_Center.html">1027 high scalability-2011-04-20-Packet Pushers: How to Build a Low Cost Data Center</a></p>
<p>Introduction: The main thrust of the Packet Pushers  Show 41  episode was to reveal and ruminate over the horrors of a successful  attack on RSA , which puts the whole world security complex at risk. Near the end, at about 46 minutes in, there was an excellent section on how to go about building out a low cost datacenter.
 
Who cares? Well, someone emailed me this exact same question awhile back and I had a pretty useless response. So here's making up for that by summarizing the recommendations from the elite Packet Pushers cabal:
 
 
  
 Look at Arista and Juniper.  
 Juniper           
 
 Has a range of stackable switches, which includes some 10 gig. 
 If your budget can stretch for it they might make a good deal on their new QFX proto-fabric product. You can't get a full sized fabric solution, but you can get a few switches together to make a two port fabric. Good solution if you are running 10 gig and only need 30 or 40 10 gig ports. Thinks Juniper would make a good deal in order to get a few re</p><p>3 0.65065897 <a title="830-lsi-3" href="../high_scalability-2007/high_scalability-2007-07-24-Major_Websites_Down%3A_Or_Why_You_Want_to_Run_in_Two_or_More_Data_Centers..html">23 high scalability-2007-07-24-Major Websites Down: Or Why You Want to Run in Two or More Data Centers.</a></p>
<p>Introduction: A lot of sites hosted in San Francisco are down because of at least 6 back-to-back power outages power outages. More details at  laughingsquid .
   
Sites like SecondLife, Craigstlist, Technorati, Yelp and all Six Apart properties, TypePad, LiveJournal and Vox are all down. The cause was an underground explosion in a transformer vault under a manhole at 560 Mission Street. Flames shot 6 feet out from the manhole cover. Over PG&E; 30,000 customers are without power.  What's perplexing is the UPS backup and diesel generators didn't kick in to bring the datacenter back on line. I've never toured that datacenter, but they usually have massive backup systems. It's probably one of those multiple simultaneous failure situations that you hope never happen in real life, but too often do. Or maybe the infrastructure wasn't rolled out completely.  Update: the cause was a cascade of failures in a tightly couples system that could never happen :-) Details at  Failure Happens: A summary of the power</p><p>4 0.64737642 <a title="830-lsi-4" href="../high_scalability-2012/high_scalability-2012-03-14-The_Azure_Outage%3A_Time_Is_a_SPOF%2C_Leap_Day_Doubly_So.html">1209 high scalability-2012-03-14-The Azure Outage: Time Is a SPOF, Leap Day Doubly So</a></p>
<p>Introduction: This is a guest post by Steve Newman, co-founder of Writely (Google Docs), tech lead on the Paxos-based synchronous replication in Megastore, and founder of cloud service provider  Scalyr.com .  
 
Microsoft’s Azure service suffered a widely publicized outage on February 28th / 29th. Microsoft recently published an excellent  postmortem . For anyone trying to run a high-availability service, this incident can teach several important lessons.
 
The central lesson is that, no matter how much work you put into redundancy, problems will arise. Murphy is strong and, I might say, creative; things go wrong. So preventative measures are important, but how you react to problems is just as important. It’s interesting to review the Azure incident in this light.
 
The postmortem is worth reading in its entirety, but here’s a quick summary: each time Azure launches a new VM, it creates a “transfer certificate” to secure communications with that VM. There was a bug in the code that determines the ce</p><p>5 0.64501333 <a title="830-lsi-5" href="../high_scalability-2008/high_scalability-2008-07-07-Five_Ways_to_Stop_Framework_Fixation_from_Crashing_Your_Scaling_Strategy.html">347 high scalability-2008-07-07-Five Ways to Stop Framework Fixation from Crashing Your Scaling Strategy</a></p>
<p>Introduction: If you've wondered why I haven't been posting lately it's because I've been on an amazing  Beach's motorcycle tour  of the  Alps  ( and ,  and ,  and ,  and ,  and ,  and ,  and ,  and ). My wife (Linda) and I rode two-up on a BMW 1200 GS through the alps in Germany, Austria, Switzerland, Italy, Slovenia, and Lichtenstein.   The trip was more beautiful than I ever imagined. We rode challenging mountain pass after mountain pass, froze in the rain, baked in the heat, woke up on excellent Italian coffee, ate slice after slice of tasty apple strudel, drank dazzling local wines, smelled the fresh cut grass as the Swiss en masse cut hay for the winter feeding of their dairy cows, rode the amazing Munich train system, listened as cow bells tinkled like wind chimes throughout small valleys, drank water from a pure alpine spring on a blisteringly hot hike, watched local German folk dancers represent their regions, and had fun in the company of fellow riders. Magical.  They say you'll ride more</p><p>6 0.62582618 <a title="830-lsi-6" href="../high_scalability-2008/high_scalability-2008-07-18-Robert_Scoble%27s_Rules_for_Successfully_Scaling_Startups.html">352 high scalability-2008-07-18-Robert Scoble's Rules for Successfully Scaling Startups</a></p>
<p>7 0.62172091 <a title="830-lsi-7" href="../high_scalability-2013/high_scalability-2013-01-16-What_if_Cars_Were_Rented_Like_We_Hire_Programmers%3F.html">1388 high scalability-2013-01-16-What if Cars Were Rented Like We Hire Programmers?</a></p>
<p>8 0.61818737 <a title="830-lsi-8" href="../high_scalability-2013/high_scalability-2013-08-19-What_can_the_Amazing_Race_to_the_South_Pole_Teach_us_About_Startups%3F.html">1503 high scalability-2013-08-19-What can the Amazing Race to the South Pole Teach us About Startups?</a></p>
<p>9 0.61667627 <a title="830-lsi-9" href="../high_scalability-2013/high_scalability-2013-08-12-100_Curse_Free_Lessons_from_Gordon_Ramsay_on_Building_Great_Software.html">1500 high scalability-2013-08-12-100 Curse Free Lessons from Gordon Ramsay on Building Great Software</a></p>
<p>10 0.60753441 <a title="830-lsi-10" href="../high_scalability-2009/high_scalability-2009-07-16-Scaling_Traffic%3A_People_Pod_Pool_of_On_Demand_Self_Driving_Robotic_Cars_who_Automatically_Refuel_from_Cheap_Solar.html">657 high scalability-2009-07-16-Scaling Traffic: People Pod Pool of On Demand Self Driving Robotic Cars who Automatically Refuel from Cheap Solar</a></p>
<p>11 0.60664612 <a title="830-lsi-11" href="../high_scalability-2010/high_scalability-2010-10-08-4_Scalability_Themes_from_Surgecon.html">917 high scalability-2010-10-08-4 Scalability Themes from Surgecon</a></p>
<p>12 0.60646218 <a title="830-lsi-12" href="../high_scalability-2014/high_scalability-2014-02-03-How_Google_Backs_Up_the_Internet_Along_With_Exabytes_of_Other_Data.html">1589 high scalability-2014-02-03-How Google Backs Up the Internet Along With Exabytes of Other Data</a></p>
<p>13 0.6022988 <a title="830-lsi-13" href="../high_scalability-2013/high_scalability-2013-07-15-Ask_HS%3A_What%27s_Wrong_with_Twitter%2C_Why_Isn%27t_One_Machine_Enough%3F.html">1491 high scalability-2013-07-15-Ask HS: What's Wrong with Twitter, Why Isn't One Machine Enough?</a></p>
<p>14 0.60111392 <a title="830-lsi-14" href="../high_scalability-2013/high_scalability-2013-03-08-Stuff_The_Internet_Says_On_Scalability_For_March_8%2C_2013.html">1420 high scalability-2013-03-08-Stuff The Internet Says On Scalability For March 8, 2013</a></p>
<p>15 0.59835392 <a title="830-lsi-15" href="../high_scalability-2013/high_scalability-2013-04-19-Stuff_The_Internet_Says_On_Scalability_For_April_19%2C_2013.html">1443 high scalability-2013-04-19-Stuff The Internet Says On Scalability For April 19, 2013</a></p>
<p>16 0.59547931 <a title="830-lsi-16" href="../high_scalability-2008/high_scalability-2008-10-26-Should_you_use_a_SAN_to_scale_your_architecture%3F_.html">430 high scalability-2008-10-26-Should you use a SAN to scale your architecture? </a></p>
<p>17 0.58963591 <a title="830-lsi-17" href="../high_scalability-2014/high_scalability-2014-04-04-Stuff_The_Internet_Says_On_Scalability_For_April_4th%2C_2014.html">1626 high scalability-2014-04-04-Stuff The Internet Says On Scalability For April 4th, 2014</a></p>
<p>18 0.58919603 <a title="830-lsi-18" href="../high_scalability-2011/high_scalability-2011-03-04-Stuff_The_Internet_Says_On_Scalability_For_March_4%2C_2011.html">999 high scalability-2011-03-04-Stuff The Internet Says On Scalability For March 4, 2011</a></p>
<p>19 0.58633727 <a title="830-lsi-19" href="../high_scalability-2013/high_scalability-2013-05-15-Lesson_from_Airbnb%3A_Give_Yourself_Permission_to_Experiment_with_Non-scalable_Changes.html">1458 high scalability-2013-05-15-Lesson from Airbnb: Give Yourself Permission to Experiment with Non-scalable Changes</a></p>
<p>20 0.58585632 <a title="830-lsi-20" href="../high_scalability-2009/high_scalability-2009-07-20-A_Scalability_Lament.html">659 high scalability-2009-07-20-A Scalability Lament</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.088), (2, 0.212), (10, 0.036), (30, 0.018), (34, 0.284), (61, 0.12), (79, 0.053), (85, 0.042)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.91480166 <a title="830-lda-1" href="../high_scalability-2010/high_scalability-2010-02-15-Scaling_Ambition_at_StackOverflow.html">777 high scalability-2010-02-15-Scaling Ambition at StackOverflow</a></p>
<p>Introduction: Joel Spolsky and Jeff   Atwood are  raising VC money  for    StackOverflow.  This is interesting for three reasons: 1) Joel has always seemed like a keep it small and grow organically type of guy, so this is a big step in a different direction. 2) It means they think there's a very big market in the Q&A; space and they mean to capture as much as the market as possible. 3) Most importantly for this blog, Joel gives some good advice on when to stay fresh and local and when it's time to jump for the brass ring, scale up your ambition, and go for VC money. Please see  Joel's blog  post for the details, but here's when to go VC:
  
 There’s a land grab going on. 
 There is a provable concept that’s repeatable. 
 The business itself could benefit from the publicity. 
 The investor will add substantial value to the business. 
 The business can potentially have a big exit or become a large, publically traded company. 
 The founders are not in it for their own personal aggrandizement. 
  
Joel t</p><p>same-blog 2 0.90895206 <a title="830-lda-2" href="../high_scalability-2010/high_scalability-2010-05-25-Strategy%3A_Rule_of_3_Admins_to_Save_Your_Sanity.html">830 high scalability-2010-05-25-Strategy: Rule of 3 Admins to Save Your Sanity</a></p>
<p>Introduction: The idea came up in this Hacker News  thread , commenting on a 37signals interview, that having three system administrators is the minimum optimal number of admins. Everyone wants to lower their costs by having each admin administer a lot of machines. The problem is when you have fewer than three admins you can never get a break from the constant corrosive pressure of always being on call. When every moment of your life you are dreading the next emergency, it eats at you. Having three admins solves that problem. With three admins you can:
  
 Go on a real vacation. The two remaining admins can switch off being on call. 
 Not be on call all the time. 
  
A larger shop will naturally have more admins so it's not as big an issue, but at smaller shops trying to minimize head count, carrying three admins (or people in those roles) might be something to consider.</p><p>3 0.79777724 <a title="830-lda-3" href="../high_scalability-2011/high_scalability-2011-08-04-Jim_Starkey_is_Creating_a_Brave_New_World_by_Rethinking_Databases_for_the_Cloud.html">1092 high scalability-2011-08-04-Jim Starkey is Creating a Brave New World by Rethinking Databases for the Cloud</a></p>
<p>Introduction: Jim Starkey , founder of  NuoDB , in this  thread  on the Cloud Computing group, delivers a  masterful post  on why he thinks the relational model is the best overall compromise amongst the different options, why NewSQL can free itself from the limitations of legacy SQL architectures, and how this creates a brave new lock free world.... 
 
I'll  [Jim Starkey]  go into more detail later in the post for those who care, but the executive summary goes like this:  Network latency is relatively high and human attention span is relatively low.  So human facing computer systems have to perform their work in a small number of trips between the client and the database server.  But the human condition leads inexorably to data complexity.  There are really only two strategies to manage this problem. One is to use coarse granularity storage, glombing together related data into a single blob and letting intelligence on the client make sense of it.  The other is storing fine granularity data on the s</p><p>4 0.77447015 <a title="830-lda-4" href="../high_scalability-2007/high_scalability-2007-10-03-Save_on_a_Load_Balancer_By_Using_Client_Side_Load_Balancing.html">109 high scalability-2007-10-03-Save on a Load Balancer By Using Client Side Load Balancing</a></p>
<p>Introduction: In   Client Side Load Balancing for Web 2.0 Applications   author Lei Zhu suggests a very interesting approach to load balancing: forget DNS round robbin, toss your expensive load balancer, and make your client do the load balancing for you. Your client maintains a list of possible servers and cycles through them. All the details are explained in the article, but it's an intriguing idea, especially for the budget conscious startup.</p><p>5 0.76789904 <a title="830-lda-5" href="../high_scalability-2009/high_scalability-2009-04-06-A_picture_is_realy_worth_a_thousand_word%2C_and_also_a_window_in_time....html">557 high scalability-2009-04-06-A picture is realy worth a thousand word, and also a window in time...</a></p>
<p>Introduction: Photograpic picture to me is window, an address to that specific moment what do your think about that?</p><p>6 0.74949962 <a title="830-lda-6" href="../high_scalability-2008/high_scalability-2008-09-22-Paper%3A_On_Delivering_Embarrassingly_Distributed_Cloud_Services.html">387 high scalability-2008-09-22-Paper: On Delivering Embarrassingly Distributed Cloud Services</a></p>
<p>7 0.73952067 <a title="830-lda-7" href="../high_scalability-2010/high_scalability-2010-07-14-DynaTrace%27s_Top_10_Performance_Problems_taken_from_Zappos%2C_Monster%2C_Thomson_and_Co.html">859 high scalability-2010-07-14-DynaTrace's Top 10 Performance Problems taken from Zappos, Monster, Thomson and Co</a></p>
<p>8 0.71996373 <a title="830-lda-8" href="../high_scalability-2014/high_scalability-2014-02-12-Paper%3A_Network_Stack_Specialization_for_Performance_.html">1594 high scalability-2014-02-12-Paper: Network Stack Specialization for Performance </a></p>
<p>9 0.71582371 <a title="830-lda-9" href="../high_scalability-2012/high_scalability-2012-12-28-Stuff_The_Internet_Says_On_Scalability_For_December_28%2C_2012.html">1378 high scalability-2012-12-28-Stuff The Internet Says On Scalability For December 28, 2012</a></p>
<p>10 0.68186617 <a title="830-lda-10" href="../high_scalability-2013/high_scalability-2013-10-28-Design_Decisions_for_Scaling_Your_High_Traffic_Feeds.html">1538 high scalability-2013-10-28-Design Decisions for Scaling Your High Traffic Feeds</a></p>
<p>11 0.67894739 <a title="830-lda-11" href="../high_scalability-2010/high_scalability-2010-12-21-SQL_%2B_NoSQL_%3D_Yes_%21.html">961 high scalability-2010-12-21-SQL + NoSQL = Yes !</a></p>
<p>12 0.67891663 <a title="830-lda-12" href="../high_scalability-2010/high_scalability-2010-11-16-Facebook%27s_New_Real-time_Messaging_System%3A_HBase_to_Store_135%2B_Billion_Messages_a_Month.html">943 high scalability-2010-11-16-Facebook's New Real-time Messaging System: HBase to Store 135+ Billion Messages a Month</a></p>
<p>13 0.67748016 <a title="830-lda-13" href="../high_scalability-2012/high_scalability-2012-05-24-Build_your_own_twitter_like_real_time_analytics_-_a_step_by_step_guide.html">1251 high scalability-2012-05-24-Build your own twitter like real time analytics - a step by step guide</a></p>
<p>14 0.67636573 <a title="830-lda-14" href="../high_scalability-2009/high_scalability-2009-09-12-How_Google_Taught_Me_to_Cache_and_Cash-In.html">703 high scalability-2009-09-12-How Google Taught Me to Cache and Cash-In</a></p>
<p>15 0.67604923 <a title="830-lda-15" href="../high_scalability-2011/high_scalability-2011-10-31-15_Ways_to_Make_Your_Application_Feel_More_Responsive_under_Google_App_Engine.html">1135 high scalability-2011-10-31-15 Ways to Make Your Application Feel More Responsive under Google App Engine</a></p>
<p>16 0.6746822 <a title="830-lda-16" href="../high_scalability-2009/high_scalability-2009-10-19-Drupal%27s_Scalability_Makeover_-_You_give_up_some_control_and_you_get_back_scalability.html">724 high scalability-2009-10-19-Drupal's Scalability Makeover - You give up some control and you get back scalability</a></p>
<p>17 0.67453825 <a title="830-lda-17" href="../high_scalability-2014/high_scalability-2014-01-06-How_HipChat_Stores_and_Indexes_Billions_of_Messages_Using_ElasticSearch_and_Redis.html">1573 high scalability-2014-01-06-How HipChat Stores and Indexes Billions of Messages Using ElasticSearch and Redis</a></p>
<p>18 0.67447269 <a title="830-lda-18" href="../high_scalability-2009/high_scalability-2009-10-02-HighScalability_has_Moved_to_Squarespace.com%21_.html">714 high scalability-2009-10-02-HighScalability has Moved to Squarespace.com! </a></p>
<p>19 0.67443973 <a title="830-lda-19" href="../high_scalability-2010/high_scalability-2010-11-30-NoCAP_%E2%80%93_Part_III_%E2%80%93_GigaSpaces_clustering_explained...html">950 high scalability-2010-11-30-NoCAP – Part III – GigaSpaces clustering explained..</a></p>
<p>20 0.67395711 <a title="830-lda-20" href="../high_scalability-2008/high_scalability-2008-07-16-The_Mother_of_All_Database_Normalization_Debates_on_Coding_Horror.html">351 high scalability-2008-07-16-The Mother of All Database Normalization Debates on Coding Horror</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
