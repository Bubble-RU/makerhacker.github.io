<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>946 high scalability-2010-11-22-Strategy: Google Sends Canary Requests into the Data Mine</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2010" href="../home/high_scalability-2010_home.html">high_scalability-2010</a> <a title="high_scalability-2010-946" href="#">high_scalability-2010-946</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>946 high scalability-2010-11-22-Strategy: Google Sends Canary Requests into the Data Mine</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2010-946-html" href="http://highscalability.com//blog/2010/11/22/strategy-google-sends-canary-requests-into-the-data-mine.html">html</a></p><p>Introduction: Google runs queries against thousands of in-memory index nodes in parallel and
then merges the results. One of the interesting problems with this approach,
explains Google's Jeff Dean in thislecture at Stanford, is theQuery of Death.A
query can cause a program to fail because of bugs or various other issues.
This means that a single query can take down an entire cluster of machines,
which is not good for availability and response times, as it takes quite a
while for thousands of machines to recover. Thus the Query of Death. New
queries are always coming into the system and when you are always rolling out
new software, it's impossible to completely get rid of the problem.Two
solutions:Test against logs. Google replays a month's worth of logs to see if
any of those queries kill anything. That helps, but Queries of Death may still
happen.Send a canary request. A request is sent to one machine. If the request
succeeds then it will probably succeed on all machines, so go ahead with the
quer</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Google runs queries against thousands of in-memory index nodes in parallel and then merges the results. [sent-1, score-0.668]
</p><p>2 One of the interesting problems with this approach, explains Google's Jeff Dean in thislecture at Stanford, is theQuery of Death. [sent-2, score-0.096]
</p><p>3 A query can cause a program to fail because of bugs or various other issues. [sent-3, score-0.649]
</p><p>4 This means that a single query can take down an entire cluster of machines, which is not good for availability and response times, as it takes quite a while for thousands of machines to recover. [sent-4, score-0.532]
</p><p>5 New queries are always coming into the system and when you are always rolling out new software, it's impossible to completely get rid of the problem. [sent-6, score-0.87]
</p><p>6 Google replays a month's worth of logs to see if any of those queries kill anything. [sent-8, score-0.703]
</p><p>7 If the request succeeds then it will probably succeed on all machines, so go ahead with the query. [sent-12, score-0.794]
</p><p>8 If the request fails the only one machine is down, no big deal. [sent-13, score-0.683]
</p><p>9 Now try the request again on another machine to verify that it really is a query of death. [sent-14, score-0.852]
</p><p>10 If the request fails a certain number of times then the request if rejected and logged for further debugging. [sent-15, score-1.416]
</p><p>11 The result is only a few servers are crashed instead of 1000s. [sent-16, score-0.169]
</p><p>12 This is a pretty clever technique, especially given the combined trends of scale-out and continuous deployment. [sent-17, score-0.406]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('request', 0.376), ('query', 0.237), ('queries', 0.229), ('fails', 0.208), ('replays', 0.207), ('succeeds', 0.195), ('crashed', 0.169), ('machines', 0.167), ('canary', 0.165), ('merges', 0.165), ('rejected', 0.161), ('verify', 0.14), ('succeed', 0.132), ('dean', 0.132), ('stanford', 0.132), ('google', 0.131), ('thousands', 0.128), ('logged', 0.126), ('death', 0.119), ('clever', 0.118), ('bugs', 0.116), ('rid', 0.111), ('rolling', 0.111), ('kill', 0.107), ('combined', 0.107), ('jeff', 0.105), ('technique', 0.105), ('machine', 0.099), ('explains', 0.096), ('times', 0.094), ('impossible', 0.093), ('ahead', 0.091), ('trends', 0.091), ('logs', 0.091), ('continuous', 0.09), ('always', 0.088), ('sent', 0.086), ('thus', 0.085), ('coming', 0.084), ('fail', 0.08), ('index', 0.079), ('month', 0.076), ('certain', 0.075), ('program', 0.075), ('cause', 0.074), ('others', 0.072), ('worth', 0.069), ('runs', 0.067), ('various', 0.067), ('completely', 0.066)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="946-tfidf-1" href="../high_scalability-2010/high_scalability-2010-11-22-Strategy%3A_Google_Sends_Canary_Requests_into_the_Data_Mine.html">946 high scalability-2010-11-22-Strategy: Google Sends Canary Requests into the Data Mine</a></p>
<p>Introduction: Google runs queries against thousands of in-memory index nodes in parallel and
then merges the results. One of the interesting problems with this approach,
explains Google's Jeff Dean in thislecture at Stanford, is theQuery of Death.A
query can cause a program to fail because of bugs or various other issues.
This means that a single query can take down an entire cluster of machines,
which is not good for availability and response times, as it takes quite a
while for thousands of machines to recover. Thus the Query of Death. New
queries are always coming into the system and when you are always rolling out
new software, it's impossible to completely get rid of the problem.Two
solutions:Test against logs. Google replays a month's worth of logs to see if
any of those queries kill anything. That helps, but Queries of Death may still
happen.Send a canary request. A request is sent to one machine. If the request
succeeds then it will probably succeed on all machines, so go ahead with the
quer</p><p>2 0.16992356 <a title="946-tfidf-2" href="../high_scalability-2010/high_scalability-2010-02-05-High_Availability_Principle_%3A_Concurrency_Control.html">772 high scalability-2010-02-05-High Availability Principle : Concurrency Control</a></p>
<p>Introduction: One important high availability principle is concurrency control.  The idea is
to allow only that much traffic through to your system which your system can
handle successfully.  For example: if your system is certified to handle a
concurrency of 100 then the 101st request should either timeout, be asked to
try later  or wait until one of the previous 100 requests finish.  The 101st
request should not be allowed to negatively impact the experience of the other
100 users.  Only the 101st request should be impacted.Read more here...</p><p>3 0.16719364 <a title="946-tfidf-3" href="../high_scalability-2012/high_scalability-2012-06-18-Google_on_Latency_Tolerant_Systems%3A_Making_a_Predictable_Whole_Out_of_Unpredictable_Parts___.html">1266 high scalability-2012-06-18-Google on Latency Tolerant Systems: Making a Predictable Whole Out of Unpredictable Parts   </a></p>
<p>Introduction: InTaming The Long Latency Tailwe coveredLuiz Barroso's exploration of the long
tail latency (some operations are really slow) problems generated by large
fanout architectures (a request is composed of potentially thousands of other
requests). You may have noticed there weren't a lot of solutions. That's where
a talk I attended,Achieving Rapid Response Times in Large Online
Services(slide deck), byJeff Dean, also of Google, comes in:In this talk, I'll
describe a collection of techniques and practices lowering response times in
large distributed systems whose components run on shared clusters of machines,
where pieces of these systems are subject to interference by other tasks, and
where unpredictable latency hiccups are the norm, not the exception.The goal
is to use software techniques to reduce variability given the increasing
variability in underlying hardware, the need to handle dynamic workloads on a
shared infrastructure, and the need to use large fanout architectures to
operate at</p><p>4 0.16297844 <a title="946-tfidf-4" href="../high_scalability-2013/high_scalability-2013-09-09-Need_Help_with_Database_Scalability%3F_Understand_I-O.html">1514 high scalability-2013-09-09-Need Help with Database Scalability? Understand I-O</a></p>
<p>Introduction: This is a guest post byZardosht Kasheff, Software Developer atTokutek, a
storage engine company that delivers 21st-Century capabilities to the leading
open source data management platforms.As software developers, we value
abstraction. The simpler the API, the more attractive it becomes. Arguably,
MongoDB's greatest strengths are its elegant API and itsagility, which let
developers simply code.But whenMongoDBruns into scalability problems onbig
data, developers need to peek underneath the covers to understand the
underlying issues and how to fix them. Without understanding, one may end up
with an inefficient solution that costs time and money. For example, one may
shard prematurely, increasing hardware and management costs, when a simpler
replication setup would do. Or, one may increase the size of a replica set
when upgrading to SSDs would suffice.This article showshow to reason about
some big data scalability problemsin an effort to find efficient
solutions.Defining the IssuesFirst, l</p><p>5 0.1539533 <a title="946-tfidf-5" href="../high_scalability-2010/high_scalability-2010-11-04-Facebook_at_13_Million_Queries_Per_Second_Recommends%3A_Minimize_Request_Variance.html">934 high scalability-2010-11-04-Facebook at 13 Million Queries Per Second Recommends: Minimize Request Variance</a></p>
<p>Introduction: Facebook gave aMySQL Tech Talk where they talked about many things MySQL, but
one of the more subtle and interesting points was their focus on controlling
the variance of request response times and not just worrying about maximizing
queries per second.But first the scalability porn. Facebook's OLTP performance
numbers were as usual, quite dramatic:Query response times: 4ms reads, 5ms
writes. Rows read per second: 450M peakNetwork bytes per second: 38GB
peakQueries per second: 13M peakRows changed per second: 3.5M peakInnoDB disk
ops per second: 5.2M peak Some thoughts on creating quality, not quantity:They
don't care about average response times, instead, they want to minimize
variance. Every click must be responded to quickly. The quality of service for
each request matters.It's OK if a query is slow as long as it is always slow.
They don't try to get the highest queries per second out of each machine. What
is important is that the edge cases are not the bad. They figure out why the
r</p><p>6 0.14256901 <a title="946-tfidf-6" href="../high_scalability-2011/high_scalability-2011-10-31-15_Ways_to_Make_Your_Application_Feel_More_Responsive_under_Google_App_Engine.html">1135 high scalability-2011-10-31-15 Ways to Make Your Application Feel More Responsive under Google App Engine</a></p>
<p>7 0.13212302 <a title="946-tfidf-7" href="../high_scalability-2008/high_scalability-2008-10-13-Challenges_from_large_scale_computing_at_Google.html">409 high scalability-2008-10-13-Challenges from large scale computing at Google</a></p>
<p>8 0.12076593 <a title="946-tfidf-8" href="../high_scalability-2010/high_scalability-2010-05-10-Sify.com_Architecture_-_A_Portal_at_3900_Requests_Per_Second.html">825 high scalability-2010-05-10-Sify.com Architecture - A Portal at 3900 Requests Per Second</a></p>
<p>9 0.1127819 <a title="946-tfidf-9" href="../high_scalability-2008/high_scalability-2008-04-08-Google_AppEngine_-_A_First_Look.html">301 high scalability-2008-04-08-Google AppEngine - A First Look</a></p>
<p>10 0.11083203 <a title="946-tfidf-10" href="../high_scalability-2012/high_scalability-2012-03-12-Google%3A_Taming_the_Long_Latency_Tail_-_When_More_Machines_Equals_Worse_Results.html">1207 high scalability-2012-03-12-Google: Taming the Long Latency Tail - When More Machines Equals Worse Results</a></p>
<p>11 0.10964637 <a title="946-tfidf-11" href="../high_scalability-2013/high_scalability-2013-10-08-F1_and_Spanner_Holistically_Compared.html">1529 high scalability-2013-10-08-F1 and Spanner Holistically Compared</a></p>
<p>12 0.10504031 <a title="946-tfidf-12" href="../high_scalability-2009/high_scalability-2009-10-30-Hot_Scalabilty_Links_for_October_30_2009.html">734 high scalability-2009-10-30-Hot Scalabilty Links for October 30 2009</a></p>
<p>13 0.099685542 <a title="946-tfidf-13" href="../high_scalability-2011/high_scalability-2011-08-29-The_Three_Ages_of_Google_-_Batch%2C_Warehouse%2C_Instant.html">1107 high scalability-2011-08-29-The Three Ages of Google - Batch, Warehouse, Instant</a></p>
<p>14 0.098817162 <a title="946-tfidf-14" href="../high_scalability-2013/high_scalability-2013-11-13-Google%3A_Multiplex_Multiple_Works_Loads_on_Computers_to_Increase_Machine_Utilization_and_Save_Money.html">1548 high scalability-2013-11-13-Google: Multiplex Multiple Works Loads on Computers to Increase Machine Utilization and Save Money</a></p>
<p>15 0.098578341 <a title="946-tfidf-15" href="../high_scalability-2011/high_scalability-2011-02-10-Database_Isolation_Levels_And_Their_Effects_on_Performance_and_Scalability.html">986 high scalability-2011-02-10-Database Isolation Levels And Their Effects on Performance and Scalability</a></p>
<p>16 0.098351002 <a title="946-tfidf-16" href="../high_scalability-2014/high_scalability-2014-02-05-Little%E2%80%99s_Law%2C_Scalability_and_Fault_Tolerance%3A_The_OS_is_your_bottleneck._What_you_can_do%3F.html">1591 high scalability-2014-02-05-Little’s Law, Scalability and Fault Tolerance: The OS is your bottleneck. What you can do?</a></p>
<p>17 0.096788742 <a title="946-tfidf-17" href="../high_scalability-2011/high_scalability-2011-07-07-Myth%3A_Google_Uses_Server_Farms_So_You_Should_Too_-_Resurrection_of_the_Big-Ass_Machines.html">1075 high scalability-2011-07-07-Myth: Google Uses Server Farms So You Should Too - Resurrection of the Big-Ass Machines</a></p>
<p>18 0.095874749 <a title="946-tfidf-18" href="../high_scalability-2010/high_scalability-2010-07-27-YeSQL%3A_An_Overview_of_the_Various_Query_Semantics_in_the_Post_Only-SQL_World.html">867 high scalability-2010-07-27-YeSQL: An Overview of the Various Query Semantics in the Post Only-SQL World</a></p>
<p>19 0.095819205 <a title="946-tfidf-19" href="../high_scalability-2011/high_scalability-2011-05-02-Stack_Overflow_Makes_Slow_Pages_100x_Faster_by_Simple_SQL_Tuning.html">1032 high scalability-2011-05-02-Stack Overflow Makes Slow Pages 100x Faster by Simple SQL Tuning</a></p>
<p>20 0.093572661 <a title="946-tfidf-20" href="../high_scalability-2012/high_scalability-2012-03-06-Ask_For_Forgiveness_Programming_-_Or_How_We%27ll_Program_1000_Cores.html">1204 high scalability-2012-03-06-Ask For Forgiveness Programming - Or How We'll Program 1000 Cores</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.153), (1, 0.107), (2, -0.018), (3, -0.003), (4, 0.002), (5, 0.022), (6, 0.051), (7, 0.059), (8, 0.008), (9, -0.033), (10, -0.014), (11, -0.014), (12, -0.018), (13, -0.023), (14, 0.02), (15, -0.016), (16, -0.054), (17, -0.051), (18, 0.057), (19, -0.006), (20, 0.089), (21, -0.032), (22, 0.034), (23, -0.06), (24, 0.031), (25, 0.062), (26, -0.051), (27, 0.035), (28, -0.045), (29, 0.038), (30, 0.018), (31, -0.03), (32, 0.022), (33, 0.051), (34, 0.013), (35, 0.065), (36, 0.054), (37, -0.05), (38, -0.068), (39, 0.029), (40, -0.007), (41, -0.079), (42, 0.004), (43, -0.07), (44, -0.03), (45, -0.035), (46, 0.031), (47, -0.013), (48, 0.037), (49, -0.017)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99072421 <a title="946-lsi-1" href="../high_scalability-2010/high_scalability-2010-11-22-Strategy%3A_Google_Sends_Canary_Requests_into_the_Data_Mine.html">946 high scalability-2010-11-22-Strategy: Google Sends Canary Requests into the Data Mine</a></p>
<p>Introduction: Google runs queries against thousands of in-memory index nodes in parallel and
then merges the results. One of the interesting problems with this approach,
explains Google's Jeff Dean in thislecture at Stanford, is theQuery of Death.A
query can cause a program to fail because of bugs or various other issues.
This means that a single query can take down an entire cluster of machines,
which is not good for availability and response times, as it takes quite a
while for thousands of machines to recover. Thus the Query of Death. New
queries are always coming into the system and when you are always rolling out
new software, it's impossible to completely get rid of the problem.Two
solutions:Test against logs. Google replays a month's worth of logs to see if
any of those queries kill anything. That helps, but Queries of Death may still
happen.Send a canary request. A request is sent to one machine. If the request
succeeds then it will probably succeed on all machines, so go ahead with the
quer</p><p>2 0.81064856 <a title="946-lsi-2" href="../high_scalability-2012/high_scalability-2012-06-18-Google_on_Latency_Tolerant_Systems%3A_Making_a_Predictable_Whole_Out_of_Unpredictable_Parts___.html">1266 high scalability-2012-06-18-Google on Latency Tolerant Systems: Making a Predictable Whole Out of Unpredictable Parts   </a></p>
<p>Introduction: InTaming The Long Latency Tailwe coveredLuiz Barroso's exploration of the long
tail latency (some operations are really slow) problems generated by large
fanout architectures (a request is composed of potentially thousands of other
requests). You may have noticed there weren't a lot of solutions. That's where
a talk I attended,Achieving Rapid Response Times in Large Online
Services(slide deck), byJeff Dean, also of Google, comes in:In this talk, I'll
describe a collection of techniques and practices lowering response times in
large distributed systems whose components run on shared clusters of machines,
where pieces of these systems are subject to interference by other tasks, and
where unpredictable latency hiccups are the norm, not the exception.The goal
is to use software techniques to reduce variability given the increasing
variability in underlying hardware, the need to handle dynamic workloads on a
shared infrastructure, and the need to use large fanout architectures to
operate at</p><p>3 0.73290354 <a title="946-lsi-3" href="../high_scalability-2010/high_scalability-2010-04-27-Paper%3A__Dapper%2C_Google%27s_Large-Scale_Distributed_Systems_Tracing_Infrastructure.html">815 high scalability-2010-04-27-Paper:  Dapper, Google's Large-Scale Distributed Systems Tracing Infrastructure</a></p>
<p>Introduction: Imagine a single search request coursing through Google's massive
infrastructure. A single request can run across thousands of machines and
involve hundreds of different subsystems. And oh by the way, you are
processing more requests per second than any other system in the world. How do
you debug such a system? How do you figure out where the problems are? How do
you determine if programmers are coding correctly? How do you keep sensitive
data secret and safe? How do ensure products don't use more resources than
they are assigned? How do you store all the data? How do you make use of
it?That's where Dapper comes in. Dapper is Google's tracing system and it was
originally created to understand the system behaviour from a search request.
Now Google's production clustersgenerate more than 1 terabyte of sampled trace
data per day. So how does Dapper do what Dapper does?Dapper is described in an
very well written and intricately detailed paper:Dapper, a Large-Scale
Distributed Systems Traci</p><p>4 0.7166568 <a title="946-lsi-4" href="../high_scalability-2013/high_scalability-2013-02-11-At_Scale_Even_Little_Wins_Pay_Off_Big_-_Google_and_Facebook_Examples.html">1404 high scalability-2013-02-11-At Scale Even Little Wins Pay Off Big - Google and Facebook Examples</a></p>
<p>Introduction: There's a popular line of thought that says don't waste time on optimization
because developing features is more important than saving money. True, you can
always add resources, but at some point, especially in a more mature part of a
product lifecycle: performance equals $$$.Two great examples of this evolution
come from Facebook and Google. The upshot is that when you spend time and
money on optimizing your tool chain you can get huge wins in performance,
control, and costs. Certainly, don't bother if you are just starting, but at
some point you may want to switch to big development efforts in improving
efficiency.Facebook and HipHopThe Facebook example is quite well known:HipHop,
a static PHP compiler released in 2010, after two years of development. PHP
because Facebook implements their web tierin PHP. They've now developed a
dynamic compiler,HipHop VM, using techniques like JIT, side exits, HipHop
bytecode, type prediction, and parallel tracelet linking.This is an incredible
devel</p><p>5 0.71212536 <a title="946-lsi-5" href="../high_scalability-2011/high_scalability-2011-08-29-The_Three_Ages_of_Google_-_Batch%2C_Warehouse%2C_Instant.html">1107 high scalability-2011-08-29-The Three Ages of Google - Batch, Warehouse, Instant</a></p>
<p>Introduction: The world has changed. And some things that should not have been forgotten,
were lost. I found these words from the Lord of the Rings echoing in my head
as I listened to a fascinating presentation by Luiz Andre Barroso,
Distinguished Engineer at Google, concerning Google's legendary past, golden
present, and apocryphal future. His talk, Warehouse-Scale Computing: Entering
the Teenage Decade, was given at theFederated Computing Research Conference.
Luiz clearly knows his stuff and was early at Google, so he has a deep and
penetrating perspective on the technology. There's much to learn from, think
about, and build.Lord of the Rings applies at two levels. At the change level,
Middle Earth went throughthree ages. While listening to Luiz talk, it seems so
has Google: Batch (indexes calculated every month), Warehouse (the datacenter
is the computer), and Instant (make it all real-time). At the "what was
forgot" level, in the Instant Age section of the talk,  a common theme was the
challenge</p><p>6 0.70987839 <a title="946-lsi-6" href="../high_scalability-2011/high_scalability-2011-02-01-Google_Strategy%3A_Tree_Distribution_of_Requests_and_Responses.html">981 high scalability-2011-02-01-Google Strategy: Tree Distribution of Requests and Responses</a></p>
<p>7 0.70440996 <a title="946-lsi-7" href="../high_scalability-2012/high_scalability-2012-03-12-Google%3A_Taming_the_Long_Latency_Tail_-_When_More_Machines_Equals_Worse_Results.html">1207 high scalability-2012-03-12-Google: Taming the Long Latency Tail - When More Machines Equals Worse Results</a></p>
<p>8 0.70131922 <a title="946-lsi-8" href="../high_scalability-2013/high_scalability-2013-11-13-Google%3A_Multiplex_Multiple_Works_Loads_on_Computers_to_Increase_Machine_Utilization_and_Save_Money.html">1548 high scalability-2013-11-13-Google: Multiplex Multiple Works Loads on Computers to Increase Machine Utilization and Save Money</a></p>
<p>9 0.69891828 <a title="946-lsi-9" href="../high_scalability-2013/high_scalability-2013-10-21-Google%27s_Sanjay_Ghemawat_on_What_Made_Google_Google_and_Great_Big_Data_Career_Advice.html">1535 high scalability-2013-10-21-Google's Sanjay Ghemawat on What Made Google Google and Great Big Data Career Advice</a></p>
<p>10 0.67803472 <a title="946-lsi-10" href="../high_scalability-2010/high_scalability-2010-11-15-How_Google%27s_Instant_Previews_Reduces_HTTP_Requests.html">941 high scalability-2010-11-15-How Google's Instant Previews Reduces HTTP Requests</a></p>
<p>11 0.66703677 <a title="946-lsi-11" href="../high_scalability-2010/high_scalability-2010-08-04-Dremel%3A_Interactive_Analysis_of_Web-Scale_Datasets_-_Data_as_a_Programming_Paradigm.html">871 high scalability-2010-08-04-Dremel: Interactive Analysis of Web-Scale Datasets - Data as a Programming Paradigm</a></p>
<p>12 0.6646381 <a title="946-lsi-12" href="../high_scalability-2012/high_scalability-2012-09-24-Google_Spanner%27s_Most_Surprising_Revelation%3A_NoSQL_is_Out_and_NewSQL_is_In.html">1328 high scalability-2012-09-24-Google Spanner's Most Surprising Revelation: NoSQL is Out and NewSQL is In</a></p>
<p>13 0.66392595 <a title="946-lsi-13" href="../high_scalability-2008/high_scalability-2008-10-13-Challenges_from_large_scale_computing_at_Google.html">409 high scalability-2008-10-13-Challenges from large scale computing at Google</a></p>
<p>14 0.66353613 <a title="946-lsi-14" href="../high_scalability-2010/high_scalability-2010-09-11-Google%27s_Colossus_Makes_Search_Real-time_by_Dumping_MapReduce.html">900 high scalability-2010-09-11-Google's Colossus Makes Search Real-time by Dumping MapReduce</a></p>
<p>15 0.66220653 <a title="946-lsi-15" href="../high_scalability-2010/high_scalability-2010-11-04-Facebook_at_13_Million_Queries_Per_Second_Recommends%3A_Minimize_Request_Variance.html">934 high scalability-2010-11-04-Facebook at 13 Million Queries Per Second Recommends: Minimize Request Variance</a></p>
<p>16 0.65632284 <a title="946-lsi-16" href="../high_scalability-2012/high_scalability-2012-07-02-C_is_for_Compute_-_Google_Compute_Engine_%28GCE%29.html">1275 high scalability-2012-07-02-C is for Compute - Google Compute Engine (GCE)</a></p>
<p>17 0.65376437 <a title="946-lsi-17" href="../high_scalability-2011/high_scalability-2011-03-24-Strategy%3A_Disk_Backup_for_Speed%2C_Tape_Backup_to_Save_Your_Bacon%2C_Just_Ask_Google.html">1010 high scalability-2011-03-24-Strategy: Disk Backup for Speed, Tape Backup to Save Your Bacon, Just Ask Google</a></p>
<p>18 0.65045065 <a title="946-lsi-18" href="../high_scalability-2010/high_scalability-2010-09-09-How_did_Google_Instant_become_Faster_with_5-7X_More_Results_Pages%3F.html">899 high scalability-2010-09-09-How did Google Instant become Faster with 5-7X More Results Pages?</a></p>
<p>19 0.64325613 <a title="946-lsi-19" href="../high_scalability-2012/high_scalability-2012-08-30-Stuff_The_Internet_Says_On_Scalability_For_August_31%2C_2012.html">1315 high scalability-2012-08-30-Stuff The Internet Says On Scalability For August 31, 2012</a></p>
<p>20 0.64078557 <a title="946-lsi-20" href="../high_scalability-2011/high_scalability-2011-11-16-Google%2B_Infrastructure_Update_-_the_JavaScript_Story.html">1143 high scalability-2011-11-16-Google+ Infrastructure Update - the JavaScript Story</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.136), (2, 0.303), (22, 0.147), (30, 0.026), (40, 0.03), (61, 0.012), (79, 0.211), (94, 0.034)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96220994 <a title="946-lda-1" href="../high_scalability-2010/high_scalability-2010-11-22-Strategy%3A_Google_Sends_Canary_Requests_into_the_Data_Mine.html">946 high scalability-2010-11-22-Strategy: Google Sends Canary Requests into the Data Mine</a></p>
<p>Introduction: Google runs queries against thousands of in-memory index nodes in parallel and
then merges the results. One of the interesting problems with this approach,
explains Google's Jeff Dean in thislecture at Stanford, is theQuery of Death.A
query can cause a program to fail because of bugs or various other issues.
This means that a single query can take down an entire cluster of machines,
which is not good for availability and response times, as it takes quite a
while for thousands of machines to recover. Thus the Query of Death. New
queries are always coming into the system and when you are always rolling out
new software, it's impossible to completely get rid of the problem.Two
solutions:Test against logs. Google replays a month's worth of logs to see if
any of those queries kill anything. That helps, but Queries of Death may still
happen.Send a canary request. A request is sent to one machine. If the request
succeeds then it will probably succeed on all machines, so go ahead with the
quer</p><p>2 0.94765717 <a title="946-lda-2" href="../high_scalability-2009/high_scalability-2009-01-13-Product%3A_Gearman_-_Open_Source_Message_Queuing_System.html">491 high scalability-2009-01-13-Product: Gearman - Open Source Message Queuing System</a></p>
<p>Introduction: Update:New Gearman Server & Library in C, MySQL UDFs.Gearmanis an open
sourcemessage queuingsystem that makes it easy to do distributed job
processing using multiple languages. With Gearman you:farm out work to other
machines, dispatching function calls to machines that are better suited to do
work, to do work in parallel, to load balance lots of function calls, to call
functions between languages, spread CPU usage around your network.Gearman is
used by companies like LiveJournal, Yahoo!, and Digg. Digg, for example, runs
300,000 jobs a day through Gearman without any issues. Most large sites use
something similar. Why would anyone ever even need a message queuing
system?breakMessage queuing is a handy way to move work off your web servers
(like image manipulation), to generate thousands of documents in the
background, to run the multiple requests in parallel needed to build a web
page, or to perform tasks that can comfortably be run in the background and
not part of the main request l</p><p>3 0.94011801 <a title="946-lda-3" href="../high_scalability-2011/high_scalability-2011-11-16-Google%2B_Infrastructure_Update_-_the_JavaScript_Story.html">1143 high scalability-2011-11-16-Google+ Infrastructure Update - the JavaScript Story</a></p>
<p>Introduction: In Google+ Is Built Using Tools You Can Use Too: Closure, Java Servlets,
JavaScript, BigTable, Colossus, Quick Turnaround we glimpsed inside Google's
technology stack for building Google+. Mark Knichel, an engineer on the
Google+ infrastructure team, has helped us look alittle deeper on how
Javascript is handled in Google+.  Here's a quick look:They loveClosure for
its library, templates, compiler, and strict type checking. Compilation is now
required for good performance. I've wondered if GWT will be killed off as have
other Google properties, but I've been told GWT is being used heavily inside
Google, so thankfully that probably won't happen.Closure templatesare used
both Java and JavaScript to render pages server-side and in the browser. Just-
in-time JavaScript. Code is split into modules so the minimum amount of
JavaScript is loaded asynchronously in the background as necessary. Navigation
happens without loading the page.Page navigation happens without page reloads.
HTML Flush. A</p><p>4 0.93771094 <a title="946-lda-4" href="../high_scalability-2013/high_scalability-2013-11-27-Hidden_History%3A_Driving_the_Last_Spike_of_the_Transcontinental_Railroad_was_an_Early_Version_of_the_Internet_of_Things.html">1555 high scalability-2013-11-27-Hidden History: Driving the Last Spike of the Transcontinental Railroad was an Early Version of the Internet of Things</a></p>
<p>Introduction: The story of driving thegolden spiketo symbolize the completion of
thetranscontinental railroad is famous in the US. What is not so well known is
the story of how it also foreshadowed changes to come as an early version of
both the Internet and the Internet of Things. But that was 1869, how can that
possibly be?Telegraph as InternetFirst, let's establish the telegraph and
cable systems was an early version of an Internet.As railroad tracks were
being laid atranscontental telegraphsystem was also being constructed.
Telegraph lines were installed parallel to the tracks making instant
communication available across the continent, faster than any horse could
ride.With the transalantic cable system information could quickly span
continents in minutes:The miles of American telegraph grew from 40 in 1846 to
12,000 in 1850 to 23,000 in 1852. In Europe it increased from 2,000 in 1849 to
110,000 in 1869. The cost of sending 10 words was $1.55 in 1850, $1 in 1870,
40Â˘ in 1890. Within 29 years of</p><p>5 0.92300987 <a title="946-lda-5" href="../high_scalability-2014/high_scalability-2014-03-28-Stuff_The_Internet_Says_On_Scalability_For_March_28th%2C_2014.html">1621 high scalability-2014-03-28-Stuff The Internet Says On Scalability For March 28th, 2014</a></p>
<p>Introduction: Hey, it's HighScalability time:Looks like amultiverse, if you can keep
it.Quotable Quotes:@abt_programming: "I am a Unix Creationist. I believe the
world was created on January 1, 1970 and as prophesized, will end on January
19, 2038" - @teropa@demisbellot: Cloud prices are hitting attractive price
points, one more 40-50% drop and there'd be little reason to go it
alone.@scott4arrows: Dentist "Do you floss regularly?" Me "Do you back up your
computer data regularly?"@avestal: "I Kickstarted the Oculus Rift, what do I
get?" You get a lesson in how capitalism works.@mtabini: "$20 charger that
cost $2 to make." Not pictured here: the $14 you pay for the 10,000 charger
iterations that never made it to production.@strlen: "I built the original
assembler in JS, because it's what I prefer to use when I need to get down to
bare metal." - Adm. Grace Hoppertedchs: I'd like to propose a new rule for
Hacker News: only if you have built the thing you're saying someone should
save money by building</p><p>6 0.92195624 <a title="946-lda-6" href="../high_scalability-2009/high_scalability-2009-10-09-Have_you_collectl%27d_yet%3F__If_not%2C_maybe_collectl-utils_will_make_it_easier_to_do_so.html">719 high scalability-2009-10-09-Have you collectl'd yet?  If not, maybe collectl-utils will make it easier to do so</a></p>
<p>7 0.91866374 <a title="946-lda-7" href="../high_scalability-2009/high_scalability-2009-05-17-Product%3A_Hadoop.html">601 high scalability-2009-05-17-Product: Hadoop</a></p>
<p>8 0.91830957 <a title="946-lda-8" href="../high_scalability-2009/high_scalability-2009-06-11-Yahoo%21_Distribution_of_Hadoop.html">627 high scalability-2009-06-11-Yahoo! Distribution of Hadoop</a></p>
<p>9 0.91204697 <a title="946-lda-9" href="../high_scalability-2007/high_scalability-2007-07-30-Build_an_Infinitely_Scalable_Infrastructure_for_%24100_Using_Amazon_Services.html">38 high scalability-2007-07-30-Build an Infinitely Scalable Infrastructure for $100 Using Amazon Services</a></p>
<p>10 0.91158253 <a title="946-lda-10" href="../high_scalability-2014/high_scalability-2014-02-03-How_Google_Backs_Up_the_Internet_Along_With_Exabytes_of_Other_Data.html">1589 high scalability-2014-02-03-How Google Backs Up the Internet Along With Exabytes of Other Data</a></p>
<p>11 0.91045481 <a title="946-lda-11" href="../high_scalability-2007/high_scalability-2007-07-16-Blog%3A_MySQL_Performance_Blog_-_Everything_about_MySQL_Performance._.html">15 high scalability-2007-07-16-Blog: MySQL Performance Blog - Everything about MySQL Performance. </a></p>
<p>12 0.91042423 <a title="946-lda-12" href="../high_scalability-2012/high_scalability-2012-07-18-Strategy%3A_Kill_Off_Multi-tenant_Instances_with_High_CPU_Stolen_Time.html">1286 high scalability-2012-07-18-Strategy: Kill Off Multi-tenant Instances with High CPU Stolen Time</a></p>
<p>13 0.90813279 <a title="946-lda-13" href="../high_scalability-2009/high_scalability-2009-06-23-Learn_How_to_Exploit_Multiple_Cores_for_Better_Performance_and_Scalability.html">636 high scalability-2009-06-23-Learn How to Exploit Multiple Cores for Better Performance and Scalability</a></p>
<p>14 0.90749389 <a title="946-lda-14" href="../high_scalability-2008/high_scalability-2008-10-15-Need_help_with_your_Hadoop_deployment%3F_This_company_may_help%21.html">415 high scalability-2008-10-15-Need help with your Hadoop deployment? This company may help!</a></p>
<p>15 0.90641963 <a title="946-lda-15" href="../high_scalability-2014/high_scalability-2014-04-07-Google_Finds%3A_Centralized_Control%2C_Distributed_Data_Architectures_Work_Better_than_Fully_Decentralized_Architectures.html">1627 high scalability-2014-04-07-Google Finds: Centralized Control, Distributed Data Architectures Work Better than Fully Decentralized Architectures</a></p>
<p>16 0.90531665 <a title="946-lda-16" href="../high_scalability-2010/high_scalability-2010-08-18-Misco%3A_A_MapReduce_Framework_for_Mobile_Systems_-_Start_of_the_Ambient_Cloud%3F.html">882 high scalability-2010-08-18-Misco: A MapReduce Framework for Mobile Systems - Start of the Ambient Cloud?</a></p>
<p>17 0.90421343 <a title="946-lda-17" href="../high_scalability-2007/high_scalability-2007-10-04-Number_of_load_balanced_servers.html">111 high scalability-2007-10-04-Number of load balanced servers</a></p>
<p>18 0.90384549 <a title="946-lda-18" href="../high_scalability-2008/high_scalability-2008-06-09-Apple%27s_iPhone_to_Use_a_Centralized_Push_Based_Notification_Architecture.html">343 high scalability-2008-06-09-Apple's iPhone to Use a Centralized Push Based Notification Architecture</a></p>
<p>19 0.90317309 <a title="946-lda-19" href="../high_scalability-2009/high_scalability-2009-07-30-Learn_How_to_Think_at_Scale.html">666 high scalability-2009-07-30-Learn How to Think at Scale</a></p>
<p>20 0.90291083 <a title="946-lda-20" href="../high_scalability-2009/high_scalability-2009-03-16-Are_Cloud_Based_Memory_Architectures_the_Next_Big_Thing%3F.html">538 high scalability-2009-03-16-Are Cloud Based Memory Architectures the Next Big Thing?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
