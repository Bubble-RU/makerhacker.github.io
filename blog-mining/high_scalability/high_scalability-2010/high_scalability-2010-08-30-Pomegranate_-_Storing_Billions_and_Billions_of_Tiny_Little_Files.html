<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>889 high scalability-2010-08-30-Pomegranate - Storing Billions and Billions of Tiny Little Files</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2010" href="../home/high_scalability-2010_home.html">high_scalability-2010</a> <a title="high_scalability-2010-889" href="#">high_scalability-2010-889</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>889 high scalability-2010-08-30-Pomegranate - Storing Billions and Billions of Tiny Little Files</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2010-889-html" href="http://highscalability.com//blog/2010/8/30/pomegranate-storing-billions-and-billions-of-tiny-little-fil.html">html</a></p><p>Introduction: Pomegranateis a novel distributed file system built over distributed tabular
storage that acts an awful lot like a NoSQL system. It's targeted at
increasing the performance of tiny object access in order to support
applications like online photo and micro-blog services, which require high
concurrency, high throughput, and low latency. Their tests seem to indicate it
works:We have demonstrate that file system over tabular storage performs well
for highly concurrent access. In our test cluster, we observedlinearly
increased more than100,000aggregate read and write requests served per second
(RPS). Rather than sitting atop the file system like almost every other K-V
store, Pomegranate is baked into file system. The idea is that the file system
API is common to every platform so it wouldn't require a separate API to use.
Every application could use it out of the box.The features of Pomegranate
are:It handles billions of small files efficiently, even in one directory;It
provide separate and</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Pomegranateis a novel distributed file system built over distributed tabular storage that acts an awful lot like a NoSQL system. [sent-1, score-0.743]
</p><p>2 Their tests seem to indicate it works:We have demonstrate that file system over tabular storage performs well for highly concurrent access. [sent-3, score-0.657]
</p><p>3 Rather than sitting atop the file system like almost every other K-V store, Pomegranate is baked into file system. [sent-5, score-0.733]
</p><p>4 Basically, there is no distributed or parallel file system that can handle billions of small files efficiently. [sent-11, score-0.831]
</p><p>5 Thus, we want to built a file system to manage billions of small files, and provide high throughput of concurrent accesses. [sent-14, score-0.659]
</p><p>6 It is built on top of other distributed file systems, such as Lustre, and only manage the namespace and small files. [sent-16, score-0.616]
</p><p>7 See the figure bellow:Pomegranate has many Metadata Servers and Metadata Storage Servers to serve metadata requests and small file read/write requests. [sent-18, score-0.719]
</p><p>8 We use distributed extendible hash to locate server from the key, because extendible hash is more adaptive to scale up and down. [sent-22, score-1.115]
</p><p>9 In file systems, directory table and inode table are always separated to support two different types of lookup. [sent-23, score-1.24]
</p><p>10 Lookups by pathname are handled by directory table, while lookups by inode number are handled by inode table. [sent-24, score-0.859]
</p><p>11 Pomegranate use a table-like directory structure to merge directory table and inode table. [sent-31, score-0.782]
</p><p>12 For file system, the key is the hash value of dentry name. [sent-33, score-0.823]
</p><p>13 Yep, it is deadly slow for small file access in traditional file systems. [sent-51, score-0.812]
</p><p>14 We replace the traditional directory table (B+ tree or hash tree) to distributed extendible hash table. [sent-52, score-1.193]
</p><p>15 Thus, to access a small file, we just need to access one table row to find the file location. [sent-55, score-0.676]
</p><p>16 We keep each small file stored sequentially in native file system. [sent-56, score-0.812]
</p><p>17 We want to compare the small file performance with other file systems. [sent-69, score-0.812]
</p><p>18 Although, we believe most distributed file systems can not handle massive small file accesses efficiently. [sent-72, score-0.937]
</p><p>19 In memory, the table slices are hashed in to a local extendible hash table both for performance and space consumption. [sent-83, score-0.949]
</p><p>20 Shown by the bellow figure,Clients issue request by hash the file name and lookup in the bitmap. [sent-84, score-0.744]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pomegranate', 0.371), ('file', 0.335), ('hash', 0.262), ('inode', 0.247), ('extendible', 0.227), ('table', 0.199), ('metadata', 0.19), ('directory', 0.168), ('files', 0.161), ('small', 0.142), ('mds', 0.136), ('mdss', 0.136), ('ring', 0.127), ('dentry', 0.124), ('tabular', 0.121), ('lookups', 0.095), ('support', 0.092), ('mdsl', 0.091), ('consistent', 0.088), ('bellow', 0.082), ('layer', 0.078), ('mkdir', 0.077), ('symlinks', 0.077), ('update', 0.076), ('distributed', 0.075), ('storage', 0.074), ('posix', 0.071), ('however', 0.071), ('filesystems', 0.067), ('meanwhile', 0.067), ('lookup', 0.065), ('namespace', 0.064), ('concurrent', 0.064), ('system', 0.063), ('locate', 0.062), ('slices', 0.062), ('propagate', 0.06), ('billions', 0.055), ('log', 0.055), ('central', 0.054), ('requests', 0.052), ('handled', 0.051), ('value', 0.051), ('key', 0.051), ('accesses', 0.05), ('snapshot', 0.05), ('photo', 0.049), ('indexes', 0.049), ('routed', 0.048), ('transaction', 0.048)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999982 <a title="889-tfidf-1" href="../high_scalability-2010/high_scalability-2010-08-30-Pomegranate_-_Storing_Billions_and_Billions_of_Tiny_Little_Files.html">889 high scalability-2010-08-30-Pomegranate - Storing Billions and Billions of Tiny Little Files</a></p>
<p>Introduction: Pomegranateis a novel distributed file system built over distributed tabular
storage that acts an awful lot like a NoSQL system. It's targeted at
increasing the performance of tiny object access in order to support
applications like online photo and micro-blog services, which require high
concurrency, high throughput, and low latency. Their tests seem to indicate it
works:We have demonstrate that file system over tabular storage performs well
for highly concurrent access. In our test cluster, we observedlinearly
increased more than100,000aggregate read and write requests served per second
(RPS). Rather than sitting atop the file system like almost every other K-V
store, Pomegranate is baked into file system. The idea is that the file system
API is common to every platform so it wouldn't require a separate API to use.
Every application could use it out of the box.The features of Pomegranate
are:It handles billions of small files efficiently, even in one directory;It
provide separate and</p><p>2 0.19041209 <a title="889-tfidf-2" href="../high_scalability-2011/high_scalability-2011-01-10-Riak%27s_Bitcask_-_A_Log-Structured_Hash_Table_for_Fast_Key-Value_Data.html">971 high scalability-2011-01-10-Riak's Bitcask - A Log-Structured Hash Table for Fast Key-Value Data</a></p>
<p>Introduction: How would you implement a key-value storage system if you were starting from
scratch? The approach Basho settled on withBitcask, their new backend for
Riak, is an interesting combination of using RAM to store a hash map of file
pointers to values and a log-structured file system for efficient writes.  In
this excellent Changelog interview, some folks from Basho describe Bitcask in
more detail.The essential Bitcask:Keys are stored in memory for fast lookups.
All keys must fit in RAM.Writes are append-only, which means writes are
strictly sequential and do not require seeking. Writes are write-through.
Every time a value is updated the data file on disk is appended and the in-
memory key index is updated with the file pointer.Read queries are satisfied
with O(1) random disk seeks. Latency is very predictable if all keys fit in
memory because there's no random seeking around through a file.For reads, the
file system cache in the kernel is used instead of writing a complicated
caching sche</p><p>3 0.16185749 <a title="889-tfidf-3" href="../high_scalability-2008/high_scalability-2008-03-17-Paper%3A_Consistent_Hashing_and_Random_Trees%3A_Distributed_Caching_Protocols_for_Relieving_Hot_Spots_on_the_World_Wide_Web.html">280 high scalability-2008-03-17-Paper: Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web</a></p>
<p>Introduction: Consistent hashing is one of those ideas that really puts the science in
computer science and reminds us why all those really smart people spend years
slaving over algorithms. Consistent hashing is "a scheme that provides hash
table functionality in a way that the addition or removal of one slot does not
significantly change the mapping of keys to slots" and was originally a way of
distributing requests among a changing population of web servers. My first
reaction to the idea was "wow, that's really smart" and I sadly realized I
would never come up with something so elegant. I then immediately saw
applications for it everywhere. And consistent hashing is used everywhere:
distributed hash tables, overlay networks, P2P, IM, caching, and CDNs. Here's
the abstract from the original paper and after the abstract are some links to
a few very good articles with accessible explanations of consistent hashing
and its applications in the real world.breakAbstract:We describe a family of
caching pro</p><p>4 0.14556652 <a title="889-tfidf-4" href="../high_scalability-2008/high_scalability-2008-01-29-Building_scalable_storage_into_application_-_Instead_of_MogileFS_OpenAFS_etc..html">229 high scalability-2008-01-29-Building scalable storage into application - Instead of MogileFS OpenAFS etc.</a></p>
<p>Introduction: I am planning the scaling of a hosted service, similar to typepad etc. and
would appreciate feedback on my plan so far.Looking into scaling storage, I
have come accross MogileFS and OpenAFS. My concern with these is I am not at
all experienced with them and as the sole tech guy I don't want to build
something into this hosting service that proves complex to update and
adminster.So, I'm thinking of building replication and scalability right into
the application, in a similar but simplified way to how MogileFS works (I
think).So, for our database table of uploaded files, here's how it currently
looks (simplified):fileid (pkey)filenameowneridFor adding the replication and
scalability, I would add a few more
columns:serveroneidservertwoidserverthreeids3At the time the user uploads a
file, it will go to a specific server (managed by the application) and the id
of that server will be placed in the "serverone" column. Then hourly or so, a
cron job will run through the "files" table, and copy</p><p>5 0.14430401 <a title="889-tfidf-5" href="../high_scalability-2008/high_scalability-2008-11-22-Google_Architecture.html">448 high scalability-2008-11-22-Google Architecture</a></p>
<p>Introduction: Update 2:Sorting 1 PB with MapReduce. PB is not peanut-butter-and-jelly
misspelled. It's 1 petabyte or 1000 terabytes or 1,000,000 gigabytes.It took
six hours and two minutes to sort 1PB (10 trillion 100-byte records) on 4,000
computersand the results were replicated thrice on 48,000 disks.Update:Greg
Lindenpoints to a new Google articleMapReduce: simplified data processing on
large clusters. Some interesting stats: 100k MapReduce jobs are executed each
day; more than 20 petabytes of data are processed per day; more than 10k
MapReduce programs have been implemented; machines are dual processor with
gigabit ethernet and 4-8 GB of memory.Google is the King of scalability.
Everyone knows Google for their large, sophisticated, and fast searching, but
they don't just shine in search. Their platform approach to building scalable
applications allows them to roll out internet scale applications at an
alarmingly high competition crushing rate. Their goal is always to build a
higher performing h</p><p>6 0.13487616 <a title="889-tfidf-6" href="../high_scalability-2012/high_scalability-2012-10-04-Stuff_The_Internet_Says_On_Scalability_For_October_5%2C_2012.html">1334 high scalability-2012-10-04-Stuff The Internet Says On Scalability For October 5, 2012</a></p>
<p>7 0.13487472 <a title="889-tfidf-7" href="../high_scalability-2009/high_scalability-2009-02-05-Beta_testers_wanted_for_ultra_high-scalability-performance_clustered_object_storage_system_designed_for_web_content_delivery.html">508 high scalability-2009-02-05-Beta testers wanted for ultra high-scalability-performance clustered object storage system designed for web content delivery</a></p>
<p>8 0.13228859 <a title="889-tfidf-8" href="../high_scalability-2009/high_scalability-2009-05-05-Drop_ACID_and_Think_About_Data.html">589 high scalability-2009-05-05-Drop ACID and Think About Data</a></p>
<p>9 0.13170615 <a title="889-tfidf-9" href="../high_scalability-2008/high_scalability-2008-03-12-YouTube_Architecture.html">274 high scalability-2008-03-12-YouTube Architecture</a></p>
<p>10 0.12807062 <a title="889-tfidf-10" href="../high_scalability-2013/high_scalability-2013-10-08-F1_and_Spanner_Holistically_Compared.html">1529 high scalability-2013-10-08-F1 and Spanner Holistically Compared</a></p>
<p>11 0.11914574 <a title="889-tfidf-11" href="../high_scalability-2012/high_scalability-2012-03-07-Scale_Indefinitely_on_S3_With_These_Secrets_of_the_S3_Masters.html">1205 high scalability-2012-03-07-Scale Indefinitely on S3 With These Secrets of the S3 Masters</a></p>
<p>12 0.11816321 <a title="889-tfidf-12" href="../high_scalability-2010/high_scalability-2010-12-21-SQL_%2B_NoSQL_%3D_Yes_%21.html">961 high scalability-2010-12-21-SQL + NoSQL = Yes !</a></p>
<p>13 0.11795909 <a title="889-tfidf-13" href="../high_scalability-2009/high_scalability-2009-03-16-Are_Cloud_Based_Memory_Architectures_the_Next_Big_Thing%3F.html">538 high scalability-2009-03-16-Are Cloud Based Memory Architectures the Next Big Thing?</a></p>
<p>14 0.11698319 <a title="889-tfidf-14" href="../high_scalability-2009/high_scalability-2009-05-22-Distributed_content_system_with_bandwidth_balancing.html">605 high scalability-2009-05-22-Distributed content system with bandwidth balancing</a></p>
<p>15 0.11629168 <a title="889-tfidf-15" href="../high_scalability-2007/high_scalability-2007-11-13-Flickr_Architecture.html">152 high scalability-2007-11-13-Flickr Architecture</a></p>
<p>16 0.11523965 <a title="889-tfidf-16" href="../high_scalability-2012/high_scalability-2012-08-14-MemSQL_Architecture_-_The_Fast_%28MVCC%2C_InMem%2C_LockFree%2C_CodeGen%29_and_Familiar_%28SQL%29.html">1304 high scalability-2012-08-14-MemSQL Architecture - The Fast (MVCC, InMem, LockFree, CodeGen) and Familiar (SQL)</a></p>
<p>17 0.11520797 <a title="889-tfidf-17" href="../high_scalability-2007/high_scalability-2007-07-15-Lustre_cluster_file_system.html">13 high scalability-2007-07-15-Lustre cluster file system</a></p>
<p>18 0.11245542 <a title="889-tfidf-18" href="../high_scalability-2013/high_scalability-2013-04-03-5_Steps_to_Benchmarking_Managed_NoSQL_-_DynamoDB_vs_Cassandra.html">1434 high scalability-2013-04-03-5 Steps to Benchmarking Managed NoSQL - DynamoDB vs Cassandra</a></p>
<p>19 0.11191649 <a title="889-tfidf-19" href="../high_scalability-2010/high_scalability-2010-08-16-Scaling_an_AWS_infrastructure_-_Tools_and_Patterns.html">881 high scalability-2010-08-16-Scaling an AWS infrastructure - Tools and Patterns</a></p>
<p>20 0.11104856 <a title="889-tfidf-20" href="../high_scalability-2008/high_scalability-2008-10-14-Implementing_the_Lustre_File_System_with_Sun_Storage%3A_High_Performance_Storage_for_High_Performance_Computing.html">411 high scalability-2008-10-14-Implementing the Lustre File System with Sun Storage: High Performance Storage for High Performance Computing</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.19), (1, 0.112), (2, -0.043), (3, -0.056), (4, -0.005), (5, 0.092), (6, 0.099), (7, -0.047), (8, -0.037), (9, 0.047), (10, 0.04), (11, -0.032), (12, -0.024), (13, -0.049), (14, 0.055), (15, 0.08), (16, -0.051), (17, 0.034), (18, -0.027), (19, -0.045), (20, 0.046), (21, 0.01), (22, -0.012), (23, 0.107), (24, -0.027), (25, -0.071), (26, 0.068), (27, -0.008), (28, -0.034), (29, -0.042), (30, -0.055), (31, -0.051), (32, -0.007), (33, -0.031), (34, -0.041), (35, 0.005), (36, -0.021), (37, -0.034), (38, 0.031), (39, -0.06), (40, -0.06), (41, -0.046), (42, 0.008), (43, -0.021), (44, -0.086), (45, 0.03), (46, -0.034), (47, 0.04), (48, 0.049), (49, 0.064)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97426385 <a title="889-lsi-1" href="../high_scalability-2010/high_scalability-2010-08-30-Pomegranate_-_Storing_Billions_and_Billions_of_Tiny_Little_Files.html">889 high scalability-2010-08-30-Pomegranate - Storing Billions and Billions of Tiny Little Files</a></p>
<p>Introduction: Pomegranateis a novel distributed file system built over distributed tabular
storage that acts an awful lot like a NoSQL system. It's targeted at
increasing the performance of tiny object access in order to support
applications like online photo and micro-blog services, which require high
concurrency, high throughput, and low latency. Their tests seem to indicate it
works:We have demonstrate that file system over tabular storage performs well
for highly concurrent access. In our test cluster, we observedlinearly
increased more than100,000aggregate read and write requests served per second
(RPS). Rather than sitting atop the file system like almost every other K-V
store, Pomegranate is baked into file system. The idea is that the file system
API is common to every platform so it wouldn't require a separate API to use.
Every application could use it out of the box.The features of Pomegranate
are:It handles billions of small files efficiently, even in one directory;It
provide separate and</p><p>2 0.84835941 <a title="889-lsi-2" href="../high_scalability-2007/high_scalability-2007-09-28-Kosmos_File_System_%28KFS%29_is_a_New_High_End_Google_File_System_Option.html">103 high scalability-2007-09-28-Kosmos File System (KFS) is a New High End Google File System Option</a></p>
<p>Introduction: There's a new clustered file system on the spindle:Kosmos File System (KFS).
Thanks to Rich Skrenta for turning me on to KFS and I think his blogpostsays
it all. KFS is an open source project written in C++ by search startupKosmix.
The team members have a goodpedigreeso there's a better than average chance
this software will be worth considering.After you stop trying to turn KFS into
"Kentucky Fried File System" in your mind, take a look at KFS' intriguing
feature set:breakIncremental scalability: New chunkserver nodes can be added
as storage needs increase; the system automatically adapts to the new
nodes.Availability: Replication is used to provide availability due to chunk
server failures. Typically, files are replicated 3-way.Per file degree of
replication: The degree of replication is configurable on a per file basis,
with a max. limit of 64.Re-replication: Whenever the degree of replication for
a file drops below the configured amount (such as, due to an extended
chunkserver outa</p><p>3 0.81731099 <a title="889-lsi-3" href="../high_scalability-2011/high_scalability-2011-01-10-Riak%27s_Bitcask_-_A_Log-Structured_Hash_Table_for_Fast_Key-Value_Data.html">971 high scalability-2011-01-10-Riak's Bitcask - A Log-Structured Hash Table for Fast Key-Value Data</a></p>
<p>Introduction: How would you implement a key-value storage system if you were starting from
scratch? The approach Basho settled on withBitcask, their new backend for
Riak, is an interesting combination of using RAM to store a hash map of file
pointers to values and a log-structured file system for efficient writes.  In
this excellent Changelog interview, some folks from Basho describe Bitcask in
more detail.The essential Bitcask:Keys are stored in memory for fast lookups.
All keys must fit in RAM.Writes are append-only, which means writes are
strictly sequential and do not require seeking. Writes are write-through.
Every time a value is updated the data file on disk is appended and the in-
memory key index is updated with the file pointer.Read queries are satisfied
with O(1) random disk seeks. Latency is very predictable if all keys fit in
memory because there's no random seeking around through a file.For reads, the
file system cache in the kernel is used instead of writing a complicated
caching sche</p><p>4 0.81522799 <a title="889-lsi-4" href="../high_scalability-2007/high_scalability-2007-07-31-BerkeleyDB_%26_other_distributed_high_performance_key-value_databases.html">50 high scalability-2007-07-31-BerkeleyDB & other distributed high performance key-value databases</a></p>
<p>Introduction: I currently use BerkeleyDB as an embedded
databasehttp://www.oracle.com/database/berkeley-db/a decision which was
initially brought on by learning that Google used BerkeleyDB for their
universal sign-on feature.Lustre looks impressive, but their white paper shows
speeds of 800 files created per second, as a good number. However, BerkeleyDB
on my mac mini does 200,000 row creations per second, and can be used as a
distributed file system.I'm having I/O scalability issues with BerkeleyDB on
one machine, and about to implement their distributed replication feature (and
go multi-machine), which in effect makes it work like a distributed file
system, but with local access speeds. That's why I was looking at Lustre.The
key feature difference between BerkeleyDB and Lustre is that BerkeleyDB has a
complete copy of all the data on each computer, making it not a viable
solution for massive sized database applications. However, if you have < 1TB
(ie, one disk) of total possible data, it seems to</p><p>5 0.78558683 <a title="889-lsi-5" href="../high_scalability-2008/high_scalability-2008-03-16-Product%3A_GlusterFS.html">278 high scalability-2008-03-16-Product: GlusterFS</a></p>
<p>Introduction: Adapted from their website:GlusterFSis a clustered file-system capable of
scaling to several peta-bytes. It aggregates various storage bricks over
Infiniband RDMA or TCP/IP interconnect into one large parallel network file
system. Storage bricks can be made of any commodity hardware such as x86-64
server with SATA-II RAID and Infiniband HBA).Cluster file systems are still
not mature for enterprise market. They are too complex to deploy and maintain
though they are extremely scalable and cheap. Can be entirely built out of
commodity OS and hardware. GlusterFS hopes to solves this problem.GlusterFS
achieved35 GBps read throughput. The GlusterFS Aggregated I/O Benchmark was
performed on 64 bricks clustered storage system over 10 Gbps Infiniband
interconnect. A cluster of 220 clients pounded the storage system with
multiple dd (disk-dump) instances, each reading / writing a 1 GB file with 1MB
block size. GlusterFS was configured with unify translator and round-robin
scheduler.The advantage</p><p>6 0.77684671 <a title="889-lsi-6" href="../high_scalability-2013/high_scalability-2013-04-17-Tachyon__-_Fault_Tolerant_Distributed_File_System_with_300_Times_Higher_Throughput_than_HDFS.html">1442 high scalability-2013-04-17-Tachyon  - Fault Tolerant Distributed File System with 300 Times Higher Throughput than HDFS</a></p>
<p>7 0.77588832 <a title="889-lsi-7" href="../high_scalability-2011/high_scalability-2011-08-10-LevelDB_-_Fast_and_Lightweight_Key-Value_Database_From_the_Authors_of_MapReduce_and_BigTable.html">1096 high scalability-2011-08-10-LevelDB - Fast and Lightweight Key-Value Database From the Authors of MapReduce and BigTable</a></p>
<p>8 0.74118841 <a title="889-lsi-8" href="../high_scalability-2007/high_scalability-2007-10-18-another_approach_to_replication.html">125 high scalability-2007-10-18-another approach to replication</a></p>
<p>9 0.72594994 <a title="889-lsi-9" href="../high_scalability-2009/high_scalability-2009-02-05-Beta_testers_wanted_for_ultra_high-scalability-performance_clustered_object_storage_system_designed_for_web_content_delivery.html">508 high scalability-2009-02-05-Beta testers wanted for ultra high-scalability-performance clustered object storage system designed for web content delivery</a></p>
<p>10 0.72462898 <a title="889-lsi-10" href="../high_scalability-2007/high_scalability-2007-08-01-Product%3A_MogileFS.html">53 high scalability-2007-08-01-Product: MogileFS</a></p>
<p>11 0.72119021 <a title="889-lsi-11" href="../high_scalability-2009/high_scalability-2009-09-16-Paper%3A_A_practical_scalable_distributed_B-tree.html">705 high scalability-2009-09-16-Paper: A practical scalable distributed B-tree</a></p>
<p>12 0.70488364 <a title="889-lsi-12" href="../high_scalability-2008/high_scalability-2008-03-17-Paper%3A_Consistent_Hashing_and_Random_Trees%3A_Distributed_Caching_Protocols_for_Relieving_Hot_Spots_on_the_World_Wide_Web.html">280 high scalability-2008-03-17-Paper: Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web</a></p>
<p>13 0.69381559 <a title="889-lsi-13" href="../high_scalability-2008/high_scalability-2008-08-17-Wuala_-_P2P_Online_Storage_Cloud.html">368 high scalability-2008-08-17-Wuala - P2P Online Storage Cloud</a></p>
<p>14 0.6926946 <a title="889-lsi-14" href="../high_scalability-2008/high_scalability-2008-03-18-Shared_filesystem_on_EC2.html">283 high scalability-2008-03-18-Shared filesystem on EC2</a></p>
<p>15 0.6890493 <a title="889-lsi-15" href="../high_scalability-2014/high_scalability-2014-05-19-A_Short_On_How_the_Wayback_Machine_Stores_More_Pages_than_Stars_in_the_Milky_Way.html">1650 high scalability-2014-05-19-A Short On How the Wayback Machine Stores More Pages than Stars in the Milky Way</a></p>
<p>16 0.68492562 <a title="889-lsi-16" href="../high_scalability-2007/high_scalability-2007-10-04-You_Can_Now_Store_All_Your_Stuff_on_Your_Own_Google_Like_File_System.html">112 high scalability-2007-10-04-You Can Now Store All Your Stuff on Your Own Google Like File System</a></p>
<p>17 0.68223572 <a title="889-lsi-17" href="../high_scalability-2007/high_scalability-2007-07-16-Paper%3A_Replication_Under_Scalable_Hashing.html">19 high scalability-2007-07-16-Paper: Replication Under Scalable Hashing</a></p>
<p>18 0.68122399 <a title="889-lsi-18" href="../high_scalability-2008/high_scalability-2008-11-24-Product%3A_Scribe_-_Facebook%27s_Scalable_Logging_System.html">449 high scalability-2008-11-24-Product: Scribe - Facebook's Scalable Logging System</a></p>
<p>19 0.67874438 <a title="889-lsi-19" href="../high_scalability-2009/high_scalability-2009-05-05-Drop_ACID_and_Think_About_Data.html">589 high scalability-2009-05-05-Drop ACID and Think About Data</a></p>
<p>20 0.67762202 <a title="889-lsi-20" href="../high_scalability-2007/high_scalability-2007-09-18-Sync_data_on_all_servers.html">98 high scalability-2007-09-18-Sync data on all servers</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.141), (2, 0.215), (10, 0.034), (17, 0.018), (40, 0.013), (47, 0.017), (61, 0.069), (76, 0.199), (79, 0.132), (94, 0.055)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.94006288 <a title="889-lda-1" href="../high_scalability-2010/high_scalability-2010-12-31-Facebook_in_20_Minutes%3A_2.7M_Photos%2C_10.2M_Comments%2C_4.6M_Messages.html">966 high scalability-2010-12-31-Facebook in 20 Minutes: 2.7M Photos, 10.2M Comments, 4.6M Messages</a></p>
<p>Introduction: To celebrate the new year Facebook hasshared the resultsof a little end of the
year introspection. It has been a fecund year for Facebook:43,869,800 changed
their status to single3,025,791 changed their status to "it's
complicated"28,460,516 changed their status to in a relationship5,974,574
changed their status to engaged36,774,801 changes their status to marriedIf
these numbers are simply to large to grasp, it doesn't get any better when you
look at happens in a mere 20 minutes:Shared links: 1,000,000 Tagged photos:
1,323,000Event invites sent out: 1,484,000Wall Posts: 1,587,000 Status
updates: 1,851,000Friend requests accepted: 1,972,000Photos uploaded:
2,716,000Comments: 10,208,000Message: 4,632,000If you want to see how Facebook
supports these huge numbers take a look at a few posts.One wonders what the
new year will bring?Related ArticlesWhat the World Eatsfrom Time Magazine A
Day in the Life of an Ancient RomanA Day in the Life of Donald DuckThe
Beatles- A Day in the LifeA Day i</p><p>2 0.91637635 <a title="889-lda-2" href="../high_scalability-2007/high_scalability-2007-12-02-nginx%3A_high_performance_smpt-pop-imap_proxy.html">172 high scalability-2007-12-02-nginx: high performance smpt-pop-imap proxy</a></p>
<p>Introduction: nginx is a high performance smtp/pop/imap proxy that lets you do custom
authorization and lookups and is very scalable. (just add nodes)Nginx by
default is a reverse proxy and this is what it is doing here for pop/imap
connections. It is also an excellelent reverse proxy for web
servers.Advantage: You dont have to have a speacial database or ldap schema.
Just an url to do auth and lookup with.A url that may be accessed by a unix or
a tcp socket. Write your own auth handler - according to your own policy.For
example:A user called atif tries to login with the pass testxyz.You pass this
infomation to a URL such assocket:/var/tmp/xyz.sockorhttp://auth.corp.mailserv
er.net:someport/someurlThe auth server replies with either a FAILURE such
asAuth-Status: Invalid Login or passwordor with a success such asAuth-Status:
OKAuth-Server: OneOfThe100ServersAuth-Port: optionalyAPortWe have implemented
it at our ISP and it has saves us a lot of headaches.This would work for both
imap and pop.I have no</p><p>3 0.91104108 <a title="889-lda-3" href="../high_scalability-2011/high_scalability-2011-12-22-Architecting_Massively-Scalable_Near-Real-Time_Risk_Analysis_Solutions.html">1161 high scalability-2011-12-22-Architecting Massively-Scalable Near-Real-Time Risk Analysis Solutions</a></p>
<p>Introduction: Constructing a scalablerisk analysis solution is a fascinating architectural
challenge. If you come from Financial Services you are sure to appreciate
that. But even architects from other domains are bound to find the challenges
fascinating, and the architectural patterns of my suggested solution highly
useful in other domains.Recently I held an interesting webinar around
architecting solutions for scalable and near-real-time risk analysis solutions
based on experience gathered with Financial Services customers. Seeing the
vast interest in the webinar, I would like to share the highlights with you
here.From an architectural point of view, risk analysis is a data-intensive
and a compute-intensive process, which also has an elaborate orchestration
logic. volumes in this domain are massive and ever-increasing, together with
an ever-increasing demand to reduce response time. These trends are aggravated
by global financial regulatory reforms set following the late-2000s financial
crisis, wh</p><p>same-blog 4 0.89667612 <a title="889-lda-4" href="../high_scalability-2010/high_scalability-2010-08-30-Pomegranate_-_Storing_Billions_and_Billions_of_Tiny_Little_Files.html">889 high scalability-2010-08-30-Pomegranate - Storing Billions and Billions of Tiny Little Files</a></p>
<p>Introduction: Pomegranateis a novel distributed file system built over distributed tabular
storage that acts an awful lot like a NoSQL system. It's targeted at
increasing the performance of tiny object access in order to support
applications like online photo and micro-blog services, which require high
concurrency, high throughput, and low latency. Their tests seem to indicate it
works:We have demonstrate that file system over tabular storage performs well
for highly concurrent access. In our test cluster, we observedlinearly
increased more than100,000aggregate read and write requests served per second
(RPS). Rather than sitting atop the file system like almost every other K-V
store, Pomegranate is baked into file system. The idea is that the file system
API is common to every platform so it wouldn't require a separate API to use.
Every application could use it out of the box.The features of Pomegranate
are:It handles billions of small files efficiently, even in one directory;It
provide separate and</p><p>5 0.88513899 <a title="889-lda-5" href="../high_scalability-2011/high_scalability-2011-09-23-Stuff_The_Internet_Says_On_Scalability_For_September_23%2C_2011.html">1122 high scalability-2011-09-23-Stuff The Internet Says On Scalability For September 23, 2011</a></p>
<p>Introduction: I'd walk a mile for HighScalability:1/12th the World Population on Facebook in
One Day; 1.8 ZettaBytesof data in 2011;1 Billion Foursquare Checkins;  2
million on Spotify;1 Million on GitHub;$1,279-per-hour, 30,000-core cluster
built on EC2;Patent trolls cost .5 trillion dollars;235 terabytes of data
collected by the U.S. Library of Congress in April.Potent
quotables:@jstogdill: Corporations over protect low value info assets (which
screws up collaboration) and under protects high value assets.
#strataconf@sbtourist: I think BigMemory-like approaches based on large put-
and-forget memory cans, are rarely a solution to performance/scalability
problems.1 Million TCP Connections. Remember when 10K was a real limit and you
had to build out boxes just to handle the load? Amazing. We don't know how
much processing can be attached to these connections, how much memory the apps
use, or what the response latency is to requests over these connections, but
still, that's cool. Good discussion on H</p><p>6 0.88507223 <a title="889-lda-6" href="../high_scalability-2009/high_scalability-2009-01-04-Paper%3A_MapReduce%3A_Simplified_Data_Processing_on_Large_Clusters.html">483 high scalability-2009-01-04-Paper: MapReduce: Simplified Data Processing on Large Clusters</a></p>
<p>7 0.8812148 <a title="889-lda-7" href="../high_scalability-2009/high_scalability-2009-07-29-Strategy%3A_Let_Google_and_Yahoo_Host_Your_Ajax_Library_-_For_Free.html">665 high scalability-2009-07-29-Strategy: Let Google and Yahoo Host Your Ajax Library - For Free</a></p>
<p>8 0.87224507 <a title="889-lda-8" href="../high_scalability-2007/high_scalability-2007-08-16-Scaling_Secret_%232%3A_Denormalizing_Your_Way_to_Speed_and_Profit.html">65 high scalability-2007-08-16-Scaling Secret #2: Denormalizing Your Way to Speed and Profit</a></p>
<p>9 0.85969239 <a title="889-lda-9" href="../high_scalability-2012/high_scalability-2012-01-23-Facebook_Timeline%3A_Brought_to_You_by_the_Power_of_Denormalization.html">1179 high scalability-2012-01-23-Facebook Timeline: Brought to You by the Power of Denormalization</a></p>
<p>10 0.8533268 <a title="889-lda-10" href="../high_scalability-2013/high_scalability-2013-12-13-Stuff_The_Internet_Says_On_Scalability_For_December_13th%2C_2013.html">1564 high scalability-2013-12-13-Stuff The Internet Says On Scalability For December 13th, 2013</a></p>
<p>11 0.85081953 <a title="889-lda-11" href="../high_scalability-2010/high_scalability-2010-04-19-The_cost_of_High_Availability_%28HA%29_with_Oracle_.html">813 high scalability-2010-04-19-The cost of High Availability (HA) with Oracle </a></p>
<p>12 0.85079718 <a title="889-lda-12" href="../high_scalability-2007/high_scalability-2007-12-07-Synchronizing_databases_in_different_geographic_locations.html">176 high scalability-2007-12-07-Synchronizing databases in different geographic locations</a></p>
<p>13 0.83343536 <a title="889-lda-13" href="../high_scalability-2013/high_scalability-2013-11-22-Stuff_The_Internet_Says_On_Scalability_For_November_22th%2C_2013.html">1552 high scalability-2013-11-22-Stuff The Internet Says On Scalability For November 22th, 2013</a></p>
<p>14 0.83326143 <a title="889-lda-14" href="../high_scalability-2012/high_scalability-2012-06-15-Stuff_The_Internet_Says_On_Scalability_For_June_15%2C_2012.html">1265 high scalability-2012-06-15-Stuff The Internet Says On Scalability For June 15, 2012</a></p>
<p>15 0.83189505 <a title="889-lda-15" href="../high_scalability-2009/high_scalability-2009-02-18-Numbers_Everyone_Should_Know.html">514 high scalability-2009-02-18-Numbers Everyone Should Know</a></p>
<p>16 0.83076781 <a title="889-lda-16" href="../high_scalability-2009/high_scalability-2009-02-21-Google_AppEngine_-_A_Second_Look.html">517 high scalability-2009-02-21-Google AppEngine - A Second Look</a></p>
<p>17 0.8299343 <a title="889-lda-17" href="../high_scalability-2008/high_scalability-2008-04-08-Google_AppEngine_-_A_First_Look.html">301 high scalability-2008-04-08-Google AppEngine - A First Look</a></p>
<p>18 0.82924485 <a title="889-lda-18" href="../high_scalability-2008/high_scalability-2008-05-27-How_I_Learned_to_Stop_Worrying_and_Love_Using_a_Lot_of_Disk_Space_to_Scale.html">327 high scalability-2008-05-27-How I Learned to Stop Worrying and Love Using a Lot of Disk Space to Scale</a></p>
<p>19 0.82770979 <a title="889-lda-19" href="../high_scalability-2010/high_scalability-2010-06-07-Six_Ways_Twitter_May_Reach_its_Big_Hairy_Audacious_Goal_of_One_Billion_Users.html">837 high scalability-2010-06-07-Six Ways Twitter May Reach its Big Hairy Audacious Goal of One Billion Users</a></p>
<p>20 0.82703269 <a title="889-lda-20" href="../high_scalability-2014/high_scalability-2014-05-16-Stuff_The_Internet_Says_On_Scalability_For_May_16th%2C_2014.html">1649 high scalability-2014-05-16-Stuff The Internet Says On Scalability For May 16th, 2014</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
