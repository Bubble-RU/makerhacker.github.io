<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>937 high scalability-2010-11-09-Paper: Hyder - Scaling Out without Partitioning </title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2010" href="../home/high_scalability-2010_home.html">high_scalability-2010</a> <a title="high_scalability-2010-937" href="#">high_scalability-2010-937</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>937 high scalability-2010-11-09-Paper: Hyder - Scaling Out without Partitioning </h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2010-937-html" href="http://highscalability.com//blog/2010/11/9/paper-hyder-scaling-out-without-partitioning.html">html</a></p><p>Introduction: Partitioning is what differentiates scaling-out from scaling-up, isn't it? I thought so too until I read  Pat Helland's blog post on Hyder , a research database at Microsoft, in which  the database is the log, no partitioning is required, and the database is multi-versioned . Not much is available on Hyder. There's the excellent summary post from Mr. Helland and these documents:  Scaling Out without Partitioning  and  Scaling Out without Partitioning  - Hyder Update  by Phil Bernstein and Colin Reid of Microsoft.
 
 
 
The idea behind Hyder as summarized by Pat Helland (see his blog for the full post):
  Hyder is a software stack for transactional record management. It can offer full database functionality and is designed to take advantage of flash in a novel way. Most approaches to scale-out use partitioning and spread the data across multiple machines leaving the application responsible for consistency.

 


In Hyder, the database is the log, no partitioning is required, and the data</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 I thought so too until I read  Pat Helland's blog post on Hyder , a research database at Microsoft, in which  the database is the log, no partitioning is required, and the database is multi-versioned . [sent-2, score-0.337]
</p><p>2 It can offer full database functionality and is designed to take advantage of flash in a novel way. [sent-7, score-0.173]
</p><p>3 In Hyder, the database is the log, no partitioning is required, and the database is multi-versioned. [sent-9, score-0.272]
</p><p>4 Raw flash (not SSDs – raw flash) offers at least 10^4 more IOPS/GB than HDD. [sent-14, score-0.166]
</p><p>5 Also, with many-core servers, computation can be squandered and Hyder leverages that abundant computation to keep a consistent view of the data as it changes. [sent-18, score-0.224]
</p><p>6 Appending a record to the log involves a send to the log controller and a response with the location in the log into which the record was appended. [sent-20, score-1.291]
</p><p>7 In this fashion, many servers can be pushing records into the log and they are allocated a location by the log controller. [sent-21, score-0.794]
</p><p>8 It turns out that this simple centralized function of assigning a log location on append will adjudicate any conflicts (as we shall see later). [sent-22, score-0.622]
</p><p>9 The Hyder stack comprises a persistent programming language like LING or SQL, an optimistic transaction protocol, and a multi-versioned binary search tree to represent the database state. [sent-23, score-0.539]
</p><p>10 The Hyder database is stored in a log but it IS a binary tree. [sent-24, score-0.559]
</p><p>11 So you can think of the database as a binary tree that is kept in the log and you find data by climbing the tree through the log. [sent-25, score-0.955]
</p><p>12 For transaction execution, each server has a cache of the last committed state. [sent-29, score-0.162]
</p><p>13 That cache is going to be close to the latest and greatest state since each server is constantly replaying the log to keep the local state accurate [recall the assumption that there are lots of cores per server and it’s OK to spend cycles from the extra cores]. [sent-30, score-0.528]
</p><p>14 So, each transaction running in a single server reads a snapshot and generates an intention log record. [sent-31, score-0.749]
</p><p>15 The transaction gets a pointer to the snapshot and generates an intention log record. [sent-32, score-0.743]
</p><p>16 The server generates updates locally appending them to the log (recall that an append is sent to the log controller which returns the log-id with its placement in the log). [sent-33, score-1.182]
</p><p>17 Updates are copy-on-write climbing up the binary tree to the root. [sent-34, score-0.412]
</p><p>18 Changes to the log are only done by appending to the log. [sent-36, score-0.492]
</p><p>19 The system-wide throughput of update transactions is bounded by the update pipeline. [sent-40, score-0.206]
</p><p>20 It is estimated this can perform 15K update transactions per second over a 1GB Ethernet and 150K update transactions per second over a 10GB Ethernet. [sent-41, score-0.338]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hyder', 0.709), ('log', 0.333), ('helland', 0.167), ('binary', 0.161), ('appending', 0.159), ('tree', 0.145), ('partitioning', 0.142), ('transaction', 0.115), ('flash', 0.108), ('climbing', 0.106), ('generates', 0.096), ('recall', 0.087), ('intention', 0.087), ('append', 0.084), ('update', 0.078), ('location', 0.076), ('record', 0.075), ('broadcast', 0.074), ('snapshot', 0.071), ('leverages', 0.071), ('fashion', 0.07), ('cheap', 0.067), ('controller', 0.066), ('database', 0.065), ('updates', 0.064), ('andscaling', 0.059), ('raw', 0.058), ('reid', 0.056), ('replaying', 0.056), ('reconstruct', 0.053), ('abundant', 0.053), ('comprises', 0.053), ('records', 0.052), ('differentiates', 0.051), ('bernstein', 0.051), ('computation', 0.05), ('transactions', 0.05), ('colin', 0.049), ('nodes', 0.049), ('pat', 0.048), ('addressable', 0.048), ('server', 0.047), ('phil', 0.045), ('shall', 0.045), ('cores', 0.045), ('assigning', 0.043), ('commits', 0.043), ('second', 0.041), ('pointer', 0.041), ('conflicts', 0.041)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="937-tfidf-1" href="../high_scalability-2010/high_scalability-2010-11-09-Paper%3A_Hyder_-_Scaling_Out_without_Partitioning_.html">937 high scalability-2010-11-09-Paper: Hyder - Scaling Out without Partitioning </a></p>
<p>Introduction: Partitioning is what differentiates scaling-out from scaling-up, isn't it? I thought so too until I read  Pat Helland's blog post on Hyder , a research database at Microsoft, in which  the database is the log, no partitioning is required, and the database is multi-versioned . Not much is available on Hyder. There's the excellent summary post from Mr. Helland and these documents:  Scaling Out without Partitioning  and  Scaling Out without Partitioning  - Hyder Update  by Phil Bernstein and Colin Reid of Microsoft.
 
 
 
The idea behind Hyder as summarized by Pat Helland (see his blog for the full post):
  Hyder is a software stack for transactional record management. It can offer full database functionality and is designed to take advantage of flash in a novel way. Most approaches to scale-out use partitioning and spread the data across multiple machines leaving the application responsible for consistency.

 


In Hyder, the database is the log, no partitioning is required, and the data</p><p>2 0.20449072 <a title="937-tfidf-2" href="../high_scalability-2007/high_scalability-2007-07-26-Product%3A_AWStats_a_Log_Analyzer.html">30 high scalability-2007-07-26-Product: AWStats a Log Analyzer</a></p>
<p>Introduction: AWStats  is a free powerful and featureful tool that generates advanced web, streaming, ftp or mail server statistics, graphically. This log analyzer works as a CGI or from command line and shows you all possible information your log contains, in few graphical web pages. It uses a partial information file to be able to process large log files, often and quickly. It can analyze log files from all major server tools like Apache log files (NCSA combined/XLF/ELF log format or common/CLF log format), WebStar, IIS (W3C log format) and a lot of other web, proxy, wap, streaming servers, mail servers and some ftp servers.</p><p>3 0.18290491 <a title="937-tfidf-3" href="../high_scalability-2013/high_scalability-2013-01-21-Processing_100_Million_Pixels_a_Day_-_Small_Amounts_of_Contention_Cause_Big_Problems_at_Scale.html">1390 high scalability-2013-01-21-Processing 100 Million Pixels a Day - Small Amounts of Contention Cause Big Problems at Scale</a></p>
<p>Introduction: This is a guest post by  Gordon Worley , a Software Engineer at  Korrelate , where they correlate (see what they did there) online purchases to offline purchases. 
 
Several weeks ago, we came into the office one morning to find every server alarm going off. Pixel log processing was behind by 8 hours and not making headway. Checking the logs, we discovered that a big client had come online during the night and was giving us 10 times more traffic than we were originally told to expect. I wouldn’t say we panicked, but the office was certainly more jittery than usual. Over the next several hours, though, thanks both to foresight and quick thinking, we were able to scale up to handle the added load and clear the backlog to return log processing to a steady state.
 
At Korrelate, we deploy  tracking pixels , also known beacons or web bugs, that our partners use to send us information about their users. These tiny web objects contain no visible content, but may include transparent 1 by 1 gif</p><p>4 0.16151482 <a title="937-tfidf-4" href="../high_scalability-2008/high_scalability-2008-01-30-How_Rackspace_Now_Uses_MapReduce_and_Hadoop_to_Query_Terabytes_of_Data.html">233 high scalability-2008-01-30-How Rackspace Now Uses MapReduce and Hadoop to Query Terabytes of Data</a></p>
<p>Introduction: How do you query hundreds of gigabytes of new data each day streaming in from over 600 hyperactive servers? If you think this sounds like the perfect battle ground for a head-to-head skirmish in the great  MapReduce Versus Database War , you would be correct.   Bill Boebel, CTO of Mailtrust (Rackspace's mail division), has generously provided a fascinating account of how they evolved their log processing system from an early amoeba'ic text file stored on each machine approach,  to a Neandertholic relational database solution that just couldn't compete, and finally to a Homo sapien'ic Hadoop based solution that works wisely for them and has virtually unlimited scalability potential.
 
Rackspace faced a now familiar problem. Lots and lots of data streaming in. Where do you store all that data? How do you do anything useful with it? In the first version of their system logs were stored in flat text files and had to be manually searched by engineers logging into each individual machine.  T</p><p>5 0.15284224 <a title="937-tfidf-5" href="../high_scalability-2011/high_scalability-2011-12-16-Stuff_The_Internet_Says_On_Scalability_For_December_16%2C_2011.html">1158 high scalability-2011-12-16-Stuff The Internet Says On Scalability For December 16, 2011</a></p>
<p>Introduction: A HighScalability is forever:
  
 eBay:  tens of millions  of lines of code; Google code base change rate per month:  50% ; Apple:  100 million downloads ; Internet:  186 Gbps  
 Quotable quotes:                  
 
  @OttmarAmann  : Scalability is not as important as managing complexity  
  @amankapur91  : Does scalability imply standardization, and then does standardization imply loss of innovation? 
 
 
 Spotify uses a P2P architecture and this paper,  Spotify – Large Scale, Low Latency, P2P Music-on-Demand Streaming , describes it. 
  The Faving spam counter-measures . Ironically, deviantART relates a gripping story of how they detected and stopped a deviant user from attacking their servers with an automated faving script which faved every 10 seconds for 24 hours a day. The same spam filter they use on the rest of the site was used. Problem solved. Would like detail on their spam filter though. 
 Interesting Google Group's thread on the  best practices for simulating transactions</p><p>6 0.15191118 <a title="937-tfidf-6" href="../high_scalability-2007/high_scalability-2007-08-30-Log_Everything_All_the_Time.html">77 high scalability-2007-08-30-Log Everything All the Time</a></p>
<p>7 0.12919992 <a title="937-tfidf-7" href="../high_scalability-2007/high_scalability-2007-07-28-Product%3A_Web_Log_Storming.html">37 high scalability-2007-07-28-Product: Web Log Storming</a></p>
<p>8 0.11912442 <a title="937-tfidf-8" href="../high_scalability-2009/high_scalability-2009-03-16-Product%3A_Smart_Inspect.html">541 high scalability-2009-03-16-Product: Smart Inspect</a></p>
<p>9 0.09582784 <a title="937-tfidf-9" href="../high_scalability-2012/high_scalability-2012-04-30-Masstree_-_Much_Faster_than_MongoDB%2C_VoltDB%2C_Redis%2C_and_Competitive_with_Memcached.html">1236 high scalability-2012-04-30-Masstree - Much Faster than MongoDB, VoltDB, Redis, and Competitive with Memcached</a></p>
<p>10 0.092831068 <a title="937-tfidf-10" href="../high_scalability-2007/high_scalability-2007-07-28-Product%3A_FastStats_Log_Analyzer_.html">35 high scalability-2007-07-28-Product: FastStats Log Analyzer </a></p>
<p>11 0.089433089 <a title="937-tfidf-11" href="../high_scalability-2009/high_scalability-2009-04-03-Collectl_interface_to_Ganglia_-_any_interest%3F.html">553 high scalability-2009-04-03-Collectl interface to Ganglia - any interest?</a></p>
<p>12 0.086515091 <a title="937-tfidf-12" href="../high_scalability-2011/high_scalability-2011-03-22-Facebook%27s_New_Realtime_Analytics_System%3A_HBase_to_Process_20_Billion_Events_Per_Day.html">1008 high scalability-2011-03-22-Facebook's New Realtime Analytics System: HBase to Process 20 Billion Events Per Day</a></p>
<p>13 0.084412001 <a title="937-tfidf-13" href="../high_scalability-2007/high_scalability-2007-10-01-Statistics_Logging_Scalability.html">105 high scalability-2007-10-01-Statistics Logging Scalability</a></p>
<p>14 0.083464362 <a title="937-tfidf-14" href="../high_scalability-2009/high_scalability-2009-04-15-Implementing_large_scale_web_analytics.html">570 high scalability-2009-04-15-Implementing large scale web analytics</a></p>
<p>15 0.083012603 <a title="937-tfidf-15" href="../high_scalability-2011/high_scalability-2011-06-21-Running_TPC-C_on_MySQL-RDS.html">1065 high scalability-2011-06-21-Running TPC-C on MySQL-RDS</a></p>
<p>16 0.082205229 <a title="937-tfidf-16" href="../high_scalability-2008/high_scalability-2008-11-24-Product%3A_Scribe_-_Facebook%27s_Scalable_Logging_System.html">449 high scalability-2008-11-24-Product: Scribe - Facebook's Scalable Logging System</a></p>
<p>17 0.080059677 <a title="937-tfidf-17" href="../high_scalability-2012/high_scalability-2012-03-12-Google%3A_Taming_the_Long_Latency_Tail_-_When_More_Machines_Equals_Worse_Results.html">1207 high scalability-2012-03-12-Google: Taming the Long Latency Tail - When More Machines Equals Worse Results</a></p>
<p>18 0.076762296 <a title="937-tfidf-18" href="../high_scalability-2012/high_scalability-2012-12-10-Switch_your_databases_to_Flash_storage._Now._Or_you%27re_doing_it_wrong..html">1369 high scalability-2012-12-10-Switch your databases to Flash storage. Now. Or you're doing it wrong.</a></p>
<p>19 0.075129896 <a title="937-tfidf-19" href="../high_scalability-2012/high_scalability-2012-10-22-Spanner_-_It%27s_About_Programmers_Building_Apps_Using_SQL_Semantics_at_NoSQL_Scale.html">1345 high scalability-2012-10-22-Spanner - It's About Programmers Building Apps Using SQL Semantics at NoSQL Scale</a></p>
<p>20 0.074315444 <a title="937-tfidf-20" href="../high_scalability-2012/high_scalability-2012-07-04-Top_Features_of_a_Scalable_Database.html">1276 high scalability-2012-07-04-Top Features of a Scalable Database</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.108), (1, 0.065), (2, -0.028), (3, -0.033), (4, 0.006), (5, 0.077), (6, 0.067), (7, 0.001), (8, 0.003), (9, 0.026), (10, 0.017), (11, -0.013), (12, -0.021), (13, -0.022), (14, 0.032), (15, 0.04), (16, -0.008), (17, 0.006), (18, -0.004), (19, 0.009), (20, 0.049), (21, -0.042), (22, -0.06), (23, 0.118), (24, 0.096), (25, -0.044), (26, -0.09), (27, 0.014), (28, 0.028), (29, -0.026), (30, -0.018), (31, -0.081), (32, -0.022), (33, -0.02), (34, -0.069), (35, -0.017), (36, -0.06), (37, 0.001), (38, 0.093), (39, -0.001), (40, 0.022), (41, 0.015), (42, 0.037), (43, -0.047), (44, -0.015), (45, -0.025), (46, 0.046), (47, 0.008), (48, -0.038), (49, -0.02)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95986825 <a title="937-lsi-1" href="../high_scalability-2010/high_scalability-2010-11-09-Paper%3A_Hyder_-_Scaling_Out_without_Partitioning_.html">937 high scalability-2010-11-09-Paper: Hyder - Scaling Out without Partitioning </a></p>
<p>Introduction: Partitioning is what differentiates scaling-out from scaling-up, isn't it? I thought so too until I read  Pat Helland's blog post on Hyder , a research database at Microsoft, in which  the database is the log, no partitioning is required, and the database is multi-versioned . Not much is available on Hyder. There's the excellent summary post from Mr. Helland and these documents:  Scaling Out without Partitioning  and  Scaling Out without Partitioning  - Hyder Update  by Phil Bernstein and Colin Reid of Microsoft.
 
 
 
The idea behind Hyder as summarized by Pat Helland (see his blog for the full post):
  Hyder is a software stack for transactional record management. It can offer full database functionality and is designed to take advantage of flash in a novel way. Most approaches to scale-out use partitioning and spread the data across multiple machines leaving the application responsible for consistency.

 


In Hyder, the database is the log, no partitioning is required, and the data</p><p>2 0.87673628 <a title="937-lsi-2" href="../high_scalability-2013/high_scalability-2013-01-21-Processing_100_Million_Pixels_a_Day_-_Small_Amounts_of_Contention_Cause_Big_Problems_at_Scale.html">1390 high scalability-2013-01-21-Processing 100 Million Pixels a Day - Small Amounts of Contention Cause Big Problems at Scale</a></p>
<p>Introduction: This is a guest post by  Gordon Worley , a Software Engineer at  Korrelate , where they correlate (see what they did there) online purchases to offline purchases. 
 
Several weeks ago, we came into the office one morning to find every server alarm going off. Pixel log processing was behind by 8 hours and not making headway. Checking the logs, we discovered that a big client had come online during the night and was giving us 10 times more traffic than we were originally told to expect. I wouldn’t say we panicked, but the office was certainly more jittery than usual. Over the next several hours, though, thanks both to foresight and quick thinking, we were able to scale up to handle the added load and clear the backlog to return log processing to a steady state.
 
At Korrelate, we deploy  tracking pixels , also known beacons or web bugs, that our partners use to send us information about their users. These tiny web objects contain no visible content, but may include transparent 1 by 1 gif</p><p>3 0.85140783 <a title="937-lsi-3" href="../high_scalability-2009/high_scalability-2009-03-16-Product%3A_Smart_Inspect.html">541 high scalability-2009-03-16-Product: Smart Inspect</a></p>
<p>Introduction: Smart Inspect  has added quite a few features specifically tailored to high scalability and high performance environments to our tool over the years. This includes the ability to log to memory and dump log files on demand (when a crash occurs for example), special backlog queue features, a log service application for central log storage and a lot more. Additionally, our SmartInspect Console (the viewer application) makes viewing, filtering and inspecting large amounts of logging data a lot easier/practical.</p><p>4 0.82779807 <a title="937-lsi-4" href="../high_scalability-2007/high_scalability-2007-07-26-Product%3A_AWStats_a_Log_Analyzer.html">30 high scalability-2007-07-26-Product: AWStats a Log Analyzer</a></p>
<p>Introduction: AWStats  is a free powerful and featureful tool that generates advanced web, streaming, ftp or mail server statistics, graphically. This log analyzer works as a CGI or from command line and shows you all possible information your log contains, in few graphical web pages. It uses a partial information file to be able to process large log files, often and quickly. It can analyze log files from all major server tools like Apache log files (NCSA combined/XLF/ELF log format or common/CLF log format), WebStar, IIS (W3C log format) and a lot of other web, proxy, wap, streaming servers, mail servers and some ftp servers.</p><p>5 0.76021433 <a title="937-lsi-5" href="../high_scalability-2007/high_scalability-2007-08-30-Log_Everything_All_the_Time.html">77 high scalability-2007-08-30-Log Everything All the Time</a></p>
<p>Introduction: This JoelOnSoftware  thread  asks the age old question of what and how to log. The usual trace/error/warning/info advice is totally useless in a large scale distributed system. Instead, you need to  log everything all the time  so you can solve problems that have already happened across a potentially huge range of servers. Yes, it can be done.  To see why the typical logging approach is broken,  imagine this scenario: Your site has been up and running great for weeks. No problems. A foreshadowing beeper goes off at 2AM. It seems some users can no longer add comments to threads. Then you hear the debugging deathknell: it's an intermittent problem and customers are pissed. Fix it. Now.  So how are you going to debug this? The monitoring system doesn't show any obvious problems or errors. You quickly post a comment and it works fine. This won't be easy. So you think. Commenting involves a bunch of servers and networks. There's the load balancer, spam filter,  web server, database server,</p><p>6 0.75561523 <a title="937-lsi-6" href="../high_scalability-2008/high_scalability-2008-11-24-Product%3A_Scribe_-_Facebook%27s_Scalable_Logging_System.html">449 high scalability-2008-11-24-Product: Scribe - Facebook's Scalable Logging System</a></p>
<p>7 0.73118478 <a title="937-lsi-7" href="../high_scalability-2008/high_scalability-2008-01-30-How_Rackspace_Now_Uses_MapReduce_and_Hadoop_to_Query_Terabytes_of_Data.html">233 high scalability-2008-01-30-How Rackspace Now Uses MapReduce and Hadoop to Query Terabytes of Data</a></p>
<p>8 0.69156545 <a title="937-lsi-8" href="../high_scalability-2007/high_scalability-2007-07-28-Product%3A_FastStats_Log_Analyzer_.html">35 high scalability-2007-07-28-Product: FastStats Log Analyzer </a></p>
<p>9 0.67230552 <a title="937-lsi-9" href="../high_scalability-2011/high_scalability-2011-08-10-LevelDB_-_Fast_and_Lightweight_Key-Value_Database_From_the_Authors_of_MapReduce_and_BigTable.html">1096 high scalability-2011-08-10-LevelDB - Fast and Lightweight Key-Value Database From the Authors of MapReduce and BigTable</a></p>
<p>10 0.67213374 <a title="937-lsi-10" href="../high_scalability-2007/high_scalability-2007-07-28-Product%3A_Web_Log_Storming.html">37 high scalability-2007-07-28-Product: Web Log Storming</a></p>
<p>11 0.66154605 <a title="937-lsi-11" href="../high_scalability-2012/high_scalability-2012-02-20-Berkeley_DB_Architecture_-_NoSQL_Before_NoSQL_was_Cool.html">1196 high scalability-2012-02-20-Berkeley DB Architecture - NoSQL Before NoSQL was Cool</a></p>
<p>12 0.64154148 <a title="937-lsi-12" href="../high_scalability-2007/high_scalability-2007-10-01-Statistics_Logging_Scalability.html">105 high scalability-2007-10-01-Statistics Logging Scalability</a></p>
<p>13 0.63934875 <a title="937-lsi-13" href="../high_scalability-2009/high_scalability-2009-04-03-Collectl_interface_to_Ganglia_-_any_interest%3F.html">553 high scalability-2009-04-03-Collectl interface to Ganglia - any interest?</a></p>
<p>14 0.62905347 <a title="937-lsi-14" href="../high_scalability-2008/high_scalability-2008-04-19-How_to_build_a_real-time_analytics_system%3F.html">304 high scalability-2008-04-19-How to build a real-time analytics system?</a></p>
<p>15 0.61321026 <a title="937-lsi-15" href="../high_scalability-2009/high_scalability-2009-04-15-Implementing_large_scale_web_analytics.html">570 high scalability-2009-04-15-Implementing large scale web analytics</a></p>
<p>16 0.58283311 <a title="937-lsi-16" href="../high_scalability-2007/high_scalability-2007-07-28-Product%3A_Web_Log_Expert.html">36 high scalability-2007-07-28-Product: Web Log Expert</a></p>
<p>17 0.57676315 <a title="937-lsi-17" href="../high_scalability-2014/high_scalability-2014-04-30-10_Tips_for_Optimizing_NGINX_and_PHP-fpm_for_High_Traffic_Sites.html">1640 high scalability-2014-04-30-10 Tips for Optimizing NGINX and PHP-fpm for High Traffic Sites</a></p>
<p>18 0.5747025 <a title="937-lsi-18" href="../high_scalability-2007/high_scalability-2007-07-30-Product%3A_SmarterStats.html">45 high scalability-2007-07-30-Product: SmarterStats</a></p>
<p>19 0.5725807 <a title="937-lsi-19" href="../high_scalability-2010/high_scalability-2010-05-20-Strategy%3A_Scale_Writes_to_734_Million_Records_Per_Day_Using_Time_Partitioning.html">829 high scalability-2010-05-20-Strategy: Scale Writes to 734 Million Records Per Day Using Time Partitioning</a></p>
<p>20 0.5574705 <a title="937-lsi-20" href="../high_scalability-2010/high_scalability-2010-04-30-Hot_Scalability_Links_for_April_30%2C_2010.html">819 high scalability-2010-04-30-Hot Scalability Links for April 30, 2010</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.15), (2, 0.194), (10, 0.088), (24, 0.02), (43, 0.221), (47, 0.017), (61, 0.024), (79, 0.056), (85, 0.056), (94, 0.059)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.97159582 <a title="937-lda-1" href="../high_scalability-2009/high_scalability-2009-02-01-More_Chips_Means_Less_Salsa.html">505 high scalability-2009-02-01-More Chips Means Less Salsa</a></p>
<p>Introduction: Yes, I just got through watching the Superbowl so chips and salsa are on my mind and in my stomach. In recreational eating more chips requires downing more salsa. With mulitcore chips it turns out as cores go up salsa goes down, salsa obviously being a metaphor for speed.     Sandia National Laboratories found in their simulations:   a significant increase in speed going from two to four multicores, but an insignificant increase from four to eight multicores. Exceeding eight multicores causes a decrease in speed. Sixteen multicores perform barely as well as two, and after that, a steep decline is registered as more cores are added. The problem is the lack of memory bandwidth as well as contention between processors over the memory bus available to each processor.      The implication for those following a diagonal scaling strategy is to work like heck to make your system fit within eight multicores. After that you'll need to consider some sort of partitioning strategy. What's interesti</p><p>2 0.91175622 <a title="937-lda-2" href="../high_scalability-2010/high_scalability-2010-09-03-Hot_Scalability_Links_For_Sep_3%2C_2010.html">893 high scalability-2010-09-03-Hot Scalability Links For Sep 3, 2010</a></p>
<p>Introduction: With summer almost gone, it's time to fall into some good links...
  
  Hibari - distributed, fault tolerant, highly available key-value store  written in Erlang. In this video Scott Lystig Fritchie gives a very good overview of the newest key-value store.  
 Tweets of Gold       
 
  lenidot : with 12 staff, @ tumblr  serves 1.5billion pageviews/month and 25,000 signups/day. Now that's scalability! 
  jmtan24 : Funny that whenever a high scalability article comes out, it always mention the shared nothing approach 
  mfeathers : When life gives you lemons, you can have decades-long conquest to convert lemons to oranges, or you can make lemonade. 
  OyvindIsene : Met an old man with mustache today, he had no opinion on  #noSQL . Note to myself: Don't grow a mustache, now or later.  
  vlad003 : Isn't it interesting how P2P distributes data while Cloud Computing centralizes it? And they're both said to be the future. 
 
 
 You may be interested in a new  DevOps Meetup  organized by Dave</p><p>3 0.89854884 <a title="937-lda-3" href="../high_scalability-2008/high_scalability-2008-12-18-Risk_Analysis_on_the_Cloud_%28Using_Excel_and_GigaSpaces%29.html">470 high scalability-2008-12-18-Risk Analysis on the Cloud (Using Excel and GigaSpaces)</a></p>
<p>Introduction: Every day brings news of either more failures of the financial systems or out-right fraud, with the $50 billion Bernard Madoff Ponzi scheme being the latest, breaking all records. This post provide a technical overview of a solution that was implemented for one of the largest banks in China. The solution illustrate how one can use Excel as a front end client and at the same time leverage cloud computing model and mapreduce as well as other patterns to scale-out risk calculations. I'm hoping that this type of approach will reduce the chances for seeing this type of fraud from happening in the future.</p><p>4 0.88806736 <a title="937-lda-4" href="../high_scalability-2014/high_scalability-2014-04-01-The_Mullet_Cloud_Selection_Pattern.html">1624 high scalability-2014-04-01-The Mullet Cloud Selection Pattern</a></p>
<p>Introduction: In a recent  thread on Hacker News  one of the commenters mentioned that they use Digital Ocean for personal stuff, but use AWS for business.
 
This DO for personal and AWS for business split has become popular enough that we can now give it a name: the  Mullet Cloud Selection Pattern -  business on the front and party on the back.
 
Providers like DO are cheap and the lightweight composable container model has an aesthetic appeal to developers. Even though it seems like much of the VM infrastructure has to be reinvented for containers, the industry often follows the lead of developer preference.
 
The mullet is dead. Long live the mullet! Developers are ever restless, always eager to move onto something new.</p><p>5 0.88180935 <a title="937-lda-5" href="../high_scalability-2012/high_scalability-2012-10-09-Batoo_JPA_-_The_new_JPA_Implementation_that_runs_over_15_times_faster....html">1336 high scalability-2012-10-09-Batoo JPA - The new JPA Implementation that runs over 15 times faster...</a></p>
<p>Introduction: This post is by  Hasan Ceylan , an Open Source software enthusiast from Istanbul. 
 
I loved the JPA 1.0 back in early 2000s. I started using it together with EJB 3.0 even before the stable releases. I loved it so much that I contributed bits and parts for JBoss 3.x implementations.
  Those were the days our company was considerably still small in size. Creating new features and applications were more priority than the performance, because there were a lot of ideas that we have and we needed to develop and market those as fast as we can. Now, we no longer needed to write tedious and error prone xml descriptions for the data model and deployment descriptors. Nor we needed to use the curse called “XDoclet”.  On the other side, our company grew steadily, our web site has become the top portal in the country for live events and ticketing. We now had the performance problems! Although the company grew considerably, due to the economics in the industry, we did not make a lot of money. The ch</p><p>same-blog 6 0.86888951 <a title="937-lda-6" href="../high_scalability-2010/high_scalability-2010-11-09-Paper%3A_Hyder_-_Scaling_Out_without_Partitioning_.html">937 high scalability-2010-11-09-Paper: Hyder - Scaling Out without Partitioning </a></p>
<p>7 0.86839157 <a title="937-lda-7" href="../high_scalability-2012/high_scalability-2012-01-27-Stuff_The_Internet_Says_On_Scalability_For_January_27%2C_2012.html">1182 high scalability-2012-01-27-Stuff The Internet Says On Scalability For January 27, 2012</a></p>
<p>8 0.8643719 <a title="937-lda-8" href="../high_scalability-2007/high_scalability-2007-07-28-Product%3A_Web_Log_Storming.html">37 high scalability-2007-07-28-Product: Web Log Storming</a></p>
<p>9 0.83976066 <a title="937-lda-9" href="../high_scalability-2007/high_scalability-2007-08-02-Multilanguage_Website.html">54 high scalability-2007-08-02-Multilanguage Website</a></p>
<p>10 0.83463365 <a title="937-lda-10" href="../high_scalability-2009/high_scalability-2009-10-22-Paper%3A_The_Case_for_RAMClouds%3A_Scalable_High-Performance_Storage_Entirely_in_DRAM_.html">726 high scalability-2009-10-22-Paper: The Case for RAMClouds: Scalable High-Performance Storage Entirely in DRAM </a></p>
<p>11 0.82978499 <a title="937-lda-11" href="../high_scalability-2011/high_scalability-2011-07-27-Making_Hadoop_1000x_Faster_for_Graph_Problems.html">1088 high scalability-2011-07-27-Making Hadoop 1000x Faster for Graph Problems</a></p>
<p>12 0.80161619 <a title="937-lda-12" href="../high_scalability-2014/high_scalability-2014-02-28-Stuff_The_Internet_Says_On_Scalability_For_February_28th%2C_2014.html">1603 high scalability-2014-02-28-Stuff The Internet Says On Scalability For February 28th, 2014</a></p>
<p>13 0.79931945 <a title="937-lda-13" href="../high_scalability-2010/high_scalability-2010-03-22-7_Secrets_to_Successfully_Scaling_with_Scalr_%28on_Amazon%29_by_Sebastian_Stadil.html">798 high scalability-2010-03-22-7 Secrets to Successfully Scaling with Scalr (on Amazon) by Sebastian Stadil</a></p>
<p>14 0.79239959 <a title="937-lda-14" href="../high_scalability-2011/high_scalability-2011-09-23-The_Real_News_is_Not_that_Facebook_Serves_Up_1_Trillion_Pages_a_Month%E2%80%A6.html">1123 high scalability-2011-09-23-The Real News is Not that Facebook Serves Up 1 Trillion Pages a Month…</a></p>
<p>15 0.77610475 <a title="937-lda-15" href="../high_scalability-2013/high_scalability-2013-06-13-Busting_4_Modern_Hardware_Myths_-_Are_Memory%2C_HDDs%2C_and_SSDs_Really_Random_Access%3F.html">1475 high scalability-2013-06-13-Busting 4 Modern Hardware Myths - Are Memory, HDDs, and SSDs Really Random Access?</a></p>
<p>16 0.77545381 <a title="937-lda-16" href="../high_scalability-2009/high_scalability-2009-10-12-High_Performance_at_Massive_Scale_%E2%80%93__Lessons_learned_at_Facebook.html">720 high scalability-2009-10-12-High Performance at Massive Scale –  Lessons learned at Facebook</a></p>
<p>17 0.77270532 <a title="937-lda-17" href="../high_scalability-2008/high_scalability-2008-06-04-LinkedIn_Architecture.html">339 high scalability-2008-06-04-LinkedIn Architecture</a></p>
<p>18 0.77253675 <a title="937-lda-18" href="../high_scalability-2010/high_scalability-2010-06-07-Six_Ways_Twitter_May_Reach_its_Big_Hairy_Audacious_Goal_of_One_Billion_Users.html">837 high scalability-2010-06-07-Six Ways Twitter May Reach its Big Hairy Audacious Goal of One Billion Users</a></p>
<p>19 0.76708806 <a title="937-lda-19" href="../high_scalability-2011/high_scalability-2011-10-24-StackExchange_Architecture_Updates_-_Running_Smoothly%2C_Amazon_4x_More_Expensive.html">1131 high scalability-2011-10-24-StackExchange Architecture Updates - Running Smoothly, Amazon 4x More Expensive</a></p>
<p>20 0.76198584 <a title="937-lda-20" href="../high_scalability-2012/high_scalability-2012-01-23-Facebook_Timeline%3A_Brought_to_You_by_the_Power_of_Denormalization.html">1179 high scalability-2012-01-23-Facebook Timeline: Brought to You by the Power of Denormalization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
