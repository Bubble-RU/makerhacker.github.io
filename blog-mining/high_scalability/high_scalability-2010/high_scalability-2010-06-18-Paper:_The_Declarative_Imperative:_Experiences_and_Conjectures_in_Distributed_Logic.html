<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>844 high scalability-2010-06-18-Paper: The Declarative Imperative: Experiences and Conjectures in Distributed Logic</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2010" href="../home/high_scalability-2010_home.html">high_scalability-2010</a> <a title="high_scalability-2010-844" href="#">high_scalability-2010-844</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>844 high scalability-2010-06-18-Paper: The Declarative Imperative: Experiences and Conjectures in Distributed Logic</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2010-844-html" href="http://highscalability.com//blog/2010/6/18/paper-the-declarative-imperative-experiences-and-conjectures.html">html</a></p><p>Introduction: The Declarative Imperative: Experiences and Conjectures in Distributed Logic  is written by UC Berkeley's  Joseph Hellerstein  for a keynote speech he gave at  PODS . The video version of the talk is  here . You may have heard about Mr. Hellerstein through the  Berkeley Orders Of Magnitude  project ( BOOM ), whose purpose is to help  people build systems that are OOM (orders of magnitude) bigger than are building today, with OOM less effort than traditional programming methodologies . A noble goal which may be why BOOM was rated as a top 10 emerging technology for 2010 by  MIT Technology Review . Quite an honor.
 
The motivation for the talk is a familiar one: it's a dark period for computer programming and if we don't learn how to write parallel programs the children of Moore's law will destroy us all. We have more and more processors, yet we are stuck on figuring out how the average programmer can exploit them. The BOOM solution is the Bloom language which is based on  Dedalus:</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Hellerstein through the  Berkeley Orders Of Magnitude  project ( BOOM ), whose purpose is to help  people build systems that are OOM (orders of magnitude) bigger than are building today, with OOM less effort than traditional programming methodologies . [sent-4, score-0.271]
</p><p>2 A noble goal which may be why BOOM was rated as a top 10 emerging technology for 2010 by  MIT Technology Review . [sent-5, score-0.07]
</p><p>3 The BOOM solution is the Bloom language which is based on  Dedalus:        Dedalus  is a temporal logic language that serves as a clean foundation for Bloom. [sent-9, score-0.356]
</p><p>4 The key insight in Dedalus is that distributed programming is about time, not about space, and programmers should focus their attention on data, invariants, and changes in time. [sent-10, score-0.139]
</p><p>5 Dedalus is an evolution of our earlier Overlog language, which in turn was based in Datalog. [sent-12, score-0.113]
</p><p>6 Where Overlog had complicated operational semantics, Dedalus is pure temporal logic with no need for the programmer to understand the behavior of the interpreter/compiler      Dedalus is  completely declarative and logic based. [sent-13, score-0.433]
</p><p>7 I have to say the  data flow language  approach makes more intuitive sense to me, but I feel this is something that I should try to understand. [sent-15, score-0.067]
</p><p>8 Hellerstein wrote an overview of the paper in a  blog post . [sent-17, score-0.066]
</p><p>9 The conjecture maps out the systems that can be implemented in an eventually consistent fashion without any coordination or waiting across sites. [sent-22, score-0.149]
</p><p>10 As a somewhat simplified rubric, this includes systems that (a) do append-only updates (no overwriting) and (b) never try to aggregate data across sites via counting, summing, voting, finding a max or min, etc. [sent-25, score-0.07]
</p><p>11 (This is more restrictive than what CALM intends: these behaviors can in fact be “used” monotonically in special cases. [sent-26, score-0.124]
</p><p>12 )  Interestingly, cross-site joins and recursion  are  included: these are monotonic operations and can easily be implemented in an eventually consistent, streaming fashion. [sent-28, score-0.282]
</p><p>13 It’s an extension or corollary of CALM to address classic distributed systems complexities of coordinating clocks across machines ( Lamport’s famous foundations  and follow-ons). [sent-31, score-0.471]
</p><p>14 As a side-note, it also argues that monotonic systems will work correctly even if messages are delivered at an earlier time than they are sent! [sent-33, score-0.465]
</p><p>15 (If you think this sounds nuts, think about restarting a process but reusing its message logs from earlier runs to save work. [sent-34, score-0.174]
</p><p>16 This one is more cosmic — it tries to explain the underlying purpose of Time in computing (and maybe in general). [sent-36, score-0.196]
</p><p>17 The basic idea is that Time (meaning both the sequentiality of program steps in a single “thread”, and coordination of steps across threads/machines) is needed for only one purpose: to prevent multiple possible states from co-occurring. [sent-37, score-0.291]
</p><p>18 the purpose of time is to seal fate  at each instantaneous moment . [sent-40, score-0.267]
</p><p>19 I’m claiming that if you are using synchronization in a program — i. [sent-43, score-0.076]
</p><p>20 (Did you think through this the last time you wrote a two consecutive lines in Java/Python/C? [sent-47, score-0.066]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('calm', 0.331), ('dedalus', 0.311), ('monotonic', 0.282), ('boom', 0.243), ('hellerstein', 0.155), ('monotonicity', 0.155), ('overlog', 0.155), ('seal', 0.141), ('corollary', 0.126), ('oom', 0.126), ('purpose', 0.126), ('logic', 0.123), ('foundations', 0.122), ('earlier', 0.113), ('fate', 0.107), ('temporal', 0.099), ('stands', 0.094), ('bloom', 0.092), ('clocks', 0.089), ('declarative', 0.088), ('cron', 0.085), ('coordination', 0.079), ('program', 0.076), ('programming', 0.075), ('hopefully', 0.071), ('noble', 0.07), ('rubric', 0.07), ('cosmic', 0.07), ('overwriting', 0.07), ('theberkeley', 0.07), ('systems', 0.07), ('steps', 0.068), ('orders', 0.068), ('language', 0.067), ('wrote', 0.066), ('thedata', 0.066), ('dummies', 0.066), ('magnitude', 0.065), ('distributed', 0.064), ('summing', 0.063), ('restrictive', 0.063), ('conjectures', 0.063), ('reusing', 0.061), ('intends', 0.061), ('lamport', 0.061), ('monotonically', 0.061), ('voting', 0.061), ('parallel', 0.06), ('invariants', 0.059), ('speech', 0.059)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999976 <a title="844-tfidf-1" href="../high_scalability-2010/high_scalability-2010-06-18-Paper%3A_The_Declarative_Imperative%3A_Experiences_and_Conjectures_in_Distributed_Logic.html">844 high scalability-2010-06-18-Paper: The Declarative Imperative: Experiences and Conjectures in Distributed Logic</a></p>
<p>Introduction: The Declarative Imperative: Experiences and Conjectures in Distributed Logic  is written by UC Berkeley's  Joseph Hellerstein  for a keynote speech he gave at  PODS . The video version of the talk is  here . You may have heard about Mr. Hellerstein through the  Berkeley Orders Of Magnitude  project ( BOOM ), whose purpose is to help  people build systems that are OOM (orders of magnitude) bigger than are building today, with OOM less effort than traditional programming methodologies . A noble goal which may be why BOOM was rated as a top 10 emerging technology for 2010 by  MIT Technology Review . Quite an honor.
 
The motivation for the talk is a familiar one: it's a dark period for computer programming and if we don't learn how to write parallel programs the children of Moore's law will destroy us all. We have more and more processors, yet we are stuck on figuring out how the average programmer can exploit them. The BOOM solution is the Bloom language which is based on  Dedalus:</p><p>2 0.27359784 <a title="844-tfidf-2" href="../high_scalability-2012/high_scalability-2012-06-27-Paper%3A_Logic_and_Lattices_for_Distributed_Programming.html">1273 high scalability-2012-06-27-Paper: Logic and Lattices for Distributed Programming</a></p>
<p>Introduction: Neil Conway  from Berkeley CS is giving an advanced level talk at  a meetup today  in San Francisco on a new paper:  Logic and Lattices for Distributed Programming  - extending set logic to support CRDT-style lattices. 
 
The description of the meetup is probably the clearest introduction to the paper:
  Developers are increasingly choosing datastores that sacrifice strong consistency guarantees in exchange for improved performance and availability. Unfortunately, writing reliable distributed programs without the benefit of strong consistency can be very challenging.

 


In this talk, I'll discuss work from our group at UC Berkeley that aims to make it easier to write distributed programs without relying on strong consistency. Bloom is a declarative programming language for distributed computing, while CALM is an analysis technique that identifies programs that are guaranteed to be eventually consistent. I'll then discuss our recent work on extending CALM to support a broader range of</p><p>3 0.087741494 <a title="844-tfidf-3" href="../high_scalability-2013/high_scalability-2013-08-30-Stuff_The_Internet_Says_On_Scalability_For_August_30%2C_2013.html">1509 high scalability-2013-08-30-Stuff The Internet Says On Scalability For August 30, 2013</a></p>
<p>Introduction: Hey, it's HighScalability time:
     ( Nerd Power: Paul Kasemir software engineer AND American Ninja Warrior )   
  Two billion documents, 30 terabytes : Github source code indexed 
 Quotable Quotes:                                                                                                          
 
  David Krakauer : We fail to make intelligent machines because engineering is about putting together stupid components to make smart objects. Evolution is about putting together smart components into intelligent aggregates. Your brain is like an ecosystem of organisms. It's not like a circuit of gates. 
  @spyced : At this point if you depend on EBS for critical services you're living in denial and I can't help you.  
  @skilpat : TIL Friedrich Engels, not Leslie Lamport, invented logical clocks in a 1844 letter to Karl Marx 
  Dan Geer : Risk is a necessary consequence of dependence. 
  @postwait : OS Rule 1. The version of /usr/bin/X you want today will never be what your OS ships</p><p>4 0.086450875 <a title="844-tfidf-4" href="../high_scalability-2011/high_scalability-2011-04-15-Stuff_The_Internet_Says_On_Scalability_For_April_15%2C_2011.html">1024 high scalability-2011-04-15-Stuff The Internet Says On Scalability For April 15, 2011</a></p>
<p>Introduction: Submitted for your reading pleasure...
 
 Luxury is an ancient notion.  There was once a Chinese mandarin who had himself wakened three times every morning simply for the pleasure of being told it was not yet time to get up .  ~Argosy
  
 We have a Qutoable Quote machine for you today:    
 
  @kevinweil : Twitter monthly signups have increased more than 50% since December, and we're now doing well over 150 million Tweets per day. 
  @ChrisShain : Prediction: Black art of query optimization will become black art of #nosql data modeling, for same reasons. Minimize IOs, query time. 
  @ui_matters : Infrastructure as a Service = no hardware headaches. Platform as a Svc = no scalability headaches. SaaS = common dev platform #amchamtech 
  @plcstpierre : Thinking about high scalability stuff... I never thought database stuff can be interesting... 
  @webdz9r : mass scalability for dynamic web content. What took us 8 machines, now take us 1 web and 1 app. 
  @joelvarty : CDN is always an aft</p><p>5 0.085087277 <a title="844-tfidf-5" href="../high_scalability-2010/high_scalability-2010-11-09-Facebook_Uses_Non-Stored_Procedures_to_Update_Social_Graphs.html">936 high scalability-2010-11-09-Facebook Uses Non-Stored Procedures to Update Social Graphs</a></p>
<p>Introduction: Facebook's Ryan Mack gave a  MySQL Tech Talk  where he talked about using what he called  Non-stored Procedures  for adding edges to Facebook's social graph. The question is: how can edges quickly be added to the social graph? The answer is ultimately one of deciding where logic should be executed, especially when locks are kept open during network hops.
 
Ryan explained a key element of the Facebook data model are the connections between people, things they've liked, and places they've checked-in. A lot of their writes are adding edges to the social graph. 
 
Currently this is a two step process, run inside a transaction:
  
 add a new edge into the graph 
 if the add was successful then increment the number of edges on a node 
  
This approach works until there's a very hot node that is being added to rapidly. For example, a popular game adds a new character and everyone likes it at the same time or a new album comes out and everyone likes it at the same time.
 
They were limited to</p><p>6 0.084881127 <a title="844-tfidf-6" href="../high_scalability-2009/high_scalability-2009-03-16-Are_Cloud_Based_Memory_Architectures_the_Next_Big_Thing%3F.html">538 high scalability-2009-03-16-Are Cloud Based Memory Architectures the Next Big Thing?</a></p>
<p>7 0.08089681 <a title="844-tfidf-7" href="../high_scalability-2009/high_scalability-2009-05-05-Drop_ACID_and_Think_About_Data.html">589 high scalability-2009-05-05-Drop ACID and Think About Data</a></p>
<p>8 0.079474159 <a title="844-tfidf-8" href="../high_scalability-2014/high_scalability-2014-02-21-Stuff_The_Internet_Says_On_Scalability_For_February_21st%2C_2014.html">1600 high scalability-2014-02-21-Stuff The Internet Says On Scalability For February 21st, 2014</a></p>
<p>9 0.078221701 <a title="844-tfidf-9" href="../high_scalability-2012/high_scalability-2012-03-06-Ask_For_Forgiveness_Programming_-_Or_How_We%27ll_Program_1000_Cores.html">1204 high scalability-2012-03-06-Ask For Forgiveness Programming - Or How We'll Program 1000 Cores</a></p>
<p>10 0.076642044 <a title="844-tfidf-10" href="../high_scalability-2011/high_scalability-2011-04-06-Netflix%3A_Run_Consistency_Checkers_All_the_time_to_Fixup_Transactions.html">1017 high scalability-2011-04-06-Netflix: Run Consistency Checkers All the time to Fixup Transactions</a></p>
<p>11 0.07382559 <a title="844-tfidf-11" href="../high_scalability-2012/high_scalability-2012-11-05-Gone_Fishin%27%3A_Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_In_The_Ambient_Cloud.html">1355 high scalability-2012-11-05-Gone Fishin': Building Super Scalable Systems: Blade Runner Meets Autonomic Computing In The Ambient Cloud</a></p>
<p>12 0.073759213 <a title="844-tfidf-12" href="../high_scalability-2009/high_scalability-2009-12-16-Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_in_the_Ambient_Cloud.html">750 high scalability-2009-12-16-Building Super Scalable Systems: Blade Runner Meets Autonomic Computing in the Ambient Cloud</a></p>
<p>13 0.073612764 <a title="844-tfidf-13" href="../high_scalability-2008/high_scalability-2008-06-09-FaceStat%27s_Rousing_Tale_of_Scaling_Woe_and_Wisdom_Won.html">344 high scalability-2008-06-09-FaceStat's Rousing Tale of Scaling Woe and Wisdom Won</a></p>
<p>14 0.072660074 <a title="844-tfidf-14" href="../high_scalability-2014/high_scalability-2014-04-03-Leslie_Lamport_to_Programmers%3A_You%27re_Doing_it_Wrong.html">1625 high scalability-2014-04-03-Leslie Lamport to Programmers: You're Doing it Wrong</a></p>
<p>15 0.071396247 <a title="844-tfidf-15" href="../high_scalability-2013/high_scalability-2013-12-20-Stuff_The_Internet_Says_On_Scalability_For_December_20th%2C_2013.html">1567 high scalability-2013-12-20-Stuff The Internet Says On Scalability For December 20th, 2013</a></p>
<p>16 0.070993938 <a title="844-tfidf-16" href="../high_scalability-2009/high_scalability-2009-06-23-Learn_How_to_Exploit_Multiple_Cores_for_Better_Performance_and_Scalability.html">636 high scalability-2009-06-23-Learn How to Exploit Multiple Cores for Better Performance and Scalability</a></p>
<p>17 0.070697382 <a title="844-tfidf-17" href="../high_scalability-2013/high_scalability-2013-04-05-Stuff_The_Internet_Says_On_Scalability_For_April_5%2C_2013.html">1436 high scalability-2013-04-05-Stuff The Internet Says On Scalability For April 5, 2013</a></p>
<p>18 0.069436066 <a title="844-tfidf-18" href="../high_scalability-2009/high_scalability-2009-11-01-Squeeze_more_performance_from_Parallelism.html">735 high scalability-2009-11-01-Squeeze more performance from Parallelism</a></p>
<p>19 0.069097988 <a title="844-tfidf-19" href="../high_scalability-2009/high_scalability-2009-08-24-How_Google_Serves_Data_from_Multiple_Datacenters.html">687 high scalability-2009-08-24-How Google Serves Data from Multiple Datacenters</a></p>
<p>20 0.068978161 <a title="844-tfidf-20" href="../high_scalability-2012/high_scalability-2012-03-26-7_Years_of_YouTube_Scalability_Lessons_in_30_Minutes.html">1215 high scalability-2012-03-26-7 Years of YouTube Scalability Lessons in 30 Minutes</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.132), (1, 0.076), (2, 0.006), (3, 0.069), (4, 0.004), (5, 0.03), (6, -0.015), (7, 0.042), (8, -0.047), (9, 0.022), (10, -0.016), (11, 0.026), (12, -0.011), (13, -0.038), (14, 0.036), (15, -0.025), (16, 0.022), (17, -0.009), (18, 0.009), (19, -0.026), (20, 0.017), (21, -0.005), (22, -0.047), (23, 0.035), (24, -0.051), (25, -0.001), (26, 0.018), (27, 0.009), (28, 0.028), (29, 0.023), (30, 0.011), (31, 0.025), (32, -0.062), (33, 0.023), (34, -0.032), (35, -0.056), (36, 0.018), (37, -0.019), (38, 0.028), (39, 0.06), (40, -0.024), (41, 0.016), (42, -0.015), (43, -0.051), (44, -0.001), (45, 0.005), (46, 0.008), (47, -0.005), (48, 0.011), (49, -0.009)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95969296 <a title="844-lsi-1" href="../high_scalability-2010/high_scalability-2010-06-18-Paper%3A_The_Declarative_Imperative%3A_Experiences_and_Conjectures_in_Distributed_Logic.html">844 high scalability-2010-06-18-Paper: The Declarative Imperative: Experiences and Conjectures in Distributed Logic</a></p>
<p>Introduction: The Declarative Imperative: Experiences and Conjectures in Distributed Logic  is written by UC Berkeley's  Joseph Hellerstein  for a keynote speech he gave at  PODS . The video version of the talk is  here . You may have heard about Mr. Hellerstein through the  Berkeley Orders Of Magnitude  project ( BOOM ), whose purpose is to help  people build systems that are OOM (orders of magnitude) bigger than are building today, with OOM less effort than traditional programming methodologies . A noble goal which may be why BOOM was rated as a top 10 emerging technology for 2010 by  MIT Technology Review . Quite an honor.
 
The motivation for the talk is a familiar one: it's a dark period for computer programming and if we don't learn how to write parallel programs the children of Moore's law will destroy us all. We have more and more processors, yet we are stuck on figuring out how the average programmer can exploit them. The BOOM solution is the Bloom language which is based on  Dedalus:</p><p>2 0.83263773 <a title="844-lsi-2" href="../high_scalability-2014/high_scalability-2014-05-01-Paper%3A_Can_Programming_Be_Liberated_From_The_Von_Neumann_Style%3F_.html">1641 high scalability-2014-05-01-Paper: Can Programming Be Liberated From The Von Neumann Style? </a></p>
<p>Introduction: Famous computer scientist  John Backus , he's the B in BNF(Backus-Naur form) and the creator of Fortran, gave a Turing Award Lecture titled  Can programming be liberated from the von Neumann style?: a functional style and its algebra of programs , that has layed out a division in programming that lives long after it was published in 1977. 
 
It's the now familiar argument for why functional programming is superior:
  

The assignment statement is the von Neumann bottleneck of programming languages and keeps us thinking in word-at-a-time terms in much the same way the computer's bottleneck does.


...


The second world of conventional programming languages is the world of statements. The primary statement in that world is the assignment statement itself. All the other statements of the language exist in order to make it possible to perform a computation that must be based on this primitive construct: the assignment statement.

  
Here's a response by Dijkstra  A review of the 1977 Turi</p><p>3 0.80300552 <a title="844-lsi-3" href="../high_scalability-2012/high_scalability-2012-06-27-Paper%3A_Logic_and_Lattices_for_Distributed_Programming.html">1273 high scalability-2012-06-27-Paper: Logic and Lattices for Distributed Programming</a></p>
<p>Introduction: Neil Conway  from Berkeley CS is giving an advanced level talk at  a meetup today  in San Francisco on a new paper:  Logic and Lattices for Distributed Programming  - extending set logic to support CRDT-style lattices. 
 
The description of the meetup is probably the clearest introduction to the paper:
  Developers are increasingly choosing datastores that sacrifice strong consistency guarantees in exchange for improved performance and availability. Unfortunately, writing reliable distributed programs without the benefit of strong consistency can be very challenging.

 


In this talk, I'll discuss work from our group at UC Berkeley that aims to make it easier to write distributed programs without relying on strong consistency. Bloom is a declarative programming language for distributed computing, while CALM is an analysis technique that identifies programs that are guaranteed to be eventually consistent. I'll then discuss our recent work on extending CALM to support a broader range of</p><p>4 0.80112809 <a title="844-lsi-4" href="../high_scalability-2014/high_scalability-2014-04-03-Leslie_Lamport_to_Programmers%3A_You%27re_Doing_it_Wrong.html">1625 high scalability-2014-04-03-Leslie Lamport to Programmers: You're Doing it Wrong</a></p>
<p>Introduction: Famous computer scientist  Leslie Lamport  is definitely not a  worse is better  kind of guy. In  Computation and State Machines  he wants to make the case that to get better programs we need to teach programmers to think better. And programmers will think better when they learn to think in terms of concepts firmly grounded in the language of mathematics.
 
I was disappointed that there was so much English in the paper. Surely it would have been more convincing if it was written as a mathematical proof. Or would it?
 
This whole topic has been argued extensively throughout thousands of years of philosophy. Mathematics has always been a strange attractor for those trying to escape a flawed human rationality. In the end as alluring as the utopia of mathematics is, it lacks a coherent theory of meaning and programming is not about rearranging ungrounded symbols, it's about manipulating and shaping meaning.
 
For programmers I think  Ludwig Wittgenstein  has the right sense of things. Mean</p><p>5 0.78478271 <a title="844-lsi-5" href="../high_scalability-2009/high_scalability-2009-04-26-Map-Reduce_for_Machine_Learning_on_Multicore.html">581 high scalability-2009-04-26-Map-Reduce for Machine Learning on Multicore</a></p>
<p>Introduction: We are at the beginning of the multicore era. Computers will have increasingly many cores (processors), but there is still no good programming framework for these architectures, and thus no simple and unified way for machine learning to take advantage of the potential speed up.      In this paper, we develop a broadly applicable parallel programming method, one that is easily applied to many different learning algorithms. Our work is in distinct contrast to the tradition in machine learning of designing (often ingenious) ways to speed up a single algorithm at a time.      Specifically, we show that algorithms that fit the Statistical Query model can be written in a certain “summation form,” which allows them to be easily parallelized on multicore computers. We adapt Google’s map-reduce  paradigm to demonstrate this parallel speed up technique on a variety of learning algorithms including locally weighted linear regression (LWLR), k-means, logistic regression (LR), naive Bayes (NB), SVM</p><p>6 0.7749089 <a title="844-lsi-6" href="../high_scalability-2012/high_scalability-2012-05-10-Paper%3A_Paxos_Made_Moderately_Complex.html">1243 high scalability-2012-05-10-Paper: Paxos Made Moderately Complex</a></p>
<p>7 0.75063044 <a title="844-lsi-7" href="../high_scalability-2008/high_scalability-2008-07-26-Google%27s_Paxos_Made_Live_%E2%80%93_An_Engineering_Perspective.html">357 high scalability-2008-07-26-Google's Paxos Made Live – An Engineering Perspective</a></p>
<p>8 0.74761307 <a title="844-lsi-8" href="../high_scalability-2012/high_scalability-2012-03-06-Ask_For_Forgiveness_Programming_-_Or_How_We%27ll_Program_1000_Cores.html">1204 high scalability-2012-03-06-Ask For Forgiveness Programming - Or How We'll Program 1000 Cores</a></p>
<p>9 0.744739 <a title="844-lsi-9" href="../high_scalability-2012/high_scalability-2012-08-16-Paper%3A_A_Provably_Correct_Scalable_Concurrent_Skip_List.html">1305 high scalability-2012-08-16-Paper: A Provably Correct Scalable Concurrent Skip List</a></p>
<p>10 0.73997122 <a title="844-lsi-10" href="../high_scalability-2009/high_scalability-2009-07-21-Paper%3A_Parallelizing_the_Web_Browser.html">660 high scalability-2009-07-21-Paper: Parallelizing the Web Browser</a></p>
<p>11 0.73877907 <a title="844-lsi-11" href="../high_scalability-2011/high_scalability-2011-09-28-Pursue_robust_indefinite_scalability_with_the_Movable_Feast_Machine.html">1127 high scalability-2011-09-28-Pursue robust indefinite scalability with the Movable Feast Machine</a></p>
<p>12 0.73396289 <a title="844-lsi-12" href="../high_scalability-2013/high_scalability-2013-04-26-Stuff_The_Internet_Says_On_Scalability_For_April_26%2C_2013.html">1447 high scalability-2013-04-26-Stuff The Internet Says On Scalability For April 26, 2013</a></p>
<p>13 0.72584087 <a title="844-lsi-13" href="../high_scalability-2010/high_scalability-2010-06-30-Paper%3A_GraphLab%3A_A_New_Framework_For_Parallel_Machine_Learning.html">850 high scalability-2010-06-30-Paper: GraphLab: A New Framework For Parallel Machine Learning</a></p>
<p>14 0.72144324 <a title="844-lsi-14" href="../high_scalability-2010/high_scalability-2010-06-09-Paper%3A_Propagation_Networks%3A_A_Flexible_and_Expressive_Substrate_for_Computation_.html">839 high scalability-2010-06-09-Paper: Propagation Networks: A Flexible and Expressive Substrate for Computation </a></p>
<p>15 0.71380353 <a title="844-lsi-15" href="../high_scalability-2010/high_scalability-2010-05-12-The_Rise_of_the_Virtual_Cellular_Machines.html">826 high scalability-2010-05-12-The Rise of the Virtual Cellular Machines</a></p>
<p>16 0.71357119 <a title="844-lsi-16" href="../high_scalability-2013/high_scalability-2013-04-25-Paper%3A_Making_reliable_distributed_systems_in_the_presence_of_software_errors.html">1446 high scalability-2013-04-25-Paper: Making reliable distributed systems in the presence of software errors</a></p>
<p>17 0.71262378 <a title="844-lsi-17" href="../high_scalability-2009/high_scalability-2009-09-16-Paper%3A_A_practical_scalable_distributed_B-tree.html">705 high scalability-2009-09-16-Paper: A practical scalable distributed B-tree</a></p>
<p>18 0.70850182 <a title="844-lsi-18" href="../high_scalability-2011/high_scalability-2011-02-02-Piccolo_-_Building_Distributed_Programs_that_are_11x_Faster_than_Hadoop.html">983 high scalability-2011-02-02-Piccolo - Building Distributed Programs that are 11x Faster than Hadoop</a></p>
<p>19 0.70675862 <a title="844-lsi-19" href="../high_scalability-2013/high_scalability-2013-10-25-Stuff_The_Internet_Says_On_Scalability_For_October_25th%2C_2013.html">1537 high scalability-2013-10-25-Stuff The Internet Says On Scalability For October 25th, 2013</a></p>
<p>20 0.7062512 <a title="844-lsi-20" href="../high_scalability-2013/high_scalability-2013-10-31-Paper%3A_Everything_You_Always_Wanted_to_Know_About_Synchronization_but_Were_Afraid_to_Ask.html">1541 high scalability-2013-10-31-Paper: Everything You Always Wanted to Know About Synchronization but Were Afraid to Ask</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.049), (2, 0.578), (5, 0.031), (10, 0.034), (30, 0.022), (40, 0.018), (56, 0.019), (61, 0.057), (79, 0.083), (94, 0.03)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99499583 <a title="844-lda-1" href="../high_scalability-2010/high_scalability-2010-06-18-Paper%3A_The_Declarative_Imperative%3A_Experiences_and_Conjectures_in_Distributed_Logic.html">844 high scalability-2010-06-18-Paper: The Declarative Imperative: Experiences and Conjectures in Distributed Logic</a></p>
<p>Introduction: The Declarative Imperative: Experiences and Conjectures in Distributed Logic  is written by UC Berkeley's  Joseph Hellerstein  for a keynote speech he gave at  PODS . The video version of the talk is  here . You may have heard about Mr. Hellerstein through the  Berkeley Orders Of Magnitude  project ( BOOM ), whose purpose is to help  people build systems that are OOM (orders of magnitude) bigger than are building today, with OOM less effort than traditional programming methodologies . A noble goal which may be why BOOM was rated as a top 10 emerging technology for 2010 by  MIT Technology Review . Quite an honor.
 
The motivation for the talk is a familiar one: it's a dark period for computer programming and if we don't learn how to write parallel programs the children of Moore's law will destroy us all. We have more and more processors, yet we are stuck on figuring out how the average programmer can exploit them. The BOOM solution is the Bloom language which is based on  Dedalus:</p><p>2 0.99035889 <a title="844-lda-2" href="../high_scalability-2009/high_scalability-2009-05-08-Eight_Best_Practices_for_Building_Scalable_Systems.html">594 high scalability-2009-05-08-Eight Best Practices for Building Scalable Systems</a></p>
<p>Introduction: Wille Faler has  created an excellent list of best practices  for building scalable and high performance systems. Here's a short  summary of his points:
   Offload the database  - Avoid hitting the database, and avoid opening transactions or connections unless you absolutely need to use them.    What a difference a cache makes  - For read heavy applications caching is the easiest way offload the database.    Cache as coarse-grained objects as possible  - Coarse-grained objects save CPU and time by requiring fewer reads to assemble objects.    Donâ&euro;&trade;t store transient state permanently  - Is it really necessary to store your transient data in the database?    Location, Location  - put things close to where they are supposed to be delivered.    Constrain concurrent access to limited resource  - it's quicker to let a single thread do work and finish rather than flooding finite resources with 200 client threads.    Staged, asynchronous processing  - separate a process using asynchronicity int</p><p>3 0.99031556 <a title="844-lda-3" href="../high_scalability-2008/high_scalability-2008-01-10-Letting_Clients_Know_What%27s_Changed%3A_Push_Me_or_Pull_Me%3F.html">205 high scalability-2008-01-10-Letting Clients Know What's Changed: Push Me or Pull Me?</a></p>
<p>Introduction: I had a false belief I thought I came here to stay We're all just visiting All just breaking like waves The oceans made me, but who came up with me? Push me, pull me, push me, or pull me out .     So true Perl Jam   (Push me Pull me lyrics)  , so true. I too have wondered how web clients should be notified of model changes. Should servers push events to clients or should clients pull events from servers? A topic worthy of its own song if ever there was one.       To pull events the client simply starts a timer and makes a request to the server. This is polling. You can either pull a complete set of fresh data or get a list of changes. The server "knows" if anything you are interested in has changed and makes those changes available to you.  Knowing what has changed can be relatively simple with a publish-subscribe type backend or you can get very complex with fine grained bit maps of attributes and keeping per client state on what I client still needs to see.     Polling is heavy man.</p><p>4 0.98982823 <a title="844-lda-4" href="../high_scalability-2011/high_scalability-2011-12-12-Netflix%3A_Developing%2C_Deploying%2C_and_Supporting_Software_According_to_the_Way_of_the_Cloud.html">1155 high scalability-2011-12-12-Netflix: Developing, Deploying, and Supporting Software According to the Way of the Cloud</a></p>
<p>Introduction: At a  Cloud Computing Meetup , Siddharth "Sid" Anand of Netflix, backed by a merry band of Netflixians, gave an interesting talk:  Keeping Movies Running Amid Thunderstorms . While the talk gave a good overview of their move to the cloud, issues with capacity planning,  thundering herds , latency problems, and  simian armageddon , I found myself most taken with how they handle  software deployment in the cloud .
 
I've worked on half a dozen or more build and deployment systems, some small, some quite large, but never for a large organization like Netflix in the cloud. The cloud has this amazing capability that has never existed before that enables a novel approach to fault-tolerant software deployments:  the ability to spin up huge numbers of instances to completely run a new release while running the old release at the same time .
 
The process goes something like: 
  
 A  canary machine  is launched first with the new software load running real traffic to sanity test the load in a p</p><p>5 0.98876125 <a title="844-lda-5" href="../high_scalability-2007/high_scalability-2007-07-31-BerkeleyDB_%26_other_distributed_high_performance_key-value_databases.html">50 high scalability-2007-07-31-BerkeleyDB & other distributed high performance key-value databases</a></p>
<p>Introduction: I currently use BerkeleyDB as an embedded database   http://www.oracle.com/database/berkeley-db/   a decision which was initially brought on by learning that Google used BerkeleyDB for their universal sign-on feature.     Lustre looks impressive, but their white paper shows speeds of 800 files created per second, as a good number.  However, BerkeleyDB on my mac mini does 200,000 row creations per second, and can be used as a distributed file system.     I'm having I/O scalability issues with BerkeleyDB on one machine, and about to implement their distributed replication feature (and go multi-machine), which in effect makes it work like a distributed file system, but with local access speeds.  That's why I was looking at Lustre.     The key feature difference between BerkeleyDB and Lustre is that BerkeleyDB has a complete copy of all the data on each computer, making it not a viable solution for massive sized database applications.  However, if you have < 1TB (ie, one disk) of total pos</p><p>6 0.98782778 <a title="844-lda-6" href="../high_scalability-2011/high_scalability-2011-01-03-Stuff_The_Internet_Says_On_Scalability_For_January_3%2C_2010.html">967 high scalability-2011-01-03-Stuff The Internet Says On Scalability For January 3, 2010</a></p>
<p>7 0.98546022 <a title="844-lda-7" href="../high_scalability-2014/high_scalability-2014-02-05-Little%E2%80%99s_Law%2C_Scalability_and_Fault_Tolerance%3A_The_OS_is_your_bottleneck._What_you_can_do%3F.html">1591 high scalability-2014-02-05-Little’s Law, Scalability and Fault Tolerance: The OS is your bottleneck. What you can do?</a></p>
<p>8 0.98407561 <a title="844-lda-8" href="../high_scalability-2010/high_scalability-2010-09-30-More_Troubles_with_Caching.html">911 high scalability-2010-09-30-More Troubles with Caching</a></p>
<p>9 0.98356098 <a title="844-lda-9" href="../high_scalability-2010/high_scalability-2010-08-12-Strategy%3A_Terminate_SSL_Connections_in_Hardware_and_Reduce_Server_Count_by_40%25.html">878 high scalability-2010-08-12-Strategy: Terminate SSL Connections in Hardware and Reduce Server Count by 40%</a></p>
<p>10 0.98246962 <a title="844-lda-10" href="../high_scalability-2011/high_scalability-2011-03-17-Are_long_VM_instance_spin-up_times_in_the_cloud_costing_you_money%3F.html">1006 high scalability-2011-03-17-Are long VM instance spin-up times in the cloud costing you money?</a></p>
<p>11 0.98153412 <a title="844-lda-11" href="../high_scalability-2008/high_scalability-2008-10-15-Outside.in_Scales_Up_with_Engine_Yard_and_moving_from_PHP_to_Ruby_on_Rails.html">417 high scalability-2008-10-15-Outside.in Scales Up with Engine Yard and moving from PHP to Ruby on Rails</a></p>
<p>12 0.98084033 <a title="844-lda-12" href="../high_scalability-2012/high_scalability-2012-02-27-Zen_and_the_Art_of_Scaling_-_A_Koan_and_Epigram_Approach.html">1199 high scalability-2012-02-27-Zen and the Art of Scaling - A Koan and Epigram Approach</a></p>
<p>13 0.98010498 <a title="844-lda-13" href="../high_scalability-2012/high_scalability-2012-02-10-Stuff_The_Internet_Says_On_Scalability_For_February_10%2C_2012.html">1190 high scalability-2012-02-10-Stuff The Internet Says On Scalability For February 10, 2012</a></p>
<p>14 0.97927129 <a title="844-lda-14" href="../high_scalability-2011/high_scalability-2011-09-27-Use_Instance_Caches_to_Save_Money%3A_Latency_%3D%3D_%24%24%24.html">1126 high scalability-2011-09-27-Use Instance Caches to Save Money: Latency == $$$</a></p>
<p>15 0.97913384 <a title="844-lda-15" href="../high_scalability-2008/high_scalability-2008-11-02-Strategy%3A_How_to_Manage_Sessions_Using_Memcached.html">436 high scalability-2008-11-02-Strategy: How to Manage Sessions Using Memcached</a></p>
<p>16 0.97906768 <a title="844-lda-16" href="../high_scalability-2010/high_scalability-2010-06-04-Strategy%3A_Cache_Larger_Chunks_-_Cache_Hit_Rate_is_a_Bad_Indicator.html">836 high scalability-2010-06-04-Strategy: Cache Larger Chunks - Cache Hit Rate is a Bad Indicator</a></p>
<p>17 0.97845328 <a title="844-lda-17" href="../high_scalability-2008/high_scalability-2008-01-25-Google%3A_Introduction_to_Distributed_System_Design.html">223 high scalability-2008-01-25-Google: Introduction to Distributed System Design</a></p>
<p>18 0.97669578 <a title="844-lda-18" href="../high_scalability-2010/high_scalability-2010-09-30-Facebook_and_Site_Failures_Caused_by_Complex%2C_Weakly_Interacting%2C_Layered_Systems.html">910 high scalability-2010-09-30-Facebook and Site Failures Caused by Complex, Weakly Interacting, Layered Systems</a></p>
<p>19 0.97605109 <a title="844-lda-19" href="../high_scalability-2008/high_scalability-2008-12-01-MySQL_Database_Scale-out_and_Replication_for_High_Growth_Businesses.html">455 high scalability-2008-12-01-MySQL Database Scale-out and Replication for High Growth Businesses</a></p>
<p>20 0.97562474 <a title="844-lda-20" href="../high_scalability-2007/high_scalability-2007-08-03-Running_Hadoop_MapReduce_on_Amazon_EC2_and_Amazon_S3.html">56 high scalability-2007-08-03-Running Hadoop MapReduce on Amazon EC2 and Amazon S3</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
