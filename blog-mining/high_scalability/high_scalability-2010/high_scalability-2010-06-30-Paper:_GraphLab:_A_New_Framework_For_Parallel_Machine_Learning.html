<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>850 high scalability-2010-06-30-Paper: GraphLab: A New Framework For Parallel Machine Learning</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2010" href="../home/high_scalability-2010_home.html">high_scalability-2010</a> <a title="high_scalability-2010-850" href="#">high_scalability-2010-850</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>850 high scalability-2010-06-30-Paper: GraphLab: A New Framework For Parallel Machine Learning</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2010-850-html" href="http://highscalability.com//blog/2010/6/30/paper-graphlab-a-new-framework-for-parallel-machine-learning.html">html</a></p><p>Introduction: In the never ending quest to figure out how to do something useful with never ending streams of data,  GraphLab: A New Framework For Parallel Machine Learning  wants to go beyond low-level programming, MapReduce, and dataflow languages with  a new parallel framework for ML (machine learning) which exploits the sparse structure and common computational patterns of ML algorithms. GraphLab enables ML experts to easily design and implement efﬁcient scalable parallel algorithms by composing problem speciﬁc computation, data-dependencies, and scheduling .   Our main contributions include:  
  
  A graph-based data model which simultaneously represents data and computational dependencies.   
  A set of concurrent access models which provide a range of sequential-consistency guarantees.   
  A sophisticated modular scheduling mechanism.   
  An aggregation framework to manage global state.   
   From the abstract:
  
  Designing and implementing efﬁcient, provably correct parallel machine lear</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 GraphLab enables ML experts to easily design and implement efﬁcient scalable parallel algorithms by composing problem speciﬁc computation, data-dependencies, and scheduling . [sent-2, score-0.522]
</p><p>2 Our main contributions include:        A graph-based data model which simultaneously represents data and computational dependencies. [sent-3, score-0.253]
</p><p>3 A sophisticated modular scheduling mechanism. [sent-5, score-0.056]
</p><p>4 An aggregation framework to manage global state. [sent-6, score-0.12]
</p><p>5 From the abstract:      Designing and implementing efﬁcient, provably correct parallel machine learning (ML) algorithms is challenging. [sent-7, score-0.523]
</p><p>6 Existing high-level parallel abstractions like MapReduce are insufﬁciently  expressive while low-level tools like MPI and Pthreads leave ML experts repeatedly solving the same design challenges. [sent-8, score-0.436]
</p><p>7 By targeting common patterns in ML, we developed GraphLab, which improves upon abstractions like MapReduce by compactly expressing asynchronous iterative algorithms with sparse computational dependencies while ensuring data consistency and achieving a high degree of parallel performance. [sent-9, score-0.771]
</p><p>8 We demonstrate the expressiveness of the GraphLab framework by designing and implementing parallel versions of belief propagation, Gibbs sampling, Co-EM, Lasso and Compressed Sensing. [sent-10, score-0.376]
</p><p>9 We show that using GraphLab we can achieve excellent parallel performance on large scale real-world problems. [sent-11, score-0.235]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('graphlab', 0.51), ('ml', 0.479), ('parallel', 0.235), ('expressive', 0.136), ('ef', 0.133), ('propagation', 0.131), ('computational', 0.131), ('cient', 0.128), ('algorithms', 0.126), ('sparse', 0.124), ('ending', 0.124), ('framework', 0.12), ('abstractions', 0.117), ('mapreduce', 0.103), ('learning', 0.096), ('articlespaper', 0.092), ('pthreads', 0.092), ('ramclouds', 0.087), ('experts', 0.084), ('expressing', 0.083), ('speci', 0.083), ('conjectures', 0.083), ('substrate', 0.08), ('compactly', 0.08), ('expressiveness', 0.08), ('provably', 0.08), ('implementing', 0.078), ('composing', 0.077), ('patterns', 0.074), ('sampling', 0.073), ('memristors', 0.073), ('mpi', 0.072), ('designing', 0.071), ('belief', 0.07), ('dataflow', 0.069), ('repeatedly', 0.068), ('imperative', 0.068), ('machine', 0.066), ('exploits', 0.066), ('quest', 0.062), ('represents', 0.061), ('contributions', 0.061), ('targeting', 0.06), ('ensuring', 0.058), ('retrieval', 0.058), ('declarative', 0.058), ('modular', 0.056), ('iterative', 0.055), ('dependencies', 0.054), ('compressed', 0.054)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="850-tfidf-1" href="../high_scalability-2010/high_scalability-2010-06-30-Paper%3A_GraphLab%3A_A_New_Framework_For_Parallel_Machine_Learning.html">850 high scalability-2010-06-30-Paper: GraphLab: A New Framework For Parallel Machine Learning</a></p>
<p>Introduction: In the never ending quest to figure out how to do something useful with never ending streams of data,  GraphLab: A New Framework For Parallel Machine Learning  wants to go beyond low-level programming, MapReduce, and dataflow languages with  a new parallel framework for ML (machine learning) which exploits the sparse structure and common computational patterns of ML algorithms. GraphLab enables ML experts to easily design and implement efﬁcient scalable parallel algorithms by composing problem speciﬁc computation, data-dependencies, and scheduling .   Our main contributions include:  
  
  A graph-based data model which simultaneously represents data and computational dependencies.   
  A set of concurrent access models which provide a range of sequential-consistency guarantees.   
  A sophisticated modular scheduling mechanism.   
  An aggregation framework to manage global state.   
   From the abstract:
  
  Designing and implementing efﬁcient, provably correct parallel machine lear</p><p>2 0.23413685 <a title="850-tfidf-2" href="../high_scalability-2012/high_scalability-2012-08-01-Prismatic_Update%3A_Machine_Learning_on_Documents_and_Users.html">1294 high scalability-2012-08-01-Prismatic Update: Machine Learning on Documents and Users</a></p>
<p>Introduction: In update to  Prismatic Architecture - Using Machine Learning on Social Networks to Figure Out What You Should Read on the Web , Jason Wolfe, even in the face of deadening fatigue from long nights spent getting their iPhone app out, has gallantly agreed to talk a little more about Primatic's approach to Machine Learning.
 
Documents and users are two areas where Prismatic applies ML (machine learning):
  ML on Documents   
 Given an HTML document:Â         
 
 learn how to extract the main text of the page (rather than the sidebar, footer, comments, etc), its title, author, best images, etc 
 determine features for relevance (e.g., what the article is about, topics, etc.) 
 
 
 The setup for most of these tasks is pretty typical. Models are trained using big batch jobs on other machines that read data from s3, save the learned parameter files to s3, and then read (and periodically refresh) the models from s3 in the ingest pipeline.  
 All of the data that flows out of the system can be</p><p>3 0.15851131 <a title="850-tfidf-3" href="../high_scalability-2012/high_scalability-2012-07-30-Prismatic_Architecture_-_Using_Machine_Learning_on_Social_Networks_to_Figure_Out_What_You_Should_Read_on_the_Web_.html">1293 high scalability-2012-07-30-Prismatic Architecture - Using Machine Learning on Social Networks to Figure Out What You Should Read on the Web </a></p>
<p>Introduction: This post on  Prismatic ’s Architecture is adapted from an email conversation with Prismatic programmer  Jason Wolfe  .  What should you read on the web today? Any thoroughly modern person must solve this dilemma every day, usually using some occult process to divine what’s important in their many feeds: Twitter, RSS, Facebook, Pinterest, G+, email, Techmeme, and an uncountable numbers of other information sources.  Jason Wolfe from Prismatic has generously agreed to describe their thoroughly modern solution for answering the “what to read question” using lots of sexy words like Machine Learning, Social Graphs, BigData, functional programming, and in-memory real-time feed processing. The result is possibly even more occult, but this or something very much like it will be how we meet the challenge of finding interesting topics and stories hidden inside infinitely deep pools of information.  A couple of things stand out about Prismatic. They want you to know that Prismatic is being built</p><p>4 0.15084456 <a title="850-tfidf-4" href="../high_scalability-2010/high_scalability-2010-06-09-Paper%3A_Propagation_Networks%3A_A_Flexible_and_Expressive_Substrate_for_Computation_.html">839 high scalability-2010-06-09-Paper: Propagation Networks: A Flexible and Expressive Substrate for Computation </a></p>
<p>Introduction: Alexey Radul in his fascinating 174 page dissertation  Propagation Networks: A Flexible and Expressive Substrate for Computation , offers to help us  break free of the tyranny of linear time by arranging computation as a network of autonomous but interconnected machines .  We can do this by  organizing computation as a network of interconnected machines of some kind, each of which is free to run when it pleases, propagating  information around the network as proves possible. The consequence of this freedom is that the structure of the aggregate does not impose an order of time.  The abstract from his thesis is : 
  In this dissertation I propose a shift in the foundations of computation. Modern programming systems are not expressive enough. The traditional image of a single computer that has global effects on a large memory is too restrictive. The propagation paradigm replaces this with computing by networks of local, independent, stateless machines interconnected with stateful storage</p><p>5 0.12102187 <a title="850-tfidf-5" href="../high_scalability-2010/high_scalability-2010-04-14-Parallel_Information_Retrieval_and_Other_Search_Engine_Goodness.html">810 high scalability-2010-04-14-Parallel Information Retrieval and Other Search Engine Goodness</a></p>
<p>Introduction: Parallel Information Retrieval  is a sample chapter in what appears to be a book-in-progress titled  Information Retrieval Implementing and Evaluation Search Engines  by  Stefan B端ttcher , Google Inc and  Charles L. A. Clarke,   Gordon V. Cormack , both of the University of Waterloo. The full table of contents is on-line and looks to be really interesting:  Information retrieval is the foundation for modern search engines. 			This text offers an introduction to the core topics underlying 			modern search technologies, including algorithms, data structures, 			indexing, retrieval, and evaluation. The emphasis is on  implementation 			and experimentation; each chapter includes exercises and suggestions 			for student projects .
 
Currently available is the full text of chapters: Introduction, Basic Techniques, Static Inverted Indices, Index Compression, and Parallel Information Retrieval. Parallel Information Retrieval is really meaty:
  

Information retrieval systems often have to deal</p><p>6 0.10940356 <a title="850-tfidf-6" href="../high_scalability-2012/high_scalability-2012-04-26-Akaros_-_an_open_source_operating_system_for_manycore_architectures.html">1234 high scalability-2012-04-26-Akaros - an open source operating system for manycore architectures</a></p>
<p>7 0.10866631 <a title="850-tfidf-7" href="../high_scalability-2010/high_scalability-2010-03-30-Running_Large_Graph_Algorithms_-_Evaluation_of_Current_State-of-the-Art_and_Lessons_Learned.html">801 high scalability-2010-03-30-Running Large Graph Algorithms - Evaluation of Current State-of-the-Art and Lessons Learned</a></p>
<p>8 0.10444107 <a title="850-tfidf-8" href="../high_scalability-2009/high_scalability-2009-04-26-Map-Reduce_for_Machine_Learning_on_Multicore.html">581 high scalability-2009-04-26-Map-Reduce for Machine Learning on Multicore</a></p>
<p>9 0.10157449 <a title="850-tfidf-9" href="../high_scalability-2008/high_scalability-2008-10-04-Is_MapReduce_going_mainstream%3F.html">401 high scalability-2008-10-04-Is MapReduce going mainstream?</a></p>
<p>10 0.10114612 <a title="850-tfidf-10" href="../high_scalability-2009/high_scalability-2009-05-31-Parallel_Programming_for_real-world.html">612 high scalability-2009-05-31-Parallel Programming for real-world</a></p>
<p>11 0.090601623 <a title="850-tfidf-11" href="../high_scalability-2013/high_scalability-2013-12-06-Stuff_The_Internet_Says_On_Scalability_For_December_6th%2C_2013.html">1559 high scalability-2013-12-06-Stuff The Internet Says On Scalability For December 6th, 2013</a></p>
<p>12 0.075317889 <a title="850-tfidf-12" href="../high_scalability-2010/high_scalability-2010-05-05-How_will_memristors_change_everything%3F_.html">823 high scalability-2010-05-05-How will memristors change everything? </a></p>
<p>13 0.075285681 <a title="850-tfidf-13" href="../high_scalability-2008/high_scalability-2008-04-23-Behind_The_Scenes_of_Google_Scalability.html">309 high scalability-2008-04-23-Behind The Scenes of Google Scalability</a></p>
<p>14 0.074552685 <a title="850-tfidf-14" href="../high_scalability-2013/high_scalability-2013-10-04-Stuff_The_Internet_Says_On_Scalability_For_October_4th%2C_2013.html">1527 high scalability-2013-10-04-Stuff The Internet Says On Scalability For October 4th, 2013</a></p>
<p>15 0.072651155 <a title="850-tfidf-15" href="../high_scalability-2009/high_scalability-2009-08-20-Dependency_Injection_and_AOP_frameworks_for_.NET_.html">685 high scalability-2009-08-20-Dependency Injection and AOP frameworks for .NET </a></p>
<p>16 0.07114923 <a title="850-tfidf-16" href="../high_scalability-2011/high_scalability-2011-09-02-Stuff_The_Internet_Says_On_Scalability_For_September_2%2C_2011.html">1109 high scalability-2011-09-02-Stuff The Internet Says On Scalability For September 2, 2011</a></p>
<p>17 0.069749989 <a title="850-tfidf-17" href="../high_scalability-2013/high_scalability-2013-02-14-When_all_the_Program%27s_a_Graph_-_Prismatic%27s_Plumbing_Library.html">1406 high scalability-2013-02-14-When all the Program's a Graph - Prismatic's Plumbing Library</a></p>
<p>18 0.069381028 <a title="850-tfidf-18" href="../high_scalability-2008/high_scalability-2008-10-01-The_Pattern_Bible_for_Distributed_Computing.html">400 high scalability-2008-10-01-The Pattern Bible for Distributed Computing</a></p>
<p>19 0.069257438 <a title="850-tfidf-19" href="../high_scalability-2011/high_scalability-2011-01-14-Stuff_The_Internet_Says_On_Scalability_For_January_14%2C_2011.html">973 high scalability-2011-01-14-Stuff The Internet Says On Scalability For January 14, 2011</a></p>
<p>20 0.068538912 <a title="850-tfidf-20" href="../high_scalability-2009/high_scalability-2009-01-04-Paper%3A_MapReduce%3A_Simplified_Data_Processing_on_Large_Clusters.html">483 high scalability-2009-01-04-Paper: MapReduce: Simplified Data Processing on Large Clusters</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.092), (1, 0.039), (2, 0.031), (3, 0.069), (4, -0.015), (5, 0.075), (6, 0.003), (7, 0.043), (8, -0.024), (9, 0.078), (10, 0.03), (11, 0.004), (12, -0.008), (13, -0.045), (14, 0.004), (15, -0.035), (16, 0.001), (17, 0.003), (18, 0.062), (19, 0.027), (20, 0.0), (21, -0.009), (22, -0.06), (23, 0.011), (24, -0.03), (25, 0.006), (26, -0.005), (27, 0.013), (28, 0.006), (29, 0.03), (30, 0.02), (31, 0.067), (32, -0.019), (33, 0.045), (34, -0.019), (35, -0.059), (36, 0.06), (37, -0.047), (38, 0.048), (39, 0.041), (40, -0.012), (41, -0.005), (42, -0.044), (43, -0.008), (44, 0.004), (45, 0.007), (46, -0.049), (47, -0.021), (48, -0.011), (49, -0.036)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95942563 <a title="850-lsi-1" href="../high_scalability-2010/high_scalability-2010-06-30-Paper%3A_GraphLab%3A_A_New_Framework_For_Parallel_Machine_Learning.html">850 high scalability-2010-06-30-Paper: GraphLab: A New Framework For Parallel Machine Learning</a></p>
<p>Introduction: In the never ending quest to figure out how to do something useful with never ending streams of data,  GraphLab: A New Framework For Parallel Machine Learning  wants to go beyond low-level programming, MapReduce, and dataflow languages with  a new parallel framework for ML (machine learning) which exploits the sparse structure and common computational patterns of ML algorithms. GraphLab enables ML experts to easily design and implement efﬁcient scalable parallel algorithms by composing problem speciﬁc computation, data-dependencies, and scheduling .   Our main contributions include:  
  
  A graph-based data model which simultaneously represents data and computational dependencies.   
  A set of concurrent access models which provide a range of sequential-consistency guarantees.   
  A sophisticated modular scheduling mechanism.   
  An aggregation framework to manage global state.   
   From the abstract:
  
  Designing and implementing efﬁcient, provably correct parallel machine lear</p><p>2 0.80897504 <a title="850-lsi-2" href="../high_scalability-2009/high_scalability-2009-04-26-Map-Reduce_for_Machine_Learning_on_Multicore.html">581 high scalability-2009-04-26-Map-Reduce for Machine Learning on Multicore</a></p>
<p>Introduction: We are at the beginning of the multicore era. Computers will have increasingly many cores (processors), but there is still no good programming framework for these architectures, and thus no simple and unified way for machine learning to take advantage of the potential speed up.      In this paper, we develop a broadly applicable parallel programming method, one that is easily applied to many different learning algorithms. Our work is in distinct contrast to the tradition in machine learning of designing (often ingenious) ways to speed up a single algorithm at a time.      Specifically, we show that algorithms that fit the Statistical Query model can be written in a certain “summation form,” which allows them to be easily parallelized on multicore computers. We adapt Google’s map-reduce  paradigm to demonstrate this parallel speed up technique on a variety of learning algorithms including locally weighted linear regression (LWLR), k-means, logistic regression (LR), naive Bayes (NB), SVM</p><p>3 0.74740356 <a title="850-lsi-3" href="../high_scalability-2009/high_scalability-2009-05-06-Dyrad.html">591 high scalability-2009-05-06-Dyrad</a></p>
<p>Introduction: The Dryad Project is investigating programming models for writing parallel and distributed programs to scale from a small cluster to a large data-center.</p><p>4 0.71370107 <a title="850-lsi-4" href="../high_scalability-2008/high_scalability-2008-09-03-MapReduce_framework_Disco.html">376 high scalability-2008-09-03-MapReduce framework Disco</a></p>
<p>Introduction: Disco  is an open-source implementation of the MapReduce framework for distributed computing.  It was started at  Nokia Research Center  as a lightweight framework for rapid scripting of distributed data processing tasks.   The Disco core is written in Erlang. The MapReduce jobs in Disco are natively described as Python programs, which makes it possible to express complex algorithmic and data processing tasks often only in tens of lines of code.</p><p>5 0.69870228 <a title="850-lsi-5" href="../high_scalability-2008/high_scalability-2008-10-04-Is_MapReduce_going_mainstream%3F.html">401 high scalability-2008-10-04-Is MapReduce going mainstream?</a></p>
<p>Introduction: Compares MapReduce to other parallel processing approaches and suggests new paradigm for clouds and grids</p><p>6 0.69748914 <a title="850-lsi-6" href="../high_scalability-2010/high_scalability-2010-06-18-Paper%3A_The_Declarative_Imperative%3A_Experiences_and_Conjectures_in_Distributed_Logic.html">844 high scalability-2010-06-18-Paper: The Declarative Imperative: Experiences and Conjectures in Distributed Logic</a></p>
<p>7 0.67277491 <a title="850-lsi-7" href="../high_scalability-2013/high_scalability-2013-09-05-Paper%3A_MillWheel%3A_Fault-Tolerant_Stream_Processing_at_Internet_Scale.html">1512 high scalability-2013-09-05-Paper: MillWheel: Fault-Tolerant Stream Processing at Internet Scale</a></p>
<p>8 0.67243379 <a title="850-lsi-8" href="../high_scalability-2010/high_scalability-2010-06-09-Paper%3A_Propagation_Networks%3A_A_Flexible_and_Expressive_Substrate_for_Computation_.html">839 high scalability-2010-06-09-Paper: Propagation Networks: A Flexible and Expressive Substrate for Computation </a></p>
<p>9 0.66164851 <a title="850-lsi-9" href="../high_scalability-2011/high_scalability-2011-02-02-Piccolo_-_Building_Distributed_Programs_that_are_11x_Faster_than_Hadoop.html">983 high scalability-2011-02-02-Piccolo - Building Distributed Programs that are 11x Faster than Hadoop</a></p>
<p>10 0.66040254 <a title="850-lsi-10" href="../high_scalability-2009/high_scalability-2009-05-31-Parallel_Programming_for_real-world.html">612 high scalability-2009-05-31-Parallel Programming for real-world</a></p>
<p>11 0.66033167 <a title="850-lsi-11" href="../high_scalability-2012/high_scalability-2012-06-27-Paper%3A_Logic_and_Lattices_for_Distributed_Programming.html">1273 high scalability-2012-06-27-Paper: Logic and Lattices for Distributed Programming</a></p>
<p>12 0.65528709 <a title="850-lsi-12" href="../high_scalability-2010/high_scalability-2010-04-14-Parallel_Information_Retrieval_and_Other_Search_Engine_Goodness.html">810 high scalability-2010-04-14-Parallel Information Retrieval and Other Search Engine Goodness</a></p>
<p>13 0.65087372 <a title="850-lsi-13" href="../high_scalability-2009/high_scalability-2009-06-22-Improving_performance_and_scalability_with_DDD.html">635 high scalability-2009-06-22-Improving performance and scalability with DDD</a></p>
<p>14 0.63782036 <a title="850-lsi-14" href="../high_scalability-2009/high_scalability-2009-01-04-Paper%3A_MapReduce%3A_Simplified_Data_Processing_on_Large_Clusters.html">483 high scalability-2009-01-04-Paper: MapReduce: Simplified Data Processing on Large Clusters</a></p>
<p>15 0.63659102 <a title="850-lsi-15" href="../high_scalability-2011/high_scalability-2011-09-28-Pursue_robust_indefinite_scalability_with_the_Movable_Feast_Machine.html">1127 high scalability-2011-09-28-Pursue robust indefinite scalability with the Movable Feast Machine</a></p>
<p>16 0.6254124 <a title="850-lsi-16" href="../high_scalability-2008/high_scalability-2008-01-25-Google%3A_Introduction_to_Distributed_System_Design.html">223 high scalability-2008-01-25-Google: Introduction to Distributed System Design</a></p>
<p>17 0.62019897 <a title="850-lsi-17" href="../high_scalability-2009/high_scalability-2009-07-21-Paper%3A_Parallelizing_the_Web_Browser.html">660 high scalability-2009-07-21-Paper: Parallelizing the Web Browser</a></p>
<p>18 0.61840045 <a title="850-lsi-18" href="../high_scalability-2011/high_scalability-2011-11-03-Paper%3A_G2_%3A_A_Graph_Processing_System_for_Diagnosing_Distributed_Systems.html">1136 high scalability-2011-11-03-Paper: G2 : A Graph Processing System for Diagnosing Distributed Systems</a></p>
<p>19 0.60999209 <a title="850-lsi-19" href="../high_scalability-2014/high_scalability-2014-05-01-Paper%3A_Can_Programming_Be_Liberated_From_The_Von_Neumann_Style%3F_.html">1641 high scalability-2014-05-01-Paper: Can Programming Be Liberated From The Von Neumann Style? </a></p>
<p>20 0.60974514 <a title="850-lsi-20" href="../high_scalability-2009/high_scalability-2009-01-05-Lessons_Learned_at_208K%3A_Towards_Debugging_Millions_of_Cores.html">484 high scalability-2009-01-05-Lessons Learned at 208K: Towards Debugging Millions of Cores</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.084), (2, 0.182), (10, 0.032), (47, 0.014), (59, 0.228), (61, 0.1), (73, 0.028), (79, 0.114), (85, 0.014), (92, 0.06), (94, 0.036)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.89105231 <a title="850-lda-1" href="../high_scalability-2012/high_scalability-2012-09-15-4_Reasons_Facebook_Dumped_HTML5_and_Went_Native.html">1323 high scalability-2012-09-15-4 Reasons Facebook Dumped HTML5 and Went Native</a></p>
<p>Introduction: Facebook made quite a splash when they released their  native iOS app , not because of their app per se, but because of their conclusion that their  biggest mistake was betting on HTML5 , so they had to go native.
 
As you might imagine this was a bit like telling a Great White Shark that its bark is worse than its bite.  A  common refrain  was Facebook simply had made a bad HTML5 site, not that HTML5 itself is bad, as plenty of other vendors have made slick well performing mobile sites.
 
An interesting and relevant conversation given the rising butt kickery of mobile. But we were lacking details. Now we aren't. If you were wondering just why Facebook ditched HTML5, Tobie Langel in  Perf Feedback - What's slowing down Mobile Facebook , lists out the reasons:
  
  Tooling / Developer APIs . Most importantly, the lack of tooling to track down memory problems.  
  Scrolling performance.  Scrolling must be fast and smooth and full featured. It's not. 
  GPU.  A clunky API and black box ap</p><p>2 0.87306345 <a title="850-lda-2" href="../high_scalability-2013/high_scalability-2013-10-23-Strategy%3A_Use_Linux_Taskset_to_Pin_Processes_or_Let_the_OS_Schedule_It%3F.html">1536 high scalability-2013-10-23-Strategy: Use Linux Taskset to Pin Processes or Let the OS Schedule It?</a></p>
<p>Introduction: This question comes from Ulysses on an  interesting thread  from the Mechanical Sympathy news group, especially given how multiple processors are now the norm:
 
Ulysses:
   
 On an 8xCPU Linux instance,  is it at all advantageous to use the Linux taskset command to pin an 8xJVM process set (co-ordinated as a www.infinispan.org distributed cache/data grid) to a specific CPU affinity set  (i.e. pin JVM0 process to CPU 0, JVM1 process to CPU1, ...., JVM7process to CPU 7) vs. just letting the Linux OS use its default mechanism for provisioning the 8xJVM process set to the available CPUs? 
 In effrort to seek an optimal point (in the full event space), what are the conceptual trade-offs in considering "searching" each permutation of provisioning an 8xJVM process set to an 8xCPU set via taskset? 
   
Given  taskset  is they key to the question, it would help to have a definition:
  

Used to set or retrieve the CPU affinity of a running process given its PID or to launch a new COMMAND with</p><p>same-blog 3 0.87293184 <a title="850-lda-3" href="../high_scalability-2010/high_scalability-2010-06-30-Paper%3A_GraphLab%3A_A_New_Framework_For_Parallel_Machine_Learning.html">850 high scalability-2010-06-30-Paper: GraphLab: A New Framework For Parallel Machine Learning</a></p>
<p>Introduction: In the never ending quest to figure out how to do something useful with never ending streams of data,  GraphLab: A New Framework For Parallel Machine Learning  wants to go beyond low-level programming, MapReduce, and dataflow languages with  a new parallel framework for ML (machine learning) which exploits the sparse structure and common computational patterns of ML algorithms. GraphLab enables ML experts to easily design and implement efﬁcient scalable parallel algorithms by composing problem speciﬁc computation, data-dependencies, and scheduling .   Our main contributions include:  
  
  A graph-based data model which simultaneously represents data and computational dependencies.   
  A set of concurrent access models which provide a range of sequential-consistency guarantees.   
  A sophisticated modular scheduling mechanism.   
  An aggregation framework to manage global state.   
   From the abstract:
  
  Designing and implementing efﬁcient, provably correct parallel machine lear</p><p>4 0.86822027 <a title="850-lda-4" href="../high_scalability-2012/high_scalability-2012-03-29-Strategy%3A_Exploit_Processor_Affinity_for_High_and_Predictable_Performance.html">1218 high scalability-2012-03-29-Strategy: Exploit Processor Affinity for High and Predictable Performance</a></p>
<p>Introduction: Martin Thompson wrote a really interesting  article  on the beneficial performance impact of taking advantage of  Processor Affinity :
  

The interesting thing I've observed is that the unpinned test will follow a step function of unpredictable performance.  Across many runs I've seen different patterns but all similar in this step function nature.  For the pinned tests I get consistent throughput with no step pattern and always the greatest throughput.

  
The idea is by assigning a thread to a particular CPU that when a thread is rescheduled to run on the same CPU, it can take advantage of the "accumulated  state in the processor, including instructions and data in the cache."  With multi-core chips the norm now, you may want to decide for yourself how to assign work to cores and not let the OS do it for you. The results are surprisingly strong.</p><p>5 0.86382002 <a title="850-lda-5" href="../high_scalability-2014/high_scalability-2014-01-20-8_Ways_Stardog_Made_its_Database_Insanely_Scalable.html">1582 high scalability-2014-01-20-8 Ways Stardog Made its Database Insanely Scalable</a></p>
<p>Introduction: Stardog  makes a commercial graph database that is a great example of what can be accomplished with a scale-up strategy on BigIron. In a  recent article  StarDog described how they made their new 2.1 release insanely scalable, improving query scalability by about 3 orders of magnitude and it can now handle 50 billion triples on a $10,000 server with 32 cores and 256 GB RAM. It can also load 20B datasets at 300,000 triples per second. 
 
What did they do that you can also do?
  
  Avoid locks by using non-blocking algorithms and data structures . For example, moving from BitSet to ConcurrentLinkedQueue. 
  Use ThreadLocal aggressively to reduce thread contention and avoid synchronization . 
  Batch LRU evictions in a single thread . Triggered by several LRU caches becoming problematic when evictions were being swamped by additions. Downside is batching increases memory pressure and GC times. 
  Move to SHA1 for hashing URIs, bnodes, and literal values . Making hash collisions nearly imp</p><p>6 0.86248517 <a title="850-lda-6" href="../high_scalability-2012/high_scalability-2012-08-30-Dramatically_Improving_Performance_by_Debugging_Brutally_Complex_Prolems.html">1314 high scalability-2012-08-30-Dramatically Improving Performance by Debugging Brutally Complex Prolems</a></p>
<p>7 0.83935004 <a title="850-lda-7" href="../high_scalability-2009/high_scalability-2009-03-11-13_Screencasts_on_How_to_Scale_Rails.html">530 high scalability-2009-03-11-13 Screencasts on How to Scale Rails</a></p>
<p>8 0.82645738 <a title="850-lda-8" href="../high_scalability-2009/high_scalability-2009-07-16-Scalable_Web_Architectures_and_Application_State.html">656 high scalability-2009-07-16-Scalable Web Architectures and Application State</a></p>
<p>9 0.8051514 <a title="850-lda-9" href="../high_scalability-2012/high_scalability-2012-07-11-FictionPress%3A_Publishing_6_Million_Works_of_Fiction_on_the_Web.html">1281 high scalability-2012-07-11-FictionPress: Publishing 6 Million Works of Fiction on the Web</a></p>
<p>10 0.78318924 <a title="850-lda-10" href="../high_scalability-2014/high_scalability-2014-04-18-Stuff_The_Internet_Says_On_Scalability_For_April_18th%2C_2014.html">1634 high scalability-2014-04-18-Stuff The Internet Says On Scalability For April 18th, 2014</a></p>
<p>11 0.7809639 <a title="850-lda-11" href="../high_scalability-2009/high_scalability-2009-05-28-Scaling_PostgreSQL_using_CUDA.html">609 high scalability-2009-05-28-Scaling PostgreSQL using CUDA</a></p>
<p>12 0.7721101 <a title="850-lda-12" href="../high_scalability-2012/high_scalability-2012-06-15-Cloud_Bursting_between_AWS_and_Rackspace.html">1264 high scalability-2012-06-15-Cloud Bursting between AWS and Rackspace</a></p>
<p>13 0.75635982 <a title="850-lda-13" href="../high_scalability-2011/high_scalability-2011-05-11-Troubleshooting_response_time_problems_%E2%80%93_why_you_cannot_trust_your_system_metrics.html">1038 high scalability-2011-05-11-Troubleshooting response time problems – why you cannot trust your system metrics</a></p>
<p>14 0.74035954 <a title="850-lda-14" href="../high_scalability-2008/high_scalability-2008-07-18-Robert_Scoble%27s_Rules_for_Successfully_Scaling_Startups.html">352 high scalability-2008-07-18-Robert Scoble's Rules for Successfully Scaling Startups</a></p>
<p>15 0.73356569 <a title="850-lda-15" href="../high_scalability-2009/high_scalability-2009-03-11-Sharding_and_Connection_Pools.html">532 high scalability-2009-03-11-Sharding and Connection Pools</a></p>
<p>16 0.72717363 <a title="850-lda-16" href="../high_scalability-2010/high_scalability-2010-02-19-Twitter%E2%80%99s_Plan_to_Analyze_100_Billion_Tweets.html">780 high scalability-2010-02-19-Twitter’s Plan to Analyze 100 Billion Tweets</a></p>
<p>17 0.72607619 <a title="850-lda-17" href="../high_scalability-2009/high_scalability-2009-09-19-Space_Based_Programming_in_.NET.html">709 high scalability-2009-09-19-Space Based Programming in .NET</a></p>
<p>18 0.72572744 <a title="850-lda-18" href="../high_scalability-2014/high_scalability-2014-04-23-Here%27s_a_1300_Year_Old_Solution_to_Resilience_-_Rebuild%2C_Rebuild%2C_Rebuild.html">1636 high scalability-2014-04-23-Here's a 1300 Year Old Solution to Resilience - Rebuild, Rebuild, Rebuild</a></p>
<p>19 0.72563058 <a title="850-lda-19" href="../high_scalability-2011/high_scalability-2011-09-07-What_Google_App_Engine_Price_Changes_Say_About_the_Future_of_Web_Architecture.html">1112 high scalability-2011-09-07-What Google App Engine Price Changes Say About the Future of Web Architecture</a></p>
<p>20 0.72423464 <a title="850-lda-20" href="../high_scalability-2008/high_scalability-2008-07-26-Google%27s_Paxos_Made_Live_%E2%80%93_An_Engineering_Perspective.html">357 high scalability-2008-07-26-Google's Paxos Made Live – An Engineering Perspective</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
