<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>795 high scalability-2010-03-16-1 Billion Reasons Why Adobe Chose HBase </title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2010" href="../home/high_scalability-2010_home.html">high_scalability-2010</a> <a title="high_scalability-2010-795" href="#">high_scalability-2010-795</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>795 high scalability-2010-03-16-1 Billion Reasons Why Adobe Chose HBase </h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2010-795-html" href="http://highscalability.com//blog/2010/3/16/1-billion-reasons-why-adobe-chose-hbase.html">html</a></p><p>Introduction: Cosmin Lehene ďťżwrote two excellent articles on Adobe's experiences with
HBase:Why we're using HBase: Part 1andWhy we're using HBase: Part 2. Adobe
needed ageneric,real-time, structured data storage and processing system that
could handle any data volume, with access times under 50ms, with no downtime
andno data loss. The article goes into great detail about their experiences
with HBase and their evaluation process, providing a "well reasoned impartial
use case from a commercial user". It talks about failure handling,
availability, write performance, read performance, random reads, sequential
scans, and consistency. One of the knocks against HBase has been it's
complexity, as it has many parts that need installation and configuration. All
is not lost according to the Adobe team:HBase is more complex than other
systems (you need Hadoop, Zookeeper, cluster machines have multiple roles). We
believe that for HBase, this is not accidental complexity and that the
argument that "HBase is not a</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Cosmin Lehene ďťżwrote two excellent articles on Adobe's experiences with HBase:Why we're using HBase: Part 1andWhy we're using HBase: Part 2. [sent-1, score-0.23]
</p><p>2 Adobe needed ageneric,real-time, structured data storage and processing system that could handle any data volume, with access times under 50ms, with no downtime andno data loss. [sent-2, score-0.226]
</p><p>3 The article goes into great detail about their experiences with HBase and their evaluation process, providing a "well reasoned impartial use case from a commercial user". [sent-3, score-0.707]
</p><p>4 It talks about failure handling, availability, write performance, read performance, random reads, sequential scans, and consistency. [sent-4, score-0.197]
</p><p>5 One of the knocks against HBase has been it's complexity, as it has many parts that need installation and configuration. [sent-5, score-0.219]
</p><p>6 All is not lost according to the Adobe team:HBase is more complex than other systems (you need Hadoop, Zookeeper, cluster machines have multiple roles). [sent-6, score-0.329]
</p><p>7 We believe that for HBase, this is not accidental complexity and that the argument that "HBase is not a good choice because it is complex" is irrelevant. [sent-7, score-0.345]
</p><p>8 Relying on decoupled components plays nice with the Unix philosophy: do one thing and do it well. [sent-9, score-0.206]
</p><p>9 Distributed storage is delegated to HDFS, so is distributed processing, cluster state goes to Zookeeper. [sent-10, score-0.327]
</p><p>10 All these systems are developed and tested separately, and are good at what they do. [sent-11, score-0.075]
</p><p>11 More than that, this allows you to scale your cluster on separate vectors. [sent-12, score-0.188]
</p><p>12 This is not optimal, but it allows for incremental investmentin either spindles, CPU or RAM. [sent-13, score-0.157]
</p><p>13 Highly recommended, especially if you need some sort of balance to the recent gush of Cassandra articles. [sent-15, score-0.125]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hbase', 0.573), ('adobe', 0.373), ('accidental', 0.153), ('cosmin', 0.153), ('impartial', 0.153), ('knocks', 0.137), ('spindles', 0.128), ('outweigh', 0.128), ('delegated', 0.128), ('reasoned', 0.128), ('scans', 0.121), ('decoupled', 0.121), ('experiences', 0.119), ('articles', 0.111), ('cluster', 0.11), ('complexity', 0.108), ('separately', 0.103), ('recommended', 0.097), ('unix', 0.094), ('philosophy', 0.092), ('roles', 0.091), ('relying', 0.091), ('hdfs', 0.089), ('goes', 0.089), ('zookeeper', 0.086), ('plays', 0.085), ('argument', 0.084), ('installation', 0.082), ('commercial', 0.081), ('downtime', 0.079), ('incremental', 0.079), ('processing', 0.078), ('allows', 0.078), ('sequential', 0.077), ('tested', 0.075), ('complex', 0.074), ('according', 0.073), ('lost', 0.072), ('wrote', 0.072), ('evaluation', 0.07), ('structured', 0.069), ('volume', 0.067), ('detail', 0.067), ('advantages', 0.066), ('balance', 0.065), ('part', 0.064), ('optimal', 0.061), ('random', 0.061), ('recent', 0.06), ('talks', 0.059)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="795-tfidf-1" href="../high_scalability-2010/high_scalability-2010-03-16-1_Billion_Reasons_Why_Adobe_Chose_HBase_.html">795 high scalability-2010-03-16-1 Billion Reasons Why Adobe Chose HBase </a></p>
<p>Introduction: Cosmin Lehene ďťżwrote two excellent articles on Adobe's experiences with
HBase:Why we're using HBase: Part 1andWhy we're using HBase: Part 2. Adobe
needed ageneric,real-time, structured data storage and processing system that
could handle any data volume, with access times under 50ms, with no downtime
andno data loss. The article goes into great detail about their experiences
with HBase and their evaluation process, providing a "well reasoned impartial
use case from a commercial user". It talks about failure handling,
availability, write performance, read performance, random reads, sequential
scans, and consistency. One of the knocks against HBase has been it's
complexity, as it has many parts that need installation and configuration. All
is not lost according to the Adobe team:HBase is more complex than other
systems (you need Hadoop, Zookeeper, cluster machines have multiple roles). We
believe that for HBase, this is not accidental complexity and that the
argument that "HBase is not a</p><p>2 0.38925591 <a title="795-tfidf-2" href="../high_scalability-2010/high_scalability-2010-11-16-Facebook%27s_New_Real-time_Messaging_System%3A_HBase_to_Store_135%2B_Billion_Messages_a_Month.html">943 high scalability-2010-11-16-Facebook's New Real-time Messaging System: HBase to Store 135+ Billion Messages a Month</a></p>
<p>Introduction: You may have read somewhere that Facebook has introduced a newSocial Inbox
integrating email, IM, SMS,  text messages, on-site Facebook messages. All-in-
all they need to store over 135 billion messages a month. Where do they store
all that stuff? Facebook's Kannan Muthukkaruppan gives the surprise answer
inThe Underlying Technology of Messages:HBase. HBase beat out MySQL,
Cassandra, and a few others.Why a surprise? Facebook created Cassandra and it
was purpose built for an inbox type application, but they found Cassandra's
eventual consistency model wasn't a good match for their new real-time
Messages product. Facebook also has an extensiveMySQL infrastructure, but they
found performance suffered as data set and indexes grew larger. And they could
have built their own, but they chose HBase.HBase is ascaleout table store
supporting very high rates of row-level updates over massive amounts of data.
Exactly what is needed for a Messaging system. HBase is also a column based
key-value sto</p><p>3 0.28570682 <a title="795-tfidf-3" href="../high_scalability-2012/high_scalability-2012-02-07-Hypertable_Routs_HBase_in_Performance_Test_--_HBase_Overwhelmed_by_Garbage_Collection.html">1189 high scalability-2012-02-07-Hypertable Routs HBase in Performance Test -- HBase Overwhelmed by Garbage Collection</a></p>
<p>Introduction: This is a guest post byDoug Judd, original creator of Hypertable and the CEO
of Hypertable, Inc.Hypertable delivers 2X better throughput in most tests --
HBase fails 41 and 167 billion record insert tests, overwhelmed by garbage
collection -- Both systems deliver similar results for random read uniform
testWe recently conducted a test comparing the performance of Hypertable
(@hypertable) version 0.9.5.5 to that of HBase (@HBase) version 0.90.4
(CDH3u2) running Zookeeper 3.3.4.  In this post, we summarize the results and
offer explanations for the discrepancies. For the full test report,
seeHypertable vs. HBase II.IntroductionHypertable and HBase are both open
source, scalable databases modeled after Google's proprietary Bigtable
database.  The primary difference between the two systems is that Hypertable
is written in C++, while HBase is written in Java.  We modeled this test after
the one described in section 7 of theBigtable paperand tuned both systems for
maximum performance.  The t</p><p>4 0.2078957 <a title="795-tfidf-4" href="../high_scalability-2011/high_scalability-2011-09-02-Stuff_The_Internet_Says_On_Scalability_For_September_2%2C_2011.html">1109 high scalability-2011-09-02-Stuff The Internet Says On Scalability For September 2, 2011</a></p>
<p>Introduction: Scale the modern way / No brush / No lather / No rub-in / Big tube 35 cents -
Drug stores / HighScalability:8868 Tweets per second during VMAs;Facebook: 250
million photos uploaded each day;Earth: 7 Billion People StrongPotent
quotables:@kevinweil: Wow, 8868 Tweets per second last night during the #VMAs.
And that's just the writes -- imagine how many reads we were
doing!@tristanbergh: #NoSQL isn't cool, it's a working kludge of existing
architectures, bowing to the current tech limits, not transcending
them@krishnan: I would love to switch the backend infra to Amazon anytime but
our top 20 customers will not allow us @ianozsvald: Learning about all the
horrible things that happen when you don't plan (@socialtiesapp) for
scalability. Trying to be creative now...After a particularly difficult
Jeopardy match, Watson asked IBM to make him a new cognitive chip so he could
continue to kick human butt. The result, a newish chip design collocates data
and computation. RAM and CPU are interconn</p><p>5 0.19230427 <a title="795-tfidf-5" href="../high_scalability-2009/high_scalability-2009-07-02-Product%3A_Hbase.html">650 high scalability-2009-07-02-Product: Hbase</a></p>
<p>Introduction: Update 3:Presentation from theNoSQL Conference:slides,video.Update 2:Jim
Wilson helps with theUnderstanding HBase and BigTableby explaining them from a
"conceptual standpoint."Update:InfoQ interview:HBase Leads Discuss Hadoop,
BigTable and Distributed Databases. "MapReduce (both Google's and Hadoop's) is
ideal for processing huge amounts of data with sizes that would not fit in a
traditional database. Neither is appropriate for transaction/single request
processing."Hbaseis the open source answer to BigTable, Google's highly
scalable distributed database. It is built on top of Hadoop (product), which
implements functionality similar to Google's GFS and Map/Reduce systems. Both
Google's GFS and Hadoop's HDFS provide a mechanism to reliably store large
amounts of data. However, there is not really a mechanism for organizing the
data and accessing only the parts that are of interest to a particular
application.Bigtable (and Hbase) provide a means for organizing and
efficiently accessing t</p><p>6 0.15722568 <a title="795-tfidf-6" href="../high_scalability-2011/high_scalability-2011-03-08-Medialets_Architecture_-__Defeating_the_Daunting_Mobile_Device_Data_Deluge.html">1000 high scalability-2011-03-08-Medialets Architecture -  Defeating the Daunting Mobile Device Data Deluge</a></p>
<p>7 0.15258594 <a title="795-tfidf-7" href="../high_scalability-2011/high_scalability-2011-03-22-Facebook%27s_New_Realtime_Analytics_System%3A_HBase_to_Process_20_Billion_Events_Per_Day.html">1008 high scalability-2011-03-22-Facebook's New Realtime Analytics System: HBase to Process 20 Billion Events Per Day</a></p>
<p>8 0.14138062 <a title="795-tfidf-8" href="../high_scalability-2011/high_scalability-2011-04-12-Caching_and_Processing_2TB_Mozilla_Crash_Reports_in_memory_with_Hazelcast.html">1020 high scalability-2011-04-12-Caching and Processing 2TB Mozilla Crash Reports in memory with Hazelcast</a></p>
<p>9 0.13550764 <a title="795-tfidf-9" href="../high_scalability-2012/high_scalability-2012-02-13-Tumblr_Architecture_-_15_Billion_Page_Views_a_Month_and_Harder_to_Scale_than_Twitter.html">1191 high scalability-2012-02-13-Tumblr Architecture - 15 Billion Page Views a Month and Harder to Scale than Twitter</a></p>
<p>10 0.13550764 <a title="795-tfidf-10" href="../high_scalability-2012/high_scalability-2012-11-19-Gone_Fishin%27%3A_Tumblr_Architecture_-_15_Billion_Page_Views_A_Month_And_Harder_To_Scale_Than_Twitter.html">1360 high scalability-2012-11-19-Gone Fishin': Tumblr Architecture - 15 Billion Page Views A Month And Harder To Scale Than Twitter</a></p>
<p>11 0.13399251 <a title="795-tfidf-11" href="../high_scalability-2013/high_scalability-2013-05-20-The_Tumblr_Architecture_Yahoo_Bought_for_a_Cool_Billion_Dollars.html">1461 high scalability-2013-05-20-The Tumblr Architecture Yahoo Bought for a Cool Billion Dollars</a></p>
<p>12 0.12839848 <a title="795-tfidf-12" href="../high_scalability-2012/high_scalability-2012-08-03-Stuff_The_Internet_Says_On_Scalability_For_August_3%2C_2012.html">1297 high scalability-2012-08-03-Stuff The Internet Says On Scalability For August 3, 2012</a></p>
<p>13 0.12723155 <a title="795-tfidf-13" href="../high_scalability-2011/high_scalability-2011-12-05-Stuff_The_Internet_Says_On_Scalability_For_December_5%2C_2011.html">1151 high scalability-2011-12-05-Stuff The Internet Says On Scalability For December 5, 2011</a></p>
<p>14 0.12708786 <a title="795-tfidf-14" href="../high_scalability-2014/high_scalability-2014-01-28-How_Next_Big_Sound_Tracks_Over_a_Trillion_Song_Plays%2C_Likes%2C_and_More_Using_a_Version_Control_System_for_Hadoop_Data.html">1586 high scalability-2014-01-28-How Next Big Sound Tracks Over a Trillion Song Plays, Likes, and More Using a Version Control System for Hadoop Data</a></p>
<p>15 0.11412726 <a title="795-tfidf-15" href="../high_scalability-2012/high_scalability-2012-12-21-Stuff_The_Internet_Says_On_Scalability_For_December_21%2C_2012.html">1375 high scalability-2012-12-21-Stuff The Internet Says On Scalability For December 21, 2012</a></p>
<p>16 0.11258804 <a title="795-tfidf-16" href="../high_scalability-2012/high_scalability-2012-06-11-Monday_Fun%3A_Seven_Databases_in_Song.html">1262 high scalability-2012-06-11-Monday Fun: Seven Databases in Song</a></p>
<p>17 0.10077517 <a title="795-tfidf-17" href="../high_scalability-2009/high_scalability-2009-07-30-Learn_How_to_Think_at_Scale.html">666 high scalability-2009-07-30-Learn How to Think at Scale</a></p>
<p>18 0.1002063 <a title="795-tfidf-18" href="../high_scalability-2008/high_scalability-2008-07-15-ZooKeeper_-_A_Reliable%2C_Scalable_Distributed_Coordination_System_.html">350 high scalability-2008-07-15-ZooKeeper - A Reliable, Scalable Distributed Coordination System </a></p>
<p>19 0.098288015 <a title="795-tfidf-19" href="../high_scalability-2010/high_scalability-2010-06-25-Hot_Scalability_Links_for_June_25%2C_2010.html">848 high scalability-2010-06-25-Hot Scalability Links for June 25, 2010</a></p>
<p>20 0.09669055 <a title="795-tfidf-20" href="../high_scalability-2013/high_scalability-2013-08-09-Stuff_The_Internet_Says_On_Scalability_For_August_9%2C_2013.html">1499 high scalability-2013-08-09-Stuff The Internet Says On Scalability For August 9, 2013</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.124), (1, 0.079), (2, -0.004), (3, 0.042), (4, 0.031), (5, 0.081), (6, 0.02), (7, -0.013), (8, 0.072), (9, 0.052), (10, 0.043), (11, 0.05), (12, 0.067), (13, -0.084), (14, -0.029), (15, 0.06), (16, 0.001), (17, -0.081), (18, -0.088), (19, -0.041), (20, -0.011), (21, 0.054), (22, -0.031), (23, -0.042), (24, -0.026), (25, -0.041), (26, 0.083), (27, 0.017), (28, -0.046), (29, -0.014), (30, -0.002), (31, 0.101), (32, 0.087), (33, -0.057), (34, 0.015), (35, 0.077), (36, 0.002), (37, 0.049), (38, 0.006), (39, -0.012), (40, 0.016), (41, 0.025), (42, 0.031), (43, 0.054), (44, -0.015), (45, 0.058), (46, 0.003), (47, 0.015), (48, 0.019), (49, -0.021)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94741893 <a title="795-lsi-1" href="../high_scalability-2010/high_scalability-2010-03-16-1_Billion_Reasons_Why_Adobe_Chose_HBase_.html">795 high scalability-2010-03-16-1 Billion Reasons Why Adobe Chose HBase </a></p>
<p>Introduction: Cosmin Lehene ďťżwrote two excellent articles on Adobe's experiences with
HBase:Why we're using HBase: Part 1andWhy we're using HBase: Part 2. Adobe
needed ageneric,real-time, structured data storage and processing system that
could handle any data volume, with access times under 50ms, with no downtime
andno data loss. The article goes into great detail about their experiences
with HBase and their evaluation process, providing a "well reasoned impartial
use case from a commercial user". It talks about failure handling,
availability, write performance, read performance, random reads, sequential
scans, and consistency. One of the knocks against HBase has been it's
complexity, as it has many parts that need installation and configuration. All
is not lost according to the Adobe team:HBase is more complex than other
systems (you need Hadoop, Zookeeper, cluster machines have multiple roles). We
believe that for HBase, this is not accidental complexity and that the
argument that "HBase is not a</p><p>2 0.75280648 <a title="795-lsi-2" href="../high_scalability-2010/high_scalability-2010-11-16-Facebook%27s_New_Real-time_Messaging_System%3A_HBase_to_Store_135%2B_Billion_Messages_a_Month.html">943 high scalability-2010-11-16-Facebook's New Real-time Messaging System: HBase to Store 135+ Billion Messages a Month</a></p>
<p>Introduction: You may have read somewhere that Facebook has introduced a newSocial Inbox
integrating email, IM, SMS,  text messages, on-site Facebook messages. All-in-
all they need to store over 135 billion messages a month. Where do they store
all that stuff? Facebook's Kannan Muthukkaruppan gives the surprise answer
inThe Underlying Technology of Messages:HBase. HBase beat out MySQL,
Cassandra, and a few others.Why a surprise? Facebook created Cassandra and it
was purpose built for an inbox type application, but they found Cassandra's
eventual consistency model wasn't a good match for their new real-time
Messages product. Facebook also has an extensiveMySQL infrastructure, but they
found performance suffered as data set and indexes grew larger. And they could
have built their own, but they chose HBase.HBase is ascaleout table store
supporting very high rates of row-level updates over massive amounts of data.
Exactly what is needed for a Messaging system. HBase is also a column based
key-value sto</p><p>3 0.73672104 <a title="795-lsi-3" href="../high_scalability-2009/high_scalability-2009-07-02-Hypertable_is_a_New_BigTable_Clone_that_Runs_on_HDFS_or_KFS.html">647 high scalability-2009-07-02-Hypertable is a New BigTable Clone that Runs on HDFS or KFS</a></p>
<p>Introduction: Update 3: Presentation from theNoSQL conference:slides,video 1,video 2.Update
2: The folks at Hypertable would like you to know that Hypertable is now
officiallysponsored by Baidu, China's Leading Search Engine.As a sponsor of
Hypertable, Baidu has committed an industrious team of engineers, numerous
servers, and support resources to improve the quality and development of the
open source technology.Update: InfoQ interview onHypertable Lead Discusses
Hadoop and Distributed Databases. Hypertable differs from HBase in that it is
a higher performance implementation of Bigtable.Skrentabloggives the heads up
onHypertable,Zvents'open-source BigTable clone. It's written in C++ and can
run on top of either HDFS or KFS. Performance looks encouraging at28M rows of
data inserted at a per-node write rate of 7mb/sec.</p><p>4 0.7017957 <a title="795-lsi-4" href="../high_scalability-2012/high_scalability-2012-02-07-Hypertable_Routs_HBase_in_Performance_Test_--_HBase_Overwhelmed_by_Garbage_Collection.html">1189 high scalability-2012-02-07-Hypertable Routs HBase in Performance Test -- HBase Overwhelmed by Garbage Collection</a></p>
<p>Introduction: This is a guest post byDoug Judd, original creator of Hypertable and the CEO
of Hypertable, Inc.Hypertable delivers 2X better throughput in most tests --
HBase fails 41 and 167 billion record insert tests, overwhelmed by garbage
collection -- Both systems deliver similar results for random read uniform
testWe recently conducted a test comparing the performance of Hypertable
(@hypertable) version 0.9.5.5 to that of HBase (@HBase) version 0.90.4
(CDH3u2) running Zookeeper 3.3.4.  In this post, we summarize the results and
offer explanations for the discrepancies. For the full test report,
seeHypertable vs. HBase II.IntroductionHypertable and HBase are both open
source, scalable databases modeled after Google's proprietary Bigtable
database.  The primary difference between the two systems is that Hypertable
is written in C++, while HBase is written in Java.  We modeled this test after
the one described in section 7 of theBigtable paperand tuned both systems for
maximum performance.  The t</p><p>5 0.63847792 <a title="795-lsi-5" href="../high_scalability-2011/high_scalability-2011-04-12-Caching_and_Processing_2TB_Mozilla_Crash_Reports_in_memory_with_Hazelcast.html">1020 high scalability-2011-04-12-Caching and Processing 2TB Mozilla Crash Reports in memory with Hazelcast</a></p>
<p>Introduction: Mozilla processes TB's of Firefox crash reports daily using HBase, Hadoop,
Python and Thrift protocol. The project is calledSocorro, a system for
collecting, processing, and displaying crash reports from clients. Today the
Socorro application stores about 2.6 million crash reports per day. During
peak traffic, it receives about 2.5K crashes per minute. In this article we
are going to demonstrate a proof of concept showing how Mozilla could
integrate Hazelcast into Socorro and achieve caching and processing 2TB of
crash reports with 50 node Hazelcast cluster. The video for the demo is
availablehere. Currently, Socorro has pythonic collectors, processors, and
middleware that communicate with HBase via the Thrift protocol. One of the
biggest limitations of the current architecture is that it is very sensitive
to latency or outages on the HBase side. If the collectors cannot store an
item in HBase then they will store it on local disk and it will not be
accessible to the processors or midd</p><p>6 0.62873191 <a title="795-lsi-6" href="../high_scalability-2009/high_scalability-2009-07-02-Product%3A_Hbase.html">650 high scalability-2009-07-02-Product: Hbase</a></p>
<p>7 0.6163137 <a title="795-lsi-7" href="../high_scalability-2012/high_scalability-2012-05-09-Cell_Architectures.html">1242 high scalability-2012-05-09-Cell Architectures</a></p>
<p>8 0.61229724 <a title="795-lsi-8" href="../high_scalability-2009/high_scalability-2009-07-02-Product%3A_Facebook%27s_Cassandra_-_A_Massive_Distributed_Store.html">649 high scalability-2009-07-02-Product: Facebook's Cassandra - A Massive Distributed Store</a></p>
<p>9 0.6004945 <a title="795-lsi-9" href="../high_scalability-2011/high_scalability-2011-09-02-Stuff_The_Internet_Says_On_Scalability_For_September_2%2C_2011.html">1109 high scalability-2011-09-02-Stuff The Internet Says On Scalability For September 2, 2011</a></p>
<p>10 0.5927397 <a title="795-lsi-10" href="../high_scalability-2013/high_scalability-2013-05-20-The_Tumblr_Architecture_Yahoo_Bought_for_a_Cool_Billion_Dollars.html">1461 high scalability-2013-05-20-The Tumblr Architecture Yahoo Bought for a Cool Billion Dollars</a></p>
<p>11 0.5920608 <a title="795-lsi-11" href="../high_scalability-2012/high_scalability-2012-02-13-Tumblr_Architecture_-_15_Billion_Page_Views_a_Month_and_Harder_to_Scale_than_Twitter.html">1191 high scalability-2012-02-13-Tumblr Architecture - 15 Billion Page Views a Month and Harder to Scale than Twitter</a></p>
<p>12 0.5920608 <a title="795-lsi-12" href="../high_scalability-2012/high_scalability-2012-11-19-Gone_Fishin%27%3A_Tumblr_Architecture_-_15_Billion_Page_Views_A_Month_And_Harder_To_Scale_Than_Twitter.html">1360 high scalability-2012-11-19-Gone Fishin': Tumblr Architecture - 15 Billion Page Views A Month And Harder To Scale Than Twitter</a></p>
<p>13 0.5869509 <a title="795-lsi-13" href="../high_scalability-2012/high_scalability-2012-08-03-Stuff_The_Internet_Says_On_Scalability_For_August_3%2C_2012.html">1297 high scalability-2012-08-03-Stuff The Internet Says On Scalability For August 3, 2012</a></p>
<p>14 0.58409852 <a title="795-lsi-14" href="../high_scalability-2012/high_scalability-2012-06-15-Stuff_The_Internet_Says_On_Scalability_For_June_15%2C_2012.html">1265 high scalability-2012-06-15-Stuff The Internet Says On Scalability For June 15, 2012</a></p>
<p>15 0.57687175 <a title="795-lsi-15" href="../high_scalability-2011/high_scalability-2011-05-17-Facebook%3A_An_Example_Canonical_Architecture_for_Scaling_Billions_of_Messages.html">1042 high scalability-2011-05-17-Facebook: An Example Canonical Architecture for Scaling Billions of Messages</a></p>
<p>16 0.57163972 <a title="795-lsi-16" href="../high_scalability-2008/high_scalability-2008-02-19-Hadoop_Getting_Closer_to_1.0_Release.html">254 high scalability-2008-02-19-Hadoop Getting Closer to 1.0 Release</a></p>
<p>17 0.56479001 <a title="795-lsi-17" href="../high_scalability-2009/high_scalability-2009-05-17-Product%3A_Hadoop.html">601 high scalability-2009-05-17-Product: Hadoop</a></p>
<p>18 0.55913758 <a title="795-lsi-18" href="../high_scalability-2009/high_scalability-2009-07-02-Product%3A_Project_Voldemort_-_A_Distributed_Database.html">651 high scalability-2009-07-02-Product: Project Voldemort - A Distributed Database</a></p>
<p>19 0.55518615 <a title="795-lsi-19" href="../high_scalability-2011/high_scalability-2011-07-08-Stuff_The_Internet_Says_On_Scalability_For_July_8%2C_2011.html">1076 high scalability-2011-07-08-Stuff The Internet Says On Scalability For July 8, 2011</a></p>
<p>20 0.55241919 <a title="795-lsi-20" href="../high_scalability-2009/high_scalability-2009-10-29-Digg_-_Looking_to_the_Future_with_Cassandra.html">732 high scalability-2009-10-29-Digg - Looking to the Future with Cassandra</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.067), (2, 0.226), (10, 0.083), (30, 0.032), (61, 0.143), (73, 0.244), (79, 0.095)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.93538195 <a title="795-lda-1" href="../high_scalability-2007/high_scalability-2007-10-18-another_approach_to_replication.html">125 high scalability-2007-10-18-another approach to replication</a></p>
<p>Introduction: File replication based on erasure codes can reduce total replicas size 2 times
and more.</p><p>2 0.9303962 <a title="795-lda-2" href="../high_scalability-2010/high_scalability-2010-11-18-Announcing_My_Webinar_on_December_14th%3A_What_Should_I_Do%3F_Choosing_SQL%2C_NoSQL_or_Both_for_Scalable_Web_Applications.html">945 high scalability-2010-11-18-Announcing My Webinar on December 14th: What Should I Do? Choosing SQL, NoSQL or Both for Scalable Web Applications</a></p>
<p>Introduction: It's time to do something a little different and for me that doesn't mean
cutting off my hair and joining a monastery, nor does it mean buying a cherry
red convertible (yet), it means doing a webinar!On December 14th, 2:00 PM -
3:00 PM EST, I'll be hosting What Should I Do? Choosing SQL, NoSQL or Both for
Scalable Web Applications.The webinar is sponsored by VoltDB, but it will be
completely vendor independent, as that's the only honor preserving and
technically accurate way of doing these things.The webinar will run about 60
minutes, with 40 minutes of speechifying and 20 minutes for questions.The
hashtag for the event on Twitter will beSQLNoSQL. I'll be monitoring that
hashtag if you have any suggestions for the webinar or if you would like to
ask questions during the webinar. The motivation for me to do the webinar was
a talk I had with another audience member at theNoSQL Evening in Palo Alto. He
said he came from a Java background and was confused about the future. His
crystal ball</p><p>3 0.93039501 <a title="795-lda-3" href="../high_scalability-2010/high_scalability-2010-12-13-Still_Time_to_Attend_My_Webinar_Tomorrow%3A_What_Should_I_Do%3F_Choosing_SQL%2C_NoSQL_or_Both_for_Scalable_Web_Applications.html">957 high scalability-2010-12-13-Still Time to Attend My Webinar Tomorrow: What Should I Do? Choosing SQL, NoSQL or Both for Scalable Web Applications</a></p>
<p>Introduction: It's time to do something a little different and for me that doesn't mean
cutting off my hair and joining a monastery, nor does it mean buying a cherry
red convertible (yet), it means doing a webinar!On December 14th, 2:00 PM -
3:00 PM EST, I'll be hosting What Should I Do? Choosing SQL, NoSQL or Both for
Scalable Web Applications.The webinar is sponsored by VoltDB, but it will be
completely vendor independent, as that's the only honor preserving and
technically accurate way of doing these things.The webinar will run about 60
minutes, with 40 minutes of speechifying and 20 minutes for questions.The
hashtag for the event on Twitter will beSQLNoSQL. I'll be monitoring that
hashtag if you have any suggestions for the webinar or if you would like to
ask questions during the webinar. The motivation for me to do the webinar was
a talk I had with another audience member at theNoSQL Evening in Palo Alto. He
said he came from a Java background and was confused about the future. His
crystal ball</p><p>4 0.8792671 <a title="795-lda-4" href="../high_scalability-2014/high_scalability-2014-01-29-10_Things_Bitly_Should_Have_Monitored.html">1587 high scalability-2014-01-29-10 Things Bitly Should Have Monitored</a></p>
<p>Introduction: Monitor, monitor, monitor. That's the advice every startup gives once they
reach a certain size. But can you ever monitor enough? If you are Bitly and
everyone will complain when you are down, probably not.Here are 10 Things We
Forgot to Monitor from Bitly, along with good stories and copious amounts of
code snippets. Well worth reading, especially after you've already started
monitoring the lower hanging fruit.An interesting revelation from the article
is that:We run bitly split across two data centers, one is a managed
environment with DELL hardware, and the second is Amazon EC2.  Fork Rate. A
strange configuration issue caused processes to be created at a rate of
several hundred a second rather than the expected 1-10/second. Flow control
packets.  A network configuration that honors flow control packets and isn't
configured to disable them, can temporarily cause dropped traffic.Swap In/Out
Rate. Measure the right thing. It's the rate memory is swapped in/out that can
impact performa</p><p>same-blog 5 0.87094474 <a title="795-lda-5" href="../high_scalability-2010/high_scalability-2010-03-16-1_Billion_Reasons_Why_Adobe_Chose_HBase_.html">795 high scalability-2010-03-16-1 Billion Reasons Why Adobe Chose HBase </a></p>
<p>Introduction: Cosmin Lehene ďťżwrote two excellent articles on Adobe's experiences with
HBase:Why we're using HBase: Part 1andWhy we're using HBase: Part 2. Adobe
needed ageneric,real-time, structured data storage and processing system that
could handle any data volume, with access times under 50ms, with no downtime
andno data loss. The article goes into great detail about their experiences
with HBase and their evaluation process, providing a "well reasoned impartial
use case from a commercial user". It talks about failure handling,
availability, write performance, read performance, random reads, sequential
scans, and consistency. One of the knocks against HBase has been it's
complexity, as it has many parts that need installation and configuration. All
is not lost according to the Adobe team:HBase is more complex than other
systems (you need Hadoop, Zookeeper, cluster machines have multiple roles). We
believe that for HBase, this is not accidental complexity and that the
argument that "HBase is not a</p><p>6 0.85046476 <a title="795-lda-6" href="../high_scalability-2008/high_scalability-2008-01-17-Load_Balancing_of_web_server_traffic.html">217 high scalability-2008-01-17-Load Balancing of web server traffic</a></p>
<p>7 0.84504205 <a title="795-lda-7" href="../high_scalability-2012/high_scalability-2012-01-17-Paper%3A_Feeding_Frenzy%3A_Selectively_Materializing_Users%E2%80%99_Event_Feeds.html">1175 high scalability-2012-01-17-Paper: Feeding Frenzy: Selectively Materializing Users’ Event Feeds</a></p>
<p>8 0.83685952 <a title="795-lda-8" href="../high_scalability-2012/high_scalability-2012-02-20-Berkeley_DB_Architecture_-_NoSQL_Before_NoSQL_was_Cool.html">1196 high scalability-2012-02-20-Berkeley DB Architecture - NoSQL Before NoSQL was Cool</a></p>
<p>9 0.82122618 <a title="795-lda-9" href="../high_scalability-2011/high_scalability-2011-01-28-Stuff_The_Internet_Says_On_Scalability_For_January_28%2C_2011.html">980 high scalability-2011-01-28-Stuff The Internet Says On Scalability For January 28, 2011</a></p>
<p>10 0.81973439 <a title="795-lda-10" href="../high_scalability-2007/high_scalability-2007-07-26-ThemBid_Architecture.html">33 high scalability-2007-07-26-ThemBid Architecture</a></p>
<p>11 0.79674029 <a title="795-lda-11" href="../high_scalability-2014/high_scalability-2014-05-02-Stuff_The_Internet_Says_On_Scalability_For_May_2nd%2C_2014.html">1642 high scalability-2014-05-02-Stuff The Internet Says On Scalability For May 2nd, 2014</a></p>
<p>12 0.78965819 <a title="795-lda-12" href="../high_scalability-2008/high_scalability-2008-12-19-Gigaspaces_curbs_latency_outliers_with_Java_Real_Time.html">471 high scalability-2008-12-19-Gigaspaces curbs latency outliers with Java Real Time</a></p>
<p>13 0.77936643 <a title="795-lda-13" href="../high_scalability-2009/high_scalability-2009-09-19-Space_Based_Programming_in_.NET.html">709 high scalability-2009-09-19-Space Based Programming in .NET</a></p>
<p>14 0.77465892 <a title="795-lda-14" href="../high_scalability-2007/high_scalability-2007-12-11-Hosting_and_CDN_for_startup_video_sharing_site.html">181 high scalability-2007-12-11-Hosting and CDN for startup video sharing site</a></p>
<p>15 0.76964957 <a title="795-lda-15" href="../high_scalability-2011/high_scalability-2011-02-10-Database_Isolation_Levels_And_Their_Effects_on_Performance_and_Scalability.html">986 high scalability-2011-02-10-Database Isolation Levels And Their Effects on Performance and Scalability</a></p>
<p>16 0.76521957 <a title="795-lda-16" href="../high_scalability-2012/high_scalability-2012-08-28-Making_Hadoop_Run_Faster.html">1313 high scalability-2012-08-28-Making Hadoop Run Faster</a></p>
<p>17 0.76442689 <a title="795-lda-17" href="../high_scalability-2012/high_scalability-2012-10-10-Antirez%3A_You_Need_to_Think_in_Terms_of_Organizing_Your_Data_for_Fetching.html">1337 high scalability-2012-10-10-Antirez: You Need to Think in Terms of Organizing Your Data for Fetching</a></p>
<p>18 0.76196533 <a title="795-lda-18" href="../high_scalability-2008/high_scalability-2008-03-19-RAD_Lab_is_Creating_a_Datacenter_Operating_System.html">284 high scalability-2008-03-19-RAD Lab is Creating a Datacenter Operating System</a></p>
<p>19 0.759278 <a title="795-lda-19" href="../high_scalability-2012/high_scalability-2012-01-30-37signals_Still_Happily_Scaling_on_Moore_RAM_and_SSDs.html">1183 high scalability-2012-01-30-37signals Still Happily Scaling on Moore RAM and SSDs</a></p>
<p>20 0.75785011 <a title="795-lda-20" href="../high_scalability-2012/high_scalability-2012-07-25-Vertical_Scaling_Ascendant_-_How_are_SSDs_Changing__Architectures%3F.html">1291 high scalability-2012-07-25-Vertical Scaling Ascendant - How are SSDs Changing  Architectures?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
