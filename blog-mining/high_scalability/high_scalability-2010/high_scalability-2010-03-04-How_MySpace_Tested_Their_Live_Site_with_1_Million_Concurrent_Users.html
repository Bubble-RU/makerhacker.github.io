<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>788 high scalability-2010-03-04-How MySpace Tested Their Live Site with 1 Million Concurrent Users</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2010" href="../home/high_scalability-2010_home.html">high_scalability-2010</a> <a title="high_scalability-2010-788" href="#">high_scalability-2010-788</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>788 high scalability-2010-03-04-How MySpace Tested Their Live Site with 1 Million Concurrent Users</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2010-788-html" href="http://highscalability.com//blog/2010/3/4/how-myspace-tested-their-live-site-with-1-million-concurrent.html">html</a></p><p>Introduction: This is a guest post by Dan Bartow, VP ofSOASTA, talking about how they pelted
MySpace with 1 million concurrent users using 800 EC2 instances. I thought
this was an interesting story because: that's a lot of users, it takes big
cajones to test your live site like that, and not everything worked out quite
as expected. I'd like to thank Dan for taking the time to write and share this
article.In December of 2009 MySpace launched a new wave of streaming music
video offerings in New Zealand, building on the previous success of MySpace
music.  These new features included the ability to watch music videos, search
for artist's videos, create lists of favorites, and more. The anticipated load
increase from a feature like this on a popular site like MySpace is huge, and
they wanted to test these features before making them live. If you manage the
infrastructure that sits behind a high traffic application you don't want any
surprises.  You want to understand your breaking points, define your cap</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 I thought this was an interesting story because: that's a lot of users, it takes big cajones to test your live site like that, and not everything worked out quite as expected. [sent-2, score-0.347]
</p><p>2 The anticipated load increase from a feature like this on a popular site like MySpace is huge, and they wanted to test these features before making them live. [sent-6, score-0.545]
</p><p>3 Testing the production infrastructure with actual anticipated load levels is the only way to understand how things will behave when peak traffic arrives. [sent-9, score-0.552]
</p><p>4 For MySpace, the goal was to test an additional 1 million concurrent users on their live site stressing the new video features. [sent-10, score-0.72]
</p><p>5 It should be noted that 1 million virtual users are only a portion of what MySpace typically has on the site during its peaks. [sent-13, score-0.356]
</p><p>6 They wanted to supplement the live traffic with test traffic to get an idea of the overall performance impact of the new launch on the entire infrastructure. [sent-14, score-0.57]
</p><p>7 In this case, the team was requesting EC2 Large instances with the following specs to act as load generators and results collectors: 7. [sent-21, score-0.516]
</p><p>8 ) below shows how the test cloud on EC2 was set up to push massive amounts of load into MySpace's datacenters. [sent-26, score-0.449]
</p><p>9 While the test is running, batches of load generators report their performance test metrics back to a single analytics service. [sent-28, score-0.763]
</p><p>10 The test was limited to using 800 EC2 instancesSOASTA is one of the largest consumers of cloud computing resources, routinely using hundreds of servers at a time across multiple cloud providers to conduct these massive load tests. [sent-32, score-0.571]
</p><p>11 Each load generator was simulating between 1,300 and 1,500 users. [sent-35, score-0.299]
</p><p>12 This level of load was about 3x what a typical CloudTest™ load generator would drive, and it put new levels of stress on the product that took some creative work by the engineering teams to solve. [sent-36, score-0.639]
</p><p>13 If you generate all of the test traffic from, say, Amazon's East coast availability zone, then you are likely going to be hitting only one Akamai point of presence. [sent-38, score-0.42]
</p><p>14 Under load, the test was generating a significant amount of data transfer and connection traffic towards a handful of Akamai datacenters. [sent-39, score-0.471]
</p><p>15 This equated to more load on those datacenters than what would probably be generated during typical peaks, but that would not necessarily be unrealistic given that this feature launch was happening for New Zealand traffic only. [sent-40, score-0.328]
</p><p>16 This stress resulted in new connections being broken or refused by Akamai at certain load levels, and generating lots of errors in the test. [sent-41, score-0.397]
</p><p>17 This is a common hurdle that needs to be overcome when generating load against production sites. [sent-42, score-0.374]
</p><p>18 This means generating load from multiple geographic locations so that the traffic is spread out over multiple datacenters. [sent-44, score-0.545]
</p><p>19 Because of the impact of the additional load, MySpace had to reposition some of their servers on-the-fly to support the features being testedDuring testing the additional virtual user traffic was stressing some of the MySpace infrastructure pretty heavily. [sent-46, score-0.71]
</p><p>20 Lessons Learned For high traffic websites, testing in production is the only way to get an accurate picture of capacity and performance. [sent-51, score-0.47]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('myspace', 0.449), ('test', 0.205), ('gb', 0.187), ('load', 0.183), ('akamai', 0.163), ('traffic', 0.145), ('testing', 0.131), ('videos', 0.124), ('capacity', 0.124), ('units', 0.121), ('generating', 0.121), ('cloudtest', 0.118), ('platformfedora', 0.118), ('soasta', 0.118), ('generator', 0.116), ('generators', 0.111), ('stressing', 0.107), ('zealand', 0.107), ('virtual', 0.102), ('geographic', 0.096), ('music', 0.093), ('stress', 0.093), ('anticipated', 0.09), ('instances', 0.085), ('favorites', 0.085), ('additional', 0.082), ('requesting', 0.08), ('thresholds', 0.08), ('artist', 0.079), ('live', 0.075), ('react', 0.071), ('production', 0.07), ('generate', 0.07), ('compute', 0.069), ('site', 0.067), ('portion', 0.065), ('levels', 0.064), ('users', 0.064), ('concurrent', 0.062), ('included', 0.062), ('servers', 0.061), ('cloud', 0.061), ('cores', 0.061), ('provisioning', 0.059), ('metrics', 0.059), ('relatively', 0.058), ('dan', 0.058), ('million', 0.058), ('specs', 0.057), ('capability', 0.056)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999923 <a title="788-tfidf-1" href="../high_scalability-2010/high_scalability-2010-03-04-How_MySpace_Tested_Their_Live_Site_with_1_Million_Concurrent_Users.html">788 high scalability-2010-03-04-How MySpace Tested Their Live Site with 1 Million Concurrent Users</a></p>
<p>Introduction: This is a guest post by Dan Bartow, VP ofSOASTA, talking about how they pelted
MySpace with 1 million concurrent users using 800 EC2 instances. I thought
this was an interesting story because: that's a lot of users, it takes big
cajones to test your live site like that, and not everything worked out quite
as expected. I'd like to thank Dan for taking the time to write and share this
article.In December of 2009 MySpace launched a new wave of streaming music
video offerings in New Zealand, building on the previous success of MySpace
music.  These new features included the ability to watch music videos, search
for artist's videos, create lists of favorites, and more. The anticipated load
increase from a feature like this on a popular site like MySpace is huge, and
they wanted to test these features before making them live. If you manage the
infrastructure that sits behind a high traffic application you don't want any
surprises.  You want to understand your breaking points, define your cap</p><p>2 0.39851591 <a title="788-tfidf-2" href="../high_scalability-2011/high_scalability-2011-03-25-Did_the_Microsoft_Stack_Kill_MySpace%3F.html">1011 high scalability-2011-03-25-Did the Microsoft Stack Kill MySpace?</a></p>
<p>Introduction: Robert Scoble wrote a fascinating case study,MySpace's death spiral: insiders
say it's due to bets on Los Angeles and Microsoft, where he reports MySpace
insiders blame the Microsoft stack on why theylost the great social network
raceto Facebook.  Does anyone know if this is true? What's the real story?I
was wondering because it doesn't seem to track with theMySpace Architecture
post that I did in 2009, where they seem happy with their choices and had
stats to back up their improvements. Why this matters is it's a fascinating
model for startups to learn from. What does it really take to succeed? Is it
the people or the stack? Is it the organization or the technology? Is it the
process or the competition? Is the quality of the site or the love of the
users? So much to consider and learn from.Some conjectures from the
article:Myspace didn't have programming talent capable of scaling the site to
compete with Facebook.Choosing the Microsoft stack made it difficult to hire
people capable of</p><p>3 0.28619975 <a title="788-tfidf-3" href="../high_scalability-2009/high_scalability-2009-02-12-MySpace_Architecture.html">511 high scalability-2009-02-12-MySpace Architecture</a></p>
<p>Introduction: Update:Presentation: Behind the Scenes at MySpace.com. Dan Farino, Chief
Systems Architect at MySpace shares details of some of MySpace's cool internal
operations tools.MySpace.com is one of the fastest growing site on the
Internet with 65 million subscribers and 260,000 new users registering each
day. Often criticized for poor performance, MySpace has had to tackle
scalability issues few other sites have faced. How did they do it?Site:
http://myspace.comInformation SourcesPresentation: Behind the Scenes at
MySpace.comInside MySpace.comPlatformASP.NET 2.0WindowsIISSQL ServerWhat's
Inside?300 million users.Pushes 100 gigabits/second to the internet. 10Gb/sec
is HTML content.4,500+ web servers windows 2003/IIS 6.0/APS.NET.1,200+ cache
servers running 64-bit Windows 2003. 16GB of objects cached in RAM.500+
database servers running 64-bit Windows and SQL Server 2005.MySpace processes
1.5 Billion page views per day and handles 2.3 million concurrent users during
the dayMembership Milestones</p><p>4 0.26872537 <a title="788-tfidf-4" href="../high_scalability-2011/high_scalability-2011-03-31-8_Lessons_We_Can_Learn_from_the_MySpace_Incident_-_Balance%2C_Vision%2C_Fearlessness.html">1014 high scalability-2011-03-31-8 Lessons We Can Learn from the MySpace Incident - Balance, Vision, Fearlessness</a></p>
<p>Introduction: A surprising amount of heat and light was generated by the wholeMicrsoft vs
MySpacediscussion. Why people feel so passionate about this I'm not quite
sure, but fortunately for us, in the best sense of the web, it generated an
amazing number of insightful comments and observations. If we stand back and
take a look at the whole incident, what can we take a way that might help us
in the future?All computer companies are technology companies first.  A
repeated theme was that you can't be an entertainment company first. You are a
technology company providing entertainment using technology. The tech can
inform the entertainment side, the entertainment side drives features, but
they really can't be separated. An awesome stack that does nothing is useless.
A great idea on a poor stack is just as useless. There's a difficult balance
that must be achieved and both management and developers must be aware that
there's something to balance.All pigs are equal. All business failures are not
technolog</p><p>5 0.22320683 <a title="788-tfidf-5" href="../high_scalability-2007/high_scalability-2007-12-28-Amazon%27s_EC2%3A_Pay_as_You_Grow_Could_Cut_Your_Costs_in_Half.html">195 high scalability-2007-12-28-Amazon's EC2: Pay as You Grow Could Cut Your Costs in Half</a></p>
<p>Introduction: Update 2: SummizeComputes Computing Resources for a Startup. Lots of nice
graphs showing Amazon is hard to beat for small machines and become less cost
efficient for well used larger machines. Long term storage costs may eat your
saving away. And out of cloud bandwidth costs are high.Update:
viaProductionScale, a nice Digital Web article onhow to setup S3 to store
media filesand how Blue Origin was able to handle 3.5 million requests and 758
GBs in bandwidth in a single day for very little $$$. Also a Right Scale
article onNetwork performance within Amazon EC2 and to Amazon S3. 75MB/s
between EC2 instances, 10.2MB/s between EC2 and S3 for download, 6.9MB/s
upload.Now that Amazon's S3 (storage service) isout of betaand EC2 (elastic
compute cloud) has added newinstance types(the class of machine you can rent)
with more CPU and more RAM, I thought it would be interesting to take a look
out how their pricing stacks up.The quick conclusion:the more you scale the
more you save. A six node co</p><p>6 0.21388459 <a title="788-tfidf-6" href="../high_scalability-2008/high_scalability-2008-05-29-Amazon_Improves_Diagonal_Scaling_Support_with_High-CPU_Instances.html">334 high scalability-2008-05-29-Amazon Improves Diagonal Scaling Support with High-CPU Instances</a></p>
<p>7 0.18524294 <a title="788-tfidf-7" href="../high_scalability-2012/high_scalability-2012-10-02-An_Epic_TripAdvisor_Update%3A_Why_Not_Run_on_the_Cloud%3F_The_Grand_Experiment..html">1331 high scalability-2012-10-02-An Epic TripAdvisor Update: Why Not Run on the Cloud? The Grand Experiment.</a></p>
<p>8 0.17134137 <a title="788-tfidf-8" href="../high_scalability-2014/high_scalability-2014-02-17-How_the_AOL.com_Architecture_Evolved_to_99.999%25_Availability%2C_8_Million_Visitors_Per_Day%2C_and_200%2C000_Requests_Per_Second.html">1597 high scalability-2014-02-17-How the AOL.com Architecture Evolved to 99.999% Availability, 8 Million Visitors Per Day, and 200,000 Requests Per Second</a></p>
<p>9 0.17120217 <a title="788-tfidf-9" href="../high_scalability-2009/high_scalability-2009-06-01-Guess_How_Many_Users_it_Takes_to_Kill_Your_Site%3F.html">614 high scalability-2009-06-01-Guess How Many Users it Takes to Kill Your Site?</a></p>
<p>10 0.1623054 <a title="788-tfidf-10" href="../high_scalability-2010/high_scalability-2010-03-16-Justin.tv%27s_Live_Video_Broadcasting_Architecture.html">796 high scalability-2010-03-16-Justin.tv's Live Video Broadcasting Architecture</a></p>
<p>11 0.16189022 <a title="788-tfidf-11" href="../high_scalability-2012/high_scalability-2012-11-15-Gone_Fishin%27%3A_Justin.Tv%27s_Live_Video_Broadcasting_Architecture.html">1359 high scalability-2012-11-15-Gone Fishin': Justin.Tv's Live Video Broadcasting Architecture</a></p>
<p>12 0.15807259 <a title="788-tfidf-12" href="../high_scalability-2009/high_scalability-2009-06-29-How_to_Succeed_at_Capacity_Planning_Without_Really_Trying_%3A__An_Interview_with_Flickr%27s_John_Allspaw_on_His_New_Book.html">643 high scalability-2009-06-29-How to Succeed at Capacity Planning Without Really Trying :  An Interview with Flickr's John Allspaw on His New Book</a></p>
<p>13 0.14417399 <a title="788-tfidf-13" href="../high_scalability-2012/high_scalability-2012-06-26-Sponsored_Post%3A_New_Relic%2C_Digital_Ocean%2C_NetDNA%2C_Torbit%2C_Reality_Check_Network%2C_Gigaspaces%2C_AiCache%2C_Logic_Monitor%2C_AppDynamics%2C_CloudSigma%2C_ManageEnine%2C_Site24x7.html">1272 high scalability-2012-06-26-Sponsored Post: New Relic, Digital Ocean, NetDNA, Torbit, Reality Check Network, Gigaspaces, AiCache, Logic Monitor, AppDynamics, CloudSigma, ManageEnine, Site24x7</a></p>
<p>14 0.14323229 <a title="788-tfidf-14" href="../high_scalability-2012/high_scalability-2012-06-05-Sponsored_Post%3A_Digital_Ocean%2C_NetDNA%2C_Torbit%2C_Velocity%2C_Reality_Check_Network%2C_Gigaspaces%2C_AiCache%2C_Logic_Monitor%2C_Attribution_Modeling%2C_AppDynamics%2C_CloudSigma%2C_ManageEnine%2C_Site24x7.html">1257 high scalability-2012-06-05-Sponsored Post: Digital Ocean, NetDNA, Torbit, Velocity, Reality Check Network, Gigaspaces, AiCache, Logic Monitor, Attribution Modeling, AppDynamics, CloudSigma, ManageEnine, Site24x7</a></p>
<p>15 0.14009781 <a title="788-tfidf-15" href="../high_scalability-2010/high_scalability-2010-05-03-MocoSpace_Architecture_-_3_Billion_Mobile_Page_Views_a_Month.html">821 high scalability-2010-05-03-MocoSpace Architecture - 3 Billion Mobile Page Views a Month</a></p>
<p>16 0.13992892 <a title="788-tfidf-16" href="../high_scalability-2011/high_scalability-2011-03-15-Sponsored_Post%3A_Schooner%2C_deviantART%2C_ScaleOut%2C_aiCache%2C_WAPT%2C_Karmasphere%2C_Kabam%2C_Newrelic%2C_Cloudkick%2C_Membase%2C_Joyent%2C_CloudSigma%2C_ManageEngine%2C_Site24x7.html">1005 high scalability-2011-03-15-Sponsored Post: Schooner, deviantART, ScaleOut, aiCache, WAPT, Karmasphere, Kabam, Newrelic, Cloudkick, Membase, Joyent, CloudSigma, ManageEngine, Site24x7</a></p>
<p>17 0.13860023 <a title="788-tfidf-17" href="../high_scalability-2011/high_scalability-2011-03-01-Sponsored_Post%3A__ScaleOut%2C_aiCache%2C_WAPT%2C_Karmasphere%2C_Kabam%2C_Opera_Solutions%2C_Newrelic%2C_Cloudkick%2C_Membase%2C_Joyent%2C_CloudSigma%2C_ManageEngine%2C_Site24x7.html">997 high scalability-2011-03-01-Sponsored Post:  ScaleOut, aiCache, WAPT, Karmasphere, Kabam, Opera Solutions, Newrelic, Cloudkick, Membase, Joyent, CloudSigma, ManageEngine, Site24x7</a></p>
<p>18 0.13774681 <a title="788-tfidf-18" href="../high_scalability-2011/high_scalability-2011-03-22-Sponsored_Post%3A_ClearStone%2C_Schooner%2C_deviantART%2C_ScaleOut%2C_aiCache%2C_WAPT%2C_Karmasphere%2C_Kabam%2C_Newrelic%2C_Cloudkick%2C_Membase%2C_Joyent%2C_CloudSigma%2C_ManageEngine%2C_Site24x7.html">1009 high scalability-2011-03-22-Sponsored Post: ClearStone, Schooner, deviantART, ScaleOut, aiCache, WAPT, Karmasphere, Kabam, Newrelic, Cloudkick, Membase, Joyent, CloudSigma, ManageEngine, Site24x7</a></p>
<p>19 0.13667294 <a title="788-tfidf-19" href="../high_scalability-2012/high_scalability-2012-11-05-Gone_Fishin%27%3A_Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_In_The_Ambient_Cloud.html">1355 high scalability-2012-11-05-Gone Fishin': Building Super Scalable Systems: Blade Runner Meets Autonomic Computing In The Ambient Cloud</a></p>
<p>20 0.13659771 <a title="788-tfidf-20" href="../high_scalability-2009/high_scalability-2009-12-16-Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_in_the_Ambient_Cloud.html">750 high scalability-2009-12-16-Building Super Scalable Systems: Blade Runner Meets Autonomic Computing in the Ambient Cloud</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.267), (1, 0.056), (2, 0.009), (3, -0.081), (4, -0.061), (5, -0.138), (6, 0.04), (7, -0.04), (8, 0.034), (9, -0.027), (10, 0.03), (11, -0.036), (12, 0.014), (13, 0.022), (14, -0.017), (15, 0.038), (16, 0.04), (17, 0.01), (18, -0.048), (19, 0.052), (20, 0.001), (21, 0.041), (22, 0.036), (23, -0.031), (24, 0.001), (25, -0.075), (26, -0.091), (27, 0.064), (28, 0.054), (29, 0.005), (30, 0.067), (31, 0.02), (32, -0.038), (33, -0.017), (34, 0.031), (35, 0.04), (36, -0.05), (37, -0.023), (38, 0.046), (39, 0.08), (40, -0.08), (41, -0.04), (42, 0.071), (43, 0.017), (44, -0.007), (45, 0.089), (46, 0.043), (47, -0.012), (48, 0.026), (49, 0.052)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96013284 <a title="788-lsi-1" href="../high_scalability-2010/high_scalability-2010-03-04-How_MySpace_Tested_Their_Live_Site_with_1_Million_Concurrent_Users.html">788 high scalability-2010-03-04-How MySpace Tested Their Live Site with 1 Million Concurrent Users</a></p>
<p>Introduction: This is a guest post by Dan Bartow, VP ofSOASTA, talking about how they pelted
MySpace with 1 million concurrent users using 800 EC2 instances. I thought
this was an interesting story because: that's a lot of users, it takes big
cajones to test your live site like that, and not everything worked out quite
as expected. I'd like to thank Dan for taking the time to write and share this
article.In December of 2009 MySpace launched a new wave of streaming music
video offerings in New Zealand, building on the previous success of MySpace
music.  These new features included the ability to watch music videos, search
for artist's videos, create lists of favorites, and more. The anticipated load
increase from a feature like this on a popular site like MySpace is huge, and
they wanted to test these features before making them live. If you manage the
infrastructure that sits behind a high traffic application you don't want any
surprises.  You want to understand your breaking points, define your cap</p><p>2 0.73283672 <a title="788-lsi-2" href="../high_scalability-2009/high_scalability-2009-06-01-Guess_How_Many_Users_it_Takes_to_Kill_Your_Site%3F.html">614 high scalability-2009-06-01-Guess How Many Users it Takes to Kill Your Site?</a></p>
<p>Introduction: Update:Here's thefirst result. Good response time until 400 users. At 1,340
users the response time was 6 seconds. And at 2000 users the site was
effectively did. An interesting point was that errors that could harm a site's
reputation started at 1000 users. Cheers to the company that had the guts to
give this a try.That which doesn't kill your site makes it stronger. Or at
least that's the capacity planning strategy John Allspaw recommends (not
really, but I'm trying to make a point here) inThe Art of Capacity
Planning:Using production traffic to define your resources ceilings in a
controlled setting allows you to see firsthand what would happen when you run
out of capacity in a particular resource. Of course I'm not suggesting that
you run your site into the ground, but better to know what your real (not
simulated) loads are while you're watching, than find out the hard way. In
addition, a lot of unexpected systemic things can happen when load increases
in a particular cluster or res</p><p>3 0.71882302 <a title="788-lsi-3" href="../high_scalability-2012/high_scalability-2012-11-22-Gone_Fishin%27%3A_PlentyOfFish_Architecture.html">1361 high scalability-2012-11-22-Gone Fishin': PlentyOfFish Architecture</a></p>
<p>Introduction: Other thanStackOverflow, PlentyOfFish is perhaps the most spectacular example
of scale-up architectures working for what your average sane person would
consider a large system. It doesn't hurt that it's also a sexy story.Update
5:PlentyOfFish Update - 6 Billion Pageviews And 32 Billion Images A
MonthUpdate 4:Jeff Atwoodcosts out Markus' scale up approach against a scale
out approach and finds scale up wanting. The discussion in the comments is as
interesting as the article. My guess is Markus doesn't want to rewrite his
software to work across a scale out cluster so even if it's more expensive
scale up works better for his needs.Update 3:POF now has 200 million imagesand
serves 10,000 images served per second. They'll be moving to a 250,000 IOPS
RamSan to handle the load. Also upgraded to a core database machine with 512
GB of RAM, 32 CPU's, SQLServer 2008 and Windows 2008.Update 2: This seems to
be aPOF Peer1 love fest infomercial. It's pretty content free, but the
production values a</p><p>4 0.71789014 <a title="788-lsi-4" href="../high_scalability-2009/high_scalability-2009-06-26-PlentyOfFish_Architecture.html">638 high scalability-2009-06-26-PlentyOfFish Architecture</a></p>
<p>Introduction: Update 5:PlentyOfFish Update - 6 Billion Pageviews And 32 Billion Images A
MonthUpdate 4:Jeff Atwoodcosts out Markus' scale up approach against a scale
out approach and finds scale up wanting. The discussion in the comments is as
interesting as the article. My guess is Markus doesn't want to rewrite his
software to work across a scale out cluster so even if it's more expensive
scale up works better for his needs.Update 3:POF now has 200 million imagesand
serves 10,000 images served per second. They'll be moving to a 250,000 IOPS
RamSan to handle the load. Also upgraded to a core database machine with 512
GB of RAM, 32 CPU's, SQLServer 2008 and Windows 2008.Update 2: This seems to
be aPOF Peer1 love fest infomercial. It's pretty content free, but the
production values are high. Lots of quirky sounds and fish swimming on the
screen.Update: by Facebook standards Read/WriteWeb says POF is worth a coolone
billion dollars. It helps to talk like Dr. Evil when saying it out
loud.PlentyOfFish i</p><p>5 0.71474272 <a title="788-lsi-5" href="../high_scalability-2009/high_scalability-2009-02-16-Handle_1_Billion_Events_Per_Day_Using_a_Memory_Grid.html">513 high scalability-2009-02-16-Handle 1 Billion Events Per Day Using a Memory Grid</a></p>
<p>Introduction: Moshe Kaplan of RockeTiershows the life cycle of an affiliate marketing
systemthat starts off as a cub handling one million events per day and ends up
a lion handling 200 million to even one billion events per day. The resulting
system uses ten commodity servers at a cost of $35,000.Mr. Kaplan's paper is
especially interesting because it documents a system architecture evolution we
may see a lot more of in the future:database centric --> cache centric -->
memory grid.As scaling and performance requirements for complicated operations
increase, leaving the entire system in memory starts to make a great deal of
sense. Why use cache at all? Why shouldn't your system be all in memory from
the start?General Approach to Evolving the System to ScaleAnalyze the system
architecture and the main business processes. Detect the main hardware
bottlenecks and the related business process causing them. Focus efforts on
points of greatest return.Rate the bottlenecks by importance and provide
immediate</p><p>6 0.70165586 <a title="788-lsi-6" href="../high_scalability-2009/high_scalability-2009-09-22-How_Ravelry_Scales_to_10_Million_Requests_Using_Rails.html">711 high scalability-2009-09-22-How Ravelry Scales to 10 Million Requests Using Rails</a></p>
<p>7 0.69069391 <a title="788-lsi-7" href="../high_scalability-2011/high_scalability-2011-05-10-Viddler_Architecture_-_7_Million_Embeds_a_Day_and_1500_Req-Sec_Peak__.html">1037 high scalability-2011-05-10-Viddler Architecture - 7 Million Embeds a Day and 1500 Req-Sec Peak  </a></p>
<p>8 0.68841511 <a title="788-lsi-8" href="../high_scalability-2008/high_scalability-2008-03-14-Problem%3A_Mobbing_the_Least_Used_Resource_Error.html">275 high scalability-2008-03-14-Problem: Mobbing the Least Used Resource Error</a></p>
<p>9 0.68725848 <a title="788-lsi-9" href="../high_scalability-2007/high_scalability-2007-12-28-Amazon%27s_EC2%3A_Pay_as_You_Grow_Could_Cut_Your_Costs_in_Half.html">195 high scalability-2007-12-28-Amazon's EC2: Pay as You Grow Could Cut Your Costs in Half</a></p>
<p>10 0.684654 <a title="788-lsi-10" href="../high_scalability-2014/high_scalability-2014-01-24-Stuff_The_Internet_Says_On_Scalability_For_January_24th%2C_2014.html">1585 high scalability-2014-01-24-Stuff The Internet Says On Scalability For January 24th, 2014</a></p>
<p>11 0.68452692 <a title="788-lsi-11" href="../high_scalability-2012/high_scalability-2012-10-02-An_Epic_TripAdvisor_Update%3A_Why_Not_Run_on_the_Cloud%3F_The_Grand_Experiment..html">1331 high scalability-2012-10-02-An Epic TripAdvisor Update: Why Not Run on the Cloud? The Grand Experiment.</a></p>
<p>12 0.68272686 <a title="788-lsi-12" href="../high_scalability-2011/high_scalability-2011-12-12-Netflix%3A_Developing%2C_Deploying%2C_and_Supporting_Software_According_to_the_Way_of_the_Cloud.html">1155 high scalability-2011-12-12-Netflix: Developing, Deploying, and Supporting Software According to the Way of the Cloud</a></p>
<p>13 0.67901999 <a title="788-lsi-13" href="../high_scalability-2012/high_scalability-2012-09-26-WordPress.com_Serves_70%2C000_req-sec_and_over_15_Gbit-sec_of_Traffic_using_NGINX.html">1329 high scalability-2012-09-26-WordPress.com Serves 70,000 req-sec and over 15 Gbit-sec of Traffic using NGINX</a></p>
<p>14 0.67835236 <a title="788-lsi-14" href="../high_scalability-2008/high_scalability-2008-02-16-S3_Failed_Because_of_Authentication_Overload.html">249 high scalability-2008-02-16-S3 Failed Because of Authentication Overload</a></p>
<p>15 0.67554355 <a title="788-lsi-15" href="../high_scalability-2009/high_scalability-2009-02-12-MySpace_Architecture.html">511 high scalability-2009-02-12-MySpace Architecture</a></p>
<p>16 0.67530519 <a title="788-lsi-16" href="../high_scalability-2007/high_scalability-2007-08-29-Skype_Failed_the_Boot_Scalability_Test%3A_Is_P2P_fundamentally_flawed%3F.html">76 high scalability-2007-08-29-Skype Failed the Boot Scalability Test: Is P2P fundamentally flawed?</a></p>
<p>17 0.67269492 <a title="788-lsi-17" href="../high_scalability-2009/high_scalability-2009-03-11-The_Implications_of_Punctuated_Scalabilium_for_Website_Architecture.html">533 high scalability-2009-03-11-The Implications of Punctuated Scalabilium for Website Architecture</a></p>
<p>18 0.67197227 <a title="788-lsi-18" href="../high_scalability-2011/high_scalability-2011-03-25-Did_the_Microsoft_Stack_Kill_MySpace%3F.html">1011 high scalability-2011-03-25-Did the Microsoft Stack Kill MySpace?</a></p>
<p>19 0.67063153 <a title="788-lsi-19" href="../high_scalability-2009/high_scalability-2009-08-31-Squarespace_Architecture_-_A_Grid_Handles_Hundreds_of_Millions_of_Requests_a_Month_.html">691 high scalability-2009-08-31-Squarespace Architecture - A Grid Handles Hundreds of Millions of Requests a Month </a></p>
<p>20 0.67022729 <a title="788-lsi-20" href="../high_scalability-2009/high_scalability-2009-05-06-Guinness_Book_of_World_Records_Anyone%3F.html">593 high scalability-2009-05-06-Guinness Book of World Records Anyone?</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.2), (2, 0.177), (10, 0.031), (30, 0.248), (38, 0.018), (40, 0.02), (51, 0.01), (61, 0.04), (77, 0.014), (79, 0.075), (85, 0.054), (94, 0.039)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.98307079 <a title="788-lda-1" href="../high_scalability-2007/high_scalability-2007-07-16-Book%3A_High_Performance_MySQL.html">16 high scalability-2007-07-16-Book: High Performance MySQL</a></p>
<p>Introduction: As users come to depend on MySQL, they find that they have to deal with issues
of reliability, scalability, and performance--issues that are not well
documented but are critical to a smoothly functioning site. This book is an
insider's guide to these little understood topics. Author Jeremy Zawodny has
managed large numbers of MySQL servers for mission-critical work at Yahoo!,
maintained years of contacts with the MySQL AB team, and presents regularly at
conferences. Jeremy and Derek have spent months experimenting, interviewing
major users of MySQL, talking to MySQL AB, benchmarking, and writing some of
their own tools in order to produce the information in this book. In High
Performance MySQL you will learn about MySQL indexing and optimization in
depth so you can make better use of these key features. You will learn
practical replication, backup, and load-balancing strategies with information
that goes beyond available tools to discuss their effects in real-life
environments. And you</p><p>2 0.97306293 <a title="788-lda-2" href="../high_scalability-2008/high_scalability-2008-04-22-Simple_NFS_failover_solution_with_symbolic_link%3F.html">308 high scalability-2008-04-22-Simple NFS failover solution with symbolic link?</a></p>
<p>Introduction: I've been trying to find a high availability file storage solution without
success. I tried GlusterFS which looks very promising but experienced problems
with stability and don't want something I can't easily control and rely on.
Other solutions are too complicated or have a SPOF.So I'm thinking of the
following setup:Two NFS servers, a primary and a warm backup. The primary
server will be rsynced with the warm backup every minute or two. I can do it
so frequently as a PHP script will know which directories have changed
recently from a database and only rsync those. Both servers will be NFS
mounted on a cluster of web servers as /mnt/nfs-primary (sym linked as
/home/websites) and /mnt/nfs-backup.I'll then use Ucarp
(http://www.ucarp.org/project/ucarp) to monitor both NFS servers availability
every couple of seconds and when one goes down, the Ucarp up script will be
set to change the symbolic link on all web servers for the /home/websites dir
from /mnt/nfs-primary to /mnt/nfs-backupThe</p><p>3 0.96513647 <a title="788-lda-3" href="../high_scalability-2009/high_scalability-2009-01-22-Heterogeneous_vs._Homogeneous_System_Architectures.html">500 high scalability-2009-01-22-Heterogeneous vs. Homogeneous System Architectures</a></p>
<p>Introduction: I follow a certain philosophy when developing system architectures. I assume
that very few systems will ever exist in a consistent form for more than a
short period of time. What constitutes a “short period of time” differs
depending on the specifics of each system, but in an effort to quantify it, I
generally find that it falls somewhere between a week and a month.The driving
forces behind the need for an ever changing architecture are largely business
requirement based. This is a side effect of the reality that software
development, in most cases, is used as a supporting role within the business
unit it serves. As business requirements (i.e. additional features, new
products, etc.) pour forth, it is the developer’s job to evolve their software
system to accommodate these requirements and provide a software based solution
to whatever problems lay ahead.Given that many businesses can be identified as
having the above characteristics, I can now begin to explain why I believe
that Hetero</p><p>4 0.96275681 <a title="788-lda-4" href="../high_scalability-2011/high_scalability-2011-04-04-Scaling_Social_Ecommerce_Architecture_Case_study.html">1016 high scalability-2011-04-04-Scaling Social Ecommerce Architecture Case study</a></p>
<p>Introduction: A recent study showed that over 92 percent of executives from leading
retailers are focusing their marketing efforts on Facebook and subsequent
applications. Furthermore, over 71 percent of users have confirmed they are
more likely to make a purchase after "liking" a brand they find online.
(source)Sears Architect Tomer Gabel provides an insightful overview on how
they built a Social Ecommerce solution for Sears.com that can handle complex
relationship quires in real time. Tomer goes through:the architectural
considerations behind their solutionwhy they chose memory over diskhow they
partitioned the data to gain scalabilitywhy they chose to execute code with
the data using GigaSpaces Map/Reduce execution frameworkhow they integrated
with Facebookwhy they chose GigaSpaces over Coherence and Terracotta for in-
memory caching and scaleIn this post I tried to summarize the main takeaway
from the interview.You can also watch the full interview (highly
recomended).Read the full storyhere</p><p>5 0.95530528 <a title="788-lda-5" href="../high_scalability-2007/high_scalability-2007-12-12-Oracle_Can_Do_Read-Write_Splitting_Too.html">182 high scalability-2007-12-12-Oracle Can Do Read-Write Splitting Too</a></p>
<p>Introduction: People sometimes wonder why Oracle isn't mentioned on this site more. Maybe it
will now asMichael Nygard reportsOracle 11g now does read/write splitting with
theirActive Data Guardproduct. Average replication latency was 1 second and
it's accomplished with standard Oracle JDBC drivers. They see a 250% increase
in transactions per service for read-write service. And a 110% improvement in
tps for read-only service was found. You see a change in hardware architecture
with the new setup. They now recommend using a primary and multiple standby
servers, a single controller per server, and a single set of disks in RAID1.
Previously the recommendation was to have a primary and secondary server with
two controllers per server and a set of mirrored disks per controller. The
changes increase performance, availability, and hardware utilization. They
also have a useful looking best practices document for High Availability
calledMaximum Availability Architecture (MAA).</p><p>6 0.94975448 <a title="788-lda-6" href="../high_scalability-2011/high_scalability-2011-02-16-Paper%3A_An_Experimental_Investigation_of_the_Akamai_Adaptive_Video_Streaming.html">991 high scalability-2011-02-16-Paper: An Experimental Investigation of the Akamai Adaptive Video Streaming</a></p>
<p>7 0.94825816 <a title="788-lda-7" href="../high_scalability-2013/high_scalability-2013-05-16-Paper%3A_Warp%3A_Multi-Key_Transactions_for_Key-Value_Stores.html">1459 high scalability-2013-05-16-Paper: Warp: Multi-Key Transactions for Key-Value Stores</a></p>
<p>8 0.94544178 <a title="788-lda-8" href="../high_scalability-2010/high_scalability-2010-02-24-Hot_Scalability_Links_for_February_24%2C_2010.html">783 high scalability-2010-02-24-Hot Scalability Links for February 24, 2010</a></p>
<p>9 0.93576515 <a title="788-lda-9" href="../high_scalability-2008/high_scalability-2008-05-29-Amazon_Improves_Diagonal_Scaling_Support_with_High-CPU_Instances.html">334 high scalability-2008-05-29-Amazon Improves Diagonal Scaling Support with High-CPU Instances</a></p>
<p>10 0.9337281 <a title="788-lda-10" href="../high_scalability-2012/high_scalability-2012-07-16-Cinchcast_Architecture_-_Producing_1%2C500_Hours_of_Audio_Every_Day.html">1284 high scalability-2012-07-16-Cinchcast Architecture - Producing 1,500 Hours of Audio Every Day</a></p>
<p>11 0.93199247 <a title="788-lda-11" href="../high_scalability-2008/high_scalability-2008-02-27-Product%3A_System_Imager_-_Automate_Deployment_and_Installs.html">263 high scalability-2008-02-27-Product: System Imager - Automate Deployment and Installs</a></p>
<p>12 0.92844856 <a title="788-lda-12" href="../high_scalability-2008/high_scalability-2008-02-25-Make_Your_Site_Run_10_Times_Faster.html">261 high scalability-2008-02-25-Make Your Site Run 10 Times Faster</a></p>
<p>13 0.92043185 <a title="788-lda-13" href="../high_scalability-2008/high_scalability-2008-05-31-Biggest_Under_Reported_Story%3A_Google%27s_BigTable_Costs_10_Times_Less_than_Amazon%27s_SimpleDB.html">336 high scalability-2008-05-31-Biggest Under Reported Story: Google's BigTable Costs 10 Times Less than Amazon's SimpleDB</a></p>
<p>same-blog 14 0.91673899 <a title="788-lda-14" href="../high_scalability-2010/high_scalability-2010-03-04-How_MySpace_Tested_Their_Live_Site_with_1_Million_Concurrent_Users.html">788 high scalability-2010-03-04-How MySpace Tested Their Live Site with 1 Million Concurrent Users</a></p>
<p>15 0.91513622 <a title="788-lda-15" href="../high_scalability-2010/high_scalability-2010-05-26-End-To-End_Performance_Study_of_Cloud_Services.html">831 high scalability-2010-05-26-End-To-End Performance Study of Cloud Services</a></p>
<p>16 0.90869659 <a title="788-lda-16" href="../high_scalability-2014/high_scalability-2014-03-24-Big%2C_Small%2C_Hot_or_Cold_-_Examples_of_Robust_Data_Pipelines_from_Stripe%2C_Tapad%2C_Etsy_and_Square.html">1618 high scalability-2014-03-24-Big, Small, Hot or Cold - Examples of Robust Data Pipelines from Stripe, Tapad, Etsy and Square</a></p>
<p>17 0.87800372 <a title="788-lda-17" href="../high_scalability-2009/high_scalability-2009-09-10-How_to_handle_so_many_socket_connection.html">699 high scalability-2009-09-10-How to handle so many socket connection</a></p>
<p>18 0.87626779 <a title="788-lda-18" href="../high_scalability-2008/high_scalability-2008-03-29-20_New_Rules_for_Faster_Web_Pages.html">291 high scalability-2008-03-29-20 New Rules for Faster Web Pages</a></p>
<p>19 0.87267482 <a title="788-lda-19" href="../high_scalability-2011/high_scalability-2011-07-18-Building_your_own_Facebook_Realtime_Analytics_System__.html">1081 high scalability-2011-07-18-Building your own Facebook Realtime Analytics System  </a></p>
<p>20 0.87131864 <a title="788-lda-20" href="../high_scalability-2008/high_scalability-2008-05-14-Scaling_an_image_upload_service.html">319 high scalability-2008-05-14-Scaling an image upload service</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
