<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>792 high scalability-2010-03-10-How FarmVille Scales - The Follow-up</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2010" href="../home/high_scalability-2010_home.html">high_scalability-2010</a> <a title="high_scalability-2010-792" href="#">high_scalability-2010-792</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>792 high scalability-2010-03-10-How FarmVille Scales - The Follow-up</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2010-792-html" href="http://highscalability.com//blog/2010/3/10/how-farmville-scales-the-follow-up.html">html</a></p><p>Introduction: Several readers had follow-up questions in response to  How FarmVille Scales to Harvest 75 Million Players a Month . Here are Luke's response to those questions (and a few of mine).
   How does social networking makes things easier or harder?    
 The primary interesting aspect of social networking games is how you wind up with a graph of connected users who need to be access each other's data on a frequent basis. This makes the overall dataset difficult if not impossible to partition. 
   What are examples of the Facebook calls you try to avoid and how they impact game play?   
 We can make a call for facebook friend data to retrieve information about your friends playing the game. Normally, we show a friend ladder at the bottom of the game that shows friend information, including name and facebook photo.   
   Can you say where your cache is, what form it takes, and how much cached there is? Do you have a peering relationship with Facebook, as one might expect at that bandwidth?</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The primary interesting aspect of social networking games is how you wind up with a graph of connected users who need to be access each other's data on a frequent basis. [sent-4, score-0.198]
</p><p>2 What are examples of the Facebook calls you try to avoid and how they impact game play? [sent-6, score-0.372]
</p><p>3 We can make a call for facebook friend data to retrieve information about your friends playing the game. [sent-7, score-0.687]
</p><p>4 Normally, we show a friend ladder at the bottom of the game that shows friend information, including name and facebook photo. [sent-8, score-1.955]
</p><p>5 What is the impact in game play when functionality is disabled in response to load? [sent-13, score-0.546]
</p><p>6 What users will see is that some part of the application doesn't work as it normally does. [sent-14, score-0.198]
</p><p>7 For example, on our neighbors page and in a friend ladder at the bottom of the flash application, you can see a list of your friend playing the game and their game stats. [sent-16, score-2.034]
</p><p>8 It saves us some work on the backend and has a relatively small effect on user experience. [sent-18, score-0.238]
</p><p>9 Do you try and stay out of there backend as much as possible? [sent-20, score-0.176]
</p><p>10 It sounds like you can play most of the game in the client without talking to the backend for long periods of time. [sent-25, score-0.722]
</p><p>11 I take it flows from the nature of the application where farms are relatively isolated from each other? [sent-26, score-0.235]
</p><p>12 Is there any attempt to clump farms together as some other multiplayer games do? [sent-27, score-0.252]
</p><p>13 Yes, one of the benefits of having an interactive client is that we have a bit of isolation between server latency and observed client latency. [sent-28, score-0.4]
</p><p>14 We do verify each action performed in the game; however, we do it asynchronously and queue the transactions on the client. [sent-29, score-0.162]
</p><p>15 The AMF transactions happen asynchronously from the client and if the server sees something it doesn't think the client should be sending, it returns to the client an "Out of Sync" message which tells the client it is in an invalid state and the client reloads itself. [sent-39, score-1.258]
</p><p>16 An example inside of FarmVille is where we have a friend ladder inside the flash at the bottom of the page. [sent-47, score-1.173]
</p><p>17 Normally, we query facebook for the name and profile picture and our own backend for the game stats and avatar data. [sent-48, score-0.922]
</p><p>18 This is a high engagement piece of the application but lower in priority than users doing farming actions. [sent-49, score-0.198]
</p><p>19 Thus, if our backend is having performance problems, we can turn that off and the friend ladder will only show facebook name and profile picture. [sent-50, score-1.586]
</p><p>20 Likewise, if facebook is having performance problems, we can turn it off and the friend ladder will not show up. [sent-51, score-1.238]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ladder', 0.427), ('friend', 0.346), ('facebook', 0.254), ('game', 0.246), ('client', 0.2), ('backend', 0.176), ('bottom', 0.156), ('normally', 0.139), ('farmville', 0.136), ('turn', 0.128), ('peering', 0.121), ('flash', 0.118), ('farms', 0.114), ('asynchronously', 0.105), ('play', 0.1), ('name', 0.097), ('playing', 0.087), ('integrate', 0.086), ('degradable', 0.085), ('reloads', 0.085), ('tohow', 0.085), ('show', 0.083), ('xhr', 0.08), ('farming', 0.077), ('games', 0.075), ('profile', 0.075), ('builder', 0.074), ('avatar', 0.074), ('disabled', 0.071), ('response', 0.071), ('thus', 0.07), ('harvest', 0.07), ('examples', 0.068), ('use', 0.068), ('invalid', 0.068), ('luke', 0.068), ('likewise', 0.065), ('comet', 0.064), ('vertica', 0.064), ('inside', 0.063), ('wind', 0.063), ('multiplayer', 0.063), ('relatively', 0.062), ('engagement', 0.062), ('neighbors', 0.062), ('questions', 0.06), ('networking', 0.06), ('application', 0.059), ('impact', 0.058), ('verify', 0.057)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="792-tfidf-1" href="../high_scalability-2010/high_scalability-2010-03-10-How_FarmVille_Scales_-_The_Follow-up.html">792 high scalability-2010-03-10-How FarmVille Scales - The Follow-up</a></p>
<p>Introduction: Several readers had follow-up questions in response to  How FarmVille Scales to Harvest 75 Million Players a Month . Here are Luke's response to those questions (and a few of mine).
   How does social networking makes things easier or harder?    
 The primary interesting aspect of social networking games is how you wind up with a graph of connected users who need to be access each other's data on a frequent basis. This makes the overall dataset difficult if not impossible to partition. 
   What are examples of the Facebook calls you try to avoid and how they impact game play?   
 We can make a call for facebook friend data to retrieve information about your friends playing the game. Normally, we show a friend ladder at the bottom of the game that shows friend information, including name and facebook photo.   
   Can you say where your cache is, what form it takes, and how much cached there is? Do you have a peering relationship with Facebook, as one might expect at that bandwidth?</p><p>2 0.26285619 <a title="792-tfidf-2" href="../high_scalability-2010/high_scalability-2010-02-08-How_FarmVille_Scales_to_Harvest_75_Million_Players_a_Month.html">774 high scalability-2010-02-08-How FarmVille Scales to Harvest 75 Million Players a Month</a></p>
<p>Introduction: Several readers had follow-up questions in response to this article. Luke's responses can be found in  How FarmVille Scales - The Follow-up .  
 
  If real farming was as comforting as it is in  Zynga's mega-hit Farmville  then my family would have probably never left those harsh North Dakota winters. None of the scary bedtime stories my Grandma used to tell about farming are true in FarmVille.  Farmers  make money, plants grow, and animals never visit the  red barn . I guess it's just that keep-your-shoes-clean back-to-the-land charm that has helped make FarmVille the "largest game in the world" in such an astonishingly short time.
 
How did FarmVille scale a web application to handle 75 million players a month? Fortunately FarmVille's Luke Rajlich has agreed to let us in on a few their challenges and secrets. Here's what Luke has to say...
 
The format of the interview was that I sent Luke a few general questions and he replied with this response:
  

FarmVille has a unique set of sc</p><p>3 0.21606661 <a title="792-tfidf-3" href="../high_scalability-2010/high_scalability-2010-09-21-Playfish%27s_Social_Gaming_Architecture_-_50_Million_Monthly_Users_and_Growing.html">904 high scalability-2010-09-21-Playfish's Social Gaming Architecture - 50 Million Monthly Users and Growing</a></p>
<p>Introduction: Ten million players a day and over fifty million players a month interact socially with friends using  Playfish  games on social platforms like The Facebook, MySpace, and the iPhone. Playfish was an early innovator in the fastest growing segment of the game industry:  social gaming , which is the love child between casual gaming and social networking. Playfish was also an early adopter of the Amazon cloud, running their system entirely on 100s of cloud servers. Playfish finds itself at the nexus of some hot trends (which may by why EA bought them for  $300 million  and they think a  $1 billion game  is possible): building games on social networks, build applications in the cloud, mobile gaming, leveraging data driven design to continuously evolve and improve systems, agile development and deployment, and  selling virtual good  as a business model.
 
How can a small company make all this happen? To explain the magic I interviewed Playfish's Jodi Moran, Senior Director of Engineering, an</p><p>4 0.17847601 <a title="792-tfidf-4" href="../high_scalability-2008/high_scalability-2008-05-31-memcached_and_Storage_of_Friend_list.html">337 high scalability-2008-05-31-memcached and Storage of Friend list</a></p>
<p>Introduction: My first post, please be gentle. I know it is long. You are all like doctors - the more info, the better the diagnosis.     -----------   What is the best way to store a list of all of your friends in the memcached cache (a simple boolean saying “yes this user is your friend”, or “no”)? Think Robert Scoble (26,000+ “friends”) on Twitter.com. He views a list of ALL existing users, and in this list, his friends are highlighted.     I came up with 4 possible methods:   --store in memcache as an array, a list of all the "yes" friend ID's   --store your friend ID's as individual elements.   --store as a hash of arrays based on last 3 digits of friend's ID -- so have up to 1000 arrays for you.   --comma-delimited string of ID's as one element     I'm using the second one because I think it is faster to update. The single array or hash of arrays feels like too much overhead calculating and updating – and even just loading – to check for existence of a friend.     The key is FRIEND[small ID#]_</p><p>5 0.17103866 <a title="792-tfidf-5" href="../high_scalability-2013/high_scalability-2013-02-13-7_Sensible_and_1_Really_Surprising_Way_EVE_Online_Scales_to_Play_Huge_Games.html">1405 high scalability-2013-02-13-7 Sensible and 1 Really Surprising Way EVE Online Scales to Play Huge Games</a></p>
<p>Introduction: "Everything in war is simple, but the﻿ simplest thing is difficult." -- Carl von Clausewitz 
  

  

  
Games are proving grounds for software architecture. They combine scale, high performance, challenging problems, a rabid user base, cost sensitivity, and the need for profit. And when games have in-game currency, like EVE Online has, there's money at play, so you can't just get away with a c'est la vie attitude. Engineering must be applied. 
 
In  Planning for war: how the EVE Online servers deal with a 3,000 person battle , we learn some techniques EVE Online uses to handle large games:
  7 Sensible...   
  Do nothing.  Most games are manageable or have spikes that quickly dissipate.  
  Run it Hot . There's nothing to throttle as servers run at 100%. Why waste money? Use all your CPU. 
  Shard it . Games are sharded by solar system and multiple solar systems run on a node. 
  Move it . Games are moved when a machine becomes overloaded. Live Node Remap, where a live game is moved to</p><p>6 0.15551615 <a title="792-tfidf-6" href="../high_scalability-2008/high_scalability-2008-10-17-Scaling_Spam_Eradication_Using_Purposeful_Games%3A_Die_Spammer_Die%21.html">422 high scalability-2008-10-17-Scaling Spam Eradication Using Purposeful Games: Die Spammer Die!</a></p>
<p>7 0.15263289 <a title="792-tfidf-7" href="../high_scalability-2011/high_scalability-2011-05-19-Zynga%27s_Z_Cloud_-_Scale_Fast_or_Fail_Fast_by_Merging_Private_and_Public_Clouds.html">1044 high scalability-2011-05-19-Zynga's Z Cloud - Scale Fast or Fail Fast by Merging Private and Public Clouds</a></p>
<p>8 0.14975259 <a title="792-tfidf-8" href="../high_scalability-2011/high_scalability-2011-09-23-The_Real_News_is_Not_that_Facebook_Serves_Up_1_Trillion_Pages_a_Month%E2%80%A6.html">1123 high scalability-2011-09-23-The Real News is Not that Facebook Serves Up 1 Trillion Pages a Month…</a></p>
<p>9 0.1480604 <a title="792-tfidf-9" href="../high_scalability-2009/high_scalability-2009-10-13-Why_are_Facebook%2C_Digg%2C_and_Twitter_so_hard_to_scale%3F.html">721 high scalability-2009-10-13-Why are Facebook, Digg, and Twitter so hard to scale?</a></p>
<p>10 0.14660616 <a title="792-tfidf-10" href="../high_scalability-2009/high_scalability-2009-01-17-Scaling_in_Games_%26_Virtual_Worlds___.html">496 high scalability-2009-01-17-Scaling in Games & Virtual Worlds   </a></p>
<p>11 0.14475188 <a title="792-tfidf-11" href="../high_scalability-2009/high_scalability-2009-10-12-High_Performance_at_Massive_Scale_%E2%80%93__Lessons_learned_at_Facebook.html">720 high scalability-2009-10-12-High Performance at Massive Scale –  Lessons learned at Facebook</a></p>
<p>12 0.13927373 <a title="792-tfidf-12" href="../high_scalability-2008/high_scalability-2008-01-10-Letting_Clients_Know_What%27s_Changed%3A_Push_Me_or_Pull_Me%3F.html">205 high scalability-2008-01-10-Letting Clients Know What's Changed: Push Me or Pull Me?</a></p>
<p>13 0.13892394 <a title="792-tfidf-13" href="../high_scalability-2012/high_scalability-2012-10-17-World_of_Warcraft%27s_Lead_designer_Rob_Pardo_on_the_Role_of_the_Cloud_in_Games.html">1342 high scalability-2012-10-17-World of Warcraft's Lead designer Rob Pardo on the Role of the Cloud in Games</a></p>
<p>14 0.13824755 <a title="792-tfidf-14" href="../high_scalability-2010/high_scalability-2010-06-22-Exploring_the_software_behind_Facebook%2C_the_world%E2%80%99s_largest_site.html">845 high scalability-2010-06-22-Exploring the software behind Facebook, the world’s largest site</a></p>
<p>15 0.12723596 <a title="792-tfidf-15" href="../high_scalability-2009/high_scalability-2009-06-10-Hive_-_A_Petabyte_Scale_Data_Warehouse_using_Hadoop.html">624 high scalability-2009-06-10-Hive - A Petabyte Scale Data Warehouse using Hadoop</a></p>
<p>16 0.12293516 <a title="792-tfidf-16" href="../high_scalability-2010/high_scalability-2010-06-10-The_Four_Meta_Secrets_of_Scaling_at_Facebook.html">840 high scalability-2010-06-10-The Four Meta Secrets of Scaling at Facebook</a></p>
<p>17 0.11428183 <a title="792-tfidf-17" href="../high_scalability-2008/high_scalability-2008-05-02-Friends_for_Sale_Architecture_-_A_300_Million_Page_View-Month_Facebook_RoR_App.html">313 high scalability-2008-05-02-Friends for Sale Architecture - A 300 Million Page View-Month Facebook RoR App</a></p>
<p>18 0.11051365 <a title="792-tfidf-18" href="../high_scalability-2008/high_scalability-2008-10-08-Strategy%3A_Flickr_-_Do_the_Essential_Work_Up-front_and_Queue_the_Rest_.html">406 high scalability-2008-10-08-Strategy: Flickr - Do the Essential Work Up-front and Queue the Rest </a></p>
<p>19 0.11015654 <a title="792-tfidf-19" href="../high_scalability-2007/high_scalability-2007-10-23-Hire_Facebook%2C_Ning%2C_and_Salesforce_to_Scale_for_You.html">129 high scalability-2007-10-23-Hire Facebook, Ning, and Salesforce to Scale for You</a></p>
<p>20 0.10946005 <a title="792-tfidf-20" href="../high_scalability-2011/high_scalability-2011-03-15-Sponsored_Post%3A_Schooner%2C_deviantART%2C_ScaleOut%2C_aiCache%2C_WAPT%2C_Karmasphere%2C_Kabam%2C_Newrelic%2C_Cloudkick%2C_Membase%2C_Joyent%2C_CloudSigma%2C_ManageEngine%2C_Site24x7.html">1005 high scalability-2011-03-15-Sponsored Post: Schooner, deviantART, ScaleOut, aiCache, WAPT, Karmasphere, Kabam, Newrelic, Cloudkick, Membase, Joyent, CloudSigma, ManageEngine, Site24x7</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.175), (1, 0.081), (2, 0.063), (3, -0.055), (4, 0.081), (5, -0.048), (6, -0.016), (7, 0.068), (8, -0.022), (9, -0.039), (10, 0.136), (11, 0.091), (12, 0.041), (13, 0.058), (14, -0.064), (15, 0.034), (16, 0.077), (17, 0.055), (18, 0.048), (19, 0.014), (20, 0.065), (21, 0.046), (22, 0.078), (23, 0.01), (24, 0.049), (25, 0.026), (26, 0.071), (27, -0.058), (28, 0.029), (29, -0.083), (30, -0.036), (31, -0.003), (32, -0.032), (33, -0.04), (34, 0.047), (35, 0.018), (36, 0.136), (37, -0.106), (38, 0.015), (39, -0.086), (40, 0.019), (41, -0.028), (42, 0.028), (43, 0.058), (44, -0.034), (45, -0.001), (46, -0.023), (47, -0.027), (48, 0.012), (49, -0.037)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97205859 <a title="792-lsi-1" href="../high_scalability-2010/high_scalability-2010-03-10-How_FarmVille_Scales_-_The_Follow-up.html">792 high scalability-2010-03-10-How FarmVille Scales - The Follow-up</a></p>
<p>Introduction: Several readers had follow-up questions in response to  How FarmVille Scales to Harvest 75 Million Players a Month . Here are Luke's response to those questions (and a few of mine).
   How does social networking makes things easier or harder?    
 The primary interesting aspect of social networking games is how you wind up with a graph of connected users who need to be access each other's data on a frequent basis. This makes the overall dataset difficult if not impossible to partition. 
   What are examples of the Facebook calls you try to avoid and how they impact game play?   
 We can make a call for facebook friend data to retrieve information about your friends playing the game. Normally, we show a friend ladder at the bottom of the game that shows friend information, including name and facebook photo.   
   Can you say where your cache is, what form it takes, and how much cached there is? Do you have a peering relationship with Facebook, as one might expect at that bandwidth?</p><p>2 0.79201877 <a title="792-lsi-2" href="../high_scalability-2010/high_scalability-2010-02-08-How_FarmVille_Scales_to_Harvest_75_Million_Players_a_Month.html">774 high scalability-2010-02-08-How FarmVille Scales to Harvest 75 Million Players a Month</a></p>
<p>Introduction: Several readers had follow-up questions in response to this article. Luke's responses can be found in  How FarmVille Scales - The Follow-up .  
 
  If real farming was as comforting as it is in  Zynga's mega-hit Farmville  then my family would have probably never left those harsh North Dakota winters. None of the scary bedtime stories my Grandma used to tell about farming are true in FarmVille.  Farmers  make money, plants grow, and animals never visit the  red barn . I guess it's just that keep-your-shoes-clean back-to-the-land charm that has helped make FarmVille the "largest game in the world" in such an astonishingly short time.
 
How did FarmVille scale a web application to handle 75 million players a month? Fortunately FarmVille's Luke Rajlich has agreed to let us in on a few their challenges and secrets. Here's what Luke has to say...
 
The format of the interview was that I sent Luke a few general questions and he replied with this response:
  

FarmVille has a unique set of sc</p><p>3 0.73166871 <a title="792-lsi-3" href="../high_scalability-2014/high_scalability-2014-03-26-Oculus_Causes_a_Rift%2C_but_the_Facebook_Deal_Will_Avoid_a_Scaling_Crisis_for_Virtual_Reality.html">1619 high scalability-2014-03-26-Oculus Causes a Rift, but the Facebook Deal Will Avoid a Scaling Crisis for Virtual Reality</a></p>
<p>Introduction: Facebook has been teasing us. While many of their recent  acquisitions  have been surprising, shocking is the only word adequately describing Facebook's  5 day whirlwind acquisition  of   Oculus  , immersive virtual reality visionaries, for a now paltry sounding $2 billion.
   The    backlash    is a pandemic, jumping across    social networks    with the speed only a meme powered by the directly unaffected can generate.  
   For more than 30 years VR has been the dream burning in the heart of every science fiction fan. Now that this future might finally be here, Facebook’s ownage makes it seem like a wonderful and hopeful timeline has been choked off, killing the Metaverse before it even had a chance to begin.  
  For the many who voted for an open future with their  Kickstarter dollars , there’s a deep and personal sense of betrayal, despite Facebook’s promise to leave Oculus alone. The intensity of the reaction is because Oculus matters to people. It's new, it's different, it create</p><p>4 0.72073579 <a title="792-lsi-4" href="../high_scalability-2012/high_scalability-2012-01-04-How_Facebook_Handled_the_New_Year%27s_Eve_Onslaught.html">1168 high scalability-2012-01-04-How Facebook Handled the New Year's Eve Onslaught</a></p>
<p>Introduction: How does Facebook handle the massive New Year's Eve traffic spike? Thanks to Mike Swift, in  Facebook gets ready for New Year's Eve , we get a little insight as to their method for the madness, nothing really detailed, but still interesting.
  Problem Setup   
 Facebook expects tha one billion+ photos will be shared on New Year's eve. 
 Facebook's 800 million users are scattered around the world. Three quarters live outside the US. Each user is linked to an average of 130 friends. 
 Photos and posts must appear in less than a second. Opening a homepage requires executing requests on a 100 different servers, and those requests have to be ranked, sorted, and privacy-checked, and then rendered. 
 Different events put different stresses on different parts of Facebook.       
 
 Photo and Video Uploads - Holidays require hundreds of terabytes of capacity  
 News Feed - News events like big sports events and the death of Steve Jobs drive user status updates 
 
 
   Coping Strategies   
  Try</p><p>5 0.70357382 <a title="792-lsi-5" href="../high_scalability-2010/high_scalability-2010-09-21-Playfish%27s_Social_Gaming_Architecture_-_50_Million_Monthly_Users_and_Growing.html">904 high scalability-2010-09-21-Playfish's Social Gaming Architecture - 50 Million Monthly Users and Growing</a></p>
<p>Introduction: Ten million players a day and over fifty million players a month interact socially with friends using  Playfish  games on social platforms like The Facebook, MySpace, and the iPhone. Playfish was an early innovator in the fastest growing segment of the game industry:  social gaming , which is the love child between casual gaming and social networking. Playfish was also an early adopter of the Amazon cloud, running their system entirely on 100s of cloud servers. Playfish finds itself at the nexus of some hot trends (which may by why EA bought them for  $300 million  and they think a  $1 billion game  is possible): building games on social networks, build applications in the cloud, mobile gaming, leveraging data driven design to continuously evolve and improve systems, agile development and deployment, and  selling virtual good  as a business model.
 
How can a small company make all this happen? To explain the magic I interviewed Playfish's Jodi Moran, Senior Director of Engineering, an</p><p>6 0.70137107 <a title="792-lsi-6" href="../high_scalability-2013/high_scalability-2013-02-13-7_Sensible_and_1_Really_Surprising_Way_EVE_Online_Scales_to_Play_Huge_Games.html">1405 high scalability-2013-02-13-7 Sensible and 1 Really Surprising Way EVE Online Scales to Play Huge Games</a></p>
<p>7 0.66886926 <a title="792-lsi-7" href="../high_scalability-2008/high_scalability-2008-05-14-New_Facebook_Chat_Feature_Scales_to_70_Million_Users_Using_Erlang.html">318 high scalability-2008-05-14-New Facebook Chat Feature Scales to 70 Million Users Using Erlang</a></p>
<p>8 0.66664028 <a title="792-lsi-8" href="../high_scalability-2009/high_scalability-2009-01-17-Scaling_in_Games_%26_Virtual_Worlds___.html">496 high scalability-2009-01-17-Scaling in Games & Virtual Worlds   </a></p>
<p>9 0.64763075 <a title="792-lsi-9" href="../high_scalability-2014/high_scalability-2014-02-26-The_WhatsApp_Architecture_Facebook_Bought_For_%2419_Billion.html">1602 high scalability-2014-02-26-The WhatsApp Architecture Facebook Bought For $19 Billion</a></p>
<p>10 0.64249104 <a title="792-lsi-10" href="../high_scalability-2010/high_scalability-2010-08-02-7_Scaling_Strategies_Facebook_Used_to_Grow_to_500_Million_Users.html">870 high scalability-2010-08-02-7 Scaling Strategies Facebook Used to Grow to 500 Million Users</a></p>
<p>11 0.63775003 <a title="792-lsi-11" href="../high_scalability-2008/high_scalability-2008-10-22-EVE_Online_Architecture.html">424 high scalability-2008-10-22-EVE Online Architecture</a></p>
<p>12 0.63460058 <a title="792-lsi-12" href="../high_scalability-2012/high_scalability-2012-09-15-4_Reasons_Facebook_Dumped_HTML5_and_Went_Native.html">1323 high scalability-2012-09-15-4 Reasons Facebook Dumped HTML5 and Went Native</a></p>
<p>13 0.63014847 <a title="792-lsi-13" href="../high_scalability-2010/high_scalability-2010-12-31-Facebook_in_20_Minutes%3A_2.7M_Photos%2C_10.2M_Comments%2C_4.6M_Messages.html">966 high scalability-2010-12-31-Facebook in 20 Minutes: 2.7M Photos, 10.2M Comments, 4.6M Messages</a></p>
<p>14 0.62576646 <a title="792-lsi-14" href="../high_scalability-2011/high_scalability-2011-09-23-The_Real_News_is_Not_that_Facebook_Serves_Up_1_Trillion_Pages_a_Month%E2%80%A6.html">1123 high scalability-2011-09-23-The Real News is Not that Facebook Serves Up 1 Trillion Pages a Month…</a></p>
<p>15 0.62350857 <a title="792-lsi-15" href="../high_scalability-2008/high_scalability-2008-10-17-Scaling_Spam_Eradication_Using_Purposeful_Games%3A_Die_Spammer_Die%21.html">422 high scalability-2008-10-17-Scaling Spam Eradication Using Purposeful Games: Die Spammer Die!</a></p>
<p>16 0.61742431 <a title="792-lsi-16" href="../high_scalability-2008/high_scalability-2008-12-13-Strategy%3A_Facebook_Tweaks_to_Handle_6_Time_as_Many_Memcached_Requests.html">464 high scalability-2008-12-13-Strategy: Facebook Tweaks to Handle 6 Time as Many Memcached Requests</a></p>
<p>17 0.60972911 <a title="792-lsi-17" href="../high_scalability-2012/high_scalability-2012-10-17-World_of_Warcraft%27s_Lead_designer_Rob_Pardo_on_the_Role_of_the_Cloud_in_Games.html">1342 high scalability-2012-10-17-World of Warcraft's Lead designer Rob Pardo on the Role of the Cloud in Games</a></p>
<p>18 0.60269928 <a title="792-lsi-18" href="../high_scalability-2010/high_scalability-2010-06-22-Exploring_the_software_behind_Facebook%2C_the_world%E2%80%99s_largest_site.html">845 high scalability-2010-06-22-Exploring the software behind Facebook, the world’s largest site</a></p>
<p>19 0.59890157 <a title="792-lsi-19" href="../high_scalability-2009/high_scalability-2009-10-12-High_Performance_at_Massive_Scale_%E2%80%93__Lessons_learned_at_Facebook.html">720 high scalability-2009-10-12-High Performance at Massive Scale –  Lessons learned at Facebook</a></p>
<p>20 0.59651864 <a title="792-lsi-20" href="../high_scalability-2010/high_scalability-2010-06-10-The_Four_Meta_Secrets_of_Scaling_at_Facebook.html">840 high scalability-2010-06-10-The Four Meta Secrets of Scaling at Facebook</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.128), (2, 0.175), (10, 0.372), (22, 0.046), (30, 0.05), (61, 0.06), (77, 0.015), (79, 0.058)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.97280431 <a title="792-lda-1" href="../high_scalability-2008/high_scalability-2008-10-26-Should_you_use_a_SAN_to_scale_your_architecture%3F_.html">430 high scalability-2008-10-26-Should you use a SAN to scale your architecture? </a></p>
<p>Introduction: This is a question everyone must struggle with when building out their datacenter. Storage choices are always the ones I have the least confidence in.  David Marks in his blog  You Can Change It Later!  asks the question   Should I get a SAN to scale my site architecture?   and answers no. A better solution is to use commodity hardware, directly attach storage on servers, and partition across servers to scale and for greater availability.  David's reasoning is interesting:
  A SAN creates a SPOF (single point of failure) that is dependent on a vendor to fly and fix when there's a problem. This can lead to long down times during this outage you have no access to your data at all.   Using easily available commodity hardware minimizes risks to your company, it's not just about saving money. Zooming over to Fry's to buy emergency equipment provides the kind of agility startups need in order to respond quickly to ever changing situations.  It's hard to beat the power and flexibility (backup</p><p>2 0.97254074 <a title="792-lda-2" href="../high_scalability-2013/high_scalability-2013-06-24-Update_on_How_29_Cloud_Price_Drops_Changed_the_Bottom_Line_of_TripAdvisor_and_Pinterest_-_Results_Mixed.html">1480 high scalability-2013-06-24-Update on How 29 Cloud Price Drops Changed the Bottom Line of TripAdvisor and Pinterest - Results Mixed</a></p>
<p>Introduction: This is a guest post by  Ali Khajeh-Hosseini , Technical Lead at  PlanForCloud . The original article was published  on their site . With 29 cloud price reductions I thought it would be interesting to see how the bottom line would change compared to an article  we published last year . The result is surprisingly little for TripAdvisor because prices for On Demand instances have  not dropped as fast  as for other other instances types. 
 
Over the last year and a half,  we counted 29 price reductions  in cloud services provided by AWS, Google Compute Engine, Windows Azure, and Rackspace Cloud. Price reductions have a direct effect on cloud users, but given the usual tiny reductions, how significant is that effect on the bottom line?
 
Last year I wrote about  cloud cost forecasts for TripAdvisor and Pinterest .  TripAdvisor was experimenting with AWS  and attempted to process 700K HTTP requests per minute on a replica of its live site, and  Pinterest was growing massively on AWS . In th</p><p>3 0.971035 <a title="792-lda-3" href="../high_scalability-2011/high_scalability-2011-06-22-It%27s_the_Fraking_IOPS_-_1_SSD_is_44%2C000_IOPS%2C_Hard_Drive_is_180.html">1066 high scalability-2011-06-22-It's the Fraking IOPS - 1 SSD is 44,000 IOPS, Hard Drive is 180</a></p>
<p>Introduction: Planning your next buildout and thinking SSDs are still far in the future? Still too expensive, too low density. Hard disks are cheap, familiar, and store lots of stuff. In this short and entertaining video Wikia's  Artur Bergman  wants to change your mind about SSDs. SSDs are for today, get with the math already.
 
Here's Artur's logic:
  
 Wikia is all SSD in production. The new Wikia file servers have a theoretical read rate of ~10GB/sec sequential, 6GB/sec random and 1.2 million IOPs. If you can't do math or love the past, you love spinning rust. If you are awesome you love SSDs. 
 SSDs are cheaper than drives using the most relevant metric: $/GB/IOPS. 1 SSD is 44,000 IOPS and one hard drive is 180 IOPS. Need 1 SSD instead of 50 hard drives. 
 With 8 million files there's a 9 minute fsck. Full backup in 12 minutes (X-25M based). 
 4 GB/sec random read average latency 1 msec. 
 2.2 GB/sec random write average latency 1 msec. 
 50TBs of SSDs in one machine for $80,000. With the densi</p><p>4 0.96641302 <a title="792-lda-4" href="../high_scalability-2012/high_scalability-2012-08-06-Paper%3A_High-Performance_Concurrency_Control_Mechanisms_for_Main-Memory_Databases.html">1299 high scalability-2012-08-06-Paper: High-Performance Concurrency Control Mechanisms for Main-Memory Databases</a></p>
<p>Introduction: If you stayed up all night watching the life reaffirming  Curiosity landing on Mars , then this paper,  High-Performance Concurrency Control Mechanisms for Main-Memory Databases , has nothing to do with that at all, but it is an excellent look at how to use optimistic MVCC schemes to reduce lock overhead on in-memory datastructures:
  A database system optimized for in-memory storage can support  much higher transaction rates than current systems.  However,  standard concurrency control methods used today do not scale to  the high transaction rates achievable by such systems. In this paper we introduce two efficient concurrency control methods specifically designed for main-memory databases. Both use multiversioning to isolate read-only transactions from updates but differ in  how atomicity is ensured: one is optimistic and one is pessimistic. To avoid expensive context switching, transactions  never block  during normal processing but they may have to wait before commit to ensure corr</p><p>5 0.95931435 <a title="792-lda-5" href="../high_scalability-2014/high_scalability-2014-04-21-This_is_why_Microsoft_won._And_why_they_lost..html">1635 high scalability-2014-04-21-This is why Microsoft won. And why they lost.</a></p>
<p>Introduction: My favorite kind of histories are those told from an insider's perspective. The story of Richard the Lionheart is full of great battles and dynastic intrigue. The story of one of his soldiers, not so much. Yet the soldiers' story, as someone who has experienced the real consequences of decisions made and actions taken, is more revealing.
 
We get such a history in  Chat Wars , a wonderful article written by David Auerbach, who in 1998 worked at Microsoft on MSN Messenger Service, Microsoft’s instant messaging app (for a related story see  The Rise and Fall of AIM, the Breakthrough AOL Never Wanted ).
 
It's as if Herodotus visited Microsoft and wrote down his experiences. It has that same sort of conversational tone, insightful on-the-ground observations, and facts no outsider might ever believe.
 
Much of the article is a play-by-play account of the cat and mouse game David plays changing Messenger to track AOL's Instant Messenger protocol changes. AOL repeatedly tried to make it so M</p><p>6 0.9527365 <a title="792-lda-6" href="../high_scalability-2010/high_scalability-2010-08-07-ArchCamp%3A_Scalable_Databases_%28NoSQL%29.html">874 high scalability-2010-08-07-ArchCamp: Scalable Databases (NoSQL)</a></p>
<p>7 0.93851429 <a title="792-lda-7" href="../high_scalability-2010/high_scalability-2010-01-27-Hot_Scalability_Links_for_January_28_2010.html">767 high scalability-2010-01-27-Hot Scalability Links for January 28 2010</a></p>
<p>8 0.93333304 <a title="792-lda-8" href="../high_scalability-2007/high_scalability-2007-12-02-a8cjdbc_-_update_verision_1.3.html">171 high scalability-2007-12-02-a8cjdbc - update verision 1.3</a></p>
<p>9 0.93331891 <a title="792-lda-9" href="../high_scalability-2007/high_scalability-2007-12-02-Database-Clustering%3A_a8cjdbc_-_update%3A_version_1.3.html">170 high scalability-2007-12-02-Database-Clustering: a8cjdbc - update: version 1.3</a></p>
<p>same-blog 10 0.9290325 <a title="792-lda-10" href="../high_scalability-2010/high_scalability-2010-03-10-How_FarmVille_Scales_-_The_Follow-up.html">792 high scalability-2010-03-10-How FarmVille Scales - The Follow-up</a></p>
<p>11 0.92763996 <a title="792-lda-11" href="../high_scalability-2011/high_scalability-2011-05-23-Evernote_Architecture_-_9_Million_Users_and_150_Million_Requests_a_Day.html">1046 high scalability-2011-05-23-Evernote Architecture - 9 Million Users and 150 Million Requests a Day</a></p>
<p>12 0.92367655 <a title="792-lda-12" href="../high_scalability-2007/high_scalability-2007-12-10-1_Master%2C_N_Slaves.html">178 high scalability-2007-12-10-1 Master, N Slaves</a></p>
<p>13 0.92074203 <a title="792-lda-13" href="../high_scalability-2009/high_scalability-2009-04-27-Some_Questions_from_a_newbie.html">584 high scalability-2009-04-27-Some Questions from a newbie</a></p>
<p>14 0.90862149 <a title="792-lda-14" href="../high_scalability-2014/high_scalability-2014-04-14-How_do_you_even_do_anything_without_using_EBS%3F.html">1631 high scalability-2014-04-14-How do you even do anything without using EBS?</a></p>
<p>15 0.89742792 <a title="792-lda-15" href="../high_scalability-2014/high_scalability-2014-01-24-Stuff_The_Internet_Says_On_Scalability_For_January_24th%2C_2014.html">1585 high scalability-2014-01-24-Stuff The Internet Says On Scalability For January 24th, 2014</a></p>
<p>16 0.88724679 <a title="792-lda-16" href="../high_scalability-2009/high_scalability-2009-08-28-Strategy%3A_Solve_Only_80_Percent_of_the_Problem.html">689 high scalability-2009-08-28-Strategy: Solve Only 80 Percent of the Problem</a></p>
<p>17 0.84623152 <a title="792-lda-17" href="../high_scalability-2012/high_scalability-2012-10-02-An_Epic_TripAdvisor_Update%3A_Why_Not_Run_on_the_Cloud%3F_The_Grand_Experiment..html">1331 high scalability-2012-10-02-An Epic TripAdvisor Update: Why Not Run on the Cloud? The Grand Experiment.</a></p>
<p>18 0.84594518 <a title="792-lda-18" href="../high_scalability-2007/high_scalability-2007-11-05-Strategy%3A_Diagonal_Scaling_-_Don%27t_Forget_to_Scale_Out_AND_Up.html">142 high scalability-2007-11-05-Strategy: Diagonal Scaling - Don't Forget to Scale Out AND Up</a></p>
<p>19 0.8393054 <a title="792-lda-19" href="../high_scalability-2012/high_scalability-2012-11-01-Cost_Analysis%3A_TripAdvisor_and_Pinterest_costs_on_the_AWS_cloud.html">1353 high scalability-2012-11-01-Cost Analysis: TripAdvisor and Pinterest costs on the AWS cloud</a></p>
<p>20 0.82997417 <a title="792-lda-20" href="../high_scalability-2008/high_scalability-2008-02-22-Kevin%27s_Great_Adventures_in_SSDland.html">257 high scalability-2008-02-22-Kevin's Great Adventures in SSDland</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
