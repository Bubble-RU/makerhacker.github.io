<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>809 high scalability-2010-04-13-Strategy: Saving Your Butt With Deferred Deletes</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2010" href="../home/high_scalability-2010_home.html">high_scalability-2010</a> <a title="high_scalability-2010-809" href="#">high_scalability-2010-809</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>809 high scalability-2010-04-13-Strategy: Saving Your Butt With Deferred Deletes</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2010-809-html" href="http://highscalability.com//blog/2010/4/13/strategy-saving-your-butt-with-deferred-deletes.html">html</a></p><p>Introduction: Deferred Deletes  is a technique where deleted items  are marked as deleted but not garbage collected until some days or  preferably weeks later .    James Hamilton talks describes this strategy in his classic  On Designing and Deploying  Internet-Scale Services: 
  

Never delete anything. Just mark it deleted. When new data comes in, record the requests on the way. Keep a rolling two week (or more) history of all changes to help recover from software or administrative errors. If someone makes a mistake and forgets the where clause on a delete statement (it has happened before and it will again), all logical copies of the data are deleted. Neither RAID nor mirroring can protect against this form of error. The ability to recover the data can make the difference between a highly embarrassing issue or a minor, barely noticeable glitch. For those systems already doing off-line backups, this additional record of data coming into the service only needs to be since the last backup. But, bein</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Deferred Deletes  is a technique where deleted items  are marked as deleted but not garbage collected until some days or  preferably weeks later . [sent-1, score-0.963]
</p><p>2 James Hamilton talks describes this strategy in his classic  On Designing and Deploying  Internet-Scale Services:      Never delete anything. [sent-2, score-0.354]
</p><p>3 When new data comes in, record the requests on the way. [sent-4, score-0.139]
</p><p>4 Keep a rolling two week (or more) history of all changes to help recover from software or administrative errors. [sent-5, score-0.55]
</p><p>5 If someone makes a mistake and forgets the where clause on a delete statement (it has happened before and it will again), all logical copies of the data are deleted. [sent-6, score-0.998]
</p><p>6 Neither RAID nor mirroring can protect against this form of error. [sent-7, score-0.212]
</p><p>7 The ability to recover the data can make the difference between a highly embarrassing issue or a minor, barely noticeable glitch. [sent-8, score-0.53]
</p><p>8 For those systems already doing off-line backups, this additional record of data coming into the service only needs to be since the last backup. [sent-9, score-0.139]
</p><p>9 But, being cautious, we recommend going farther back anyway. [sent-10, score-0.252]
</p><p>10 Mistakes happen and James says in  Stonebraker  on CAP Theorem and Databases  that:   Deferred  delete is not  full protection but it has saves my butt more than once  and I’m a  believer. [sent-11, score-0.619]
</p><p>11 If you   have an  application error, administrative error, or database   implementation  bug that losses data, then it is simply gone unless you   have an  offline copy. [sent-12, score-0.699]
</p><p>12 This, by the way, is why I’m a big fan of deferred    delete. [sent-13, score-0.539]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('deferred', 0.44), ('delete', 0.276), ('administrative', 0.215), ('deleted', 0.192), ('recover', 0.176), ('butt', 0.164), ('james', 0.16), ('farther', 0.154), ('cautious', 0.154), ('forgets', 0.147), ('losses', 0.147), ('record', 0.139), ('error', 0.137), ('embarrassing', 0.13), ('preferably', 0.13), ('clause', 0.13), ('mirroring', 0.124), ('marked', 0.118), ('noticeable', 0.118), ('minor', 0.11), ('barely', 0.106), ('theorem', 0.102), ('fan', 0.099), ('recommend', 0.098), ('neither', 0.097), ('mistake', 0.096), ('statement', 0.094), ('collected', 0.093), ('saves', 0.093), ('logical', 0.091), ('bug', 0.091), ('offline', 0.09), ('hamilton', 0.09), ('protect', 0.088), ('rolling', 0.088), ('mark', 0.087), ('protection', 0.086), ('backups', 0.083), ('copies', 0.083), ('technique', 0.083), ('happened', 0.081), ('cap', 0.081), ('unless', 0.08), ('garbage', 0.079), ('describes', 0.078), ('raid', 0.077), ('weeks', 0.076), ('gone', 0.076), ('deploying', 0.073), ('week', 0.071)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="809-tfidf-1" href="../high_scalability-2010/high_scalability-2010-04-13-Strategy%3A_Saving_Your_Butt_With_Deferred_Deletes.html">809 high scalability-2010-04-13-Strategy: Saving Your Butt With Deferred Deletes</a></p>
<p>Introduction: Deferred Deletes  is a technique where deleted items  are marked as deleted but not garbage collected until some days or  preferably weeks later .    James Hamilton talks describes this strategy in his classic  On Designing and Deploying  Internet-Scale Services: 
  

Never delete anything. Just mark it deleted. When new data comes in, record the requests on the way. Keep a rolling two week (or more) history of all changes to help recover from software or administrative errors. If someone makes a mistake and forgets the where clause on a delete statement (it has happened before and it will again), all logical copies of the data are deleted. Neither RAID nor mirroring can protect against this form of error. The ability to recover the data can make the difference between a highly embarrassing issue or a minor, barely noticeable glitch. For those systems already doing off-line backups, this additional record of data coming into the service only needs to be since the last backup. But, bein</p><p>2 0.08211495 <a title="809-tfidf-2" href="../high_scalability-2011/high_scalability-2011-03-24-Strategy%3A_Disk_Backup_for_Speed%2C_Tape_Backup_to_Save_Your_Bacon%2C_Just_Ask_Google.html">1010 high scalability-2011-03-24-Strategy: Disk Backup for Speed, Tape Backup to Save Your Bacon, Just Ask Google</a></p>
<p>Introduction: In  Stack Overflow Architecture Update - Now At 95 Million Page Views A Month , a commenter expressed surprise about Stack Overflow's backup strategy: 
  

Backup is to disk for fast retrieval and to tape for historical archiving.

  
The comment was:
  

Really? People still do this? I know some organizations invested a tremendous amount in automated, robotic tape backup, but seriously, a site founded in 2008 is backing up to tape?

   The Case of the Missing Gmail Accounts  
I admit that I was surprised at this strategy too. In this age of copying data to disk three times for safety, I also wondered if tape backups were still necessary? Then, like in a movie, an event happened that made sense of everything, Google suffered the quintessential  #firstworldproblem , gmail accounts went missing! Queue emphatic music. And what's more they were taking a long time to come back. There was a palpable fear in the land that email accounts might never be restored. Think about that. They might ne</p><p>3 0.081064112 <a title="809-tfidf-3" href="../high_scalability-2010/high_scalability-2010-09-30-Facebook_and_Site_Failures_Caused_by_Complex%2C_Weakly_Interacting%2C_Layered_Systems.html">910 high scalability-2010-09-30-Facebook and Site Failures Caused by Complex, Weakly Interacting, Layered Systems</a></p>
<p>Introduction: Facebook has been so reliable that when a site outage does occur it's a definite learning opportunity. Fortunately for us we can learn something because in  More Details on Today's Outage , Facebook's  Robert Johnson  gave a pretty candid explanation of what caused a rare 2.5 hour period of down time for Facebook. It wasn't a simple problem. The root causes were feedback loops and transient spikes caused ultimately by the complexity of weakly interacting layers in modern systems. You know, the kind everyone is building these days. Problems like this are notoriously hard to fix and finding a real solution may send Facebook back to the whiteboard. There's a technical debt that must be paid. 
 
The outline and my interpretation (reading between the lines) of what happened is:
  
 Remember that Facebook  caches everything . They have   28  terabytes  of  memcached  data on 800 servers. The database is the system of record, but memory is where the action is. So when a problem happens that i</p><p>4 0.080664419 <a title="809-tfidf-4" href="../high_scalability-2013/high_scalability-2013-03-06-Low_Level_Scalability_Solutions_-_The_Aggregation_Collection.html">1418 high scalability-2013-03-06-Low Level Scalability Solutions - The Aggregation Collection</a></p>
<p>Introduction: What good are problems without solutions? In  42 Monster Problems That Attack As Loads Increase  we talked about problems. In this first post (OK, there was an earlier post, but I'm doing some reorganizing), we'll cover what I call  aggregation  strategies.
 
Keep in mind these are low level architecture type suggestions of how to structure the components of your code and how they interact. We're not talking about massive scale-out clusters here, but of what your applications might like like internally, way below the service level interface level. There's a lot more to the world than evented architectures.
 
Aggregation simply means we aren't using stupid queues. Our queues will be smart. We are deeply aware of queues as containers of work that eventually dictate how the entire system performs. As work containers we know intimately what requests and data sit in our queues and we can use that intelligence to our great advantage.
  Prioritize Work  
The key idea to it all is an almost mi</p><p>5 0.079774022 <a title="809-tfidf-5" href="../high_scalability-2014/high_scalability-2014-02-03-How_Google_Backs_Up_the_Internet_Along_With_Exabytes_of_Other_Data.html">1589 high scalability-2014-02-03-How Google Backs Up the Internet Along With Exabytes of Other Data</a></p>
<p>Introduction: Raymond Blum    leads a team of Site Reliability Engineers charged with keeping Google's data secret and keeping it safe. Of course Google would never say how much data this actually is, but from comments it seems that it is not yet a    yottabyte   , but is many    exabytes    in size. GMail alone is approaching low exabytes of data. 
   Mr. Blum, in the video    How Google Backs Up the Internet   , explained common backup strategies don’t work for Google for a very googly sounding reason: typically they    scale effort with capacity   . If backing up twice as much data requires twice as much stuff to do it, where stuff is time, energy, space, etc., it won’t work, it doesn’t scale.  You have to find efficiencies so that capacity can scale faster than the effort needed to support that capacity. A different plan is needed when making the jump from backing up one exabyte to backing up two exabytes. And the talk is largely about how Google makes that happen. 
   Some major themes of the t</p><p>6 0.077042021 <a title="809-tfidf-6" href="../high_scalability-2013/high_scalability-2013-03-04-7_Life_Saving_Scalability_Defenses_Against_Load_Monster_Attacks.html">1415 high scalability-2013-03-04-7 Life Saving Scalability Defenses Against Load Monster Attacks</a></p>
<p>7 0.073508263 <a title="809-tfidf-7" href="../high_scalability-2012/high_scalability-2012-07-09-Data_Replication_in_NoSQL_Databases.html">1279 high scalability-2012-07-09-Data Replication in NoSQL Databases</a></p>
<p>8 0.071644396 <a title="809-tfidf-8" href="../high_scalability-2012/high_scalability-2012-03-14-The_Azure_Outage%3A_Time_Is_a_SPOF%2C_Leap_Day_Doubly_So.html">1209 high scalability-2012-03-14-The Azure Outage: Time Is a SPOF, Leap Day Doubly So</a></p>
<p>9 0.07064566 <a title="809-tfidf-9" href="../high_scalability-2008/high_scalability-2008-09-22-Paper%3A_On_Delivering_Embarrassingly_Distributed_Cloud_Services.html">387 high scalability-2008-09-22-Paper: On Delivering Embarrassingly Distributed Cloud Services</a></p>
<p>10 0.069871441 <a title="809-tfidf-10" href="../high_scalability-2014/high_scalability-2014-05-09-Stuff_The_Internet_Says_On_Scalability_For_May_9th%2C_2014.html">1645 high scalability-2014-05-09-Stuff The Internet Says On Scalability For May 9th, 2014</a></p>
<p>11 0.067145683 <a title="809-tfidf-11" href="../high_scalability-2010/high_scalability-2010-10-18-NoCAP.html">921 high scalability-2010-10-18-NoCAP</a></p>
<p>12 0.062517248 <a title="809-tfidf-12" href="../high_scalability-2010/high_scalability-2010-10-24-Hot_Scalability_Links_For_Oct_24%2C_2010.html">926 high scalability-2010-10-24-Hot Scalability Links For Oct 24, 2010</a></p>
<p>13 0.061935514 <a title="809-tfidf-13" href="../high_scalability-2014/high_scalability-2014-01-31-Stuff_The_Internet_Says_On_Scalability_For_January_31st%2C_2014.html">1588 high scalability-2014-01-31-Stuff The Internet Says On Scalability For January 31st, 2014</a></p>
<p>14 0.061871208 <a title="809-tfidf-14" href="../high_scalability-2010/high_scalability-2010-04-08-Hot_Scalability_Links_for_April_8%2C_2010.html">806 high scalability-2010-04-08-Hot Scalability Links for April 8, 2010</a></p>
<p>15 0.061673291 <a title="809-tfidf-15" href="../high_scalability-2008/high_scalability-2008-09-03-Some_Facebook_Secrets_to_Better_Operations.html">378 high scalability-2008-09-03-Some Facebook Secrets to Better Operations</a></p>
<p>16 0.059722774 <a title="809-tfidf-16" href="../high_scalability-2008/high_scalability-2008-10-08-Strategy%3A_Flickr_-_Do_the_Essential_Work_Up-front_and_Queue_the_Rest_.html">406 high scalability-2008-10-08-Strategy: Flickr - Do the Essential Work Up-front and Queue the Rest </a></p>
<p>17 0.058999512 <a title="809-tfidf-17" href="../high_scalability-2010/high_scalability-2010-11-30-NoCAP_%E2%80%93_Part_III_%E2%80%93_GigaSpaces_clustering_explained...html">950 high scalability-2010-11-30-NoCAP – Part III – GigaSpaces clustering explained..</a></p>
<p>18 0.058721948 <a title="809-tfidf-18" href="../high_scalability-2012/high_scalability-2012-09-15-4_Reasons_Facebook_Dumped_HTML5_and_Went_Native.html">1323 high scalability-2012-09-15-4 Reasons Facebook Dumped HTML5 and Went Native</a></p>
<p>19 0.058430504 <a title="809-tfidf-19" href="../high_scalability-2013/high_scalability-2013-06-13-Busting_4_Modern_Hardware_Myths_-_Are_Memory%2C_HDDs%2C_and_SSDs_Really_Random_Access%3F.html">1475 high scalability-2013-06-13-Busting 4 Modern Hardware Myths - Are Memory, HDDs, and SSDs Really Random Access?</a></p>
<p>20 0.057307012 <a title="809-tfidf-20" href="../high_scalability-2012/high_scalability-2012-05-25-Stuff_The_Internet_Says_On_Scalability_For_May_25%2C_2012.html">1252 high scalability-2012-05-25-Stuff The Internet Says On Scalability For May 25, 2012</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.088), (1, 0.05), (2, -0.023), (3, 0.025), (4, 0.013), (5, 0.009), (6, 0.008), (7, -0.007), (8, -0.007), (9, -0.027), (10, -0.012), (11, 0.019), (12, 0.008), (13, -0.015), (14, 0.015), (15, 0.014), (16, 0.028), (17, 0.019), (18, 0.001), (19, 0.019), (20, -0.005), (21, -0.001), (22, 0.021), (23, 0.018), (24, -0.021), (25, -0.007), (26, 0.021), (27, 0.026), (28, -0.006), (29, 0.014), (30, -0.016), (31, -0.012), (32, 0.028), (33, 0.009), (34, -0.013), (35, 0.03), (36, 0.007), (37, 0.014), (38, 0.003), (39, 0.022), (40, 0.015), (41, -0.039), (42, -0.005), (43, 0.012), (44, 0.019), (45, -0.036), (46, 0.014), (47, 0.008), (48, -0.04), (49, -0.009)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.90071607 <a title="809-lsi-1" href="../high_scalability-2010/high_scalability-2010-04-13-Strategy%3A_Saving_Your_Butt_With_Deferred_Deletes.html">809 high scalability-2010-04-13-Strategy: Saving Your Butt With Deferred Deletes</a></p>
<p>Introduction: Deferred Deletes  is a technique where deleted items  are marked as deleted but not garbage collected until some days or  preferably weeks later .    James Hamilton talks describes this strategy in his classic  On Designing and Deploying  Internet-Scale Services: 
  

Never delete anything. Just mark it deleted. When new data comes in, record the requests on the way. Keep a rolling two week (or more) history of all changes to help recover from software or administrative errors. If someone makes a mistake and forgets the where clause on a delete statement (it has happened before and it will again), all logical copies of the data are deleted. Neither RAID nor mirroring can protect against this form of error. The ability to recover the data can make the difference between a highly embarrassing issue or a minor, barely noticeable glitch. For those systems already doing off-line backups, this additional record of data coming into the service only needs to be since the last backup. But, bein</p><p>2 0.76123816 <a title="809-lsi-2" href="../high_scalability-2007/high_scalability-2007-07-24-Major_Websites_Down%3A_Or_Why_You_Want_to_Run_in_Two_or_More_Data_Centers..html">23 high scalability-2007-07-24-Major Websites Down: Or Why You Want to Run in Two or More Data Centers.</a></p>
<p>Introduction: A lot of sites hosted in San Francisco are down because of at least 6 back-to-back power outages power outages. More details at  laughingsquid .
   
Sites like SecondLife, Craigstlist, Technorati, Yelp and all Six Apart properties, TypePad, LiveJournal and Vox are all down. The cause was an underground explosion in a transformer vault under a manhole at 560 Mission Street. Flames shot 6 feet out from the manhole cover. Over PG&E; 30,000 customers are without power.  What's perplexing is the UPS backup and diesel generators didn't kick in to bring the datacenter back on line. I've never toured that datacenter, but they usually have massive backup systems. It's probably one of those multiple simultaneous failure situations that you hope never happen in real life, but too often do. Or maybe the infrastructure wasn't rolled out completely.  Update: the cause was a cascade of failures in a tightly couples system that could never happen :-) Details at  Failure Happens: A summary of the power</p><p>3 0.72596705 <a title="809-lsi-3" href="../high_scalability-2014/high_scalability-2014-02-03-How_Google_Backs_Up_the_Internet_Along_With_Exabytes_of_Other_Data.html">1589 high scalability-2014-02-03-How Google Backs Up the Internet Along With Exabytes of Other Data</a></p>
<p>Introduction: Raymond Blum    leads a team of Site Reliability Engineers charged with keeping Google's data secret and keeping it safe. Of course Google would never say how much data this actually is, but from comments it seems that it is not yet a    yottabyte   , but is many    exabytes    in size. GMail alone is approaching low exabytes of data. 
   Mr. Blum, in the video    How Google Backs Up the Internet   , explained common backup strategies don’t work for Google for a very googly sounding reason: typically they    scale effort with capacity   . If backing up twice as much data requires twice as much stuff to do it, where stuff is time, energy, space, etc., it won’t work, it doesn’t scale.  You have to find efficiencies so that capacity can scale faster than the effort needed to support that capacity. A different plan is needed when making the jump from backing up one exabyte to backing up two exabytes. And the talk is largely about how Google makes that happen. 
   Some major themes of the t</p><p>4 0.72397411 <a title="809-lsi-4" href="../high_scalability-2012/high_scalability-2012-03-14-The_Azure_Outage%3A_Time_Is_a_SPOF%2C_Leap_Day_Doubly_So.html">1209 high scalability-2012-03-14-The Azure Outage: Time Is a SPOF, Leap Day Doubly So</a></p>
<p>Introduction: This is a guest post by Steve Newman, co-founder of Writely (Google Docs), tech lead on the Paxos-based synchronous replication in Megastore, and founder of cloud service provider  Scalyr.com .  
 
Microsoft’s Azure service suffered a widely publicized outage on February 28th / 29th. Microsoft recently published an excellent  postmortem . For anyone trying to run a high-availability service, this incident can teach several important lessons.
 
The central lesson is that, no matter how much work you put into redundancy, problems will arise. Murphy is strong and, I might say, creative; things go wrong. So preventative measures are important, but how you react to problems is just as important. It’s interesting to review the Azure incident in this light.
 
The postmortem is worth reading in its entirety, but here’s a quick summary: each time Azure launches a new VM, it creates a “transfer certificate” to secure communications with that VM. There was a bug in the code that determines the ce</p><p>5 0.72290808 <a title="809-lsi-5" href="../high_scalability-2011/high_scalability-2011-07-20-Netflix%3A_Harden_Systems_Using_a_Barrel_of_Problem_Causing_Monkeys_-_Latency%2C_Conformity%2C_Doctor%2C_Janitor%2C_Security%2C_Internationalization%2C_Chaos.html">1083 high scalability-2011-07-20-Netflix: Harden Systems Using a Barrel of Problem Causing Monkeys - Latency, Conformity, Doctor, Janitor, Security, Internationalization, Chaos</a></p>
<p>Introduction: With a new Planet of the Apes coming out, this may be a touchy subject with our new overlords, but Netflix is using a whole lot more trouble injecting monkeys to test and iteratively harden their systems. We learned previously how Netflix used  Chaos Monkey , a tool to test failover handling by continuously failing EC2 nodes. That was just a start. More monkeys have been added to the barrel. Node failure is just one problem in a system. Imagine a problem and you can imagine creating a monkey to test if your system is handling that problem properly. Yury Izrailevsky talks about just this approach in this very interesting post:  The Netflix Simian Army .
 
I know what you are thinking, if monkeys are so great then why has Netflix been down lately.  Dmuino addressed  this potential embarrassment, putting all fears of cloud inferiority to rest:
  

Unfortunately we're not running 100% on the cloud today. We're working on it, and we could use more help. The latest outage was caused by a com</p><p>6 0.71620256 <a title="809-lsi-6" href="../high_scalability-2010/high_scalability-2010-09-30-Facebook_and_Site_Failures_Caused_by_Complex%2C_Weakly_Interacting%2C_Layered_Systems.html">910 high scalability-2010-09-30-Facebook and Site Failures Caused by Complex, Weakly Interacting, Layered Systems</a></p>
<p>7 0.71246213 <a title="809-lsi-7" href="../high_scalability-2012/high_scalability-2012-07-13-Stuff_The_Internet_Says_On_Scalability_For_July_13%2C_2012.html">1283 high scalability-2012-07-13-Stuff The Internet Says On Scalability For July 13, 2012</a></p>
<p>8 0.7083565 <a title="809-lsi-8" href="../high_scalability-2011/high_scalability-2011-04-27-Heroku_Emergency_Strategy%3A_Incident_Command_System_and_8_Hour_Ops_Rotations_for_Fresh_Minds.html">1030 high scalability-2011-04-27-Heroku Emergency Strategy: Incident Command System and 8 Hour Ops Rotations for Fresh Minds</a></p>
<p>9 0.70789242 <a title="809-lsi-9" href="../high_scalability-2011/high_scalability-2011-04-06-Netflix%3A_Run_Consistency_Checkers_All_the_time_to_Fixup_Transactions.html">1017 high scalability-2011-04-06-Netflix: Run Consistency Checkers All the time to Fixup Transactions</a></p>
<p>10 0.69523144 <a title="809-lsi-10" href="../high_scalability-2010/high_scalability-2010-12-20-Netflix%3A_Use_Less_Chatty_Protocols_in_the_Cloud_-_Plus_26_Fixes.html">960 high scalability-2010-12-20-Netflix: Use Less Chatty Protocols in the Cloud - Plus 26 Fixes</a></p>
<p>11 0.69164199 <a title="809-lsi-11" href="../high_scalability-2013/high_scalability-2013-12-23-What_Happens_While_Your_Brain_Sleeps_is_Surprisingly_Like_How_Computers_Stay_Sane.html">1568 high scalability-2013-12-23-What Happens While Your Brain Sleeps is Surprisingly Like How Computers Stay Sane</a></p>
<p>12 0.68687868 <a title="809-lsi-12" href="../high_scalability-2012/high_scalability-2012-07-20-Stuff_The_Internet_Says_On_Scalability_For_July_20%2C_2012.html">1287 high scalability-2012-07-20-Stuff The Internet Says On Scalability For July 20, 2012</a></p>
<p>13 0.68666375 <a title="809-lsi-13" href="../high_scalability-2010/high_scalability-2010-10-08-4_Scalability_Themes_from_Surgecon.html">917 high scalability-2010-10-08-4 Scalability Themes from Surgecon</a></p>
<p>14 0.68504155 <a title="809-lsi-14" href="../high_scalability-2012/high_scalability-2012-11-30-Stuff_The_Internet_Says_On_Scalability_For_November_30%2C_2012.html">1365 high scalability-2012-11-30-Stuff The Internet Says On Scalability For November 30, 2012</a></p>
<p>15 0.68500453 <a title="809-lsi-15" href="../high_scalability-2008/high_scalability-2008-07-15-ZooKeeper_-_A_Reliable%2C_Scalable_Distributed_Coordination_System_.html">350 high scalability-2008-07-15-ZooKeeper - A Reliable, Scalable Distributed Coordination System </a></p>
<p>16 0.68368614 <a title="809-lsi-16" href="../high_scalability-2012/high_scalability-2012-09-21-Stuff_The_Internet_Says_On_Scalability_For_September_21%2C_2012.html">1327 high scalability-2012-09-21-Stuff The Internet Says On Scalability For September 21, 2012</a></p>
<p>17 0.67933941 <a title="809-lsi-17" href="../high_scalability-2012/high_scalability-2012-12-14-Stuff_The_Internet_Says_On_Scalability_For_December_14%2C_2012.html">1372 high scalability-2012-12-14-Stuff The Internet Says On Scalability For December 14, 2012</a></p>
<p>18 0.67911661 <a title="809-lsi-18" href="../high_scalability-2014/high_scalability-2014-03-03-The_%E2%80%9CFour_Hamiltons%E2%80%9D_Framework_for_Mitigating_Faults_in_the_Cloud%3A_Avoid_it%2C_Mask_it%2C_Bound_it%2C_Fix_it_Fast.html">1604 high scalability-2014-03-03-The “Four Hamiltons” Framework for Mitigating Faults in the Cloud: Avoid it, Mask it, Bound it, Fix it Fast</a></p>
<p>19 0.67360049 <a title="809-lsi-19" href="../high_scalability-2010/high_scalability-2010-04-28-Elasticity_for_the_Enterprise_--_Ensuring_Continuous_High_Availability_in_a_Disaster_Failure_Scenario.html">816 high scalability-2010-04-28-Elasticity for the Enterprise -- Ensuring Continuous High Availability in a Disaster Failure Scenario</a></p>
<p>20 0.67057657 <a title="809-lsi-20" href="../high_scalability-2011/high_scalability-2011-12-09-Stuff_The_Internet_Says_On_Scalability_For_December_9%2C_2011.html">1154 high scalability-2011-12-09-Stuff The Internet Says On Scalability For December 9, 2011</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.097), (2, 0.228), (10, 0.052), (30, 0.056), (50, 0.292), (61, 0.05), (79, 0.035), (85, 0.048), (94, 0.041)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.9122296 <a title="809-lda-1" href="../high_scalability-2012/high_scalability-2012-11-12-Gone_Fishin%27%3A_Hilarious_Video%3A_Relational_Database_Vs_NoSQL_Fanbois.html">1357 high scalability-2012-11-12-Gone Fishin': Hilarious Video: Relational Database Vs NoSQL Fanbois</a></p>
<p>Introduction: This is an all time favorite post. Even though I've seen this video a hundred times I still can't help but laugh... 
 
This is so funny I laughed until I cried! Definitely NSFW. OMG it's hilarious, but it's also not a bad overview of the issues. Especially loved:  You read the latest post on HighScalability.com and think you are a f*cking Google and architect and parrot slogans like Web Scale and Sharding but you have no idea what the f*ck you are talking about . There are so many more gems like that.
 
  
  
  
  
    
 
Thanks to Alex Popescu for posting this onÂ  MongoDB is Web Scale . Whoever made this deserves a Webby.</p><p>2 0.88268542 <a title="809-lda-2" href="../high_scalability-2010/high_scalability-2010-09-05-Hilarious_Video%3A__Relational_Database_vs_NoSQL_Fanbois.html">895 high scalability-2010-09-05-Hilarious Video:  Relational Database vs NoSQL Fanbois</a></p>
<p>Introduction: This is so funny I laughed until I cried! Definitely NSFW. OMG it's hilarious, but it's also not a bad overview of the issues. Especially loved:  You read the latest post on HighScalability.com and think you are a f*cking Google and architect and parrot slogans like Web Scale and Sharding but you have no idea what the f*ck you are talking about . There are so many more gems like that.
 
  
  
  
  
    
 
Thanks to Alex Popescu for posting this onÂ  MongoDB is Web Scale . Whoever made this deserves a Webby.</p><p>same-blog 3 0.87685329 <a title="809-lda-3" href="../high_scalability-2010/high_scalability-2010-04-13-Strategy%3A_Saving_Your_Butt_With_Deferred_Deletes.html">809 high scalability-2010-04-13-Strategy: Saving Your Butt With Deferred Deletes</a></p>
<p>Introduction: Deferred Deletes  is a technique where deleted items  are marked as deleted but not garbage collected until some days or  preferably weeks later .    James Hamilton talks describes this strategy in his classic  On Designing and Deploying  Internet-Scale Services: 
  

Never delete anything. Just mark it deleted. When new data comes in, record the requests on the way. Keep a rolling two week (or more) history of all changes to help recover from software or administrative errors. If someone makes a mistake and forgets the where clause on a delete statement (it has happened before and it will again), all logical copies of the data are deleted. Neither RAID nor mirroring can protect against this form of error. The ability to recover the data can make the difference between a highly embarrassing issue or a minor, barely noticeable glitch. For those systems already doing off-line backups, this additional record of data coming into the service only needs to be since the last backup. But, bein</p><p>4 0.87597507 <a title="809-lda-4" href="../high_scalability-2009/high_scalability-2009-11-26-What_I%27m_Thankful_For_on_Thanksgiving.html">747 high scalability-2009-11-26-What I'm Thankful For on Thanksgiving</a></p>
<p>Introduction: I try to keep this blog targeted and on topic. So even though I may be thankful for the  song  of the tinniest  sparrow  at  sunrise , I'll save you from all that. It's hard to tie scalability and the  giving of thanks  together, especially as it sometimes occurs to me that this blog may be a self-indulgent waste of time. But I think I found a sentiment in  A New THEORY of AWESOMENESS and MIRACLES  by  James Bridle  that manages to marry the topic of this blog and giving thanks meaningfully together:
  

I distrust commercial definitions of innovation, and particularly of awesomeness. It’s an overused term. When I think of awesomeness, I want something awe-inspiring, vast and mind-expanding.


  So I started thinking about things that I think are awesome, or miraculous, and for me, it kept coming back to scale and complexity.  


     


We’re not actually very good about thinking about scale and complexity in real terms, so we have to use metaphors and examples. Douglas Adams writes s</p><p>5 0.83702523 <a title="809-lda-5" href="../high_scalability-2009/high_scalability-2009-04-29-Presentations%3A_MySQL_Conference_%26_Expo_2009.html">586 high scalability-2009-04-29-Presentations: MySQL Conference & Expo 2009</a></p>
<p>Introduction: The Presentations of the   MySQL Conference & Expo 2009   held April 20-23 in Santa Clara is available on the above link.  They include:
  
 Beginner's Guide to Website Performance with MySQL and memcached by Adam Donnison 
 
 Calpont: Open Source Columnar Storage Engine for Scalable MySQL DW by Jim Tommaney 
 
 Creating Quick and Powerful Web Applications with MySQL, GlassFish, and NetBeans by Arun Gupta 
 
 Deep-inspecting MySQL with DTrace by Domas Mituzas 
 
 Distributed Innodb Caching with memcached by Matthew Yonkovit and Yves Trudeau 
 
 Improving Performance by Running MySQL Multiple Times by MC Brown 
 
 Introduction to Using DTrace with MySQL by Vince Carbone 
 
 MySQL Cluster 7.0 - New Features by Johan Andersson 
 
 Optimizing MySQL Performance with ZFS by Allan Packer 
 
 SAN Performance on a Internal Disk Budget: The Coming Solid State Disk Revolution by Matthew Yonkovit 
 
 This is Not a Web App: The Evolution of a MySQL Deployment at Google by Mark Callaghan</p><p>6 0.76525736 <a title="809-lda-6" href="../high_scalability-2008/high_scalability-2008-02-03-Product%3A_Collectl_-_Performance_Data_Collector.html">237 high scalability-2008-02-03-Product: Collectl - Performance Data Collector</a></p>
<p>7 0.76381946 <a title="809-lda-7" href="../high_scalability-2009/high_scalability-2009-07-16-Scaling_Traffic%3A_People_Pod_Pool_of_On_Demand_Self_Driving_Robotic_Cars_who_Automatically_Refuel_from_Cheap_Solar.html">657 high scalability-2009-07-16-Scaling Traffic: People Pod Pool of On Demand Self Driving Robotic Cars who Automatically Refuel from Cheap Solar</a></p>
<p>8 0.75981164 <a title="809-lda-8" href="../high_scalability-2013/high_scalability-2013-06-28-Stuff_The_Internet_Says_On_Scalability_For_June_28%2C_2013.html">1484 high scalability-2013-06-28-Stuff The Internet Says On Scalability For June 28, 2013</a></p>
<p>9 0.70281196 <a title="809-lda-9" href="../high_scalability-2009/high_scalability-2009-07-27-Handle_700_Percent_More_Requests_Using_Squid_and_APC_Cache.html">662 high scalability-2009-07-27-Handle 700 Percent More Requests Using Squid and APC Cache</a></p>
<p>10 0.68360865 <a title="809-lda-10" href="../high_scalability-2012/high_scalability-2012-09-04-Changing_Architectures%3A_New_Datacenter_Networks_Will_Set_Your_Code_and_Data_Free___.html">1316 high scalability-2012-09-04-Changing Architectures: New Datacenter Networks Will Set Your Code and Data Free   </a></p>
<p>11 0.67878109 <a title="809-lda-11" href="../high_scalability-2009/high_scalability-2009-07-25-Latency_is_Everywhere_and_it_Costs_You_Sales_-_How_to_Crush_it.html">661 high scalability-2009-07-25-Latency is Everywhere and it Costs You Sales - How to Crush it</a></p>
<p>12 0.67088807 <a title="809-lda-12" href="../high_scalability-2011/high_scalability-2011-09-13-Must_see%3A_5_Steps_to_Scaling_MongoDB_%28Or_Any_DB%29_in_8_Minutes.html">1114 high scalability-2011-09-13-Must see: 5 Steps to Scaling MongoDB (Or Any DB) in 8 Minutes</a></p>
<p>13 0.66874605 <a title="809-lda-13" href="../high_scalability-2012/high_scalability-2012-05-11-Stuff_The_Internet_Says_On_Scalability_For_May_11%2C_2012.html">1244 high scalability-2012-05-11-Stuff The Internet Says On Scalability For May 11, 2012</a></p>
<p>14 0.66387409 <a title="809-lda-14" href="../high_scalability-2011/high_scalability-2011-06-20-35%2B_Use_Cases_for_Choosing_Your_Next_NoSQL_Database.html">1064 high scalability-2011-06-20-35+ Use Cases for Choosing Your Next NoSQL Database</a></p>
<p>15 0.66298705 <a title="809-lda-15" href="../high_scalability-2007/high_scalability-2007-07-10-mixi.jp__Architecture.html">5 high scalability-2007-07-10-mixi.jp  Architecture</a></p>
<p>16 0.66289318 <a title="809-lda-16" href="../high_scalability-2013/high_scalability-2013-02-15-Stuff_The_Internet_Says_On_Scalability_For_February_15%2C_2013.html">1407 high scalability-2013-02-15-Stuff The Internet Says On Scalability For February 15, 2013</a></p>
<p>17 0.66232318 <a title="809-lda-17" href="../high_scalability-2010/high_scalability-2010-11-15-Strategy%3A_Biggest_Performance_Impact_is_to_Reduce_the_Number_of_HTTP_Requests.html">942 high scalability-2010-11-15-Strategy: Biggest Performance Impact is to Reduce the Number of HTTP Requests</a></p>
<p>18 0.66180348 <a title="809-lda-18" href="../high_scalability-2008/high_scalability-2008-02-21-Tracking_usage_of_public_resources_-_throttling_accesses_per_hour.html">256 high scalability-2008-02-21-Tracking usage of public resources - throttling accesses per hour</a></p>
<p>19 0.66013718 <a title="809-lda-19" href="../high_scalability-2012/high_scalability-2012-10-24-Saving_Cash_Using_Less_Cache_-__90%25_Savings_in_the_Caching_Tier.html">1346 high scalability-2012-10-24-Saving Cash Using Less Cache -  90% Savings in the Caching Tier</a></p>
<p>20 0.65991277 <a title="809-lda-20" href="../high_scalability-2010/high_scalability-2010-05-03-MocoSpace_Architecture_-_3_Billion_Mobile_Page_Views_a_Month.html">821 high scalability-2010-05-03-MocoSpace Architecture - 3 Billion Mobile Page Views a Month</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
