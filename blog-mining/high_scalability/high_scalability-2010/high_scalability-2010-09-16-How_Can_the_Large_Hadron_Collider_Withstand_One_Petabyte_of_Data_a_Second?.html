<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>901 high scalability-2010-09-16-How Can the Large Hadron Collider Withstand One Petabyte of Data a Second?</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2010" href="../home/high_scalability-2010_home.html">high_scalability-2010</a> <a title="high_scalability-2010-901" href="#">high_scalability-2010-901</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>901 high scalability-2010-09-16-How Can the Large Hadron Collider Withstand One Petabyte of Data a Second?</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2010-901-html" href="http://highscalability.com//blog/2010/9/16/how-can-the-large-hadron-collider-withstand-one-petabyte-of.html">html</a></p><p>Introduction: Why is there something rather than nothing? That's the kind of question
theLarge Hadron Colliderin CERN is hopefully poised to answer. And what is the
output of this beautiful 17-mile long,6 billion dollar wabi-sabishproton
smashing machine? Data. Great heaping torrents of Grand Canyon sized data. 15
million gigabytes every year. That's 1000 times the information printed in
books every year. It's so much data 10,000 scientists will use agridof80,000+
computers, in 300 computer centers , in 50 different countries just to help
make sense of it all.How will all this data be collected, transported, stored,
and analyzed? It turns out, using what amounts to sort of Internet of
Particles instead of an Internet of Things.Two good articles have recently
shed some electro-magnetic energy in the human visible spectrum on the IT
aspects of the collider:LHC computing grid pushes petabytes of data, beats
expectations by John Timmer on Ars Technica and an overview of theBrookhaven's
RHIC/ATLAS Comput</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 It's so much data 10,000 scientists will use agridof80,000+ computers, in 300 computer centers , in 50 different countries just to help make sense of it all. [sent-8, score-0.222]
</p><p>2 Two good articles have recently shed some electro-magnetic energy in the human visible spectrum on the IT aspects of the collider:LHC computing grid pushes petabytes of data, beats expectations by John Timmer on Ars Technica and an overview of theBrookhaven's RHIC/ATLAS Computing Facility. [sent-11, score-0.269]
</p><p>3 For Main Detectors Generate the DataFirst we need to know what produces the data: detectors that implement particular science experiments. [sent-13, score-0.353]
</p><p>4 The detectors are sensors that "watch" the collision for various attributes and send those samples on for analysis. [sent-14, score-0.428]
</p><p>5 In principle these are no different than a temperature sensor, but in practice some of the detectors have more parts than the Space Station. [sent-15, score-0.294]
</p><p>6 SummarySome of the interesting factoids about the system, taken from the above mentioned articles, and from some great comments, are:More data is produced by the detectors than can be stored. [sent-25, score-0.446]
</p><p>7 2,000 CPUs at each of the four detectors filter the readings so only the interesting collisions are kept. [sent-26, score-0.392]
</p><p>8 They're reconstructed and reformatted in large computing clusters for further decisions. [sent-31, score-0.254]
</p><p>9 Though the data stream is serial, events are discrete -- so it's easy to buffer writes across a number of systems that feed into the same large storage pool. [sent-33, score-0.176]
</p><p>10 Data is streamed to the main compute facility on a dedicated 10Gbps connection. [sent-34, score-0.221]
</p><p>11 The grid is set up in a tiered system to distribute data down a tree of sites:Tier-0: the Large Hadron Collider located at CERN. [sent-40, score-0.232]
</p><p>12 Only a fraction of the raw data will be stored, processed, and analyzed at Tier-1 facilities. [sent-43, score-0.157]
</p><p>13 Tier-2 facilities provide data storage and processing capacities for more in-depth user analysis and simulation. [sent-45, score-0.239]
</p><p>14 Brookhaven's RHIC/ATLAS Computing Facility is the only Tier-1 computing facility for ATLAS in the United States. [sent-47, score-0.19]
</p><p>15 The amount of data the comes in from CERN would fill a CD per second. [sent-51, score-0.156]
</p><p>16 Instead, you run over it once and select the only the data that is interesting for your analysis and thereafter your jobs only look at that. [sent-66, score-0.23]
</p><p>17 The network works so well that they've been able to invert the typical execute the job where the data is paradigm which says it's more efficient to send code to where the data is than it is to send data to the code. [sent-67, score-0.443]
</p><p>18 Filter out unwanted data before it hits the main part of your system. [sent-79, score-0.168]
</p><p>19 It's quite surprising that a fast network makes it possible to send data to code instead of code to data. [sent-82, score-0.168]
</p><p>20 SciDB - A Science-Oriented DBMS At 100 PetabytesOpen Science Grid Consortium - The Open Science Grid was created in order to facilitate data analysis from the Large Hadron Collider, and about 70% of its 300,000 computing-hours per day are dedicated to the analysis of data from particle colliders. [sent-96, score-0.425]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hadron', 0.387), ('collider', 0.361), ('detectors', 0.294), ('atlas', 0.294), ('lhc', 0.269), ('cern', 0.225), ('grid', 0.125), ('data', 0.107), ('lhcb', 0.099), ('facility', 0.095), ('computing', 0.095), ('physicists', 0.09), ('physicist', 0.084), ('particles', 0.077), ('analysis', 0.073), ('collision', 0.073), ('detector', 0.071), ('large', 0.069), ('dedicated', 0.065), ('tape', 0.065), ('send', 0.061), ('main', 0.061), ('facilities', 0.059), ('science', 0.059), ('scientists', 0.058), ('disks', 0.058), ('countries', 0.057), ('sensor', 0.056), ('experiments', 0.054), ('filter', 0.053), ('dataset', 0.052), ('gigabytes', 0.051), ('jobs', 0.05), ('analyzed', 0.05), ('anywhere', 0.049), ('appear', 0.049), ('articles', 0.049), ('fill', 0.049), ('torrents', 0.045), ('heaping', 0.045), ('ars', 0.045), ('cox', 0.045), ('crunched', 0.045), ('factoids', 0.045), ('higgs', 0.045), ('miniature', 0.045), ('readings', 0.045), ('reconstructed', 0.045), ('reformatted', 0.045), ('spheres', 0.045)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999976 <a title="901-tfidf-1" href="../high_scalability-2010/high_scalability-2010-09-16-How_Can_the_Large_Hadron_Collider_Withstand_One_Petabyte_of_Data_a_Second%3F.html">901 high scalability-2010-09-16-How Can the Large Hadron Collider Withstand One Petabyte of Data a Second?</a></p>
<p>Introduction: Why is there something rather than nothing? That's the kind of question
theLarge Hadron Colliderin CERN is hopefully poised to answer. And what is the
output of this beautiful 17-mile long,6 billion dollar wabi-sabishproton
smashing machine? Data. Great heaping torrents of Grand Canyon sized data. 15
million gigabytes every year. That's 1000 times the information printed in
books every year. It's so much data 10,000 scientists will use agridof80,000+
computers, in 300 computer centers , in 50 different countries just to help
make sense of it all.How will all this data be collected, transported, stored,
and analyzed? It turns out, using what amounts to sort of Internet of
Particles instead of an Internet of Things.Two good articles have recently
shed some electro-magnetic energy in the human visible spectrum on the IT
aspects of the collider:LHC computing grid pushes petabytes of data, beats
expectations by John Timmer on Ars Technica and an overview of theBrookhaven's
RHIC/ATLAS Comput</p><p>2 0.29750666 <a title="901-tfidf-2" href="../high_scalability-2010/high_scalability-2010-09-16-Strategy%3A_Buy_New%2C_Don%27t_Fix_the_Old.html">902 high scalability-2010-09-16-Strategy: Buy New, Don't Fix the Old</a></p>
<p>Introduction: This strategy is from theLarge Hadron Colliderproject:Improvements in
performance per Watt have caused CERN to no longer sign hardware support
contracts longer than three years. Machines run until they die. They have a
very high utilization of equipment ('duty cycle', 7 x 24 x 365). Replacing
hardware makes more sense because of the lower cost and the power savings of
new hardware.</p><p>3 0.1326592 <a title="901-tfidf-3" href="../high_scalability-2012/high_scalability-2012-11-05-Gone_Fishin%27%3A_Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_In_The_Ambient_Cloud.html">1355 high scalability-2012-11-05-Gone Fishin': Building Super Scalable Systems: Blade Runner Meets Autonomic Computing In The Ambient Cloud</a></p>
<p>Introduction: All in all this is still my favorite post and I still think it's an accurate
vision of a future. Not everyone agrees, but I guess we'll see..."But it is
not complicated. [There's] just a lot of it." \--Richard Feynmanon how the
immense variety of the world arises from simple rules.Contents:Have We Reached
the End of Scaling?Applications Become Black Boxes Using Markets to Scale and
Control CostsLet's Welcome our Neo-Feudal OverlordsThe Economic Argument for
the Ambient CloudWhat Will Kill the Cloud?The Amazing Collective Compute Power
of the Ambient CloudUsing the Ambient Cloud as an Application
RuntimeApplications as Virtual StatesConclusionWe have not yet begun to scale.
The world is still fundamentally disconnected and for all our wisdom we are
still in the earliest days of learning how to build truly large planet-scaling
applications.Today 350 million users on Facebook is a lot of users and five
million followers on Twitter is a lot of followers. This may seem like a lot
now, but c</p><p>4 0.13257818 <a title="901-tfidf-4" href="../high_scalability-2009/high_scalability-2009-12-16-Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_in_the_Ambient_Cloud.html">750 high scalability-2009-12-16-Building Super Scalable Systems: Blade Runner Meets Autonomic Computing in the Ambient Cloud</a></p>
<p>Introduction: "But it is not complicated. [There's] just a lot of it." \--Richard Feynmanon
how the immense variety of the world arises from simple rules.Contents:Have We
Reached the End of Scaling?Applications Become Black Boxes Using Markets to
Scale and Control CostsLet's Welcome our Neo-Feudal OverlordsThe Economic
Argument for the Ambient CloudWhat Will Kill the Cloud?The Amazing Collective
Compute Power of the Ambient CloudUsing the Ambient Cloud as an Application
RuntimeApplications as Virtual StatesConclusionWe have not yet begun to scale.
The world is still fundamentally disconnected and for all our wisdom we are
still in the earliest days of learning how to build truly large planet-scaling
applications.Today 350 million users on Facebook is a lot of users and five
million followers on Twitter is a lot of followers. This may seem like a lot
now, but consider we have no planet wide applications yet. None.Tomorrow the
numbers foreshadow a newCambrian explosionof connectivity that will look as</p><p>5 0.12084085 <a title="901-tfidf-5" href="../high_scalability-2010/high_scalability-2010-06-03-Hot_Scalability_Links_for_June_3%2C_2010.html">835 high scalability-2010-06-03-Hot Scalability Links for June 3, 2010</a></p>
<p>Introduction: How Big is a Yottabyte? Not so big that the NSA can't hope to store it
saysCrunchGear: There are a thousand gigabytes in a terabyte, a thousand
terabytes in a petabyte, a thousand petabytes in an exabyte, a thousand
exabytes in a zettabyte, and a thousand zettabytes in a yottabyte. In other
words, a yottabyte is 1,000,000,000,000,000GB.The CMS data aggregation system.
The Large Hadron Collider project is usingMongoDB as a cache.Here we discuss a
new data aggregation system which consumes, indexes and delivers information
from different relational and non-relational data sources to answer cross
data-service queries and explore meta-data associated with petabytes of
experimental data.Google I/O 2010 Videosare up available (many of them
anyway). You might be particularly interested inGoogle Storage for Developers,
Building high-throughput data pipelines with Google App Engine, Batch data
processing with App Engine,BigQuery and Prediction APIs, andMeasure in
milliseconds redux: Meet Speed</p><p>6 0.117555 <a title="901-tfidf-6" href="../high_scalability-2010/high_scalability-2010-12-06-What_the_heck_are_you_actually_using_NoSQL_for%3F.html">954 high scalability-2010-12-06-What the heck are you actually using NoSQL for?</a></p>
<p>7 0.11188807 <a title="901-tfidf-7" href="../high_scalability-2009/high_scalability-2009-03-16-Are_Cloud_Based_Memory_Architectures_the_Next_Big_Thing%3F.html">538 high scalability-2009-03-16-Are Cloud Based Memory Architectures the Next Big Thing?</a></p>
<p>8 0.099810347 <a title="901-tfidf-8" href="../high_scalability-2010/high_scalability-2010-03-02-Using_the_Ambient_Cloud_as_an_Application_Runtime.html">786 high scalability-2010-03-02-Using the Ambient Cloud as an Application Runtime</a></p>
<p>9 0.099308431 <a title="901-tfidf-9" href="../high_scalability-2013/high_scalability-2013-10-04-Stuff_The_Internet_Says_On_Scalability_For_October_4th%2C_2013.html">1527 high scalability-2013-10-04-Stuff The Internet Says On Scalability For October 4th, 2013</a></p>
<p>10 0.098385118 <a title="901-tfidf-10" href="../high_scalability-2012/high_scalability-2012-02-02-The_Data-Scope_Project_-_6PB_storage%2C_500GBytes-sec_sequential_IO%2C_20M_IOPS%2C_130TFlops.html">1186 high scalability-2012-02-02-The Data-Scope Project - 6PB storage, 500GBytes-sec sequential IO, 20M IOPS, 130TFlops</a></p>
<p>11 0.096580677 <a title="901-tfidf-11" href="../high_scalability-2008/high_scalability-2008-11-22-Google_Architecture.html">448 high scalability-2008-11-22-Google Architecture</a></p>
<p>12 0.094314843 <a title="901-tfidf-12" href="../high_scalability-2010/high_scalability-2010-03-30-Running_Large_Graph_Algorithms_-_Evaluation_of_Current_State-of-the-Art_and_Lessons_Learned.html">801 high scalability-2010-03-30-Running Large Graph Algorithms - Evaluation of Current State-of-the-Art and Lessons Learned</a></p>
<p>13 0.092088982 <a title="901-tfidf-13" href="../high_scalability-2010/high_scalability-2010-02-01-What_Will_Kill_the_Cloud%3F.html">768 high scalability-2010-02-01-What Will Kill the Cloud?</a></p>
<p>14 0.091600947 <a title="901-tfidf-14" href="../high_scalability-2007/high_scalability-2007-07-30-Product%3A_Sun_Utility_Computing.html">46 high scalability-2007-07-30-Product: Sun Utility Computing</a></p>
<p>15 0.091207266 <a title="901-tfidf-15" href="../high_scalability-2009/high_scalability-2009-08-31-Squarespace_Architecture_-_A_Grid_Handles_Hundreds_of_Millions_of_Requests_a_Month_.html">691 high scalability-2009-08-31-Squarespace Architecture - A Grid Handles Hundreds of Millions of Requests a Month </a></p>
<p>16 0.090882435 <a title="901-tfidf-16" href="../high_scalability-2014/high_scalability-2014-02-03-How_Google_Backs_Up_the_Internet_Along_With_Exabytes_of_Other_Data.html">1589 high scalability-2014-02-03-How Google Backs Up the Internet Along With Exabytes of Other Data</a></p>
<p>17 0.089821063 <a title="901-tfidf-17" href="../high_scalability-2011/high_scalability-2011-03-08-Medialets_Architecture_-__Defeating_the_Daunting_Mobile_Device_Data_Deluge.html">1000 high scalability-2011-03-08-Medialets Architecture -  Defeating the Daunting Mobile Device Data Deluge</a></p>
<p>18 0.089376457 <a title="901-tfidf-18" href="../high_scalability-2011/high_scalability-2011-07-15-Stuff_The_Internet_Says_On_Scalability_For_July_15%2C_2011.html">1080 high scalability-2011-07-15-Stuff The Internet Says On Scalability For July 15, 2011</a></p>
<p>19 0.08886978 <a title="901-tfidf-19" href="../high_scalability-2010/high_scalability-2010-02-15-The_Amazing_Collective_Compute_Power_of_the_Ambient_Cloud.html">778 high scalability-2010-02-15-The Amazing Collective Compute Power of the Ambient Cloud</a></p>
<p>20 0.087263331 <a title="901-tfidf-20" href="../high_scalability-2012/high_scalability-2012-08-30-Stuff_The_Internet_Says_On_Scalability_For_August_31%2C_2012.html">1315 high scalability-2012-08-30-Stuff The Internet Says On Scalability For August 31, 2012</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.171), (1, 0.084), (2, 0.034), (3, 0.039), (4, -0.069), (5, 0.023), (6, 0.016), (7, 0.029), (8, 0.006), (9, 0.06), (10, 0.023), (11, -0.01), (12, 0.042), (13, 0.012), (14, 0.063), (15, 0.027), (16, -0.031), (17, 0.006), (18, -0.024), (19, 0.052), (20, -0.049), (21, -0.006), (22, 0.012), (23, 0.029), (24, 0.015), (25, -0.046), (26, -0.01), (27, 0.052), (28, -0.079), (29, 0.028), (30, -0.016), (31, -0.003), (32, -0.002), (33, 0.028), (34, -0.004), (35, -0.012), (36, 0.034), (37, 0.014), (38, -0.011), (39, 0.005), (40, -0.003), (41, -0.05), (42, 0.017), (43, -0.005), (44, 0.053), (45, -0.041), (46, -0.003), (47, -0.019), (48, -0.054), (49, -0.008)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94919664 <a title="901-lsi-1" href="../high_scalability-2010/high_scalability-2010-09-16-How_Can_the_Large_Hadron_Collider_Withstand_One_Petabyte_of_Data_a_Second%3F.html">901 high scalability-2010-09-16-How Can the Large Hadron Collider Withstand One Petabyte of Data a Second?</a></p>
<p>Introduction: Why is there something rather than nothing? That's the kind of question
theLarge Hadron Colliderin CERN is hopefully poised to answer. And what is the
output of this beautiful 17-mile long,6 billion dollar wabi-sabishproton
smashing machine? Data. Great heaping torrents of Grand Canyon sized data. 15
million gigabytes every year. That's 1000 times the information printed in
books every year. It's so much data 10,000 scientists will use agridof80,000+
computers, in 300 computer centers , in 50 different countries just to help
make sense of it all.How will all this data be collected, transported, stored,
and analyzed? It turns out, using what amounts to sort of Internet of
Particles instead of an Internet of Things.Two good articles have recently
shed some electro-magnetic energy in the human visible spectrum on the IT
aspects of the collider:LHC computing grid pushes petabytes of data, beats
expectations by John Timmer on Ars Technica and an overview of theBrookhaven's
RHIC/ATLAS Comput</p><p>2 0.83783019 <a title="901-lsi-2" href="../high_scalability-2010/high_scalability-2010-07-07-Strategy%3A_Recompute_Instead_of_Remember_Big_Data.html">852 high scalability-2010-07-07-Strategy: Recompute Instead of Remember Big Data</a></p>
<p>Introduction: Professor Lance Fortnow, in his blog post Drowning in Data, says complexity
has taught him this lesson:When storage is expensive, it is cheaper to
recompute what you've already computed. And that's the world we now live in:
Storage is pretty cheap but data acquisition and computation are even
cheaper.Jouni, one of the commenters, thinks the opposite is true:storage is
cheap, but computation is expensive. When you are dealing with massive data,
the size of the data set is very often determined by the amount of computing
power available for a certain price.With such data, a linear-time algorithm
takes O(1) seconds to finish, while a quadratic-time algorithm requires O(n)
seconds. But as computing power increases exponentially over time, the
quadratic algorithm gets exponentially slower.For me it's not a matter of
which is true, both positions can be true, but what's interesting is to think
that storage and computation are in some cases fungible. Your architecture can
decide which tradeof</p><p>3 0.81489927 <a title="901-lsi-3" href="../high_scalability-2010/high_scalability-2010-05-05-How_will_memristors_change_everything%3F_.html">823 high scalability-2010-05-05-How will memristors change everything? </a></p>
<p>Introduction: A non-random sample of my tech friends shows that not many have heard
ofmemristors(though I do suspect vote tampering). I'd read a little about
memristors in2008when the initial hubbub about the existence of memristors was
raised. I, however,  immediately filed them into that comforting conceptual
bucket of potentially revolutionary technologies I didn't have to worry about
because like most wondertech, nothing would ever come of it. Wrong. After
watchingFinding the Missing Memristorby R. Stanley Williams I've had to change
my mind. Memristors have gone from "maybe never" to holy cow this could happen
soon and it could change everything.Let's assume for the sake of dreaming
memristors do prove out. How will we design systems when we have access to a
new material that is two orders of magnitude more efficient from a power
perspective than traditional transistor technologies, contains multiple
petabits (1 petabit = 128TB) of persistent storage, and can be reconfigured to
be either memory</p><p>4 0.79177672 <a title="901-lsi-4" href="../high_scalability-2012/high_scalability-2012-02-02-The_Data-Scope_Project_-_6PB_storage%2C_500GBytes-sec_sequential_IO%2C_20M_IOPS%2C_130TFlops.html">1186 high scalability-2012-02-02-The Data-Scope Project - 6PB storage, 500GBytes-sec sequential IO, 20M IOPS, 130TFlops</a></p>
<p>Introduction: "Data is everywhere, never be at a single location. Not scalable, not
maintainable."-Alex SzalayWhile Galileo played life and death doctrinal games
over the mysteries revealed by the telescope, another revolution went
unnoticed, the microscope gave up mystery after mystery and nobody yet
understood how subversive would be what it revealed. For the first time these
new tools of perceptual augmentation allowed humans to peek behind the veil of
appearance. A new new eye driving human invention and discovery for hundreds
of years.Data is anothermaterialthat hides, revealing itself only when we look
at different scales and investigate its underlying patterns. If the universe
is trulymade of information, then we are looking into truly primal stuff. A
new eye is needed for Data and an ambitious project calledData-scopeaims to be
the lens.A detailedpaperon the Data-Scope tells more about what it is:The
Data-Scope is a new scientific instrument, capable of 'observing' immense
volumes of data fr</p><p>5 0.77058196 <a title="901-lsi-5" href="../high_scalability-2010/high_scalability-2010-08-18-Misco%3A_A_MapReduce_Framework_for_Mobile_Systems_-_Start_of_the_Ambient_Cloud%3F.html">882 high scalability-2010-08-18-Misco: A MapReduce Framework for Mobile Systems - Start of the Ambient Cloud?</a></p>
<p>Introduction: Misco: A MapReduce Framework for Mobile Systems is a very exciting paper to me
because it's really one of the first explorations of some of the ideas in
Building Super Scalable Systems: Blade Runner Meets Autonomic Computing in the
Ambient Cloud. What they are trying to do is efficiently distribute work
across a set cellphones using a now familiar MapReduce interface. Usually we
think of MapReduce as working across large data center hosted clusters. Here,
the cluster nodes are cellphones not contained in any data center, but compute
nodes potentially distributed everywhere.I talked withAdam Dou, one of the
paper's authors, and he said they don't see cellphone clusters replacing
dedicated computer clusters, primarily because of thepower requiredfor both
network communication and the map-reduce computations. Large multi-terabyte
jobs aren't in the cards...yet. Adam estimates computationally that cellphones
are performing similarly to desktops of ten years ago. Instead, they want to
focus</p><p>6 0.76988953 <a title="901-lsi-6" href="../high_scalability-2008/high_scalability-2008-09-25-GridGain%3A__One_Compute_Grid%2C_Many_Data_Grids.html">393 high scalability-2008-09-25-GridGain:  One Compute Grid, Many Data Grids</a></p>
<p>7 0.75982642 <a title="901-lsi-7" href="../high_scalability-2008/high_scalability-2008-09-26-Lucasfilm%3A_The_Real_Magic_is_in_the_Data_Center.html">396 high scalability-2008-09-26-Lucasfilm: The Real Magic is in the Data Center</a></p>
<p>8 0.75794798 <a title="901-lsi-8" href="../high_scalability-2009/high_scalability-2009-07-30-Learn_How_to_Think_at_Scale.html">666 high scalability-2009-07-30-Learn How to Think at Scale</a></p>
<p>9 0.75361651 <a title="901-lsi-9" href="../high_scalability-2013/high_scalability-2013-03-27-The_Changing_Face_of_Scale_-_The_Downside_of_Scaling_in_the_Contextual_Age_.html">1430 high scalability-2013-03-27-The Changing Face of Scale - The Downside of Scaling in the Contextual Age </a></p>
<p>10 0.74860656 <a title="901-lsi-10" href="../high_scalability-2012/high_scalability-2012-07-20-Stuff_The_Internet_Says_On_Scalability_For_July_20%2C_2012.html">1287 high scalability-2012-07-20-Stuff The Internet Says On Scalability For July 20, 2012</a></p>
<p>11 0.74843609 <a title="901-lsi-11" href="../high_scalability-2008/high_scalability-2008-11-22-Google_Architecture.html">448 high scalability-2008-11-22-Google Architecture</a></p>
<p>12 0.74768209 <a title="901-lsi-12" href="../high_scalability-2007/high_scalability-2007-08-21-What_does_the_next_generation_data_center_look_like%3F.html">69 high scalability-2007-08-21-What does the next generation data center look like?</a></p>
<p>13 0.7412917 <a title="901-lsi-13" href="../high_scalability-2014/high_scalability-2014-02-21-Stuff_The_Internet_Says_On_Scalability_For_February_21st%2C_2014.html">1600 high scalability-2014-02-21-Stuff The Internet Says On Scalability For February 21st, 2014</a></p>
<p>14 0.73824841 <a title="901-lsi-14" href="../high_scalability-2011/high_scalability-2011-05-05-Paper%3A_A_Study_of_Practical_Deduplication.html">1035 high scalability-2011-05-05-Paper: A Study of Practical Deduplication</a></p>
<p>15 0.73238128 <a title="901-lsi-15" href="../high_scalability-2011/high_scalability-2011-03-08-Medialets_Architecture_-__Defeating_the_Daunting_Mobile_Device_Data_Deluge.html">1000 high scalability-2011-03-08-Medialets Architecture -  Defeating the Daunting Mobile Device Data Deluge</a></p>
<p>16 0.72486341 <a title="901-lsi-16" href="../high_scalability-2013/high_scalability-2013-11-15-Stuff_The_Internet_Says_On_Scalability_For_November_15th%2C_2013.html">1549 high scalability-2013-11-15-Stuff The Internet Says On Scalability For November 15th, 2013</a></p>
<p>17 0.72414237 <a title="901-lsi-17" href="../high_scalability-2012/high_scalability-2012-09-11-How_big_is_a_Petabyte%2C_Exabyte%2C_Zettabyte%2C_or_a_Yottabyte%3F.html">1320 high scalability-2012-09-11-How big is a Petabyte, Exabyte, Zettabyte, or a Yottabyte?</a></p>
<p>18 0.72391927 <a title="901-lsi-18" href="../high_scalability-2007/high_scalability-2007-10-20-Should_you_build_your_next_website_using_3tera%27s_grid_OS%3F.html">126 high scalability-2007-10-20-Should you build your next website using 3tera's grid OS?</a></p>
<p>19 0.71902949 <a title="901-lsi-19" href="../high_scalability-2010/high_scalability-2010-12-08-How_To_Get_Experience_Working_With_Large_Datasets.html">956 high scalability-2010-12-08-How To Get Experience Working With Large Datasets</a></p>
<p>20 0.71865863 <a title="901-lsi-20" href="../high_scalability-2010/high_scalability-2010-03-02-Using_the_Ambient_Cloud_as_an_Application_Runtime.html">786 high scalability-2010-03-02-Using the Ambient Cloud as an Application Runtime</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.115), (2, 0.159), (8, 0.012), (10, 0.053), (26, 0.017), (30, 0.03), (39, 0.227), (42, 0.014), (47, 0.015), (61, 0.056), (77, 0.022), (79, 0.103), (85, 0.046), (90, 0.022), (94, 0.027)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.86097944 <a title="901-lda-1" href="../high_scalability-2010/high_scalability-2010-09-16-How_Can_the_Large_Hadron_Collider_Withstand_One_Petabyte_of_Data_a_Second%3F.html">901 high scalability-2010-09-16-How Can the Large Hadron Collider Withstand One Petabyte of Data a Second?</a></p>
<p>Introduction: Why is there something rather than nothing? That's the kind of question
theLarge Hadron Colliderin CERN is hopefully poised to answer. And what is the
output of this beautiful 17-mile long,6 billion dollar wabi-sabishproton
smashing machine? Data. Great heaping torrents of Grand Canyon sized data. 15
million gigabytes every year. That's 1000 times the information printed in
books every year. It's so much data 10,000 scientists will use agridof80,000+
computers, in 300 computer centers , in 50 different countries just to help
make sense of it all.How will all this data be collected, transported, stored,
and analyzed? It turns out, using what amounts to sort of Internet of
Particles instead of an Internet of Things.Two good articles have recently
shed some electro-magnetic energy in the human visible spectrum on the IT
aspects of the collider:LHC computing grid pushes petabytes of data, beats
expectations by John Timmer on Ars Technica and an overview of theBrookhaven's
RHIC/ATLAS Comput</p><p>2 0.83864778 <a title="901-lda-2" href="../high_scalability-2009/high_scalability-2009-04-29-How_to_choice_and_build_perfect_server.html">585 high scalability-2009-04-29-How to choice and build perfect server</a></p>
<p>Introduction: There are a lot of questions about the server components, and how to choice
and/or build perfect server with consider the power consumption. So I decide
towrite about this topic.Key Points:What kind of components the servers
needsThe Green Computing and the Servers components.How much power the server
consume.Choice the right components: Processors, HDD, RAID, MemoryBuild
Server, or buy?</p><p>3 0.82645369 <a title="901-lda-3" href="../high_scalability-2010/high_scalability-2010-06-16-Hot_Scalability_Links_for_June_16%2C_2010.html">842 high scalability-2010-06-16-Hot Scalability Links for June 16, 2010</a></p>
<p>Introduction: You're Doing it Wrong by Poul-Henning Kamp. Don't look so guilty, he's not
talking about you know what, he's talking about writing high-performance
server programs: Not just wrong as in not perfect, but wrong as in wasting
half, or more, of your performance. What good is an O(log2(n)) algorithm if
those operations cause page faults and slow disk operations? For most relevant
datasets an O(n) or even an O(n^2) algorithm, which avoids page faults, will
run circles around it. A Microsoft Windows Azure primer: the basicsby Peter
Bright. Nice article explaining the basics of Azure and how it compares to
Google and Amazon.A call to change the name from NoSQL to Postmodern
Databases. Interesting idea, but the problem is the same one I have for
Postmodern Art, when is it? I always feel like I'm in the post-post modern
period, yet for art it's really in the early 1900s. Let's save future
developers from this existential time crisis.Constructions from Dots and
Linesby Marko A. Rodriguez, Peter N</p><p>4 0.81989634 <a title="901-lda-4" href="../high_scalability-2013/high_scalability-2013-08-07-RAFT_-_In_Search_of_an_Understandable_Consensus_Algorithm.html">1498 high scalability-2013-08-07-RAFT - In Search of an Understandable Consensus Algorithm</a></p>
<p>Introduction: If like many humans you've found evenPaxos Made Simplea bit difficult to
understand, you might enjoy RAFT as described inIn Search of an Understandable
Consensus Algorithm by Stanford'sDiego Ongaroand John Ousterhout. The video
presentation of the paper is given byJohn Ousterhout. Both the paper and the
video are delightfully accessible.mcherm has a good summary of the paper:A
consensus algorithm is: a cluster of servers should record a series of records
("log entries") in response to requests from clients of the cluster. (It may
also take action based on those entries.) It does so in a way that guarantees
that the responses seen by clients of the cluster will be consistent EVEN in
the face of servers crashing in unpredictable ways (but not loosing data that
was synched to disk), and networks introducing unpredictable delays or
communication blockages.Here's what Raft does. First, it elects a leader, then
the leader records the master version of the log, telling other cluster
servers w</p><p>5 0.81129885 <a title="901-lda-5" href="../high_scalability-2009/high_scalability-2009-07-08-Servers_Component_-_How_to_choice_and_build_perfect_server.html">653 high scalability-2009-07-08-Servers Component - How to choice and build perfect server</a></p>
<p>Introduction: There are a lot of questions about how the server components, and how to build
perfect server with consider the power consumption. TodayI will discussthe
Server components, and how we can choice better server components with
consider the power consumption, efficacy, performance, and price.Key
points:What kind of components the servers needs?The Green Computing and the
Servers componentsHow much power the server consumeChoice the right
components:ProcessorHard Disk DriveMemoryOperating systemBuild Server, or buy?</p><p>6 0.80616999 <a title="901-lda-6" href="../high_scalability-2011/high_scalability-2011-11-18-Stuff_The_Internet_Says_On_Scalability_For_November_18%2C_2011.html">1145 high scalability-2011-11-18-Stuff The Internet Says On Scalability For November 18, 2011</a></p>
<p>7 0.78190726 <a title="901-lda-7" href="../high_scalability-2009/high_scalability-2009-04-15-Using_HTTP_cache_headers_effectively.html">571 high scalability-2009-04-15-Using HTTP cache headers effectively</a></p>
<p>8 0.77891821 <a title="901-lda-8" href="../high_scalability-2011/high_scalability-2011-11-29-DataSift_Architecture%3A_Realtime_Datamining_at_120%2C000_Tweets_Per_Second.html">1148 high scalability-2011-11-29-DataSift Architecture: Realtime Datamining at 120,000 Tweets Per Second</a></p>
<p>9 0.76449829 <a title="901-lda-9" href="../high_scalability-2009/high_scalability-2009-08-31-Scaling_MySQL_on_Amazon_Web_Services.html">690 high scalability-2009-08-31-Scaling MySQL on Amazon Web Services</a></p>
<p>10 0.75995433 <a title="901-lda-10" href="../high_scalability-2012/high_scalability-2012-02-07-Hypertable_Routs_HBase_in_Performance_Test_--_HBase_Overwhelmed_by_Garbage_Collection.html">1189 high scalability-2012-02-07-Hypertable Routs HBase in Performance Test -- HBase Overwhelmed by Garbage Collection</a></p>
<p>11 0.75334114 <a title="901-lda-11" href="../high_scalability-2013/high_scalability-2013-12-11-Using_Node.js_PayPal_Doubles_RPS%2C_Lowers_Latency%2C_with_Fewer_Developers%2C_but_Where_Do_the_Improvements_Really_Come_From%3F.html">1563 high scalability-2013-12-11-Using Node.js PayPal Doubles RPS, Lowers Latency, with Fewer Developers, but Where Do the Improvements Really Come From?</a></p>
<p>12 0.74110502 <a title="901-lda-12" href="../high_scalability-2014/high_scalability-2014-04-28-How_Disqus_Went_Realtime_with_165K_Messages_Per_Second_and_Less_than_.2_Seconds_Latency.html">1638 high scalability-2014-04-28-How Disqus Went Realtime with 165K Messages Per Second and Less than .2 Seconds Latency</a></p>
<p>13 0.73997229 <a title="901-lda-13" href="../high_scalability-2014/high_scalability-2014-01-28-How_Next_Big_Sound_Tracks_Over_a_Trillion_Song_Plays%2C_Likes%2C_and_More_Using_a_Version_Control_System_for_Hadoop_Data.html">1586 high scalability-2014-01-28-How Next Big Sound Tracks Over a Trillion Song Plays, Likes, and More Using a Version Control System for Hadoop Data</a></p>
<p>14 0.73938972 <a title="901-lda-14" href="../high_scalability-2009/high_scalability-2009-08-11-13_Scalability_Best_Practices.html">679 high scalability-2009-08-11-13 Scalability Best Practices</a></p>
<p>15 0.73896819 <a title="901-lda-15" href="../high_scalability-2011/high_scalability-2011-08-22-Strategy%3A_Run_a_Scalable%2C_Available%2C_and_Cheap_Static_Site_on_S3_or_GitHub.html">1102 high scalability-2011-08-22-Strategy: Run a Scalable, Available, and Cheap Static Site on S3 or GitHub</a></p>
<p>16 0.73586774 <a title="901-lda-16" href="../high_scalability-2008/high_scalability-2008-08-18-Code_deployment_tools.html">369 high scalability-2008-08-18-Code deployment tools</a></p>
<p>17 0.72625929 <a title="901-lda-17" href="../high_scalability-2010/high_scalability-2010-06-03-Hot_Scalability_Links_for_June_3%2C_2010.html">835 high scalability-2010-06-03-Hot Scalability Links for June 3, 2010</a></p>
<p>18 0.71846098 <a title="901-lda-18" href="../high_scalability-2010/high_scalability-2010-07-13-DbShards_Part_Deux_-_The_Internals.html">857 high scalability-2010-07-13-DbShards Part Deux - The Internals</a></p>
<p>19 0.71713585 <a title="901-lda-19" href="../high_scalability-2011/high_scalability-2011-09-02-Stuff_The_Internet_Says_On_Scalability_For_September_2%2C_2011.html">1109 high scalability-2011-09-02-Stuff The Internet Says On Scalability For September 2, 2011</a></p>
<p>20 0.71653914 <a title="901-lda-20" href="../high_scalability-2009/high_scalability-2009-10-06-Building_a_Unique_Data_Warehouse.html">716 high scalability-2009-10-06-Building a Unique Data Warehouse</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
