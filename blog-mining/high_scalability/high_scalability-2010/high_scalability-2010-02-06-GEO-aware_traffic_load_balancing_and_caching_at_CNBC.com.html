<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>773 high scalability-2010-02-06-GEO-aware traffic load balancing and caching at CNBC.com</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2010" href="../home/high_scalability-2010_home.html">high_scalability-2010</a> <a title="high_scalability-2010-773" href="#">high_scalability-2010-773</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>773 high scalability-2010-02-06-GEO-aware traffic load balancing and caching at CNBC.com</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2010-773-html" href="http://highscalability.com//blog/2010/2/6/geo-aware-traffic-load-balancing-and-caching-at-cnbccom.html">html</a></p><p>Introduction: CNBC, like many large web sites, relied  on a CDN for content delivery.
Recently, we started looking  to see if we could improve this model.  Our
criteria was:- improve response time- have better control over traffic (real
time reporting, change management and alerting)- better utilize internal
datacenters and their infrastructure- shield users from any troubles at the
origin infrastructure- cost outAfter researching the market, we turned to two
vendors: Dyn (Dynamic Network Services) andaiScaler. We' have had   about a
year worth of experience withaiScaler (search for "CNBC" to see my previous
post ), but Dyn was a new vendor for us.  We started building our relationship
at Velocity conference in the summer of 2009.Dyn has recently started offering
a geo-aware DNS load balancing solution, using Anycast and the distributed
nature of their DNS presence to enable a key component of what we were trying
to achieve: steer users to geographically closest origin point. The traffic
balancing r</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Recently, we started looking  to see if we could improve this model. [sent-2, score-0.062]
</p><p>2 We started building our relationship at Velocity conference in the summer of 2009. [sent-5, score-0.062]
</p><p>3 Dyn has recently started offering a geo-aware DNS load balancing solution, using Anycast and the distributed nature of their DNS presence to enable a key component of what we were trying to achieve: steer users to geographically closest origin point. [sent-6, score-0.704]
</p><p>4 In principle, to direct  a user to geographically closest origin point, one has to have an idea as to the user's location. [sent-10, score-0.636]
</p><p>5 A very traditional way of doing that required some form of a DB, mapping IP addresses to locations. [sent-11, score-0.062]
</p><p>6 A very different, albeit less granular, way to accomplish the same is to use Internet routing (BGP protocol) to advertise routes to the same IP addresses from multiple points of presence. [sent-13, score-0.126]
</p><p>7 Each cluster is positioned at a major peering point : US East Coast,  West Coast, one in EU and one in Asia. [sent-21, score-0.078]
</p><p>8 Through magic of routing, users in Asia will have their DNS requests come to one's DNS servers in Asia, EU to EU and so on. [sent-24, score-0.065]
</p><p>9 It is easy to see how this implied knowledge of requestor's geo location can now be used to direct their traffic in a certain, location-specific way. [sent-25, score-0.533]
</p><p>10 The DNS request will naturally flow to the closest Dyn DNS cluster. [sent-32, score-0.125]
</p><p>11 The DNS servers at the said cluster have implied awareness of their location. [sent-33, score-0.097]
</p><p>12 Based on that, DNS server infers that the requests are also coming from users in the same geo area and based on that and set of rules we configure, it directs requesting user to proper origin point forwww. [sent-34, score-0.719]
</p><p>13 For origin points, we've chosen our own datacenters,  each with multiple gigabits of egress capacity, at East and West coasts of US. [sent-37, score-0.444]
</p><p>14 Just 4 common 1RU blade servers, 2 at each location, are all we needed to deliver all of the traffic to our US user base. [sent-39, score-0.247]
</p><p>15 The latest iteration ofaiScaler product, v6, has been tested to in excess of 250,000 RPS  per common HP DL360 server. [sent-40, score-0.092]
</p><p>16 com peak at over 3000 RPS, so we have a lot of excess capacity for any possible traffic spikes. [sent-45, score-0.28]
</p><p>17 Here're our results so far:- we were able to shave about 1 sec (about 30%! [sent-46, score-0.115]
</p><p>18 - our CDN traffic has seen about 80% reduction as well - complete with 80%  reduction in CDN fees- we're now better utilizing our own datacenters capacity-  we now have ability to instantaneously affect  our caching rules or load distribution. [sent-49, score-0.499]
</p><p>19 Summary:  Dyn's Dynamic and Geo-aware DNS load balancing solution andaiScaler's proven caching software have enabled a top-tier financial news website to shave 30% off response time, save money, have better, real-time monitoring, reporting and alerting setup. [sent-51, score-0.412]
</p><p>20 And lastly: the above doesn't constitute, in any way, shape or form, an endorsement of the mentioned products, vendors and/or solutions, by CNBC, NBC, GE or any of its subsidiaries. [sent-53, score-0.124]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('dns', 0.324), ('origin', 0.323), ('eu', 0.23), ('dyn', 0.199), ('coast', 0.195), ('cnbc', 0.192), ('traffic', 0.188), ('rps', 0.156), ('asia', 0.152), ('withaiscaler', 0.141), ('cms', 0.136), ('alerting', 0.131), ('closest', 0.125), ('west', 0.117), ('shave', 0.115), ('east', 0.115), ('anycast', 0.111), ('geo', 0.107), ('reporting', 0.102), ('bgp', 0.102), ('cdn', 0.1), ('implied', 0.097), ('excess', 0.092), ('send', 0.087), ('rules', 0.087), ('location', 0.082), ('datacenters', 0.082), ('point', 0.078), ('reduction', 0.071), ('geographically', 0.07), ('requests', 0.065), ('vendors', 0.064), ('coasts', 0.064), ('requestor', 0.064), ('albeit', 0.064), ('ge', 0.064), ('balancing', 0.064), ('started', 0.062), ('addresses', 0.062), ('nbc', 0.06), ('uucp', 0.06), ('steer', 0.06), ('endorsement', 0.06), ('direct', 0.059), ('user', 0.059), ('russia', 0.057), ('busier', 0.057), ('egress', 0.057), ('valve', 0.057), ('distro', 0.055)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="773-tfidf-1" href="../high_scalability-2010/high_scalability-2010-02-06-GEO-aware_traffic_load_balancing_and_caching_at_CNBC.com.html">773 high scalability-2010-02-06-GEO-aware traffic load balancing and caching at CNBC.com</a></p>
<p>Introduction: CNBC, like many large web sites, relied  on a CDN for content delivery.
Recently, we started looking  to see if we could improve this model.  Our
criteria was:- improve response time- have better control over traffic (real
time reporting, change management and alerting)- better utilize internal
datacenters and their infrastructure- shield users from any troubles at the
origin infrastructure- cost outAfter researching the market, we turned to two
vendors: Dyn (Dynamic Network Services) andaiScaler. We' have had   about a
year worth of experience withaiScaler (search for "CNBC" to see my previous
post ), but Dyn was a new vendor for us.  We started building our relationship
at Velocity conference in the summer of 2009.Dyn has recently started offering
a geo-aware DNS load balancing solution, using Anycast and the distributed
nature of their DNS presence to enable a key component of what we were trying
to achieve: steer users to geographically closest origin point. The traffic
balancing r</p><p>2 0.27482149 <a title="773-tfidf-2" href="../high_scalability-2009/high_scalability-2009-04-16-Serving_250M_quotes-day_at_CNBC.com_with_aiCache.html">573 high scalability-2009-04-16-Serving 250M quotes-day at CNBC.com with aiCache</a></p>
<p>Introduction: As traffic to cnbc.com continued to grow, we found ourselves in an all-too-
familiar situation where one feels that a BIG change in how things are done
was in order, the status-quo was a road to nowhere. The spending on HW, amount
of space and power required to host additional servers, less-than-stellar
response times, having to resort to frequent "micro"-caching and similar
tricks to try to improve code performance - all of these were surfacing in
plain sight, hard to ignore.While code base could clearly be improved, the
limited Dev resources and having to innovate to stay competitive always limits
ability to go about refactoring. So how can one go about addressing
performance and other needs without a full blown effort across the entire team
? For us, the answer was aiCache - a Web caching and application acceleration
product (aicache.com).breakThe idea behind caching is simple - handle the
requests before they ever hit your regular Apache<->JK<->Java<->Database
response generation t</p><p>3 0.25104761 <a title="773-tfidf-3" href="../high_scalability-2013/high_scalability-2013-09-16-The_Hidden_DNS_Tax_-_Cascading_Timeouts_and_Errors.html">1517 high scalability-2013-09-16-The Hidden DNS Tax - Cascading Timeouts and Errors</a></p>
<p>Introduction: This is a guest post byNick Burling, VP of Product Management
ofBluestripe.Readers of High Scalability know are well versed in performance
optimization techniques. Reverse proxies, Varnish, Redis -- you hear about
them daily. But what you may not realize is that one of the oldest
technologies in your stack can be one of your biggest bottlenecks: DNS.People
don't spend a lot of time thinking about DNS. It's not sexy. It's an
infrastructure service, and it's just supposed to work.At BlueStripe, we work
with many teams running applications that support millions of web requests a
day. We keep seeing DNS delays and errors that the platform operations team
never knows about. It's so common we've start calling it theHidden DNS
Tax.What is the Hidden DNS Tax?The Hidden DNS Tax is a hard-to-see performance
hit your users take from DNS timeouts and errors in your back-end
architecture. We've seen it bring down the main web application for a Fortune
10 company.A DNS lookup can fail, the user gets</p><p>4 0.16163214 <a title="773-tfidf-4" href="../high_scalability-2014/high_scalability-2014-02-17-How_the_AOL.com_Architecture_Evolved_to_99.999%25_Availability%2C_8_Million_Visitors_Per_Day%2C_and_200%2C000_Requests_Per_Second.html">1597 high scalability-2014-02-17-How the AOL.com Architecture Evolved to 99.999% Availability, 8 Million Visitors Per Day, and 200,000 Requests Per Second</a></p>
<p>Introduction: This is a guest post byDave HaglerSystems Architect at AOL.The AOL homepages
receive more than8 million visitors per day.  That's more daily viewers than
Good Morning America or the Today Show on television.  Over a billion page
views are served each month.  AOL.com has been a major internet destination
since 1996, and still has a strong following of loyal users.The architecture
for AOL.com is in it's 5th generation.  It has essentially been rebuilt from
scratch 5 times over two decades.  The current architecture was designed 6
years ago.  Pieces have been upgraded and new components have been added along
the way, but the overall design remains largely intact.  The code, tools,
development and deployment processes are highly tuned over 6 years of
continual improvement, making the AOL.com architecture battle tested and very
stable.The engineering team is made up of developers, testers, and operations
andtotals around 25 people.  The majority are in Dulles, Virginia with a
smaller team i</p><p>5 0.14634924 <a title="773-tfidf-5" href="../high_scalability-2008/high_scalability-2008-03-27-Amazon_Announces_Static_IP_Addresses_and_Multiple_Datacenter_Operation.html">289 high scalability-2008-03-27-Amazon Announces Static IP Addresses and Multiple Datacenter Operation</a></p>
<p>Introduction: Amazon is fixing two of their major problems: no static IP addresses and
single datacenter operation. By adding these two new features developers can
finally build a no apology system on Amazon. Before you always had to throw in
an apology or two. No, we don't have low failover times because of the silly
DNS games and unexceptionable DNS update and propagation times and no, we
don't operate in more than one datacenter. No more. Now Amazon is
addingElastic IP AddressesandAvailability Zones.Elastic IP addresses are far
better than normal IP addresses because they are both in tight withJessica
Albaand they are:breakStatic IP addresses designed for dynamic cloud
computing. An Elastic IP address is associated with your account, not a
particular instance, and you control that address until you choose to
explicitly release it. Unlike traditional static IP addresses, however,
Elastic IP addresses allow you to mask instance or availability zone failures
by programmatically remapping your public</p><p>6 0.1436508 <a title="773-tfidf-6" href="../high_scalability-2008/high_scalability-2008-03-28-How_to_Get_DNS_Names_of_a_Web_Server.html">290 high scalability-2008-03-28-How to Get DNS Names of a Web Server</a></p>
<p>7 0.12481849 <a title="773-tfidf-7" href="../high_scalability-2009/high_scalability-2009-04-21-What_CDN_would_you_recommend%3F.html">576 high scalability-2009-04-21-What CDN would you recommend?</a></p>
<p>8 0.12039539 <a title="773-tfidf-8" href="../high_scalability-2010/high_scalability-2010-03-16-Justin.tv%27s_Live_Video_Broadcasting_Architecture.html">796 high scalability-2010-03-16-Justin.tv's Live Video Broadcasting Architecture</a></p>
<p>9 0.1203413 <a title="773-tfidf-9" href="../high_scalability-2012/high_scalability-2012-11-15-Gone_Fishin%27%3A_Justin.Tv%27s_Live_Video_Broadcasting_Architecture.html">1359 high scalability-2012-11-15-Gone Fishin': Justin.Tv's Live Video Broadcasting Architecture</a></p>
<p>10 0.11591496 <a title="773-tfidf-10" href="../high_scalability-2013/high_scalability-2013-12-02-Evolution_of_Bazaarvoice%E2%80%99s_Architecture_to_500M_Unique_Users_Per_Month.html">1557 high scalability-2013-12-02-Evolution of Bazaarvoice’s Architecture to 500M Unique Users Per Month</a></p>
<p>11 0.1118378 <a title="773-tfidf-11" href="../high_scalability-2012/high_scalability-2012-07-23-State_of_the_CDN%3A_More_Traffic%2C_Stable_Prices%2C_More_Products%2C_Profits_-_Not_So_Much.html">1289 high scalability-2012-07-23-State of the CDN: More Traffic, Stable Prices, More Products, Profits - Not So Much</a></p>
<p>12 0.10836665 <a title="773-tfidf-12" href="../high_scalability-2007/high_scalability-2007-10-30-Feedblendr_Architecture_-_Using_EC2_to_Scale.html">138 high scalability-2007-10-30-Feedblendr Architecture - Using EC2 to Scale</a></p>
<p>13 0.10335331 <a title="773-tfidf-13" href="../high_scalability-2008/high_scalability-2008-09-09-Content_Delivery_Networks_%28CDN%29_%E2%80%93_a_comprehensive_list_of_providers.html">382 high scalability-2008-09-09-Content Delivery Networks (CDN) – a comprehensive list of providers</a></p>
<p>14 0.10329983 <a title="773-tfidf-14" href="../high_scalability-2010/high_scalability-2010-03-04-How_MySpace_Tested_Their_Live_Site_with_1_Million_Concurrent_Users.html">788 high scalability-2010-03-04-How MySpace Tested Their Live Site with 1 Million Concurrent Users</a></p>
<p>15 0.10319521 <a title="773-tfidf-15" href="../high_scalability-2012/high_scalability-2012-10-08-How_UltraDNS_Handles_Hundreds_of_Thousands_of_Zones_and_Tens_of_Millions_of_Records.html">1335 high scalability-2012-10-08-How UltraDNS Handles Hundreds of Thousands of Zones and Tens of Millions of Records</a></p>
<p>16 0.09878131 <a title="773-tfidf-16" href="../high_scalability-2012/high_scalability-2012-10-02-An_Epic_TripAdvisor_Update%3A_Why_Not_Run_on_the_Cloud%3F_The_Grand_Experiment..html">1331 high scalability-2012-10-02-An Epic TripAdvisor Update: Why Not Run on the Cloud? The Grand Experiment.</a></p>
<p>17 0.098483346 <a title="773-tfidf-17" href="../high_scalability-2011/high_scalability-2011-09-23-The_Real_News_is_Not_that_Facebook_Serves_Up_1_Trillion_Pages_a_Month%E2%80%A6.html">1123 high scalability-2011-09-23-The Real News is Not that Facebook Serves Up 1 Trillion Pages a Month…</a></p>
<p>18 0.098190509 <a title="773-tfidf-18" href="../high_scalability-2012/high_scalability-2012-06-05-Sponsored_Post%3A_Digital_Ocean%2C_NetDNA%2C_Torbit%2C_Velocity%2C_Reality_Check_Network%2C_Gigaspaces%2C_AiCache%2C_Logic_Monitor%2C_Attribution_Modeling%2C_AppDynamics%2C_CloudSigma%2C_ManageEnine%2C_Site24x7.html">1257 high scalability-2012-06-05-Sponsored Post: Digital Ocean, NetDNA, Torbit, Velocity, Reality Check Network, Gigaspaces, AiCache, Logic Monitor, Attribution Modeling, AppDynamics, CloudSigma, ManageEnine, Site24x7</a></p>
<p>19 0.097510666 <a title="773-tfidf-19" href="../high_scalability-2012/high_scalability-2012-06-26-Sponsored_Post%3A_New_Relic%2C_Digital_Ocean%2C_NetDNA%2C_Torbit%2C_Reality_Check_Network%2C_Gigaspaces%2C_AiCache%2C_Logic_Monitor%2C_AppDynamics%2C_CloudSigma%2C_ManageEnine%2C_Site24x7.html">1272 high scalability-2012-06-26-Sponsored Post: New Relic, Digital Ocean, NetDNA, Torbit, Reality Check Network, Gigaspaces, AiCache, Logic Monitor, AppDynamics, CloudSigma, ManageEnine, Site24x7</a></p>
<p>20 0.096145809 <a title="773-tfidf-20" href="../high_scalability-2008/high_scalability-2008-01-22-The_high_scalability_community.html">220 high scalability-2008-01-22-The high scalability community</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.17), (1, 0.044), (2, -0.036), (3, -0.088), (4, -0.048), (5, -0.091), (6, 0.012), (7, -0.025), (8, -0.02), (9, 0.008), (10, -0.019), (11, 0.0), (12, -0.028), (13, -0.054), (14, 0.019), (15, 0.042), (16, 0.074), (17, 0.033), (18, -0.018), (19, -0.069), (20, 0.003), (21, 0.051), (22, 0.019), (23, -0.026), (24, 0.012), (25, 0.029), (26, -0.062), (27, 0.02), (28, -0.029), (29, -0.032), (30, -0.001), (31, 0.041), (32, -0.01), (33, 0.024), (34, 0.042), (35, 0.017), (36, -0.002), (37, -0.029), (38, -0.031), (39, 0.002), (40, 0.025), (41, 0.03), (42, 0.021), (43, -0.001), (44, 0.006), (45, 0.074), (46, -0.015), (47, 0.035), (48, 0.003), (49, 0.029)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97074753 <a title="773-lsi-1" href="../high_scalability-2010/high_scalability-2010-02-06-GEO-aware_traffic_load_balancing_and_caching_at_CNBC.com.html">773 high scalability-2010-02-06-GEO-aware traffic load balancing and caching at CNBC.com</a></p>
<p>Introduction: CNBC, like many large web sites, relied  on a CDN for content delivery.
Recently, we started looking  to see if we could improve this model.  Our
criteria was:- improve response time- have better control over traffic (real
time reporting, change management and alerting)- better utilize internal
datacenters and their infrastructure- shield users from any troubles at the
origin infrastructure- cost outAfter researching the market, we turned to two
vendors: Dyn (Dynamic Network Services) andaiScaler. We' have had   about a
year worth of experience withaiScaler (search for "CNBC" to see my previous
post ), but Dyn was a new vendor for us.  We started building our relationship
at Velocity conference in the summer of 2009.Dyn has recently started offering
a geo-aware DNS load balancing solution, using Anycast and the distributed
nature of their DNS presence to enable a key component of what we were trying
to achieve: steer users to geographically closest origin point. The traffic
balancing r</p><p>2 0.77709275 <a title="773-lsi-2" href="../high_scalability-2008/high_scalability-2008-03-08-DNS-Record_TTL_on_worst_case_scenarios.html">270 high scalability-2008-03-08-DNS-Record TTL on worst case scenarios</a></p>
<p>Introduction: i didnt find a nearly good solution for this problem yet:imagine, you're
responsible for a small CDN network (static images), with two different
datacenter. the balancing for the two DC is done with a anycast nameservice (a
nameserver in every DC, user gets on nearest location). so, one of the
scenario is that one of the datacenters goes down completly. you can do a
monitoring on the nameserver and only route to the dc which is still alive, no
problem. But what about the TTL from the DNS-Records? Tiny TTLs like 2 min.
are often ignored by several ISP (e.g. AOL). so, the client doesn't get the IP
from the other Datacenter. what could be a solution in this scenario?</p><p>3 0.77569306 <a title="773-lsi-3" href="../high_scalability-2013/high_scalability-2013-09-16-The_Hidden_DNS_Tax_-_Cascading_Timeouts_and_Errors.html">1517 high scalability-2013-09-16-The Hidden DNS Tax - Cascading Timeouts and Errors</a></p>
<p>Introduction: This is a guest post byNick Burling, VP of Product Management
ofBluestripe.Readers of High Scalability know are well versed in performance
optimization techniques. Reverse proxies, Varnish, Redis -- you hear about
them daily. But what you may not realize is that one of the oldest
technologies in your stack can be one of your biggest bottlenecks: DNS.People
don't spend a lot of time thinking about DNS. It's not sexy. It's an
infrastructure service, and it's just supposed to work.At BlueStripe, we work
with many teams running applications that support millions of web requests a
day. We keep seeing DNS delays and errors that the platform operations team
never knows about. It's so common we've start calling it theHidden DNS
Tax.What is the Hidden DNS Tax?The Hidden DNS Tax is a hard-to-see performance
hit your users take from DNS timeouts and errors in your back-end
architecture. We've seen it bring down the main web application for a Fortune
10 company.A DNS lookup can fail, the user gets</p><p>4 0.76012099 <a title="773-lsi-4" href="../high_scalability-2009/high_scalability-2009-04-16-Serving_250M_quotes-day_at_CNBC.com_with_aiCache.html">573 high scalability-2009-04-16-Serving 250M quotes-day at CNBC.com with aiCache</a></p>
<p>Introduction: As traffic to cnbc.com continued to grow, we found ourselves in an all-too-
familiar situation where one feels that a BIG change in how things are done
was in order, the status-quo was a road to nowhere. The spending on HW, amount
of space and power required to host additional servers, less-than-stellar
response times, having to resort to frequent "micro"-caching and similar
tricks to try to improve code performance - all of these were surfacing in
plain sight, hard to ignore.While code base could clearly be improved, the
limited Dev resources and having to innovate to stay competitive always limits
ability to go about refactoring. So how can one go about addressing
performance and other needs without a full blown effort across the entire team
? For us, the answer was aiCache - a Web caching and application acceleration
product (aicache.com).breakThe idea behind caching is simple - handle the
requests before they ever hit your regular Apache<->JK<->Java<->Database
response generation t</p><p>5 0.7310583 <a title="773-lsi-5" href="../high_scalability-2012/high_scalability-2012-09-26-WordPress.com_Serves_70%2C000_req-sec_and_over_15_Gbit-sec_of_Traffic_using_NGINX.html">1329 high scalability-2012-09-26-WordPress.com Serves 70,000 req-sec and over 15 Gbit-sec of Traffic using NGINX</a></p>
<p>Introduction: This is a guest post by Barry Abrahamson, Chief Systems Wrangler at
Automattic, andNginx's Coufounder Andrew Alexeev.WordPress.com serves more
than 33 million sites attracting over 339 million people and 3.4 billion pages
each month. Since April 2008, WordPress.com has experienced about 4.4 times
growth in page views. WordPress.com VIP hosts many popular sites including
CNN's Political Ticker, NFL, Time Inc's The Page, People Magazine's Style
Watch, corporate blogs for Flickr and KROQ, and many more. Automattic operates
two thousand servers in twelve, globally distributed, data centers.
WordPress.com customer data is instantly replicated between different
locations to provide an extremely reliable and fast web experience for
hundreds of millions of visitors.ProblemWordPress.com, which began in 2005,
started on shared hosting, much like all of the WordPress.org sites. It was
soon moved to a single dedicated server and then to two servers. In late 2005,
WordPress.com opened to the public</p><p>6 0.72376639 <a title="773-lsi-6" href="../high_scalability-2013/high_scalability-2013-02-06-Super_Bowl_Advertisers_Ready_for_the_Traffic%3F_Nope..It%27s_Lights_Out..html">1401 high scalability-2013-02-06-Super Bowl Advertisers Ready for the Traffic? Nope..It's Lights Out.</a></p>
<p>7 0.69951355 <a title="773-lsi-7" href="../high_scalability-2014/high_scalability-2014-02-17-How_the_AOL.com_Architecture_Evolved_to_99.999%25_Availability%2C_8_Million_Visitors_Per_Day%2C_and_200%2C000_Requests_Per_Second.html">1597 high scalability-2014-02-17-How the AOL.com Architecture Evolved to 99.999% Availability, 8 Million Visitors Per Day, and 200,000 Requests Per Second</a></p>
<p>8 0.69875675 <a title="773-lsi-8" href="../high_scalability-2012/high_scalability-2012-10-08-How_UltraDNS_Handles_Hundreds_of_Thousands_of_Zones_and_Tens_of_Millions_of_Records.html">1335 high scalability-2012-10-08-How UltraDNS Handles Hundreds of Thousands of Zones and Tens of Millions of Records</a></p>
<p>9 0.68583131 <a title="773-lsi-9" href="../high_scalability-2010/high_scalability-2010-03-26-Strategy%3A_Caching_404s_Saved_the_Onion_66%25_on_Server_Time.html">800 high scalability-2010-03-26-Strategy: Caching 404s Saved the Onion 66% on Server Time</a></p>
<p>10 0.68441635 <a title="773-lsi-10" href="../high_scalability-2012/high_scalability-2012-06-18-The_Clever_Ways_Chrome_Hides_Latency_by_Anticipating_Your_Every_Need.html">1267 high scalability-2012-06-18-The Clever Ways Chrome Hides Latency by Anticipating Your Every Need</a></p>
<p>11 0.68410009 <a title="773-lsi-11" href="../high_scalability-2008/high_scalability-2008-01-28-Product%3A_ISPMan_Centralized_ISP_Management_System_.html">228 high scalability-2008-01-28-Product: ISPMan Centralized ISP Management System </a></p>
<p>12 0.68360299 <a title="773-lsi-12" href="../high_scalability-2012/high_scalability-2012-11-15-Gone_Fishin%27%3A_Justin.Tv%27s_Live_Video_Broadcasting_Architecture.html">1359 high scalability-2012-11-15-Gone Fishin': Justin.Tv's Live Video Broadcasting Architecture</a></p>
<p>13 0.68335193 <a title="773-lsi-13" href="../high_scalability-2010/high_scalability-2010-03-16-Justin.tv%27s_Live_Video_Broadcasting_Architecture.html">796 high scalability-2010-03-16-Justin.tv's Live Video Broadcasting Architecture</a></p>
<p>14 0.68290591 <a title="773-lsi-14" href="../high_scalability-2014/high_scalability-2014-01-29-10_Things_Bitly_Should_Have_Monitored.html">1587 high scalability-2014-01-29-10 Things Bitly Should Have Monitored</a></p>
<p>15 0.67999727 <a title="773-lsi-15" href="../high_scalability-2007/high_scalability-2007-10-30-Feedblendr_Architecture_-_Using_EC2_to_Scale.html">138 high scalability-2007-10-30-Feedblendr Architecture - Using EC2 to Scale</a></p>
<p>16 0.67875159 <a title="773-lsi-16" href="../high_scalability-2013/high_scalability-2013-01-23-Building_Redundant_Datacenter_Networks_is_Not_For_Sissies_-_Use_an_Outside_WAN_Backbone.html">1392 high scalability-2013-01-23-Building Redundant Datacenter Networks is Not For Sissies - Use an Outside WAN Backbone</a></p>
<p>17 0.67259169 <a title="773-lsi-17" href="../high_scalability-2010/high_scalability-2010-03-04-How_MySpace_Tested_Their_Live_Site_with_1_Million_Concurrent_Users.html">788 high scalability-2010-03-04-How MySpace Tested Their Live Site with 1 Million Concurrent Users</a></p>
<p>18 0.67181969 <a title="773-lsi-18" href="../high_scalability-2008/high_scalability-2008-03-28-How_to_Get_DNS_Names_of_a_Web_Server.html">290 high scalability-2008-03-28-How to Get DNS Names of a Web Server</a></p>
<p>19 0.66850978 <a title="773-lsi-19" href="../high_scalability-2011/high_scalability-2011-02-10-Dispelling_the_New_SSL_Myth.html">987 high scalability-2011-02-10-Dispelling the New SSL Myth</a></p>
<p>20 0.66536736 <a title="773-lsi-20" href="../high_scalability-2009/high_scalability-2009-03-11-The_Implications_of_Punctuated_Scalabilium_for_Website_Architecture.html">533 high scalability-2009-03-11-The Implications of Punctuated Scalabilium for Website Architecture</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.127), (2, 0.153), (10, 0.041), (30, 0.077), (32, 0.012), (47, 0.029), (56, 0.025), (61, 0.065), (77, 0.02), (79, 0.089), (81, 0.191), (85, 0.016), (93, 0.032), (94, 0.041)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.88973051 <a title="773-lda-1" href="../high_scalability-2009/high_scalability-2009-03-16-Cisco_and_Sun_to_Compete_for_Unified_Computing%3F.html">540 high scalability-2009-03-16-Cisco and Sun to Compete for Unified Computing?</a></p>
<p>Introduction: A recentInfoWorld articleclaims that "With Cisco expected to enter the blade
market and Sun expected to offer networking equipment, things could get
interesting awfully fast." How does this effect your infrastructure strategy
and decisions?Would you consider to build scalable web applications on the
Cisco Unified Computing System? Or would you consider to build a router out of
a server with the use of OpenSolaris and Project Crossbow as the article
suggests? Will any of these initiatives change the way we build scalable web
infrastructure or are these just attempts to sale these systems?What do you
think?</p><p>same-blog 2 0.8702246 <a title="773-lda-2" href="../high_scalability-2010/high_scalability-2010-02-06-GEO-aware_traffic_load_balancing_and_caching_at_CNBC.com.html">773 high scalability-2010-02-06-GEO-aware traffic load balancing and caching at CNBC.com</a></p>
<p>Introduction: CNBC, like many large web sites, relied  on a CDN for content delivery.
Recently, we started looking  to see if we could improve this model.  Our
criteria was:- improve response time- have better control over traffic (real
time reporting, change management and alerting)- better utilize internal
datacenters and their infrastructure- shield users from any troubles at the
origin infrastructure- cost outAfter researching the market, we turned to two
vendors: Dyn (Dynamic Network Services) andaiScaler. We' have had   about a
year worth of experience withaiScaler (search for "CNBC" to see my previous
post ), but Dyn was a new vendor for us.  We started building our relationship
at Velocity conference in the summer of 2009.Dyn has recently started offering
a geo-aware DNS load balancing solution, using Anycast and the distributed
nature of their DNS presence to enable a key component of what we were trying
to achieve: steer users to geographically closest origin point. The traffic
balancing r</p><p>3 0.831056 <a title="773-lda-3" href="../high_scalability-2007/high_scalability-2007-07-16-Paper%3A_Replication_Under_Scalable_Hashing.html">19 high scalability-2007-07-16-Paper: Replication Under Scalable Hashing</a></p>
<p>Introduction: Replication Under Scalable Hashing: A Family of Algorithms for
ScalableDecentralized Data DistributionFrom the abstract:Typical algorithms
for decentralized data distributionwork best in a system that is fully built
before it first used;adding or removing components results in either
extensivereorganization of data or load imbalance in the system.We have
developed a family of decentralized algorithms,RUSH (Replication Under
Scalable Hashing), thatmaps replicated objects to a scalable collection of
storageservers or disks. RUSH algorithms distribute objects toservers
according to user-specified server weighting. Whileall RUSH variants support
addition of servers to the system,different variants have different
characteristics withrespect to lookup time in petabyte-scale systems,
performancewith mirroring (as opposed to redundancy codes),and storage server
removal. All RUSH variants redistributeas few objects as possible when new
servers areadded or existing servers are removed, and all v</p><p>4 0.80212069 <a title="773-lda-4" href="../high_scalability-2007/high_scalability-2007-09-26-Use_a_CDN_to_Instantly_Improve_Your_Website%27s_Performance_by_20%25_or_More_.html">100 high scalability-2007-09-26-Use a CDN to Instantly Improve Your Website's Performance by 20% or More </a></p>
<p>Introduction: If you have a lot of static content to store and you aren't looking forward to
setting up and maintaining your own giganto SAN, maybe you can push off a lot
of the hard lifting to a CDN?Jesse Robbins at O'Reilly Radar posts that you
have a lot more options now because the number ofContent Distribution Networks
have doubled since last year. In fact, Dan Rayburn says there are now28
CDNproviders in the market. Hopefully you can find reasonable pricing at one
of them.Other than easing your burden, why might a CDN work for you? Because
it makes your site faster and customers like that. How can a CDN so
dramatically improve your site's performance?breakSteve Saunders, author
ofHigh Performance Web Sites: Essential Knowledge for Front-End Engineers, has
using a CDN has one of his "Thirteen Simple Rules for Speeding Up Your Web
Site."About CDNs Steve says:Remember that 80-90% of the end-user response time
is spent downloading all the components in the page: images, stylesheets,
scripts, Flash</p><p>5 0.79734021 <a title="773-lda-5" href="../high_scalability-2007/high_scalability-2007-10-26-How_Gravatar_scales_on_WordPress.com_hardware.html">133 high scalability-2007-10-26-How Gravatar scales on WordPress.com hardware</a></p>
<p>Introduction: Automattic recently purchase Gravatar and have switched the server onto their
hosting platform. WordPress.com host over 1.7 million blogs with well over
60'000 new posts submitted each day generating 10 - 12 million page views per
day.Barry on WordPress.com has agreat post on the changes they've introduced
to help Gravatar scale.</p><p>6 0.79163712 <a title="773-lda-6" href="../high_scalability-2012/high_scalability-2012-12-21-Stuff_The_Internet_Says_On_Scalability_For_December_21%2C_2012.html">1375 high scalability-2012-12-21-Stuff The Internet Says On Scalability For December 21, 2012</a></p>
<p>7 0.77821553 <a title="773-lda-7" href="../high_scalability-2011/high_scalability-2011-02-08-Mollom_Architecture_-_Killing_Over_373_Million_Spams_at_100_Requests_Per_Second.html">985 high scalability-2011-02-08-Mollom Architecture - Killing Over 373 Million Spams at 100 Requests Per Second</a></p>
<p>8 0.77577257 <a title="773-lda-8" href="../high_scalability-2014/high_scalability-2014-01-08-Under_Snowden%27s_Light_Software_Architecture_Choices_Become_Murky.html">1575 high scalability-2014-01-08-Under Snowden's Light Software Architecture Choices Become Murky</a></p>
<p>9 0.76772398 <a title="773-lda-9" href="../high_scalability-2014/high_scalability-2014-03-24-Big%2C_Small%2C_Hot_or_Cold_-_Examples_of_Robust_Data_Pipelines_from_Stripe%2C_Tapad%2C_Etsy_and_Square.html">1618 high scalability-2014-03-24-Big, Small, Hot or Cold - Examples of Robust Data Pipelines from Stripe, Tapad, Etsy and Square</a></p>
<p>10 0.76715851 <a title="773-lda-10" href="../high_scalability-2007/high_scalability-2007-12-28-Amazon%27s_EC2%3A_Pay_as_You_Grow_Could_Cut_Your_Costs_in_Half.html">195 high scalability-2007-12-28-Amazon's EC2: Pay as You Grow Could Cut Your Costs in Half</a></p>
<p>11 0.75872606 <a title="773-lda-11" href="../high_scalability-2012/high_scalability-2012-09-28-Stuff_The_Internet_Says_On_Scalability_For_September_28%2C_2012.html">1330 high scalability-2012-09-28-Stuff The Internet Says On Scalability For September 28, 2012</a></p>
<p>12 0.75772101 <a title="773-lda-12" href="../high_scalability-2013/high_scalability-2013-05-01-Myth%3A_Eric_Brewer_on_Why_Banks_are_BASE_Not_ACID_-_Availability_Is_Revenue_.html">1450 high scalability-2013-05-01-Myth: Eric Brewer on Why Banks are BASE Not ACID - Availability Is Revenue </a></p>
<p>13 0.75678152 <a title="773-lda-13" href="../high_scalability-2011/high_scalability-2011-09-02-Stuff_The_Internet_Says_On_Scalability_For_September_2%2C_2011.html">1109 high scalability-2011-09-02-Stuff The Internet Says On Scalability For September 2, 2011</a></p>
<p>14 0.75588894 <a title="773-lda-14" href="../high_scalability-2009/high_scalability-2009-04-21-What_CDN_would_you_recommend%3F.html">576 high scalability-2009-04-21-What CDN would you recommend?</a></p>
<p>15 0.75503683 <a title="773-lda-15" href="../high_scalability-2009/high_scalability-2009-04-16-Serving_250M_quotes-day_at_CNBC.com_with_aiCache.html">573 high scalability-2009-04-16-Serving 250M quotes-day at CNBC.com with aiCache</a></p>
<p>16 0.75415421 <a title="773-lda-16" href="../high_scalability-2011/high_scalability-2011-03-25-Did_the_Microsoft_Stack_Kill_MySpace%3F.html">1011 high scalability-2011-03-25-Did the Microsoft Stack Kill MySpace?</a></p>
<p>17 0.75369471 <a title="773-lda-17" href="../high_scalability-2010/high_scalability-2010-08-16-Scaling_an_AWS_infrastructure_-_Tools_and_Patterns.html">881 high scalability-2010-08-16-Scaling an AWS infrastructure - Tools and Patterns</a></p>
<p>18 0.75367647 <a title="773-lda-18" href="../high_scalability-2010/high_scalability-2010-07-02-Hot_Scalability_Links_for_July_2%2C_2010.html">851 high scalability-2010-07-02-Hot Scalability Links for July 2, 2010</a></p>
<p>19 0.75364083 <a title="773-lda-19" href="../high_scalability-2013/high_scalability-2013-06-14-Stuff_The_Internet_Says_On_Scalability_For_June_14%2C_2013.html">1476 high scalability-2013-06-14-Stuff The Internet Says On Scalability For June 14, 2013</a></p>
<p>20 0.75344503 <a title="773-lda-20" href="../high_scalability-2010/high_scalability-2010-07-13-DbShards_Part_Deux_-_The_Internals.html">857 high scalability-2010-07-13-DbShards Part Deux - The Internals</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
