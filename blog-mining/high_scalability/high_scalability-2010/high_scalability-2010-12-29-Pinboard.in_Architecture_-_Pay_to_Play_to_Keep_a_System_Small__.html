<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>965 high scalability-2010-12-29-Pinboard.in Architecture - Pay to Play to Keep a System Small  </title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2010" href="../home/high_scalability-2010_home.html">high_scalability-2010</a> <a title="high_scalability-2010-965" href="#">high_scalability-2010-965</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>965 high scalability-2010-12-29-Pinboard.in Architecture - Pay to Play to Keep a System Small  </h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2010-965-html" href="http://highscalability.com//blog/2010/12/29/pinboardin-architecture-pay-to-play-to-keep-a-system-small.html">html</a></p><p>Introduction: How do you keep a system small enough, while still being successful, that a
simple scale-up strategy becomes the preferred architecture?StackOverflow, for
example, could stick with a tool chain they were comfortable with because they
had a natural brake on how fast they could grow: there are only so many
programmers in the world. If this doesn't work for you, here's another natural
braking strategy to consider:charge for your service.Paul Houlesummarized this
nicely as:avoid scaling problems by building a service that's profitable at a
small scale.This interesting point, one I hadn't properly considered before,
was brought up by Maciej Ceglowski, co-founder ofPinboard.in, in an interview
with Leo Laporte and Amber MacArthur on their theirnet@nightshow.Pinboard is a
lean, mean, pay for bookmarking machine, a timely replacement for the nearly
departed Delicious. And as a self professed anti-social bookmarking site, it
emphasizes speed over socializing. Maciej considers Pinboard a persona</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 When the demise of Delicious was announced, if Pinboard had been a free site they'd have been down immediately, but being a paid site helped flatten out their growth curve. [sent-10, score-0.621]
</p><p>2 What you really want is a big server sucking down your bookmarks from wherever you might bookmark them, and that's just what Pinboard does. [sent-13, score-0.362]
</p><p>3 It requires payment for services to cover costs and keep the user base manageable. [sent-15, score-0.175]
</p><p>4 They built the site slowly and want to keep it small, simple fast, and reliable because that's a better way to run things. [sent-17, score-0.225]
</p><p>5 The database fits entirely in RAM and page load times have improved by a factor of 10 or more. [sent-24, score-0.179]
</p><p>6 Perl scripts run background tasks:Downloading outside feeds, caching pages for users with the archive feature enabled, handling incoming email, generating tag clouds, and running backups. [sent-30, score-0.39]
</p><p>7 Features like most popular bookmarks are generated by a cron job that is generally run each night, but are turned off when the load becomes too high. [sent-32, score-0.248]
</p><p>8 Sphinx is used for the search engine and for global tag pages. [sent-37, score-0.199]
</p><p>9 So the site seems conceptually confusing during a period of growth that's OK. [sent-43, score-0.244]
</p><p>10 The advantage of being a paid site is you don't get the rush of new users, so you can stay small. [sent-48, score-0.283]
</p><p>11 When the demise of Delicious was announced, if they would have been a free site they would have been down immediately, but being a paid site helps smooth out the growth. [sent-49, score-0.552]
</p><p>12 For an extra $25/year all your bookmarks can be cached and searched. [sent-57, score-0.197]
</p><p>13 These help ensure the site will never lose data and be very fast. [sent-59, score-0.175]
</p><p>14 Those first steps from where you go to a tiny site to a medium or medium large site are painful and expensive. [sent-74, score-0.53]
</p><p>15 Astorm of new usersadded over seven million bookmarks, more than were collected over the entire lifetime of the service, and traffic to the site was over a hundred times normal. [sent-80, score-0.283]
</p><p>16 As a result normal background tasks like search, archiving, and polling outside feeds were suspended. [sent-81, score-0.206]
</p><p>17 Look at outlier page load times, not median page load times to judge the quality of your service. [sent-83, score-0.348]
</p><p>18 It's not acceptable if a page can take multiple seconds to load even is most of the page load times are acceptable. [sent-84, score-0.299]
</p><p>19 " Other sites will let you share links with friends and discover new and interesting content, but no other site acts like a personal archive and that's Pinboard's niche. [sent-87, score-0.467]
</p><p>20 The service wasn't handling a huge number of requests to begin with--a few hundred per minute at peak--but that number increased about tenfold to over 2,500 requests per minute. [sent-92, score-0.217]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pinboard', 0.542), ('delicious', 0.351), ('bookmarks', 0.197), ('site', 0.175), ('bookmarking', 0.157), ('gb', 0.137), ('bookmark', 0.116), ('pinboardby', 0.116), ('archive', 0.11), ('paid', 0.108), ('maciej', 0.105), ('tag', 0.095), ('demise', 0.094), ('medium', 0.09), ('runs', 0.085), ('personal', 0.082), ('qualities', 0.078), ('feeds', 0.074), ('payment', 0.073), ('background', 0.072), ('page', 0.069), ('growth', 0.069), ('small', 0.068), ('announced', 0.063), ('outside', 0.06), ('times', 0.059), ('number', 0.058), ('master', 0.057), ('search', 0.056), ('dollars', 0.054), ('users', 0.053), ('brake', 0.052), ('amber', 0.052), ('layoffs', 0.052), ('learnedhave', 0.052), ('tenfold', 0.052), ('friends', 0.052), ('night', 0.052), ('cover', 0.052), ('load', 0.051), ('natural', 0.051), ('keep', 0.05), ('shots', 0.049), ('outlier', 0.049), ('sucking', 0.049), ('exodus', 0.049), ('hundred', 0.049), ('strategy', 0.048), ('engine', 0.048), ('sites', 0.048)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="965-tfidf-1" href="../high_scalability-2010/high_scalability-2010-12-29-Pinboard.in_Architecture_-_Pay_to_Play_to_Keep_a_System_Small__.html">965 high scalability-2010-12-29-Pinboard.in Architecture - Pay to Play to Keep a System Small  </a></p>
<p>Introduction: How do you keep a system small enough, while still being successful, that a
simple scale-up strategy becomes the preferred architecture?StackOverflow, for
example, could stick with a tool chain they were comfortable with because they
had a natural brake on how fast they could grow: there are only so many
programmers in the world. If this doesn't work for you, here's another natural
braking strategy to consider:charge for your service.Paul Houlesummarized this
nicely as:avoid scaling problems by building a service that's profitable at a
small scale.This interesting point, one I hadn't properly considered before,
was brought up by Maciej Ceglowski, co-founder ofPinboard.in, in an interview
with Leo Laporte and Amber MacArthur on their theirnet@nightshow.Pinboard is a
lean, mean, pay for bookmarking machine, a timely replacement for the nearly
departed Delicious. And as a self professed anti-social bookmarking site, it
emphasizes speed over socializing. Maciej considers Pinboard a persona</p><p>2 0.17860456 <a title="965-tfidf-2" href="../high_scalability-2011/high_scalability-2011-06-29-Second_Hand_Seizure_%3A_A_New_Cause_of_Site_Death.html">1070 high scalability-2011-06-29-Second Hand Seizure : A New Cause of Site Death</a></p>
<p>Introduction: Like a digital SWAT team that implodes the wrong door on a raid, the FBI
seized multiple racks of computers from DigitalOne, theseracks host websites
from many clients that just happened to be in the same racks as whomever they
are investigating. Downed sites include Instapaper, Curbed Network,
andPinboard. With thedensity of serversthese days many 1000s of sites could
easily have been effected.Sites like Pinboard were victims by association,
they did not inhale. This is an association sites have no control over. On a
shared hosting service, you have no control over your fellow VM mates. In a
cloud or a managed service, you have no control over which racks your servers
are in. So like second hand smoke, you get the disease by random association.
There's something inherently unfair about that.Acomment by illumin8 shows just
how Darth insidious this process can be:A popular method used by hackers is to
sign up for a virtual server with a stolen credit card. If they are careful
and only a</p><p>3 0.1374952 <a title="965-tfidf-3" href="../high_scalability-2007/high_scalability-2007-12-28-Amazon%27s_EC2%3A_Pay_as_You_Grow_Could_Cut_Your_Costs_in_Half.html">195 high scalability-2007-12-28-Amazon's EC2: Pay as You Grow Could Cut Your Costs in Half</a></p>
<p>Introduction: Update 2: SummizeComputes Computing Resources for a Startup. Lots of nice
graphs showing Amazon is hard to beat for small machines and become less cost
efficient for well used larger machines. Long term storage costs may eat your
saving away. And out of cloud bandwidth costs are high.Update:
viaProductionScale, a nice Digital Web article onhow to setup S3 to store
media filesand how Blue Origin was able to handle 3.5 million requests and 758
GBs in bandwidth in a single day for very little $$$. Also a Right Scale
article onNetwork performance within Amazon EC2 and to Amazon S3. 75MB/s
between EC2 instances, 10.2MB/s between EC2 and S3 for download, 6.9MB/s
upload.Now that Amazon's S3 (storage service) isout of betaand EC2 (elastic
compute cloud) has added newinstance types(the class of machine you can rent)
with more CPU and more RAM, I thought it would be interesting to take a look
out how their pricing stacks up.The quick conclusion:the more you scale the
more you save. A six node co</p><p>4 0.11812041 <a title="965-tfidf-4" href="../high_scalability-2009/high_scalability-2009-06-26-PlentyOfFish_Architecture.html">638 high scalability-2009-06-26-PlentyOfFish Architecture</a></p>
<p>Introduction: Update 5:PlentyOfFish Update - 6 Billion Pageviews And 32 Billion Images A
MonthUpdate 4:Jeff Atwoodcosts out Markus' scale up approach against a scale
out approach and finds scale up wanting. The discussion in the comments is as
interesting as the article. My guess is Markus doesn't want to rewrite his
software to work across a scale out cluster so even if it's more expensive
scale up works better for his needs.Update 3:POF now has 200 million imagesand
serves 10,000 images served per second. They'll be moving to a 250,000 IOPS
RamSan to handle the load. Also upgraded to a core database machine with 512
GB of RAM, 32 CPU's, SQLServer 2008 and Windows 2008.Update 2: This seems to
be aPOF Peer1 love fest infomercial. It's pretty content free, but the
production values are high. Lots of quirky sounds and fish swimming on the
screen.Update: by Facebook standards Read/WriteWeb says POF is worth a coolone
billion dollars. It helps to talk like Dr. Evil when saying it out
loud.PlentyOfFish i</p><p>5 0.11789972 <a title="965-tfidf-5" href="../high_scalability-2012/high_scalability-2012-11-22-Gone_Fishin%27%3A_PlentyOfFish_Architecture.html">1361 high scalability-2012-11-22-Gone Fishin': PlentyOfFish Architecture</a></p>
<p>Introduction: Other thanStackOverflow, PlentyOfFish is perhaps the most spectacular example
of scale-up architectures working for what your average sane person would
consider a large system. It doesn't hurt that it's also a sexy story.Update
5:PlentyOfFish Update - 6 Billion Pageviews And 32 Billion Images A
MonthUpdate 4:Jeff Atwoodcosts out Markus' scale up approach against a scale
out approach and finds scale up wanting. The discussion in the comments is as
interesting as the article. My guess is Markus doesn't want to rewrite his
software to work across a scale out cluster so even if it's more expensive
scale up works better for his needs.Update 3:POF now has 200 million imagesand
serves 10,000 images served per second. They'll be moving to a 250,000 IOPS
RamSan to handle the load. Also upgraded to a core database machine with 512
GB of RAM, 32 CPU's, SQLServer 2008 and Windows 2008.Update 2: This seems to
be aPOF Peer1 love fest infomercial. It's pretty content free, but the
production values a</p><p>6 0.11298167 <a title="965-tfidf-6" href="../high_scalability-2007/high_scalability-2007-10-02-Secrets_to_Fotolog%27s_Scaling_Success.html">106 high scalability-2007-10-02-Secrets to Fotolog's Scaling Success</a></p>
<p>7 0.11244836 <a title="965-tfidf-7" href="../high_scalability-2008/high_scalability-2008-01-29-When_things_aren%27t_scalable.html">232 high scalability-2008-01-29-When things aren't scalable</a></p>
<p>8 0.11152757 <a title="965-tfidf-8" href="../high_scalability-2009/high_scalability-2009-10-13-Why_are_Facebook%2C_Digg%2C_and_Twitter_so_hard_to_scale%3F.html">721 high scalability-2009-10-13-Why are Facebook, Digg, and Twitter so hard to scale?</a></p>
<p>9 0.11101387 <a title="965-tfidf-9" href="../high_scalability-2007/high_scalability-2007-07-06-Start_Here.html">1 high scalability-2007-07-06-Start Here</a></p>
<p>10 0.11022516 <a title="965-tfidf-10" href="../high_scalability-2012/high_scalability-2012-11-05-Gone_Fishin%27%3A_Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_In_The_Ambient_Cloud.html">1355 high scalability-2012-11-05-Gone Fishin': Building Super Scalable Systems: Blade Runner Meets Autonomic Computing In The Ambient Cloud</a></p>
<p>11 0.1102007 <a title="965-tfidf-11" href="../high_scalability-2009/high_scalability-2009-12-16-Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_in_the_Ambient_Cloud.html">750 high scalability-2009-12-16-Building Super Scalable Systems: Blade Runner Meets Autonomic Computing in the Ambient Cloud</a></p>
<p>12 0.11005444 <a title="965-tfidf-12" href="../high_scalability-2008/high_scalability-2008-05-31-Biggest_Under_Reported_Story%3A_Google%27s_BigTable_Costs_10_Times_Less_than_Amazon%27s_SimpleDB.html">336 high scalability-2008-05-31-Biggest Under Reported Story: Google's BigTable Costs 10 Times Less than Amazon's SimpleDB</a></p>
<p>13 0.10880812 <a title="965-tfidf-13" href="../high_scalability-2011/high_scalability-2011-08-22-Strategy%3A_Run_a_Scalable%2C_Available%2C_and_Cheap_Static_Site_on_S3_or_GitHub.html">1102 high scalability-2011-08-22-Strategy: Run a Scalable, Available, and Cheap Static Site on S3 or GitHub</a></p>
<p>14 0.10773022 <a title="965-tfidf-14" href="../high_scalability-2009/high_scalability-2009-08-16-ThePort__Network__Architecture.html">682 high scalability-2009-08-16-ThePort  Network  Architecture</a></p>
<p>15 0.10747974 <a title="965-tfidf-15" href="../high_scalability-2007/high_scalability-2007-11-13-Flickr_Architecture.html">152 high scalability-2007-11-13-Flickr Architecture</a></p>
<p>16 0.10732508 <a title="965-tfidf-16" href="../high_scalability-2008/high_scalability-2008-05-02-Friends_for_Sale_Architecture_-_A_300_Million_Page_View-Month_Facebook_RoR_App.html">313 high scalability-2008-05-02-Friends for Sale Architecture - A 300 Million Page View-Month Facebook RoR App</a></p>
<p>17 0.10504461 <a title="965-tfidf-17" href="../high_scalability-2007/high_scalability-2007-08-22-How_many_machines_do_you_need_to_run_your_site%3F.html">70 high scalability-2007-08-22-How many machines do you need to run your site?</a></p>
<p>18 0.1029887 <a title="965-tfidf-18" href="../high_scalability-2010/high_scalability-2010-04-12-Poppen.de_Architecture.html">808 high scalability-2010-04-12-Poppen.de Architecture</a></p>
<p>19 0.10215375 <a title="965-tfidf-19" href="../high_scalability-2013/high_scalability-2013-08-13-In_Memoriam%3A_Lavabit_Architecture_-_Creating_a_Scalable_Email_Service.html">1501 high scalability-2013-08-13-In Memoriam: Lavabit Architecture - Creating a Scalable Email Service</a></p>
<p>20 0.10040127 <a title="965-tfidf-20" href="../high_scalability-2012/high_scalability-2012-11-15-Gone_Fishin%27%3A_Justin.Tv%27s_Live_Video_Broadcasting_Architecture.html">1359 high scalability-2012-11-15-Gone Fishin': Justin.Tv's Live Video Broadcasting Architecture</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.2), (1, 0.091), (2, -0.035), (3, -0.111), (4, -0.005), (5, -0.087), (6, -0.027), (7, -0.01), (8, 0.043), (9, 0.014), (10, -0.015), (11, -0.039), (12, -0.001), (13, 0.029), (14, 0.04), (15, -0.016), (16, -0.061), (17, -0.027), (18, 0.042), (19, 0.03), (20, -0.007), (21, -0.002), (22, 0.003), (23, -0.012), (24, -0.019), (25, -0.044), (26, -0.022), (27, 0.001), (28, 0.034), (29, -0.029), (30, 0.064), (31, 0.02), (32, -0.051), (33, -0.036), (34, -0.012), (35, 0.01), (36, 0.002), (37, 0.026), (38, -0.08), (39, 0.007), (40, 0.017), (41, -0.023), (42, -0.013), (43, 0.034), (44, 0.005), (45, 0.006), (46, -0.017), (47, -0.03), (48, -0.013), (49, -0.037)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97869563 <a title="965-lsi-1" href="../high_scalability-2010/high_scalability-2010-12-29-Pinboard.in_Architecture_-_Pay_to_Play_to_Keep_a_System_Small__.html">965 high scalability-2010-12-29-Pinboard.in Architecture - Pay to Play to Keep a System Small  </a></p>
<p>Introduction: How do you keep a system small enough, while still being successful, that a
simple scale-up strategy becomes the preferred architecture?StackOverflow, for
example, could stick with a tool chain they were comfortable with because they
had a natural brake on how fast they could grow: there are only so many
programmers in the world. If this doesn't work for you, here's another natural
braking strategy to consider:charge for your service.Paul Houlesummarized this
nicely as:avoid scaling problems by building a service that's profitable at a
small scale.This interesting point, one I hadn't properly considered before,
was brought up by Maciej Ceglowski, co-founder ofPinboard.in, in an interview
with Leo Laporte and Amber MacArthur on their theirnet@nightshow.Pinboard is a
lean, mean, pay for bookmarking machine, a timely replacement for the nearly
departed Delicious. And as a self professed anti-social bookmarking site, it
emphasizes speed over socializing. Maciej considers Pinboard a persona</p><p>2 0.87493688 <a title="965-lsi-2" href="../high_scalability-2009/high_scalability-2009-09-22-How_Ravelry_Scales_to_10_Million_Requests_Using_Rails.html">711 high scalability-2009-09-22-How Ravelry Scales to 10 Million Requests Using Rails</a></p>
<p>Introduction: Tim Brayhas a wonderfulinterview with Casey Forbes, creator of Ravelry, a Ruby
on Rails site supporting a 400,000+ strong community of dedicated knitters and
crocheters.Casey and his small team have done great things with Ravelry. It is
a very focused site that provides a lot of value for users. And users
absolutely adore the site. That's obvious from their enthusiastic comments and
rocket fast adoption of Ravelry.Ten years ago a site like Ravelry would have
been a multi-million dollar operation. Today Casey is the sole engineer for
Ravelry and to run it takes only a few people. He was able to code it in 4
months working nights and weekends. Take a look down below of all the
technologies used to make Ravelry and you'll see how it is constructed almost
completely from free of the shelf software that Casey has stitched together
into a complete system. There's an amazing amount of leverage in today's
ecosystem when you combine all the quality tools, languages, storage,
bandwidth and hosti</p><p>3 0.83359367 <a title="965-lsi-3" href="../high_scalability-2007/high_scalability-2007-10-02-Secrets_to_Fotolog%27s_Scaling_Success.html">106 high scalability-2007-10-02-Secrets to Fotolog's Scaling Success</a></p>
<p>Introduction: Fotolog, a social blogging site centered around photos, grew from about 300
thousand users in 2004 to over 11 million users in 2007. Though they initially
experienced the inevitable pains of rapid growth, they overcame their problems
and now manage over 300 million photos and 800,000 new photos are added each
day. Generating all that fabulous content are 20 million unique monthly
visitors and a volunteer army of 30,000 new users each day. They did so well a
very impressed suitor bought them out for a cool $90 million. That's scale
meets success by anyone standards. How did they do it?Site:
http://www.fotolog.comInformation SourcesScaling the World's Largest Photo
Blogging CommunityCongrats to Fotolog on $90mm sale to Hi-MediaFotolog
overtaking Flickr?Fotolog Hits 11 Million Members and 300 Million Photos
PostedSite of the Week: Fotolog.comby PC MagazineCEO John Borthwick's Blog.DBA
Frank Mash's BlogFotolog, lessons learntby John BorthwickThe
PlatformJavaPHPSunSolaris 10MySQLApacheHiber</p><p>4 0.80222273 <a title="965-lsi-4" href="../high_scalability-2008/high_scalability-2008-01-29-When_things_aren%27t_scalable.html">232 high scalability-2008-01-29-When things aren't scalable</a></p>
<p>Introduction: OK,I know this site is for scalable web site design. But as there aren't any
sites I can find for graceful failure under "slashdotted" like pressure I'll
ask here.Does anyone have a sensible way, once you have a "web application"
that either won't scale, or can't scale, that you can give some users a good
consistent experience and bounce other users to a busy site page. I have seen
sites do this to varying degrees, some of which work better than others, but
no explanations beyond simply bouncing requests to a "we're busy page server"
when you have more than a given number of connections. This is obviously
useless as a web page likely requires multiple connection (ignoring keep-
alive, pipelining etc) multiple connection to completely render properly.The
normal problem is users getting a page and not the "furniture" for that page
like images or css. Other problems are having to wait ages to get the busy
page or the site being slow even if you do "get in". And some site let a user
"in" a</p><p>5 0.78022373 <a title="965-lsi-5" href="../high_scalability-2011/high_scalability-2011-08-31-Pud_is_the_Anti-Stack_-_Windows%2C_CFML%2C_Dropbox%2C_Xeround%2C_JungleDisk%2C_ELB.html">1108 high scalability-2011-08-31-Pud is the Anti-Stack - Windows, CFML, Dropbox, Xeround, JungleDisk, ELB</a></p>
<p>Introduction: Pud off*ckedcomany.com (FC) fame, a favorite site of the dot bomb era, and a
site I absolutely loved until my company became featured, has given us a look
at his backend: Why Must You Laugh At My Back End. For those whose don't
remember FC's history, TechCrunch published afitting eulogy:[FC] first went
live in 2000, chronicling failing and troubled companies in its unique and
abrasive style after the dot com bust. Within a year it had a massive audience
and was getting serious mainstream press attention. As the startup economy
became better in 2004, much of the attention the site received went away. But
a large and loyal audience remains at the site, coming back day after day for
its unique slant on the news. At its peak, FC had 4 million unique monthly
visitors.Delightfully, FC was not a real-names kind of site. Hard witty
cynicism ruled and not a single cat picture was in sight. It was a blast of
fun when all around was the enclosing dark.So when I saw Pud's post I was
quite interest</p><p>6 0.76209062 <a title="965-lsi-6" href="../high_scalability-2010/high_scalability-2010-04-12-Poppen.de_Architecture.html">808 high scalability-2010-04-12-Poppen.de Architecture</a></p>
<p>7 0.75549513 <a title="965-lsi-7" href="../high_scalability-2008/high_scalability-2008-06-09-FaceStat%27s_Rousing_Tale_of_Scaling_Woe_and_Wisdom_Won.html">344 high scalability-2008-06-09-FaceStat's Rousing Tale of Scaling Woe and Wisdom Won</a></p>
<p>8 0.75075936 <a title="965-lsi-8" href="../high_scalability-2011/high_scalability-2011-08-22-Strategy%3A_Run_a_Scalable%2C_Available%2C_and_Cheap_Static_Site_on_S3_or_GitHub.html">1102 high scalability-2011-08-22-Strategy: Run a Scalable, Available, and Cheap Static Site on S3 or GitHub</a></p>
<p>9 0.74760568 <a title="965-lsi-9" href="../high_scalability-2007/high_scalability-2007-11-12-Slashdot_Architecture_-_How_the_Old_Man_of_the_Internet_Learned_to_Scale.html">150 high scalability-2007-11-12-Slashdot Architecture - How the Old Man of the Internet Learned to Scale</a></p>
<p>10 0.74703544 <a title="965-lsi-10" href="../high_scalability-2013/high_scalability-2013-08-26-Reddit%3A_Lessons_Learned_from_Mistakes_Made_Scaling_to_1_Billion_Pageviews_a_Month.html">1507 high scalability-2013-08-26-Reddit: Lessons Learned from Mistakes Made Scaling to 1 Billion Pageviews a Month</a></p>
<p>11 0.74338883 <a title="965-lsi-11" href="../high_scalability-2008/high_scalability-2008-08-16-Strategy%3A_Serve_Pre-generated_Static_Files_Instead_Of_Dynamic_Pages.html">365 high scalability-2008-08-16-Strategy: Serve Pre-generated Static Files Instead Of Dynamic Pages</a></p>
<p>12 0.73843867 <a title="965-lsi-12" href="../high_scalability-2007/high_scalability-2007-10-28-Scaling_Early_Stage_Startups.html">136 high scalability-2007-10-28-Scaling Early Stage Startups</a></p>
<p>13 0.7371875 <a title="965-lsi-13" href="../high_scalability-2009/high_scalability-2009-05-31-Need_help_on_Site_loading_%26_database_optimization_-_URGENT.html">611 high scalability-2009-05-31-Need help on Site loading & database optimization - URGENT</a></p>
<p>14 0.73609465 <a title="965-lsi-14" href="../high_scalability-2012/high_scalability-2012-11-22-Gone_Fishin%27%3A_PlentyOfFish_Architecture.html">1361 high scalability-2012-11-22-Gone Fishin': PlentyOfFish Architecture</a></p>
<p>15 0.73367316 <a title="965-lsi-15" href="../high_scalability-2009/high_scalability-2009-06-26-PlentyOfFish_Architecture.html">638 high scalability-2009-06-26-PlentyOfFish Architecture</a></p>
<p>16 0.73179561 <a title="965-lsi-16" href="../high_scalability-2009/high_scalability-2009-06-01-Guess_How_Many_Users_it_Takes_to_Kill_Your_Site%3F.html">614 high scalability-2009-06-01-Guess How Many Users it Takes to Kill Your Site?</a></p>
<p>17 0.73140413 <a title="965-lsi-17" href="../high_scalability-2008/high_scalability-2008-11-03-How_Sites_are_Scaling_Up_for_the_Election_Night_Crush.html">437 high scalability-2008-11-03-How Sites are Scaling Up for the Election Night Crush</a></p>
<p>18 0.72472405 <a title="965-lsi-18" href="../high_scalability-2007/high_scalability-2007-10-16-How_Scalable_are_Single_Page_Ajax_Apps%3F.html">124 high scalability-2007-10-16-How Scalable are Single Page Ajax Apps?</a></p>
<p>19 0.72411686 <a title="965-lsi-19" href="../high_scalability-2007/high_scalability-2007-07-06-Start_Here.html">1 high scalability-2007-07-06-Start Here</a></p>
<p>20 0.71698296 <a title="965-lsi-20" href="../high_scalability-2011/high_scalability-2011-10-24-StackExchange_Architecture_Updates_-_Running_Smoothly%2C_Amazon_4x_More_Expensive.html">1131 high scalability-2011-10-24-StackExchange Architecture Updates - Running Smoothly, Amazon 4x More Expensive</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.071), (2, 0.186), (10, 0.057), (19, 0.011), (30, 0.031), (37, 0.238), (40, 0.036), (47, 0.011), (61, 0.116), (76, 0.011), (77, 0.026), (79, 0.094), (94, 0.035)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.93925887 <a title="965-lda-1" href="../high_scalability-2011/high_scalability-2011-05-02-The_Updated_Big_List_of_Articles_on_the_Amazon_Outage.html">1033 high scalability-2011-05-02-The Updated Big List of Articles on the Amazon Outage</a></p>
<p>Introduction: SinceThe Big List Of Articles On The Amazon Outage was published we've a had
few updates that people might not have seen. Amazon of course released their
Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East
Region. Netlix shared theirLessons Learned from the AWS Outage as did Heroku
(How Heroku Survived the Amazon Outage), Smug Mug (How SmugMug survived the
Amazonpocalypse), and SimpleGeo (How SimpleGeo Stayed Up During the AWS
Downtime). The curious thing from my perspective is the general lack of
response to Amazon's explanation. I expected more discussion. There's been
almost none that I've seen. My guess is very few people understand what Amazon
was talking about enough to comment whereas almost everyone feels qualified to
talk about the event itself.Lesson for crisis handlers: deep dive post-mortems
that are timely, long, honestish, and highly technical are the most effective
means of staunching the downward spiral of media attention. Amazon's
Explanation of</p><p>2 0.91255111 <a title="965-lda-2" href="../high_scalability-2011/high_scalability-2011-04-25-The_Big_List_of_Articles_on_the_Amazon_Outage.html">1029 high scalability-2011-04-25-The Big List of Articles on the Amazon Outage</a></p>
<p>Introduction: Please seeThe Updated Big List Of Articles On The Amazon Outage for a new
improved list.So many great articles have been written on the Amazon Outage.
Some aim at being helpful, some chastise developers for being so stupid, some
chastise Amazon for being so incompetent, some talk about the pain they and
their companies have experienced, and some even predict the downfall of the
cloud. Still others say we have seen a sea change in future of the cloud, a
prediction that's hard to disagree with, though the shape of the change
remains...cloudy.I'll try to keep this list update as more information comes
out. There will be a lot for developers to consider going forward. If there's
a resource you think should be added, just let me know.Amazon's Explanation of
What HappenedSummary of the Amazon EC2 and Amazon RDS Service Disruption in
the US East RegionHackers News thread on AWS Service Disruption Post Mortem
Quite Funny Commentary on the SummaryExperiences from Specific Companies, Both
Good a</p><p>same-blog 3 0.85749555 <a title="965-lda-3" href="../high_scalability-2010/high_scalability-2010-12-29-Pinboard.in_Architecture_-_Pay_to_Play_to_Keep_a_System_Small__.html">965 high scalability-2010-12-29-Pinboard.in Architecture - Pay to Play to Keep a System Small  </a></p>
<p>Introduction: How do you keep a system small enough, while still being successful, that a
simple scale-up strategy becomes the preferred architecture?StackOverflow, for
example, could stick with a tool chain they were comfortable with because they
had a natural brake on how fast they could grow: there are only so many
programmers in the world. If this doesn't work for you, here's another natural
braking strategy to consider:charge for your service.Paul Houlesummarized this
nicely as:avoid scaling problems by building a service that's profitable at a
small scale.This interesting point, one I hadn't properly considered before,
was brought up by Maciej Ceglowski, co-founder ofPinboard.in, in an interview
with Leo Laporte and Amber MacArthur on their theirnet@nightshow.Pinboard is a
lean, mean, pay for bookmarking machine, a timely replacement for the nearly
departed Delicious. And as a self professed anti-social bookmarking site, it
emphasizes speed over socializing. Maciej considers Pinboard a persona</p><p>4 0.85443074 <a title="965-lda-4" href="../high_scalability-2008/high_scalability-2008-04-29-Strategy%3A_Sample_to_Reduce_Data_Set.html">311 high scalability-2008-04-29-Strategy: Sample to Reduce Data Set</a></p>
<p>Introduction: Update:Arjenlinks to videoSupporting Scalable Online Statistical
Processingwhich shows"rather than doing complete aggregates, use statistical
sampling to provide a reasonable estimate (unbiased guess) of the result."When
you have a lot of data,samplingallows you to draw conclusions from a much
smaller amount of data. That's why sampling is a scalability solution. If you
don't have to process all your data to get the information you need then
you've made the problem smaller and you'll need fewer resources and you'll get
more timely results.breakSampling is not useful when you need a complete list
that matches a specific criteria. If you need to know the exact set of people
who bought a car in the last week then sampling won't help.But, if you want to
know many people bought a car then you could take a sample and then create
estimate of the full data-set. The difference is you won't really know the
exact car count. You'll have a confidence interval saying how confident you
are in your es</p><p>5 0.82260859 <a title="965-lda-5" href="../high_scalability-2011/high_scalability-2011-10-27-Strategy%3A_Survive_a_Comet_Strike_in_the_East_With_Reserved_Instances_in_the_West.html">1133 high scalability-2011-10-27-Strategy: Survive a Comet Strike in the East With Reserved Instances in the West</a></p>
<p>Introduction: Ali Sadat ofMuleSoft gave interesting presentation at Saturday's Talk Cloudy
to Me! event about their experiences movingMule iON, their ESB (enterprise
service bus) product to the cloud.First, a little aboutTalk Cloudy to Me. This
event is the second one day cloud event put on by Sebastian Stadil, founder of
Scalr, as part of the Cloud Computing Meetup group, also created by Sebastian.
Sebastian has become a master at running these mini-conference style events.
Really a quality job by him and his dedicated crew. These events are free,
sponsored by various vendors; they are short, 11-5; the food is good, Thai;
the venue is nice, eBay; they are on topic, with cloud and other speakers
giving 30-45 minute talks.  More on the event when the video comes out. I type
as fast as I can but I can't do much without the video.Back to Ali Sadat.
While he gave a lot of lessons--the integration will your billing system will
take a lot longer than you think; talk to your Amazon account reps as they
hav</p><p>6 0.82097942 <a title="965-lda-6" href="../high_scalability-2008/high_scalability-2008-05-03-Product%3A_nginx.html">314 high scalability-2008-05-03-Product: nginx</a></p>
<p>7 0.81839055 <a title="965-lda-7" href="../high_scalability-2007/high_scalability-2007-09-28-Kosmos_File_System_%28KFS%29_is_a_New_High_End_Google_File_System_Option.html">103 high scalability-2007-09-28-Kosmos File System (KFS) is a New High End Google File System Option</a></p>
<p>8 0.81594664 <a title="965-lda-8" href="../high_scalability-2012/high_scalability-2012-12-31-Designing_for_Resiliency_will_be_so_2013.html">1379 high scalability-2012-12-31-Designing for Resiliency will be so 2013</a></p>
<p>9 0.81464815 <a title="965-lda-9" href="../high_scalability-2010/high_scalability-2010-09-01-Scale-out_vs_Scale-up.html">891 high scalability-2010-09-01-Scale-out vs Scale-up</a></p>
<p>10 0.77775794 <a title="965-lda-10" href="../high_scalability-2013/high_scalability-2013-03-04-7_Life_Saving_Scalability_Defenses_Against_Load_Monster_Attacks.html">1415 high scalability-2013-03-04-7 Life Saving Scalability Defenses Against Load Monster Attacks</a></p>
<p>11 0.7674579 <a title="965-lda-11" href="../high_scalability-2012/high_scalability-2012-12-03-Resiliency_is_the_New_Normal_-_A_Deep_Look_at_What_It_Means_and_How_to_Build_It.html">1366 high scalability-2012-12-03-Resiliency is the New Normal - A Deep Look at What It Means and How to Build It</a></p>
<p>12 0.75952786 <a title="965-lda-12" href="../high_scalability-2009/high_scalability-2009-06-05-SSL_RPC_API_Scalability.html">620 high scalability-2009-06-05-SSL RPC API Scalability</a></p>
<p>13 0.75684553 <a title="965-lda-13" href="../high_scalability-2011/high_scalability-2011-08-29-The_Three_Ages_of_Google_-_Batch%2C_Warehouse%2C_Instant.html">1107 high scalability-2011-08-29-The Three Ages of Google - Batch, Warehouse, Instant</a></p>
<p>14 0.73616052 <a title="965-lda-14" href="../high_scalability-2013/high_scalability-2013-04-23-Facebook_Secrets_of_Web_Performance.html">1444 high scalability-2013-04-23-Facebook Secrets of Web Performance</a></p>
<p>15 0.73421293 <a title="965-lda-15" href="../high_scalability-2008/high_scalability-2008-03-17-Microsoft%27s_New_Database_Cloud_Ready_to_Rumble_with_Amazon.html">279 high scalability-2008-03-17-Microsoft's New Database Cloud Ready to Rumble with Amazon</a></p>
<p>16 0.72754562 <a title="965-lda-16" href="../high_scalability-2013/high_scalability-2013-03-06-Low_Level_Scalability_Solutions_-_The_Aggregation_Collection.html">1418 high scalability-2013-03-06-Low Level Scalability Solutions - The Aggregation Collection</a></p>
<p>17 0.71845162 <a title="965-lda-17" href="../high_scalability-2011/high_scalability-2011-09-07-What_Google_App_Engine_Price_Changes_Say_About_the_Future_of_Web_Architecture.html">1112 high scalability-2011-09-07-What Google App Engine Price Changes Say About the Future of Web Architecture</a></p>
<p>18 0.71793091 <a title="965-lda-18" href="../high_scalability-2011/high_scalability-2011-07-06-11_Common_Web_Use_Cases_Solved_in_Redis.html">1074 high scalability-2011-07-06-11 Common Web Use Cases Solved in Redis</a></p>
<p>19 0.7169022 <a title="965-lda-19" href="../high_scalability-2012/high_scalability-2012-01-30-37signals_Still_Happily_Scaling_on_Moore_RAM_and_SSDs.html">1183 high scalability-2012-01-30-37signals Still Happily Scaling on Moore RAM and SSDs</a></p>
<p>20 0.71683943 <a title="965-lda-20" href="../high_scalability-2012/high_scalability-2012-10-10-Antirez%3A_You_Need_to_Think_in_Terms_of_Organizing_Your_Data_for_Fetching.html">1337 high scalability-2012-10-10-Antirez: You Need to Think in Terms of Organizing Your Data for Fetching</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
