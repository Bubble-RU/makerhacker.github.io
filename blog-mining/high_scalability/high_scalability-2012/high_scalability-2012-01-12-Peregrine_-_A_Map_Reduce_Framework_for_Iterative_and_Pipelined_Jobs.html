<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1173 high scalability-2012-01-12-Peregrine - A Map Reduce Framework for Iterative and Pipelined Jobs</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2012" href="../home/high_scalability-2012_home.html">high_scalability-2012</a> <a title="high_scalability-2012-1173" href="#">high_scalability-2012-1173</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1173 high scalability-2012-01-12-Peregrine - A Map Reduce Framework for Iterative and Pipelined Jobs</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2012-1173-html" href="http://highscalability.com//blog/2012/1/12/peregrine-a-map-reduce-framework-for-iterative-and-pipelined.html">html</a></p><p>Introduction: The Peregrine falcon is a bird of prey, famous for its  high speed diving   attacks , feeding primarily on much slower Hadoops. Wait, sorry, it is Kevin Burton of Spinn3r's new  Peregrine project -- a  new FAST modern map reduce framework optimized for iterative and pipelined map reduce jobs -- that feeds on Hadoops.
 
If you don't know Kevin, he does a lot of excellent technical work that he's kind enough to share it on  his blog . Only he hasn't been blogging much lately, he's been heads down working on Peregrine. Now that Peregrine has been released, here's a short email interview with Kevin on why you might want to take up  falconry , the ancient sport of MapReduce.
  What does Spinn3r do that Peregrine is important to you?  
Ideally it was designed to execute pagerank but many iterative applications that we deploy and WANT to deploy (k-means) would be horribly inefficient under Hadoop as it doesn't have any support for merging and joining IO between tasks.  It also doesn't support</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Wait, sorry, it is Kevin Burton of Spinn3r's new  Peregrine project -- a  new FAST modern map reduce framework optimized for iterative and pipelined map reduce jobs -- that feeds on Hadoops. [sent-2, score-0.808]
</p><p>2 Ideally it was designed to execute pagerank but many iterative applications that we deploy and WANT to deploy (k-means) would be horribly inefficient under Hadoop as it doesn't have any support for merging and joining IO between tasks. [sent-7, score-0.32]
</p><p>3 It also doesn't support pipeline jobs from one MR task to another without first writing intermediate data to the filesystem. [sent-8, score-0.223]
</p><p>4 This allows us to do some zero copy IO and also fadvise away pages from the VFS page cache to prevent Linux from becoming confused and swapping. [sent-12, score-0.185]
</p><p>5 There are other optimizations including support for fallocate so that map chunks are extent based to provide for contiguous data allocation on disk. [sent-14, score-0.481]
</p><p>6 We also avoid using the JVM heap at all and allocate memory directly from the OS. [sent-16, score-0.191]
</p><p>7 there is just a core 64-128MB of memory used for smaller data structures in the JVM (and classes, etc) but the bulk of the memory used for caches and data structures, us used directly in the OS via anonymous mmap and mmap. [sent-19, score-0.32]
</p><p>8 This means if you have two algorithms, one that uses mmap and another that uses the heap, you have to ditch the efficient mmap version and move all your data into the heap (which is not going to improve performance). [sent-22, score-0.582]
</p><p>9 around 20k lines of code   That directly shuffling IO to the reducer nodes is a better design than Hadoop's intermediate shuffling. [sent-30, score-0.338]
</p><p>10 This also mandates using fully async IO to get past the C10k problem   That support for merge() along side map and reduce (ALA MapReduceMerge) was important and that we wanted it integrated. [sent-31, score-0.368]
</p><p>11 so I have to be amazingly inefficient to even break EVEN with the current indirect shuffling that Hadoop does. [sent-43, score-0.411]
</p><p>12 Essentially Hadoop takes all output from a map job, then writes it to the local filesystem. [sent-44, score-0.229]
</p><p>13 What does "running iterative jobs across partitions of data" mean? [sent-46, score-0.254]
</p><p>14 If you have two MR jobs , say MR0 and MR1 , and MR1 needs to join against the output from MR0, there is essentially no efficient way to do this with Hadoop because the HDFS blocks aren't stored on disk in a deterministic location. [sent-47, score-0.38]
</p><p>15 You basically have to merge the output form MR0 and MR1 into MR' and then sort the records which is amazingly inefficient. [sent-48, score-0.285]
</p><p>16 Peregrine just does a fast merge join on disk of the two files and then does a reduce() over the joined output. [sent-53, score-0.286]
</p><p>17 This also means that you can take some invariant/static data, and keep it in Peregrine and constantly join against it across hundreds of jobs without having to copy the IO every time. [sent-54, score-0.167]
</p><p>18 This isn't to say at all that Hadoop isn't an amazingly useful and powerful tool. [sent-62, score-0.185]
</p><p>19 But if you're doing an iterative computation with discrete data structures and you need to join against iterations and performance is a must then you should investigate using Peregrine. [sent-64, score-0.411]
</p><p>20 Related Articles        A new map reduce framework for iterative and pipelined jobs. [sent-65, score-0.489]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('peregrine', 0.563), ('mr', 0.237), ('mmap', 0.196), ('io', 0.174), ('iterative', 0.169), ('shuffling', 0.16), ('hadoop', 0.152), ('map', 0.147), ('heap', 0.139), ('fadvise', 0.125), ('fallocate', 0.125), ('flumejava', 0.113), ('amazingly', 0.112), ('pr', 0.107), ('jvm', 0.106), ('ala', 0.102), ('kevin', 0.1), ('runtime', 0.094), ('merge', 0.091), ('iterations', 0.088), ('reduce', 0.087), ('pipelined', 0.086), ('netty', 0.086), ('jobs', 0.085), ('pregel', 0.083), ('output', 0.082), ('join', 0.082), ('inefficient', 0.082), ('extent', 0.077), ('say', 0.073), ('structures', 0.072), ('support', 0.069), ('intermediate', 0.069), ('async', 0.065), ('optimizations', 0.063), ('initially', 0.062), ('pages', 0.06), ('etc', 0.06), ('os', 0.058), ('disk', 0.058), ('indirect', 0.057), ('reducer', 0.057), ('files', 0.055), ('compute', 0.055), ('sport', 0.053), ('articlesa', 0.053), ('onhis', 0.053), ('falcon', 0.053), ('directly', 0.052), ('improve', 0.051)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="1173-tfidf-1" href="../high_scalability-2012/high_scalability-2012-01-12-Peregrine_-_A_Map_Reduce_Framework_for_Iterative_and_Pipelined_Jobs.html">1173 high scalability-2012-01-12-Peregrine - A Map Reduce Framework for Iterative and Pipelined Jobs</a></p>
<p>Introduction: The Peregrine falcon is a bird of prey, famous for its  high speed diving   attacks , feeding primarily on much slower Hadoops. Wait, sorry, it is Kevin Burton of Spinn3r's new  Peregrine project -- a  new FAST modern map reduce framework optimized for iterative and pipelined map reduce jobs -- that feeds on Hadoops.
 
If you don't know Kevin, he does a lot of excellent technical work that he's kind enough to share it on  his blog . Only he hasn't been blogging much lately, he's been heads down working on Peregrine. Now that Peregrine has been released, here's a short email interview with Kevin on why you might want to take up  falconry , the ancient sport of MapReduce.
  What does Spinn3r do that Peregrine is important to you?  
Ideally it was designed to execute pagerank but many iterative applications that we deploy and WANT to deploy (k-means) would be horribly inefficient under Hadoop as it doesn't have any support for merging and joining IO between tasks.  It also doesn't support</p><p>2 0.14815325 <a title="1173-tfidf-2" href="../high_scalability-2009/high_scalability-2009-07-30-Learn_How_to_Think_at_Scale.html">666 high scalability-2009-07-30-Learn How to Think at Scale</a></p>
<p>Introduction: Aaron Kimball of Cloudera gives a wonderful 23 minute presentation titled  Cloudera Hadoop Training: Thinking at Scale Cloudera  which talks about "common challenges and general best practices for scaling with your data." As a company Cloudera offers "enterprise-level support to users of Apache Hadoop." Part of that offering is a really useful series of  tutorial videos on the Hadoop ecosystem .   Like TV lawyer Perry Mason (or is it Harmon Rabb?), Aaron gradually builds his case. He opens with the problem of storing lots of data. Then a blistering cross examination of the problem of building distributed systems to analyze that data sets up a powerful closing argument. With so much testimony behind him, on closing Aaron really brings it home with why shared nothing systems like map-reduce are the right solution on how to query lots of data. They jury loved it.   Here's the video  Thinking at Scale . And here's a summary of some of the lessons learned from the talk:
  Lessons Learned</p><p>3 0.14065428 <a title="1173-tfidf-3" href="../high_scalability-2008/high_scalability-2008-09-28-Product%3A_Happy_%3D_Hadoop_%2B_Python.html">397 high scalability-2008-09-28-Product: Happy = Hadoop + Python</a></p>
<p>Introduction: Has a Java only Hadoop been getting you down? Now you can be  Happy . Happy is a  framework for writing map-reduce programs for Hadoop using Jython. It files off the sharp edges on Hadoop and makes writing map-reduce programs a breeze . There's really no history yet on Happy, but I'm delighted at the idea of being able to map-reduce in  other languages . The more ways the better.  From the website:
   Happy is a framework that allows Hadoop jobs to be written and run in Python 2.2 using Jython. It is an  easy way to write map-reduce programs for Hadoop, and includes some new useful features as well.  The current release supports Hadoop 0.17.2.  Map-reduce jobs in Happy are defined by sub-classing happy.HappyJob and implementing a  map(records, task) and reduce(key, values, task) function. Then you create an instance of the  class, set the job parameters (such as inputs and outputs) and call run().  When you call run(), Happy serializes your job instance and copies it and all accompanyi</p><p>4 0.13942969 <a title="1173-tfidf-4" href="../high_scalability-2008/high_scalability-2008-11-22-Google_Architecture.html">448 high scalability-2008-11-22-Google Architecture</a></p>
<p>Introduction: Update 2:   Sorting 1 PB with MapReduce . PB is not peanut-butter-and-jelly misspelled. It's 1 petabyte or 1000 terabytes or 1,000,000 gigabytes.  It took six hours and two minutes to sort 1PB (10 trillion 100-byte records) on 4,000 computers  and the results were replicated thrice on 48,000 disks.  Update:   Greg Linden  points to a new Google article  MapReduce: simplified data processing on large clusters . Some interesting stats: 100k MapReduce jobs are executed each day; more than 20 petabytes of data are processed per day; more than 10k MapReduce programs have been implemented; machines are dual processor with gigabit ethernet and 4-8 GB of memory.  Google is the King of scalability.  Everyone knows Google for their large,  sophisticated, and fast searching, but they don't just shine in search. Their platform approach to building scalable applications allows them to roll out internet scale applications at an alarmingly high competition crushing rate. Their goal is always to build</p><p>5 0.13501582 <a title="1173-tfidf-5" href="../high_scalability-2009/high_scalability-2009-05-17-Product%3A_Hadoop.html">601 high scalability-2009-05-17-Product: Hadoop</a></p>
<p>Introduction: Update 5:   Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds  and has its  green cred questioned  because it took 40 times the number of machines Greenplum used to do the same work.   Update 4:   Introduction to Pig . Pig allows you to skip programming Hadoop at the low map-reduce level. You don't have to know Java. Using the Pig Latin language, which is a scripting data flow language, you can think about your problem as a data flow program. 10 lines of Pig Latin = 200 lines of Java.   Update 3 : Scaling Hadoop to  4000 nodes at Yahoo! .  30,000 cores with nearly 16PB of raw disk; sorted 6TB of data completed in 37 minutes; 14,000 map tasks writes (reads) 360 MB (about 3 blocks) of data into a single file with a total of 5.04 TB for the whole job.  Update 2 : Hadoop  Summit and Data-Intensive Computing Symposium Videos and Slides . Topics include: Pig, JAQL, Hbase, Hive, Data-Intensive Scalable Computing, Clouds and ManyCore: The Revolution, Simplicity and Complexity</p><p>6 0.10965931 <a title="1173-tfidf-6" href="../high_scalability-2014/high_scalability-2014-01-20-8_Ways_Stardog_Made_its_Database_Insanely_Scalable.html">1582 high scalability-2014-01-20-8 Ways Stardog Made its Database Insanely Scalable</a></p>
<p>7 0.10791834 <a title="1173-tfidf-7" href="../high_scalability-2010/high_scalability-2010-10-15-Troubles_with_Sharding_-_What_can_we_learn_from_the_Foursquare_Incident%3F.html">920 high scalability-2010-10-15-Troubles with Sharding - What can we learn from the Foursquare Incident?</a></p>
<p>8 0.10021712 <a title="1173-tfidf-8" href="../high_scalability-2011/high_scalability-2011-01-04-Map-Reduce_With_Ruby_Using_Hadoop.html">968 high scalability-2011-01-04-Map-Reduce With Ruby Using Hadoop</a></p>
<p>9 0.1002029 <a title="1173-tfidf-9" href="../high_scalability-2008/high_scalability-2008-01-30-How_Rackspace_Now_Uses_MapReduce_and_Hadoop_to_Query_Terabytes_of_Data.html">233 high scalability-2008-01-30-How Rackspace Now Uses MapReduce and Hadoop to Query Terabytes of Data</a></p>
<p>10 0.099465579 <a title="1173-tfidf-10" href="../high_scalability-2011/high_scalability-2011-09-19-Big_Iron_Returns_with_BigMemory.html">1118 high scalability-2011-09-19-Big Iron Returns with BigMemory</a></p>
<p>11 0.097426079 <a title="1173-tfidf-11" href="../high_scalability-2011/high_scalability-2011-03-08-Medialets_Architecture_-__Defeating_the_Daunting_Mobile_Device_Data_Deluge.html">1000 high scalability-2011-03-08-Medialets Architecture -  Defeating the Daunting Mobile Device Data Deluge</a></p>
<p>12 0.096524864 <a title="1173-tfidf-12" href="../high_scalability-2012/high_scalability-2012-04-03-Hazelcast_2.0%3A_Big_Data_In-Memory.html">1221 high scalability-2012-04-03-Hazelcast 2.0: Big Data In-Memory</a></p>
<p>13 0.091961332 <a title="1173-tfidf-13" href="../high_scalability-2012/high_scalability-2012-07-25-Vertical_Scaling_Ascendant_-_How_are_SSDs_Changing__Architectures%3F.html">1291 high scalability-2012-07-25-Vertical Scaling Ascendant - How are SSDs Changing  Architectures?</a></p>
<p>14 0.09175133 <a title="1173-tfidf-14" href="../high_scalability-2009/high_scalability-2009-06-11-Yahoo%21_Distribution_of_Hadoop.html">627 high scalability-2009-06-11-Yahoo! Distribution of Hadoop</a></p>
<p>15 0.09135817 <a title="1173-tfidf-15" href="../high_scalability-2012/high_scalability-2012-08-28-Making_Hadoop_Run_Faster.html">1313 high scalability-2012-08-28-Making Hadoop Run Faster</a></p>
<p>16 0.090426609 <a title="1173-tfidf-16" href="../high_scalability-2012/high_scalability-2012-07-30-Prismatic_Architecture_-_Using_Machine_Learning_on_Social_Networks_to_Figure_Out_What_You_Should_Read_on_the_Web_.html">1293 high scalability-2012-07-30-Prismatic Architecture - Using Machine Learning on Social Networks to Figure Out What You Should Read on the Web </a></p>
<p>17 0.086131349 <a title="1173-tfidf-17" href="../high_scalability-2009/high_scalability-2009-03-16-Are_Cloud_Based_Memory_Architectures_the_Next_Big_Thing%3F.html">538 high scalability-2009-03-16-Are Cloud Based Memory Architectures the Next Big Thing?</a></p>
<p>18 0.084487289 <a title="1173-tfidf-18" href="../high_scalability-2011/high_scalability-2011-01-28-Stuff_The_Internet_Says_On_Scalability_For_January_28%2C_2011.html">980 high scalability-2011-01-28-Stuff The Internet Says On Scalability For January 28, 2011</a></p>
<p>19 0.083939813 <a title="1173-tfidf-19" href="../high_scalability-2014/high_scalability-2014-03-24-Big%2C_Small%2C_Hot_or_Cold_-_Examples_of_Robust_Data_Pipelines_from_Stripe%2C_Tapad%2C_Etsy_and_Square.html">1618 high scalability-2014-03-24-Big, Small, Hot or Cold - Examples of Robust Data Pipelines from Stripe, Tapad, Etsy and Square</a></p>
<p>20 0.083649755 <a title="1173-tfidf-20" href="../high_scalability-2011/high_scalability-2011-07-27-Making_Hadoop_1000x_Faster_for_Graph_Problems.html">1088 high scalability-2011-07-27-Making Hadoop 1000x Faster for Graph Problems</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.153), (1, 0.069), (2, -0.002), (3, 0.012), (4, 0.01), (5, 0.064), (6, 0.074), (7, 0.047), (8, 0.024), (9, 0.033), (10, 0.055), (11, -0.038), (12, 0.073), (13, -0.055), (14, 0.037), (15, -0.038), (16, -0.045), (17, -0.015), (18, -0.033), (19, 0.041), (20, -0.024), (21, 0.025), (22, 0.055), (23, 0.067), (24, -0.01), (25, 0.017), (26, 0.029), (27, -0.043), (28, -0.027), (29, 0.032), (30, 0.039), (31, 0.055), (32, -0.011), (33, -0.012), (34, 0.011), (35, 0.013), (36, -0.043), (37, 0.018), (38, -0.029), (39, -0.027), (40, -0.001), (41, 0.048), (42, -0.02), (43, 0.007), (44, 0.012), (45, -0.011), (46, 0.022), (47, -0.028), (48, -0.018), (49, -0.01)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95549071 <a title="1173-lsi-1" href="../high_scalability-2012/high_scalability-2012-01-12-Peregrine_-_A_Map_Reduce_Framework_for_Iterative_and_Pipelined_Jobs.html">1173 high scalability-2012-01-12-Peregrine - A Map Reduce Framework for Iterative and Pipelined Jobs</a></p>
<p>Introduction: The Peregrine falcon is a bird of prey, famous for its  high speed diving   attacks , feeding primarily on much slower Hadoops. Wait, sorry, it is Kevin Burton of Spinn3r's new  Peregrine project -- a  new FAST modern map reduce framework optimized for iterative and pipelined map reduce jobs -- that feeds on Hadoops.
 
If you don't know Kevin, he does a lot of excellent technical work that he's kind enough to share it on  his blog . Only he hasn't been blogging much lately, he's been heads down working on Peregrine. Now that Peregrine has been released, here's a short email interview with Kevin on why you might want to take up  falconry , the ancient sport of MapReduce.
  What does Spinn3r do that Peregrine is important to you?  
Ideally it was designed to execute pagerank but many iterative applications that we deploy and WANT to deploy (k-means) would be horribly inefficient under Hadoop as it doesn't have any support for merging and joining IO between tasks.  It also doesn't support</p><p>2 0.83411479 <a title="1173-lsi-2" href="../high_scalability-2008/high_scalability-2008-09-28-Product%3A_Happy_%3D_Hadoop_%2B_Python.html">397 high scalability-2008-09-28-Product: Happy = Hadoop + Python</a></p>
<p>Introduction: Has a Java only Hadoop been getting you down? Now you can be  Happy . Happy is a  framework for writing map-reduce programs for Hadoop using Jython. It files off the sharp edges on Hadoop and makes writing map-reduce programs a breeze . There's really no history yet on Happy, but I'm delighted at the idea of being able to map-reduce in  other languages . The more ways the better.  From the website:
   Happy is a framework that allows Hadoop jobs to be written and run in Python 2.2 using Jython. It is an  easy way to write map-reduce programs for Hadoop, and includes some new useful features as well.  The current release supports Hadoop 0.17.2.  Map-reduce jobs in Happy are defined by sub-classing happy.HappyJob and implementing a  map(records, task) and reduce(key, values, task) function. Then you create an instance of the  class, set the job parameters (such as inputs and outputs) and call run().  When you call run(), Happy serializes your job instance and copies it and all accompanyi</p><p>3 0.83295178 <a title="1173-lsi-3" href="../high_scalability-2013/high_scalability-2013-04-24-Strategy%3A_Using_Lots_of_RAM_Often_Cheaper_than_Using_a_Hadoop_Cluster.html">1445 high scalability-2013-04-24-Strategy: Using Lots of RAM Often Cheaper than Using a Hadoop Cluster</a></p>
<p>Introduction: Solving problems while saving money is always a problem. In  Nobody ever got ﬁred for using Hadoop on a cluster  they give some counter-intuitive advice by showing a big-memory server may  provide better performance per dollar than a cluster:
  
  For jobs where the input data is multi-terabyte or larger a Hadoop cluster is the right solution.  
 For smaller problems memory has reached a GB/$ ratio where it is technically and financially feasible to use a single server with 100s of GB of DRAM rather than a cluster. Given the majority of analytics jobs do not process huge data sets, a cluster doesn't need to be your first option. Scaling up RAM saves on programmer time, reduces programmer effort, improved accuracy, and reduces hardware costs.</p><p>4 0.82577348 <a title="1173-lsi-4" href="../high_scalability-2009/high_scalability-2009-05-17-Product%3A_Hadoop.html">601 high scalability-2009-05-17-Product: Hadoop</a></p>
<p>Introduction: Update 5:   Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds  and has its  green cred questioned  because it took 40 times the number of machines Greenplum used to do the same work.   Update 4:   Introduction to Pig . Pig allows you to skip programming Hadoop at the low map-reduce level. You don't have to know Java. Using the Pig Latin language, which is a scripting data flow language, you can think about your problem as a data flow program. 10 lines of Pig Latin = 200 lines of Java.   Update 3 : Scaling Hadoop to  4000 nodes at Yahoo! .  30,000 cores with nearly 16PB of raw disk; sorted 6TB of data completed in 37 minutes; 14,000 map tasks writes (reads) 360 MB (about 3 blocks) of data into a single file with a total of 5.04 TB for the whole job.  Update 2 : Hadoop  Summit and Data-Intensive Computing Symposium Videos and Slides . Topics include: Pig, JAQL, Hbase, Hive, Data-Intensive Scalable Computing, Clouds and ManyCore: The Revolution, Simplicity and Complexity</p><p>5 0.82563752 <a title="1173-lsi-5" href="../high_scalability-2011/high_scalability-2011-01-04-Map-Reduce_With_Ruby_Using_Hadoop.html">968 high scalability-2011-01-04-Map-Reduce With Ruby Using Hadoop</a></p>
<p>Introduction: A demonstration, with repeatable steps, of how to quickly fire-up a Hadoop cluster on Amazon EC2, load data onto the HDFS (Hadoop Distributed File-System), write map-reduce scripts in Ruby and use them to run a map-reduce job on your Hadoop cluster. You will  not  need to ssh into the cluster, as all tasks are run from your local machine. Below I am using my MacBook Pro as my local machine, but the steps I have provided should be reproducible on other platforms running bash and Java.
  

  

 Fire-Up Your Hadoop Cluster 

I choose the  Cloudera distribution of Hadoop  which is still 100% Apache licensed, but has some additional benefits. One of these benefits is that it is released by  Doug Cutting , who started Hadoop and drove it’s development at Yahoo! He also started  Lucene , which is another of my favourite Apache Projects, so I have good faith that he knows what he is doing. Another benefit, as you will see, is that it is simple to fire-up a Hadoop cluster.


I am going to use C</p><p>6 0.81107187 <a title="1173-lsi-6" href="../high_scalability-2012/high_scalability-2012-08-28-Making_Hadoop_Run_Faster.html">1313 high scalability-2012-08-28-Making Hadoop Run Faster</a></p>
<p>7 0.80185026 <a title="1173-lsi-7" href="../high_scalability-2012/high_scalability-2012-06-15-Stuff_The_Internet_Says_On_Scalability_For_June_15%2C_2012.html">1265 high scalability-2012-06-15-Stuff The Internet Says On Scalability For June 15, 2012</a></p>
<p>8 0.77469772 <a title="1173-lsi-8" href="../high_scalability-2013/high_scalability-2013-01-07-Analyzing_billions_of_credit_card_transactions_and_serving_low-latency_insights_in_the_cloud.html">1382 high scalability-2013-01-07-Analyzing billions of credit card transactions and serving low-latency insights in the cloud</a></p>
<p>9 0.7546553 <a title="1173-lsi-9" href="../high_scalability-2009/high_scalability-2009-08-03-Building_a_Data_Intensive_Web_Application_with_Cloudera%2C_Hadoop%2C_Hive%2C_Pig%2C_and_EC2.html">669 high scalability-2009-08-03-Building a Data Intensive Web Application with Cloudera, Hadoop, Hive, Pig, and EC2</a></p>
<p>10 0.74918151 <a title="1173-lsi-10" href="../high_scalability-2010/high_scalability-2010-07-20-Strategy%3A_Consider_When_a_Service_Starts_Billing_in_Your_Algorithm_Cost.html">862 high scalability-2010-07-20-Strategy: Consider When a Service Starts Billing in Your Algorithm Cost</a></p>
<p>11 0.74798107 <a title="1173-lsi-11" href="../high_scalability-2011/high_scalability-2011-07-08-Stuff_The_Internet_Says_On_Scalability_For_July_8%2C_2011.html">1076 high scalability-2011-07-08-Stuff The Internet Says On Scalability For July 8, 2011</a></p>
<p>12 0.74715543 <a title="1173-lsi-12" href="../high_scalability-2008/high_scalability-2008-11-14-Paper%3A_Pig_Latin%3A_A_Not-So-Foreign_Language_for_Data_Processing.html">443 high scalability-2008-11-14-Paper: Pig Latin: A Not-So-Foreign Language for Data Processing</a></p>
<p>13 0.73765618 <a title="1173-lsi-13" href="../high_scalability-2009/high_scalability-2009-07-30-Learn_How_to_Think_at_Scale.html">666 high scalability-2009-07-30-Learn How to Think at Scale</a></p>
<p>14 0.72578681 <a title="1173-lsi-14" href="../high_scalability-2013/high_scalability-2013-04-17-Tachyon__-_Fault_Tolerant_Distributed_File_System_with_300_Times_Higher_Throughput_than_HDFS.html">1442 high scalability-2013-04-17-Tachyon  - Fault Tolerant Distributed File System with 300 Times Higher Throughput than HDFS</a></p>
<p>15 0.71850955 <a title="1173-lsi-15" href="../high_scalability-2008/high_scalability-2008-02-19-Hadoop_Getting_Closer_to_1.0_Release.html">254 high scalability-2008-02-19-Hadoop Getting Closer to 1.0 Release</a></p>
<p>16 0.71615535 <a title="1173-lsi-16" href="../high_scalability-2011/high_scalability-2011-03-08-Medialets_Architecture_-__Defeating_the_Daunting_Mobile_Device_Data_Deluge.html">1000 high scalability-2011-03-08-Medialets Architecture -  Defeating the Daunting Mobile Device Data Deluge</a></p>
<p>17 0.70795035 <a title="1173-lsi-17" href="../high_scalability-2008/high_scalability-2008-10-15-Hadoop_-_A_Primer.html">414 high scalability-2008-10-15-Hadoop - A Primer</a></p>
<p>18 0.7079252 <a title="1173-lsi-18" href="../high_scalability-2014/high_scalability-2014-01-28-How_Next_Big_Sound_Tracks_Over_a_Trillion_Song_Plays%2C_Likes%2C_and_More_Using_a_Version_Control_System_for_Hadoop_Data.html">1586 high scalability-2014-01-28-How Next Big Sound Tracks Over a Trillion Song Plays, Likes, and More Using a Version Control System for Hadoop Data</a></p>
<p>19 0.70138013 <a title="1173-lsi-19" href="../high_scalability-2009/high_scalability-2009-09-17-Hot_Links_for_2009-9-17_.html">707 high scalability-2009-09-17-Hot Links for 2009-9-17 </a></p>
<p>20 0.70031172 <a title="1173-lsi-20" href="../high_scalability-2009/high_scalability-2009-06-11-Yahoo%21_Distribution_of_Hadoop.html">627 high scalability-2009-06-11-Yahoo! Distribution of Hadoop</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.129), (2, 0.156), (10, 0.027), (30, 0.016), (40, 0.016), (47, 0.032), (61, 0.099), (75, 0.196), (79, 0.126), (85, 0.034), (94, 0.06)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.9757334 <a title="1173-lda-1" href="../high_scalability-2012/high_scalability-2012-09-11-How_big_is_a_Petabyte%2C_Exabyte%2C_Zettabyte%2C_or_a_Yottabyte%3F.html">1320 high scalability-2012-09-11-How big is a Petabyte, Exabyte, Zettabyte, or a Yottabyte?</a></p>
<p>Introduction: This is an intuitive look at large data sizes By Julian Bunn in  Globally Interconnected Object Databases .
  Bytes(8 bits)   
 0.1 bytes:  A binary decision  
 1 byte:  A single character  
 10 bytes:  A single word  
 100 bytes:  A telegram  OR  A punched card  
   Kilobyte (1000 bytes)   
 1 Kilobyte:  A very short story  
 2 Kilobytes: A Typewritten page 
 10 Kilobytes:  An encyclopaedic page  OR  A deck of punched cards  
 50 Kilobytes: A compressed document image page 
 100 Kilobytes:  A low-resolution photograph  
 200 Kilobytes: A box of punched cards 
 500 Kilobytes: A very heavy box of punched cards 
   Megabyte (1 000 000 bytes)   
 1 Megabyte:  A small novel  OR  A 3.5 inch floppy disk  
 2 Megabytes: A high resolution photograph 
 5 Megabytes:  The complete works of Shakespeare  OR 30 seconds of TV-quality video 
 10 Megabytes: A minute of high-fidelity sound OR A digital chest X-ray 
 20 Megabytes:  A box of floppy disks  
 50 Megabytes: A digital mammogram 
 100 Megabyte</p><p>same-blog 2 0.88468391 <a title="1173-lda-2" href="../high_scalability-2012/high_scalability-2012-01-12-Peregrine_-_A_Map_Reduce_Framework_for_Iterative_and_Pipelined_Jobs.html">1173 high scalability-2012-01-12-Peregrine - A Map Reduce Framework for Iterative and Pipelined Jobs</a></p>
<p>Introduction: The Peregrine falcon is a bird of prey, famous for its  high speed diving   attacks , feeding primarily on much slower Hadoops. Wait, sorry, it is Kevin Burton of Spinn3r's new  Peregrine project -- a  new FAST modern map reduce framework optimized for iterative and pipelined map reduce jobs -- that feeds on Hadoops.
 
If you don't know Kevin, he does a lot of excellent technical work that he's kind enough to share it on  his blog . Only he hasn't been blogging much lately, he's been heads down working on Peregrine. Now that Peregrine has been released, here's a short email interview with Kevin on why you might want to take up  falconry , the ancient sport of MapReduce.
  What does Spinn3r do that Peregrine is important to you?  
Ideally it was designed to execute pagerank but many iterative applications that we deploy and WANT to deploy (k-means) would be horribly inefficient under Hadoop as it doesn't have any support for merging and joining IO between tasks.  It also doesn't support</p><p>3 0.88166547 <a title="1173-lda-3" href="../high_scalability-2013/high_scalability-2013-11-07-Paper%3A_Tempest%3A_Scalable_Time-Critical_Web_Services_Platform.html">1544 high scalability-2013-11-07-Paper: Tempest: Scalable Time-Critical Web Services Platform</a></p>
<p>Introduction: An interesting and different implementation approach:  Tempest: Scalable Time-Critical Web Services Platform : 
  

Tempest is a new framework for developing time-critical web services. Tempest enables developers to build scalable, fault-tolerant services that can then be automatically replicated and deployed across clusters of computing nodes. The platform automatically adapts to load ﬂuctuations, reacts when components fail, and ensures consistency between replicas by repairing when inconsistencies do occur. Tempest relies on a family of epidemic protocols and on Ricochet, a reliable time critical multicast protocol with probabilistic guarantees.


Tempest is built around a novel storage abstraction called the TempestCollection in which application developers store the state of a service. Our platform handles the replication of this state across clones of the service, persistence, and failure handling. To minimize the need for specialized knowledge on the part of the application deve</p><p>4 0.8743068 <a title="1173-lda-4" href="../high_scalability-2010/high_scalability-2010-07-24-4_New_Podcasts_for_Scalable_Summertime_Reading.html">864 high scalability-2010-07-24-4 New Podcasts for Scalable Summertime Reading</a></p>
<p>Introduction: It's trendy today to say "I don't read blogs anymore, I just  let the  random chance of my social network guide me to new and interesting  content." #fail. While someone says this I imagine them flicking their  hair back in a "I can't be bothered with true understanding" disdain. And  where does random chance get its content? From people like these. So:  support your local blog! 
 
If you would like to be a part of random chance, here are a few new podcasts/blogs/vidcasts that you may not know about and that I've found interesting:
  
  DevOps Cafe . With this  new video series where John and Damon visit high  performing companies and record an insider's tour of the tools and  processes those companies are using to solve their DevOps problems , DevOps is a profession that finally seems to be realizing their own value. In the first episode John Paul Ramirez takes the crew on a tour of Shopzilla's application lifecycle metrics and dashboard. The second episode feature John Allspaw, VP of</p><p>5 0.87043798 <a title="1173-lda-5" href="../high_scalability-2007/high_scalability-2007-07-15-Lustre_cluster_file_system.html">13 high scalability-2007-07-15-Lustre cluster file system</a></p>
<p>Introduction: Lustre速  is a scalable, secure, robust, highly-available cluster file system. It is designed, developed and maintained by Cluster File Systems, Inc.  The central goal is the development of a next-generation cluster file system which can serve clusters with 10,000's of nodes, provide petabytes of storage, and move 100's of GB/sec with state-of-the-art security and management infrastructure.  Lustre runs on many of the largest Linux clusters in the world, and is included by CFS's partners as a core component of their cluster offering (examples include HP StorageWorks SFS, and the Cray XT3 and XD1 supercomputers). Today's users have also demonstrated that Lustre scales down as well as it scales up, and runs in production on clusters as small as 4 and as large as 25,000 nodes.  The latest version of Lustre is always available from Cluster File Systems, Inc. Public Open Source releases of Lustre are available under the GNU General Public License. These releases are found here, and are used</p><p>6 0.8637926 <a title="1173-lda-6" href="../high_scalability-2013/high_scalability-2013-11-22-Stuff_The_Internet_Says_On_Scalability_For_November_22th%2C_2013.html">1552 high scalability-2013-11-22-Stuff The Internet Says On Scalability For November 22th, 2013</a></p>
<p>7 0.82913071 <a title="1173-lda-7" href="../high_scalability-2014/high_scalability-2014-05-16-Stuff_The_Internet_Says_On_Scalability_For_May_16th%2C_2014.html">1649 high scalability-2014-05-16-Stuff The Internet Says On Scalability For May 16th, 2014</a></p>
<p>8 0.81720531 <a title="1173-lda-8" href="../high_scalability-2009/high_scalability-2009-04-26-Scale-up_vs._Scale-out%3A_A_Case_Study_by_IBM_using_Nutch-Lucene.html">583 high scalability-2009-04-26-Scale-up vs. Scale-out: A Case Study by IBM using Nutch-Lucene</a></p>
<p>9 0.81212538 <a title="1173-lda-9" href="../high_scalability-2012/high_scalability-2012-08-22-Cloud_Deployment%3A_It%E2%80%99s_All_About_Cloud_Automation.html">1309 high scalability-2012-08-22-Cloud Deployment: It’s All About Cloud Automation</a></p>
<p>10 0.80227721 <a title="1173-lda-10" href="../high_scalability-2008/high_scalability-2008-04-30-Rather_small_site_architecture..html">312 high scalability-2008-04-30-Rather small site architecture.</a></p>
<p>11 0.7973237 <a title="1173-lda-11" href="../high_scalability-2010/high_scalability-2010-03-09-Sponsored_Post%3A_Job_Openings_-_Squarespace.html">791 high scalability-2010-03-09-Sponsored Post: Job Openings - Squarespace</a></p>
<p>12 0.79681492 <a title="1173-lda-12" href="../high_scalability-2012/high_scalability-2012-01-24-The_State_of_NoSQL_in_2012.html">1180 high scalability-2012-01-24-The State of NoSQL in 2012</a></p>
<p>13 0.79664886 <a title="1173-lda-13" href="../high_scalability-2013/high_scalability-2013-12-06-Stuff_The_Internet_Says_On_Scalability_For_December_6th%2C_2013.html">1559 high scalability-2013-12-06-Stuff The Internet Says On Scalability For December 6th, 2013</a></p>
<p>14 0.79277462 <a title="1173-lda-14" href="../high_scalability-2009/high_scalability-2009-04-21-What_CDN_would_you_recommend%3F.html">576 high scalability-2009-04-21-What CDN would you recommend?</a></p>
<p>15 0.79269457 <a title="1173-lda-15" href="../high_scalability-2008/high_scalability-2008-04-08-Google_AppEngine_-_A_First_Look.html">301 high scalability-2008-04-08-Google AppEngine - A First Look</a></p>
<p>16 0.79206169 <a title="1173-lda-16" href="../high_scalability-2013/high_scalability-2013-04-29-AWS_v_GCE_Face-off_and_Why_Innovation_Needs_Lower_Cost_Infrastructures.html">1448 high scalability-2013-04-29-AWS v GCE Face-off and Why Innovation Needs Lower Cost Infrastructures</a></p>
<p>17 0.79055583 <a title="1173-lda-17" href="../high_scalability-2010/high_scalability-2010-07-02-Hot_Scalability_Links_for_July_2%2C_2010.html">851 high scalability-2010-07-02-Hot Scalability Links for July 2, 2010</a></p>
<p>18 0.79027587 <a title="1173-lda-18" href="../high_scalability-2007/high_scalability-2007-10-23-Hire_Facebook%2C_Ning%2C_and_Salesforce_to_Scale_for_You.html">129 high scalability-2007-10-23-Hire Facebook, Ning, and Salesforce to Scale for You</a></p>
<p>19 0.78962392 <a title="1173-lda-19" href="../high_scalability-2013/high_scalability-2013-01-18-Stuff_The_Internet_Says_On_Scalability_For_January_18%2C_2013.html">1389 high scalability-2013-01-18-Stuff The Internet Says On Scalability For January 18, 2013</a></p>
<p>20 0.78946978 <a title="1173-lda-20" href="../high_scalability-2010/high_scalability-2010-02-12-Hot_Scalability_Links_for_February_12%2C_2010.html">776 high scalability-2010-02-12-Hot Scalability Links for February 12, 2010</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
