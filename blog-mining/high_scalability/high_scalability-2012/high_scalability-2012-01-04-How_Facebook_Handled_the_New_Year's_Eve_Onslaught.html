<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1168 high scalability-2012-01-04-How Facebook Handled the New Year's Eve Onslaught</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2012" href="../home/high_scalability-2012_home.html">high_scalability-2012</a> <a title="high_scalability-2012-1168" href="#">high_scalability-2012-1168</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1168 high scalability-2012-01-04-How Facebook Handled the New Year's Eve Onslaught</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2012-1168-html" href="http://highscalability.com//blog/2012/1/4/how-facebook-handled-the-new-years-eve-onslaught.html">html</a></p><p>Introduction: How does Facebook handle the massive New Year's Eve traffic spike? Thanks to
Mike Swift, in Facebook gets ready for New Year's Eve, we get a little insight
as to their method for the madness, nothing really detailed, but still
interesting.Problem SetupFacebook expects tha one billion+ photos will be
shared on New Year's eve.Facebook's 800 million users are scattered around the
world. Three quarters live outside the US. Each user is linked to an average
of 130 friends.Photos and posts must appear in less than a second. Opening a
homepage requires executing requests on a 100 different servers, and those
requests have to be ranked, sorted, and privacy-checked, and then
rendered.Different events put different stresses on different parts of
Facebook. Photo and Video Uploads - Holidays require hundreds of terabytes of
capacity News Feed - News events like big sports events and the death of Steve
Jobs drive user status updatesCoping StrategiesTry to predictthe surge in
traffic. Run checkson h</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('eve', 0.326), ('events', 0.243), ('steve', 0.157), ('photos', 0.155), ('capacity', 0.149), ('tha', 0.146), ('year', 0.145), ('bin', 0.137), ('articlesfacebook', 0.137), ('surges', 0.137), ('homepage', 0.137), ('swift', 0.137), ('madness', 0.131), ('verdict', 0.131), ('status', 0.13), ('holidays', 0.126), ('sankar', 0.126), ('quarters', 0.126), ('expects', 0.122), ('krishna', 0.122)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="1168-tfidf-1" href="../high_scalability-2012/high_scalability-2012-01-04-How_Facebook_Handled_the_New_Year%27s_Eve_Onslaught.html">1168 high scalability-2012-01-04-How Facebook Handled the New Year's Eve Onslaught</a></p>
<p>Introduction: How does Facebook handle the massive New Year's Eve traffic spike? Thanks to
Mike Swift, in Facebook gets ready for New Year's Eve, we get a little insight
as to their method for the madness, nothing really detailed, but still
interesting.Problem SetupFacebook expects tha one billion+ photos will be
shared on New Year's eve.Facebook's 800 million users are scattered around the
world. Three quarters live outside the US. Each user is linked to an average
of 130 friends.Photos and posts must appear in less than a second. Opening a
homepage requires executing requests on a 100 different servers, and those
requests have to be ranked, sorted, and privacy-checked, and then
rendered.Different events put different stresses on different parts of
Facebook. Photo and Video Uploads - Holidays require hundreds of terabytes of
capacity News Feed - News events like big sports events and the death of Steve
Jobs drive user status updatesCoping StrategiesTry to predictthe surge in
traffic. Run checkson h</p><p>2 0.24180888 <a title="1168-tfidf-2" href="../high_scalability-2008/high_scalability-2008-10-22-EVE_Online_Architecture.html">424 high scalability-2008-10-22-EVE Online Architecture</a></p>
<p>Introduction: Sorry, the content for this post apparently did not make the transition from
the old HighScalability website, it's all messed up, but there's still a some
useful content...EVE Onlineis "The World's Largest Game Universe", a massively
multiplayer online game (MMO) made by CCP. EVE Online's Architecture is
unusual for a MMOG because it doesn't divide the player load among different
servers or shards. Instead, the same cluster handles the entire EVE universe.
It is an interesting to compare this with theArchitecture of the Second Life
Grid. How do they manage to scale?Information SourcesEVE Insider Dev BlogEVE
Online FAQMassively - Eve Evolved: Eve Online's Server Modeland itsdiscussion
on SlashdotEVE Online ForumsMassively Multiplayer Game Development
2PlatformStackless Pythonused for both server and client game logic. It allows
programmers to reap the benefits of thread-based programming without the
performance and complexity problems associated with conventional threads.SQL
ServerBlade</p><p>3 0.19574779 <a title="1168-tfidf-3" href="../high_scalability-2013/high_scalability-2013-02-13-7_Sensible_and_1_Really_Surprising_Way_EVE_Online_Scales_to_Play_Huge_Games.html">1405 high scalability-2013-02-13-7 Sensible and 1 Really Surprising Way EVE Online Scales to Play Huge Games</a></p>
<p>Introduction: "Everything in war is simple, but theďťż simplest thing is difficult." -- Carl
von ClausewitzGames are proving grounds for software architecture. They
combine scale, high performance, challenging problems, a rabid user base, cost
sensitivity, and the need for profit. And when games have in-game currency,
like EVE Online has, there's money at play, so you can't just get away with a
c'est la vie attitude. Engineering must be applied. InPlanning for war: how
the EVE Online servers deal with a 3,000 person battle, we learn some
techniques EVE Online uses to handle large games:7 Sensible...Do nothing.Most
games are manageable or have spikes that quickly dissipate. Run it Hot.
There's nothing to throttle as servers run at 100%. Why waste money? Use all
your CPU.Shard it. Games are sharded by solar system and multiple solar
systems run on a node.Move it. Games are moved when a machine becomes
overloaded. Live Node Remap, where a live game is moved to a new node without
disconnecting users, is n</p><p>4 0.16378513 <a title="1168-tfidf-4" href="../high_scalability-2010/high_scalability-2010-12-31-Facebook_in_20_Minutes%3A_2.7M_Photos%2C_10.2M_Comments%2C_4.6M_Messages.html">966 high scalability-2010-12-31-Facebook in 20 Minutes: 2.7M Photos, 10.2M Comments, 4.6M Messages</a></p>
<p>Introduction: To celebrate the new year Facebook hasshared the resultsof a little end of the
year introspection. It has been a fecund year for Facebook:43,869,800 changed
their status to single3,025,791 changed their status to "it's
complicated"28,460,516 changed their status to in a relationship5,974,574
changed their status to engaged36,774,801 changes their status to marriedIf
these numbers are simply to large to grasp, it doesn't get any better when you
look at happens in a mere 20 minutes:Shared links: 1,000,000 Tagged photos:
1,323,000Event invites sent out: 1,484,000Wall Posts: 1,587,000 Status
updates: 1,851,000Friend requests accepted: 1,972,000Photos uploaded:
2,716,000Comments: 10,208,000Message: 4,632,000If you want to see how Facebook
supports these huge numbers take a look at a few posts.One wonders what the
new year will bring?Related ArticlesWhat the World Eatsfrom Time Magazine A
Day in the Life of an Ancient RomanA Day in the Life of Donald DuckThe
Beatles- A Day in the LifeA Day i</p><p>5 0.1476744 <a title="1168-tfidf-5" href="../high_scalability-2010/high_scalability-2010-06-10-The_Four_Meta_Secrets_of_Scaling_at_Facebook.html">840 high scalability-2010-06-10-The Four Meta Secrets of Scaling at Facebook</a></p>
<p>Introduction: Aditya Agarwal, Director of Engineering at Facebook, gave an excellent Scale
at Facebooktalk that covers their architecture, but the talk is really more
about how to scale an organization by preserving the best parts of its
culture. The key take home of the talk is: You can get the code right, you can
get the products right, but you need to get the culture right first. If you
don't get the culture right then your company won't scale.This leads into the
four meta secrets of scaling at Facebook:Scaling takes IterationDon't Over
DesignChoose the right tool for the job, but realize that your choice comes
with overhead.Get the culture right. Move Fast \- break things. Huge Impact \-
small teams. Be bold \- innovate.Some Background Facebook is big: 400 million
active users; users spend an average of 20 minutes a day; 5 billion pieces of
content (status updates, comments, likes, photo uploads, video uploads, chat
messages, inbox messages, group events, fan pages, friend connections) are
share</p><p>6 0.13897303 <a title="1168-tfidf-6" href="../high_scalability-2010/high_scalability-2010-02-08-How_FarmVille_Scales_to_Harvest_75_Million_Players_a_Month.html">774 high scalability-2010-02-08-How FarmVille Scales to Harvest 75 Million Players a Month</a></p>
<p>7 0.11784588 <a title="1168-tfidf-7" href="../high_scalability-2010/high_scalability-2010-01-13-10_Hot_Scalability_Links_for_January_13%2C_2010.html">760 high scalability-2010-01-13-10 Hot Scalability Links for January 13, 2010</a></p>
<p>8 0.11347423 <a title="1168-tfidf-8" href="../high_scalability-2012/high_scalability-2012-02-21-Pixable_Architecture_-_Crawling%2C_Analyzing%2C_and_Ranking_20_Million_Photos_a_Day.html">1197 high scalability-2012-02-21-Pixable Architecture - Crawling, Analyzing, and Ranking 20 Million Photos a Day</a></p>
<p>9 0.11230461 <a title="1168-tfidf-9" href="../high_scalability-2007/high_scalability-2007-11-13-Flickr_Architecture.html">152 high scalability-2007-11-13-Flickr Architecture</a></p>
<p>10 0.10566946 <a title="1168-tfidf-10" href="../high_scalability-2011/high_scalability-2011-09-23-The_Real_News_is_Not_that_Facebook_Serves_Up_1_Trillion_Pages_a_Month%E2%80%A6.html">1123 high scalability-2011-09-23-The Real News is Not that Facebook Serves Up 1 Trillion Pages a Month…</a></p>
<p>11 0.10394613 <a title="1168-tfidf-11" href="../high_scalability-2007/high_scalability-2007-10-02-Secrets_to_Fotolog%27s_Scaling_Success.html">106 high scalability-2007-10-02-Secrets to Fotolog's Scaling Success</a></p>
<p>12 0.10054224 <a title="1168-tfidf-12" href="../high_scalability-2009/high_scalability-2009-06-29-How_to_Succeed_at_Capacity_Planning_Without_Really_Trying_%3A__An_Interview_with_Flickr%27s_John_Allspaw_on_His_New_Book.html">643 high scalability-2009-06-29-How to Succeed at Capacity Planning Without Really Trying :  An Interview with Flickr's John Allspaw on His New Book</a></p>
<p>13 0.098951206 <a title="1168-tfidf-13" href="../high_scalability-2012/high_scalability-2012-04-09-The_Instagram_Architecture_Facebook_Bought_for_a_Cool_Billion_Dollars.html">1224 high scalability-2012-04-09-The Instagram Architecture Facebook Bought for a Cool Billion Dollars</a></p>
<p>14 0.097350046 <a title="1168-tfidf-14" href="../high_scalability-2009/high_scalability-2009-02-16-Handle_1_Billion_Events_Per_Day_Using_a_Memory_Grid.html">513 high scalability-2009-02-16-Handle 1 Billion Events Per Day Using a Memory Grid</a></p>
<p>15 0.096336722 <a title="1168-tfidf-15" href="../high_scalability-2011/high_scalability-2011-03-22-Facebook%27s_New_Realtime_Analytics_System%3A_HBase_to_Process_20_Billion_Events_Per_Day.html">1008 high scalability-2011-03-22-Facebook's New Realtime Analytics System: HBase to Process 20 Billion Events Per Day</a></p>
<p>16 0.092720062 <a title="1168-tfidf-16" href="../high_scalability-2009/high_scalability-2009-10-12-High_Performance_at_Massive_Scale_%E2%80%93__Lessons_learned_at_Facebook.html">720 high scalability-2009-10-12-High Performance at Massive Scale –  Lessons learned at Facebook</a></p>
<p>17 0.09109246 <a title="1168-tfidf-17" href="../high_scalability-2011/high_scalability-2011-12-06-Instagram_Architecture%3A_14_Million_users%2C_Terabytes_of_Photos%2C_100s_of_Instances%2C_Dozens_of_Technologies.html">1152 high scalability-2011-12-06-Instagram Architecture: 14 Million users, Terabytes of Photos, 100s of Instances, Dozens of Technologies</a></p>
<p>18 0.088883564 <a title="1168-tfidf-18" href="../high_scalability-2014/high_scalability-2014-03-31-How_WhatsApp_Grew_to_Nearly_500_Million_Users%2C_11%2C000_cores%2C_and_70_Million_Messages_a_Second.html">1622 high scalability-2014-03-31-How WhatsApp Grew to Nearly 500 Million Users, 11,000 cores, and 70 Million Messages a Second</a></p>
<p>19 0.085682981 <a title="1168-tfidf-19" href="../high_scalability-2007/high_scalability-2007-10-23-Hire_Facebook%2C_Ning%2C_and_Salesforce_to_Scale_for_You.html">129 high scalability-2007-10-23-Hire Facebook, Ning, and Salesforce to Scale for You</a></p>
<p>20 0.084474899 <a title="1168-tfidf-20" href="../high_scalability-2012/high_scalability-2012-11-15-Gone_Fishin%27%3A_Justin.Tv%27s_Live_Video_Broadcasting_Architecture.html">1359 high scalability-2012-11-15-Gone Fishin': Justin.Tv's Live Video Broadcasting Architecture</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.136), (1, 0.075), (2, 0.016), (3, -0.039), (4, 0.01), (5, -0.076), (6, -0.039), (7, 0.071), (8, 0.034), (9, -0.008), (10, 0.053), (11, 0.036), (12, 0.05), (13, 0.013), (14, -0.044), (15, 0.095), (16, 0.034), (17, -0.005), (18, -0.007), (19, 0.058), (20, 0.048), (21, 0.037), (22, 0.042), (23, -0.011), (24, -0.003), (25, -0.033), (26, 0.006), (27, 0.028), (28, 0.03), (29, -0.016), (30, 0.015), (31, -0.002), (32, -0.044), (33, -0.035), (34, 0.043), (35, -0.002), (36, 0.073), (37, -0.123), (38, 0.019), (39, -0.038), (40, 0.013), (41, -0.03), (42, -0.014), (43, 0.032), (44, -0.01), (45, 0.001), (46, 0.013), (47, -0.037), (48, 0.001), (49, -0.027)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95467281 <a title="1168-lsi-1" href="../high_scalability-2012/high_scalability-2012-01-04-How_Facebook_Handled_the_New_Year%27s_Eve_Onslaught.html">1168 high scalability-2012-01-04-How Facebook Handled the New Year's Eve Onslaught</a></p>
<p>Introduction: How does Facebook handle the massive New Year's Eve traffic spike? Thanks to
Mike Swift, in Facebook gets ready for New Year's Eve, we get a little insight
as to their method for the madness, nothing really detailed, but still
interesting.Problem SetupFacebook expects tha one billion+ photos will be
shared on New Year's eve.Facebook's 800 million users are scattered around the
world. Three quarters live outside the US. Each user is linked to an average
of 130 friends.Photos and posts must appear in less than a second. Opening a
homepage requires executing requests on a 100 different servers, and those
requests have to be ranked, sorted, and privacy-checked, and then
rendered.Different events put different stresses on different parts of
Facebook. Photo and Video Uploads - Holidays require hundreds of terabytes of
capacity News Feed - News events like big sports events and the death of Steve
Jobs drive user status updatesCoping StrategiesTry to predictthe surge in
traffic. Run checkson h</p><p>2 0.77270979 <a title="1168-lsi-2" href="../high_scalability-2010/high_scalability-2010-12-31-Facebook_in_20_Minutes%3A_2.7M_Photos%2C_10.2M_Comments%2C_4.6M_Messages.html">966 high scalability-2010-12-31-Facebook in 20 Minutes: 2.7M Photos, 10.2M Comments, 4.6M Messages</a></p>
<p>Introduction: To celebrate the new year Facebook hasshared the resultsof a little end of the
year introspection. It has been a fecund year for Facebook:43,869,800 changed
their status to single3,025,791 changed their status to "it's
complicated"28,460,516 changed their status to in a relationship5,974,574
changed their status to engaged36,774,801 changes their status to marriedIf
these numbers are simply to large to grasp, it doesn't get any better when you
look at happens in a mere 20 minutes:Shared links: 1,000,000 Tagged photos:
1,323,000Event invites sent out: 1,484,000Wall Posts: 1,587,000 Status
updates: 1,851,000Friend requests accepted: 1,972,000Photos uploaded:
2,716,000Comments: 10,208,000Message: 4,632,000If you want to see how Facebook
supports these huge numbers take a look at a few posts.One wonders what the
new year will bring?Related ArticlesWhat the World Eatsfrom Time Magazine A
Day in the Life of an Ancient RomanA Day in the Life of Donald DuckThe
Beatles- A Day in the LifeA Day i</p><p>3 0.76020074 <a title="1168-lsi-3" href="../high_scalability-2010/high_scalability-2010-03-10-How_FarmVille_Scales_-_The_Follow-up.html">792 high scalability-2010-03-10-How FarmVille Scales - The Follow-up</a></p>
<p>Introduction: Several readers had follow-up questions in response toHow FarmVille Scales to
Harvest 75 Million Players a Month. Here are Luke's response to those
questions (and a few of mine).How does social networking makes things easier
or harder?The primary interesting aspect of social networking games is how you
wind up with a graph of connected users who need to be access each other's
data on a frequent basis. This makes the overall dataset difficult if not
impossible to partition.What are examples of the Facebook calls you try to
avoid and how they impact game play?We can make a call for facebook friend
data to retrieve information about your friends playing the game. Normally, we
show a friend ladder at the bottom of the game that shows friend information,
including name and facebook photo. Can you say where your cache is, what form
it takes, and how much cached there is? Do you have a peering relationship
with Facebook, as one might expect at that bandwidth?We use memcache as our
caching tec</p><p>4 0.7416029 <a title="1168-lsi-4" href="../high_scalability-2010/high_scalability-2010-08-02-7_Scaling_Strategies_Facebook_Used_to_Grow_to_500_Million_Users.html">870 high scalability-2010-08-02-7 Scaling Strategies Facebook Used to Grow to 500 Million Users</a></p>
<p>Introduction: Robert Johnson, a director of engineering at Facebook, celebrated Facebook's
monumental achievement of reaching 500 million users by sharing thescaling
principles that helped reach that milestone. In case you weren't suitably
impressed by the 500 million user number, Robert ratchets up the numbers game
with these impressive figures:1 million users per engineer500 million active
users100 billion hits per day50 billion photos2 trillion objects cached, with
hundreds of millions of requests per second130TB of logs every dayHow did
Facebook get to this point?People Matter Most. It's people who build and run
systems. The best tools for scaling are an engineering and operations teams
that can handle anything.Scale Horizontally. Handling exponentially growing
traffic requires spreading load arbitrarily across many machines. Using
different databases for tables like accounts and profiles only doubles
capacity. This approach hurts efficiency, but efficiency is a separate effort
from scaling, eff</p><p>5 0.70447832 <a title="1168-lsi-5" href="../high_scalability-2011/high_scalability-2011-09-23-The_Real_News_is_Not_that_Facebook_Serves_Up_1_Trillion_Pages_a_Month%E2%80%A6.html">1123 high scalability-2011-09-23-The Real News is Not that Facebook Serves Up 1 Trillion Pages a Month…</a></p>
<p>Introduction: There's some amount of debate whether Facebookreallycrossed over the one
trillion page view per month threshold. While one report says it did,another
respected firm says it did not; that its monthly page views are a mere 467
billion per month.In the big scheme of things, the discrepancy is somewhat
irrelevant, as neither show thetrueload on Facebook's infrastructure - which
is far more impressive a set of numbers than its externally measured "page
view" metric.  Mashable reported in "Facebook Surpasses 1 Trillion Pageviews
per Month" that the social networking giant saw "approximately 870 million
unique visitors in June and 860 million in July" and followed up with some per
visitor statistics, indicating "each visitor averaged approximately 1,160 page
views in July and 40 per visit -- enormous by any standard. Time spent on the
site was around 25 minutes per user."From an architectural standpoint it's
notjustabout the page views. It's about requests and responses, many of which
occur u</p><p>6 0.70141912 <a title="1168-lsi-6" href="../high_scalability-2010/high_scalability-2010-02-08-How_FarmVille_Scales_to_Harvest_75_Million_Players_a_Month.html">774 high scalability-2010-02-08-How FarmVille Scales to Harvest 75 Million Players a Month</a></p>
<p>7 0.69775653 <a title="1168-lsi-7" href="../high_scalability-2010/high_scalability-2010-06-10-The_Four_Meta_Secrets_of_Scaling_at_Facebook.html">840 high scalability-2010-06-10-The Four Meta Secrets of Scaling at Facebook</a></p>
<p>8 0.69100058 <a title="1168-lsi-8" href="../high_scalability-2014/high_scalability-2014-02-26-The_WhatsApp_Architecture_Facebook_Bought_For_%2419_Billion.html">1602 high scalability-2014-02-26-The WhatsApp Architecture Facebook Bought For $19 Billion</a></p>
<p>9 0.66994452 <a title="1168-lsi-9" href="../high_scalability-2014/high_scalability-2014-03-26-Oculus_Causes_a_Rift%2C_but_the_Facebook_Deal_Will_Avoid_a_Scaling_Crisis_for_Virtual_Reality.html">1619 high scalability-2014-03-26-Oculus Causes a Rift, but the Facebook Deal Will Avoid a Scaling Crisis for Virtual Reality</a></p>
<p>10 0.66301364 <a title="1168-lsi-10" href="../high_scalability-2009/high_scalability-2009-10-12-High_Performance_at_Massive_Scale_%E2%80%93__Lessons_learned_at_Facebook.html">720 high scalability-2009-10-12-High Performance at Massive Scale –  Lessons learned at Facebook</a></p>
<p>11 0.65571684 <a title="1168-lsi-11" href="../high_scalability-2013/high_scalability-2013-02-13-7_Sensible_and_1_Really_Surprising_Way_EVE_Online_Scales_to_Play_Huge_Games.html">1405 high scalability-2013-02-13-7 Sensible and 1 Really Surprising Way EVE Online Scales to Play Huge Games</a></p>
<p>12 0.61811304 <a title="1168-lsi-12" href="../high_scalability-2008/high_scalability-2008-10-22-EVE_Online_Architecture.html">424 high scalability-2008-10-22-EVE Online Architecture</a></p>
<p>13 0.61397797 <a title="1168-lsi-13" href="../high_scalability-2010/high_scalability-2010-09-21-Playfish%27s_Social_Gaming_Architecture_-_50_Million_Monthly_Users_and_Growing.html">904 high scalability-2010-09-21-Playfish's Social Gaming Architecture - 50 Million Monthly Users and Growing</a></p>
<p>14 0.60489213 <a title="1168-lsi-14" href="../high_scalability-2013/high_scalability-2013-02-25-SongPop_Scales_to_1_Million_Active_Users_on_GAE%2C_Showing_PaaS_is_not_Pass%C3%A9.html">1412 high scalability-2013-02-25-SongPop Scales to 1 Million Active Users on GAE, Showing PaaS is not Passé</a></p>
<p>15 0.59803069 <a title="1168-lsi-15" href="../high_scalability-2010/high_scalability-2010-01-11-Have_We_Reached_the_End_of_Scaling%3F.html">758 high scalability-2010-01-11-Have We Reached the End of Scaling?</a></p>
<p>16 0.59715176 <a title="1168-lsi-16" href="../high_scalability-2014/high_scalability-2014-02-13-Snabb_Switch_-_Skip_the_OS_and_Get_40_million_Requests_Per_Second_in_Lua.html">1595 high scalability-2014-02-13-Snabb Switch - Skip the OS and Get 40 million Requests Per Second in Lua</a></p>
<p>17 0.59547365 <a title="1168-lsi-17" href="../high_scalability-2008/high_scalability-2008-12-13-Strategy%3A_Facebook_Tweaks_to_Handle_6_Time_as_Many_Memcached_Requests.html">464 high scalability-2008-12-13-Strategy: Facebook Tweaks to Handle 6 Time as Many Memcached Requests</a></p>
<p>18 0.59481478 <a title="1168-lsi-18" href="../high_scalability-2007/high_scalability-2007-10-23-Hire_Facebook%2C_Ning%2C_and_Salesforce_to_Scale_for_You.html">129 high scalability-2007-10-23-Hire Facebook, Ning, and Salesforce to Scale for You</a></p>
<p>19 0.59195995 <a title="1168-lsi-19" href="../high_scalability-2012/high_scalability-2012-04-16-Instagram_Architecture_Update%3A_What%E2%80%99s_new_with_Instagram%3F.html">1228 high scalability-2012-04-16-Instagram Architecture Update: What’s new with Instagram?</a></p>
<p>20 0.58015347 <a title="1168-lsi-20" href="../high_scalability-2013/high_scalability-2013-04-23-Facebook_Secrets_of_Web_Performance.html">1444 high scalability-2013-04-23-Facebook Secrets of Web Performance</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.142), (2, 0.161), (40, 0.028), (51, 0.345), (61, 0.09), (79, 0.072), (85, 0.044), (94, 0.034)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.91037899 <a title="1168-lda-1" href="../high_scalability-2010/high_scalability-2010-04-30-Behind_the_scenes_of_an_online_marketplace.html">818 high scalability-2010-04-30-Behind the scenes of an online marketplace</a></p>
<p>Introduction: In a presentation originally held at the 4. O2 Hosting Event in Hamburg, I
spoke about the technology at a large online marketplace in Germany
calledHitmeister. Some of the topics discussed include:what makes up a
marketplace? technicallysystem principlesdevelopment patternstools
philosophydata modelhardwareI am looking forward to comments and suggestions
for both the presentation and our work.</p><p>same-blog 2 0.88257927 <a title="1168-lda-2" href="../high_scalability-2012/high_scalability-2012-01-04-How_Facebook_Handled_the_New_Year%27s_Eve_Onslaught.html">1168 high scalability-2012-01-04-How Facebook Handled the New Year's Eve Onslaught</a></p>
<p>Introduction: How does Facebook handle the massive New Year's Eve traffic spike? Thanks to
Mike Swift, in Facebook gets ready for New Year's Eve, we get a little insight
as to their method for the madness, nothing really detailed, but still
interesting.Problem SetupFacebook expects tha one billion+ photos will be
shared on New Year's eve.Facebook's 800 million users are scattered around the
world. Three quarters live outside the US. Each user is linked to an average
of 130 friends.Photos and posts must appear in less than a second. Opening a
homepage requires executing requests on a 100 different servers, and those
requests have to be ranked, sorted, and privacy-checked, and then
rendered.Different events put different stresses on different parts of
Facebook. Photo and Video Uploads - Holidays require hundreds of terabytes of
capacity News Feed - News events like big sports events and the death of Steve
Jobs drive user status updatesCoping StrategiesTry to predictthe surge in
traffic. Run checkson h</p><p>3 0.78321666 <a title="1168-lda-3" href="../high_scalability-2007/high_scalability-2007-09-18-Sync_data_on_all_servers.html">98 high scalability-2007-09-18-Sync data on all servers</a></p>
<p>Introduction: I have a few apache servers ( arround 11 atm ) serving a small amount of data
( arround 44 gigs right now ).For some time I have been using rsync to keep
all the content equal on all servers, but the amount of data has been growing,
and rsync takes a few too much time to "compare" all data from source to
destination, and create a lot of I/O.I have been taking a look at MogileFS, it
seems a good and reliable option, but as the fuse module is not finished, we
should have to rewrite all our apps, and its not an option atm.Any ideas?I
just want a "real time, non resource-hungry" solution alternative for rsync.
If I get more features on the way, then they are welcome :)Why I prefer to use
a Distributed File System instead of using NAS + NFS?- I need 2 NAS, if I dont
want a point of failure, and NAS hard is expensive.- Non-shared hardware, all
server has their own local disks.- As files are replicated, I can save a lot
of money, RAID is not a MUST.Thnx in advance for your help and sorry for</p><p>4 0.77849531 <a title="1168-lda-4" href="../high_scalability-2010/high_scalability-2010-06-08-Sponsored_Post%3A__Jobs%3A_Digg%2C_Huffington_Post_Events%3A__Velocity_Conference%2C_Social_Developer_Summit.html">838 high scalability-2010-06-08-Sponsored Post:  Jobs: Digg, Huffington Post Events:  Velocity Conference, Social Developer Summit</a></p>
<p>Introduction: Sponsored JobsGet Your High Scalability Fix at DiggHuffington Post: Linux
System AdministratorSponsored EventsVelocity Web Performance and Operations
Conference - Fast by DefaultSocial Developer Summit (HighScalability.com
readers save 15% with discount code SDSHS) Get Your High Scalability Fix at
Digg Interested in working on cutting-edge high-scale infrastructure at Digg?
We're making a big investment in scaling and have committed to the NoSQL (Not
only SQL) path withCassandra. We're using other open-source infrastructure to
help us scale including Hadoop, RabbitMQ, Zookeeper, Thrift, HDFS and Lucene.
We're rewriting Digg from the ground up and we need amazing developers to join
our world-class team. If you think you are up for the challenge, or you know
someone who might be, take a look at ourjobs pagefor more
information.Huffington Post: Linux System AdministratorHuffingtonPost has an
immediate need for a Linux System Administrator in our New York SoHo
officeEssential Skills and Ex</p><p>5 0.74629331 <a title="1168-lda-5" href="../high_scalability-2014/high_scalability-2014-04-10-Paper%3A_Scalable_Atomic_Visibility_with_RAMP_Transactions_-_Scale_Linearly_to_100_Servers.html">1629 high scalability-2014-04-10-Paper: Scalable Atomic Visibility with RAMP Transactions - Scale Linearly to 100 Servers</a></p>
<p>Introduction: We are not yet at the End of History for database theory asPeter Bailisand
theDatabase Groupat UC Berkeley continue to prove with a greatcompanion blog
post to their new paper: Scalable Atomic Visibility with RAMP Transactions. I
like the approach of pairing a blog post with a paper. A paper almost by
definition is formal, but a blog post can help put a paper in context and give
it some heart.From the abstract:Databases can provide scalability by
partitioning data across several servers. However, multi-partition, multi-
operation transactional access is often expensive, employing coordination-
intensive locking, validation, or scheduling mechanisms. Accordingly, many
real-world systems avoid mechanisms that provide useful semantics formulti-
partition operations. This leads to incorrect behavior for a large class of
applications including secondary indexing,foreign key enforcement, and
materialized view maintenance. In this work, we identify a new isolation model
--Read Atomic (RA) iso</p><p>6 0.73961473 <a title="1168-lda-6" href="../high_scalability-2009/high_scalability-2009-01-02-Strategy%3A_Understanding_Your_Data_Leads_to_the_Best_Scalability_Solutions.html">481 high scalability-2009-01-02-Strategy: Understanding Your Data Leads to the Best Scalability Solutions</a></p>
<p>7 0.73218089 <a title="1168-lda-7" href="../high_scalability-2009/high_scalability-2009-11-16-Building_Scalable_Systems_Using_Data_as_a_Composite_Material.html">741 high scalability-2009-11-16-Building Scalable Systems Using Data as a Composite Material</a></p>
<p>8 0.72979164 <a title="1168-lda-8" href="../high_scalability-2011/high_scalability-2011-10-28-Stuff_The_Internet_Says_On_Scalability_For_October_28%2C_2011.html">1134 high scalability-2011-10-28-Stuff The Internet Says On Scalability For October 28, 2011</a></p>
<p>9 0.72170353 <a title="1168-lda-9" href="../high_scalability-2012/high_scalability-2012-06-25-StubHub_Architecture%3A_The_Surprising_Complexity_Behind_the_World%E2%80%99s_Largest_Ticket_Marketplace.html">1271 high scalability-2012-06-25-StubHub Architecture: The Surprising Complexity Behind the World’s Largest Ticket Marketplace</a></p>
<p>10 0.70952153 <a title="1168-lda-10" href="../high_scalability-2014/high_scalability-2014-05-07-Update_on_Disqus%3A_It%27s_Still_About_Realtime%2C_But_Go_Demolishes_Python.html">1644 high scalability-2014-05-07-Update on Disqus: It's Still About Realtime, But Go Demolishes Python</a></p>
<p>11 0.70844907 <a title="1168-lda-11" href="../high_scalability-2008/high_scalability-2008-04-07-Lazy_web_sites_run_faster.html">298 high scalability-2008-04-07-Lazy web sites run faster</a></p>
<p>12 0.70334393 <a title="1168-lda-12" href="../high_scalability-2010/high_scalability-2010-06-22-Sponsored_Post%3A__Jobs%3A_Etsy%2C_Digg%2C_Huffington_Post_Event%3A_Velocity_Conference.html">846 high scalability-2010-06-22-Sponsored Post:  Jobs: Etsy, Digg, Huffington Post Event: Velocity Conference</a></p>
<p>13 0.70246458 <a title="1168-lda-13" href="../high_scalability-2009/high_scalability-2009-02-09-Paper%3A_Consensus_Protocols%3A_Two-Phase_Commit__.html">510 high scalability-2009-02-09-Paper: Consensus Protocols: Two-Phase Commit  </a></p>
<p>14 0.69818604 <a title="1168-lda-14" href="../high_scalability-2010/high_scalability-2010-12-03-GPU_vs_CPU_Smackdown_%3A_The_Rise_of_Throughput-Oriented_Architectures.html">953 high scalability-2010-12-03-GPU vs CPU Smackdown : The Rise of Throughput-Oriented Architectures</a></p>
<p>15 0.68874359 <a title="1168-lda-15" href="../high_scalability-2007/high_scalability-2007-10-30-Feedblendr_Architecture_-_Using_EC2_to_Scale.html">138 high scalability-2007-10-30-Feedblendr Architecture - Using EC2 to Scale</a></p>
<p>16 0.66191417 <a title="1168-lda-16" href="../high_scalability-2009/high_scalability-2009-06-10-Paper%3A_Graph_Databases_and_the_Future_of_Large-Scale_Knowledge_Management.html">626 high scalability-2009-06-10-Paper: Graph Databases and the Future of Large-Scale Knowledge Management</a></p>
<p>17 0.63604313 <a title="1168-lda-17" href="../high_scalability-2014/high_scalability-2014-04-28-How_Disqus_Went_Realtime_with_165K_Messages_Per_Second_and_Less_than_.2_Seconds_Latency.html">1638 high scalability-2014-04-28-How Disqus Went Realtime with 165K Messages Per Second and Less than .2 Seconds Latency</a></p>
<p>18 0.62259233 <a title="1168-lda-18" href="../high_scalability-2007/high_scalability-2007-08-09-Lots_of_questions_for_high_scalability_-_high_availability.html">63 high scalability-2007-08-09-Lots of questions for high scalability - high availability</a></p>
<p>19 0.6135751 <a title="1168-lda-19" href="../high_scalability-2008/high_scalability-2008-05-28-Job_queue_and_search_engine.html">332 high scalability-2008-05-28-Job queue and search engine</a></p>
<p>20 0.612481 <a title="1168-lda-20" href="../high_scalability-2007/high_scalability-2007-10-02-Secrets_to_Fotolog%27s_Scaling_Success.html">106 high scalability-2007-10-02-Secrets to Fotolog's Scaling Success</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
