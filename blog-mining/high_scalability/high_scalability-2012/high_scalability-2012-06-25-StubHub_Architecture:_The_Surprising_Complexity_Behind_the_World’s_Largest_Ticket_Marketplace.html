<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1271 high scalability-2012-06-25-StubHub Architecture: The Surprising Complexity Behind the World’s Largest Ticket Marketplace</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2012" href="../home/high_scalability-2012_home.html">high_scalability-2012</a> <a title="high_scalability-2012-1271" href="#">high_scalability-2012-1271</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1271 high scalability-2012-06-25-StubHub Architecture: The Surprising Complexity Behind the World’s Largest Ticket Marketplace</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2012-1271-html" href="http://highscalability.com//blog/2012/6/25/stubhub-architecture-the-surprising-complexity-behind-the-wo.html">html</a></p><p>Introduction: StubHub is an interesting architecture to take a look at because, as market
makers for tickets, they are in a different business than we normally get to
consider.StubHub is surprisingly large, growing at 20% a year, serving 800K
complex pages per hour, selling 5 million tickets per year, and handling 2
million API calls per hour. And the ticket space is surprisingly rich in
complexity. StubHub's traffic is tricky. It's bursty, centering around
unpredictable game outcomes, events, schedules, and seasons. There's a lot of
money involved. There are a lot of different actors involved. There are a lot
of complex business processes involved. And StubHub has several complementary
but very different parts of their business: they have an ad server component
serving ads to sites like ESPN, a rich interactive UI, and a real-time ticket
market component.Most interesting to me is how StubHub is bringing into the
digital realm the once quintessentially high-touch physical world of tickets,
point-of-</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 StubHub is surprisingly large, growing at 20% a year, serving 800K complex pages per hour, selling 5 million tickets per year, and handling 2 million API calls per hour. [sent-2, score-0.535]
</p><p>2 And StubHub has several complementary but very different parts of their business: they have an ad server component serving ads to sites like ESPN, a rich interactive UI, and a real-time ticket market component. [sent-9, score-0.364]
</p><p>3 They are making it happen with deep electronic integration into organizations (like Major League Baseball) and a Lifecycle Bus that moves complex business processes out of the application space. [sent-11, score-0.323]
</p><p>4 It's an interesting problem made more complex by having to move forward while dealing with legacy systems built when getting business building features out the door was the priority. [sent-12, score-0.351]
</p><p>5 Providing a marketplace for buyers and sellers of tickets. [sent-17, score-0.493]
</p><p>6 An escrow model is used to provide trust and safety for buyers a sellers. [sent-19, score-0.299]
</p><p>7 When a playoff game finishes, for example, there's a buyer frenzy for tickets for the next game. [sent-33, score-0.383]
</p><p>8 Bulk upload allows sheets of tickets to be uploaded into the system. [sent-40, score-0.314]
</p><p>9 The Ticket table gets hammered with the activity of buyers and sellers and the natural trafficburstinessaround events. [sent-43, score-0.493]
</p><p>10 An active market can cause the mobile clients to become out of date with the current state of the system so buyers are reacting to old data. [sent-44, score-0.294]
</p><p>11 Internal Feed - Contains sensitive information, like account information, that is used for dashboards, who sellers are, what sales are happening, sales velocities, heat maps, etc. [sent-49, score-0.321]
</p><p>12 Shopping experiences like selecting tickets from an interactive graphic of the stadium are powered by LCS. [sent-63, score-0.329]
</p><p>13 If they needed to swap out a new Solr index to put in a new schema, they could turn the valve off, let messages back up in the message broker, and open up the valve again an messages would start flowing again. [sent-80, score-0.46]
</p><p>14 When things go bad it takes a lot of working with buyers and sellers to straighten problems out. [sent-112, score-0.493]
</p><p>15 ExposeAPIsto sellers so they can integrate these features into their systems. [sent-116, score-0.317]
</p><p>16 Electronic integration with Major League Baseball so they can transfer a ticket directly to buyer from a seller before the seller has physical possession of the tickets. [sent-119, score-0.664]
</p><p>17 When the seller confirms the order there agents that willcapture the money from the buyer's escrow account, email the buyer saying the seller has confirmed and where to find the tickets. [sent-129, score-0.737]
</p><p>18 When the tickets are confirmed payment is released to the seller. [sent-130, score-0.369]
</p><p>19 The business process looks like: unconfirmed -> auto confirm; confirmed -> barcode reissue and Disburse PDF; fulfilled. [sent-138, score-0.336]
</p><p>20 Allyou have to do is write agents that live off the same order lifecycle to implement new fulfillment modes. [sent-139, score-0.371]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sellers', 0.264), ('tickets', 0.251), ('ticket', 0.246), ('stubhub', 0.233), ('buyers', 0.229), ('legacy', 0.17), ('dye', 0.155), ('seller', 0.143), ('buyer', 0.132), ('lifecycle', 0.131), ('valve', 0.126), ('confirmed', 0.118), ('business', 0.113), ('fulfillment', 0.109), ('unconfirmed', 0.105), ('allthe', 0.105), ('league', 0.095), ('baseball', 0.091), ('solr', 0.085), ('customer', 0.082), ('message', 0.08), ('cm', 0.078), ('lcs', 0.078), ('stadium', 0.078), ('usingsplunk', 0.078), ('per', 0.072), ('agents', 0.072), ('electronic', 0.072), ('escrow', 0.07), ('valves', 0.07), ('broker', 0.07), ('processes', 0.07), ('complex', 0.068), ('listener', 0.066), ('market', 0.065), ('messages', 0.064), ('agent', 0.063), ('sheets', 0.063), ('render', 0.062), ('staged', 0.061), ('order', 0.059), ('genre', 0.059), ('events', 0.058), ('maps', 0.058), ('confirm', 0.057), ('account', 0.057), ('ads', 0.053), ('integrate', 0.053), ('blocking', 0.053), ('approved', 0.052)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="1271-tfidf-1" href="../high_scalability-2012/high_scalability-2012-06-25-StubHub_Architecture%3A_The_Surprising_Complexity_Behind_the_World%E2%80%99s_Largest_Ticket_Marketplace.html">1271 high scalability-2012-06-25-StubHub Architecture: The Surprising Complexity Behind the World’s Largest Ticket Marketplace</a></p>
<p>Introduction: StubHub is an interesting architecture to take a look at because, as market
makers for tickets, they are in a different business than we normally get to
consider.StubHub is surprisingly large, growing at 20% a year, serving 800K
complex pages per hour, selling 5 million tickets per year, and handling 2
million API calls per hour. And the ticket space is surprisingly rich in
complexity. StubHub's traffic is tricky. It's bursty, centering around
unpredictable game outcomes, events, schedules, and seasons. There's a lot of
money involved. There are a lot of different actors involved. There are a lot
of complex business processes involved. And StubHub has several complementary
but very different parts of their business: they have an ad server component
serving ads to sites like ESPN, a rich interactive UI, and a real-time ticket
market component.Most interesting to me is how StubHub is bringing into the
digital realm the once quintessentially high-touch physical world of tickets,
point-of-</p><p>2 0.14665842 <a title="1271-tfidf-2" href="../high_scalability-2007/high_scalability-2007-09-18-Amazon_Architecture.html">96 high scalability-2007-09-18-Amazon Architecture</a></p>
<p>Introduction: This is a wonderfully informative Amazon update based on Joachim Rohde's
discovery of an interview with Amazon's CTO. You'll learn about how Amazon
organizes their teams around services, the CAP theorem of building scalable
systems, how they deploy software, and a lot more. Many new additions from the
ACM Queue article have also been included.Amazon grew from a tiny online
bookstore to one of the largest stores on earth. They did it while pioneering
new and interesting ways to rate, review, and recommend products. Greg Linden
shared is version of Amazon's birth pangs in a series of blog articlesSite:
http://amazon.comInformation SourcesEarly Amazon by Greg LindenHow Linux saved
Amazon millionsInterview Werner Vogels- Amazon's CTOAsynchronous Architectures
- a nicesummaryof Werner Vogels' talk by Chris LoosleyLearning from the Amazon
technology platform- A Conversation with Werner VogelsWerner Vogels' Weblog-
building scalable and robust distributed
systemsPlatformLinuxOracleC++PerlMaso</p><p>3 0.13762547 <a title="1271-tfidf-3" href="../high_scalability-2011/high_scalability-2011-03-08-Medialets_Architecture_-__Defeating_the_Daunting_Mobile_Device_Data_Deluge.html">1000 high scalability-2011-03-08-Medialets Architecture -  Defeating the Daunting Mobile Device Data Deluge</a></p>
<p>Introduction: Mobile developers have a huge scaling problem ahead: doing something useful
with massive continuous streams of telemetry data from millions and millions
of devices. This is a really good problem to have. It means smartphone sales
are finally fulfilling their destiny: slaughtering PCsin the sales arena. And
it also means mobile devices aren't just containers for simple standalone apps
anymore, they are becoming the dominant interface to giant backend
systems.While developers are now rocking mobile development on the client
side, their next challenge is how to code those tricky backend bits. A company
facing those same exact problems right now isMedialets, a mobile rich media ad
platform. What they do is help publishers create high quality interactive ads,
though for our purposes their ad stuff isn't that interesting. What I did find
really interesting about their system is how they are tackling the problem of
defeating the mobile device data deluge.Each day Medialets munches on billions</p><p>4 0.13745371 <a title="1271-tfidf-4" href="../high_scalability-2013/high_scalability-2013-08-28-Sean_Hull%27s_20_Biggest_Bottlenecks_that_Reduce_and_Slow_Down_Scalability.html">1508 high scalability-2013-08-28-Sean Hull's 20 Biggest Bottlenecks that Reduce and Slow Down Scalability</a></p>
<p>Introduction: This article is a lightly edited version of 20 Obstacles to Scalability bySean
Hull (with permission) from the always excellent and thought provoking ACM
Queue.1. TWO-PHASE COMMITNormally when data is changed in a database, it is
written both to memory and to disk. When a commit happens, a relational
database makes a commitment to freeze the data somewhere on real storage
media. Remember, memory doesn't survive a crash or reboot. Even if the data is
cached in memory, the database still has to write it to disk. MySQL binary
logs or Oracle redo logs fit the bill.With a MySQL cluster or distributed file
system such as DRBD (Distributed Replicated Block Device) or Amazon Multi-AZ
(Multi-Availability Zone), a commit occurs not only locally, but also at the
remote end. A two-phase commit means waiting for an acknowledgment from the
far end. Because of network and other latency, those commits can be slowed
down by milliseconds, as though all the cars on a highway were slowed down by
heavy loa</p><p>5 0.13620311 <a title="1271-tfidf-5" href="../high_scalability-2009/high_scalability-2009-12-16-Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_in_the_Ambient_Cloud.html">750 high scalability-2009-12-16-Building Super Scalable Systems: Blade Runner Meets Autonomic Computing in the Ambient Cloud</a></p>
<p>Introduction: "But it is not complicated. [There's] just a lot of it." \--Richard Feynmanon
how the immense variety of the world arises from simple rules.Contents:Have We
Reached the End of Scaling?Applications Become Black Boxes Using Markets to
Scale and Control CostsLet's Welcome our Neo-Feudal OverlordsThe Economic
Argument for the Ambient CloudWhat Will Kill the Cloud?The Amazing Collective
Compute Power of the Ambient CloudUsing the Ambient Cloud as an Application
RuntimeApplications as Virtual StatesConclusionWe have not yet begun to scale.
The world is still fundamentally disconnected and for all our wisdom we are
still in the earliest days of learning how to build truly large planet-scaling
applications.Today 350 million users on Facebook is a lot of users and five
million followers on Twitter is a lot of followers. This may seem like a lot
now, but consider we have no planet wide applications yet. None.Tomorrow the
numbers foreshadow a newCambrian explosionof connectivity that will look as</p><p>6 0.13616201 <a title="1271-tfidf-6" href="../high_scalability-2012/high_scalability-2012-11-05-Gone_Fishin%27%3A_Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_In_The_Ambient_Cloud.html">1355 high scalability-2012-11-05-Gone Fishin': Building Super Scalable Systems: Blade Runner Meets Autonomic Computing In The Ambient Cloud</a></p>
<p>7 0.13217744 <a title="1271-tfidf-7" href="../high_scalability-2011/high_scalability-2011-08-01-Peecho_Architecture_-_scalability_on_a_shoestring.html">1090 high scalability-2011-08-01-Peecho Architecture - scalability on a shoestring</a></p>
<p>8 0.1301786 <a title="1271-tfidf-8" href="../high_scalability-2010/high_scalability-2010-12-06-What_the_heck_are_you_actually_using_NoSQL_for%3F.html">954 high scalability-2010-12-06-What the heck are you actually using NoSQL for?</a></p>
<p>9 0.12830758 <a title="1271-tfidf-9" href="../high_scalability-2012/high_scalability-2012-05-07-Startups_are_Creating_a_New_System_of_the_World_for_IT.html">1240 high scalability-2012-05-07-Startups are Creating a New System of the World for IT</a></p>
<p>10 0.12417393 <a title="1271-tfidf-10" href="../high_scalability-2013/high_scalability-2013-04-15-Scaling_Pinterest_-_From_0_to_10s_of_Billions_of_Page_Views_a_Month_in_Two_Years.html">1440 high scalability-2013-04-15-Scaling Pinterest - From 0 to 10s of Billions of Page Views a Month in Two Years</a></p>
<p>11 0.1236824 <a title="1271-tfidf-11" href="../high_scalability-2013/high_scalability-2013-08-13-In_Memoriam%3A_Lavabit_Architecture_-_Creating_a_Scalable_Email_Service.html">1501 high scalability-2013-08-13-In Memoriam: Lavabit Architecture - Creating a Scalable Email Service</a></p>
<p>12 0.11878244 <a title="1271-tfidf-12" href="../high_scalability-2011/high_scalability-2011-03-22-Facebook%27s_New_Realtime_Analytics_System%3A_HBase_to_Process_20_Billion_Events_Per_Day.html">1008 high scalability-2011-03-22-Facebook's New Realtime Analytics System: HBase to Process 20 Billion Events Per Day</a></p>
<p>13 0.11489639 <a title="1271-tfidf-13" href="../high_scalability-2010/high_scalability-2010-10-15-Troubles_with_Sharding_-_What_can_we_learn_from_the_Foursquare_Incident%3F.html">920 high scalability-2010-10-15-Troubles with Sharding - What can we learn from the Foursquare Incident?</a></p>
<p>14 0.11437011 <a title="1271-tfidf-14" href="../high_scalability-2007/high_scalability-2007-07-30-Build_an_Infinitely_Scalable_Infrastructure_for_%24100_Using_Amazon_Services.html">38 high scalability-2007-07-30-Build an Infinitely Scalable Infrastructure for $100 Using Amazon Services</a></p>
<p>15 0.11364668 <a title="1271-tfidf-15" href="../high_scalability-2008/high_scalability-2008-05-02-Friends_for_Sale_Architecture_-_A_300_Million_Page_View-Month_Facebook_RoR_App.html">313 high scalability-2008-05-02-Friends for Sale Architecture - A 300 Million Page View-Month Facebook RoR App</a></p>
<p>16 0.11280011 <a title="1271-tfidf-16" href="../high_scalability-2008/high_scalability-2008-10-27-Notify.me_Architecture_-_Synchronicity_Kills.html">431 high scalability-2008-10-27-Notify.me Architecture - Synchronicity Kills</a></p>
<p>17 0.1125094 <a title="1271-tfidf-17" href="../high_scalability-2013/high_scalability-2013-03-18-Beyond_Threads_and_Callbacks_-_Application_Architecture_Pros_and_Cons.html">1425 high scalability-2013-03-18-Beyond Threads and Callbacks - Application Architecture Pros and Cons</a></p>
<p>18 0.11221427 <a title="1271-tfidf-18" href="../high_scalability-2008/high_scalability-2008-01-30-How_Rackspace_Now_Uses_MapReduce_and_Hadoop_to_Query_Terabytes_of_Data.html">233 high scalability-2008-01-30-How Rackspace Now Uses MapReduce and Hadoop to Query Terabytes of Data</a></p>
<p>19 0.11166988 <a title="1271-tfidf-19" href="../high_scalability-2011/high_scalability-2011-06-27-TripAdvisor_Architecture_-_40M_Visitors%2C_200M_Dynamic_Page_Views%2C_30TB_Data.html">1068 high scalability-2011-06-27-TripAdvisor Architecture - 40M Visitors, 200M Dynamic Page Views, 30TB Data</a></p>
<p>20 0.11125705 <a title="1271-tfidf-20" href="../high_scalability-2013/high_scalability-2013-12-13-Stuff_The_Internet_Says_On_Scalability_For_December_13th%2C_2013.html">1564 high scalability-2013-12-13-Stuff The Internet Says On Scalability For December 13th, 2013</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.244), (1, 0.103), (2, -0.003), (3, -0.035), (4, 0.027), (5, -0.018), (6, 0.025), (7, 0.029), (8, -0.008), (9, -0.038), (10, -0.016), (11, 0.064), (12, -0.002), (13, -0.016), (14, 0.041), (15, 0.027), (16, -0.023), (17, -0.005), (18, 0.039), (19, -0.025), (20, 0.007), (21, -0.057), (22, 0.021), (23, 0.025), (24, 0.008), (25, -0.021), (26, -0.055), (27, 0.032), (28, 0.024), (29, 0.04), (30, -0.004), (31, 0.004), (32, -0.005), (33, -0.041), (34, -0.003), (35, -0.022), (36, -0.009), (37, 0.007), (38, 0.029), (39, -0.013), (40, -0.018), (41, -0.016), (42, 0.009), (43, 0.045), (44, -0.001), (45, -0.031), (46, -0.016), (47, 0.004), (48, -0.051), (49, -0.015)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.971641 <a title="1271-lsi-1" href="../high_scalability-2012/high_scalability-2012-06-25-StubHub_Architecture%3A_The_Surprising_Complexity_Behind_the_World%E2%80%99s_Largest_Ticket_Marketplace.html">1271 high scalability-2012-06-25-StubHub Architecture: The Surprising Complexity Behind the World’s Largest Ticket Marketplace</a></p>
<p>Introduction: StubHub is an interesting architecture to take a look at because, as market
makers for tickets, they are in a different business than we normally get to
consider.StubHub is surprisingly large, growing at 20% a year, serving 800K
complex pages per hour, selling 5 million tickets per year, and handling 2
million API calls per hour. And the ticket space is surprisingly rich in
complexity. StubHub's traffic is tricky. It's bursty, centering around
unpredictable game outcomes, events, schedules, and seasons. There's a lot of
money involved. There are a lot of different actors involved. There are a lot
of complex business processes involved. And StubHub has several complementary
but very different parts of their business: they have an ad server component
serving ads to sites like ESPN, a rich interactive UI, and a real-time ticket
market component.Most interesting to me is how StubHub is bringing into the
digital realm the once quintessentially high-touch physical world of tickets,
point-of-</p><p>2 0.84513062 <a title="1271-lsi-2" href="../high_scalability-2008/high_scalability-2008-10-27-Notify.me_Architecture_-_Synchronicity_Kills.html">431 high scalability-2008-10-27-Notify.me Architecture - Synchronicity Kills</a></p>
<p>Introduction: What's cool about starting a new project is you finally have a chance to do it
right. You of course eventually mess everything up in your own way, but for
that one moment the world has a perfect order, a rightness that feels
satisfying and good. Arne Claassen, the CTO of notify.me, a brand new real
time notification delivery service, is in this honeymoon period now.Arne has
been gracious enough to share with us his philosophy of how to build a
notification service. I think you'll find it fascinating because Arne goes
into a lot of useful detail about how his system works.His main design
philosophy is to minimize the bottlenecks that form around synchronous access,
that is whensome resource is requested and the requestor ties up more
resources, waiting for a response. If the requested resource can't be
delivered in a timely manner, more and more requests pile up until the server
can't accept any new ones. Nobody gets what they want and you have an outage.
Breaking synchronous operations</p><p>3 0.82863939 <a title="1271-lsi-3" href="../high_scalability-2014/high_scalability-2014-04-18-Stuff_The_Internet_Says_On_Scalability_For_April_18th%2C_2014.html">1634 high scalability-2014-04-18-Stuff The Internet Says On Scalability For April 18th, 2014</a></p>
<p>Introduction: Hey, it's HighScalability time:Scaling to thetop of "Bun Mountain"in Hong
Kong44 trillion gigabytes: size of the digital universe by 2020;6 Times: we
have six times more "stuff" than the generation before us.Quotable
Quotes:Facebook: Our warehouse stores upwards of 300 PB of Hive data, with an
incoming daily rate of about 600 TB.@windley: The problem with the Internet of
Things is right now it's more like the CompuServe of ThingsChip Overclock: If
you want to eventually generate revenue, you must first optimize for developer
productivity; everything else is negotiable.@igrigorik: if you are gzipping
your images.. you're doing it wrong: http://bit.ly/1pb8JhK  \- check your
server configs! and your CDN... :)Seth Lloyd: The arrow of time is an arrow of
increasing correlations.@kitmacgillivray: When will Google enable / require
all android apps to have full deep search integration so all content is
visible to the engine?Neal Ford: Yesterday's best practices become tomorrow's
anti-patterns.</p><p>4 0.82633644 <a title="1271-lsi-4" href="../high_scalability-2014/high_scalability-2014-05-16-Stuff_The_Internet_Says_On_Scalability_For_May_16th%2C_2014.html">1649 high scalability-2014-05-16-Stuff The Internet Says On Scalability For May 16th, 2014</a></p>
<p>Introduction: Hey, it's HighScalability time:Cross Sectionof anUndersea Cable. It's
industrial art. Theparts. Thestory.400,000,000,000: Wayback Machine pages
indexed;100 billion: Google searches per month; 10 million: Snapchat monthly
user growth.Quotable Quotes:@duncanjw: The Great Rewrite - many apps will be
rewritten not just replatformed over next 10 years says @cote
#openstacksummit@RFFlores: The Openstack conundrum. If you don't adopt it, you
will regret it in the future. If you do adopt it, you will regret it
nowelementai: I love Redis so much, it became like a superglue where "just
enough" performance is needed to resolve a bottleneck problem, but you don't
have resources to rewrite a whole thing in something fast.@antirez: "when
software engineering is reduced to plumbing together generic systems, software
engineers lose their sense of ownership"Tom Akehurst: Microservices vs.
monolith is a false dichotomy.@joestump: "Keep in mind that any piece of butt-
based infrastructure can fail at any</p><p>5 0.82382452 <a title="1271-lsi-5" href="../high_scalability-2014/high_scalability-2014-01-06-How_HipChat_Stores_and_Indexes_Billions_of_Messages_Using_ElasticSearch_and_Redis.html">1573 high scalability-2014-01-06-How HipChat Stores and Indexes Billions of Messages Using ElasticSearch and Redis</a></p>
<p>Introduction: This article is from an interview withZuhaib Siddique, a production engineer
atHipChat, makers ofgroup chat and IM for teams.HipChat started in an unusual
space, one you might not think would have much promise, enterprise group
messaging, but as we are learning there is gold in them thereenterprise hills.
Which is why Atlassian, makers of well thought of tools like JIRA and
Confluence,acquired HipChat in 2012.And in a tale not often heard, the
resources and connections of a larger parent have actually helped HipChat
enter anexponential growth cycle. Having reached the 1.2 billion message
storage mark they are now doubling the number of messages sent, stored, and
indexed every few months.That kind of growth puts a lot of pressure on a once
adequate infrastructure. HipChat exhibited a common scaling pattern. Start
simple, experience traffic spikes, and then think what do we do now? Using
bigger computers is usually the first and best answer. And they did that. That
gave them some breathi</p><p>6 0.82100135 <a title="1271-lsi-6" href="../high_scalability-2013/high_scalability-2013-09-13-Stuff_The_Internet_Says_On_Scalability_For_September_13%2C_2013.html">1516 high scalability-2013-09-13-Stuff The Internet Says On Scalability For September 13, 2013</a></p>
<p>7 0.80472088 <a title="1271-lsi-7" href="../high_scalability-2012/high_scalability-2012-06-20-iDoneThis_-_Scaling_an_Email-based_App_from_Scratch.html">1269 high scalability-2012-06-20-iDoneThis - Scaling an Email-based App from Scratch</a></p>
<p>8 0.80228806 <a title="1271-lsi-8" href="../high_scalability-2013/high_scalability-2013-11-04-ESPN%27s_Architecture_at_Scale_-_Operating_at_100%2C000_Duh_Nuh_Nuhs_Per_Second.html">1542 high scalability-2013-11-04-ESPN's Architecture at Scale - Operating at 100,000 Duh Nuh Nuhs Per Second</a></p>
<p>9 0.80069619 <a title="1271-lsi-9" href="../high_scalability-2011/high_scalability-2011-02-08-Mollom_Architecture_-_Killing_Over_373_Million_Spams_at_100_Requests_Per_Second.html">985 high scalability-2011-02-08-Mollom Architecture - Killing Over 373 Million Spams at 100 Requests Per Second</a></p>
<p>10 0.80057448 <a title="1271-lsi-10" href="../high_scalability-2010/high_scalability-2010-05-17-7_Lessons_Learned_While_Building_Reddit_to_270_Million_Page_Views_a_Month.html">828 high scalability-2010-05-17-7 Lessons Learned While Building Reddit to 270 Million Page Views a Month</a></p>
<p>11 0.79672968 <a title="1271-lsi-11" href="../high_scalability-2014/high_scalability-2014-03-11-Building_a_Social_Music_Service_Using_AWS%2C_Scala%2C_Akka%2C_Play%2C_MongoDB%2C_and_Elasticsearch.html">1609 high scalability-2014-03-11-Building a Social Music Service Using AWS, Scala, Akka, Play, MongoDB, and Elasticsearch</a></p>
<p>12 0.79549283 <a title="1271-lsi-12" href="../high_scalability-2013/high_scalability-2013-11-22-Stuff_The_Internet_Says_On_Scalability_For_November_22th%2C_2013.html">1552 high scalability-2013-11-22-Stuff The Internet Says On Scalability For November 22th, 2013</a></p>
<p>13 0.79545885 <a title="1271-lsi-13" href="../high_scalability-2011/high_scalability-2011-06-27-TripAdvisor_Architecture_-_40M_Visitors%2C_200M_Dynamic_Page_Views%2C_30TB_Data.html">1068 high scalability-2011-06-27-TripAdvisor Architecture - 40M Visitors, 200M Dynamic Page Views, 30TB Data</a></p>
<p>14 0.79403323 <a title="1271-lsi-14" href="../high_scalability-2013/high_scalability-2013-06-18-Scaling_Mailbox_-_From_0_to_One_Million_Users_in_6_Weeks_and_100_Million_Messages_Per_Day.html">1477 high scalability-2013-06-18-Scaling Mailbox - From 0 to One Million Users in 6 Weeks and 100 Million Messages Per Day</a></p>
<p>15 0.79318494 <a title="1271-lsi-15" href="../high_scalability-2008/high_scalability-2008-07-15-ZooKeeper_-_A_Reliable%2C_Scalable_Distributed_Coordination_System_.html">350 high scalability-2008-07-15-ZooKeeper - A Reliable, Scalable Distributed Coordination System </a></p>
<p>16 0.78964597 <a title="1271-lsi-16" href="../high_scalability-2013/high_scalability-2013-12-20-Stuff_The_Internet_Says_On_Scalability_For_December_20th%2C_2013.html">1567 high scalability-2013-12-20-Stuff The Internet Says On Scalability For December 20th, 2013</a></p>
<p>17 0.78819543 <a title="1271-lsi-17" href="../high_scalability-2013/high_scalability-2013-03-29-Stuff_The_Internet_Says_On_Scalability_For_March_29%2C_2013.html">1431 high scalability-2013-03-29-Stuff The Internet Says On Scalability For March 29, 2013</a></p>
<p>18 0.78794694 <a title="1271-lsi-18" href="../high_scalability-2011/high_scalability-2011-09-26-17_Techniques_Used_to_Scale_Turntable.fm_and_Labmeeting_to_Millions_of_Users.html">1124 high scalability-2011-09-26-17 Techniques Used to Scale Turntable.fm and Labmeeting to Millions of Users</a></p>
<p>19 0.78746152 <a title="1271-lsi-19" href="../high_scalability-2014/high_scalability-2014-02-14-Stuff_The_Internet_Says_On_Scalability_For_February_14th%2C_2014.html">1596 high scalability-2014-02-14-Stuff The Internet Says On Scalability For February 14th, 2014</a></p>
<p>20 0.78728426 <a title="1271-lsi-20" href="../high_scalability-2012/high_scalability-2012-05-04-Stuff_The_Internet_Says_On_Scalability_For_May_4%2C_2012.html">1239 high scalability-2012-05-04-Stuff The Internet Says On Scalability For May 4, 2012</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.12), (2, 0.188), (10, 0.053), (30, 0.03), (51, 0.203), (56, 0.011), (61, 0.085), (73, 0.011), (77, 0.019), (79, 0.071), (85, 0.051), (94, 0.048), (96, 0.013)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.95638663 <a title="1271-lda-1" href="../high_scalability-2012/high_scalability-2012-01-04-How_Facebook_Handled_the_New_Year%27s_Eve_Onslaught.html">1168 high scalability-2012-01-04-How Facebook Handled the New Year's Eve Onslaught</a></p>
<p>Introduction: How does Facebook handle the massive New Year's Eve traffic spike? Thanks to
Mike Swift, in Facebook gets ready for New Year's Eve, we get a little insight
as to their method for the madness, nothing really detailed, but still
interesting.Problem SetupFacebook expects tha one billion+ photos will be
shared on New Year's eve.Facebook's 800 million users are scattered around the
world. Three quarters live outside the US. Each user is linked to an average
of 130 friends.Photos and posts must appear in less than a second. Opening a
homepage requires executing requests on a 100 different servers, and those
requests have to be ranked, sorted, and privacy-checked, and then
rendered.Different events put different stresses on different parts of
Facebook. Photo and Video Uploads - Holidays require hundreds of terabytes of
capacity News Feed - News events like big sports events and the death of Steve
Jobs drive user status updatesCoping StrategiesTry to predictthe surge in
traffic. Run checkson h</p><p>2 0.90031558 <a title="1271-lda-2" href="../high_scalability-2007/high_scalability-2007-09-18-Sync_data_on_all_servers.html">98 high scalability-2007-09-18-Sync data on all servers</a></p>
<p>Introduction: I have a few apache servers ( arround 11 atm ) serving a small amount of data
( arround 44 gigs right now ).For some time I have been using rsync to keep
all the content equal on all servers, but the amount of data has been growing,
and rsync takes a few too much time to "compare" all data from source to
destination, and create a lot of I/O.I have been taking a look at MogileFS, it
seems a good and reliable option, but as the fuse module is not finished, we
should have to rewrite all our apps, and its not an option atm.Any ideas?I
just want a "real time, non resource-hungry" solution alternative for rsync.
If I get more features on the way, then they are welcome :)Why I prefer to use
a Distributed File System instead of using NAS + NFS?- I need 2 NAS, if I dont
want a point of failure, and NAS hard is expensive.- Non-shared hardware, all
server has their own local disks.- As files are replicated, I can save a lot
of money, RAID is not a MUST.Thnx in advance for your help and sorry for</p><p>3 0.89913267 <a title="1271-lda-3" href="../high_scalability-2014/high_scalability-2014-04-10-Paper%3A_Scalable_Atomic_Visibility_with_RAMP_Transactions_-_Scale_Linearly_to_100_Servers.html">1629 high scalability-2014-04-10-Paper: Scalable Atomic Visibility with RAMP Transactions - Scale Linearly to 100 Servers</a></p>
<p>Introduction: We are not yet at the End of History for database theory asPeter Bailisand
theDatabase Groupat UC Berkeley continue to prove with a greatcompanion blog
post to their new paper: Scalable Atomic Visibility with RAMP Transactions. I
like the approach of pairing a blog post with a paper. A paper almost by
definition is formal, but a blog post can help put a paper in context and give
it some heart.From the abstract:Databases can provide scalability by
partitioning data across several servers. However, multi-partition, multi-
operation transactional access is often expensive, employing coordination-
intensive locking, validation, or scheduling mechanisms. Accordingly, many
real-world systems avoid mechanisms that provide useful semantics formulti-
partition operations. This leads to incorrect behavior for a large class of
applications including secondary indexing,foreign key enforcement, and
materialized view maintenance. In this work, we identify a new isolation model
--Read Atomic (RA) iso</p><p>4 0.89406979 <a title="1271-lda-4" href="../high_scalability-2011/high_scalability-2011-10-28-Stuff_The_Internet_Says_On_Scalability_For_October_28%2C_2011.html">1134 high scalability-2011-10-28-Stuff The Internet Says On Scalability For October 28, 2011</a></p>
<p>Introduction: You deserve a HighScalability today:S3: 566 Billion Objects, 370K
requests/sec;Titan: 38,400-processor, 20-petaflop1,000,000 daily users and no
cache. Wooga flash game with 50K DB updates/second, Ruby backend. They hit an
IO wall with MySQL at 1000 DB updates/sec. They needed more so they went with
Redis. Not quite honest to say no cache was used as everything is RAM, but
maybe that's the point. Use a lot of automation. Inactive users are archived.
Moved away from EBS. Making dynamic sitesscale like static sites by Wim
Godden. Use Varnish, Nginx, and memcached. The Lifecycle of a Web Page on
StumbleUpon infographic. 2.2 mllion web pages are added to StumbleUpon each
month. Nice discussion ofbounce rate. James Hamilton with an excellent
overview of the Storage Infrastructure Behind Facebook Messages. That's 6B+
messages a day.Scaling Twilio. Twilio has scaled traffic by more 100x over the
past year, and expanded their server infrastructure from a few servers to
100â&euro;˛s running in the clou</p><p>same-blog 5 0.89181793 <a title="1271-lda-5" href="../high_scalability-2012/high_scalability-2012-06-25-StubHub_Architecture%3A_The_Surprising_Complexity_Behind_the_World%E2%80%99s_Largest_Ticket_Marketplace.html">1271 high scalability-2012-06-25-StubHub Architecture: The Surprising Complexity Behind the World’s Largest Ticket Marketplace</a></p>
<p>Introduction: StubHub is an interesting architecture to take a look at because, as market
makers for tickets, they are in a different business than we normally get to
consider.StubHub is surprisingly large, growing at 20% a year, serving 800K
complex pages per hour, selling 5 million tickets per year, and handling 2
million API calls per hour. And the ticket space is surprisingly rich in
complexity. StubHub's traffic is tricky. It's bursty, centering around
unpredictable game outcomes, events, schedules, and seasons. There's a lot of
money involved. There are a lot of different actors involved. There are a lot
of complex business processes involved. And StubHub has several complementary
but very different parts of their business: they have an ad server component
serving ads to sites like ESPN, a rich interactive UI, and a real-time ticket
market component.Most interesting to me is how StubHub is bringing into the
digital realm the once quintessentially high-touch physical world of tickets,
point-of-</p><p>6 0.8828069 <a title="1271-lda-6" href="../high_scalability-2009/high_scalability-2009-11-16-Building_Scalable_Systems_Using_Data_as_a_Composite_Material.html">741 high scalability-2009-11-16-Building Scalable Systems Using Data as a Composite Material</a></p>
<p>7 0.87955862 <a title="1271-lda-7" href="../high_scalability-2009/high_scalability-2009-01-02-Strategy%3A_Understanding_Your_Data_Leads_to_the_Best_Scalability_Solutions.html">481 high scalability-2009-01-02-Strategy: Understanding Your Data Leads to the Best Scalability Solutions</a></p>
<p>8 0.87755734 <a title="1271-lda-8" href="../high_scalability-2010/high_scalability-2010-04-30-Behind_the_scenes_of_an_online_marketplace.html">818 high scalability-2010-04-30-Behind the scenes of an online marketplace</a></p>
<p>9 0.87746286 <a title="1271-lda-9" href="../high_scalability-2014/high_scalability-2014-05-07-Update_on_Disqus%3A_It%27s_Still_About_Realtime%2C_But_Go_Demolishes_Python.html">1644 high scalability-2014-05-07-Update on Disqus: It's Still About Realtime, But Go Demolishes Python</a></p>
<p>10 0.8659398 <a title="1271-lda-10" href="../high_scalability-2007/high_scalability-2007-10-30-Feedblendr_Architecture_-_Using_EC2_to_Scale.html">138 high scalability-2007-10-30-Feedblendr Architecture - Using EC2 to Scale</a></p>
<p>11 0.85279441 <a title="1271-lda-11" href="../high_scalability-2010/high_scalability-2010-12-03-GPU_vs_CPU_Smackdown_%3A_The_Rise_of_Throughput-Oriented_Architectures.html">953 high scalability-2010-12-03-GPU vs CPU Smackdown : The Rise of Throughput-Oriented Architectures</a></p>
<p>12 0.84658074 <a title="1271-lda-12" href="../high_scalability-2009/high_scalability-2009-02-09-Paper%3A_Consensus_Protocols%3A_Two-Phase_Commit__.html">510 high scalability-2009-02-09-Paper: Consensus Protocols: Two-Phase Commit  </a></p>
<p>13 0.83378112 <a title="1271-lda-13" href="../high_scalability-2010/high_scalability-2010-06-08-Sponsored_Post%3A__Jobs%3A_Digg%2C_Huffington_Post_Events%3A__Velocity_Conference%2C_Social_Developer_Summit.html">838 high scalability-2010-06-08-Sponsored Post:  Jobs: Digg, Huffington Post Events:  Velocity Conference, Social Developer Summit</a></p>
<p>14 0.82895905 <a title="1271-lda-14" href="../high_scalability-2014/high_scalability-2014-04-28-How_Disqus_Went_Realtime_with_165K_Messages_Per_Second_and_Less_than_.2_Seconds_Latency.html">1638 high scalability-2014-04-28-How Disqus Went Realtime with 165K Messages Per Second and Less than .2 Seconds Latency</a></p>
<p>15 0.81865847 <a title="1271-lda-15" href="../high_scalability-2009/high_scalability-2009-06-10-Paper%3A_Graph_Databases_and_the_Future_of_Large-Scale_Knowledge_Management.html">626 high scalability-2009-06-10-Paper: Graph Databases and the Future of Large-Scale Knowledge Management</a></p>
<p>16 0.80647987 <a title="1271-lda-16" href="../high_scalability-2007/high_scalability-2007-10-02-Secrets_to_Fotolog%27s_Scaling_Success.html">106 high scalability-2007-10-02-Secrets to Fotolog's Scaling Success</a></p>
<p>17 0.79996645 <a title="1271-lda-17" href="../high_scalability-2008/high_scalability-2008-04-07-Lazy_web_sites_run_faster.html">298 high scalability-2008-04-07-Lazy web sites run faster</a></p>
<p>18 0.79925972 <a title="1271-lda-18" href="../high_scalability-2011/high_scalability-2011-11-25-Stuff_The_Internet_Says_On_Scalability_For_November_25%2C_2011.html">1147 high scalability-2011-11-25-Stuff The Internet Says On Scalability For November 25, 2011</a></p>
<p>19 0.79439747 <a title="1271-lda-19" href="../high_scalability-2008/high_scalability-2008-07-26-Google%27s_Paxos_Made_Live_%E2%80%93_An_Engineering_Perspective.html">357 high scalability-2008-07-26-Google's Paxos Made Live – An Engineering Perspective</a></p>
<p>20 0.79208356 <a title="1271-lda-20" href="../high_scalability-2012/high_scalability-2012-12-14-Stuff_The_Internet_Says_On_Scalability_For_December_14%2C_2012.html">1372 high scalability-2012-12-14-Stuff The Internet Says On Scalability For December 14, 2012</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
