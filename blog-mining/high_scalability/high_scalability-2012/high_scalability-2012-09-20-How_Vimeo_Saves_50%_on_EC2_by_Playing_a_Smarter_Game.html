<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1326 high scalability-2012-09-20-How Vimeo Saves 50% on EC2 by Playing a Smarter Game</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2012" href="../home/high_scalability-2012_home.html">high_scalability-2012</a> <a title="high_scalability-2012-1326" href="#">high_scalability-2012-1326</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1326 high scalability-2012-09-20-How Vimeo Saves 50% on EC2 by Playing a Smarter Game</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2012-1326-html" href="http://highscalability.com//blog/2012/9/20/how-vimeo-saves-50-on-ec2-by-playing-a-smarter-game.html">html</a></p><p>Introduction: Nothing shows how much software architectures have changed than the
intelligent scheduling of computation over differently priced compute
resources. This isn't just a false economy either. Vimeo saves up to 50% on
their video transcoding bill by intelligently playing the spot, reserved, and
on-demand markets. If you are ready for some advanced reindeer games then take
a look at  Vimeo EC2 transcoding where they explain their thinking. Even if
you don't like their rules, it's the strategy that matters. This presentation
was from 2011, so it would be interesting to see if the new reserved instance
market has made a difference in their strategy. Here's Vimeo's approach for
minimizing costs using spot, reserved, and on-demand instances:Never bid more
than threshold. It is currently set to 80% of on-demand price.Not more than 10
open spot requests at any time.Bid 10% more than the average price over last
hourBuy reserve instance capacity to meet non-peak hour loads.Use spots for
low priorit</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('vimeo', 0.478), ('spot', 0.389), ('reserved', 0.288), ('bid', 0.233), ('transcoding', 0.233), ('expiry', 0.15), ('reindeer', 0.143), ('articlesbuilding', 0.133), ('jobs', 0.119), ('reserve', 0.117), ('marked', 0.115), ('spots', 0.111), ('retries', 0.111), ('economy', 0.11), ('priced', 0.11), ('runner', 0.108), ('ambient', 0.105), ('autonomic', 0.102), ('blade', 0.1), ('false', 0.098)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="1326-tfidf-1" href="../high_scalability-2012/high_scalability-2012-09-20-How_Vimeo_Saves_50%25_on_EC2_by_Playing_a_Smarter_Game.html">1326 high scalability-2012-09-20-How Vimeo Saves 50% on EC2 by Playing a Smarter Game</a></p>
<p>Introduction: Nothing shows how much software architectures have changed than the
intelligent scheduling of computation over differently priced compute
resources. This isn't just a false economy either. Vimeo saves up to 50% on
their video transcoding bill by intelligently playing the spot, reserved, and
on-demand markets. If you are ready for some advanced reindeer games then take
a look at  Vimeo EC2 transcoding where they explain their thinking. Even if
you don't like their rules, it's the strategy that matters. This presentation
was from 2011, so it would be interesting to see if the new reserved instance
market has made a difference in their strategy. Here's Vimeo's approach for
minimizing costs using spot, reserved, and on-demand instances:Never bid more
than threshold. It is currently set to 80% of on-demand price.Not more than 10
open spot requests at any time.Bid 10% more than the average price over last
hourBuy reserve instance capacity to meet non-peak hour loads.Use spots for
low priorit</p><p>2 0.20488493 <a title="1326-tfidf-2" href="../high_scalability-2012/high_scalability-2012-12-12-Pinterest_Cut_Costs_from_%2454_to_%2420_Per_Hour_by_Automatically_Shutting_Down_Systems.html">1371 high scalability-2012-12-12-Pinterest Cut Costs from $54 to $20 Per Hour by Automatically Shutting Down Systems</a></p>
<p>Introduction: We've long known one of the virtues of the cloud is, through the magic of
services and automation, that systems can be shut or tuned down when not in
use. What may be surprising is how much money can be saved. This aspect of
cloudiness got a lot of pub atAWS re:Invent and is being rebranded under the
term Cost-Aware Architecture. Aninteresting examplewas given by Ryan Park,
Pinterest's technical operations lead:20% of their systems are shutdown after
hours in response to traffic loadsReserved instances are used for standard
traffic On-demand and spot instances are used to handle the elastic load
throughout the day. When more servers are needed for an auto-scaled service,
spot requests are opened and on-demand instances are started at the same time.
Most services are targeted to run at about 50% on-demand and 50% spot.Watchdog
processes continually check what's running. More instances are launched when
needed and terminated when not needed. If spot prices spike and spot instances
are sh</p><p>3 0.17137226 <a title="1326-tfidf-3" href="../high_scalability-2012/high_scalability-2012-10-18-Save_up_to_30%25_by_Selecting_Better_Performing_Amazon_Instances.html">1343 high scalability-2012-10-18-Save up to 30% by Selecting Better Performing Amazon Instances</a></p>
<p>Introduction: If you like the idea of exploiting market inconsistencies to lower your costs
then you will love thispaperand video from the Hot Cloud '12
conference:Exploiting Hardware Heterogeneity within the Same Instance Type of
Amazon EC2.The conclusion is interesting and is a source of good
guidance:Amazon EC2 uses diversified hardware to host the same type of
instance.  The hardware diversity results in performance variation.In general,
the variation between the fast instances and slow  instances can reach 40%. In
some applications, the variation can even approach up to 60%.  By selecting
fast instances within the same instance type,  Amazon EC2 users can acquire up
to 30% of cost saving, if the fast instances have a relatively low
probability.The abstract:Cloud computing providers might start with near-
homogeneous hardware environment. Over time, the homogeneous environment will
most likely evolve into heterogeneous one because of possible upgrades and
replacement of outdated hardware. In tur</p><p>4 0.13085604 <a title="1326-tfidf-4" href="../high_scalability-2010/high_scalability-2010-01-17-Applications_Become_Black_Boxes_Using_Markets_to_Scale_and_Control_Costs.html">761 high scalability-2010-01-17-Applications Become Black Boxes Using Markets to Scale and Control Costs</a></p>
<p>Introduction: This is an excerpt from my articleBuilding Super Scalable Systems: Blade
Runner Meets Autonomic Computing in the Ambient Cloud.We tend to think compute
of resources as residing primarily in datacenters. Given the fast pace of
innovation we will likely see compute resources become pervasive. Some will
reside in datacenters, but compute resources can be anywhere, not just in the
datacenter, we'll actually see the bulk of compute resources live outside of
datacenters in the future.Given the diversity of compute resources it's
reasonable to assume they won't be homogeneous or conform to a standard API.
They will specialize by service. Programmers will have to use those
specialized service interfaces to build applications that are adaptive enough
to take advantage of whatever leverage they can find, whenever and wherever
they can find it. Once found the application will have to reorganize on the
fly to use whatever new resources it has found and let go of whatever
resources it doesn't have</p><p>5 0.12794337 <a title="1326-tfidf-5" href="../high_scalability-2009/high_scalability-2009-12-16-Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_in_the_Ambient_Cloud.html">750 high scalability-2009-12-16-Building Super Scalable Systems: Blade Runner Meets Autonomic Computing in the Ambient Cloud</a></p>
<p>Introduction: "But it is not complicated. [There's] just a lot of it." \--Richard Feynmanon
how the immense variety of the world arises from simple rules.Contents:Have We
Reached the End of Scaling?Applications Become Black Boxes Using Markets to
Scale and Control CostsLet's Welcome our Neo-Feudal OverlordsThe Economic
Argument for the Ambient CloudWhat Will Kill the Cloud?The Amazing Collective
Compute Power of the Ambient CloudUsing the Ambient Cloud as an Application
RuntimeApplications as Virtual StatesConclusionWe have not yet begun to scale.
The world is still fundamentally disconnected and for all our wisdom we are
still in the earliest days of learning how to build truly large planet-scaling
applications.Today 350 million users on Facebook is a lot of users and five
million followers on Twitter is a lot of followers. This may seem like a lot
now, but consider we have no planet wide applications yet. None.Tomorrow the
numbers foreshadow a newCambrian explosionof connectivity that will look as</p><p>6 0.12784846 <a title="1326-tfidf-6" href="../high_scalability-2012/high_scalability-2012-11-05-Gone_Fishin%27%3A_Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_In_The_Ambient_Cloud.html">1355 high scalability-2012-11-05-Gone Fishin': Building Super Scalable Systems: Blade Runner Meets Autonomic Computing In The Ambient Cloud</a></p>
<p>7 0.12310079 <a title="1326-tfidf-7" href="../high_scalability-2012/high_scalability-2012-11-01-Cost_Analysis%3A_TripAdvisor_and_Pinterest_costs_on_the_AWS_cloud.html">1353 high scalability-2012-11-01-Cost Analysis: TripAdvisor and Pinterest costs on the AWS cloud</a></p>
<p>8 0.11652779 <a title="1326-tfidf-8" href="../high_scalability-2013/high_scalability-2013-06-26-Leveraging_Cloud_Computing_at_Yelp_-_102_Million_Monthly_Vistors_and_39_Million_Reviews.html">1482 high scalability-2013-06-26-Leveraging Cloud Computing at Yelp - 102 Million Monthly Vistors and 39 Million Reviews</a></p>
<p>9 0.11494219 <a title="1326-tfidf-9" href="../high_scalability-2011/high_scalability-2011-11-18-Stuff_The_Internet_Says_On_Scalability_For_November_18%2C_2011.html">1145 high scalability-2011-11-18-Stuff The Internet Says On Scalability For November 18, 2011</a></p>
<p>10 0.10831867 <a title="1326-tfidf-10" href="../high_scalability-2010/high_scalability-2010-03-02-Using_the_Ambient_Cloud_as_an_Application_Runtime.html">786 high scalability-2010-03-02-Using the Ambient Cloud as an Application Runtime</a></p>
<p>11 0.096040413 <a title="1326-tfidf-11" href="../high_scalability-2012/high_scalability-2012-05-11-Stuff_The_Internet_Says_On_Scalability_For_May_11%2C_2012.html">1244 high scalability-2012-05-11-Stuff The Internet Says On Scalability For May 11, 2012</a></p>
<p>12 0.095421657 <a title="1326-tfidf-12" href="../high_scalability-2010/high_scalability-2010-02-01-What_Will_Kill_the_Cloud%3F.html">768 high scalability-2010-02-01-What Will Kill the Cloud?</a></p>
<p>13 0.094742753 <a title="1326-tfidf-13" href="../high_scalability-2010/high_scalability-2010-02-15-The_Amazing_Collective_Compute_Power_of_the_Ambient_Cloud.html">778 high scalability-2010-02-15-The Amazing Collective Compute Power of the Ambient Cloud</a></p>
<p>14 0.085270241 <a title="1326-tfidf-14" href="../high_scalability-2012/high_scalability-2012-10-02-An_Epic_TripAdvisor_Update%3A_Why_Not_Run_on_the_Cloud%3F_The_Grand_Experiment..html">1331 high scalability-2012-10-02-An Epic TripAdvisor Update: Why Not Run on the Cloud? The Grand Experiment.</a></p>
<p>15 0.08395721 <a title="1326-tfidf-15" href="../high_scalability-2007/high_scalability-2007-07-30-Build_an_Infinitely_Scalable_Infrastructure_for_%24100_Using_Amazon_Services.html">38 high scalability-2007-07-30-Build an Infinitely Scalable Infrastructure for $100 Using Amazon Services</a></p>
<p>16 0.082466349 <a title="1326-tfidf-16" href="../high_scalability-2013/high_scalability-2013-06-24-Update_on_How_29_Cloud_Price_Drops_Changed_the_Bottom_Line_of_TripAdvisor_and_Pinterest_-_Results_Mixed.html">1480 high scalability-2013-06-24-Update on How 29 Cloud Price Drops Changed the Bottom Line of TripAdvisor and Pinterest - Results Mixed</a></p>
<p>17 0.081644818 <a title="1326-tfidf-17" href="../high_scalability-2011/high_scalability-2011-10-27-Strategy%3A_Survive_a_Comet_Strike_in_the_East_With_Reserved_Instances_in_the_West.html">1133 high scalability-2011-10-27-Strategy: Survive a Comet Strike in the East With Reserved Instances in the West</a></p>
<p>18 0.078835987 <a title="1326-tfidf-18" href="../high_scalability-2011/high_scalability-2011-12-28-Strategy%3A_Guaranteed_Availability_Requires_Reserving_Instances_in_Specific_Zones.html">1165 high scalability-2011-12-28-Strategy: Guaranteed Availability Requires Reserving Instances in Specific Zones</a></p>
<p>19 0.077240445 <a title="1326-tfidf-19" href="../high_scalability-2011/high_scalability-2011-08-05-Stuff_The_Internet_Says_On_Scalability_For_August_5%2C_2011.html">1093 high scalability-2011-08-05-Stuff The Internet Says On Scalability For August 5, 2011</a></p>
<p>20 0.077225737 <a title="1326-tfidf-20" href="../high_scalability-2010/high_scalability-2010-03-16-Justin.tv%27s_Live_Video_Broadcasting_Architecture.html">796 high scalability-2010-03-16-Justin.tv's Live Video Broadcasting Architecture</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.094), (1, 0.046), (2, 0.017), (3, 0.057), (4, -0.057), (5, -0.07), (6, 0.019), (7, -0.004), (8, 0.005), (9, -0.06), (10, 0.006), (11, -0.031), (12, 0.051), (13, 0.006), (14, -0.002), (15, 0.01), (16, -0.048), (17, 0.028), (18, 0.004), (19, 0.033), (20, -0.059), (21, 0.002), (22, -0.003), (23, -0.015), (24, -0.014), (25, -0.024), (26, 0.006), (27, 0.028), (28, 0.029), (29, -0.058), (30, 0.028), (31, 0.064), (32, 0.001), (33, 0.044), (34, -0.015), (35, -0.082), (36, -0.057), (37, -0.063), (38, -0.035), (39, -0.024), (40, 0.05), (41, -0.008), (42, 0.04), (43, 0.007), (44, 0.019), (45, -0.035), (46, -0.025), (47, -0.027), (48, -0.037), (49, 0.029)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95493013 <a title="1326-lsi-1" href="../high_scalability-2012/high_scalability-2012-09-20-How_Vimeo_Saves_50%25_on_EC2_by_Playing_a_Smarter_Game.html">1326 high scalability-2012-09-20-How Vimeo Saves 50% on EC2 by Playing a Smarter Game</a></p>
<p>Introduction: Nothing shows how much software architectures have changed than the
intelligent scheduling of computation over differently priced compute
resources. This isn't just a false economy either. Vimeo saves up to 50% on
their video transcoding bill by intelligently playing the spot, reserved, and
on-demand markets. If you are ready for some advanced reindeer games then take
a look at  Vimeo EC2 transcoding where they explain their thinking. Even if
you don't like their rules, it's the strategy that matters. This presentation
was from 2011, so it would be interesting to see if the new reserved instance
market has made a difference in their strategy. Here's Vimeo's approach for
minimizing costs using spot, reserved, and on-demand instances:Never bid more
than threshold. It is currently set to 80% of on-demand price.Not more than 10
open spot requests at any time.Bid 10% more than the average price over last
hourBuy reserve instance capacity to meet non-peak hour loads.Use spots for
low priorit</p><p>2 0.74776518 <a title="1326-lsi-2" href="../high_scalability-2012/high_scalability-2012-12-12-Pinterest_Cut_Costs_from_%2454_to_%2420_Per_Hour_by_Automatically_Shutting_Down_Systems.html">1371 high scalability-2012-12-12-Pinterest Cut Costs from $54 to $20 Per Hour by Automatically Shutting Down Systems</a></p>
<p>Introduction: We've long known one of the virtues of the cloud is, through the magic of
services and automation, that systems can be shut or tuned down when not in
use. What may be surprising is how much money can be saved. This aspect of
cloudiness got a lot of pub atAWS re:Invent and is being rebranded under the
term Cost-Aware Architecture. Aninteresting examplewas given by Ryan Park,
Pinterest's technical operations lead:20% of their systems are shutdown after
hours in response to traffic loadsReserved instances are used for standard
traffic On-demand and spot instances are used to handle the elastic load
throughout the day. When more servers are needed for an auto-scaled service,
spot requests are opened and on-demand instances are started at the same time.
Most services are targeted to run at about 50% on-demand and 50% spot.Watchdog
processes continually check what's running. More instances are launched when
needed and terminated when not needed. If spot prices spike and spot instances
are sh</p><p>3 0.72909862 <a title="1326-lsi-3" href="../high_scalability-2012/high_scalability-2012-10-18-Save_up_to_30%25_by_Selecting_Better_Performing_Amazon_Instances.html">1343 high scalability-2012-10-18-Save up to 30% by Selecting Better Performing Amazon Instances</a></p>
<p>Introduction: If you like the idea of exploiting market inconsistencies to lower your costs
then you will love thispaperand video from the Hot Cloud '12
conference:Exploiting Hardware Heterogeneity within the Same Instance Type of
Amazon EC2.The conclusion is interesting and is a source of good
guidance:Amazon EC2 uses diversified hardware to host the same type of
instance.  The hardware diversity results in performance variation.In general,
the variation between the fast instances and slow  instances can reach 40%. In
some applications, the variation can even approach up to 60%.  By selecting
fast instances within the same instance type,  Amazon EC2 users can acquire up
to 30% of cost saving, if the fast instances have a relatively low
probability.The abstract:Cloud computing providers might start with near-
homogeneous hardware environment. Over time, the homogeneous environment will
most likely evolve into heterogeneous one because of possible upgrades and
replacement of outdated hardware. In tur</p><p>4 0.70338446 <a title="1326-lsi-4" href="../high_scalability-2011/high_scalability-2011-12-28-Strategy%3A_Guaranteed_Availability_Requires_Reserving_Instances_in_Specific_Zones.html">1165 high scalability-2011-12-28-Strategy: Guaranteed Availability Requires Reserving Instances in Specific Zones</a></p>
<p>Introduction: When EC2 first started the mental model was of a magic Pez dispenser supplying
an infinite stream of instances in any desired flavor. If you needed an
instance, because of a either a failure or traffic spike, it would be there.
As amazing as EC2 is, this model turned out to be optimistic.  From athread on
the Amazon discussion forum we learn any dispenser has limits:As Availability
Zones grow over time, our ability to continue to expand them can become
constrained. In these scenarios, we will prevent customers from launching in
the constrained zone if they do not yet have existing resources in that zone.
We also might remove the constrained zone entirely from the list of options
for new customers. This means that occasionally, different customers will see
a different number of Availability Zones in a particular Region. Both
approaches aim to help customers avoid accidentally starting to build up their
infrastructure in an Availability Zone where they might have less ability to
expand.T</p><p>5 0.69994593 <a title="1326-lsi-5" href="../high_scalability-2010/high_scalability-2010-04-19-Strategy%3A_Order_Two_Mediums_Instead_of_Two_Smalls_and_the_EC2_Buffet.html">812 high scalability-2010-04-19-Strategy: Order Two Mediums Instead of Two Smalls and the EC2 Buffet</a></p>
<p>Introduction: Vaibhav PuranikinWeb serving in the cloud - our experiences with nginx and
instance sizesdescribes their experience trying to maximum traffic and minimum
their web serving costs on EC2. Initially they tested with two m1.small
instance types and then they the switched to two c1.mediums instance types.
The m1s are the standard instance types and the c1s are the high CPU instance
types. Obviously the mediums have greater capability, but the cost difference
was interesting:In the long term they will save money using the larger
instances and not autoscaling. With the small instances, traffic bursts caused
autoscaling to kick in. New instances were started in response to load. The
instances woud be up for a short period of time and then spin down again. This
constant churn costs a lot of money. Selecting the larger instance sizes,
which are capable of handling the load without autoscaling, turn out to save
money even though they are more expensive. Starting new instances also takes a
few min</p><p>6 0.69442022 <a title="1326-lsi-6" href="../high_scalability-2013/high_scalability-2013-01-02-Why_Pinterest_Uses_the_Cloud_Instead_of_Going_Solo_-_To_Be_Or_Not_To_Be.html">1380 high scalability-2013-01-02-Why Pinterest Uses the Cloud Instead of Going Solo - To Be Or Not To Be</a></p>
<p>7 0.67027807 <a title="1326-lsi-7" href="../high_scalability-2012/high_scalability-2012-11-01-Cost_Analysis%3A_TripAdvisor_and_Pinterest_costs_on_the_AWS_cloud.html">1353 high scalability-2012-11-01-Cost Analysis: TripAdvisor and Pinterest costs on the AWS cloud</a></p>
<p>8 0.6675247 <a title="1326-lsi-8" href="../high_scalability-2012/high_scalability-2012-07-18-Strategy%3A_Kill_Off_Multi-tenant_Instances_with_High_CPU_Stolen_Time.html">1286 high scalability-2012-07-18-Strategy: Kill Off Multi-tenant Instances with High CPU Stolen Time</a></p>
<p>9 0.66092277 <a title="1326-lsi-9" href="../high_scalability-2013/high_scalability-2013-06-24-Update_on_How_29_Cloud_Price_Drops_Changed_the_Bottom_Line_of_TripAdvisor_and_Pinterest_-_Results_Mixed.html">1480 high scalability-2013-06-24-Update on How 29 Cloud Price Drops Changed the Bottom Line of TripAdvisor and Pinterest - Results Mixed</a></p>
<p>10 0.64293796 <a title="1326-lsi-10" href="../high_scalability-2012/high_scalability-2012-05-21-Pinterest_Architecture_Update_-_18_Million_Visitors%2C_10x_Growth%2C12_Employees%2C_410_TB_of_Data.html">1248 high scalability-2012-05-21-Pinterest Architecture Update - 18 Million Visitors, 10x Growth,12 Employees, 410 TB of Data</a></p>
<p>11 0.62908232 <a title="1326-lsi-11" href="../high_scalability-2013/high_scalability-2013-06-26-Leveraging_Cloud_Computing_at_Yelp_-_102_Million_Monthly_Vistors_and_39_Million_Reviews.html">1482 high scalability-2013-06-26-Leveraging Cloud Computing at Yelp - 102 Million Monthly Vistors and 39 Million Reviews</a></p>
<p>12 0.61854815 <a title="1326-lsi-12" href="../high_scalability-2010/high_scalability-2010-01-17-Applications_Become_Black_Boxes_Using_Markets_to_Scale_and_Control_Costs.html">761 high scalability-2010-01-17-Applications Become Black Boxes Using Markets to Scale and Control Costs</a></p>
<p>13 0.59182811 <a title="1326-lsi-13" href="../high_scalability-2013/high_scalability-2013-11-05-10_Things_You_Should_Know_About_AWS.html">1543 high scalability-2013-11-05-10 Things You Should Know About AWS</a></p>
<p>14 0.57966459 <a title="1326-lsi-14" href="../high_scalability-2013/high_scalability-2013-04-29-AWS_v_GCE_Face-off_and_Why_Innovation_Needs_Lower_Cost_Infrastructures.html">1448 high scalability-2013-04-29-AWS v GCE Face-off and Why Innovation Needs Lower Cost Infrastructures</a></p>
<p>15 0.55344212 <a title="1326-lsi-15" href="../high_scalability-2009/high_scalability-2009-12-16-Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_in_the_Ambient_Cloud.html">750 high scalability-2009-12-16-Building Super Scalable Systems: Blade Runner Meets Autonomic Computing in the Ambient Cloud</a></p>
<p>16 0.55323517 <a title="1326-lsi-16" href="../high_scalability-2012/high_scalability-2012-11-05-Gone_Fishin%27%3A_Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_In_The_Ambient_Cloud.html">1355 high scalability-2012-11-05-Gone Fishin': Building Super Scalable Systems: Blade Runner Meets Autonomic Computing In The Ambient Cloud</a></p>
<p>17 0.55223989 <a title="1326-lsi-17" href="../high_scalability-2013/high_scalability-2013-05-06-7_Not_So_Sexy_Tips_for_Saving_Money_On_Amazon.html">1452 high scalability-2013-05-06-7 Not So Sexy Tips for Saving Money On Amazon</a></p>
<p>18 0.54965824 <a title="1326-lsi-18" href="../high_scalability-2013/high_scalability-2013-02-20-Smart_Companies_Fail_Because_they_Do_Everything_Right_-_Staying_Alive_to_Scale.html">1410 high scalability-2013-02-20-Smart Companies Fail Because they Do Everything Right - Staying Alive to Scale</a></p>
<p>19 0.54714429 <a title="1326-lsi-19" href="../high_scalability-2012/high_scalability-2012-10-02-An_Epic_TripAdvisor_Update%3A_Why_Not_Run_on_the_Cloud%3F_The_Grand_Experiment..html">1331 high scalability-2012-10-02-An Epic TripAdvisor Update: Why Not Run on the Cloud? The Grand Experiment.</a></p>
<p>20 0.54612201 <a title="1326-lsi-20" href="../high_scalability-2010/high_scalability-2010-03-09-Applications_as_Virtual_States.html">790 high scalability-2010-03-09-Applications as Virtual States</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.053), (2, 0.158), (10, 0.103), (47, 0.371), (61, 0.035), (79, 0.156)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.86107659 <a title="1326-lda-1" href="../high_scalability-2012/high_scalability-2012-09-20-How_Vimeo_Saves_50%25_on_EC2_by_Playing_a_Smarter_Game.html">1326 high scalability-2012-09-20-How Vimeo Saves 50% on EC2 by Playing a Smarter Game</a></p>
<p>Introduction: Nothing shows how much software architectures have changed than the
intelligent scheduling of computation over differently priced compute
resources. This isn't just a false economy either. Vimeo saves up to 50% on
their video transcoding bill by intelligently playing the spot, reserved, and
on-demand markets. If you are ready for some advanced reindeer games then take
a look at  Vimeo EC2 transcoding where they explain their thinking. Even if
you don't like their rules, it's the strategy that matters. This presentation
was from 2011, so it would be interesting to see if the new reserved instance
market has made a difference in their strategy. Here's Vimeo's approach for
minimizing costs using spot, reserved, and on-demand instances:Never bid more
than threshold. It is currently set to 80% of on-demand price.Not more than 10
open spot requests at any time.Bid 10% more than the average price over last
hourBuy reserve instance capacity to meet non-peak hour loads.Use spots for
low priorit</p><p>2 0.79440838 <a title="1326-lda-2" href="../high_scalability-2007/high_scalability-2007-08-03-Scaling_IMAP_and_POP3.html">57 high scalability-2007-08-03-Scaling IMAP and POP3</a></p>
<p>Introduction: Just thought I'd drop a brief suggestion to anyone building a large mail
system. Our solution for scaling mail pickup was to develop a sharded
architecture whereby accounts are spread across a cluster of servers, each
with imap/pop3 capability. Then we use a cluster of reverse proxies
(Perdition) speaking to the backend imap/pop3 servers . The benefit of this
approach is you can use simply use round-robin or HA loadbalancing on the
perdition servers that end users connect to (e.g. admins can easily move
accounts around on the backend storage servers without affecting end users).
Perdition manages routing users to the appropriate backend servers and has
MySQL support. What we also liked about this approach was that it had no
dependency on a distributed or networked filesystem, so less chance of
corruption or data consistency issues. When an individual server reaches
capacity, we just off load users to a less used server. If any server goes
offline, it only affects the fraction of users</p><p>3 0.7696436 <a title="1326-lda-3" href="../high_scalability-2007/high_scalability-2007-09-06-Scaling_IMAP_and_POP3.html">81 high scalability-2007-09-06-Scaling IMAP and POP3</a></p>
<p>Introduction: Another scalability strategy brought to you by Erik Osterman:Just thought I'd
drop a brief suggestion to anyone building a large mail system. Our solution
for scaling mail pickup was to develop a sharded architecture whereby accounts
are spread across a cluster of servers, each with imap/pop3 capability. Then
we use a cluster of reverse proxies (Perdition) speaking to the backend
imap/pop3 servers .The benefit of this approach is you can use simply use
round-robin or HA load balancing on the perdition servers that end users
connect to (e.g. admins can easily move accounts around on the backend storage
servers without affecting end users). Perdition manages routing users to the
appropriate backend servers and has MySQL support.What we also liked about
this approach was that it had no dependency on a distributed or networked file
system, so less chance of corruption or data consistency issues. When an
individual server reaches capacity, we just off load users to a less used
server. If an</p><p>4 0.7648685 <a title="1326-lda-4" href="../high_scalability-2010/high_scalability-2010-07-07-Strategy%3A_Recompute_Instead_of_Remember_Big_Data.html">852 high scalability-2010-07-07-Strategy: Recompute Instead of Remember Big Data</a></p>
<p>Introduction: Professor Lance Fortnow, in his blog post Drowning in Data, says complexity
has taught him this lesson:When storage is expensive, it is cheaper to
recompute what you've already computed. And that's the world we now live in:
Storage is pretty cheap but data acquisition and computation are even
cheaper.Jouni, one of the commenters, thinks the opposite is true:storage is
cheap, but computation is expensive. When you are dealing with massive data,
the size of the data set is very often determined by the amount of computing
power available for a certain price.With such data, a linear-time algorithm
takes O(1) seconds to finish, while a quadratic-time algorithm requires O(n)
seconds. But as computing power increases exponentially over time, the
quadratic algorithm gets exponentially slower.For me it's not a matter of
which is true, both positions can be true, but what's interesting is to think
that storage and computation are in some cases fungible. Your architecture can
decide which tradeof</p><p>5 0.76429534 <a title="1326-lda-5" href="../high_scalability-2010/high_scalability-2010-01-13-10_Hot_Scalability_Links_for_January_13%2C_2010.html">760 high scalability-2010-01-13-10 Hot Scalability Links for January 13, 2010</a></p>
<p>Introduction: Has Amazon EC2 become over subscribed?by Alan Williamson. Systemic problems
hit AWS as users experience problems across Amazon's infrastructure. It seems
the strange attractor of a cloud may be the same as for a shared hosting
service.Understanding Infrastructure 2.0by James Urquhart.We need to take a
systems view of our entire infrastructure, and build our automation around the
end-to-end architecture of that system.Hey You, Get Off of My Cloud: Exploring
Information Leakage in Third-Party Compute Clouds.We show that it is possible
to map the internal cloud infrastructure.Hadoop World: Building Data Intensive
Apps with Hadoop and EC2 by Pete Skomoroch.Dives into detail about how he
built TrendingTopics.org using Hadoop and EC2.A Crash Course in Modern
Hardwareby Cliff Click. Yes, your mind will hurt after watching this. And no,
you probably don't know what your microprocessor is doing anymore.EVE
Scalability Explainedby James Harrison.This post aims to demystify EVE's
architecture and</p><p>6 0.73413467 <a title="1326-lda-6" href="../high_scalability-2007/high_scalability-2007-11-21-n-phase_commit_for_FS_writes%2C_reads_stay_local.html">163 high scalability-2007-11-21-n-phase commit for FS writes, reads stay local</a></p>
<p>7 0.73279381 <a title="1326-lda-7" href="../high_scalability-2009/high_scalability-2009-08-08-Yahoo%21%27s_PNUTS_Database%3A_Too_Hot%2C_Too_Cold_or_Just_Right%3F.html">676 high scalability-2009-08-08-Yahoo!'s PNUTS Database: Too Hot, Too Cold or Just Right?</a></p>
<p>8 0.7254768 <a title="1326-lda-8" href="../high_scalability-2009/high_scalability-2009-09-17-Infinispan_narrows_the_gap_between_open_source_and_commercial_data_caches_.html">708 high scalability-2009-09-17-Infinispan narrows the gap between open source and commercial data caches </a></p>
<p>9 0.72119606 <a title="1326-lda-9" href="../high_scalability-2011/high_scalability-2011-12-30-Stuff_The_Internet_Says_On_Scalability_For_December_30%2C_2011.html">1166 high scalability-2011-12-30-Stuff The Internet Says On Scalability For December 30, 2011</a></p>
<p>10 0.7013154 <a title="1326-lda-10" href="../high_scalability-2007/high_scalability-2007-09-17-Blog%3A_Adding_Simplicity_by_Dan_Pritchett.html">94 high scalability-2007-09-17-Blog: Adding Simplicity by Dan Pritchett</a></p>
<p>11 0.69032568 <a title="1326-lda-11" href="../high_scalability-2011/high_scalability-2011-06-06-NoSQL_Pain%3F_Learn_How_to_Read-write_Scale_Without_a_Complete_Re-write.html">1054 high scalability-2011-06-06-NoSQL Pain? Learn How to Read-write Scale Without a Complete Re-write</a></p>
<p>12 0.68849742 <a title="1326-lda-12" href="../high_scalability-2007/high_scalability-2007-11-07-What_CDN_would_you_recommend%3F.html">144 high scalability-2007-11-07-What CDN would you recommend?</a></p>
<p>13 0.68489772 <a title="1326-lda-13" href="../high_scalability-2009/high_scalability-2009-12-30-Terrastore_-_Scalable%2C_elastic%2C_consistent_document_store..html">756 high scalability-2009-12-30-Terrastore - Scalable, elastic, consistent document store.</a></p>
<p>14 0.66024816 <a title="1326-lda-14" href="../high_scalability-2009/high_scalability-2009-03-30-Ebay_history_and_architecture.html">550 high scalability-2009-03-30-Ebay history and architecture</a></p>
<p>15 0.6594196 <a title="1326-lda-15" href="../high_scalability-2013/high_scalability-2013-10-11-Stuff_The_Internet_Says_On_Scalability_For_October_11th%2C_2013.html">1530 high scalability-2013-10-11-Stuff The Internet Says On Scalability For October 11th, 2013</a></p>
<p>16 0.64936596 <a title="1326-lda-16" href="../high_scalability-2011/high_scalability-2011-06-15-101_Questions_to_Ask_When_Considering_a_NoSQL_Database.html">1062 high scalability-2011-06-15-101 Questions to Ask When Considering a NoSQL Database</a></p>
<p>17 0.60884321 <a title="1326-lda-17" href="../high_scalability-2009/high_scalability-2009-07-31-NSFW%3A_Hilarious_Fault-Tolerance_Cartoon_.html">667 high scalability-2009-07-31-NSFW: Hilarious Fault-Tolerance Cartoon </a></p>
<p>18 0.59640276 <a title="1326-lda-18" href="../high_scalability-2007/high_scalability-2007-07-24-Product%3A__Hibernate_Shards.html">24 high scalability-2007-07-24-Product:  Hibernate Shards</a></p>
<p>19 0.5958761 <a title="1326-lda-19" href="../high_scalability-2012/high_scalability-2012-07-18-Strategy%3A_Kill_Off_Multi-tenant_Instances_with_High_CPU_Stolen_Time.html">1286 high scalability-2012-07-18-Strategy: Kill Off Multi-tenant Instances with High CPU Stolen Time</a></p>
<p>20 0.59119225 <a title="1326-lda-20" href="../high_scalability-2013/high_scalability-2013-04-15-Scaling_Pinterest_-_From_0_to_10s_of_Billions_of_Page_Views_a_Month_in_Two_Years.html">1440 high scalability-2013-04-15-Scaling Pinterest - From 0 to 10s of Billions of Page Views a Month in Two Years</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
