<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1265 high scalability-2012-06-15-Stuff The Internet Says On Scalability For June 15, 2012</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2012" href="../home/high_scalability-2012_home.html">high_scalability-2012</a> <a title="high_scalability-2012-1265" href="#">high_scalability-2012-1265</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1265 high scalability-2012-06-15-Stuff The Internet Says On Scalability For June 15, 2012</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2012-1265-html" href="http://highscalability.com//blog/2012/6/15/stuff-the-internet-says-on-scalability-for-june-15-2012.html">html</a></p><p>Introduction: It's HighScalability Time:100PB: Facebook HDFS Cluster;One Trillion: Objects
in S3Quotable quotes:@mwinkle: Listening to NASA big data challenges at
‪#hadoopSummit‬, the square kilometer array project will produce 700tb per
second. TB. Per second.@imrantech: #hadoopsummit‬ @twitter - 400M tweets,
80-100TB per day@r39132: At Netflix talk at ‪#hadoopsummit‬ : 2 B hours
streamed in Q4 2011, 75% of the 30M daily movie starts are sourced from
recommendations@nattybnatkins: Run job. Identify bottleneck. Address
bottleneck. Repeat. Sage wisdom from @tlipcon on optimizing MR jobs ‬
‪#HadoopSummit‬@chiradeep:  mainframe cost of operation - $5k per MIP per year
‪#hadoopsummit‬@MCanalytics: #hadoopsummit‬ Yahoo metrics - 140pb on 42k nodes
with 500 users on 360k Hadoop jobs for 100b events/day Holy smokes!@M_Wein:
Domain expertise is the wave of the future: it's more about "Hadoop and
Healthcare" than "Using Bayesian counters with Hadoop"
‪#hadoopsummit‬@JohnM_Haddad: Netflix at ‪#Hadoopsummit‬ l</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hadoopsummit', 0.667), ('hadoop', 0.166), ('spdy', 0.156), ('namenode', 0.142), ('hdfs', 0.124), ('unexpected', 0.115), ('clause', 0.113), ('nginx', 0.1), ('forward', 0.097), ('hash', 0.097), ('caused', 0.096), ('per', 0.085), ('summit', 0.084), ('moves', 0.076), ('thread', 0.074), ('rama', 0.071), ('overviews', 0.071), ('workhorse', 0.071), ('unencrypted', 0.071), ('incidents', 0.071)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="1265-tfidf-1" href="../high_scalability-2012/high_scalability-2012-06-15-Stuff_The_Internet_Says_On_Scalability_For_June_15%2C_2012.html">1265 high scalability-2012-06-15-Stuff The Internet Says On Scalability For June 15, 2012</a></p>
<p>Introduction: It's HighScalability Time:100PB: Facebook HDFS Cluster;One Trillion: Objects
in S3Quotable quotes:@mwinkle: Listening to NASA big data challenges at
‪#hadoopSummit‬, the square kilometer array project will produce 700tb per
second. TB. Per second.@imrantech: #hadoopsummit‬ @twitter - 400M tweets,
80-100TB per day@r39132: At Netflix talk at ‪#hadoopsummit‬ : 2 B hours
streamed in Q4 2011, 75% of the 30M daily movie starts are sourced from
recommendations@nattybnatkins: Run job. Identify bottleneck. Address
bottleneck. Repeat. Sage wisdom from @tlipcon on optimizing MR jobs ‬
‪#HadoopSummit‬@chiradeep:  mainframe cost of operation - $5k per MIP per year
‪#hadoopsummit‬@MCanalytics: #hadoopsummit‬ Yahoo metrics - 140pb on 42k nodes
with 500 users on 360k Hadoop jobs for 100b events/day Holy smokes!@M_Wein:
Domain expertise is the wave of the future: it's more about "Hadoop and
Healthcare" than "Using Bayesian counters with Hadoop"
‪#hadoopsummit‬@JohnM_Haddad: Netflix at ‪#Hadoopsummit‬ l</p><p>2 0.19379076 <a title="1265-tfidf-2" href="../high_scalability-2009/high_scalability-2009-05-17-Product%3A_Hadoop.html">601 high scalability-2009-05-17-Product: Hadoop</a></p>
<p>Introduction: Update 5:Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62
Secondsand has itsgreen cred questionedbecause it took 40 times the number of
machines Greenplum used to do the same work.Update 4:Introduction to Pig. Pig
allows you to skip programming Hadoop at the low map-reduce level. You don't
have to know Java. Using the Pig Latin language, which is a scripting data
flow language, you can think about your problem as a data flow program. 10
lines of Pig Latin = 200 lines of Java.Update 3: Scaling Hadoop to4000 nodes
at Yahoo!. 30,000 cores with nearly 16PB of raw disk; sorted 6TB of data
completed in 37 minutes; 14,000 map tasks writes (reads) 360 MB (about 3
blocks) of data into a single file with a total of 5.04 TB for the whole
job.Update 2: HadoopSummit and Data-Intensive Computing Symposium Videos and
Slides. Topics include: Pig, JAQL, Hbase, Hive, Data-Intensive Scalable
Computing, Clouds and ManyCore: The Revolution, Simplicity and Complexity in
Data Systems at Scale, Han</p><p>3 0.1090711 <a title="1265-tfidf-3" href="../high_scalability-2009/high_scalability-2009-06-11-Yahoo%21_Distribution_of_Hadoop.html">627 high scalability-2009-06-11-Yahoo! Distribution of Hadoop</a></p>
<p>Introduction: Many people in the Apache Hadoop community have asked Yahoo! to publish the
version of Apache Hadoop they test and deploy across their large Hadoop
clusters. As a service to the Hadoop community, Yahoo is releasing the Yahoo!
Distribution of Hadoop -- a source code distribution that is based entirely on
code found in the Apache Hadoop project.This source distribution includes code
patches that they have added to improve the stability and performance of their
clusters. In all cases, these patches have already been contributed back to
Apache, but they may not yet be available in an Apache release of Hadoop.Read
more and get the Hadoop distribution from Yahoo</p><p>4 0.10733782 <a title="1265-tfidf-4" href="../high_scalability-2014/high_scalability-2014-04-04-Stuff_The_Internet_Says_On_Scalability_For_April_4th%2C_2014.html">1626 high scalability-2014-04-04-Stuff The Internet Says On Scalability For April 4th, 2014</a></p>
<p>Introduction: Hey, it's HighScalability time:The world ends not with a bang, but with1
exaFLOP of bitcoinwhimpers.Quotable Quotes:@EtienneRoy: Algorithm:  you must
encode and leverage your ignorance, not only your knowledge #hadoopsummit -
enthrallingChris Brenny: A material is nothing without a process. While the
constituent formulation imbues the final product with fundamental properties,
the bridge between material and function has a dramatic effect on its
perception and use.@gallifreyan: Using AWS c1, m1, m2? @adrianco says don't.
c3, m3, r3 are now better and cheaper. #cloudconnect #ccevent@christianhern:
Mobile banking in the UK: 1,800 transactions per MINUTE. A "seismic shift"
that banks were unprepared forWhile we are waiting for that epic article
deeply comparing Google's Cloud with AWS, we have Adrian Cockcroft's highly
hopped slide comparing the two. Google: no enterprise customers, no
reservation options, need more regions and zones, need lower inter-zone
latency, no SSD options. AWS: no</p><p>5 0.10540948 <a title="1265-tfidf-5" href="../high_scalability-2011/high_scalability-2011-01-04-Map-Reduce_With_Ruby_Using_Hadoop.html">968 high scalability-2011-01-04-Map-Reduce With Ruby Using Hadoop</a></p>
<p>Introduction: A demonstration, with repeatable steps, of how to quickly fire-up a Hadoop
cluster on Amazon EC2, load data onto the HDFS (Hadoop Distributed File-
System), write map-reduce scripts in Ruby and use them to run a map-reduce job
on your Hadoop cluster. You willnotneed to ssh into the cluster, as all tasks
are run from your local machine. Below I am using my MacBook Pro as my local
machine, but the steps I have provided should be reproducible on other
platforms running bash and Java.Fire-Up Your Hadoop ClusterI choose
theCloudera distribution of Hadoopwhich is still 100% Apache licensed, but has
some additional benefits. One of these benefits is that it is released byDoug
Cutting, who started Hadoop and drove it’s development at Yahoo! He also
startedLucene, which is another of my favourite Apache Projects, so I have
good faith that he knows what he is doing. Another benefit, as you will see,
is that it is simple to fire-up a Hadoop cluster.I am going to use
Cloudera’sWhirr script, which</p><p>6 0.10508361 <a title="1265-tfidf-6" href="../high_scalability-2012/high_scalability-2012-08-03-Stuff_The_Internet_Says_On_Scalability_For_August_3%2C_2012.html">1297 high scalability-2012-08-03-Stuff The Internet Says On Scalability For August 3, 2012</a></p>
<p>7 0.096907236 <a title="1265-tfidf-7" href="../high_scalability-2008/high_scalability-2008-05-03-Product%3A_nginx.html">314 high scalability-2008-05-03-Product: nginx</a></p>
<p>8 0.096450023 <a title="1265-tfidf-8" href="../high_scalability-2012/high_scalability-2012-10-04-Stuff_The_Internet_Says_On_Scalability_For_October_5%2C_2012.html">1334 high scalability-2012-10-04-Stuff The Internet Says On Scalability For October 5, 2012</a></p>
<p>9 0.095309615 <a title="1265-tfidf-9" href="../high_scalability-2014/high_scalability-2014-03-24-Big%2C_Small%2C_Hot_or_Cold_-_Examples_of_Robust_Data_Pipelines_from_Stripe%2C_Tapad%2C_Etsy_and_Square.html">1618 high scalability-2014-03-24-Big, Small, Hot or Cold - Examples of Robust Data Pipelines from Stripe, Tapad, Etsy and Square</a></p>
<p>10 0.091749333 <a title="1265-tfidf-10" href="../high_scalability-2010/high_scalability-2010-02-19-Twitter%E2%80%99s_Plan_to_Analyze_100_Billion_Tweets.html">780 high scalability-2010-02-19-Twitter’s Plan to Analyze 100 Billion Tweets</a></p>
<p>11 0.091665462 <a title="1265-tfidf-11" href="../high_scalability-2011/high_scalability-2011-09-23-Stuff_The_Internet_Says_On_Scalability_For_September_23%2C_2011.html">1122 high scalability-2011-09-23-Stuff The Internet Says On Scalability For September 23, 2011</a></p>
<p>12 0.09149716 <a title="1265-tfidf-12" href="../high_scalability-2009/high_scalability-2009-07-30-Learn_How_to_Think_at_Scale.html">666 high scalability-2009-07-30-Learn How to Think at Scale</a></p>
<p>13 0.088331208 <a title="1265-tfidf-13" href="../high_scalability-2010/high_scalability-2010-04-12-Poppen.de_Architecture.html">808 high scalability-2010-04-12-Poppen.de Architecture</a></p>
<p>14 0.087945662 <a title="1265-tfidf-14" href="../high_scalability-2010/high_scalability-2010-07-02-Hot_Scalability_Links_for_July_2%2C_2010.html">851 high scalability-2010-07-02-Hot Scalability Links for July 2, 2010</a></p>
<p>15 0.087443165 <a title="1265-tfidf-15" href="../high_scalability-2012/high_scalability-2012-08-28-Making_Hadoop_Run_Faster.html">1313 high scalability-2012-08-28-Making Hadoop Run Faster</a></p>
<p>16 0.087269798 <a title="1265-tfidf-16" href="../high_scalability-2011/high_scalability-2011-07-27-Making_Hadoop_1000x_Faster_for_Graph_Problems.html">1088 high scalability-2011-07-27-Making Hadoop 1000x Faster for Graph Problems</a></p>
<p>17 0.086592421 <a title="1265-tfidf-17" href="../high_scalability-2010/high_scalability-2010-11-16-Facebook%27s_New_Real-time_Messaging_System%3A_HBase_to_Store_135%2B_Billion_Messages_a_Month.html">943 high scalability-2010-11-16-Facebook's New Real-time Messaging System: HBase to Store 135+ Billion Messages a Month</a></p>
<p>18 0.085768729 <a title="1265-tfidf-18" href="../high_scalability-2008/high_scalability-2008-01-30-How_Rackspace_Now_Uses_MapReduce_and_Hadoop_to_Query_Terabytes_of_Data.html">233 high scalability-2008-01-30-How Rackspace Now Uses MapReduce and Hadoop to Query Terabytes of Data</a></p>
<p>19 0.08542411 <a title="1265-tfidf-19" href="../high_scalability-2011/high_scalability-2011-03-08-Medialets_Architecture_-__Defeating_the_Daunting_Mobile_Device_Data_Deluge.html">1000 high scalability-2011-03-08-Medialets Architecture -  Defeating the Daunting Mobile Device Data Deluge</a></p>
<p>20 0.085353553 <a title="1265-tfidf-20" href="../high_scalability-2008/high_scalability-2008-10-15-Hadoop_-_A_Primer.html">414 high scalability-2008-10-15-Hadoop - A Primer</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.156), (1, 0.077), (2, 0.001), (3, 0.027), (4, 0.022), (5, 0.017), (6, 0.039), (7, 0.062), (8, 0.06), (9, 0.053), (10, 0.051), (11, -0.021), (12, 0.093), (13, -0.085), (14, -0.022), (15, -0.012), (16, 0.028), (17, -0.003), (18, -0.047), (19, 0.039), (20, 0.016), (21, 0.04), (22, 0.024), (23, 0.037), (24, -0.008), (25, 0.008), (26, 0.051), (27, 0.007), (28, 0.014), (29, 0.027), (30, 0.039), (31, 0.047), (32, -0.01), (33, 0.025), (34, 0.001), (35, 0.023), (36, -0.029), (37, 0.078), (38, -0.028), (39, -0.02), (40, 0.039), (41, 0.012), (42, 0.008), (43, -0.021), (44, 0.034), (45, 0.018), (46, 0.012), (47, 0.024), (48, -0.028), (49, 0.021)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96029687 <a title="1265-lsi-1" href="../high_scalability-2012/high_scalability-2012-06-15-Stuff_The_Internet_Says_On_Scalability_For_June_15%2C_2012.html">1265 high scalability-2012-06-15-Stuff The Internet Says On Scalability For June 15, 2012</a></p>
<p>Introduction: It's HighScalability Time:100PB: Facebook HDFS Cluster;One Trillion: Objects
in S3Quotable quotes:@mwinkle: Listening to NASA big data challenges at
‪#hadoopSummit‬, the square kilometer array project will produce 700tb per
second. TB. Per second.@imrantech: #hadoopsummit‬ @twitter - 400M tweets,
80-100TB per day@r39132: At Netflix talk at ‪#hadoopsummit‬ : 2 B hours
streamed in Q4 2011, 75% of the 30M daily movie starts are sourced from
recommendations@nattybnatkins: Run job. Identify bottleneck. Address
bottleneck. Repeat. Sage wisdom from @tlipcon on optimizing MR jobs ‬
‪#HadoopSummit‬@chiradeep:  mainframe cost of operation - $5k per MIP per year
‪#hadoopsummit‬@MCanalytics: #hadoopsummit‬ Yahoo metrics - 140pb on 42k nodes
with 500 users on 360k Hadoop jobs for 100b events/day Holy smokes!@M_Wein:
Domain expertise is the wave of the future: it's more about "Hadoop and
Healthcare" than "Using Bayesian counters with Hadoop"
‪#hadoopsummit‬@JohnM_Haddad: Netflix at ‪#Hadoopsummit‬ l</p><p>2 0.80120891 <a title="1265-lsi-2" href="../high_scalability-2011/high_scalability-2011-01-04-Map-Reduce_With_Ruby_Using_Hadoop.html">968 high scalability-2011-01-04-Map-Reduce With Ruby Using Hadoop</a></p>
<p>Introduction: A demonstration, with repeatable steps, of how to quickly fire-up a Hadoop
cluster on Amazon EC2, load data onto the HDFS (Hadoop Distributed File-
System), write map-reduce scripts in Ruby and use them to run a map-reduce job
on your Hadoop cluster. You willnotneed to ssh into the cluster, as all tasks
are run from your local machine. Below I am using my MacBook Pro as my local
machine, but the steps I have provided should be reproducible on other
platforms running bash and Java.Fire-Up Your Hadoop ClusterI choose
theCloudera distribution of Hadoopwhich is still 100% Apache licensed, but has
some additional benefits. One of these benefits is that it is released byDoug
Cutting, who started Hadoop and drove it’s development at Yahoo! He also
startedLucene, which is another of my favourite Apache Projects, so I have
good faith that he knows what he is doing. Another benefit, as you will see,
is that it is simple to fire-up a Hadoop cluster.I am going to use
Cloudera’sWhirr script, which</p><p>3 0.77870047 <a title="1265-lsi-3" href="../high_scalability-2009/high_scalability-2009-05-17-Product%3A_Hadoop.html">601 high scalability-2009-05-17-Product: Hadoop</a></p>
<p>Introduction: Update 5:Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62
Secondsand has itsgreen cred questionedbecause it took 40 times the number of
machines Greenplum used to do the same work.Update 4:Introduction to Pig. Pig
allows you to skip programming Hadoop at the low map-reduce level. You don't
have to know Java. Using the Pig Latin language, which is a scripting data
flow language, you can think about your problem as a data flow program. 10
lines of Pig Latin = 200 lines of Java.Update 3: Scaling Hadoop to4000 nodes
at Yahoo!. 30,000 cores with nearly 16PB of raw disk; sorted 6TB of data
completed in 37 minutes; 14,000 map tasks writes (reads) 360 MB (about 3
blocks) of data into a single file with a total of 5.04 TB for the whole
job.Update 2: HadoopSummit and Data-Intensive Computing Symposium Videos and
Slides. Topics include: Pig, JAQL, Hbase, Hive, Data-Intensive Scalable
Computing, Clouds and ManyCore: The Revolution, Simplicity and Complexity in
Data Systems at Scale, Han</p><p>4 0.77849281 <a title="1265-lsi-4" href="../high_scalability-2013/high_scalability-2013-04-24-Strategy%3A_Using_Lots_of_RAM_Often_Cheaper_than_Using_a_Hadoop_Cluster.html">1445 high scalability-2013-04-24-Strategy: Using Lots of RAM Often Cheaper than Using a Hadoop Cluster</a></p>
<p>Introduction: Solving problems while saving money is always a problem. In Nobody ever got
ďŹ red for using Hadoop on a cluster they give some counter-intuitive advice by
showing a big-memory server may  provide better performance per dollar than a
cluster:For jobs where the input data is multi-terabyte or larger a Hadoop
cluster is the right solution.For smaller problems memory has reached a GB/$
ratio where it is technically and financially feasible to use a single server
with 100s of GB of DRAM rather than a cluster. Given the majority of analytics
jobs do not process huge data sets, a cluster doesn't need to be your first
option. Scaling up RAM saves on programmer time, reduces programmer effort,
improved accuracy, and reduces hardware costs.</p><p>5 0.7741155 <a title="1265-lsi-5" href="../high_scalability-2011/high_scalability-2011-07-08-Stuff_The_Internet_Says_On_Scalability_For_July_8%2C_2011.html">1076 high scalability-2011-07-08-Stuff The Internet Says On Scalability For July 8, 2011</a></p>
<p>Introduction: Submitted for your scaling pleasure: Facebook confirms 750 million users,
sharing 4 billion items daily;Yahoo: 42,000 Hadoop nodes storing 180-200
petabytes; Formspring hits 25 million users.Zynga's Cadir Lee: It's not the
amount of hardware that matters. It's the architecture of the application. You
have to work at making your app architecture so that it takes advantage of
Amazon. You have to have complete fluidity with the storage tier, the web
tier. We are running our own data centers. We are looking more at doing our
own data centers with more of a private cloud.Love the sensing making
described by Hunch's Infographic on their Taste Graph. 500 million people, 200
million items, 30 billion edges. 48 processors. 1 TB RAM. IsMongoDB is the New
MySQL? Stephen O'Grady thinks so using a worse is better argument: wide
adoption by applications, enterprise inroads, simple feature set, and the
number complainers. Who plays PostgreSQL in this movie?Java is the startup
founder getting kicked o</p><p>6 0.76566136 <a title="1265-lsi-6" href="../high_scalability-2010/high_scalability-2010-07-02-Hot_Scalability_Links_for_July_2%2C_2010.html">851 high scalability-2010-07-02-Hot Scalability Links for July 2, 2010</a></p>
<p>7 0.76470488 <a title="1265-lsi-7" href="../high_scalability-2008/high_scalability-2008-11-14-Paper%3A_Pig_Latin%3A_A_Not-So-Foreign_Language_for_Data_Processing.html">443 high scalability-2008-11-14-Paper: Pig Latin: A Not-So-Foreign Language for Data Processing</a></p>
<p>8 0.74374056 <a title="1265-lsi-8" href="../high_scalability-2010/high_scalability-2010-07-20-Strategy%3A_Consider_When_a_Service_Starts_Billing_in_Your_Algorithm_Cost.html">862 high scalability-2010-07-20-Strategy: Consider When a Service Starts Billing in Your Algorithm Cost</a></p>
<p>9 0.74350423 <a title="1265-lsi-9" href="../high_scalability-2012/high_scalability-2012-01-12-Peregrine_-_A_Map_Reduce_Framework_for_Iterative_and_Pipelined_Jobs.html">1173 high scalability-2012-01-12-Peregrine - A Map Reduce Framework for Iterative and Pipelined Jobs</a></p>
<p>10 0.73957628 <a title="1265-lsi-10" href="../high_scalability-2009/high_scalability-2009-06-11-Yahoo%21_Distribution_of_Hadoop.html">627 high scalability-2009-06-11-Yahoo! Distribution of Hadoop</a></p>
<p>11 0.72800678 <a title="1265-lsi-11" href="../high_scalability-2008/high_scalability-2008-09-28-Product%3A_Happy_%3D_Hadoop_%2B_Python.html">397 high scalability-2008-09-28-Product: Happy = Hadoop + Python</a></p>
<p>12 0.71220785 <a title="1265-lsi-12" href="../high_scalability-2012/high_scalability-2012-08-28-Making_Hadoop_Run_Faster.html">1313 high scalability-2012-08-28-Making Hadoop Run Faster</a></p>
<p>13 0.70947933 <a title="1265-lsi-13" href="../high_scalability-2010/high_scalability-2010-02-19-Twitter%E2%80%99s_Plan_to_Analyze_100_Billion_Tweets.html">780 high scalability-2010-02-19-Twitter’s Plan to Analyze 100 Billion Tweets</a></p>
<p>14 0.70767456 <a title="1265-lsi-14" href="../high_scalability-2012/high_scalability-2012-12-21-Stuff_The_Internet_Says_On_Scalability_For_December_21%2C_2012.html">1375 high scalability-2012-12-21-Stuff The Internet Says On Scalability For December 21, 2012</a></p>
<p>15 0.70739126 <a title="1265-lsi-15" href="../high_scalability-2012/high_scalability-2012-01-13-Stuff_The_Internet_Says_On_Scalability_For_January_13%2C_2012.html">1174 high scalability-2012-01-13-Stuff The Internet Says On Scalability For January 13, 2012</a></p>
<p>16 0.6799255 <a title="1265-lsi-16" href="../high_scalability-2009/high_scalability-2009-09-17-Hot_Links_for_2009-9-17_.html">707 high scalability-2009-09-17-Hot Links for 2009-9-17 </a></p>
<p>17 0.6785267 <a title="1265-lsi-17" href="../high_scalability-2008/high_scalability-2008-02-19-Hadoop_Getting_Closer_to_1.0_Release.html">254 high scalability-2008-02-19-Hadoop Getting Closer to 1.0 Release</a></p>
<p>18 0.67070538 <a title="1265-lsi-18" href="../high_scalability-2009/high_scalability-2009-08-03-Building_a_Data_Intensive_Web_Application_with_Cloudera%2C_Hadoop%2C_Hive%2C_Pig%2C_and_EC2.html">669 high scalability-2009-08-03-Building a Data Intensive Web Application with Cloudera, Hadoop, Hive, Pig, and EC2</a></p>
<p>19 0.66779876 <a title="1265-lsi-19" href="../high_scalability-2007/high_scalability-2007-08-03-Running_Hadoop_MapReduce_on_Amazon_EC2_and_Amazon_S3.html">56 high scalability-2007-08-03-Running Hadoop MapReduce on Amazon EC2 and Amazon S3</a></p>
<p>20 0.66502869 <a title="1265-lsi-20" href="../high_scalability-2012/high_scalability-2012-07-27-Stuff_The_Internet_Says_On_Scalability_For_July_27%2C_2012.html">1292 high scalability-2012-07-27-Stuff The Internet Says On Scalability For July 27, 2012</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.125), (2, 0.185), (10, 0.056), (29, 0.155), (30, 0.04), (40, 0.012), (47, 0.015), (61, 0.046), (76, 0.024), (77, 0.028), (79, 0.117), (85, 0.033), (94, 0.07), (99, 0.011)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.91877896 <a title="1265-lda-1" href="../high_scalability-2012/high_scalability-2012-06-15-Stuff_The_Internet_Says_On_Scalability_For_June_15%2C_2012.html">1265 high scalability-2012-06-15-Stuff The Internet Says On Scalability For June 15, 2012</a></p>
<p>Introduction: It's HighScalability Time:100PB: Facebook HDFS Cluster;One Trillion: Objects
in S3Quotable quotes:@mwinkle: Listening to NASA big data challenges at
‪#hadoopSummit‬, the square kilometer array project will produce 700tb per
second. TB. Per second.@imrantech: #hadoopsummit‬ @twitter - 400M tweets,
80-100TB per day@r39132: At Netflix talk at ‪#hadoopsummit‬ : 2 B hours
streamed in Q4 2011, 75% of the 30M daily movie starts are sourced from
recommendations@nattybnatkins: Run job. Identify bottleneck. Address
bottleneck. Repeat. Sage wisdom from @tlipcon on optimizing MR jobs ‬
‪#HadoopSummit‬@chiradeep:  mainframe cost of operation - $5k per MIP per year
‪#hadoopsummit‬@MCanalytics: #hadoopsummit‬ Yahoo metrics - 140pb on 42k nodes
with 500 users on 360k Hadoop jobs for 100b events/day Holy smokes!@M_Wein:
Domain expertise is the wave of the future: it's more about "Hadoop and
Healthcare" than "Using Bayesian counters with Hadoop"
‪#hadoopsummit‬@JohnM_Haddad: Netflix at ‪#Hadoopsummit‬ l</p><p>2 0.90440613 <a title="1265-lda-2" href="../high_scalability-2010/high_scalability-2010-06-22-Exploring_the_software_behind_Facebook%2C_the_world%E2%80%99s_largest_site.html">845 high scalability-2010-06-22-Exploring the software behind Facebook, the world’s largest site</a></p>
<p>Introduction: Peter Alguacil at Pingdom wrote a HighScalability worthy article on Facebook's
architecture: Exploring the software behind Facebook, the world's largest
site. It covers the challenges Facebook faces, the software Facebook uses, and
the techniques Facebook uses to keep on scaling. Definitely worth a look.</p><p>3 0.89151347 <a title="1265-lda-3" href="../high_scalability-2008/high_scalability-2008-01-28-Product%3A_ISPMan_Centralized_ISP_Management_System_.html">228 high scalability-2008-01-28-Product: ISPMan Centralized ISP Management System </a></p>
<p>Introduction: FromFRESH Portsand their website:ISPmanis an ISP management software written
in perl, using an LDAPbackend to manage virtual hosts for an ISP. It can be
used to manage,DNS, virtual hosts for apache config, postfix configuration,
cyrusmail boxes, proftpd etc.ISPMan was written as a management tool for the
network at 4unet wherebetween 30 to 50 domains are hosted and the number is
crazily growing.Managing these domains and their users was a little time
consuming,and needed an Administrator who knows linux and these
daemonsfluently. Now the help-desk can easily manage the domains and
users.LDAP data can be easily replicated site wide, and mail box server canbe
scaled from 1 to n as required. An LDAP entry called maildroptells the SMTP
server (postfix) where to deliver the mail. The SMTPservers can be
loadbalanced with one of many load balancingtechniques. The program is written
with scalability and Highavailability in mind.This may not be the right
software for you if you want to run a sm</p><p>4 0.85991055 <a title="1265-lda-4" href="../high_scalability-2007/high_scalability-2007-08-29-Skype_Failed_the_Boot_Scalability_Test%3A_Is_P2P_fundamentally_flawed%3F.html">76 high scalability-2007-08-29-Skype Failed the Boot Scalability Test: Is P2P fundamentally flawed?</a></p>
<p>Introduction: Skype's 220 millions users lost service for a stunning two days. The primary
cause for Skype's nightmare (can you imagine the beeper storm that went off?)
was a massive global roll-out of aWindow's patchtriggering the simultaneous
reboot of millions of machines across the globe. The secondary cause was a bug
in Skype's software that prevented "self-healing" in the face of such attacks.
The flood of log-in requests and a lack of "peer-to-peer resources" melted
their system.breakWho's fault is it? Is Skype to blame? Is Microsoft to blame?
Or is the peer-to-peer model itself fundamentally flawed in some way?Let's be
real, how could Skype possibly test booting 220 million servers over a random
configuration of resources? Answer: they can't. Yes, it's Skype's
responsibility, but they are in a bit of a pickle on this one.The boot
scenario is one of the most basic and one of the most difficult scalability
scenarios to plan for and test. You can't simulate the viciousness of real-
life conditi</p><p>5 0.85923535 <a title="1265-lda-5" href="../high_scalability-2012/high_scalability-2012-08-02-Ask_DuckDuckGo%3A_Is_there_Anything_you_Want_to_Know_About_DDG%3F.html">1295 high scalability-2012-08-02-Ask DuckDuckGo: Is there Anything you Want to Know About DDG?</a></p>
<p>Introduction: Next week I'm going to have the pleasure of interviewingGabriel Weinberg,
founder of rebel search engineDuckDuckGo. Is there anything you would like to
know about DuckDuckGo that I can ask Gabe? Pleasecontact meor comment on this
thread with your deepest desires.</p><p>6 0.85609579 <a title="1265-lda-6" href="../high_scalability-2014/high_scalability-2014-03-14-Stuff_The_Internet_Says_On_Scalability_For_March_14th%2C_2014.html">1612 high scalability-2014-03-14-Stuff The Internet Says On Scalability For March 14th, 2014</a></p>
<p>7 0.85581434 <a title="1265-lda-7" href="../high_scalability-2011/high_scalability-2011-09-23-Stuff_The_Internet_Says_On_Scalability_For_September_23%2C_2011.html">1122 high scalability-2011-09-23-Stuff The Internet Says On Scalability For September 23, 2011</a></p>
<p>8 0.85134131 <a title="1265-lda-8" href="../high_scalability-2009/high_scalability-2009-02-21-Google_AppEngine_-_A_Second_Look.html">517 high scalability-2009-02-21-Google AppEngine - A Second Look</a></p>
<p>9 0.85131156 <a title="1265-lda-9" href="../high_scalability-2010/high_scalability-2010-03-05-Strategy%3A_Planning_for_a_Power_Outage_Google_Style.html">789 high scalability-2010-03-05-Strategy: Planning for a Power Outage Google Style</a></p>
<p>10 0.85048401 <a title="1265-lda-10" href="../high_scalability-2014/high_scalability-2014-02-13-Snabb_Switch_-_Skip_the_OS_and_Get_40_million_Requests_Per_Second_in_Lua.html">1595 high scalability-2014-02-13-Snabb Switch - Skip the OS and Get 40 million Requests Per Second in Lua</a></p>
<p>11 0.85030735 <a title="1265-lda-11" href="../high_scalability-2009/high_scalability-2009-06-19-GemFire_6.0%3A_New_innovations_in_data_management.html">633 high scalability-2009-06-19-GemFire 6.0: New innovations in data management</a></p>
<p>12 0.85010952 <a title="1265-lda-12" href="../high_scalability-2009/high_scalability-2009-10-06-Building_a_Unique_Data_Warehouse.html">716 high scalability-2009-10-06-Building a Unique Data Warehouse</a></p>
<p>13 0.84983778 <a title="1265-lda-13" href="../high_scalability-2013/high_scalability-2013-03-29-Stuff_The_Internet_Says_On_Scalability_For_March_29%2C_2013.html">1431 high scalability-2013-03-29-Stuff The Internet Says On Scalability For March 29, 2013</a></p>
<p>14 0.84943444 <a title="1265-lda-14" href="../high_scalability-2013/high_scalability-2013-09-13-Stuff_The_Internet_Says_On_Scalability_For_September_13%2C_2013.html">1516 high scalability-2013-09-13-Stuff The Internet Says On Scalability For September 13, 2013</a></p>
<p>15 0.8493647 <a title="1265-lda-15" href="../high_scalability-2007/high_scalability-2007-12-28-Amazon%27s_EC2%3A_Pay_as_You_Grow_Could_Cut_Your_Costs_in_Half.html">195 high scalability-2007-12-28-Amazon's EC2: Pay as You Grow Could Cut Your Costs in Half</a></p>
<p>16 0.848755 <a title="1265-lda-16" href="../high_scalability-2012/high_scalability-2012-02-02-The_Data-Scope_Project_-_6PB_storage%2C_500GBytes-sec_sequential_IO%2C_20M_IOPS%2C_130TFlops.html">1186 high scalability-2012-02-02-The Data-Scope Project - 6PB storage, 500GBytes-sec sequential IO, 20M IOPS, 130TFlops</a></p>
<p>17 0.84854895 <a title="1265-lda-17" href="../high_scalability-2014/high_scalability-2014-01-10-Stuff_The_Internet_Says_On_Scalability_For_January_10th%2C_2014.html">1576 high scalability-2014-01-10-Stuff The Internet Says On Scalability For January 10th, 2014</a></p>
<p>18 0.84823096 <a title="1265-lda-18" href="../high_scalability-2013/high_scalability-2013-06-14-Stuff_The_Internet_Says_On_Scalability_For_June_14%2C_2013.html">1476 high scalability-2013-06-14-Stuff The Internet Says On Scalability For June 14, 2013</a></p>
<p>19 0.84808922 <a title="1265-lda-19" href="../high_scalability-2013/high_scalability-2013-01-18-Stuff_The_Internet_Says_On_Scalability_For_January_18%2C_2013.html">1389 high scalability-2013-01-18-Stuff The Internet Says On Scalability For January 18, 2013</a></p>
<p>20 0.8480134 <a title="1265-lda-20" href="../high_scalability-2013/high_scalability-2013-06-21-Stuff_The_Internet_Says_On_Scalability_For_June_21%2C_2013.html">1479 high scalability-2013-06-21-Stuff The Internet Says On Scalability For June 21, 2013</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
