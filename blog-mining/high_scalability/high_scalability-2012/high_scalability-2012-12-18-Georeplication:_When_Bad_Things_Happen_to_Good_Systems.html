<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1374 high scalability-2012-12-18-Georeplication: When Bad Things Happen to Good Systems</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2012" href="../home/high_scalability-2012_home.html">high_scalability-2012</a> <a title="high_scalability-2012-1374" href="#">high_scalability-2012-1374</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1374 high scalability-2012-12-18-Georeplication: When Bad Things Happen to Good Systems</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2012-1374-html" href="http://highscalability.com//blog/2012/12/18/georeplication-when-bad-things-happen-to-good-systems.html">html</a></p><p>Introduction: Georeplication is one of the standard techniques for dealing when bad things--failure and latency--happen to good systems. The problem is always: how do you do that?  Murat Demirbas , Associate Professor at SUNY Buffalo, has a couple of really good posts that can help:  MDCC: Multi-Data Center Consistency  and  Making Geo-Replicated Systems Fast as Possible, Consistent when Necessary . 
 
In  MDCC: Multi-Data Center Consistency  Murat discusses a paper that says synchronous wide-area replication can be feasible. There's a quick and clear explanation of Paxos and various optimizations that is worth the price of admission. We find that strong consistency doesn't have to be lost across a WAN:
  

The good thing about using Paxos over the WAN is you /almost/ get the full CAP  (all three properties: consistency, availability, and partition-freedom). As we discussed earlier (Paxos taught), Paxos is CP, that is, in the presence of a partition, Paxos keeps consistency over availability. But, P</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Murat Demirbas , Associate Professor at SUNY Buffalo, has a couple of really good posts that can help:  MDCC: Multi-Data Center Consistency  and  Making Geo-Replicated Systems Fast as Possible, Consistent when Necessary . [sent-3, score-0.078]
</p><p>2 In  MDCC: Multi-Data Center Consistency  Murat discusses a paper that says synchronous wide-area replication can be feasible. [sent-4, score-0.083]
</p><p>3 There's a quick and clear explanation of Paxos and various optimizations that is worth the price of admission. [sent-5, score-0.07]
</p><p>4 We find that strong consistency doesn't have to be lost across a WAN:     The good thing about using Paxos over the WAN is you /almost/ get the full CAP  (all three properties: consistency, availability, and partition-freedom). [sent-6, score-0.294]
</p><p>5 As we discussed earlier (Paxos taught), Paxos is CP, that is, in the presence of a partition, Paxos keeps consistency over availability. [sent-7, score-0.294]
</p><p>6 But, Paxos can still provide availability if there is a majority partition. [sent-8, score-0.159]
</p><p>7 Now, over a WAN, what are the chances of having a partition that does not leave a majority? [sent-9, score-0.204]
</p><p>8 While it is possible to have a data center partitioned off the Internet due to a calamity, what are the chances of several knocked off at the same time. [sent-11, score-0.447]
</p><p>9 So, availability is also looking good for MDCC protocol using Paxos over WAN. [sent-12, score-0.066]
</p><p>10 In  Making Geo-Replicated Systems Fast as Possible, Consistent when Necessary  Murat describes a paper that tries to hide the price of WAN latency for some classes of operations. [sent-13, score-0.207]
</p><p>11 In particular:     To alleviate this latency versus consistency tension, this paper proposes RedBlue consistency, which enables blue operations to be fast/asynchronous (and eventually consistent) while the remaining red operations are strongly-consistent/synchronous (and slow). [sent-14, score-1.138]
</p><p>12 So a program is partitioned into red and blue operations, which run with different consistency levels. [sent-15, score-0.804]
</p><p>13 While red operations must be executed in the same order at all sites (which make them slow), the order of execution of blue operations can vary from site to site (allowing them to be executed without requiring coordination across sites). [sent-16, score-0.928]
</p><p>14 "In systems where every operation is labeled red, RedBlue consistency is equivalent to serializability; in systems where every operation is labeled blue, RedBlue consistency allows the same set of behaviors as eventual consistency. [sent-17, score-1.242]
</p><p>15 ;  On designing and deploying Internet scale services . [sent-19, score-0.053]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('paxos', 0.388), ('consistency', 0.294), ('wan', 0.285), ('murat', 0.273), ('mdcc', 0.269), ('redblue', 0.269), ('blue', 0.224), ('red', 0.203), ('labeled', 0.161), ('chances', 0.129), ('operations', 0.1), ('consistent', 0.096), ('executed', 0.094), ('majority', 0.093), ('calamity', 0.09), ('knocked', 0.09), ('innards', 0.09), ('ramclouds', 0.084), ('paper', 0.083), ('partitioned', 0.083), ('center', 0.082), ('boon', 0.08), ('posts', 0.078), ('serializability', 0.077), ('seda', 0.077), ('partition', 0.075), ('cp', 0.075), ('internet', 0.074), ('megastore', 0.071), ('price', 0.07), ('behaviors', 0.07), ('holiday', 0.07), ('demirbas', 0.07), ('tension', 0.068), ('associate', 0.067), ('proposes', 0.067), ('professor', 0.067), ('alleviate', 0.067), ('operation', 0.067), ('availability', 0.066), ('scalable', 0.065), ('systems', 0.064), ('possible', 0.063), ('taught', 0.063), ('vary', 0.058), ('slow', 0.055), ('dram', 0.055), ('sites', 0.055), ('hide', 0.054), ('services', 0.053)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="1374-tfidf-1" href="../high_scalability-2012/high_scalability-2012-12-18-Georeplication%3A_When_Bad_Things_Happen_to_Good_Systems.html">1374 high scalability-2012-12-18-Georeplication: When Bad Things Happen to Good Systems</a></p>
<p>Introduction: Georeplication is one of the standard techniques for dealing when bad things--failure and latency--happen to good systems. The problem is always: how do you do that?  Murat Demirbas , Associate Professor at SUNY Buffalo, has a couple of really good posts that can help:  MDCC: Multi-Data Center Consistency  and  Making Geo-Replicated Systems Fast as Possible, Consistent when Necessary . 
 
In  MDCC: Multi-Data Center Consistency  Murat discusses a paper that says synchronous wide-area replication can be feasible. There's a quick and clear explanation of Paxos and various optimizations that is worth the price of admission. We find that strong consistency doesn't have to be lost across a WAN:
  

The good thing about using Paxos over the WAN is you /almost/ get the full CAP  (all three properties: consistency, availability, and partition-freedom). As we discussed earlier (Paxos taught), Paxos is CP, that is, in the presence of a partition, Paxos keeps consistency over availability. But, P</p><p>2 0.22179301 <a title="1374-tfidf-2" href="../high_scalability-2012/high_scalability-2012-05-10-Paper%3A_Paxos_Made_Moderately_Complex.html">1243 high scalability-2012-05-10-Paper: Paxos Made Moderately Complex</a></p>
<p>Introduction: If you are a normal human being and find the  Paxos protocol  confusing, then this paper,  Paxos Made Moderately Complex , is a great find. Robbert van Renesse from Cornell University has written a clear and well written paper with excellent explanations.
 
The Abstract:
  For anybody who has ever tried to implement it, Paxos is by no means a simple protocol, even though it is based on relatively simple invariants. This paper provides imperative pseudo-code for the full Paxos (or Multi-Paxos) protocol without shying away from discussing various implementation details. The initial description avoids optimizations that complicate comprehension. Next we discuss liveness, and list various optimizations that make the protocol practical.   Related Articles   
  Paxos on HighScalability.com</p><p>3 0.20064175 <a title="1374-tfidf-3" href="../high_scalability-2010/high_scalability-2010-08-12-Think_of_Latency_as_a_Pseudo-permanent_Network_Partition.html">879 high scalability-2010-08-12-Think of Latency as a Pseudo-permanent Network Partition</a></p>
<p>Introduction: The title of this post is a quote from Ilya Grigorik's post  Weak Consistency and CAP Implications . Besides the article being excellent, I thought this idea had something to add to the great NoSQL versus RDBMS debate, where  Mike Stonebraker  makes the argument that network partitions are rare so designing eventually consistent systems for such rare occurrence is not worth losing ACID semantics over. Even if network partitions are rare, latency between datacenters is not rare, so the game is still on.
 
The rare-partition argument seems to flow from a centralized-distributed view of systems. Such systems are scale-out in that they grow by adding distributed nodes, but the nodes generally do not cross datacenter boundaries. The assumption is the network is fast enough that distributed operations are roughly homogenous between nodes.
 
In a fully-distributed system the nodes can be dispersed across datacenters, which gives operations a widely variable performance profile. Because everyt</p><p>4 0.20033891 <a title="1374-tfidf-4" href="../high_scalability-2007/high_scalability-2007-10-10-WAN_Accelerate_Your_Way_to_Lightening_Fast_Transfers_Between_Data_Centers.html">119 high scalability-2007-10-10-WAN Accelerate Your Way to Lightening Fast Transfers Between Data Centers</a></p>
<p>Introduction: How do you keep in sync a crescendo of data between data centers over a slow WAN? That's the question   Alberto   posted a few weeks ago. Normally I'm not into all boy bands, but I was frustrated there wasn't a really good answer for his problem. It occurred to me later a WAN accelerator might help turn his slow WAN link into more of a LAN, so the overhead of copying files across the WAN wouldn't be so limiting. Many might not consider a WAN accelerator in this situation, but since my friend Damon Ennis works at the WAN accelerator vendor   Silver Peak  , I thought I would ask him if their product would help. Not surprisingly his answer is yes! Potentially a lot, depending on the nature of your data. Here's a no BS overview of their product:           What is it? - Scalable WAN Accelerator from Silver Peak (http://www.silver-peak.com)   What does it do? - You can send 5x-100x times more data across your expensive, low-bandwidth WAN link.   Why should you care? - Your data centers becom</p><p>5 0.19371779 <a title="1374-tfidf-5" href="../high_scalability-2008/high_scalability-2008-07-26-Google%27s_Paxos_Made_Live_%E2%80%93_An_Engineering_Perspective.html">357 high scalability-2008-07-26-Google's Paxos Made Live – An Engineering Perspective</a></p>
<p>Introduction: This is an unusually well written and  useful paper . It talks in detail about experiences implementing a complex project, something we don't see very often. They shockingly even admit that creating a working implementation of Paxos was more difficult than just translating the pseudo code. Imagine that, programmers aren't merely typists! I particularly like the explanation of the Paxos algorithm and why anyone would care about it, working with disk corruption, using leases to support simultaneous reads, using epoch numbers to indicate a new master election, using snapshots to prevent unbounded logs, using MultiOp to implement database transactions, how they tested the system, and their openness with the various problems they had. A lot to learn here.  From the paper:  We describe our experience building a fault-tolerant data-base using the Paxos consensus algorithm. Despite the existing literature in the field, building such a database proved to be non-trivial. We describe selected alg</p><p>6 0.17155465 <a title="1374-tfidf-6" href="../high_scalability-2009/high_scalability-2009-08-24-How_Google_Serves_Data_from_Multiple_Datacenters.html">687 high scalability-2009-08-24-How Google Serves Data from Multiple Datacenters</a></p>
<p>7 0.16268636 <a title="1374-tfidf-7" href="../high_scalability-2011/high_scalability-2011-11-23-Paper%3A_Don%E2%80%99t_Settle_for_Eventual%3A_Scalable_Causal_Consistency_for_Wide-Area_Storage_with_COPS.html">1146 high scalability-2011-11-23-Paper: Don’t Settle for Eventual: Scalable Causal Consistency for Wide-Area Storage with COPS</a></p>
<p>8 0.15296154 <a title="1374-tfidf-8" href="../high_scalability-2011/high_scalability-2011-12-08-Update_on_Scalable_Causal_Consistency_For_Wide-Area_Storage_With_COPS.html">1153 high scalability-2011-12-08-Update on Scalable Causal Consistency For Wide-Area Storage With COPS</a></p>
<p>9 0.15213448 <a title="1374-tfidf-9" href="../high_scalability-2013/high_scalability-2013-05-01-Myth%3A_Eric_Brewer_on_Why_Banks_are_BASE_Not_ACID_-_Availability_Is_Revenue_.html">1450 high scalability-2013-05-01-Myth: Eric Brewer on Why Banks are BASE Not ACID - Availability Is Revenue </a></p>
<p>10 0.14660767 <a title="1374-tfidf-10" href="../high_scalability-2009/high_scalability-2009-06-30-Hot_New_Trend%3A_Linking_Clouds_Through_Cheap_IP_VPNs_Instead_of_Private_Lines_.html">645 high scalability-2009-06-30-Hot New Trend: Linking Clouds Through Cheap IP VPNs Instead of Private Lines </a></p>
<p>11 0.14283493 <a title="1374-tfidf-11" href="../high_scalability-2009/high_scalability-2009-03-10-Paper%3A_Consensus_Protocols%3A_Paxos___.html">529 high scalability-2009-03-10-Paper: Consensus Protocols: Paxos   </a></p>
<p>12 0.13708454 <a title="1374-tfidf-12" href="../high_scalability-2012/high_scalability-2012-10-22-Spanner_-_It%27s_About_Programmers_Building_Apps_Using_SQL_Semantics_at_NoSQL_Scale.html">1345 high scalability-2012-10-22-Spanner - It's About Programmers Building Apps Using SQL Semantics at NoSQL Scale</a></p>
<p>13 0.11849879 <a title="1374-tfidf-13" href="../high_scalability-2009/high_scalability-2009-09-20-PaxosLease%3A_Diskless_Paxos_for_Leases.html">710 high scalability-2009-09-20-PaxosLease: Diskless Paxos for Leases</a></p>
<p>14 0.11514132 <a title="1374-tfidf-14" href="../high_scalability-2013/high_scalability-2013-10-08-F1_and_Spanner_Holistically_Compared.html">1529 high scalability-2013-10-08-F1 and Spanner Holistically Compared</a></p>
<p>15 0.11314189 <a title="1374-tfidf-15" href="../high_scalability-2013/high_scalability-2013-05-03-Stuff_The_Internet_Says_On_Scalability_For_May_3%2C_2013.html">1451 high scalability-2013-05-03-Stuff The Internet Says On Scalability For May 3, 2013</a></p>
<p>16 0.11288305 <a title="1374-tfidf-16" href="../high_scalability-2011/high_scalability-2011-01-11-Google_Megastore_-_3_Billion_Writes_and_20_Billion_Read_Transactions_Daily.html">972 high scalability-2011-01-11-Google Megastore - 3 Billion Writes and 20 Billion Read Transactions Daily</a></p>
<p>17 0.11210467 <a title="1374-tfidf-17" href="../high_scalability-2012/high_scalability-2012-10-12-Stuff_The_Internet_Says_On_Scalability_For_October_12%2C_2012.html">1339 high scalability-2012-10-12-Stuff The Internet Says On Scalability For October 12, 2012</a></p>
<p>18 0.10352086 <a title="1374-tfidf-18" href="../high_scalability-2011/high_scalability-2011-04-06-Netflix%3A_Run_Consistency_Checkers_All_the_time_to_Fixup_Transactions.html">1017 high scalability-2011-04-06-Netflix: Run Consistency Checkers All the time to Fixup Transactions</a></p>
<p>19 0.10254118 <a title="1374-tfidf-19" href="../high_scalability-2009/high_scalability-2009-02-03-Paper%3A_Optimistic_Replication.html">507 high scalability-2009-02-03-Paper: Optimistic Replication</a></p>
<p>20 0.097268887 <a title="1374-tfidf-20" href="../high_scalability-2010/high_scalability-2010-03-03-Hot_Scalability_Links_for_March_3%2C_2010.html">787 high scalability-2010-03-03-Hot Scalability Links for March 3, 2010</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.118), (1, 0.069), (2, 0.005), (3, 0.074), (4, 0.004), (5, 0.067), (6, -0.002), (7, -0.017), (8, -0.077), (9, -0.016), (10, -0.012), (11, 0.009), (12, -0.108), (13, -0.055), (14, 0.066), (15, 0.067), (16, 0.086), (17, 0.003), (18, 0.011), (19, -0.094), (20, 0.106), (21, 0.093), (22, -0.041), (23, 0.001), (24, -0.151), (25, -0.047), (26, 0.0), (27, 0.006), (28, 0.031), (29, -0.114), (30, 0.031), (31, -0.034), (32, -0.107), (33, 0.026), (34, -0.006), (35, -0.001), (36, 0.003), (37, 0.049), (38, -0.025), (39, 0.033), (40, -0.029), (41, 0.061), (42, 0.005), (43, -0.014), (44, 0.03), (45, -0.001), (46, 0.022), (47, 0.025), (48, -0.085), (49, -0.08)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97715056 <a title="1374-lsi-1" href="../high_scalability-2012/high_scalability-2012-12-18-Georeplication%3A_When_Bad_Things_Happen_to_Good_Systems.html">1374 high scalability-2012-12-18-Georeplication: When Bad Things Happen to Good Systems</a></p>
<p>Introduction: Georeplication is one of the standard techniques for dealing when bad things--failure and latency--happen to good systems. The problem is always: how do you do that?  Murat Demirbas , Associate Professor at SUNY Buffalo, has a couple of really good posts that can help:  MDCC: Multi-Data Center Consistency  and  Making Geo-Replicated Systems Fast as Possible, Consistent when Necessary . 
 
In  MDCC: Multi-Data Center Consistency  Murat discusses a paper that says synchronous wide-area replication can be feasible. There's a quick and clear explanation of Paxos and various optimizations that is worth the price of admission. We find that strong consistency doesn't have to be lost across a WAN:
  

The good thing about using Paxos over the WAN is you /almost/ get the full CAP  (all three properties: consistency, availability, and partition-freedom). As we discussed earlier (Paxos taught), Paxos is CP, that is, in the presence of a partition, Paxos keeps consistency over availability. But, P</p><p>2 0.85310358 <a title="1374-lsi-2" href="../high_scalability-2013/high_scalability-2013-05-01-Myth%3A_Eric_Brewer_on_Why_Banks_are_BASE_Not_ACID_-_Availability_Is_Revenue_.html">1450 high scalability-2013-05-01-Myth: Eric Brewer on Why Banks are BASE Not ACID - Availability Is Revenue </a></p>
<p>Introduction: In   NoSQL: Past, Present, Future    Eric Brewer  has a particularly fine section on explaining the often hard to understand ideas of   BASE   (Basically Available, Soft State, Eventually Consistent),   ACID   (Atomicity, Consistency, Isolation, Durability),   CAP   (Consistency Availability, Partition Tolerance), in terms of a pernicious long standing myth about the sanctity of consistency in banking.
    Myth   : Money is important, so banks   must   use transactions to keep money safe and consistent, right? 
    Reality   : Banking transactions are inconsistent, particularly for ATMs. ATMs are designed to have a normal case behaviour and a partition mode behaviour. In partition mode Availability is chosen over Consistency. 
   Why?   1)  Availability correlates with revenue and consistency generally does not.  2)  Historically there was never an idea of perfect communication so everything was partitioned.
   Your ATM transaction must go through so Availability is more important than</p><p>3 0.84206355 <a title="1374-lsi-3" href="../high_scalability-2011/high_scalability-2011-12-08-Update_on_Scalable_Causal_Consistency_For_Wide-Area_Storage_With_COPS.html">1153 high scalability-2011-12-08-Update on Scalable Causal Consistency For Wide-Area Storage With COPS</a></p>
<p>Introduction: Here are a few updates on the article  Paper: Don’t Settle For Eventual: Scalable Causal Consistency For Wide-Area Storage With COPS  from Mike Freedman and Wyatt Lloyd.
 
 Q: How software architectures could change in response to casual+ consistency? 
 
 A : I don't really think they would much. Somebody would still run a two-tier architecture in their datacenter:  a front-tier of webservers running both (say) PHP and our client library, and a back tier of storage nodes running COPS.  (I'm not sure if it was obvious given the discussion of our "thick" client -- you should think of the COPS client dropping in where a memcache client library does...albeit ours has per-session state.)
 
 
 
 Q: Why not just use vector clocks? 
 
 A : The problem with vector clocks and scalability has always been that the size of vector clocks in O(N), where N is the number of nodes.  So if we want to scale to a datacenter with 10K nodes, each piece of metadata must have size O(10K).  And in fact, vector</p><p>4 0.82276291 <a title="1374-lsi-4" href="../high_scalability-2011/high_scalability-2011-11-23-Paper%3A_Don%E2%80%99t_Settle_for_Eventual%3A_Scalable_Causal_Consistency_for_Wide-Area_Storage_with_COPS.html">1146 high scalability-2011-11-23-Paper: Don’t Settle for Eventual: Scalable Causal Consistency for Wide-Area Storage with COPS</a></p>
<p>Introduction: Teams from  Princeton  and CMU are  working together  to solve one of the most difficult problems in the repertoire: scalable geo-distributed data stores. Major companies like Google and Facebook have been working on multiple datacenter database functionality for some time, but there's still a general lack of available systems that work for complex data scenarios.
 
The ideas in this paper-- Don’t Settle for Eventual: Scalable Causal Consistency for Wide-Area Storage with COPS --are different. It's not another eventually consistent system, or a traditional transaction oriented system, or a replication based system, or a system that punts on the issue. It's something new, a causally consistent system that achieves  ALPS  system properties. Move over CAP, NoSQL, etc, we have another acronym: ALPS - Available (operations always complete successfully), Low-latency (operations complete quickly (single digit milliseconds)), Partition-tolerant (operates with a partition), and Scalable (just a</p><p>5 0.79870981 <a title="1374-lsi-5" href="../high_scalability-2010/high_scalability-2010-08-12-Think_of_Latency_as_a_Pseudo-permanent_Network_Partition.html">879 high scalability-2010-08-12-Think of Latency as a Pseudo-permanent Network Partition</a></p>
<p>Introduction: The title of this post is a quote from Ilya Grigorik's post  Weak Consistency and CAP Implications . Besides the article being excellent, I thought this idea had something to add to the great NoSQL versus RDBMS debate, where  Mike Stonebraker  makes the argument that network partitions are rare so designing eventually consistent systems for such rare occurrence is not worth losing ACID semantics over. Even if network partitions are rare, latency between datacenters is not rare, so the game is still on.
 
The rare-partition argument seems to flow from a centralized-distributed view of systems. Such systems are scale-out in that they grow by adding distributed nodes, but the nodes generally do not cross datacenter boundaries. The assumption is the network is fast enough that distributed operations are roughly homogenous between nodes.
 
In a fully-distributed system the nodes can be dispersed across datacenters, which gives operations a widely variable performance profile. Because everyt</p><p>6 0.7586658 <a title="1374-lsi-6" href="../high_scalability-2007/high_scalability-2007-10-03-Paper%3A_Brewer%27s_Conjecture_and_the_Feasibility_of_Consistent_Available_Partition-Tolerant_Web_Services.html">108 high scalability-2007-10-03-Paper: Brewer's Conjecture and the Feasibility of Consistent Available Partition-Tolerant Web Services</a></p>
<p>7 0.75709134 <a title="1374-lsi-7" href="../high_scalability-2012/high_scalability-2012-06-27-Paper%3A_Logic_and_Lattices_for_Distributed_Programming.html">1273 high scalability-2012-06-27-Paper: Logic and Lattices for Distributed Programming</a></p>
<p>8 0.75161552 <a title="1374-lsi-8" href="../high_scalability-2012/high_scalability-2012-05-10-Paper%3A_Paxos_Made_Moderately_Complex.html">1243 high scalability-2012-05-10-Paper: Paxos Made Moderately Complex</a></p>
<p>9 0.74552357 <a title="1374-lsi-9" href="../high_scalability-2013/high_scalability-2013-05-16-Paper%3A_Warp%3A_Multi-Key_Transactions_for_Key-Value_Stores.html">1459 high scalability-2013-05-16-Paper: Warp: Multi-Key Transactions for Key-Value Stores</a></p>
<p>10 0.73145556 <a title="1374-lsi-10" href="../high_scalability-2010/high_scalability-2010-09-01-Paper%3A_The_Case_for_Determinism_in_Database_Systems__.html">890 high scalability-2010-09-01-Paper: The Case for Determinism in Database Systems  </a></p>
<p>11 0.70914745 <a title="1374-lsi-11" href="../high_scalability-2009/high_scalability-2009-03-10-Paper%3A_Consensus_Protocols%3A_Paxos___.html">529 high scalability-2009-03-10-Paper: Consensus Protocols: Paxos   </a></p>
<p>12 0.70909464 <a title="1374-lsi-12" href="../high_scalability-2009/high_scalability-2009-02-03-Paper%3A_Optimistic_Replication.html">507 high scalability-2009-02-03-Paper: Optimistic Replication</a></p>
<p>13 0.70700979 <a title="1374-lsi-13" href="../high_scalability-2010/high_scalability-2010-12-23-Paper%3A_CRDTs%3A_Consistency_without_concurrency_control.html">963 high scalability-2010-12-23-Paper: CRDTs: Consistency without concurrency control</a></p>
<p>14 0.69093716 <a title="1374-lsi-14" href="../high_scalability-2009/high_scalability-2009-08-08-Yahoo%21%27s_PNUTS_Database%3A_Too_Hot%2C_Too_Cold_or_Just_Right%3F.html">676 high scalability-2009-08-08-Yahoo!'s PNUTS Database: Too Hot, Too Cold or Just Right?</a></p>
<p>15 0.68436551 <a title="1374-lsi-15" href="../high_scalability-2009/high_scalability-2009-08-24-How_Google_Serves_Data_from_Multiple_Datacenters.html">687 high scalability-2009-08-24-How Google Serves Data from Multiple Datacenters</a></p>
<p>16 0.64718908 <a title="1374-lsi-16" href="../high_scalability-2009/high_scalability-2009-06-10-Managing_cross_partition_transactions_in_a_distributed_KV_system.html">625 high scalability-2009-06-10-Managing cross partition transactions in a distributed KV system</a></p>
<p>17 0.63025159 <a title="1374-lsi-17" href="../high_scalability-2014/high_scalability-2014-04-10-Paper%3A_Scalable_Atomic_Visibility_with_RAMP_Transactions_-_Scale_Linearly_to_100_Servers.html">1629 high scalability-2014-04-10-Paper: Scalable Atomic Visibility with RAMP Transactions - Scale Linearly to 100 Servers</a></p>
<p>18 0.62959689 <a title="1374-lsi-18" href="../high_scalability-2013/high_scalability-2013-05-23-Paper%3A_Calvin%3A_Fast_Distributed_Transactions_for_Partitioned_Database_Systems.html">1463 high scalability-2013-05-23-Paper: Calvin: Fast Distributed Transactions for Partitioned Database Systems</a></p>
<p>19 0.61171287 <a title="1374-lsi-19" href="../high_scalability-2010/high_scalability-2010-11-30-NoCAP_%E2%80%93_Part_III_%E2%80%93_GigaSpaces_clustering_explained...html">950 high scalability-2010-11-30-NoCAP – Part III – GigaSpaces clustering explained..</a></p>
<p>20 0.59673887 <a title="1374-lsi-20" href="../high_scalability-2009/high_scalability-2009-02-09-Paper%3A_Consensus_Protocols%3A_Two-Phase_Commit__.html">510 high scalability-2009-02-09-Paper: Consensus Protocols: Two-Phase Commit  </a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.117), (2, 0.206), (10, 0.014), (25, 0.014), (30, 0.011), (32, 0.111), (40, 0.03), (43, 0.021), (47, 0.013), (70, 0.011), (79, 0.134), (85, 0.069), (90, 0.054), (94, 0.098)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.92068136 <a title="1374-lda-1" href="../high_scalability-2012/high_scalability-2012-12-18-Georeplication%3A_When_Bad_Things_Happen_to_Good_Systems.html">1374 high scalability-2012-12-18-Georeplication: When Bad Things Happen to Good Systems</a></p>
<p>Introduction: Georeplication is one of the standard techniques for dealing when bad things--failure and latency--happen to good systems. The problem is always: how do you do that?  Murat Demirbas , Associate Professor at SUNY Buffalo, has a couple of really good posts that can help:  MDCC: Multi-Data Center Consistency  and  Making Geo-Replicated Systems Fast as Possible, Consistent when Necessary . 
 
In  MDCC: Multi-Data Center Consistency  Murat discusses a paper that says synchronous wide-area replication can be feasible. There's a quick and clear explanation of Paxos and various optimizations that is worth the price of admission. We find that strong consistency doesn't have to be lost across a WAN:
  

The good thing about using Paxos over the WAN is you /almost/ get the full CAP  (all three properties: consistency, availability, and partition-freedom). As we discussed earlier (Paxos taught), Paxos is CP, that is, in the presence of a partition, Paxos keeps consistency over availability. But, P</p><p>2 0.90268081 <a title="1374-lda-2" href="../high_scalability-2008/high_scalability-2008-02-18-How_to_deal_with_an_I-O_bottleneck_to_disk%3F.html">251 high scalability-2008-02-18-How to deal with an I-O bottleneck to disk?</a></p>
<p>Introduction: A site I'm working with has an I/O bottleneck.     They're using a static server to deliver all of the pictures/video content/zip downloads ecetera but now that the bandwith out of that server is approaching 50Mbit/second the latency on serving small files has increased to become unacceptable.     I'm curious how other people have dealt with this situation.     Seperating into two different servers would require a significant change to the sites architecutre (because the premise is that all uploads go into one server, all subdirectorie are created in one directory, etc.) and may not really solve the problem.</p><p>3 0.88185066 <a title="1374-lda-3" href="../high_scalability-2013/high_scalability-2013-09-13-Stuff_The_Internet_Says_On_Scalability_For_September_13%2C_2013.html">1516 high scalability-2013-09-13-Stuff The Internet Says On Scalability For September 13, 2013</a></p>
<p>Introduction: Hey, it's HighScalability time (this week is a fall harvest basket overflowing with good nutritious wisdom):
     ( Voyager: 36 yrs & 11B miles has reached interstellar space. So it begins... )   
  170 million : metrics Twitter collects every minute;  350 million : Snapchat daily photo shares 
 Quotable Quotes:                                                  
 
  @blowmage : OH: “Guys, databases don't know how to sort things. That's why NoSQL uses JavaScript.” 
  Nokia insider : I look back and I think Nokia was just a very big company that started to maintain its position more than innovate for new opportunities. 
  Paulo Siqueira : Ignoring scalability is not as bad as it sounds—if you use the proper tools. 
  David Rosenthal : The relationship between diversity and risk is very complex. 
  Jaime Teevan : the exact same result list will seem more relevant to you if it is returned just a fraction of a second faster. 
  @aphyr : I use Redis as a queue #leaveDBalone 
 
 
 
 Hey, I've</p><p>4 0.88148868 <a title="1374-lda-4" href="../high_scalability-2008/high_scalability-2008-08-12-Strategy%3A_Limit_The_New%2C_Not_The_Old.html">363 high scalability-2008-08-12-Strategy: Limit The New, Not The Old</a></p>
<p>Introduction: One of the most popular and effective scalability strategies is to impose limits (  GAE Quotas  ,   Fotolog  ,   Facebook  ) as a means of protecting a website against service destroying   traffic spikes  . Twitter will reportedly   limit the number followers   to 2,000 in order to thwart follow spam. This may also allow Twitter to make some bank by   going freemium   and charging for adding more followers.     Agree or disagree with Twitter's strategies, the more interesting aspect for me is how do you introduce new policies into an already established ecosystem?      One approach is the big bang. Introduce all changes at once and let everyone adjust. If users don't like it they can move on. The hope is, however, most users won't be impacted by the changes and that those who are will understand it's all for the greater good of their beloved service. Casualties are assumed, but the damage will probably be minor.      Now in Twitter's case the people with the most followers tend to be o</p><p>5 0.88031071 <a title="1374-lda-5" href="../high_scalability-2014/high_scalability-2014-03-14-Stuff_The_Internet_Says_On_Scalability_For_March_14th%2C_2014.html">1612 high scalability-2014-03-14-Stuff The Internet Says On Scalability For March 14th, 2014</a></p>
<p>Introduction: Hey, it's HighScalability time:
     LifeExplorer Cells in 3D    
 Quotable Quotes:                                          
 
  The Master Switch : History shows a typical progression of information technologies: from somebody’s hobby to somebody’s industry; from jury-rigged contraption to slick production marvel; from a freely accessible channel to one strictly controlled by a single corporation or cartel—from open to closed system. 
  @adrianco : #qconlondon @russmiles on PaaS "As old as I am, a leaky abstraction would be awful..." 
  @Obdurodon : "Scaling is hard.  Let's make excuses." 
  @TomRoyce : @jeffjarvis the rot is deep... The New Jersey pols just used Tesla to shake down the car dealers. 
  @CompSciFact : "The cheapest, fastest and most reliable components of a computer system are those that aren't there." -- Gordon Bell 
  @glyph : “Eventually consistent” is just another way to say “not consistent right now”. 
  @nutshell : LinkedIn is shutting down access to their APIs</p><p>6 0.87861735 <a title="1374-lda-6" href="../high_scalability-2010/high_scalability-2010-12-28-Netflix%3A_Continually_Test_by_Failing_Servers_with_Chaos_Monkey.html">964 high scalability-2010-12-28-Netflix: Continually Test by Failing Servers with Chaos Monkey</a></p>
<p>7 0.87822139 <a title="1374-lda-7" href="../high_scalability-2014/high_scalability-2014-02-10-13_Simple_Tricks_for_Scaling_Python_and_Django_with_Apache_from_HackerEarth.html">1593 high scalability-2014-02-10-13 Simple Tricks for Scaling Python and Django with Apache from HackerEarth</a></p>
<p>8 0.87666857 <a title="1374-lda-8" href="../high_scalability-2010/high_scalability-2010-03-05-Strategy%3A_Planning_for_a_Power_Outage_Google_Style.html">789 high scalability-2010-03-05-Strategy: Planning for a Power Outage Google Style</a></p>
<p>9 0.87467718 <a title="1374-lda-9" href="../high_scalability-2013/high_scalability-2013-02-11-At_Scale_Even_Little_Wins_Pay_Off_Big_-_Google_and_Facebook_Examples.html">1404 high scalability-2013-02-11-At Scale Even Little Wins Pay Off Big - Google and Facebook Examples</a></p>
<p>10 0.87088645 <a title="1374-lda-10" href="../high_scalability-2010/high_scalability-2010-07-22-How_can_we_spark_the_movement_of_research_out_of_the_Ivory_Tower_and_into_production%3F.html">863 high scalability-2010-07-22-How can we spark the movement of research out of the Ivory Tower and into production?</a></p>
<p>11 0.87086731 <a title="1374-lda-11" href="../high_scalability-2008/high_scalability-2008-04-18-Scaling_Mania_at_MySQL_Conference_2008.html">303 high scalability-2008-04-18-Scaling Mania at MySQL Conference 2008</a></p>
<p>12 0.86881411 <a title="1374-lda-12" href="../high_scalability-2012/high_scalability-2012-01-13-Stuff_The_Internet_Says_On_Scalability_For_January_13%2C_2012.html">1174 high scalability-2012-01-13-Stuff The Internet Says On Scalability For January 13, 2012</a></p>
<p>13 0.86759359 <a title="1374-lda-13" href="../high_scalability-2014/high_scalability-2014-03-12-Paper%3A_Scalable_Eventually_Consistent_Counters_over_Unreliable_Networks.html">1611 high scalability-2014-03-12-Paper: Scalable Eventually Consistent Counters over Unreliable Networks</a></p>
<p>14 0.86671656 <a title="1374-lda-14" href="../high_scalability-2012/high_scalability-2012-09-04-Changing_Architectures%3A_New_Datacenter_Networks_Will_Set_Your_Code_and_Data_Free___.html">1316 high scalability-2012-09-04-Changing Architectures: New Datacenter Networks Will Set Your Code and Data Free   </a></p>
<p>15 0.86658138 <a title="1374-lda-15" href="../high_scalability-2012/high_scalability-2012-05-14-DynamoDB_Talk_Notes_and_the_SSD_Hot_S3_Cold_Pattern.html">1245 high scalability-2012-05-14-DynamoDB Talk Notes and the SSD Hot S3 Cold Pattern</a></p>
<p>16 0.86625963 <a title="1374-lda-16" href="../high_scalability-2008/high_scalability-2008-03-25-Paper%3A_On_Designing_and_Deploying_Internet-Scale_Services.html">288 high scalability-2008-03-25-Paper: On Designing and Deploying Internet-Scale Services</a></p>
<p>17 0.86563116 <a title="1374-lda-17" href="../high_scalability-2009/high_scalability-2009-02-05-Beta_testers_wanted_for_ultra_high-scalability-performance_clustered_object_storage_system_designed_for_web_content_delivery.html">508 high scalability-2009-02-05-Beta testers wanted for ultra high-scalability-performance clustered object storage system designed for web content delivery</a></p>
<p>18 0.86513519 <a title="1374-lda-18" href="../high_scalability-2008/high_scalability-2008-03-04-Manage_Downtime_Risk_by_Connecting_Multiple_Data_Centers_into_a_Secure_Virtual_LAN.html">266 high scalability-2008-03-04-Manage Downtime Risk by Connecting Multiple Data Centers into a Secure Virtual LAN</a></p>
<p>19 0.86505908 <a title="1374-lda-19" href="../high_scalability-2008/high_scalability-2008-03-27-Amazon_Announces_Static_IP_Addresses_and_Multiple_Datacenter_Operation.html">289 high scalability-2008-03-27-Amazon Announces Static IP Addresses and Multiple Datacenter Operation</a></p>
<p>20 0.86389476 <a title="1374-lda-20" href="../high_scalability-2009/high_scalability-2009-10-26-Facebook%27s_Memcached_Multiget_Hole%3A_More_machines_%21%3D_More_Capacity_.html">728 high scalability-2009-10-26-Facebook's Memcached Multiget Hole: More machines != More Capacity </a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
