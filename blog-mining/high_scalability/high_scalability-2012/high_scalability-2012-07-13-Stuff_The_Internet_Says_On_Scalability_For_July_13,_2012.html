<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1283 high scalability-2012-07-13-Stuff The Internet Says On Scalability For July 13, 2012</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2012" href="../home/high_scalability-2012_home.html">high_scalability-2012</a> <a title="high_scalability-2012-1283" href="#">high_scalability-2012-1283</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1283 high scalability-2012-07-13-Stuff The Internet Says On Scalability For July 13, 2012</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2012-1283-html" href="http://highscalability.com//blog/2012/7/13/stuff-the-internet-says-on-scalability-for-july-13-2012.html">html</a></p><p>Introduction: It's HighScalability Time (Good luck today):
  
 A Friday the 13th Postmorterama:                       
 
 James Hamilton with some  high powered perspective  on the report for the Fukushima Nuclear Accident. Apparently they haven't heard of the blameless post-mortem. Lots of interesting stuff, but this is a potentially disaster saving general lesson learned: operators can’t figure out what is happening or take appropriate action without detailed visibility into the state of the system. 
 Evernote with a nicely detailed note  on a recent outage . A kernel panic happened while upgrading two new “shard” servers with 3x as much RAM, SSDs instead of 15krpm disks, bonded networking, and an updated kernel. They had to revert and shite loves to happen when other shite happens. 
 Heroku with their  postmortem on what happened  when AWS went down. They lost 30% of their instances across 3 AZs in the US-East region. Rich detail on the impact of the AWS, but not much on what they can do about it</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Lots of interesting stuff, but this is a potentially disaster saving general lesson learned: operators can’t figure out what is happening or take appropriate action without detailed visibility into the state of the system. [sent-3, score-0.185]
</p><p>2 Evernote with a nicely detailed note  on a recent outage . [sent-4, score-0.105]
</p><p>3 A kernel panic happened while upgrading two new “shard” servers with 3x as much RAM, SSDs instead of 15krpm disks, bonded networking, and an updated kernel. [sent-5, score-0.16]
</p><p>4 They had to revert and shite loves to happen when other shite happens. [sent-6, score-0.328]
</p><p>5 They lost 30% of their instances across 3 AZs in the US-East region. [sent-8, score-0.082]
</p><p>6 Run with extra load, create custom stats, master the shell for analytics, verbose logging is good, test weak points, test by running code, keep it consistent, and many more. [sent-16, score-0.078]
</p><p>7 Malloc overhead was very high so redesigned memory usage. [sent-24, score-0.082]
</p><p>8 Phabricator, for example, is their internal code-review tool, that was extended using Open Graph to publish a single Open Graph action against a "diff" object. [sent-28, score-0.08]
</p><p>9 StayFitB tracks workouts using Open Graph actions each time an employee badges in to the fitness center. [sent-32, score-0.15]
</p><p>10 Ricky Ho with another great article, this time it's an expansive description of the  Couchbase Architecture . [sent-42, score-0.078]
</p><p>11 Jimmy Xiang with a detailed explanation of how log splitting is used to "recover lost updates from region server failures. [sent-45, score-0.187]
</p><p>12 Nice slide deck covering  scalability factors, vertical vs horizontal, stateless applications, connection management, segmenting traffic, segmenting responsbility, clustering, and much more. [sent-51, score-0.312]
</p><p>13 Don't get a divorce Mommy and Daddy memcached,  I hate it when you argue . [sent-54, score-0.087]
</p><p>14 Reagents: Expressing and Composing Fine-grained Concurrency : We introduce reagents, a set of combinators for concisely expressing concurrency algorithms. [sent-58, score-0.318]
</p><p>15 Learn all about: Random subset, Heavy hitter detection, Randomized statistics, and Realtime analytics. [sent-61, score-0.087]
</p><p>16 Presentation: Progressive Architectures at the Royal Bank of Scotland : Its architecture introduces a non-allocating queue specialized for multiple producers and single consumers. [sent-66, score-0.079]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('reagents', 0.192), ('stayfitb', 0.192), ('shite', 0.164), ('expressing', 0.156), ('segmenting', 0.156), ('sebastian', 0.12), ('detailed', 0.105), ('graph', 0.092), ('apower', 0.087), ('julien', 0.087), ('hitter', 0.087), ('counterparts', 0.087), ('somehigh', 0.087), ('sillycon', 0.087), ('protip', 0.087), ('bonded', 0.087), ('concisely', 0.087), ('divorce', 0.087), ('floss', 0.087), ('fukushima', 0.087), ('phabricator', 0.087), ('uncovered', 0.087), ('lost', 0.082), ('statistics', 0.082), ('nbc', 0.082), ('blameless', 0.082), ('redesigned', 0.082), ('cloudbees', 0.082), ('clos', 0.082), ('hbase', 0.082), ('aws', 0.08), ('action', 0.08), ('specialized', 0.079), ('verbose', 0.078), ('royal', 0.078), ('expansive', 0.078), ('olympic', 0.078), ('airship', 0.078), ('jaykreps', 0.078), ('randomized', 0.078), ('hedlund', 0.078), ('badges', 0.075), ('progressive', 0.075), ('fabrics', 0.075), ('combinators', 0.075), ('malloc', 0.075), ('fitness', 0.075), ('panic', 0.073), ('brad', 0.073), ('daddy', 0.073)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="1283-tfidf-1" href="../high_scalability-2012/high_scalability-2012-07-13-Stuff_The_Internet_Says_On_Scalability_For_July_13%2C_2012.html">1283 high scalability-2012-07-13-Stuff The Internet Says On Scalability For July 13, 2012</a></p>
<p>Introduction: It's HighScalability Time (Good luck today):
  
 A Friday the 13th Postmorterama:                       
 
 James Hamilton with some  high powered perspective  on the report for the Fukushima Nuclear Accident. Apparently they haven't heard of the blameless post-mortem. Lots of interesting stuff, but this is a potentially disaster saving general lesson learned: operators can’t figure out what is happening or take appropriate action without detailed visibility into the state of the system. 
 Evernote with a nicely detailed note  on a recent outage . A kernel panic happened while upgrading two new “shard” servers with 3x as much RAM, SSDs instead of 15krpm disks, bonded networking, and an updated kernel. They had to revert and shite loves to happen when other shite happens. 
 Heroku with their  postmortem on what happened  when AWS went down. They lost 30% of their instances across 3 AZs in the US-East region. Rich detail on the impact of the AWS, but not much on what they can do about it</p><p>2 0.13080709 <a title="1283-tfidf-2" href="../high_scalability-2010/high_scalability-2010-03-22-7_Secrets_to_Successfully_Scaling_with_Scalr_%28on_Amazon%29_by_Sebastian_Stadil.html">798 high scalability-2010-03-22-7 Secrets to Successfully Scaling with Scalr (on Amazon) by Sebastian Stadil</a></p>
<p>Introduction: This is a part interview part guest with Sebastian Stadil, founder of  Scalr , a cheaper open-source version of RightScale. Scalr takes care of all the web site infrastructure bits to on Amazon (and other clouds) so you don’t have to.
 
I first met Sebastian at one of the original Silicon Valley Cloud Computing Group meetups, a group which he founded. The meetings started in the tiny offices of Intalio where Sebastian was working with this new fangled Amazon thing to create an auto-scaling server farm on EC2. I remember distinctly how Sebastian met everyone at the door with a handshake and a smile, making us all feel welcome. Later I took one of the classes he  created on how to use AWS. I guess he figured all this cloud stuff was going somewhere and decided to start Scalr.
 
My only regret about this post is that the name Amazon does not begin with the letter ‘S’, that would have made for an epic title.
  Getting to Know You  
In this section are the responses Sebastian gave to a few</p><p>3 0.12846571 <a title="1283-tfidf-3" href="../high_scalability-2009/high_scalability-2009-03-16-Are_Cloud_Based_Memory_Architectures_the_Next_Big_Thing%3F.html">538 high scalability-2009-03-16-Are Cloud Based Memory Architectures the Next Big Thing?</a></p>
<p>Introduction: We are on the edge of two potent technological changes: Clouds and Memory Based Architectures. This evolution will rip open a chasm where new players can enter and prosper. Google is the master of disk. You can't beat them at a game they perfected. Disk based databases like SimpleDB and BigTable are complicated beasts, typical last gasp products of any aging technology before a change. The next era is the age of Memory and Cloud which will allow for new players to succeed. The tipping point will be soon.   Let's take a short trip down web architecture lane:
  It's 1993: Yahoo runs on FreeBSD, Apache, Perl scripts and a SQL database   It's 1995: Scale-up the database.   It's 1998: LAMP   It's 1999: Stateless + Load Balanced + Database + SAN   It's 2001: In-memory data-grid.   It's 2003: Add a caching layer.   It's 2004: Add scale-out and partitioning.   It's 2005: Add asynchronous job scheduling and maybe a distributed file system.   It's 2007: Move it all into the cloud.   It's 2008: C</p><p>4 0.1245615 <a title="1283-tfidf-4" href="../high_scalability-2011/high_scalability-2011-03-22-Facebook%27s_New_Realtime_Analytics_System%3A_HBase_to_Process_20_Billion_Events_Per_Day.html">1008 high scalability-2011-03-22-Facebook's New Realtime Analytics System: HBase to Process 20 Billion Events Per Day</a></p>
<p>Introduction: Facebook did it again. They've built another system capable of doing something useful with ginormous streams of realtime data. Last time we saw Facebook release their  New Real-Time Messaging System: HBase To Store 135+ Billion Messages A Month . This time it's a realtime analytics system handling  over 20 billion events per day (200,000 events per second) with a lag of less than 30 seconds . 
 
Alex Himel, Engineering Manager at Facebook,  explains what they've built  ( video ) and the scale required:
  

Social plugins have become an important and growing source of traffic for millions of websites over the past year. We released a new version of Insights for Websites last week to give site owners better analytics on how people interact with their content and to help them optimize their websites in real time. To accomplish this, we had to engineer a system that could process over 20 billion events per day (200,000 events per second) with a lag of less than 30 seconds. 

  
Alex does a</p><p>5 0.12221128 <a title="1283-tfidf-5" href="../high_scalability-2010/high_scalability-2010-12-06-What_the_heck_are_you_actually_using_NoSQL_for%3F.html">954 high scalability-2010-12-06-What the heck are you actually using NoSQL for?</a></p>
<p>Introduction: It's a truism that we should choose the  right tool for the job . Everyone says that. And who can disagree? The problem is this is not helpful advice without being able to answer more specific questions like: What jobs are the tools good at? Will they work on jobs like mine? Is it worth the risk to try something new when all my people know something else and we have a deadline to meet? How can I make all the tools work together?
 
In the NoSQL space this kind of real-world data is still a bit vague. When asked, vendors tend to give very general answers like NoSQL is good for BigData or key-value access. What does that mean for for the developer in the trenches faced with the task of solving a specific problem and there are a dozen confusing choices and no obvious winner? Not a lot. It's often hard to take that next step and imagine how their specific problems could be solved in a way that's worth taking the trouble and risk.
 
Let's change that. What problems are you using NoSQL to sol</p><p>6 0.12130208 <a title="1283-tfidf-6" href="../high_scalability-2010/high_scalability-2010-03-30-Running_Large_Graph_Algorithms_-_Evaluation_of_Current_State-of-the-Art_and_Lessons_Learned.html">801 high scalability-2010-03-30-Running Large Graph Algorithms - Evaluation of Current State-of-the-Art and Lessons Learned</a></p>
<p>7 0.1193144 <a title="1283-tfidf-7" href="../high_scalability-2011/high_scalability-2011-10-27-Strategy%3A_Survive_a_Comet_Strike_in_the_East_With_Reserved_Instances_in_the_West.html">1133 high scalability-2011-10-27-Strategy: Survive a Comet Strike in the East With Reserved Instances in the West</a></p>
<p>8 0.10873675 <a title="1283-tfidf-8" href="../high_scalability-2009/high_scalability-2009-12-16-Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_in_the_Ambient_Cloud.html">750 high scalability-2009-12-16-Building Super Scalable Systems: Blade Runner Meets Autonomic Computing in the Ambient Cloud</a></p>
<p>9 0.10873675 <a title="1283-tfidf-9" href="../high_scalability-2012/high_scalability-2012-11-05-Gone_Fishin%27%3A_Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_In_The_Ambient_Cloud.html">1355 high scalability-2012-11-05-Gone Fishin': Building Super Scalable Systems: Blade Runner Meets Autonomic Computing In The Ambient Cloud</a></p>
<p>10 0.10842438 <a title="1283-tfidf-10" href="../high_scalability-2014/high_scalability-2014-02-14-Stuff_The_Internet_Says_On_Scalability_For_February_14th%2C_2014.html">1596 high scalability-2014-02-14-Stuff The Internet Says On Scalability For February 14th, 2014</a></p>
<p>11 0.10788819 <a title="1283-tfidf-11" href="../high_scalability-2012/high_scalability-2012-05-07-Startups_are_Creating_a_New_System_of_the_World_for_IT.html">1240 high scalability-2012-05-07-Startups are Creating a New System of the World for IT</a></p>
<p>12 0.10514054 <a title="1283-tfidf-12" href="../high_scalability-2014/high_scalability-2014-03-03-The_%E2%80%9CFour_Hamiltons%E2%80%9D_Framework_for_Mitigating_Faults_in_the_Cloud%3A_Avoid_it%2C_Mask_it%2C_Bound_it%2C_Fix_it_Fast.html">1604 high scalability-2014-03-03-The “Four Hamiltons” Framework for Mitigating Faults in the Cloud: Avoid it, Mask it, Bound it, Fix it Fast</a></p>
<p>13 0.10398856 <a title="1283-tfidf-13" href="../high_scalability-2010/high_scalability-2010-05-05-How_will_memristors_change_everything%3F_.html">823 high scalability-2010-05-05-How will memristors change everything? </a></p>
<p>14 0.1023583 <a title="1283-tfidf-14" href="../high_scalability-2009/high_scalability-2009-06-13-Neo4j_-_a_Graph_Database_that_Kicks_Buttox.html">628 high scalability-2009-06-13-Neo4j - a Graph Database that Kicks Buttox</a></p>
<p>15 0.10221127 <a title="1283-tfidf-15" href="../high_scalability-2007/high_scalability-2007-08-30-Log_Everything_All_the_Time.html">77 high scalability-2007-08-30-Log Everything All the Time</a></p>
<p>16 0.10195517 <a title="1283-tfidf-16" href="../high_scalability-2013/high_scalability-2013-12-13-Stuff_The_Internet_Says_On_Scalability_For_December_13th%2C_2013.html">1564 high scalability-2013-12-13-Stuff The Internet Says On Scalability For December 13th, 2013</a></p>
<p>17 0.1018587 <a title="1283-tfidf-17" href="../high_scalability-2011/high_scalability-2011-03-08-Medialets_Architecture_-__Defeating_the_Daunting_Mobile_Device_Data_Deluge.html">1000 high scalability-2011-03-08-Medialets Architecture -  Defeating the Daunting Mobile Device Data Deluge</a></p>
<p>18 0.1014059 <a title="1283-tfidf-18" href="../high_scalability-2012/high_scalability-2012-08-03-Stuff_The_Internet_Says_On_Scalability_For_August_3%2C_2012.html">1297 high scalability-2012-08-03-Stuff The Internet Says On Scalability For August 3, 2012</a></p>
<p>19 0.10124338 <a title="1283-tfidf-19" href="../high_scalability-2010/high_scalability-2010-10-15-Troubles_with_Sharding_-_What_can_we_learn_from_the_Foursquare_Incident%3F.html">920 high scalability-2010-10-15-Troubles with Sharding - What can we learn from the Foursquare Incident?</a></p>
<p>20 0.10041084 <a title="1283-tfidf-20" href="../high_scalability-2014/high_scalability-2014-02-21-Stuff_The_Internet_Says_On_Scalability_For_February_21st%2C_2014.html">1600 high scalability-2014-02-21-Stuff The Internet Says On Scalability For February 21st, 2014</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.226), (1, 0.101), (2, -0.003), (3, 0.051), (4, 0.016), (5, 0.036), (6, 0.015), (7, -0.001), (8, 0.037), (9, -0.004), (10, 0.029), (11, -0.024), (12, -0.003), (13, -0.039), (14, -0.064), (15, 0.024), (16, 0.027), (17, 0.059), (18, 0.025), (19, 0.049), (20, -0.012), (21, -0.022), (22, -0.044), (23, 0.024), (24, -0.031), (25, 0.0), (26, -0.043), (27, 0.046), (28, -0.019), (29, -0.005), (30, -0.004), (31, -0.033), (32, 0.067), (33, -0.02), (34, -0.009), (35, 0.043), (36, 0.0), (37, 0.001), (38, 0.022), (39, 0.008), (40, -0.013), (41, -0.012), (42, -0.012), (43, 0.036), (44, 0.052), (45, -0.026), (46, 0.039), (47, 0.012), (48, 0.004), (49, -0.024)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96256632 <a title="1283-lsi-1" href="../high_scalability-2012/high_scalability-2012-07-13-Stuff_The_Internet_Says_On_Scalability_For_July_13%2C_2012.html">1283 high scalability-2012-07-13-Stuff The Internet Says On Scalability For July 13, 2012</a></p>
<p>Introduction: It's HighScalability Time (Good luck today):
  
 A Friday the 13th Postmorterama:                       
 
 James Hamilton with some  high powered perspective  on the report for the Fukushima Nuclear Accident. Apparently they haven't heard of the blameless post-mortem. Lots of interesting stuff, but this is a potentially disaster saving general lesson learned: operators can’t figure out what is happening or take appropriate action without detailed visibility into the state of the system. 
 Evernote with a nicely detailed note  on a recent outage . A kernel panic happened while upgrading two new “shard” servers with 3x as much RAM, SSDs instead of 15krpm disks, bonded networking, and an updated kernel. They had to revert and shite loves to happen when other shite happens. 
 Heroku with their  postmortem on what happened  when AWS went down. They lost 30% of their instances across 3 AZs in the US-East region. Rich detail on the impact of the AWS, but not much on what they can do about it</p><p>2 0.81031507 <a title="1283-lsi-2" href="../high_scalability-2012/high_scalability-2012-08-03-Stuff_The_Internet_Says_On_Scalability_For_August_3%2C_2012.html">1297 high scalability-2012-08-03-Stuff The Internet Says On Scalability For August 3, 2012</a></p>
<p>Introduction: It's HighScalability Time:
  
 Quotable Quotes:                       
 
  Ross Tur : the tricks you learned to make things big are not the same tricks you can apply to make things infinite.  
  @gclaramunt : Son, I'm getting old, but let me tell you a secret: programming is hard, and high scalability and concurrent programming... frigging hard! 
 
 
  @Carnage4Life : At Apple the iOS team didn't see iPhone hardware or hardware team see OS until it shipped 
  @adrianco :  #ebspio caps iops but latency variance is much lower than EBS 
  @bernardgolden : RT @peakscale: A culture of automation is 10x more important than deployment/test/monkey thing you'd like to discuss < devops calling 
  @JayCollier : 50 years ago, school standardization was needed for scale. Now, scalability and flexibility (variability) can coexist. #FOL2012 
  @adrianco : Compared to vanilla EBS many times better for random reads. Bandwidth limits both for sequential and writes.  #ebspio  
  @SQLPerfTips : More hardw</p><p>3 0.80774218 <a title="1283-lsi-3" href="../high_scalability-2012/high_scalability-2012-11-30-Stuff_The_Internet_Says_On_Scalability_For_November_30%2C_2012.html">1365 high scalability-2012-11-30-Stuff The Internet Says On Scalability For November 30, 2012</a></p>
<p>Introduction: We're back and it's HighScalability Time:
  
  1B Tweets Every 2.5 Days : Twitter.  1 billion transactions/day : Salesforce.  
 Storing  700 terabytes of data into a single gram of DNA . Downside, reading  is very slow . And any data might conflict with the messages aliens have already inserted. 
 Assuming my infonome is 1 TB, it would cost $1,338,333 to store my existence in Amazon Glacier for a long nowish 10,000 years. #notbad 
 Quotable Quotes:                                              
 
  @cloudpundit : @Werner: "I've hugged a lot of servers in my life, and believe me, they do not hug you back. They hate you." #reinvent 
  @jinman : Werner #reinvent The commandments of 21st century architectures 1) Controllable, 2)Resilient, 3)Adaptive and 4) Data Driven #cloud 
  @dandonovan78 : Wow. Netflix video streaming has grown from 1M hours to 1 BILLION hours a month in less than 4 years. Insane. #scalability #aws #reinvent 
  @sandfoxuk : Linear scalability - the spherical cow of clou</p><p>4 0.80159408 <a title="1283-lsi-4" href="../high_scalability-2014/high_scalability-2014-04-25-Stuff_The_Internet_Says_On_Scalability_For_April_25th%2C_2014.html">1637 high scalability-2014-04-25-Stuff The Internet Says On Scalability For April 25th, 2014</a></p>
<p>Introduction: Hey, it's HighScalability time:
    New World Record  BASE jumping  from World's Tallest Building. #crazy   
  30 billion : total Pinterest pins;  500,000,000 : What'sApp users (700 million photos and 100 million videos every single day);  1 billion : Facebook active users on phones and tablets. 
 Quotable Quotes:                                                                                                                    
 
  @jimplush : Google spent 2.3 billion on infrastructure in Q1. Remember that when you say you want to be "the Google of something" 
 Clay Shirky: I think one of the things that happened to the P2P market for infrastructure is that users preference for predictable pricing vs resource-sensitive pricing is so overwhelming that they will overpay to anyone who can promise flat prices. And because the logic of centralization vs decentralization is so price sensitive, I don't think there is any logical reason to assume a broadly stable class of apps, separate from c</p><p>5 0.78044534 <a title="1283-lsi-5" href="../high_scalability-2013/high_scalability-2013-11-15-Stuff_The_Internet_Says_On_Scalability_For_November_15th%2C_2013.html">1549 high scalability-2013-11-15-Stuff The Internet Says On Scalability For November 15th, 2013</a></p>
<p>Introduction: Hey, it's HighScalability time:
    Test your sense of scale. Is this image of something microscopic or macroscopic?  Find out .   
 Quotable Quotes:                           
 
  fidotron : It feels like we've gone in one big circle, where first we move the DB on to a separate machine for performance, yet now more computation will go back to being done nearer the data (like Hadoop) and we'll try to pretend it's all just one giant computer again. 
  @pbailis : Building systems from the ground up with distribution, scale, and availability in mind is much easier than retrofitting single-node systems. 
  @merv : #awsreinvent Jassy: Netflix has 10,000s of EC2 instances. They are the final deployment scenario: All In. And others are coming. 
  Edward Capriolo : YARN... Either it is really complicated or I have brain damage 
  @djspiewak : Eventually, Node.js will reinvent the “IO promise” and realize that flattening your callback effects is actually quite nice. 
  @jimblomo : A Note on Dis</p><p>6 0.77327192 <a title="1283-lsi-6" href="../high_scalability-2012/high_scalability-2012-07-06-Stuff_The_Internet_Says_On_Scalability_For_July_6%2C_2012.html">1278 high scalability-2012-07-06-Stuff The Internet Says On Scalability For July 6, 2012</a></p>
<p>7 0.77217573 <a title="1283-lsi-7" href="../high_scalability-2011/high_scalability-2011-09-23-Stuff_The_Internet_Says_On_Scalability_For_September_23%2C_2011.html">1122 high scalability-2011-09-23-Stuff The Internet Says On Scalability For September 23, 2011</a></p>
<p>8 0.77209818 <a title="1283-lsi-8" href="../high_scalability-2013/high_scalability-2013-06-28-Stuff_The_Internet_Says_On_Scalability_For_June_28%2C_2013.html">1484 high scalability-2013-06-28-Stuff The Internet Says On Scalability For June 28, 2013</a></p>
<p>9 0.76903421 <a title="1283-lsi-9" href="../high_scalability-2013/high_scalability-2013-06-21-Stuff_The_Internet_Says_On_Scalability_For_June_21%2C_2013.html">1479 high scalability-2013-06-21-Stuff The Internet Says On Scalability For June 21, 2013</a></p>
<p>10 0.76845676 <a title="1283-lsi-10" href="../high_scalability-2012/high_scalability-2012-09-21-Stuff_The_Internet_Says_On_Scalability_For_September_21%2C_2012.html">1327 high scalability-2012-09-21-Stuff The Internet Says On Scalability For September 21, 2012</a></p>
<p>11 0.76760846 <a title="1283-lsi-11" href="../high_scalability-2011/high_scalability-2011-09-30-Stuff_The_Internet_Says_On_Scalability_For_September_30%2C_2011.html">1129 high scalability-2011-09-30-Stuff The Internet Says On Scalability For September 30, 2011</a></p>
<p>12 0.76653337 <a title="1283-lsi-12" href="../high_scalability-2014/high_scalability-2014-03-07-Stuff_The_Internet_Says_On_Scalability_For_March_7th%2C_2014.html">1607 high scalability-2014-03-07-Stuff The Internet Says On Scalability For March 7th, 2014</a></p>
<p>13 0.76607251 <a title="1283-lsi-13" href="../high_scalability-2012/high_scalability-2012-05-11-Stuff_The_Internet_Says_On_Scalability_For_May_11%2C_2012.html">1244 high scalability-2012-05-11-Stuff The Internet Says On Scalability For May 11, 2012</a></p>
<p>14 0.76380682 <a title="1283-lsi-14" href="../high_scalability-2013/high_scalability-2013-10-11-Stuff_The_Internet_Says_On_Scalability_For_October_11th%2C_2013.html">1530 high scalability-2013-10-11-Stuff The Internet Says On Scalability For October 11th, 2013</a></p>
<p>15 0.76093578 <a title="1283-lsi-15" href="../high_scalability-2013/high_scalability-2013-04-12-Stuff_The_Internet_Says_On_Scalability_For_April_12%2C_2013.html">1439 high scalability-2013-04-12-Stuff The Internet Says On Scalability For April 12, 2013</a></p>
<p>16 0.76088613 <a title="1283-lsi-16" href="../high_scalability-2013/high_scalability-2013-07-19-Stuff_The_Internet_Says_On_Scalability_For_July_19%2C_2013.html">1494 high scalability-2013-07-19-Stuff The Internet Says On Scalability For July 19, 2013</a></p>
<p>17 0.76074797 <a title="1283-lsi-17" href="../high_scalability-2012/high_scalability-2012-02-10-Stuff_The_Internet_Says_On_Scalability_For_February_10%2C_2012.html">1190 high scalability-2012-02-10-Stuff The Internet Says On Scalability For February 10, 2012</a></p>
<p>18 0.75894797 <a title="1283-lsi-18" href="../high_scalability-2013/high_scalability-2013-05-03-Stuff_The_Internet_Says_On_Scalability_For_May_3%2C_2013.html">1451 high scalability-2013-05-03-Stuff The Internet Says On Scalability For May 3, 2013</a></p>
<p>19 0.7589218 <a title="1283-lsi-19" href="../high_scalability-2011/high_scalability-2011-08-05-Stuff_The_Internet_Says_On_Scalability_For_August_5%2C_2011.html">1093 high scalability-2011-08-05-Stuff The Internet Says On Scalability For August 5, 2011</a></p>
<p>20 0.75717723 <a title="1283-lsi-20" href="../high_scalability-2012/high_scalability-2012-07-20-Stuff_The_Internet_Says_On_Scalability_For_July_20%2C_2012.html">1287 high scalability-2012-07-20-Stuff The Internet Says On Scalability For July 20, 2012</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.128), (2, 0.608), (10, 0.027), (40, 0.013), (61, 0.025), (79, 0.075), (85, 0.044), (94, 0.014)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.99580485 <a title="1283-lda-1" href="../high_scalability-2012/high_scalability-2012-02-10-Stuff_The_Internet_Says_On_Scalability_For_February_10%2C_2012.html">1190 high scalability-2012-02-10-Stuff The Internet Says On Scalability For February 10, 2012</a></p>
<p>Introduction: HighScalability Tested, Mother Approved:
  
  12,233TPS : Twitter @ Super Bowl;  11 Million Slices : Dominos @ Super Bowl;  500K requests per second : S3;  
  The great mobile money drain . Mobile: high resource costs, low revenue. Mobile traffic on Plenty of Fish is  growing at 3% a month , rising to 3 Billion pageviews a month, 40% of signups are mobile, and all traffic will soon be 60-70% mobile. The problem: how do you make money on mobile? 
  Time to chuck microprocessors for a networks of cells?   How Networks of Biological Cells Solve Distributed Computing Problems : Computer scientists prove that networks of cells can compute as efficiently as networks of computers linked via the internet. We believe that there is a need for a network model, where nodes are by design below the computation and communication capabilities of Turing machines. 
  Unrelated?   GDrive at last  and  S3 Drops Storage Pricing . 
 If you are  StackOverflow and your data is overflowing , what do you do? Mo</p><p>2 0.99562401 <a title="1283-lda-2" href="../high_scalability-2011/high_scalability-2011-12-12-Netflix%3A_Developing%2C_Deploying%2C_and_Supporting_Software_According_to_the_Way_of_the_Cloud.html">1155 high scalability-2011-12-12-Netflix: Developing, Deploying, and Supporting Software According to the Way of the Cloud</a></p>
<p>Introduction: At a  Cloud Computing Meetup , Siddharth "Sid" Anand of Netflix, backed by a merry band of Netflixians, gave an interesting talk:  Keeping Movies Running Amid Thunderstorms . While the talk gave a good overview of their move to the cloud, issues with capacity planning,  thundering herds , latency problems, and  simian armageddon , I found myself most taken with how they handle  software deployment in the cloud .
 
I've worked on half a dozen or more build and deployment systems, some small, some quite large, but never for a large organization like Netflix in the cloud. The cloud has this amazing capability that has never existed before that enables a novel approach to fault-tolerant software deployments:  the ability to spin up huge numbers of instances to completely run a new release while running the old release at the same time .
 
The process goes something like: 
  
 A  canary machine  is launched first with the new software load running real traffic to sanity test the load in a p</p><p>3 0.99554569 <a title="1283-lda-3" href="../high_scalability-2011/high_scalability-2011-01-03-Stuff_The_Internet_Says_On_Scalability_For_January_3%2C_2010.html">967 high scalability-2011-01-03-Stuff The Internet Says On Scalability For January 3, 2010</a></p>
<p>Introduction: Submitted for your reading pleasure...
  
 Quotable Quotes           
 
  @hofmanndavid : Performance and scalability anxiety makes developers want to catch the flying butterflies 
  @tivrfoa :  "Scalability solutions aren't magic. They involve partitioning, indexing and replication." Twitter engineer  
 Alan Perlis:  Fools ignore complexity; pragmatists suffer it; experts avoid it; geniuses remove it.  
 
 
  CIO update: Post-mortem on the Skype outage . Interesting tale of a cascading collapse in complex, distributed, interactive systems. For more background see the highly illuminating  Explaining Supernodes  by Dan York. 
  RethinkDB and SSD Databases. SSD was not a revolution  by Kevin Burton.  What’s really shocking to me, is that while SSD and flash storage is very exciting, it wasn’t as revolutionary in 2010 as I would have liked to have seen.  
  The case for Datastore-Side-Scripting . Russell Sullivan predicts real-time web applications are going in the direction of being enti</p><p>4 0.99539131 <a title="1283-lda-4" href="../high_scalability-2008/high_scalability-2008-01-10-Letting_Clients_Know_What%27s_Changed%3A_Push_Me_or_Pull_Me%3F.html">205 high scalability-2008-01-10-Letting Clients Know What's Changed: Push Me or Pull Me?</a></p>
<p>Introduction: I had a false belief I thought I came here to stay We're all just visiting All just breaking like waves The oceans made me, but who came up with me? Push me, pull me, push me, or pull me out .     So true Perl Jam   (Push me Pull me lyrics)  , so true. I too have wondered how web clients should be notified of model changes. Should servers push events to clients or should clients pull events from servers? A topic worthy of its own song if ever there was one.       To pull events the client simply starts a timer and makes a request to the server. This is polling. You can either pull a complete set of fresh data or get a list of changes. The server "knows" if anything you are interested in has changed and makes those changes available to you.  Knowing what has changed can be relatively simple with a publish-subscribe type backend or you can get very complex with fine grained bit maps of attributes and keeping per client state on what I client still needs to see.     Polling is heavy man.</p><p>5 0.99461114 <a title="1283-lda-5" href="../high_scalability-2007/high_scalability-2007-07-31-BerkeleyDB_%26_other_distributed_high_performance_key-value_databases.html">50 high scalability-2007-07-31-BerkeleyDB & other distributed high performance key-value databases</a></p>
<p>Introduction: I currently use BerkeleyDB as an embedded database   http://www.oracle.com/database/berkeley-db/   a decision which was initially brought on by learning that Google used BerkeleyDB for their universal sign-on feature.     Lustre looks impressive, but their white paper shows speeds of 800 files created per second, as a good number.  However, BerkeleyDB on my mac mini does 200,000 row creations per second, and can be used as a distributed file system.     I'm having I/O scalability issues with BerkeleyDB on one machine, and about to implement their distributed replication feature (and go multi-machine), which in effect makes it work like a distributed file system, but with local access speeds.  That's why I was looking at Lustre.     The key feature difference between BerkeleyDB and Lustre is that BerkeleyDB has a complete copy of all the data on each computer, making it not a viable solution for massive sized database applications.  However, if you have < 1TB (ie, one disk) of total pos</p><p>same-blog 6 0.99446642 <a title="1283-lda-6" href="../high_scalability-2012/high_scalability-2012-07-13-Stuff_The_Internet_Says_On_Scalability_For_July_13%2C_2012.html">1283 high scalability-2012-07-13-Stuff The Internet Says On Scalability For July 13, 2012</a></p>
<p>7 0.99342138 <a title="1283-lda-7" href="../high_scalability-2014/high_scalability-2014-02-05-Little%E2%80%99s_Law%2C_Scalability_and_Fault_Tolerance%3A_The_OS_is_your_bottleneck._What_you_can_do%3F.html">1591 high scalability-2014-02-05-Little’s Law, Scalability and Fault Tolerance: The OS is your bottleneck. What you can do?</a></p>
<p>8 0.99160755 <a title="1283-lda-8" href="../high_scalability-2009/high_scalability-2009-05-08-Eight_Best_Practices_for_Building_Scalable_Systems.html">594 high scalability-2009-05-08-Eight Best Practices for Building Scalable Systems</a></p>
<p>9 0.99134195 <a title="1283-lda-9" href="../high_scalability-2009/high_scalability-2009-06-27-Scaling_Twitter%3A_Making_Twitter_10000_Percent_Faster.html">639 high scalability-2009-06-27-Scaling Twitter: Making Twitter 10000 Percent Faster</a></p>
<p>10 0.99132472 <a title="1283-lda-10" href="../high_scalability-2012/high_scalability-2012-02-27-Zen_and_the_Art_of_Scaling_-_A_Koan_and_Epigram_Approach.html">1199 high scalability-2012-02-27-Zen and the Art of Scaling - A Koan and Epigram Approach</a></p>
<p>11 0.99132246 <a title="1283-lda-11" href="../high_scalability-2011/high_scalability-2011-03-17-Are_long_VM_instance_spin-up_times_in_the_cloud_costing_you_money%3F.html">1006 high scalability-2011-03-17-Are long VM instance spin-up times in the cloud costing you money?</a></p>
<p>12 0.98948818 <a title="1283-lda-12" href="../high_scalability-2009/high_scalability-2009-03-30-Lavabit_Architecture_-_Creating_a_Scalable_Email_Service.html">551 high scalability-2009-03-30-Lavabit Architecture - Creating a Scalable Email Service</a></p>
<p>13 0.98946571 <a title="1283-lda-13" href="../high_scalability-2009/high_scalability-2009-10-16-Paper%3A_Scaling_Online_Social_Networks_without_Pains.html">723 high scalability-2009-10-16-Paper: Scaling Online Social Networks without Pains</a></p>
<p>14 0.98732394 <a title="1283-lda-14" href="../high_scalability-2008/high_scalability-2008-12-01-MySQL_Database_Scale-out_and_Replication_for_High_Growth_Businesses.html">455 high scalability-2008-12-01-MySQL Database Scale-out and Replication for High Growth Businesses</a></p>
<p>15 0.9868409 <a title="1283-lda-15" href="../high_scalability-2010/high_scalability-2010-06-18-Paper%3A_The_Declarative_Imperative%3A_Experiences_and_Conjectures_in_Distributed_Logic.html">844 high scalability-2010-06-18-Paper: The Declarative Imperative: Experiences and Conjectures in Distributed Logic</a></p>
<p>16 0.98631644 <a title="1283-lda-16" href="../high_scalability-2010/high_scalability-2010-09-30-More_Troubles_with_Caching.html">911 high scalability-2010-09-30-More Troubles with Caching</a></p>
<p>17 0.98582518 <a title="1283-lda-17" href="../high_scalability-2010/high_scalability-2010-09-30-Facebook_and_Site_Failures_Caused_by_Complex%2C_Weakly_Interacting%2C_Layered_Systems.html">910 high scalability-2010-09-30-Facebook and Site Failures Caused by Complex, Weakly Interacting, Layered Systems</a></p>
<p>18 0.98435211 <a title="1283-lda-18" href="../high_scalability-2009/high_scalability-2009-07-27-Handle_700_Percent_More_Requests_Using_Squid_and_APC_Cache.html">662 high scalability-2009-07-27-Handle 700 Percent More Requests Using Squid and APC Cache</a></p>
<p>19 0.98421741 <a title="1283-lda-19" href="../high_scalability-2010/high_scalability-2010-08-12-Strategy%3A_Terminate_SSL_Connections_in_Hardware_and_Reduce_Server_Count_by_40%25.html">878 high scalability-2010-08-12-Strategy: Terminate SSL Connections in Hardware and Reduce Server Count by 40%</a></p>
<p>20 0.98245782 <a title="1283-lda-20" href="../high_scalability-2008/high_scalability-2008-02-12-We_want_to_cache_a_lot_%3A%29_How_do_we_go_about_it_%3F.html">247 high scalability-2008-02-12-We want to cache a lot :) How do we go about it ?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
