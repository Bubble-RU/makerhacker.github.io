<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1266 high scalability-2012-06-18-Google on Latency Tolerant Systems: Making a Predictable Whole Out of Unpredictable Parts   </title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2012" href="../home/high_scalability-2012_home.html">high_scalability-2012</a> <a title="high_scalability-2012-1266" href="#">high_scalability-2012-1266</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1266 high scalability-2012-06-18-Google on Latency Tolerant Systems: Making a Predictable Whole Out of Unpredictable Parts   </h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2012-1266-html" href="http://highscalability.com//blog/2012/6/18/google-on-latency-tolerant-systems-making-a-predictable-whol.html">html</a></p><p>Introduction: In   Taming The Long Latency Tail   we covered   Luiz Barroso  ’s exploration of the long tail latency (some operations are really slow) problems generated by large fanout architectures (a request is composed of potentially thousands of other requests). You may have noticed there weren’t a lot of solutions. That’s where a talk I attended,   Achieving Rapid Response Times in Large Online Services   (  slide deck  ), by  Jeff Dean , also of Google, comes in:
  
  In this talk, I’ll describe a collection of techniques and practices lowering response times in large distributed systems whose components run on shared clusters of machines, where pieces of these systems are subject to interference by other tasks, and where unpredictable latency hiccups are the norm, not the exception. 

  
 The goal is to use software techniques to reduce variability given the increasing variability in underlying hardware, the need to handle dynamic workloads on a shared infrastructure, and the need to use lar</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In   Taming The Long Latency Tail   we covered   Luiz Barroso  ’s exploration of the long tail latency (some operations are really slow) problems generated by large fanout architectures (a request is composed of potentially thousands of other requests). [sent-1, score-0.811]
</p><p>2 The goal is to use software techniques to reduce variability given the increasing variability in underlying hardware, the need to handle dynamic workloads on a shared infrastructure, and the need to use large fanout architectures to operate at scale. [sent-4, score-0.999]
</p><p>3 Two forces motivate Google’s work on latency tolerance:       Large fanout architectures . [sent-5, score-0.503]
</p><p>4 The core idea is that small performance hiccups on a few machines causes higher overall latencies and the more machines the worse the tail latency. [sent-7, score-0.494]
</p><p>5 Only 1% of requests will take over a second with a server that has a 1ms average response time and a one second 99th percentile latency. [sent-9, score-0.463]
</p><p>6 If a request has to access 100 servers, now 63% of all requests will take over a second. [sent-10, score-0.428]
</p><p>7 Others try to reduce variability by overprovisioning or running only like workloads on machines. [sent-19, score-0.368]
</p><p>8 Fault Tolerant vs Latency Tolerant Systems     Dean makes a fascinating analogy between creating fault tolerant and latency tolerant systems. [sent-24, score-0.608]
</p><p>9 In the same way latency tolerant systems can   make a predictable whole out of unpredictable parts  . [sent-26, score-0.515]
</p><p>10 Cross Request Adaptation Strategies    The idea behind these strategies is to examine recent behavior and take action to improve latency of future requests within  tens of seconds or minutes . [sent-41, score-0.532]
</p><p>11 Collect distributions of each dimension, make histograms, and try to even out work to machines by looking at std deviations and distributions. [sent-51, score-0.524]
</p><p>12 Collect distributions of each dimension, make histograms, and try to even out work to machines by looking at std deviations and distributions. [sent-54, score-0.524]
</p><p>13 Keep measuring latency and when the latency improves return the partition to service. [sent-62, score-0.486]
</p><p>14 Within-request Adaptation Strategies    The idea behind these strategies is to fix a slow request  as it is happening . [sent-65, score-0.371]
</p><p>15 The strategies are:       Canary requests     Backup requests with cross-server cancellation   Tainted results      Backup Requests with Cross-Server Cancellation     Backup requests are the idea of sending requests out to multiple replicas, but in a particular way. [sent-66, score-1.075]
</p><p>16 You might think this is just a lot of extra traffic, but remember, the goal is to squeeze down the 99th percentile distribution, so the backup requests, even with what seems like a long wait time really bring down the tail latency and standard deviation. [sent-70, score-0.681]
</p><p>17 Bigtable, for example, used backup requests after two milliseconds, which dramatically dropped the 99th percentile by 43 percent on an idle system. [sent-72, score-0.68]
</p><p>18 With a loaded system the reduction was 38 percent with only one percent extra disk seeks. [sent-73, score-0.446]
</p><p>19 Backups requests with cancellations gives the same distribution as an unloaded cluster. [sent-74, score-0.366]
</p><p>20 With these latency tolerance techniques you are taking a loaded cluster with high variability and making it perform like an unloaded cluster. [sent-75, score-0.788]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('variability', 0.247), ('requests', 0.245), ('tolerant', 0.208), ('latency', 0.192), ('fanout', 0.184), ('request', 0.183), ('percentile', 0.151), ('machines', 0.146), ('strategiesthe', 0.133), ('percent', 0.123), ('unloaded', 0.121), ('unpredictable', 0.115), ('jobs', 0.113), ('std', 0.108), ('deviations', 0.108), ('histograms', 0.108), ('techniques', 0.106), ('hiccups', 0.104), ('partition', 0.102), ('tail', 0.098), ('distributions', 0.096), ('interference', 0.096), ('strategies', 0.095), ('wait', 0.094), ('overloaded', 0.094), ('large', 0.093), ('slow', 0.093), ('replicas', 0.093), ('dropped', 0.092), ('adaptation', 0.092), ('google', 0.089), ('dimension', 0.085), ('dimensions', 0.079), ('extra', 0.077), ('cpu', 0.07), ('backup', 0.069), ('tasks', 0.068), ('warehouse', 0.068), ('response', 0.067), ('collect', 0.067), ('work', 0.066), ('partitions', 0.063), ('disk', 0.062), ('send', 0.062), ('architectures', 0.061), ('loaded', 0.061), ('workloads', 0.061), ('tolerance', 0.061), ('spelling', 0.06), ('overprovisioning', 0.06)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000004 <a title="1266-tfidf-1" href="../high_scalability-2012/high_scalability-2012-06-18-Google_on_Latency_Tolerant_Systems%3A_Making_a_Predictable_Whole_Out_of_Unpredictable_Parts___.html">1266 high scalability-2012-06-18-Google on Latency Tolerant Systems: Making a Predictable Whole Out of Unpredictable Parts   </a></p>
<p>Introduction: In   Taming The Long Latency Tail   we covered   Luiz Barroso  ’s exploration of the long tail latency (some operations are really slow) problems generated by large fanout architectures (a request is composed of potentially thousands of other requests). You may have noticed there weren’t a lot of solutions. That’s where a talk I attended,   Achieving Rapid Response Times in Large Online Services   (  slide deck  ), by  Jeff Dean , also of Google, comes in:
  
  In this talk, I’ll describe a collection of techniques and practices lowering response times in large distributed systems whose components run on shared clusters of machines, where pieces of these systems are subject to interference by other tasks, and where unpredictable latency hiccups are the norm, not the exception. 

  
 The goal is to use software techniques to reduce variability given the increasing variability in underlying hardware, the need to handle dynamic workloads on a shared infrastructure, and the need to use lar</p><p>2 0.29252401 <a title="1266-tfidf-2" href="../high_scalability-2012/high_scalability-2012-03-12-Google%3A_Taming_the_Long_Latency_Tail_-_When_More_Machines_Equals_Worse_Results.html">1207 high scalability-2012-03-12-Google: Taming the Long Latency Tail - When More Machines Equals Worse Results</a></p>
<p>Introduction: Likewise the current belief that, in the case of artificial machines the very large and the very small are equally feasible and lasting is a manifest error. Thus, for example, a small obelisk or column or other solid figure can certainly be laid down or set up without danger of breaking, while the large ones will go to pieces under the slightest provocation, and that purely on account of their own weight. -- Galileo  
Galileo observed how things broke if they were naively scaled up. Interestingly, Google noticed a similar pattern when building larger software systems using the same techniques used to build smaller systems. 
 
 Luiz André Barroso , Distinguished Engineer at Google, talks about this fundamental property of scaling systems in his fascinating talk,  Warehouse-Scale Computing: Entering the Teenage Decade . Google found the larger the scale the greater the impact of latency variability. When a request is implemented by work done in parallel, as is common with today's service</p><p>3 0.25299981 <a title="1266-tfidf-3" href="../high_scalability-2009/high_scalability-2009-07-25-Latency_is_Everywhere_and_it_Costs_You_Sales_-_How_to_Crush_it.html">661 high scalability-2009-07-25-Latency is Everywhere and it Costs You Sales - How to Crush it</a></p>
<p>Introduction: Update 8 :  The Cost of Latency  by James Hamilton. James summarizing some latency info from      Steve Souder ,   Greg Linden , and   Marissa Mayer .      Speed [is] an undervalued and under-discussed asset on the web. 
 
 Update 7:   How do you know when you need more memcache servers? . Dathan Pattishall talks about using memcache not to scale, but to reduce latency and reduce I/O spikes, and how to use stats to know when more servers are needed.  Update 6:   Stock Traders Find Speed Pays, in Milliseconds . Goldman Sachs is making record profits off a  500 millisecond  trading advantage. Yes, latency matters. As an interesting aside, Libet found 500 msecs is about the time it takes the brain to weave together an experience of consciousness from all our sensor inputs.  Update 5:   Shopzilla's Site Redo - You Get What You Measure . At the  Velocity  conference Phil Dixon, from Shopzilla, presented data showing a 5 second speed up resulted in a 25% increase in page views, a 10% increas</p><p>4 0.23920378 <a title="1266-tfidf-4" href="../high_scalability-2013/high_scalability-2013-11-13-Google%3A_Multiplex_Multiple_Works_Loads_on_Computers_to_Increase_Machine_Utilization_and_Save_Money.html">1548 high scalability-2013-11-13-Google: Multiplex Multiple Works Loads on Computers to Increase Machine Utilization and Save Money</a></p>
<p>Introduction: Jeff Dean  gave a talk at   SFBay ACM   and at about 3 minutes in he goes over how Google runs jobs on computers, which is different than how most shops distribute workloads.
   It’s common for machines to be dedicated to one service, say run a database, run a cache, run this, or run that. The logic is: 
  
 
  Better control over responsiveness as you generally know the traffic loads a machine will experience and you can over provision a box to be safe. 

 
 
  Easier to manage, load balance, configure, upgrade, create and make highly available. Since you know what a machine does another machine can be provisioned to do the same work. 

 
    The problem is monocropping hardware though conceptually clean for humans and safe for applications, is hugely wasteful. Machines are woefully underutilized, even in a virtualized world. 
  What Google does is use a  shared environment  in a datacenter where all kinds of stuff run on each computer. Batch computation and interactive computations a</p><p>5 0.17829539 <a title="1266-tfidf-5" href="../high_scalability-2013/high_scalability-2013-02-11-At_Scale_Even_Little_Wins_Pay_Off_Big_-_Google_and_Facebook_Examples.html">1404 high scalability-2013-02-11-At Scale Even Little Wins Pay Off Big - Google and Facebook Examples</a></p>
<p>Introduction: There's a popular line of thought that says don't waste time on optimization because developing features is more important than saving money. True, you can always add resources, but at some point, especially in a more mature part of a product lifecycle: performance equals $$$.   Two great examples of this evolution come from Facebook and Google. The upshot is that when you spend time and money on optimizing your tool chain you can get huge wins in performance, control, and costs. Certainly, don’t bother if you are just starting, but at some point you may want to switch to big development efforts in improving efficiency. 
   Facebook and HipHop   
 The Facebook example is quite well known:    HipHop   , a static PHP compiler released in 2010, after two years of development. PHP because Facebook implements their web tier    in PHP   . They've now developed a dynamic compiler,    HipHop VM   , using techniques like JIT, side exits, HipHop bytecode, type prediction, and parallel tracelet l</p><p>6 0.17597975 <a title="1266-tfidf-6" href="../high_scalability-2014/high_scalability-2014-02-05-Little%E2%80%99s_Law%2C_Scalability_and_Fault_Tolerance%3A_The_OS_is_your_bottleneck._What_you_can_do%3F.html">1591 high scalability-2014-02-05-Little’s Law, Scalability and Fault Tolerance: The OS is your bottleneck. What you can do?</a></p>
<p>7 0.16826116 <a title="1266-tfidf-7" href="../high_scalability-2013/high_scalability-2013-02-27-42_Monster_Problems_that_Attack_as_Loads_Increase.html">1413 high scalability-2013-02-27-42 Monster Problems that Attack as Loads Increase</a></p>
<p>8 0.16719364 <a title="1266-tfidf-8" href="../high_scalability-2010/high_scalability-2010-11-22-Strategy%3A_Google_Sends_Canary_Requests_into_the_Data_Mine.html">946 high scalability-2010-11-22-Strategy: Google Sends Canary Requests into the Data Mine</a></p>
<p>9 0.16385652 <a title="1266-tfidf-9" href="../high_scalability-2013/high_scalability-2013-03-11-Low_Level_Scalability_Solutions_-_The_Conditioning_Collection.html">1421 high scalability-2013-03-11-Low Level Scalability Solutions - The Conditioning Collection</a></p>
<p>10 0.15129128 <a title="1266-tfidf-10" href="../high_scalability-2010/high_scalability-2010-02-05-High_Availability_Principle_%3A_Concurrency_Control.html">772 high scalability-2010-02-05-High Availability Principle : Concurrency Control</a></p>
<p>11 0.15008734 <a title="1266-tfidf-11" href="../high_scalability-2008/high_scalability-2008-11-22-Google_Architecture.html">448 high scalability-2008-11-22-Google Architecture</a></p>
<p>12 0.14859401 <a title="1266-tfidf-12" href="../high_scalability-2010/high_scalability-2010-12-20-Netflix%3A_Use_Less_Chatty_Protocols_in_the_Cloud_-_Plus_26_Fixes.html">960 high scalability-2010-12-20-Netflix: Use Less Chatty Protocols in the Cloud - Plus 26 Fixes</a></p>
<p>13 0.14805619 <a title="1266-tfidf-13" href="../high_scalability-2012/high_scalability-2012-04-03-Hazelcast_2.0%3A_Big_Data_In-Memory.html">1221 high scalability-2012-04-03-Hazelcast 2.0: Big Data In-Memory</a></p>
<p>14 0.14802691 <a title="1266-tfidf-14" href="../high_scalability-2009/high_scalability-2009-10-26-Facebook%27s_Memcached_Multiget_Hole%3A_More_machines_%21%3D_More_Capacity_.html">728 high scalability-2009-10-26-Facebook's Memcached Multiget Hole: More machines != More Capacity </a></p>
<p>15 0.14751989 <a title="1266-tfidf-15" href="../high_scalability-2010/high_scalability-2010-10-15-Troubles_with_Sharding_-_What_can_we_learn_from_the_Foursquare_Incident%3F.html">920 high scalability-2010-10-15-Troubles with Sharding - What can we learn from the Foursquare Incident?</a></p>
<p>16 0.14563182 <a title="1266-tfidf-16" href="../high_scalability-2013/high_scalability-2013-03-25-AppBackplane_-_A_Framework_for_Supporting_Multiple_Application_Architectures.html">1429 high scalability-2013-03-25-AppBackplane - A Framework for Supporting Multiple Application Architectures</a></p>
<p>17 0.13918889 <a title="1266-tfidf-17" href="../high_scalability-2013/high_scalability-2013-03-18-Beyond_Threads_and_Callbacks_-_Application_Architecture_Pros_and_Cons.html">1425 high scalability-2013-03-18-Beyond Threads and Callbacks - Application Architecture Pros and Cons</a></p>
<p>18 0.13826795 <a title="1266-tfidf-18" href="../high_scalability-2009/high_scalability-2009-03-16-Are_Cloud_Based_Memory_Architectures_the_Next_Big_Thing%3F.html">538 high scalability-2009-03-16-Are Cloud Based Memory Architectures the Next Big Thing?</a></p>
<p>19 0.13819784 <a title="1266-tfidf-19" href="../high_scalability-2013/high_scalability-2013-12-04-How_Can_Batching_Requests_Actually_Reduce_Latency%3F.html">1558 high scalability-2013-12-04-How Can Batching Requests Actually Reduce Latency?</a></p>
<p>20 0.13507476 <a title="1266-tfidf-20" href="../high_scalability-2010/high_scalability-2010-04-12-Poppen.de_Architecture.html">808 high scalability-2010-04-12-Poppen.de Architecture</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.24), (1, 0.167), (2, -0.027), (3, -0.01), (4, -0.033), (5, -0.007), (6, 0.144), (7, 0.13), (8, -0.092), (9, -0.037), (10, -0.006), (11, -0.022), (12, 0.013), (13, -0.037), (14, 0.012), (15, -0.002), (16, -0.021), (17, -0.038), (18, 0.059), (19, 0.009), (20, 0.087), (21, 0.028), (22, 0.068), (23, -0.052), (24, 0.007), (25, 0.064), (26, -0.008), (27, 0.073), (28, -0.034), (29, -0.034), (30, 0.098), (31, -0.05), (32, 0.05), (33, 0.044), (34, 0.039), (35, 0.055), (36, 0.032), (37, -0.06), (38, -0.1), (39, -0.02), (40, 0.06), (41, -0.041), (42, 0.017), (43, -0.072), (44, -0.037), (45, -0.058), (46, 0.06), (47, -0.065), (48, 0.049), (49, -0.068)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98528165 <a title="1266-lsi-1" href="../high_scalability-2012/high_scalability-2012-06-18-Google_on_Latency_Tolerant_Systems%3A_Making_a_Predictable_Whole_Out_of_Unpredictable_Parts___.html">1266 high scalability-2012-06-18-Google on Latency Tolerant Systems: Making a Predictable Whole Out of Unpredictable Parts   </a></p>
<p>Introduction: In   Taming The Long Latency Tail   we covered   Luiz Barroso  ’s exploration of the long tail latency (some operations are really slow) problems generated by large fanout architectures (a request is composed of potentially thousands of other requests). You may have noticed there weren’t a lot of solutions. That’s where a talk I attended,   Achieving Rapid Response Times in Large Online Services   (  slide deck  ), by  Jeff Dean , also of Google, comes in:
  
  In this talk, I’ll describe a collection of techniques and practices lowering response times in large distributed systems whose components run on shared clusters of machines, where pieces of these systems are subject to interference by other tasks, and where unpredictable latency hiccups are the norm, not the exception. 

  
 The goal is to use software techniques to reduce variability given the increasing variability in underlying hardware, the need to handle dynamic workloads on a shared infrastructure, and the need to use lar</p><p>2 0.88245577 <a title="1266-lsi-2" href="../high_scalability-2012/high_scalability-2012-03-12-Google%3A_Taming_the_Long_Latency_Tail_-_When_More_Machines_Equals_Worse_Results.html">1207 high scalability-2012-03-12-Google: Taming the Long Latency Tail - When More Machines Equals Worse Results</a></p>
<p>Introduction: Likewise the current belief that, in the case of artificial machines the very large and the very small are equally feasible and lasting is a manifest error. Thus, for example, a small obelisk or column or other solid figure can certainly be laid down or set up without danger of breaking, while the large ones will go to pieces under the slightest provocation, and that purely on account of their own weight. -- Galileo  
Galileo observed how things broke if they were naively scaled up. Interestingly, Google noticed a similar pattern when building larger software systems using the same techniques used to build smaller systems. 
 
 Luiz André Barroso , Distinguished Engineer at Google, talks about this fundamental property of scaling systems in his fascinating talk,  Warehouse-Scale Computing: Entering the Teenage Decade . Google found the larger the scale the greater the impact of latency variability. When a request is implemented by work done in parallel, as is common with today's service</p><p>3 0.86214286 <a title="1266-lsi-3" href="../high_scalability-2011/high_scalability-2011-02-01-Google_Strategy%3A_Tree_Distribution_of_Requests_and_Responses.html">981 high scalability-2011-02-01-Google Strategy: Tree Distribution of Requests and Responses</a></p>
<p>Introduction: If a large number of leaf node machines send requests to a central root node then that root node can become overwhelmed:
  
 The CPU becomes a bottleneck, for either processing requests or sending replies, because it can't possibly deal with the flood of requests. 
 The network interface becomes a bottleneck because a wide fan-in causes TCP drops and retransmissions, which causes latency. Then clients start retrying requests which quickly causes a spiral of death in an undisciplined system. 
  
One solution to this problem is a strategy given by Dr.  Jeff Dean , Head of Google's School of Infrastructure Wizardry, in this  Stanford video presentation :  Tree Distribution of Requests and Responses .
 
 
 
Instead of having a root node connected to leaves in a flat topology, the idea is to create a tree of nodes. So a root node talks to a number of parent nodes and the parent nodes talk to a number of leaf nodes. Requests are pushed down the tree through the parents and only hit a subset</p><p>4 0.83635843 <a title="1266-lsi-4" href="../high_scalability-2013/high_scalability-2013-12-04-How_Can_Batching_Requests_Actually_Reduce_Latency%3F.html">1558 high scalability-2013-12-04-How Can Batching Requests Actually Reduce Latency?</a></p>
<p>Introduction: Jeremy Edberg gave a talk on  Scaling Reddit from 1 Million to 1 Billion–Pitfalls and Lessons  and  one of the issues  they had was that they:
  

Did not account for increased latency after moving to EC2. In the datacenter they had submillisecond access between machines so it was possible to make a 1000 calls to memache for one page load. Not so on EC2. Memcache access times increased 10x to a millisecond which made their old approach unusable. Fix was to batch calls to memcache so a large number of gets are in one request.

  
Dave Pacheco had an  interesting question  about batching requests and its impact on latency:
  

 I was confused about the memcached problem after moving to the cloud.  I understand why network latency may have gone from submillisecond to milliseconds, but how could you improve latency by batching requests? Shouldn't that improve efficiency, not latency, at the possible expense of latency (since some requests will wait on the client as they get batched)?</p><p>5 0.83012921 <a title="1266-lsi-5" href="../high_scalability-2010/high_scalability-2010-11-22-Strategy%3A_Google_Sends_Canary_Requests_into_the_Data_Mine.html">946 high scalability-2010-11-22-Strategy: Google Sends Canary Requests into the Data Mine</a></p>
<p>Introduction: Google runs queries against thousands of in-memory index nodes in parallel and then merges the results. One of the interesting problems with this approach, explains Google's Jeff Dean in this  lecture at Stanford , is the  Query of Death .
 
A query can cause a program to fail because of bugs or various other issues. This means that a single query can take down an entire cluster of machines, which is not good for availability and response times, as it takes quite a while for thousands of machines to recover. Thus the Query of Death. New queries are always coming into the system and when you are always rolling out new software, it's impossible to completely get rid of the problem.
 
Two solutions:
  
  Test against logs . Google replays a month's worth of logs to see if any of those queries kill anything. That helps, but Queries of Death may still happen. 
  Send a canary request . A request is sent to one machine. If the request succeeds then it will probably succeedÂ on all machines, s</p><p>6 0.81076151 <a title="1266-lsi-6" href="../high_scalability-2009/high_scalability-2009-07-25-Latency_is_Everywhere_and_it_Costs_You_Sales_-_How_to_Crush_it.html">661 high scalability-2009-07-25-Latency is Everywhere and it Costs You Sales - How to Crush it</a></p>
<p>7 0.8064034 <a title="1266-lsi-7" href="../high_scalability-2013/high_scalability-2013-11-13-Google%3A_Multiplex_Multiple_Works_Loads_on_Computers_to_Increase_Machine_Utilization_and_Save_Money.html">1548 high scalability-2013-11-13-Google: Multiplex Multiple Works Loads on Computers to Increase Machine Utilization and Save Money</a></p>
<p>8 0.74430972 <a title="1266-lsi-8" href="../high_scalability-2013/high_scalability-2013-02-11-At_Scale_Even_Little_Wins_Pay_Off_Big_-_Google_and_Facebook_Examples.html">1404 high scalability-2013-02-11-At Scale Even Little Wins Pay Off Big - Google and Facebook Examples</a></p>
<p>9 0.73262429 <a title="1266-lsi-9" href="../high_scalability-2013/high_scalability-2013-02-27-42_Monster_Problems_that_Attack_as_Loads_Increase.html">1413 high scalability-2013-02-27-42 Monster Problems that Attack as Loads Increase</a></p>
<p>10 0.73056591 <a title="1266-lsi-10" href="../high_scalability-2010/high_scalability-2010-04-27-Paper%3A__Dapper%2C_Google%27s_Large-Scale_Distributed_Systems_Tracing_Infrastructure.html">815 high scalability-2010-04-27-Paper:  Dapper, Google's Large-Scale Distributed Systems Tracing Infrastructure</a></p>
<p>11 0.71661526 <a title="1266-lsi-11" href="../high_scalability-2009/high_scalability-2009-10-01-Moving_Beyond_End-to-End_Path_Information_to_Optimize_CDN_Performance.html">712 high scalability-2009-10-01-Moving Beyond End-to-End Path Information to Optimize CDN Performance</a></p>
<p>12 0.71248537 <a title="1266-lsi-12" href="../high_scalability-2014/high_scalability-2014-02-05-Little%E2%80%99s_Law%2C_Scalability_and_Fault_Tolerance%3A_The_OS_is_your_bottleneck._What_you_can_do%3F.html">1591 high scalability-2014-02-05-Little’s Law, Scalability and Fault Tolerance: The OS is your bottleneck. What you can do?</a></p>
<p>13 0.70909166 <a title="1266-lsi-13" href="../high_scalability-2008/high_scalability-2008-02-16-S3_Failed_Because_of_Authentication_Overload.html">249 high scalability-2008-02-16-S3 Failed Because of Authentication Overload</a></p>
<p>14 0.70624495 <a title="1266-lsi-14" href="../high_scalability-2009/high_scalability-2009-01-13-Product%3A_Gearman_-_Open_Source_Message_Queuing_System.html">491 high scalability-2009-01-13-Product: Gearman - Open Source Message Queuing System</a></p>
<p>15 0.70562953 <a title="1266-lsi-15" href="../high_scalability-2013/high_scalability-2013-03-11-Low_Level_Scalability_Solutions_-_The_Conditioning_Collection.html">1421 high scalability-2013-03-11-Low Level Scalability Solutions - The Conditioning Collection</a></p>
<p>16 0.70540017 <a title="1266-lsi-16" href="../high_scalability-2010/high_scalability-2010-02-05-High_Availability_Principle_%3A_Concurrency_Control.html">772 high scalability-2010-02-05-High Availability Principle : Concurrency Control</a></p>
<p>17 0.70517147 <a title="1266-lsi-17" href="../high_scalability-2008/high_scalability-2008-10-08-Strategy%3A_Flickr_-_Do_the_Essential_Work_Up-front_and_Queue_the_Rest_.html">406 high scalability-2008-10-08-Strategy: Flickr - Do the Essential Work Up-front and Queue the Rest </a></p>
<p>18 0.70398593 <a title="1266-lsi-18" href="../high_scalability-2010/high_scalability-2010-12-20-Netflix%3A_Use_Less_Chatty_Protocols_in_the_Cloud_-_Plus_26_Fixes.html">960 high scalability-2010-12-20-Netflix: Use Less Chatty Protocols in the Cloud - Plus 26 Fixes</a></p>
<p>19 0.69238007 <a title="1266-lsi-19" href="../high_scalability-2010/high_scalability-2010-11-15-Strategy%3A_Biggest_Performance_Impact_is_to_Reduce_the_Number_of_HTTP_Requests.html">942 high scalability-2010-11-15-Strategy: Biggest Performance Impact is to Reduce the Number of HTTP Requests</a></p>
<p>20 0.69108045 <a title="1266-lsi-20" href="../high_scalability-2012/high_scalability-2012-04-17-YouTube_Strategy%3A_Adding_Jitter_isn%27t_a_Bug.html">1229 high scalability-2012-04-17-YouTube Strategy: Adding Jitter isn't a Bug</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.082), (2, 0.256), (10, 0.079), (30, 0.035), (35, 0.123), (47, 0.027), (61, 0.094), (77, 0.021), (79, 0.139), (85, 0.023), (94, 0.045)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.95411694 <a title="1266-lda-1" href="../high_scalability-2009/high_scalability-2009-11-11-Hot_Scalability_Links_for_Nov_11_2009__.html">740 high scalability-2009-11-11-Hot Scalability Links for Nov 11 2009  </a></p>
<p>Introduction: The Cost of Latency  by James Hamilton. James summarizes latency info from  Steve Souder ,  Greg Linden , and  Marissa Mayer .  Speed [is] an undervalued and under-discussed asset on the web.  
  Dynamo - Part I: a followup and re-rebuttals . Dynamo under attack as having Design flaws and the resounding rebuttal in response. 
  Programming Bits and Atoms . Thinking about programming and scaling as a problem in physics. Absolutely fascinating and inspiring. 
  Scaling Servers with the Cloud: Amazon S3 . Build a static site using S3 for pennies. An oldly but still a goody idea. 
  Are Wireless Road Trains the Cure for Traffic Congestion?   The concept of road trains--up to eight vehicles zooming down the road together--has long been considered a faster, safer, and greener way of traveling long distances by car.  
  Erlang at Facebook  by Eugene Letuchy. How Facebook uses Erlang to implement Chat, AIM Presence, and Chat Jabber support.  
  Yahoo Open Sources Traffic Server .  Traffic Serv</p><p>same-blog 2 0.93872482 <a title="1266-lda-2" href="../high_scalability-2012/high_scalability-2012-06-18-Google_on_Latency_Tolerant_Systems%3A_Making_a_Predictable_Whole_Out_of_Unpredictable_Parts___.html">1266 high scalability-2012-06-18-Google on Latency Tolerant Systems: Making a Predictable Whole Out of Unpredictable Parts   </a></p>
<p>Introduction: In   Taming The Long Latency Tail   we covered   Luiz Barroso  ’s exploration of the long tail latency (some operations are really slow) problems generated by large fanout architectures (a request is composed of potentially thousands of other requests). You may have noticed there weren’t a lot of solutions. That’s where a talk I attended,   Achieving Rapid Response Times in Large Online Services   (  slide deck  ), by  Jeff Dean , also of Google, comes in:
  
  In this talk, I’ll describe a collection of techniques and practices lowering response times in large distributed systems whose components run on shared clusters of machines, where pieces of these systems are subject to interference by other tasks, and where unpredictable latency hiccups are the norm, not the exception. 

  
 The goal is to use software techniques to reduce variability given the increasing variability in underlying hardware, the need to handle dynamic workloads on a shared infrastructure, and the need to use lar</p><p>3 0.92885005 <a title="1266-lda-3" href="../high_scalability-2014/high_scalability-2014-05-14-Google_Says_Cloud_Prices_Will_Follow_Moore%E2%80%99s_Law%3A_Are_We_All_Renters_Now%3F.html">1647 high scalability-2014-05-14-Google Says Cloud Prices Will Follow Moore’s Law: Are We All Renters Now?</a></p>
<p>Introduction: After Google  cut prices  on their Google Cloud Platform Amazon quickly followed with their own  price cuts . Even more interesting is what the future holds for pricing. The near future looks great. After that? We'll see.
 
Adrian Cockcroft highlights that Google thinks  prices should follow  Moore’s law, which means we should expect prices to halve every 18-24 months.
 
That's good news. Greater cost certainty means you can make much more aggressive build out plans. With the savings you can hire more people, handle more customers, and add those media rich features you thought you couldn't afford. Design is directly related to costs.
 
Without Google competing with Amazon there's little doubt the price reduction curve would be much less favorable.
 
As a late cloud entrant Google is now in a customer acquisition phase, so they are willing to pay for customers, which means lower prices are an acceptable cost of doing business. Profit and high margins are not the objective. Getting marke</p><p>4 0.92728782 <a title="1266-lda-4" href="../high_scalability-2011/high_scalability-2011-12-08-Update_on_Scalable_Causal_Consistency_For_Wide-Area_Storage_With_COPS.html">1153 high scalability-2011-12-08-Update on Scalable Causal Consistency For Wide-Area Storage With COPS</a></p>
<p>Introduction: Here are a few updates on the article  Paper: Don’t Settle For Eventual: Scalable Causal Consistency For Wide-Area Storage With COPS  from Mike Freedman and Wyatt Lloyd.
 
 Q: How software architectures could change in response to casual+ consistency? 
 
 A : I don't really think they would much. Somebody would still run a two-tier architecture in their datacenter:  a front-tier of webservers running both (say) PHP and our client library, and a back tier of storage nodes running COPS.  (I'm not sure if it was obvious given the discussion of our "thick" client -- you should think of the COPS client dropping in where a memcache client library does...albeit ours has per-session state.)
 
 
 
 Q: Why not just use vector clocks? 
 
 A : The problem with vector clocks and scalability has always been that the size of vector clocks in O(N), where N is the number of nodes.  So if we want to scale to a datacenter with 10K nodes, each piece of metadata must have size O(10K).  And in fact, vector</p><p>5 0.92447132 <a title="1266-lda-5" href="../high_scalability-2011/high_scalability-2011-09-07-What_Google_App_Engine_Price_Changes_Say_About_the_Future_of_Web_Architecture.html">1112 high scalability-2011-09-07-What Google App Engine Price Changes Say About the Future of Web Architecture</a></p>
<p>Introduction: When I was a child, I spake as a child, I understood as a child, I thought as a child: but when I became a man, I put away childish things . -- Corinthians
 
 With this new pricing, developments will be driven by the costs .  I like to optimize my apps to make them better or faster, but to optimize them just to make them cheaper is a waste of time.  -- Sylvain on  Google Groups 
 
The dream is dead. Google App Engine's bold  pay for what you use  dream dies as it leaves childish things behind and becomes a  real product .  Pricing will change . Architectures will change. Customers will change. Hearts and minds will change. But Google App Engine  will survive.   
 
Google is  shutting down  many of its  projects . GAE is not among them. Do we have GAE's pricing change to thank for it surving the  more wood behind  more deadly arrows push? Without a radical and quick shift towards profitably GAE would no doubt be a historical footnote in the long scroll of good ideas. The urgency involve</p><p>6 0.92353445 <a title="1266-lda-6" href="../high_scalability-2012/high_scalability-2012-03-12-Google%3A_Taming_the_Long_Latency_Tail_-_When_More_Machines_Equals_Worse_Results.html">1207 high scalability-2012-03-12-Google: Taming the Long Latency Tail - When More Machines Equals Worse Results</a></p>
<p>7 0.92332071 <a title="1266-lda-7" href="../high_scalability-2010/high_scalability-2010-02-19-Twitter%E2%80%99s_Plan_to_Analyze_100_Billion_Tweets.html">780 high scalability-2010-02-19-Twitter’s Plan to Analyze 100 Billion Tweets</a></p>
<p>8 0.92327309 <a title="1266-lda-8" href="../high_scalability-2011/high_scalability-2011-01-10-Riak%27s_Bitcask_-_A_Log-Structured_Hash_Table_for_Fast_Key-Value_Data.html">971 high scalability-2011-01-10-Riak's Bitcask - A Log-Structured Hash Table for Fast Key-Value Data</a></p>
<p>9 0.9209398 <a title="1266-lda-9" href="../high_scalability-2011/high_scalability-2011-03-24-Strategy%3A_Disk_Backup_for_Speed%2C_Tape_Backup_to_Save_Your_Bacon%2C_Just_Ask_Google.html">1010 high scalability-2011-03-24-Strategy: Disk Backup for Speed, Tape Backup to Save Your Bacon, Just Ask Google</a></p>
<p>10 0.92000246 <a title="1266-lda-10" href="../high_scalability-2013/high_scalability-2013-04-12-Stuff_The_Internet_Says_On_Scalability_For_April_12%2C_2013.html">1439 high scalability-2013-04-12-Stuff The Internet Says On Scalability For April 12, 2013</a></p>
<p>11 0.91937745 <a title="1266-lda-11" href="../high_scalability-2012/high_scalability-2012-07-25-Vertical_Scaling_Ascendant_-_How_are_SSDs_Changing__Architectures%3F.html">1291 high scalability-2012-07-25-Vertical Scaling Ascendant - How are SSDs Changing  Architectures?</a></p>
<p>12 0.9185673 <a title="1266-lda-12" href="../high_scalability-2009/high_scalability-2009-07-25-Latency_is_Everywhere_and_it_Costs_You_Sales_-_How_to_Crush_it.html">661 high scalability-2009-07-25-Latency is Everywhere and it Costs You Sales - How to Crush it</a></p>
<p>13 0.91855752 <a title="1266-lda-13" href="../high_scalability-2007/high_scalability-2007-08-29-Skype_Failed_the_Boot_Scalability_Test%3A_Is_P2P_fundamentally_flawed%3F.html">76 high scalability-2007-08-29-Skype Failed the Boot Scalability Test: Is P2P fundamentally flawed?</a></p>
<p>14 0.91852498 <a title="1266-lda-14" href="../high_scalability-2013/high_scalability-2013-07-15-Ask_HS%3A_What%27s_Wrong_with_Twitter%2C_Why_Isn%27t_One_Machine_Enough%3F.html">1491 high scalability-2013-07-15-Ask HS: What's Wrong with Twitter, Why Isn't One Machine Enough?</a></p>
<p>15 0.91841507 <a title="1266-lda-15" href="../high_scalability-2008/high_scalability-2008-04-21-The_Search_for_the_Source_of_Data_-_How_SimpleDB_Differs_from_a_RDBMS.html">306 high scalability-2008-04-21-The Search for the Source of Data - How SimpleDB Differs from a RDBMS</a></p>
<p>16 0.91788006 <a title="1266-lda-16" href="../high_scalability-2009/high_scalability-2009-03-11-The_Implications_of_Punctuated_Scalabilium_for_Website_Architecture.html">533 high scalability-2009-03-11-The Implications of Punctuated Scalabilium for Website Architecture</a></p>
<p>17 0.91689926 <a title="1266-lda-17" href="../high_scalability-2013/high_scalability-2013-05-17-Stuff_The_Internet_Says_On_Scalability_For_May_17%2C_2013.html">1460 high scalability-2013-05-17-Stuff The Internet Says On Scalability For May 17, 2013</a></p>
<p>18 0.9163903 <a title="1266-lda-18" href="../high_scalability-2009/high_scalability-2009-06-05-HotPads_Shows_the_True_Cost_of_Hosting_on_Amazon.html">619 high scalability-2009-06-05-HotPads Shows the True Cost of Hosting on Amazon</a></p>
<p>19 0.91570556 <a title="1266-lda-19" href="../high_scalability-2013/high_scalability-2013-06-13-Busting_4_Modern_Hardware_Myths_-_Are_Memory%2C_HDDs%2C_and_SSDs_Really_Random_Access%3F.html">1475 high scalability-2013-06-13-Busting 4 Modern Hardware Myths - Are Memory, HDDs, and SSDs Really Random Access?</a></p>
<p>20 0.91523033 <a title="1266-lda-20" href="../high_scalability-2007/high_scalability-2007-10-10-WAN_Accelerate_Your_Way_to_Lightening_Fast_Transfers_Between_Data_Centers.html">119 high scalability-2007-10-10-WAN Accelerate Your Way to Lightening Fast Transfers Between Data Centers</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
