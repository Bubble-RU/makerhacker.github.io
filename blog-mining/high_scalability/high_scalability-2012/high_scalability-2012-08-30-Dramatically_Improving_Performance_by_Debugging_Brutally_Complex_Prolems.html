<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1314 high scalability-2012-08-30-Dramatically Improving Performance by Debugging Brutally Complex Prolems</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2012" href="../home/high_scalability-2012_home.html">high_scalability-2012</a> <a title="high_scalability-2012-1314" href="#">high_scalability-2012-1314</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1314 high scalability-2012-08-30-Dramatically Improving Performance by Debugging Brutally Complex Prolems</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2012-1314-html" href="http://highscalability.com//blog/2012/8/30/dramatically-improving-performance-by-debugging-brutally-com.html">html</a></p><p>Introduction: Debugging complex problems is 90% persistence and 50% cool tools. Brendan Gregg in  10 Performance Wins  tells a fascinating story of how a team at Joyent solved some weird and challenging performance issues deep in the OS. It took lots of effort,  DTrace ,  Flame Graphs ,  USE Method , and writing custom tools when necessary. Here's a quick summary of the solved cases:
  
  Monitoring. 1000x improvement . An application blocked while paging anonymous memory back in. It was also blocked during file system fsync() calls. The application was misconfigured and sometimes briefly exceeded available memory, getting page out. 
  Riak. 2x improvement . The Erlang VM used half the CPU count it was supposed to, so CPUs remained unused.  Fix was a configuration change. 
  MySQL. 380x improvement . Reads were slow. Cause was correlated writes. Fix was to tune the cache flush interval on the storage controller. 
  Various. 2800x improvement . Large systems calls to getvmusage() could take a few sec</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Brendan Gregg in  10 Performance Wins  tells a fascinating story of how a team at Joyent solved some weird and challenging performance issues deep in the OS. [sent-2, score-0.22]
</p><p>2 Here's a quick summary of the solved cases:      Monitoring. [sent-4, score-0.124]
</p><p>3 An application blocked while paging anonymous memory back in. [sent-6, score-0.539]
</p><p>4 It was also blocked during file system fsync() calls. [sent-7, score-0.275]
</p><p>5 The application was misconfigured and sometimes briefly exceeded available memory, getting page out. [sent-8, score-0.33]
</p><p>6 The Erlang VM used half the CPU count it was supposed to, so CPUs remained unused. [sent-11, score-0.178]
</p><p>7 Fix was to tune the cache flush interval on the storage controller. [sent-17, score-0.2]
</p><p>8 Cause was a priority inversion that caused packets not to be processed. [sent-21, score-0.497]
</p><p>9 Cause was a kernel function that had become expensive. [sent-27, score-0.132]
</p><p>10 Mutex connection for malloc from multiple threads slowed down performance. [sent-31, score-0.211]
</p><p>11 Calls to   getvmusage() held an address space lock while bitcask blocked on mmap. [sent-39, score-0.467]
</p><p>12 This causes TCP listen drops and slow query responses. [sent-40, score-0.156]
</p><p>13 Programs would not get the expected portion of CPU because the scheduler wasn't adjusting priorities fast enough. [sent-44, score-0.373]
</p><p>14 Slow network performance was traced to packets not being coalesced. [sent-48, score-0.242]
</p><p>15 The article contains many juicy details, but the take home is a general process for dramatically improving performance by debugging brutally complex problems. [sent-50, score-0.429]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('fix', 0.401), ('getvmusage', 0.292), ('blocked', 0.275), ('debugging', 0.143), ('cause', 0.141), ('packets', 0.137), ('adjusting', 0.133), ('kernel', 0.132), ('priority', 0.129), ('brutally', 0.125), ('misconfigured', 0.125), ('preempt', 0.125), ('solved', 0.124), ('caused', 0.12), ('flame', 0.119), ('bitcask', 0.119), ('fsync', 0.119), ('malloc', 0.114), ('inversion', 0.111), ('exceeded', 0.108), ('gregg', 0.108), ('correlated', 0.108), ('remained', 0.105), ('traced', 0.105), ('brendan', 0.105), ('flush', 0.103), ('slowed', 0.097), ('interval', 0.097), ('fragmentation', 0.097), ('briefly', 0.097), ('weird', 0.096), ('changing', 0.094), ('paging', 0.094), ('mutex', 0.093), ('anonymous', 0.093), ('cpu', 0.092), ('endless', 0.09), ('juicy', 0.089), ('priorities', 0.084), ('heap', 0.081), ('portion', 0.08), ('drops', 0.079), ('memory', 0.077), ('listen', 0.077), ('scheduler', 0.076), ('wins', 0.076), ('supposed', 0.073), ('held', 0.073), ('joyent', 0.073), ('dramatically', 0.072)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="1314-tfidf-1" href="../high_scalability-2012/high_scalability-2012-08-30-Dramatically_Improving_Performance_by_Debugging_Brutally_Complex_Prolems.html">1314 high scalability-2012-08-30-Dramatically Improving Performance by Debugging Brutally Complex Prolems</a></p>
<p>Introduction: Debugging complex problems is 90% persistence and 50% cool tools. Brendan Gregg in  10 Performance Wins  tells a fascinating story of how a team at Joyent solved some weird and challenging performance issues deep in the OS. It took lots of effort,  DTrace ,  Flame Graphs ,  USE Method , and writing custom tools when necessary. Here's a quick summary of the solved cases:
  
  Monitoring. 1000x improvement . An application blocked while paging anonymous memory back in. It was also blocked during file system fsync() calls. The application was misconfigured and sometimes briefly exceeded available memory, getting page out. 
  Riak. 2x improvement . The Erlang VM used half the CPU count it was supposed to, so CPUs remained unused.  Fix was a configuration change. 
  MySQL. 380x improvement . Reads were slow. Cause was correlated writes. Fix was to tune the cache flush interval on the storage controller. 
  Various. 2800x improvement . Large systems calls to getvmusage() could take a few sec</p><p>2 0.16185446 <a title="1314-tfidf-2" href="../high_scalability-2013/high_scalability-2013-03-18-Beyond_Threads_and_Callbacks_-_Application_Architecture_Pros_and_Cons.html">1425 high scalability-2013-03-18-Beyond Threads and Callbacks - Application Architecture Pros and Cons</a></p>
<p>Introduction: There's not a lot  of talk about application architectures at the process level. You have your threads, pools of threads, and you have your callback models. That's about it. Languages/frameworks making a virtue out of simple models, like Go and Erlang, do so at the price of control. It's difficult to make a low latency well conditioned application when a power full tool, like work scheduling, is taken out of the hands of the programmer.
 
But that's not all there is my friend. We'll dive into different ways an application can be composed across threads of control.
 
Your favorite language may not give you access to all the capabilities we are going to talk about, but lately there has been a sort of revival in considering performance important, especially for controlling  latency variance , so I think it's time to talk about these kind of issues. When it was do everything in the thread of a web server thread pool none of these issues really mattered. But now that developers are creating</p><p>3 0.14582251 <a title="1314-tfidf-3" href="../high_scalability-2013/high_scalability-2013-02-27-42_Monster_Problems_that_Attack_as_Loads_Increase.html">1413 high scalability-2013-02-27-42 Monster Problems that Attack as Loads Increase</a></p>
<p>Introduction: For solutions take a look at:  7 Life Saving Scalability Defenses Against Load Monster Attacks . 
 
This is a look at all the bad things that can happen to your carefully crafted program as loads increase: all hell breaks lose. Sure, you can scale out or scale up, but you can also choose to program better. Make your system handle larger loads. This saves money because fewer boxes are needed and it will make the entire application more reliable and have better response times. And it can be quite satisfying as a programmer.
  Large Number Of Objects  
We usually get into scaling problems when the number of objects gets  larger. Clearly resource usage of all types is stressed as the number of objects grow.
  Continuous Failures Makes An Infinite Event Stream  
During large network failure scenarios there is never time for the system recover. We are in a continual state of stress.
  Lots of High Priority Work  
For example, rerouting is a high priority activity. If there is a large amount</p><p>4 0.13928795 <a title="1314-tfidf-4" href="../high_scalability-2013/high_scalability-2013-05-13-The_Secret_to_10_Million_Concurrent_Connections_-The_Kernel_is_the_Problem%2C_Not_the_Solution.html">1456 high scalability-2013-05-13-The Secret to 10 Million Concurrent Connections -The Kernel is the Problem, Not the Solution</a></p>
<p>Introduction: Now that we have the    C10K concurrent connection problem    licked, how do we level up and support 10 million concurrent connections? Impossible you say. Nope, systems right now are delivering 10 million concurrent connections using techniques that are as radical as they may be unfamiliar. 
   To learn how it’s done we turn to    Robert Graham   , CEO of Errata Security, and his absolutely fantastic talk at    Shmoocon 2013    called    C10M Defending The Internet At Scale   . 
  Robert has a brilliant way of framing the problem that I’ve never heard of before. He starts with a little bit of history, relating how Unix wasn’t originally designed to be a general server OS, it was designed to be a control system for a telephone network. It was the telephone network that actually transported the data so there was a clean separation between the control plane and the data plane. The  problem is we now use Unix servers as part of the data plane , which we shouldn’t do at all. If we were des</p><p>5 0.12065151 <a title="1314-tfidf-5" href="../high_scalability-2013/high_scalability-2013-03-25-AppBackplane_-_A_Framework_for_Supporting_Multiple_Application_Architectures.html">1429 high scalability-2013-03-25-AppBackplane - A Framework for Supporting Multiple Application Architectures</a></p>
<p>Introduction: Hidden in every computer is a hardware backplane for moving signals around. Hidden in every application are ways of moving messages around and giving code CPU time to process them. Unhiding those capabilities and making them first class facilities for the programmer to control is the idea behind AppBackplane.
 
This goes directly against the trend of hiding everything from the programmer and doing it all automagically. Which is great, until it doesn't work. Then it sucks. And the approach of giving the programmer all the power also sucks, until it's tuned to work together and performance is incredible even under increasing loads. Then it's great.
 
These are two different curves going in opposite directions. You need to decide for your application which curve you need to be on.
 
AppBackplane is an example framework supporting the multiple  application architectures we talked about in  Beyond Threads And Callbacks .  It provides a scheduling system that supports continuous and high loa</p><p>6 0.1091243 <a title="1314-tfidf-6" href="../high_scalability-2011/high_scalability-2011-06-01-Why_is_your_network_so_slow%3F_Your_switch_should_tell_you..html">1051 high scalability-2011-06-01-Why is your network so slow? Your switch should tell you.</a></p>
<p>7 0.10728183 <a title="1314-tfidf-7" href="../high_scalability-2011/high_scalability-2011-01-10-Riak%27s_Bitcask_-_A_Log-Structured_Hash_Table_for_Fast_Key-Value_Data.html">971 high scalability-2011-01-10-Riak's Bitcask - A Log-Structured Hash Table for Fast Key-Value Data</a></p>
<p>8 0.10587927 <a title="1314-tfidf-8" href="../high_scalability-2010/high_scalability-2010-10-15-Troubles_with_Sharding_-_What_can_we_learn_from_the_Foursquare_Incident%3F.html">920 high scalability-2010-10-15-Troubles with Sharding - What can we learn from the Foursquare Incident?</a></p>
<p>9 0.099479616 <a title="1314-tfidf-9" href="../high_scalability-2013/high_scalability-2013-04-10-Check_Yourself_Before_You_Wreck_Yourself_-_Avocado%27s_5_Early_Stages_of_Architecture_Evolution.html">1438 high scalability-2013-04-10-Check Yourself Before You Wreck Yourself - Avocado's 5 Early Stages of Architecture Evolution</a></p>
<p>10 0.098122038 <a title="1314-tfidf-10" href="../high_scalability-2009/high_scalability-2009-08-11-13_Scalability_Best_Practices.html">679 high scalability-2009-08-11-13 Scalability Best Practices</a></p>
<p>11 0.097473159 <a title="1314-tfidf-11" href="../high_scalability-2007/high_scalability-2007-12-21-Strategy%3A_Limit_Result_Sets.html">189 high scalability-2007-12-21-Strategy: Limit Result Sets</a></p>
<p>12 0.09350846 <a title="1314-tfidf-12" href="../high_scalability-2012/high_scalability-2012-05-16-Big_List_of_20_Common_Bottlenecks.html">1246 high scalability-2012-05-16-Big List of 20 Common Bottlenecks</a></p>
<p>13 0.090489529 <a title="1314-tfidf-13" href="../high_scalability-2008/high_scalability-2008-12-13-Strategy%3A_Facebook_Tweaks_to_Handle_6_Time_as_Many_Memcached_Requests.html">464 high scalability-2008-12-13-Strategy: Facebook Tweaks to Handle 6 Time as Many Memcached Requests</a></p>
<p>14 0.088523187 <a title="1314-tfidf-14" href="../high_scalability-2011/high_scalability-2011-05-11-Troubleshooting_response_time_problems_%E2%80%93_why_you_cannot_trust_your_system_metrics.html">1038 high scalability-2011-05-11-Troubleshooting response time problems – why you cannot trust your system metrics</a></p>
<p>15 0.086595125 <a title="1314-tfidf-15" href="../high_scalability-2012/high_scalability-2012-01-13-Stuff_The_Internet_Says_On_Scalability_For_January_13%2C_2012.html">1174 high scalability-2012-01-13-Stuff The Internet Says On Scalability For January 13, 2012</a></p>
<p>16 0.085920185 <a title="1314-tfidf-16" href="../high_scalability-2012/high_scalability-2012-10-04-Stuff_The_Internet_Says_On_Scalability_For_October_5%2C_2012.html">1334 high scalability-2012-10-04-Stuff The Internet Says On Scalability For October 5, 2012</a></p>
<p>17 0.084764138 <a title="1314-tfidf-17" href="../high_scalability-2012/high_scalability-2012-11-26-BigData_using_Erlang%2C_C_and_Lisp_to_Fight_the_Tsunami_of_Mobile_Data.html">1362 high scalability-2012-11-26-BigData using Erlang, C and Lisp to Fight the Tsunami of Mobile Data</a></p>
<p>18 0.083637029 <a title="1314-tfidf-18" href="../high_scalability-2012/high_scalability-2012-03-14-The_Azure_Outage%3A_Time_Is_a_SPOF%2C_Leap_Day_Doubly_So.html">1209 high scalability-2012-03-14-The Azure Outage: Time Is a SPOF, Leap Day Doubly So</a></p>
<p>19 0.083288707 <a title="1314-tfidf-19" href="../high_scalability-2011/high_scalability-2011-03-14-6_Lessons_from_Dropbox_-_One_Million_Files_Saved_Every_15_minutes.html">1003 high scalability-2011-03-14-6 Lessons from Dropbox - One Million Files Saved Every 15 minutes</a></p>
<p>20 0.081831291 <a title="1314-tfidf-20" href="../high_scalability-2008/high_scalability-2008-02-16-S3_Failed_Because_of_Authentication_Overload.html">249 high scalability-2008-02-16-S3 Failed Because of Authentication Overload</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.117), (1, 0.078), (2, -0.014), (3, -0.012), (4, 0.004), (5, 0.015), (6, 0.064), (7, 0.076), (8, -0.081), (9, -0.04), (10, -0.012), (11, -0.029), (12, 0.035), (13, 0.004), (14, -0.056), (15, -0.045), (16, 0.034), (17, -0.008), (18, -0.033), (19, 0.015), (20, 0.016), (21, -0.032), (22, 0.022), (23, 0.034), (24, 0.032), (25, 0.036), (26, -0.001), (27, 0.014), (28, 0.028), (29, 0.033), (30, 0.036), (31, -0.033), (32, 0.059), (33, 0.002), (34, 0.051), (35, 0.057), (36, -0.035), (37, 0.058), (38, 0.005), (39, 0.022), (40, -0.056), (41, -0.021), (42, 0.026), (43, 0.05), (44, 0.032), (45, -0.018), (46, -0.003), (47, 0.036), (48, 0.009), (49, 0.05)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96968782 <a title="1314-lsi-1" href="../high_scalability-2012/high_scalability-2012-08-30-Dramatically_Improving_Performance_by_Debugging_Brutally_Complex_Prolems.html">1314 high scalability-2012-08-30-Dramatically Improving Performance by Debugging Brutally Complex Prolems</a></p>
<p>Introduction: Debugging complex problems is 90% persistence and 50% cool tools. Brendan Gregg in  10 Performance Wins  tells a fascinating story of how a team at Joyent solved some weird and challenging performance issues deep in the OS. It took lots of effort,  DTrace ,  Flame Graphs ,  USE Method , and writing custom tools when necessary. Here's a quick summary of the solved cases:
  
  Monitoring. 1000x improvement . An application blocked while paging anonymous memory back in. It was also blocked during file system fsync() calls. The application was misconfigured and sometimes briefly exceeded available memory, getting page out. 
  Riak. 2x improvement . The Erlang VM used half the CPU count it was supposed to, so CPUs remained unused.  Fix was a configuration change. 
  MySQL. 380x improvement . Reads were slow. Cause was correlated writes. Fix was to tune the cache flush interval on the storage controller. 
  Various. 2800x improvement . Large systems calls to getvmusage() could take a few sec</p><p>2 0.72453141 <a title="1314-lsi-2" href="../high_scalability-2013/high_scalability-2013-10-23-Strategy%3A_Use_Linux_Taskset_to_Pin_Processes_or_Let_the_OS_Schedule_It%3F.html">1536 high scalability-2013-10-23-Strategy: Use Linux Taskset to Pin Processes or Let the OS Schedule It?</a></p>
<p>Introduction: This question comes from Ulysses on an  interesting thread  from the Mechanical Sympathy news group, especially given how multiple processors are now the norm:
 
Ulysses:
   
 On an 8xCPU Linux instance,  is it at all advantageous to use the Linux taskset command to pin an 8xJVM process set (co-ordinated as a www.infinispan.org distributed cache/data grid) to a specific CPU affinity set  (i.e. pin JVM0 process to CPU 0, JVM1 process to CPU1, ...., JVM7process to CPU 7) vs. just letting the Linux OS use its default mechanism for provisioning the 8xJVM process set to the available CPUs? 
 In effrort to seek an optimal point (in the full event space), what are the conceptual trade-offs in considering "searching" each permutation of provisioning an 8xJVM process set to an 8xCPU set via taskset? 
   
Given  taskset  is they key to the question, it would help to have a definition:
  

Used to set or retrieve the CPU affinity of a running process given its PID or to launch a new COMMAND with</p><p>3 0.7157743 <a title="1314-lsi-3" href="../high_scalability-2012/high_scalability-2012-03-29-Strategy%3A_Exploit_Processor_Affinity_for_High_and_Predictable_Performance.html">1218 high scalability-2012-03-29-Strategy: Exploit Processor Affinity for High and Predictable Performance</a></p>
<p>Introduction: Martin Thompson wrote a really interesting  article  on the beneficial performance impact of taking advantage of  Processor Affinity :
  

The interesting thing I've observed is that the unpinned test will follow a step function of unpredictable performance.  Across many runs I've seen different patterns but all similar in this step function nature.  For the pinned tests I get consistent throughput with no step pattern and always the greatest throughput.

  
The idea is by assigning a thread to a particular CPU that when a thread is rescheduled to run on the same CPU, it can take advantage of the "accumulated  state in the processor, including instructions and data in the cache."  With multi-core chips the norm now, you may want to decide for yourself how to assign work to cores and not let the OS do it for you. The results are surprisingly strong.</p><p>4 0.71001858 <a title="1314-lsi-4" href="../high_scalability-2013/high_scalability-2013-05-13-The_Secret_to_10_Million_Concurrent_Connections_-The_Kernel_is_the_Problem%2C_Not_the_Solution.html">1456 high scalability-2013-05-13-The Secret to 10 Million Concurrent Connections -The Kernel is the Problem, Not the Solution</a></p>
<p>Introduction: Now that we have the    C10K concurrent connection problem    licked, how do we level up and support 10 million concurrent connections? Impossible you say. Nope, systems right now are delivering 10 million concurrent connections using techniques that are as radical as they may be unfamiliar. 
   To learn how it’s done we turn to    Robert Graham   , CEO of Errata Security, and his absolutely fantastic talk at    Shmoocon 2013    called    C10M Defending The Internet At Scale   . 
  Robert has a brilliant way of framing the problem that I’ve never heard of before. He starts with a little bit of history, relating how Unix wasn’t originally designed to be a general server OS, it was designed to be a control system for a telephone network. It was the telephone network that actually transported the data so there was a clean separation between the control plane and the data plane. The  problem is we now use Unix servers as part of the data plane , which we shouldn’t do at all. If we were des</p><p>5 0.69879031 <a title="1314-lsi-5" href="../high_scalability-2011/high_scalability-2011-05-11-Troubleshooting_response_time_problems_%E2%80%93_why_you_cannot_trust_your_system_metrics.html">1038 high scalability-2011-05-11-Troubleshooting response time problems – why you cannot trust your system metrics</a></p>
<p>Introduction: Production Monitoring  is about ensuring the stability and health of our system, that also  includes the application. A lot of times we encounter production systems  that concentrate on System Monitoring, under the assumption that a  stable system leads to stable and healthy applications. So let’s see  what System Monitoring can tell us about our  Application .
 
Let’s take a very simple two tier Web Application:
        
This is a simple multi-tier eCommerce solution. Users are concerned  about bad performance when they do a search. Let's see what we can find  out about it if performance is not satisfactory. We start by looking at a  couple of simple metrics.
  CPU Utilization  
The best known operating system metric is CPU utilization, but it is  also the most misunderstood. This metric tells us how much time the CPU  spent executing code in the last interval and how much more it could  execute theoretically. Like all other utilization measures it tells us  something about the capaci</p><p>6 0.69657338 <a title="1314-lsi-6" href="../high_scalability-2012/high_scalability-2012-05-02-12_Ways_to_Increase_Throughput_by_32X_and_Reduce_Latency_by__20X.html">1237 high scalability-2012-05-02-12 Ways to Increase Throughput by 32X and Reduce Latency by  20X</a></p>
<p>7 0.69409859 <a title="1314-lsi-7" href="../high_scalability-2014/high_scalability-2014-05-21-9_Principles_of_High_Performance_Programs.html">1652 high scalability-2014-05-21-9 Principles of High Performance Programs</a></p>
<p>8 0.68548054 <a title="1314-lsi-8" href="../high_scalability-2013/high_scalability-2013-02-27-42_Monster_Problems_that_Attack_as_Loads_Increase.html">1413 high scalability-2013-02-27-42 Monster Problems that Attack as Loads Increase</a></p>
<p>9 0.67965174 <a title="1314-lsi-9" href="../high_scalability-2012/high_scalability-2012-09-10-Russ%E2%80%99_10_Ingredient_Recipe_for_Making_1_Million_TPS_on_%245K_Hardware.html">1319 high scalability-2012-09-10-Russ’ 10 Ingredient Recipe for Making 1 Million TPS on $5K Hardware</a></p>
<p>10 0.67751122 <a title="1314-lsi-10" href="../high_scalability-2010/high_scalability-2010-07-14-DynaTrace%27s_Top_10_Performance_Problems_taken_from_Zappos%2C_Monster%2C_Thomson_and_Co.html">859 high scalability-2010-07-14-DynaTrace's Top 10 Performance Problems taken from Zappos, Monster, Thomson and Co</a></p>
<p>11 0.66896021 <a title="1314-lsi-11" href="../high_scalability-2011/high_scalability-2011-03-14-6_Lessons_from_Dropbox_-_One_Million_Files_Saved_Every_15_minutes.html">1003 high scalability-2011-03-14-6 Lessons from Dropbox - One Million Files Saved Every 15 minutes</a></p>
<p>12 0.66165614 <a title="1314-lsi-12" href="../high_scalability-2012/high_scalability-2012-05-16-Big_List_of_20_Common_Bottlenecks.html">1246 high scalability-2012-05-16-Big List of 20 Common Bottlenecks</a></p>
<p>13 0.65495133 <a title="1314-lsi-13" href="../high_scalability-2013/high_scalability-2013-06-06-Paper%3A_Memory_Barriers%3A_a_Hardware_View_for_Software_Hackers.html">1471 high scalability-2013-06-06-Paper: Memory Barriers: a Hardware View for Software Hackers</a></p>
<p>14 0.65435475 <a title="1314-lsi-14" href="../high_scalability-2013/high_scalability-2013-06-13-Busting_4_Modern_Hardware_Myths_-_Are_Memory%2C_HDDs%2C_and_SSDs_Really_Random_Access%3F.html">1475 high scalability-2013-06-13-Busting 4 Modern Hardware Myths - Are Memory, HDDs, and SSDs Really Random Access?</a></p>
<p>15 0.65370858 <a title="1314-lsi-15" href="../high_scalability-2011/high_scalability-2011-06-01-Why_is_your_network_so_slow%3F_Your_switch_should_tell_you..html">1051 high scalability-2011-06-01-Why is your network so slow? Your switch should tell you.</a></p>
<p>16 0.64490128 <a title="1314-lsi-16" href="../high_scalability-2013/high_scalability-2013-03-18-Beyond_Threads_and_Callbacks_-_Application_Architecture_Pros_and_Cons.html">1425 high scalability-2013-03-18-Beyond Threads and Callbacks - Application Architecture Pros and Cons</a></p>
<p>17 0.64044875 <a title="1314-lsi-17" href="../high_scalability-2014/high_scalability-2014-05-07-Update_on_Disqus%3A_It%27s_Still_About_Realtime%2C_But_Go_Demolishes_Python.html">1644 high scalability-2014-05-07-Update on Disqus: It's Still About Realtime, But Go Demolishes Python</a></p>
<p>18 0.61765164 <a title="1314-lsi-18" href="../high_scalability-2013/high_scalability-2013-12-23-What_Happens_While_Your_Brain_Sleeps_is_Surprisingly_Like_How_Computers_Stay_Sane.html">1568 high scalability-2013-12-23-What Happens While Your Brain Sleeps is Surprisingly Like How Computers Stay Sane</a></p>
<p>19 0.6152457 <a title="1314-lsi-19" href="../high_scalability-2013/high_scalability-2013-03-25-AppBackplane_-_A_Framework_for_Supporting_Multiple_Application_Architectures.html">1429 high scalability-2013-03-25-AppBackplane - A Framework for Supporting Multiple Application Architectures</a></p>
<p>20 0.60682011 <a title="1314-lsi-20" href="../high_scalability-2014/high_scalability-2014-01-20-8_Ways_Stardog_Made_its_Database_Insanely_Scalable.html">1582 high scalability-2014-01-20-8 Ways Stardog Made its Database Insanely Scalable</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.121), (2, 0.197), (10, 0.105), (40, 0.04), (59, 0.299), (61, 0.026), (79, 0.08), (85, 0.042)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.89310497 <a title="1314-lda-1" href="../high_scalability-2009/high_scalability-2009-03-11-13_Screencasts_on_How_to_Scale_Rails.html">530 high scalability-2009-03-11-13 Screencasts on How to Scale Rails</a></p>
<p>Introduction: Gregg Pollack has made 13 screen casts on how to scale rails:    Episode #1 - Page Responsiveness  Episode #2 - Page Caching    Episode #3 - Cache Expiration    Episode #4 - New Relic RPM   Episode #5 - Advanced Page Caching   Episode #6 - Action Caching    Episode #7 - Fragment Caching   Episode #8 - Memcached    Episode #9 - Taylor Weibley & Databases    Episode #10 - Client-side Caching    Episode #11 - Advanced HTTP Caching    Episode #12 - Jesse Newland & Deployment   Episode #13 - Jim Gochee & Advanced RPM  For a good InfoQ interview with Greg take a look at  Gregg Pollack and the How-To of Scaling Rails .</p><p>same-blog 2 0.88309032 <a title="1314-lda-2" href="../high_scalability-2012/high_scalability-2012-08-30-Dramatically_Improving_Performance_by_Debugging_Brutally_Complex_Prolems.html">1314 high scalability-2012-08-30-Dramatically Improving Performance by Debugging Brutally Complex Prolems</a></p>
<p>Introduction: Debugging complex problems is 90% persistence and 50% cool tools. Brendan Gregg in  10 Performance Wins  tells a fascinating story of how a team at Joyent solved some weird and challenging performance issues deep in the OS. It took lots of effort,  DTrace ,  Flame Graphs ,  USE Method , and writing custom tools when necessary. Here's a quick summary of the solved cases:
  
  Monitoring. 1000x improvement . An application blocked while paging anonymous memory back in. It was also blocked during file system fsync() calls. The application was misconfigured and sometimes briefly exceeded available memory, getting page out. 
  Riak. 2x improvement . The Erlang VM used half the CPU count it was supposed to, so CPUs remained unused.  Fix was a configuration change. 
  MySQL. 380x improvement . Reads were slow. Cause was correlated writes. Fix was to tune the cache flush interval on the storage controller. 
  Various. 2800x improvement . Large systems calls to getvmusage() could take a few sec</p><p>3 0.87520027 <a title="1314-lda-3" href="../high_scalability-2009/high_scalability-2009-07-16-Scalable_Web_Architectures_and_Application_State.html">656 high scalability-2009-07-16-Scalable Web Architectures and Application State</a></p>
<p>Introduction: In this article we follow a hypothetical programmer, Damian, on his quest to make his web application scalable.    Read the full article on Bytepawn</p><p>4 0.82686007 <a title="1314-lda-4" href="../high_scalability-2012/high_scalability-2012-09-15-4_Reasons_Facebook_Dumped_HTML5_and_Went_Native.html">1323 high scalability-2012-09-15-4 Reasons Facebook Dumped HTML5 and Went Native</a></p>
<p>Introduction: Facebook made quite a splash when they released their  native iOS app , not because of their app per se, but because of their conclusion that their  biggest mistake was betting on HTML5 , so they had to go native.
 
As you might imagine this was a bit like telling a Great White Shark that its bark is worse than its bite.  A  common refrain  was Facebook simply had made a bad HTML5 site, not that HTML5 itself is bad, as plenty of other vendors have made slick well performing mobile sites.
 
An interesting and relevant conversation given the rising butt kickery of mobile. But we were lacking details. Now we aren't. If you were wondering just why Facebook ditched HTML5, Tobie Langel in  Perf Feedback - What's slowing down Mobile Facebook , lists out the reasons:
  
  Tooling / Developer APIs . Most importantly, the lack of tooling to track down memory problems.  
  Scrolling performance.  Scrolling must be fast and smooth and full featured. It's not. 
  GPU.  A clunky API and black box ap</p><p>5 0.82112831 <a title="1314-lda-5" href="../high_scalability-2013/high_scalability-2013-10-23-Strategy%3A_Use_Linux_Taskset_to_Pin_Processes_or_Let_the_OS_Schedule_It%3F.html">1536 high scalability-2013-10-23-Strategy: Use Linux Taskset to Pin Processes or Let the OS Schedule It?</a></p>
<p>Introduction: This question comes from Ulysses on an  interesting thread  from the Mechanical Sympathy news group, especially given how multiple processors are now the norm:
 
Ulysses:
   
 On an 8xCPU Linux instance,  is it at all advantageous to use the Linux taskset command to pin an 8xJVM process set (co-ordinated as a www.infinispan.org distributed cache/data grid) to a specific CPU affinity set  (i.e. pin JVM0 process to CPU 0, JVM1 process to CPU1, ...., JVM7process to CPU 7) vs. just letting the Linux OS use its default mechanism for provisioning the 8xJVM process set to the available CPUs? 
 In effrort to seek an optimal point (in the full event space), what are the conceptual trade-offs in considering "searching" each permutation of provisioning an 8xJVM process set to an 8xCPU set via taskset? 
   
Given  taskset  is they key to the question, it would help to have a definition:
  

Used to set or retrieve the CPU affinity of a running process given its PID or to launch a new COMMAND with</p><p>6 0.81798476 <a title="1314-lda-6" href="../high_scalability-2014/high_scalability-2014-01-20-8_Ways_Stardog_Made_its_Database_Insanely_Scalable.html">1582 high scalability-2014-01-20-8 Ways Stardog Made its Database Insanely Scalable</a></p>
<p>7 0.80669755 <a title="1314-lda-7" href="../high_scalability-2012/high_scalability-2012-03-29-Strategy%3A_Exploit_Processor_Affinity_for_High_and_Predictable_Performance.html">1218 high scalability-2012-03-29-Strategy: Exploit Processor Affinity for High and Predictable Performance</a></p>
<p>8 0.74767834 <a title="1314-lda-8" href="../high_scalability-2012/high_scalability-2012-07-11-FictionPress%3A_Publishing_6_Million_Works_of_Fiction_on_the_Web.html">1281 high scalability-2012-07-11-FictionPress: Publishing 6 Million Works of Fiction on the Web</a></p>
<p>9 0.74370122 <a title="1314-lda-9" href="../high_scalability-2010/high_scalability-2010-06-30-Paper%3A_GraphLab%3A_A_New_Framework_For_Parallel_Machine_Learning.html">850 high scalability-2010-06-30-Paper: GraphLab: A New Framework For Parallel Machine Learning</a></p>
<p>10 0.72761565 <a title="1314-lda-10" href="../high_scalability-2009/high_scalability-2009-05-28-Scaling_PostgreSQL_using_CUDA.html">609 high scalability-2009-05-28-Scaling PostgreSQL using CUDA</a></p>
<p>11 0.71200323 <a title="1314-lda-11" href="../high_scalability-2012/high_scalability-2012-06-15-Cloud_Bursting_between_AWS_and_Rackspace.html">1264 high scalability-2012-06-15-Cloud Bursting between AWS and Rackspace</a></p>
<p>12 0.70835668 <a title="1314-lda-12" href="../high_scalability-2014/high_scalability-2014-04-18-Stuff_The_Internet_Says_On_Scalability_For_April_18th%2C_2014.html">1634 high scalability-2014-04-18-Stuff The Internet Says On Scalability For April 18th, 2014</a></p>
<p>13 0.69645727 <a title="1314-lda-13" href="../high_scalability-2013/high_scalability-2013-02-13-7_Sensible_and_1_Really_Surprising_Way_EVE_Online_Scales_to_Play_Huge_Games.html">1405 high scalability-2013-02-13-7 Sensible and 1 Really Surprising Way EVE Online Scales to Play Huge Games</a></p>
<p>14 0.6843428 <a title="1314-lda-14" href="../high_scalability-2011/high_scalability-2011-05-11-Troubleshooting_response_time_problems_%E2%80%93_why_you_cannot_trust_your_system_metrics.html">1038 high scalability-2011-05-11-Troubleshooting response time problems – why you cannot trust your system metrics</a></p>
<p>15 0.65832585 <a title="1314-lda-15" href="../high_scalability-2011/high_scalability-2011-09-19-Big_Iron_Returns_with_BigMemory.html">1118 high scalability-2011-09-19-Big Iron Returns with BigMemory</a></p>
<p>16 0.65780431 <a title="1314-lda-16" href="../high_scalability-2012/high_scalability-2012-10-02-An_Epic_TripAdvisor_Update%3A_Why_Not_Run_on_the_Cloud%3F_The_Grand_Experiment..html">1331 high scalability-2012-10-02-An Epic TripAdvisor Update: Why Not Run on the Cloud? The Grand Experiment.</a></p>
<p>17 0.6559785 <a title="1314-lda-17" href="../high_scalability-2012/high_scalability-2012-05-16-Big_List_of_20_Common_Bottlenecks.html">1246 high scalability-2012-05-16-Big List of 20 Common Bottlenecks</a></p>
<p>18 0.6531747 <a title="1314-lda-18" href="../high_scalability-2012/high_scalability-2012-12-10-Switch_your_databases_to_Flash_storage._Now._Or_you%27re_doing_it_wrong..html">1369 high scalability-2012-12-10-Switch your databases to Flash storage. Now. Or you're doing it wrong.</a></p>
<p>19 0.65299398 <a title="1314-lda-19" href="../high_scalability-2012/high_scalability-2012-12-12-Pinterest_Cut_Costs_from_%2454_to_%2420_Per_Hour_by_Automatically_Shutting_Down_Systems.html">1371 high scalability-2012-12-12-Pinterest Cut Costs from $54 to $20 Per Hour by Automatically Shutting Down Systems</a></p>
<p>20 0.6529122 <a title="1314-lda-20" href="../high_scalability-2012/high_scalability-2012-03-30-Stuff_The_Internet_Says_On_Scalability_For_March_30%2C_2012.html">1219 high scalability-2012-03-30-Stuff The Internet Says On Scalability For March 30, 2012</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
