<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1307 high scalability-2012-08-20-The Performance of Distributed Data-Structures Running on a "Cache-Coherent" In-Memory Data Grid</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2012" href="../home/high_scalability-2012_home.html">high_scalability-2012</a> <a title="high_scalability-2012-1307" href="#">high_scalability-2012-1307</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1307 high scalability-2012-08-20-The Performance of Distributed Data-Structures Running on a "Cache-Coherent" In-Memory Data Grid</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2012-1307-html" href="http://highscalability.com//blog/2012/8/20/the-performance-of-distributed-data-structures-running-on-a.html">html</a></p><p>Introduction: This is a guest post by Ron Pressler, the founder and CEO of  Parallel Universe , a Y Combinator company building advanced middleware for real-time applications. 
 
A little over a month ago, we open-sourced a new in-memory data grid called  Galaxy . An in-memory data grid, or IMDG, is a clustered data storage and processing middleware that uses RAM as the authoritative and primary storage, and distributes data over a cluster for purposes of data and processing scalability and high-availability. A common feature of IMDGs is co-location of code and data, meaning that application code runs on all cluster nodes, each instance processing those data items residing in the local node's RAM.
 
While quite a few commercial and open-source IMDGs are available (like Terracotta, Gigaspaces, Oracle Coherence, GemFire, Websphere eXtreme Scale, Infinispan and Hazelcast), Galaxy has adopted a completely different architecture from all other IMDGs, to service some usage scenarios ill-fitted to the othe</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 DHTs assign a data item to one or more cluster nodes based on a static hash value computed for each item's key (those systems provide access to items by keys). [sent-7, score-0.595]
</p><p>2 A short overview of Galaxy's implementation and its implications   As the data-structure we're about to discuss is the B+-tree, in order to avoid confusion, we shall henceforth call the Galaxy cluster nodes machines, while the tree nodes will be called, simply, nodes. [sent-26, score-0.718]
</p><p>3 The item resides in the local RAM of both owner and sharers in an object called a cache-line (the name has been adopted from that used by L1 caches, the inspiration Galaxy's architecture). [sent-32, score-0.544]
</p><p>4 Quite often, the former sharers will again request to share the item for future reads, requiring another roundtrip per sharer. [sent-35, score-0.743]
</p><p>5 We will analyze amortized cost (the worst-case cost of a sequence of operations, each having a different individual cost) of the insert operation. [sent-43, score-0.486]
</p><p>6 We will assume that the tree is implemented using Galaxy such that each node is one Galaxy data-item, and that each node has a capacity of  b >2 children ( b  is the tree's  fanout ). [sent-45, score-0.455]
</p><p>7 We will denote the number of machines in the cluster as  M , and the number of elements stored in the tree (the size of the data set) as  n . [sent-46, score-0.644]
</p><p>8 Let us also assume that some algorithm has distributed the nodes among machines such that below a certain level  L  of the tree, all nodes are exclusively owned by one machine — i. [sent-47, score-0.766]
</p><p>9 This means that the tree's root node is shared by all  M  machines, the nodes at level 2 are each shared by   machines, at the level below that by  , and so on. [sent-54, score-0.593]
</p><p>10 Because of this, updating the root requires  M  network roundtrips, updating a node at the level below the root requires   roundtrips etc. [sent-55, score-0.641]
</p><p>11 If it overflows, the leaf splits into two — each new leaf containing half of the old leaf's element, and then inserts the new leaf as a child into the parent node. [sent-57, score-0.81]
</p><p>12 Let's now count the number of network roundtrips required, in the worst-case, by a sequence of   consecutive insert operations, with  . [sent-60, score-0.59]
</p><p>13 To find the amortized cost for one insert operation, we divide by the number of operations,  , and get:       Now, if you look at the definition of  L  and at the drawing, you'll realize that  , so, finally, we get:       Let's examine this result. [sent-67, score-0.459]
</p><p>14 If  M  is constant and  n  increases, then the number of nodes stored on each machine increases as well, and more nodes are at levels below  L . [sent-70, score-0.703]
</p><p>15 In real life, though, this doesn't happen, as the number of tree nodes that can be stored on each machine is bounded, so when the data-set grows very large, we  must  increase the number of machines. [sent-72, score-0.731]
</p><p>16 If, on the other hand, we increase the number of machines while  keeping n constant , the amortized cost rises. [sent-73, score-0.463]
</p><p>17 If we distribute  the same amount of data  to more machines, more of the nodes will be closer to level  L , and more inter-machine communication will be needed, since each machine is now responsible for a smaller subset of the data. [sent-75, score-0.47]
</p><p>18 If we decide, then, that we want each machine to store and process up to  C  objects — or tree nodes in our case —  M  becomes related to  n  ( ), and now our cost becomes:       True, this is not the whole story. [sent-79, score-0.604]
</p><p>19 Once  k  exceeds  h , the height of the tree will grow, requiring some re-balancing, possibly moving some tree nodes from one machine to another. [sent-80, score-0.843]
</p><p>20 The O(1) amortized behavior means that throughput is unaffected by the number of machines (accepting our assumption that machines are added to accommodate processing more data), but the  latency  of a single, rare, operation might be. [sent-85, score-0.56]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('galaxy', 0.406), ('sharers', 0.269), ('nodes', 0.222), ('tree', 0.221), ('roundtrips', 0.219), ('item', 0.215), ('roundtrip', 0.188), ('amortized', 0.166), ('leaf', 0.166), ('insert', 0.15), ('imdgs', 0.139), ('inserts', 0.13), ('node', 0.117), ('machine', 0.108), ('parent', 0.102), ('shared', 0.096), ('machines', 0.093), ('number', 0.09), ('universe', 0.086), ('items', 0.085), ('splits', 0.08), ('denote', 0.077), ('levell', 0.077), ('modifications', 0.074), ('data', 0.073), ('requiring', 0.071), ('entail', 0.07), ('owned', 0.068), ('network', 0.067), ('communication', 0.067), ('abstraction', 0.066), ('operation', 0.065), ('sequence', 0.064), ('leaves', 0.064), ('root', 0.062), ('constant', 0.061), ('rare', 0.06), ('dhts', 0.06), ('overflows', 0.06), ('heuristics', 0.06), ('owner', 0.06), ('color', 0.058), ('analysis', 0.057), ('updating', 0.057), ('nevertheless', 0.055), ('write', 0.054), ('processing', 0.053), ('distributed', 0.053), ('shall', 0.053), ('cost', 0.053)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="1307-tfidf-1" href="../high_scalability-2012/high_scalability-2012-08-20-The_Performance_of_Distributed_Data-Structures_Running_on_a_%22Cache-Coherent%22_In-Memory_Data_Grid.html">1307 high scalability-2012-08-20-The Performance of Distributed Data-Structures Running on a "Cache-Coherent" In-Memory Data Grid</a></p>
<p>Introduction: This is a guest post by Ron Pressler, the founder and CEO of  Parallel Universe , a Y Combinator company building advanced middleware for real-time applications. 
 
A little over a month ago, we open-sourced a new in-memory data grid called  Galaxy . An in-memory data grid, or IMDG, is a clustered data storage and processing middleware that uses RAM as the authoritative and primary storage, and distributes data over a cluster for purposes of data and processing scalability and high-availability. A common feature of IMDGs is co-location of code and data, meaning that application code runs on all cluster nodes, each instance processing those data items residing in the local node's RAM.
 
While quite a few commercial and open-source IMDGs are available (like Terracotta, Gigaspaces, Oracle Coherence, GemFire, Websphere eXtreme Scale, Infinispan and Hazelcast), Galaxy has adopted a completely different architecture from all other IMDGs, to service some usage scenarios ill-fitted to the othe</p><p>2 0.26524657 <a title="1307-tfidf-2" href="../high_scalability-2011/high_scalability-2011-02-01-Google_Strategy%3A_Tree_Distribution_of_Requests_and_Responses.html">981 high scalability-2011-02-01-Google Strategy: Tree Distribution of Requests and Responses</a></p>
<p>Introduction: If a large number of leaf node machines send requests to a central root node then that root node can become overwhelmed:
  
 The CPU becomes a bottleneck, for either processing requests or sending replies, because it can't possibly deal with the flood of requests. 
 The network interface becomes a bottleneck because a wide fan-in causes TCP drops and retransmissions, which causes latency. Then clients start retrying requests which quickly causes a spiral of death in an undisciplined system. 
  
One solution to this problem is a strategy given by Dr.  Jeff Dean , Head of Google's School of Infrastructure Wizardry, in this  Stanford video presentation :  Tree Distribution of Requests and Responses .
 
 
 
Instead of having a root node connected to leaves in a flat topology, the idea is to create a tree of nodes. So a root node talks to a number of parent nodes and the parent nodes talk to a number of leaf nodes. Requests are pushed down the tree through the parents and only hit a subset</p><p>3 0.18285078 <a title="1307-tfidf-3" href="../high_scalability-2012/high_scalability-2012-04-30-Masstree_-_Much_Faster_than_MongoDB%2C_VoltDB%2C_Redis%2C_and_Competitive_with_Memcached.html">1236 high scalability-2012-04-30-Masstree - Much Faster than MongoDB, VoltDB, Redis, and Competitive with Memcached</a></p>
<p>Introduction: The  EuroSys 2012  system conference has an excellent live blog summary of their talks for:  Day 1 ,  Day 2 ,  Day 3  (thanks  Henry at the Paper Trail blog ). Summaries for each of the accepted papers are  here .
 
One of the more interesting papers from a NoSQL perspective was  Cache Craftiness for Fast Multicore Key-Value Storage , a wonderfully detailed description of the low level techniques used to implement Masstree:
  

A storage system specialized for key-value data in which all data ﬁts in memory, but must persist across server restarts. It supports arbitrary, variable-length keys. It allows range queries over those keys: clients can traverse subsets of the database, or the whole database, in sorted order by key. On a 16-core machine Masstree achieves six to ten million operations per second on parts A–C of the Yahoo! Cloud Serving Benchmark benchmark, more than 30 as fast as VoltDB [5] or MongoDB [2].

  
If you are looking for innovative detailed high performance design, t</p><p>4 0.14876394 <a title="1307-tfidf-4" href="../high_scalability-2012/high_scalability-2012-03-12-Google%3A_Taming_the_Long_Latency_Tail_-_When_More_Machines_Equals_Worse_Results.html">1207 high scalability-2012-03-12-Google: Taming the Long Latency Tail - When More Machines Equals Worse Results</a></p>
<p>Introduction: Likewise the current belief that, in the case of artificial machines the very large and the very small are equally feasible and lasting is a manifest error. Thus, for example, a small obelisk or column or other solid figure can certainly be laid down or set up without danger of breaking, while the large ones will go to pieces under the slightest provocation, and that purely on account of their own weight. -- Galileo  
Galileo observed how things broke if they were naively scaled up. Interestingly, Google noticed a similar pattern when building larger software systems using the same techniques used to build smaller systems. 
 
 Luiz André Barroso , Distinguished Engineer at Google, talks about this fundamental property of scaling systems in his fascinating talk,  Warehouse-Scale Computing: Entering the Teenage Decade . Google found the larger the scale the greater the impact of latency variability. When a request is implemented by work done in parallel, as is common with today's service</p><p>5 0.14070615 <a title="1307-tfidf-5" href="../high_scalability-2009/high_scalability-2009-07-30-Learn_How_to_Think_at_Scale.html">666 high scalability-2009-07-30-Learn How to Think at Scale</a></p>
<p>Introduction: Aaron Kimball of Cloudera gives a wonderful 23 minute presentation titled  Cloudera Hadoop Training: Thinking at Scale Cloudera  which talks about "common challenges and general best practices for scaling with your data." As a company Cloudera offers "enterprise-level support to users of Apache Hadoop." Part of that offering is a really useful series of  tutorial videos on the Hadoop ecosystem .   Like TV lawyer Perry Mason (or is it Harmon Rabb?), Aaron gradually builds his case. He opens with the problem of storing lots of data. Then a blistering cross examination of the problem of building distributed systems to analyze that data sets up a powerful closing argument. With so much testimony behind him, on closing Aaron really brings it home with why shared nothing systems like map-reduce are the right solution on how to query lots of data. They jury loved it.   Here's the video  Thinking at Scale . And here's a summary of some of the lessons learned from the talk:
  Lessons Learned</p><p>6 0.13299373 <a title="1307-tfidf-6" href="../high_scalability-2012/high_scalability-2012-07-27-Stuff_The_Internet_Says_On_Scalability_For_July_27%2C_2012.html">1292 high scalability-2012-07-27-Stuff The Internet Says On Scalability For July 27, 2012</a></p>
<p>7 0.13253689 <a title="1307-tfidf-7" href="../high_scalability-2011/high_scalability-2011-11-14-Using_Gossip_Protocols_for_Failure_Detection%2C_Monitoring%2C_Messaging_and_Other_Good_Things.html">1142 high scalability-2011-11-14-Using Gossip Protocols for Failure Detection, Monitoring, Messaging and Other Good Things</a></p>
<p>8 0.13243219 <a title="1307-tfidf-8" href="../high_scalability-2011/high_scalability-2011-04-12-Caching_and_Processing_2TB_Mozilla_Crash_Reports_in_memory_with_Hazelcast.html">1020 high scalability-2011-04-12-Caching and Processing 2TB Mozilla Crash Reports in memory with Hazelcast</a></p>
<p>9 0.12701601 <a title="1307-tfidf-9" href="../high_scalability-2011/high_scalability-2011-11-07-10_Core_Architecture_Pattern_Variations_for_Achieving_Scalability.html">1138 high scalability-2011-11-07-10 Core Architecture Pattern Variations for Achieving Scalability</a></p>
<p>10 0.12315562 <a title="1307-tfidf-10" href="../high_scalability-2009/high_scalability-2009-05-05-Drop_ACID_and_Think_About_Data.html">589 high scalability-2009-05-05-Drop ACID and Think About Data</a></p>
<p>11 0.1217669 <a title="1307-tfidf-11" href="../high_scalability-2010/high_scalability-2010-08-12-Think_of_Latency_as_a_Pseudo-permanent_Network_Partition.html">879 high scalability-2010-08-12-Think of Latency as a Pseudo-permanent Network Partition</a></p>
<p>12 0.12123414 <a title="1307-tfidf-12" href="../high_scalability-2010/high_scalability-2010-10-15-Troubles_with_Sharding_-_What_can_we_learn_from_the_Foursquare_Incident%3F.html">920 high scalability-2010-10-15-Troubles with Sharding - What can we learn from the Foursquare Incident?</a></p>
<p>13 0.11807588 <a title="1307-tfidf-13" href="../high_scalability-2009/high_scalability-2009-03-16-Are_Cloud_Based_Memory_Architectures_the_Next_Big_Thing%3F.html">538 high scalability-2009-03-16-Are Cloud Based Memory Architectures the Next Big Thing?</a></p>
<p>14 0.11669226 <a title="1307-tfidf-14" href="../high_scalability-2010/high_scalability-2010-11-09-Facebook_Uses_Non-Stored_Procedures_to_Update_Social_Graphs.html">936 high scalability-2010-11-09-Facebook Uses Non-Stored Procedures to Update Social Graphs</a></p>
<p>15 0.11560459 <a title="1307-tfidf-15" href="../high_scalability-2013/high_scalability-2013-09-09-Need_Help_with_Database_Scalability%3F_Understand_I-O.html">1514 high scalability-2013-09-09-Need Help with Database Scalability? Understand I-O</a></p>
<p>16 0.1126354 <a title="1307-tfidf-16" href="../high_scalability-2009/high_scalability-2009-09-16-Paper%3A_A_practical_scalable_distributed_B-tree.html">705 high scalability-2009-09-16-Paper: A practical scalable distributed B-tree</a></p>
<p>17 0.11177137 <a title="1307-tfidf-17" href="../high_scalability-2012/high_scalability-2012-07-04-Top_Features_of_a_Scalable_Database.html">1276 high scalability-2012-07-04-Top Features of a Scalable Database</a></p>
<p>18 0.11105986 <a title="1307-tfidf-18" href="../high_scalability-2010/high_scalability-2010-05-20-Strategy%3A_Scale_Writes_to_734_Million_Records_Per_Day_Using_Time_Partitioning.html">829 high scalability-2010-05-20-Strategy: Scale Writes to 734 Million Records Per Day Using Time Partitioning</a></p>
<p>19 0.11069208 <a title="1307-tfidf-19" href="../high_scalability-2012/high_scalability-2012-01-13-Stuff_The_Internet_Says_On_Scalability_For_January_13%2C_2012.html">1174 high scalability-2012-01-13-Stuff The Internet Says On Scalability For January 13, 2012</a></p>
<p>20 0.10942368 <a title="1307-tfidf-20" href="../high_scalability-2013/high_scalability-2013-03-11-Low_Level_Scalability_Solutions_-_The_Conditioning_Collection.html">1421 high scalability-2013-03-11-Low Level Scalability Solutions - The Conditioning Collection</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.191), (1, 0.121), (2, -0.007), (3, 0.036), (4, -0.035), (5, 0.104), (6, 0.084), (7, 0.021), (8, -0.066), (9, 0.001), (10, 0.038), (11, 0.038), (12, -0.016), (13, -0.036), (14, 0.046), (15, 0.032), (16, -0.012), (17, -0.032), (18, 0.028), (19, 0.009), (20, -0.007), (21, 0.029), (22, 0.03), (23, 0.019), (24, -0.012), (25, -0.024), (26, 0.016), (27, 0.001), (28, 0.0), (29, -0.01), (30, 0.063), (31, 0.006), (32, -0.005), (33, 0.047), (34, -0.022), (35, 0.02), (36, 0.024), (37, -0.058), (38, -0.012), (39, -0.005), (40, -0.01), (41, -0.054), (42, 0.024), (43, 0.009), (44, -0.004), (45, 0.044), (46, 0.027), (47, -0.001), (48, -0.034), (49, 0.025)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97216183 <a title="1307-lsi-1" href="../high_scalability-2012/high_scalability-2012-08-20-The_Performance_of_Distributed_Data-Structures_Running_on_a_%22Cache-Coherent%22_In-Memory_Data_Grid.html">1307 high scalability-2012-08-20-The Performance of Distributed Data-Structures Running on a "Cache-Coherent" In-Memory Data Grid</a></p>
<p>Introduction: This is a guest post by Ron Pressler, the founder and CEO of  Parallel Universe , a Y Combinator company building advanced middleware for real-time applications. 
 
A little over a month ago, we open-sourced a new in-memory data grid called  Galaxy . An in-memory data grid, or IMDG, is a clustered data storage and processing middleware that uses RAM as the authoritative and primary storage, and distributes data over a cluster for purposes of data and processing scalability and high-availability. A common feature of IMDGs is co-location of code and data, meaning that application code runs on all cluster nodes, each instance processing those data items residing in the local node's RAM.
 
While quite a few commercial and open-source IMDGs are available (like Terracotta, Gigaspaces, Oracle Coherence, GemFire, Websphere eXtreme Scale, Infinispan and Hazelcast), Galaxy has adopted a completely different architecture from all other IMDGs, to service some usage scenarios ill-fitted to the othe</p><p>2 0.84887093 <a title="1307-lsi-2" href="../high_scalability-2011/high_scalability-2011-11-14-Using_Gossip_Protocols_for_Failure_Detection%2C_Monitoring%2C_Messaging_and_Other_Good_Things.html">1142 high scalability-2011-11-14-Using Gossip Protocols for Failure Detection, Monitoring, Messaging and Other Good Things</a></p>
<p>Introduction: When building a system on top of a set of wildly uncooperative and unruly computers you have knowledge problems: knowing when other nodes are dead; knowing when nodes become alive; getting information about other nodes so you can make local decisions, like knowing which node should handle a request based on a scheme for assigning nodes to a certain range of users; learning about new configuration data; agreeing on data values; and so on.
 
How do you solve these problems? 
 
A common centralized approach is to use a database and all nodes query it for information. Obvious availability and performance issues for large distributed clusters. Another approach is to use  Paxos , a protocol for solving consensus in a network to maintain strict consistency requirements for small groups of unreliable processes. Not practical when larger number of nodes are involved.
 
So what's the super cool decentralized way to bring order to large clusters?
 
 Gossip protocols , which maintain relaxed consi</p><p>3 0.81612903 <a title="1307-lsi-3" href="../high_scalability-2011/high_scalability-2011-04-12-Caching_and_Processing_2TB_Mozilla_Crash_Reports_in_memory_with_Hazelcast.html">1020 high scalability-2011-04-12-Caching and Processing 2TB Mozilla Crash Reports in memory with Hazelcast</a></p>
<p>Introduction: Mozilla processes TB's of Firefox crash reports daily using HBase, Hadoop, Python and Thrift protocol. The project is called  Socorro , a system for collecting, processing, and displaying crash reports from clients. Today the Socorro application stores about 2.6 million crash reports per day. During peak traffic, it receives about 2.5K crashes per minute. 
 
In this article we are going to demonstrate a proof of concept showing how Mozilla could integrate Hazelcast into Socorro and achieve caching and processing 2TB of crash reports with 50 node Hazelcast cluster. The video for the demo is available  here .
 
      
 
Currently, Socorro has pythonic collectors, processors, and middleware that communicate with HBase via the Thrift protocol. One of the biggest limitations of the current architecture is that it is very sensitive to latency or outages on the HBase side. If the collectors cannot store an item in HBase then they will store it on local disk and it will not be accessible to th</p><p>4 0.80268759 <a title="1307-lsi-4" href="../high_scalability-2012/high_scalability-2012-04-03-Hazelcast_2.0%3A_Big_Data_In-Memory.html">1221 high scalability-2012-04-03-Hazelcast 2.0: Big Data In-Memory</a></p>
<p>Introduction: As it is said in the recent article  "Google: Taming the Long Latency Tail - When More Machines Equals Worse Results"  , latency variability has greater impact in larger scale clusters where a typical request is composed of multiple distributed/parallel requests. The overall response time dramatically decreases if latency of each request is not consistent and low. 
 
In dynamically scalable partitioned storage systems, whether it is a NoSQL database, filesystem or in-memory data grid, changes in the cluster (adding or removing a node) can lead to big data moves in the network to re-balance the cluster. Re-balancing will be needed for both primary and backup data on those nodes. If a node crashes for example, dead node’s data has to be re-owned (become primary) by other node(s) and also its backup has to be taken immediately to be fail-safe again. Shuffling MBs of data around has a negative effect in the cluster as it consumes your valuable resources such as network, CPU and RAM. It mig</p><p>5 0.79979509 <a title="1307-lsi-5" href="../high_scalability-2011/high_scalability-2011-02-02-Piccolo_-_Building_Distributed_Programs_that_are_11x_Faster_than_Hadoop.html">983 high scalability-2011-02-02-Piccolo - Building Distributed Programs that are 11x Faster than Hadoop</a></p>
<p>Introduction: Piccolo  (not  this  or  this ) is a system for distributed computing, Piccolo is a n ew data-centric programming model for writing parallel in-memory applications in data centers .  Unlike existing data-ﬂow models, Piccolo allows computation running on different machines to share distributed, mutable state via a key-value table interface. T  raditional data-centric models (such as Hadoop) which present the user a single object at a time to operate on, Piccolo exposes a global table interface which is available to all parts of the computation simultaneously. This allows users to specify programs in an intuitive manner very similar to that of writing programs for a single machine. 
 
Using an in-memory key-value store is a very different approach from the canonical map-reduce, which is based on using distributed file systems. The results are impressive:
  

Experiments have shown that Piccolo is fast and pro-vides excellent scaling for many applications. The performance of PageRank and</p><p>6 0.79769677 <a title="1307-lsi-6" href="../high_scalability-2012/high_scalability-2012-04-30-Masstree_-_Much_Faster_than_MongoDB%2C_VoltDB%2C_Redis%2C_and_Competitive_with_Memcached.html">1236 high scalability-2012-04-30-Masstree - Much Faster than MongoDB, VoltDB, Redis, and Competitive with Memcached</a></p>
<p>7 0.7932955 <a title="1307-lsi-7" href="../high_scalability-2009/high_scalability-2009-07-30-Learn_How_to_Think_at_Scale.html">666 high scalability-2009-07-30-Learn How to Think at Scale</a></p>
<p>8 0.77725017 <a title="1307-lsi-8" href="../high_scalability-2009/high_scalability-2009-12-30-Terrastore_-_Scalable%2C_elastic%2C_consistent_document_store..html">756 high scalability-2009-12-30-Terrastore - Scalable, elastic, consistent document store.</a></p>
<p>9 0.77502841 <a title="1307-lsi-9" href="../high_scalability-2009/high_scalability-2009-09-16-Paper%3A_A_practical_scalable_distributed_B-tree.html">705 high scalability-2009-09-16-Paper: A practical scalable distributed B-tree</a></p>
<p>10 0.77389395 <a title="1307-lsi-10" href="../high_scalability-2011/high_scalability-2011-02-01-Google_Strategy%3A_Tree_Distribution_of_Requests_and_Responses.html">981 high scalability-2011-02-01-Google Strategy: Tree Distribution of Requests and Responses</a></p>
<p>11 0.7655319 <a title="1307-lsi-11" href="../high_scalability-2012/high_scalability-2012-06-22-Stuff_The_Internet_Says_On_Scalability_For_June_22%2C_2012.html">1270 high scalability-2012-06-22-Stuff The Internet Says On Scalability For June 22, 2012</a></p>
<p>12 0.76008165 <a title="1307-lsi-12" href="../high_scalability-2012/high_scalability-2012-04-05-Big_Data_Counting%3A_How_to_count_a_billion_distinct_objects_using_only_1.5KB_of_Memory.html">1222 high scalability-2012-04-05-Big Data Counting: How to count a billion distinct objects using only 1.5KB of Memory</a></p>
<p>13 0.75958258 <a title="1307-lsi-13" href="../high_scalability-2010/high_scalability-2010-12-23-Paper%3A_CRDTs%3A_Consistency_without_concurrency_control.html">963 high scalability-2010-12-23-Paper: CRDTs: Consistency without concurrency control</a></p>
<p>14 0.7557261 <a title="1307-lsi-14" href="../high_scalability-2009/high_scalability-2009-05-05-Drop_ACID_and_Think_About_Data.html">589 high scalability-2009-05-05-Drop ACID and Think About Data</a></p>
<p>15 0.7542631 <a title="1307-lsi-15" href="../high_scalability-2008/high_scalability-2008-07-15-ZooKeeper_-_A_Reliable%2C_Scalable_Distributed_Coordination_System_.html">350 high scalability-2008-07-15-ZooKeeper - A Reliable, Scalable Distributed Coordination System </a></p>
<p>16 0.75205708 <a title="1307-lsi-16" href="../high_scalability-2013/high_scalability-2013-11-25-How_To_Make_an_Infinitely_Scalable_Relational_Database_Management_System_%28RDBMS%29.html">1553 high scalability-2013-11-25-How To Make an Infinitely Scalable Relational Database Management System (RDBMS)</a></p>
<p>17 0.75106126 <a title="1307-lsi-17" href="../high_scalability-2013/high_scalability-2013-10-08-F1_and_Spanner_Holistically_Compared.html">1529 high scalability-2013-10-08-F1 and Spanner Holistically Compared</a></p>
<p>18 0.74985397 <a title="1307-lsi-18" href="../high_scalability-2010/high_scalability-2010-12-01-8_Commonly_Used_Scalable_System_Design_Patterns.html">951 high scalability-2010-12-01-8 Commonly Used Scalable System Design Patterns</a></p>
<p>19 0.74418753 <a title="1307-lsi-19" href="../high_scalability-2009/high_scalability-2009-06-19-GemFire_6.0%3A_New_innovations_in_data_management.html">633 high scalability-2009-06-19-GemFire 6.0: New innovations in data management</a></p>
<p>20 0.73953003 <a title="1307-lsi-20" href="../high_scalability-2012/high_scalability-2012-11-26-BigData_using_Erlang%2C_C_and_Lisp_to_Fight_the_Tsunami_of_Mobile_Data.html">1362 high scalability-2012-11-26-BigData using Erlang, C and Lisp to Fight the Tsunami of Mobile Data</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.103), (2, 0.188), (10, 0.066), (14, 0.02), (17, 0.03), (30, 0.024), (47, 0.024), (61, 0.093), (79, 0.104), (82, 0.177), (85, 0.026), (94, 0.047)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.89055032 <a title="1307-lda-1" href="../high_scalability-2012/high_scalability-2012-08-20-The_Performance_of_Distributed_Data-Structures_Running_on_a_%22Cache-Coherent%22_In-Memory_Data_Grid.html">1307 high scalability-2012-08-20-The Performance of Distributed Data-Structures Running on a "Cache-Coherent" In-Memory Data Grid</a></p>
<p>Introduction: This is a guest post by Ron Pressler, the founder and CEO of  Parallel Universe , a Y Combinator company building advanced middleware for real-time applications. 
 
A little over a month ago, we open-sourced a new in-memory data grid called  Galaxy . An in-memory data grid, or IMDG, is a clustered data storage and processing middleware that uses RAM as the authoritative and primary storage, and distributes data over a cluster for purposes of data and processing scalability and high-availability. A common feature of IMDGs is co-location of code and data, meaning that application code runs on all cluster nodes, each instance processing those data items residing in the local node's RAM.
 
While quite a few commercial and open-source IMDGs are available (like Terracotta, Gigaspaces, Oracle Coherence, GemFire, Websphere eXtreme Scale, Infinispan and Hazelcast), Galaxy has adopted a completely different architecture from all other IMDGs, to service some usage scenarios ill-fitted to the othe</p><p>2 0.89040846 <a title="1307-lda-2" href="../high_scalability-2011/high_scalability-2011-05-20-Stuff_The_Internet_Says_On_Scalability_For_May_20%2C_2011.html">1045 high scalability-2011-05-20-Stuff The Internet Says On Scalability For May 20, 2011</a></p>
<p>Introduction: Submitted for your reading pleasure on this beautiful morning: 
 
 
  
 
  
  Group Decision Making in Honey Bee Swarms . In distributed computing systems nodes reach a  quorum  when deciding what to do as a group. It turns out bees also use quorum logic when deciding on where to nest! Bees do it a bit differently of course:   A scout bee votes for a site by spending time at it, somehow the scouts act and interact so that their numbers rise faster at superior sites, and somehow the bees at each site monitor their numbers there so that they know whether they've reached the threshold number (quorum) and can proceed to initiating the swarm's move to this site.  Ants use similar mechanisms to control foraging.   Distributed systems may share common mechanisms based on their nature as being a distributed system,  the components may not matter that much. 
 
 Fire! Fire!  Brent Chapman shows how to put that IT fire out in  Incident Command for IT: What We Can Learn from the Fire Department .</p><p>3 0.88108873 <a title="1307-lda-3" href="../high_scalability-2008/high_scalability-2008-05-25-Product%3A_Condor__-_Compute_Intensive_Workload_Management.html">326 high scalability-2008-05-25-Product: Condor  - Compute Intensive Workload Management</a></p>
<p>Introduction: From their website:   Condor  is a specialized workload management system for compute-intensive jobs. Like other full-featured batch systems, Condor provides a job queueing mechanism, scheduling policy, priority scheme, resource monitoring, and resource management. Users submit their serial or parallel jobs to Condor, Condor places them into a queue, chooses when and where to run the jobs based upon a policy, carefully monitors their progress, and ultimately informs the user upon completion.  While providing functionality similar to that of a more traditional batch queueing system, Condor's novel architecture allows it to succeed in areas where traditional scheduling systems fail. Condor can be used to manage a cluster of dedicated compute nodes (such as a "Beowulf" cluster). In addition, unique mechanisms enable Condor to effectively harness wasted CPU power from otherwise idle desktop workstations. For instance, Condor can be configured to only use desktop machines where the keyboard</p><p>4 0.8761977 <a title="1307-lda-4" href="../high_scalability-2012/high_scalability-2012-09-10-Russ%E2%80%99_10_Ingredient_Recipe_for_Making_1_Million_TPS_on_%245K_Hardware.html">1319 high scalability-2012-09-10-Russ’ 10 Ingredient Recipe for Making 1 Million TPS on $5K Hardware</a></p>
<p>Introduction: My name is  Russell Sullivan , I am the author of AlchemyDB: a highly flexible NoSQL/SQL/DocumentStore/GraphDB-datastore built on top of redis. I have spent the last several years trying to find a way to sanely house multiple datastore-genres under one roof while (almost paradoxically) pushing performance to its limits.    I recently joined the NoSQL company    Aerospike    (formerly Citrusleaf) with the goal of incrementally grafting AlchemyDB’s flexible data-modeling capabilities onto Aerospike’s high-velocity horizontally-scalable key-value data-fabric. We recently completed a peak-performance    TPS optimization project   : starting at 200K TPS, pushing to the recent community edition launch at 500K TPS, and finally arriving at our 2012 goal:    1M TPS on $5K hardware   .    Getting to one million over-the-wire client-server database-requests per-second on a single machine costing $5K is a balance between trimming overhead on many axes and using a shared nothing architecture to   i</p><p>5 0.86178845 <a title="1307-lda-5" href="../high_scalability-2012/high_scalability-2012-12-14-Stuff_The_Internet_Says_On_Scalability_For_December_14%2C_2012.html">1372 high scalability-2012-12-14-Stuff The Internet Says On Scalability For December 14, 2012</a></p>
<p>Introduction: In a hole in the Internet there lived HighScalability:
  
  $140 Billion : trivial cost of Google fiber everywhere;  5,200 GB : data for every person on Earth;  6 hours : time it takes for a 25-GPU cluster to crack all the passwords;  
 Quoteable Quotes:               
 
  hnriot : Good architecture eliminates the need for prayer. 
  @adrianco : we break AWS, they fix it. Stuff that's breaking now is mostly stuff other clouds haven't got to yet. 
  Scalability Rules : Design for 20x capacity. • Implement for 3x capacity. • Deploy for ~1.5x capacity. 
 
 
 Fast typing Aaron Delp with his  AWS re:Invent Werner Vogel Keynote Live Blog .  Some key points: Decompose into small loosely coupled, stateless building blocks; Automate your application and processes; Let Business levers control the system; Architect with cost in mind; Protecting your customer is the first priority; In production, deploy to at least two availability zones; Integrate security into your application from the ground up</p><p>6 0.85074973 <a title="1307-lda-6" href="../high_scalability-2007/high_scalability-2007-10-23-Hire_Facebook%2C_Ning%2C_and_Salesforce_to_Scale_for_You.html">129 high scalability-2007-10-23-Hire Facebook, Ning, and Salesforce to Scale for You</a></p>
<p>7 0.84463835 <a title="1307-lda-7" href="../high_scalability-2008/high_scalability-2008-05-31-memcached_and_Storage_of_Friend_list.html">337 high scalability-2008-05-31-memcached and Storage of Friend list</a></p>
<p>8 0.83132297 <a title="1307-lda-8" href="../high_scalability-2007/high_scalability-2007-10-15-Olympic_Site_Architecture.html">123 high scalability-2007-10-15-Olympic Site Architecture</a></p>
<p>9 0.82068467 <a title="1307-lda-9" href="../high_scalability-2010/high_scalability-2010-06-28-VoltDB_Decapitates_Six_SQL_Urban_Myths_and_Delivers_Internet_Scale_OLTP_in_the_Process.html">849 high scalability-2010-06-28-VoltDB Decapitates Six SQL Urban Myths and Delivers Internet Scale OLTP in the Process</a></p>
<p>10 0.82045603 <a title="1307-lda-10" href="../high_scalability-2011/high_scalability-2011-09-07-What_Google_App_Engine_Price_Changes_Say_About_the_Future_of_Web_Architecture.html">1112 high scalability-2011-09-07-What Google App Engine Price Changes Say About the Future of Web Architecture</a></p>
<p>11 0.81993943 <a title="1307-lda-11" href="../high_scalability-2014/high_scalability-2014-05-16-Stuff_The_Internet_Says_On_Scalability_For_May_16th%2C_2014.html">1649 high scalability-2014-05-16-Stuff The Internet Says On Scalability For May 16th, 2014</a></p>
<p>12 0.81930095 <a title="1307-lda-12" href="../high_scalability-2009/high_scalability-2009-01-20-Product%3A_Amazon%27s_SimpleDB.html">498 high scalability-2009-01-20-Product: Amazon's SimpleDB</a></p>
<p>13 0.81867242 <a title="1307-lda-13" href="../high_scalability-2007/high_scalability-2007-08-22-Wikimedia_architecture.html">72 high scalability-2007-08-22-Wikimedia architecture</a></p>
<p>14 0.81747246 <a title="1307-lda-14" href="../high_scalability-2008/high_scalability-2008-04-08-Google_AppEngine_-_A_First_Look.html">301 high scalability-2008-04-08-Google AppEngine - A First Look</a></p>
<p>15 0.81703496 <a title="1307-lda-15" href="../high_scalability-2012/high_scalability-2012-02-02-The_Data-Scope_Project_-_6PB_storage%2C_500GBytes-sec_sequential_IO%2C_20M_IOPS%2C_130TFlops.html">1186 high scalability-2012-02-02-The Data-Scope Project - 6PB storage, 500GBytes-sec sequential IO, 20M IOPS, 130TFlops</a></p>
<p>16 0.81677067 <a title="1307-lda-16" href="../high_scalability-2012/high_scalability-2012-07-25-Vertical_Scaling_Ascendant_-_How_are_SSDs_Changing__Architectures%3F.html">1291 high scalability-2012-07-25-Vertical Scaling Ascendant - How are SSDs Changing  Architectures?</a></p>
<p>17 0.81666517 <a title="1307-lda-17" href="../high_scalability-2009/high_scalability-2009-06-05-HotPads_Shows_the_True_Cost_of_Hosting_on_Amazon.html">619 high scalability-2009-06-05-HotPads Shows the True Cost of Hosting on Amazon</a></p>
<p>18 0.81647098 <a title="1307-lda-18" href="../high_scalability-2012/high_scalability-2012-10-04-LinkedIn_Moved_from_Rails_to_Node%3A__27_Servers_Cut_and_Up_to_20x_Faster.html">1333 high scalability-2012-10-04-LinkedIn Moved from Rails to Node:  27 Servers Cut and Up to 20x Faster</a></p>
<p>19 0.81638157 <a title="1307-lda-19" href="../high_scalability-2011/high_scalability-2011-11-29-DataSift_Architecture%3A_Realtime_Datamining_at_120%2C000_Tweets_Per_Second.html">1148 high scalability-2011-11-29-DataSift Architecture: Realtime Datamining at 120,000 Tweets Per Second</a></p>
<p>20 0.81617165 <a title="1307-lda-20" href="../high_scalability-2012/high_scalability-2012-01-24-The_State_of_NoSQL_in_2012.html">1180 high scalability-2012-01-24-The State of NoSQL in 2012</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
