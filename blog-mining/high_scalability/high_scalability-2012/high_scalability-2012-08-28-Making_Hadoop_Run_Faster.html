<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1313 high scalability-2012-08-28-Making Hadoop Run Faster</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2012" href="../home/high_scalability-2012_home.html">high_scalability-2012</a> <a title="high_scalability-2012-1313" href="#">high_scalability-2012-1313</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1313 high scalability-2012-08-28-Making Hadoop Run Faster</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2012-1313-html" href="http://highscalability.com//blog/2012/8/28/making-hadoop-run-faster.html">html</a></p><p>Introduction: Making Hadoop Run FasterOne of the challenges in processing data is that the
speed at which we can input data is quite often much faster than the speed at
which we can process it. This problem becomes even more pronounced in the
context of Big Data, where the volume of data keeps on growing, along with a
corresponding need for more insights, and thus the need for more complex
processing also increases.Batch Processing to the RescueHadoop was designed to
deal with this challenge in the following ways:1. Use a distributed file
system: This enables us to spread the load and grow our system as needed.2.
Optimize for write speed: To enable fast writes the Hadoop architecture was
designed so that writes are first logged, and then processed. This enables
fairly fast write speeds.3. Use batch processing (Map/Reduce) to balance the
speed for the data feeds with the processing speed.Batch Processing
ChallengesThe challenge with batch-processing is that it assumes that the
feeds come in bursts. I</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Making Hadoop Run FasterOne of the challenges in processing data is that the speed at which we can input data is quite often much faster than the speed at which we can process it. [sent-1, score-1.115]
</p><p>2 This problem becomes even more pronounced in the context of Big Data, where the volume of data keeps on growing, along with a corresponding need for more insights, and thus the need for more complex processing also increases. [sent-2, score-0.988]
</p><p>3 Use batch processing (Map/Reduce) to balance the speed for the data feeds with the processing speed. [sent-9, score-1.696]
</p><p>4 Batch Processing ChallengesThe challenge with batch-processing is that it assumes that the feeds come in bursts. [sent-10, score-0.37]
</p><p>5 If our data feeds come in on a continuous basis, the entire assumption and architecture behind batch processing starts to break down. [sent-11, score-1.189]
</p><p>6 If we increase the batch window, the result is higher latency between the time the data comes in until the time we actually get it into our reports and insights. [sent-12, score-0.38]
</p><p>7 Moreover, the number of hours is finite -- in many systems the batch window is done on a daily basis. [sent-13, score-0.485]
</p><p>8 Often, the assumption is that most of the processing can be done during off-peak hours. [sent-14, score-0.638]
</p><p>9 But as the volume gets bigger, the time it takes to process the data gets longer, until it reaches the limit of the hours in a day and then we face dealing with a continuously growing backlog. [sent-15, score-0.664]
</p><p>10 In addition, if we experience a failure during the processing we might not have enough time to re-process. [sent-16, score-0.52]
</p><p>11 Making Hadoop Run FasterWe can make our Hadoop system run faster by pre-processing some of the work before it gets into our Hadoop system. [sent-17, score-0.308]
</p><p>12 We can also move the types of workload for which batch processing isn't a good fit out of the Hadoop Map/Reduce system and use Stream Processing, as Google did. [sent-18, score-0.824]
</p><p>13 Speed Things Up Through Stream-Based ProcessingThe concept of stream-based processing is fairly simple. [sent-19, score-0.603]
</p><p>14 Instead of logging the data first and then processing it, we can process it as it comes in. [sent-20, score-0.783]
</p><p>15 A good analogy to explain the difference is a manufacturing pipeline. [sent-21, score-0.266]
</p><p>16 Think about a car manufacturing pipeline: Compare the process of first putting all the parts together and then assembling them piece by piece, versus a process in which you package each unit at the manufacturer and only send the pre-packaged parts to the assembly line. [sent-22, score-1.463]
</p><p>17 Putting stream-based processing at the front is analogous to pre- packaging our parts before  they get to the assembly line, which is in our case is the Hadoop batch processing system. [sent-25, score-1.799]
</p><p>18 As in manufacturing, even if we pre-package the parts at the manufacturer we still need an assembly line to put all the parts together. [sent-26, score-0.78]
</p><p>19 In the same way, stream-based processing is not meant to replace our Hadoop system, but rather to reduce the amount of work that the system needs to deal with, and to make the work that does go into the Hadoop process easier, and thus faster, to process. [sent-27, score-0.957]
</p><p>20 In-memory stream processing can make a good stream processing system, as Curt Monash's points out on his research traditional databases will eventually end up in RAM. [sent-28, score-1.274]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('processing', 0.52), ('hadoop', 0.292), ('batch', 0.235), ('feeds', 0.229), ('assembly', 0.207), ('manufacturing', 0.198), ('manufacturer', 0.174), ('parts', 0.168), ('assumption', 0.118), ('process', 0.118), ('stream', 0.117), ('window', 0.109), ('speed', 0.105), ('faster', 0.093), ('gets', 0.093), ('piece', 0.09), ('data', 0.087), ('volume', 0.085), ('enables', 0.084), ('assembling', 0.084), ('fairly', 0.083), ('pronounced', 0.081), ('putting', 0.081), ('thus', 0.079), ('analogous', 0.079), ('finite', 0.075), ('challenge', 0.073), ('context', 0.072), ('packaging', 0.07), ('system', 0.069), ('assumes', 0.068), ('analogy', 0.068), ('curt', 0.067), ('hours', 0.066), ('deal', 0.065), ('corresponding', 0.064), ('moreover', 0.064), ('line', 0.063), ('monash', 0.062), ('reaches', 0.061), ('growing', 0.061), ('logged', 0.059), ('writes', 0.059), ('diagram', 0.058), ('comes', 0.058), ('car', 0.057), ('demonstrate', 0.055), ('pipeline', 0.055), ('work', 0.053), ('insights', 0.053)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000002 <a title="1313-tfidf-1" href="../high_scalability-2012/high_scalability-2012-08-28-Making_Hadoop_Run_Faster.html">1313 high scalability-2012-08-28-Making Hadoop Run Faster</a></p>
<p>Introduction: Making Hadoop Run FasterOne of the challenges in processing data is that the
speed at which we can input data is quite often much faster than the speed at
which we can process it. This problem becomes even more pronounced in the
context of Big Data, where the volume of data keeps on growing, along with a
corresponding need for more insights, and thus the need for more complex
processing also increases.Batch Processing to the RescueHadoop was designed to
deal with this challenge in the following ways:1. Use a distributed file
system: This enables us to spread the load and grow our system as needed.2.
Optimize for write speed: To enable fast writes the Hadoop architecture was
designed so that writes are first logged, and then processed. This enables
fairly fast write speeds.3. Use batch processing (Map/Reduce) to balance the
speed for the data feeds with the processing speed.Batch Processing
ChallengesThe challenge with batch-processing is that it assumes that the
feeds come in bursts. I</p><p>2 0.16719547 <a title="1313-tfidf-2" href="../high_scalability-2010/high_scalability-2010-09-08-4_General_Core_Scalability_Patterns.html">897 high scalability-2010-09-08-4 General Core Scalability Patterns</a></p>
<p>Introduction: Jesper Soderlund put together an excellent list of four general scalability
patterns and four subpatterns in his postScalability patterns and an
interesting story:Load distribution- Spread the system load across multiple
processing unitsLoad balancing / load sharing- Spreading the load across many
components with equal properties for handling the requestPartitioning-
Spreading the load across many components by routing an individual request to
a component that owns that data specificVertical partitioning- Spreading the
load across the functional boundaries of a problem space, separate functions
being handled by different processing unitsHorizontal partitioning- Spreading
a single type of data element across many instances, according to some
partitioning key, e.g. hashing the player id and doing a modulus operation,
etc. Quite often referred to as sharding.Queuing and batch \- Achieve
efficiencies of scale by processing batches of data, usually because the
overhead of an operation is am</p><p>3 0.16641219 <a title="1313-tfidf-3" href="../high_scalability-2009/high_scalability-2009-08-03-Building_a_Data_Intensive_Web_Application_with_Cloudera%2C_Hadoop%2C_Hive%2C_Pig%2C_and_EC2.html">669 high scalability-2009-08-03-Building a Data Intensive Web Application with Cloudera, Hadoop, Hive, Pig, and EC2</a></p>
<p>Introduction: This tutorial will show you how to use Amazon EC2 and Cloudera's Distribution
for Hadoop to run batch jobs for a data intensive web application.During the
tutorial, we will perform the following data processing steps....read more on
Cloudera website</p><p>4 0.16167958 <a title="1313-tfidf-4" href="../high_scalability-2011/high_scalability-2011-07-27-Making_Hadoop_1000x_Faster_for_Graph_Problems.html">1088 high scalability-2011-07-27-Making Hadoop 1000x Faster for Graph Problems</a></p>
<p>Introduction: Dr. Daniel Abadi, author of theDBMS Musings blog and Cofounder ofHadapt, which
offers a product improving Hadoop performance by 50x on relational data, is
now taking his talents to graph data in Hadoop's tremendous inefficiency on
graph data management (and how to avoid it), which shares the secrets of
getting Hadoop to perform 1000x better on graph data.TL;DR:Analysing graph
data is at the heart ofimportant data mining problems.Hadoop is the tool of
choice for many of these problems.Hadoop style MapReduce works best on
KeyValue processing, not graph processing, and can be well over a factor of
1000 less efficient than it needs to be.Hadoop inefficiency has consequences
in real world. Inefficiencies on graph data problems like improving power
utilization, minimizing carbon emissions, and improving product designs, leads
to a lot value being left on the table in the form of negative environmental
consequences, increased server costs, increased data center space, and
increased energy cos</p><p>5 0.16114375 <a title="1313-tfidf-5" href="../high_scalability-2009/high_scalability-2009-05-17-Product%3A_Hadoop.html">601 high scalability-2009-05-17-Product: Hadoop</a></p>
<p>Introduction: Update 5:Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62
Secondsand has itsgreen cred questionedbecause it took 40 times the number of
machines Greenplum used to do the same work.Update 4:Introduction to Pig. Pig
allows you to skip programming Hadoop at the low map-reduce level. You don't
have to know Java. Using the Pig Latin language, which is a scripting data
flow language, you can think about your problem as a data flow program. 10
lines of Pig Latin = 200 lines of Java.Update 3: Scaling Hadoop to4000 nodes
at Yahoo!. 30,000 cores with nearly 16PB of raw disk; sorted 6TB of data
completed in 37 minutes; 14,000 map tasks writes (reads) 360 MB (about 3
blocks) of data into a single file with a total of 5.04 TB for the whole
job.Update 2: HadoopSummit and Data-Intensive Computing Symposium Videos and
Slides. Topics include: Pig, JAQL, Hbase, Hive, Data-Intensive Scalable
Computing, Clouds and ManyCore: The Revolution, Simplicity and Complexity in
Data Systems at Scale, Han</p><p>6 0.1574185 <a title="1313-tfidf-6" href="../high_scalability-2013/high_scalability-2013-01-07-Analyzing_billions_of_credit_card_transactions_and_serving_low-latency_insights_in_the_cloud.html">1382 high scalability-2013-01-07-Analyzing billions of credit card transactions and serving low-latency insights in the cloud</a></p>
<p>7 0.15340465 <a title="1313-tfidf-7" href="../high_scalability-2009/high_scalability-2009-06-11-Yahoo%21_Distribution_of_Hadoop.html">627 high scalability-2009-06-11-Yahoo! Distribution of Hadoop</a></p>
<p>8 0.14927897 <a title="1313-tfidf-8" href="../high_scalability-2014/high_scalability-2014-03-24-Big%2C_Small%2C_Hot_or_Cold_-_Examples_of_Robust_Data_Pipelines_from_Stripe%2C_Tapad%2C_Etsy_and_Square.html">1618 high scalability-2014-03-24-Big, Small, Hot or Cold - Examples of Robust Data Pipelines from Stripe, Tapad, Etsy and Square</a></p>
<p>9 0.14853221 <a title="1313-tfidf-9" href="../high_scalability-2009/high_scalability-2009-07-30-Learn_How_to_Think_at_Scale.html">666 high scalability-2009-07-30-Learn How to Think at Scale</a></p>
<p>10 0.14415415 <a title="1313-tfidf-10" href="../high_scalability-2008/high_scalability-2008-10-15-Hadoop_-_A_Primer.html">414 high scalability-2008-10-15-Hadoop - A Primer</a></p>
<p>11 0.14026915 <a title="1313-tfidf-11" href="../high_scalability-2011/high_scalability-2011-03-08-Medialets_Architecture_-__Defeating_the_Daunting_Mobile_Device_Data_Deluge.html">1000 high scalability-2011-03-08-Medialets Architecture -  Defeating the Daunting Mobile Device Data Deluge</a></p>
<p>12 0.13916145 <a title="1313-tfidf-12" href="../high_scalability-2008/high_scalability-2008-09-28-Product%3A_Happy_%3D_Hadoop_%2B_Python.html">397 high scalability-2008-09-28-Product: Happy = Hadoop + Python</a></p>
<p>13 0.13555382 <a title="1313-tfidf-13" href="../high_scalability-2011/high_scalability-2011-12-22-Architecting_Massively-Scalable_Near-Real-Time_Risk_Analysis_Solutions.html">1161 high scalability-2011-12-22-Architecting Massively-Scalable Near-Real-Time Risk Analysis Solutions</a></p>
<p>14 0.13460037 <a title="1313-tfidf-14" href="../high_scalability-2011/high_scalability-2011-09-06-Big_Data_Application_Platform.html">1110 high scalability-2011-09-06-Big Data Application Platform</a></p>
<p>15 0.13439861 <a title="1313-tfidf-15" href="../high_scalability-2011/high_scalability-2011-01-04-Map-Reduce_With_Ruby_Using_Hadoop.html">968 high scalability-2011-01-04-Map-Reduce With Ruby Using Hadoop</a></p>
<p>16 0.13108623 <a title="1313-tfidf-16" href="../high_scalability-2009/high_scalability-2009-07-02-Product%3A_Hbase.html">650 high scalability-2009-07-02-Product: Hbase</a></p>
<p>17 0.1290963 <a title="1313-tfidf-17" href="../high_scalability-2008/high_scalability-2008-01-30-How_Rackspace_Now_Uses_MapReduce_and_Hadoop_to_Query_Terabytes_of_Data.html">233 high scalability-2008-01-30-How Rackspace Now Uses MapReduce and Hadoop to Query Terabytes of Data</a></p>
<p>18 0.12832369 <a title="1313-tfidf-18" href="../high_scalability-2007/high_scalability-2007-07-12-FeedBurner_Architecture.html">7 high scalability-2007-07-12-FeedBurner Architecture</a></p>
<p>19 0.12437218 <a title="1313-tfidf-19" href="../high_scalability-2012/high_scalability-2012-07-30-Prismatic_Architecture_-_Using_Machine_Learning_on_Social_Networks_to_Figure_Out_What_You_Should_Read_on_the_Web_.html">1293 high scalability-2012-07-30-Prismatic Architecture - Using Machine Learning on Social Networks to Figure Out What You Should Read on the Web </a></p>
<p>20 0.12334289 <a title="1313-tfidf-20" href="../high_scalability-2012/high_scalability-2012-03-27-Big_Data_In_the_Cloud_Using_Cloudify.html">1216 high scalability-2012-03-27-Big Data In the Cloud Using Cloudify</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.185), (1, 0.09), (2, -0.007), (3, 0.053), (4, 0.015), (5, 0.07), (6, 0.082), (7, 0.055), (8, 0.056), (9, 0.063), (10, 0.078), (11, 0.019), (12, 0.085), (13, -0.11), (14, 0.092), (15, -0.028), (16, -0.015), (17, -0.025), (18, -0.023), (19, 0.065), (20, -0.016), (21, 0.034), (22, 0.132), (23, 0.038), (24, 0.052), (25, 0.019), (26, 0.016), (27, 0.04), (28, -0.016), (29, 0.079), (30, 0.096), (31, 0.12), (32, -0.008), (33, 0.023), (34, -0.035), (35, 0.022), (36, -0.045), (37, 0.015), (38, 0.003), (39, -0.049), (40, 0.033), (41, 0.025), (42, -0.087), (43, -0.0), (44, 0.091), (45, 0.031), (46, -0.034), (47, -0.076), (48, -0.046), (49, -0.011)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97426522 <a title="1313-lsi-1" href="../high_scalability-2012/high_scalability-2012-08-28-Making_Hadoop_Run_Faster.html">1313 high scalability-2012-08-28-Making Hadoop Run Faster</a></p>
<p>Introduction: Making Hadoop Run FasterOne of the challenges in processing data is that the
speed at which we can input data is quite often much faster than the speed at
which we can process it. This problem becomes even more pronounced in the
context of Big Data, where the volume of data keeps on growing, along with a
corresponding need for more insights, and thus the need for more complex
processing also increases.Batch Processing to the RescueHadoop was designed to
deal with this challenge in the following ways:1. Use a distributed file
system: This enables us to spread the load and grow our system as needed.2.
Optimize for write speed: To enable fast writes the Hadoop architecture was
designed so that writes are first logged, and then processed. This enables
fairly fast write speeds.3. Use batch processing (Map/Reduce) to balance the
speed for the data feeds with the processing speed.Batch Processing
ChallengesThe challenge with batch-processing is that it assumes that the
feeds come in bursts. I</p><p>2 0.8631683 <a title="1313-lsi-2" href="../high_scalability-2009/high_scalability-2009-08-03-Building_a_Data_Intensive_Web_Application_with_Cloudera%2C_Hadoop%2C_Hive%2C_Pig%2C_and_EC2.html">669 high scalability-2009-08-03-Building a Data Intensive Web Application with Cloudera, Hadoop, Hive, Pig, and EC2</a></p>
<p>Introduction: This tutorial will show you how to use Amazon EC2 and Cloudera's Distribution
for Hadoop to run batch jobs for a data intensive web application.During the
tutorial, we will perform the following data processing steps....read more on
Cloudera website</p><p>3 0.84247363 <a title="1313-lsi-3" href="../high_scalability-2013/high_scalability-2013-01-07-Analyzing_billions_of_credit_card_transactions_and_serving_low-latency_insights_in_the_cloud.html">1382 high scalability-2013-01-07-Analyzing billions of credit card transactions and serving low-latency insights in the cloud</a></p>
<p>Introduction: This is a guest post byIvan de PradoandPere Ferrera, founders ofDatasalt, the
company behindPangoolandSplout SQLBig Data open-source projects.The amount of
payments performed using credit cards is huge. It is clear that there is
inherent value in the data that can be derived from analyzing all the
transactions. Client fidelity, demographics, heat maps of activity, shop
recommendations, and many other statistics are useful to both clients and
shops for improving their relationship with the market. AtDatasaltwe have
developed a system in collaboration with theBBVA bankthat is able to analyze
years of data and serve insights and statistics to different low-latency web
and mobile applications.The main challenge we faced besides processing Big
Data input is thatthe output was also Big Data, and even bigger than the
input. And this output needed to be served quickly, under high load.The
solution we developed has an infrastructure cost of just a few thousands of
dollars per month thanks to th</p><p>4 0.82961828 <a title="1313-lsi-4" href="../high_scalability-2009/high_scalability-2009-05-17-Product%3A_Hadoop.html">601 high scalability-2009-05-17-Product: Hadoop</a></p>
<p>Introduction: Update 5:Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62
Secondsand has itsgreen cred questionedbecause it took 40 times the number of
machines Greenplum used to do the same work.Update 4:Introduction to Pig. Pig
allows you to skip programming Hadoop at the low map-reduce level. You don't
have to know Java. Using the Pig Latin language, which is a scripting data
flow language, you can think about your problem as a data flow program. 10
lines of Pig Latin = 200 lines of Java.Update 3: Scaling Hadoop to4000 nodes
at Yahoo!. 30,000 cores with nearly 16PB of raw disk; sorted 6TB of data
completed in 37 minutes; 14,000 map tasks writes (reads) 360 MB (about 3
blocks) of data into a single file with a total of 5.04 TB for the whole
job.Update 2: HadoopSummit and Data-Intensive Computing Symposium Videos and
Slides. Topics include: Pig, JAQL, Hbase, Hive, Data-Intensive Scalable
Computing, Clouds and ManyCore: The Revolution, Simplicity and Complexity in
Data Systems at Scale, Han</p><p>5 0.80945331 <a title="1313-lsi-5" href="../high_scalability-2012/high_scalability-2012-01-12-Peregrine_-_A_Map_Reduce_Framework_for_Iterative_and_Pipelined_Jobs.html">1173 high scalability-2012-01-12-Peregrine - A Map Reduce Framework for Iterative and Pipelined Jobs</a></p>
<p>Introduction: The Peregrine falcon is a bird of prey, famous for itshigh speed
divingattacks, feeding primarily on much slower Hadoops. Wait, sorry, it is
Kevin Burton of Spinn3r's newPeregrine project--anew FAST modern map reduce
framework optimized for iterative and pipelined map reduce jobs--that feeds on
Hadoops.If you don't know Kevin, he does a lot of excellent technical work
that he's kind enough to share it onhis blog. Only he hasn't been blogging
much lately, he's been heads down working on Peregrine. Now that Peregrine has
been released, here's a short email interview with Kevin on why you might want
to take upfalconry, the ancient sport of MapReduce.What does Spinn3r do that
Peregrine is important to you?Ideally it was designed to execute pagerank but
many iterative applications that we deploy and WANT to deploy (k-means) would
be horribly inefficient under Hadoop as it doesn't have any support for
merging and joining IO between tasks.  It also doesn't support pipeline jobs
from one MR ta</p><p>6 0.78539872 <a title="1313-lsi-6" href="../high_scalability-2013/high_scalability-2013-04-24-Strategy%3A_Using_Lots_of_RAM_Often_Cheaper_than_Using_a_Hadoop_Cluster.html">1445 high scalability-2013-04-24-Strategy: Using Lots of RAM Often Cheaper than Using a Hadoop Cluster</a></p>
<p>7 0.76961851 <a title="1313-lsi-7" href="../high_scalability-2014/high_scalability-2014-03-24-Big%2C_Small%2C_Hot_or_Cold_-_Examples_of_Robust_Data_Pipelines_from_Stripe%2C_Tapad%2C_Etsy_and_Square.html">1618 high scalability-2014-03-24-Big, Small, Hot or Cold - Examples of Robust Data Pipelines from Stripe, Tapad, Etsy and Square</a></p>
<p>8 0.76952493 <a title="1313-lsi-8" href="../high_scalability-2011/high_scalability-2011-01-04-Map-Reduce_With_Ruby_Using_Hadoop.html">968 high scalability-2011-01-04-Map-Reduce With Ruby Using Hadoop</a></p>
<p>9 0.76686174 <a title="1313-lsi-9" href="../high_scalability-2012/high_scalability-2012-06-15-Stuff_The_Internet_Says_On_Scalability_For_June_15%2C_2012.html">1265 high scalability-2012-06-15-Stuff The Internet Says On Scalability For June 15, 2012</a></p>
<p>10 0.76619077 <a title="1313-lsi-10" href="../high_scalability-2009/high_scalability-2009-07-30-Learn_How_to_Think_at_Scale.html">666 high scalability-2009-07-30-Learn How to Think at Scale</a></p>
<p>11 0.76049602 <a title="1313-lsi-11" href="../high_scalability-2008/high_scalability-2008-09-28-Product%3A_Happy_%3D_Hadoop_%2B_Python.html">397 high scalability-2008-09-28-Product: Happy = Hadoop + Python</a></p>
<p>12 0.74849951 <a title="1313-lsi-12" href="../high_scalability-2008/high_scalability-2008-11-14-Paper%3A_Pig_Latin%3A_A_Not-So-Foreign_Language_for_Data_Processing.html">443 high scalability-2008-11-14-Paper: Pig Latin: A Not-So-Foreign Language for Data Processing</a></p>
<p>13 0.74539918 <a title="1313-lsi-13" href="../high_scalability-2010/high_scalability-2010-07-20-Strategy%3A_Consider_When_a_Service_Starts_Billing_in_Your_Algorithm_Cost.html">862 high scalability-2010-07-20-Strategy: Consider When a Service Starts Billing in Your Algorithm Cost</a></p>
<p>14 0.72164446 <a title="1313-lsi-14" href="../high_scalability-2014/high_scalability-2014-01-28-How_Next_Big_Sound_Tracks_Over_a_Trillion_Song_Plays%2C_Likes%2C_and_More_Using_a_Version_Control_System_for_Hadoop_Data.html">1586 high scalability-2014-01-28-How Next Big Sound Tracks Over a Trillion Song Plays, Likes, and More Using a Version Control System for Hadoop Data</a></p>
<p>15 0.7210688 <a title="1313-lsi-15" href="../high_scalability-2008/high_scalability-2008-09-03-MapReduce_framework_Disco.html">376 high scalability-2008-09-03-MapReduce framework Disco</a></p>
<p>16 0.71258348 <a title="1313-lsi-16" href="../high_scalability-2011/high_scalability-2011-03-08-Medialets_Architecture_-__Defeating_the_Daunting_Mobile_Device_Data_Deluge.html">1000 high scalability-2011-03-08-Medialets Architecture -  Defeating the Daunting Mobile Device Data Deluge</a></p>
<p>17 0.7061584 <a title="1313-lsi-17" href="../high_scalability-2008/high_scalability-2008-10-15-Hadoop_-_A_Primer.html">414 high scalability-2008-10-15-Hadoop - A Primer</a></p>
<p>18 0.70117247 <a title="1313-lsi-18" href="../high_scalability-2010/high_scalability-2010-12-08-How_To_Get_Experience_Working_With_Large_Datasets.html">956 high scalability-2010-12-08-How To Get Experience Working With Large Datasets</a></p>
<p>19 0.68698907 <a title="1313-lsi-19" href="../high_scalability-2009/high_scalability-2009-07-02-Product%3A_Hbase.html">650 high scalability-2009-07-02-Product: Hbase</a></p>
<p>20 0.67954749 <a title="1313-lsi-20" href="../high_scalability-2010/high_scalability-2010-02-19-Twitter%E2%80%99s_Plan_to_Analyze_100_Billion_Tweets.html">780 high scalability-2010-02-19-Twitter’s Plan to Analyze 100 Billion Tweets</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.166), (2, 0.23), (30, 0.018), (40, 0.026), (56, 0.022), (61, 0.099), (73, 0.104), (79, 0.153), (85, 0.015), (94, 0.078)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.97015798 <a title="1313-lda-1" href="../high_scalability-2012/high_scalability-2012-01-17-Paper%3A_Feeding_Frenzy%3A_Selectively_Materializing_Users%E2%80%99_Event_Feeds.html">1175 high scalability-2012-01-17-Paper: Feeding Frenzy: Selectively Materializing Users’ Event Feeds</a></p>
<p>Introduction: How do you scale an inbox that has multiple highly volatile feeds? That's a
problem faced by social networks like Tumblr, Facebook, and Twitter. Follow a
few hundred event sources and it's hard to scalably order an inbox so that you
see a correct view as event sources continually publish new events.This can be
considered like aview materializationproblem in a database. In a database a
view is a virtual table defined by a query that can be accessed like a table.
Materialization refers to when the data behind the view is created. If a view
is a join on several tables and that join is performed when the view is
accessed, then performance will be slow. If the view is precomputed access to
the view will be fast, but more resources are used, especially considering
that the view may never be accessed.Your wall/inbox/stream is a view on all
the people/things you follow. If you never look at your inbox then
materializing the view in your inbox is a waste of resources, yet you'll be
mad if displ</p><p>2 0.96469986 <a title="1313-lda-2" href="../high_scalability-2011/high_scalability-2011-01-28-Stuff_The_Internet_Says_On_Scalability_For_January_28%2C_2011.html">980 high scalability-2011-01-28-Stuff The Internet Says On Scalability For January 28, 2011</a></p>
<p>Introduction: Submitted for your reading pleasure...Something we get to say more often than
you might expect - funny NoSQL comic: How to Write a CV (SFW)Playtomic shows
hows how tohandle over 300 million events per day, in real time, on a budget.
More Speed, at $80,000 a Millisecond. Doeslatency matter? Oh yes..."On the
Chicago to New York route in the US, three milliseconds can mean the
difference between US$2,000 a month and US$250,000 a month."Quotable
Quotes@jkalucki: Throwing 1,920 CPUs and 4TB of RAM at an annoyance, as you
do. @jointheflock@hkanji: Scale can come quick and come hard. Be
prepared.@elenacarstoiu: When you say #Cloud, everybody's thinking lower cost.
Agility, scalability and fast access are advantages far more
important.@BillGates: From Melinda - Research proves we can save newborn lives
at scale Kosmix with a fascinating look atCassandra on SSD,  summarizing some
of what they've learned over the past year running SSD's and more recently
running Cassandra on SSD. Why run somethi</p><p>3 0.9642449 <a title="1313-lda-3" href="../high_scalability-2007/high_scalability-2007-07-26-ThemBid_Architecture.html">33 high scalability-2007-07-26-ThemBid Architecture</a></p>
<p>Introduction: ThemBid provides a market where people needing work done broadcast their
request and accept bids from people competing for the job. Unlike many of the
sites profiled at HighScalability, ThemBid is not in the popular press as
often as Paris Hilton. It's not a media darling or a giant of the industry.
But what I like is they have a strategy, a point-of-view for building websites
and were gracious enough to share very detailed instructions on how to go
about building a website. They even delve into actual installation details of
the various software packages they use. Anyone can benefit by taking a look at
their work.Site: http://www.thembid.com/Information SourcesBuild Scalable Web
2.0 Sites with Ubuntu, Symfony, and LighttpdPlatformLinux
(Ubuntu)SymfonyLighttpdPHPeAcceleratorEclipseMuninAWStatsWhat's Inside?The
StatsStarted work in December of 2006 and had a full demo by March 2007.One
developer/sys admin worked with a part-time graphics designer.Targeted a few
thousand users after laun</p><p>4 0.96117216 <a title="1313-lda-4" href="../high_scalability-2014/high_scalability-2014-01-29-10_Things_Bitly_Should_Have_Monitored.html">1587 high scalability-2014-01-29-10 Things Bitly Should Have Monitored</a></p>
<p>Introduction: Monitor, monitor, monitor. That's the advice every startup gives once they
reach a certain size. But can you ever monitor enough? If you are Bitly and
everyone will complain when you are down, probably not.Here are 10 Things We
Forgot to Monitor from Bitly, along with good stories and copious amounts of
code snippets. Well worth reading, especially after you've already started
monitoring the lower hanging fruit.An interesting revelation from the article
is that:We run bitly split across two data centers, one is a managed
environment with DELL hardware, and the second is Amazon EC2.  Fork Rate. A
strange configuration issue caused processes to be created at a rate of
several hundred a second rather than the expected 1-10/second. Flow control
packets.  A network configuration that honors flow control packets and isn't
configured to disable them, can temporarily cause dropped traffic.Swap In/Out
Rate. Measure the right thing. It's the rate memory is swapped in/out that can
impact performa</p><p>5 0.95653749 <a title="1313-lda-5" href="../high_scalability-2010/high_scalability-2010-12-13-Still_Time_to_Attend_My_Webinar_Tomorrow%3A_What_Should_I_Do%3F_Choosing_SQL%2C_NoSQL_or_Both_for_Scalable_Web_Applications.html">957 high scalability-2010-12-13-Still Time to Attend My Webinar Tomorrow: What Should I Do? Choosing SQL, NoSQL or Both for Scalable Web Applications</a></p>
<p>Introduction: It's time to do something a little different and for me that doesn't mean
cutting off my hair and joining a monastery, nor does it mean buying a cherry
red convertible (yet), it means doing a webinar!On December 14th, 2:00 PM -
3:00 PM EST, I'll be hosting What Should I Do? Choosing SQL, NoSQL or Both for
Scalable Web Applications.The webinar is sponsored by VoltDB, but it will be
completely vendor independent, as that's the only honor preserving and
technically accurate way of doing these things.The webinar will run about 60
minutes, with 40 minutes of speechifying and 20 minutes for questions.The
hashtag for the event on Twitter will beSQLNoSQL. I'll be monitoring that
hashtag if you have any suggestions for the webinar or if you would like to
ask questions during the webinar. The motivation for me to do the webinar was
a talk I had with another audience member at theNoSQL Evening in Palo Alto. He
said he came from a Java background and was confused about the future. His
crystal ball</p><p>6 0.95653677 <a title="1313-lda-6" href="../high_scalability-2010/high_scalability-2010-11-18-Announcing_My_Webinar_on_December_14th%3A_What_Should_I_Do%3F_Choosing_SQL%2C_NoSQL_or_Both_for_Scalable_Web_Applications.html">945 high scalability-2010-11-18-Announcing My Webinar on December 14th: What Should I Do? Choosing SQL, NoSQL or Both for Scalable Web Applications</a></p>
<p>same-blog 7 0.95370531 <a title="1313-lda-7" href="../high_scalability-2012/high_scalability-2012-08-28-Making_Hadoop_Run_Faster.html">1313 high scalability-2012-08-28-Making Hadoop Run Faster</a></p>
<p>8 0.9508127 <a title="1313-lda-8" href="../high_scalability-2014/high_scalability-2014-05-02-Stuff_The_Internet_Says_On_Scalability_For_May_2nd%2C_2014.html">1642 high scalability-2014-05-02-Stuff The Internet Says On Scalability For May 2nd, 2014</a></p>
<p>9 0.94830668 <a title="1313-lda-9" href="../high_scalability-2009/high_scalability-2009-02-21-Google_AppEngine_-_A_Second_Look.html">517 high scalability-2009-02-21-Google AppEngine - A Second Look</a></p>
<p>10 0.94331849 <a title="1313-lda-10" href="../high_scalability-2011/high_scalability-2011-05-10-Viddler_Architecture_-_7_Million_Embeds_a_Day_and_1500_Req-Sec_Peak__.html">1037 high scalability-2011-05-10-Viddler Architecture - 7 Million Embeds a Day and 1500 Req-Sec Peak  </a></p>
<p>11 0.9430849 <a title="1313-lda-11" href="../high_scalability-2009/high_scalability-2009-09-19-Space_Based_Programming_in_.NET.html">709 high scalability-2009-09-19-Space Based Programming in .NET</a></p>
<p>12 0.94244879 <a title="1313-lda-12" href="../high_scalability-2008/high_scalability-2008-04-08-Google_AppEngine_-_A_First_Look.html">301 high scalability-2008-04-08-Google AppEngine - A First Look</a></p>
<p>13 0.94186461 <a title="1313-lda-13" href="../high_scalability-2011/high_scalability-2011-08-22-Strategy%3A_Run_a_Scalable%2C_Available%2C_and_Cheap_Static_Site_on_S3_or_GitHub.html">1102 high scalability-2011-08-22-Strategy: Run a Scalable, Available, and Cheap Static Site on S3 or GitHub</a></p>
<p>14 0.94154412 <a title="1313-lda-14" href="../high_scalability-2008/high_scalability-2008-01-29-Building_scalable_storage_into_application_-_Instead_of_MogileFS_OpenAFS_etc..html">229 high scalability-2008-01-29-Building scalable storage into application - Instead of MogileFS OpenAFS etc.</a></p>
<p>15 0.93986285 <a title="1313-lda-15" href="../high_scalability-2008/high_scalability-2008-03-03-Read_This_Site_and_Ace_Your_Next_Interview%21.html">264 high scalability-2008-03-03-Read This Site and Ace Your Next Interview!</a></p>
<p>16 0.93985736 <a title="1313-lda-16" href="../high_scalability-2010/high_scalability-2010-06-07-Six_Ways_Twitter_May_Reach_its_Big_Hairy_Audacious_Goal_of_One_Billion_Users.html">837 high scalability-2010-06-07-Six Ways Twitter May Reach its Big Hairy Audacious Goal of One Billion Users</a></p>
<p>17 0.93849951 <a title="1313-lda-17" href="../high_scalability-2010/high_scalability-2010-11-29-Stuff_the_Internet_Says_on_Scalability_For_November_29th%2C_2010.html">949 high scalability-2010-11-29-Stuff the Internet Says on Scalability For November 29th, 2010</a></p>
<p>18 0.93749726 <a title="1313-lda-18" href="../high_scalability-2010/high_scalability-2010-02-12-Hot_Scalability_Links_for_February_12%2C_2010.html">776 high scalability-2010-02-12-Hot Scalability Links for February 12, 2010</a></p>
<p>19 0.93718874 <a title="1313-lda-19" href="../high_scalability-2012/high_scalability-2012-07-02-C_is_for_Compute_-_Google_Compute_Engine_%28GCE%29.html">1275 high scalability-2012-07-02-C is for Compute - Google Compute Engine (GCE)</a></p>
<p>20 0.93682593 <a title="1313-lda-20" href="../high_scalability-2014/high_scalability-2014-05-16-Stuff_The_Internet_Says_On_Scalability_For_May_16th%2C_2014.html">1649 high scalability-2014-05-16-Stuff The Internet Says On Scalability For May 16th, 2014</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
