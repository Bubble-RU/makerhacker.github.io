<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1098 high scalability-2011-08-15-Should any cloud be considered one availability zone? The Amazon experience says yes.</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2011" href="../home/high_scalability-2011_home.html">high_scalability-2011</a> <a title="high_scalability-2011-1098" href="#">high_scalability-2011-1098</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1098 high scalability-2011-08-15-Should any cloud be considered one availability zone? The Amazon experience says yes.</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2011-1098-html" href="http://highscalability.com//blog/2011/8/15/should-any-cloud-be-considered-one-availability-zone-the-ama.html">html</a></p><p>Introduction: Amazon has a very will written account of their 8/8/2011 downtime:  Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region . Power failed, backup generators failed to kick in, there weren't enough resources for EBS volumes to recover, API servers where overwhelmed, a DNS failure caused failovers to alternate availability zones to fail, a double fault occurred as the power event interrupted the repair of a different bug. All kind of typical stuff that just seems to happen.
 
Considering the  previous outage , the big question for programmers is: what does this mean? What does it mean for how systems should be structured? Have we learned something that can't be unlearned?
 
The Amazon post has lots of good insights into how EBS and RDS work, plus lessons learned. The short of the problem is large + complex = high probability of failure. The immediate fixes are adding more resources, more redundancy, more isolation between components, more automation, re</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Power failed, backup generators failed to kick in, there weren't enough resources for EBS volumes to recover, API servers where overwhelmed, a DNS failure caused failovers to alternate availability zones to fail, a double fault occurred as the power event interrupted the repair of a different bug. [sent-2, score-1.456]
</p><p>2 What does it mean for how systems should be structured? [sent-5, score-0.09]
</p><p>3 The short of the problem is large + complex = high probability of failure. [sent-8, score-0.183]
</p><p>4 The immediate fixes are adding more resources, more redundancy, more isolation between components, more automation, reduce recovery times, and build software that is more aware of large scale failure modes. [sent-9, score-0.247]
</p><p>5 We can predict, however, problems like this will continue to happen, not because of any incompetence by Amazon, but because: large + complex make  cascading failure  an inherent characteristic of the system. [sent-12, score-0.484]
</p><p>6 At some level of complexity any cloud/region/datacenter could be reasonably considered a single failure domain and should be treated accordingly, regardless of the heroic software infrastructure created to carve out availability zones. [sent-13, score-0.76]
</p><p>7 Viewing a region as a single point of failure implies to be really safe you would need to be in multiple regions, which is to say multiple locations. [sent-14, score-0.369]
</p><p>8 Diversity as mother nature's means of robustness would indicate using different providers as a good strategy. [sent-15, score-0.273]
</p><p>9 Something a lot of people have been saying for a while, but with more evidence coming in, that conclusion is even stronger now. [sent-16, score-0.326]
</p><p>10 For most projects this conclusion doesn't really matter all that much. [sent-18, score-0.145]
</p><p>11 100% uptime is extremely expensive and Amazon will usually keep your infrastructure up and working. [sent-19, score-0.087]
</p><p>12 Most of the time multiple Availability Zones are all you need. [sent-20, score-0.101]
</p><p>13 All this diversity of course is very expensive and and very complicated. [sent-23, score-0.243]
</p><p>14 Another option is a retreat into radical simplicity. [sent-30, score-0.135]
</p><p>15 Related Articles      Amazon Discussion Forums     Apache Libcloud  - a unified interface to the cloud. [sent-33, score-0.083]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('amazon', 0.261), ('double', 0.202), ('ebs', 0.198), ('failure', 0.167), ('diversity', 0.156), ('zones', 0.146), ('rds', 0.145), ('conclusion', 0.145), ('failed', 0.135), ('failovers', 0.135), ('incompetence', 0.135), ('retreat', 0.135), ('accordingly', 0.127), ('eu', 0.121), ('articlesamazon', 0.121), ('interrupted', 0.121), ('cake', 0.11), ('synchronizing', 0.11), ('carve', 0.11), ('heroic', 0.107), ('recovering', 0.107), ('earned', 0.102), ('multiple', 0.101), ('availability', 0.099), ('alternate', 0.099), ('overwhelmed', 0.097), ('robustness', 0.096), ('reasonably', 0.096), ('characteristic', 0.096), ('problem', 0.095), ('complexity', 0.095), ('mother', 0.094), ('generators', 0.093), ('occurred', 0.092), ('rightscale', 0.091), ('stronger', 0.091), ('mean', 0.09), ('evidence', 0.09), ('radically', 0.089), ('probability', 0.088), ('expensive', 0.087), ('treated', 0.086), ('cascading', 0.086), ('repair', 0.084), ('kick', 0.083), ('indicate', 0.083), ('eat', 0.083), ('unified', 0.083), ('west', 0.082), ('fixes', 0.08)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="1098-tfidf-1" href="../high_scalability-2011/high_scalability-2011-08-15-Should_any_cloud_be_considered_one_availability_zone%3F_The_Amazon_experience_says_yes..html">1098 high scalability-2011-08-15-Should any cloud be considered one availability zone? The Amazon experience says yes.</a></p>
<p>Introduction: Amazon has a very will written account of their 8/8/2011 downtime:  Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region . Power failed, backup generators failed to kick in, there weren't enough resources for EBS volumes to recover, API servers where overwhelmed, a DNS failure caused failovers to alternate availability zones to fail, a double fault occurred as the power event interrupted the repair of a different bug. All kind of typical stuff that just seems to happen.
 
Considering the  previous outage , the big question for programmers is: what does this mean? What does it mean for how systems should be structured? Have we learned something that can't be unlearned?
 
The Amazon post has lots of good insights into how EBS and RDS work, plus lessons learned. The short of the problem is large + complex = high probability of failure. The immediate fixes are adding more resources, more redundancy, more isolation between components, more automation, re</p><p>2 0.23431148 <a title="1098-tfidf-2" href="../high_scalability-2011/high_scalability-2011-04-25-The_Big_List_of_Articles_on_the_Amazon_Outage.html">1029 high scalability-2011-04-25-The Big List of Articles on the Amazon Outage</a></p>
<p>Introduction: Please see  The Updated Big List Of Articles On The Amazon Outage  for a new improved list. 
 
So many great articles have been written on the Amazon Outage. Some aim at being helpful, some chastise developers for being so stupid, some chastise Amazon for being so incompetent, some talk about the pain they and their companies have experienced, and some even predict the downfall of the cloud. Still others say we have seen a sea change in future of the cloud, a prediction that's hard to disagree with, though the shape of the change remains...cloudy.
 
I'll try to keep this list update as more information comes out. There will be a lot for developers to consider going forward. If there's a resource you think should be added, just let me know.
  Amazon's Explanation of What Happened   
  Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region  
  Hackers News thread on AWS Service Disruption Post Mortem   
  Quite Funny Commentary on the Summary  
   Experiences f</p><p>3 0.21025582 <a title="1098-tfidf-3" href="../high_scalability-2011/high_scalability-2011-05-02-The_Updated_Big_List_of_Articles_on_the_Amazon_Outage.html">1033 high scalability-2011-05-02-The Updated Big List of Articles on the Amazon Outage</a></p>
<p>Introduction: Since  The Big List Of Articles On The Amazon Outage  was published we've a had few updates that people might not have seen. Amazon of course released their  Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region . Netlix shared their  Lessons Learned from the AWS Outage  as did Heroku ( How Heroku Survived the Amazon Outage ), Smug Mug ( How SmugMug survived the Amazonpocalypse ), and SimpleGeo ( How SimpleGeo Stayed Up During the AWS Downtime ). 
 
The curious thing from my perspective is the general lack of response to Amazon's explanation. I expected more discussion. There's been almost none that I've seen. My guess is very few people understand what Amazon was talking about enough to comment whereas almost everyone feels qualified to talk about the event itself.
 
 Lesson for crisis handlers : deep dive post-mortems that are timely, long, honestish, and highly technical are the most effective means of staunching the downward spiral of media attention.</p><p>4 0.20709045 <a title="1098-tfidf-4" href="../high_scalability-2014/high_scalability-2014-04-14-How_do_you_even_do_anything_without_using_EBS%3F.html">1631 high scalability-2014-04-14-How do you even do anything without using EBS?</a></p>
<p>Introduction: In a recent thread on Hacker News discussing  recent AWS price changes ,  seldo  mentioned they use AWS for business, they just never use EBS on AWS. A good question was asked:
  

How do you even do anything without using EBS?

  
Amazon certainly makes using EBS the easiest path. And EBS has a better reliability record as of late, but it's still often recommended to not use EBS. This avoids a single point of failure at the cost of a lot of complexity, though as AWS uses EBS internally, not using EBS may not save you if you use other AWS services like RDS or ELB.
 
If you don't want to use EBS, it's hard to know where to even start. A dilemma to which Kevin Nuckolls  gives a great answer :
  

Well, you break your services out onto stateless and stateful machines. After that, you make sure that each of your stateful services is resilient to individual node failure. I prefer to believe that if you can't roll your entire infrastructure over to new nodes monthly then you're unprepared fo</p><p>5 0.16586497 <a title="1098-tfidf-5" href="../high_scalability-2010/high_scalability-2010-04-28-Elasticity_for_the_Enterprise_--_Ensuring_Continuous_High_Availability_in_a_Disaster_Failure_Scenario.html">816 high scalability-2010-04-28-Elasticity for the Enterprise -- Ensuring Continuous High Availability in a Disaster Failure Scenario</a></p>
<p>Introduction: Many enterprises' high-availability architecture is based on the assumption that you can prevent failure from happening by putting all your critical data in a centralized database, back it up with expensive storage, and replicate it somehow between the sites. As I argued in one of my previous posts ( Why Existing Databases (RAC) are So Breakable! ) many of those assumptions are broken at their core, as storage is doomed to failure just like any other device, expensive hardware doesn’t make things any better and database replication is often not enough.
 
One of the main lessons that we can take from the likes of Amazon and Google is that the right way to ensure continuous high availability is by designing our system to cope with failure. We need to assume that what we tend to think of as unthinkable will probably happen, as that’s the nature of failure. So rather than trying to prevent failures, we need to build a system that will tolerate them.
 
As we can learn from a  recent outage</p><p>6 0.16428469 <a title="1098-tfidf-6" href="../high_scalability-2014/high_scalability-2014-03-03-The_%E2%80%9CFour_Hamiltons%E2%80%9D_Framework_for_Mitigating_Faults_in_the_Cloud%3A_Avoid_it%2C_Mask_it%2C_Bound_it%2C_Fix_it_Fast.html">1604 high scalability-2014-03-03-The “Four Hamiltons” Framework for Mitigating Faults in the Cloud: Avoid it, Mask it, Bound it, Fix it Fast</a></p>
<p>7 0.16334963 <a title="1098-tfidf-7" href="../high_scalability-2007/high_scalability-2007-07-30-Build_an_Infinitely_Scalable_Infrastructure_for_%24100_Using_Amazon_Services.html">38 high scalability-2007-07-30-Build an Infinitely Scalable Infrastructure for $100 Using Amazon Services</a></p>
<p>8 0.15376636 <a title="1098-tfidf-8" href="../high_scalability-2008/high_scalability-2008-03-27-Amazon_Announces_Static_IP_Addresses_and_Multiple_Datacenter_Operation.html">289 high scalability-2008-03-27-Amazon Announces Static IP Addresses and Multiple Datacenter Operation</a></p>
<p>9 0.14475614 <a title="1098-tfidf-9" href="../high_scalability-2010/high_scalability-2010-07-08-Cloud_AWS_Infrastructure_vs._Physical_Infrastructure.html">853 high scalability-2010-07-08-Cloud AWS Infrastructure vs. Physical Infrastructure</a></p>
<p>10 0.14168009 <a title="1098-tfidf-10" href="../high_scalability-2012/high_scalability-2012-10-26-Stuff_The_Internet_Says_On_Scalability_For_October_26%2C_2012.html">1348 high scalability-2012-10-26-Stuff The Internet Says On Scalability For October 26, 2012</a></p>
<p>11 0.14041287 <a title="1098-tfidf-11" href="../high_scalability-2010/high_scalability-2010-08-16-Scaling_an_AWS_infrastructure_-_Tools_and_Patterns.html">881 high scalability-2010-08-16-Scaling an AWS infrastructure - Tools and Patterns</a></p>
<p>12 0.13988639 <a title="1098-tfidf-12" href="../high_scalability-2013/high_scalability-2013-02-04-Is_Provisioned_IOPS_Better%3F_Yes%2C_it_Delivers_More_Consistent_and_Higher_Performance_IO.html">1398 high scalability-2013-02-04-Is Provisioned IOPS Better? Yes, it Delivers More Consistent and Higher Performance IO</a></p>
<p>13 0.13791652 <a title="1098-tfidf-13" href="../high_scalability-2007/high_scalability-2007-09-18-Amazon_Architecture.html">96 high scalability-2007-09-18-Amazon Architecture</a></p>
<p>14 0.12979527 <a title="1098-tfidf-14" href="../high_scalability-2011/high_scalability-2011-10-27-Strategy%3A_Survive_a_Comet_Strike_in_the_East_With_Reserved_Instances_in_the_West.html">1133 high scalability-2011-10-27-Strategy: Survive a Comet Strike in the East With Reserved Instances in the West</a></p>
<p>15 0.12467467 <a title="1098-tfidf-15" href="../high_scalability-2010/high_scalability-2010-03-05-Strategy%3A_Planning_for_a_Power_Outage_Google_Style.html">789 high scalability-2010-03-05-Strategy: Planning for a Power Outage Google Style</a></p>
<p>16 0.1187285 <a title="1098-tfidf-16" href="../high_scalability-2011/high_scalability-2011-09-21-5_Scalability_Poisons_and_3_Cloud_Scalability_Antidotes.html">1121 high scalability-2011-09-21-5 Scalability Poisons and 3 Cloud Scalability Antidotes</a></p>
<p>17 0.11252781 <a title="1098-tfidf-17" href="../high_scalability-2008/high_scalability-2008-12-30-Scalability_Perspectives_%235%3A_Werner_Vogels_%E2%80%93_The_Amazon_Technology_Platform.html">480 high scalability-2008-12-30-Scalability Perspectives #5: Werner Vogels – The Amazon Technology Platform</a></p>
<p>18 0.11127427 <a title="1098-tfidf-18" href="../high_scalability-2010/high_scalability-2010-05-26-End-To-End_Performance_Study_of_Cloud_Services.html">831 high scalability-2010-05-26-End-To-End Performance Study of Cloud Services</a></p>
<p>19 0.11035844 <a title="1098-tfidf-19" href="../high_scalability-2010/high_scalability-2010-10-22-Paper%3A_Netflix%E2%80%99s_Transition_to_High-Availability_Storage_Systems_.html">925 high scalability-2010-10-22-Paper: Netflix’s Transition to High-Availability Storage Systems </a></p>
<p>20 0.10753338 <a title="1098-tfidf-20" href="../high_scalability-2012/high_scalability-2012-10-02-An_Epic_TripAdvisor_Update%3A_Why_Not_Run_on_the_Cloud%3F_The_Grand_Experiment..html">1331 high scalability-2012-10-02-An Epic TripAdvisor Update: Why Not Run on the Cloud? The Grand Experiment.</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.17), (1, 0.101), (2, -0.011), (3, 0.09), (4, -0.053), (5, -0.078), (6, 0.02), (7, -0.123), (8, 0.066), (9, -0.155), (10, -0.035), (11, -0.022), (12, 0.016), (13, -0.078), (14, -0.009), (15, -0.003), (16, 0.062), (17, 0.006), (18, -0.024), (19, 0.055), (20, 0.038), (21, 0.015), (22, 0.009), (23, 0.057), (24, -0.091), (25, -0.027), (26, 0.023), (27, 0.048), (28, 0.042), (29, 0.021), (30, -0.067), (31, -0.027), (32, 0.091), (33, -0.096), (34, 0.042), (35, -0.029), (36, 0.046), (37, 0.082), (38, -0.015), (39, 0.011), (40, 0.061), (41, -0.077), (42, -0.065), (43, -0.057), (44, 0.061), (45, -0.037), (46, -0.006), (47, 0.013), (48, 0.045), (49, -0.012)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97647583 <a title="1098-lsi-1" href="../high_scalability-2011/high_scalability-2011-08-15-Should_any_cloud_be_considered_one_availability_zone%3F_The_Amazon_experience_says_yes..html">1098 high scalability-2011-08-15-Should any cloud be considered one availability zone? The Amazon experience says yes.</a></p>
<p>Introduction: Amazon has a very will written account of their 8/8/2011 downtime:  Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region . Power failed, backup generators failed to kick in, there weren't enough resources for EBS volumes to recover, API servers where overwhelmed, a DNS failure caused failovers to alternate availability zones to fail, a double fault occurred as the power event interrupted the repair of a different bug. All kind of typical stuff that just seems to happen.
 
Considering the  previous outage , the big question for programmers is: what does this mean? What does it mean for how systems should be structured? Have we learned something that can't be unlearned?
 
The Amazon post has lots of good insights into how EBS and RDS work, plus lessons learned. The short of the problem is large + complex = high probability of failure. The immediate fixes are adding more resources, more redundancy, more isolation between components, more automation, re</p><p>2 0.90491211 <a title="1098-lsi-2" href="../high_scalability-2011/high_scalability-2011-04-25-The_Big_List_of_Articles_on_the_Amazon_Outage.html">1029 high scalability-2011-04-25-The Big List of Articles on the Amazon Outage</a></p>
<p>Introduction: Please see  The Updated Big List Of Articles On The Amazon Outage  for a new improved list. 
 
So many great articles have been written on the Amazon Outage. Some aim at being helpful, some chastise developers for being so stupid, some chastise Amazon for being so incompetent, some talk about the pain they and their companies have experienced, and some even predict the downfall of the cloud. Still others say we have seen a sea change in future of the cloud, a prediction that's hard to disagree with, though the shape of the change remains...cloudy.
 
I'll try to keep this list update as more information comes out. There will be a lot for developers to consider going forward. If there's a resource you think should be added, just let me know.
  Amazon's Explanation of What Happened   
  Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region  
  Hackers News thread on AWS Service Disruption Post Mortem   
  Quite Funny Commentary on the Summary  
   Experiences f</p><p>3 0.88369668 <a title="1098-lsi-3" href="../high_scalability-2011/high_scalability-2011-05-02-The_Updated_Big_List_of_Articles_on_the_Amazon_Outage.html">1033 high scalability-2011-05-02-The Updated Big List of Articles on the Amazon Outage</a></p>
<p>Introduction: Since  The Big List Of Articles On The Amazon Outage  was published we've a had few updates that people might not have seen. Amazon of course released their  Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region . Netlix shared their  Lessons Learned from the AWS Outage  as did Heroku ( How Heroku Survived the Amazon Outage ), Smug Mug ( How SmugMug survived the Amazonpocalypse ), and SimpleGeo ( How SimpleGeo Stayed Up During the AWS Downtime ). 
 
The curious thing from my perspective is the general lack of response to Amazon's explanation. I expected more discussion. There's been almost none that I've seen. My guess is very few people understand what Amazon was talking about enough to comment whereas almost everyone feels qualified to talk about the event itself.
 
 Lesson for crisis handlers : deep dive post-mortems that are timely, long, honestish, and highly technical are the most effective means of staunching the downward spiral of media attention.</p><p>4 0.83176845 <a title="1098-lsi-4" href="../high_scalability-2014/high_scalability-2014-04-14-How_do_you_even_do_anything_without_using_EBS%3F.html">1631 high scalability-2014-04-14-How do you even do anything without using EBS?</a></p>
<p>Introduction: In a recent thread on Hacker News discussing  recent AWS price changes ,  seldo  mentioned they use AWS for business, they just never use EBS on AWS. A good question was asked:
  

How do you even do anything without using EBS?

  
Amazon certainly makes using EBS the easiest path. And EBS has a better reliability record as of late, but it's still often recommended to not use EBS. This avoids a single point of failure at the cost of a lot of complexity, though as AWS uses EBS internally, not using EBS may not save you if you use other AWS services like RDS or ELB.
 
If you don't want to use EBS, it's hard to know where to even start. A dilemma to which Kevin Nuckolls  gives a great answer :
  

Well, you break your services out onto stateless and stateful machines. After that, you make sure that each of your stateful services is resilient to individual node failure. I prefer to believe that if you can't roll your entire infrastructure over to new nodes monthly then you're unprepared fo</p><p>5 0.77842212 <a title="1098-lsi-5" href="../high_scalability-2009/high_scalability-2009-04-07-Six_Lessons_Learned_Deploying_a_Large-scale_Infrastructure_in_Amazon_EC2_.html">559 high scalability-2009-04-07-Six Lessons Learned Deploying a Large-scale Infrastructure in Amazon EC2 </a></p>
<p>Introduction: Lessons learned from  OpenX's large-scale deployment  to Amazon EC2:
   Expect failures; what's more, embrace them      Fully automate your infrastructure deployments     Design your infrastructure so that it scales horizontally     Establish clear measurable goals     Be prepared to quickly identify and eliminate bottlenecks     Play wack-a-mole for a while, until things get stable</p><p>6 0.71885335 <a title="1098-lsi-6" href="../high_scalability-2013/high_scalability-2013-02-04-Is_Provisioned_IOPS_Better%3F_Yes%2C_it_Delivers_More_Consistent_and_Higher_Performance_IO.html">1398 high scalability-2013-02-04-Is Provisioned IOPS Better? Yes, it Delivers More Consistent and Higher Performance IO</a></p>
<p>7 0.70374578 <a title="1098-lsi-7" href="../high_scalability-2010/high_scalability-2010-04-28-Elasticity_for_the_Enterprise_--_Ensuring_Continuous_High_Availability_in_a_Disaster_Failure_Scenario.html">816 high scalability-2010-04-28-Elasticity for the Enterprise -- Ensuring Continuous High Availability in a Disaster Failure Scenario</a></p>
<p>8 0.70016158 <a title="1098-lsi-8" href="../high_scalability-2011/high_scalability-2011-10-27-Strategy%3A_Survive_a_Comet_Strike_in_the_East_With_Reserved_Instances_in_the_West.html">1133 high scalability-2011-10-27-Strategy: Survive a Comet Strike in the East With Reserved Instances in the West</a></p>
<p>9 0.69487482 <a title="1098-lsi-9" href="../high_scalability-2012/high_scalability-2012-10-26-Stuff_The_Internet_Says_On_Scalability_For_October_26%2C_2012.html">1348 high scalability-2012-10-26-Stuff The Internet Says On Scalability For October 26, 2012</a></p>
<p>10 0.68917137 <a title="1098-lsi-10" href="../high_scalability-2011/high_scalability-2011-07-20-Netflix%3A_Harden_Systems_Using_a_Barrel_of_Problem_Causing_Monkeys_-_Latency%2C_Conformity%2C_Doctor%2C_Janitor%2C_Security%2C_Internationalization%2C_Chaos.html">1083 high scalability-2011-07-20-Netflix: Harden Systems Using a Barrel of Problem Causing Monkeys - Latency, Conformity, Doctor, Janitor, Security, Internationalization, Chaos</a></p>
<p>11 0.68821174 <a title="1098-lsi-11" href="../high_scalability-2007/high_scalability-2007-10-30-Paper%3A_Dynamo%3A_Amazon%E2%80%99s_Highly_Available_Key-value_Store.html">139 high scalability-2007-10-30-Paper: Dynamo: Amazon’s Highly Available Key-value Store</a></p>
<p>12 0.66543752 <a title="1098-lsi-12" href="../high_scalability-2010/high_scalability-2010-12-28-Netflix%3A_Continually_Test_by_Failing_Servers_with_Chaos_Monkey.html">964 high scalability-2010-12-28-Netflix: Continually Test by Failing Servers with Chaos Monkey</a></p>
<p>13 0.64612263 <a title="1098-lsi-13" href="../high_scalability-2014/high_scalability-2014-03-03-The_%E2%80%9CFour_Hamiltons%E2%80%9D_Framework_for_Mitigating_Faults_in_the_Cloud%3A_Avoid_it%2C_Mask_it%2C_Bound_it%2C_Fix_it_Fast.html">1604 high scalability-2014-03-03-The “Four Hamiltons” Framework for Mitigating Faults in the Cloud: Avoid it, Mask it, Bound it, Fix it Fast</a></p>
<p>14 0.6437093 <a title="1098-lsi-14" href="../high_scalability-2008/high_scalability-2008-03-27-Amazon_Announces_Static_IP_Addresses_and_Multiple_Datacenter_Operation.html">289 high scalability-2008-03-27-Amazon Announces Static IP Addresses and Multiple Datacenter Operation</a></p>
<p>15 0.64322186 <a title="1098-lsi-15" href="../high_scalability-2013/high_scalability-2013-05-29-Amazon%3A_Creating_a_Customer_Utopia_One_Culture_Hack_at_a_Time.html">1466 high scalability-2013-05-29-Amazon: Creating a Customer Utopia One Culture Hack at a Time</a></p>
<p>16 0.64095074 <a title="1098-lsi-16" href="../high_scalability-2007/high_scalability-2007-07-24-Major_Websites_Down%3A_Or_Why_You_Want_to_Run_in_Two_or_More_Data_Centers..html">23 high scalability-2007-07-24-Major Websites Down: Or Why You Want to Run in Two or More Data Centers.</a></p>
<p>17 0.63887924 <a title="1098-lsi-17" href="../high_scalability-2007/high_scalability-2007-07-30-Build_an_Infinitely_Scalable_Infrastructure_for_%24100_Using_Amazon_Services.html">38 high scalability-2007-07-30-Build an Infinitely Scalable Infrastructure for $100 Using Amazon Services</a></p>
<p>18 0.63788247 <a title="1098-lsi-18" href="../high_scalability-2013/high_scalability-2013-06-05-A_Simple_6_Step_Transition_Guide_for_Moving_Away_from_X_to_AWS_.html">1470 high scalability-2013-06-05-A Simple 6 Step Transition Guide for Moving Away from X to AWS </a></p>
<p>19 0.63355035 <a title="1098-lsi-19" href="../high_scalability-2008/high_scalability-2008-12-29-100%25_on_Amazon_Web_Services%3A_Soocial.com_-_a_lesson_of_porting_your_service_to_Amazon.html">477 high scalability-2008-12-29-100% on Amazon Web Services: Soocial.com - a lesson of porting your service to Amazon</a></p>
<p>20 0.63182062 <a title="1098-lsi-20" href="../high_scalability-2010/high_scalability-2010-07-08-Cloud_AWS_Infrastructure_vs._Physical_Infrastructure.html">853 high scalability-2010-07-08-Cloud AWS Infrastructure vs. Physical Infrastructure</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.097), (2, 0.184), (10, 0.103), (20, 0.185), (40, 0.02), (56, 0.034), (61, 0.078), (77, 0.03), (79, 0.14), (94, 0.041)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.93561471 <a title="1098-lda-1" href="../high_scalability-2008/high_scalability-2008-08-18-Forum_sort_order.html">370 high scalability-2008-08-18-Forum sort order</a></p>
<p>Introduction: G'day,     I noticed the default sort order for the forum is to show the posts with the most replies first. That seems a bit odd for a forum. Would it not make sense to show the posts with the most recently replies first?     It is possible to re-sort the forum threads that way by clicking on the "Last post" header (twice). It would seem like a more sensible default.     I've checked and I see the same behaviour as both a registered (logged in) and anonymous user.     Cheers -   Callum  .</p><p>2 0.92472285 <a title="1098-lda-2" href="../high_scalability-2007/high_scalability-2007-07-24-Major_Websites_Down%3A_Or_Why_You_Want_to_Run_in_Two_or_More_Data_Centers..html">23 high scalability-2007-07-24-Major Websites Down: Or Why You Want to Run in Two or More Data Centers.</a></p>
<p>Introduction: A lot of sites hosted in San Francisco are down because of at least 6 back-to-back power outages power outages. More details at  laughingsquid .
   
Sites like SecondLife, Craigstlist, Technorati, Yelp and all Six Apart properties, TypePad, LiveJournal and Vox are all down. The cause was an underground explosion in a transformer vault under a manhole at 560 Mission Street. Flames shot 6 feet out from the manhole cover. Over PG&E; 30,000 customers are without power.  What's perplexing is the UPS backup and diesel generators didn't kick in to bring the datacenter back on line. I've never toured that datacenter, but they usually have massive backup systems. It's probably one of those multiple simultaneous failure situations that you hope never happen in real life, but too often do. Or maybe the infrastructure wasn't rolled out completely.  Update: the cause was a cascade of failures in a tightly couples system that could never happen :-) Details at  Failure Happens: A summary of the power</p><p>same-blog 3 0.89229518 <a title="1098-lda-3" href="../high_scalability-2011/high_scalability-2011-08-15-Should_any_cloud_be_considered_one_availability_zone%3F_The_Amazon_experience_says_yes..html">1098 high scalability-2011-08-15-Should any cloud be considered one availability zone? The Amazon experience says yes.</a></p>
<p>Introduction: Amazon has a very will written account of their 8/8/2011 downtime:  Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region . Power failed, backup generators failed to kick in, there weren't enough resources for EBS volumes to recover, API servers where overwhelmed, a DNS failure caused failovers to alternate availability zones to fail, a double fault occurred as the power event interrupted the repair of a different bug. All kind of typical stuff that just seems to happen.
 
Considering the  previous outage , the big question for programmers is: what does this mean? What does it mean for how systems should be structured? Have we learned something that can't be unlearned?
 
The Amazon post has lots of good insights into how EBS and RDS work, plus lessons learned. The short of the problem is large + complex = high probability of failure. The immediate fixes are adding more resources, more redundancy, more isolation between components, more automation, re</p><p>4 0.86028314 <a title="1098-lda-4" href="../high_scalability-2011/high_scalability-2011-02-24-Strategy%3A_Eliminate_Unnecessary_SQL.html">995 high scalability-2011-02-24-Strategy: Eliminate Unnecessary SQL</a></p>
<p>Introduction: MySQL Expert Ronald Bradford explains how one key way to improve the scalability of a MySQL server, and undoubtedly nearly every other server, is to  eliminate unnecessary SQL , saying  the most efficient way to improve an SQL statement is to eliminate it :
  

The MySQL kernel can only physically process a certain number of SQL statements for a given time period (e.g. per second). Regardless of the type of machine you have, there is a physical limit. If you eliminate SQL statements that are unwarranted and unnecessary, you automatically enable more important SQL statements to run. There are numerous other downstream affects, however this is the simple math. To run more SQL, reduce the number of SQL you need to run.

  
Ronald shows how to use  mk-query-digest  to look at query execution times and determine which ones can be profitably whacked. 
  Related Articles   
 Quora:  What are the best methods for optimizing PHP/MySQL code for speed without caching?</p><p>5 0.83916342 <a title="1098-lda-5" href="../high_scalability-2013/high_scalability-2013-12-18-How_to_get_started_with_sizing_and_capacity_planning%2C_assuming_you_don%27t_know_the_software_behavior%3F.html">1566 high scalability-2013-12-18-How to get started with sizing and capacity planning, assuming you don't know the software behavior?</a></p>
<p>Introduction: Here's a common situation and question from the  mechanical-sympathy  Google group by Avinash Agrawal on the black art of capacity planning:
  

How to get started with sizing and capacity planning, assuming we don't know the software behavior and its completely new product to deal with?

  
 Gil Tene , Vice President of Technology and CTO & Co-Founder, wrote a very  understandable and useful answer  that is worth highlighting:
  

Start with requirements. I see way too many "capacity planning" exercises that go off spending weeks measuring some irrelevant metrics about a system (like how many widgets per hour can this thing do) without knowing what they actually need it to do.


There are two key sets of metrics to state here: the "how much" set and the "how bad" set:


In the "How Much" part, you need to establish, based on expected business needs, Numbers for things (like connections, users, streams, transactions or messages per second) that you expect to interact with at the peak t</p><p>6 0.83472526 <a title="1098-lda-6" href="../high_scalability-2014/high_scalability-2014-03-19-Strategy%3A_Three_Techniques_to_Survive_Traffic_Surges_by_Quickly_Scaling_Your_Site.html">1615 high scalability-2014-03-19-Strategy: Three Techniques to Survive Traffic Surges by Quickly Scaling Your Site</a></p>
<p>7 0.82287896 <a title="1098-lda-7" href="../high_scalability-2010/high_scalability-2010-05-10-Sify.com_Architecture_-_A_Portal_at_3900_Requests_Per_Second.html">825 high scalability-2010-05-10-Sify.com Architecture - A Portal at 3900 Requests Per Second</a></p>
<p>8 0.81991839 <a title="1098-lda-8" href="../high_scalability-2014/high_scalability-2014-02-14-Stuff_The_Internet_Says_On_Scalability_For_February_14th%2C_2014.html">1596 high scalability-2014-02-14-Stuff The Internet Says On Scalability For February 14th, 2014</a></p>
<p>9 0.81187373 <a title="1098-lda-9" href="../high_scalability-2007/high_scalability-2007-11-05-Strategy%3A_Diagonal_Scaling_-_Don%27t_Forget_to_Scale_Out_AND_Up.html">142 high scalability-2007-11-05-Strategy: Diagonal Scaling - Don't Forget to Scale Out AND Up</a></p>
<p>10 0.80914569 <a title="1098-lda-10" href="../high_scalability-2012/high_scalability-2012-05-21-Pinterest_Architecture_Update_-_18_Million_Visitors%2C_10x_Growth%2C12_Employees%2C_410_TB_of_Data.html">1248 high scalability-2012-05-21-Pinterest Architecture Update - 18 Million Visitors, 10x Growth,12 Employees, 410 TB of Data</a></p>
<p>11 0.80868798 <a title="1098-lda-11" href="../high_scalability-2011/high_scalability-2011-09-07-What_Google_App_Engine_Price_Changes_Say_About_the_Future_of_Web_Architecture.html">1112 high scalability-2011-09-07-What Google App Engine Price Changes Say About the Future of Web Architecture</a></p>
<p>12 0.80868292 <a title="1098-lda-12" href="../high_scalability-2012/high_scalability-2012-02-02-The_Data-Scope_Project_-_6PB_storage%2C_500GBytes-sec_sequential_IO%2C_20M_IOPS%2C_130TFlops.html">1186 high scalability-2012-02-02-The Data-Scope Project - 6PB storage, 500GBytes-sec sequential IO, 20M IOPS, 130TFlops</a></p>
<p>13 0.80812877 <a title="1098-lda-13" href="../high_scalability-2009/high_scalability-2009-01-20-Product%3A_Amazon%27s_SimpleDB.html">498 high scalability-2009-01-20-Product: Amazon's SimpleDB</a></p>
<p>14 0.80742908 <a title="1098-lda-14" href="../high_scalability-2008/high_scalability-2008-03-27-Amazon_Announces_Static_IP_Addresses_and_Multiple_Datacenter_Operation.html">289 high scalability-2008-03-27-Amazon Announces Static IP Addresses and Multiple Datacenter Operation</a></p>
<p>15 0.80731159 <a title="1098-lda-15" href="../high_scalability-2009/high_scalability-2009-10-02-HighScalability_has_Moved_to_Squarespace.com%21_.html">714 high scalability-2009-10-02-HighScalability has Moved to Squarespace.com! </a></p>
<p>16 0.80680358 <a title="1098-lda-16" href="../high_scalability-2012/high_scalability-2012-11-01-Cost_Analysis%3A_TripAdvisor_and_Pinterest_costs_on_the_AWS_cloud.html">1353 high scalability-2012-11-01-Cost Analysis: TripAdvisor and Pinterest costs on the AWS cloud</a></p>
<p>17 0.80621296 <a title="1098-lda-17" href="../high_scalability-2011/high_scalability-2011-05-15-Building_a_Database_remote_availability_site.html">1041 high scalability-2011-05-15-Building a Database remote availability site</a></p>
<p>18 0.80598611 <a title="1098-lda-18" href="../high_scalability-2012/high_scalability-2012-12-12-Pinterest_Cut_Costs_from_%2454_to_%2420_Per_Hour_by_Automatically_Shutting_Down_Systems.html">1371 high scalability-2012-12-12-Pinterest Cut Costs from $54 to $20 Per Hour by Automatically Shutting Down Systems</a></p>
<p>19 0.80492878 <a title="1098-lda-19" href="../high_scalability-2014/high_scalability-2014-04-04-Stuff_The_Internet_Says_On_Scalability_For_April_4th%2C_2014.html">1626 high scalability-2014-04-04-Stuff The Internet Says On Scalability For April 4th, 2014</a></p>
<p>20 0.80479962 <a title="1098-lda-20" href="../high_scalability-2011/high_scalability-2011-12-19-How_Twitter_Stores_250_Million_Tweets_a_Day_Using_MySQL.html">1159 high scalability-2011-12-19-How Twitter Stores 250 Million Tweets a Day Using MySQL</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
