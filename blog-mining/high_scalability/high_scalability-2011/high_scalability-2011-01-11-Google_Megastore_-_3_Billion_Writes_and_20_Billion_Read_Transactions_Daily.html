<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>972 high scalability-2011-01-11-Google Megastore - 3 Billion Writes and 20 Billion Read Transactions Daily</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2011" href="../home/high_scalability-2011_home.html">high_scalability-2011</a> <a title="high_scalability-2011-972" href="#">high_scalability-2011-972</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>972 high scalability-2011-01-11-Google Megastore - 3 Billion Writes and 20 Billion Read Transactions Daily</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2011-972-html" href="http://highscalability.com//blog/2011/1/11/google-megastore-3-billion-writes-and-20-billion-read-transa.html">html</a></p><p>Introduction: A giant step into the fully distributed future has been taken by the Google App Engine team with the release of their  High Replication Datastore . The HRD is targeted at mission critical applications that require data replicated to at least three datacenters, full ACID semantics for  entity groups , and lower consistency guarantees across entity groups.
 
This is a major accomplishment. Few organizations can implement a true multi-datacenter datastore. Other than SimpleDB, how many other publicly accessible database services can operate out of multiple datacenters? Now that capability can be had by anyone. But there is a price, literally and otherwise. Because the HRD uses three times the resources as Google App Engine's Master/Slave datastatore, it will cost three times as much. And because it is a distributed database, with all that implies in the CAP sense, developers will have to be very careful in how they architect their applications because as costs increased, reliability incre</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The HRD is targeted at mission critical applications that require data replicated to at least three datacenters, full ACID semantics for  entity groups , and lower consistency guarantees across entity groups. [sent-2, score-1.466]
</p><p>2 This is why HRD is targeted ay mission critical applications, you gotta want it, otherwise the Master/Slave datastore makes a lot more sense. [sent-10, score-0.447]
</p><p>3 Some Megastore   highlights:     Megastore  blends the scalability of a NoSQL datastore with the convenience of a traditional RDBMS . [sent-15, score-0.366]
</p><p>4 It is novel in that it blends the scalability of a NoSQL datastore with the convenience of a traditional RDBMS. [sent-18, score-0.438]
</p><p>5 NoSQL design space: we partition the datastore and replicate each partition separately, providing full ACID semantics within partitions, but only limited consistency guarantees across them. [sent-22, score-0.53]
</p><p>6 Entity groups  are now a unit of consistency as well as a unit of transactionality. [sent-31, score-0.409]
</p><p>7 The underlying data is stored in a scalable NoSQL datastore in each datacenter. [sent-34, score-0.269]
</p><p>8 The App Engine Datastore doesn't support transactions across multiple entity groups because it will greatly limit the write throughput when not operating on an entity group, though Megastore does support these operations. [sent-35, score-0.751]
</p><p>9 Entity groups are an apriori grouping of data for fast operations. [sent-36, score-0.267]
</p><p>10 Examples of entity groups are: an email account for a user; a blog would have a profile entity group and more groups to hold posts and meta data for each blog. [sent-38, score-1.181]
</p><p>11 Each application will have to find natural ways to draw entity group boundaries. [sent-39, score-0.42]
</p><p>12 Groups with too much unrelated data will cause unrelated writes to be serialized which degrades throughput. [sent-41, score-0.235]
</p><p>13 Queries that require strongly consistent results must be restricted to a single entity group. [sent-43, score-0.558]
</p><p>14 Queries across entity groups may return stale results  This is a major change for programmers. [sent-44, score-0.543]
</p><p>15 The Master/Slave datastore defaulted to strongly consistent results for all queries, because reads and writes were from the master replica by default. [sent-45, score-0.484]
</p><p>16 The Master/Slave datastore was subject to periodic maintenance windows. [sent-50, score-0.317]
</p><p>17 Writes to a single entity group are strongly consistent. [sent-54, score-0.507]
</p><p>18 Writes are limited to an estimated 1 per second per entity group, so HRD is not a good match when high usage is expected. [sent-55, score-0.325]
</p><p>19 So if you planned to use an expensive HRD for critical data and the less expensive Master/Slave for less critical data, you can't do that. [sent-67, score-0.298]
</p><p>20 The reasoning is the migration will require a read-only period and the application is in the best position to know how to minimize that downtime. [sent-73, score-0.233]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hrd', 0.601), ('entity', 0.325), ('datastore', 0.269), ('megastore', 0.234), ('groups', 0.218), ('semantics', 0.112), ('consistency', 0.097), ('group', 0.095), ('strongly', 0.087), ('datacenters', 0.084), ('unrelated', 0.08), ('synchronous', 0.08), ('acid', 0.079), ('migration', 0.077), ('writes', 0.075), ('critical', 0.074), ('google', 0.073), ('convenience', 0.072), ('replication', 0.072), ('interactive', 0.068), ('reasoning', 0.062), ('engine', 0.06), ('applications', 0.057), ('planned', 0.056), ('three', 0.055), ('coordination', 0.055), ('increased', 0.055), ('write', 0.055), ('nosql', 0.054), ('consistent', 0.053), ('targeted', 0.052), ('mission', 0.052), ('guarantees', 0.052), ('traditional', 0.051), ('apriori', 0.049), ('chocked', 0.049), ('ore', 0.049), ('maintenance', 0.048), ('expensive', 0.047), ('application', 0.047), ('accessible', 0.047), ('unit', 0.047), ('require', 0.047), ('must', 0.046), ('blends', 0.046), ('normalizing', 0.046), ('suitably', 0.046), ('fi', 0.046), ('multiple', 0.046), ('fully', 0.046)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="972-tfidf-1" href="../high_scalability-2011/high_scalability-2011-01-11-Google_Megastore_-_3_Billion_Writes_and_20_Billion_Read_Transactions_Daily.html">972 high scalability-2011-01-11-Google Megastore - 3 Billion Writes and 20 Billion Read Transactions Daily</a></p>
<p>Introduction: A giant step into the fully distributed future has been taken by the Google App Engine team with the release of their  High Replication Datastore . The HRD is targeted at mission critical applications that require data replicated to at least three datacenters, full ACID semantics for  entity groups , and lower consistency guarantees across entity groups.
 
This is a major accomplishment. Few organizations can implement a true multi-datacenter datastore. Other than SimpleDB, how many other publicly accessible database services can operate out of multiple datacenters? Now that capability can be had by anyone. But there is a price, literally and otherwise. Because the HRD uses three times the resources as Google App Engine's Master/Slave datastatore, it will cost three times as much. And because it is a distributed database, with all that implies in the CAP sense, developers will have to be very careful in how they architect their applications because as costs increased, reliability incre</p><p>2 0.29171443 <a title="972-tfidf-2" href="../high_scalability-2009/high_scalability-2009-08-24-How_Google_Serves_Data_from_Multiple_Datacenters.html">687 high scalability-2009-08-24-How Google Serves Data from Multiple Datacenters</a></p>
<p>Introduction: Update:   Streamy Explains CAP and HBase's Approach to CAP .  We plan to employ inter-cluster replication, with each cluster located in a single DC.  Remote replication will introduce some eventual consistency into the system, but each cluster will continue to be strongly consistent.   Ryan Barrett, Google App Engine datastore lead, gave this talk   Transactions Across Datacenters (and Other Weekend Projects)   at the Google I/O 2009 conference.   While the talk doesn't necessarily break new technical ground, Ryan does an excellent job explaining and evaluating the different options you have when architecting a system to work across multiple datacenters. This is called  multihoming ,  operating from multiple datacenters simultaneously.  As multihoming is one of the most challenging tasks in all computing, Ryan's clear and thoughtful style comfortably leads you through the various options. On the trip you learn:
   The different  multi-homing options  are: Backups, Master-Slave, Multi-M</p><p>3 0.23045926 <a title="972-tfidf-3" href="../high_scalability-2008/high_scalability-2008-05-27-How_I_Learned_to_Stop_Worrying_and_Love_Using_a_Lot_of_Disk_Space_to_Scale.html">327 high scalability-2008-05-27-How I Learned to Stop Worrying and Love Using a Lot of Disk Space to Scale</a></p>
<p>Introduction: Update 3 : ReadWriteWeb says  Google App Engine Announces New Pricing Plans, APIs, Open Access . Pricing is specified but I'm not sure what to make of it yet. An image manipulation library is added (thus the need to pay for more CPU :-) and memcached support has been added. Memcached will help resolve the can't write for every read problem that pops up when keeping counters.  Update 2 : onGWT.com threw a GAE load party and a lot of people came. The results at  Load test : Google App Engine = 1, Community = 0 . GAE handled a peak of 35 requests/second and a sustained 10 requests/second. Some think performance was good, others not so good. My GMT watch broke and I was late to arrive. Maybe next time. Also added a few new design rules from the post.   Update : Added a few new rules gleaned from the  GAE Meetup : Design By Explicit Cost Model and Puts are Precious.  How do you structure your database using a distributed hash table like  BigTable ? The answer isn't what you might expect. If</p><p>4 0.16480137 <a title="972-tfidf-4" href="../high_scalability-2011/high_scalability-2011-02-18-Stuff_The_Internet_Says_On_Scalability_For_February_18%2C_2011.html">992 high scalability-2011-02-18-Stuff The Internet Says On Scalability For February 18, 2011</a></p>
<p>Introduction: Submitted for your reading pleasure on this cold and rainy Friday...
  
 Quotable Quotes:        
 
  CarryMillsap : You can't hardware yourself out of a performance problem you softwared yourself into. 
  @juokaz : schema-less databases doesn't mean data should have no structure 
 
 
 Scalability Porn:      
 
  3 Months To The First Million Users, Just 6 Weeks To The Second Million For Instagram  
 S tudy by the USC Annenberg School for Communication & Journalism  estimates:  in 2007, humankind was able to store 2.9 × 1020 optimally compressed bytes, communicate almost 2 × 1021 bytes, and carry out 6.4 × 1018 instructions per second on general-purpose computers. 
 
 
 Hadoop has hit a scalability limit at a whopping 4,000 machines and are looking to create the  next generation architecture . Their target is clusters of 10,000 machines and 200,000 cores.  The fundamental idea of the re-architecture is to divide the two major functions of the Job Tracker, resource management and job sc</p><p>5 0.16330095 <a title="972-tfidf-5" href="../high_scalability-2008/high_scalability-2008-04-08-Google_AppEngine_-_A_First_Look.html">301 high scalability-2008-04-08-Google AppEngine - A First Look</a></p>
<p>Introduction: I haven't developed an AppEngine application yet, I'm just taking a look around their documentation and seeing what stands out  for me. It's not the much speculated   super cluster VM  . AppEngine is solidly grounded in code and structure. It reminds me a little of the guy who ran a   website out of S3   with a splash of   Heroku   thrown in as a chaser.      The idea is clearly to take advantage of our   massive multi-core future   by creating a shared nothing infrastructure based firmly on a core set of infinitely scalable database, storage and CPU services. Don't forget Google also has a few other services to leverage: email, login, blogs, video, search, ads, metrics, and apps.  A shared nothing request is a simple beast. By its very nature shared nothing architectures must be composed of services which are themselves already scalable and Google is signing up to supply that scalable infrastructure. Google has been busy creating a platform of out-of-the-box scalable services to build</p><p>6 0.15113656 <a title="972-tfidf-6" href="../high_scalability-2009/high_scalability-2009-02-21-Google_AppEngine_-_A_Second_Look.html">517 high scalability-2009-02-21-Google AppEngine - A Second Look</a></p>
<p>7 0.14475976 <a title="972-tfidf-7" href="../high_scalability-2012/high_scalability-2012-10-22-Spanner_-_It%27s_About_Programmers_Building_Apps_Using_SQL_Semantics_at_NoSQL_Scale.html">1345 high scalability-2012-10-22-Spanner - It's About Programmers Building Apps Using SQL Semantics at NoSQL Scale</a></p>
<p>8 0.14378382 <a title="972-tfidf-8" href="../high_scalability-2009/high_scalability-2009-02-18-Numbers_Everyone_Should_Know.html">514 high scalability-2009-02-18-Numbers Everyone Should Know</a></p>
<p>9 0.14006634 <a title="972-tfidf-9" href="../high_scalability-2008/high_scalability-2008-01-25-Application_Database_and_DAL_Architecture.html">222 high scalability-2008-01-25-Application Database and DAL Architecture</a></p>
<p>10 0.12871249 <a title="972-tfidf-10" href="../high_scalability-2010/high_scalability-2010-08-12-Think_of_Latency_as_a_Pseudo-permanent_Network_Partition.html">879 high scalability-2010-08-12-Think of Latency as a Pseudo-permanent Network Partition</a></p>
<p>11 0.12345656 <a title="972-tfidf-11" href="../high_scalability-2009/high_scalability-2009-05-05-Drop_ACID_and_Think_About_Data.html">589 high scalability-2009-05-05-Drop ACID and Think About Data</a></p>
<p>12 0.12242119 <a title="972-tfidf-12" href="../high_scalability-2010/high_scalability-2010-12-06-What_the_heck_are_you_actually_using_NoSQL_for%3F.html">954 high scalability-2010-12-06-What the heck are you actually using NoSQL for?</a></p>
<p>13 0.11836552 <a title="972-tfidf-13" href="../high_scalability-2009/high_scalability-2009-12-16-Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_in_the_Ambient_Cloud.html">750 high scalability-2009-12-16-Building Super Scalable Systems: Blade Runner Meets Autonomic Computing in the Ambient Cloud</a></p>
<p>14 0.11824114 <a title="972-tfidf-14" href="../high_scalability-2012/high_scalability-2012-11-05-Gone_Fishin%27%3A_Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_In_The_Ambient_Cloud.html">1355 high scalability-2012-11-05-Gone Fishin': Building Super Scalable Systems: Blade Runner Meets Autonomic Computing In The Ambient Cloud</a></p>
<p>15 0.11786713 <a title="972-tfidf-15" href="../high_scalability-2009/high_scalability-2009-08-08-Yahoo%21%27s_PNUTS_Database%3A_Too_Hot%2C_Too_Cold_or_Just_Right%3F.html">676 high scalability-2009-08-08-Yahoo!'s PNUTS Database: Too Hot, Too Cold or Just Right?</a></p>
<p>16 0.11354651 <a title="972-tfidf-16" href="../high_scalability-2012/high_scalability-2012-09-24-Google_Spanner%27s_Most_Surprising_Revelation%3A_NoSQL_is_Out_and_NewSQL_is_In.html">1328 high scalability-2012-09-24-Google Spanner's Most Surprising Revelation: NoSQL is Out and NewSQL is In</a></p>
<p>17 0.11307663 <a title="972-tfidf-17" href="../high_scalability-2010/high_scalability-2010-09-01-Paper%3A_The_Case_for_Determinism_in_Database_Systems__.html">890 high scalability-2010-09-01-Paper: The Case for Determinism in Database Systems  </a></p>
<p>18 0.11288305 <a title="972-tfidf-18" href="../high_scalability-2012/high_scalability-2012-12-18-Georeplication%3A_When_Bad_Things_Happen_to_Good_Systems.html">1374 high scalability-2012-12-18-Georeplication: When Bad Things Happen to Good Systems</a></p>
<p>19 0.11239448 <a title="972-tfidf-19" href="../high_scalability-2009/high_scalability-2009-03-16-Are_Cloud_Based_Memory_Architectures_the_Next_Big_Thing%3F.html">538 high scalability-2009-03-16-Are Cloud Based Memory Architectures the Next Big Thing?</a></p>
<p>20 0.10857625 <a title="972-tfidf-20" href="../high_scalability-2011/high_scalability-2011-11-23-Paper%3A_Don%E2%80%99t_Settle_for_Eventual%3A_Scalable_Causal_Consistency_for_Wide-Area_Storage_with_COPS.html">1146 high scalability-2011-11-23-Paper: Don’t Settle for Eventual: Scalable Causal Consistency for Wide-Area Storage with COPS</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.197), (1, 0.089), (2, 0.006), (3, 0.06), (4, 0.004), (5, 0.097), (6, 0.013), (7, -0.053), (8, -0.007), (9, -0.06), (10, -0.005), (11, -0.009), (12, -0.114), (13, 0.01), (14, 0.061), (15, 0.028), (16, -0.037), (17, -0.055), (18, 0.071), (19, -0.063), (20, 0.084), (21, 0.063), (22, 0.007), (23, -0.075), (24, -0.051), (25, -0.023), (26, 0.005), (27, -0.035), (28, 0.002), (29, -0.069), (30, 0.01), (31, -0.082), (32, -0.002), (33, -0.018), (34, -0.046), (35, -0.004), (36, -0.028), (37, -0.001), (38, 0.029), (39, -0.013), (40, -0.024), (41, 0.044), (42, -0.052), (43, 0.0), (44, 0.03), (45, -0.012), (46, 0.013), (47, 0.024), (48, -0.026), (49, -0.009)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95132715 <a title="972-lsi-1" href="../high_scalability-2011/high_scalability-2011-01-11-Google_Megastore_-_3_Billion_Writes_and_20_Billion_Read_Transactions_Daily.html">972 high scalability-2011-01-11-Google Megastore - 3 Billion Writes and 20 Billion Read Transactions Daily</a></p>
<p>Introduction: A giant step into the fully distributed future has been taken by the Google App Engine team with the release of their  High Replication Datastore . The HRD is targeted at mission critical applications that require data replicated to at least three datacenters, full ACID semantics for  entity groups , and lower consistency guarantees across entity groups.
 
This is a major accomplishment. Few organizations can implement a true multi-datacenter datastore. Other than SimpleDB, how many other publicly accessible database services can operate out of multiple datacenters? Now that capability can be had by anyone. But there is a price, literally and otherwise. Because the HRD uses three times the resources as Google App Engine's Master/Slave datastatore, it will cost three times as much. And because it is a distributed database, with all that implies in the CAP sense, developers will have to be very careful in how they architect their applications because as costs increased, reliability incre</p><p>2 0.87579191 <a title="972-lsi-2" href="../high_scalability-2009/high_scalability-2009-08-24-How_Google_Serves_Data_from_Multiple_Datacenters.html">687 high scalability-2009-08-24-How Google Serves Data from Multiple Datacenters</a></p>
<p>Introduction: Update:   Streamy Explains CAP and HBase's Approach to CAP .  We plan to employ inter-cluster replication, with each cluster located in a single DC.  Remote replication will introduce some eventual consistency into the system, but each cluster will continue to be strongly consistent.   Ryan Barrett, Google App Engine datastore lead, gave this talk   Transactions Across Datacenters (and Other Weekend Projects)   at the Google I/O 2009 conference.   While the talk doesn't necessarily break new technical ground, Ryan does an excellent job explaining and evaluating the different options you have when architecting a system to work across multiple datacenters. This is called  multihoming ,  operating from multiple datacenters simultaneously.  As multihoming is one of the most challenging tasks in all computing, Ryan's clear and thoughtful style comfortably leads you through the various options. On the trip you learn:
   The different  multi-homing options  are: Backups, Master-Slave, Multi-M</p><p>3 0.81961638 <a title="972-lsi-3" href="../high_scalability-2009/high_scalability-2009-08-08-Yahoo%21%27s_PNUTS_Database%3A_Too_Hot%2C_Too_Cold_or_Just_Right%3F.html">676 high scalability-2009-08-08-Yahoo!'s PNUTS Database: Too Hot, Too Cold or Just Right?</a></p>
<p>Introduction: So far every massively scalable database is a bundle of compromises. For some the weak guarantees of Amazon's  eventual consistency  model are too cold. For many the strong guarantees of standard RDBMS  distributed transactions  are too hot. Google App Engine tries to get it just right with  entity groups . Yahoo! is also trying to get is just right by offering per-record timeline consistency, which hopes to serve up a heaping bowl of  rich database functionality and low latency at massive scale :
   We describe PNUTS [Platform for Nimble Universal Table Storage], a massively parallel and geographically distributed database system for Yahoo!â&euro;&trade;s web applications. PNUTS provides data storage organized as hashed or ordered tables, low latency for large numbers of con-current requests including updates and queries, and novel per-record consistency guarantees. It is a hosted, centrally managed, and geographically distributed service, and utilizes automated load-balancing and failover to redu</p><p>4 0.80418259 <a title="972-lsi-4" href="../high_scalability-2011/high_scalability-2011-11-23-Paper%3A_Don%E2%80%99t_Settle_for_Eventual%3A_Scalable_Causal_Consistency_for_Wide-Area_Storage_with_COPS.html">1146 high scalability-2011-11-23-Paper: Don’t Settle for Eventual: Scalable Causal Consistency for Wide-Area Storage with COPS</a></p>
<p>Introduction: Teams from  Princeton  and CMU are  working together  to solve one of the most difficult problems in the repertoire: scalable geo-distributed data stores. Major companies like Google and Facebook have been working on multiple datacenter database functionality for some time, but there's still a general lack of available systems that work for complex data scenarios.
 
The ideas in this paper-- Don’t Settle for Eventual: Scalable Causal Consistency for Wide-Area Storage with COPS --are different. It's not another eventually consistent system, or a traditional transaction oriented system, or a replication based system, or a system that punts on the issue. It's something new, a causally consistent system that achieves  ALPS  system properties. Move over CAP, NoSQL, etc, we have another acronym: ALPS - Available (operations always complete successfully), Low-latency (operations complete quickly (single digit milliseconds)), Partition-tolerant (operates with a partition), and Scalable (just a</p><p>5 0.77897227 <a title="972-lsi-5" href="../high_scalability-2013/high_scalability-2013-05-01-Myth%3A_Eric_Brewer_on_Why_Banks_are_BASE_Not_ACID_-_Availability_Is_Revenue_.html">1450 high scalability-2013-05-01-Myth: Eric Brewer on Why Banks are BASE Not ACID - Availability Is Revenue </a></p>
<p>Introduction: In   NoSQL: Past, Present, Future    Eric Brewer  has a particularly fine section on explaining the often hard to understand ideas of   BASE   (Basically Available, Soft State, Eventually Consistent),   ACID   (Atomicity, Consistency, Isolation, Durability),   CAP   (Consistency Availability, Partition Tolerance), in terms of a pernicious long standing myth about the sanctity of consistency in banking.
    Myth   : Money is important, so banks   must   use transactions to keep money safe and consistent, right? 
    Reality   : Banking transactions are inconsistent, particularly for ATMs. ATMs are designed to have a normal case behaviour and a partition mode behaviour. In partition mode Availability is chosen over Consistency. 
   Why?   1)  Availability correlates with revenue and consistency generally does not.  2)  Historically there was never an idea of perfect communication so everything was partitioned.
   Your ATM transaction must go through so Availability is more important than</p><p>6 0.7700904 <a title="972-lsi-6" href="../high_scalability-2012/high_scalability-2012-10-22-Spanner_-_It%27s_About_Programmers_Building_Apps_Using_SQL_Semantics_at_NoSQL_Scale.html">1345 high scalability-2012-10-22-Spanner - It's About Programmers Building Apps Using SQL Semantics at NoSQL Scale</a></p>
<p>7 0.76875627 <a title="972-lsi-7" href="../high_scalability-2011/high_scalability-2011-12-08-Update_on_Scalable_Causal_Consistency_For_Wide-Area_Storage_With_COPS.html">1153 high scalability-2011-12-08-Update on Scalable Causal Consistency For Wide-Area Storage With COPS</a></p>
<p>8 0.7587046 <a title="972-lsi-8" href="../high_scalability-2010/high_scalability-2010-09-01-Paper%3A_The_Case_for_Determinism_in_Database_Systems__.html">890 high scalability-2010-09-01-Paper: The Case for Determinism in Database Systems  </a></p>
<p>9 0.75211692 <a title="972-lsi-9" href="../high_scalability-2014/high_scalability-2014-04-10-Paper%3A_Scalable_Atomic_Visibility_with_RAMP_Transactions_-_Scale_Linearly_to_100_Servers.html">1629 high scalability-2014-04-10-Paper: Scalable Atomic Visibility with RAMP Transactions - Scale Linearly to 100 Servers</a></p>
<p>10 0.7404815 <a title="972-lsi-10" href="../high_scalability-2008/high_scalability-2008-05-27-How_I_Learned_to_Stop_Worrying_and_Love_Using_a_Lot_of_Disk_Space_to_Scale.html">327 high scalability-2008-05-27-How I Learned to Stop Worrying and Love Using a Lot of Disk Space to Scale</a></p>
<p>11 0.73860276 <a title="972-lsi-11" href="../high_scalability-2013/high_scalability-2013-11-07-Paper%3A_Tempest%3A_Scalable_Time-Critical_Web_Services_Platform.html">1544 high scalability-2013-11-07-Paper: Tempest: Scalable Time-Critical Web Services Platform</a></p>
<p>12 0.717866 <a title="972-lsi-12" href="../high_scalability-2009/high_scalability-2009-02-03-Paper%3A_Optimistic_Replication.html">507 high scalability-2009-02-03-Paper: Optimistic Replication</a></p>
<p>13 0.7176761 <a title="972-lsi-13" href="../high_scalability-2010/high_scalability-2010-12-23-Paper%3A_CRDTs%3A_Consistency_without_concurrency_control.html">963 high scalability-2010-12-23-Paper: CRDTs: Consistency without concurrency control</a></p>
<p>14 0.71670306 <a title="972-lsi-14" href="../high_scalability-2012/high_scalability-2012-12-18-Georeplication%3A_When_Bad_Things_Happen_to_Good_Systems.html">1374 high scalability-2012-12-18-Georeplication: When Bad Things Happen to Good Systems</a></p>
<p>15 0.71390319 <a title="972-lsi-15" href="../high_scalability-2013/high_scalability-2013-05-23-Paper%3A_Calvin%3A_Fast_Distributed_Transactions_for_Partitioned_Database_Systems.html">1463 high scalability-2013-05-23-Paper: Calvin: Fast Distributed Transactions for Partitioned Database Systems</a></p>
<p>16 0.70895422 <a title="972-lsi-16" href="../high_scalability-2010/high_scalability-2010-08-12-Think_of_Latency_as_a_Pseudo-permanent_Network_Partition.html">879 high scalability-2010-08-12-Think of Latency as a Pseudo-permanent Network Partition</a></p>
<p>17 0.70289695 <a title="972-lsi-17" href="../high_scalability-2011/high_scalability-2011-04-06-Netflix%3A_Run_Consistency_Checkers_All_the_time_to_Fixup_Transactions.html">1017 high scalability-2011-04-06-Netflix: Run Consistency Checkers All the time to Fixup Transactions</a></p>
<p>18 0.69153953 <a title="972-lsi-18" href="../high_scalability-2009/high_scalability-2009-02-18-Numbers_Everyone_Should_Know.html">514 high scalability-2009-02-18-Numbers Everyone Should Know</a></p>
<p>19 0.68811268 <a title="972-lsi-19" href="../high_scalability-2012/high_scalability-2012-09-24-Google_Spanner%27s_Most_Surprising_Revelation%3A_NoSQL_is_Out_and_NewSQL_is_In.html">1328 high scalability-2012-09-24-Google Spanner's Most Surprising Revelation: NoSQL is Out and NewSQL is In</a></p>
<p>20 0.67807031 <a title="972-lsi-20" href="../high_scalability-2009/high_scalability-2009-02-21-Google_AppEngine_-_A_Second_Look.html">517 high scalability-2009-02-21-Google AppEngine - A Second Look</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.135), (2, 0.168), (10, 0.034), (30, 0.05), (40, 0.018), (47, 0.035), (57, 0.157), (61, 0.101), (79, 0.153), (85, 0.043), (94, 0.015)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.93978703 <a title="972-lda-1" href="../high_scalability-2007/high_scalability-2007-11-18-Reverse_Proxy.html">159 high scalability-2007-11-18-Reverse Proxy</a></p>
<p>Introduction: Hi,      I saw an year ago that Netapp sold netcache to blu-coat, my site is a heavy NetCache user and we cached 83% of our site. We tested with Blue-coat and F5 WA and we are not getting same performce as NetCache.      Any of you guys have the same issue? or somebody knows another product can handle much traffic?     Thanks   Rodrigo</p><p>2 0.93672597 <a title="972-lda-2" href="../high_scalability-2008/high_scalability-2008-10-29-CTL_-_Distributed_Control_Dispatching_Framework_.html">433 high scalability-2008-10-29-CTL - Distributed Control Dispatching Framework </a></p>
<p>Introduction: CTL  is a  flexible distributed control dispatching framework that enables you to break management processes into reusable control modules and execute them in distributed fashion over the network .  From their website:  CTL is a flexible distributed control dispatching framework that enables you to break management processes into reusable control modules and execute them in distributed fashion over the network.  What does CTL do? CTL helps you leverage your current scripts and tools to easily automate any kind of distributed systems management or application provisioning task. Its good for simplifiying large-scale scripting efforts or as another tool in your toolbox that helps you speed through your daily mix of ad-hoc administration tasks.  What are CTL's features? CTL has many features, but the general highlights are:  * Execute sophisticated procedures in distributed environments - Aren't you tired of writing and then endlessly modifying scripts that loop over nodes and invoke remot</p><p>3 0.92592543 <a title="972-lda-3" href="../high_scalability-2012/high_scalability-2012-03-19-LinkedIn%3A_Creating_a_Low_Latency_Change_Data_Capture_System_with_Databus.html">1211 high scalability-2012-03-19-LinkedIn: Creating a Low Latency Change Data Capture System with Databus</a></p>
<p>Introduction: This is a guest post by  Siddharth Anand , a senior member of LinkedIn's Distributed Data Systems team.  
 
Over the past 3 years, I've had the good fortune to work with many emerging NoSQL products in the context of  supporting the needs of a high-traffic, customer facing web site.
 
In 2010, I helped Netflix to successfully   transition its web scale use-cases from Oracle to SimpleDB  , AWS' hosted database service. On completion of that migration, we started a second migration, this time from SimpleDB to Cassandra. The first transition was key to our move from our own data center to AWS' cloud. The second was key to our expansion from one AWS Region to multiple geographically-distributed Regions -- today Netflix serves traffic out of two AWS Regions, one in Virginia, the other in Ireland ( F1 ). Both of these transitions have been successful, but have involved integration pain points such as the creation of database replication technology.
 
In December 2011, I moved to LinkedIn's D</p><p>4 0.92433172 <a title="972-lda-4" href="../high_scalability-2011/high_scalability-2011-11-17-Five_Misconceptions_on_Cloud_Portability.html">1144 high scalability-2011-11-17-Five Misconceptions on Cloud Portability</a></p>
<p>Introduction: The term "cloud portability" is often considered a synonym for "Cloud  API  portability," which implies a series of misconceptions.
 
If we break away from dogma, we can find that what we really looking for in cloud portability is Application portability between clouds which can be a vastly simpler requirement, as we can achieve application portability without settling on a common Cloud  API. 
 
In this post i'll be covering five common misconceptions people have  WRT  to cloud portability.
  
  Cloud portability = Cloud API portability . API portability is easy; cloud API portability is not.  
  The main incentive for Cloud Portability is - Avoiding Vendor lock-in .Cloud portability is more about business agility than it is about vendor lock-in.  
  Cloud portability isnâ&euro;&trade;t for startups . Every startup that is expecting rapid growth should re-examine their deployments and plan for cloud portability rather than wait to be forced to make the switch when you are least prepared to do so.</p><p>5 0.92062873 <a title="972-lda-5" href="../high_scalability-2011/high_scalability-2011-01-04-Map-Reduce_With_Ruby_Using_Hadoop.html">968 high scalability-2011-01-04-Map-Reduce With Ruby Using Hadoop</a></p>
<p>Introduction: A demonstration, with repeatable steps, of how to quickly fire-up a Hadoop cluster on Amazon EC2, load data onto the HDFS (Hadoop Distributed File-System), write map-reduce scripts in Ruby and use them to run a map-reduce job on your Hadoop cluster. You will  not  need to ssh into the cluster, as all tasks are run from your local machine. Below I am using my MacBook Pro as my local machine, but the steps I have provided should be reproducible on other platforms running bash and Java.
  

  

 Fire-Up Your Hadoop Cluster 

I choose the  Cloudera distribution of Hadoop  which is still 100% Apache licensed, but has some additional benefits. One of these benefits is that it is released by  Doug Cutting , who started Hadoop and drove it’s development at Yahoo! He also started  Lucene , which is another of my favourite Apache Projects, so I have good faith that he knows what he is doing. Another benefit, as you will see, is that it is simple to fire-up a Hadoop cluster.


I am going to use C</p><p>same-blog 6 0.91742355 <a title="972-lda-6" href="../high_scalability-2011/high_scalability-2011-01-11-Google_Megastore_-_3_Billion_Writes_and_20_Billion_Read_Transactions_Daily.html">972 high scalability-2011-01-11-Google Megastore - 3 Billion Writes and 20 Billion Read Transactions Daily</a></p>
<p>7 0.90314811 <a title="972-lda-7" href="../high_scalability-2010/high_scalability-2010-04-09-Vagrant_-_Build_and_Deploy_Virtualized_Development_Environments_Using_Ruby.html">807 high scalability-2010-04-09-Vagrant - Build and Deploy Virtualized Development Environments Using Ruby</a></p>
<p>8 0.89322865 <a title="972-lda-8" href="../high_scalability-2011/high_scalability-2011-11-07-10_Core_Architecture_Pattern_Variations_for_Achieving_Scalability.html">1138 high scalability-2011-11-07-10 Core Architecture Pattern Variations for Achieving Scalability</a></p>
<p>9 0.89040381 <a title="972-lda-9" href="../high_scalability-2010/high_scalability-2010-07-13-DbShards_Part_Deux_-_The_Internals.html">857 high scalability-2010-07-13-DbShards Part Deux - The Internals</a></p>
<p>10 0.88666016 <a title="972-lda-10" href="../high_scalability-2010/high_scalability-2010-07-11-So%2C_Why_is_Twitter_Really_Not_Using_Cassandra_to_Store_Tweets%3F.html">855 high scalability-2010-07-11-So, Why is Twitter Really Not Using Cassandra to Store Tweets?</a></p>
<p>11 0.88015795 <a title="972-lda-11" href="../high_scalability-2008/high_scalability-2008-01-29-When_things_aren%27t_scalable.html">232 high scalability-2008-01-29-When things aren't scalable</a></p>
<p>12 0.87894744 <a title="972-lda-12" href="../high_scalability-2009/high_scalability-2009-10-28-Need_for_change_in_your_IT_infrastructure_.html">731 high scalability-2009-10-28-Need for change in your IT infrastructure </a></p>
<p>13 0.86804819 <a title="972-lda-13" href="../high_scalability-2007/high_scalability-2007-07-11-Friendster_Architecture.html">6 high scalability-2007-07-11-Friendster Architecture</a></p>
<p>14 0.86437351 <a title="972-lda-14" href="../high_scalability-2008/high_scalability-2008-07-16-The_Mother_of_All_Database_Normalization_Debates_on_Coding_Horror.html">351 high scalability-2008-07-16-The Mother of All Database Normalization Debates on Coding Horror</a></p>
<p>15 0.86353183 <a title="972-lda-15" href="../high_scalability-2013/high_scalability-2013-01-11-Stuff_The_Internet_Says_On_Scalability_For_January_11%2C_2013.html">1385 high scalability-2013-01-11-Stuff The Internet Says On Scalability For January 11, 2013</a></p>
<p>16 0.85818052 <a title="972-lda-16" href="../high_scalability-2012/high_scalability-2012-07-27-Stuff_The_Internet_Says_On_Scalability_For_July_27%2C_2012.html">1292 high scalability-2012-07-27-Stuff The Internet Says On Scalability For July 27, 2012</a></p>
<p>17 0.85803539 <a title="972-lda-17" href="../high_scalability-2012/high_scalability-2012-11-05-Gone_Fishin%27%3A_Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_In_The_Ambient_Cloud.html">1355 high scalability-2012-11-05-Gone Fishin': Building Super Scalable Systems: Blade Runner Meets Autonomic Computing In The Ambient Cloud</a></p>
<p>18 0.85802764 <a title="972-lda-18" href="../high_scalability-2009/high_scalability-2009-12-16-Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_in_the_Ambient_Cloud.html">750 high scalability-2009-12-16-Building Super Scalable Systems: Blade Runner Meets Autonomic Computing in the Ambient Cloud</a></p>
<p>19 0.85777098 <a title="972-lda-19" href="../high_scalability-2008/high_scalability-2008-01-17-Moving_old_to_new._Do_not_be_afraid_of_the_re-write_--_but_take_some_help.html">218 high scalability-2008-01-17-Moving old to new. Do not be afraid of the re-write -- but take some help</a></p>
<p>20 0.85643572 <a title="972-lda-20" href="../high_scalability-2013/high_scalability-2013-12-06-Stuff_The_Internet_Says_On_Scalability_For_December_6th%2C_2013.html">1559 high scalability-2013-12-06-Stuff The Internet Says On Scalability For December 6th, 2013</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
