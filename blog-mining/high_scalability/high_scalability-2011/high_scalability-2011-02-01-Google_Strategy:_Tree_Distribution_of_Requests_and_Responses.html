<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>981 high scalability-2011-02-01-Google Strategy: Tree Distribution of Requests and Responses</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2011" href="../home/high_scalability-2011_home.html">high_scalability-2011</a> <a title="high_scalability-2011-981" href="#">high_scalability-2011-981</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>981 high scalability-2011-02-01-Google Strategy: Tree Distribution of Requests and Responses</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2011-981-html" href="http://highscalability.com//blog/2011/2/1/google-strategy-tree-distribution-of-requests-and-responses.html">html</a></p><p>Introduction: If a large number of leaf node machines send requests to a central root node then that root node can become overwhelmed:
  
 The CPU becomes a bottleneck, for either processing requests or sending replies, because it can't possibly deal with the flood of requests. 
 The network interface becomes a bottleneck because a wide fan-in causes TCP drops and retransmissions, which causes latency. Then clients start retrying requests which quickly causes a spiral of death in an undisciplined system. 
  
One solution to this problem is a strategy given by Dr.  Jeff Dean , Head of Google's School of Infrastructure Wizardry, in this  Stanford video presentation :  Tree Distribution of Requests and Responses .
 
 
 
Instead of having a root node connected to leaves in a flat topology, the idea is to create a tree of nodes. So a root node talks to a number of parent nodes and the parent nodes talk to a number of leaf nodes. Requests are pushed down the tree through the parents and only hit a subset</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 If a large number of leaf node machines send requests to a central root node then that root node can become overwhelmed:     The CPU becomes a bottleneck, for either processing requests or sending replies, because it can't possibly deal with the flood of requests. [sent-1, score-2.306]
</p><p>2 The network interface becomes a bottleneck because a wide fan-in causes TCP drops and retransmissions, which causes latency. [sent-2, score-0.659]
</p><p>3 Then clients start retrying requests which quickly causes a spiral of death in an undisciplined system. [sent-3, score-0.641]
</p><p>4 One solution to this problem is a strategy given by Dr. [sent-4, score-0.053]
</p><p>5 Instead of having a root node connected to leaves in a flat topology, the idea is to create a tree of nodes. [sent-6, score-1.104]
</p><p>6 So a root node talks to a number of parent nodes and the parent nodes talk to a number of leaf nodes. [sent-7, score-1.936]
</p><p>7 Requests are pushed down the tree through the parents and only hit a subset of the leaf nodes. [sent-8, score-0.888]
</p><p>8 With this solution:      Fan-in at each level of the tree is manageable . [sent-9, score-0.289]
</p><p>9 The CPU cost of processing requests and responses is spread out across all the parents, which reduces the CPU and network bottlenecks. [sent-10, score-0.529]
</p><p>10 Ideally the parent can provide a level of response filtering so the root only sees a subset of the response data. [sent-12, score-1.27]
</p><p>11 This further reduces the network and CPU needed by the root. [sent-13, score-0.058]
</p><p>12 The parent can be collocated with leaves on one rack, which keeps all that traffic off your datacenter networks. [sent-15, score-0.79]
</p><p>13 Parents return the best 20-30 responses out of the 30 leaves the parent is responsible for. [sent-17, score-0.862]
</p><p>14 This is a large degree of data reduction compared to the case the root had to process all that data directly. [sent-18, score-0.383]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('parent', 0.44), ('root', 0.321), ('leaves', 0.294), ('leaf', 0.286), ('tree', 0.235), ('parents', 0.186), ('node', 0.183), ('requests', 0.183), ('causes', 0.171), ('filtering', 0.132), ('responses', 0.128), ('subset', 0.123), ('wizardry', 0.12), ('retrying', 0.12), ('retransmissions', 0.113), ('cpu', 0.111), ('bottleneck', 0.105), ('reduces', 0.099), ('spiral', 0.098), ('collocated', 0.098), ('flood', 0.09), ('overwhelmed', 0.087), ('replies', 0.083), ('ideally', 0.083), ('becomes', 0.082), ('dean', 0.076), ('stanford', 0.076), ('school', 0.075), ('topology', 0.074), ('drops', 0.072), ('flat', 0.071), ('death', 0.069), ('nodes', 0.067), ('response', 0.067), ('reduction', 0.067), ('number', 0.066), ('sees', 0.066), ('rack', 0.065), ('head', 0.063), ('degree', 0.062), ('processing', 0.061), ('jeff', 0.061), ('pushed', 0.058), ('network', 0.058), ('possibly', 0.058), ('keeps', 0.056), ('sending', 0.054), ('level', 0.054), ('solution', 0.053), ('central', 0.052)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="981-tfidf-1" href="../high_scalability-2011/high_scalability-2011-02-01-Google_Strategy%3A_Tree_Distribution_of_Requests_and_Responses.html">981 high scalability-2011-02-01-Google Strategy: Tree Distribution of Requests and Responses</a></p>
<p>Introduction: If a large number of leaf node machines send requests to a central root node then that root node can become overwhelmed:
  
 The CPU becomes a bottleneck, for either processing requests or sending replies, because it can't possibly deal with the flood of requests. 
 The network interface becomes a bottleneck because a wide fan-in causes TCP drops and retransmissions, which causes latency. Then clients start retrying requests which quickly causes a spiral of death in an undisciplined system. 
  
One solution to this problem is a strategy given by Dr.  Jeff Dean , Head of Google's School of Infrastructure Wizardry, in this  Stanford video presentation :  Tree Distribution of Requests and Responses .
 
 
 
Instead of having a root node connected to leaves in a flat topology, the idea is to create a tree of nodes. So a root node talks to a number of parent nodes and the parent nodes talk to a number of leaf nodes. Requests are pushed down the tree through the parents and only hit a subset</p><p>2 0.26524657 <a title="981-tfidf-2" href="../high_scalability-2012/high_scalability-2012-08-20-The_Performance_of_Distributed_Data-Structures_Running_on_a_%22Cache-Coherent%22_In-Memory_Data_Grid.html">1307 high scalability-2012-08-20-The Performance of Distributed Data-Structures Running on a "Cache-Coherent" In-Memory Data Grid</a></p>
<p>Introduction: This is a guest post by Ron Pressler, the founder and CEO of  Parallel Universe , a Y Combinator company building advanced middleware for real-time applications. 
 
A little over a month ago, we open-sourced a new in-memory data grid called  Galaxy . An in-memory data grid, or IMDG, is a clustered data storage and processing middleware that uses RAM as the authoritative and primary storage, and distributes data over a cluster for purposes of data and processing scalability and high-availability. A common feature of IMDGs is co-location of code and data, meaning that application code runs on all cluster nodes, each instance processing those data items residing in the local node's RAM.
 
While quite a few commercial and open-source IMDGs are available (like Terracotta, Gigaspaces, Oracle Coherence, GemFire, Websphere eXtreme Scale, Infinispan and Hazelcast), Galaxy has adopted a completely different architecture from all other IMDGs, to service some usage scenarios ill-fitted to the othe</p><p>3 0.25627047 <a title="981-tfidf-3" href="../high_scalability-2012/high_scalability-2012-03-12-Google%3A_Taming_the_Long_Latency_Tail_-_When_More_Machines_Equals_Worse_Results.html">1207 high scalability-2012-03-12-Google: Taming the Long Latency Tail - When More Machines Equals Worse Results</a></p>
<p>Introduction: Likewise the current belief that, in the case of artificial machines the very large and the very small are equally feasible and lasting is a manifest error. Thus, for example, a small obelisk or column or other solid figure can certainly be laid down or set up without danger of breaking, while the large ones will go to pieces under the slightest provocation, and that purely on account of their own weight. -- Galileo  
Galileo observed how things broke if they were naively scaled up. Interestingly, Google noticed a similar pattern when building larger software systems using the same techniques used to build smaller systems. 
 
 Luiz André Barroso , Distinguished Engineer at Google, talks about this fundamental property of scaling systems in his fascinating talk,  Warehouse-Scale Computing: Entering the Teenage Decade . Google found the larger the scale the greater the impact of latency variability. When a request is implemented by work done in parallel, as is common with today's service</p><p>4 0.18633582 <a title="981-tfidf-4" href="../high_scalability-2012/high_scalability-2012-04-30-Masstree_-_Much_Faster_than_MongoDB%2C_VoltDB%2C_Redis%2C_and_Competitive_with_Memcached.html">1236 high scalability-2012-04-30-Masstree - Much Faster than MongoDB, VoltDB, Redis, and Competitive with Memcached</a></p>
<p>Introduction: The  EuroSys 2012  system conference has an excellent live blog summary of their talks for:  Day 1 ,  Day 2 ,  Day 3  (thanks  Henry at the Paper Trail blog ). Summaries for each of the accepted papers are  here .
 
One of the more interesting papers from a NoSQL perspective was  Cache Craftiness for Fast Multicore Key-Value Storage , a wonderfully detailed description of the low level techniques used to implement Masstree:
  

A storage system specialized for key-value data in which all data ﬁts in memory, but must persist across server restarts. It supports arbitrary, variable-length keys. It allows range queries over those keys: clients can traverse subsets of the database, or the whole database, in sorted order by key. On a 16-core machine Masstree achieves six to ten million operations per second on parts A–C of the Yahoo! Cloud Serving Benchmark benchmark, more than 30 as fast as VoltDB [5] or MongoDB [2].

  
If you are looking for innovative detailed high performance design, t</p><p>5 0.12213776 <a title="981-tfidf-5" href="../high_scalability-2011/high_scalability-2011-11-07-10_Core_Architecture_Pattern_Variations_for_Achieving_Scalability.html">1138 high scalability-2011-11-07-10 Core Architecture Pattern Variations for Achieving Scalability</a></p>
<p>Introduction: Srinath Perera has put together a  strong list of architecture patterns  based on three meta patterns:  distribution, caching, and asynchronous processing. He contends these three are the primal patterns and the following patterns are but different combinations:
  
  LB (Load Balancers) + Shared nothing Units . Units that do not share anything with each other fronted with a load balancer that routes incoming messages to a unit based on some criteria. 
  LB + Stateless Nodes + Scalable Storage . Several stateless nodes talking to a scalable storage, and a load balancer distributes load among the nodes. 
  Peer to Peer Architectures (Distributed Hash Table (DHT) and Content Addressable Networks (CAN)) . Algorithm for scaling up logarithmically. 
  Distributed Queues . Queue implementation (FIFO delivery) implemented as a network service. 
  Publish/Subscribe Paradigm . Network publish subscribe brokers that route messages to each other. 
  Gossip and Nature-inspired Architectures . Each</p><p>6 0.11442891 <a title="981-tfidf-6" href="../high_scalability-2012/high_scalability-2012-06-18-Google_on_Latency_Tolerant_Systems%3A_Making_a_Predictable_Whole_Out_of_Unpredictable_Parts___.html">1266 high scalability-2012-06-18-Google on Latency Tolerant Systems: Making a Predictable Whole Out of Unpredictable Parts   </a></p>
<p>7 0.11376063 <a title="981-tfidf-7" href="../high_scalability-2013/high_scalability-2013-03-11-Low_Level_Scalability_Solutions_-_The_Conditioning_Collection.html">1421 high scalability-2013-03-11-Low Level Scalability Solutions - The Conditioning Collection</a></p>
<p>8 0.11063499 <a title="981-tfidf-8" href="../high_scalability-2011/high_scalability-2011-11-14-Using_Gossip_Protocols_for_Failure_Detection%2C_Monitoring%2C_Messaging_and_Other_Good_Things.html">1142 high scalability-2011-11-14-Using Gossip Protocols for Failure Detection, Monitoring, Messaging and Other Good Things</a></p>
<p>9 0.10337118 <a title="981-tfidf-9" href="../high_scalability-2013/high_scalability-2013-10-08-F1_and_Spanner_Holistically_Compared.html">1529 high scalability-2013-10-08-F1 and Spanner Holistically Compared</a></p>
<p>10 0.10008232 <a title="981-tfidf-10" href="../high_scalability-2013/high_scalability-2013-02-27-42_Monster_Problems_that_Attack_as_Loads_Increase.html">1413 high scalability-2013-02-27-42 Monster Problems that Attack as Loads Increase</a></p>
<p>11 0.094012231 <a title="981-tfidf-11" href="../high_scalability-2008/high_scalability-2008-02-16-S3_Failed_Because_of_Authentication_Overload.html">249 high scalability-2008-02-16-S3 Failed Because of Authentication Overload</a></p>
<p>12 0.089671247 <a title="981-tfidf-12" href="../high_scalability-2011/high_scalability-2011-06-01-Why_is_your_network_so_slow%3F_Your_switch_should_tell_you..html">1051 high scalability-2011-06-01-Why is your network so slow? Your switch should tell you.</a></p>
<p>13 0.087748878 <a title="981-tfidf-13" href="../high_scalability-2009/high_scalability-2009-07-30-Learn_How_to_Think_at_Scale.html">666 high scalability-2009-07-30-Learn How to Think at Scale</a></p>
<p>14 0.087712891 <a title="981-tfidf-14" href="../high_scalability-2014/high_scalability-2014-03-31-How_WhatsApp_Grew_to_Nearly_500_Million_Users%2C_11%2C000_cores%2C_and_70_Million_Messages_a_Second.html">1622 high scalability-2014-03-31-How WhatsApp Grew to Nearly 500 Million Users, 11,000 cores, and 70 Million Messages a Second</a></p>
<p>15 0.084974825 <a title="981-tfidf-15" href="../high_scalability-2009/high_scalability-2009-10-26-Facebook%27s_Memcached_Multiget_Hole%3A_More_machines_%21%3D_More_Capacity_.html">728 high scalability-2009-10-26-Facebook's Memcached Multiget Hole: More machines != More Capacity </a></p>
<p>16 0.083124697 <a title="981-tfidf-16" href="../high_scalability-2012/high_scalability-2012-09-04-Changing_Architectures%3A_New_Datacenter_Networks_Will_Set_Your_Code_and_Data_Free___.html">1316 high scalability-2012-09-04-Changing Architectures: New Datacenter Networks Will Set Your Code and Data Free   </a></p>
<p>17 0.076651685 <a title="981-tfidf-17" href="../high_scalability-2014/high_scalability-2014-02-05-Little%E2%80%99s_Law%2C_Scalability_and_Fault_Tolerance%3A_The_OS_is_your_bottleneck._What_you_can_do%3F.html">1591 high scalability-2014-02-05-Little’s Law, Scalability and Fault Tolerance: The OS is your bottleneck. What you can do?</a></p>
<p>18 0.076542929 <a title="981-tfidf-18" href="../high_scalability-2012/high_scalability-2012-07-04-Top_Features_of_a_Scalable_Database.html">1276 high scalability-2012-07-04-Top Features of a Scalable Database</a></p>
<p>19 0.075582527 <a title="981-tfidf-19" href="../high_scalability-2011/high_scalability-2011-05-11-Troubleshooting_response_time_problems_%E2%80%93_why_you_cannot_trust_your_system_metrics.html">1038 high scalability-2011-05-11-Troubleshooting response time problems – why you cannot trust your system metrics</a></p>
<p>20 0.074141257 <a title="981-tfidf-20" href="../high_scalability-2010/high_scalability-2010-12-20-Netflix%3A_Use_Less_Chatty_Protocols_in_the_Cloud_-_Plus_26_Fixes.html">960 high scalability-2010-12-20-Netflix: Use Less Chatty Protocols in the Cloud - Plus 26 Fixes</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.101), (1, 0.082), (2, -0.006), (3, -0.002), (4, -0.033), (5, 0.017), (6, 0.093), (7, 0.065), (8, -0.06), (9, 0.001), (10, 0.016), (11, -0.007), (12, -0.009), (13, -0.026), (14, 0.028), (15, 0.031), (16, 0.018), (17, -0.013), (18, 0.031), (19, -0.022), (20, 0.037), (21, 0.038), (22, 0.038), (23, -0.029), (24, 0.026), (25, 0.043), (26, 0.023), (27, 0.016), (28, 0.006), (29, -0.022), (30, 0.076), (31, -0.031), (32, 0.026), (33, 0.028), (34, 0.039), (35, 0.065), (36, 0.025), (37, -0.064), (38, -0.057), (39, -0.002), (40, 0.036), (41, -0.038), (42, 0.028), (43, -0.027), (44, -0.023), (45, 0.004), (46, -0.0), (47, -0.012), (48, -0.012), (49, 0.044)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97919911 <a title="981-lsi-1" href="../high_scalability-2011/high_scalability-2011-02-01-Google_Strategy%3A_Tree_Distribution_of_Requests_and_Responses.html">981 high scalability-2011-02-01-Google Strategy: Tree Distribution of Requests and Responses</a></p>
<p>Introduction: If a large number of leaf node machines send requests to a central root node then that root node can become overwhelmed:
  
 The CPU becomes a bottleneck, for either processing requests or sending replies, because it can't possibly deal with the flood of requests. 
 The network interface becomes a bottleneck because a wide fan-in causes TCP drops and retransmissions, which causes latency. Then clients start retrying requests which quickly causes a spiral of death in an undisciplined system. 
  
One solution to this problem is a strategy given by Dr.  Jeff Dean , Head of Google's School of Infrastructure Wizardry, in this  Stanford video presentation :  Tree Distribution of Requests and Responses .
 
 
 
Instead of having a root node connected to leaves in a flat topology, the idea is to create a tree of nodes. So a root node talks to a number of parent nodes and the parent nodes talk to a number of leaf nodes. Requests are pushed down the tree through the parents and only hit a subset</p><p>2 0.81427783 <a title="981-lsi-2" href="../high_scalability-2012/high_scalability-2012-06-18-Google_on_Latency_Tolerant_Systems%3A_Making_a_Predictable_Whole_Out_of_Unpredictable_Parts___.html">1266 high scalability-2012-06-18-Google on Latency Tolerant Systems: Making a Predictable Whole Out of Unpredictable Parts   </a></p>
<p>Introduction: In   Taming The Long Latency Tail   we covered   Luiz Barroso  ’s exploration of the long tail latency (some operations are really slow) problems generated by large fanout architectures (a request is composed of potentially thousands of other requests). You may have noticed there weren’t a lot of solutions. That’s where a talk I attended,   Achieving Rapid Response Times in Large Online Services   (  slide deck  ), by  Jeff Dean , also of Google, comes in:
  
  In this talk, I’ll describe a collection of techniques and practices lowering response times in large distributed systems whose components run on shared clusters of machines, where pieces of these systems are subject to interference by other tasks, and where unpredictable latency hiccups are the norm, not the exception. 

  
 The goal is to use software techniques to reduce variability given the increasing variability in underlying hardware, the need to handle dynamic workloads on a shared infrastructure, and the need to use lar</p><p>3 0.7763828 <a title="981-lsi-3" href="../high_scalability-2012/high_scalability-2012-03-12-Google%3A_Taming_the_Long_Latency_Tail_-_When_More_Machines_Equals_Worse_Results.html">1207 high scalability-2012-03-12-Google: Taming the Long Latency Tail - When More Machines Equals Worse Results</a></p>
<p>Introduction: Likewise the current belief that, in the case of artificial machines the very large and the very small are equally feasible and lasting is a manifest error. Thus, for example, a small obelisk or column or other solid figure can certainly be laid down or set up without danger of breaking, while the large ones will go to pieces under the slightest provocation, and that purely on account of their own weight. -- Galileo  
Galileo observed how things broke if they were naively scaled up. Interestingly, Google noticed a similar pattern when building larger software systems using the same techniques used to build smaller systems. 
 
 Luiz André Barroso , Distinguished Engineer at Google, talks about this fundamental property of scaling systems in his fascinating talk,  Warehouse-Scale Computing: Entering the Teenage Decade . Google found the larger the scale the greater the impact of latency variability. When a request is implemented by work done in parallel, as is common with today's service</p><p>4 0.762609 <a title="981-lsi-4" href="../high_scalability-2009/high_scalability-2009-10-01-Moving_Beyond_End-to-End_Path_Information_to_Optimize_CDN_Performance.html">712 high scalability-2009-10-01-Moving Beyond End-to-End Path Information to Optimize CDN Performance</a></p>
<p>Introduction: You go through the expense of installing CDNs all over the globe to make sure users always have a node close by and you notice something curious and furious: clients still experience poor latencies. What's up with that? What do you do to find the problem?

If you are Google you build a tool (WhyHigh) to figure out what's up. This paper is about the tool and the unexpected problem of   high latencies   on CDNs. The main problems they found: inefficient routing to nearby nodes and packet queuing. But more useful is the architecture of WhyHigh and how it goes about identifying bottle necks. And even more useful is the general belief in creating sophisticated tools to understand and improve your service. That's what professionals do.

From the abstract:
   
Replicating content across a geographically distributed set of servers and redirecting clients to the closest server in terms of latency has emerged as a common paradigm for improving client performance. In this paper, we analyze latenc</p><p>5 0.715132 <a title="981-lsi-5" href="../high_scalability-2013/high_scalability-2013-12-04-How_Can_Batching_Requests_Actually_Reduce_Latency%3F.html">1558 high scalability-2013-12-04-How Can Batching Requests Actually Reduce Latency?</a></p>
<p>Introduction: Jeremy Edberg gave a talk on  Scaling Reddit from 1 Million to 1 Billion–Pitfalls and Lessons  and  one of the issues  they had was that they:
  

Did not account for increased latency after moving to EC2. In the datacenter they had submillisecond access between machines so it was possible to make a 1000 calls to memache for one page load. Not so on EC2. Memcache access times increased 10x to a millisecond which made their old approach unusable. Fix was to batch calls to memcache so a large number of gets are in one request.

  
Dave Pacheco had an  interesting question  about batching requests and its impact on latency:
  

 I was confused about the memcached problem after moving to the cloud.  I understand why network latency may have gone from submillisecond to milliseconds, but how could you improve latency by batching requests? Shouldn't that improve efficiency, not latency, at the possible expense of latency (since some requests will wait on the client as they get batched)?</p><p>6 0.70332742 <a title="981-lsi-6" href="../high_scalability-2009/high_scalability-2009-07-25-Latency_is_Everywhere_and_it_Costs_You_Sales_-_How_to_Crush_it.html">661 high scalability-2009-07-25-Latency is Everywhere and it Costs You Sales - How to Crush it</a></p>
<p>7 0.6832788 <a title="981-lsi-7" href="../high_scalability-2010/high_scalability-2010-11-15-Strategy%3A_Biggest_Performance_Impact_is_to_Reduce_the_Number_of_HTTP_Requests.html">942 high scalability-2010-11-15-Strategy: Biggest Performance Impact is to Reduce the Number of HTTP Requests</a></p>
<p>8 0.67955065 <a title="981-lsi-8" href="../high_scalability-2010/high_scalability-2010-11-22-Strategy%3A_Google_Sends_Canary_Requests_into_the_Data_Mine.html">946 high scalability-2010-11-22-Strategy: Google Sends Canary Requests into the Data Mine</a></p>
<p>9 0.67530221 <a title="981-lsi-9" href="../high_scalability-2010/high_scalability-2010-12-20-Netflix%3A_Use_Less_Chatty_Protocols_in_the_Cloud_-_Plus_26_Fixes.html">960 high scalability-2010-12-20-Netflix: Use Less Chatty Protocols in the Cloud - Plus 26 Fixes</a></p>
<p>10 0.66919851 <a title="981-lsi-10" href="../high_scalability-2013/high_scalability-2013-03-11-Low_Level_Scalability_Solutions_-_The_Conditioning_Collection.html">1421 high scalability-2013-03-11-Low Level Scalability Solutions - The Conditioning Collection</a></p>
<p>11 0.65760481 <a title="981-lsi-11" href="../high_scalability-2012/high_scalability-2012-08-20-The_Performance_of_Distributed_Data-Structures_Running_on_a_%22Cache-Coherent%22_In-Memory_Data_Grid.html">1307 high scalability-2012-08-20-The Performance of Distributed Data-Structures Running on a "Cache-Coherent" In-Memory Data Grid</a></p>
<p>12 0.65495205 <a title="981-lsi-12" href="../high_scalability-2013/high_scalability-2013-02-27-42_Monster_Problems_that_Attack_as_Loads_Increase.html">1413 high scalability-2013-02-27-42 Monster Problems that Attack as Loads Increase</a></p>
<p>13 0.65365165 <a title="981-lsi-13" href="../high_scalability-2010/high_scalability-2010-02-05-High_Availability_Principle_%3A_Concurrency_Control.html">772 high scalability-2010-02-05-High Availability Principle : Concurrency Control</a></p>
<p>14 0.65220344 <a title="981-lsi-14" href="../high_scalability-2010/high_scalability-2010-11-15-How_Google%27s_Instant_Previews_Reduces_HTTP_Requests.html">941 high scalability-2010-11-15-How Google's Instant Previews Reduces HTTP Requests</a></p>
<p>15 0.64919025 <a title="981-lsi-15" href="../high_scalability-2014/high_scalability-2014-03-31-How_WhatsApp_Grew_to_Nearly_500_Million_Users%2C_11%2C000_cores%2C_and_70_Million_Messages_a_Second.html">1622 high scalability-2014-03-31-How WhatsApp Grew to Nearly 500 Million Users, 11,000 cores, and 70 Million Messages a Second</a></p>
<p>16 0.6357379 <a title="981-lsi-16" href="../high_scalability-2013/high_scalability-2013-11-13-Google%3A_Multiplex_Multiple_Works_Loads_on_Computers_to_Increase_Machine_Utilization_and_Save_Money.html">1548 high scalability-2013-11-13-Google: Multiplex Multiple Works Loads on Computers to Increase Machine Utilization and Save Money</a></p>
<p>17 0.62984699 <a title="981-lsi-17" href="../high_scalability-2011/high_scalability-2011-09-15-Paper%3A_It%27s_Time_for_Low_Latency_-_Inventing_the_1_Microsecond_Datacenter.html">1116 high scalability-2011-09-15-Paper: It's Time for Low Latency - Inventing the 1 Microsecond Datacenter</a></p>
<p>18 0.62228882 <a title="981-lsi-18" href="../high_scalability-2011/high_scalability-2011-03-09-Google_and_Netflix_Strategy%3A_Use_Partial_Responses_to_Reduce_Request_Sizes.html">1001 high scalability-2011-03-09-Google and Netflix Strategy: Use Partial Responses to Reduce Request Sizes</a></p>
<p>19 0.62096369 <a title="981-lsi-19" href="../high_scalability-2012/high_scalability-2012-04-03-Hazelcast_2.0%3A_Big_Data_In-Memory.html">1221 high scalability-2012-04-03-Hazelcast 2.0: Big Data In-Memory</a></p>
<p>20 0.6178413 <a title="981-lsi-20" href="../high_scalability-2013/high_scalability-2013-03-04-7_Life_Saving_Scalability_Defenses_Against_Load_Monster_Attacks.html">1415 high scalability-2013-03-04-7 Life Saving Scalability Defenses Against Load Monster Attacks</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.117), (2, 0.161), (10, 0.045), (14, 0.255), (56, 0.043), (61, 0.087), (77, 0.017), (79, 0.072), (83, 0.01), (85, 0.017), (94, 0.068)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.88876134 <a title="981-lda-1" href="../high_scalability-2008/high_scalability-2008-10-07-Help_a_Scoble_out._What_should_Robert_ask_in_his_scalability_interview%3F.html">405 high scalability-2008-10-07-Help a Scoble out. What should Robert ask in his scalability interview?</a></p>
<p>Introduction: One of the cool things about Mr. Scoble is he doesn't pretend to know everything, which can be an deadly boring affliction in this field. In this case Robert is asking for help in an upcoming interview. Maybe we can help? Here's Robert's plight:     I’m really freaked out. I have one of the biggest interviews of my life coming up and I’m way under qualified to host it.  It’s on Thursday and it’s about Scalability and Performance of Web Services.  Look at who will be on. Matt Mullenweg, founder of Automattic, the company behind WordPress (and behind this blog). Paul Bucheit, one of the founders of FriendFeed and the creator of Gmail (he’s also the guy who gave Google the “don’t be evil” admonishion). Nat Brown, CTO of iLike, which got six million users on Facebook in about 10 days.       What would you ask?</p><p>2 0.8757391 <a title="981-lda-2" href="../high_scalability-2009/high_scalability-2009-10-21-Manage_virtualized_sprawl_with_VRMs.html">725 high scalability-2009-10-21-Manage virtualized sprawl with VRMs</a></p>
<p>Introduction: The essence of my work is coming into daily contact with innovative technologies. A recent example was at the request of a partner company who wanted to answer- which one of these tools will best solve my virtualized datacenter headache? After initial analysis all the products could be classified as tools that troubleshoot VM sprawl, but there was no universally accepted term for them. The most descriptive term  that I found was Virtual Resource Manager (VRM) from  DynamicOps . As I delved deeper into their workings, the distinction between VRMs and Private Clouds became blurred. What are the differences?
 
Read more at:  http://bigdatamatters.com/bigdatamatters/2009/10/cloud-vs-vrm.html</p><p>same-blog 3 0.87440652 <a title="981-lda-3" href="../high_scalability-2011/high_scalability-2011-02-01-Google_Strategy%3A_Tree_Distribution_of_Requests_and_Responses.html">981 high scalability-2011-02-01-Google Strategy: Tree Distribution of Requests and Responses</a></p>
<p>Introduction: If a large number of leaf node machines send requests to a central root node then that root node can become overwhelmed:
  
 The CPU becomes a bottleneck, for either processing requests or sending replies, because it can't possibly deal with the flood of requests. 
 The network interface becomes a bottleneck because a wide fan-in causes TCP drops and retransmissions, which causes latency. Then clients start retrying requests which quickly causes a spiral of death in an undisciplined system. 
  
One solution to this problem is a strategy given by Dr.  Jeff Dean , Head of Google's School of Infrastructure Wizardry, in this  Stanford video presentation :  Tree Distribution of Requests and Responses .
 
 
 
Instead of having a root node connected to leaves in a flat topology, the idea is to create a tree of nodes. So a root node talks to a number of parent nodes and the parent nodes talk to a number of leaf nodes. Requests are pushed down the tree through the parents and only hit a subset</p><p>4 0.844091 <a title="981-lda-4" href="../high_scalability-2009/high_scalability-2009-05-14-Who_Has_the_Most_Web_Servers%3F.html">599 high scalability-2009-05-14-Who Has the Most Web Servers?</a></p>
<p>Introduction: An  interesting post  on DataCenterKnowledge!
  
 1&1 Internet: 55,000 servers 
 Rackspace: 50,038 servers 
 The Planet: 48,500 servers 
 Akamai Technologies: 48,000 servers 
 OVH: 40,000 servers 
 SBC Communications: 29,193 servers 
 Verizon: 25,788 servers 
 Time Warner Cable: 24,817 servers 
 SoftLayer: 21,000 servers 
 AT&T;: 20,268 servers 
 iWeb: 10,000 servers 
 How about  Google , Microsoft,  Amazon ,  eBay , Yahoo, GoDaddy, Facebook? Check out the post on DataCenterKnowledge and of course here on highscalability.com!</p><p>5 0.83229029 <a title="981-lda-5" href="../high_scalability-2008/high_scalability-2008-11-13-CloudCamp_London_2%3A_private_clouds_and_standardisation.html">441 high scalability-2008-11-13-CloudCamp London 2: private clouds and standardisation</a></p>
<p>Introduction: CloudCamp returned to London yesterday, organised with the help of Skills Matter at the Crypt on the Clarkenwell green. The main topics of this cloud/grid computing community meeting were service-level agreements, connecting private and public clouds and standardisation issues.</p><p>6 0.82037663 <a title="981-lda-6" href="../high_scalability-2012/high_scalability-2012-05-28-The_Anatomy_of_Search_Technology%3A_Crawling_using_Combinators.html">1253 high scalability-2012-05-28-The Anatomy of Search Technology: Crawling using Combinators</a></p>
<p>7 0.80460775 <a title="981-lda-7" href="../high_scalability-2009/high_scalability-2009-09-04-Hot_Links_for_2009-9-4_.html">694 high scalability-2009-09-04-Hot Links for 2009-9-4 </a></p>
<p>8 0.79359543 <a title="981-lda-8" href="../high_scalability-2009/high_scalability-2009-01-17-Intro_to_Caching%2CCaching_algorithms_and_caching_frameworks_part_1.html">495 high scalability-2009-01-17-Intro to Caching,Caching algorithms and caching frameworks part 1</a></p>
<p>9 0.78702921 <a title="981-lda-9" href="../high_scalability-2009/high_scalability-2009-03-12-QCon_London_2009%3A_Database_projects_to_watch_closely.html">537 high scalability-2009-03-12-QCon London 2009: Database projects to watch closely</a></p>
<p>10 0.77237201 <a title="981-lda-10" href="../high_scalability-2007/high_scalability-2007-10-21-Paper%3A_Standardizing_Storage_Clusters_%28with_pNFS%29.html">128 high scalability-2007-10-21-Paper: Standardizing Storage Clusters (with pNFS)</a></p>
<p>11 0.76191062 <a title="981-lda-11" href="../high_scalability-2009/high_scalability-2009-01-08-Paper%3A_Sharding_with_Oracle_Database.html">487 high scalability-2009-01-08-Paper: Sharding with Oracle Database</a></p>
<p>12 0.75396991 <a title="981-lda-12" href="../high_scalability-2009/high_scalability-2009-11-24-Hot_Scalability_Links_for_Nov_24_2009.html">744 high scalability-2009-11-24-Hot Scalability Links for Nov 24 2009</a></p>
<p>13 0.74381959 <a title="981-lda-13" href="../high_scalability-2012/high_scalability-2012-07-06-Stuff_The_Internet_Says_On_Scalability_For_July_6%2C_2012.html">1278 high scalability-2012-07-06-Stuff The Internet Says On Scalability For July 6, 2012</a></p>
<p>14 0.68891102 <a title="981-lda-14" href="../high_scalability-2012/high_scalability-2012-04-25-The_Anatomy_of_Search_Technology%3A_blekko%E2%80%99s_NoSQL_database.html">1233 high scalability-2012-04-25-The Anatomy of Search Technology: blekko’s NoSQL database</a></p>
<p>15 0.68060929 <a title="981-lda-15" href="../high_scalability-2009/high_scalability-2009-07-20-A_Scalability_Lament.html">659 high scalability-2009-07-20-A Scalability Lament</a></p>
<p>16 0.67896318 <a title="981-lda-16" href="../high_scalability-2012/high_scalability-2012-01-24-The_State_of_NoSQL_in_2012.html">1180 high scalability-2012-01-24-The State of NoSQL in 2012</a></p>
<p>17 0.67669249 <a title="981-lda-17" href="../high_scalability-2012/high_scalability-2012-08-20-The_Performance_of_Distributed_Data-Structures_Running_on_a_%22Cache-Coherent%22_In-Memory_Data_Grid.html">1307 high scalability-2012-08-20-The Performance of Distributed Data-Structures Running on a "Cache-Coherent" In-Memory Data Grid</a></p>
<p>18 0.6732825 <a title="981-lda-18" href="../high_scalability-2012/high_scalability-2012-10-04-Stuff_The_Internet_Says_On_Scalability_For_October_5%2C_2012.html">1334 high scalability-2012-10-04-Stuff The Internet Says On Scalability For October 5, 2012</a></p>
<p>19 0.67232561 <a title="981-lda-19" href="../high_scalability-2009/high_scalability-2009-09-22-How_Ravelry_Scales_to_10_Million_Requests_Using_Rails.html">711 high scalability-2009-09-22-How Ravelry Scales to 10 Million Requests Using Rails</a></p>
<p>20 0.67151463 <a title="981-lda-20" href="../high_scalability-2012/high_scalability-2012-09-26-WordPress.com_Serves_70%2C000_req-sec_and_over_15_Gbit-sec_of_Traffic_using_NGINX.html">1329 high scalability-2012-09-26-WordPress.com Serves 70,000 req-sec and over 15 Gbit-sec of Traffic using NGINX</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
