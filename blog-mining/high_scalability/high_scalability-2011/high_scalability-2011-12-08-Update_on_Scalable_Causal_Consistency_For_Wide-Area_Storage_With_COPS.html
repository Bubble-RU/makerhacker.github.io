<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1153 high scalability-2011-12-08-Update on Scalable Causal Consistency For Wide-Area Storage With COPS</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2011" href="../home/high_scalability-2011_home.html">high_scalability-2011</a> <a title="high_scalability-2011-1153" href="#">high_scalability-2011-1153</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1153 high scalability-2011-12-08-Update on Scalable Causal Consistency For Wide-Area Storage With COPS</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2011-1153-html" href="http://highscalability.com//blog/2011/12/8/update-on-scalable-causal-consistency-for-wide-area-storage.html">html</a></p><p>Introduction: Here are a few updates on the article  Paper: Don’t Settle For Eventual: Scalable Causal Consistency For Wide-Area Storage With COPS  from Mike Freedman and Wyatt Lloyd.
 
 Q: How software architectures could change in response to casual+ consistency? 
 
 A : I don't really think they would much. Somebody would still run a two-tier architecture in their datacenter:  a front-tier of webservers running both (say) PHP and our client library, and a back tier of storage nodes running COPS.  (I'm not sure if it was obvious given the discussion of our "thick" client -- you should think of the COPS client dropping in where a memcache client library does...albeit ours has per-session state.)
 
 
 
 Q: Why not just use vector clocks? 
 
 A : The problem with vector clocks and scalability has always been that the size of vector clocks in O(N), where N is the number of nodes.  So if we want to scale to a datacenter with 10K nodes, each piece of metadata must have size O(10K).  And in fact, vector</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Somebody would still run a two-tier architecture in their datacenter:  a front-tier of webservers running both (say) PHP and our client library, and a back tier of storage nodes running COPS. [sent-4, score-0.159]
</p><p>2 (I'm not sure if it was obvious given the discussion of our "thick" client -- you should think of the COPS client dropping in where a memcache client library does. [sent-5, score-0.427]
</p><p>3 A : The problem with vector clocks and scalability has always been that the size of vector clocks in O(N), where N is the number of nodes. [sent-10, score-1.072]
</p><p>4 So if we want to scale to a datacenter with 10K nodes, each piece of metadata must have size O(10K). [sent-11, score-0.184]
</p><p>5 And in fact, vector clocks alone only allow you to learn Lamport's happens-before relationship -- they don't actually govern how you would enforce causal consistency in the system. [sent-12, score-1.387]
</p><p>6 You'd still need to employ either a serialization point or explicit dependency checking, as we do, in order to provide the desired consistency across the datacenter. [sent-13, score-0.718]
</p><p>7 Distributed systems protocols (and DB systems) typically think about sites that include all the data they wish to establish causal ordering over. [sent-14, score-0.47]
</p><p>8 In COPS, a datacenter node only has a shard of the total data set, so you are fundamentally going to need to do some consistency enforcement or verification protocol. [sent-15, score-0.68]
</p><p>9 In short:  Vector clocks give you potential ordering between two observed operations. [sent-16, score-0.368]
</p><p>10 We use explicit dependency metadata to tell a server what other operation it depends on, because that operation likely resides only on other servers in the cluster! [sent-17, score-0.484]
</p><p>11 A :  I think it introduces a different consistency model. [sent-19, score-0.451]
</p><p>12 To my knowledge, Cassandra doesn't actually allow you to achieve *any* consistency guarantees *across* keys, it's support for transactions and strong consistency is limited to each row (as is BigTable, PNUTS, etc. [sent-20, score-1.014]
</p><p>13 COPS allows you to achieve gain stronger consistency than eventual *across* rows/tablets/servers, while preserving the ALPS properties. [sent-22, score-0.745]
</p><p>14 This is why one of the directions of future work we are looking at is actually seeing how difficult it would be to "port" COPS' algorithms to Cassandra (or maybe even sharded MySQL) to provide cross-key consistency properties. [sent-23, score-0.521]
</p><p>15 Likelihood ^ as consistency v from: strong -> causal -> eventual. [sent-25, score-0.737]
</p><p>16 A : In your post, you noticed we have to deal with conflicting updates to the same key in a similar way to eventual consistency, either applying the last-writer-wins rule or some app-specific function. [sent-26, score-0.47]
</p><p>17 This is a good point, and one of the things I like about eventual consistency is that it forces people to acknowledge that conflicts are possible and that you have to deal with them. [sent-27, score-0.872]
</p><p>18 What's sort of surprising to many is that they are possible even with strong consistency (linearizability). [sent-28, score-0.51]
</p><p>19 Even if two updates don't conflict explicitly, they can still conflict logically. [sent-29, score-0.282]
</p><p>20 For instance, if two people (U and I) write to the same key (lock initially 0, U: lock = 1, I: lock = 1) at about the same time, the updates will be ordered by real time, but they should really conflict (the second lock = 1 should fail! [sent-30, score-0.431]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cops', 0.399), ('consistency', 0.391), ('clocks', 0.268), ('vector', 0.268), ('causal', 0.249), ('eventual', 0.167), ('conflict', 0.159), ('lock', 0.149), ('updates', 0.123), ('conflicts', 0.117), ('explicit', 0.106), ('dependency', 0.101), ('ordering', 0.1), ('client', 0.099), ('datacenter', 0.097), ('strong', 0.097), ('operation', 0.095), ('metadata', 0.087), ('cassandra', 0.086), ('freedman', 0.085), ('alps', 0.08), ('acknowledge', 0.08), ('linearizability', 0.08), ('govern', 0.08), ('thick', 0.076), ('wyatt', 0.076), ('lamport', 0.073), ('enforcement', 0.071), ('actually', 0.07), ('library', 0.07), ('likelihood', 0.067), ('pnuts', 0.066), ('achieve', 0.065), ('casual', 0.065), ('preserving', 0.065), ('verification', 0.063), ('settle', 0.063), ('conflicting', 0.062), ('establish', 0.061), ('enforce', 0.061), ('either', 0.061), ('webservers', 0.06), ('think', 0.06), ('possible', 0.06), ('directions', 0.06), ('sort', 0.059), ('employ', 0.059), ('fundamentally', 0.058), ('stronger', 0.057), ('deal', 0.057)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="1153-tfidf-1" href="../high_scalability-2011/high_scalability-2011-12-08-Update_on_Scalable_Causal_Consistency_For_Wide-Area_Storage_With_COPS.html">1153 high scalability-2011-12-08-Update on Scalable Causal Consistency For Wide-Area Storage With COPS</a></p>
<p>Introduction: Here are a few updates on the article  Paper: Don’t Settle For Eventual: Scalable Causal Consistency For Wide-Area Storage With COPS  from Mike Freedman and Wyatt Lloyd.
 
 Q: How software architectures could change in response to casual+ consistency? 
 
 A : I don't really think they would much. Somebody would still run a two-tier architecture in their datacenter:  a front-tier of webservers running both (say) PHP and our client library, and a back tier of storage nodes running COPS.  (I'm not sure if it was obvious given the discussion of our "thick" client -- you should think of the COPS client dropping in where a memcache client library does...albeit ours has per-session state.)
 
 
 
 Q: Why not just use vector clocks? 
 
 A : The problem with vector clocks and scalability has always been that the size of vector clocks in O(N), where N is the number of nodes.  So if we want to scale to a datacenter with 10K nodes, each piece of metadata must have size O(10K).  And in fact, vector</p><p>2 0.59634894 <a title="1153-tfidf-2" href="../high_scalability-2011/high_scalability-2011-11-23-Paper%3A_Don%E2%80%99t_Settle_for_Eventual%3A_Scalable_Causal_Consistency_for_Wide-Area_Storage_with_COPS.html">1146 high scalability-2011-11-23-Paper: Don’t Settle for Eventual: Scalable Causal Consistency for Wide-Area Storage with COPS</a></p>
<p>Introduction: Teams from  Princeton  and CMU are  working together  to solve one of the most difficult problems in the repertoire: scalable geo-distributed data stores. Major companies like Google and Facebook have been working on multiple datacenter database functionality for some time, but there's still a general lack of available systems that work for complex data scenarios.
 
The ideas in this paper-- Don’t Settle for Eventual: Scalable Causal Consistency for Wide-Area Storage with COPS --are different. It's not another eventually consistent system, or a traditional transaction oriented system, or a replication based system, or a system that punts on the issue. It's something new, a causally consistent system that achieves  ALPS  system properties. Move over CAP, NoSQL, etc, we have another acronym: ALPS - Available (operations always complete successfully), Low-latency (operations complete quickly (single digit milliseconds)), Partition-tolerant (operates with a partition), and Scalable (just a</p><p>3 0.21791182 <a title="1153-tfidf-3" href="../high_scalability-2009/high_scalability-2009-08-24-How_Google_Serves_Data_from_Multiple_Datacenters.html">687 high scalability-2009-08-24-How Google Serves Data from Multiple Datacenters</a></p>
<p>Introduction: Update:   Streamy Explains CAP and HBase's Approach to CAP .  We plan to employ inter-cluster replication, with each cluster located in a single DC.  Remote replication will introduce some eventual consistency into the system, but each cluster will continue to be strongly consistent.   Ryan Barrett, Google App Engine datastore lead, gave this talk   Transactions Across Datacenters (and Other Weekend Projects)   at the Google I/O 2009 conference.   While the talk doesn't necessarily break new technical ground, Ryan does an excellent job explaining and evaluating the different options you have when architecting a system to work across multiple datacenters. This is called  multihoming ,  operating from multiple datacenters simultaneously.  As multihoming is one of the most challenging tasks in all computing, Ryan's clear and thoughtful style comfortably leads you through the various options. On the trip you learn:
   The different  multi-homing options  are: Backups, Master-Slave, Multi-M</p><p>4 0.19277966 <a title="1153-tfidf-4" href="../high_scalability-2011/high_scalability-2011-04-06-Netflix%3A_Run_Consistency_Checkers_All_the_time_to_Fixup_Transactions.html">1017 high scalability-2011-04-06-Netflix: Run Consistency Checkers All the time to Fixup Transactions</a></p>
<p>Introduction: You  might have  consistency problems if you have: multiple datastores in multiple datacenters, without distributed transactions, and with the ability to alternately execute out of each datacenter;  syncing protocols that can fail or sync stale data; distributed clients that cache data and then write old back to the central store; a NoSQL database that doesn't have transactions between updates of multiple related key-value records; application level integrity checks; client driven  optimistic locking .
 
Sounds a lot like many evolving, loosely coupled, autonomous, distributed systems these days. How do you solve these consistency problems? Siddharth "Sid" Anand of Netflix talks about how they solved theirs in his excellent presentation,  NoSQL @ Netflix : Part 1 , given to a packed crowd at a  Cloud Computing Meetup . 
 
You might be inclined to say how silly it is to have these problems in the first place, but just hold on. See if you might share some of their problems, before gettin</p><p>5 0.1855095 <a title="1153-tfidf-5" href="../high_scalability-2009/high_scalability-2009-08-08-Yahoo%21%27s_PNUTS_Database%3A_Too_Hot%2C_Too_Cold_or_Just_Right%3F.html">676 high scalability-2009-08-08-Yahoo!'s PNUTS Database: Too Hot, Too Cold or Just Right?</a></p>
<p>Introduction: So far every massively scalable database is a bundle of compromises. For some the weak guarantees of Amazon's  eventual consistency  model are too cold. For many the strong guarantees of standard RDBMS  distributed transactions  are too hot. Google App Engine tries to get it just right with  entity groups . Yahoo! is also trying to get is just right by offering per-record timeline consistency, which hopes to serve up a heaping bowl of  rich database functionality and low latency at massive scale :
   We describe PNUTS [Platform for Nimble Universal Table Storage], a massively parallel and geographically distributed database system for Yahoo!â&euro;&trade;s web applications. PNUTS provides data storage organized as hashed or ordered tables, low latency for large numbers of con-current requests including updates and queries, and novel per-record consistency guarantees. It is a hosted, centrally managed, and geographically distributed service, and utilizes automated load-balancing and failover to redu</p><p>6 0.18130729 <a title="1153-tfidf-6" href="../high_scalability-2010/high_scalability-2010-03-03-Hot_Scalability_Links_for_March_3%2C_2010.html">787 high scalability-2010-03-03-Hot Scalability Links for March 3, 2010</a></p>
<p>7 0.15992816 <a title="1153-tfidf-7" href="../high_scalability-2010/high_scalability-2010-08-12-Think_of_Latency_as_a_Pseudo-permanent_Network_Partition.html">879 high scalability-2010-08-12-Think of Latency as a Pseudo-permanent Network Partition</a></p>
<p>8 0.15296154 <a title="1153-tfidf-8" href="../high_scalability-2012/high_scalability-2012-12-18-Georeplication%3A_When_Bad_Things_Happen_to_Good_Systems.html">1374 high scalability-2012-12-18-Georeplication: When Bad Things Happen to Good Systems</a></p>
<p>9 0.15290944 <a title="1153-tfidf-9" href="../high_scalability-2013/high_scalability-2013-05-01-Myth%3A_Eric_Brewer_on_Why_Banks_are_BASE_Not_ACID_-_Availability_Is_Revenue_.html">1450 high scalability-2013-05-01-Myth: Eric Brewer on Why Banks are BASE Not ACID - Availability Is Revenue </a></p>
<p>10 0.14796588 <a title="1153-tfidf-10" href="../high_scalability-2011/high_scalability-2011-04-13-Paper%3A_NoSQL_Databases_-_NoSQL_Introduction_and_Overview.html">1022 high scalability-2011-04-13-Paper: NoSQL Databases - NoSQL Introduction and Overview</a></p>
<p>11 0.14762197 <a title="1153-tfidf-11" href="../high_scalability-2010/high_scalability-2010-12-23-Paper%3A_CRDTs%3A_Consistency_without_concurrency_control.html">963 high scalability-2010-12-23-Paper: CRDTs: Consistency without concurrency control</a></p>
<p>12 0.1260722 <a title="1153-tfidf-12" href="../high_scalability-2012/high_scalability-2012-10-22-Spanner_-_It%27s_About_Programmers_Building_Apps_Using_SQL_Semantics_at_NoSQL_Scale.html">1345 high scalability-2012-10-22-Spanner - It's About Programmers Building Apps Using SQL Semantics at NoSQL Scale</a></p>
<p>13 0.114968 <a title="1153-tfidf-13" href="../high_scalability-2009/high_scalability-2009-05-05-Drop_ACID_and_Think_About_Data.html">589 high scalability-2009-05-05-Drop ACID and Think About Data</a></p>
<p>14 0.11473629 <a title="1153-tfidf-14" href="../high_scalability-2013/high_scalability-2013-06-13-Busting_4_Modern_Hardware_Myths_-_Are_Memory%2C_HDDs%2C_and_SSDs_Really_Random_Access%3F.html">1475 high scalability-2013-06-13-Busting 4 Modern Hardware Myths - Are Memory, HDDs, and SSDs Really Random Access?</a></p>
<p>15 0.10805307 <a title="1153-tfidf-15" href="../high_scalability-2010/high_scalability-2010-12-06-What_the_heck_are_you_actually_using_NoSQL_for%3F.html">954 high scalability-2010-12-06-What the heck are you actually using NoSQL for?</a></p>
<p>16 0.098925032 <a title="1153-tfidf-16" href="../high_scalability-2009/high_scalability-2009-02-03-Paper%3A_Optimistic_Replication.html">507 high scalability-2009-02-03-Paper: Optimistic Replication</a></p>
<p>17 0.096336037 <a title="1153-tfidf-17" href="../high_scalability-2013/high_scalability-2013-12-13-Stuff_The_Internet_Says_On_Scalability_For_December_13th%2C_2013.html">1564 high scalability-2013-12-13-Stuff The Internet Says On Scalability For December 13th, 2013</a></p>
<p>18 0.092048526 <a title="1153-tfidf-18" href="../high_scalability-2013/high_scalability-2013-12-16-22_Recommendations_for_Building_Effective_High_Traffic_Web_Software.html">1565 high scalability-2013-12-16-22 Recommendations for Building Effective High Traffic Web Software</a></p>
<p>19 0.091311581 <a title="1153-tfidf-19" href="../high_scalability-2010/high_scalability-2010-10-15-Troubles_with_Sharding_-_What_can_we_learn_from_the_Foursquare_Incident%3F.html">920 high scalability-2010-10-15-Troubles with Sharding - What can we learn from the Foursquare Incident?</a></p>
<p>20 0.08882118 <a title="1153-tfidf-20" href="../high_scalability-2014/high_scalability-2014-05-02-Stuff_The_Internet_Says_On_Scalability_For_May_2nd%2C_2014.html">1642 high scalability-2014-05-02-Stuff The Internet Says On Scalability For May 2nd, 2014</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.141), (1, 0.09), (2, -0.004), (3, 0.059), (4, 0.019), (5, 0.094), (6, -0.002), (7, -0.004), (8, -0.047), (9, -0.061), (10, 0.003), (11, 0.071), (12, -0.116), (13, -0.043), (14, 0.059), (15, 0.062), (16, 0.049), (17, 0.015), (18, 0.001), (19, -0.097), (20, 0.108), (21, 0.094), (22, -0.026), (23, -0.018), (24, -0.116), (25, -0.068), (26, 0.056), (27, -0.024), (28, 0.035), (29, -0.172), (30, 0.041), (31, -0.039), (32, -0.069), (33, 0.007), (34, -0.007), (35, -0.01), (36, -0.032), (37, 0.075), (38, -0.046), (39, 0.017), (40, -0.016), (41, 0.103), (42, -0.029), (43, 0.036), (44, 0.026), (45, -0.044), (46, 0.011), (47, 0.023), (48, 0.017), (49, -0.031)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96476805 <a title="1153-lsi-1" href="../high_scalability-2011/high_scalability-2011-12-08-Update_on_Scalable_Causal_Consistency_For_Wide-Area_Storage_With_COPS.html">1153 high scalability-2011-12-08-Update on Scalable Causal Consistency For Wide-Area Storage With COPS</a></p>
<p>Introduction: Here are a few updates on the article  Paper: Don’t Settle For Eventual: Scalable Causal Consistency For Wide-Area Storage With COPS  from Mike Freedman and Wyatt Lloyd.
 
 Q: How software architectures could change in response to casual+ consistency? 
 
 A : I don't really think they would much. Somebody would still run a two-tier architecture in their datacenter:  a front-tier of webservers running both (say) PHP and our client library, and a back tier of storage nodes running COPS.  (I'm not sure if it was obvious given the discussion of our "thick" client -- you should think of the COPS client dropping in where a memcache client library does...albeit ours has per-session state.)
 
 
 
 Q: Why not just use vector clocks? 
 
 A : The problem with vector clocks and scalability has always been that the size of vector clocks in O(N), where N is the number of nodes.  So if we want to scale to a datacenter with 10K nodes, each piece of metadata must have size O(10K).  And in fact, vector</p><p>2 0.9244343 <a title="1153-lsi-2" href="../high_scalability-2011/high_scalability-2011-11-23-Paper%3A_Don%E2%80%99t_Settle_for_Eventual%3A_Scalable_Causal_Consistency_for_Wide-Area_Storage_with_COPS.html">1146 high scalability-2011-11-23-Paper: Don’t Settle for Eventual: Scalable Causal Consistency for Wide-Area Storage with COPS</a></p>
<p>Introduction: Teams from  Princeton  and CMU are  working together  to solve one of the most difficult problems in the repertoire: scalable geo-distributed data stores. Major companies like Google and Facebook have been working on multiple datacenter database functionality for some time, but there's still a general lack of available systems that work for complex data scenarios.
 
The ideas in this paper-- Don’t Settle for Eventual: Scalable Causal Consistency for Wide-Area Storage with COPS --are different. It's not another eventually consistent system, or a traditional transaction oriented system, or a replication based system, or a system that punts on the issue. It's something new, a causally consistent system that achieves  ALPS  system properties. Move over CAP, NoSQL, etc, we have another acronym: ALPS - Available (operations always complete successfully), Low-latency (operations complete quickly (single digit milliseconds)), Partition-tolerant (operates with a partition), and Scalable (just a</p><p>3 0.898247 <a title="1153-lsi-3" href="../high_scalability-2013/high_scalability-2013-05-01-Myth%3A_Eric_Brewer_on_Why_Banks_are_BASE_Not_ACID_-_Availability_Is_Revenue_.html">1450 high scalability-2013-05-01-Myth: Eric Brewer on Why Banks are BASE Not ACID - Availability Is Revenue </a></p>
<p>Introduction: In   NoSQL: Past, Present, Future    Eric Brewer  has a particularly fine section on explaining the often hard to understand ideas of   BASE   (Basically Available, Soft State, Eventually Consistent),   ACID   (Atomicity, Consistency, Isolation, Durability),   CAP   (Consistency Availability, Partition Tolerance), in terms of a pernicious long standing myth about the sanctity of consistency in banking.
    Myth   : Money is important, so banks   must   use transactions to keep money safe and consistent, right? 
    Reality   : Banking transactions are inconsistent, particularly for ATMs. ATMs are designed to have a normal case behaviour and a partition mode behaviour. In partition mode Availability is chosen over Consistency. 
   Why?   1)  Availability correlates with revenue and consistency generally does not.  2)  Historically there was never an idea of perfect communication so everything was partitioned.
   Your ATM transaction must go through so Availability is more important than</p><p>4 0.8928411 <a title="1153-lsi-4" href="../high_scalability-2012/high_scalability-2012-12-18-Georeplication%3A_When_Bad_Things_Happen_to_Good_Systems.html">1374 high scalability-2012-12-18-Georeplication: When Bad Things Happen to Good Systems</a></p>
<p>Introduction: Georeplication is one of the standard techniques for dealing when bad things--failure and latency--happen to good systems. The problem is always: how do you do that?  Murat Demirbas , Associate Professor at SUNY Buffalo, has a couple of really good posts that can help:  MDCC: Multi-Data Center Consistency  and  Making Geo-Replicated Systems Fast as Possible, Consistent when Necessary . 
 
In  MDCC: Multi-Data Center Consistency  Murat discusses a paper that says synchronous wide-area replication can be feasible. There's a quick and clear explanation of Paxos and various optimizations that is worth the price of admission. We find that strong consistency doesn't have to be lost across a WAN:
  

The good thing about using Paxos over the WAN is you /almost/ get the full CAP  (all three properties: consistency, availability, and partition-freedom). As we discussed earlier (Paxos taught), Paxos is CP, that is, in the presence of a partition, Paxos keeps consistency over availability. But, P</p><p>5 0.82541722 <a title="1153-lsi-5" href="../high_scalability-2010/high_scalability-2010-08-12-Think_of_Latency_as_a_Pseudo-permanent_Network_Partition.html">879 high scalability-2010-08-12-Think of Latency as a Pseudo-permanent Network Partition</a></p>
<p>Introduction: The title of this post is a quote from Ilya Grigorik's post  Weak Consistency and CAP Implications . Besides the article being excellent, I thought this idea had something to add to the great NoSQL versus RDBMS debate, where  Mike Stonebraker  makes the argument that network partitions are rare so designing eventually consistent systems for such rare occurrence is not worth losing ACID semantics over. Even if network partitions are rare, latency between datacenters is not rare, so the game is still on.
 
The rare-partition argument seems to flow from a centralized-distributed view of systems. Such systems are scale-out in that they grow by adding distributed nodes, but the nodes generally do not cross datacenter boundaries. The assumption is the network is fast enough that distributed operations are roughly homogenous between nodes.
 
In a fully-distributed system the nodes can be dispersed across datacenters, which gives operations a widely variable performance profile. Because everyt</p><p>6 0.7934655 <a title="1153-lsi-6" href="../high_scalability-2009/high_scalability-2009-08-24-How_Google_Serves_Data_from_Multiple_Datacenters.html">687 high scalability-2009-08-24-How Google Serves Data from Multiple Datacenters</a></p>
<p>7 0.77436614 <a title="1153-lsi-7" href="../high_scalability-2009/high_scalability-2009-08-08-Yahoo%21%27s_PNUTS_Database%3A_Too_Hot%2C_Too_Cold_or_Just_Right%3F.html">676 high scalability-2009-08-08-Yahoo!'s PNUTS Database: Too Hot, Too Cold or Just Right?</a></p>
<p>8 0.76768625 <a title="1153-lsi-8" href="../high_scalability-2010/high_scalability-2010-09-01-Paper%3A_The_Case_for_Determinism_in_Database_Systems__.html">890 high scalability-2010-09-01-Paper: The Case for Determinism in Database Systems  </a></p>
<p>9 0.73031008 <a title="1153-lsi-9" href="../high_scalability-2012/high_scalability-2012-06-27-Paper%3A_Logic_and_Lattices_for_Distributed_Programming.html">1273 high scalability-2012-06-27-Paper: Logic and Lattices for Distributed Programming</a></p>
<p>10 0.7247628 <a title="1153-lsi-10" href="../high_scalability-2007/high_scalability-2007-10-03-Paper%3A_Brewer%27s_Conjecture_and_the_Feasibility_of_Consistent_Available_Partition-Tolerant_Web_Services.html">108 high scalability-2007-10-03-Paper: Brewer's Conjecture and the Feasibility of Consistent Available Partition-Tolerant Web Services</a></p>
<p>11 0.70750821 <a title="1153-lsi-11" href="../high_scalability-2013/high_scalability-2013-05-16-Paper%3A_Warp%3A_Multi-Key_Transactions_for_Key-Value_Stores.html">1459 high scalability-2013-05-16-Paper: Warp: Multi-Key Transactions for Key-Value Stores</a></p>
<p>12 0.69735026 <a title="1153-lsi-12" href="../high_scalability-2009/high_scalability-2009-02-03-Paper%3A_Optimistic_Replication.html">507 high scalability-2009-02-03-Paper: Optimistic Replication</a></p>
<p>13 0.69669628 <a title="1153-lsi-13" href="../high_scalability-2010/high_scalability-2010-12-23-Paper%3A_CRDTs%3A_Consistency_without_concurrency_control.html">963 high scalability-2010-12-23-Paper: CRDTs: Consistency without concurrency control</a></p>
<p>14 0.67205 <a title="1153-lsi-14" href="../high_scalability-2011/high_scalability-2011-04-06-Netflix%3A_Run_Consistency_Checkers_All_the_time_to_Fixup_Transactions.html">1017 high scalability-2011-04-06-Netflix: Run Consistency Checkers All the time to Fixup Transactions</a></p>
<p>15 0.64920038 <a title="1153-lsi-15" href="../high_scalability-2009/high_scalability-2009-03-10-Paper%3A_Consensus_Protocols%3A_Paxos___.html">529 high scalability-2009-03-10-Paper: Consensus Protocols: Paxos   </a></p>
<p>16 0.64424789 <a title="1153-lsi-16" href="../high_scalability-2013/high_scalability-2013-05-23-Paper%3A_Calvin%3A_Fast_Distributed_Transactions_for_Partitioned_Database_Systems.html">1463 high scalability-2013-05-23-Paper: Calvin: Fast Distributed Transactions for Partitioned Database Systems</a></p>
<p>17 0.62701625 <a title="1153-lsi-17" href="../high_scalability-2011/high_scalability-2011-01-11-Google_Megastore_-_3_Billion_Writes_and_20_Billion_Read_Transactions_Daily.html">972 high scalability-2011-01-11-Google Megastore - 3 Billion Writes and 20 Billion Read Transactions Daily</a></p>
<p>18 0.61504155 <a title="1153-lsi-18" href="../high_scalability-2012/high_scalability-2012-05-10-Paper%3A_Paxos_Made_Moderately_Complex.html">1243 high scalability-2012-05-10-Paper: Paxos Made Moderately Complex</a></p>
<p>19 0.60776126 <a title="1153-lsi-19" href="../high_scalability-2010/high_scalability-2010-03-03-Hot_Scalability_Links_for_March_3%2C_2010.html">787 high scalability-2010-03-03-Hot Scalability Links for March 3, 2010</a></p>
<p>20 0.60441941 <a title="1153-lsi-20" href="../high_scalability-2014/high_scalability-2014-04-10-Paper%3A_Scalable_Atomic_Visibility_with_RAMP_Transactions_-_Scale_Linearly_to_100_Servers.html">1629 high scalability-2014-04-10-Paper: Scalable Atomic Visibility with RAMP Transactions - Scale Linearly to 100 Servers</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.103), (2, 0.184), (10, 0.052), (35, 0.156), (40, 0.022), (47, 0.021), (51, 0.015), (61, 0.176), (68, 0.024), (79, 0.114), (85, 0.032), (94, 0.011)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.97135592 <a title="1153-lda-1" href="../high_scalability-2009/high_scalability-2009-11-11-Hot_Scalability_Links_for_Nov_11_2009__.html">740 high scalability-2009-11-11-Hot Scalability Links for Nov 11 2009  </a></p>
<p>Introduction: The Cost of Latency  by James Hamilton. James summarizes latency info from  Steve Souder ,  Greg Linden , and  Marissa Mayer .  Speed [is] an undervalued and under-discussed asset on the web.  
  Dynamo - Part I: a followup and re-rebuttals . Dynamo under attack as having Design flaws and the resounding rebuttal in response. 
  Programming Bits and Atoms . Thinking about programming and scaling as a problem in physics. Absolutely fascinating and inspiring. 
  Scaling Servers with the Cloud: Amazon S3 . Build a static site using S3 for pennies. An oldly but still a goody idea. 
  Are Wireless Road Trains the Cure for Traffic Congestion?   The concept of road trains--up to eight vehicles zooming down the road together--has long been considered a faster, safer, and greener way of traveling long distances by car.  
  Erlang at Facebook  by Eugene Letuchy. How Facebook uses Erlang to implement Chat, AIM Presence, and Chat Jabber support.  
  Yahoo Open Sources Traffic Server .  Traffic Serv</p><p>same-blog 2 0.90965545 <a title="1153-lda-2" href="../high_scalability-2011/high_scalability-2011-12-08-Update_on_Scalable_Causal_Consistency_For_Wide-Area_Storage_With_COPS.html">1153 high scalability-2011-12-08-Update on Scalable Causal Consistency For Wide-Area Storage With COPS</a></p>
<p>Introduction: Here are a few updates on the article  Paper: Don’t Settle For Eventual: Scalable Causal Consistency For Wide-Area Storage With COPS  from Mike Freedman and Wyatt Lloyd.
 
 Q: How software architectures could change in response to casual+ consistency? 
 
 A : I don't really think they would much. Somebody would still run a two-tier architecture in their datacenter:  a front-tier of webservers running both (say) PHP and our client library, and a back tier of storage nodes running COPS.  (I'm not sure if it was obvious given the discussion of our "thick" client -- you should think of the COPS client dropping in where a memcache client library does...albeit ours has per-session state.)
 
 
 
 Q: Why not just use vector clocks? 
 
 A : The problem with vector clocks and scalability has always been that the size of vector clocks in O(N), where N is the number of nodes.  So if we want to scale to a datacenter with 10K nodes, each piece of metadata must have size O(10K).  And in fact, vector</p><p>3 0.88375205 <a title="1153-lda-3" href="../high_scalability-2008/high_scalability-2008-11-11-Arhcitecture_for_content_management.html">440 high scalability-2008-11-11-Arhcitecture for content management</a></p>
<p>Introduction: Hi,   I am looking for logical architecture of content management of portal. Say an org has got lot of business process and integrates with few applicaitons and it is portal based application. How does it look to have architecture framework for this type of fucntionality.</p><p>4 0.86923021 <a title="1153-lda-4" href="../high_scalability-2014/high_scalability-2014-05-14-Google_Says_Cloud_Prices_Will_Follow_Moore%E2%80%99s_Law%3A_Are_We_All_Renters_Now%3F.html">1647 high scalability-2014-05-14-Google Says Cloud Prices Will Follow Moore’s Law: Are We All Renters Now?</a></p>
<p>Introduction: After Google  cut prices  on their Google Cloud Platform Amazon quickly followed with their own  price cuts . Even more interesting is what the future holds for pricing. The near future looks great. After that? We'll see.
 
Adrian Cockcroft highlights that Google thinks  prices should follow  Moore’s law, which means we should expect prices to halve every 18-24 months.
 
That's good news. Greater cost certainty means you can make much more aggressive build out plans. With the savings you can hire more people, handle more customers, and add those media rich features you thought you couldn't afford. Design is directly related to costs.
 
Without Google competing with Amazon there's little doubt the price reduction curve would be much less favorable.
 
As a late cloud entrant Google is now in a customer acquisition phase, so they are willing to pay for customers, which means lower prices are an acceptable cost of doing business. Profit and high margins are not the objective. Getting marke</p><p>5 0.86792672 <a title="1153-lda-5" href="../high_scalability-2011/high_scalability-2011-07-29-Stuff_The_Internet_Says_On_Scalability_For_July_29%2C_2011.html">1089 high scalability-2011-07-29-Stuff The Internet Says On Scalability For July 29, 2011</a></p>
<p>Introduction: Submitted for your end of July scaling pleasure: 
  
  YouTube : 3 billion videos viewed a day; 48 hours of footage uploaded every minute.  64 core Tilera chip . 
 Google wants to be your CDN. They figure the only way to make the web faster...is to host it.  Page Speed Service - Web Performance, Delivered . An eventually for pay service that caches your website and distributes it around the world. No cost information. Your speed may vary. See the longish list of  limitations . 
 Nobody said anything interesting on scalability this week! A disaster of non-quotable proportions. If I missed something, now is your chance.  
  Moving an Elephant: Large Scale Hadoop Data Migration at Facebook . Paul Yang describes the greatest westward expansion since the land bridge across the Bering Strait. It's a story of moving a 30PB Hadoop cluster from an over populated datacenter to the wide open spaces of a new continent. Unlike the early settlers, Facebook did not move the boxes over, that would dis</p><p>6 0.8666172 <a title="1153-lda-6" href="../high_scalability-2013/high_scalability-2013-05-20-The_Tumblr_Architecture_Yahoo_Bought_for_a_Cool_Billion_Dollars.html">1461 high scalability-2013-05-20-The Tumblr Architecture Yahoo Bought for a Cool Billion Dollars</a></p>
<p>7 0.86646479 <a title="1153-lda-7" href="../high_scalability-2012/high_scalability-2012-02-13-Tumblr_Architecture_-_15_Billion_Page_Views_a_Month_and_Harder_to_Scale_than_Twitter.html">1191 high scalability-2012-02-13-Tumblr Architecture - 15 Billion Page Views a Month and Harder to Scale than Twitter</a></p>
<p>8 0.86646467 <a title="1153-lda-8" href="../high_scalability-2012/high_scalability-2012-11-19-Gone_Fishin%27%3A_Tumblr_Architecture_-_15_Billion_Page_Views_A_Month_And_Harder_To_Scale_Than_Twitter.html">1360 high scalability-2012-11-19-Gone Fishin': Tumblr Architecture - 15 Billion Page Views A Month And Harder To Scale Than Twitter</a></p>
<p>9 0.86613739 <a title="1153-lda-9" href="../high_scalability-2011/high_scalability-2011-11-14-Using_Gossip_Protocols_for_Failure_Detection%2C_Monitoring%2C_Messaging_and_Other_Good_Things.html">1142 high scalability-2011-11-14-Using Gossip Protocols for Failure Detection, Monitoring, Messaging and Other Good Things</a></p>
<p>10 0.86000419 <a title="1153-lda-10" href="../high_scalability-2007/high_scalability-2007-11-12-Slashdot_Architecture_-_How_the_Old_Man_of_the_Internet_Learned_to_Scale.html">150 high scalability-2007-11-12-Slashdot Architecture - How the Old Man of the Internet Learned to Scale</a></p>
<p>11 0.85652483 <a title="1153-lda-11" href="../high_scalability-2012/high_scalability-2012-05-23-Averages%2C_web_performance_data%2C_and_how_your_analytics_product_is_lying_to_you__.html">1250 high scalability-2012-05-23-Averages, web performance data, and how your analytics product is lying to you  </a></p>
<p>12 0.85642403 <a title="1153-lda-12" href="../high_scalability-2012/high_scalability-2012-10-10-Antirez%3A_You_Need_to_Think_in_Terms_of_Organizing_Your_Data_for_Fetching.html">1337 high scalability-2012-10-10-Antirez: You Need to Think in Terms of Organizing Your Data for Fetching</a></p>
<p>13 0.85546571 <a title="1153-lda-13" href="../high_scalability-2011/high_scalability-2011-04-28-PaaS_on_OpenStack_-_Run_Applications_on_Any_Cloud%2C_Any_Time_Using_Any_Thing.html">1031 high scalability-2011-04-28-PaaS on OpenStack - Run Applications on Any Cloud, Any Time Using Any Thing</a></p>
<p>14 0.85532558 <a title="1153-lda-14" href="../high_scalability-2012/high_scalability-2012-05-09-Cell_Architectures.html">1242 high scalability-2012-05-09-Cell Architectures</a></p>
<p>15 0.85482085 <a title="1153-lda-15" href="../high_scalability-2010/high_scalability-2010-07-12-Creating_Scalable_Digital_Libraries.html">856 high scalability-2010-07-12-Creating Scalable Digital Libraries</a></p>
<p>16 0.85429937 <a title="1153-lda-16" href="../high_scalability-2013/high_scalability-2013-02-22-Stuff_The_Internet_Says_On_Scalability_For_February_22%2C_2013.html">1411 high scalability-2013-02-22-Stuff The Internet Says On Scalability For February 22, 2013</a></p>
<p>17 0.85400742 <a title="1153-lda-17" href="../high_scalability-2008/high_scalability-2008-09-10-Shard_servers_--_go_big_or_small%3F.html">383 high scalability-2008-09-10-Shard servers -- go big or small?</a></p>
<p>18 0.85307199 <a title="1153-lda-18" href="../high_scalability-2009/high_scalability-2009-06-10-Paper%3A_Graph_Databases_and_the_Future_of_Large-Scale_Knowledge_Management.html">626 high scalability-2009-06-10-Paper: Graph Databases and the Future of Large-Scale Knowledge Management</a></p>
<p>19 0.85185552 <a title="1153-lda-19" href="../high_scalability-2009/high_scalability-2009-08-05-Anti-RDBMS%3A_A_list_of_distributed_key-value_stores.html">670 high scalability-2009-08-05-Anti-RDBMS: A list of distributed key-value stores</a></p>
<p>20 0.84934258 <a title="1153-lda-20" href="../high_scalability-2010/high_scalability-2010-10-28-Notes_from_A_NOSQL_Evening_in_Palo_Alto_.html">931 high scalability-2010-10-28-Notes from A NOSQL Evening in Palo Alto </a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
