<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1083 high scalability-2011-07-20-Netflix: Harden Systems Using a Barrel of Problem Causing Monkeys - Latency, Conformity, Doctor, Janitor, Security, Internationalization, Chaos</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2011" href="../home/high_scalability-2011_home.html">high_scalability-2011</a> <a title="high_scalability-2011-1083" href="#">high_scalability-2011-1083</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1083 high scalability-2011-07-20-Netflix: Harden Systems Using a Barrel of Problem Causing Monkeys - Latency, Conformity, Doctor, Janitor, Security, Internationalization, Chaos</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2011-1083-html" href="http://highscalability.com//blog/2011/7/20/netflix-harden-systems-using-a-barrel-of-problem-causing-mon.html">html</a></p><p>Introduction: With a new Planet of the Apes coming out, this may be a touchy subject with
our new overlords, but Netflix is using a whole lot more trouble injecting
monkeys to test and iteratively harden their systems. We learned previously
how Netflix used Chaos Monkey, a tool to test failover handling by
continuously failing EC2 nodes. That was just a start. More monkeys have been
added to the barrel. Node failure is just one problem in a system. Imagine a
problem and you can imagine creating a monkey to test if your system is
handling that problem properly. Yury Izrailevsky talks about just this
approach in this very interesting post: The Netflix Simian Army.I know what
you are thinking, if monkeys are so great then why has Netflix been down
lately. Dmuino addressedthis potential embarrassment, putting all fears of
cloud inferiority to rest:Unfortunately we're not running 100% on the cloud
today. We're working on it, and we could use more help. The latest outage was
caused by a component that sti</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('monkeys', 0.65), ('monkey', 0.282), ('chaos', 0.155), ('netflix', 0.15), ('test', 0.126), ('cleanly', 0.113), ('trouble', 0.092), ('imagine', 0.087), ('health', 0.081), ('outage', 0.081), ('security', 0.079), ('fears', 0.077), ('embarrassment', 0.077), ('apes', 0.077), ('crisp', 0.077), ('failurethe', 0.077), ('harden', 0.077), ('improperly', 0.077), ('overlords', 0.077), ('simian', 0.077)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="1083-tfidf-1" href="../high_scalability-2011/high_scalability-2011-07-20-Netflix%3A_Harden_Systems_Using_a_Barrel_of_Problem_Causing_Monkeys_-_Latency%2C_Conformity%2C_Doctor%2C_Janitor%2C_Security%2C_Internationalization%2C_Chaos.html">1083 high scalability-2011-07-20-Netflix: Harden Systems Using a Barrel of Problem Causing Monkeys - Latency, Conformity, Doctor, Janitor, Security, Internationalization, Chaos</a></p>
<p>Introduction: With a new Planet of the Apes coming out, this may be a touchy subject with
our new overlords, but Netflix is using a whole lot more trouble injecting
monkeys to test and iteratively harden their systems. We learned previously
how Netflix used Chaos Monkey, a tool to test failover handling by
continuously failing EC2 nodes. That was just a start. More monkeys have been
added to the barrel. Node failure is just one problem in a system. Imagine a
problem and you can imagine creating a monkey to test if your system is
handling that problem properly. Yury Izrailevsky talks about just this
approach in this very interesting post: The Netflix Simian Army.I know what
you are thinking, if monkeys are so great then why has Netflix been down
lately. Dmuino addressedthis potential embarrassment, putting all fears of
cloud inferiority to rest:Unfortunately we're not running 100% on the cloud
today. We're working on it, and we could use more help. The latest outage was
caused by a component that sti</p><p>2 0.14535451 <a title="1083-tfidf-2" href="../high_scalability-2012/high_scalability-2012-07-18-Strategy%3A_Kill_Off_Multi-tenant_Instances_with_High_CPU_Stolen_Time.html">1286 high scalability-2012-07-18-Strategy: Kill Off Multi-tenant Instances with High CPU Stolen Time</a></p>
<p>Introduction: Are all instances created equal? Perhaps because under multi-tenancy multiple
virtual machines run on the same physical host, not all applications will run
equally well on every instance. In that case it makes sense to measure and
move to a better performing instance. That's the interesting idea
from@botchagalupe:Imagine something like a "performance monkey" where an
infrastructure is so bound that it can kill lower performing instances
automatically.@adriancosays Netflix has throught of doing the same: We've
looked at killing off multi-tenant instances that have high CPU stolen
time...Related ArticlesHost server CPU utilization in Amazon EC2 cloud</p><p>3 0.094386667 <a title="1083-tfidf-3" href="../high_scalability-2012/high_scalability-2012-05-07-Startups_are_Creating_a_New_System_of_the_World_for_IT.html">1240 high scalability-2012-05-07-Startups are Creating a New System of the World for IT</a></p>
<p>Introduction: It remains that, from the same principles, I now demonstrate the frame of the
System of the World.-- Isaac NewtonThe practice of IT reminds me a lot of the
practice of science before Isaac Newton. Aristotelianism was dead, but there
was nothing to replace it. Then Newton came along, created a scientific
revolution with hisSystem of the World. And everything changed. That was New
System of the World number one.New System of the World number two was written
about by the incomparable Neal Stephenson in his incredible Baroque Cycle
series. It explores the singular creation of a new way of organizing society
grounded in new modes of thought in business, religion, politics, and science.
Our modern world emerged Enlightened as it could from this roiling cauldron of
forces.In IT we may have had a Leonardo da Vinci or even a Galileo, but we've
never had our Newton. Maybe we don't need a towering genius to make everything
clear? For years startups, like the frenetically inventive age of the 17th</p><p>4 0.090760887 <a title="1083-tfidf-4" href="../high_scalability-2013/high_scalability-2013-01-04-Stuff_The_Internet_Says_On_Scalability_For_January_4%2C_2013.html">1381 high scalability-2013-01-04-Stuff The Internet Says On Scalability For January 4, 2013</a></p>
<p>Introduction: It's HighScalability time:37 Billion:Reddit Pageviews; $3 Billion:Alibaba
Sales in a Day; 100 Million:IronMQ Message a Day@tomasdev: "Divide and Conquer
- The Scalability Technique" seems like divide and conquer is the solution for
everything.Innovation is all around you. To see it merely open your eyes as
much as you've closed your heart.Summary of the December 24, 2012 Amazon ELB
Service Event in the US-East Region. A developer accidently caused some ELB
data to be deleted. It's interesting to consider thisincident in the context
of resiliencyand DevOps. Developers working on production, considered good, we
should be full stack, models should be open, etc. Yet Amazon is responding by
closing down production systems, just like the bad old days. Couldn't we have
our cake and eat it too if the data was backed up and could have been restored
quickly? Also,Amazon's EC2 Outage: A Closer LookNetflixtakes a closer look at
the Grinch who stole Christmas eve video: devices streaming when the o</p><p>5 0.085774347 <a title="1083-tfidf-5" href="../high_scalability-2010/high_scalability-2010-09-30-Facebook_and_Site_Failures_Caused_by_Complex%2C_Weakly_Interacting%2C_Layered_Systems.html">910 high scalability-2010-09-30-Facebook and Site Failures Caused by Complex, Weakly Interacting, Layered Systems</a></p>
<p>Introduction: Facebook has been so reliable that when a site outage does occur it's a
definite learning opportunity. Fortunately for us we can learn something
because in More Details on Today's Outage, Facebook'sRobert Johnson gave a
pretty candid explanation of what caused a rare 2.5 hour period of down time
for Facebook. It wasn't a simple problem. The root causes were feedback loops
and transient spikes caused ultimately by the complexity of weakly interacting
layers in modern systems. You know, the kind everyone is building these days.
Problems like this are notoriously hard to fix and finding a real solution may
send Facebook back to the whiteboard. There's a technical debt that must be
paid. The outline and my interpretation (reading between the lines) of what
happened is:Remember that Facebookcaches everything. They
have28terabytesofmemcacheddata on 800 servers. The database is the system of
record, but memory is where the action is. So when a problem happens that
involves the caching layer,</p><p>6 0.083624169 <a title="1083-tfidf-6" href="../high_scalability-2013/high_scalability-2013-04-12-Stuff_The_Internet_Says_On_Scalability_For_April_12%2C_2013.html">1439 high scalability-2013-04-12-Stuff The Internet Says On Scalability For April 12, 2013</a></p>
<p>7 0.083348833 <a title="1083-tfidf-7" href="../high_scalability-2010/high_scalability-2010-10-15-Troubles_with_Sharding_-_What_can_we_learn_from_the_Foursquare_Incident%3F.html">920 high scalability-2010-10-15-Troubles with Sharding - What can we learn from the Foursquare Incident?</a></p>
<p>8 0.083264843 <a title="1083-tfidf-8" href="../high_scalability-2011/high_scalability-2011-05-02-The_Updated_Big_List_of_Articles_on_the_Amazon_Outage.html">1033 high scalability-2011-05-02-The Updated Big List of Articles on the Amazon Outage</a></p>
<p>9 0.082539439 <a title="1083-tfidf-9" href="../high_scalability-2011/high_scalability-2011-06-29-Second_Hand_Seizure_%3A_A_New_Cause_of_Site_Death.html">1070 high scalability-2011-06-29-Second Hand Seizure : A New Cause of Site Death</a></p>
<p>10 0.081090353 <a title="1083-tfidf-10" href="../high_scalability-2013/high_scalability-2013-04-10-Check_Yourself_Before_You_Wreck_Yourself_-_Avocado%27s_5_Early_Stages_of_Architecture_Evolution.html">1438 high scalability-2013-04-10-Check Yourself Before You Wreck Yourself - Avocado's 5 Early Stages of Architecture Evolution</a></p>
<p>11 0.080295816 <a title="1083-tfidf-11" href="../high_scalability-2011/high_scalability-2011-12-12-Netflix%3A_Developing%2C_Deploying%2C_and_Supporting_Software_According_to_the_Way_of_the_Cloud.html">1155 high scalability-2011-12-12-Netflix: Developing, Deploying, and Supporting Software According to the Way of the Cloud</a></p>
<p>12 0.077983022 <a title="1083-tfidf-12" href="../high_scalability-2010/high_scalability-2010-12-28-Netflix%3A_Continually_Test_by_Failing_Servers_with_Chaos_Monkey.html">964 high scalability-2010-12-28-Netflix: Continually Test by Failing Servers with Chaos Monkey</a></p>
<p>13 0.077767469 <a title="1083-tfidf-13" href="../high_scalability-2010/high_scalability-2010-03-05-Strategy%3A_Planning_for_a_Power_Outage_Google_Style.html">789 high scalability-2010-03-05-Strategy: Planning for a Power Outage Google Style</a></p>
<p>14 0.077671133 <a title="1083-tfidf-14" href="../high_scalability-2011/high_scalability-2011-04-25-The_Big_List_of_Articles_on_the_Amazon_Outage.html">1029 high scalability-2011-04-25-The Big List of Articles on the Amazon Outage</a></p>
<p>15 0.075276218 <a title="1083-tfidf-15" href="../high_scalability-2012/high_scalability-2012-12-31-Designing_for_Resiliency_will_be_so_2013.html">1379 high scalability-2012-12-31-Designing for Resiliency will be so 2013</a></p>
<p>16 0.074798241 <a title="1083-tfidf-16" href="../high_scalability-2014/high_scalability-2014-03-03-The_%E2%80%9CFour_Hamiltons%E2%80%9D_Framework_for_Mitigating_Faults_in_the_Cloud%3A_Avoid_it%2C_Mask_it%2C_Bound_it%2C_Fix_it_Fast.html">1604 high scalability-2014-03-03-The “Four Hamiltons” Framework for Mitigating Faults in the Cloud: Avoid it, Mask it, Bound it, Fix it Fast</a></p>
<p>17 0.073442437 <a title="1083-tfidf-17" href="../high_scalability-2010/high_scalability-2010-07-08-Cloud_AWS_Infrastructure_vs._Physical_Infrastructure.html">853 high scalability-2010-07-08-Cloud AWS Infrastructure vs. Physical Infrastructure</a></p>
<p>18 0.071511865 <a title="1083-tfidf-18" href="../high_scalability-2014/high_scalability-2014-06-05-Cloud_Architecture_Revolution.html">1654 high scalability-2014-06-05-Cloud Architecture Revolution</a></p>
<p>19 0.069919609 <a title="1083-tfidf-19" href="../high_scalability-2012/high_scalability-2012-11-05-Gone_Fishin%27%3A_Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_In_The_Ambient_Cloud.html">1355 high scalability-2012-11-05-Gone Fishin': Building Super Scalable Systems: Blade Runner Meets Autonomic Computing In The Ambient Cloud</a></p>
<p>20 0.069864824 <a title="1083-tfidf-20" href="../high_scalability-2009/high_scalability-2009-12-16-Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_in_the_Ambient_Cloud.html">750 high scalability-2009-12-16-Building Super Scalable Systems: Blade Runner Meets Autonomic Computing in the Ambient Cloud</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.118), (1, 0.058), (2, -0.026), (3, 0.036), (4, -0.008), (5, -0.06), (6, 0.048), (7, -0.03), (8, 0.001), (9, -0.077), (10, -0.023), (11, 0.031), (12, 0.019), (13, -0.042), (14, 0.002), (15, -0.027), (16, 0.036), (17, 0.01), (18, -0.012), (19, 0.037), (20, 0.014), (21, -0.002), (22, 0.008), (23, -0.005), (24, -0.054), (25, 0.015), (26, -0.01), (27, 0.013), (28, 0.0), (29, 0.019), (30, -0.034), (31, 0.005), (32, 0.026), (33, -0.009), (34, 0.028), (35, 0.03), (36, -0.016), (37, 0.014), (38, 0.001), (39, 0.014), (40, -0.01), (41, -0.049), (42, 0.017), (43, -0.012), (44, -0.009), (45, -0.03), (46, 0.047), (47, 0.015), (48, -0.007), (49, 0.002)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96195489 <a title="1083-lsi-1" href="../high_scalability-2011/high_scalability-2011-07-20-Netflix%3A_Harden_Systems_Using_a_Barrel_of_Problem_Causing_Monkeys_-_Latency%2C_Conformity%2C_Doctor%2C_Janitor%2C_Security%2C_Internationalization%2C_Chaos.html">1083 high scalability-2011-07-20-Netflix: Harden Systems Using a Barrel of Problem Causing Monkeys - Latency, Conformity, Doctor, Janitor, Security, Internationalization, Chaos</a></p>
<p>Introduction: With a new Planet of the Apes coming out, this may be a touchy subject with
our new overlords, but Netflix is using a whole lot more trouble injecting
monkeys to test and iteratively harden their systems. We learned previously
how Netflix used Chaos Monkey, a tool to test failover handling by
continuously failing EC2 nodes. That was just a start. More monkeys have been
added to the barrel. Node failure is just one problem in a system. Imagine a
problem and you can imagine creating a monkey to test if your system is
handling that problem properly. Yury Izrailevsky talks about just this
approach in this very interesting post: The Netflix Simian Army.I know what
you are thinking, if monkeys are so great then why has Netflix been down
lately. Dmuino addressedthis potential embarrassment, putting all fears of
cloud inferiority to rest:Unfortunately we're not running 100% on the cloud
today. We're working on it, and we could use more help. The latest outage was
caused by a component that sti</p><p>2 0.80182523 <a title="1083-lsi-2" href="../high_scalability-2010/high_scalability-2010-12-28-Netflix%3A_Continually_Test_by_Failing_Servers_with_Chaos_Monkey.html">964 high scalability-2010-12-28-Netflix: Continually Test by Failing Servers with Chaos Monkey</a></p>
<p>Introduction: In 5 Lessons We've Learned Using AWS, Netflix's John Ciancutti says the best
way to avoid failure is to fail constantly. In the cloud it's expected
instances can fail at any time, so you always have to be prepared. In the real
world we prepare by running drills. Remember all those exciting fire drills?
It's not just fire drills of course. The military, football teams, fire
fighters, beach rescue, virtually any entity that must react quickly and
efficiently to disaster hones their responsiveness by running drills.Netflix
aggressively moves this strategy into the cloud by randomly failing servers
using a tool they built called Chaos Monkey. The idea is:If we aren't
constantly testing our ability to succeed despite failure, then it isn't
likely to work when it matters most - in the event of an unexpected
outage.They respond to failures bydegrading service, but they always
respond:If the recommendations system is down they'll show popular titles
instead.If the search system is slow then th</p><p>3 0.79931486 <a title="1083-lsi-3" href="../high_scalability-2012/high_scalability-2012-03-14-The_Azure_Outage%3A_Time_Is_a_SPOF%2C_Leap_Day_Doubly_So.html">1209 high scalability-2012-03-14-The Azure Outage: Time Is a SPOF, Leap Day Doubly So</a></p>
<p>Introduction: This is a guest post by Steve Newman, co-founder of Writely (Google Docs),
tech lead on the Paxos-based synchronous replication in Megastore, and founder
of cloud service providerScalyr.com.Microsoft's Azure service suffered a
widely publicized outage on February 28th / 29th. Microsoft recently published
an excellentpostmortem. For anyone trying to run a high-availability service,
this incident can teach several important lessons.The central lesson is that,
no matter how much work you put into redundancy, problems will arise. Murphy
is strong and, I might say, creative; things go wrong. So preventative
measures are important, but how you react to problems is just as important.
It's interesting to review the Azure incident in this light.The postmortem is
worth reading in its entirety, but here's a quick summary: each time Azure
launches a new VM, it creates a "transfer certificate" to secure
communications with that VM. There was a bug in the code that determines the
certificate expirat</p><p>4 0.74418724 <a title="1083-lsi-4" href="../high_scalability-2014/high_scalability-2014-03-03-The_%E2%80%9CFour_Hamiltons%E2%80%9D_Framework_for_Mitigating_Faults_in_the_Cloud%3A_Avoid_it%2C_Mask_it%2C_Bound_it%2C_Fix_it_Fast.html">1604 high scalability-2014-03-03-The “Four Hamiltons” Framework for Mitigating Faults in the Cloud: Avoid it, Mask it, Bound it, Fix it Fast</a></p>
<p>Introduction: This is a guest post byPatrick Eaton, Software Engineer and Distributed
Systems Architect at Stackdriver.Stackdriver provides intelligent monitoring-
as-a-service for cloud hosted applications.  Behind this easy-to-use service
is a large distributed system for collecting and storing metrics and events,
monitoring and alerting on them, analyzing them, and serving up all the
results in a web UI.  Because we ourselves run in the cloud (mostly on AWS),
we spend a lot of time thinking about how to deal with faults in the cloud.
We have developed a framework for thinking about fault mitigation for large,
cloud-hosted systems.  We endearingly call this framework the "Four Hamiltons"
because it is inspired by an article from James Hamilton, the Vice President
and Distinguished Engineer at Amazon Web Services.The article that led to this
framework is called "The Power Failure Seen Around the World".  Hamilton
analyzes the causes of the power outage that affected Super Bowl XLVII in
early 2013.</p><p>5 0.74014443 <a title="1083-lsi-5" href="../high_scalability-2011/high_scalability-2011-08-15-Should_any_cloud_be_considered_one_availability_zone%3F_The_Amazon_experience_says_yes..html">1098 high scalability-2011-08-15-Should any cloud be considered one availability zone? The Amazon experience says yes.</a></p>
<p>Introduction: Amazon has a very will written account of their 8/8/2011 downtime:Summary of
the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West
Region. Power failed, backup generators failed to kick in, there weren't
enough resources for EBS volumes to recover, API servers where overwhelmed, a
DNS failure caused failovers to alternate availability zones to fail, a double
fault occurred as the power event interrupted the repair of a different bug.
All kind of typical stuff that just seems to happen.Considering the previous
outage, the big question for programmers is: what does this mean? What does it
mean for how systems should be structured? Have we learned something that
can't be unlearned?The Amazon post has lots of good insights into how EBS and
RDS work, plus lessons learned. The short of the problem is large + complex =
high probability of failure. The immediate fixes are adding more resources,
more redundancy, more isolation between components, more automation, reduce
recove</p><p>6 0.72675276 <a title="1083-lsi-6" href="../high_scalability-2011/high_scalability-2011-04-27-Heroku_Emergency_Strategy%3A_Incident_Command_System_and_8_Hour_Ops_Rotations_for_Fresh_Minds.html">1030 high scalability-2011-04-27-Heroku Emergency Strategy: Incident Command System and 8 Hour Ops Rotations for Fresh Minds</a></p>
<p>7 0.69938678 <a title="1083-lsi-7" href="../high_scalability-2014/high_scalability-2014-04-08-Microservices_-_Not_a_free_lunch%21.html">1628 high scalability-2014-04-08-Microservices - Not a free lunch!</a></p>
<p>8 0.69650775 <a title="1083-lsi-8" href="../high_scalability-2012/high_scalability-2012-12-05-5_Ways_to_Make_Cloud_Failure_Not_an_Option.html">1367 high scalability-2012-12-05-5 Ways to Make Cloud Failure Not an Option</a></p>
<p>9 0.69395673 <a title="1083-lsi-9" href="../high_scalability-2011/high_scalability-2011-10-27-Strategy%3A_Survive_a_Comet_Strike_in_the_East_With_Reserved_Instances_in_the_West.html">1133 high scalability-2011-10-27-Strategy: Survive a Comet Strike in the East With Reserved Instances in the West</a></p>
<p>10 0.69141012 <a title="1083-lsi-10" href="../high_scalability-2013/high_scalability-2013-06-05-A_Simple_6_Step_Transition_Guide_for_Moving_Away_from_X_to_AWS_.html">1470 high scalability-2013-06-05-A Simple 6 Step Transition Guide for Moving Away from X to AWS </a></p>
<p>11 0.68987596 <a title="1083-lsi-11" href="../high_scalability-2013/high_scalability-2013-12-23-What_Happens_While_Your_Brain_Sleeps_is_Surprisingly_Like_How_Computers_Stay_Sane.html">1568 high scalability-2013-12-23-What Happens While Your Brain Sleeps is Surprisingly Like How Computers Stay Sane</a></p>
<p>12 0.68812507 <a title="1083-lsi-12" href="../high_scalability-2011/high_scalability-2011-12-12-Netflix%3A_Developing%2C_Deploying%2C_and_Supporting_Software_According_to_the_Way_of_the_Cloud.html">1155 high scalability-2011-12-12-Netflix: Developing, Deploying, and Supporting Software According to the Way of the Cloud</a></p>
<p>13 0.67853445 <a title="1083-lsi-13" href="../high_scalability-2014/high_scalability-2014-04-01-The_Mullet_Cloud_Selection_Pattern.html">1624 high scalability-2014-04-01-The Mullet Cloud Selection Pattern</a></p>
<p>14 0.6782499 <a title="1083-lsi-14" href="../high_scalability-2014/high_scalability-2014-04-14-How_do_you_even_do_anything_without_using_EBS%3F.html">1631 high scalability-2014-04-14-How do you even do anything without using EBS?</a></p>
<p>15 0.67070019 <a title="1083-lsi-15" href="../high_scalability-2008/high_scalability-2008-09-30-Scalability_Worst_Practices.html">398 high scalability-2008-09-30-Scalability Worst Practices</a></p>
<p>16 0.66525316 <a title="1083-lsi-16" href="../high_scalability-2010/high_scalability-2010-03-05-Strategy%3A_Planning_for_a_Power_Outage_Google_Style.html">789 high scalability-2010-03-05-Strategy: Planning for a Power Outage Google Style</a></p>
<p>17 0.65973949 <a title="1083-lsi-17" href="../high_scalability-2007/high_scalability-2007-09-18-Amazon_Architecture.html">96 high scalability-2007-09-18-Amazon Architecture</a></p>
<p>18 0.65834898 <a title="1083-lsi-18" href="../high_scalability-2007/high_scalability-2007-08-29-Skype_Failed_the_Boot_Scalability_Test%3A_Is_P2P_fundamentally_flawed%3F.html">76 high scalability-2007-08-29-Skype Failed the Boot Scalability Test: Is P2P fundamentally flawed?</a></p>
<p>19 0.65572667 <a title="1083-lsi-19" href="../high_scalability-2010/high_scalability-2010-04-28-Elasticity_for_the_Enterprise_--_Ensuring_Continuous_High_Availability_in_a_Disaster_Failure_Scenario.html">816 high scalability-2010-04-28-Elasticity for the Enterprise -- Ensuring Continuous High Availability in a Disaster Failure Scenario</a></p>
<p>20 0.65389538 <a title="1083-lsi-20" href="../high_scalability-2014/high_scalability-2014-01-13-NYTimes_Architecture%3A_No_Head%2C_No_Master%2C_No_Single_Point_of_Failure.html">1577 high scalability-2014-01-13-NYTimes Architecture: No Head, No Master, No Single Point of Failure</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.091), (2, 0.2), (5, 0.011), (10, 0.035), (11, 0.023), (16, 0.292), (25, 0.026), (30, 0.024), (61, 0.04), (77, 0.017), (79, 0.078), (85, 0.029), (94, 0.055)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.93013829 <a title="1083-lda-1" href="../high_scalability-2007/high_scalability-2007-10-03-Why_most_large-scale_Web_sites_are_not_written_in_Java.html">110 high scalability-2007-10-03-Why most large-scale Web sites are not written in Java</a></p>
<p>Introduction: Thereis alot ofinformation in the blogosphere describing the architecture of
many popular sites, such as Google, Amazon, eBay, LinkedIn, TypePad, WikiPedia
and others.I've summarized this issue in a blog posthereI would really
appreciate your opinion on this matter.</p><p>same-blog 2 0.83600068 <a title="1083-lda-2" href="../high_scalability-2011/high_scalability-2011-07-20-Netflix%3A_Harden_Systems_Using_a_Barrel_of_Problem_Causing_Monkeys_-_Latency%2C_Conformity%2C_Doctor%2C_Janitor%2C_Security%2C_Internationalization%2C_Chaos.html">1083 high scalability-2011-07-20-Netflix: Harden Systems Using a Barrel of Problem Causing Monkeys - Latency, Conformity, Doctor, Janitor, Security, Internationalization, Chaos</a></p>
<p>Introduction: With a new Planet of the Apes coming out, this may be a touchy subject with
our new overlords, but Netflix is using a whole lot more trouble injecting
monkeys to test and iteratively harden their systems. We learned previously
how Netflix used Chaos Monkey, a tool to test failover handling by
continuously failing EC2 nodes. That was just a start. More monkeys have been
added to the barrel. Node failure is just one problem in a system. Imagine a
problem and you can imagine creating a monkey to test if your system is
handling that problem properly. Yury Izrailevsky talks about just this
approach in this very interesting post: The Netflix Simian Army.I know what
you are thinking, if monkeys are so great then why has Netflix been down
lately. Dmuino addressedthis potential embarrassment, putting all fears of
cloud inferiority to rest:Unfortunately we're not running 100% on the cloud
today. We're working on it, and we could use more help. The latest outage was
caused by a component that sti</p><p>3 0.79587674 <a title="1083-lda-3" href="../high_scalability-2008/high_scalability-2008-09-23-Event%3A_CloudCamp_Silicon_Valley_Unconference_on_30th_September.html">388 high scalability-2008-09-23-Event: CloudCamp Silicon Valley Unconference on 30th September</a></p>
<p>Introduction: CloudCampis an interesting unconference where early adapters of Cloud
Computing technologies exchange ideas. With the rapid change occurring in the
industry, we need a place we can meet to share our experiences, challenges and
solutions. At CloudCamp, you are encouraged you to share your thoughts in
several open discussions, as we strive for the advancement of Cloud Computing.
End users, IT professionals and vendors are all encouraged to
participate.CloudCamp Silicon Valley 08is scheduled for Tuesday, September 30,
2008 from 06:00 PM - 10:00 PM in Sun Microsystems' EBC Briefing Center15
Network CircleMenlo Park, CA 94025CloudCamp follows an interactive, unscripted
unconference format. You can propose your own session or you can attend a
session proposed by someone else. Either way, you are encouraged to engage in
the discussion and “Vote with your feet”, which means … “find another session
if you don’t find the session helpful”. Pick and choose from the
conversations; rant and rave, or</p><p>4 0.78138524 <a title="1083-lda-4" href="../high_scalability-2014/high_scalability-2014-01-14-Ask_HS%3A_Design_and_Implementation_of_scalable_services%3F.html">1578 high scalability-2014-01-14-Ask HS: Design and Implementation of scalable services?</a></p>
<p>Introduction: We have written agents deployed/distributed across the network. Agents sends
data every 15 Secs may be even 5 secs. Working on a service/system to which
all agent can post data/tuples with marginal payload. Upto 5% drop rate is
acceptable. Ultimately the data will be segregated and stored into DBMS System
(currently we are using MSQL).Question(s) I am looking for answer1.
Client/Server Communication: Agent(s) can post data. Status of sending data is
not that important. But there is a remote where Agent(s) to be notified if the
server side system generates an event based on the data sent.- Lot of advices
from internet suggests using Message Bus (ActiveMQ) for async communication.
Multicast and UDP are the alternatives.2. Persistence: After some evaluation
data to be stored in DBMS System.- End of processing data is an aggregated
record for which MySql looks scalable. But on the volume of data is
exponential. Considering HBase as an option.Looking if there are any
alternatives for above</p><p>5 0.76497817 <a title="1083-lda-5" href="../high_scalability-2013/high_scalability-2013-12-04-How_Can_Batching_Requests_Actually_Reduce_Latency%3F.html">1558 high scalability-2013-12-04-How Can Batching Requests Actually Reduce Latency?</a></p>
<p>Introduction: Jeremy Edberg gave a talk on Scaling Reddit from 1 Million to 1 Billion-
Pitfalls and Lessons andone of the issuesthey had was that they:Did not
account for increased latency after moving to EC2. In the datacenter they had
submillisecond access between machines so it was possible to make a 1000 calls
to memache for one page load. Not so on EC2. Memcache access times increased
10x to a millisecond which made their old approach unusable. Fix was to batch
calls to memcache so a large number of gets are in one request.Dave Pacheco
had aninteresting questionabout batching requests and its impact on latency:I
was confused about the memcached problem after moving to the cloud. I
understand why network latency may have gone from submillisecond to
milliseconds, but how could you improve latency by batching requests?
Shouldn't that improve efficiency, not latency, at the possible expense of
latency (since some requests will wait on the client as they get batched)?
Jeremy cleared it up by saying:</p><p>6 0.76373273 <a title="1083-lda-6" href="../high_scalability-2014/high_scalability-2014-04-30-10_Tips_for_Optimizing_NGINX_and_PHP-fpm_for_High_Traffic_Sites.html">1640 high scalability-2014-04-30-10 Tips for Optimizing NGINX and PHP-fpm for High Traffic Sites</a></p>
<p>7 0.76317507 <a title="1083-lda-7" href="../high_scalability-2011/high_scalability-2011-07-01-Stuff_The_Internet_Says_On_Scalability_For_July_1%2C_2011.html">1071 high scalability-2011-07-01-Stuff The Internet Says On Scalability For July 1, 2011</a></p>
<p>8 0.71672273 <a title="1083-lda-8" href="../high_scalability-2009/high_scalability-2009-01-05-Lessons_Learned_at_208K%3A_Towards_Debugging_Millions_of_Cores.html">484 high scalability-2009-01-05-Lessons Learned at 208K: Towards Debugging Millions of Cores</a></p>
<p>9 0.70134199 <a title="1083-lda-9" href="../high_scalability-2011/high_scalability-2011-03-14-Twitter_by_the_Numbers_-_460%2C000_New_Accounts_and_140_Million_Tweets_Per_Day.html">1004 high scalability-2011-03-14-Twitter by the Numbers - 460,000 New Accounts and 140 Million Tweets Per Day</a></p>
<p>10 0.69986856 <a title="1083-lda-10" href="../high_scalability-2010/high_scalability-2010-03-09-Applications_as_Virtual_States.html">790 high scalability-2010-03-09-Applications as Virtual States</a></p>
<p>11 0.68095392 <a title="1083-lda-11" href="../high_scalability-2014/high_scalability-2014-02-21-Stuff_The_Internet_Says_On_Scalability_For_February_21st%2C_2014.html">1600 high scalability-2014-02-21-Stuff The Internet Says On Scalability For February 21st, 2014</a></p>
<p>12 0.6473552 <a title="1083-lda-12" href="../high_scalability-2014/high_scalability-2014-05-21-9_Principles_of_High_Performance_Programs.html">1652 high scalability-2014-05-21-9 Principles of High Performance Programs</a></p>
<p>13 0.64139342 <a title="1083-lda-13" href="../high_scalability-2013/high_scalability-2013-06-19-Paper%3A_MegaPipe%3A_A_New_Programming_Interface_for_Scalable_Network_I-O.html">1478 high scalability-2013-06-19-Paper: MegaPipe: A New Programming Interface for Scalable Network I-O</a></p>
<p>14 0.63944584 <a title="1083-lda-14" href="../high_scalability-2014/high_scalability-2014-02-26-The_WhatsApp_Architecture_Facebook_Bought_For_%2419_Billion.html">1602 high scalability-2014-02-26-The WhatsApp Architecture Facebook Bought For $19 Billion</a></p>
<p>15 0.63775742 <a title="1083-lda-15" href="../high_scalability-2012/high_scalability-2012-05-14-DynamoDB_Talk_Notes_and_the_SSD_Hot_S3_Cold_Pattern.html">1245 high scalability-2012-05-14-DynamoDB Talk Notes and the SSD Hot S3 Cold Pattern</a></p>
<p>16 0.63706511 <a title="1083-lda-16" href="../high_scalability-2010/high_scalability-2010-11-09-Facebook_Uses_Non-Stored_Procedures_to_Update_Social_Graphs.html">936 high scalability-2010-11-09-Facebook Uses Non-Stored Procedures to Update Social Graphs</a></p>
<p>17 0.63691849 <a title="1083-lda-17" href="../high_scalability-2008/high_scalability-2008-03-12-YouTube_Architecture.html">274 high scalability-2008-03-12-YouTube Architecture</a></p>
<p>18 0.63666844 <a title="1083-lda-18" href="../high_scalability-2014/high_scalability-2014-04-25-Stuff_The_Internet_Says_On_Scalability_For_April_25th%2C_2014.html">1637 high scalability-2014-04-25-Stuff The Internet Says On Scalability For April 25th, 2014</a></p>
<p>19 0.63638192 <a title="1083-lda-19" href="../high_scalability-2013/high_scalability-2013-08-30-Stuff_The_Internet_Says_On_Scalability_For_August_30%2C_2013.html">1509 high scalability-2013-08-30-Stuff The Internet Says On Scalability For August 30, 2013</a></p>
<p>20 0.63613087 <a title="1083-lda-20" href="../high_scalability-2010/high_scalability-2010-08-02-7_Scaling_Strategies_Facebook_Used_to_Grow_to_500_Million_Users.html">870 high scalability-2010-08-02-7 Scaling Strategies Facebook Used to Grow to 500 Million Users</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
