<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1142 high scalability-2011-11-14-Using Gossip Protocols for Failure Detection, Monitoring, Messaging and Other Good Things</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2011" href="../home/high_scalability-2011_home.html">high_scalability-2011</a> <a title="high_scalability-2011-1142" href="#">high_scalability-2011-1142</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1142 high scalability-2011-11-14-Using Gossip Protocols for Failure Detection, Monitoring, Messaging and Other Good Things</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2011-1142-html" href="http://highscalability.com//blog/2011/11/14/using-gossip-protocols-for-failure-detection-monitoring-mess.html">html</a></p><p>Introduction: When building a system on top of a set of wildly uncooperative and unruly computers you have knowledge problems: knowing when other nodes are dead; knowing when nodes become alive; getting information about other nodes so you can make local decisions, like knowing which node should handle a request based on a scheme for assigning nodes to a certain range of users; learning about new configuration data; agreeing on data values; and so on.
 
How do you solve these problems? 
 
A common centralized approach is to use a database and all nodes query it for information. Obvious availability and performance issues for large distributed clusters. Another approach is to use  Paxos , a protocol for solving consensus in a network to maintain strict consistency requirements for small groups of unreliable processes. Not practical when larger number of nodes are involved.
 
So what's the super cool decentralized way to bring order to large clusters?
 
 Gossip protocols , which maintain relaxed consi</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Data propagates through the system node by node like a virus. [sent-11, score-0.882]
</p><p>2 Cassandra, for example, uses what's called an  anti-entropy version  of the gossip protocol for repairing unread data using  Merkle Trees . [sent-17, score-0.761]
</p><p>3 Riak uses a  gossip protocol  to  share and communicate ring state and bucket properties around the cluster . [sent-18, score-0.735]
</p><p>4 For a detailed look at using gossip protocols take a look at  GEMS: Gossip-Enabled Monitoring Service for Scalable Heterogeneous Distributed Systems  by Rajagopal Subramaniyan, Pirabhu Raman, Alan George, and Matthew Radlinski. [sent-19, score-0.699]
</p><p>5 I really like this paper because of how marvelously well written and clear it is on how to use gossip protocols to detect node failures and load balance based on data sampled from other other nodes. [sent-20, score-1.162]
</p><p>6 By combining the reachability data from a lot of different nodes you can quickly determine when a node is down. [sent-28, score-0.746]
</p><p>7 It's not enough to simply say because your node can't contact another node that the other node is down. [sent-31, score-1.14]
</p><p>8 It's quite possible that your node is broken and the other node is fine. [sent-32, score-0.76]
</p><p>9 But if other nodes in the system also see that other node is dead then you can with some confidence conclude that that node is dead. [sent-33, score-1.071]
</p><p>10 Via a gossip protocol exchanging this kind of reachability data. [sent-36, score-0.839]
</p><p>11 In embedded systems the backplane often has traces between nodes so a local system can get an independent source of confirmation that a given node is dead, or alive, or transitioning between the two states. [sent-37, score-0.737]
</p><p>12 The paper covers the obvious issue of scaling as the number of nodes increases by dividing nodes into groups and introducing a hierarchy of layers at which node information is aggregated. [sent-39, score-1.001]
</p><p>13 They found running the gossip protocol used less than 60 Kbps of bandwidth and less than 2% of CPU for a system of 128 nodes. [sent-40, score-0.716]
</p><p>14 One thing I would add is the communication subsystem can also contribute what it learns about reachability, we don't just have to rely on a gossip heartbeat. [sent-41, score-0.718]
</p><p>15 If the communication layer can't reach a node that fact can be noted in a reachability table. [sent-42, score-0.617]
</p><p>16 Using Gossip as a Form of Messaging     In addition to failure detection, the paper shows how to transmit node and subsystem properties between nodes. [sent-44, score-0.805]
</p><p>17 This can scale as far as the gossip protocol can scale. [sent-54, score-0.671]
</p><p>18 What goes to another level is that they use an architecture I've used on several products, sending subsystem information so software modules on a node can send information to other modules on other nodes. [sent-55, score-1.019]
</p><p>19 Cook     Distributed Storage Systems  by Swaroop C H    The promise, and limitations, of gossip protocols  by    Ken Birman    Phi Accrual Failure Detection  by Naohiro Hayashibara, Xavier Défago, Rami Yared and Takuya Katayam. [sent-67, score-0.699]
</p><p>20 Accrual failure detection is based on two primary ideas: that failure detection should be flexible by being decoupled from the application being monitored, and that outputting a continuous level of "suspicion" regarding how confident the monitor is that a node has failed. [sent-68, score-0.914]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('gossip', 0.522), ('node', 0.38), ('nodes', 0.198), ('gems', 0.192), ('protocols', 0.177), ('reachability', 0.168), ('detection', 0.159), ('protocol', 0.149), ('gossiping', 0.134), ('subsystem', 0.127), ('local', 0.114), ('modules', 0.111), ('accrual', 0.089), ('renesse', 0.089), ('robbert', 0.089), ('failure', 0.083), ('paper', 0.083), ('information', 0.082), ('propagates', 0.077), ('reconciliation', 0.077), ('send', 0.076), ('sent', 0.074), ('monitoring', 0.069), ('communication', 0.069), ('transmit', 0.068), ('dead', 0.068), ('properties', 0.064), ('ruin', 0.064), ('knowing', 0.061), ('van', 0.061), ('obvious', 0.06), ('alive', 0.058), ('heterogeneous', 0.057), ('consensus', 0.056), ('distributed', 0.054), ('werner', 0.053), ('approach', 0.052), ('flow', 0.052), ('level', 0.05), ('requirements', 0.048), ('mark', 0.048), ('system', 0.045), ('scheme', 0.045), ('unruly', 0.045), ('huang', 0.045), ('repairing', 0.045), ('subtleties', 0.045), ('swaroop', 0.045), ('unread', 0.045), ('limitations', 0.044)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="1142-tfidf-1" href="../high_scalability-2011/high_scalability-2011-11-14-Using_Gossip_Protocols_for_Failure_Detection%2C_Monitoring%2C_Messaging_and_Other_Good_Things.html">1142 high scalability-2011-11-14-Using Gossip Protocols for Failure Detection, Monitoring, Messaging and Other Good Things</a></p>
<p>Introduction: When building a system on top of a set of wildly uncooperative and unruly computers you have knowledge problems: knowing when other nodes are dead; knowing when nodes become alive; getting information about other nodes so you can make local decisions, like knowing which node should handle a request based on a scheme for assigning nodes to a certain range of users; learning about new configuration data; agreeing on data values; and so on.
 
How do you solve these problems? 
 
A common centralized approach is to use a database and all nodes query it for information. Obvious availability and performance issues for large distributed clusters. Another approach is to use  Paxos , a protocol for solving consensus in a network to maintain strict consistency requirements for small groups of unreliable processes. Not practical when larger number of nodes are involved.
 
So what's the super cool decentralized way to bring order to large clusters?
 
 Gossip protocols , which maintain relaxed consi</p><p>2 0.16323878 <a title="1142-tfidf-2" href="../high_scalability-2013/high_scalability-2013-03-11-Low_Level_Scalability_Solutions_-_The_Conditioning_Collection.html">1421 high scalability-2013-03-11-Low Level Scalability Solutions - The Conditioning Collection</a></p>
<p>Introduction: We talked about  42 Monster Problems That Attack As Loads Increase . And in  The Aggregation Collection  we talked about the value of prioritizing work and making smart queues as a way of absorbing and not reflecting traffic spikes.
 
Now we move on to our next batch of strategies where the theme is  conditioning , which is the idea of shaping and controlling flows of work within your application...
  Use Resources Proportional To a Fixed Limit  
This is probably the most important rule for achieving scalability within an application. What it means:
  
 Find the resource that has a fixed limit that you know you can support. For example, a guarantee to handle a certain number of objects in memory. So if we always use resources proportional to the number of objects it is likely we can prevent resource exhaustion. 
 Devise ways of tying what you need to do to the individual resources. 
  
Some examples:
  
 Keep a list of purchase orders with line items over $20 (or whatever). Do not keep</p><p>3 0.1497926 <a title="1142-tfidf-3" href="../high_scalability-2012/high_scalability-2012-05-10-Paper%3A_Paxos_Made_Moderately_Complex.html">1243 high scalability-2012-05-10-Paper: Paxos Made Moderately Complex</a></p>
<p>Introduction: If you are a normal human being and find the  Paxos protocol  confusing, then this paper,  Paxos Made Moderately Complex , is a great find. Robbert van Renesse from Cornell University has written a clear and well written paper with excellent explanations.
 
The Abstract:
  For anybody who has ever tried to implement it, Paxos is by no means a simple protocol, even though it is based on relatively simple invariants. This paper provides imperative pseudo-code for the full Paxos (or Multi-Paxos) protocol without shying away from discussing various implementation details. The initial description avoids optimizations that complicate comprehension. Next we discuss liveness, and list various optimizations that make the protocol practical.   Related Articles   
  Paxos on HighScalability.com</p><p>4 0.14074074 <a title="1142-tfidf-4" href="../high_scalability-2010/high_scalability-2010-10-15-Troubles_with_Sharding_-_What_can_we_learn_from_the_Foursquare_Incident%3F.html">920 high scalability-2010-10-15-Troubles with Sharding - What can we learn from the Foursquare Incident?</a></p>
<p>Introduction: For everything given something seems to be taken. Caching is a great scalability solution, but caching also  comes with problems .  Sharding  is a great scalability solution, but as Foursquare recently revealed in a  post-mortem  about their 17 hours of downtime, sharding also has problems. MongoDB, the database Foursquare uses, also contributed their  post-mortem  of what went wrong too.
 
Now that everyone has shared and resharded, what can we learn to help us skip these mistakes and quickly move on to a different set of mistakes?
 
First, like for  Facebook , huge props to Foursquare and MongoDB for being upfront and honest about their problems. This helps everyone get better and is a sign we work in a pretty cool industry.
 
Second, overall, the fault didn't flow from evil hearts or gross negligence. As usual the cause was more mundane: a key system, that could be a little more robust, combined with a very popular application built by a small group of people, under immense pressure</p><p>5 0.13788822 <a title="1142-tfidf-5" href="../high_scalability-2013/high_scalability-2013-12-11-Using_Node.js_PayPal_Doubles_RPS%2C_Lowers_Latency%2C_with_Fewer_Developers%2C_but_Where_Do_the_Improvements_Really_Come_From%3F.html">1563 high scalability-2013-12-11-Using Node.js PayPal Doubles RPS, Lowers Latency, with Fewer Developers, but Where Do the Improvements Really Come From?</a></p>
<p>Introduction: PayPal gives yet  another glowing report  of an app rewritten in node.js experiencing substantial performance improvements. PayPal rewrote their account overview page, one of the most trafficked apps on the website, which was previously written in King Java.
 
The benefits:
  
 Full-stack engineers. Using JavaScript on both the front-end and the back-end removed an artificial boundary between the browser and server, allowing engineers to code both. 
  Built almost twice as fast with fewer people  
 Written in 33% fewer lines of code  
 Constructed with 40% fewer files 
 Double the requests per second vs. the Java application. 
 35% decrease in the average response time for the same page. 
  
A common pro Java response is an argument like clearly these people don't know how to program Java. Or rewriting an application usually makes it faster. Or the benchmark is faulty. And so on. Consider it noted. These are all potential factors.
 
 Baron Schwartz  from VividCortex has a different tak</p><p>6 0.13684462 <a title="1142-tfidf-6" href="../high_scalability-2009/high_scalability-2009-07-30-Learn_How_to_Think_at_Scale.html">666 high scalability-2009-07-30-Learn How to Think at Scale</a></p>
<p>7 0.1333138 <a title="1142-tfidf-7" href="../high_scalability-2009/high_scalability-2009-03-10-Paper%3A_Consensus_Protocols%3A_Paxos___.html">529 high scalability-2009-03-10-Paper: Consensus Protocols: Paxos   </a></p>
<p>8 0.13253689 <a title="1142-tfidf-8" href="../high_scalability-2012/high_scalability-2012-08-20-The_Performance_of_Distributed_Data-Structures_Running_on_a_%22Cache-Coherent%22_In-Memory_Data_Grid.html">1307 high scalability-2012-08-20-The Performance of Distributed Data-Structures Running on a "Cache-Coherent" In-Memory Data Grid</a></p>
<p>9 0.1305653 <a title="1142-tfidf-9" href="../high_scalability-2009/high_scalability-2009-09-16-Paper%3A_A_practical_scalable_distributed_B-tree.html">705 high scalability-2009-09-16-Paper: A practical scalable distributed B-tree</a></p>
<p>10 0.12791528 <a title="1142-tfidf-10" href="../high_scalability-2011/high_scalability-2011-04-12-Caching_and_Processing_2TB_Mozilla_Crash_Reports_in_memory_with_Hazelcast.html">1020 high scalability-2011-04-12-Caching and Processing 2TB Mozilla Crash Reports in memory with Hazelcast</a></p>
<p>11 0.12691534 <a title="1142-tfidf-11" href="../high_scalability-2010/high_scalability-2010-12-20-Netflix%3A_Use_Less_Chatty_Protocols_in_the_Cloud_-_Plus_26_Fixes.html">960 high scalability-2010-12-20-Netflix: Use Less Chatty Protocols in the Cloud - Plus 26 Fixes</a></p>
<p>12 0.12660278 <a title="1142-tfidf-12" href="../high_scalability-2014/high_scalability-2014-03-31-How_WhatsApp_Grew_to_Nearly_500_Million_Users%2C_11%2C000_cores%2C_and_70_Million_Messages_a_Second.html">1622 high scalability-2014-03-31-How WhatsApp Grew to Nearly 500 Million Users, 11,000 cores, and 70 Million Messages a Second</a></p>
<p>13 0.12503143 <a title="1142-tfidf-13" href="../high_scalability-2009/high_scalability-2009-05-05-Drop_ACID_and_Think_About_Data.html">589 high scalability-2009-05-05-Drop ACID and Think About Data</a></p>
<p>14 0.1247362 <a title="1142-tfidf-14" href="../high_scalability-2013/high_scalability-2013-02-27-42_Monster_Problems_that_Attack_as_Loads_Increase.html">1413 high scalability-2013-02-27-42 Monster Problems that Attack as Loads Increase</a></p>
<p>15 0.12211573 <a title="1142-tfidf-15" href="../high_scalability-2009/high_scalability-2009-07-02-Product%3A_Project_Voldemort_-_A_Distributed_Database.html">651 high scalability-2009-07-02-Product: Project Voldemort - A Distributed Database</a></p>
<p>16 0.11391845 <a title="1142-tfidf-16" href="../high_scalability-2012/high_scalability-2012-04-03-Hazelcast_2.0%3A_Big_Data_In-Memory.html">1221 high scalability-2012-04-03-Hazelcast 2.0: Big Data In-Memory</a></p>
<p>17 0.11256082 <a title="1142-tfidf-17" href="../high_scalability-2012/high_scalability-2012-11-05-Gone_Fishin%27%3A_Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_In_The_Ambient_Cloud.html">1355 high scalability-2012-11-05-Gone Fishin': Building Super Scalable Systems: Blade Runner Meets Autonomic Computing In The Ambient Cloud</a></p>
<p>18 0.11251395 <a title="1142-tfidf-18" href="../high_scalability-2009/high_scalability-2009-12-16-Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_in_the_Ambient_Cloud.html">750 high scalability-2009-12-16-Building Super Scalable Systems: Blade Runner Meets Autonomic Computing in the Ambient Cloud</a></p>
<p>19 0.11124487 <a title="1142-tfidf-19" href="../high_scalability-2009/high_scalability-2009-08-31-Squarespace_Architecture_-_A_Grid_Handles_Hundreds_of_Millions_of_Requests_a_Month_.html">691 high scalability-2009-08-31-Squarespace Architecture - A Grid Handles Hundreds of Millions of Requests a Month </a></p>
<p>20 0.11063499 <a title="1142-tfidf-20" href="../high_scalability-2011/high_scalability-2011-02-01-Google_Strategy%3A_Tree_Distribution_of_Requests_and_Responses.html">981 high scalability-2011-02-01-Google Strategy: Tree Distribution of Requests and Responses</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.188), (1, 0.082), (2, 0.008), (3, 0.05), (4, -0.016), (5, 0.059), (6, 0.094), (7, 0.014), (8, -0.064), (9, -0.005), (10, 0.019), (11, 0.066), (12, -0.021), (13, -0.077), (14, 0.045), (15, 0.033), (16, 0.036), (17, 0.008), (18, -0.004), (19, -0.03), (20, 0.028), (21, 0.069), (22, -0.033), (23, 0.046), (24, -0.05), (25, 0.002), (26, 0.096), (27, 0.036), (28, 0.011), (29, -0.041), (30, 0.019), (31, -0.017), (32, -0.022), (33, -0.018), (34, 0.017), (35, -0.015), (36, -0.03), (37, -0.041), (38, -0.023), (39, -0.0), (40, -0.017), (41, -0.046), (42, 0.024), (43, 0.028), (44, -0.006), (45, 0.074), (46, 0.014), (47, 0.051), (48, -0.056), (49, -0.008)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96056229 <a title="1142-lsi-1" href="../high_scalability-2011/high_scalability-2011-11-14-Using_Gossip_Protocols_for_Failure_Detection%2C_Monitoring%2C_Messaging_and_Other_Good_Things.html">1142 high scalability-2011-11-14-Using Gossip Protocols for Failure Detection, Monitoring, Messaging and Other Good Things</a></p>
<p>Introduction: When building a system on top of a set of wildly uncooperative and unruly computers you have knowledge problems: knowing when other nodes are dead; knowing when nodes become alive; getting information about other nodes so you can make local decisions, like knowing which node should handle a request based on a scheme for assigning nodes to a certain range of users; learning about new configuration data; agreeing on data values; and so on.
 
How do you solve these problems? 
 
A common centralized approach is to use a database and all nodes query it for information. Obvious availability and performance issues for large distributed clusters. Another approach is to use  Paxos , a protocol for solving consensus in a network to maintain strict consistency requirements for small groups of unreliable processes. Not practical when larger number of nodes are involved.
 
So what's the super cool decentralized way to bring order to large clusters?
 
 Gossip protocols , which maintain relaxed consi</p><p>2 0.86048186 <a title="1142-lsi-2" href="../high_scalability-2009/high_scalability-2009-03-10-Paper%3A_Consensus_Protocols%3A_Paxos___.html">529 high scalability-2009-03-10-Paper: Consensus Protocols: Paxos   </a></p>
<p>Introduction: Update:   Barbara Liskov’s Turing Award, and Byzantine Fault Tolerance .  Henry Robinson has created an excellent series of articles on consensus protocols. We already covered his  2 Phase Commit  article and he also has a  3 Phase Commit  article showing how to handle 2PC under single node failures.  But that is not enough!  3PC works well under node failures, but fails for network failures. So another consensus mechanism is needed that handles both network and node failures. And that's  Paxos .  Paxos correctly handles both types of failures, but it does this by becoming inaccessible if too many components fail. This is the "liveness" property of protocols. Paxos waits until the faults are fixed. Read queries can be handled, but updates will be blocked until the protocol thinks it can make forward progress.   The liveness of Paxos is primarily dependent on network stability. In a distributed heterogeneous environment you are at risk of losing the ability to make updates. Users hate t</p><p>3 0.80022252 <a title="1142-lsi-3" href="../high_scalability-2009/high_scalability-2009-09-16-Paper%3A_A_practical_scalable_distributed_B-tree.html">705 high scalability-2009-09-16-Paper: A practical scalable distributed B-tree</a></p>
<p>Introduction: We've seen a lot of  NoSQL  action lately built around distributed hash tables. Btrees are getting jealous. Btrees, once the king of the database world, want their throne back.  Paul Buchheit  surfaced a paper:  A practical scalable distributed B-tree  by Marcos K. Aguilera and Wojciech Golab, that might help spark a revolution.  From the Abstract:
   We propose a new algorithm for a practical, fault tolerant, and scalable B-tree distributed over a set of servers. Our algorithm supports practical features not present in prior work: transactions that allow atomic execution of multiple operations over multiple B-trees, online migration of B-tree nodes between servers, and dynamic addition and removal of servers. Moreover, our algorithm is conceptually simple: we use transactions to manipulate B-tree nodes so that clients need not use complicated concurrency and locking protocols used in prior work. To execute these transactions quickly, we rely on three techniques: (1) We use optimistic</p><p>4 0.77657092 <a title="1142-lsi-4" href="../high_scalability-2009/high_scalability-2009-02-09-Paper%3A_Consensus_Protocols%3A_Two-Phase_Commit__.html">510 high scalability-2009-02-09-Paper: Consensus Protocols: Two-Phase Commit  </a></p>
<p>Introduction: Henry Robinson has created an excellent series of articles on consensus protocols. Henry starts with a very useful discussion of what all this talk about consensus really means:  The consensus problem is the problem of getting a set of nodes in a distributed system to agree on something - it might be a value, a course of action or a decision. Achieving consensus allows a distributed system to act as a single entity, with every individual node aware of and in agreement with the actions of the whole of the network.   In this article Henry tackles Two-Phase Commit, the protocol most databases use to arrive at a consensus for database writes. The article is very well written with lots of pretty and informative pictures. He did a really good job.  In conclusion we learn 2PC is very efficient, a minimal number of messages are exchanged and latency is low. The problem is when a co-ordinator fails availability is dramatically reduced. This is why 2PC isn't generally used on highly distributed</p><p>5 0.76090729 <a title="1142-lsi-5" href="../high_scalability-2009/high_scalability-2009-12-30-Terrastore_-_Scalable%2C_elastic%2C_consistent_document_store..html">756 high scalability-2009-12-30-Terrastore - Scalable, elastic, consistent document store.</a></p>
<p>Introduction: Terrastore  is a new-born document store which provides advanced scalability and elasticity features without sacrificing consistency.   Here are a few highlights:
  
 Ubiquitous: based on the universally supported HTTP protocol.  
 Distributed: nodes can run and live everywhere on your network.  
 Elastic: you can add and remove nodes dynamically to/from your running cluster with no downtime and no changes at all to your configuration.  
 Scalable at the data layer: documents are partitioned and distributed among your nodes, with automatic and transparent re-balancing when nodes join and leave.  
 Scalable at the computational layer: query and update operations are distributed to the nodes which actually holds the queried/updated data, minimizing network traffic and spreading computational load.  
 Consistent: providing per-document consistency, you're guaranteed to always get the latest value of a single document, with read committed isolation for concurrent modifications. 
 Schemales</p><p>6 0.7433036 <a title="1142-lsi-6" href="../high_scalability-2008/high_scalability-2008-03-17-Paper%3A_Consistent_Hashing_and_Random_Trees%3A_Distributed_Caching_Protocols_for_Relieving_Hot_Spots_on_the_World_Wide_Web.html">280 high scalability-2008-03-17-Paper: Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web</a></p>
<p>7 0.73349375 <a title="1142-lsi-7" href="../high_scalability-2008/high_scalability-2008-07-15-ZooKeeper_-_A_Reliable%2C_Scalable_Distributed_Coordination_System_.html">350 high scalability-2008-07-15-ZooKeeper - A Reliable, Scalable Distributed Coordination System </a></p>
<p>8 0.73028904 <a title="1142-lsi-8" href="../high_scalability-2012/high_scalability-2012-08-20-The_Performance_of_Distributed_Data-Structures_Running_on_a_%22Cache-Coherent%22_In-Memory_Data_Grid.html">1307 high scalability-2012-08-20-The Performance of Distributed Data-Structures Running on a "Cache-Coherent" In-Memory Data Grid</a></p>
<p>9 0.72988427 <a title="1142-lsi-9" href="../high_scalability-2009/high_scalability-2009-07-02-Product%3A_Project_Voldemort_-_A_Distributed_Database.html">651 high scalability-2009-07-02-Product: Project Voldemort - A Distributed Database</a></p>
<p>10 0.71470535 <a title="1142-lsi-10" href="../high_scalability-2010/high_scalability-2010-09-01-Paper%3A_The_Case_for_Determinism_in_Database_Systems__.html">890 high scalability-2010-09-01-Paper: The Case for Determinism in Database Systems  </a></p>
<p>11 0.7143324 <a title="1142-lsi-11" href="../high_scalability-2008/high_scalability-2008-03-08-Product%3A_DRBD_-_Distributed_Replicated_Block_Device.html">271 high scalability-2008-03-08-Product: DRBD - Distributed Replicated Block Device</a></p>
<p>12 0.71428341 <a title="1142-lsi-12" href="../high_scalability-2011/high_scalability-2011-11-23-Paper%3A_Don%E2%80%99t_Settle_for_Eventual%3A_Scalable_Causal_Consistency_for_Wide-Area_Storage_with_COPS.html">1146 high scalability-2011-11-23-Paper: Don’t Settle for Eventual: Scalable Causal Consistency for Wide-Area Storage with COPS</a></p>
<p>13 0.71099025 <a title="1142-lsi-13" href="../high_scalability-2009/high_scalability-2009-08-08-Yahoo%21%27s_PNUTS_Database%3A_Too_Hot%2C_Too_Cold_or_Just_Right%3F.html">676 high scalability-2009-08-08-Yahoo!'s PNUTS Database: Too Hot, Too Cold or Just Right?</a></p>
<p>14 0.70831859 <a title="1142-lsi-14" href="../high_scalability-2014/high_scalability-2014-03-12-Paper%3A_Scalable_Eventually_Consistent_Counters_over_Unreliable_Networks.html">1611 high scalability-2014-03-12-Paper: Scalable Eventually Consistent Counters over Unreliable Networks</a></p>
<p>15 0.70767659 <a title="1142-lsi-15" href="../high_scalability-2014/high_scalability-2014-03-31-How_WhatsApp_Grew_to_Nearly_500_Million_Users%2C_11%2C000_cores%2C_and_70_Million_Messages_a_Second.html">1622 high scalability-2014-03-31-How WhatsApp Grew to Nearly 500 Million Users, 11,000 cores, and 70 Million Messages a Second</a></p>
<p>16 0.70575607 <a title="1142-lsi-16" href="../high_scalability-2009/high_scalability-2009-02-03-Paper%3A_Optimistic_Replication.html">507 high scalability-2009-02-03-Paper: Optimistic Replication</a></p>
<p>17 0.69188452 <a title="1142-lsi-17" href="../high_scalability-2010/high_scalability-2010-12-23-Paper%3A_CRDTs%3A_Consistency_without_concurrency_control.html">963 high scalability-2010-12-23-Paper: CRDTs: Consistency without concurrency control</a></p>
<p>18 0.69066536 <a title="1142-lsi-18" href="../high_scalability-2011/high_scalability-2011-04-12-Caching_and_Processing_2TB_Mozilla_Crash_Reports_in_memory_with_Hazelcast.html">1020 high scalability-2011-04-12-Caching and Processing 2TB Mozilla Crash Reports in memory with Hazelcast</a></p>
<p>19 0.69030499 <a title="1142-lsi-19" href="../high_scalability-2008/high_scalability-2008-08-17-Wuala_-_P2P_Online_Storage_Cloud.html">368 high scalability-2008-08-17-Wuala - P2P Online Storage Cloud</a></p>
<p>20 0.6805923 <a title="1142-lsi-20" href="../high_scalability-2011/high_scalability-2011-01-27-Comet_-_An_Example_of_the_New_Key-Code_Databases.html">979 high scalability-2011-01-27-Comet - An Example of the New Key-Code Databases</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.081), (2, 0.184), (8, 0.019), (10, 0.041), (30, 0.024), (40, 0.02), (47, 0.011), (61, 0.336), (77, 0.013), (79, 0.109), (85, 0.023), (94, 0.048)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.98687351 <a title="1142-lda-1" href="../high_scalability-2008/high_scalability-2008-07-07-Five_Ways_to_Stop_Framework_Fixation_from_Crashing_Your_Scaling_Strategy.html">347 high scalability-2008-07-07-Five Ways to Stop Framework Fixation from Crashing Your Scaling Strategy</a></p>
<p>Introduction: If you've wondered why I haven't been posting lately it's because I've been on an amazing  Beach's motorcycle tour  of the  Alps  ( and ,  and ,  and ,  and ,  and ,  and ,  and ,  and ). My wife (Linda) and I rode two-up on a BMW 1200 GS through the alps in Germany, Austria, Switzerland, Italy, Slovenia, and Lichtenstein.   The trip was more beautiful than I ever imagined. We rode challenging mountain pass after mountain pass, froze in the rain, baked in the heat, woke up on excellent Italian coffee, ate slice after slice of tasty apple strudel, drank dazzling local wines, smelled the fresh cut grass as the Swiss en masse cut hay for the winter feeding of their dairy cows, rode the amazing Munich train system, listened as cow bells tinkled like wind chimes throughout small valleys, drank water from a pure alpine spring on a blisteringly hot hike, watched local German folk dancers represent their regions, and had fun in the company of fellow riders. Magical.  They say you'll ride more</p><p>2 0.97881472 <a title="1142-lda-2" href="../high_scalability-2012/high_scalability-2012-07-20-Stuff_The_Internet_Says_On_Scalability_For_July_20%2C_2012.html">1287 high scalability-2012-07-20-Stuff The Internet Says On Scalability For July 20, 2012</a></p>
<p>Introduction: It's HighScalability Time:
  
 4 Trillion Objects:  Windows Azure Storage  
 Quotable Quotes:         
 
  @benjchristensen : “What if we could make the data dense and cheap instead of sparse and expensive?” James Gosling @liquidrinc 
  @sinetpd360 : People trying new things and sharing is what helps create scalability. Jim Rickabaugh #siis2012 
  @rbranson : This h1.4xlarge running 160GB PostgreSQL database pushing ~17,200 index scan rows/sec. r_await is 0.79ms, box is 92% idle. 
  @sturadnidge : faster net and disk greatly reduces repair time and impact so we can load up the instances with far more dat 
 
 
 With Amazon  announcing 2TB SSD instances  the age of SSD has almost arrived. Netflix has already  published a very thorough post  on the wonderfulness of SSD for both performance and taming the  long latency tail . They see  100K IOPS or 1GByte/sec  on a untuned system. Netflix projects: The hi1.4xlarge configuration is about half the system cost for the same throughput; The mea</p><p>3 0.97321546 <a title="1142-lda-3" href="../high_scalability-2010/high_scalability-2010-10-28-NoSQL_Took_Away_the_Relational_Model_and_Gave_Nothing_Back.html">930 high scalability-2010-10-28-NoSQL Took Away the Relational Model and Gave Nothing Back</a></p>
<p>Introduction: Update : Benjamin Black said he was the source of the quote and also said I was wrong about what he meant. His real point:  The meaning of the statement was that NoSQL systems (really the various map-reduce systems) are lacking a standard model for describing and querying and that developing one should be a high priority task for them. 
 
At the  A NoSQL Evening in Palo Alto , an audience member, sorry, I couldn't tell who, said something I found really interesting:  NoSQL took away the relational model and gave nothing back.  
 
  The idea being that   NoSQL has focussed on ease of use, scalability, performance, etc, but it has lost the idea of how data relates to other data. True to its name, the relational model is very good at capturing a managing relationships. With NoSQL all relationships have been pushed back onto the poor programmer to implement in code rather than the database managing it. We've sacrificed usability. NoSQL is about concurrency, latency, and scalability, but it</p><p>4 0.97163981 <a title="1142-lda-4" href="../high_scalability-2013/high_scalability-2013-02-22-Stuff_The_Internet_Says_On_Scalability_For_February_22%2C_2013.html">1411 high scalability-2013-02-22-Stuff The Internet Says On Scalability For February 22, 2013</a></p>
<p>Introduction: Hey, it's HighScalability time: 
  
 Quotable Quotes:                                    
 
  @p337er : I have committed some truly horrendous crimes against scalability today. 
  @ErrataRob : doubling performance doesn't double scalability. 
  @rsingel : In 2008 when Yahoo.com  linked out, I had a Wired story get 1M visitors in an hour from their homepage. 
  @philiph : Lets solve this scalability problem with a queuing system 
  @jaykreps : Transferring data across data centers? Read this page and go tune your TCP buffer sizes... 
  @gwestr : In which the node community showers schadenfreude upon the rails community for "scalability is not my problem" architectures 
  @pbailis :  Makes sense, though I think there's a tradeoff re: coordination and scalability (always homogeneous vs dynamically heterogenous) 
  @pembleton : To summarize Yoav's philosophy: we started as quick as we can and then we accelerated #operationgrandma in #reversim 
  @surfichris : “We chose Heroku because we be</p><p>5 0.9571355 <a title="1142-lda-5" href="../high_scalability-2010/high_scalability-2010-03-10-Saying_Yes_to_NoSQL%3B_Going_Steady_with_Cassandra_at_Digg.html">793 high scalability-2010-03-10-Saying Yes to NoSQL; Going Steady with Cassandra at Digg</a></p>
<p>Introduction: The last six months have been exciting for Digg's engineering team. We're working on a soup-to-nuts rewrite. Not only are we rewriting all our application code, but we're also rolling out a new client and server architecture. And if that doesn't sound like a big enough challenge, we're replacing most of our infrastructure components and moving away from LAMP.
 
Perhaps our most significant infrastructure change is abandoning MySQL in favor of a NoSQL alternative. To someone like me who's been building systems almost exclusively on relational databases for almost 20 years, this feels like a bold move.
  What's Wrong with MySQL?  
Our primary motivation for moving away from MySQL is the increasing difficulty of building a high performance, write intensive, application on a data set that is growing quickly, with no end in sight. This growth has forced us into horizontal and vertical partitioning strategies that have eliminated most of the value of a relational database, while still incurr</p><p>6 0.95417839 <a title="1142-lda-6" href="../high_scalability-2007/high_scalability-2007-12-05-Easier_Production_Releases_.html">173 high scalability-2007-12-05-Easier Production Releases </a></p>
<p>7 0.95155191 <a title="1142-lda-7" href="../high_scalability-2007/high_scalability-2007-11-12-Slashdot_Architecture_-_How_the_Old_Man_of_the_Internet_Learned_to_Scale.html">150 high scalability-2007-11-12-Slashdot Architecture - How the Old Man of the Internet Learned to Scale</a></p>
<p>8 0.94991523 <a title="1142-lda-8" href="../high_scalability-2012/high_scalability-2012-01-31-Performance_in_the_Cloud%3A_Business_Jitter_is_Bad.html">1184 high scalability-2012-01-31-Performance in the Cloud: Business Jitter is Bad</a></p>
<p>9 0.94983017 <a title="1142-lda-9" href="../high_scalability-2009/high_scalability-2009-08-08-1dbase_vs._many_and_cloud_hosting_vs._dedicated_server%28s%29%3F.html">675 high scalability-2009-08-08-1dbase vs. many and cloud hosting vs. dedicated server(s)?</a></p>
<p>same-blog 10 0.94614524 <a title="1142-lda-10" href="../high_scalability-2011/high_scalability-2011-11-14-Using_Gossip_Protocols_for_Failure_Detection%2C_Monitoring%2C_Messaging_and_Other_Good_Things.html">1142 high scalability-2011-11-14-Using Gossip Protocols for Failure Detection, Monitoring, Messaging and Other Good Things</a></p>
<p>11 0.94479078 <a title="1142-lda-11" href="../high_scalability-2008/high_scalability-2008-05-28-Job_queue_and_search_engine.html">332 high scalability-2008-05-28-Job queue and search engine</a></p>
<p>12 0.94364226 <a title="1142-lda-12" href="../high_scalability-2008/high_scalability-2008-01-11-FTP_Sanity%3A_Redundancy%2C_archiving%2C_consolidation..html">208 high scalability-2008-01-11-FTP Sanity: Redundancy, archiving, consolidation.</a></p>
<p>13 0.94334036 <a title="1142-lda-13" href="../high_scalability-2009/high_scalability-2009-01-25-Where_do_I_start%3F.html">501 high scalability-2009-01-25-Where do I start?</a></p>
<p>14 0.94226295 <a title="1142-lda-14" href="../high_scalability-2009/high_scalability-2009-11-26-Kngine_Snippet_Search_New_Indexing_Technology.html">746 high scalability-2009-11-26-Kngine Snippet Search New Indexing Technology</a></p>
<p>15 0.9409793 <a title="1142-lda-15" href="../high_scalability-2007/high_scalability-2007-11-05-Quick_question_about_efficiently_implementing_Facebook_%27news_feed%27_like_functionality.html">141 high scalability-2007-11-05-Quick question about efficiently implementing Facebook 'news feed' like functionality</a></p>
<p>16 0.93723184 <a title="1142-lda-16" href="../high_scalability-2009/high_scalability-2009-11-09-10_NoSQL_Systems_Reviewed.html">739 high scalability-2009-11-09-10 NoSQL Systems Reviewed</a></p>
<p>17 0.93511736 <a title="1142-lda-17" href="../high_scalability-2008/high_scalability-2008-05-19-Conference%3A_Infoscale_2008_in_Italy_%28June_4-6%29.html">322 high scalability-2008-05-19-Conference: Infoscale 2008 in Italy (June 4-6)</a></p>
<p>18 0.93393534 <a title="1142-lda-18" href="../high_scalability-2009/high_scalability-2009-01-16-Just-In-Time_Scalability%3A_Agile_Methods_to_Support_Massive_Growth_%28IMVU_case_study%29.html">493 high scalability-2009-01-16-Just-In-Time Scalability: Agile Methods to Support Massive Growth (IMVU case study)</a></p>
<p>19 0.93244821 <a title="1142-lda-19" href="../high_scalability-2009/high_scalability-2009-04-24-INFOSCALE_2009_in_June_in_Hong_Kong.html">580 high scalability-2009-04-24-INFOSCALE 2009 in June in Hong Kong</a></p>
<p>20 0.93198478 <a title="1142-lda-20" href="../high_scalability-2012/high_scalability-2012-08-13-Ask_HighScalability%3A_Facing_scaling_issues_with_news_feeds_on_Redis.__Any_advice%3F.html">1303 high scalability-2012-08-13-Ask HighScalability: Facing scaling issues with news feeds on Redis.  Any advice?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
