<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1165 high scalability-2011-12-28-Strategy: Guaranteed Availability Requires Reserving Instances in Specific Zones</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2011" href="../home/high_scalability-2011_home.html">high_scalability-2011</a> <a title="high_scalability-2011-1165" href="#">high_scalability-2011-1165</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1165 high scalability-2011-12-28-Strategy: Guaranteed Availability Requires Reserving Instances in Specific Zones</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2011-1165-html" href="http://highscalability.com//blog/2011/12/28/strategy-guaranteed-availability-requires-reserving-instance.html">html</a></p><p>Introduction: When EC2 first started the mental model was of a magic Pez dispenser supplying an infinite stream of instances in any desired flavor. If you needed an instance, because of a either a failure or traffic spike, it would be there. As amazing as EC2 is, this model turned out to be optimistic.  
 
From a  thread on the Amazon discussion forum  we learn any dispenser has limits:
  

As Availability Zones grow over time, our ability to continue to expand them can become constrained. In these scenarios, we will prevent customers from launching in the constrained zone if they do not yet have existing resources in that zone. We also might remove the constrained zone entirely from the list of options for new customers. This means that occasionally, different customers will see a different number of Availability Zones in a particular Region. Both approaches aim to help customers avoid accidentally starting to build up their infrastructure in an Availability Zone where they might have less ability</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 When EC2 first started the mental model was of a magic Pez dispenser supplying an infinite stream of instances in any desired flavor. [sent-1, score-1.082]
</p><p>2 As amazing as EC2 is, this model turned out to be optimistic. [sent-3, score-0.192]
</p><p>3 From a  thread on the Amazon discussion forum  we learn any dispenser has limits:     As Availability Zones grow over time, our ability to continue to expand them can become constrained. [sent-4, score-0.639]
</p><p>4 In these scenarios, we will prevent customers from launching in the constrained zone if they do not yet have existing resources in that zone. [sent-5, score-0.932]
</p><p>5 We also might remove the constrained zone entirely from the list of options for new customers. [sent-6, score-0.635]
</p><p>6 This means that occasionally, different customers will see a different number of Availability Zones in a particular Region. [sent-7, score-0.326]
</p><p>7 Both approaches aim to help customers avoid accidentally starting to build up their infrastructure in an Availability Zone where they might have less ability to expand. [sent-8, score-0.494]
</p><p>8 The solution : if you need guaranteed resources in different zones, purchase  Reserved Instances . [sent-9, score-0.064]
</p><p>9 There's no way to know if the instance types you are interested in are available in an availability zone, so reserving instances is the only solution. [sent-11, score-0.593]
</p><p>10 Architecturally this is a pain and removes part of the win of the cloud. [sent-12, score-0.231]
</p><p>11 Having nailed up instances is nearly one step from dedicated machines in a colo. [sent-13, score-0.447]
</p><p>12 A much more complicated scenario, but I guess you have to run out of Pez eventually. [sent-15, score-0.078]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('zone', 0.355), ('dispenser', 0.323), ('pez', 0.323), ('zones', 0.238), ('instances', 0.232), ('constrained', 0.205), ('reserved', 0.176), ('scenario', 0.147), ('athread', 0.146), ('availability', 0.143), ('nailed', 0.137), ('reserving', 0.131), ('customers', 0.131), ('architecturally', 0.126), ('assure', 0.126), ('accidentally', 0.122), ('supplying', 0.116), ('occasionally', 0.102), ('mental', 0.101), ('desired', 0.092), ('ability', 0.091), ('launching', 0.09), ('instance', 0.087), ('aim', 0.087), ('guaranteed', 0.087), ('removes', 0.084), ('forum', 0.083), ('spike', 0.083), ('infinite', 0.081), ('resources', 0.079), ('machines', 0.078), ('guess', 0.078), ('pain', 0.078), ('expand', 0.077), ('entirely', 0.075), ('disaster', 0.073), ('scenarios', 0.073), ('prevent', 0.072), ('unless', 0.072), ('count', 0.072), ('magic', 0.071), ('enough', 0.069), ('win', 0.069), ('means', 0.067), ('model', 0.066), ('continue', 0.065), ('different', 0.064), ('turned', 0.063), ('approaches', 0.063), ('amazing', 0.063)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="1165-tfidf-1" href="../high_scalability-2011/high_scalability-2011-12-28-Strategy%3A_Guaranteed_Availability_Requires_Reserving_Instances_in_Specific_Zones.html">1165 high scalability-2011-12-28-Strategy: Guaranteed Availability Requires Reserving Instances in Specific Zones</a></p>
<p>Introduction: When EC2 first started the mental model was of a magic Pez dispenser supplying an infinite stream of instances in any desired flavor. If you needed an instance, because of a either a failure or traffic spike, it would be there. As amazing as EC2 is, this model turned out to be optimistic.  
 
From a  thread on the Amazon discussion forum  we learn any dispenser has limits:
  

As Availability Zones grow over time, our ability to continue to expand them can become constrained. In these scenarios, we will prevent customers from launching in the constrained zone if they do not yet have existing resources in that zone. We also might remove the constrained zone entirely from the list of options for new customers. This means that occasionally, different customers will see a different number of Availability Zones in a particular Region. Both approaches aim to help customers avoid accidentally starting to build up their infrastructure in an Availability Zone where they might have less ability</p><p>2 0.23499638 <a title="1165-tfidf-2" href="../high_scalability-2014/high_scalability-2014-03-03-The_%E2%80%9CFour_Hamiltons%E2%80%9D_Framework_for_Mitigating_Faults_in_the_Cloud%3A_Avoid_it%2C_Mask_it%2C_Bound_it%2C_Fix_it_Fast.html">1604 high scalability-2014-03-03-The “Four Hamiltons” Framework for Mitigating Faults in the Cloud: Avoid it, Mask it, Bound it, Fix it Fast</a></p>
<p>Introduction: This is a guest post by  Patrick Eaton , Software Engineer and Distributed Systems Architect at Stackdriver. 
 
Stackdriver provides intelligent monitoring-as-a-service for cloud hosted applications.  Behind this easy-to-use service is a large distributed system for collecting and storing metrics and events, monitoring and alerting on them, analyzing them, and serving up all the results in a web UI.  Because we ourselves run in the cloud (mostly on AWS), we spend a lot of time thinking about how to deal with faults in the cloud.  We have developed a framework for thinking about fault mitigation for large, cloud-hosted systems.  We endearingly call this framework the “Four Hamiltons” because it is inspired by an article from James Hamilton, the Vice President and Distinguished Engineer at Amazon Web Services.
   The article that led to this framework is called “   The Power Failure Seen Around the World   ”  .  Hamilton analyzes the causes of the power outage that affected Super Bowl XL</p><p>3 0.1624478 <a title="1165-tfidf-3" href="../high_scalability-2008/high_scalability-2008-03-27-Amazon_Announces_Static_IP_Addresses_and_Multiple_Datacenter_Operation.html">289 high scalability-2008-03-27-Amazon Announces Static IP Addresses and Multiple Datacenter Operation</a></p>
<p>Introduction: Amazon is fixing two of their major problems: no static IP addresses and single datacenter operation. By adding these two new features developers can finally build a no apology system on Amazon. Before you always had to throw in an apology or two. No, we don't have low failover times because of the silly DNS games and unexceptionable DNS update and propagation times and no, we don't operate in more than one datacenter. No more. Now Amazon is adding   Elastic IP Addresses   and   Availability Zones  .     Elastic IP addresses are far better than normal IP addresses because they are both in tight with   Jessica Alba   and they are:        Static IP addresses designed for dynamic cloud computing. An Elastic IP address is associated with your account, not a particular instance, and you control that address until you choose to explicitly release it. Unlike traditional static IP addresses, however, Elastic IP addresses allow you to mask instance or availability zone failures by programmatica</p><p>4 0.15873457 <a title="1165-tfidf-4" href="../high_scalability-2012/high_scalability-2012-10-02-An_Epic_TripAdvisor_Update%3A_Why_Not_Run_on_the_Cloud%3F_The_Grand_Experiment..html">1331 high scalability-2012-10-02-An Epic TripAdvisor Update: Why Not Run on the Cloud? The Grand Experiment.</a></p>
<p>Introduction: This is a guest post by  Shawn Hsiao ,  Luke Massa , and  Victor Luu . Shawn runs  TripAdvisor ’s Technical Operations team, Luke and Victor interned on his team this past summer. This post is introduced by  Andy Gelfond , TripAdvisor’s head of engineering.   It's been a little over a year since our last post about the  TripAdvisor architecture . It has been an exciting year. Our business and team continues to grow, we are now an independent public company, and we have continued to keep/scale our development process and culture as we have grown - we still run dozens of independent teams, and each team continues to work across the entire stack. All that has changed are the numbers:
  
 56M visitors per month 
 350M+ pages requests a day 
 120TB+ of warehouse data running on a large Hadoop cluster, and quickly growing 
  
We also had a very successful college intern program that brought on over 60 interns this past summer, all who were quickly on boarded and doing the same kind of work a</p><p>5 0.14079243 <a title="1165-tfidf-5" href="../high_scalability-2010/high_scalability-2010-04-28-Elasticity_for_the_Enterprise_--_Ensuring_Continuous_High_Availability_in_a_Disaster_Failure_Scenario.html">816 high scalability-2010-04-28-Elasticity for the Enterprise -- Ensuring Continuous High Availability in a Disaster Failure Scenario</a></p>
<p>Introduction: Many enterprises' high-availability architecture is based on the assumption that you can prevent failure from happening by putting all your critical data in a centralized database, back it up with expensive storage, and replicate it somehow between the sites. As I argued in one of my previous posts ( Why Existing Databases (RAC) are So Breakable! ) many of those assumptions are broken at their core, as storage is doomed to failure just like any other device, expensive hardware doesn’t make things any better and database replication is often not enough.
 
One of the main lessons that we can take from the likes of Amazon and Google is that the right way to ensure continuous high availability is by designing our system to cope with failure. We need to assume that what we tend to think of as unthinkable will probably happen, as that’s the nature of failure. So rather than trying to prevent failures, we need to build a system that will tolerate them.
 
As we can learn from a  recent outage</p><p>6 0.14044482 <a title="1165-tfidf-6" href="../high_scalability-2010/high_scalability-2010-04-19-Strategy%3A_Order_Two_Mediums_Instead_of_Two_Smalls_and_the_EC2_Buffet.html">812 high scalability-2010-04-19-Strategy: Order Two Mediums Instead of Two Smalls and the EC2 Buffet</a></p>
<p>7 0.12323785 <a title="1165-tfidf-7" href="../high_scalability-2011/high_scalability-2011-10-27-Strategy%3A_Survive_a_Comet_Strike_in_the_East_With_Reserved_Instances_in_the_West.html">1133 high scalability-2011-10-27-Strategy: Survive a Comet Strike in the East With Reserved Instances in the West</a></p>
<p>8 0.12145878 <a title="1165-tfidf-8" href="../high_scalability-2012/high_scalability-2012-11-01-Cost_Analysis%3A_TripAdvisor_and_Pinterest_costs_on_the_AWS_cloud.html">1353 high scalability-2012-11-01-Cost Analysis: TripAdvisor and Pinterest costs on the AWS cloud</a></p>
<p>9 0.11274422 <a title="1165-tfidf-9" href="../high_scalability-2011/high_scalability-2011-09-07-What_Google_App_Engine_Price_Changes_Say_About_the_Future_of_Web_Architecture.html">1112 high scalability-2011-09-07-What Google App Engine Price Changes Say About the Future of Web Architecture</a></p>
<p>10 0.10965198 <a title="1165-tfidf-10" href="../high_scalability-2008/high_scalability-2008-08-27-Updating_distributed_web_applications.html">372 high scalability-2008-08-27-Updating distributed web applications</a></p>
<p>11 0.1069449 <a title="1165-tfidf-11" href="../high_scalability-2013/high_scalability-2013-06-26-Leveraging_Cloud_Computing_at_Yelp_-_102_Million_Monthly_Vistors_and_39_Million_Reviews.html">1482 high scalability-2013-06-26-Leveraging Cloud Computing at Yelp - 102 Million Monthly Vistors and 39 Million Reviews</a></p>
<p>12 0.10528516 <a title="1165-tfidf-12" href="../high_scalability-2012/high_scalability-2012-12-12-Pinterest_Cut_Costs_from_%2454_to_%2420_Per_Hour_by_Automatically_Shutting_Down_Systems.html">1371 high scalability-2012-12-12-Pinterest Cut Costs from $54 to $20 Per Hour by Automatically Shutting Down Systems</a></p>
<p>13 0.10075936 <a title="1165-tfidf-13" href="../high_scalability-2012/high_scalability-2012-07-25-Vertical_Scaling_Ascendant_-_How_are_SSDs_Changing__Architectures%3F.html">1291 high scalability-2012-07-25-Vertical Scaling Ascendant - How are SSDs Changing  Architectures?</a></p>
<p>14 0.09875191 <a title="1165-tfidf-14" href="../high_scalability-2012/high_scalability-2012-07-18-Strategy%3A_Kill_Off_Multi-tenant_Instances_with_High_CPU_Stolen_Time.html">1286 high scalability-2012-07-18-Strategy: Kill Off Multi-tenant Instances with High CPU Stolen Time</a></p>
<p>15 0.096762463 <a title="1165-tfidf-15" href="../high_scalability-2011/high_scalability-2011-08-15-Should_any_cloud_be_considered_one_availability_zone%3F_The_Amazon_experience_says_yes..html">1098 high scalability-2011-08-15-Should any cloud be considered one availability zone? The Amazon experience says yes.</a></p>
<p>16 0.092324212 <a title="1165-tfidf-16" href="../high_scalability-2012/high_scalability-2012-10-08-How_UltraDNS_Handles_Hundreds_of_Thousands_of_Zones_and_Tens_of_Millions_of_Records.html">1335 high scalability-2012-10-08-How UltraDNS Handles Hundreds of Thousands of Zones and Tens of Millions of Records</a></p>
<p>17 0.089393377 <a title="1165-tfidf-17" href="../high_scalability-2010/high_scalability-2010-07-08-Cloud_AWS_Infrastructure_vs._Physical_Infrastructure.html">853 high scalability-2010-07-08-Cloud AWS Infrastructure vs. Physical Infrastructure</a></p>
<p>18 0.088255301 <a title="1165-tfidf-18" href="../high_scalability-2011/high_scalability-2011-06-13-Automation_on_AWS_with_Ruby_and_Puppet.html">1058 high scalability-2011-06-13-Automation on AWS with Ruby and Puppet</a></p>
<p>19 0.086613171 <a title="1165-tfidf-19" href="../high_scalability-2012/high_scalability-2012-05-11-Stuff_The_Internet_Says_On_Scalability_For_May_11%2C_2012.html">1244 high scalability-2012-05-11-Stuff The Internet Says On Scalability For May 11, 2012</a></p>
<p>20 0.086540386 <a title="1165-tfidf-20" href="../high_scalability-2010/high_scalability-2010-10-15-Troubles_with_Sharding_-_What_can_we_learn_from_the_Foursquare_Incident%3F.html">920 high scalability-2010-10-15-Troubles with Sharding - What can we learn from the Foursquare Incident?</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.123), (1, 0.068), (2, -0.011), (3, 0.023), (4, -0.061), (5, -0.07), (6, 0.043), (7, -0.072), (8, 0.044), (9, -0.14), (10, -0.026), (11, -0.012), (12, 0.04), (13, -0.064), (14, 0.0), (15, 0.003), (16, 0.029), (17, 0.021), (18, -0.001), (19, 0.047), (20, -0.014), (21, 0.03), (22, 0.018), (23, -0.033), (24, -0.062), (25, -0.029), (26, -0.031), (27, 0.002), (28, 0.025), (29, -0.043), (30, 0.042), (31, 0.033), (32, 0.001), (33, 0.036), (34, -0.05), (35, -0.049), (36, -0.039), (37, -0.07), (38, -0.017), (39, 0.007), (40, 0.02), (41, 0.019), (42, 0.033), (43, -0.006), (44, -0.054), (45, -0.02), (46, -0.02), (47, 0.034), (48, -0.032), (49, 0.001)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97519433 <a title="1165-lsi-1" href="../high_scalability-2011/high_scalability-2011-12-28-Strategy%3A_Guaranteed_Availability_Requires_Reserving_Instances_in_Specific_Zones.html">1165 high scalability-2011-12-28-Strategy: Guaranteed Availability Requires Reserving Instances in Specific Zones</a></p>
<p>Introduction: When EC2 first started the mental model was of a magic Pez dispenser supplying an infinite stream of instances in any desired flavor. If you needed an instance, because of a either a failure or traffic spike, it would be there. As amazing as EC2 is, this model turned out to be optimistic.  
 
From a  thread on the Amazon discussion forum  we learn any dispenser has limits:
  

As Availability Zones grow over time, our ability to continue to expand them can become constrained. In these scenarios, we will prevent customers from launching in the constrained zone if they do not yet have existing resources in that zone. We also might remove the constrained zone entirely from the list of options for new customers. This means that occasionally, different customers will see a different number of Availability Zones in a particular Region. Both approaches aim to help customers avoid accidentally starting to build up their infrastructure in an Availability Zone where they might have less ability</p><p>2 0.85717446 <a title="1165-lsi-2" href="../high_scalability-2010/high_scalability-2010-04-19-Strategy%3A_Order_Two_Mediums_Instead_of_Two_Smalls_and_the_EC2_Buffet.html">812 high scalability-2010-04-19-Strategy: Order Two Mediums Instead of Two Smalls and the EC2 Buffet</a></p>
<p>Introduction: Vaibhav Puranik  in  Web  serving in the cloud – our experiences with nginx and instance sizes  describes their experience trying to maximum traffic and minimum their web serving costs on EC2. Initially they tested with two m1.small instance types and then they the switched to two c1.mediums instance types. The m1s are the standard instance types and the c1s are the high CPU instance types. Obviously the mediums have greater capability, but the cost difference was interesting:
  
 In the long term they will save money using the larger instances and not autoscaling. With the small instances, traffic bursts caused autoscaling to kick in. New instances were started in response to load. The instances woud be up for a short period of time and then spin down again. This constant churn costs a lot of money. Selecting the larger instance sizes, which are capable of handling the load without autoscaling, turn out to save money even though they are more expensive. Starting new instances also tak</p><p>3 0.83460999 <a title="1165-lsi-3" href="../high_scalability-2012/high_scalability-2012-12-12-Pinterest_Cut_Costs_from_%2454_to_%2420_Per_Hour_by_Automatically_Shutting_Down_Systems.html">1371 high scalability-2012-12-12-Pinterest Cut Costs from $54 to $20 Per Hour by Automatically Shutting Down Systems</a></p>
<p>Introduction: We've long known one of the virtues of the cloud is, through the magic of services and automation, that systems can be shut or tuned down when not in use. What may be surprising is how much money can be saved. 
 
This aspect of cloudiness got a lot of pub at  AWS re:Invent  and is being rebranded under the term  Cost-Aware Architecture . An  interesting example  was given by Ryan Park, Pinterest’s technical operations lead:
  
 20% of their systems are shutdown after hours in response to traffic loads 
 Reserved instances are used for standard traffic  
 On-demand and spot instances are used to handle the elastic load throughout the day. When more servers are needed for an auto-scaled service,  spot requests are opened and on-demand instances are started at the same time. Most services are targeted to run at about 50% on-demand and 50% spot. 
 Watchdog processes continually check what's running. More instances are launched when needed and terminated when not needed. If spot prices spik</p><p>4 0.81652457 <a title="1165-lsi-4" href="../high_scalability-2013/high_scalability-2013-11-05-10_Things_You_Should_Know_About_AWS.html">1543 high scalability-2013-11-05-10 Things You Should Know About AWS</a></p>
<p>Introduction: Authored by  Chris Fregly   :  Former Netflix Streaming Platform Engineer, AWS Certified Solution Architect and Purveyor of fluxcapacitor.com.  
   Ahead of the upcoming 2nd annual re:Invent conference, inspired by Simone Brunozzi’s recent    presentation    at an AWS Meetup in San Francisco, and collected from a few of my recent    Fluxcapacitor.com    consulting engagements, I’ve compiled a list of 10 useful time and clock-tick saving tips about AWS. 
    1) Query AWS resource metadata  
      
   Can’t remember the EBS-Optimized IO throughput of your c1.xlarge cluster?  How about the size limit of an S3 object on a single PUT?     awsnow.info    is the answer to all of your AWS-resource metadata questions.  Interested in integrating awsnow.info with your application?  You’re in luck.  There’s now a    REST API   , as well! 
   Note:  These are default soft limits and will vary by account. 
    2) Tame your S3 buckets  
       
   Delete an entire S3 bucket with a single CLI command:</p><p>5 0.78495586 <a title="1165-lsi-5" href="../high_scalability-2012/high_scalability-2012-10-02-An_Epic_TripAdvisor_Update%3A_Why_Not_Run_on_the_Cloud%3F_The_Grand_Experiment..html">1331 high scalability-2012-10-02-An Epic TripAdvisor Update: Why Not Run on the Cloud? The Grand Experiment.</a></p>
<p>Introduction: This is a guest post by  Shawn Hsiao ,  Luke Massa , and  Victor Luu . Shawn runs  TripAdvisor ’s Technical Operations team, Luke and Victor interned on his team this past summer. This post is introduced by  Andy Gelfond , TripAdvisor’s head of engineering.   It's been a little over a year since our last post about the  TripAdvisor architecture . It has been an exciting year. Our business and team continues to grow, we are now an independent public company, and we have continued to keep/scale our development process and culture as we have grown - we still run dozens of independent teams, and each team continues to work across the entire stack. All that has changed are the numbers:
  
 56M visitors per month 
 350M+ pages requests a day 
 120TB+ of warehouse data running on a large Hadoop cluster, and quickly growing 
  
We also had a very successful college intern program that brought on over 60 interns this past summer, all who were quickly on boarded and doing the same kind of work a</p><p>6 0.77508253 <a title="1165-lsi-6" href="../high_scalability-2012/high_scalability-2012-11-01-Cost_Analysis%3A_TripAdvisor_and_Pinterest_costs_on_the_AWS_cloud.html">1353 high scalability-2012-11-01-Cost Analysis: TripAdvisor and Pinterest costs on the AWS cloud</a></p>
<p>7 0.76342219 <a title="1165-lsi-7" href="../high_scalability-2012/high_scalability-2012-10-25-Not_All_Regions_are_Created_Equal_-_South_America_Es_Bueno.html">1347 high scalability-2012-10-25-Not All Regions are Created Equal - South America Es Bueno</a></p>
<p>8 0.75021315 <a title="1165-lsi-8" href="../high_scalability-2012/high_scalability-2012-07-18-Strategy%3A_Kill_Off_Multi-tenant_Instances_with_High_CPU_Stolen_Time.html">1286 high scalability-2012-07-18-Strategy: Kill Off Multi-tenant Instances with High CPU Stolen Time</a></p>
<p>9 0.74812734 <a title="1165-lsi-9" href="../high_scalability-2012/high_scalability-2012-05-21-Pinterest_Architecture_Update_-_18_Million_Visitors%2C_10x_Growth%2C12_Employees%2C_410_TB_of_Data.html">1248 high scalability-2012-05-21-Pinterest Architecture Update - 18 Million Visitors, 10x Growth,12 Employees, 410 TB of Data</a></p>
<p>10 0.74756402 <a title="1165-lsi-10" href="../high_scalability-2012/high_scalability-2012-09-20-How_Vimeo_Saves_50%25_on_EC2_by_Playing_a_Smarter_Game.html">1326 high scalability-2012-09-20-How Vimeo Saves 50% on EC2 by Playing a Smarter Game</a></p>
<p>11 0.73884058 <a title="1165-lsi-11" href="../high_scalability-2013/high_scalability-2013-06-24-Update_on_How_29_Cloud_Price_Drops_Changed_the_Bottom_Line_of_TripAdvisor_and_Pinterest_-_Results_Mixed.html">1480 high scalability-2013-06-24-Update on How 29 Cloud Price Drops Changed the Bottom Line of TripAdvisor and Pinterest - Results Mixed</a></p>
<p>12 0.73332298 <a title="1165-lsi-12" href="../high_scalability-2011/high_scalability-2011-06-13-Automation_on_AWS_with_Ruby_and_Puppet.html">1058 high scalability-2011-06-13-Automation on AWS with Ruby and Puppet</a></p>
<p>13 0.713606 <a title="1165-lsi-13" href="../high_scalability-2012/high_scalability-2012-10-18-Save_up_to_30%25_by_Selecting_Better_Performing_Amazon_Instances.html">1343 high scalability-2012-10-18-Save up to 30% by Selecting Better Performing Amazon Instances</a></p>
<p>14 0.71257281 <a title="1165-lsi-14" href="../high_scalability-2013/high_scalability-2013-06-26-Leveraging_Cloud_Computing_at_Yelp_-_102_Million_Monthly_Vistors_and_39_Million_Reviews.html">1482 high scalability-2013-06-26-Leveraging Cloud Computing at Yelp - 102 Million Monthly Vistors and 39 Million Reviews</a></p>
<p>15 0.69609654 <a title="1165-lsi-15" href="../high_scalability-2013/high_scalability-2013-01-02-Why_Pinterest_Uses_the_Cloud_Instead_of_Going_Solo_-_To_Be_Or_Not_To_Be.html">1380 high scalability-2013-01-02-Why Pinterest Uses the Cloud Instead of Going Solo - To Be Or Not To Be</a></p>
<p>16 0.69204056 <a title="1165-lsi-16" href="../high_scalability-2011/high_scalability-2011-10-27-Strategy%3A_Survive_a_Comet_Strike_in_the_East_With_Reserved_Instances_in_the_West.html">1133 high scalability-2011-10-27-Strategy: Survive a Comet Strike in the East With Reserved Instances in the West</a></p>
<p>17 0.67157984 <a title="1165-lsi-17" href="../high_scalability-2012/high_scalability-2012-08-02-Strategy%3A_Use_Spare_Region_Capacity_to_Survive_Availability_Zone_Failures.html">1296 high scalability-2012-08-02-Strategy: Use Spare Region Capacity to Survive Availability Zone Failures</a></p>
<p>18 0.66847521 <a title="1165-lsi-18" href="../high_scalability-2013/high_scalability-2013-04-29-AWS_v_GCE_Face-off_and_Why_Innovation_Needs_Lower_Cost_Infrastructures.html">1448 high scalability-2013-04-29-AWS v GCE Face-off and Why Innovation Needs Lower Cost Infrastructures</a></p>
<p>19 0.64462894 <a title="1165-lsi-19" href="../high_scalability-2014/high_scalability-2014-03-03-The_%E2%80%9CFour_Hamiltons%E2%80%9D_Framework_for_Mitigating_Faults_in_the_Cloud%3A_Avoid_it%2C_Mask_it%2C_Bound_it%2C_Fix_it_Fast.html">1604 high scalability-2014-03-03-The “Four Hamiltons” Framework for Mitigating Faults in the Cloud: Avoid it, Mask it, Bound it, Fix it Fast</a></p>
<p>20 0.62340355 <a title="1165-lsi-20" href="../high_scalability-2011/high_scalability-2011-07-20-Netflix%3A_Harden_Systems_Using_a_Barrel_of_Problem_Causing_Monkeys_-_Latency%2C_Conformity%2C_Doctor%2C_Janitor%2C_Security%2C_Internationalization%2C_Chaos.html">1083 high scalability-2011-07-20-Netflix: Harden Systems Using a Barrel of Problem Causing Monkeys - Latency, Conformity, Doctor, Janitor, Security, Internationalization, Chaos</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.075), (2, 0.21), (10, 0.156), (27, 0.01), (61, 0.072), (77, 0.017), (79, 0.137), (84, 0.214)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.91717482 <a title="1165-lda-1" href="../high_scalability-2011/high_scalability-2011-12-28-Strategy%3A_Guaranteed_Availability_Requires_Reserving_Instances_in_Specific_Zones.html">1165 high scalability-2011-12-28-Strategy: Guaranteed Availability Requires Reserving Instances in Specific Zones</a></p>
<p>Introduction: When EC2 first started the mental model was of a magic Pez dispenser supplying an infinite stream of instances in any desired flavor. If you needed an instance, because of a either a failure or traffic spike, it would be there. As amazing as EC2 is, this model turned out to be optimistic.  
 
From a  thread on the Amazon discussion forum  we learn any dispenser has limits:
  

As Availability Zones grow over time, our ability to continue to expand them can become constrained. In these scenarios, we will prevent customers from launching in the constrained zone if they do not yet have existing resources in that zone. We also might remove the constrained zone entirely from the list of options for new customers. This means that occasionally, different customers will see a different number of Availability Zones in a particular Region. Both approaches aim to help customers avoid accidentally starting to build up their infrastructure in an Availability Zone where they might have less ability</p><p>2 0.9035005 <a title="1165-lda-2" href="../high_scalability-2013/high_scalability-2013-01-09-The_Story_of_How_Turning_Disk_Into_a_Service_Lead_to_a_Deluge_of_Density.html">1384 high scalability-2013-01-09-The Story of How Turning Disk Into a Service Lead to a Deluge of Density</a></p>
<p>Introduction: We usually think of the wonderful advantages of  service oriented architectures  as a software thing, but it also applies to hardware. In  Security Now 385 , that Doyen of Disk,  Steve Gibson , tells the fascinating story (@ about 41:30) of how moving to a service oriented architecture in hard drives, modeling a drive as a linear stream of sectors, helped create the amazing high density disk drives we enjoy today.
 
When drives switched to use the  IDE  (integrated drive electronics) interface, the controller function moved into the drive instead of the computer. No longer were low level drive signals moved across cables and into the motherboard. Now we just ask the drive for the desired sector and the drive takes care of it.
 
This allowed manufacturers to do anything they wanted to behind the IDE interface. The drive stopped being dumb, it became smart, providing a sort of sector service. Density sky rocketed because there was no dependency on the computer. All the internals could co</p><p>3 0.86843717 <a title="1165-lda-3" href="../high_scalability-2011/high_scalability-2011-08-25-Colmux_-_Finding_Memory_Leaks%2C_High_I-O_Wait_Times%2C_and_Hotness_on_3000_Node_Clusters.html">1104 high scalability-2011-08-25-Colmux - Finding Memory Leaks, High I-O Wait Times, and Hotness on 3000 Node Clusters</a></p>
<p>Introduction: Todd had originally posted an entry on  collectl  here at  Collectl - Performance Data Collector . Collectl collects real-time data from a large number of subsystems like buddyinfo, cpu, disk, inodes, infiniband, lustre, memory, network, nfs, processes, quadrics, slabs, sockets and tcp, all using one tool and in one consistent format.
 
Since then a lot has happened.  It's now part of both Fedora and Debian distros, not to mention several others. There has also been a pretty good summary written up by  Joe Brockmeier . It's also pretty well documented (I like to think) on  sourceforge . There have also been a few blog postings by   Martin Bach   on his blog.
 
Anyhow, awhile back I released a new version of collectl-utils and gave a complete face-lift to one of the utilities, colmux, which is a collectl multiplexor.  This tool has the ability to run collectl on multiple systems, which in turn send all their output back to colmux.  Colmux then sorts the output on a user-specified column</p><p>4 0.8601315 <a title="1165-lda-4" href="../high_scalability-2008/high_scalability-2008-01-16-Strategy%3A_Asynchronous_Queued__Virus_Scanning.html">215 high scalability-2008-01-16-Strategy: Asynchronous Queued  Virus Scanning</a></p>
<p>Introduction: Atif Ghaffar  has a nice strategy to deal with virus checking uploads:
   Upload item into a safe area. If necessary, the uploader blocks waiting for a result.    Queue a work order into a job system so all the work can be distributed throughout your cluster.    A service in your cluster performs the virus scan and informs the uploader of the result.    Move the vetted item into your system.  This removes the CPU bottleneck from your web servers and distributes it through your cluster.  Keep your web servers providing prompt service to users. Let your cluster do the heavy lifting. This minimizes response time and maximizes throughput. A similar system can be used for creating thumbnails, transcoding, copyright checks, updating indexes, event notification or any other kind of intensive work.</p><p>5 0.85148025 <a title="1165-lda-5" href="../high_scalability-2009/high_scalability-2009-05-06-DyradLINQ.html">592 high scalability-2009-05-06-DyradLINQ</a></p>
<p>Introduction: The goal of DryadLINQ is to make distributed computing on large compute cluster simple enough for ordinary programmers. DryadLINQ combines two important pieces of Microsoft technology: the Dryad distributed execution engine and the .NET Language Integrated Query (LINQ).</p><p>6 0.80830222 <a title="1165-lda-6" href="../high_scalability-2012/high_scalability-2012-11-01-Cost_Analysis%3A_TripAdvisor_and_Pinterest_costs_on_the_AWS_cloud.html">1353 high scalability-2012-11-01-Cost Analysis: TripAdvisor and Pinterest costs on the AWS cloud</a></p>
<p>7 0.80003256 <a title="1165-lda-7" href="../high_scalability-2007/high_scalability-2007-11-05-Strategy%3A_Diagonal_Scaling_-_Don%27t_Forget_to_Scale_Out_AND_Up.html">142 high scalability-2007-11-05-Strategy: Diagonal Scaling - Don't Forget to Scale Out AND Up</a></p>
<p>8 0.7956984 <a title="1165-lda-8" href="../high_scalability-2014/high_scalability-2014-01-24-Stuff_The_Internet_Says_On_Scalability_For_January_24th%2C_2014.html">1585 high scalability-2014-01-24-Stuff The Internet Says On Scalability For January 24th, 2014</a></p>
<p>9 0.79375017 <a title="1165-lda-9" href="../high_scalability-2007/high_scalability-2007-07-16-Paper%3A_The_Clustered_Storage_Revolution.html">20 high scalability-2007-07-16-Paper: The Clustered Storage Revolution</a></p>
<p>10 0.79247487 <a title="1165-lda-10" href="../high_scalability-2011/high_scalability-2011-05-23-Evernote_Architecture_-_9_Million_Users_and_150_Million_Requests_a_Day.html">1046 high scalability-2011-05-23-Evernote Architecture - 9 Million Users and 150 Million Requests a Day</a></p>
<p>11 0.78851068 <a title="1165-lda-11" href="../high_scalability-2008/high_scalability-2008-02-22-Kevin%27s_Great_Adventures_in_SSDland.html">257 high scalability-2008-02-22-Kevin's Great Adventures in SSDland</a></p>
<p>12 0.78826666 <a title="1165-lda-12" href="../high_scalability-2012/high_scalability-2012-12-12-Pinterest_Cut_Costs_from_%2454_to_%2420_Per_Hour_by_Automatically_Shutting_Down_Systems.html">1371 high scalability-2012-12-12-Pinterest Cut Costs from $54 to $20 Per Hour by Automatically Shutting Down Systems</a></p>
<p>13 0.78685158 <a title="1165-lda-13" href="../high_scalability-2007/high_scalability-2007-11-16-Product%3A_lbpool_-_Load_Balancing_JDBC_Pool.html">157 high scalability-2007-11-16-Product: lbpool - Load Balancing JDBC Pool</a></p>
<p>14 0.78426254 <a title="1165-lda-14" href="../high_scalability-2012/high_scalability-2012-10-02-An_Epic_TripAdvisor_Update%3A_Why_Not_Run_on_the_Cloud%3F_The_Grand_Experiment..html">1331 high scalability-2012-10-02-An Epic TripAdvisor Update: Why Not Run on the Cloud? The Grand Experiment.</a></p>
<p>15 0.78341329 <a title="1165-lda-15" href="../high_scalability-2014/high_scalability-2014-04-14-How_do_you_even_do_anything_without_using_EBS%3F.html">1631 high scalability-2014-04-14-How do you even do anything without using EBS?</a></p>
<p>16 0.78046709 <a title="1165-lda-16" href="../high_scalability-2013/high_scalability-2013-07-15-Ask_HS%3A_What%27s_Wrong_with_Twitter%2C_Why_Isn%27t_One_Machine_Enough%3F.html">1491 high scalability-2013-07-15-Ask HS: What's Wrong with Twitter, Why Isn't One Machine Enough?</a></p>
<p>17 0.77930725 <a title="1165-lda-17" href="../high_scalability-2012/high_scalability-2012-07-25-Vertical_Scaling_Ascendant_-_How_are_SSDs_Changing__Architectures%3F.html">1291 high scalability-2012-07-25-Vertical Scaling Ascendant - How are SSDs Changing  Architectures?</a></p>
<p>18 0.77880377 <a title="1165-lda-18" href="../high_scalability-2011/high_scalability-2011-12-19-How_Twitter_Stores_250_Million_Tweets_a_Day_Using_MySQL.html">1159 high scalability-2011-12-19-How Twitter Stores 250 Million Tweets a Day Using MySQL</a></p>
<p>19 0.77858913 <a title="1165-lda-19" href="../high_scalability-2014/high_scalability-2014-04-03-Leslie_Lamport_to_Programmers%3A_You%27re_Doing_it_Wrong.html">1625 high scalability-2014-04-03-Leslie Lamport to Programmers: You're Doing it Wrong</a></p>
<p>20 0.77573943 <a title="1165-lda-20" href="../high_scalability-2012/high_scalability-2012-12-10-Switch_your_databases_to_Flash_storage._Now._Or_you%27re_doing_it_wrong..html">1369 high scalability-2012-12-10-Switch your databases to Flash storage. Now. Or you're doing it wrong.</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
