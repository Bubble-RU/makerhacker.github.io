<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1068 high scalability-2011-06-27-TripAdvisor Architecture - 40M Visitors, 200M Dynamic Page Views, 30TB Data</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2011" href="../home/high_scalability-2011_home.html">high_scalability-2011</a> <a title="high_scalability-2011-1068" href="#">high_scalability-2011-1068</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1068 high scalability-2011-06-27-TripAdvisor Architecture - 40M Visitors, 200M Dynamic Page Views, 30TB Data</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2011-1068-html" href="http://highscalability.com//blog/2011/6/27/tripadvisor-architecture-40m-visitors-200m-dynamic-page-view.html">html</a></p><p>Introduction: This is a guest post by  Andy Gelfond , VP of Engineering for TripAdvisor. Andy has been with TripAdvisor for six and a half years, wrote a lot of code in the earlier days, and has been building and running a first class engineering and operations team that is responsible for the worlds largest travel site. There's an update for this article at  An Epic TripAdvisor Update: Why Not Run On The Cloud? The Grand Experiment .  
 
For  TripAdvisor , scalability is woven into our organization on many levels - data center, software architecture, development/deployment/operations, and, most importantly, within the culture and organization. It is not enough to have a scalable data center, or a scalable software architecture. The process of designing, coding, testing, and deploying code also needs to be scalable. All of this starts with hiring and a culture and an organization that values and supports a distributed, fast, and effective development and operation of a complex and highly scalable co</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Andy has been with TripAdvisor for six and a half years, wrote a lot of code in the earlier days, and has been building and running a first class engineering and operations team that is responsible for the worlds largest travel site. [sent-2, score-0.547]
</p><p>2 Either have plenty nodes in a cluster (web servers, service machines), or have true N+1 redundancy (databases and the datacenter itself)      Flow of control     Flow of control is very simple: request URL's are parsed, content is collected from various services, and then applied to a template. [sent-13, score-0.254]
</p><p>3 A Java servlet parses the url and cookie info and determines the content it needs, making calls to the various services. [sent-20, score-0.296]
</p><p>4 The service API's are defined as Java interfaces, and a servlet will make anywhere from 0 to a dozen service requests. [sent-21, score-0.453]
</p><p>5 Each service has an API that is optimized for the business and/or usage pattern - for example, fetch the reviews for a member, or the reviews for a location. [sent-24, score-0.55]
</p><p>6 Release process is shared and rotated among various senior engineers and engineering managers    Numerous test frameworks available: Unit, Functional, Load, Smoke, Selenium, Load, and a test lab. [sent-42, score-0.376]
</p><p>7 Each of these teams has their own business objectives, and each team is able to, and responsible for, all aspects of their business. [sent-45, score-0.392]
</p><p>8 We do not have the traditional Architect, Coder, QA roles    Our Operations team is one team that is responsible for the platform that all of these other teams use: Datacenter, Software infrastructure, DevOps, Warehousing. [sent-48, score-0.342]
</p><p>9 This team includes two technical operations engineers and two site operatons engineers who are responsible for the datacenters and software infrastructure. [sent-50, score-0.515]
</p><p>10 Summer Fridays - time shift your weeks during the summer and free up your Fridays    Yearly charity day - the entire company goes out and contributes their day to a local charity - painting, gardening, etc    TripAdvisor Charitable Foundation. [sent-58, score-0.304]
</p><p>11 We do not have "architects" - at TripAdvisor, if you design something, your code it, and if you code it you test it. [sent-63, score-0.525]
</p><p>12 It is better to deliver 20 projects with 10 bugs and miss 5 projects by two days than to deliver 10 projects that are all perfect and on time. [sent-69, score-0.497]
</p><p>13 For example, we have rewritten our members functionality as we scaled from tens of thousands, to millions, to tens of millions. [sent-73, score-0.29]
</p><p>14 It is far better to do two queries (get the set of reviews with their member ids, then get all of the member from this set of ids and merge it at the app level) than do a join. [sent-85, score-0.342]
</p><p>15 It is also easier to keep your content type scalable - we can add new content types in a very modular manner as each content type stands alone. [sent-87, score-0.369]
</p><p>16 This also aligns well with our service oriented approach, where a service is supported by a database. [sent-88, score-0.262]
</p><p>17 Having a known set of chunky (optimized for the wire) protocols that are aligned to the business and usage patterns makes assembling pages easier, and allows you to scale out each service according to business needs. [sent-93, score-0.361]
</p><p>18 If you are smart, you will get your design reviewed, your code reviewed, and you will write tests and appropriate monitoring scripts. [sent-117, score-0.277]
</p><p>19 All engineers are invited to a weekly design review. [sent-122, score-0.25]
</p><p>20 If you have a project that is going to impact others (database, memory usage, new servlets, new libraries, anything of significance) you are expected to present your design at design review and discuss it. [sent-123, score-0.313]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tripadvisor', 0.187), ('code', 0.167), ('reviews', 0.152), ('engineers', 0.14), ('service', 0.131), ('content', 0.123), ('js', 0.119), ('business', 0.115), ('projects', 0.111), ('tens', 0.11), ('design', 0.11), ('teams', 0.108), ('java', 0.107), ('responsible', 0.104), ('machines', 0.102), ('stateless', 0.1), ('dozen', 0.098), ('css', 0.097), ('member', 0.095), ('velocity', 0.095), ('spy', 0.094), ('servlet', 0.093), ('review', 0.093), ('patches', 0.092), ('hire', 0.09), ('apac', 0.085), ('charitable', 0.085), ('gelfond', 0.085), ('rentals', 0.085), ('mode', 0.083), ('deliver', 0.082), ('test', 0.081), ('calls', 0.08), ('pairs', 0.077), ('day', 0.076), ('charity', 0.076), ('flights', 0.076), ('end', 0.075), ('engineering', 0.074), ('sem', 0.073), ('travel', 0.071), ('reviewed', 0.071), ('drdb', 0.071), ('crm', 0.071), ('members', 0.07), ('mistakes', 0.068), ('pools', 0.067), ('listings', 0.067), ('operations', 0.066), ('team', 0.065)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000007 <a title="1068-tfidf-1" href="../high_scalability-2011/high_scalability-2011-06-27-TripAdvisor_Architecture_-_40M_Visitors%2C_200M_Dynamic_Page_Views%2C_30TB_Data.html">1068 high scalability-2011-06-27-TripAdvisor Architecture - 40M Visitors, 200M Dynamic Page Views, 30TB Data</a></p>
<p>Introduction: This is a guest post by  Andy Gelfond , VP of Engineering for TripAdvisor. Andy has been with TripAdvisor for six and a half years, wrote a lot of code in the earlier days, and has been building and running a first class engineering and operations team that is responsible for the worlds largest travel site. There's an update for this article at  An Epic TripAdvisor Update: Why Not Run On The Cloud? The Grand Experiment .  
 
For  TripAdvisor , scalability is woven into our organization on many levels - data center, software architecture, development/deployment/operations, and, most importantly, within the culture and organization. It is not enough to have a scalable data center, or a scalable software architecture. The process of designing, coding, testing, and deploying code also needs to be scalable. All of this starts with hiring and a culture and an organization that values and supports a distributed, fast, and effective development and operation of a complex and highly scalable co</p><p>2 0.30357054 <a title="1068-tfidf-2" href="../high_scalability-2011/high_scalability-2011-07-01-TripAdvisor_Strategy%3A_No_Architects%2C_Engineers_Work_Across_the_Entire_Stack.html">1072 high scalability-2011-07-01-TripAdvisor Strategy: No Architects, Engineers Work Across the Entire Stack</a></p>
<p>Introduction: If you are an  insect , don't work at TripAdvisor, specialization is out. One of the most commented on strategies from the  TripAdvisor  architecture article is their rather opinionated take on the role of engineers in the organization.
 
Typically engineers live in a box. They are specialized, they do database work and not much else, and they just do programming, not much else. TripAdvisor takes the road less traveled:
  
  Engineers work across entire stack  - HTML, CSS, JS, Java, scripting. If you do not know something, you learn it. The only thing that gets in the way of delivering your project is you, as you are expected to work at all levels - design, code, test, monitoring, CSS, JS, Java, SQL, scripting. 
  We do not have "architects."   At TripAdvisor, if you design something, your code it, and if you code it you test it. Engineers who do not like to go outside their comfort zone, or who feel certain work is "beneath" them will simply get in the way. 
  
A radical take for an e</p><p>3 0.25688401 <a title="1068-tfidf-3" href="../high_scalability-2012/high_scalability-2012-05-07-Startups_are_Creating_a_New_System_of_the_World_for_IT.html">1240 high scalability-2012-05-07-Startups are Creating a New System of the World for IT</a></p>
<p>Introduction: It remains that, from the same principles, I now demonstrate the frame of the System of the World.  -- Isaac Newton
 
The practice of IT reminds me a lot of the practice of science before Isaac Newton. Aristotelianism was dead, but there was nothing to replace it. Then Newton came along, created a scientific revolution with his  System of the World . And everything changed. That was New System of the World number one.
 
New System of the World number two was written about by the incomparable Neal Stephenson in his incredible  Baroque Cycle  series. It explores the singular creation of a new way of organizing society grounded in new modes of thought in business, religion, politics, and science. Our modern world emerged Enlightened as it could from this roiling cauldron of forces.
 
In IT we may have had a Leonardo da Vinci or even a Galileo, but we’ve never had our Newton. Maybe we don't need a towering genius to make everything clear? For years startups, like the frenetically inventive</p><p>4 0.23028727 <a title="1068-tfidf-4" href="../high_scalability-2014/high_scalability-2014-02-17-How_the_AOL.com_Architecture_Evolved_to_99.999%25_Availability%2C_8_Million_Visitors_Per_Day%2C_and_200%2C000_Requests_Per_Second.html">1597 high scalability-2014-02-17-How the AOL.com Architecture Evolved to 99.999% Availability, 8 Million Visitors Per Day, and 200,000 Requests Per Second</a></p>
<p>Introduction: This is a guest post by  Dave Hagler  Systems Architect at AOL. 
  The AOL homepages receive more than  8 million visitors per day .  That’s more daily viewers than Good Morning America or the Today Show on television.  Over a billion page views are served each month.  AOL.com has been a major internet destination since 1996, and still has a strong following of loyal users.
   The architecture for AOL.com is in it’s 5th generation .  It has essentially been rebuilt from scratch 5 times over two decades.  The current architecture was designed 6 years ago.  Pieces have been upgraded and new components have been added along the way, but the overall design remains largely intact.  The code, tools, development and deployment processes are highly tuned over 6 years of continual improvement, making the AOL.com architecture battle tested and very stable.
  The engineering team is made up of developers, testers, and operations and  totals around 25 people .  The majority are in Dulles, Virginia</p><p>5 0.21953933 <a title="1068-tfidf-5" href="../high_scalability-2012/high_scalability-2012-10-02-An_Epic_TripAdvisor_Update%3A_Why_Not_Run_on_the_Cloud%3F_The_Grand_Experiment..html">1331 high scalability-2012-10-02-An Epic TripAdvisor Update: Why Not Run on the Cloud? The Grand Experiment.</a></p>
<p>Introduction: This is a guest post by  Shawn Hsiao ,  Luke Massa , and  Victor Luu . Shawn runs  TripAdvisor ’s Technical Operations team, Luke and Victor interned on his team this past summer. This post is introduced by  Andy Gelfond , TripAdvisor’s head of engineering.   It's been a little over a year since our last post about the  TripAdvisor architecture . It has been an exciting year. Our business and team continues to grow, we are now an independent public company, and we have continued to keep/scale our development process and culture as we have grown - we still run dozens of independent teams, and each team continues to work across the entire stack. All that has changed are the numbers:
  
 56M visitors per month 
 350M+ pages requests a day 
 120TB+ of warehouse data running on a large Hadoop cluster, and quickly growing 
  
We also had a very successful college intern program that brought on over 60 interns this past summer, all who were quickly on boarded and doing the same kind of work a</p><p>6 0.20485446 <a title="1068-tfidf-6" href="../high_scalability-2011/high_scalability-2011-07-26-Sponsored_Post%3A_BetterWorks%2C_New_Relic%2C_eHarmony%2C_TripAdvisor%2C_NoSQL_Now%21%2C_Surge%2C_Tungsten%2C_Aconex%2C_Mathworks%2C_AppDynamics%2C__ScaleOut%2C_Couchbase%2C_CloudSigma%2C_ManageEngine%2C_Site24x7.html">1086 high scalability-2011-07-26-Sponsored Post: BetterWorks, New Relic, eHarmony, TripAdvisor, NoSQL Now!, Surge, Tungsten, Aconex, Mathworks, AppDynamics,  ScaleOut, Couchbase, CloudSigma, ManageEngine, Site24x7</a></p>
<p>7 0.20159639 <a title="1068-tfidf-7" href="../high_scalability-2013/high_scalability-2013-10-29-Sponsored_Post%3A_Apple%2C_NuoDB%2C_ScaleOut%2C_FreeAgent%2C_CloudStats.me%2C_Intechnica%2C_MongoDB%2C_Stackdriver%2C_BlueStripe%2C_Booking%2C_Rackspace%2C_AiCache%2C_Aerospike%2C_New_Relic%2C_LogicMonitor%2C_AppDynamics%2C_ManageEngine%2C_Site24x7.html">1539 high scalability-2013-10-29-Sponsored Post: Apple, NuoDB, ScaleOut, FreeAgent, CloudStats.me, Intechnica, MongoDB, Stackdriver, BlueStripe, Booking, Rackspace, AiCache, Aerospike, New Relic, LogicMonitor, AppDynamics, ManageEngine, Site24x7</a></p>
<p>8 0.19589868 <a title="1068-tfidf-8" href="../high_scalability-2013/high_scalability-2013-11-12-Sponsored_Post%3A_Klout%2C_Apple%2C_NuoDB%2C_ScaleOut%2C_FreeAgent%2C_CloudStats.me%2C_Intechnica%2C_MongoDB%2C_Stackdriver%2C_BlueStripe%2C_Booking%2C_AiCache%2C_Aerospike%2C_New_Relic%2C_LogicMonitor%2C_AppDynamics%2C_ManageEngine%2C_Site24x7.html">1547 high scalability-2013-11-12-Sponsored Post: Klout, Apple, NuoDB, ScaleOut, FreeAgent, CloudStats.me, Intechnica, MongoDB, Stackdriver, BlueStripe, Booking, AiCache, Aerospike, New Relic, LogicMonitor, AppDynamics, ManageEngine, Site24x7</a></p>
<p>9 0.19373357 <a title="1068-tfidf-9" href="../high_scalability-2010/high_scalability-2010-10-19-Sponsored_Post%3A_Playfish%2C_Electronic_Arts%2C_Tagged%2C_Undertone%2C_Box.net%2C_Wiredrive%2C_Joyent%2C_DeviantART%2C_CloudSigma%2C_ManageEngine%2C_Site24x7.html">922 high scalability-2010-10-19-Sponsored Post: Playfish, Electronic Arts, Tagged, Undertone, Box.net, Wiredrive, Joyent, DeviantART, CloudSigma, ManageEngine, Site24x7</a></p>
<p>10 0.19050945 <a title="1068-tfidf-10" href="../high_scalability-2011/high_scalability-2011-07-12-Sponsored_Post%3A_New_Relic%2C_eHarmony%2C_TripAdvisor%2C_NoSQL_Now%21%2C_Surge%2C_BioWare%2C_Tungsten%2C_deviantART%2C_Aconex%2C_Hadapt%2C_Mathworks%2C_AppDynamics%2C__ScaleOut%2C_Membase%2C_CloudSigma%2C_ManageEngine%2C_Site24x7.html">1079 high scalability-2011-07-12-Sponsored Post: New Relic, eHarmony, TripAdvisor, NoSQL Now!, Surge, BioWare, Tungsten, deviantART, Aconex, Hadapt, Mathworks, AppDynamics,  ScaleOut, Membase, CloudSigma, ManageEngine, Site24x7</a></p>
<p>11 0.1895 <a title="1068-tfidf-11" href="../high_scalability-2011/high_scalability-2011-07-05-Sponsored_Post%3A_TripAdvisor%2C_eHarmony%2C_NoSQL_Now%21%2C_Surge%2C_BioWare%2C_Tungsten%2C_deviantART%2C_Aconex%2C_Hadapt%2C_Mathworks%2C_AppDynamics%2C__ScaleOut%2C_Membase%2C_CloudSigma%2C_ManageEngine%2C_Site24x7.html">1073 high scalability-2011-07-05-Sponsored Post: TripAdvisor, eHarmony, NoSQL Now!, Surge, BioWare, Tungsten, deviantART, Aconex, Hadapt, Mathworks, AppDynamics,  ScaleOut, Membase, CloudSigma, ManageEngine, Site24x7</a></p>
<p>12 0.18894687 <a title="1068-tfidf-12" href="../high_scalability-2007/high_scalability-2007-09-18-Amazon_Architecture.html">96 high scalability-2007-09-18-Amazon Architecture</a></p>
<p>13 0.1889016 <a title="1068-tfidf-13" href="../high_scalability-2013/high_scalability-2013-04-30-Sponsored_Post%3A_Spotify%2C_Evernote%2C_Surge%2C_Rackspace%2C_Simple%2C_Amazon%2C_Booking%2C_aiCache%2C_Aerospike%2C_Percona%2C_ScaleOut%2C_New_Relic%2C_LogicMonitor%2C_AppDynamics%2C_ManageEngine%2C_Site24x7.html">1449 high scalability-2013-04-30-Sponsored Post: Spotify, Evernote, Surge, Rackspace, Simple, Amazon, Booking, aiCache, Aerospike, Percona, ScaleOut, New Relic, LogicMonitor, AppDynamics, ManageEngine, Site24x7</a></p>
<p>14 0.1875429 <a title="1068-tfidf-14" href="../high_scalability-2010/high_scalability-2010-11-09-Sponsored_Post%3A_Imo%2C_Membase%2C_Playfish%2C_Electronic_Arts%2C_Tagged%2C_Undertone%2C_Joyent%2C_Appirio%2C_Tuenti%2C_CloudSigma%2C_ManageEngine%2C_Site24x7.html">938 high scalability-2010-11-09-Sponsored Post: Imo, Membase, Playfish, Electronic Arts, Tagged, Undertone, Joyent, Appirio, Tuenti, CloudSigma, ManageEngine, Site24x7</a></p>
<p>15 0.18709299 <a title="1068-tfidf-15" href="../high_scalability-2011/high_scalability-2011-06-14-A_TripAdvisor_Short.html">1059 high scalability-2011-06-14-A TripAdvisor Short</a></p>
<p>16 0.18390702 <a title="1068-tfidf-16" href="../high_scalability-2013/high_scalability-2013-10-15-Sponsored_Post%3A_Apple%2C_ScaleOut%2C_FreeAgent%2C_CloudStats.me%2C_Intechnica%2C_Couchbase%2C_MongoDB%2C_Stackdriver%2C_BlueStripe%2C_Booking%2C_Rackspace%2C_AiCache%2C_Aerospike%2C_New_Relic%2C_LogicMonitor%2C_AppDynamics%2C_ManageEngine%2C_Site24x7.html">1532 high scalability-2013-10-15-Sponsored Post: Apple, ScaleOut, FreeAgent, CloudStats.me, Intechnica, Couchbase, MongoDB, Stackdriver, BlueStripe, Booking, Rackspace, AiCache, Aerospike, New Relic, LogicMonitor, AppDynamics, ManageEngine, Site24x7</a></p>
<p>17 0.18113764 <a title="1068-tfidf-17" href="../high_scalability-2013/high_scalability-2013-04-16-Sponsored_Post%3A_Surge%2C_Rackspace%2C_Simple%2C_Fitbit%2C_Amazon%2C_Booking%2C_aiCache%2C_Aerospike%2C_Percona%2C_ScaleOut%2C_New_Relic%2C_LogicMonitor%2C_AppDynamics%2C_ManageEngine%2C_Site24x7.html">1441 high scalability-2013-04-16-Sponsored Post: Surge, Rackspace, Simple, Fitbit, Amazon, Booking, aiCache, Aerospike, Percona, ScaleOut, New Relic, LogicMonitor, AppDynamics, ManageEngine, Site24x7</a></p>
<p>18 0.18108606 <a title="1068-tfidf-18" href="../high_scalability-2012/high_scalability-2012-11-27-Sponsored_Post%3A__Akiban%2C_Booking%2C_Teradata_Aster%2C_Hadapt%2C_Zoosk%2C_Aerospike%2C_Server_Stack%2C_Wiredrive%2C_NY_Times%2C_CouchConf%2C_FiftyThree%2C_Percona%2C_ScaleOut%2C_New_Relic%2C_NetDNA%2C_GigaSpaces%2C_AiCache%2C_Logic_Monitor%2C_AppDynamics.html">1363 high scalability-2012-11-27-Sponsored Post:  Akiban, Booking, Teradata Aster, Hadapt, Zoosk, Aerospike, Server Stack, Wiredrive, NY Times, CouchConf, FiftyThree, Percona, ScaleOut, New Relic, NetDNA, GigaSpaces, AiCache, Logic Monitor, AppDynamics</a></p>
<p>19 0.17947061 <a title="1068-tfidf-19" href="../high_scalability-2013/high_scalability-2013-04-02-Sponsored_Post%3A_Rackspace%2C_Simple%2C_Fitbit%2C_Amazon%2C_Booking%2C_aiCache%2C_Aerospike%2C_Percona%2C_ScaleOut%2C_New_Relic%2C_LogicMonitor%2C_AppDynamics%2C_ManageEngine%2C_Site24x7.html">1433 high scalability-2013-04-02-Sponsored Post: Rackspace, Simple, Fitbit, Amazon, Booking, aiCache, Aerospike, Percona, ScaleOut, New Relic, LogicMonitor, AppDynamics, ManageEngine, Site24x7</a></p>
<p>20 0.17931315 <a title="1068-tfidf-20" href="../high_scalability-2013/high_scalability-2013-05-14-Sponsored_Post%3A_Dow_Jones%2C_Spotify%2C_Evernote%2C_Surge%2C_Rackspace%2C_Amazon%2C_Booking%2C_aiCache%2C_Aerospike%2C_Percona%2C_ScaleOut%2C_New_Relic%2C_LogicMonitor%2C_AppDynamics%2C_ManageEngine%2C_Site24x7.html">1457 high scalability-2013-05-14-Sponsored Post: Dow Jones, Spotify, Evernote, Surge, Rackspace, Amazon, Booking, aiCache, Aerospike, Percona, ScaleOut, New Relic, LogicMonitor, AppDynamics, ManageEngine, Site24x7</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.373), (1, 0.063), (2, -0.014), (3, -0.045), (4, 0.079), (5, -0.101), (6, 0.052), (7, -0.054), (8, -0.037), (9, 0.018), (10, -0.062), (11, 0.02), (12, 0.007), (13, -0.096), (14, -0.023), (15, 0.041), (16, -0.013), (17, -0.02), (18, 0.013), (19, -0.029), (20, -0.035), (21, -0.02), (22, 0.038), (23, -0.013), (24, -0.029), (25, -0.004), (26, -0.037), (27, -0.031), (28, 0.016), (29, 0.023), (30, -0.046), (31, 0.098), (32, -0.048), (33, 0.06), (34, 0.01), (35, -0.035), (36, -0.008), (37, -0.017), (38, 0.035), (39, 0.041), (40, -0.03), (41, -0.002), (42, 0.058), (43, 0.019), (44, -0.011), (45, -0.04), (46, 0.051), (47, -0.001), (48, -0.049), (49, -0.008)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97559249 <a title="1068-lsi-1" href="../high_scalability-2011/high_scalability-2011-06-27-TripAdvisor_Architecture_-_40M_Visitors%2C_200M_Dynamic_Page_Views%2C_30TB_Data.html">1068 high scalability-2011-06-27-TripAdvisor Architecture - 40M Visitors, 200M Dynamic Page Views, 30TB Data</a></p>
<p>Introduction: This is a guest post by  Andy Gelfond , VP of Engineering for TripAdvisor. Andy has been with TripAdvisor for six and a half years, wrote a lot of code in the earlier days, and has been building and running a first class engineering and operations team that is responsible for the worlds largest travel site. There's an update for this article at  An Epic TripAdvisor Update: Why Not Run On The Cloud? The Grand Experiment .  
 
For  TripAdvisor , scalability is woven into our organization on many levels - data center, software architecture, development/deployment/operations, and, most importantly, within the culture and organization. It is not enough to have a scalable data center, or a scalable software architecture. The process of designing, coding, testing, and deploying code also needs to be scalable. All of this starts with hiring and a culture and an organization that values and supports a distributed, fast, and effective development and operation of a complex and highly scalable co</p><p>2 0.84139788 <a title="1068-lsi-2" href="../high_scalability-2012/high_scalability-2012-10-08-How_UltraDNS_Handles_Hundreds_of_Thousands_of_Zones_and_Tens_of_Millions_of_Records.html">1335 high scalability-2012-10-08-How UltraDNS Handles Hundreds of Thousands of Zones and Tens of Millions of Records</a></p>
<p>Introduction: This is a guest post by  Jeffrey Damick , Principal Software Engineer for  Neustar .   Jeffrey has overseen the software architecture for  UltraDNS  for last two and half years as it went through substantial revitalization.  
 
UltraDNS is one the top the DNS providers, serving many  top-level domains (TLDs)  as well as  second-level domains (SLDs) .  This requires handling of several hundreds of thousands of zones with many containing millions of records each.  Even with all of its success UltraDNS had fallen into a rut several years ago, its release schedule had become haphazard at best and the team was struggling to keep up with feature requests in a waterfall development style.
  Development  
Realizing that something had to be done the team came together and identified the most important areas to attack first.  We began with the code base, stabilizing our flagship proprietary C++ DNS server, instituting common best practices and automation.   Testing was very manual and not easily</p><p>3 0.82653987 <a title="1068-lsi-3" href="../high_scalability-2014/high_scalability-2014-02-17-How_the_AOL.com_Architecture_Evolved_to_99.999%25_Availability%2C_8_Million_Visitors_Per_Day%2C_and_200%2C000_Requests_Per_Second.html">1597 high scalability-2014-02-17-How the AOL.com Architecture Evolved to 99.999% Availability, 8 Million Visitors Per Day, and 200,000 Requests Per Second</a></p>
<p>Introduction: This is a guest post by  Dave Hagler  Systems Architect at AOL. 
  The AOL homepages receive more than  8 million visitors per day .  That’s more daily viewers than Good Morning America or the Today Show on television.  Over a billion page views are served each month.  AOL.com has been a major internet destination since 1996, and still has a strong following of loyal users.
   The architecture for AOL.com is in it’s 5th generation .  It has essentially been rebuilt from scratch 5 times over two decades.  The current architecture was designed 6 years ago.  Pieces have been upgraded and new components have been added along the way, but the overall design remains largely intact.  The code, tools, development and deployment processes are highly tuned over 6 years of continual improvement, making the AOL.com architecture battle tested and very stable.
  The engineering team is made up of developers, testers, and operations and  totals around 25 people .  The majority are in Dulles, Virginia</p><p>4 0.81801641 <a title="1068-lsi-4" href="../high_scalability-2012/high_scalability-2012-06-20-iDoneThis_-_Scaling_an_Email-based_App_from_Scratch.html">1269 high scalability-2012-06-20-iDoneThis - Scaling an Email-based App from Scratch</a></p>
<p>Introduction: This is a guest post by Rodrigo Guzman, CTO of  iDoneThis , which makes status reporting happen at your company with the lightest possible touch. 
 
 iDoneThis  is a simple management application that emails your team at the end of every day to ask, "What'd you get done today?"  Just reply with a few lines of what you got done. The following morning everyone on your team gets a digest with what the team accomplished the previous day to keep everyone in the loop and kickstart another awesome day.
 
Before we launched, we built iDoneThis over a weekend in the most rudimentary way possible.  I kid you not, we sent the first few batches of daily emails using the BCC field of a Gmail inbox.  The upshot is that we’ve had users on the site from Day 3 of its existence on.
 
We’ve gone from launch in January 2011 when we sent hundreds of emails out per day by hand to sending out over 1 million emails and handling over 200,000 incoming emails per month.  In total, customers have recorded over 1.</p><p>5 0.80117702 <a title="1068-lsi-5" href="../high_scalability-2014/high_scalability-2014-04-08-Microservices_-_Not_a_free_lunch%21.html">1628 high scalability-2014-04-08-Microservices - Not a free lunch!</a></p>
<p>Introduction: This is a guest post by  Benjamin Wootton , CTO of  Contino , a London based consultancy specialising in applying DevOps and Continuous Delivery to software delivery projects.  
 
Microservices are a style of software architecture that involves delivering systems as a set of very small, granular, independent  collaborating services.
 
Though they aren't a particularly new idea, Microservices seem to have exploded in popularity this year, with articles, conference tracks, and Twitter streams waxing lyrical about the benefits of building software systems in this style.
 
This popularity is partly off the back of trends such as Cloud, DevOps and Continuous Delivery coming together as enablers for this kind of approach, and partly off the back of great work at companies such as Netflix who have very visibly applied the pattern to great effect.
 
Let me say up front that I am a fan of the approach.  Microservices architectures have lots of very real and significant benefits:
  
 The service</p><p>6 0.78032857 <a title="1068-lsi-6" href="../high_scalability-2011/high_scalability-2011-07-01-TripAdvisor_Strategy%3A_No_Architects%2C_Engineers_Work_Across_the_Entire_Stack.html">1072 high scalability-2011-07-01-TripAdvisor Strategy: No Architects, Engineers Work Across the Entire Stack</a></p>
<p>7 0.77534795 <a title="1068-lsi-7" href="../high_scalability-2011/high_scalability-2011-09-26-17_Techniques_Used_to_Scale_Turntable.fm_and_Labmeeting_to_Millions_of_Users.html">1124 high scalability-2011-09-26-17 Techniques Used to Scale Turntable.fm and Labmeeting to Millions of Users</a></p>
<p>8 0.77527469 <a title="1068-lsi-8" href="../high_scalability-2012/high_scalability-2012-10-09-Batoo_JPA_-_The_new_JPA_Implementation_that_runs_over_15_times_faster....html">1336 high scalability-2012-10-09-Batoo JPA - The new JPA Implementation that runs over 15 times faster...</a></p>
<p>9 0.77484453 <a title="1068-lsi-9" href="../high_scalability-2013/high_scalability-2013-11-04-ESPN%27s_Architecture_at_Scale_-_Operating_at_100%2C000_Duh_Nuh_Nuhs_Per_Second.html">1542 high scalability-2013-11-04-ESPN's Architecture at Scale - Operating at 100,000 Duh Nuh Nuhs Per Second</a></p>
<p>10 0.7721284 <a title="1068-lsi-10" href="../high_scalability-2012/high_scalability-2012-06-25-StubHub_Architecture%3A_The_Surprising_Complexity_Behind_the_World%E2%80%99s_Largest_Ticket_Marketplace.html">1271 high scalability-2012-06-25-StubHub Architecture: The Surprising Complexity Behind the World’s Largest Ticket Marketplace</a></p>
<p>11 0.76877415 <a title="1068-lsi-11" href="../high_scalability-2013/high_scalability-2013-09-13-Stuff_The_Internet_Says_On_Scalability_For_September_13%2C_2013.html">1516 high scalability-2013-09-13-Stuff The Internet Says On Scalability For September 13, 2013</a></p>
<p>12 0.76868469 <a title="1068-lsi-12" href="../high_scalability-2012/high_scalability-2012-07-16-Cinchcast_Architecture_-_Producing_1%2C500_Hours_of_Audio_Every_Day.html">1284 high scalability-2012-07-16-Cinchcast Architecture - Producing 1,500 Hours of Audio Every Day</a></p>
<p>13 0.76727009 <a title="1068-lsi-13" href="../high_scalability-2013/high_scalability-2013-12-16-22_Recommendations_for_Building_Effective_High_Traffic_Web_Software.html">1565 high scalability-2013-12-16-22 Recommendations for Building Effective High Traffic Web Software</a></p>
<p>14 0.76653504 <a title="1068-lsi-14" href="../high_scalability-2011/high_scalability-2011-02-08-Mollom_Architecture_-_Killing_Over_373_Million_Spams_at_100_Requests_Per_Second.html">985 high scalability-2011-02-08-Mollom Architecture - Killing Over 373 Million Spams at 100 Requests Per Second</a></p>
<p>15 0.76624924 <a title="1068-lsi-15" href="../high_scalability-2009/high_scalability-2009-08-16-ThePort__Network__Architecture.html">682 high scalability-2009-08-16-ThePort  Network  Architecture</a></p>
<p>16 0.7643097 <a title="1068-lsi-16" href="../high_scalability-2013/high_scalability-2013-06-03-GOV.UK_-_Not_Your_Father%27s_Stack.html">1469 high scalability-2013-06-03-GOV.UK - Not Your Father's Stack</a></p>
<p>17 0.76388735 <a title="1068-lsi-17" href="../high_scalability-2014/high_scalability-2014-05-09-Stuff_The_Internet_Says_On_Scalability_For_May_9th%2C_2014.html">1645 high scalability-2014-05-09-Stuff The Internet Says On Scalability For May 9th, 2014</a></p>
<p>18 0.76279742 <a title="1068-lsi-18" href="../high_scalability-2013/high_scalability-2013-12-02-Evolution_of_Bazaarvoice%E2%80%99s_Architecture_to_500M_Unique_Users_Per_Month.html">1557 high scalability-2013-12-02-Evolution of Bazaarvoice’s Architecture to 500M Unique Users Per Month</a></p>
<p>19 0.76144093 <a title="1068-lsi-19" href="../high_scalability-2012/high_scalability-2012-01-09-The_Etsy_Saga%3A_From_Silos_to_Happy_to_Billions_of_Pageviews_a_Month.html">1171 high scalability-2012-01-09-The Etsy Saga: From Silos to Happy to Billions of Pageviews a Month</a></p>
<p>20 0.76122099 <a title="1068-lsi-20" href="../high_scalability-2009/high_scalability-2009-08-31-Squarespace_Architecture_-_A_Grid_Handles_Hundreds_of_Millions_of_Requests_a_Month_.html">691 high scalability-2009-08-31-Squarespace Architecture - A Grid Handles Hundreds of Millions of Requests a Month </a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.17), (2, 0.194), (10, 0.225), (30, 0.031), (40, 0.028), (49, 0.012), (61, 0.077), (77, 0.016), (79, 0.107), (85, 0.048), (94, 0.034)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.98544586 <a title="1068-lda-1" href="../high_scalability-2014/high_scalability-2014-04-21-This_is_why_Microsoft_won._And_why_they_lost..html">1635 high scalability-2014-04-21-This is why Microsoft won. And why they lost.</a></p>
<p>Introduction: My favorite kind of histories are those told from an insider's perspective. The story of Richard the Lionheart is full of great battles and dynastic intrigue. The story of one of his soldiers, not so much. Yet the soldiers' story, as someone who has experienced the real consequences of decisions made and actions taken, is more revealing.
 
We get such a history in  Chat Wars , a wonderful article written by David Auerbach, who in 1998 worked at Microsoft on MSN Messenger Service, Microsoft’s instant messaging app (for a related story see  The Rise and Fall of AIM, the Breakthrough AOL Never Wanted ).
 
It's as if Herodotus visited Microsoft and wrote down his experiences. It has that same sort of conversational tone, insightful on-the-ground observations, and facts no outsider might ever believe.
 
Much of the article is a play-by-play account of the cat and mouse game David plays changing Messenger to track AOL's Instant Messenger protocol changes. AOL repeatedly tried to make it so M</p><p>2 0.98427701 <a title="1068-lda-2" href="../high_scalability-2011/high_scalability-2011-05-23-Evernote_Architecture_-_9_Million_Users_and_150_Million_Requests_a_Day.html">1046 high scalability-2011-05-23-Evernote Architecture - 9 Million Users and 150 Million Requests a Day</a></p>
<p>Introduction: The folks at  Evernote  were kind enough to write up an overview of their architecture in a post titled  Architectural Digest . Dave Engberg describes their approach to networking, sharding, user storage, search, and some other custom services.
 
Evernote is a cool application, partially realizing  Vannevar Bush 's amazing  vision  of a  memex . Wikipedia describes Evernote's features succinctly: 
  

Evernote is a suite of software and services designed for notetaking and archiving. A "note" can be a piece of formattable text, a full webpage or webpage excerpt, a photograph, a voice memo, or a handwritten "ink" note. Notes can also have file attachments. Notes can then be sorted into folders, tagged, annotated, edited, given comments, and searched. Evernote supports a number of operating system platforms (including Android, Mac OS X, iOS, Microsoft Windows and WebOS), and also offers online synchronization and backup services.

  
Key here is that Evernote stores a lot of data, that m</p><p>3 0.98250085 <a title="1068-lda-3" href="../high_scalability-2014/high_scalability-2014-01-24-Stuff_The_Internet_Says_On_Scalability_For_January_24th%2C_2014.html">1585 high scalability-2014-01-24-Stuff The Internet Says On Scalability For January 24th, 2014</a></p>
<p>Introduction: Hey, it's HighScalability time:
     Gorgeous image from Scientific American's  Your Brain by the Numbers    
 Quotable Quotes:                           
 
  @jezhumble : Google does everything off trunk despite 10k devs across 40 offices.  
  @KentLangley : "in 2016. When it goes online, the SKA is expected to produce 700 terabytes of data each day"  
  Jonathan Marks : It's actually a talk about how NOT to be creative. And what he [John Cleese] describes is the way most international broadcasters operated for most of their existence. They were content factories, slave to an artificial transmission schedule. Because they didn't take time to be creative, then ended up sounding like a tape machine. They were run by a computer algorithm. Not a human soul. There was never room for a creative pause. Routine was the solution. And that's creativities biggest enemy. 
 
 
 
  40% better single-threaded performance in MariaDB . Using perf, cache misses were found and the fix was using the righ</p><p>4 0.96805781 <a title="1068-lda-4" href="../high_scalability-2010/high_scalability-2010-03-10-How_FarmVille_Scales_-_The_Follow-up.html">792 high scalability-2010-03-10-How FarmVille Scales - The Follow-up</a></p>
<p>Introduction: Several readers had follow-up questions in response to  How FarmVille Scales to Harvest 75 Million Players a Month . Here are Luke's response to those questions (and a few of mine).
   How does social networking makes things easier or harder?    
 The primary interesting aspect of social networking games is how you wind up with a graph of connected users who need to be access each other's data on a frequent basis. This makes the overall dataset difficult if not impossible to partition. 
   What are examples of the Facebook calls you try to avoid and how they impact game play?   
 We can make a call for facebook friend data to retrieve information about your friends playing the game. Normally, we show a friend ladder at the bottom of the game that shows friend information, including name and facebook photo.   
   Can you say where your cache is, what form it takes, and how much cached there is? Do you have a peering relationship with Facebook, as one might expect at that bandwidth?</p><p>5 0.96529895 <a title="1068-lda-5" href="../high_scalability-2013/high_scalability-2013-09-23-Salesforce_Architecture_-_How_they_Handle_1.3_Billion_Transactions_a_Day.html">1521 high scalability-2013-09-23-Salesforce Architecture - How they Handle 1.3 Billion Transactions a Day</a></p>
<p>Introduction: This is a guest post written by  Claude Johnson , a Lead Site Reliability Engineer at  salesforce.com . 
 
The following is an architectural overview of salesforce.com’s core platform and applications. Other systems such as Heroku's Dyno architecture or the subsystems of other products such as work.com and do.com are specifically not covered by this material, although database.com is. The idea is to share with the technology community some insight about how salesforce.com does what it does. Any mistakes or omissions are mine.
 
This is by no means comprehensive but if there is interest, the author would be happy to tackle other areas of how salesforce.com works. Salesforce.com is interested in being more open with the technology communities that we have not previously interacted with. Here’s to the start of “Opening the Kimono” about how we work.
 
Since 1999, salesforce.com has been singularly focused on building technologies for business that are delivered over the Internet, displaci</p><p>6 0.96484601 <a title="1068-lda-6" href="../high_scalability-2013/high_scalability-2013-06-24-Update_on_How_29_Cloud_Price_Drops_Changed_the_Bottom_Line_of_TripAdvisor_and_Pinterest_-_Results_Mixed.html">1480 high scalability-2013-06-24-Update on How 29 Cloud Price Drops Changed the Bottom Line of TripAdvisor and Pinterest - Results Mixed</a></p>
<p>7 0.96370757 <a title="1068-lda-7" href="../high_scalability-2012/high_scalability-2012-10-02-An_Epic_TripAdvisor_Update%3A_Why_Not_Run_on_the_Cloud%3F_The_Grand_Experiment..html">1331 high scalability-2012-10-02-An Epic TripAdvisor Update: Why Not Run on the Cloud? The Grand Experiment.</a></p>
<p>8 0.96112955 <a title="1068-lda-8" href="../high_scalability-2010/high_scalability-2010-01-27-Hot_Scalability_Links_for_January_28_2010.html">767 high scalability-2010-01-27-Hot Scalability Links for January 28 2010</a></p>
<p>9 0.95828861 <a title="1068-lda-9" href="../high_scalability-2009/high_scalability-2009-04-27-Some_Questions_from_a_newbie.html">584 high scalability-2009-04-27-Some Questions from a newbie</a></p>
<p>10 0.95652109 <a title="1068-lda-10" href="../high_scalability-2007/high_scalability-2007-11-05-Strategy%3A_Diagonal_Scaling_-_Don%27t_Forget_to_Scale_Out_AND_Up.html">142 high scalability-2007-11-05-Strategy: Diagonal Scaling - Don't Forget to Scale Out AND Up</a></p>
<p>11 0.95474213 <a title="1068-lda-11" href="../high_scalability-2008/high_scalability-2008-02-22-Kevin%27s_Great_Adventures_in_SSDland.html">257 high scalability-2008-02-22-Kevin's Great Adventures in SSDland</a></p>
<p>12 0.95030665 <a title="1068-lda-12" href="../high_scalability-2012/high_scalability-2012-11-01-Cost_Analysis%3A_TripAdvisor_and_Pinterest_costs_on_the_AWS_cloud.html">1353 high scalability-2012-11-01-Cost Analysis: TripAdvisor and Pinterest costs on the AWS cloud</a></p>
<p>13 0.94330341 <a title="1068-lda-13" href="../high_scalability-2014/high_scalability-2014-04-14-How_do_you_even_do_anything_without_using_EBS%3F.html">1631 high scalability-2014-04-14-How do you even do anything without using EBS?</a></p>
<p>14 0.93984318 <a title="1068-lda-14" href="../high_scalability-2008/high_scalability-2008-10-26-Should_you_use_a_SAN_to_scale_your_architecture%3F_.html">430 high scalability-2008-10-26-Should you use a SAN to scale your architecture? </a></p>
<p>15 0.93984222 <a title="1068-lda-15" href="../high_scalability-2013/high_scalability-2013-11-05-10_Things_You_Should_Know_About_AWS.html">1543 high scalability-2013-11-05-10 Things You Should Know About AWS</a></p>
<p>same-blog 16 0.93903863 <a title="1068-lda-16" href="../high_scalability-2011/high_scalability-2011-06-27-TripAdvisor_Architecture_-_40M_Visitors%2C_200M_Dynamic_Page_Views%2C_30TB_Data.html">1068 high scalability-2011-06-27-TripAdvisor Architecture - 40M Visitors, 200M Dynamic Page Views, 30TB Data</a></p>
<p>17 0.93182957 <a title="1068-lda-17" href="../high_scalability-2012/high_scalability-2012-12-12-Pinterest_Cut_Costs_from_%2454_to_%2420_Per_Hour_by_Automatically_Shutting_Down_Systems.html">1371 high scalability-2012-12-12-Pinterest Cut Costs from $54 to $20 Per Hour by Automatically Shutting Down Systems</a></p>
<p>18 0.92992938 <a title="1068-lda-18" href="../high_scalability-2011/high_scalability-2011-06-22-It%27s_the_Fraking_IOPS_-_1_SSD_is_44%2C000_IOPS%2C_Hard_Drive_is_180.html">1066 high scalability-2011-06-22-It's the Fraking IOPS - 1 SSD is 44,000 IOPS, Hard Drive is 180</a></p>
<p>19 0.92965651 <a title="1068-lda-19" href="../high_scalability-2013/high_scalability-2013-07-08-The_Architecture_Twitter_Uses_to_Deal_with_150M_Active_Users%2C_300K_QPS%2C_a_22_MB-S_Firehose%2C_and_Send_Tweets_in_Under_5_Seconds.html">1488 high scalability-2013-07-08-The Architecture Twitter Uses to Deal with 150M Active Users, 300K QPS, a 22 MB-S Firehose, and Send Tweets in Under 5 Seconds</a></p>
<p>20 0.91994941 <a title="1068-lda-20" href="../high_scalability-2012/high_scalability-2012-12-10-Switch_your_databases_to_Flash_storage._Now._Or_you%27re_doing_it_wrong..html">1369 high scalability-2012-12-10-Switch your databases to Flash storage. Now. Or you're doing it wrong.</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
