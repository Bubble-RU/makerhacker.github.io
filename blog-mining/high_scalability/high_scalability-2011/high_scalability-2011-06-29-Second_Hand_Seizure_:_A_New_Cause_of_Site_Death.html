<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1070 high scalability-2011-06-29-Second Hand Seizure : A New Cause of Site Death</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2011" href="../home/high_scalability-2011_home.html">high_scalability-2011</a> <a title="high_scalability-2011-1070" href="#">high_scalability-2011-1070</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1070 high scalability-2011-06-29-Second Hand Seizure : A New Cause of Site Death</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2011-1070-html" href="http://highscalability.com//blog/2011/6/29/second-hand-seizure-a-new-cause-of-site-death.html">html</a></p><p>Introduction: Like a digital SWAT team that implodes the wrong door on a raid, the FBI  seized multiple racks of computers  from DigitalOne, these  racks  host websites from many clients that just happened to be in the same racks as whomever they are investigating. Downed sites include Instapaper, Curbed Network, and  Pinboard . With the  density of servers  these days many 1000s of sites could easily have been effected.
 
Sites like Pinboard were victims by association, they did not inhale. This is an association sites have no control over. On a shared hosting service, you have no control over your fellow VM mates. In a cloud or a managed service, you have no control over which racks your servers are in. So like second hand smoke, you get the disease by random association. There's something inherently unfair about that.
 
A  comment by illumin8  shows just how Darth insidious this process can be:
  

A popular method used by hackers is to sign up for a virtual server with a stolen credit card. If t</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Like a digital SWAT team that implodes the wrong door on a raid, the FBI  seized multiple racks of computers  from DigitalOne, these  racks  host websites from many clients that just happened to be in the same racks as whomever they are investigating. [sent-1, score-0.746]
</p><p>2 With the  density of servers  these days many 1000s of sites could easily have been effected. [sent-3, score-0.222]
</p><p>3 This is an association sites have no control over. [sent-5, score-0.264]
</p><p>4 In a cloud or a managed service, you have no control over which racks your servers are in. [sent-7, score-0.253]
</p><p>5 A New Disaster Scenario - Law Enforcement    We are used to data center downtime for more prosaic reasons like natural disasters, power failures, infrastructure meltdown. [sent-13, score-0.156]
</p><p>6 com, which implies the FBI resorts to these measures when the hosting services doesn't cooperate fully with the FBI. [sent-19, score-0.191]
</p><p>7 Do you really want them to fight tooth and nail when it would mean taking down so many unrelated sites? [sent-21, score-0.177]
</p><p>8 To get the servers working in an external lab it would make sense to get the entire rack. [sent-24, score-0.181]
</p><p>9 If data was replicated across multiple data centers would all the data centers be impacted? [sent-30, score-0.177]
</p><p>10 If data was replicated across multiple SANs would all the SANs be taken? [sent-31, score-0.177]
</p><p>11 If you had an architecture that could failover to another data center would that be allowed to run? [sent-32, score-0.259]
</p><p>12 If you run a 1000 servers on AWS with local storage would they take all 1000 servers which would in turn delete all the local storage from your VM mates? [sent-33, score-0.362]
</p><p>13 What if they transitively extend the seizure to all shared resources, like virtual drives and network equipment? [sent-34, score-0.235]
</p><p>14 How Pinboard is Dealing With It   Pinboard is one of the site effected by the seizure and there's a good  Hacker News thread  discussing their professional response. [sent-36, score-0.306]
</p><p>15 There's really no difference between this use case and any other data center failure scenario, stuff can fail at any time, this is just another reason, handle it. [sent-50, score-0.153]
</p><p>16 One possible difference is that a legal disaster could potentially do more damage than any natural disaster. [sent-51, score-0.277]
</p><p>17 Make sure nodes are in multiple racks and if possible multiple availability zones. [sent-58, score-0.377]
</p><p>18 Make sure to have a failure plan and to test that plan. [sent-60, score-0.204]
</p><p>19 Netflix: Continually Test By Failing Servers With Chaos Monkey  - how to test your Second Hand Seizure remedies   To guard against a true death scenario think about true geographical and legal diversity. [sent-61, score-0.501]
</p><p>20 Given that governments cooperate and the recent weakness of the DNS system, this is not a slam dunk, but it would reduce the risks, at a considerable expense. [sent-63, score-0.281]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('fbi', 0.469), ('seizes', 0.235), ('seizure', 0.235), ('pinboard', 0.2), ('racks', 0.182), ('sites', 0.151), ('knocking', 0.142), ('association', 0.113), ('sans', 0.11), ('cooperate', 0.11), ('would', 0.11), ('scenario', 0.107), ('hacking', 0.106), ('disaster', 0.106), ('legal', 0.102), ('center', 0.087), ('hosting', 0.081), ('offline', 0.078), ('true', 0.078), ('test', 0.077), ('vm', 0.072), ('servers', 0.071), ('effected', 0.071), ('verne', 0.071), ('disease', 0.071), ('dunk', 0.071), ('mates', 0.071), ('raided', 0.071), ('natural', 0.069), ('pay', 0.069), ('nail', 0.067), ('agency', 0.067), ('whomever', 0.067), ('restoration', 0.067), ('offsite', 0.067), ('multiple', 0.067), ('digital', 0.066), ('failure', 0.066), ('civil', 0.064), ('smoke', 0.064), ('unfair', 0.064), ('hand', 0.063), ('failover', 0.062), ('unclear', 0.061), ('downed', 0.061), ('stolen', 0.061), ('governments', 0.061), ('understandably', 0.061), ('sure', 0.061), ('guard', 0.059)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="1070-tfidf-1" href="../high_scalability-2011/high_scalability-2011-06-29-Second_Hand_Seizure_%3A_A_New_Cause_of_Site_Death.html">1070 high scalability-2011-06-29-Second Hand Seizure : A New Cause of Site Death</a></p>
<p>Introduction: Like a digital SWAT team that implodes the wrong door on a raid, the FBI  seized multiple racks of computers  from DigitalOne, these  racks  host websites from many clients that just happened to be in the same racks as whomever they are investigating. Downed sites include Instapaper, Curbed Network, and  Pinboard . With the  density of servers  these days many 1000s of sites could easily have been effected.
 
Sites like Pinboard were victims by association, they did not inhale. This is an association sites have no control over. On a shared hosting service, you have no control over your fellow VM mates. In a cloud or a managed service, you have no control over which racks your servers are in. So like second hand smoke, you get the disease by random association. There's something inherently unfair about that.
 
A  comment by illumin8  shows just how Darth insidious this process can be:
  

A popular method used by hackers is to sign up for a virtual server with a stolen credit card. If t</p><p>2 0.17860456 <a title="1070-tfidf-2" href="../high_scalability-2010/high_scalability-2010-12-29-Pinboard.in_Architecture_-_Pay_to_Play_to_Keep_a_System_Small__.html">965 high scalability-2010-12-29-Pinboard.in Architecture - Pay to Play to Keep a System Small  </a></p>
<p>Introduction: How do you keep a system small enough, while still being successful, that a simple scale-up strategy becomes the preferred architecture?  StackOverflow , for example, could stick with a tool chain they were comfortable with because they had a natural brake on how fast they could grow: there are only so many programmers in the world. If this doesn't work for you, here's another natural braking strategy to consider:  charge for your service .  Paul Houle  summarized this nicely as:  avoid scaling problems by building a service that's profitable at a small scale .
 
This interesting point, one I hadn't properly considered before, was brought up by Maciej Ceglowski, co-founder of  Pinboard.in , in an interview with Leo Laporte and Amber MacArthur on their their  net@night  show.
 
Pinboard is a lean, mean, pay for bookmarking machine, a timely replacement for the nearly departed Delicious. And as a self professed anti-social bookmarking site, it  emphasizes speed over socializing . Maciej</p><p>3 0.12510926 <a title="1070-tfidf-3" href="../high_scalability-2007/high_scalability-2007-08-22-How_many_machines_do_you_need_to_run_your_site%3F.html">70 high scalability-2007-08-22-How many machines do you need to run your site?</a></p>
<p>Introduction: Amazingly   TechCrunch   runs their website on one web server and one database server, according to the fascinating survey   What the Webâ&euro;&trade;s most popular sites are running on   by   Pingdom  , a  provider of uptime and response time monitoring.      Early we learned   PlentyOfFish   catches and releases many millions of hits a day on just 1 web server and three database servers.   Google   runs a   Dalek   army full of servers.   YouSendIt  , a company making it easy to send and receive large files, has 24 web servers,  3 database servers, 170 storage servers, and a few miscellaneous servers.   Vimeo  , a video sharing company, has 100 servers for streaming video, 4 web servers, and 2 database servers.   Meebo  , an AJAX based instant messaging company, uses 40 servers to handle messaging, over 40 web servers,  and 10 servers for forums, jabber, testing, and so on.   FeedBurner  , a news feed management company, has 70 web servers, 15 database servers, and 10 miscellaneous servers. Now</p><p>4 0.11196477 <a title="1070-tfidf-4" href="../high_scalability-2009/high_scalability-2009-12-16-Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_in_the_Ambient_Cloud.html">750 high scalability-2009-12-16-Building Super Scalable Systems: Blade Runner Meets Autonomic Computing in the Ambient Cloud</a></p>
<p>Introduction: "But it is not complicated. [There's] just a lot of it." \--Richard Feynmanon
how the immense variety of the world arises from simple rules.Contents:Have We
Reached the End of Scaling?Applications Become Black Boxes Using Markets to
Scale and Control CostsLet's Welcome our Neo-Feudal OverlordsThe Economic
Argument for the Ambient CloudWhat Will Kill the Cloud?The Amazing Collective
Compute Power of the Ambient CloudUsing the Ambient Cloud as an Application
RuntimeApplications as Virtual StatesConclusionWe have not yet begun to scale.
The world is still fundamentally disconnected and for all our wisdom we are
still in the earliest days of learning how to build truly large planet-scaling
applications.Today 350 million users on Facebook is a lot of users and five
million followers on Twitter is a lot of followers. This may seem like a lot
now, but consider we have no planet wide applications yet. None.Tomorrow the
numbers foreshadow a newCambrian explosionof connectivity that will look as</p><p>5 0.11190691 <a title="1070-tfidf-5" href="../high_scalability-2012/high_scalability-2012-11-05-Gone_Fishin%27%3A_Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_In_The_Ambient_Cloud.html">1355 high scalability-2012-11-05-Gone Fishin': Building Super Scalable Systems: Blade Runner Meets Autonomic Computing In The Ambient Cloud</a></p>
<p>Introduction: All in all this is still my favorite post and I still think it's an accurate
vision of a future. Not everyone agrees, but I guess we'll see..."But it is
not complicated. [There's] just a lot of it." \--Richard Feynmanon how the
immense variety of the world arises from simple rules.Contents:Have We Reached
the End of Scaling?Applications Become Black Boxes Using Markets to Scale and
Control CostsLet's Welcome our Neo-Feudal OverlordsThe Economic Argument for
the Ambient CloudWhat Will Kill the Cloud?The Amazing Collective Compute Power
of the Ambient CloudUsing the Ambient Cloud as an Application
RuntimeApplications as Virtual StatesConclusionWe have not yet begun to scale.
The world is still fundamentally disconnected and for all our wisdom we are
still in the earliest days of learning how to build truly large planet-scaling
applications.Today 350 million users on Facebook is a lot of users and five
million followers on Twitter is a lot of followers. This may seem like a lot
now, but c</p><p>6 0.11134319 <a title="1070-tfidf-6" href="../high_scalability-2011/high_scalability-2011-10-24-StackExchange_Architecture_Updates_-_Running_Smoothly%2C_Amazon_4x_More_Expensive.html">1131 high scalability-2011-10-24-StackExchange Architecture Updates - Running Smoothly, Amazon 4x More Expensive</a></p>
<p>7 0.10345817 <a title="1070-tfidf-7" href="../high_scalability-2010/high_scalability-2010-06-14-How_scalable_could_be_a_cPanel_Hosting_service%3F.html">841 high scalability-2010-06-14-How scalable could be a cPanel Hosting service?</a></p>
<p>8 0.10239813 <a title="1070-tfidf-8" href="../high_scalability-2008/high_scalability-2008-01-06-Email_Architecture.html">202 high scalability-2008-01-06-Email Architecture</a></p>
<p>9 0.097160928 <a title="1070-tfidf-9" href="../high_scalability-2014/high_scalability-2014-02-17-How_the_AOL.com_Architecture_Evolved_to_99.999%25_Availability%2C_8_Million_Visitors_Per_Day%2C_and_200%2C000_Requests_Per_Second.html">1597 high scalability-2014-02-17-How the AOL.com Architecture Evolved to 99.999% Availability, 8 Million Visitors Per Day, and 200,000 Requests Per Second</a></p>
<p>10 0.095143825 <a title="1070-tfidf-10" href="../high_scalability-2010/high_scalability-2010-07-08-Cloud_AWS_Infrastructure_vs._Physical_Infrastructure.html">853 high scalability-2010-07-08-Cloud AWS Infrastructure vs. Physical Infrastructure</a></p>
<p>11 0.093547918 <a title="1070-tfidf-11" href="../high_scalability-2010/high_scalability-2010-10-15-Troubles_with_Sharding_-_What_can_we_learn_from_the_Foursquare_Incident%3F.html">920 high scalability-2010-10-15-Troubles with Sharding - What can we learn from the Foursquare Incident?</a></p>
<p>12 0.093330763 <a title="1070-tfidf-12" href="../high_scalability-2007/high_scalability-2007-10-20-Should_you_build_your_next_website_using_3tera%27s_grid_OS%3F.html">126 high scalability-2007-10-20-Should you build your next website using 3tera's grid OS?</a></p>
<p>13 0.092983015 <a title="1070-tfidf-13" href="../high_scalability-2008/high_scalability-2008-11-22-Google_Architecture.html">448 high scalability-2008-11-22-Google Architecture</a></p>
<p>14 0.092230476 <a title="1070-tfidf-14" href="../high_scalability-2007/high_scalability-2007-07-06-Start_Here.html">1 high scalability-2007-07-06-Start Here</a></p>
<p>15 0.091596954 <a title="1070-tfidf-15" href="../high_scalability-2012/high_scalability-2012-06-26-Sponsored_Post%3A_New_Relic%2C_Digital_Ocean%2C_NetDNA%2C_Torbit%2C_Reality_Check_Network%2C_Gigaspaces%2C_AiCache%2C_Logic_Monitor%2C_AppDynamics%2C_CloudSigma%2C_ManageEnine%2C_Site24x7.html">1272 high scalability-2012-06-26-Sponsored Post: New Relic, Digital Ocean, NetDNA, Torbit, Reality Check Network, Gigaspaces, AiCache, Logic Monitor, AppDynamics, CloudSigma, ManageEnine, Site24x7</a></p>
<p>16 0.089381143 <a title="1070-tfidf-16" href="../high_scalability-2008/high_scalability-2008-02-25-Any_Suggestions_for_the_Architecture_Template%3F.html">259 high scalability-2008-02-25-Any Suggestions for the Architecture Template?</a></p>
<p>17 0.089381143 <a title="1070-tfidf-17" href="../high_scalability-2008/high_scalability-2008-02-25-Architecture_Template_Advice_Needed.html">260 high scalability-2008-02-25-Architecture Template Advice Needed</a></p>
<p>18 0.088844351 <a title="1070-tfidf-18" href="../high_scalability-2010/high_scalability-2010-04-28-Elasticity_for_the_Enterprise_--_Ensuring_Continuous_High_Availability_in_a_Disaster_Failure_Scenario.html">816 high scalability-2010-04-28-Elasticity for the Enterprise -- Ensuring Continuous High Availability in a Disaster Failure Scenario</a></p>
<p>19 0.088309698 <a title="1070-tfidf-19" href="../high_scalability-2012/high_scalability-2012-06-05-Sponsored_Post%3A_Digital_Ocean%2C_NetDNA%2C_Torbit%2C_Velocity%2C_Reality_Check_Network%2C_Gigaspaces%2C_AiCache%2C_Logic_Monitor%2C_Attribution_Modeling%2C_AppDynamics%2C_CloudSigma%2C_ManageEnine%2C_Site24x7.html">1257 high scalability-2012-06-05-Sponsored Post: Digital Ocean, NetDNA, Torbit, Velocity, Reality Check Network, Gigaspaces, AiCache, Logic Monitor, Attribution Modeling, AppDynamics, CloudSigma, ManageEnine, Site24x7</a></p>
<p>20 0.087253056 <a title="1070-tfidf-20" href="../high_scalability-2010/high_scalability-2010-01-17-Applications_Become_Black_Boxes_Using_Markets_to_Scale_and_Control_Costs.html">761 high scalability-2010-01-17-Applications Become Black Boxes Using Markets to Scale and Control Costs</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.185), (1, 0.077), (2, 0.003), (3, -0.038), (4, -0.042), (5, -0.074), (6, 0.021), (7, -0.053), (8, 0.013), (9, -0.014), (10, -0.028), (11, -0.011), (12, -0.02), (13, -0.036), (14, 0.079), (15, 0.005), (16, 0.034), (17, 0.002), (18, -0.002), (19, 0.025), (20, -0.016), (21, 0.003), (22, -0.007), (23, -0.001), (24, -0.056), (25, -0.021), (26, 0.013), (27, 0.022), (28, -0.019), (29, 0.002), (30, -0.009), (31, -0.004), (32, 0.013), (33, -0.063), (34, 0.011), (35, 0.02), (36, 0.033), (37, 0.062), (38, -0.012), (39, 0.042), (40, 0.045), (41, -0.041), (42, -0.004), (43, 0.03), (44, 0.051), (45, -0.001), (46, -0.021), (47, -0.02), (48, -0.021), (49, -0.039)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96241003 <a title="1070-lsi-1" href="../high_scalability-2011/high_scalability-2011-06-29-Second_Hand_Seizure_%3A_A_New_Cause_of_Site_Death.html">1070 high scalability-2011-06-29-Second Hand Seizure : A New Cause of Site Death</a></p>
<p>Introduction: Like a digital SWAT team that implodes the wrong door on a raid, the FBI  seized multiple racks of computers  from DigitalOne, these  racks  host websites from many clients that just happened to be in the same racks as whomever they are investigating. Downed sites include Instapaper, Curbed Network, and  Pinboard . With the  density of servers  these days many 1000s of sites could easily have been effected.
 
Sites like Pinboard were victims by association, they did not inhale. This is an association sites have no control over. On a shared hosting service, you have no control over your fellow VM mates. In a cloud or a managed service, you have no control over which racks your servers are in. So like second hand smoke, you get the disease by random association. There's something inherently unfair about that.
 
A  comment by illumin8  shows just how Darth insidious this process can be:
  

A popular method used by hackers is to sign up for a virtual server with a stolen credit card. If t</p><p>2 0.85931021 <a title="1070-lsi-2" href="../high_scalability-2007/high_scalability-2007-07-24-Major_Websites_Down%3A_Or_Why_You_Want_to_Run_in_Two_or_More_Data_Centers..html">23 high scalability-2007-07-24-Major Websites Down: Or Why You Want to Run in Two or More Data Centers.</a></p>
<p>Introduction: A lot of sites hosted in San Francisco are down because of at least 6 back-to-back power outages power outages. More details at  laughingsquid .
   
Sites like SecondLife, Craigstlist, Technorati, Yelp and all Six Apart properties, TypePad, LiveJournal and Vox are all down. The cause was an underground explosion in a transformer vault under a manhole at 560 Mission Street. Flames shot 6 feet out from the manhole cover. Over PG&E; 30,000 customers are without power.  What's perplexing is the UPS backup and diesel generators didn't kick in to bring the datacenter back on line. I've never toured that datacenter, but they usually have massive backup systems. It's probably one of those multiple simultaneous failure situations that you hope never happen in real life, but too often do. Or maybe the infrastructure wasn't rolled out completely.  Update: the cause was a cascade of failures in a tightly couples system that could never happen :-) Details at  Failure Happens: A summary of the power</p><p>3 0.79407889 <a title="1070-lsi-3" href="../high_scalability-2008/high_scalability-2008-10-26-Should_you_use_a_SAN_to_scale_your_architecture%3F_.html">430 high scalability-2008-10-26-Should you use a SAN to scale your architecture? </a></p>
<p>Introduction: This is a question everyone must struggle with when building out their datacenter. Storage choices are always the ones I have the least confidence in.  David Marks in his blog  You Can Change It Later!  asks the question   Should I get a SAN to scale my site architecture?   and answers no. A better solution is to use commodity hardware, directly attach storage on servers, and partition across servers to scale and for greater availability.  David's reasoning is interesting:
  A SAN creates a SPOF (single point of failure) that is dependent on a vendor to fly and fix when there's a problem. This can lead to long down times during this outage you have no access to your data at all.   Using easily available commodity hardware minimizes risks to your company, it's not just about saving money. Zooming over to Fry's to buy emergency equipment provides the kind of agility startups need in order to respond quickly to ever changing situations.  It's hard to beat the power and flexibility (backup</p><p>4 0.77827704 <a title="1070-lsi-4" href="../high_scalability-2011/high_scalability-2011-10-24-StackExchange_Architecture_Updates_-_Running_Smoothly%2C_Amazon_4x_More_Expensive.html">1131 high scalability-2011-10-24-StackExchange Architecture Updates - Running Smoothly, Amazon 4x More Expensive</a></p>
<p>Introduction: We've had a few articles on the  StackOverlflow Architecture  and  Stack Overflow Architecture Update - Now at 95 Million Page Views a Month . Time for another update. This time from a podcast. Every week or so Jeff, Joel and guests sit around and converse. The result is a  podcast . In a recent  podcast  they talked about some of their recent architecture issues, problems, and updates. And since I wrote this article before my vacation, they've also published a new architecture update article:  The Stack Exchange Architecture – 2011 Edition, Episode 1 .
 
My overall impression is they are in a comfortable place, adding new sites, adding new features, making a house a home.
 
Notable for their scale-up architecture, you might expect with their growth that they would slam into a wall. Not so. They've been able to scale-up the power of individual servers by adding more CPU and RAM. SSD has been added in some cases. Even their flagship StackOverflow product runs on a single server. New mac</p><p>5 0.745983 <a title="1070-lsi-5" href="../high_scalability-2007/high_scalability-2007-08-29-Skype_Failed_the_Boot_Scalability_Test%3A_Is_P2P_fundamentally_flawed%3F.html">76 high scalability-2007-08-29-Skype Failed the Boot Scalability Test: Is P2P fundamentally flawed?</a></p>
<p>Introduction: Skype's 220 millions users lost service for a stunning two days. The primary cause for Skype's nightmare (can you imagine the beeper storm that went off?) was a massive global roll-out of a  Window's patch  triggering the simultaneous reboot of millions of machines across the globe.  The secondary cause was a bug in Skype's software that prevented "self-healing" in the face of such attacks. The flood of log-in requests and a lack of "peer-to-peer resources" melted their system.
   
Who's fault is it? Is Skype to blame? Is Microsoft to blame? Or is the peer-to-peer model itself fundamentally flawed in some way?  Let's be real, how could Skype possibly test booting 220 million servers over a random configuration of resources? Answer: they can't. Yes, it's Skype's responsibility, but they are in a bit of a pickle on this one.  The boot scenario is one of the most basic and one of the most difficult scalability scenarios to plan for and test. You can't simulate the viciousness of real-life</p><p>6 0.74363279 <a title="1070-lsi-6" href="../high_scalability-2010/high_scalability-2010-04-28-Elasticity_for_the_Enterprise_--_Ensuring_Continuous_High_Availability_in_a_Disaster_Failure_Scenario.html">816 high scalability-2010-04-28-Elasticity for the Enterprise -- Ensuring Continuous High Availability in a Disaster Failure Scenario</a></p>
<p>7 0.74025452 <a title="1070-lsi-7" href="../high_scalability-2011/high_scalability-2011-08-15-Should_any_cloud_be_considered_one_availability_zone%3F_The_Amazon_experience_says_yes..html">1098 high scalability-2011-08-15-Should any cloud be considered one availability zone? The Amazon experience says yes.</a></p>
<p>8 0.73780096 <a title="1070-lsi-8" href="../high_scalability-2014/high_scalability-2014-01-06-How_HipChat_Stores_and_Indexes_Billions_of_Messages_Using_ElasticSearch_and_Redis.html">1573 high scalability-2014-01-06-How HipChat Stores and Indexes Billions of Messages Using ElasticSearch and Redis</a></p>
<p>9 0.73180848 <a title="1070-lsi-9" href="../high_scalability-2007/high_scalability-2007-09-18-Amazon_Architecture.html">96 high scalability-2007-09-18-Amazon Architecture</a></p>
<p>10 0.73148197 <a title="1070-lsi-10" href="../high_scalability-2014/high_scalability-2014-02-03-How_Google_Backs_Up_the_Internet_Along_With_Exabytes_of_Other_Data.html">1589 high scalability-2014-02-03-How Google Backs Up the Internet Along With Exabytes of Other Data</a></p>
<p>11 0.7275216 <a title="1070-lsi-11" href="../high_scalability-2007/high_scalability-2007-10-20-Should_you_build_your_next_website_using_3tera%27s_grid_OS%3F.html">126 high scalability-2007-10-20-Should you build your next website using 3tera's grid OS?</a></p>
<p>12 0.72641838 <a title="1070-lsi-12" href="../high_scalability-2007/high_scalability-2007-10-30-Paper%3A_Dynamo%3A_Amazon%E2%80%99s_Highly_Available_Key-value_Store.html">139 high scalability-2007-10-30-Paper: Dynamo: Amazon’s Highly Available Key-value Store</a></p>
<p>13 0.72199607 <a title="1070-lsi-13" href="../high_scalability-2007/high_scalability-2007-07-25-Product%3A_NetApp_MetroCluster_Software.html">28 high scalability-2007-07-25-Product: NetApp MetroCluster Software</a></p>
<p>14 0.72058558 <a title="1070-lsi-14" href="../high_scalability-2008/high_scalability-2008-03-04-Manage_Downtime_Risk_by_Connecting_Multiple_Data_Centers_into_a_Secure_Virtual_LAN.html">266 high scalability-2008-03-04-Manage Downtime Risk by Connecting Multiple Data Centers into a Secure Virtual LAN</a></p>
<p>15 0.71912044 <a title="1070-lsi-15" href="../high_scalability-2012/high_scalability-2012-03-14-The_Azure_Outage%3A_Time_Is_a_SPOF%2C_Leap_Day_Doubly_So.html">1209 high scalability-2012-03-14-The Azure Outage: Time Is a SPOF, Leap Day Doubly So</a></p>
<p>16 0.71598858 <a title="1070-lsi-16" href="../high_scalability-2009/high_scalability-2009-08-31-Squarespace_Architecture_-_A_Grid_Handles_Hundreds_of_Millions_of_Requests_a_Month_.html">691 high scalability-2009-08-31-Squarespace Architecture - A Grid Handles Hundreds of Millions of Requests a Month </a></p>
<p>17 0.7157321 <a title="1070-lsi-17" href="../high_scalability-2011/high_scalability-2011-08-26-Stuff_The_Internet_Says_On_Scalability_For_August_26%2C_2011.html">1106 high scalability-2011-08-26-Stuff The Internet Says On Scalability For August 26, 2011</a></p>
<p>18 0.71553254 <a title="1070-lsi-18" href="../high_scalability-2009/high_scalability-2009-07-28-37signals_Architecture.html">663 high scalability-2009-07-28-37signals Architecture</a></p>
<p>19 0.71525991 <a title="1070-lsi-19" href="../high_scalability-2014/high_scalability-2014-03-03-The_%E2%80%9CFour_Hamiltons%E2%80%9D_Framework_for_Mitigating_Faults_in_the_Cloud%3A_Avoid_it%2C_Mask_it%2C_Bound_it%2C_Fix_it_Fast.html">1604 high scalability-2014-03-03-The “Four Hamiltons” Framework for Mitigating Faults in the Cloud: Avoid it, Mask it, Bound it, Fix it Fast</a></p>
<p>20 0.7133745 <a title="1070-lsi-20" href="../high_scalability-2007/high_scalability-2007-11-26-Scale_to_China.html">165 high scalability-2007-11-26-Scale to China</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.116), (2, 0.19), (10, 0.059), (13, 0.214), (30, 0.015), (37, 0.026), (40, 0.016), (47, 0.01), (61, 0.078), (63, 0.015), (79, 0.092), (85, 0.037), (94, 0.046)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.89062315 <a title="1070-lda-1" href="../high_scalability-2012/high_scalability-2012-07-12-4_Strategies_for_Punching_Down_Traffic_Spikes.html">1282 high scalability-2012-07-12-4 Strategies for Punching Down Traffic Spikes</a></p>
<p>Introduction: Travis Reeder in  Spikability - An Application's Ability to Handle Unknown and/or Inconsistent Load  gives four good ways of handling spikey loads:
  
  Have more resources than you'll ever need . Estimate the maximum traffic you'll need and keep that many servers running. Downside is you are paying for capacity you aren't using. 
  Disable features during high loads . Reduce load by disabling features or substituting in lighter weight features. Downside is users to have access to features. 
  Auto scaling . Launch new servers in response to load. Downsides are it's complicated to setup and slow to respond. Random spikes will cause cycling of instances going up and down. 
  Use message queues . Queues soak up work requests during traffic spikes. More servers can be started to process work from the queue. Resources aren't wasted and features are disabled. Downside is increased latency.</p><p>2 0.88357073 <a title="1070-lda-2" href="../high_scalability-2007/high_scalability-2007-07-27-Product%3A_Munin_Monitoriting_Tool.html">34 high scalability-2007-07-27-Product: Munin Monitoriting Tool</a></p>
<p>Introduction: Munin  the monitoring tool surveys all your computers and remembers what it saw. It presents all the information in graphs through a web interface. Its emphasis is on plug and play capabilities. After completing a installation a high number of monitoring plugins will be playing with no more effort.  Using Munin you can easily monitor the performance of your computers, networks, SANs, applications, weather measurements and whatever comes to mind. It makes it easy to determine "what's different today" when a performance problem crops up. It makes it easy to see how you're doing capacity-wise on any resources.</p><p>3 0.88345987 <a title="1070-lda-3" href="../high_scalability-2008/high_scalability-2008-12-29-100%25_on_Amazon_Web_Services%3A_Soocial.com_-_a_lesson_of_porting_your_service_to_Amazon.html">477 high scalability-2008-12-29-100% on Amazon Web Services: Soocial.com - a lesson of porting your service to Amazon</a></p>
<p>Introduction: Simone Brunozzi, technology evangelist for Amazon Web Services in Europe, describes how Soocial.com was fully ported to Amazon web services.         ----------------   This period of the year I decided to dedicate some time to better understand how our customers use AWS, therefore I spent some online time with Stefan Fountain and the nice guys at Soocial.com, a "one address book solution to contact management", and I would like to share with you some details of their IT infrastructure, which now runs 100% on Amazon Web Services!    In the last few months, they've been working hard to cope with tens of thousands of users and to get ready to easily scale to millions. To make this possible, they decided to move ALL their architecture to Amazon Web Services. Despite the fact that they were quite happy with their previous hosting provider, Amazon proved to be the way to go.    -----------------           Read the rest of the article here .</p><p>4 0.86655021 <a title="1070-lda-4" href="../high_scalability-2010/high_scalability-2010-01-11-Have_We_Reached_the_End_of_Scaling%3F.html">758 high scalability-2010-01-11-Have We Reached the End of Scaling?</a></p>
<p>Introduction: This is an excerpt from my article  Building Super Scalable Systems: Blade Runner Meets Autonomic Computing in the Ambient Cloud.  
 
Have we reached the end of scaling? That's what I asked myself one day after noticing a bunch of "The End of" headlines. We've reached  The End of History  because the Western liberal democracy is the "end point of humanity's sociocultural evolution and the final form of human government."  We've reached  The End of Science  because of the "fact that there aren't going to be any obvious, cataclysmic revolutions." We've even reached  The End of Theory  because all answers can be found in the continuous stream of data we're collecting. And doesn't always seem like we're at  The End of the World ?
 
Motivated by the prospect of everything ending, I began to wonder: have we really reached The End of Scaling?
 
For a while I thought this might be true. The reason I thought the End of Scaling might be near is because of the slow down of potential articles at m</p><p>same-blog 5 0.8645727 <a title="1070-lda-5" href="../high_scalability-2011/high_scalability-2011-06-29-Second_Hand_Seizure_%3A_A_New_Cause_of_Site_Death.html">1070 high scalability-2011-06-29-Second Hand Seizure : A New Cause of Site Death</a></p>
<p>Introduction: Like a digital SWAT team that implodes the wrong door on a raid, the FBI  seized multiple racks of computers  from DigitalOne, these  racks  host websites from many clients that just happened to be in the same racks as whomever they are investigating. Downed sites include Instapaper, Curbed Network, and  Pinboard . With the  density of servers  these days many 1000s of sites could easily have been effected.
 
Sites like Pinboard were victims by association, they did not inhale. This is an association sites have no control over. On a shared hosting service, you have no control over your fellow VM mates. In a cloud or a managed service, you have no control over which racks your servers are in. So like second hand smoke, you get the disease by random association. There's something inherently unfair about that.
 
A  comment by illumin8  shows just how Darth insidious this process can be:
  

A popular method used by hackers is to sign up for a virtual server with a stolen credit card. If t</p><p>6 0.85704839 <a title="1070-lda-6" href="../high_scalability-2014/high_scalability-2014-01-10-Stuff_The_Internet_Says_On_Scalability_For_January_10th%2C_2014.html">1576 high scalability-2014-01-10-Stuff The Internet Says On Scalability For January 10th, 2014</a></p>
<p>7 0.85664803 <a title="1070-lda-7" href="../high_scalability-2008/high_scalability-2008-09-23-The_7_Stages_of_Scaling_Web_Apps.html">391 high scalability-2008-09-23-The 7 Stages of Scaling Web Apps</a></p>
<p>8 0.85057569 <a title="1070-lda-8" href="../high_scalability-2007/high_scalability-2007-10-04-You_Can_Now_Store_All_Your_Stuff_on_Your_Own_Google_Like_File_System.html">112 high scalability-2007-10-04-You Can Now Store All Your Stuff on Your Own Google Like File System</a></p>
<p>9 0.81871277 <a title="1070-lda-9" href="../high_scalability-2013/high_scalability-2013-03-29-Stuff_The_Internet_Says_On_Scalability_For_March_29%2C_2013.html">1431 high scalability-2013-03-29-Stuff The Internet Says On Scalability For March 29, 2013</a></p>
<p>10 0.8165226 <a title="1070-lda-10" href="../high_scalability-2009/high_scalability-2009-07-29-Strategy%3A_Devirtualize_for_More_Vroom.html">664 high scalability-2009-07-29-Strategy: Devirtualize for More Vroom</a></p>
<p>11 0.80681348 <a title="1070-lda-11" href="../high_scalability-2013/high_scalability-2013-04-25-Paper%3A_Making_reliable_distributed_systems_in_the_presence_of_software_errors.html">1446 high scalability-2013-04-25-Paper: Making reliable distributed systems in the presence of software errors</a></p>
<p>12 0.8052603 <a title="1070-lda-12" href="../high_scalability-2010/high_scalability-2010-09-01-Paper%3A_The_Case_for_Determinism_in_Database_Systems__.html">890 high scalability-2010-09-01-Paper: The Case for Determinism in Database Systems  </a></p>
<p>13 0.77739072 <a title="1070-lda-13" href="../high_scalability-2007/high_scalability-2007-08-22-Wikimedia_architecture.html">72 high scalability-2007-08-22-Wikimedia architecture</a></p>
<p>14 0.77722418 <a title="1070-lda-14" href="../high_scalability-2009/high_scalability-2009-01-20-Product%3A_Amazon%27s_SimpleDB.html">498 high scalability-2009-01-20-Product: Amazon's SimpleDB</a></p>
<p>15 0.77499741 <a title="1070-lda-15" href="../high_scalability-2014/high_scalability-2014-05-16-Stuff_The_Internet_Says_On_Scalability_For_May_16th%2C_2014.html">1649 high scalability-2014-05-16-Stuff The Internet Says On Scalability For May 16th, 2014</a></p>
<p>16 0.77433407 <a title="1070-lda-16" href="../high_scalability-2010/high_scalability-2010-02-10-ElasticSearch_-_Open_Source%2C_Distributed%2C_RESTful_Search_Engine.html">775 high scalability-2010-02-10-ElasticSearch - Open Source, Distributed, RESTful Search Engine</a></p>
<p>17 0.77325654 <a title="1070-lda-17" href="../high_scalability-2014/high_scalability-2014-02-21-Stuff_The_Internet_Says_On_Scalability_For_February_21st%2C_2014.html">1600 high scalability-2014-02-21-Stuff The Internet Says On Scalability For February 21st, 2014</a></p>
<p>18 0.77275568 <a title="1070-lda-18" href="../high_scalability-2012/high_scalability-2012-03-14-The_Azure_Outage%3A_Time_Is_a_SPOF%2C_Leap_Day_Doubly_So.html">1209 high scalability-2012-03-14-The Azure Outage: Time Is a SPOF, Leap Day Doubly So</a></p>
<p>19 0.77271456 <a title="1070-lda-19" href="../high_scalability-2011/high_scalability-2011-09-07-What_Google_App_Engine_Price_Changes_Say_About_the_Future_of_Web_Architecture.html">1112 high scalability-2011-09-07-What Google App Engine Price Changes Say About the Future of Web Architecture</a></p>
<p>20 0.7723816 <a title="1070-lda-20" href="../high_scalability-2014/high_scalability-2014-04-25-Stuff_The_Internet_Says_On_Scalability_For_April_25th%2C_2014.html">1637 high scalability-2014-04-25-Stuff The Internet Says On Scalability For April 25th, 2014</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
