<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>983 high scalability-2011-02-02-Piccolo - Building Distributed Programs that are 11x Faster than Hadoop</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2011" href="../home/high_scalability-2011_home.html">high_scalability-2011</a> <a title="high_scalability-2011-983" href="#">high_scalability-2011-983</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>983 high scalability-2011-02-02-Piccolo - Building Distributed Programs that are 11x Faster than Hadoop</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2011-983-html" href="http://highscalability.com//blog/2011/2/2/piccolo-building-distributed-programs-that-are-11x-faster-th.html">html</a></p><p>Introduction: Piccolo  (not  this  or  this ) is a system for distributed computing, Piccolo is a n ew data-centric programming model for writing parallel in-memory applications in data centers .  Unlike existing data-ﬂow models, Piccolo allows computation running on different machines to share distributed, mutable state via a key-value table interface. T  raditional data-centric models (such as Hadoop) which present the user a single object at a time to operate on, Piccolo exposes a global table interface which is available to all parts of the computation simultaneously. This allows users to specify programs in an intuitive manner very similar to that of writing programs for a single machine. 
 
Using an in-memory key-value store is a very different approach from the canonical map-reduce, which is based on using distributed file systems. The results are impressive:
  

Experiments have shown that Piccolo is fast and pro-vides excellent scaling for many applications. The performance of PageRank and</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Piccolo  (not  this  or  this ) is a system for distributed computing, Piccolo is a n ew data-centric programming model for writing parallel in-memory applications in data centers . [sent-1, score-0.061]
</p><p>2 Unlike existing data-ﬂow models, Piccolo allows computation running on different machines to share distributed, mutable state via a key-value table interface. [sent-2, score-0.439]
</p><p>3 T  raditional data-centric models (such as Hadoop) which present the user a single object at a time to operate on, Piccolo exposes a global table interface which is available to all parts of the computation simultaneously. [sent-3, score-0.21]
</p><p>4 This allows users to specify programs in an intuitive manner very similar to that of writing programs for a single machine. [sent-4, score-0.27]
</p><p>5 Using an in-memory key-value store is a very different approach from the canonical map-reduce, which is based on using distributed file systems. [sent-5, score-0.061]
</p><p>6 Our distributed webcrawler can easily saturate a 100 Mbps internet uplink when running on 12 machines. [sent-9, score-0.193]
</p><p>7 In Piccolo, programmers organize the computation around a series of application ker-nel functions, where each kernel is launched as multi-ple instances concurrently executing on many compute nodes. [sent-13, score-0.227]
</p><p>8 Kernel instances share distributed, mutable state using a set of in-memory tables whose entries reside in the memory of different compute nodes. [sent-14, score-0.404]
</p><p>9 Kernel instances share state exclusively via the key-value table interface with get and put primitives. [sent-15, score-0.39]
</p><p>10 The underlying Piccolo run-time sends messages to read and modify table entries stored in the memory of remote nodes. [sent-16, score-0.179]
</p><p>11 First, it allows for natural and efficient implementations for applications that require sharing of intermediate state such as k-means computation, n-body simulation, PageRank calculation etc. [sent-18, score-0.122]
</p><p>12 For example, a distributed crawler can learn of newly discovered pages quickly as a result of state up-dates done by ongoing web crawls. [sent-20, score-0.184]
</p><p>13 Piccolo includes a number of optimizations to ensure that using this table interface is not just easy, but also fast (taken from the home page):           Locality  - To ensure locality of execution, tables are explicitly partitioned across machines. [sent-21, score-0.546]
</p><p>14 User code that interacts with the tables can specify a locality preference: this ensures that the code is executed locally with the data it is accessing. [sent-22, score-0.277]
</p><p>15 Load-balancing  - Not all load is created equal - often some partition of a computation will take much longer then others. [sent-23, score-0.106]
</p><p>16 To address this Piccolo can migrate tasks away from busy machines to take advantage of otherwise idle workers, all while preserving the locality preferences and the correctness of the program. [sent-25, score-0.246]
</p><p>17 Piccolo makes checkpointing and restoration easy and fast, allowing for quick recovery in case of failures. [sent-27, score-0.13]
</p><p>18 Synchronization  - Managing the correct synchronization and update across a distributed system can be complicated and slow. [sent-28, score-0.116]
</p><p>19 Piccolo addresses this by allowing users to defer synchronization logic to the system. [sent-29, score-0.135]
</p><p>20 Instead of explicitly locking tables in order to perform updates, users can attach accumulation functions to a table: these are used automatically by the framework to correctly combine concurrent updates to a table entry. [sent-30, score-0.439]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('piccolo', 0.878), ('table', 0.12), ('computation', 0.106), ('tables', 0.095), ('locality', 0.091), ('state', 0.084), ('pagerank', 0.076), ('kernel', 0.074), ('mutable', 0.068), ('explicitly', 0.066), ('programs', 0.063), ('distributed', 0.061), ('entries', 0.059), ('synchronization', 0.055), ('specify', 0.054), ('writing', 0.052), ('share', 0.051), ('interface', 0.051), ('webcrawler', 0.05), ('instances', 0.047), ('avideo', 0.047), ('accumulation', 0.047), ('modi', 0.047), ('restoration', 0.047), ('partitioned', 0.046), ('checkpointing', 0.043), ('ow', 0.043), ('saturate', 0.043), ('preferences', 0.042), ('home', 0.041), ('defer', 0.04), ('wastes', 0.04), ('machines', 0.04), ('allowing', 0.04), ('functions', 0.039), ('uplink', 0.039), ('preference', 0.039), ('exposing', 0.039), ('models', 0.039), ('crawler', 0.039), ('allows', 0.038), ('preserving', 0.038), ('exclusively', 0.037), ('interacts', 0.037), ('mbps', 0.036), ('attach', 0.036), ('updates', 0.036), ('fast', 0.036), ('ed', 0.036), ('correctness', 0.035)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="983-tfidf-1" href="../high_scalability-2011/high_scalability-2011-02-02-Piccolo_-_Building_Distributed_Programs_that_are_11x_Faster_than_Hadoop.html">983 high scalability-2011-02-02-Piccolo - Building Distributed Programs that are 11x Faster than Hadoop</a></p>
<p>Introduction: Piccolo  (not  this  or  this ) is a system for distributed computing, Piccolo is a n ew data-centric programming model for writing parallel in-memory applications in data centers .  Unlike existing data-ﬂow models, Piccolo allows computation running on different machines to share distributed, mutable state via a key-value table interface. T  raditional data-centric models (such as Hadoop) which present the user a single object at a time to operate on, Piccolo exposes a global table interface which is available to all parts of the computation simultaneously. This allows users to specify programs in an intuitive manner very similar to that of writing programs for a single machine. 
 
Using an in-memory key-value store is a very different approach from the canonical map-reduce, which is based on using distributed file systems. The results are impressive:
  

Experiments have shown that Piccolo is fast and pro-vides excellent scaling for many applications. The performance of PageRank and</p><p>2 0.067648828 <a title="983-tfidf-2" href="../high_scalability-2008/high_scalability-2008-11-22-Google_Architecture.html">448 high scalability-2008-11-22-Google Architecture</a></p>
<p>Introduction: Update 2:   Sorting 1 PB with MapReduce . PB is not peanut-butter-and-jelly misspelled. It's 1 petabyte or 1000 terabytes or 1,000,000 gigabytes.  It took six hours and two minutes to sort 1PB (10 trillion 100-byte records) on 4,000 computers  and the results were replicated thrice on 48,000 disks.  Update:   Greg Linden  points to a new Google article  MapReduce: simplified data processing on large clusters . Some interesting stats: 100k MapReduce jobs are executed each day; more than 20 petabytes of data are processed per day; more than 10k MapReduce programs have been implemented; machines are dual processor with gigabit ethernet and 4-8 GB of memory.  Google is the King of scalability.  Everyone knows Google for their large,  sophisticated, and fast searching, but they don't just shine in search. Their platform approach to building scalable applications allows them to roll out internet scale applications at an alarmingly high competition crushing rate. Their goal is always to build</p><p>3 0.066583909 <a title="983-tfidf-3" href="../high_scalability-2009/high_scalability-2009-03-16-Are_Cloud_Based_Memory_Architectures_the_Next_Big_Thing%3F.html">538 high scalability-2009-03-16-Are Cloud Based Memory Architectures the Next Big Thing?</a></p>
<p>Introduction: We are on the edge of two potent technological changes: Clouds and Memory Based Architectures. This evolution will rip open a chasm where new players can enter and prosper. Google is the master of disk. You can't beat them at a game they perfected. Disk based databases like SimpleDB and BigTable are complicated beasts, typical last gasp products of any aging technology before a change. The next era is the age of Memory and Cloud which will allow for new players to succeed. The tipping point will be soon.   Let's take a short trip down web architecture lane:
  It's 1993: Yahoo runs on FreeBSD, Apache, Perl scripts and a SQL database   It's 1995: Scale-up the database.   It's 1998: LAMP   It's 1999: Stateless + Load Balanced + Database + SAN   It's 2001: In-memory data-grid.   It's 2003: Add a caching layer.   It's 2004: Add scale-out and partitioning.   It's 2005: Add asynchronous job scheduling and maybe a distributed file system.   It's 2007: Move it all into the cloud.   It's 2008: C</p><p>4 0.064648539 <a title="983-tfidf-4" href="../high_scalability-2011/high_scalability-2011-04-14-Strategy%3A_Cache_Application_Start_State_to_Reduce_Spin-up_Times.html">1023 high scalability-2011-04-14-Strategy: Cache Application Start State to Reduce Spin-up Times</a></p>
<p>Introduction: Using this strategy, Valyala, a commenter on  Are Long VM Instance Spin-Up Times In The Cloud Costing You Money? , was able to reduce their GAE application start-up times from 15 seconds down to to 1.5 seconds:
  

Spin-up time for newly added Google AppEngine instances can be reduced using initial state caching. Usually the majority of spin-up time for the newly created GAE instance is spent in the pre-populating of the initial state, which is created from many data pieces loaded from slow data sources such as GAE's datastore. If the initial state is identical among GAE instances, then the entire state can be serialized and stored in a shared memory (either in the memcache or in the datastore) by the first created instance, so newly created instances could load and quickly unserialize the state from a single blob loaded from shared memory instead of spending a lot of time for creation of the state from multiple data pieces loaded from the datastore.


I reduced spin-up time for new in</p><p>5 0.063585259 <a title="983-tfidf-5" href="../high_scalability-2009/high_scalability-2009-07-30-Learn_How_to_Think_at_Scale.html">666 high scalability-2009-07-30-Learn How to Think at Scale</a></p>
<p>Introduction: Aaron Kimball of Cloudera gives a wonderful 23 minute presentation titled  Cloudera Hadoop Training: Thinking at Scale Cloudera  which talks about "common challenges and general best practices for scaling with your data." As a company Cloudera offers "enterprise-level support to users of Apache Hadoop." Part of that offering is a really useful series of  tutorial videos on the Hadoop ecosystem .   Like TV lawyer Perry Mason (or is it Harmon Rabb?), Aaron gradually builds his case. He opens with the problem of storing lots of data. Then a blistering cross examination of the problem of building distributed systems to analyze that data sets up a powerful closing argument. With so much testimony behind him, on closing Aaron really brings it home with why shared nothing systems like map-reduce are the right solution on how to query lots of data. They jury loved it.   Here's the video  Thinking at Scale . And here's a summary of some of the lessons learned from the talk:
  Lessons Learned</p><p>6 0.061723731 <a title="983-tfidf-6" href="../high_scalability-2009/high_scalability-2009-11-01-Squeeze_more_performance_from_Parallelism.html">735 high scalability-2009-11-01-Squeeze more performance from Parallelism</a></p>
<p>7 0.061228026 <a title="983-tfidf-7" href="../high_scalability-2010/high_scalability-2010-05-20-Strategy%3A_Scale_Writes_to_734_Million_Records_Per_Day_Using_Time_Partitioning.html">829 high scalability-2010-05-20-Strategy: Scale Writes to 734 Million Records Per Day Using Time Partitioning</a></p>
<p>8 0.057443283 <a title="983-tfidf-8" href="../high_scalability-2013/high_scalability-2013-10-08-F1_and_Spanner_Holistically_Compared.html">1529 high scalability-2013-10-08-F1 and Spanner Holistically Compared</a></p>
<p>9 0.057217181 <a title="983-tfidf-9" href="../high_scalability-2009/high_scalability-2009-05-06-Dyrad.html">591 high scalability-2009-05-06-Dyrad</a></p>
<p>10 0.0556801 <a title="983-tfidf-10" href="../high_scalability-2013/high_scalability-2013-03-18-Beyond_Threads_and_Callbacks_-_Application_Architecture_Pros_and_Cons.html">1425 high scalability-2013-03-18-Beyond Threads and Callbacks - Application Architecture Pros and Cons</a></p>
<p>11 0.053143036 <a title="983-tfidf-11" href="../high_scalability-2010/high_scalability-2010-08-30-Pomegranate_-_Storing_Billions_and_Billions_of_Tiny_Little_Files.html">889 high scalability-2010-08-30-Pomegranate - Storing Billions and Billions of Tiny Little Files</a></p>
<p>12 0.052989703 <a title="983-tfidf-12" href="../high_scalability-2008/high_scalability-2008-05-05-HSCALE_-__Handling_200_Million_Transactions_Per_Month_Using_Transparent_Partitioning_With_MySQL_Proxy.html">315 high scalability-2008-05-05-HSCALE -  Handling 200 Million Transactions Per Month Using Transparent Partitioning With MySQL Proxy</a></p>
<p>13 0.052973889 <a title="983-tfidf-13" href="../high_scalability-2013/high_scalability-2013-12-13-Stuff_The_Internet_Says_On_Scalability_For_December_13th%2C_2013.html">1564 high scalability-2013-12-13-Stuff The Internet Says On Scalability For December 13th, 2013</a></p>
<p>14 0.052719269 <a title="983-tfidf-14" href="../high_scalability-2013/high_scalability-2013-05-13-The_Secret_to_10_Million_Concurrent_Connections_-The_Kernel_is_the_Problem%2C_Not_the_Solution.html">1456 high scalability-2013-05-13-The Secret to 10 Million Concurrent Connections -The Kernel is the Problem, Not the Solution</a></p>
<p>15 0.052715722 <a title="983-tfidf-15" href="../high_scalability-2009/high_scalability-2009-01-04-Paper%3A_MapReduce%3A_Simplified_Data_Processing_on_Large_Clusters.html">483 high scalability-2009-01-04-Paper: MapReduce: Simplified Data Processing on Large Clusters</a></p>
<p>16 0.052449495 <a title="983-tfidf-16" href="../high_scalability-2009/high_scalability-2009-12-16-Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_in_the_Ambient_Cloud.html">750 high scalability-2009-12-16-Building Super Scalable Systems: Blade Runner Meets Autonomic Computing in the Ambient Cloud</a></p>
<p>17 0.052427404 <a title="983-tfidf-17" href="../high_scalability-2012/high_scalability-2012-11-05-Gone_Fishin%27%3A_Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_In_The_Ambient_Cloud.html">1355 high scalability-2012-11-05-Gone Fishin': Building Super Scalable Systems: Blade Runner Meets Autonomic Computing In The Ambient Cloud</a></p>
<p>18 0.052339993 <a title="983-tfidf-18" href="../high_scalability-2012/high_scalability-2012-03-06-Ask_For_Forgiveness_Programming_-_Or_How_We%27ll_Program_1000_Cores.html">1204 high scalability-2012-03-06-Ask For Forgiveness Programming - Or How We'll Program 1000 Cores</a></p>
<p>19 0.051493667 <a title="983-tfidf-19" href="../high_scalability-2010/high_scalability-2010-12-06-What_the_heck_are_you_actually_using_NoSQL_for%3F.html">954 high scalability-2010-12-06-What the heck are you actually using NoSQL for?</a></p>
<p>20 0.051002011 <a title="983-tfidf-20" href="../high_scalability-2010/high_scalability-2010-12-21-SQL_%2B_NoSQL_%3D_Yes_%21.html">961 high scalability-2010-12-21-SQL + NoSQL = Yes !</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.097), (1, 0.05), (2, -0.003), (3, 0.013), (4, -0.009), (5, 0.051), (6, 0.031), (7, 0.001), (8, -0.029), (9, 0.01), (10, 0.031), (11, 0.02), (12, 0.006), (13, -0.034), (14, 0.002), (15, -0.011), (16, -0.035), (17, -0.007), (18, 0.039), (19, 0.0), (20, -0.009), (21, -0.001), (22, -0.034), (23, 0.01), (24, -0.006), (25, -0.014), (26, -0.006), (27, 0.007), (28, 0.031), (29, 0.008), (30, 0.01), (31, 0.018), (32, -0.039), (33, 0.018), (34, -0.012), (35, -0.031), (36, 0.021), (37, -0.03), (38, 0.01), (39, -0.002), (40, -0.017), (41, 0.018), (42, -0.017), (43, -0.024), (44, -0.022), (45, 0.012), (46, -0.006), (47, 0.01), (48, -0.013), (49, 0.028)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94804198 <a title="983-lsi-1" href="../high_scalability-2011/high_scalability-2011-02-02-Piccolo_-_Building_Distributed_Programs_that_are_11x_Faster_than_Hadoop.html">983 high scalability-2011-02-02-Piccolo - Building Distributed Programs that are 11x Faster than Hadoop</a></p>
<p>Introduction: Piccolo  (not  this  or  this ) is a system for distributed computing, Piccolo is a n ew data-centric programming model for writing parallel in-memory applications in data centers .  Unlike existing data-ﬂow models, Piccolo allows computation running on different machines to share distributed, mutable state via a key-value table interface. T  raditional data-centric models (such as Hadoop) which present the user a single object at a time to operate on, Piccolo exposes a global table interface which is available to all parts of the computation simultaneously. This allows users to specify programs in an intuitive manner very similar to that of writing programs for a single machine. 
 
Using an in-memory key-value store is a very different approach from the canonical map-reduce, which is based on using distributed file systems. The results are impressive:
  

Experiments have shown that Piccolo is fast and pro-vides excellent scaling for many applications. The performance of PageRank and</p><p>2 0.73666799 <a title="983-lsi-2" href="../high_scalability-2010/high_scalability-2010-06-30-Paper%3A_GraphLab%3A_A_New_Framework_For_Parallel_Machine_Learning.html">850 high scalability-2010-06-30-Paper: GraphLab: A New Framework For Parallel Machine Learning</a></p>
<p>Introduction: In the never ending quest to figure out how to do something useful with never ending streams of data,  GraphLab: A New Framework For Parallel Machine Learning  wants to go beyond low-level programming, MapReduce, and dataflow languages with  a new parallel framework for ML (machine learning) which exploits the sparse structure and common computational patterns of ML algorithms. GraphLab enables ML experts to easily design and implement efﬁcient scalable parallel algorithms by composing problem speciﬁc computation, data-dependencies, and scheduling .   Our main contributions include:  
  
  A graph-based data model which simultaneously represents data and computational dependencies.   
  A set of concurrent access models which provide a range of sequential-consistency guarantees.   
  A sophisticated modular scheduling mechanism.   
  An aggregation framework to manage global state.   
   From the abstract:
  
  Designing and implementing efﬁcient, provably correct parallel machine lear</p><p>3 0.73285741 <a title="983-lsi-3" href="../high_scalability-2009/high_scalability-2009-11-01-Squeeze_more_performance_from_Parallelism.html">735 high scalability-2009-11-01-Squeeze more performance from Parallelism</a></p>
<p>Introduction: In many posts, such as:  The Future of the Parallelism and its Challenges  I mentioned that synchronization the access to the shared resource is the major challenge to write parallel code.
 
The synchronization and coordination take long time from the overall execution time, which reduce the benefits of the parallelism; the synchronization and coordination also reduce the scalability.
 
There are many forms of synchronization and coordination, such as:
  
 Create Task object in frameworks such as: Microsoft TPL, Intel TDD, and Parallel Runtime Library. Create and enqueue task objects require synchronization that it’s takes long time especially if we create it into recursive work such as: Quick Sort algorithm. 
 Synchronization the access to shared data. 
  
But there are a few techniques to avoid these issues, such as: Shared-Nothing, Actor Model, and Hyper Object (A.K.A. Combinable Object). Simply if we reduce the shared data by re-architect our code this will gives us a huge benefits</p><p>4 0.73103356 <a title="983-lsi-4" href="../high_scalability-2009/high_scalability-2009-09-16-Paper%3A_A_practical_scalable_distributed_B-tree.html">705 high scalability-2009-09-16-Paper: A practical scalable distributed B-tree</a></p>
<p>Introduction: We've seen a lot of  NoSQL  action lately built around distributed hash tables. Btrees are getting jealous. Btrees, once the king of the database world, want their throne back.  Paul Buchheit  surfaced a paper:  A practical scalable distributed B-tree  by Marcos K. Aguilera and Wojciech Golab, that might help spark a revolution.  From the Abstract:
   We propose a new algorithm for a practical, fault tolerant, and scalable B-tree distributed over a set of servers. Our algorithm supports practical features not present in prior work: transactions that allow atomic execution of multiple operations over multiple B-trees, online migration of B-tree nodes between servers, and dynamic addition and removal of servers. Moreover, our algorithm is conceptually simple: we use transactions to manipulate B-tree nodes so that clients need not use complicated concurrency and locking protocols used in prior work. To execute these transactions quickly, we rely on three techniques: (1) We use optimistic</p><p>5 0.72862679 <a title="983-lsi-5" href="../high_scalability-2013/high_scalability-2013-10-31-Paper%3A_Everything_You_Always_Wanted_to_Know_About_Synchronization_but_Were_Afraid_to_Ask.html">1541 high scalability-2013-10-31-Paper: Everything You Always Wanted to Know About Synchronization but Were Afraid to Ask</a></p>
<p>Introduction: Awesome paper on how particular synchronization mechanisms scale on multi-core architectures:  Everything You Always Wanted to Know About Synchronization but Were Afraid to Ask .
 
The goal is to pick a locking approach that doesn't degrade as the number of cores increase. Like everything else in life, that doesn't appear to be generically possible:
  
 None of the nine locking schemes we consider consistently outperforms any other one, on all target architectures or workloads. Strictly speaking, to seek optimality,  a lock algorithm should thus be selected based on the hardware platform and the expected workload .  
  
Abstract:
  

This paper presents the most exhaustive study of synchronization to date. We span multiple layers, from hardware cache-coherence protocols up to high-level concurrent software. We do so on different types architectures, from single-socket – uniform and nonuniform – to multi-socket – directory and broadcastbased – many-cores. We draw a set of observations t</p><p>6 0.72828722 <a title="983-lsi-6" href="../high_scalability-2009/high_scalability-2009-04-26-Map-Reduce_for_Machine_Learning_on_Multicore.html">581 high scalability-2009-04-26-Map-Reduce for Machine Learning on Multicore</a></p>
<p>7 0.71997976 <a title="983-lsi-7" href="../high_scalability-2012/high_scalability-2012-04-05-Big_Data_Counting%3A_How_to_count_a_billion_distinct_objects_using_only_1.5KB_of_Memory.html">1222 high scalability-2012-04-05-Big Data Counting: How to count a billion distinct objects using only 1.5KB of Memory</a></p>
<p>8 0.71394724 <a title="983-lsi-8" href="../high_scalability-2012/high_scalability-2012-08-06-Paper%3A_High-Performance_Concurrency_Control_Mechanisms_for_Main-Memory_Databases.html">1299 high scalability-2012-08-06-Paper: High-Performance Concurrency Control Mechanisms for Main-Memory Databases</a></p>
<p>9 0.71317834 <a title="983-lsi-9" href="../high_scalability-2010/high_scalability-2010-06-18-Paper%3A_The_Declarative_Imperative%3A_Experiences_and_Conjectures_in_Distributed_Logic.html">844 high scalability-2010-06-18-Paper: The Declarative Imperative: Experiences and Conjectures in Distributed Logic</a></p>
<p>10 0.69703168 <a title="983-lsi-10" href="../high_scalability-2010/high_scalability-2010-12-16-7_Design_Patterns_for_Almost-infinite_Scalability.html">958 high scalability-2010-12-16-7 Design Patterns for Almost-infinite Scalability</a></p>
<p>11 0.69586128 <a title="983-lsi-11" href="../high_scalability-2012/high_scalability-2012-08-20-The_Performance_of_Distributed_Data-Structures_Running_on_a_%22Cache-Coherent%22_In-Memory_Data_Grid.html">1307 high scalability-2012-08-20-The Performance of Distributed Data-Structures Running on a "Cache-Coherent" In-Memory Data Grid</a></p>
<p>12 0.67802572 <a title="983-lsi-12" href="../high_scalability-2013/high_scalability-2013-08-30-Stuff_The_Internet_Says_On_Scalability_For_August_30%2C_2013.html">1509 high scalability-2013-08-30-Stuff The Internet Says On Scalability For August 30, 2013</a></p>
<p>13 0.67200536 <a title="983-lsi-13" href="../high_scalability-2014/high_scalability-2014-04-10-Paper%3A_Scalable_Atomic_Visibility_with_RAMP_Transactions_-_Scale_Linearly_to_100_Servers.html">1629 high scalability-2014-04-10-Paper: Scalable Atomic Visibility with RAMP Transactions - Scale Linearly to 100 Servers</a></p>
<p>14 0.6675384 <a title="983-lsi-14" href="../high_scalability-2010/high_scalability-2010-05-12-The_Rise_of_the_Virtual_Cellular_Machines.html">826 high scalability-2010-05-12-The Rise of the Virtual Cellular Machines</a></p>
<p>15 0.66228247 <a title="983-lsi-15" href="../high_scalability-2013/high_scalability-2013-10-25-Stuff_The_Internet_Says_On_Scalability_For_October_25th%2C_2013.html">1537 high scalability-2013-10-25-Stuff The Internet Says On Scalability For October 25th, 2013</a></p>
<p>16 0.66216022 <a title="983-lsi-16" href="../high_scalability-2011/high_scalability-2011-09-28-Pursue_robust_indefinite_scalability_with_the_Movable_Feast_Machine.html">1127 high scalability-2011-09-28-Pursue robust indefinite scalability with the Movable Feast Machine</a></p>
<p>17 0.6597358 <a title="983-lsi-17" href="../high_scalability-2010/high_scalability-2010-12-23-Paper%3A_CRDTs%3A_Consistency_without_concurrency_control.html">963 high scalability-2010-12-23-Paper: CRDTs: Consistency without concurrency control</a></p>
<p>18 0.65499467 <a title="983-lsi-18" href="../high_scalability-2012/high_scalability-2012-07-04-Top_Features_of_a_Scalable_Database.html">1276 high scalability-2012-07-04-Top Features of a Scalable Database</a></p>
<p>19 0.65479904 <a title="983-lsi-19" href="../high_scalability-2008/high_scalability-2008-07-26-Google%27s_Paxos_Made_Live_%E2%80%93_An_Engineering_Perspective.html">357 high scalability-2008-07-26-Google's Paxos Made Live – An Engineering Perspective</a></p>
<p>20 0.65247101 <a title="983-lsi-20" href="../high_scalability-2009/high_scalability-2009-06-23-Learn_How_to_Exploit_Multiple_Cores_for_Better_Performance_and_Scalability.html">636 high scalability-2009-06-23-Learn How to Exploit Multiple Cores for Better Performance and Scalability</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.376), (2, 0.204), (4, 0.01), (10, 0.032), (30, 0.027), (40, 0.025), (51, 0.013), (61, 0.061), (77, 0.012), (79, 0.071), (85, 0.015), (91, 0.017), (94, 0.027)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.99350137 <a title="983-lda-1" href="../high_scalability-2010/high_scalability-2010-03-23-Digg%3A_4000%25_Performance_Increase_by_Sorting_in_PHP_Rather_than_MySQL.html">799 high scalability-2010-03-23-Digg: 4000% Performance Increase by Sorting in PHP Rather than MySQL</a></p>
<p>Introduction: O'Reilly Radar's James Turner conducted a  very informative interview  with Joe Stump, current CTO of  SimpleGeo  and former lead architect at  Digg , in which Joe makes some of his usually insightful comments on his experience using Cassandra vs MySQL. As Digg started out with a  MySQL oriented architecture  and has recently been  moving full speed  to Cassandra, his observations on some of their lessons learned and the motivation for the move are especially valuable. Here are some of the key takeaways you find useful:
  
  Precompute on writes, make reads fast . This is an oldie as a scaling strategy, but it's valuable to see how SimpleGeo is applying it to their problem of finding entities within a certain geographical region. Using Cassandra they've built two clusters: one for indexes and one for records. The records cluster, as you might imagine, is a simple data lookup. The index cluster has a carefully constructed key for every lookup scenario. The indexes are computed on the wr</p><p>2 0.99337411 <a title="983-lda-2" href="../high_scalability-2012/high_scalability-2012-10-17-World_of_Warcraft%27s_Lead_designer_Rob_Pardo_on_the_Role_of_the_Cloud_in_Games.html">1342 high scalability-2012-10-17-World of Warcraft's Lead designer Rob Pardo on the Role of the Cloud in Games</a></p>
<p>Introduction: In a really far ranging and insightful interview by Steve Peterson:  Game Industry Legends: Rob Pardo , where the future of gaming is discussed, there was a section on how the cloud might be used in games. I know there are a lot of game developers in my audience, so I thought it might be useful:
  Q. If the game is free-to-play but I have to download 10 gigabytes to try it out, that can keep me from trying it. That's part of what cloud gaming is trying to overcome; do you think cloud gaming is going to make some inroads because of those technical issues?

 


 Rob Pardo : I certainly think there's a lot of potential in cloud gaming, it's just picking the right games. There's a lot of hurdles for cloud gaming to overcome, one of which is having to have all these servers to be able to host all these games, since it's not like you need any less computing power to play these games. First of all you need someone with the infrastructure that can have all those games up there. Another issue t</p><p>3 0.99077469 <a title="983-lda-3" href="../high_scalability-2011/high_scalability-2011-07-18-New_Relic_Architecture_-_Collecting_20%2B_Billion_Metrics_a_Day.html">1082 high scalability-2011-07-18-New Relic Architecture - Collecting 20+ Billion Metrics a Day</a></p>
<p>Introduction: This is a guest post by  Brian Doll , Application Performance Engineer at New Relic. 
 
New Relic’s multitenant, SaaS web application monitoring service collects and persists over 100,000 metrics every second on a sustained basis, while still delivering an average page load time of 1.5 seconds.  We believe that good architecture and good tools can help you handle an extremely large amount of data while still providing extremely fast service.  Here we'll show you how we do it.
  
  New Relic is Application Performance Management (APM) as a Service 
  In-app agent instrumentation (bytecode instrumentation, etc.) 
  Support for 5 programming languages (Ruby, Java, PHP, .NET, Python) 
  175,000+ app processes monitored globally 
  10,000+ customers 
   The Stats   
  20+ Billion application metrics collected every day 
  1.7+ Billion web page metrics collected every week 
  Each "timeslice" metric is about 250 bytes 
  100k timeslice records inserted every second 
  7 Billion new rows of d</p><p>4 0.99042338 <a title="983-lda-4" href="../high_scalability-2010/high_scalability-2010-10-21-What_is_Network-based_Application_Virtualization_and_Why_Do_You_Need_It%3F.html">924 high scalability-2010-10-21-What is Network-based Application Virtualization and Why Do You Need It?</a></p>
<p>Introduction: With all the attention being paid these days to VDI (virtual desktop infrastructure) and application virtualization and server virtualization andvirtualization it’s easy to forget about network-based application virtualization. But it’s the one virtualization technique you  shouldn’t  forget because it is a foundational technology upon which myriad other solutions will be enabled.
   WHAT IS NETWORK-BASED APPLICATION VIRTUALIZATION?    
This term may not be familiar to you but that’s because since its inception oh, more than a decade ago, it’s always just been called “server virtualization”. After the turn of the century (I love saying that, by the way) it was always referred to as  service virtualization  in  SOA  and XML circles. With the rise of the likes of VMware and Citrix and Microsoft server virtualization solutions, it’s become impossible to just use the term “server virtualization” and “service virtualization” is just as ambiguous so it seems appropriate to giv</p><p>5 0.99017251 <a title="983-lda-5" href="../high_scalability-2012/high_scalability-2012-03-07-Scale_Indefinitely_on_S3_With_These_Secrets_of_the_S3_Masters.html">1205 high scalability-2012-03-07-Scale Indefinitely on S3 With These Secrets of the S3 Masters</a></p>
<p>Introduction: In a great article,  Amazon S3 Performance Tips & Tricks , Doug Grismore, Director of Storage Operations for AWS, has outed the secret arcana normally reserved for Premium Developer Support customers on how to really use S3:
  
  Size matters . Workloads with less than 50-100 total requests per second don't require any special effort. Customers that routinely perform thousands of requests per second need a plan. 
  Automated partitioning . Automated systems scale S3 horizontally by continuously splitting data into partitions based on high request rates and the number of keys in a partition (which leads to slow lookups). Lessons you've learned with  sharding  may also apply to S3.    
  Avoid hot spots . Like most sharding schemes, you want to avoid hot spots by the smart selection of key names. S3 objects are stored in  buckets .  Each object is identified using a key. Keys are kept in sorted order. Keys in S3 are partitioned by prefix. Objects that sort together are stored together, s</p><p>6 0.98949182 <a title="983-lda-6" href="../high_scalability-2012/high_scalability-2012-08-16-Stuff_The_Internet_Says_On_Scalability_For_August_17%2C_2012.html">1306 high scalability-2012-08-16-Stuff The Internet Says On Scalability For August 17, 2012</a></p>
<p>7 0.98835218 <a title="983-lda-7" href="../high_scalability-2010/high_scalability-2010-01-18-The_Missing_Piece_in_the_Virtualization_Stack_%28Part_1%29.html">762 high scalability-2010-01-18-The Missing Piece in the Virtualization Stack (Part 1)</a></p>
<p>8 0.98470515 <a title="983-lda-8" href="../high_scalability-2014/high_scalability-2014-05-01-Paper%3A_Can_Programming_Be_Liberated_From_The_Von_Neumann_Style%3F_.html">1641 high scalability-2014-05-01-Paper: Can Programming Be Liberated From The Von Neumann Style? </a></p>
<p>9 0.98437703 <a title="983-lda-9" href="../high_scalability-2014/high_scalability-2014-01-14-SharePoint_VPS_solution.html">1579 high scalability-2014-01-14-SharePoint VPS solution</a></p>
<p>10 0.98202109 <a title="983-lda-10" href="../high_scalability-2011/high_scalability-2011-09-14-Big_List_of_Scalabilty_Conferences.html">1115 high scalability-2011-09-14-Big List of Scalabilty Conferences</a></p>
<p>11 0.98201168 <a title="983-lda-11" href="../high_scalability-2007/high_scalability-2007-07-15-Book%3A_Building_Scalable_Web_Sites.html">10 high scalability-2007-07-15-Book: Building Scalable Web Sites</a></p>
<p>12 0.98169988 <a title="983-lda-12" href="../high_scalability-2007/high_scalability-2007-10-14-Newbie_in_scalability_design_issues.html">121 high scalability-2007-10-14-Newbie in scalability design issues</a></p>
<p>13 0.97924328 <a title="983-lda-13" href="../high_scalability-2009/high_scalability-2009-12-28-Zynga_Needs_a_Server-side_Systems_Engineer.html">755 high scalability-2009-12-28-Zynga Needs a Server-side Systems Engineer</a></p>
<p>14 0.97910172 <a title="983-lda-14" href="../high_scalability-2009/high_scalability-2009-01-29-Event%3A_MySQL_Conference_%26_Expo_2009.html">504 high scalability-2009-01-29-Event: MySQL Conference & Expo 2009</a></p>
<p>15 0.97730565 <a title="983-lda-15" href="../high_scalability-2008/high_scalability-2008-10-22-EVE_Online_Architecture.html">424 high scalability-2008-10-22-EVE Online Architecture</a></p>
<p>16 0.97682041 <a title="983-lda-16" href="../high_scalability-2010/high_scalability-2010-05-04-Business_continuity_with_real-time_data_integration.html">822 high scalability-2010-05-04-Business continuity with real-time data integration</a></p>
<p>17 0.97640568 <a title="983-lda-17" href="../high_scalability-2009/high_scalability-2009-04-08-Learned_lessons_from_the_largest_player_%28Flickr%2C_YouTube%2C_Google%2C_etc%29.html">560 high scalability-2009-04-08-Learned lessons from the largest player (Flickr, YouTube, Google, etc)</a></p>
<p>18 0.97557086 <a title="983-lda-18" href="../high_scalability-2010/high_scalability-2010-05-06-Going_global_on_EC2.html">824 high scalability-2010-05-06-Going global on EC2</a></p>
<p>19 0.97543669 <a title="983-lda-19" href="../high_scalability-2008/high_scalability-2008-12-16-Facebook_is_Hiring.html">466 high scalability-2008-12-16-Facebook is Hiring</a></p>
<p>20 0.97414672 <a title="983-lda-20" href="../high_scalability-2008/high_scalability-2008-03-28-How_to_Get_DNS_Names_of_a_Web_Server.html">290 high scalability-2008-03-28-How to Get DNS Names of a Web Server</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
