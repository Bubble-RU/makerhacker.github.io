<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>968 high scalability-2011-01-04-Map-Reduce With Ruby Using Hadoop</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2011" href="../home/high_scalability-2011_home.html">high_scalability-2011</a> <a title="high_scalability-2011-968" href="#">high_scalability-2011-968</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>968 high scalability-2011-01-04-Map-Reduce With Ruby Using Hadoop</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2011-968-html" href="http://highscalability.com//blog/2011/1/4/map-reduce-with-ruby-using-hadoop.html">html</a></p><p>Introduction: A demonstration, with repeatable steps, of how to quickly fire-up a Hadoop cluster on Amazon EC2, load data onto the HDFS (Hadoop Distributed File-System), write map-reduce scripts in Ruby and use them to run a map-reduce job on your Hadoop cluster. You will  not  need to ssh into the cluster, as all tasks are run from your local machine. Below I am using my MacBook Pro as my local machine, but the steps I have provided should be reproducible on other platforms running bash and Java.
  

  

 Fire-Up Your Hadoop Cluster 

I choose the  Cloudera distribution of Hadoop  which is still 100% Apache licensed, but has some additional benefits. One of these benefits is that it is released by  Doug Cutting , who started Hadoop and drove it’s development at Yahoo! He also started  Lucene , which is another of my favourite Apache Projects, so I have good faith that he knows what he is doing. Another benefit, as you will see, is that it is simple to fire-up a Hadoop cluster.


I am going to use C</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 I am going to use Cloudera’s  Whirr script , which will allow me to fire up a production ready Hadoop cluster on  Amazon EC2  directly from my laptop. [sent-8, score-0.165]
</p><p>2 If you are not familiar with Maven, you can install it via Homebrew on Mac OS X using the brew command below. [sent-37, score-0.228]
</p><p>3 sudo brew update sudo brew install maven   Once the dependencies are installed we can build the whirr tool. [sent-39, score-0.92]
</p><p>4 0+23 mvn clean install mvn package -Ppackage   In true Maven style, it will download a long list of dependencies the first time you build this. [sent-42, score-0.216]
</p><p>5 Let’s sanity check the whirr script…   bin/whirr version   You should see something like “Apache Whirr 0. [sent-45, score-0.444]
</p><p>6 0, device=/dev/sda2, durable=false, isBootDevice=false]], supportsImage=Not(is64Bit())]]] Completed launch of myhadoopcluster Web UI available at http://ec2-72-44-45-199. [sent-120, score-0.171]
</p><p>7 sh       Running proxy to Hadoop cluster at    ec2-72-44-45-199. [sent-176, score-0.187]
</p><p>8 The above will output the hostname that you can access the cluster at. [sent-181, score-0.34]
</p><p>9 Use this hostname to view the cluster in your web browser. [sent-187, score-0.235]
</p><p>10 On Amazon EC2 this new hostname will be the internal hostname of data-node server, which is visible because you are tunnelling through the SOCKS proxy. [sent-192, score-0.246]
</p><p>11 You can interact with Hadoop and HDFS with the  hadoop  command. [sent-198, score-0.394]
</p><p>12 We do not have Hadoop installed on our local machine. [sent-199, score-0.19]
</p><p>13 Therefore, we can either log into one of our Hadoop cluster machines and run the hadoop command from there, or install hadoop on our local machine. [sent-200, score-1.178]
</p><p>14 profile which hadoop # should output "/usr/local/hadoop/bin/hadoop" hadoop version # should output "Hadoop 0. [sent-225, score-0.998]
</p><p>15 xml /usr/local/hadoop/conf/   Now run your first command from your local machine to interact with HDFS. [sent-232, score-0.186]
</p><p>16 hadoop fs -ls /   You should see the following output which lists the root on the Hadoop filesystem. [sent-234, score-0.61]
</p><p>17 Uploading Your Data To HDFS (Hadoop Distributed FileSystem)   hadoop fs -mkdir input hadoop fs -put /usr/share/dict/words input/ hadoop fs -ls input   You should see output similar to the following, which list the  words  file on the remote HDFS. [sent-253, score-1.673]
</p><p>18 Since my local user is “phil”, Hadoop has added the file under /user/phil on HDFS. [sent-254, score-0.181]
</p><p>19 Found 1 items -rw-r--r--   3 phil supergroup    2486813 2010-12-30 18:43 /user/phil/input/words   Congratulations! [sent-255, score-0.171]
</p><p>20 You have just uploaded your first file to the Hadoop Distributed File-System on your cluster in the cloud. [sent-256, score-0.165]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('whirr', 0.444), ('hadoop', 0.394), ('null', 0.267), ('hdfs', 0.18), ('myhadoopcluster', 0.171), ('supergroup', 0.171), ('false', 0.153), ('mac', 0.144), ('drwxrwxrwx', 0.136), ('isbootdevice', 0.136), ('providerid', 0.136), ('local', 0.128), ('id', 0.126), ('hostname', 0.123), ('cluster', 0.112), ('fs', 0.111), ('output', 0.105), ('os', 0.095), ('install', 0.092), ('socks', 0.087), ('durable', 0.085), ('sudo', 0.083), ('name', 0.082), ('brew', 0.078), ('proxy', 0.075), ('dn', 0.068), ('imageid', 0.068), ('jt', 0.068), ('paravirtual', 0.068), ('privateaddress', 0.068), ('privateaddresses', 0.068), ('publicaddress', 0.068), ('publicaddresses', 0.068), ('supportsimage', 0.068), ('usermetadata', 0.068), ('ll', 0.068), ('installed', 0.062), ('cdh', 0.062), ('mvn', 0.062), ('tea', 0.062), ('tt', 0.062), ('re', 0.059), ('command', 0.058), ('echo', 0.058), ('terminal', 0.058), ('arch', 0.058), ('directory', 0.054), ('script', 0.053), ('file', 0.053), ('device', 0.052)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="968-tfidf-1" href="../high_scalability-2011/high_scalability-2011-01-04-Map-Reduce_With_Ruby_Using_Hadoop.html">968 high scalability-2011-01-04-Map-Reduce With Ruby Using Hadoop</a></p>
<p>Introduction: A demonstration, with repeatable steps, of how to quickly fire-up a Hadoop cluster on Amazon EC2, load data onto the HDFS (Hadoop Distributed File-System), write map-reduce scripts in Ruby and use them to run a map-reduce job on your Hadoop cluster. You will  not  need to ssh into the cluster, as all tasks are run from your local machine. Below I am using my MacBook Pro as my local machine, but the steps I have provided should be reproducible on other platforms running bash and Java.
  

  

 Fire-Up Your Hadoop Cluster 

I choose the  Cloudera distribution of Hadoop  which is still 100% Apache licensed, but has some additional benefits. One of these benefits is that it is released by  Doug Cutting , who started Hadoop and drove it’s development at Yahoo! He also started  Lucene , which is another of my favourite Apache Projects, so I have good faith that he knows what he is doing. Another benefit, as you will see, is that it is simple to fire-up a Hadoop cluster.


I am going to use C</p><p>2 0.24283022 <a title="968-tfidf-2" href="../high_scalability-2009/high_scalability-2009-06-11-Yahoo%21_Distribution_of_Hadoop.html">627 high scalability-2009-06-11-Yahoo! Distribution of Hadoop</a></p>
<p>Introduction: Many people in the Apache Hadoop community have asked Yahoo! to publish the version of Apache Hadoop they test and deploy across their large Hadoop clusters. As a service to the Hadoop community, Yahoo is releasing the Yahoo! Distribution of Hadoop -- a source code distribution that is based entirely on code found in the Apache Hadoop project.   This source distribution includes code patches that they have added to improve the stability and performance of their clusters. In all cases, these patches have already been contributed back to Apache, but they may not yet be available in an Apache release of Hadoop.    Read more and get the Hadoop distribution from Yahoo</p><p>3 0.207223 <a title="968-tfidf-3" href="../high_scalability-2008/high_scalability-2008-09-28-Product%3A_Happy_%3D_Hadoop_%2B_Python.html">397 high scalability-2008-09-28-Product: Happy = Hadoop + Python</a></p>
<p>Introduction: Has a Java only Hadoop been getting you down? Now you can be  Happy . Happy is a  framework for writing map-reduce programs for Hadoop using Jython. It files off the sharp edges on Hadoop and makes writing map-reduce programs a breeze . There's really no history yet on Happy, but I'm delighted at the idea of being able to map-reduce in  other languages . The more ways the better.  From the website:
   Happy is a framework that allows Hadoop jobs to be written and run in Python 2.2 using Jython. It is an  easy way to write map-reduce programs for Hadoop, and includes some new useful features as well.  The current release supports Hadoop 0.17.2.  Map-reduce jobs in Happy are defined by sub-classing happy.HappyJob and implementing a  map(records, task) and reduce(key, values, task) function. Then you create an instance of the  class, set the job parameters (such as inputs and outputs) and call run().  When you call run(), Happy serializes your job instance and copies it and all accompanyi</p><p>4 0.19821298 <a title="968-tfidf-4" href="../high_scalability-2009/high_scalability-2009-05-17-Product%3A_Hadoop.html">601 high scalability-2009-05-17-Product: Hadoop</a></p>
<p>Introduction: Update 5:   Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds  and has its  green cred questioned  because it took 40 times the number of machines Greenplum used to do the same work.   Update 4:   Introduction to Pig . Pig allows you to skip programming Hadoop at the low map-reduce level. You don't have to know Java. Using the Pig Latin language, which is a scripting data flow language, you can think about your problem as a data flow program. 10 lines of Pig Latin = 200 lines of Java.   Update 3 : Scaling Hadoop to  4000 nodes at Yahoo! .  30,000 cores with nearly 16PB of raw disk; sorted 6TB of data completed in 37 minutes; 14,000 map tasks writes (reads) 360 MB (about 3 blocks) of data into a single file with a total of 5.04 TB for the whole job.  Update 2 : Hadoop  Summit and Data-Intensive Computing Symposium Videos and Slides . Topics include: Pig, JAQL, Hbase, Hive, Data-Intensive Scalable Computing, Clouds and ManyCore: The Revolution, Simplicity and Complexity</p><p>5 0.16344121 <a title="968-tfidf-5" href="../high_scalability-2008/high_scalability-2008-10-15-Hadoop_-_A_Primer.html">414 high scalability-2008-10-15-Hadoop - A Primer</a></p>
<p>Introduction: Hadoop is a distributed computing platform written in Java. It incorporates features similar to those of the   Google File System and of MapReduce to process vast amounts of data     "Hadoop is a Free Java software framework that supports data intensive distributed applications running on large clusters of commodity computers. It enables applications to easily scale out to thousands of nodes and petabytes of data" (Wikipedia)         * What platform does Hadoop run on?       * Java 1.5.x or higher, preferably from Sun       * Linux       * Windows for development       * Solaris</p><p>6 0.16111599 <a title="968-tfidf-6" href="../high_scalability-2011/high_scalability-2011-07-27-Making_Hadoop_1000x_Faster_for_Graph_Problems.html">1088 high scalability-2011-07-27-Making Hadoop 1000x Faster for Graph Problems</a></p>
<p>7 0.15698224 <a title="968-tfidf-7" href="../high_scalability-2010/high_scalability-2010-07-20-Strategy%3A_Consider_When_a_Service_Starts_Billing_in_Your_Algorithm_Cost.html">862 high scalability-2010-07-20-Strategy: Consider When a Service Starts Billing in Your Algorithm Cost</a></p>
<p>8 0.13439861 <a title="968-tfidf-8" href="../high_scalability-2012/high_scalability-2012-08-28-Making_Hadoop_Run_Faster.html">1313 high scalability-2012-08-28-Making Hadoop Run Faster</a></p>
<p>9 0.1324828 <a title="968-tfidf-9" href="../high_scalability-2009/high_scalability-2009-07-02-Product%3A_Hbase.html">650 high scalability-2009-07-02-Product: Hbase</a></p>
<p>10 0.1317393 <a title="968-tfidf-10" href="../high_scalability-2008/high_scalability-2008-01-30-How_Rackspace_Now_Uses_MapReduce_and_Hadoop_to_Query_Terabytes_of_Data.html">233 high scalability-2008-01-30-How Rackspace Now Uses MapReduce and Hadoop to Query Terabytes of Data</a></p>
<p>11 0.12871674 <a title="968-tfidf-11" href="../high_scalability-2008/high_scalability-2008-02-19-Hadoop_Getting_Closer_to_1.0_Release.html">254 high scalability-2008-02-19-Hadoop Getting Closer to 1.0 Release</a></p>
<p>12 0.12129965 <a title="968-tfidf-12" href="../high_scalability-2013/high_scalability-2013-04-24-Strategy%3A_Using_Lots_of_RAM_Often_Cheaper_than_Using_a_Hadoop_Cluster.html">1445 high scalability-2013-04-24-Strategy: Using Lots of RAM Often Cheaper than Using a Hadoop Cluster</a></p>
<p>13 0.120519 <a title="968-tfidf-13" href="../high_scalability-2008/high_scalability-2008-01-29-Building_scalable_storage_into_application_-_Instead_of_MogileFS_OpenAFS_etc..html">229 high scalability-2008-01-29-Building scalable storage into application - Instead of MogileFS OpenAFS etc.</a></p>
<p>14 0.11133411 <a title="968-tfidf-14" href="../high_scalability-2007/high_scalability-2007-08-03-Running_Hadoop_MapReduce_on_Amazon_EC2_and_Amazon_S3.html">56 high scalability-2007-08-03-Running Hadoop MapReduce on Amazon EC2 and Amazon S3</a></p>
<p>15 0.11091457 <a title="968-tfidf-15" href="../high_scalability-2009/high_scalability-2009-08-03-Building_a_Data_Intensive_Web_Application_with_Cloudera%2C_Hadoop%2C_Hive%2C_Pig%2C_and_EC2.html">669 high scalability-2009-08-03-Building a Data Intensive Web Application with Cloudera, Hadoop, Hive, Pig, and EC2</a></p>
<p>16 0.10561118 <a title="968-tfidf-16" href="../high_scalability-2011/high_scalability-2011-03-08-Medialets_Architecture_-__Defeating_the_Daunting_Mobile_Device_Data_Deluge.html">1000 high scalability-2011-03-08-Medialets Architecture -  Defeating the Daunting Mobile Device Data Deluge</a></p>
<p>17 0.10540948 <a title="968-tfidf-17" href="../high_scalability-2012/high_scalability-2012-06-15-Stuff_The_Internet_Says_On_Scalability_For_June_15%2C_2012.html">1265 high scalability-2012-06-15-Stuff The Internet Says On Scalability For June 15, 2012</a></p>
<p>18 0.10021712 <a title="968-tfidf-18" href="../high_scalability-2012/high_scalability-2012-01-12-Peregrine_-_A_Map_Reduce_Framework_for_Iterative_and_Pipelined_Jobs.html">1173 high scalability-2012-01-12-Peregrine - A Map Reduce Framework for Iterative and Pipelined Jobs</a></p>
<p>19 0.099967994 <a title="968-tfidf-19" href="../high_scalability-2014/high_scalability-2014-03-24-Big%2C_Small%2C_Hot_or_Cold_-_Examples_of_Robust_Data_Pipelines_from_Stripe%2C_Tapad%2C_Etsy_and_Square.html">1618 high scalability-2014-03-24-Big, Small, Hot or Cold - Examples of Robust Data Pipelines from Stripe, Tapad, Etsy and Square</a></p>
<p>20 0.096934937 <a title="968-tfidf-20" href="../high_scalability-2009/high_scalability-2009-09-17-Hot_Links_for_2009-9-17_.html">707 high scalability-2009-09-17-Hot Links for 2009-9-17 </a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.127), (1, 0.038), (2, 0.007), (3, 0.002), (4, 0.009), (5, 0.049), (6, 0.122), (7, -0.014), (8, 0.118), (9, 0.091), (10, 0.054), (11, -0.016), (12, 0.126), (13, -0.149), (14, 0.054), (15, -0.063), (16, -0.028), (17, -0.017), (18, -0.083), (19, 0.029), (20, -0.012), (21, 0.054), (22, 0.064), (23, 0.054), (24, -0.019), (25, 0.055), (26, 0.093), (27, -0.023), (28, 0.004), (29, 0.004), (30, 0.07), (31, 0.103), (32, -0.025), (33, -0.0), (34, 0.022), (35, 0.045), (36, -0.081), (37, 0.014), (38, -0.014), (39, -0.06), (40, -0.001), (41, 0.044), (42, -0.069), (43, -0.057), (44, 0.015), (45, 0.032), (46, -0.004), (47, 0.059), (48, -0.053), (49, 0.048)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98702008 <a title="968-lsi-1" href="../high_scalability-2011/high_scalability-2011-01-04-Map-Reduce_With_Ruby_Using_Hadoop.html">968 high scalability-2011-01-04-Map-Reduce With Ruby Using Hadoop</a></p>
<p>Introduction: A demonstration, with repeatable steps, of how to quickly fire-up a Hadoop cluster on Amazon EC2, load data onto the HDFS (Hadoop Distributed File-System), write map-reduce scripts in Ruby and use them to run a map-reduce job on your Hadoop cluster. You will  not  need to ssh into the cluster, as all tasks are run from your local machine. Below I am using my MacBook Pro as my local machine, but the steps I have provided should be reproducible on other platforms running bash and Java.
  

  

 Fire-Up Your Hadoop Cluster 

I choose the  Cloudera distribution of Hadoop  which is still 100% Apache licensed, but has some additional benefits. One of these benefits is that it is released by  Doug Cutting , who started Hadoop and drove it’s development at Yahoo! He also started  Lucene , which is another of my favourite Apache Projects, so I have good faith that he knows what he is doing. Another benefit, as you will see, is that it is simple to fire-up a Hadoop cluster.


I am going to use C</p><p>2 0.9157601 <a title="968-lsi-2" href="../high_scalability-2008/high_scalability-2008-09-28-Product%3A_Happy_%3D_Hadoop_%2B_Python.html">397 high scalability-2008-09-28-Product: Happy = Hadoop + Python</a></p>
<p>Introduction: Has a Java only Hadoop been getting you down? Now you can be  Happy . Happy is a  framework for writing map-reduce programs for Hadoop using Jython. It files off the sharp edges on Hadoop and makes writing map-reduce programs a breeze . There's really no history yet on Happy, but I'm delighted at the idea of being able to map-reduce in  other languages . The more ways the better.  From the website:
   Happy is a framework that allows Hadoop jobs to be written and run in Python 2.2 using Jython. It is an  easy way to write map-reduce programs for Hadoop, and includes some new useful features as well.  The current release supports Hadoop 0.17.2.  Map-reduce jobs in Happy are defined by sub-classing happy.HappyJob and implementing a  map(records, task) and reduce(key, values, task) function. Then you create an instance of the  class, set the job parameters (such as inputs and outputs) and call run().  When you call run(), Happy serializes your job instance and copies it and all accompanyi</p><p>3 0.9101395 <a title="968-lsi-3" href="../high_scalability-2009/high_scalability-2009-06-11-Yahoo%21_Distribution_of_Hadoop.html">627 high scalability-2009-06-11-Yahoo! Distribution of Hadoop</a></p>
<p>Introduction: Many people in the Apache Hadoop community have asked Yahoo! to publish the version of Apache Hadoop they test and deploy across their large Hadoop clusters. As a service to the Hadoop community, Yahoo is releasing the Yahoo! Distribution of Hadoop -- a source code distribution that is based entirely on code found in the Apache Hadoop project.   This source distribution includes code patches that they have added to improve the stability and performance of their clusters. In all cases, these patches have already been contributed back to Apache, but they may not yet be available in an Apache release of Hadoop.    Read more and get the Hadoop distribution from Yahoo</p><p>4 0.88630307 <a title="968-lsi-4" href="../high_scalability-2008/high_scalability-2008-11-14-Paper%3A_Pig_Latin%3A_A_Not-So-Foreign_Language_for_Data_Processing.html">443 high scalability-2008-11-14-Paper: Pig Latin: A Not-So-Foreign Language for Data Processing</a></p>
<p>Introduction: Yahoo has developed a new language called Pig Latin that fit in a sweet spot between high-level declarative querying in the spirit of SQL, and low-level, procedural programming `a la map-reduce and combines best of both worlds.  The accompanying system, Pig, is fully implemented, and compiles Pig Latin into physical plans that are executed over Hadoop, an open-source, map-reduce implementation. Pig has just graduated from the Apache Incubator and joined Hadoop as a subproject.  The paper has a few examples of how engineers at Yahoo! are using Pig to dramatically reduce the time required for the development and execution of their data analysis tasks, compared to using Hadoop directly.  References:  Apache Pig Wiki</p><p>5 0.7950418 <a title="968-lsi-5" href="../high_scalability-2009/high_scalability-2009-05-17-Product%3A_Hadoop.html">601 high scalability-2009-05-17-Product: Hadoop</a></p>
<p>Introduction: Update 5:   Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62 Seconds  and has its  green cred questioned  because it took 40 times the number of machines Greenplum used to do the same work.   Update 4:   Introduction to Pig . Pig allows you to skip programming Hadoop at the low map-reduce level. You don't have to know Java. Using the Pig Latin language, which is a scripting data flow language, you can think about your problem as a data flow program. 10 lines of Pig Latin = 200 lines of Java.   Update 3 : Scaling Hadoop to  4000 nodes at Yahoo! .  30,000 cores with nearly 16PB of raw disk; sorted 6TB of data completed in 37 minutes; 14,000 map tasks writes (reads) 360 MB (about 3 blocks) of data into a single file with a total of 5.04 TB for the whole job.  Update 2 : Hadoop  Summit and Data-Intensive Computing Symposium Videos and Slides . Topics include: Pig, JAQL, Hbase, Hive, Data-Intensive Scalable Computing, Clouds and ManyCore: The Revolution, Simplicity and Complexity</p><p>6 0.78724015 <a title="968-lsi-6" href="../high_scalability-2010/high_scalability-2010-07-20-Strategy%3A_Consider_When_a_Service_Starts_Billing_in_Your_Algorithm_Cost.html">862 high scalability-2010-07-20-Strategy: Consider When a Service Starts Billing in Your Algorithm Cost</a></p>
<p>7 0.76541978 <a title="968-lsi-7" href="../high_scalability-2008/high_scalability-2008-10-15-Hadoop_-_A_Primer.html">414 high scalability-2008-10-15-Hadoop - A Primer</a></p>
<p>8 0.74240035 <a title="968-lsi-8" href="../high_scalability-2009/high_scalability-2009-08-03-Building_a_Data_Intensive_Web_Application_with_Cloudera%2C_Hadoop%2C_Hive%2C_Pig%2C_and_EC2.html">669 high scalability-2009-08-03-Building a Data Intensive Web Application with Cloudera, Hadoop, Hive, Pig, and EC2</a></p>
<p>9 0.71604377 <a title="968-lsi-9" href="../high_scalability-2013/high_scalability-2013-04-24-Strategy%3A_Using_Lots_of_RAM_Often_Cheaper_than_Using_a_Hadoop_Cluster.html">1445 high scalability-2013-04-24-Strategy: Using Lots of RAM Often Cheaper than Using a Hadoop Cluster</a></p>
<p>10 0.6946876 <a title="968-lsi-10" href="../high_scalability-2012/high_scalability-2012-06-15-Stuff_The_Internet_Says_On_Scalability_For_June_15%2C_2012.html">1265 high scalability-2012-06-15-Stuff The Internet Says On Scalability For June 15, 2012</a></p>
<p>11 0.68508673 <a title="968-lsi-11" href="../high_scalability-2012/high_scalability-2012-01-12-Peregrine_-_A_Map_Reduce_Framework_for_Iterative_and_Pipelined_Jobs.html">1173 high scalability-2012-01-12-Peregrine - A Map Reduce Framework for Iterative and Pipelined Jobs</a></p>
<p>12 0.67332655 <a title="968-lsi-12" href="../high_scalability-2008/high_scalability-2008-02-19-Hadoop_Getting_Closer_to_1.0_Release.html">254 high scalability-2008-02-19-Hadoop Getting Closer to 1.0 Release</a></p>
<p>13 0.65527153 <a title="968-lsi-13" href="../high_scalability-2012/high_scalability-2012-08-28-Making_Hadoop_Run_Faster.html">1313 high scalability-2012-08-28-Making Hadoop Run Faster</a></p>
<p>14 0.60769707 <a title="968-lsi-14" href="../high_scalability-2009/high_scalability-2009-07-02-Product%3A_Hbase.html">650 high scalability-2009-07-02-Product: Hbase</a></p>
<p>15 0.60508257 <a title="968-lsi-15" href="../high_scalability-2009/high_scalability-2009-09-17-Hot_Links_for_2009-9-17_.html">707 high scalability-2009-09-17-Hot Links for 2009-9-17 </a></p>
<p>16 0.58871371 <a title="968-lsi-16" href="../high_scalability-2013/high_scalability-2013-01-07-Analyzing_billions_of_credit_card_transactions_and_serving_low-latency_insights_in_the_cloud.html">1382 high scalability-2013-01-07-Analyzing billions of credit card transactions and serving low-latency insights in the cloud</a></p>
<p>17 0.5830515 <a title="968-lsi-17" href="../high_scalability-2011/high_scalability-2011-07-27-Making_Hadoop_1000x_Faster_for_Graph_Problems.html">1088 high scalability-2011-07-27-Making Hadoop 1000x Faster for Graph Problems</a></p>
<p>18 0.57872587 <a title="968-lsi-18" href="../high_scalability-2009/high_scalability-2009-07-02-Hypertable_is_a_New_BigTable_Clone_that_Runs_on_HDFS_or_KFS.html">647 high scalability-2009-07-02-Hypertable is a New BigTable Clone that Runs on HDFS or KFS</a></p>
<p>19 0.57454181 <a title="968-lsi-19" href="../high_scalability-2013/high_scalability-2013-04-17-Tachyon__-_Fault_Tolerant_Distributed_File_System_with_300_Times_Higher_Throughput_than_HDFS.html">1442 high scalability-2013-04-17-Tachyon  - Fault Tolerant Distributed File System with 300 Times Higher Throughput than HDFS</a></p>
<p>20 0.5692085 <a title="968-lsi-20" href="../high_scalability-2010/high_scalability-2010-07-02-Hot_Scalability_Links_for_July_2%2C_2010.html">851 high scalability-2010-07-02-Hot Scalability Links for July 2, 2010</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.098), (2, 0.17), (10, 0.037), (23, 0.017), (30, 0.032), (47, 0.025), (57, 0.322), (61, 0.053), (79, 0.048), (85, 0.028), (94, 0.071)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.88060087 <a title="968-lda-1" href="../high_scalability-2007/high_scalability-2007-11-18-Reverse_Proxy.html">159 high scalability-2007-11-18-Reverse Proxy</a></p>
<p>Introduction: Hi,      I saw an year ago that Netapp sold netcache to blu-coat, my site is a heavy NetCache user and we cached 83% of our site. We tested with Blue-coat and F5 WA and we are not getting same performce as NetCache.      Any of you guys have the same issue? or somebody knows another product can handle much traffic?     Thanks   Rodrigo</p><p>same-blog 2 0.81806856 <a title="968-lda-2" href="../high_scalability-2011/high_scalability-2011-01-04-Map-Reduce_With_Ruby_Using_Hadoop.html">968 high scalability-2011-01-04-Map-Reduce With Ruby Using Hadoop</a></p>
<p>Introduction: A demonstration, with repeatable steps, of how to quickly fire-up a Hadoop cluster on Amazon EC2, load data onto the HDFS (Hadoop Distributed File-System), write map-reduce scripts in Ruby and use them to run a map-reduce job on your Hadoop cluster. You will  not  need to ssh into the cluster, as all tasks are run from your local machine. Below I am using my MacBook Pro as my local machine, but the steps I have provided should be reproducible on other platforms running bash and Java.
  

  

 Fire-Up Your Hadoop Cluster 

I choose the  Cloudera distribution of Hadoop  which is still 100% Apache licensed, but has some additional benefits. One of these benefits is that it is released by  Doug Cutting , who started Hadoop and drove it’s development at Yahoo! He also started  Lucene , which is another of my favourite Apache Projects, so I have good faith that he knows what he is doing. Another benefit, as you will see, is that it is simple to fire-up a Hadoop cluster.


I am going to use C</p><p>3 0.80668461 <a title="968-lda-3" href="../high_scalability-2011/high_scalability-2011-11-17-Five_Misconceptions_on_Cloud_Portability.html">1144 high scalability-2011-11-17-Five Misconceptions on Cloud Portability</a></p>
<p>Introduction: The term "cloud portability" is often considered a synonym for "Cloud  API  portability," which implies a series of misconceptions.
 
If we break away from dogma, we can find that what we really looking for in cloud portability is Application portability between clouds which can be a vastly simpler requirement, as we can achieve application portability without settling on a common Cloud  API. 
 
In this post i'll be covering five common misconceptions people have  WRT  to cloud portability.
  
  Cloud portability = Cloud API portability . API portability is easy; cloud API portability is not.  
  The main incentive for Cloud Portability is - Avoiding Vendor lock-in .Cloud portability is more about business agility than it is about vendor lock-in.  
  Cloud portability isnâ&euro;&trade;t for startups . Every startup that is expecting rapid growth should re-examine their deployments and plan for cloud portability rather than wait to be forced to make the switch when you are least prepared to do so.</p><p>4 0.78530818 <a title="968-lda-4" href="../high_scalability-2008/high_scalability-2008-10-29-CTL_-_Distributed_Control_Dispatching_Framework_.html">433 high scalability-2008-10-29-CTL - Distributed Control Dispatching Framework </a></p>
<p>Introduction: CTL  is a  flexible distributed control dispatching framework that enables you to break management processes into reusable control modules and execute them in distributed fashion over the network .  From their website:  CTL is a flexible distributed control dispatching framework that enables you to break management processes into reusable control modules and execute them in distributed fashion over the network.  What does CTL do? CTL helps you leverage your current scripts and tools to easily automate any kind of distributed systems management or application provisioning task. Its good for simplifiying large-scale scripting efforts or as another tool in your toolbox that helps you speed through your daily mix of ad-hoc administration tasks.  What are CTL's features? CTL has many features, but the general highlights are:  * Execute sophisticated procedures in distributed environments - Aren't you tired of writing and then endlessly modifying scripts that loop over nodes and invoke remot</p><p>5 0.75497037 <a title="968-lda-5" href="../high_scalability-2011/high_scalability-2011-11-07-10_Core_Architecture_Pattern_Variations_for_Achieving_Scalability.html">1138 high scalability-2011-11-07-10 Core Architecture Pattern Variations for Achieving Scalability</a></p>
<p>Introduction: Srinath Perera has put together a  strong list of architecture patterns  based on three meta patterns:  distribution, caching, and asynchronous processing. He contends these three are the primal patterns and the following patterns are but different combinations:
  
  LB (Load Balancers) + Shared nothing Units . Units that do not share anything with each other fronted with a load balancer that routes incoming messages to a unit based on some criteria. 
  LB + Stateless Nodes + Scalable Storage . Several stateless nodes talking to a scalable storage, and a load balancer distributes load among the nodes. 
  Peer to Peer Architectures (Distributed Hash Table (DHT) and Content Addressable Networks (CAN)) . Algorithm for scaling up logarithmically. 
  Distributed Queues . Queue implementation (FIFO delivery) implemented as a network service. 
  Publish/Subscribe Paradigm . Network publish subscribe brokers that route messages to each other. 
  Gossip and Nature-inspired Architectures . Each</p><p>6 0.75181746 <a title="968-lda-6" href="../high_scalability-2009/high_scalability-2009-10-28-Need_for_change_in_your_IT_infrastructure_.html">731 high scalability-2009-10-28-Need for change in your IT infrastructure </a></p>
<p>7 0.73355138 <a title="968-lda-7" href="../high_scalability-2012/high_scalability-2012-03-19-LinkedIn%3A_Creating_a_Low_Latency_Change_Data_Capture_System_with_Databus.html">1211 high scalability-2012-03-19-LinkedIn: Creating a Low Latency Change Data Capture System with Databus</a></p>
<p>8 0.73019725 <a title="968-lda-8" href="../high_scalability-2010/high_scalability-2010-04-09-Vagrant_-_Build_and_Deploy_Virtualized_Development_Environments_Using_Ruby.html">807 high scalability-2010-04-09-Vagrant - Build and Deploy Virtualized Development Environments Using Ruby</a></p>
<p>9 0.67899674 <a title="968-lda-9" href="../high_scalability-2009/high_scalability-2009-04-03-Collectl_interface_to_Ganglia_-_any_interest%3F.html">553 high scalability-2009-04-03-Collectl interface to Ganglia - any interest?</a></p>
<p>10 0.67138821 <a title="968-lda-10" href="../high_scalability-2007/high_scalability-2007-07-11-Friendster_Architecture.html">6 high scalability-2007-07-11-Friendster Architecture</a></p>
<p>11 0.66329348 <a title="968-lda-11" href="../high_scalability-2008/high_scalability-2008-01-17-Moving_old_to_new._Do_not_be_afraid_of_the_re-write_--_but_take_some_help.html">218 high scalability-2008-01-17-Moving old to new. Do not be afraid of the re-write -- but take some help</a></p>
<p>12 0.64383531 <a title="968-lda-12" href="../high_scalability-2008/high_scalability-2008-01-29-When_things_aren%27t_scalable.html">232 high scalability-2008-01-29-When things aren't scalable</a></p>
<p>13 0.6432277 <a title="968-lda-13" href="../high_scalability-2010/high_scalability-2010-07-11-So%2C_Why_is_Twitter_Really_Not_Using_Cassandra_to_Store_Tweets%3F.html">855 high scalability-2010-07-11-So, Why is Twitter Really Not Using Cassandra to Store Tweets?</a></p>
<p>14 0.63930148 <a title="968-lda-14" href="../high_scalability-2011/high_scalability-2011-03-14-6_Lessons_from_Dropbox_-_One_Million_Files_Saved_Every_15_minutes.html">1003 high scalability-2011-03-14-6 Lessons from Dropbox - One Million Files Saved Every 15 minutes</a></p>
<p>15 0.63335991 <a title="968-lda-15" href="../high_scalability-2008/high_scalability-2008-07-16-The_Mother_of_All_Database_Normalization_Debates_on_Coding_Horror.html">351 high scalability-2008-07-16-The Mother of All Database Normalization Debates on Coding Horror</a></p>
<p>16 0.61080265 <a title="968-lda-16" href="../high_scalability-2013/high_scalability-2013-01-11-Stuff_The_Internet_Says_On_Scalability_For_January_11%2C_2013.html">1385 high scalability-2013-01-11-Stuff The Internet Says On Scalability For January 11, 2013</a></p>
<p>17 0.60967368 <a title="968-lda-17" href="../high_scalability-2011/high_scalability-2011-01-11-Google_Megastore_-_3_Billion_Writes_and_20_Billion_Read_Transactions_Daily.html">972 high scalability-2011-01-11-Google Megastore - 3 Billion Writes and 20 Billion Read Transactions Daily</a></p>
<p>18 0.59607035 <a title="968-lda-18" href="../high_scalability-2010/high_scalability-2010-07-13-DbShards_Part_Deux_-_The_Internals.html">857 high scalability-2010-07-13-DbShards Part Deux - The Internals</a></p>
<p>19 0.59087986 <a title="968-lda-19" href="../high_scalability-2013/high_scalability-2013-08-26-Reddit%3A_Lessons_Learned_from_Mistakes_Made_Scaling_to_1_Billion_Pageviews_a_Month.html">1507 high scalability-2013-08-26-Reddit: Lessons Learned from Mistakes Made Scaling to 1 Billion Pageviews a Month</a></p>
<p>20 0.58342451 <a title="968-lda-20" href="../high_scalability-2011/high_scalability-2011-07-26-Web_2.0_Killed_the_Middleware_Star.html">1087 high scalability-2011-07-26-Web 2.0 Killed the Middleware Star</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
