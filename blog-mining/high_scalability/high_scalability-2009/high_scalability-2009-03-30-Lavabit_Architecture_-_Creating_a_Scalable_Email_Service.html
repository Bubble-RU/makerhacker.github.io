<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>551 high scalability-2009-03-30-Lavabit Architecture - Creating a Scalable Email Service</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2009" href="../home/high_scalability-2009_home.html">high_scalability-2009</a> <a title="high_scalability-2009-551" href="#">high_scalability-2009-551</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>551 high scalability-2009-03-30-Lavabit Architecture - Creating a Scalable Email Service</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2009-551-html" href="http://highscalability.com//blog/2009/3/30/lavabit-architecture-creating-a-scalable-email-service.html">html</a></p><p>Introduction: Ladar Levison of Lavabit has written an  incredible article on how  they took a centralized off-the-shelf email server that could handle only few thousand users and built their own custom distributed infrastructure for handling hundreds of thousands of email users.  Lavabit processes 70 gigabytes of data per day, is made up of 26 servers, hosts 260,000 email addresses, and processes 600,000 emails a day. That's a lot of email.   Lavabit's mission has a little edge to it too:
   Lavabit was founded as a direct reaction to the larger free e-mail services available. We felt it was possible to create an e-mail service that was fast, reliable, feature rich and didn't achieve profitability by prostituting its user base to marketers.   
What I really like about this article is that Lavabit has some challenging elements in dealing with different email protocols while being able to scale to a lot of users. There's more going on than just trying to scale out a database. Many products contain com</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Ladar Levison of Lavabit has written an  incredible article on how  they took a centralized off-the-shelf email server that could handle only few thousand users and built their own custom distributed infrastructure for handling hundreds of thousands of email users. [sent-1, score-0.707]
</p><p>2 Lavabit processes 70 gigabytes of data per day, is made up of 26 servers, hosts 260,000 email addresses, and processes 600,000 emails a day. [sent-2, score-0.559]
</p><p>3 Lavabit's mission has a little edge to it too:    Lavabit was founded as a direct reaction to the larger free e-mail services available. [sent-4, score-0.38]
</p><p>4 We felt it was possible to create an e-mail service that was fast, reliable, feature rich and didn't achieve profitability by prostituting its user base to marketers. [sent-5, score-0.265]
</p><p>5 What I really like about this article is that Lavabit has some challenging elements in dealing with different email protocols while being able to scale to a lot of users. [sent-6, score-0.53]
</p><p>6 Many products contain complicated bits like this, so it's interesting to see how Ladar handled them. [sent-8, score-0.158]
</p><p>7 Putting in this extra work in is what Ladar thinks makes Lavabit different:    One of the ways to gain an advantage over your competition is to invest the time and money needed to build systems that are better than what is easily available to your competition. [sent-10, score-0.277]
</p><p>8 It is the custom platform we developed that has allowed us to thrive while many other free email companies either stopped offering their service for free, or shut down altogether. [sent-11, score-0.699]
</p><p>9 Since Ladar was so thorough I saved article as a separate html file. [sent-12, score-0.281]
</p><p>10 Please select the visit link to read the entire article. [sent-13, score-0.147]
</p><p>11 I'd like to thank Ladar again for taking the time and making the effort to document their architecture for the benefit of the community at large to learn from. [sent-14, score-0.209]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ladar', 0.626), ('lavabit', 0.567), ('email', 0.212), ('profitability', 0.098), ('article', 0.094), ('custom', 0.083), ('free', 0.082), ('reaction', 0.076), ('shut', 0.075), ('processes', 0.075), ('emails', 0.074), ('thrive', 0.074), ('thorough', 0.073), ('felt', 0.072), ('stopped', 0.068), ('invest', 0.065), ('gigabytes', 0.065), ('thank', 0.064), ('elements', 0.063), ('founded', 0.062), ('mission', 0.059), ('challenging', 0.059), ('hosts', 0.058), ('html', 0.058), ('contain', 0.058), ('thinks', 0.056), ('competition', 0.056), ('saved', 0.056), ('addresses', 0.055), ('centralized', 0.054), ('allowed', 0.054), ('bits', 0.053), ('direct', 0.052), ('dealing', 0.052), ('thousand', 0.052), ('gain', 0.052), ('offering', 0.051), ('select', 0.05), ('protocols', 0.05), ('benefit', 0.05), ('document', 0.049), ('visit', 0.049), ('edge', 0.049), ('rich', 0.048), ('extra', 0.048), ('link', 0.048), ('putting', 0.047), ('handled', 0.047), ('base', 0.047), ('effort', 0.046)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="551-tfidf-1" href="../high_scalability-2009/high_scalability-2009-03-30-Lavabit_Architecture_-_Creating_a_Scalable_Email_Service.html">551 high scalability-2009-03-30-Lavabit Architecture - Creating a Scalable Email Service</a></p>
<p>Introduction: Ladar Levison of Lavabit has written an  incredible article on how  they took a centralized off-the-shelf email server that could handle only few thousand users and built their own custom distributed infrastructure for handling hundreds of thousands of email users.  Lavabit processes 70 gigabytes of data per day, is made up of 26 servers, hosts 260,000 email addresses, and processes 600,000 emails a day. That's a lot of email.   Lavabit's mission has a little edge to it too:
   Lavabit was founded as a direct reaction to the larger free e-mail services available. We felt it was possible to create an e-mail service that was fast, reliable, feature rich and didn't achieve profitability by prostituting its user base to marketers.   
What I really like about this article is that Lavabit has some challenging elements in dealing with different email protocols while being able to scale to a lot of users. There's more going on than just trying to scale out a database. Many products contain com</p><p>2 0.20740019 <a title="551-tfidf-2" href="../high_scalability-2013/high_scalability-2013-08-13-In_Memoriam%3A_Lavabit_Architecture_-_Creating_a_Scalable_Email_Service.html">1501 high scalability-2013-08-13-In Memoriam: Lavabit Architecture - Creating a Scalable Email Service</a></p>
<p>Introduction: With Lavabit  shutting down Â under  murky circumstances , it seems fitting to  repost an old  (2009), yet still very good post by  Ladar Levison  on Lavabit's architecture. I don't know how much of this information is still current, but it should give you a general idea what Lavabit was all about.
  
 Getting to Know You 
 What is the name of your system and where can we find out more about it? 

 Note: these links are no longer valid... 


Lavabit   http://lavabit.com      http://lavabit.com/network.html    http://lavabit.com/about.html 

 What is your system for? 

Lavabit is a mid-sized email service provider. We currently have about 140,000 registered users with more than 260,000 email addresses. While most of our accounts belong to individual users, we also provide corporate email services to approximately 70 companies.

 Why did you decide to build this system? 

We built the system to compete against the other large free email providers, with an emphasis on serving the privacy c</p><p>3 0.13068821 <a title="551-tfidf-3" href="../high_scalability-2008/high_scalability-2008-01-24-Mailinator_Architecture.html">221 high scalability-2008-01-24-Mailinator Architecture</a></p>
<p>Introduction: Update:  A fun exploration of applied searching in  How to search for the word "pen1s" in 185 emails every second . When indexOf doesn't cut it you just trie harder.   Has a drunken friend ever inspired you to create a first of its kind internet service that is loved by millions, deemed subversive by thousands, all while handling over 1.2 billion emails a year on one  rickity  old server? That's how Paul Tyma came to build Mailinator.   Mailinator is a free no-setup web service for thwarting evil spammers by creating throw-away registration email addresses. If you don't give web sites you real email address they can't spam you. They spam Mailinator instead :-)   I love design with a point-of-view and Mailinator has a big giant harry one: performance first, second, and last. Why? Because Mailinator is free and that allows Paul to showcase his different perspective on design. While competitors buy big Iron to handle load, Paul uses a big idea instead: pick the right problem and create a</p><p>4 0.11422063 <a title="551-tfidf-4" href="../high_scalability-2008/high_scalability-2008-01-06-Email_Architecture.html">202 high scalability-2008-01-06-Email Architecture</a></p>
<p>Introduction: I would like to know email architecture used by large ISPs.. or even used by google.      Can someone point me to some sites??       Thanks..</p><p>5 0.10702461 <a title="551-tfidf-5" href="../high_scalability-2012/high_scalability-2012-06-20-iDoneThis_-_Scaling_an_Email-based_App_from_Scratch.html">1269 high scalability-2012-06-20-iDoneThis - Scaling an Email-based App from Scratch</a></p>
<p>Introduction: This is a guest post by Rodrigo Guzman, CTO of  iDoneThis , which makes status reporting happen at your company with the lightest possible touch. 
 
 iDoneThis  is a simple management application that emails your team at the end of every day to ask, "What'd you get done today?"  Just reply with a few lines of what you got done. The following morning everyone on your team gets a digest with what the team accomplished the previous day to keep everyone in the loop and kickstart another awesome day.
 
Before we launched, we built iDoneThis over a weekend in the most rudimentary way possible.  I kid you not, we sent the first few batches of daily emails using the BCC field of a Gmail inbox.  The upshot is that weâve had users on the site from Day 3 of its existence on.
 
Weâve gone from launch in January 2011 when we sent hundreds of emails out per day by hand to sending out over 1 million emails and handling over 200,000 incoming emails per month.  In total, customers have recorded over 1.</p><p>6 0.088945575 <a title="551-tfidf-6" href="../high_scalability-2008/high_scalability-2008-02-19-Building_a_email_communication_system.html">253 high scalability-2008-02-19-Building a email communication system</a></p>
<p>7 0.070943251 <a title="551-tfidf-7" href="../high_scalability-2010/high_scalability-2010-01-25-Let%27s_Welcome_our_Neo-Feudal_Overlords.html">765 high scalability-2010-01-25-Let's Welcome our Neo-Feudal Overlords</a></p>
<p>8 0.066373304 <a title="551-tfidf-8" href="../high_scalability-2013/high_scalability-2013-06-18-Scaling_Mailbox_-_From_0_to_One_Million_Users_in_6_Weeks_and_100_Million_Messages_Per_Day.html">1477 high scalability-2013-06-18-Scaling Mailbox - From 0 to One Million Users in 6 Weeks and 100 Million Messages Per Day</a></p>
<p>9 0.065164261 <a title="551-tfidf-9" href="../high_scalability-2009/high_scalability-2009-12-16-Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_in_the_Ambient_Cloud.html">750 high scalability-2009-12-16-Building Super Scalable Systems: Blade Runner Meets Autonomic Computing in the Ambient Cloud</a></p>
<p>10 0.065139391 <a title="551-tfidf-10" href="../high_scalability-2012/high_scalability-2012-11-05-Gone_Fishin%27%3A_Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_In_The_Ambient_Cloud.html">1355 high scalability-2012-11-05-Gone Fishin': Building Super Scalable Systems: Blade Runner Meets Autonomic Computing In The Ambient Cloud</a></p>
<p>11 0.060735669 <a title="551-tfidf-11" href="../high_scalability-2007/high_scalability-2007-09-18-Amazon_Architecture.html">96 high scalability-2007-09-18-Amazon Architecture</a></p>
<p>12 0.059206039 <a title="551-tfidf-12" href="../high_scalability-2011/high_scalability-2011-12-05-Stuff_The_Internet_Says_On_Scalability_For_December_5%2C_2011.html">1151 high scalability-2011-12-05-Stuff The Internet Says On Scalability For December 5, 2011</a></p>
<p>13 0.05881691 <a title="551-tfidf-13" href="../high_scalability-2013/high_scalability-2013-04-10-Check_Yourself_Before_You_Wreck_Yourself_-_Avocado%27s_5_Early_Stages_of_Architecture_Evolution.html">1438 high scalability-2013-04-10-Check Yourself Before You Wreck Yourself - Avocado's 5 Early Stages of Architecture Evolution</a></p>
<p>14 0.058814712 <a title="551-tfidf-14" href="../high_scalability-2012/high_scalability-2012-09-18-Sponsored_Post%3A_NY_Times%2C_CouchConf%2C_Surge%2C_FiftyThree%2C_ROBLOX%2C_Percona%2C_ElasticHosts%2C_Atlantic.Net%2C_ScaleOut%2C_New_Relic%2C_NetDNA%2C_GigaSpaces%2C_AiCache%2C_Logic_Monitor%2C_AppDynamics%2C_CloudSigma%2C_ManageEngine%2C_Site24x7.html">1324 high scalability-2012-09-18-Sponsored Post: NY Times, CouchConf, Surge, FiftyThree, ROBLOX, Percona, ElasticHosts, Atlantic.Net, ScaleOut, New Relic, NetDNA, GigaSpaces, AiCache, Logic Monitor, AppDynamics, CloudSigma, ManageEngine, Site24x7</a></p>
<p>15 0.058481194 <a title="551-tfidf-15" href="../high_scalability-2012/high_scalability-2012-09-05-Sponsored_Post%3A_Surge%2C_FiftyThree%2C_ROBLOX%2C_Percona%2C_Palantir%2C_ElasticHosts%2C_Atlantic.Net%2C_ScaleOut%2C_New_Relic%2C_NetDNA%2C_GigaSpaces%2C_AiCache%2C_Logic_Monitor%2C_AppDynamics%2C_CloudSigma%2C_ManageEngine%2C_Site24x7.html">1317 high scalability-2012-09-05-Sponsored Post: Surge, FiftyThree, ROBLOX, Percona, Palantir, ElasticHosts, Atlantic.Net, ScaleOut, New Relic, NetDNA, GigaSpaces, AiCache, Logic Monitor, AppDynamics, CloudSigma, ManageEngine, Site24x7</a></p>
<p>16 0.056586973 <a title="551-tfidf-16" href="../high_scalability-2012/high_scalability-2012-07-25-Sponsored_Post%3A_ElasticHosts%2C_Atlantic.Net%2C_ScaleOut%2C_ground%28ctrl%29%2C_New_Relic%2C_NetDNA%2C_Torbit%2C_GigaSpaces%2C_AiCache%2C_Logic_Monitor%2C_AppDynamics%2C_CloudSigma%2C_ManageEngine%2C_Site24x7.html">1290 high scalability-2012-07-25-Sponsored Post: ElasticHosts, Atlantic.Net, ScaleOut, ground(ctrl), New Relic, NetDNA, Torbit, GigaSpaces, AiCache, Logic Monitor, AppDynamics, CloudSigma, ManageEngine, Site24x7</a></p>
<p>17 0.056284089 <a title="551-tfidf-17" href="../high_scalability-2012/high_scalability-2012-08-21-Sponsored_Post%3A_ROBLOX%2C_Percona%2C_Palantir%2C_ElasticHosts%2C_Atlantic.Net%2C_ScaleOut%2C_ground%28ctrl%29%2C_New_Relic%2C_NetDNA%2C_GigaSpaces%2C_AiCache%2C_Logic_Monitor%2C_AppDynamics%2C_CloudSigma%2C_ManageEngine%2C_Site24x7.html">1308 high scalability-2012-08-21-Sponsored Post: ROBLOX, Percona, Palantir, ElasticHosts, Atlantic.Net, ScaleOut, ground(ctrl), New Relic, NetDNA, GigaSpaces, AiCache, Logic Monitor, AppDynamics, CloudSigma, ManageEngine, Site24x7</a></p>
<p>18 0.056085296 <a title="551-tfidf-18" href="../high_scalability-2012/high_scalability-2012-08-07-Sponsored_Post%3A_Palantir%2C_Percona%2C_ElasticHosts%2C_Atlantic.Net%2C_ScaleOut%2C_ground%28ctrl%29%2C_New_Relic%2C_NetDNA%2C_GigaSpaces%2C_AiCache%2C_Logic_Monitor%2C_AppDynamics%2C_CloudSigma%2C_ManageEngine%2C_Site24x7.html">1300 high scalability-2012-08-07-Sponsored Post: Palantir, Percona, ElasticHosts, Atlantic.Net, ScaleOut, ground(ctrl), New Relic, NetDNA, GigaSpaces, AiCache, Logic Monitor, AppDynamics, CloudSigma, ManageEngine, Site24x7</a></p>
<p>19 0.055907313 <a title="551-tfidf-19" href="../high_scalability-2012/high_scalability-2012-01-17-Sponsored_Post%3A_Next_Big_Sound%2C_ElasticHosts%2C_1%261%2C_Red_5_Studios%2C_SingleHop%2C_Spokeo%2C_Callfire%2C_Attribution_Modeling%2C_Logic_Monitor%2C_New_Relic%2C_ScaleOut%2C_AppDynamics%2C_CloudSigma%2C_ManageEngine%2C_Site24x7.html">1176 high scalability-2012-01-17-Sponsored Post: Next Big Sound, ElasticHosts, 1&1, Red 5 Studios, SingleHop, Spokeo, Callfire, Attribution Modeling, Logic Monitor, New Relic, ScaleOut, AppDynamics, CloudSigma, ManageEngine, Site24x7</a></p>
<p>20 0.055634264 <a title="551-tfidf-20" href="../high_scalability-2007/high_scalability-2007-09-06-Product%3A_Perdition_Mail_Retrieval_Proxy.html">80 high scalability-2007-09-06-Product: Perdition Mail Retrieval Proxy</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.109), (1, 0.017), (2, -0.004), (3, -0.019), (4, -0.001), (5, -0.034), (6, -0.0), (7, -0.001), (8, -0.002), (9, 0.012), (10, -0.015), (11, 0.027), (12, 0.005), (13, -0.011), (14, 0.038), (15, 0.006), (16, -0.016), (17, -0.0), (18, -0.011), (19, 0.006), (20, -0.013), (21, -0.025), (22, -0.014), (23, -0.014), (24, 0.011), (25, -0.013), (26, 0.029), (27, 0.006), (28, -0.029), (29, -0.001), (30, -0.02), (31, 0.004), (32, -0.024), (33, -0.013), (34, -0.024), (35, -0.026), (36, 0.021), (37, 0.019), (38, 0.025), (39, 0.003), (40, 0.018), (41, 0.058), (42, 0.038), (43, 0.002), (44, 0.003), (45, 0.026), (46, 0.034), (47, -0.025), (48, -0.023), (49, 0.014)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94641352 <a title="551-lsi-1" href="../high_scalability-2009/high_scalability-2009-03-30-Lavabit_Architecture_-_Creating_a_Scalable_Email_Service.html">551 high scalability-2009-03-30-Lavabit Architecture - Creating a Scalable Email Service</a></p>
<p>Introduction: Ladar Levison of Lavabit has written an  incredible article on how  they took a centralized off-the-shelf email server that could handle only few thousand users and built their own custom distributed infrastructure for handling hundreds of thousands of email users.  Lavabit processes 70 gigabytes of data per day, is made up of 26 servers, hosts 260,000 email addresses, and processes 600,000 emails a day. That's a lot of email.   Lavabit's mission has a little edge to it too:
   Lavabit was founded as a direct reaction to the larger free e-mail services available. We felt it was possible to create an e-mail service that was fast, reliable, feature rich and didn't achieve profitability by prostituting its user base to marketers.   
What I really like about this article is that Lavabit has some challenging elements in dealing with different email protocols while being able to scale to a lot of users. There's more going on than just trying to scale out a database. Many products contain com</p><p>2 0.77567977 <a title="551-lsi-2" href="../high_scalability-2008/high_scalability-2008-01-24-Mailinator_Architecture.html">221 high scalability-2008-01-24-Mailinator Architecture</a></p>
<p>Introduction: Update:  A fun exploration of applied searching in  How to search for the word "pen1s" in 185 emails every second . When indexOf doesn't cut it you just trie harder.   Has a drunken friend ever inspired you to create a first of its kind internet service that is loved by millions, deemed subversive by thousands, all while handling over 1.2 billion emails a year on one  rickity  old server? That's how Paul Tyma came to build Mailinator.   Mailinator is a free no-setup web service for thwarting evil spammers by creating throw-away registration email addresses. If you don't give web sites you real email address they can't spam you. They spam Mailinator instead :-)   I love design with a point-of-view and Mailinator has a big giant harry one: performance first, second, and last. Why? Because Mailinator is free and that allows Paul to showcase his different perspective on design. While competitors buy big Iron to handle load, Paul uses a big idea instead: pick the right problem and create a</p><p>3 0.76926625 <a title="551-lsi-3" href="../high_scalability-2008/high_scalability-2008-02-19-Building_a_email_communication_system.html">253 high scalability-2008-02-19-Building a email communication system</a></p>
<p>Introduction: hi,   the website i work for is looking to build a email system that can handle a fair few emails (up to a hundred thousand a day).     These comprise emails like registration emails, newsletters, lots of user triggered emails and overnight emails.     At present we queue them in SQL and feed them into an smtp server on one of our web servers when the queue drops below a certain level.     this has caused our mail system to crash as well as hammer our DB server (shared!!!).     We have got an architecture of what we want to build but thought there might be something we could buy off the shelf that allowed us to keep templated emails, lists of recipients, schedule sends etc and report on it. We can't find anything     What do big websites like amazon etc use or people a little smaller but who still send loads of mail (flickr, ebuyer, or other ecommerce sites)     Cheers     tarqs</p><p>4 0.76740485 <a title="551-lsi-4" href="../high_scalability-2012/high_scalability-2012-06-20-iDoneThis_-_Scaling_an_Email-based_App_from_Scratch.html">1269 high scalability-2012-06-20-iDoneThis - Scaling an Email-based App from Scratch</a></p>
<p>Introduction: This is a guest post by Rodrigo Guzman, CTO of  iDoneThis , which makes status reporting happen at your company with the lightest possible touch. 
 
 iDoneThis  is a simple management application that emails your team at the end of every day to ask, "What'd you get done today?"  Just reply with a few lines of what you got done. The following morning everyone on your team gets a digest with what the team accomplished the previous day to keep everyone in the loop and kickstart another awesome day.
 
Before we launched, we built iDoneThis over a weekend in the most rudimentary way possible.  I kid you not, we sent the first few batches of daily emails using the BCC field of a Gmail inbox.  The upshot is that weâve had users on the site from Day 3 of its existence on.
 
Weâve gone from launch in January 2011 when we sent hundreds of emails out per day by hand to sending out over 1 million emails and handling over 200,000 incoming emails per month.  In total, customers have recorded over 1.</p><p>5 0.76659232 <a title="551-lsi-5" href="../high_scalability-2013/high_scalability-2013-06-18-Scaling_Mailbox_-_From_0_to_One_Million_Users_in_6_Weeks_and_100_Million_Messages_Per_Day.html">1477 high scalability-2013-06-18-Scaling Mailbox - From 0 to One Million Users in 6 Weeks and 100 Million Messages Per Day</a></p>
<p>Introduction: You know your product is doing well when most of your  early blog posts  deal with the status of the waiting list of hundreds of thousands of users eagerly waiting to download your product. That's the enviable position  Mailbox , a free mobile email management app, found themselves early in their release cycle.Â 
 
Hasn't email been done already? Apparently not. Mailbox scaled to one million users in a paltry six weeks with a team of  about 14 people . AsÂ of April they were delivering over  100 million messages per day .
 
How did they do it?Â Mailbox engineering lead,  Sean Beausoleil ,Â gave anÂ  informative interview on readwrite.com  on how Mailbox planned to scale...Â 
  
  Gather signals early . A pre-release launch video helped generate interest, but it also allowed them to gauge early interest before even releasing. From the overwhelming response they knew they would need to have to scale and scale quickly.Â  
  Have something unique . The average person might not think a mailbox app</p><p>6 0.71960938 <a title="551-lsi-6" href="../high_scalability-2013/high_scalability-2013-08-13-In_Memoriam%3A_Lavabit_Architecture_-_Creating_a_Scalable_Email_Service.html">1501 high scalability-2013-08-13-In Memoriam: Lavabit Architecture - Creating a Scalable Email Service</a></p>
<p>7 0.70897925 <a title="551-lsi-7" href="../high_scalability-2011/high_scalability-2011-02-08-Mollom_Architecture_-_Killing_Over_373_Million_Spams_at_100_Requests_Per_Second.html">985 high scalability-2011-02-08-Mollom Architecture - Killing Over 373 Million Spams at 100 Requests Per Second</a></p>
<p>8 0.69269764 <a title="551-lsi-8" href="../high_scalability-2007/high_scalability-2007-09-06-Product%3A_Perdition_Mail_Retrieval_Proxy.html">80 high scalability-2007-09-06-Product: Perdition Mail Retrieval Proxy</a></p>
<p>9 0.68857604 <a title="551-lsi-9" href="../high_scalability-2014/high_scalability-2014-04-21-This_is_why_Microsoft_won._And_why_they_lost..html">1635 high scalability-2014-04-21-This is why Microsoft won. And why they lost.</a></p>
<p>10 0.67868239 <a title="551-lsi-10" href="../high_scalability-2014/high_scalability-2014-04-28-How_Disqus_Went_Realtime_with_165K_Messages_Per_Second_and_Less_than_.2_Seconds_Latency.html">1638 high scalability-2014-04-28-How Disqus Went Realtime with 165K Messages Per Second and Less than .2 Seconds Latency</a></p>
<p>11 0.67343247 <a title="551-lsi-11" href="../high_scalability-2008/high_scalability-2008-01-06-Email_Architecture.html">202 high scalability-2008-01-06-Email Architecture</a></p>
<p>12 0.66759175 <a title="551-lsi-12" href="../high_scalability-2007/high_scalability-2007-08-17-What_is_the_best_hosting_option%3F.html">67 high scalability-2007-08-17-What is the best hosting option?</a></p>
<p>13 0.66347998 <a title="551-lsi-13" href="../high_scalability-2008/high_scalability-2008-01-28-Product%3A_ISPMan_Centralized_ISP_Management_System_.html">228 high scalability-2008-01-28-Product: ISPMan Centralized ISP Management System </a></p>
<p>14 0.66241574 <a title="551-lsi-14" href="../high_scalability-2008/high_scalability-2008-01-13-A_Note_on_How_to_Create_Teasers_When_Posting_.html">210 high scalability-2008-01-13-A Note on How to Create Teasers When Posting </a></p>
<p>15 0.649571 <a title="551-lsi-15" href="../high_scalability-2011/high_scalability-2011-02-22-Is_Node.js_Becoming_a_Part_of_the_Stack%3F_SimpleGeo_Says_Yes..html">993 high scalability-2011-02-22-Is Node.js Becoming a Part of the Stack? SimpleGeo Says Yes.</a></p>
<p>16 0.64768374 <a title="551-lsi-16" href="../high_scalability-2014/high_scalability-2014-01-06-How_HipChat_Stores_and_Indexes_Billions_of_Messages_Using_ElasticSearch_and_Redis.html">1573 high scalability-2014-01-06-How HipChat Stores and Indexes Billions of Messages Using ElasticSearch and Redis</a></p>
<p>17 0.6473431 <a title="551-lsi-17" href="../high_scalability-2011/high_scalability-2011-06-27-TripAdvisor_Architecture_-_40M_Visitors%2C_200M_Dynamic_Page_Views%2C_30TB_Data.html">1068 high scalability-2011-06-27-TripAdvisor Architecture - 40M Visitors, 200M Dynamic Page Views, 30TB Data</a></p>
<p>18 0.64689302 <a title="551-lsi-18" href="../high_scalability-2013/high_scalability-2013-07-17-How_do_you_create_a_100th_Monkey_software_development_culture%3F.html">1492 high scalability-2013-07-17-How do you create a 100th Monkey software development culture?</a></p>
<p>19 0.6449675 <a title="551-lsi-19" href="../high_scalability-2011/high_scalability-2011-03-28-Aztec_Empire_Strategy%3A_Use_Dual_Pipes_in_Your_Aqueduct_for_High_Availability.html">1012 high scalability-2011-03-28-Aztec Empire Strategy: Use Dual Pipes in Your Aqueduct for High Availability</a></p>
<p>20 0.64420736 <a title="551-lsi-20" href="../high_scalability-2013/high_scalability-2013-10-07-Ask_HS%3A_Is_Microsoft_the_Right_Technology_for_a_Scalable_Web-based_System%3F.html">1528 high scalability-2013-10-07-Ask HS: Is Microsoft the Right Technology for a Scalable Web-based System?</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.134), (2, 0.574), (40, 0.026), (61, 0.06), (79, 0.017), (94, 0.046)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.99741995 <a title="551-lda-1" href="../high_scalability-2009/high_scalability-2009-10-16-Paper%3A_Scaling_Online_Social_Networks_without_Pains.html">723 high scalability-2009-10-16-Paper: Scaling Online Social Networks without Pains</a></p>
<p>Introduction: We saw inÂ  Why are Facebook, Digg, and Twitter so hard to scale? Â scaling social networks is a lot harder than you might think. This paper,  Scaling Online Social Networks without Pains ,Â from a team at Telefonica Research in Spain hopes to meet the challenge of status distribution, user generated content distribution, and managing the social graph through a technique they call  One-Hop Replication Â (OHR). OHR  abstracts and delegates the complexity ofÂ scaling up from the social network application . The abstract:   
 Online Social Networks (OSN) face serious scalability challengesÂ due to their rapid growth and popularity. To addressÂ this issue we present a novel approach to scale up OSN calledÂ One Hop Replication (OHR). Our system combines partitioningÂ and replication in a middleware to transparentlyÂ scale up a centralized OSN design, and therefore, avoid theÂ OSN application to undergo the costly transition to a fullyÂ distributed system to meet its scalability needs.Â OHR exploits some</p><p>2 0.9939881 <a title="551-lda-2" href="../high_scalability-2008/high_scalability-2008-01-10-Letting_Clients_Know_What%27s_Changed%3A_Push_Me_or_Pull_Me%3F.html">205 high scalability-2008-01-10-Letting Clients Know What's Changed: Push Me or Pull Me?</a></p>
<p>Introduction: I had a false belief I thought I came here to stay We're all just visiting All just breaking like waves The oceans made me, but who came up with me? Push me, pull me, push me, or pull me out .     So true Perl Jam   (Push me Pull me lyrics)  , so true. I too have wondered how web clients should be notified of model changes. Should servers push events to clients or should clients pull events from servers? A topic worthy of its own song if ever there was one.       To pull events the client simply starts a timer and makes a request to the server. This is polling. You can either pull a complete set of fresh data or get a list of changes. The server "knows" if anything you are interested in has changed and makes those changes available to you.  Knowing what has changed can be relatively simple with a publish-subscribe type backend or you can get very complex with fine grained bit maps of attributes and keeping per client state on what I client still needs to see.     Polling is heavy man.</p><p>3 0.99356443 <a title="551-lda-3" href="../high_scalability-2011/high_scalability-2011-01-03-Stuff_The_Internet_Says_On_Scalability_For_January_3%2C_2010.html">967 high scalability-2011-01-03-Stuff The Internet Says On Scalability For January 3, 2010</a></p>
<p>Introduction: Submitted for your reading pleasure...
  
 Quotable Quotes           
 
  @hofmanndavid : Performance and scalability anxiety makes developers want to catch the flying butterflies 
  @tivrfoa :  "Scalability solutions aren't magic. They involve partitioning, indexing and replication." Twitter engineer  
 Alan Perlis: Â Fools ignore complexity; pragmatists suffer it; experts avoid it; geniuses remove it.  
 
 
  CIO update: Post-mortem on the Skype outage . Interesting tale of a cascading collapse in complex,Â distributed, interactive systems. For more background see the highlyÂ illuminatingÂ  Explaining Supernodes Â by Dan York. 
  RethinkDB and SSD Databases. SSD was not a revolution Â by Kevin Burton.Â  Whatâs really shocking to me, is that while SSD and flash storage is very exciting, it wasnât as revolutionary in 2010 as I would have liked to have seen.  
  The case for Datastore-Side-Scripting . Russell SullivanÂ predicts real-time web applications are going in the direction of being enti</p><p>same-blog 4 0.99335712 <a title="551-lda-4" href="../high_scalability-2009/high_scalability-2009-03-30-Lavabit_Architecture_-_Creating_a_Scalable_Email_Service.html">551 high scalability-2009-03-30-Lavabit Architecture - Creating a Scalable Email Service</a></p>
<p>Introduction: Ladar Levison of Lavabit has written an  incredible article on how  they took a centralized off-the-shelf email server that could handle only few thousand users and built their own custom distributed infrastructure for handling hundreds of thousands of email users.  Lavabit processes 70 gigabytes of data per day, is made up of 26 servers, hosts 260,000 email addresses, and processes 600,000 emails a day. That's a lot of email.   Lavabit's mission has a little edge to it too:
   Lavabit was founded as a direct reaction to the larger free e-mail services available. We felt it was possible to create an e-mail service that was fast, reliable, feature rich and didn't achieve profitability by prostituting its user base to marketers.   
What I really like about this article is that Lavabit has some challenging elements in dealing with different email protocols while being able to scale to a lot of users. There's more going on than just trying to scale out a database. Many products contain com</p><p>5 0.99156898 <a title="551-lda-5" href="../high_scalability-2012/high_scalability-2012-02-10-Stuff_The_Internet_Says_On_Scalability_For_February_10%2C_2012.html">1190 high scalability-2012-02-10-Stuff The Internet Says On Scalability For February 10, 2012</a></p>
<p>Introduction: HighScalability Tested, Mother Approved:
  
  12,233TPS : Twitter @ Super Bowl;Â  11 Million Slices : Dominos @ Super Bowl;  500K requests per second : S3;Â  
  The great mobile money drain . Mobile: high resource costs, low revenue. Mobile traffic on Plenty of Fish isÂ  growing at 3% a month , rising to 3 Billion pageviews a month, 40% of signups are mobile,Â and all traffic will soon be 60-70% mobile. The problem: how do you make money on mobile? 
  Time to chuck microprocessors for a networks of cells? Â  How Networks of Biological Cells Solve Distributed Computing Problems : Computer scientists prove that networks of cells can compute as efficiently as networks of computers linked via the internet. We believe that there is a need for a network model, where nodes are by design below the computation and communication capabilities of Turing machines. 
  Unrelated? Â  GDrive at last Â andÂ  S3 Drops Storage Pricing . 
 If you are  StackOverflow and your data is overflowing , what do you do? Mo</p><p>6 0.9903931 <a title="551-lda-6" href="../high_scalability-2011/high_scalability-2011-12-12-Netflix%3A_Developing%2C_Deploying%2C_and_Supporting_Software_According_to_the_Way_of_the_Cloud.html">1155 high scalability-2011-12-12-Netflix: Developing, Deploying, and Supporting Software According to the Way of the Cloud</a></p>
<p>7 0.99022675 <a title="551-lda-7" href="../high_scalability-2012/high_scalability-2012-02-27-Zen_and_the_Art_of_Scaling_-_A_Koan_and_Epigram_Approach.html">1199 high scalability-2012-02-27-Zen and the Art of Scaling - A Koan and Epigram Approach</a></p>
<p>8 0.99009943 <a title="551-lda-8" href="../high_scalability-2009/high_scalability-2009-05-08-Eight_Best_Practices_for_Building_Scalable_Systems.html">594 high scalability-2009-05-08-Eight Best Practices for Building Scalable Systems</a></p>
<p>9 0.98850465 <a title="551-lda-9" href="../high_scalability-2009/high_scalability-2009-06-27-Scaling_Twitter%3A_Making_Twitter_10000_Percent_Faster.html">639 high scalability-2009-06-27-Scaling Twitter: Making Twitter 10000 Percent Faster</a></p>
<p>10 0.98844004 <a title="551-lda-10" href="../high_scalability-2008/high_scalability-2008-12-01-MySQL_Database_Scale-out_and_Replication_for_High_Growth_Businesses.html">455 high scalability-2008-12-01-MySQL Database Scale-out and Replication for High Growth Businesses</a></p>
<p>11 0.98733962 <a title="551-lda-11" href="../high_scalability-2014/high_scalability-2014-02-05-Little%E2%80%99s_Law%2C_Scalability_and_Fault_Tolerance%3A_The_OS_is_your_bottleneck._What_you_can_do%3F.html">1591 high scalability-2014-02-05-Littleâs Law, Scalability and Fault Tolerance: The OS is your bottleneck. What you can do?</a></p>
<p>12 0.98416811 <a title="551-lda-12" href="../high_scalability-2010/high_scalability-2010-09-30-More_Troubles_with_Caching.html">911 high scalability-2010-09-30-More Troubles with Caching</a></p>
<p>13 0.98389608 <a title="551-lda-13" href="../high_scalability-2010/high_scalability-2010-08-12-Strategy%3A_Terminate_SSL_Connections_in_Hardware_and_Reduce_Server_Count_by_40%25.html">878 high scalability-2010-08-12-Strategy: Terminate SSL Connections in Hardware and Reduce Server Count by 40%</a></p>
<p>14 0.98379308 <a title="551-lda-14" href="../high_scalability-2011/high_scalability-2011-03-17-Are_long_VM_instance_spin-up_times_in_the_cloud_costing_you_money%3F.html">1006 high scalability-2011-03-17-Are long VM instance spin-up times in the cloud costing you money?</a></p>
<p>15 0.98271537 <a title="551-lda-15" href="../high_scalability-2012/high_scalability-2012-07-13-Stuff_The_Internet_Says_On_Scalability_For_July_13%2C_2012.html">1283 high scalability-2012-07-13-Stuff The Internet Says On Scalability For July 13, 2012</a></p>
<p>16 0.98159027 <a title="551-lda-16" href="../high_scalability-2010/high_scalability-2010-06-04-Strategy%3A_Cache_Larger_Chunks_-_Cache_Hit_Rate_is_a_Bad_Indicator.html">836 high scalability-2010-06-04-Strategy: Cache Larger Chunks - Cache Hit Rate is a Bad Indicator</a></p>
<p>17 0.98133433 <a title="551-lda-17" href="../high_scalability-2009/high_scalability-2009-07-27-Handle_700_Percent_More_Requests_Using_Squid_and_APC_Cache.html">662 high scalability-2009-07-27-Handle 700 Percent More Requests Using Squid and APC Cache</a></p>
<p>18 0.97729975 <a title="551-lda-18" href="../high_scalability-2007/high_scalability-2007-07-31-BerkeleyDB_%26_other_distributed_high_performance_key-value_databases.html">50 high scalability-2007-07-31-BerkeleyDB & other distributed high performance key-value databases</a></p>
<p>19 0.97673845 <a title="551-lda-19" href="../high_scalability-2010/high_scalability-2010-07-20-Sponsored_Post%3A__ezRez%2C_VoltDB_and_Digg_are_Hiring.html">861 high scalability-2010-07-20-Sponsored Post:  ezRez, VoltDB and Digg are Hiring</a></p>
<p>20 0.97565532 <a title="551-lda-20" href="../high_scalability-2010/high_scalability-2010-06-18-Paper%3A_The_Declarative_Imperative%3A_Experiences_and_Conjectures_in_Distributed_Logic.html">844 high scalability-2010-06-18-Paper: The Declarative Imperative: Experiences and Conjectures in Distributed Logic</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
