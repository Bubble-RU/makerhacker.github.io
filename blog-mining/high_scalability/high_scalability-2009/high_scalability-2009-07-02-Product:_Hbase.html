<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>650 high scalability-2009-07-02-Product: Hbase</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2009" href="../home/high_scalability-2009_home.html">high_scalability-2009</a> <a title="high_scalability-2009-650" href="#">high_scalability-2009-650</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>650 high scalability-2009-07-02-Product: Hbase</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2009-650-html" href="http://highscalability.com//blog/2009/7/2/product-hbase.html">html</a></p><p>Introduction: Update 3:Presentation from theNoSQL Conference:slides,video.Update 2:Jim
Wilson helps with theUnderstanding HBase and BigTableby explaining them from a
"conceptual standpoint."Update:InfoQ interview:HBase Leads Discuss Hadoop,
BigTable and Distributed Databases. "MapReduce (both Google's and Hadoop's) is
ideal for processing huge amounts of data with sizes that would not fit in a
traditional database. Neither is appropriate for transaction/single request
processing."Hbaseis the open source answer to BigTable, Google's highly
scalable distributed database. It is built on top of Hadoop (product), which
implements functionality similar to Google's GFS and Map/Reduce systems. Both
Google's GFS and Hadoop's HDFS provide a mechanism to reliably store large
amounts of data. However, there is not really a mechanism for organizing the
data and accessing only the parts that are of interest to a particular
application.Bigtable (and Hbase) provide a means for organizing and
efficiently accessing t</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Update 2:Jim Wilson helps with theUnderstanding HBase and BigTableby explaining them from a "conceptual standpoint. [sent-2, score-0.109]
</p><p>2 "MapReduce (both Google's and Hadoop's) is ideal for processing huge amounts of data with sizes that would not fit in a traditional database. [sent-4, score-0.611]
</p><p>3 It is built on top of Hadoop (product), which implements functionality similar to Google's GFS and Map/Reduce systems. [sent-7, score-0.177]
</p><p>4 Both Google's GFS and Hadoop's HDFS provide a mechanism to reliably store large amounts of data. [sent-8, score-0.553]
</p><p>5 However, there is not really a mechanism for organizing the data and accessing only the parts that are of interest to a particular application. [sent-9, score-0.798]
</p><p>6 Bigtable (and Hbase) provide a means for organizing and efficiently accessing these large data sets. [sent-10, score-0.641]
</p><p>7 Hbase is still not ready for production, but it's a glimpse into the power that will soon be available to your average website builder. [sent-11, score-0.411]
</p><p>8 They have huge core competencies in data center roll out and they will continually improve their stack. [sent-13, score-0.501]
</p><p>9 It will be interesting to see how these sorts of tools along withSoftware as a Servicecan be leveraged to create the next generation of systems. [sent-14, score-0.394]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hbase', 0.272), ('gfs', 0.27), ('organizing', 0.263), ('hadoop', 0.259), ('accessing', 0.213), ('bigtable', 0.202), ('competencies', 0.193), ('glimpse', 0.193), ('mechanism', 0.173), ('amounts', 0.164), ('google', 0.163), ('conceptual', 0.153), ('infoq', 0.15), ('thenosql', 0.147), ('leveraged', 0.144), ('wilson', 0.142), ('reliably', 0.121), ('jim', 0.119), ('neither', 0.114), ('huge', 0.113), ('hdfs', 0.113), ('sorts', 0.109), ('explaining', 0.109), ('implements', 0.106), ('roll', 0.103), ('update', 0.102), ('ideal', 0.102), ('sizes', 0.095), ('provide', 0.095), ('continually', 0.092), ('appropriate', 0.091), ('leads', 0.091), ('discuss', 0.09), ('ahead', 0.085), ('interest', 0.082), ('interview', 0.082), ('soon', 0.08), ('presentation', 0.079), ('generation', 0.072), ('conference', 0.072), ('mapreduce', 0.071), ('functionality', 0.071), ('still', 0.071), ('fit', 0.071), ('efficiently', 0.07), ('along', 0.069), ('average', 0.067), ('parts', 0.067), ('however', 0.066), ('traditional', 0.066)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="650-tfidf-1" href="../high_scalability-2009/high_scalability-2009-07-02-Product%3A_Hbase.html">650 high scalability-2009-07-02-Product: Hbase</a></p>
<p>Introduction: Update 3:Presentation from theNoSQL Conference:slides,video.Update 2:Jim
Wilson helps with theUnderstanding HBase and BigTableby explaining them from a
"conceptual standpoint."Update:InfoQ interview:HBase Leads Discuss Hadoop,
BigTable and Distributed Databases. "MapReduce (both Google's and Hadoop's) is
ideal for processing huge amounts of data with sizes that would not fit in a
traditional database. Neither is appropriate for transaction/single request
processing."Hbaseis the open source answer to BigTable, Google's highly
scalable distributed database. It is built on top of Hadoop (product), which
implements functionality similar to Google's GFS and Map/Reduce systems. Both
Google's GFS and Hadoop's HDFS provide a mechanism to reliably store large
amounts of data. However, there is not really a mechanism for organizing the
data and accessing only the parts that are of interest to a particular
application.Bigtable (and Hbase) provide a means for organizing and
efficiently accessing t</p><p>2 0.21896197 <a title="650-tfidf-2" href="../high_scalability-2010/high_scalability-2010-11-16-Facebook%27s_New_Real-time_Messaging_System%3A_HBase_to_Store_135%2B_Billion_Messages_a_Month.html">943 high scalability-2010-11-16-Facebook's New Real-time Messaging System: HBase to Store 135+ Billion Messages a Month</a></p>
<p>Introduction: You may have read somewhere that Facebook has introduced a newSocial Inbox
integrating email, IM, SMS,  text messages, on-site Facebook messages. All-in-
all they need to store over 135 billion messages a month. Where do they store
all that stuff? Facebook's Kannan Muthukkaruppan gives the surprise answer
inThe Underlying Technology of Messages:HBase. HBase beat out MySQL,
Cassandra, and a few others.Why a surprise? Facebook created Cassandra and it
was purpose built for an inbox type application, but they found Cassandra's
eventual consistency model wasn't a good match for their new real-time
Messages product. Facebook also has an extensiveMySQL infrastructure, but they
found performance suffered as data set and indexes grew larger. And they could
have built their own, but they chose HBase.HBase is ascaleout table store
supporting very high rates of row-level updates over massive amounts of data.
Exactly what is needed for a Messaging system. HBase is also a column based
key-value sto</p><p>3 0.20411843 <a title="650-tfidf-3" href="../high_scalability-2008/high_scalability-2008-11-22-Google_Architecture.html">448 high scalability-2008-11-22-Google Architecture</a></p>
<p>Introduction: Update 2:Sorting 1 PB with MapReduce. PB is not peanut-butter-and-jelly
misspelled. It's 1 petabyte or 1000 terabytes or 1,000,000 gigabytes.It took
six hours and two minutes to sort 1PB (10 trillion 100-byte records) on 4,000
computersand the results were replicated thrice on 48,000 disks.Update:Greg
Lindenpoints to a new Google articleMapReduce: simplified data processing on
large clusters. Some interesting stats: 100k MapReduce jobs are executed each
day; more than 20 petabytes of data are processed per day; more than 10k
MapReduce programs have been implemented; machines are dual processor with
gigabit ethernet and 4-8 GB of memory.Google is the King of scalability.
Everyone knows Google for their large, sophisticated, and fast searching, but
they don't just shine in search. Their platform approach to building scalable
applications allows them to roll out internet scale applications at an
alarmingly high competition crushing rate. Their goal is always to build a
higher performing h</p><p>4 0.19230427 <a title="650-tfidf-4" href="../high_scalability-2010/high_scalability-2010-03-16-1_Billion_Reasons_Why_Adobe_Chose_HBase_.html">795 high scalability-2010-03-16-1 Billion Reasons Why Adobe Chose HBase </a></p>
<p>Introduction: Cosmin Lehene ďťżwrote two excellent articles on Adobe's experiences with
HBase:Why we're using HBase: Part 1andWhy we're using HBase: Part 2. Adobe
needed ageneric,real-time, structured data storage and processing system that
could handle any data volume, with access times under 50ms, with no downtime
andno data loss. The article goes into great detail about their experiences
with HBase and their evaluation process, providing a "well reasoned impartial
use case from a commercial user". It talks about failure handling,
availability, write performance, read performance, random reads, sequential
scans, and consistency. One of the knocks against HBase has been it's
complexity, as it has many parts that need installation and configuration. All
is not lost according to the Adobe team:HBase is more complex than other
systems (you need Hadoop, Zookeeper, cluster machines have multiple roles). We
believe that for HBase, this is not accidental complexity and that the
argument that "HBase is not a</p><p>5 0.17657292 <a title="650-tfidf-5" href="../high_scalability-2009/high_scalability-2009-05-17-Product%3A_Hadoop.html">601 high scalability-2009-05-17-Product: Hadoop</a></p>
<p>Introduction: Update 5:Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62
Secondsand has itsgreen cred questionedbecause it took 40 times the number of
machines Greenplum used to do the same work.Update 4:Introduction to Pig. Pig
allows you to skip programming Hadoop at the low map-reduce level. You don't
have to know Java. Using the Pig Latin language, which is a scripting data
flow language, you can think about your problem as a data flow program. 10
lines of Pig Latin = 200 lines of Java.Update 3: Scaling Hadoop to4000 nodes
at Yahoo!. 30,000 cores with nearly 16PB of raw disk; sorted 6TB of data
completed in 37 minutes; 14,000 map tasks writes (reads) 360 MB (about 3
blocks) of data into a single file with a total of 5.04 TB for the whole
job.Update 2: HadoopSummit and Data-Intensive Computing Symposium Videos and
Slides. Topics include: Pig, JAQL, Hbase, Hive, Data-Intensive Scalable
Computing, Clouds and ManyCore: The Revolution, Simplicity and Complexity in
Data Systems at Scale, Han</p><p>6 0.17295788 <a title="650-tfidf-6" href="../high_scalability-2009/high_scalability-2009-07-02-Hypertable_is_a_New_BigTable_Clone_that_Runs_on_HDFS_or_KFS.html">647 high scalability-2009-07-02-Hypertable is a New BigTable Clone that Runs on HDFS or KFS</a></p>
<p>7 0.15116072 <a title="650-tfidf-7" href="../high_scalability-2008/high_scalability-2008-04-23-Behind_The_Scenes_of_Google_Scalability.html">309 high scalability-2008-04-23-Behind The Scenes of Google Scalability</a></p>
<p>8 0.14659065 <a title="650-tfidf-8" href="../high_scalability-2009/high_scalability-2009-06-11-Yahoo%21_Distribution_of_Hadoop.html">627 high scalability-2009-06-11-Yahoo! Distribution of Hadoop</a></p>
<p>9 0.14640118 <a title="650-tfidf-9" href="../high_scalability-2008/high_scalability-2008-10-15-Hadoop_-_A_Primer.html">414 high scalability-2008-10-15-Hadoop - A Primer</a></p>
<p>10 0.14199482 <a title="650-tfidf-10" href="../high_scalability-2012/high_scalability-2012-02-07-Hypertable_Routs_HBase_in_Performance_Test_--_HBase_Overwhelmed_by_Garbage_Collection.html">1189 high scalability-2012-02-07-Hypertable Routs HBase in Performance Test -- HBase Overwhelmed by Garbage Collection</a></p>
<p>11 0.14023875 <a title="650-tfidf-11" href="../high_scalability-2011/high_scalability-2011-07-27-Making_Hadoop_1000x_Faster_for_Graph_Problems.html">1088 high scalability-2011-07-27-Making Hadoop 1000x Faster for Graph Problems</a></p>
<p>12 0.1324828 <a title="650-tfidf-12" href="../high_scalability-2011/high_scalability-2011-01-04-Map-Reduce_With_Ruby_Using_Hadoop.html">968 high scalability-2011-01-04-Map-Reduce With Ruby Using Hadoop</a></p>
<p>13 0.13165298 <a title="650-tfidf-13" href="../high_scalability-2011/high_scalability-2011-03-08-Medialets_Architecture_-__Defeating_the_Daunting_Mobile_Device_Data_Deluge.html">1000 high scalability-2011-03-08-Medialets Architecture -  Defeating the Daunting Mobile Device Data Deluge</a></p>
<p>14 0.13108623 <a title="650-tfidf-14" href="../high_scalability-2012/high_scalability-2012-08-28-Making_Hadoop_Run_Faster.html">1313 high scalability-2012-08-28-Making Hadoop Run Faster</a></p>
<p>15 0.12535018 <a title="650-tfidf-15" href="../high_scalability-2011/high_scalability-2011-03-22-Facebook%27s_New_Realtime_Analytics_System%3A_HBase_to_Process_20_Billion_Events_Per_Day.html">1008 high scalability-2011-03-22-Facebook's New Realtime Analytics System: HBase to Process 20 Billion Events Per Day</a></p>
<p>16 0.12500702 <a title="650-tfidf-16" href="../high_scalability-2008/high_scalability-2008-05-27-How_I_Learned_to_Stop_Worrying_and_Love_Using_a_Lot_of_Disk_Space_to_Scale.html">327 high scalability-2008-05-27-How I Learned to Stop Worrying and Love Using a Lot of Disk Space to Scale</a></p>
<p>17 0.12439718 <a title="650-tfidf-17" href="../high_scalability-2008/high_scalability-2008-01-28-Howto_setup_GFS-GNBD.html">227 high scalability-2008-01-28-Howto setup GFS-GNBD</a></p>
<p>18 0.11830788 <a title="650-tfidf-18" href="../high_scalability-2009/high_scalability-2009-02-21-Google_AppEngine_-_A_Second_Look.html">517 high scalability-2009-02-21-Google AppEngine - A Second Look</a></p>
<p>19 0.11685464 <a title="650-tfidf-19" href="../high_scalability-2008/high_scalability-2008-09-28-Product%3A_Happy_%3D_Hadoop_%2B_Python.html">397 high scalability-2008-09-28-Product: Happy = Hadoop + Python</a></p>
<p>20 0.10878265 <a title="650-tfidf-20" href="../high_scalability-2014/high_scalability-2014-01-28-How_Next_Big_Sound_Tracks_Over_a_Trillion_Song_Plays%2C_Likes%2C_and_More_Using_a_Version_Control_System_for_Hadoop_Data.html">1586 high scalability-2014-01-28-How Next Big Sound Tracks Over a Trillion Song Plays, Likes, and More Using a Version Control System for Hadoop Data</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.142), (1, 0.041), (2, 0.026), (3, 0.087), (4, 0.038), (5, 0.093), (6, 0.036), (7, 0.025), (8, 0.135), (9, 0.12), (10, 0.068), (11, -0.018), (12, 0.077), (13, -0.106), (14, 0.067), (15, -0.025), (16, -0.095), (17, -0.069), (18, 0.022), (19, -0.038), (20, 0.04), (21, 0.098), (22, 0.064), (23, -0.056), (24, 0.021), (25, 0.008), (26, 0.113), (27, 0.051), (28, -0.066), (29, 0.032), (30, 0.061), (31, 0.103), (32, 0.078), (33, -0.05), (34, -0.015), (35, 0.07), (36, -0.017), (37, 0.022), (38, 0.009), (39, 0.023), (40, 0.04), (41, 0.035), (42, 0.026), (43, -0.011), (44, 0.006), (45, -0.015), (46, 0.001), (47, -0.007), (48, 0.002), (49, 0.037)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9770599 <a title="650-lsi-1" href="../high_scalability-2009/high_scalability-2009-07-02-Product%3A_Hbase.html">650 high scalability-2009-07-02-Product: Hbase</a></p>
<p>Introduction: Update 3:Presentation from theNoSQL Conference:slides,video.Update 2:Jim
Wilson helps with theUnderstanding HBase and BigTableby explaining them from a
"conceptual standpoint."Update:InfoQ interview:HBase Leads Discuss Hadoop,
BigTable and Distributed Databases. "MapReduce (both Google's and Hadoop's) is
ideal for processing huge amounts of data with sizes that would not fit in a
traditional database. Neither is appropriate for transaction/single request
processing."Hbaseis the open source answer to BigTable, Google's highly
scalable distributed database. It is built on top of Hadoop (product), which
implements functionality similar to Google's GFS and Map/Reduce systems. Both
Google's GFS and Hadoop's HDFS provide a mechanism to reliably store large
amounts of data. However, there is not really a mechanism for organizing the
data and accessing only the parts that are of interest to a particular
application.Bigtable (and Hbase) provide a means for organizing and
efficiently accessing t</p><p>2 0.79205608 <a title="650-lsi-2" href="../high_scalability-2009/high_scalability-2009-07-02-Hypertable_is_a_New_BigTable_Clone_that_Runs_on_HDFS_or_KFS.html">647 high scalability-2009-07-02-Hypertable is a New BigTable Clone that Runs on HDFS or KFS</a></p>
<p>Introduction: Update 3: Presentation from theNoSQL conference:slides,video 1,video 2.Update
2: The folks at Hypertable would like you to know that Hypertable is now
officiallysponsored by Baidu, China's Leading Search Engine.As a sponsor of
Hypertable, Baidu has committed an industrious team of engineers, numerous
servers, and support resources to improve the quality and development of the
open source technology.Update: InfoQ interview onHypertable Lead Discusses
Hadoop and Distributed Databases. Hypertable differs from HBase in that it is
a higher performance implementation of Bigtable.Skrentabloggives the heads up
onHypertable,Zvents'open-source BigTable clone. It's written in C++ and can
run on top of either HDFS or KFS. Performance looks encouraging at28M rows of
data inserted at a per-node write rate of 7mb/sec.</p><p>3 0.77125609 <a title="650-lsi-3" href="../high_scalability-2009/high_scalability-2009-05-17-Product%3A_Hadoop.html">601 high scalability-2009-05-17-Product: Hadoop</a></p>
<p>Introduction: Update 5:Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62
Secondsand has itsgreen cred questionedbecause it took 40 times the number of
machines Greenplum used to do the same work.Update 4:Introduction to Pig. Pig
allows you to skip programming Hadoop at the low map-reduce level. You don't
have to know Java. Using the Pig Latin language, which is a scripting data
flow language, you can think about your problem as a data flow program. 10
lines of Pig Latin = 200 lines of Java.Update 3: Scaling Hadoop to4000 nodes
at Yahoo!. 30,000 cores with nearly 16PB of raw disk; sorted 6TB of data
completed in 37 minutes; 14,000 map tasks writes (reads) 360 MB (about 3
blocks) of data into a single file with a total of 5.04 TB for the whole
job.Update 2: HadoopSummit and Data-Intensive Computing Symposium Videos and
Slides. Topics include: Pig, JAQL, Hbase, Hive, Data-Intensive Scalable
Computing, Clouds and ManyCore: The Revolution, Simplicity and Complexity in
Data Systems at Scale, Han</p><p>4 0.73278224 <a title="650-lsi-4" href="../high_scalability-2008/high_scalability-2008-10-15-Hadoop_-_A_Primer.html">414 high scalability-2008-10-15-Hadoop - A Primer</a></p>
<p>Introduction: Hadoop is a distributed computing platform written in Java. It incorporates
features similar to those of theGoogle File System and of MapReduce to process
vast amounts of data"Hadoop is a Free Java software framework that supports
data intensive distributed applications running on large clusters of commodity
computers. It enables applications to easily scale out to thousands of nodes
and petabytes of data" (Wikipedia)* What platform does Hadoop run on?* Java
1.5.x or higher, preferably from Sun* Linux* Windows for development* Solaris</p><p>5 0.71126407 <a title="650-lsi-5" href="../high_scalability-2010/high_scalability-2010-03-16-1_Billion_Reasons_Why_Adobe_Chose_HBase_.html">795 high scalability-2010-03-16-1 Billion Reasons Why Adobe Chose HBase </a></p>
<p>Introduction: Cosmin Lehene ďťżwrote two excellent articles on Adobe's experiences with
HBase:Why we're using HBase: Part 1andWhy we're using HBase: Part 2. Adobe
needed ageneric,real-time, structured data storage and processing system that
could handle any data volume, with access times under 50ms, with no downtime
andno data loss. The article goes into great detail about their experiences
with HBase and their evaluation process, providing a "well reasoned impartial
use case from a commercial user". It talks about failure handling,
availability, write performance, read performance, random reads, sequential
scans, and consistency. One of the knocks against HBase has been it's
complexity, as it has many parts that need installation and configuration. All
is not lost according to the Adobe team:HBase is more complex than other
systems (you need Hadoop, Zookeeper, cluster machines have multiple roles). We
believe that for HBase, this is not accidental complexity and that the
argument that "HBase is not a</p><p>6 0.69300318 <a title="650-lsi-6" href="../high_scalability-2008/high_scalability-2008-11-14-Paper%3A_Pig_Latin%3A_A_Not-So-Foreign_Language_for_Data_Processing.html">443 high scalability-2008-11-14-Paper: Pig Latin: A Not-So-Foreign Language for Data Processing</a></p>
<p>7 0.69098383 <a title="650-lsi-7" href="../high_scalability-2009/high_scalability-2009-08-03-Building_a_Data_Intensive_Web_Application_with_Cloudera%2C_Hadoop%2C_Hive%2C_Pig%2C_and_EC2.html">669 high scalability-2009-08-03-Building a Data Intensive Web Application with Cloudera, Hadoop, Hive, Pig, and EC2</a></p>
<p>8 0.69035017 <a title="650-lsi-8" href="../high_scalability-2011/high_scalability-2011-01-04-Map-Reduce_With_Ruby_Using_Hadoop.html">968 high scalability-2011-01-04-Map-Reduce With Ruby Using Hadoop</a></p>
<p>9 0.67473835 <a title="650-lsi-9" href="../high_scalability-2008/high_scalability-2008-02-19-Hadoop_Getting_Closer_to_1.0_Release.html">254 high scalability-2008-02-19-Hadoop Getting Closer to 1.0 Release</a></p>
<p>10 0.66717213 <a title="650-lsi-10" href="../high_scalability-2008/high_scalability-2008-10-15-Need_help_with_your_Hadoop_deployment%3F_This_company_may_help%21.html">415 high scalability-2008-10-15-Need help with your Hadoop deployment? This company may help!</a></p>
<p>11 0.66574693 <a title="650-lsi-11" href="../high_scalability-2009/high_scalability-2009-06-11-Yahoo%21_Distribution_of_Hadoop.html">627 high scalability-2009-06-11-Yahoo! Distribution of Hadoop</a></p>
<p>12 0.66437805 <a title="650-lsi-12" href="../high_scalability-2013/high_scalability-2013-09-05-Paper%3A_MillWheel%3A_Fault-Tolerant_Stream_Processing_at_Internet_Scale.html">1512 high scalability-2013-09-05-Paper: MillWheel: Fault-Tolerant Stream Processing at Internet Scale</a></p>
<p>13 0.65567482 <a title="650-lsi-13" href="../high_scalability-2012/high_scalability-2012-08-28-Making_Hadoop_Run_Faster.html">1313 high scalability-2012-08-28-Making Hadoop Run Faster</a></p>
<p>14 0.65421009 <a title="650-lsi-14" href="../high_scalability-2010/high_scalability-2010-08-04-Dremel%3A_Interactive_Analysis_of_Web-Scale_Datasets_-_Data_as_a_Programming_Paradigm.html">871 high scalability-2010-08-04-Dremel: Interactive Analysis of Web-Scale Datasets - Data as a Programming Paradigm</a></p>
<p>15 0.63730276 <a title="650-lsi-15" href="../high_scalability-2009/high_scalability-2009-01-04-Paper%3A_MapReduce%3A_Simplified_Data_Processing_on_Large_Clusters.html">483 high scalability-2009-01-04-Paper: MapReduce: Simplified Data Processing on Large Clusters</a></p>
<p>16 0.62644339 <a title="650-lsi-16" href="../high_scalability-2013/high_scalability-2013-04-24-Strategy%3A_Using_Lots_of_RAM_Often_Cheaper_than_Using_a_Hadoop_Cluster.html">1445 high scalability-2013-04-24-Strategy: Using Lots of RAM Often Cheaper than Using a Hadoop Cluster</a></p>
<p>17 0.62375689 <a title="650-lsi-17" href="../high_scalability-2012/high_scalability-2012-06-15-Stuff_The_Internet_Says_On_Scalability_For_June_15%2C_2012.html">1265 high scalability-2012-06-15-Stuff The Internet Says On Scalability For June 15, 2012</a></p>
<p>18 0.61567795 <a title="650-lsi-18" href="../high_scalability-2009/high_scalability-2009-07-30-Learn_How_to_Think_at_Scale.html">666 high scalability-2009-07-30-Learn How to Think at Scale</a></p>
<p>19 0.61461675 <a title="650-lsi-19" href="../high_scalability-2008/high_scalability-2008-01-13-Google_Reveals_New_MapReduce_Stats.html">211 high scalability-2008-01-13-Google Reveals New MapReduce Stats</a></p>
<p>20 0.61413592 <a title="650-lsi-20" href="../high_scalability-2013/high_scalability-2013-01-07-Analyzing_billions_of_credit_card_transactions_and_serving_low-latency_insights_in_the_cloud.html">1382 high scalability-2013-01-07-Analyzing billions of credit card transactions and serving low-latency insights in the cloud</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.153), (2, 0.107), (25, 0.159), (30, 0.014), (40, 0.024), (61, 0.083), (77, 0.044), (79, 0.304)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95172888 <a title="650-lda-1" href="../high_scalability-2009/high_scalability-2009-07-02-Product%3A_Hbase.html">650 high scalability-2009-07-02-Product: Hbase</a></p>
<p>Introduction: Update 3:Presentation from theNoSQL Conference:slides,video.Update 2:Jim
Wilson helps with theUnderstanding HBase and BigTableby explaining them from a
"conceptual standpoint."Update:InfoQ interview:HBase Leads Discuss Hadoop,
BigTable and Distributed Databases. "MapReduce (both Google's and Hadoop's) is
ideal for processing huge amounts of data with sizes that would not fit in a
traditional database. Neither is appropriate for transaction/single request
processing."Hbaseis the open source answer to BigTable, Google's highly
scalable distributed database. It is built on top of Hadoop (product), which
implements functionality similar to Google's GFS and Map/Reduce systems. Both
Google's GFS and Hadoop's HDFS provide a mechanism to reliably store large
amounts of data. However, there is not really a mechanism for organizing the
data and accessing only the parts that are of interest to a particular
application.Bigtable (and Hbase) provide a means for organizing and
efficiently accessing t</p><p>2 0.91782618 <a title="650-lda-2" href="../high_scalability-2008/high_scalability-2008-10-10-Useful_Corporate_Blogs_that_Talk_About_Scalability.html">408 high scalability-2008-10-10-Useful Corporate Blogs that Talk About Scalability</a></p>
<p>Introduction: Some intrepid company blogs are posting their technical challenges and how
they solve them. I wish more would open up and talk about what they are doing
as it helps everyone move forward. Here are a few blogs documenting their
encounters with the bleeding edge:FlickrDiggLinkedInFacebookAmazon Web
Services blogTwitter blogReddit blogPhotobucket blogSecond Life
blogPlentyofFish blogJoyent's BlogAny others that should be added?</p><p>3 0.90329522 <a title="650-lda-3" href="../high_scalability-2010/high_scalability-2010-02-25-Paper%3A_High_Performance_Scalable_Data_Stores_.html">784 high scalability-2010-02-25-Paper: High Performance Scalable Data Stores </a></p>
<p>Introduction: The world of scalable databases is not a simple one. They come in every race,
creed, and color. Rick Cattell has brought some harmony to that world by
publishingHigh Performance Scalable Data Stores, a nicely detailed one stop
shop paper comparing scalable databases soley on the content of their
character. Ironically, the first step in that evaluation is dividing the world
into four groups:Key-value stores: Redis, Scalaris, Voldmort, and
Riak.Document stores: Couch DB, MongoDB, and SimpleDB.Record stores: BigTable,
HBase, HyperTable, and Cassandra.Scalable RDBMSs: MySQL Cluster, ScaleDB,
Drizzle, and VoltDB.The paper describes each system and then compares them on
the dimensions of Concurrency Control, Data Storage Replication, Transaction
Model, General Comments, Maturity, K-hits, License Language.And the winner is:
there are no winners. Yet. Rick concludes by pointing to a great convergence:I
believe that a few of these systems will gain critical mass and key players,
and will pull a</p><p>4 0.89509767 <a title="650-lda-4" href="../high_scalability-2012/high_scalability-2012-07-05-10_Golden_Principles_For_Building_Successful_Mobile-Web_Applications.html">1277 high scalability-2012-07-05-10 Golden Principles For Building Successful Mobile-Web Applications</a></p>
<p>Introduction: Wildly popular VC blogger Fred Wilson defines in an excellent 27 minute
videothe ten most important criteria he uses when deciding to give the gold,
that is, fund a web application. Note, this video isfrom 2010, so no doubt the
ideas are still valid, but the importance of mobile vs web apps has probably
shifted to mobile, as Mr. Wilson says in a recent post: mobile is growing like
a weed. Speed- speed is more than a feature, it's a requirement. Mainstream
users are unforgiving. If something is slow they won't use it. Pingdom is used
to track speed across their portfolio. A trend they've noticed is that as an
application slows down they don't grow as quickly. Instant Utility- a service
must be instantly useful to users. Lengthy setup and configuration is a
killer. Tricks like crawling the web to populate information you expect to get
from your users later makes the service initially useful. YouTube won, for
example, with instant availability of uploaded video.Voice- Consumer software
is</p><p>5 0.88645059 <a title="650-lda-5" href="../high_scalability-2012/high_scalability-2012-01-05-Shutterfly_Saw_a_Speedup_of_500%25_With_Flashcache.html">1169 high scalability-2012-01-05-Shutterfly Saw a Speedup of 500% With Flashcache</a></p>
<p>Introduction: In the "should I or shouldn't I" debate around deploying SSD, it always helps
to have real-world data. Fiesta! with alive-blog summary of a presentation by
Kenny Gorman onShutterfly on MongoDB Performance Tuning.What if you still need
more performance after doing all of this tuning? One option is to use SSDs.
Shutterfly usesFacebook's flashcache: kernel module to cache data on SSD.
Designed for MySQL/InnoDB. SSD in front of a disk, but exposed as a single
mount point. This only makes sense when you have lots of physical I/O.
Shutterfly saw a speedup of 500% w/ flashcache. A benefit is that you can
delay sharding: less complexity.The wholeseries of posts has a lot of great
information and is worth a longer look, especially if you are considering
using MongoDB. Related ArticlesSlides forMongoSF 2011 slides: MongoDB
Performance TuningSSD+HDD sharding setup for large and permanently growing
collectionsImlementing MongoDB at Shutterfly by Kenny GormanMongoSV follow
upMongoSV 2011 Conference</p><p>6 0.88483834 <a title="650-lda-6" href="../high_scalability-2010/high_scalability-2010-08-04-Dremel%3A_Interactive_Analysis_of_Web-Scale_Datasets_-_Data_as_a_Programming_Paradigm.html">871 high scalability-2010-08-04-Dremel: Interactive Analysis of Web-Scale Datasets - Data as a Programming Paradigm</a></p>
<p>7 0.88318086 <a title="650-lda-7" href="../high_scalability-2009/high_scalability-2009-08-13-Reconnoiter_-_Large-Scale_Trending_and_Fault-Detection.html">680 high scalability-2009-08-13-Reconnoiter - Large-Scale Trending and Fault-Detection</a></p>
<p>8 0.88041627 <a title="650-lda-8" href="../high_scalability-2013/high_scalability-2013-02-08-Stuff_The_Internet_Says_On_Scalability_For_February_8%2C_2013.html">1403 high scalability-2013-02-08-Stuff The Internet Says On Scalability For February 8, 2013</a></p>
<p>9 0.88024437 <a title="650-lda-9" href="../high_scalability-2011/high_scalability-2011-08-18-Paper%3A_The_Akamai_Network_-_61%2C000_servers%2C_1%2C000__networks%2C_70_countries_.html">1100 high scalability-2011-08-18-Paper: The Akamai Network - 61,000 servers, 1,000  networks, 70 countries </a></p>
<p>10 0.86930943 <a title="650-lda-10" href="../high_scalability-2013/high_scalability-2013-03-08-Stuff_The_Internet_Says_On_Scalability_For_March_8%2C_2013.html">1420 high scalability-2013-03-08-Stuff The Internet Says On Scalability For March 8, 2013</a></p>
<p>11 0.86861092 <a title="650-lda-11" href="../high_scalability-2008/high_scalability-2008-11-22-Google_Architecture.html">448 high scalability-2008-11-22-Google Architecture</a></p>
<p>12 0.86766112 <a title="650-lda-12" href="../high_scalability-2013/high_scalability-2013-07-19-Stuff_The_Internet_Says_On_Scalability_For_July_19%2C_2013.html">1494 high scalability-2013-07-19-Stuff The Internet Says On Scalability For July 19, 2013</a></p>
<p>13 0.86458099 <a title="650-lda-13" href="../high_scalability-2007/high_scalability-2007-10-02-Some_Real_Financial_Numbers_for_Your_Startup.html">107 high scalability-2007-10-02-Some Real Financial Numbers for Your Startup</a></p>
<p>14 0.86212331 <a title="650-lda-14" href="../high_scalability-2010/high_scalability-2010-03-02-Using_the_Ambient_Cloud_as_an_Application_Runtime.html">786 high scalability-2010-03-02-Using the Ambient Cloud as an Application Runtime</a></p>
<p>15 0.85861421 <a title="650-lda-15" href="../high_scalability-2008/high_scalability-2008-05-19-Twitter_as_a_scalability_case_study.html">323 high scalability-2008-05-19-Twitter as a scalability case study</a></p>
<p>16 0.85830826 <a title="650-lda-16" href="../high_scalability-2008/high_scalability-2008-08-27-Updating_distributed_web_applications.html">372 high scalability-2008-08-27-Updating distributed web applications</a></p>
<p>17 0.85285598 <a title="650-lda-17" href="../high_scalability-2013/high_scalability-2013-07-01-PRISM%3A_The_Amazingly_Low_Cost_of_%C2%ADUsing_BigData_to_Know_More_About_You_in_Under_a_Minute.html">1485 high scalability-2013-07-01-PRISM: The Amazingly Low Cost of ­Using BigData to Know More About You in Under a Minute</a></p>
<p>18 0.8520261 <a title="650-lda-18" href="../high_scalability-2009/high_scalability-2009-04-26-Map-Reduce_for_Machine_Learning_on_Multicore.html">581 high scalability-2009-04-26-Map-Reduce for Machine Learning on Multicore</a></p>
<p>19 0.85194349 <a title="650-lda-19" href="../high_scalability-2008/high_scalability-2008-09-05-Product%3A_Tungsten_Replicator.html">380 high scalability-2008-09-05-Product: Tungsten Replicator</a></p>
<p>20 0.84597552 <a title="650-lda-20" href="../high_scalability-2012/high_scalability-2012-01-25-Google_Goes_MoreSQL_with_Tenzing_-__SQL_Over_MapReduce_.html">1181 high scalability-2012-01-25-Google Goes MoreSQL with Tenzing -  SQL Over MapReduce </a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
