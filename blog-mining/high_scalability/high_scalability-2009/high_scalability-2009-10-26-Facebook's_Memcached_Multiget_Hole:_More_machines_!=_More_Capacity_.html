<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>728 high scalability-2009-10-26-Facebook's Memcached Multiget Hole: More machines != More Capacity </title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2009" href="../home/high_scalability-2009_home.html">high_scalability-2009</a> <a title="high_scalability-2009-728" href="#">high_scalability-2009-728</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>728 high scalability-2009-10-26-Facebook's Memcached Multiget Hole: More machines != More Capacity </h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2009-728-html" href="http://highscalability.com//blog/2009/10/26/facebooks-memcached-multiget-hole-more-machines-more-capacit.html">html</a></p><p>Introduction: When you are on the bleeding edge of scale like Facebook is, you run into some
interesting problems. As of 2008Facebook had over 800 memcached
serverssupplying over 28 terabytes of cache. With those staggering numbers
it's a fair bet to think they've seen their share of Dr. House worthy
memcached problems.Jeff Rothschild, Vice President of Technology at
Facebook,describes one such problemthey've dubbed theMultiget Hole.You fall
into the multiget hole when memcached servers areCPU bound, adding more
memcached servers seems like the right way to add more capacity so more
requests can be served, but against all logic adding servers doesn't help
serve more requests. This puts you in a hole that simply adding more servers
can't dig you out of. What's the treatment?Dr. House would immediately notice
the hidden clue, we are talking requests not memory. We aren't running out of
memory to store stuff, we arerunning out of CPU power to process requests.
What happens when you add more servers is</p><br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('multiget', 0.395), ('friends', 0.335), ('hole', 0.273), ('requests', 0.256), ('adding', 0.209), ('memcached', 0.193), ('keys', 0.185), ('request', 0.152), ('rothschild', 0.145), ('pool', 0.143), ('cpu', 0.135), ('servers', 0.132), ('retrieved', 0.114), ('number', 0.108), ('half', 0.107), ('classic', 0.102), ('server', 0.101), ('house', 0.099), ('add', 0.09), ('bound', 0.086)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999982 <a title="728-tfidf-1" href="../high_scalability-2009/high_scalability-2009-10-26-Facebook%27s_Memcached_Multiget_Hole%3A_More_machines_%21%3D_More_Capacity_.html">728 high scalability-2009-10-26-Facebook's Memcached Multiget Hole: More machines != More Capacity </a></p>
<p>Introduction: When you are on the bleeding edge of scale like Facebook is, you run into some
interesting problems. As of 2008Facebook had over 800 memcached
serverssupplying over 28 terabytes of cache. With those staggering numbers
it's a fair bet to think they've seen their share of Dr. House worthy
memcached problems.Jeff Rothschild, Vice President of Technology at
Facebook,describes one such problemthey've dubbed theMultiget Hole.You fall
into the multiget hole when memcached servers areCPU bound, adding more
memcached servers seems like the right way to add more capacity so more
requests can be served, but against all logic adding servers doesn't help
serve more requests. This puts you in a hole that simply adding more servers
can't dig you out of. What's the treatment?Dr. House would immediately notice
the hidden clue, we are talking requests not memory. We aren't running out of
memory to store stuff, we arerunning out of CPU power to process requests.
What happens when you add more servers is</p><p>2 0.19316085 <a title="728-tfidf-2" href="../high_scalability-2009/high_scalability-2009-10-13-Why_are_Facebook%2C_Digg%2C_and_Twitter_so_hard_to_scale%3F.html">721 high scalability-2009-10-13-Why are Facebook, Digg, and Twitter so hard to scale?</a></p>
<p>Introduction: Real-time social graphs (connectivity between people, places, and things).
That's why scaling Facebook is hardsays Jeff Rothschild, Vice President of
Technology at Facebook. Social networking sites like Facebook, Digg, and
Twitter are simply harder than traditional websites to scale. Why is that? Why
would social networking sites be any more difficult to scale than traditional
web sites? Let's find out.Traditional websites are easier to scale than social
networking sites for two reasons:They usually access only their own data and
common cached data.Only 1-2% of users are active on the site at one
time.Imagine a huge site like Yahoo. When you come to Yahoo they can get your
profile record with one get and that's enough to build your view of the
website for you. It's relatively straightforward to scale systems based around
single records usingdistributed hashing schemes. And since only a few percent
of the people are on the site at once it takes comparatively little RAM cache
to handle a</p><p>3 0.17484626 <a title="728-tfidf-3" href="../high_scalability-2008/high_scalability-2008-05-31-memcached_and_Storage_of_Friend_list.html">337 high scalability-2008-05-31-memcached and Storage of Friend list</a></p>
<p>Introduction: My first post, please be gentle. I know it is long. You are all like doctors -
the more info, the better the diagnosis.-----------What is the best way to
store a list of all of your friends in the memcached cache (a simple boolean
saying “yes this user is your friend”, or “no”)? Think Robert Scoble (26,000+
“friends”) on Twitter.com. He views a list of ALL existing users, and in this
list, his friends are highlighted.I came up with 4 possible methods:--store in
memcache as an array, a list of all the "yes" friend ID's--store your friend
ID's as individual elements.--store as a hash of arrays based on last 3 digits
of friend's ID -- so have up to 1000 arrays for you.--comma-delimited string
of ID's as one elementI'm using the second one because I think it is faster to
update. The single array or hash of arrays feels like too much overhead
calculating and updating – and even just loading – to check for existence of a
friend.The key is FRIEND[small ID#]_[big ID#]. The value is 1.This way</p><p>4 0.16985953 <a title="728-tfidf-4" href="../high_scalability-2008/high_scalability-2008-08-04-A_Bunch_of_Great_Strategies_for_Using_Memcached_and_MySQL_Better_Together.html">360 high scalability-2008-08-04-A Bunch of Great Strategies for Using Memcached and MySQL Better Together</a></p>
<p>Introduction: The primero recommendation for speeding up a website is almost always to add
cache and more cache. And after that add a little more cache just in case.
Memcached is almost always given as the recommended cache to use. What we
don't often hear is how to effectively use a cache in our own products. MySQL
hosted two excellent webinars (referenced below) on the subject of how to
deploy and use memcached. The star of the show, other than MySQL of course, is
Farhan Mashraqi of Fotolog. You may recall we did an earlier article on
Fotolog inSecrets to Fotolog's Scaling Success, which was one of my personal
favorites.Fotolog, as they themselves point out, is probably the largest site
nobody has ever heard of, pulling in more page views than even Flickr. Fotolog
has 51 instances of memcached on 21 servers with 175G in use and 254G
available. As a large successful photo-blogging site they have very demanding
performance and scaling requirements. To meet those requirements they've
developed a soph</p><p>5 0.14802691 <a title="728-tfidf-5" href="../high_scalability-2012/high_scalability-2012-06-18-Google_on_Latency_Tolerant_Systems%3A_Making_a_Predictable_Whole_Out_of_Unpredictable_Parts___.html">1266 high scalability-2012-06-18-Google on Latency Tolerant Systems: Making a Predictable Whole Out of Unpredictable Parts   </a></p>
<p>Introduction: InTaming The Long Latency Tailwe coveredLuiz Barroso's exploration of the long
tail latency (some operations are really slow) problems generated by large
fanout architectures (a request is composed of potentially thousands of other
requests). You may have noticed there weren't a lot of solutions. That's where
a talk I attended,Achieving Rapid Response Times in Large Online
Services(slide deck), byJeff Dean, also of Google, comes in:In this talk, I'll
describe a collection of techniques and practices lowering response times in
large distributed systems whose components run on shared clusters of machines,
where pieces of these systems are subject to interference by other tasks, and
where unpredictable latency hiccups are the norm, not the exception.The goal
is to use software techniques to reduce variability given the increasing
variability in underlying hardware, the need to handle dynamic workloads on a
shared infrastructure, and the need to use large fanout architectures to
operate at</p><p>6 0.14784361 <a title="728-tfidf-6" href="../high_scalability-2014/high_scalability-2014-05-12-4_Architecture_Issues_When_Scaling_Web_Applications%3A_Bottlenecks%2C_Database%2C_CPU%2C_IO.html">1646 high scalability-2014-05-12-4 Architecture Issues When Scaling Web Applications: Bottlenecks, Database, CPU, IO</a></p>
<p>7 0.14222062 <a title="728-tfidf-7" href="../high_scalability-2008/high_scalability-2008-05-02-Friends_for_Sale_Architecture_-_A_300_Million_Page_View-Month_Facebook_RoR_App.html">313 high scalability-2008-05-02-Friends for Sale Architecture - A 300 Million Page View-Month Facebook RoR App</a></p>
<p>8 0.14061299 <a title="728-tfidf-8" href="../high_scalability-2014/high_scalability-2014-02-05-Little%E2%80%99s_Law%2C_Scalability_and_Fault_Tolerance%3A_The_OS_is_your_bottleneck._What_you_can_do%3F.html">1591 high scalability-2014-02-05-Little’s Law, Scalability and Fault Tolerance: The OS is your bottleneck. What you can do?</a></p>
<p>9 0.13041212 <a title="728-tfidf-9" href="../high_scalability-2009/high_scalability-2009-06-27-Scaling_Twitter%3A_Making_Twitter_10000_Percent_Faster.html">639 high scalability-2009-06-27-Scaling Twitter: Making Twitter 10000 Percent Faster</a></p>
<p>10 0.12554626 <a title="728-tfidf-10" href="../high_scalability-2010/high_scalability-2010-04-12-Poppen.de_Architecture.html">808 high scalability-2010-04-12-Poppen.de Architecture</a></p>
<p>11 0.12382018 <a title="728-tfidf-11" href="../high_scalability-2008/high_scalability-2008-12-13-Strategy%3A_Facebook_Tweaks_to_Handle_6_Time_as_Many_Memcached_Requests.html">464 high scalability-2008-12-13-Strategy: Facebook Tweaks to Handle 6 Time as Many Memcached Requests</a></p>
<p>12 0.12333869 <a title="728-tfidf-12" href="../high_scalability-2013/high_scalability-2013-02-27-42_Monster_Problems_that_Attack_as_Loads_Increase.html">1413 high scalability-2013-02-27-42 Monster Problems that Attack as Loads Increase</a></p>
<p>13 0.1231522 <a title="728-tfidf-13" href="../high_scalability-2009/high_scalability-2009-10-28-And_the_winner_is%3A_MySQL_or_Memcached_or_Tokyo_Tyrant%3F.html">729 high scalability-2009-10-28-And the winner is: MySQL or Memcached or Tokyo Tyrant?</a></p>
<p>14 0.12184554 <a title="728-tfidf-14" href="../high_scalability-2012/high_scalability-2012-03-07-Scale_Indefinitely_on_S3_With_These_Secrets_of_the_S3_Masters.html">1205 high scalability-2012-03-07-Scale Indefinitely on S3 With These Secrets of the S3 Masters</a></p>
<p>15 0.12140059 <a title="728-tfidf-15" href="../high_scalability-2010/high_scalability-2010-12-20-Netflix%3A_Use_Less_Chatty_Protocols_in_the_Cloud_-_Plus_26_Fixes.html">960 high scalability-2010-12-20-Netflix: Use Less Chatty Protocols in the Cloud - Plus 26 Fixes</a></p>
<p>16 0.11876667 <a title="728-tfidf-16" href="../high_scalability-2013/high_scalability-2013-03-11-Low_Level_Scalability_Solutions_-_The_Conditioning_Collection.html">1421 high scalability-2013-03-11-Low Level Scalability Solutions - The Conditioning Collection</a></p>
<p>17 0.11853931 <a title="728-tfidf-17" href="../high_scalability-2013/high_scalability-2013-04-10-Check_Yourself_Before_You_Wreck_Yourself_-_Avocado%27s_5_Early_Stages_of_Architecture_Evolution.html">1438 high scalability-2013-04-10-Check Yourself Before You Wreck Yourself - Avocado's 5 Early Stages of Architecture Evolution</a></p>
<p>18 0.11665728 <a title="728-tfidf-18" href="../high_scalability-2014/high_scalability-2014-02-10-13_Simple_Tricks_for_Scaling_Python_and_Django_with_Apache_from_HackerEarth.html">1593 high scalability-2014-02-10-13 Simple Tricks for Scaling Python and Django with Apache from HackerEarth</a></p>
<p>19 0.11348413 <a title="728-tfidf-19" href="../high_scalability-2009/high_scalability-2009-07-25-Latency_is_Everywhere_and_it_Costs_You_Sales_-_How_to_Crush_it.html">661 high scalability-2009-07-25-Latency is Everywhere and it Costs You Sales - How to Crush it</a></p>
<p>20 0.1129206 <a title="728-tfidf-20" href="../high_scalability-2009/high_scalability-2009-07-27-Handle_700_Percent_More_Requests_Using_Squid_and_APC_Cache.html">662 high scalability-2009-07-27-Handle 700 Percent More Requests Using Squid and APC Cache</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.173), (1, 0.105), (2, -0.009), (3, -0.134), (4, -0.014), (5, -0.01), (6, 0.055), (7, 0.049), (8, -0.067), (9, -0.029), (10, 0.014), (11, 0.035), (12, 0.013), (13, 0.054), (14, -0.026), (15, -0.001), (16, 0.002), (17, 0.011), (18, 0.02), (19, 0.048), (20, 0.006), (21, -0.005), (22, 0.079), (23, -0.048), (24, 0.065), (25, -0.007), (26, 0.061), (27, 0.04), (28, -0.004), (29, -0.043), (30, 0.036), (31, -0.061), (32, 0.006), (33, 0.026), (34, 0.007), (35, 0.001), (36, 0.057), (37, -0.005), (38, -0.004), (39, -0.009), (40, 0.045), (41, 0.024), (42, -0.033), (43, -0.098), (44, 0.016), (45, -0.038), (46, 0.03), (47, -0.044), (48, 0.005), (49, -0.003)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96663123 <a title="728-lsi-1" href="../high_scalability-2009/high_scalability-2009-10-26-Facebook%27s_Memcached_Multiget_Hole%3A_More_machines_%21%3D_More_Capacity_.html">728 high scalability-2009-10-26-Facebook's Memcached Multiget Hole: More machines != More Capacity </a></p>
<p>Introduction: When you are on the bleeding edge of scale like Facebook is, you run into some
interesting problems. As of 2008Facebook had over 800 memcached
serverssupplying over 28 terabytes of cache. With those staggering numbers
it's a fair bet to think they've seen their share of Dr. House worthy
memcached problems.Jeff Rothschild, Vice President of Technology at
Facebook,describes one such problemthey've dubbed theMultiget Hole.You fall
into the multiget hole when memcached servers areCPU bound, adding more
memcached servers seems like the right way to add more capacity so more
requests can be served, but against all logic adding servers doesn't help
serve more requests. This puts you in a hole that simply adding more servers
can't dig you out of. What's the treatment?Dr. House would immediately notice
the hidden clue, we are talking requests not memory. We aren't running out of
memory to store stuff, we arerunning out of CPU power to process requests.
What happens when you add more servers is</p><p>2 0.76712561 <a title="728-lsi-2" href="../high_scalability-2008/high_scalability-2008-01-10-Letting_Clients_Know_What%27s_Changed%3A_Push_Me_or_Pull_Me%3F.html">205 high scalability-2008-01-10-Letting Clients Know What's Changed: Push Me or Pull Me?</a></p>
<p>Introduction: I had a false beliefI thought I came here to stayWe're all just visitingAll
just breaking like wavesThe oceans made me, but who came up with me?Push me,
pull me, push me, or pull me out .So true Perl Jam(Push me Pull me lyrics), so
true. I too have wondered how web clients should be notified of model changes.
Should servers push events to clients or should clients pull events from
servers? A topic worthy of its own song if ever there was one.breakTo pull
events the client simply starts a timer and makes a request to the server.
This is polling. You can either pull a complete set of fresh data or get a
list of changes. The server "knows" if anything you are interested in has
changed and makes those changes available to you. Knowing what has changed can
be relatively simple with a publish-subscribe type backend or you can get very
complex with fine grained bit maps of attributes and keeping per client state
on what I client still needs to see.Polling is heavy man. Imagine all your
client</p><p>3 0.73643893 <a title="728-lsi-3" href="../high_scalability-2013/high_scalability-2013-02-27-42_Monster_Problems_that_Attack_as_Loads_Increase.html">1413 high scalability-2013-02-27-42 Monster Problems that Attack as Loads Increase</a></p>
<p>Introduction: For solutions take a look at:7 Life Saving Scalability Defenses Against Load
Monster Attacks.This is a look at all the bad things that can happen to your
carefully crafted program as loads increase: all hell breaks lose. Sure, you
can scale out or scale up, but you can also choose to program better. Make
your system handle larger loads. This saves money because fewer boxes are
needed and it will make the entire application more reliable and have better
response times. And it can be quite satisfying as a programmer.Large Number Of
ObjectsWe usually get into scaling problems when the number of objects gets
larger. Clearly resource usage of all types is stressed as the number of
objects grow.Continuous Failures Makes An Infinite Event StreamDuring large
network failure scenarios there is never time for the system recover. We are
in a continual state of stress.Lots of High Priority WorkFor example,
rerouting is a high priority activity. If there is a large amount of rerouting
work that can</p><p>4 0.73230469 <a title="728-lsi-4" href="../high_scalability-2012/high_scalability-2012-06-18-Google_on_Latency_Tolerant_Systems%3A_Making_a_Predictable_Whole_Out_of_Unpredictable_Parts___.html">1266 high scalability-2012-06-18-Google on Latency Tolerant Systems: Making a Predictable Whole Out of Unpredictable Parts   </a></p>
<p>Introduction: InTaming The Long Latency Tailwe coveredLuiz Barroso's exploration of the long
tail latency (some operations are really slow) problems generated by large
fanout architectures (a request is composed of potentially thousands of other
requests). You may have noticed there weren't a lot of solutions. That's where
a talk I attended,Achieving Rapid Response Times in Large Online
Services(slide deck), byJeff Dean, also of Google, comes in:In this talk, I'll
describe a collection of techniques and practices lowering response times in
large distributed systems whose components run on shared clusters of machines,
where pieces of these systems are subject to interference by other tasks, and
where unpredictable latency hiccups are the norm, not the exception.The goal
is to use software techniques to reduce variability given the increasing
variability in underlying hardware, the need to handle dynamic workloads on a
shared infrastructure, and the need to use large fanout architectures to
operate at</p><p>5 0.72220755 <a title="728-lsi-5" href="../high_scalability-2008/high_scalability-2008-05-14-New_Facebook_Chat_Feature_Scales_to_70_Million_Users_Using_Erlang.html">318 high scalability-2008-05-14-New Facebook Chat Feature Scales to 70 Million Users Using Erlang</a></p>
<p>Introduction: Update: Erlang at Facebook by Eugene Letuchy. How Facebook uses Erlang to
implement Chat, AIM Presence, and Chat Jabber support. I've done
someXMPPdevelopment so when I readFacebook was making a Jabber chat clientI
was really curious how they would make it work. While core XMPP is
straightforward, a number of protocol extensions like discovery, forms, chat
states, pubsub, multi user chat, and privacy lists really up the
implementation complexity. Some real engineering challenges were involved to
make this puppy scale and perform. It's not clear what extensions they've
implemented, but a blog entry by Facebook's Eugene Letuchy hits some of the
architectural challenges they faced and how they overcame them.A web based
Jabber client poses a few problems because XMPP, like most IM protocols, is an
asynchronous event driven system that pretty much assumes you have a full time
open connection. After logging in the server sends a client roster information
and presence information. Your client</p><p>6 0.71912187 <a title="728-lsi-6" href="../high_scalability-2008/high_scalability-2008-12-13-Strategy%3A_Facebook_Tweaks_to_Handle_6_Time_as_Many_Memcached_Requests.html">464 high scalability-2008-12-13-Strategy: Facebook Tweaks to Handle 6 Time as Many Memcached Requests</a></p>
<p>7 0.71490169 <a title="728-lsi-7" href="../high_scalability-2013/high_scalability-2013-03-11-Low_Level_Scalability_Solutions_-_The_Conditioning_Collection.html">1421 high scalability-2013-03-11-Low Level Scalability Solutions - The Conditioning Collection</a></p>
<p>8 0.71005452 <a title="728-lsi-8" href="../high_scalability-2013/high_scalability-2013-12-04-How_Can_Batching_Requests_Actually_Reduce_Latency%3F.html">1558 high scalability-2013-12-04-How Can Batching Requests Actually Reduce Latency?</a></p>
<p>9 0.70702165 <a title="728-lsi-9" href="../high_scalability-2007/high_scalability-2007-07-23-GoogleTalk_Architecture.html">21 high scalability-2007-07-23-GoogleTalk Architecture</a></p>
<p>10 0.69345278 <a title="728-lsi-10" href="../high_scalability-2014/high_scalability-2014-02-05-Little%E2%80%99s_Law%2C_Scalability_and_Fault_Tolerance%3A_The_OS_is_your_bottleneck._What_you_can_do%3F.html">1591 high scalability-2014-02-05-Little’s Law, Scalability and Fault Tolerance: The OS is your bottleneck. What you can do?</a></p>
<p>11 0.68649518 <a title="728-lsi-11" href="../high_scalability-2014/high_scalability-2014-03-31-How_WhatsApp_Grew_to_Nearly_500_Million_Users%2C_11%2C000_cores%2C_and_70_Million_Messages_a_Second.html">1622 high scalability-2014-03-31-How WhatsApp Grew to Nearly 500 Million Users, 11,000 cores, and 70 Million Messages a Second</a></p>
<p>12 0.67941755 <a title="728-lsi-12" href="../high_scalability-2010/high_scalability-2010-12-20-Netflix%3A_Use_Less_Chatty_Protocols_in_the_Cloud_-_Plus_26_Fixes.html">960 high scalability-2010-12-20-Netflix: Use Less Chatty Protocols in the Cloud - Plus 26 Fixes</a></p>
<p>13 0.67796898 <a title="728-lsi-13" href="../high_scalability-2008/high_scalability-2008-04-30-Rather_small_site_architecture..html">312 high scalability-2008-04-30-Rather small site architecture.</a></p>
<p>14 0.67417091 <a title="728-lsi-14" href="../high_scalability-2014/high_scalability-2014-02-10-13_Simple_Tricks_for_Scaling_Python_and_Django_with_Apache_from_HackerEarth.html">1593 high scalability-2014-02-10-13 Simple Tricks for Scaling Python and Django with Apache from HackerEarth</a></p>
<p>15 0.67184818 <a title="728-lsi-15" href="../high_scalability-2013/high_scalability-2013-05-13-The_Secret_to_10_Million_Concurrent_Connections_-The_Kernel_is_the_Problem%2C_Not_the_Solution.html">1456 high scalability-2013-05-13-The Secret to 10 Million Concurrent Connections -The Kernel is the Problem, Not the Solution</a></p>
<p>16 0.67043215 <a title="728-lsi-16" href="../high_scalability-2009/high_scalability-2009-07-25-Latency_is_Everywhere_and_it_Costs_You_Sales_-_How_to_Crush_it.html">661 high scalability-2009-07-25-Latency is Everywhere and it Costs You Sales - How to Crush it</a></p>
<p>17 0.66785067 <a title="728-lsi-17" href="../high_scalability-2008/high_scalability-2008-02-21-Tracking_usage_of_public_resources_-_throttling_accesses_per_hour.html">256 high scalability-2008-02-21-Tracking usage of public resources - throttling accesses per hour</a></p>
<p>18 0.66776323 <a title="728-lsi-18" href="../high_scalability-2010/high_scalability-2010-10-26-Scaling_DISQUS_to_75_Million_Comments_and_17%2C000_RPS.html">928 high scalability-2010-10-26-Scaling DISQUS to 75 Million Comments and 17,000 RPS</a></p>
<p>19 0.66630083 <a title="728-lsi-19" href="../high_scalability-2013/high_scalability-2013-04-10-Check_Yourself_Before_You_Wreck_Yourself_-_Avocado%27s_5_Early_Stages_of_Architecture_Evolution.html">1438 high scalability-2013-04-10-Check Yourself Before You Wreck Yourself - Avocado's 5 Early Stages of Architecture Evolution</a></p>
<p>20 0.66351628 <a title="728-lsi-20" href="../high_scalability-2008/high_scalability-2008-02-16-S3_Failed_Because_of_Authentication_Overload.html">249 high scalability-2008-02-16-S3 Failed Because of Authentication Overload</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.105), (2, 0.257), (10, 0.02), (16, 0.015), (40, 0.018), (43, 0.028), (47, 0.014), (61, 0.078), (65, 0.134), (77, 0.014), (79, 0.113), (85, 0.067), (94, 0.044)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.9598164 <a title="728-lda-1" href="../high_scalability-2007/high_scalability-2007-11-17-Can_How_Bees_Solve_their_Load_Balancing_Problems_Help_Build_More_Scalable_Websites%3F.html">158 high scalability-2007-11-17-Can How Bees Solve their Load Balancing Problems Help Build More Scalable Websites?</a></p>
<p>Introduction: Bees have a similar problem to website servers: how to do a lot of work with
limited resources in an ever changing environment. Usually lessons from
biology are hard to apply to computer problems. Nature throws hardware at
problems. Billions and billions of cells cooperate at different levels of
organizations to find food, fight lions, and make sure your DNA is passed
on.Nature's software is "simple," but her hardware rocks. We do the opposite.
For us hardware is in short supply so we use limited hardware and leverage
"smart" software to work around our inability to throw hardware at problems.
But we might be able to borrow some load balancing techniques from bees. What
do bees do that we can learn from?Bees do a dance to indicate the quality and
location of a nectar source. When a bee finds a better source they do a better
dance and resources shift to the new location. This approach may seem
inefficient, but it turns out to be "optimal for the unpredictable nectar
world." Craig Tovey</p><p>2 0.95607805 <a title="728-lda-2" href="../high_scalability-2014/high_scalability-2014-01-17-Stuff_The_Internet_Says_On_Scalability_For_January_17th%2C_2014.html">1581 high scalability-2014-01-17-Stuff The Internet Says On Scalability For January 17th, 2014</a></p>
<p>Introduction: Hey, it's HighScalability time:From the stunningScale of the Universe -
Interactive Flash Animation$7 trillion: US spend on patrolling oil sea-
lanes;82 billion: files served by MaxCDN in 5 monthsQuotable Quotes:
@StephenFleming: "Money doesn't solve scaling problems, but the actual
solutions to scaling problems always cost money."
http://daringfireball.net/2014/01/googles_acquisition_of_nestDavid Rosenthal:
Robert Puttnam in Making Democracy Work and Bowling Alone has shown the vast
difference in economic success between high-trust and low-trust
societies.@kylefox: That's a huge advantage of SaaS businesses: you can be
liberal with refunds & goodwill credits w/o impacting the bottom line
much.Thomas B. Roberts: That's the essence of science: Ask the impertinent
question, and you are on your way to pertinent science.Benjamin K. Bergen:
Simulation is an iceberg. By consciously reflecting, as you just have been
doing, you can see the tip--the intentional, conscious imagery. But many of
t</p><p>3 0.9444133 <a title="728-lda-3" href="../high_scalability-2010/high_scalability-2010-03-26-Strategy%3A_Caching_404s_Saved_the_Onion_66%25_on_Server_Time.html">800 high scalability-2010-03-26-Strategy: Caching 404s Saved the Onion 66% on Server Time</a></p>
<p>Introduction: In the articleThe Onion Uses Django, And Why It Matters To Us, a lot of
interesting points are made about their ambitious infrastructure move from
Drupal/PHP to Django/Python: the move wasn't that hard, it just took time and
work because of their previous experience moving the A.V. Club website; churn
in core framework APIs make it more attractive to move than stay; supporting
the structure of older versions of the site is an unsolved problem; the built-
in Django admin saved a lot of work; group development is easier with "fewer
specialized or hacked together pieces"; they use IRC for distributed
development; sphinx for full-text search; nginx is the media server and
reverse proxy; haproxy made the launch process a 5 second procedure;
capistrano for deployment; clean component separation makes moving easier; Git
for version control; ORM with complicated querysets is a performance problem;
memcached for caching rendered pages; the CDN checks for updates every 10
minutes; videos, articl</p><p>4 0.94115585 <a title="728-lda-4" href="../high_scalability-2012/high_scalability-2012-11-30-Stuff_The_Internet_Says_On_Scalability_For_November_30%2C_2012.html">1365 high scalability-2012-11-30-Stuff The Internet Says On Scalability For November 30, 2012</a></p>
<p>Introduction: We're back and it's HighScalability Time:1B Tweets Every 2.5 Days: Twitter.1
billion transactions/day: Salesforce. Storing700 terabytes of data into a
single gram of DNA. Downside, readingis very slow. And any data might conflict
with the messages aliens have already inserted.Assuming my infonome is 1 TB,
it would cost $1,338,333 to store my existence in Amazon Glacier for a long
nowish 10,000 years. #notbadQuotable Quotes:@cloudpundit: @Werner: "I've
hugged a lot of servers in my life, and believe me, they do not hug you back.
They hate you." #reinvent@jinman: Werner #reinvent The commandments of 21st
century architectures 1) Controllable, 2)Resilient, 3)Adaptive and 4) Data
Driven #cloud@dandonovan78: Wow. Netflix video streaming has grown from 1M
hours to 1 BILLION hours a month in less than 4 years. Insane. #scalability
#aws #reinvent@sandfoxuk: Linear scalability - the spherical cow of cloud
systems…. #PlanningFail@rbranson: the year is 2020. Atomic clocks now embedded
in ARM SoCs</p><p>same-blog 5 0.93933111 <a title="728-lda-5" href="../high_scalability-2009/high_scalability-2009-10-26-Facebook%27s_Memcached_Multiget_Hole%3A_More_machines_%21%3D_More_Capacity_.html">728 high scalability-2009-10-26-Facebook's Memcached Multiget Hole: More machines != More Capacity </a></p>
<p>Introduction: When you are on the bleeding edge of scale like Facebook is, you run into some
interesting problems. As of 2008Facebook had over 800 memcached
serverssupplying over 28 terabytes of cache. With those staggering numbers
it's a fair bet to think they've seen their share of Dr. House worthy
memcached problems.Jeff Rothschild, Vice President of Technology at
Facebook,describes one such problemthey've dubbed theMultiget Hole.You fall
into the multiget hole when memcached servers areCPU bound, adding more
memcached servers seems like the right way to add more capacity so more
requests can be served, but against all logic adding servers doesn't help
serve more requests. This puts you in a hole that simply adding more servers
can't dig you out of. What's the treatment?Dr. House would immediately notice
the hidden clue, we are talking requests not memory. We aren't running out of
memory to store stuff, we arerunning out of CPU power to process requests.
What happens when you add more servers is</p><p>6 0.91402936 <a title="728-lda-6" href="../high_scalability-2013/high_scalability-2013-08-09-Stuff_The_Internet_Says_On_Scalability_For_August_9%2C_2013.html">1499 high scalability-2013-08-09-Stuff The Internet Says On Scalability For August 9, 2013</a></p>
<p>7 0.9093734 <a title="728-lda-7" href="../high_scalability-2012/high_scalability-2012-12-21-Stuff_The_Internet_Says_On_Scalability_For_December_21%2C_2012.html">1375 high scalability-2012-12-21-Stuff The Internet Says On Scalability For December 21, 2012</a></p>
<p>8 0.90933508 <a title="728-lda-8" href="../high_scalability-2013/high_scalability-2013-05-17-Stuff_The_Internet_Says_On_Scalability_For_May_17%2C_2013.html">1460 high scalability-2013-05-17-Stuff The Internet Says On Scalability For May 17, 2013</a></p>
<p>9 0.90918291 <a title="728-lda-9" href="../high_scalability-2011/high_scalability-2011-10-24-StackExchange_Architecture_Updates_-_Running_Smoothly%2C_Amazon_4x_More_Expensive.html">1131 high scalability-2011-10-24-StackExchange Architecture Updates - Running Smoothly, Amazon 4x More Expensive</a></p>
<p>10 0.90849739 <a title="728-lda-10" href="../high_scalability-2009/high_scalability-2009-04-04-Digg_Architecture.html">554 high scalability-2009-04-04-Digg Architecture</a></p>
<p>11 0.90841335 <a title="728-lda-11" href="../high_scalability-2013/high_scalability-2013-04-05-Stuff_The_Internet_Says_On_Scalability_For_April_5%2C_2013.html">1436 high scalability-2013-04-05-Stuff The Internet Says On Scalability For April 5, 2013</a></p>
<p>12 0.90615648 <a title="728-lda-12" href="../high_scalability-2013/high_scalability-2013-08-30-Stuff_The_Internet_Says_On_Scalability_For_August_30%2C_2013.html">1509 high scalability-2013-08-30-Stuff The Internet Says On Scalability For August 30, 2013</a></p>
<p>13 0.90588284 <a title="728-lda-13" href="../high_scalability-2010/high_scalability-2010-06-28-VoltDB_Decapitates_Six_SQL_Urban_Myths_and_Delivers_Internet_Scale_OLTP_in_the_Process.html">849 high scalability-2010-06-28-VoltDB Decapitates Six SQL Urban Myths and Delivers Internet Scale OLTP in the Process</a></p>
<p>14 0.9057371 <a title="728-lda-14" href="../high_scalability-2007/high_scalability-2007-08-29-Skype_Failed_the_Boot_Scalability_Test%3A_Is_P2P_fundamentally_flawed%3F.html">76 high scalability-2007-08-29-Skype Failed the Boot Scalability Test: Is P2P fundamentally flawed?</a></p>
<p>15 0.9052977 <a title="728-lda-15" href="../high_scalability-2013/high_scalability-2013-04-12-Stuff_The_Internet_Says_On_Scalability_For_April_12%2C_2013.html">1439 high scalability-2013-04-12-Stuff The Internet Says On Scalability For April 12, 2013</a></p>
<p>16 0.90524197 <a title="728-lda-16" href="../high_scalability-2012/high_scalability-2012-08-02-Strategy%3A_Use_Spare_Region_Capacity_to_Survive_Availability_Zone_Failures.html">1296 high scalability-2012-08-02-Strategy: Use Spare Region Capacity to Survive Availability Zone Failures</a></p>
<p>17 0.90505123 <a title="728-lda-17" href="../high_scalability-2009/high_scalability-2009-07-25-Latency_is_Everywhere_and_it_Costs_You_Sales_-_How_to_Crush_it.html">661 high scalability-2009-07-25-Latency is Everywhere and it Costs You Sales - How to Crush it</a></p>
<p>18 0.90481257 <a title="728-lda-18" href="../high_scalability-2013/high_scalability-2013-06-13-Busting_4_Modern_Hardware_Myths_-_Are_Memory%2C_HDDs%2C_and_SSDs_Really_Random_Access%3F.html">1475 high scalability-2013-06-13-Busting 4 Modern Hardware Myths - Are Memory, HDDs, and SSDs Really Random Access?</a></p>
<p>19 0.90435469 <a title="728-lda-19" href="../high_scalability-2011/high_scalability-2011-04-12-Caching_and_Processing_2TB_Mozilla_Crash_Reports_in_memory_with_Hazelcast.html">1020 high scalability-2011-04-12-Caching and Processing 2TB Mozilla Crash Reports in memory with Hazelcast</a></p>
<p>20 0.90435106 <a title="728-lda-20" href="../high_scalability-2013/high_scalability-2013-04-10-Check_Yourself_Before_You_Wreck_Yourself_-_Avocado%27s_5_Early_Stages_of_Architecture_Evolution.html">1438 high scalability-2013-04-10-Check Yourself Before You Wreck Yourself - Avocado's 5 Early Stages of Architecture Evolution</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
