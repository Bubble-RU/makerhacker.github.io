<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>505 high scalability-2009-02-01-More Chips Means Less Salsa</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2009" href="../home/high_scalability-2009_home.html">high_scalability-2009</a> <a title="high_scalability-2009-505" href="#">high_scalability-2009-505</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>505 high scalability-2009-02-01-More Chips Means Less Salsa</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2009-505-html" href="http://highscalability.com//blog/2009/2/1/more-chips-means-less-salsa.html">html</a></p><p>Introduction: Yes, I just got through watching the Superbowl so chips and salsa are on my mind and in my stomach. In recreational eating more chips requires downing more salsa. With mulitcore chips it turns out as cores go up salsa goes down, salsa obviously being a metaphor for speed.     Sandia National Laboratories found in their simulations:   a significant increase in speed going from two to four multicores, but an insignificant increase from four to eight multicores. Exceeding eight multicores causes a decrease in speed. Sixteen multicores perform barely as well as two, and after that, a steep decline is registered as more cores are added. The problem is the lack of memory bandwidth as well as contention between processors over the memory bus available to each processor.      The implication for those following a diagonal scaling strategy is to work like heck to make your system fit within eight multicores. After that you'll need to consider some sort of partitioning strategy. What's interesti</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Yes, I just got through watching the Superbowl so chips and salsa are on my mind and in my stomach. [sent-1, score-0.944]
</p><p>2 In recreational eating more chips requires downing more salsa. [sent-2, score-0.409]
</p><p>3 With mulitcore chips it turns out as cores go up salsa goes down, salsa obviously being a metaphor for speed. [sent-3, score-1.664]
</p><p>4 Sandia National Laboratories found in their simulations:   a significant increase in speed going from two to four multicores, but an insignificant increase from four to eight multicores. [sent-4, score-0.961]
</p><p>5 Sixteen multicores perform barely as well as two, and after that, a steep decline is registered as more cores are added. [sent-6, score-1.128]
</p><p>6 The problem is the lack of memory bandwidth as well as contention between processors over the memory bus available to each processor. [sent-7, score-0.501]
</p><p>7 The implication for those following a diagonal scaling strategy is to work like heck to make your system fit within eight multicores. [sent-8, score-0.779]
</p><p>8 After that you'll need to consider some sort of partitioning strategy. [sent-9, score-0.162]
</p><p>9 What's interesting is the research on where the cutoff point will be. [sent-10, score-0.058]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('salsa', 0.498), ('multicores', 0.451), ('eight', 0.284), ('chips', 0.257), ('diagonal', 0.15), ('laboratories', 0.15), ('insignificant', 0.15), ('steep', 0.15), ('sixteen', 0.141), ('superbowl', 0.141), ('decline', 0.13), ('simulations', 0.13), ('four', 0.123), ('national', 0.122), ('exceeding', 0.119), ('cores', 0.114), ('metaphor', 0.108), ('eating', 0.107), ('barely', 0.098), ('heck', 0.096), ('implication', 0.096), ('bus', 0.088), ('registered', 0.086), ('decrease', 0.086), ('watching', 0.085), ('increase', 0.084), ('obviously', 0.077), ('contention', 0.073), ('causes', 0.071), ('yes', 0.068), ('turns', 0.068), ('lack', 0.066), ('processors', 0.066), ('significant', 0.063), ('partitioning', 0.06), ('memory', 0.058), ('research', 0.058), ('fit', 0.055), ('mind', 0.055), ('following', 0.052), ('sort', 0.052), ('well', 0.051), ('two', 0.05), ('consider', 0.05), ('got', 0.049), ('perform', 0.048), ('strategy', 0.046), ('requires', 0.045), ('goes', 0.044), ('bandwidth', 0.041)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="505-tfidf-1" href="../high_scalability-2009/high_scalability-2009-02-01-More_Chips_Means_Less_Salsa.html">505 high scalability-2009-02-01-More Chips Means Less Salsa</a></p>
<p>Introduction: Yes, I just got through watching the Superbowl so chips and salsa are on my mind and in my stomach. In recreational eating more chips requires downing more salsa. With mulitcore chips it turns out as cores go up salsa goes down, salsa obviously being a metaphor for speed.     Sandia National Laboratories found in their simulations:   a significant increase in speed going from two to four multicores, but an insignificant increase from four to eight multicores. Exceeding eight multicores causes a decrease in speed. Sixteen multicores perform barely as well as two, and after that, a steep decline is registered as more cores are added. The problem is the lack of memory bandwidth as well as contention between processors over the memory bus available to each processor.      The implication for those following a diagonal scaling strategy is to work like heck to make your system fit within eight multicores. After that you'll need to consider some sort of partitioning strategy. What's interesti</p><p>2 0.10368659 <a title="505-tfidf-2" href="../high_scalability-2010/high_scalability-2010-11-09-The_Tera-Scale_Effect_.html">939 high scalability-2010-11-09-The Tera-Scale Effect </a></p>
<p>Introduction: In the past year, Intel issued a series of powerful chips under the new  Nehalem microarchitecture  , with large numbers of cores and extensive memory capacity. This new class of chips is is part of a bigger Intel initiative referred to as  Tera-Scale Computing  . Cisco has released their  Unified Computing System   (UCS) equipped with a unique extended memory and high speed network within the box, which is specifically geared to take advantage of this type of CPU architecture .
 
This new class of hardware has the potential to revolutionize the IT landscape as we know it.
 
In  this post, I want to focus primarily on the potential implications on application architecture, more specifically on the application platform landscape.   more...</p><p>3 0.09288767 <a title="505-tfidf-3" href="../high_scalability-2009/high_scalability-2009-03-12-Google_TechTalk%3A_Amdahl%27s_Law_in_the_Multicore_Era.html">534 high scalability-2009-03-12-Google TechTalk: Amdahl's Law in the Multicore Era</a></p>
<p>Introduction: Over the last several decades computer architects have been phenomenally successful turning the transistor bounty provided by Moore's Law into chips with ever increasing single-threaded performance. During many of these successful years, however, many researchers paid scant attention to multiprocessor work. Now as vendors turn to multicore chips, researchers are reacting with more papers on multi-threaded systems. While this is good, we are concerned that further work on single-thread performance will be squashed.     To help understand future high-level trade-offs, we develop a corollary to Amdahl's Law for multicore chips [Hill & Marty, IEEE Computer 2008]. It models fixed chip resources for alternative designs that use symmetric cores, asymmetric cores, or dynamic techniques that allow cores to work together on sequential execution. Our results encourage multicore designers to view performance of the entire chip rather than focus on core efficiencies. Moreover, we observe that obtai</p><p>4 0.069961995 <a title="505-tfidf-4" href="../high_scalability-2009/high_scalability-2009-06-23-Learn_How_to_Exploit_Multiple_Cores_for_Better_Performance_and_Scalability.html">636 high scalability-2009-06-23-Learn How to Exploit Multiple Cores for Better Performance and Scalability</a></p>
<p>Introduction: InfoQueue has this  excellent talk by Brian Goetz  on the new features being added to Java SE 7 that will allow programmers to fully exploit our massively multi-processor future. While the talk is about Java it's really more general than that and there's a lot to learn here for everyone.  Brian starts with a short, coherent, and compelling explanation of why programmers can't expect to be saved by ever faster CPUs and why we must learn to exploit the strengths of multiple core computers to make our software go faster.   Some techniques for exploiting multiple cores are given in an equally  short, coherent, and compelling explanation of why divide and conquer as the secret to multi-core bliss, fork-join, how the Java approach differs from map-reduce, and lots of other juicy topics.   The multi-core "problem" is only going to get worse. Tilera founder Anant Agarwal  estimates by 2017  embedded processors could have 4,096 cores, server CPUs might have 512 cores and desktop chips could use</p><p>5 0.062580608 <a title="505-tfidf-5" href="../high_scalability-2012/high_scalability-2012-03-06-Ask_For_Forgiveness_Programming_-_Or_How_We%27ll_Program_1000_Cores.html">1204 high scalability-2012-03-06-Ask For Forgiveness Programming - Or How We'll Program 1000 Cores</a></p>
<p>Introduction: The argument for a massively multicore future is now familiar: while clock speeds have leveled off, device density is increasing, so the future is cheap chips with hundreds and thousands of cores. That’s the inexorable logic behind our multicore future.
 
The unsolved question that lurks deep in the dark part of a programmer’s mind is: how on earth are we to program these things? For problems that aren’t   embarrassingly parallel   , we really have no idea. IBM Research’s    David Ungar    has an idea. And it’s radical in the extreme...     Grace Hopper    once advised “It's easier to ask for forgiveness than it is to get permission.” I wonder if she had any idea that her strategy for dealing with human bureaucracy would the same strategy David Ungar thinks will help us tame  the technological bureaucracy of 1000+ core systems?    You may recognize David as the co-creator of the    Self programming    language, inspiration for the HotSpot technology in the JVM and the prototype model u</p><p>6 0.06153113 <a title="505-tfidf-6" href="../high_scalability-2011/high_scalability-2011-04-08-Stuff_The_Internet_Says_On_Scalability_For_April_8%2C_2011.html">1019 high scalability-2011-04-08-Stuff The Internet Says On Scalability For April 8, 2011</a></p>
<p>7 0.059514299 <a title="505-tfidf-7" href="../high_scalability-2007/high_scalability-2007-11-05-Strategy%3A_Diagonal_Scaling_-_Don%27t_Forget_to_Scale_Out_AND_Up.html">142 high scalability-2007-11-05-Strategy: Diagonal Scaling - Don't Forget to Scale Out AND Up</a></p>
<p>8 0.059306763 <a title="505-tfidf-8" href="../high_scalability-2010/high_scalability-2010-12-17-Stuff_the_Internet_Says_on_Scalability_For_December_17th%2C_2010.html">959 high scalability-2010-12-17-Stuff the Internet Says on Scalability For December 17th, 2010</a></p>
<p>9 0.054747507 <a title="505-tfidf-9" href="../high_scalability-2010/high_scalability-2010-02-01-What_Will_Kill_the_Cloud%3F.html">768 high scalability-2010-02-01-What Will Kill the Cloud?</a></p>
<p>10 0.051889919 <a title="505-tfidf-10" href="../high_scalability-2012/high_scalability-2012-03-29-Strategy%3A_Exploit_Processor_Affinity_for_High_and_Predictable_Performance.html">1218 high scalability-2012-03-29-Strategy: Exploit Processor Affinity for High and Predictable Performance</a></p>
<p>11 0.050856017 <a title="505-tfidf-11" href="../high_scalability-2014/high_scalability-2014-05-21-9_Principles_of_High_Performance_Programs.html">1652 high scalability-2014-05-21-9 Principles of High Performance Programs</a></p>
<p>12 0.05022449 <a title="505-tfidf-12" href="../high_scalability-2011/high_scalability-2011-09-16-Stuff_The_Internet_Says_On_Scalability_For_September_16%2C_2011.html">1117 high scalability-2011-09-16-Stuff The Internet Says On Scalability For September 16, 2011</a></p>
<p>13 0.049506377 <a title="505-tfidf-13" href="../high_scalability-2012/high_scalability-2012-02-02-The_Data-Scope_Project_-_6PB_storage%2C_500GBytes-sec_sequential_IO%2C_20M_IOPS%2C_130TFlops.html">1186 high scalability-2012-02-02-The Data-Scope Project - 6PB storage, 500GBytes-sec sequential IO, 20M IOPS, 130TFlops</a></p>
<p>14 0.048683606 <a title="505-tfidf-14" href="../high_scalability-2008/high_scalability-2008-12-14-Scaling_MySQL_on_a_256-way_T5440_server_using_Solaris_ZFS_and_Java_1.7.html">465 high scalability-2008-12-14-Scaling MySQL on a 256-way T5440 server using Solaris ZFS and Java 1.7</a></p>
<p>15 0.047418877 <a title="505-tfidf-15" href="../high_scalability-2010/high_scalability-2010-02-15-The_Amazing_Collective_Compute_Power_of_the_Ambient_Cloud.html">778 high scalability-2010-02-15-The Amazing Collective Compute Power of the Ambient Cloud</a></p>
<p>16 0.046244152 <a title="505-tfidf-16" href="../high_scalability-2012/high_scalability-2012-07-04-Top_Features_of_a_Scalable_Database.html">1276 high scalability-2012-07-04-Top Features of a Scalable Database</a></p>
<p>17 0.045908835 <a title="505-tfidf-17" href="../high_scalability-2009/high_scalability-2009-07-25-Latency_is_Everywhere_and_it_Costs_You_Sales_-_How_to_Crush_it.html">661 high scalability-2009-07-25-Latency is Everywhere and it Costs You Sales - How to Crush it</a></p>
<p>18 0.045405336 <a title="505-tfidf-18" href="../high_scalability-2009/high_scalability-2009-01-17-Scaling_in_Games_%26_Virtual_Worlds___.html">496 high scalability-2009-01-17-Scaling in Games & Virtual Worlds   </a></p>
<p>19 0.044440996 <a title="505-tfidf-19" href="../high_scalability-2013/high_scalability-2013-08-30-Stuff_The_Internet_Says_On_Scalability_For_August_30%2C_2013.html">1509 high scalability-2013-08-30-Stuff The Internet Says On Scalability For August 30, 2013</a></p>
<p>20 0.04349909 <a title="505-tfidf-20" href="../high_scalability-2008/high_scalability-2008-10-17-A_High_Performance_Memory_Database_for_Web_Application_Caches.html">421 high scalability-2008-10-17-A High Performance Memory Database for Web Application Caches</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.052), (1, 0.041), (2, 0.007), (3, 0.016), (4, -0.017), (5, 0.012), (6, 0.004), (7, 0.032), (8, -0.047), (9, 0.001), (10, -0.002), (11, -0.029), (12, 0.02), (13, 0.033), (14, -0.003), (15, -0.004), (16, 0.016), (17, 0.006), (18, -0.031), (19, 0.025), (20, -0.018), (21, -0.012), (22, -0.015), (23, -0.002), (24, 0.011), (25, -0.013), (26, 0.014), (27, -0.018), (28, 0.036), (29, 0.018), (30, 0.017), (31, 0.004), (32, 0.005), (33, -0.008), (34, 0.025), (35, -0.019), (36, 0.006), (37, -0.011), (38, -0.005), (39, -0.013), (40, -0.002), (41, 0.013), (42, 0.042), (43, -0.015), (44, 0.012), (45, 0.006), (46, -0.013), (47, -0.01), (48, 0.002), (49, 0.022)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95355451 <a title="505-lsi-1" href="../high_scalability-2009/high_scalability-2009-02-01-More_Chips_Means_Less_Salsa.html">505 high scalability-2009-02-01-More Chips Means Less Salsa</a></p>
<p>Introduction: Yes, I just got through watching the Superbowl so chips and salsa are on my mind and in my stomach. In recreational eating more chips requires downing more salsa. With mulitcore chips it turns out as cores go up salsa goes down, salsa obviously being a metaphor for speed.     Sandia National Laboratories found in their simulations:   a significant increase in speed going from two to four multicores, but an insignificant increase from four to eight multicores. Exceeding eight multicores causes a decrease in speed. Sixteen multicores perform barely as well as two, and after that, a steep decline is registered as more cores are added. The problem is the lack of memory bandwidth as well as contention between processors over the memory bus available to each processor.      The implication for those following a diagonal scaling strategy is to work like heck to make your system fit within eight multicores. After that you'll need to consider some sort of partitioning strategy. What's interesti</p><p>2 0.73036331 <a title="505-lsi-2" href="../high_scalability-2009/high_scalability-2009-03-12-Google_TechTalk%3A_Amdahl%27s_Law_in_the_Multicore_Era.html">534 high scalability-2009-03-12-Google TechTalk: Amdahl's Law in the Multicore Era</a></p>
<p>Introduction: Over the last several decades computer architects have been phenomenally successful turning the transistor bounty provided by Moore's Law into chips with ever increasing single-threaded performance. During many of these successful years, however, many researchers paid scant attention to multiprocessor work. Now as vendors turn to multicore chips, researchers are reacting with more papers on multi-threaded systems. While this is good, we are concerned that further work on single-thread performance will be squashed.     To help understand future high-level trade-offs, we develop a corollary to Amdahl's Law for multicore chips [Hill & Marty, IEEE Computer 2008]. It models fixed chip resources for alternative designs that use symmetric cores, asymmetric cores, or dynamic techniques that allow cores to work together on sequential execution. Our results encourage multicore designers to view performance of the entire chip rather than focus on core efficiencies. Moreover, we observe that obtai</p><p>3 0.71165419 <a title="505-lsi-3" href="../high_scalability-2012/high_scalability-2012-09-10-Russ%E2%80%99_10_Ingredient_Recipe_for_Making_1_Million_TPS_on_%245K_Hardware.html">1319 high scalability-2012-09-10-Russ’ 10 Ingredient Recipe for Making 1 Million TPS on $5K Hardware</a></p>
<p>Introduction: My name is  Russell Sullivan , I am the author of AlchemyDB: a highly flexible NoSQL/SQL/DocumentStore/GraphDB-datastore built on top of redis. I have spent the last several years trying to find a way to sanely house multiple datastore-genres under one roof while (almost paradoxically) pushing performance to its limits.    I recently joined the NoSQL company    Aerospike    (formerly Citrusleaf) with the goal of incrementally grafting AlchemyDB’s flexible data-modeling capabilities onto Aerospike’s high-velocity horizontally-scalable key-value data-fabric. We recently completed a peak-performance    TPS optimization project   : starting at 200K TPS, pushing to the recent community edition launch at 500K TPS, and finally arriving at our 2012 goal:    1M TPS on $5K hardware   .    Getting to one million over-the-wire client-server database-requests per-second on a single machine costing $5K is a balance between trimming overhead on many axes and using a shared nothing architecture to   i</p><p>4 0.70462435 <a title="505-lsi-4" href="../high_scalability-2012/high_scalability-2012-03-29-Strategy%3A_Exploit_Processor_Affinity_for_High_and_Predictable_Performance.html">1218 high scalability-2012-03-29-Strategy: Exploit Processor Affinity for High and Predictable Performance</a></p>
<p>Introduction: Martin Thompson wrote a really interesting  article  on the beneficial performance impact of taking advantage of  Processor Affinity :
  

The interesting thing I've observed is that the unpinned test will follow a step function of unpredictable performance.  Across many runs I've seen different patterns but all similar in this step function nature.  For the pinned tests I get consistent throughput with no step pattern and always the greatest throughput.

  
The idea is by assigning a thread to a particular CPU that when a thread is rescheduled to run on the same CPU, it can take advantage of the "accumulated  state in the processor, including instructions and data in the cache."  With multi-core chips the norm now, you may want to decide for yourself how to assign work to cores and not let the OS do it for you. The results are surprisingly strong.</p><p>5 0.70053154 <a title="505-lsi-5" href="../high_scalability-2013/high_scalability-2013-10-23-Strategy%3A_Use_Linux_Taskset_to_Pin_Processes_or_Let_the_OS_Schedule_It%3F.html">1536 high scalability-2013-10-23-Strategy: Use Linux Taskset to Pin Processes or Let the OS Schedule It?</a></p>
<p>Introduction: This question comes from Ulysses on an  interesting thread  from the Mechanical Sympathy news group, especially given how multiple processors are now the norm:
 
Ulysses:
   
 On an 8xCPU Linux instance,  is it at all advantageous to use the Linux taskset command to pin an 8xJVM process set (co-ordinated as a www.infinispan.org distributed cache/data grid) to a specific CPU affinity set  (i.e. pin JVM0 process to CPU 0, JVM1 process to CPU1, ...., JVM7process to CPU 7) vs. just letting the Linux OS use its default mechanism for provisioning the 8xJVM process set to the available CPUs? 
 In effrort to seek an optimal point (in the full event space), what are the conceptual trade-offs in considering "searching" each permutation of provisioning an 8xJVM process set to an 8xCPU set via taskset? 
   
Given  taskset  is they key to the question, it would help to have a definition:
  

Used to set or retrieve the CPU affinity of a running process given its PID or to launch a new COMMAND with</p><p>6 0.68435055 <a title="505-lsi-6" href="../high_scalability-2013/high_scalability-2013-06-06-Paper%3A_Memory_Barriers%3A_a_Hardware_View_for_Software_Hackers.html">1471 high scalability-2013-06-06-Paper: Memory Barriers: a Hardware View for Software Hackers</a></p>
<p>7 0.67964429 <a title="505-lsi-7" href="../high_scalability-2009/high_scalability-2009-06-23-Learn_How_to_Exploit_Multiple_Cores_for_Better_Performance_and_Scalability.html">636 high scalability-2009-06-23-Learn How to Exploit Multiple Cores for Better Performance and Scalability</a></p>
<p>8 0.67647785 <a title="505-lsi-8" href="../high_scalability-2010/high_scalability-2010-11-09-The_Tera-Scale_Effect_.html">939 high scalability-2010-11-09-The Tera-Scale Effect </a></p>
<p>9 0.63328022 <a title="505-lsi-9" href="../high_scalability-2012/high_scalability-2012-05-02-12_Ways_to_Increase_Throughput_by_32X_and_Reduce_Latency_by__20X.html">1237 high scalability-2012-05-02-12 Ways to Increase Throughput by 32X and Reduce Latency by  20X</a></p>
<p>10 0.62672204 <a title="505-lsi-10" href="../high_scalability-2013/high_scalability-2013-05-13-The_Secret_to_10_Million_Concurrent_Connections_-The_Kernel_is_the_Problem%2C_Not_the_Solution.html">1456 high scalability-2013-05-13-The Secret to 10 Million Concurrent Connections -The Kernel is the Problem, Not the Solution</a></p>
<p>11 0.62340635 <a title="505-lsi-11" href="../high_scalability-2014/high_scalability-2014-01-03-Stuff_The_Internet_Says_On_Scalability_For_January_3rd%2C_2014.html">1572 high scalability-2014-01-03-Stuff The Internet Says On Scalability For January 3rd, 2014</a></p>
<p>12 0.61379629 <a title="505-lsi-12" href="../high_scalability-2014/high_scalability-2014-01-31-Stuff_The_Internet_Says_On_Scalability_For_January_31st%2C_2014.html">1588 high scalability-2014-01-31-Stuff The Internet Says On Scalability For January 31st, 2014</a></p>
<p>13 0.61335921 <a title="505-lsi-13" href="../high_scalability-2010/high_scalability-2010-05-12-The_Rise_of_the_Virtual_Cellular_Machines.html">826 high scalability-2010-05-12-The Rise of the Virtual Cellular Machines</a></p>
<p>14 0.60378146 <a title="505-lsi-14" href="../high_scalability-2013/high_scalability-2013-06-13-Busting_4_Modern_Hardware_Myths_-_Are_Memory%2C_HDDs%2C_and_SSDs_Really_Random_Access%3F.html">1475 high scalability-2013-06-13-Busting 4 Modern Hardware Myths - Are Memory, HDDs, and SSDs Really Random Access?</a></p>
<p>15 0.60373437 <a title="505-lsi-15" href="../high_scalability-2014/high_scalability-2014-05-21-9_Principles_of_High_Performance_Programs.html">1652 high scalability-2014-05-21-9 Principles of High Performance Programs</a></p>
<p>16 0.59962147 <a title="505-lsi-16" href="../high_scalability-2012/high_scalability-2012-05-16-Big_List_of_20_Common_Bottlenecks.html">1246 high scalability-2012-05-16-Big List of 20 Common Bottlenecks</a></p>
<p>17 0.59509277 <a title="505-lsi-17" href="../high_scalability-2010/high_scalability-2010-10-04-Paper%3A_An_Analysis_of_Linux_Scalability_to_Many_Cores__.html">914 high scalability-2010-10-04-Paper: An Analysis of Linux Scalability to Many Cores  </a></p>
<p>18 0.59235972 <a title="505-lsi-18" href="../high_scalability-2010/high_scalability-2010-12-03-GPU_vs_CPU_Smackdown_%3A_The_Rise_of_Throughput-Oriented_Architectures.html">953 high scalability-2010-12-03-GPU vs CPU Smackdown : The Rise of Throughput-Oriented Architectures</a></p>
<p>19 0.58597517 <a title="505-lsi-19" href="../high_scalability-2011/high_scalability-2011-09-28-Pursue_robust_indefinite_scalability_with_the_Movable_Feast_Machine.html">1127 high scalability-2011-09-28-Pursue robust indefinite scalability with the Movable Feast Machine</a></p>
<p>20 0.57840151 <a title="505-lsi-20" href="../high_scalability-2012/high_scalability-2012-02-17-Stuff_The_Internet_Says_On_Scalability_For_February_17%2C_2012.html">1195 high scalability-2012-02-17-Stuff The Internet Says On Scalability For February 17, 2012</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.091), (2, 0.105), (10, 0.098), (43, 0.509), (61, 0.045), (85, 0.014)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.81720626 <a title="505-lda-1" href="../high_scalability-2009/high_scalability-2009-02-01-More_Chips_Means_Less_Salsa.html">505 high scalability-2009-02-01-More Chips Means Less Salsa</a></p>
<p>Introduction: Yes, I just got through watching the Superbowl so chips and salsa are on my mind and in my stomach. In recreational eating more chips requires downing more salsa. With mulitcore chips it turns out as cores go up salsa goes down, salsa obviously being a metaphor for speed.     Sandia National Laboratories found in their simulations:   a significant increase in speed going from two to four multicores, but an insignificant increase from four to eight multicores. Exceeding eight multicores causes a decrease in speed. Sixteen multicores perform barely as well as two, and after that, a steep decline is registered as more cores are added. The problem is the lack of memory bandwidth as well as contention between processors over the memory bus available to each processor.      The implication for those following a diagonal scaling strategy is to work like heck to make your system fit within eight multicores. After that you'll need to consider some sort of partitioning strategy. What's interesti</p><p>2 0.67647785 <a title="505-lda-2" href="../high_scalability-2010/high_scalability-2010-09-03-Hot_Scalability_Links_For_Sep_3%2C_2010.html">893 high scalability-2010-09-03-Hot Scalability Links For Sep 3, 2010</a></p>
<p>Introduction: With summer almost gone, it's time to fall into some good links...
  
  Hibari - distributed, fault tolerant, highly available key-value store  written in Erlang. In this video Scott Lystig Fritchie gives a very good overview of the newest key-value store.  
 Tweets of Gold       
 
  lenidot : with 12 staff, @ tumblr  serves 1.5billion pageviews/month and 25,000 signups/day. Now that's scalability! 
  jmtan24 : Funny that whenever a high scalability article comes out, it always mention the shared nothing approach 
  mfeathers : When life gives you lemons, you can have decades-long conquest to convert lemons to oranges, or you can make lemonade. 
  OyvindIsene : Met an old man with mustache today, he had no opinion on  #noSQL . Note to myself: Don't grow a mustache, now or later.  
  vlad003 : Isn't it interesting how P2P distributes data while Cloud Computing centralizes it? And they're both said to be the future. 
 
 
 You may be interested in a new  DevOps Meetup  organized by Dave</p><p>3 0.57910866 <a title="505-lda-3" href="../high_scalability-2014/high_scalability-2014-04-01-The_Mullet_Cloud_Selection_Pattern.html">1624 high scalability-2014-04-01-The Mullet Cloud Selection Pattern</a></p>
<p>Introduction: In a recent  thread on Hacker News  one of the commenters mentioned that they use Digital Ocean for personal stuff, but use AWS for business.
 
This DO for personal and AWS for business split has become popular enough that we can now give it a name: the  Mullet Cloud Selection Pattern -  business on the front and party on the back.
 
Providers like DO are cheap and the lightweight composable container model has an aesthetic appeal to developers. Even though it seems like much of the VM infrastructure has to be reinvented for containers, the industry often follows the lead of developer preference.
 
The mullet is dead. Long live the mullet! Developers are ever restless, always eager to move onto something new.</p><p>4 0.56078452 <a title="505-lda-4" href="../high_scalability-2009/high_scalability-2009-10-22-Paper%3A_The_Case_for_RAMClouds%3A_Scalable_High-Performance_Storage_Entirely_in_DRAM_.html">726 high scalability-2009-10-22-Paper: The Case for RAMClouds: Scalable High-Performance Storage Entirely in DRAM </a></p>
<p>Introduction: Stanford Info Lab is taking pains to document a direction we've been moving for a while now, using RAM not just as a cache, but as the primary storage medium. Many  quality products  have built on this model. Even if the vision isn't radical, the paper does produce a lot of data backing up the transition, which is in itself helpful. From the The Abstract:   
 Disk-oriented approaches to online storage are becoming increasingly problematic: they do not scale grace-fully to meet the needs of large-scale Web applications, and improvements in disk capacity have far out-stripped improvements in access latency and bandwidth. This paper argues for a new approach to datacenter storage called RAMCloud, where information is kept entirely in DRAM and large-scale systems are created by aggregating the main memories of thousands of commodity servers. We believe that RAMClouds can provide durable and available storage with 100-1000x the throughput of disk-based systems and 100-1000x lower access lat</p><p>5 0.54548025 <a title="505-lda-5" href="../high_scalability-2008/high_scalability-2008-12-18-Risk_Analysis_on_the_Cloud_%28Using_Excel_and_GigaSpaces%29.html">470 high scalability-2008-12-18-Risk Analysis on the Cloud (Using Excel and GigaSpaces)</a></p>
<p>Introduction: Every day brings news of either more failures of the financial systems or out-right fraud, with the $50 billion Bernard Madoff Ponzi scheme being the latest, breaking all records. This post provide a technical overview of a solution that was implemented for one of the largest banks in China. The solution illustrate how one can use Excel as a front end client and at the same time leverage cloud computing model and mapreduce as well as other patterns to scale-out risk calculations. I'm hoping that this type of approach will reduce the chances for seeing this type of fraud from happening in the future.</p><p>6 0.53424495 <a title="505-lda-6" href="../high_scalability-2012/high_scalability-2012-10-09-Batoo_JPA_-_The_new_JPA_Implementation_that_runs_over_15_times_faster....html">1336 high scalability-2012-10-09-Batoo JPA - The new JPA Implementation that runs over 15 times faster...</a></p>
<p>7 0.53226292 <a title="505-lda-7" href="../high_scalability-2012/high_scalability-2012-01-27-Stuff_The_Internet_Says_On_Scalability_For_January_27%2C_2012.html">1182 high scalability-2012-01-27-Stuff The Internet Says On Scalability For January 27, 2012</a></p>
<p>8 0.51991117 <a title="505-lda-8" href="../high_scalability-2007/high_scalability-2007-07-28-Product%3A_Web_Log_Storming.html">37 high scalability-2007-07-28-Product: Web Log Storming</a></p>
<p>9 0.50160807 <a title="505-lda-9" href="../high_scalability-2007/high_scalability-2007-08-02-Multilanguage_Website.html">54 high scalability-2007-08-02-Multilanguage Website</a></p>
<p>10 0.47253382 <a title="505-lda-10" href="../high_scalability-2011/high_scalability-2011-07-27-Making_Hadoop_1000x_Faster_for_Graph_Problems.html">1088 high scalability-2011-07-27-Making Hadoop 1000x Faster for Graph Problems</a></p>
<p>11 0.46479669 <a title="505-lda-11" href="../high_scalability-2010/high_scalability-2010-11-09-Paper%3A_Hyder_-_Scaling_Out_without_Partitioning_.html">937 high scalability-2010-11-09-Paper: Hyder - Scaling Out without Partitioning </a></p>
<p>12 0.40311736 <a title="505-lda-12" href="../high_scalability-2014/high_scalability-2014-02-28-Stuff_The_Internet_Says_On_Scalability_For_February_28th%2C_2014.html">1603 high scalability-2014-02-28-Stuff The Internet Says On Scalability For February 28th, 2014</a></p>
<p>13 0.39497492 <a title="505-lda-13" href="../high_scalability-2008/high_scalability-2008-06-08-Search_fast_in_million_rows.html">342 high scalability-2008-06-08-Search fast in million rows</a></p>
<p>14 0.37746266 <a title="505-lda-14" href="../high_scalability-2009/high_scalability-2009-10-12-High_Performance_at_Massive_Scale_%E2%80%93__Lessons_learned_at_Facebook.html">720 high scalability-2009-10-12-High Performance at Massive Scale –  Lessons learned at Facebook</a></p>
<p>15 0.3722012 <a title="505-lda-15" href="../high_scalability-2011/high_scalability-2011-09-23-The_Real_News_is_Not_that_Facebook_Serves_Up_1_Trillion_Pages_a_Month%E2%80%A6.html">1123 high scalability-2011-09-23-The Real News is Not that Facebook Serves Up 1 Trillion Pages a Month…</a></p>
<p>16 0.3678109 <a title="505-lda-16" href="../high_scalability-2010/high_scalability-2010-03-22-7_Secrets_to_Successfully_Scaling_with_Scalr_%28on_Amazon%29_by_Sebastian_Stadil.html">798 high scalability-2010-03-22-7 Secrets to Successfully Scaling with Scalr (on Amazon) by Sebastian Stadil</a></p>
<p>17 0.36430955 <a title="505-lda-17" href="../high_scalability-2013/high_scalability-2013-06-13-Busting_4_Modern_Hardware_Myths_-_Are_Memory%2C_HDDs%2C_and_SSDs_Really_Random_Access%3F.html">1475 high scalability-2013-06-13-Busting 4 Modern Hardware Myths - Are Memory, HDDs, and SSDs Really Random Access?</a></p>
<p>18 0.34604323 <a title="505-lda-18" href="../high_scalability-2011/high_scalability-2011-10-24-StackExchange_Architecture_Updates_-_Running_Smoothly%2C_Amazon_4x_More_Expensive.html">1131 high scalability-2011-10-24-StackExchange Architecture Updates - Running Smoothly, Amazon 4x More Expensive</a></p>
<p>19 0.3436774 <a title="505-lda-19" href="../high_scalability-2008/high_scalability-2008-06-04-LinkedIn_Architecture.html">339 high scalability-2008-06-04-LinkedIn Architecture</a></p>
<p>20 0.34292844 <a title="505-lda-20" href="../high_scalability-2007/high_scalability-2007-07-28-Product%3A_Web_Log_Expert.html">36 high scalability-2007-07-28-Product: Web Log Expert</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
