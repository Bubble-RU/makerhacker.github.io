<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>553 high scalability-2009-04-03-Collectl interface to Ganglia - any interest?</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2009" href="../home/high_scalability-2009_home.html">high_scalability-2009</a> <a title="high_scalability-2009-553" href="#">high_scalability-2009-553</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>553 high scalability-2009-04-03-Collectl interface to Ganglia - any interest?</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2009-553-html" href="http://highscalability.com//blog/2009/4/3/collectl-interface-to-ganglia-any-interest.html">html</a></p><p>Introduction: It's been awhile since I've said anything about collectl and I wanted to let
this group know I'm currently working on an interface to ganglia since I've
seen a variety of posts ranging from how much data to log and where to log it
as well as which tools/mechanism to use for logging. From my perspective there
are essentially 2 camps on the monitoring front - one says to have distributed
agents all sending their data to a central point, but don't send too much or
too often. The other camp (which is the one I'm in) says do it all locally
with a highly efficient data collector, because you need a lot of data (I also
read a post in here about logging everything) and you can't possibly monitors
100s or 1Ks of nodes remotely at the granularity necessary to get anything
meaningful.Enter collectl and its evolving interface for ganglia. This will
allow you to log lots of detailed data on local nodes at the usual 10 sec
interval (or more frequent if you prefer) at about 0.1% system overhead while</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 It's been awhile since I've said anything about collectl and I wanted to let this group know I'm currently working on an interface to ganglia since I've seen a variety of posts ranging from how much data to log and where to log it as well as which tools/mechanism to use for logging. [sent-1, score-2.296]
</p><p>2 From my perspective there are essentially 2 camps on the monitoring front - one says to have distributed agents all sending their data to a central point, but don't send too much or too often. [sent-2, score-0.984]
</p><p>3 This will allow you to log lots of detailed data on local nodes at the usual 10 sec interval (or more frequent if you prefer) at about 0. [sent-5, score-1.001]
</p><p>4 1% system overhead while sending a subset at a lower rate to the ganglia gmonds. [sent-6, score-0.639]
</p><p>5 This would give you the best of both worlds but I don't know if people are too married to the centralized concept to try something different. [sent-7, score-0.635]
</p><p>6 I don't know how many people who follow this forum have actually tried it, I know at least a few of you have, but to learn more just go to http://collectl. [sent-8, score-0.558]
</p><p>7 net/ and look at some of documentation or just download the rpm and type 'collectl'. [sent-10, score-0.315]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('collectl', 0.339), ('ganglia', 0.269), ('log', 0.201), ('married', 0.19), ('sending', 0.183), ('camps', 0.169), ('sec', 0.169), ('camp', 0.165), ('interval', 0.148), ('awhile', 0.146), ('remotely', 0.144), ('granularity', 0.14), ('interface', 0.138), ('agents', 0.138), ('says', 0.136), ('know', 0.135), ('anything', 0.129), ('collector', 0.126), ('rpm', 0.125), ('monitors', 0.125), ('worlds', 0.125), ('evolving', 0.118), ('forum', 0.115), ('documentation', 0.114), ('frequent', 0.114), ('nodes', 0.112), ('prefer', 0.11), ('since', 0.109), ('locally', 0.107), ('ranging', 0.106), ('subset', 0.104), ('essentially', 0.102), ('usual', 0.1), ('logging', 0.098), ('centralized', 0.097), ('possibly', 0.097), ('tried', 0.093), ('perspective', 0.092), ('central', 0.088), ('concept', 0.088), ('posts', 0.088), ('variety', 0.087), ('wanted', 0.085), ('overhead', 0.083), ('detailed', 0.081), ('follow', 0.08), ('necessary', 0.079), ('said', 0.078), ('data', 0.076), ('download', 0.076)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="553-tfidf-1" href="../high_scalability-2009/high_scalability-2009-04-03-Collectl_interface_to_Ganglia_-_any_interest%3F.html">553 high scalability-2009-04-03-Collectl interface to Ganglia - any interest?</a></p>
<p>Introduction: It's been awhile since I've said anything about collectl and I wanted to let
this group know I'm currently working on an interface to ganglia since I've
seen a variety of posts ranging from how much data to log and where to log it
as well as which tools/mechanism to use for logging. From my perspective there
are essentially 2 camps on the monitoring front - one says to have distributed
agents all sending their data to a central point, but don't send too much or
too often. The other camp (which is the one I'm in) says do it all locally
with a highly efficient data collector, because you need a lot of data (I also
read a post in here about logging everything) and you can't possibly monitors
100s or 1Ks of nodes remotely at the granularity necessary to get anything
meaningful.Enter collectl and its evolving interface for ganglia. This will
allow you to log lots of detailed data on local nodes at the usual 10 sec
interval (or more frequent if you prefer) at about 0.1% system overhead while</p><p>2 0.36072877 <a title="553-tfidf-2" href="../high_scalability-2009/high_scalability-2009-04-06-How_do_you_monitor_the_performance_of_your_cluster%3F.html">558 high scalability-2009-04-06-How do you monitor the performance of your cluster?</a></p>
<p>Introduction: I had posted a note the other day about collectl and its ganglia interface but
perhaps I wasn't provocative enough to get any responses so let me ask it a
different way, specifically how do people monitor their clusters and more
importantly how often? Do you monitor to get a general sense of what the
system is doing OR do you monitor with the expectation that when something
goes wrong you'll have enough data to diagnose the problem? Or both? I suspect
both...Many cluster-based monitoring tools tend to have a data collection
daemon running on each target node which periodically sends data to some
central management station. That machine typically writes the data to some
database from which it can then extract historical plots. Some even put up
graphics in real-time.From my experience working with large clusters - and I'm
talking either many hundreds or even 1000s of nodes, most have to limit both
the amount of data they manage centrally as well as the frequency that they
collect it, oth</p><p>3 0.18829516 <a title="553-tfidf-3" href="../high_scalability-2007/high_scalability-2007-08-30-Log_Everything_All_the_Time.html">77 high scalability-2007-08-30-Log Everything All the Time</a></p>
<p>Introduction: breakThis JoelOnSoftwarethreadasks the age old question of what and how to
log. The usual trace/error/warning/info advice is totally useless in a large
scale distributed system. Instead, you need tolog everything all the timeso
you can solve problems that have already happened across a potentially huge
range of servers. Yes, it can be done.To see why the typical logging approach
is broken, imagine this scenario: Your site has been up and running great for
weeks. No problems. A foreshadowing beeper goes off at 2AM. It seems some
users can no longer add comments to threads. Then you hear the debugging
deathknell: it's an intermittent problem and customers are pissed. Fix it.
Now.So how are you going to debug this? The monitoring system doesn't show any
obvious problems or errors. You quickly post a comment and it works fine. This
won't be easy. So you think. Commenting involves a bunch of servers and
networks. There's the load balancer, spam filter, web server, database server,
caching s</p><p>4 0.17951819 <a title="553-tfidf-4" href="../high_scalability-2011/high_scalability-2011-08-25-Colmux_-_Finding_Memory_Leaks%2C_High_I-O_Wait_Times%2C_and_Hotness_on_3000_Node_Clusters.html">1104 high scalability-2011-08-25-Colmux - Finding Memory Leaks, High I-O Wait Times, and Hotness on 3000 Node Clusters</a></p>
<p>Introduction: Todd had originally posted an entry oncollectlhere atCollectl - Performance
Data Collector. Collectl collects real-time data from a large number of
subsystems like buddyinfo, cpu, disk, inodes, infiniband, lustre, memory,
network, nfs, processes, quadrics, slabs, sockets and tcp, all using one tool
and in one consistent format.Since then a lot has happened.  It's now part of
both Fedora and Debian distros, not to mention several others. There has also
been a pretty good summary written up byJoe Brockmeier. It's also pretty well
documented (I like to think) onsourceforge. There have also been a few blog
postings by Martin Bachon his blog.Anyhow, awhile back I released a new
version of collectl-utils and gave a complete face-lift to one of the
utilities, colmux, which is a collectl multiplexor.  This tool has the ability
to run collectl on multiple systems, which in turn send all their output back
to colmux.  Colmux then sorts the output on a user-specified column and
reports the 'top-n'</p><p>5 0.17342745 <a title="553-tfidf-5" href="../high_scalability-2009/high_scalability-2009-10-09-Have_you_collectl%27d_yet%3F__If_not%2C_maybe_collectl-utils_will_make_it_easier_to_do_so.html">719 high scalability-2009-10-09-Have you collectl'd yet?  If not, maybe collectl-utils will make it easier to do so</a></p>
<p>Introduction: I'm not sure how many people who follow this have even tried collectl but I
wanted to let you all know that I just released a set of utilities called
strangely enough collectl-utils, which you can get athttp://collectl-
utils.sourceforge.net. One web-based utility called colplot gives you the
ability to very easily plot data from multiple systems in a way that makes
correlating them over time very easy.Another utility called colmux lets you
look at multiple systems in real time. In fact if you go the page that
describes it in more detail you'll see a photo which shows the CPU loads on
192 systems one a second, one set of data/line! in fact the display so wide it
takes 3 large monitors side-by-side to see it all and even though you can't
actually read the displays you can easily see which systems are loaded and
which aren't.Anyhow give it a look and let me know what you think.-mark</p><p>6 0.14575593 <a title="553-tfidf-6" href="../high_scalability-2008/high_scalability-2008-01-30-How_Rackspace_Now_Uses_MapReduce_and_Hadoop_to_Query_Terabytes_of_Data.html">233 high scalability-2008-01-30-How Rackspace Now Uses MapReduce and Hadoop to Query Terabytes of Data</a></p>
<p>7 0.12468021 <a title="553-tfidf-7" href="../high_scalability-2013/high_scalability-2013-01-21-Processing_100_Million_Pixels_a_Day_-_Small_Amounts_of_Contention_Cause_Big_Problems_at_Scale.html">1390 high scalability-2013-01-21-Processing 100 Million Pixels a Day - Small Amounts of Contention Cause Big Problems at Scale</a></p>
<p>8 0.12037378 <a title="553-tfidf-8" href="../high_scalability-2008/high_scalability-2008-05-19-Twitter_as_a_scalability_case_study.html">323 high scalability-2008-05-19-Twitter as a scalability case study</a></p>
<p>9 0.11759448 <a title="553-tfidf-9" href="../high_scalability-2008/high_scalability-2008-02-03-Product%3A_Collectl_-_Performance_Data_Collector.html">237 high scalability-2008-02-03-Product: Collectl - Performance Data Collector</a></p>
<p>10 0.11425724 <a title="553-tfidf-10" href="../high_scalability-2007/high_scalability-2007-07-26-Product%3A_AWStats_a_Log_Analyzer.html">30 high scalability-2007-07-26-Product: AWStats a Log Analyzer</a></p>
<p>11 0.10644431 <a title="553-tfidf-11" href="../high_scalability-2007/high_scalability-2007-07-28-Product%3A_Web_Log_Storming.html">37 high scalability-2007-07-28-Product: Web Log Storming</a></p>
<p>12 0.1050081 <a title="553-tfidf-12" href="../high_scalability-2009/high_scalability-2009-03-16-Product%3A_Smart_Inspect.html">541 high scalability-2009-03-16-Product: Smart Inspect</a></p>
<p>13 0.097988546 <a title="553-tfidf-13" href="../high_scalability-2008/high_scalability-2008-11-24-Product%3A_Scribe_-_Facebook%27s_Scalable_Logging_System.html">449 high scalability-2008-11-24-Product: Scribe - Facebook's Scalable Logging System</a></p>
<p>14 0.089433089 <a title="553-tfidf-14" href="../high_scalability-2010/high_scalability-2010-11-09-Paper%3A_Hyder_-_Scaling_Out_without_Partitioning_.html">937 high scalability-2010-11-09-Paper: Hyder - Scaling Out without Partitioning </a></p>
<p>15 0.086451098 <a title="553-tfidf-15" href="../high_scalability-2014/high_scalability-2014-01-14-Ask_HS%3A_Design_and_Implementation_of_scalable_services%3F.html">1578 high scalability-2014-01-14-Ask HS: Design and Implementation of scalable services?</a></p>
<p>16 0.081448123 <a title="553-tfidf-16" href="../high_scalability-2011/high_scalability-2011-03-22-Facebook%27s_New_Realtime_Analytics_System%3A_HBase_to_Process_20_Billion_Events_Per_Day.html">1008 high scalability-2011-03-22-Facebook's New Realtime Analytics System: HBase to Process 20 Billion Events Per Day</a></p>
<p>17 0.076701649 <a title="553-tfidf-17" href="../high_scalability-2007/high_scalability-2007-10-01-Statistics_Logging_Scalability.html">105 high scalability-2007-10-01-Statistics Logging Scalability</a></p>
<p>18 0.076154724 <a title="553-tfidf-18" href="../high_scalability-2010/high_scalability-2010-05-03-MocoSpace_Architecture_-_3_Billion_Mobile_Page_Views_a_Month.html">821 high scalability-2010-05-03-MocoSpace Architecture - 3 Billion Mobile Page Views a Month</a></p>
<p>19 0.074839354 <a title="553-tfidf-19" href="../high_scalability-2013/high_scalability-2013-07-17-How_do_you_create_a_100th_Monkey_software_development_culture%3F.html">1492 high scalability-2013-07-17-How do you create a 100th Monkey software development culture?</a></p>
<p>20 0.07434047 <a title="553-tfidf-20" href="../high_scalability-2010/high_scalability-2010-12-06-What_the_heck_are_you_actually_using_NoSQL_for%3F.html">954 high scalability-2010-12-06-What the heck are you actually using NoSQL for?</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.138), (1, 0.047), (2, -0.023), (3, -0.017), (4, 0.019), (5, 0.008), (6, 0.045), (7, 0.036), (8, 0.029), (9, -0.006), (10, -0.009), (11, 0.044), (12, 0.025), (13, -0.04), (14, 0.094), (15, 0.0), (16, 0.035), (17, 0.001), (18, -0.055), (19, -0.0), (20, -0.009), (21, -0.048), (22, -0.053), (23, 0.132), (24, 0.045), (25, -0.044), (26, -0.043), (27, 0.011), (28, -0.056), (29, -0.022), (30, -0.04), (31, -0.102), (32, 0.03), (33, 0.002), (34, -0.039), (35, 0.044), (36, -0.002), (37, -0.04), (38, 0.047), (39, 0.014), (40, 0.011), (41, 0.029), (42, 0.013), (43, -0.009), (44, -0.01), (45, 0.045), (46, 0.02), (47, -0.003), (48, 0.0), (49, -0.022)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.91502631 <a title="553-lsi-1" href="../high_scalability-2009/high_scalability-2009-04-03-Collectl_interface_to_Ganglia_-_any_interest%3F.html">553 high scalability-2009-04-03-Collectl interface to Ganglia - any interest?</a></p>
<p>Introduction: It's been awhile since I've said anything about collectl and I wanted to let
this group know I'm currently working on an interface to ganglia since I've
seen a variety of posts ranging from how much data to log and where to log it
as well as which tools/mechanism to use for logging. From my perspective there
are essentially 2 camps on the monitoring front - one says to have distributed
agents all sending their data to a central point, but don't send too much or
too often. The other camp (which is the one I'm in) says do it all locally
with a highly efficient data collector, because you need a lot of data (I also
read a post in here about logging everything) and you can't possibly monitors
100s or 1Ks of nodes remotely at the granularity necessary to get anything
meaningful.Enter collectl and its evolving interface for ganglia. This will
allow you to log lots of detailed data on local nodes at the usual 10 sec
interval (or more frequent if you prefer) at about 0.1% system overhead while</p><p>2 0.8001712 <a title="553-lsi-2" href="../high_scalability-2007/high_scalability-2007-08-30-Log_Everything_All_the_Time.html">77 high scalability-2007-08-30-Log Everything All the Time</a></p>
<p>Introduction: breakThis JoelOnSoftwarethreadasks the age old question of what and how to
log. The usual trace/error/warning/info advice is totally useless in a large
scale distributed system. Instead, you need tolog everything all the timeso
you can solve problems that have already happened across a potentially huge
range of servers. Yes, it can be done.To see why the typical logging approach
is broken, imagine this scenario: Your site has been up and running great for
weeks. No problems. A foreshadowing beeper goes off at 2AM. It seems some
users can no longer add comments to threads. Then you hear the debugging
deathknell: it's an intermittent problem and customers are pissed. Fix it.
Now.So how are you going to debug this? The monitoring system doesn't show any
obvious problems or errors. You quickly post a comment and it works fine. This
won't be easy. So you think. Commenting involves a bunch of servers and
networks. There's the load balancer, spam filter, web server, database server,
caching s</p><p>3 0.77556628 <a title="553-lsi-3" href="../high_scalability-2013/high_scalability-2013-01-21-Processing_100_Million_Pixels_a_Day_-_Small_Amounts_of_Contention_Cause_Big_Problems_at_Scale.html">1390 high scalability-2013-01-21-Processing 100 Million Pixels a Day - Small Amounts of Contention Cause Big Problems at Scale</a></p>
<p>Introduction: This is a guest post byGordon Worley, a Software Engineer atKorrelate, where
they correlate (see what they did there) online purchases to offline
purchases.Several weeks ago, we came into the office one morning to find every
server alarm going off. Pixel log processing was behind by 8 hours and not
making headway. Checking the logs, we discovered that a big client had come
online during the night and was giving us 10 times more traffic than we were
originally told to expect. I wouldn't say we panicked, but the office was
certainly more jittery than usual. Over the next several hours, though, thanks
both to foresight and quick thinking, we were able to scale up to handle the
added load and clear the backlog to return log processing to a steady state.At
Korrelate, we deploytracking pixels, also known beacons or web bugs, that our
partners use to send us information about their users. These tiny web objects
contain no visible content, but may include transparent 1 by 1 gifs or
Javascript,</p><p>4 0.7416749 <a title="553-lsi-4" href="../high_scalability-2008/high_scalability-2008-11-24-Product%3A_Scribe_-_Facebook%27s_Scalable_Logging_System.html">449 high scalability-2008-11-24-Product: Scribe - Facebook's Scalable Logging System</a></p>
<p>Introduction: InLog Everything All the TimeI advocate applications shouldn't bother logging
at all. Why waste all that time and code? No, wait, that's not right. I preach
logging everything all the time. Doh. Facebook obviously feels similarly which
is why they opened sourcedScribe, their internal logging system, capable of
logging 10s of billions of messages per day. These messages include access
logs, performance statistics, actions that went to News Feed, and many
others.Imagine hundreds of thousands of machines across many geographical
dispersed datacenters just aching to send their precious log payload to the
central repository off all knowledge. Because really, when you combine all the
meta data with all the events you pretty much have a complete picture of your
operations. Once in the central repository logs can be scanned, indexed,
summarized, aggregated, refactored, diced, data cubed, and mined for every
scrap of potentially useful information.Just imagine the log stream from all
of Faceboo</p><p>5 0.71726733 <a title="553-lsi-5" href="../high_scalability-2009/high_scalability-2009-03-16-Product%3A_Smart_Inspect.html">541 high scalability-2009-03-16-Product: Smart Inspect</a></p>
<p>Introduction: Smart Inspecthas added quite a few features specifically tailored to
highscalability and high performance environments to our tool over the
years.This includes the ability to log to memory and dump log files on
demand(when a crash occurs for example), special backlog queue features, a
logservice application for central log storage and a lot more.
Additionally,our SmartInspect Console (the viewer application) makes viewing,
filteringand inspecting large amounts of logging data a lot easier/practical.</p><p>6 0.68345195 <a title="553-lsi-6" href="../high_scalability-2007/high_scalability-2007-07-26-Product%3A_AWStats_a_Log_Analyzer.html">30 high scalability-2007-07-26-Product: AWStats a Log Analyzer</a></p>
<p>7 0.67179227 <a title="553-lsi-7" href="../high_scalability-2008/high_scalability-2008-01-30-How_Rackspace_Now_Uses_MapReduce_and_Hadoop_to_Query_Terabytes_of_Data.html">233 high scalability-2008-01-30-How Rackspace Now Uses MapReduce and Hadoop to Query Terabytes of Data</a></p>
<p>8 0.66291225 <a title="553-lsi-8" href="../high_scalability-2008/high_scalability-2008-02-03-Product%3A_Collectl_-_Performance_Data_Collector.html">237 high scalability-2008-02-03-Product: Collectl - Performance Data Collector</a></p>
<p>9 0.66116112 <a title="553-lsi-9" href="../high_scalability-2009/high_scalability-2009-04-06-How_do_you_monitor_the_performance_of_your_cluster%3F.html">558 high scalability-2009-04-06-How do you monitor the performance of your cluster?</a></p>
<p>10 0.65588707 <a title="553-lsi-10" href="../high_scalability-2013/high_scalability-2013-08-07-RAFT_-_In_Search_of_an_Understandable_Consensus_Algorithm.html">1498 high scalability-2013-08-07-RAFT - In Search of an Understandable Consensus Algorithm</a></p>
<p>11 0.6527698 <a title="553-lsi-11" href="../high_scalability-2010/high_scalability-2010-11-09-Paper%3A_Hyder_-_Scaling_Out_without_Partitioning_.html">937 high scalability-2010-11-09-Paper: Hyder - Scaling Out without Partitioning </a></p>
<p>12 0.64904606 <a title="553-lsi-12" href="../high_scalability-2009/high_scalability-2009-10-09-Have_you_collectl%27d_yet%3F__If_not%2C_maybe_collectl-utils_will_make_it_easier_to_do_so.html">719 high scalability-2009-10-09-Have you collectl'd yet?  If not, maybe collectl-utils will make it easier to do so</a></p>
<p>13 0.64794129 <a title="553-lsi-13" href="../high_scalability-2011/high_scalability-2011-08-25-Colmux_-_Finding_Memory_Leaks%2C_High_I-O_Wait_Times%2C_and_Hotness_on_3000_Node_Clusters.html">1104 high scalability-2011-08-25-Colmux - Finding Memory Leaks, High I-O Wait Times, and Hotness on 3000 Node Clusters</a></p>
<p>14 0.64523852 <a title="553-lsi-14" href="../high_scalability-2007/high_scalability-2007-07-30-Product%3A_SmarterStats.html">45 high scalability-2007-07-30-Product: SmarterStats</a></p>
<p>15 0.61965001 <a title="553-lsi-15" href="../high_scalability-2007/high_scalability-2007-10-01-Statistics_Logging_Scalability.html">105 high scalability-2007-10-01-Statistics Logging Scalability</a></p>
<p>16 0.61696446 <a title="553-lsi-16" href="../high_scalability-2008/high_scalability-2008-04-19-How_to_build_a_real-time_analytics_system%3F.html">304 high scalability-2008-04-19-How to build a real-time analytics system?</a></p>
<p>17 0.61005622 <a title="553-lsi-17" href="../high_scalability-2008/high_scalability-2008-04-02-Product%3A_Supervisor_-__Monitor_and_Control_Your_Processes.html">295 high scalability-2008-04-02-Product: Supervisor -  Monitor and Control Your Processes</a></p>
<p>18 0.6009165 <a title="553-lsi-18" href="../high_scalability-2007/high_scalability-2007-07-28-Product%3A_Web_Log_Expert.html">36 high scalability-2007-07-28-Product: Web Log Expert</a></p>
<p>19 0.60015237 <a title="553-lsi-19" href="../high_scalability-2009/high_scalability-2009-01-08-file_synchronization_solutions.html">488 high scalability-2009-01-08-file synchronization solutions</a></p>
<p>20 0.59820771 <a title="553-lsi-20" href="../high_scalability-2009/high_scalability-2009-04-15-Implementing_large_scale_web_analytics.html">570 high scalability-2009-04-15-Implementing large scale web analytics</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.174), (2, 0.213), (10, 0.071), (57, 0.246), (61, 0.108), (77, 0.025), (94, 0.057)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.95033604 <a title="553-lda-1" href="../high_scalability-2007/high_scalability-2007-11-18-Reverse_Proxy.html">159 high scalability-2007-11-18-Reverse Proxy</a></p>
<p>Introduction: Hi,I saw an year ago that Netapp sold netcache to blu-coat, my site is a heavy
NetCache user and we cached 83% of our site. We tested with Blue-coat and F5
WA and we are not getting same performce as NetCache.Any of you guys have the
same issue? or somebody knows another product can handle much
traffic?ThanksRodrigo</p><p>2 0.91928804 <a title="553-lda-2" href="../high_scalability-2011/high_scalability-2011-11-17-Five_Misconceptions_on_Cloud_Portability.html">1144 high scalability-2011-11-17-Five Misconceptions on Cloud Portability</a></p>
<p>Introduction: The term "cloud portability" is often considered a synonym for
"CloudAPIportability," which implies a series of misconceptions.If we break
away from dogma, we can find that what we really looking for in cloud
portability is Application portability between clouds which can be a vastly
simpler requirement, as we can achieve application portability without
settling on a common CloudAPI.In this post i'll be covering five common
misconceptions people haveWRTto cloud portability.Cloud portability = Cloud
API portability. API portability is easy; cloud API portability is not.The
main incentive for Cloud Portability is - Avoiding Vendor lock-in.Cloud
portability is more about business agility than it is about vendor lock-
in.Cloud portability isn't for startups. Every startup that is expecting rapid
growth should re-examine their deployments and plan for cloud portability
rather than wait to be forced to make the switch when you are least prepared
to do so.Cloud portability = Compromising on t</p><p>3 0.9064585 <a title="553-lda-3" href="../high_scalability-2009/high_scalability-2009-10-28-Need_for_change_in_your_IT_infrastructure_.html">731 high scalability-2009-10-28-Need for change in your IT infrastructure </a></p>
<p>Introduction: Companies earnings outstripforecasts, consumer confidence is retuning and city
bonuses areback. What does this mean for business? Growth! After the recent
years of cost cutting in IT budgets, there is the sudden fear induced from
increased demand. Pre-existing trouble points in IT infrastructures that have
lain dormant will suddenly be exposed. Monthly reporting and real time
analytics will suffer as data grows. IT departments across the land will be
crying out "The engine canna take no more captain". What can be done?.entry-
bodyWhat we need is a scalable system that grows with the business. A system
that can handle sudden increases in data growth without falling over. There
are two core principles to a scalable system (1) Users experience constant QoS
as demand grows (2) System Architects can grow system capacity proportionally
with the available resources. In other words, if demand increases twofold, it
is "enough" to purchase twice the hardware.This is linear growth. Is it enough
t</p><p>4 0.89887226 <a title="553-lda-4" href="../high_scalability-2011/high_scalability-2011-01-04-Map-Reduce_With_Ruby_Using_Hadoop.html">968 high scalability-2011-01-04-Map-Reduce With Ruby Using Hadoop</a></p>
<p>Introduction: A demonstration, with repeatable steps, of how to quickly fire-up a Hadoop
cluster on Amazon EC2, load data onto the HDFS (Hadoop Distributed File-
System), write map-reduce scripts in Ruby and use them to run a map-reduce job
on your Hadoop cluster. You willnotneed to ssh into the cluster, as all tasks
are run from your local machine. Below I am using my MacBook Pro as my local
machine, but the steps I have provided should be reproducible on other
platforms running bash and Java.Fire-Up Your Hadoop ClusterI choose
theCloudera distribution of Hadoopwhich is still 100% Apache licensed, but has
some additional benefits. One of these benefits is that it is released byDoug
Cutting, who started Hadoop and drove it’s development at Yahoo! He also
startedLucene, which is another of my favourite Apache Projects, so I have
good faith that he knows what he is doing. Another benefit, as you will see,
is that it is simple to fire-up a Hadoop cluster.I am going to use
Cloudera’sWhirr script, which</p><p>5 0.87604475 <a title="553-lda-5" href="../high_scalability-2010/high_scalability-2010-04-09-Vagrant_-_Build_and_Deploy_Virtualized_Development_Environments_Using_Ruby.html">807 high scalability-2010-04-09-Vagrant - Build and Deploy Virtualized Development Environments Using Ruby</a></p>
<p>Introduction: One of the cool things we are seeing is more tools and tool chains for
performing very high level operations quite simply.Vagrantis such a tool
forbuilding and distributing virtualized development environments.Web
developers use virtual environments every day with their web applications.
From EC2 and Rackspace Cloud to specialized solutions such as EngineYard and
Heroku, virtualization is the tool of choice for easy deployment and
infrastructure management. Vagrant aims to take those very same principles and
put them to work in the heart of the application lifecycle. By providing easy
to configure, lightweight, reproducible, and portable virtual machines
targeted at development environments, Vagrant helps maximize your productivity
and flexibility.If you've created a build and deployment system before Vagrant
does a lot of the work for you:Automated virtual machine creation
usingOracle's VirtualBoxAutomated provisioning of virtual environments
usingchefForward ports to the host machine</p><p>6 0.87174439 <a title="553-lda-6" href="../high_scalability-2008/high_scalability-2008-10-29-CTL_-_Distributed_Control_Dispatching_Framework_.html">433 high scalability-2008-10-29-CTL - Distributed Control Dispatching Framework </a></p>
<p>7 0.86584735 <a title="553-lda-7" href="../high_scalability-2012/high_scalability-2012-03-19-LinkedIn%3A_Creating_a_Low_Latency_Change_Data_Capture_System_with_Databus.html">1211 high scalability-2012-03-19-LinkedIn: Creating a Low Latency Change Data Capture System with Databus</a></p>
<p>same-blog 8 0.84963995 <a title="553-lda-8" href="../high_scalability-2009/high_scalability-2009-04-03-Collectl_interface_to_Ganglia_-_any_interest%3F.html">553 high scalability-2009-04-03-Collectl interface to Ganglia - any interest?</a></p>
<p>9 0.84821159 <a title="553-lda-9" href="../high_scalability-2008/high_scalability-2008-01-17-Moving_old_to_new._Do_not_be_afraid_of_the_re-write_--_but_take_some_help.html">218 high scalability-2008-01-17-Moving old to new. Do not be afraid of the re-write -- but take some help</a></p>
<p>10 0.84135979 <a title="553-lda-10" href="../high_scalability-2011/high_scalability-2011-11-07-10_Core_Architecture_Pattern_Variations_for_Achieving_Scalability.html">1138 high scalability-2011-11-07-10 Core Architecture Pattern Variations for Achieving Scalability</a></p>
<p>11 0.82811427 <a title="553-lda-11" href="../high_scalability-2007/high_scalability-2007-07-11-Friendster_Architecture.html">6 high scalability-2007-07-11-Friendster Architecture</a></p>
<p>12 0.80056256 <a title="553-lda-12" href="../high_scalability-2010/high_scalability-2010-07-11-So%2C_Why_is_Twitter_Really_Not_Using_Cassandra_to_Store_Tweets%3F.html">855 high scalability-2010-07-11-So, Why is Twitter Really Not Using Cassandra to Store Tweets?</a></p>
<p>13 0.79445583 <a title="553-lda-13" href="../high_scalability-2008/high_scalability-2008-01-29-When_things_aren%27t_scalable.html">232 high scalability-2008-01-29-When things aren't scalable</a></p>
<p>14 0.77969623 <a title="553-lda-14" href="../high_scalability-2008/high_scalability-2008-07-16-The_Mother_of_All_Database_Normalization_Debates_on_Coding_Horror.html">351 high scalability-2008-07-16-The Mother of All Database Normalization Debates on Coding Horror</a></p>
<p>15 0.77331322 <a title="553-lda-15" href="../high_scalability-2011/high_scalability-2011-07-26-Web_2.0_Killed_the_Middleware_Star.html">1087 high scalability-2011-07-26-Web 2.0 Killed the Middleware Star</a></p>
<p>16 0.75732601 <a title="553-lda-16" href="../high_scalability-2007/high_scalability-2007-08-10-How_do_we_make_a_large_real-time_search_engine%3F.html">64 high scalability-2007-08-10-How do we make a large real-time search engine?</a></p>
<p>17 0.75388163 <a title="553-lda-17" href="../high_scalability-2012/high_scalability-2012-09-26-WordPress.com_Serves_70%2C000_req-sec_and_over_15_Gbit-sec_of_Traffic_using_NGINX.html">1329 high scalability-2012-09-26-WordPress.com Serves 70,000 req-sec and over 15 Gbit-sec of Traffic using NGINX</a></p>
<p>18 0.75298655 <a title="553-lda-18" href="../high_scalability-2009/high_scalability-2009-08-31-Squarespace_Architecture_-_A_Grid_Handles_Hundreds_of_Millions_of_Requests_a_Month_.html">691 high scalability-2009-08-31-Squarespace Architecture - A Grid Handles Hundreds of Millions of Requests a Month </a></p>
<p>19 0.75218797 <a title="553-lda-19" href="../high_scalability-2013/high_scalability-2013-08-26-Reddit%3A_Lessons_Learned_from_Mistakes_Made_Scaling_to_1_Billion_Pageviews_a_Month.html">1507 high scalability-2013-08-26-Reddit: Lessons Learned from Mistakes Made Scaling to 1 Billion Pageviews a Month</a></p>
<p>20 0.75135303 <a title="553-lda-20" href="../high_scalability-2012/high_scalability-2012-08-27-Zoosk_-_The_Engineering_behind_Real_Time_Communications.html">1312 high scalability-2012-08-27-Zoosk - The Engineering behind Real Time Communications</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
