<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>514 high scalability-2009-02-18-Numbers Everyone Should Know</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2009" href="../home/high_scalability-2009_home.html">high_scalability-2009</a> <a title="high_scalability-2009-514" href="#">high_scalability-2009-514</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>514 high scalability-2009-02-18-Numbers Everyone Should Know</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2009-514-html" href="http://highscalability.com//blog/2009/2/18/numbers-everyone-should-know.html">html</a></p><p>Introduction: Google AppEngine Numbers  
This group of numbers is from Brett Slatkin in  Building Scalable Web Apps with Google App Engine .
  Writes are expensive!   Datastore is transactional: writes require disk access   Disk access means disk seeks   Rule of thumb: 10ms for a disk seek   Simple math: 1s / 10ms = 100 seeks/sec maximum   Depends on: * The size and shape of your data * Doing work in batches (batch puts and gets) 
 Reads are cheap! 
   Reads do not need to be transactional, just consistent   Data is read from disk once, then it's easily cached   All subsequent reads come straight from memory   Rule of thumb: 250usec for 1MB of data from memory   Simple math: 1s / 250usec = 4GB/sec maximum * For a 1MB entity, that's 4000 fetches/sec 
 Numbers Miscellaneous 
This group of numbers is from a presentation  Jeff Dean  gave at a Engineering All-Hands Meeting at Google.      L1 cache reference 0.5 ns    Branch mispredict 5 ns    L2 cache reference 7 ns    Mutex lock/unlock 100 ns    Main me</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Datastore is transactional: writes require disk access   Disk access means disk seeks   Rule of thumb: 10ms for a disk seek   Simple math: 1s / 10ms = 100 seeks/sec maximum   Depends on: * The size and shape of your data * Doing work in batches (batch puts and gets)   Reads are cheap! [sent-3, score-0.413]
</p><p>2 Given the the number of writes that can be made per second is so limited, a high write load serializes and slows down the whole process. [sent-22, score-0.314]
</p><p>3 Reads are cheap so we replace having a single easily read counter with having to make multiple reads to recover the actual count. [sent-31, score-0.436]
</p><p>4 Frequently updated shared variables are expensive so we shard and parallelize those writes. [sent-32, score-0.331]
</p><p>5 But to scale writes you need to partition and once you partition it becomes difficult to keep any shared state like counters. [sent-34, score-0.294]
</p><p>6 As a comment is made you get a sequence number and that's the order comments are displayed. [sent-39, score-0.643]
</p><p>7 But as we saw in the last section shared state like a single counter won't scale in high write environments. [sent-40, score-0.461]
</p><p>8 A sharded counter won't work in this situation either because summing the shared counters isn't transactional. [sent-41, score-0.515]
</p><p>9 There's no way to guarantee each comment will get back the sequence number it allocated so we could have duplicates. [sent-42, score-0.502]
</p><p>10 So what is needed for a key is something unique and alphabetical so when searching through comments you can go forward and backward using only keys. [sent-44, score-0.459]
</p><p>11 BigTable knows how to get things by keys so you must make keys that return data in the proper order. [sent-48, score-0.364]
</p><p>12 In the grand old tradition of making unique keys we just keep appending stuff until it becomes unique. [sent-49, score-0.293]
</p><p>13 The suggested key for GAE is:  time stamp  + user ID + user comment ID. [sent-50, score-0.576]
</p><p>14 What we need then is a sequence number for each user's comments. [sent-57, score-0.287]
</p><p>15 Our goal is to remove write contention so we want to parallelize writes. [sent-59, score-0.287]
</p><p>16 When a user adds a comment it's added to a user's comment list and a sequence number is allocated. [sent-62, score-0.844]
</p><p>17 So each comment add is guaranteed to be unique because updates in an Entity Group are serialized. [sent-64, score-0.408]
</p><p>18 The resulting key is guaranteed unique and sorts properly in alphabetical order. [sent-65, score-0.4]
</p><p>19 When paging a query is made across entity groups using the ID index. [sent-66, score-0.299]
</p><p>20 The idea of keeping per user comment indexes is out there. [sent-71, score-0.342]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('counter', 0.221), ('comment', 0.215), ('sequence', 0.211), ('alphabetical', 0.207), ('nsread', 0.207), ('keys', 0.182), ('entity', 0.152), ('sequentially', 0.149), ('shared', 0.148), ('paging', 0.147), ('writes', 0.146), ('comments', 0.141), ('nssend', 0.138), ('reads', 0.137), ('count', 0.135), ('user', 0.127), ('mb', 0.123), ('unique', 0.111), ('gae', 0.109), ('bigtable', 0.108), ('stamp', 0.107), ('thumb', 0.105), ('reference', 0.102), ('contention', 0.1), ('parallelize', 0.095), ('appengine', 0.094), ('write', 0.092), ('transactional', 0.091), ('disk', 0.089), ('shard', 0.088), ('math', 0.083), ('guaranteed', 0.082), ('cheap', 0.078), ('counters', 0.076), ('number', 0.076), ('numbers', 0.074), ('bytes', 0.073), ('situation', 0.07), ('cleverly', 0.069), ('counterswe', 0.069), ('deangave', 0.069), ('inbuilding', 0.069), ('mispredict', 0.069), ('nsbranch', 0.069), ('nscompress', 0.069), ('nsdisk', 0.069), ('nsmain', 0.069), ('nsmutex', 0.069), ('nsround', 0.069), ('zippy', 0.069)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="514-tfidf-1" href="../high_scalability-2009/high_scalability-2009-02-18-Numbers_Everyone_Should_Know.html">514 high scalability-2009-02-18-Numbers Everyone Should Know</a></p>
<p>Introduction: Google AppEngine Numbers  
This group of numbers is from Brett Slatkin in  Building Scalable Web Apps with Google App Engine .
  Writes are expensive!   Datastore is transactional: writes require disk access   Disk access means disk seeks   Rule of thumb: 10ms for a disk seek   Simple math: 1s / 10ms = 100 seeks/sec maximum   Depends on: * The size and shape of your data * Doing work in batches (batch puts and gets) 
 Reads are cheap! 
   Reads do not need to be transactional, just consistent   Data is read from disk once, then it's easily cached   All subsequent reads come straight from memory   Rule of thumb: 250usec for 1MB of data from memory   Simple math: 1s / 250usec = 4GB/sec maximum * For a 1MB entity, that's 4000 fetches/sec 
 Numbers Miscellaneous 
This group of numbers is from a presentation  Jeff Dean  gave at a Engineering All-Hands Meeting at Google.      L1 cache reference 0.5 ns    Branch mispredict 5 ns    L2 cache reference 7 ns    Mutex lock/unlock 100 ns    Main me</p><p>2 0.31927207 <a title="514-tfidf-2" href="../high_scalability-2011/high_scalability-2011-01-26-Google_Pro_Tip%3A_Use_Back-of-the-envelope-calculations_to_Choose_the_Best_Design.html">978 high scalability-2011-01-26-Google Pro Tip: Use Back-of-the-envelope-calculations to Choose the Best Design</a></p>
<p>Introduction: How do you know which is the "best" design for a given problem? If, for example, you were given the problem of generating an image search results page of 30 thumbnails, would you load images sequentially? In parallel? Would you cache? How would you decide?
 
  If you could harness the  power of the multiverse  you could try every possible option in the design space and see which worked best. But that's crazy impractical, isn't it?
 
Another option is to consider the  order of various algorithm  alternatives. As a prophet for the Golden Age of  Computational Thinking , Google would definitely do this, but what else might Google do?
  Use Back-of-the-envelope Calculations to Evaluate Different Designs  
 Jeff Dean , Head of Google's School of Infrastructure Wizardry—instrumental in many of Google's key systems: ad serving, BigTable; search, MapReduce, ProtocolBuffers—advocates evaluating different designs using  back-of-the-envelope calculations . He gives the full story in this  Stanfor</p><p>3 0.30648389 <a title="514-tfidf-3" href="../high_scalability-2008/high_scalability-2008-05-27-How_I_Learned_to_Stop_Worrying_and_Love_Using_a_Lot_of_Disk_Space_to_Scale.html">327 high scalability-2008-05-27-How I Learned to Stop Worrying and Love Using a Lot of Disk Space to Scale</a></p>
<p>Introduction: Update 3 : ReadWriteWeb says  Google App Engine Announces New Pricing Plans, APIs, Open Access . Pricing is specified but I'm not sure what to make of it yet. An image manipulation library is added (thus the need to pay for more CPU :-) and memcached support has been added. Memcached will help resolve the can't write for every read problem that pops up when keeping counters.  Update 2 : onGWT.com threw a GAE load party and a lot of people came. The results at  Load test : Google App Engine = 1, Community = 0 . GAE handled a peak of 35 requests/second and a sustained 10 requests/second. Some think performance was good, others not so good. My GMT watch broke and I was late to arrive. Maybe next time. Also added a few new design rules from the post.   Update : Added a few new rules gleaned from the  GAE Meetup : Design By Explicit Cost Model and Puts are Precious.  How do you structure your database using a distributed hash table like  BigTable ? The answer isn't what you might expect. If</p><p>4 0.19274458 <a title="514-tfidf-4" href="../high_scalability-2009/high_scalability-2009-02-21-Google_AppEngine_-_A_Second_Look.html">517 high scalability-2009-02-21-Google AppEngine - A Second Look</a></p>
<p>Introduction: Update 6: :  Back to the Future for Data Storage .  We are in the middle of a renaissance in data storage with the application of many new ideas and techniques; there's huge potential for breaking out of thinking about data storage in just one way.   Update 5 :  Building Scalable Web Applications with Google App Engine  by Brett Slatkin.
 
 Update 4 :  Why Google App Engine is broken and what Google must do to fix it  by Aral Balkan.  We don't care that it can scale. We care that it does scale. And that it scales when you need it the most.   Issues: 1MB limit on data structures; 1MB limit on data structures; the short-term high CPU quota; quotas in general; Admin? What's that?  Update 3 : BigTable  Blues . Catherine Devlin couldn't port an application to GAE because it can't do basic filtering and can't search 5,000 records without timing out: "Querying from 5000 records - too much for the mighty BigTable, apparently." Followup:  not the future database . "90% of the work of this proje</p><p>5 0.17767158 <a title="514-tfidf-5" href="../high_scalability-2010/high_scalability-2010-10-15-Troubles_with_Sharding_-_What_can_we_learn_from_the_Foursquare_Incident%3F.html">920 high scalability-2010-10-15-Troubles with Sharding - What can we learn from the Foursquare Incident?</a></p>
<p>Introduction: For everything given something seems to be taken. Caching is a great scalability solution, but caching also  comes with problems .  Sharding  is a great scalability solution, but as Foursquare recently revealed in a  post-mortem  about their 17 hours of downtime, sharding also has problems. MongoDB, the database Foursquare uses, also contributed their  post-mortem  of what went wrong too.
 
Now that everyone has shared and resharded, what can we learn to help us skip these mistakes and quickly move on to a different set of mistakes?
 
First, like for  Facebook , huge props to Foursquare and MongoDB for being upfront and honest about their problems. This helps everyone get better and is a sign we work in a pretty cool industry.
 
Second, overall, the fault didn't flow from evil hearts or gross negligence. As usual the cause was more mundane: a key system, that could be a little more robust, combined with a very popular application built by a small group of people, under immense pressure</p><p>6 0.1682815 <a title="514-tfidf-6" href="../high_scalability-2009/high_scalability-2009-08-24-How_Google_Serves_Data_from_Multiple_Datacenters.html">687 high scalability-2009-08-24-How Google Serves Data from Multiple Datacenters</a></p>
<p>7 0.15432666 <a title="514-tfidf-7" href="../high_scalability-2011/high_scalability-2011-09-07-What_Google_App_Engine_Price_Changes_Say_About_the_Future_of_Web_Architecture.html">1112 high scalability-2011-09-07-What Google App Engine Price Changes Say About the Future of Web Architecture</a></p>
<p>8 0.14378382 <a title="514-tfidf-8" href="../high_scalability-2011/high_scalability-2011-01-11-Google_Megastore_-_3_Billion_Writes_and_20_Billion_Read_Transactions_Daily.html">972 high scalability-2011-01-11-Google Megastore - 3 Billion Writes and 20 Billion Read Transactions Daily</a></p>
<p>9 0.14185093 <a title="514-tfidf-9" href="../high_scalability-2011/high_scalability-2011-03-22-Facebook%27s_New_Realtime_Analytics_System%3A_HBase_to_Process_20_Billion_Events_Per_Day.html">1008 high scalability-2011-03-22-Facebook's New Realtime Analytics System: HBase to Process 20 Billion Events Per Day</a></p>
<p>10 0.14154199 <a title="514-tfidf-10" href="../high_scalability-2009/high_scalability-2009-03-16-Are_Cloud_Based_Memory_Architectures_the_Next_Big_Thing%3F.html">538 high scalability-2009-03-16-Are Cloud Based Memory Architectures the Next Big Thing?</a></p>
<p>11 0.14112096 <a title="514-tfidf-11" href="../high_scalability-2009/high_scalability-2009-08-06-An_Unorthodox_Approach_to_Database_Design_%3A_The_Coming_of_the_Shard.html">672 high scalability-2009-08-06-An Unorthodox Approach to Database Design : The Coming of the Shard</a></p>
<p>12 0.14096756 <a title="514-tfidf-12" href="../high_scalability-2012/high_scalability-2012-03-07-Scale_Indefinitely_on_S3_With_These_Secrets_of_the_S3_Masters.html">1205 high scalability-2012-03-07-Scale Indefinitely on S3 With These Secrets of the S3 Masters</a></p>
<p>13 0.13354024 <a title="514-tfidf-13" href="../high_scalability-2014/high_scalability-2014-03-12-Paper%3A_Scalable_Eventually_Consistent_Counters_over_Unreliable_Networks.html">1611 high scalability-2014-03-12-Paper: Scalable Eventually Consistent Counters over Unreliable Networks</a></p>
<p>14 0.13111539 <a title="514-tfidf-14" href="../high_scalability-2013/high_scalability-2013-04-15-Scaling_Pinterest_-_From_0_to_10s_of_Billions_of_Page_Views_a_Month_in_Two_Years.html">1440 high scalability-2013-04-15-Scaling Pinterest - From 0 to 10s of Billions of Page Views a Month in Two Years</a></p>
<p>15 0.13019846 <a title="514-tfidf-15" href="../high_scalability-2013/high_scalability-2013-09-09-Need_Help_with_Database_Scalability%3F_Understand_I-O.html">1514 high scalability-2013-09-09-Need Help with Database Scalability? Understand I-O</a></p>
<p>16 0.12925465 <a title="514-tfidf-16" href="../high_scalability-2012/high_scalability-2012-10-22-Spanner_-_It%27s_About_Programmers_Building_Apps_Using_SQL_Semantics_at_NoSQL_Scale.html">1345 high scalability-2012-10-22-Spanner - It's About Programmers Building Apps Using SQL Semantics at NoSQL Scale</a></p>
<p>17 0.12778534 <a title="514-tfidf-17" href="../high_scalability-2007/high_scalability-2007-11-13-Flickr_Architecture.html">152 high scalability-2007-11-13-Flickr Architecture</a></p>
<p>18 0.1224717 <a title="514-tfidf-18" href="../high_scalability-2008/high_scalability-2008-04-08-Google_AppEngine_-_A_First_Look.html">301 high scalability-2008-04-08-Google AppEngine - A First Look</a></p>
<p>19 0.11965584 <a title="514-tfidf-19" href="../high_scalability-2013/high_scalability-2013-06-13-Busting_4_Modern_Hardware_Myths_-_Are_Memory%2C_HDDs%2C_and_SSDs_Really_Random_Access%3F.html">1475 high scalability-2013-06-13-Busting 4 Modern Hardware Myths - Are Memory, HDDs, and SSDs Really Random Access?</a></p>
<p>20 0.11655425 <a title="514-tfidf-20" href="../high_scalability-2008/high_scalability-2008-12-28-How_to_Organize_a_Database_Table%E2%80%99s_Keys_for_Scalability.html">476 high scalability-2008-12-28-How to Organize a Database Table’s Keys for Scalability</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.194), (1, 0.147), (2, -0.037), (3, -0.037), (4, 0.001), (5, 0.099), (6, 0.023), (7, 0.039), (8, 0.007), (9, -0.078), (10, 0.001), (11, -0.003), (12, -0.079), (13, 0.083), (14, 0.037), (15, -0.009), (16, -0.133), (17, -0.032), (18, 0.067), (19, -0.004), (20, 0.019), (21, -0.015), (22, 0.033), (23, -0.003), (24, -0.024), (25, -0.036), (26, 0.065), (27, -0.008), (28, 0.026), (29, -0.012), (30, 0.051), (31, -0.113), (32, 0.053), (33, 0.031), (34, -0.04), (35, -0.009), (36, 0.026), (37, 0.014), (38, 0.055), (39, -0.046), (40, -0.064), (41, 0.1), (42, -0.028), (43, 0.004), (44, 0.06), (45, 0.017), (46, -0.007), (47, 0.013), (48, 0.033), (49, 0.051)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9661808 <a title="514-lsi-1" href="../high_scalability-2009/high_scalability-2009-02-18-Numbers_Everyone_Should_Know.html">514 high scalability-2009-02-18-Numbers Everyone Should Know</a></p>
<p>Introduction: Google AppEngine Numbers  
This group of numbers is from Brett Slatkin in  Building Scalable Web Apps with Google App Engine .
  Writes are expensive!   Datastore is transactional: writes require disk access   Disk access means disk seeks   Rule of thumb: 10ms for a disk seek   Simple math: 1s / 10ms = 100 seeks/sec maximum   Depends on: * The size and shape of your data * Doing work in batches (batch puts and gets) 
 Reads are cheap! 
   Reads do not need to be transactional, just consistent   Data is read from disk once, then it's easily cached   All subsequent reads come straight from memory   Rule of thumb: 250usec for 1MB of data from memory   Simple math: 1s / 250usec = 4GB/sec maximum * For a 1MB entity, that's 4000 fetches/sec 
 Numbers Miscellaneous 
This group of numbers is from a presentation  Jeff Dean  gave at a Engineering All-Hands Meeting at Google.      L1 cache reference 0.5 ns    Branch mispredict 5 ns    L2 cache reference 7 ns    Mutex lock/unlock 100 ns    Main me</p><p>2 0.82792932 <a title="514-lsi-2" href="../high_scalability-2008/high_scalability-2008-05-27-How_I_Learned_to_Stop_Worrying_and_Love_Using_a_Lot_of_Disk_Space_to_Scale.html">327 high scalability-2008-05-27-How I Learned to Stop Worrying and Love Using a Lot of Disk Space to Scale</a></p>
<p>Introduction: Update 3 : ReadWriteWeb says  Google App Engine Announces New Pricing Plans, APIs, Open Access . Pricing is specified but I'm not sure what to make of it yet. An image manipulation library is added (thus the need to pay for more CPU :-) and memcached support has been added. Memcached will help resolve the can't write for every read problem that pops up when keeping counters.  Update 2 : onGWT.com threw a GAE load party and a lot of people came. The results at  Load test : Google App Engine = 1, Community = 0 . GAE handled a peak of 35 requests/second and a sustained 10 requests/second. Some think performance was good, others not so good. My GMT watch broke and I was late to arrive. Maybe next time. Also added a few new design rules from the post.   Update : Added a few new rules gleaned from the  GAE Meetup : Design By Explicit Cost Model and Puts are Precious.  How do you structure your database using a distributed hash table like  BigTable ? The answer isn't what you might expect. If</p><p>3 0.72585726 <a title="514-lsi-3" href="../high_scalability-2011/high_scalability-2011-10-31-15_Ways_to_Make_Your_Application_Feel_More_Responsive_under_Google_App_Engine.html">1135 high scalability-2011-10-31-15 Ways to Make Your Application Feel More Responsive under Google App Engine</a></p>
<p>Introduction: Small Imrovements , makers of a hosted, lightweight feedback platform, have written an excellent article on  Performance issues on GAE, and how we resolved them . They show how they trimmed most of their requests to between 300ms and 800ms, some still take 2 seconds when memcache is stale, and others clock in at 150ms. Not  zippy  overall, but acceptable, especially if you really like GAE's PaaS promise.
 
What's tricky with PaaS is if your performance is poor, there's often not a lot you can do about it. But the folks at Small Improvements have been clever and diligent, giving many specific details and timings. Though their advice is specifically for GAE, it will apply to a lot of different situations as well.
 
Here are the 15 ways they made small performance improvements: 
  
  Understand App Engine has bad days . App Engine can have bad days where performance can degrade. Your design needs to take this potential for high latency variability into account. Don't always assume the bes</p><p>4 0.70633137 <a title="514-lsi-4" href="../high_scalability-2009/high_scalability-2009-04-08-N%2B1%2Bcaching_is_ok%3F.html">561 high scalability-2009-04-08-N+1+caching is ok?</a></p>
<p>Introduction: Hibernate and iBATIS and other similar tools have documentation with recommendations for avoiding the "N+1 select" problem.  The problem being that if you wanted to retrieve a set of widgets from a table, one query would be used to to retrieve all the ids of the matching widgets (select widget_id from widget where ...) and then for each id, another select is used to retrieve the details of that widget (select * from widget where widget_id = ?).  If you have 100 widgets, it requires 101 queries to get the details of them all.     I can see why this is bad, but what if you're doing entity caching?  i.e. If you run the first query to get your list of ids, and then for each widget you retrive it from the cache.  Surely in that case, N+1(+caching) is good?  Assuming of course that there is a high probability of all of the matching entities being in the cache.     I may be asking a daft question here - one whose answer is obviously implied by the large scalable mechanisms for storing data th</p><p>5 0.70175612 <a title="514-lsi-5" href="../high_scalability-2011/high_scalability-2011-01-11-Google_Megastore_-_3_Billion_Writes_and_20_Billion_Read_Transactions_Daily.html">972 high scalability-2011-01-11-Google Megastore - 3 Billion Writes and 20 Billion Read Transactions Daily</a></p>
<p>Introduction: A giant step into the fully distributed future has been taken by the Google App Engine team with the release of their  High Replication Datastore . The HRD is targeted at mission critical applications that require data replicated to at least three datacenters, full ACID semantics for  entity groups , and lower consistency guarantees across entity groups.
 
This is a major accomplishment. Few organizations can implement a true multi-datacenter datastore. Other than SimpleDB, how many other publicly accessible database services can operate out of multiple datacenters? Now that capability can be had by anyone. But there is a price, literally and otherwise. Because the HRD uses three times the resources as Google App Engine's Master/Slave datastatore, it will cost three times as much. And because it is a distributed database, with all that implies in the CAP sense, developers will have to be very careful in how they architect their applications because as costs increased, reliability incre</p><p>6 0.69748271 <a title="514-lsi-6" href="../high_scalability-2012/high_scalability-2012-03-07-Scale_Indefinitely_on_S3_With_These_Secrets_of_the_S3_Masters.html">1205 high scalability-2012-03-07-Scale Indefinitely on S3 With These Secrets of the S3 Masters</a></p>
<p>7 0.68684727 <a title="514-lsi-7" href="../high_scalability-2009/high_scalability-2009-02-21-Google_AppEngine_-_A_Second_Look.html">517 high scalability-2009-02-21-Google AppEngine - A Second Look</a></p>
<p>8 0.68422943 <a title="514-lsi-8" href="../high_scalability-2011/high_scalability-2011-01-26-Google_Pro_Tip%3A_Use_Back-of-the-envelope-calculations_to_Choose_the_Best_Design.html">978 high scalability-2011-01-26-Google Pro Tip: Use Back-of-the-envelope-calculations to Choose the Best Design</a></p>
<p>9 0.68225294 <a title="514-lsi-9" href="../high_scalability-2011/high_scalability-2011-08-10-LevelDB_-_Fast_and_Lightweight_Key-Value_Database_From_the_Authors_of_MapReduce_and_BigTable.html">1096 high scalability-2011-08-10-LevelDB - Fast and Lightweight Key-Value Database From the Authors of MapReduce and BigTable</a></p>
<p>10 0.68021345 <a title="514-lsi-10" href="../high_scalability-2012/high_scalability-2012-04-05-Big_Data_Counting%3A_How_to_count_a_billion_distinct_objects_using_only_1.5KB_of_Memory.html">1222 high scalability-2012-04-05-Big Data Counting: How to count a billion distinct objects using only 1.5KB of Memory</a></p>
<p>11 0.67504179 <a title="514-lsi-11" href="../high_scalability-2012/high_scalability-2012-10-22-Spanner_-_It%27s_About_Programmers_Building_Apps_Using_SQL_Semantics_at_NoSQL_Scale.html">1345 high scalability-2012-10-22-Spanner - It's About Programmers Building Apps Using SQL Semantics at NoSQL Scale</a></p>
<p>12 0.67126781 <a title="514-lsi-12" href="../high_scalability-2010/high_scalability-2010-05-17-7_Lessons_Learned_While_Building_Reddit_to_270_Million_Page_Views_a_Month.html">828 high scalability-2010-05-17-7 Lessons Learned While Building Reddit to 270 Million Page Views a Month</a></p>
<p>13 0.6690045 <a title="514-lsi-13" href="../high_scalability-2013/high_scalability-2013-09-09-Need_Help_with_Database_Scalability%3F_Understand_I-O.html">1514 high scalability-2013-09-09-Need Help with Database Scalability? Understand I-O</a></p>
<p>14 0.66439271 <a title="514-lsi-14" href="../high_scalability-2013/high_scalability-2013-06-13-Busting_4_Modern_Hardware_Myths_-_Are_Memory%2C_HDDs%2C_and_SSDs_Really_Random_Access%3F.html">1475 high scalability-2013-06-13-Busting 4 Modern Hardware Myths - Are Memory, HDDs, and SSDs Really Random Access?</a></p>
<p>15 0.66246361 <a title="514-lsi-15" href="../high_scalability-2008/high_scalability-2008-07-16-The_Mother_of_All_Database_Normalization_Debates_on_Coding_Horror.html">351 high scalability-2008-07-16-The Mother of All Database Normalization Debates on Coding Horror</a></p>
<p>16 0.66039723 <a title="514-lsi-16" href="../high_scalability-2009/high_scalability-2009-08-24-How_Google_Serves_Data_from_Multiple_Datacenters.html">687 high scalability-2009-08-24-How Google Serves Data from Multiple Datacenters</a></p>
<p>17 0.66003627 <a title="514-lsi-17" href="../high_scalability-2008/high_scalability-2008-03-18-Database_Design_101.html">281 high scalability-2008-03-18-Database Design 101</a></p>
<p>18 0.65943813 <a title="514-lsi-18" href="../high_scalability-2010/high_scalability-2010-01-22-How_BuddyPoke_Scales_on_Facebook_Using_Google_App_Engine.html">763 high scalability-2010-01-22-How BuddyPoke Scales on Facebook Using Google App Engine</a></p>
<p>19 0.65600485 <a title="514-lsi-19" href="../high_scalability-2013/high_scalability-2013-06-07-Stuff_The_Internet_Says_On_Scalability_For_June_7%2C_2013.html">1472 high scalability-2013-06-07-Stuff The Internet Says On Scalability For June 7, 2013</a></p>
<p>20 0.64673549 <a title="514-lsi-20" href="../high_scalability-2009/high_scalability-2009-08-06-An_Unorthodox_Approach_to_Database_Design_%3A_The_Coming_of_the_Shard.html">672 high scalability-2009-08-06-An Unorthodox Approach to Database Design : The Coming of the Shard</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.129), (2, 0.199), (10, 0.056), (27, 0.015), (40, 0.039), (51, 0.011), (52, 0.014), (61, 0.056), (76, 0.043), (77, 0.034), (79, 0.105), (85, 0.048), (86, 0.084), (94, 0.069)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9527154 <a title="514-lda-1" href="../high_scalability-2009/high_scalability-2009-02-18-Numbers_Everyone_Should_Know.html">514 high scalability-2009-02-18-Numbers Everyone Should Know</a></p>
<p>Introduction: Google AppEngine Numbers  
This group of numbers is from Brett Slatkin in  Building Scalable Web Apps with Google App Engine .
  Writes are expensive!   Datastore is transactional: writes require disk access   Disk access means disk seeks   Rule of thumb: 10ms for a disk seek   Simple math: 1s / 10ms = 100 seeks/sec maximum   Depends on: * The size and shape of your data * Doing work in batches (batch puts and gets) 
 Reads are cheap! 
   Reads do not need to be transactional, just consistent   Data is read from disk once, then it's easily cached   All subsequent reads come straight from memory   Rule of thumb: 250usec for 1MB of data from memory   Simple math: 1s / 250usec = 4GB/sec maximum * For a 1MB entity, that's 4000 fetches/sec 
 Numbers Miscellaneous 
This group of numbers is from a presentation  Jeff Dean  gave at a Engineering All-Hands Meeting at Google.      L1 cache reference 0.5 ns    Branch mispredict 5 ns    L2 cache reference 7 ns    Mutex lock/unlock 100 ns    Main me</p><p>2 0.93476778 <a title="514-lda-2" href="../high_scalability-2014/high_scalability-2014-04-04-Stuff_The_Internet_Says_On_Scalability_For_April_4th%2C_2014.html">1626 high scalability-2014-04-04-Stuff The Internet Says On Scalability For April 4th, 2014</a></p>
<p>Introduction: Hey, it's HighScalability time:
    The world ends not with a bang, but with  1 exaFLOP of bitcoin  whimpers.   
 Quotable Quotes:                               
 
  @EtienneRoy : Algorithm:  you must encode and leverage your ignorance, not only your knowledge #hadoopsummit - enthralling 
  Chris Brenny : A material is nothing without a process. While the constituent formulation imbues the final product with fundamental properties, the bridge between material and function has a dramatic effect on its perception and use. 
  @gallifreya n: Using AWS c1, m1, m2? @adrianco says don't. c3, m3, r3 are now better and cheaper. #cloudconnect #ccevent 
  @christianhern : Mobile banking in the UK: 1,800 transactions per MINUTE. A "seismic shift" that banks were unprepared for 
 
 
 
 While we are waiting for that epic article deeply comparing Google's Cloud with AWS, we have Adrian Cockcroft's highly hopped  slide comparing the two . Google: no enterprise customers, no reservation options, need m</p><p>3 0.93129957 <a title="514-lda-3" href="../high_scalability-2010/high_scalability-2010-03-05-Strategy%3A_Planning_for_a_Power_Outage_Google_Style.html">789 high scalability-2010-03-05-Strategy: Planning for a Power Outage Google Style</a></p>
<p>Introduction: We can all learn from problems. The Google App Engine team has created a teachable moment through a remarkably honest and forthcoming  post-mortem for February 24th, 2010 outage  post, chronicling in elaborate detail a power outage that took down Google App Engine for a few hours.
 
The world is ending! The cloud is unreliable! Jump ship! Not. This is not evidence that the cloud is a beautiful, powerful and unsinkable ship that goes down on its maiden voyage. Stuff happens, no matter how well you prepare. If you think private datacenters don't go down, well, then I have some rearangeable deck chairs to sell you. The goal is to keep improving and minimizing those failure windows. From that perspective there is a lot to learn from the problems the Google App Engine team encountered and how they plan to fix them.
 
Please read the article for all the juicy details, but here's what struck me as key:
 
   
  
  Power fails. Plan for it . This seems to happen with unexpected frequency for su</p><p>4 0.9311462 <a title="514-lda-4" href="../high_scalability-2011/high_scalability-2011-09-23-Stuff_The_Internet_Says_On_Scalability_For_September_23%2C_2011.html">1122 high scalability-2011-09-23-Stuff The Internet Says On Scalability For September 23, 2011</a></p>
<p>Introduction: I'd walk a mile for  HighScalability : 
  
  1/12th the World Population on Facebook in One Day ;  1.8 ZettaBytes  of data in 2011;  1 Billion Foursquare Checkins ;   2 million on Spotify ;  1 Million on GitHub ;  $1,279-per-hour, 30,000-core cluster built on EC2 ;  Patent trolls cost .5 trillion dollars ;  235 terabytes of data collected by the U.S. Library of Congress in April . 
 Potent quotables:                       
 
  @jstogdill  : Corporations over protect low value info assets (which screws up collaboration) and under protects high value assets. #strataconf 
  @sbtourist  : I think BigMemory-like approaches based on large put-and-forget memory cans, are rarely a solution to performance/scalability problems. 
 
 
  1 Million TCP Connections . Remember when 10K was a real limit and you had to build out boxes just to handle the load? Amazing. We don't know how much processing can be attached to these connections, how much memory the apps use, or what the response latency is to</p><p>5 0.92939001 <a title="514-lda-5" href="../high_scalability-2008/high_scalability-2008-08-24-A_Scalable%2C_Commodity_Data_Center_Network_Architecture.html">371 high scalability-2008-08-24-A Scalable, Commodity Data Center Network Architecture</a></p>
<p>Introduction: Looks interesting...  Abstract: Todayâ&euro;&trade;s data centers may contain tens of thousands of computers with significant aggregate bandwidth requirements. The network architecture typically consists of a tree of routing and switching elements with progressively more specialized and expensive equipment moving up the network hierarchy. Unfortunately, even when deploying the highest-end IP switches/routers, resulting topologies may only support 50% of the aggregate bandwidth available at the edge of the network, while still incurring tremendous cost. Nonuniform bandwidth among data center nodes complicates application design and limits overall system performance. In this paper, we show how to leverage largely commodity Ethernet switches to support the full aggregate bandwidth of clusters consisting of tens of thousands of elements. Similar to how clusters of commodity computers have largely replaced more specialized SMPs and MPPs, we argue that appropriately architected and interconnected commodi</p><p>6 0.92794323 <a title="514-lda-6" href="../high_scalability-2011/high_scalability-2011-01-26-Google_Pro_Tip%3A_Use_Back-of-the-envelope-calculations_to_Choose_the_Best_Design.html">978 high scalability-2011-01-26-Google Pro Tip: Use Back-of-the-envelope-calculations to Choose the Best Design</a></p>
<p>7 0.92690378 <a title="514-lda-7" href="../high_scalability-2009/high_scalability-2009-11-04-Damn%2C_Which_Database_do_I_Use_Now%3F.html">736 high scalability-2009-11-04-Damn, Which Database do I Use Now?</a></p>
<p>8 0.92604232 <a title="514-lda-8" href="../high_scalability-2014/high_scalability-2014-03-14-Stuff_The_Internet_Says_On_Scalability_For_March_14th%2C_2014.html">1612 high scalability-2014-03-14-Stuff The Internet Says On Scalability For March 14th, 2014</a></p>
<p>9 0.92597997 <a title="514-lda-9" href="../high_scalability-2013/high_scalability-2013-01-18-Stuff_The_Internet_Says_On_Scalability_For_January_18%2C_2013.html">1389 high scalability-2013-01-18-Stuff The Internet Says On Scalability For January 18, 2013</a></p>
<p>10 0.92579019 <a title="514-lda-10" href="../high_scalability-2014/high_scalability-2014-02-26-The_WhatsApp_Architecture_Facebook_Bought_For_%2419_Billion.html">1602 high scalability-2014-02-26-The WhatsApp Architecture Facebook Bought For $19 Billion</a></p>
<p>11 0.92410749 <a title="514-lda-11" href="../high_scalability-2014/high_scalability-2014-01-10-Stuff_The_Internet_Says_On_Scalability_For_January_10th%2C_2014.html">1576 high scalability-2014-01-10-Stuff The Internet Says On Scalability For January 10th, 2014</a></p>
<p>12 0.92402077 <a title="514-lda-12" href="../high_scalability-2009/high_scalability-2009-02-21-Google_AppEngine_-_A_Second_Look.html">517 high scalability-2009-02-21-Google AppEngine - A Second Look</a></p>
<p>13 0.92329127 <a title="514-lda-13" href="../high_scalability-2011/high_scalability-2011-09-16-Stuff_The_Internet_Says_On_Scalability_For_September_16%2C_2011.html">1117 high scalability-2011-09-16-Stuff The Internet Says On Scalability For September 16, 2011</a></p>
<p>14 0.92320895 <a title="514-lda-14" href="../high_scalability-2013/high_scalability-2013-03-29-Stuff_The_Internet_Says_On_Scalability_For_March_29%2C_2013.html">1431 high scalability-2013-03-29-Stuff The Internet Says On Scalability For March 29, 2013</a></p>
<p>15 0.92299557 <a title="514-lda-15" href="../high_scalability-2012/high_scalability-2012-01-23-Facebook_Timeline%3A_Brought_to_You_by_the_Power_of_Denormalization.html">1179 high scalability-2012-01-23-Facebook Timeline: Brought to You by the Power of Denormalization</a></p>
<p>16 0.92295891 <a title="514-lda-16" href="../high_scalability-2014/high_scalability-2014-02-21-Stuff_The_Internet_Says_On_Scalability_For_February_21st%2C_2014.html">1600 high scalability-2014-02-21-Stuff The Internet Says On Scalability For February 21st, 2014</a></p>
<p>17 0.92295641 <a title="514-lda-17" href="../high_scalability-2009/high_scalability-2009-10-06-Building_a_Unique_Data_Warehouse.html">716 high scalability-2009-10-06-Building a Unique Data Warehouse</a></p>
<p>18 0.92271876 <a title="514-lda-18" href="../high_scalability-2009/high_scalability-2009-01-20-Product%3A_Amazon%27s_SimpleDB.html">498 high scalability-2009-01-20-Product: Amazon's SimpleDB</a></p>
<p>19 0.92263597 <a title="514-lda-19" href="../high_scalability-2013/high_scalability-2013-01-07-Analyzing_billions_of_credit_card_transactions_and_serving_low-latency_insights_in_the_cloud.html">1382 high scalability-2013-01-07-Analyzing billions of credit card transactions and serving low-latency insights in the cloud</a></p>
<p>20 0.92240512 <a title="514-lda-20" href="../high_scalability-2012/high_scalability-2012-08-03-Stuff_The_Internet_Says_On_Scalability_For_August_3%2C_2012.html">1297 high scalability-2012-08-03-Stuff The Internet Says On Scalability For August 3, 2012</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
