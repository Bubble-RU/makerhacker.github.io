<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>581 high scalability-2009-04-26-Map-Reduce for Machine Learning on Multicore</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2009" href="../home/high_scalability-2009_home.html">high_scalability-2009</a> <a title="high_scalability-2009-581" href="#">high_scalability-2009-581</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>581 high scalability-2009-04-26-Map-Reduce for Machine Learning on Multicore</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2009-581-html" href="http://highscalability.com//blog/2009/4/27/map-reduce-for-machine-learning-on-multicore.html">html</a></p><p>Introduction: We are at the beginning of the multicore era. Computers will have increasingly
many cores (processors), but there is still no good programming framework for
these architectures, and thus no simple and unified way for machine learning
to take advantage of the potential speed up.In this paper, we develop a
broadly applicable parallel programming method, one that is easily applied to
many different learning algorithms. Our work is in distinct contrast to the
tradition in machine learning of designing (often ingenious) ways to speed up
a single algorithm at a time.Specifically, we show that algorithms that fit
the Statistical Query model can be written in a certain “summation form,”
which allows them to be easily parallelized on multicore computers. We adapt
Google’s map-reduce paradigm to demonstrate this parallel speed up technique
on a variety of learning algorithms including locally weighted linear
regression (LWLR), k-means, logistic regression (LR), naive Bayes (NB), SVM,
ICA, PCA, g</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Computers will have increasingly many cores (processors), but there is still no good programming framework for these architectures, and thus no simple and unified way for machine learning to take advantage of the potential speed up. [sent-2, score-1.065]
</p><p>2 In this paper, we develop a broadly applicable parallel programming method, one that is easily applied to many different learning algorithms. [sent-3, score-0.957]
</p><p>3 Our work is in distinct contrast to the tradition in machine learning of designing (often ingenious) ways to speed up a single algorithm at a time. [sent-4, score-1.054]
</p><p>4 Specifically, we show that algorithms that fit the Statistical Query model can be written in a certain “summation form,” which allows them to be easily parallelized on multicore computers. [sent-5, score-0.72]
</p><p>5 Our experimental results show basically linear speedup with an increasing number of processors. [sent-7, score-0.721]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('regression', 0.294), ('learning', 0.267), ('multicore', 0.229), ('em', 0.194), ('gaussian', 0.194), ('ingenious', 0.194), ('nn', 0.194), ('linear', 0.189), ('bayes', 0.182), ('weighted', 0.158), ('broadly', 0.158), ('speed', 0.157), ('tradition', 0.154), ('pdf', 0.138), ('algorithms', 0.132), ('applicable', 0.127), ('speedup', 0.127), ('naive', 0.126), ('show', 0.125), ('parallelized', 0.123), ('statistical', 0.122), ('unified', 0.12), ('experimental', 0.12), ('adapt', 0.116), ('distinct', 0.115), ('contrast', 0.113), ('easily', 0.111), ('demonstrate', 0.11), ('parallel', 0.109), ('paradigm', 0.104), ('locally', 0.103), ('programming', 0.102), ('beginning', 0.101), ('increasingly', 0.1), ('technique', 0.098), ('machine', 0.092), ('study', 0.092), ('method', 0.092), ('basically', 0.089), ('processors', 0.085), ('computers', 0.085), ('variety', 0.084), ('applied', 0.083), ('algorithm', 0.081), ('thus', 0.079), ('designing', 0.075), ('potential', 0.074), ('cores', 0.074), ('download', 0.072), ('increasing', 0.071)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="581-tfidf-1" href="../high_scalability-2009/high_scalability-2009-04-26-Map-Reduce_for_Machine_Learning_on_Multicore.html">581 high scalability-2009-04-26-Map-Reduce for Machine Learning on Multicore</a></p>
<p>Introduction: We are at the beginning of the multicore era. Computers will have increasingly
many cores (processors), but there is still no good programming framework for
these architectures, and thus no simple and unified way for machine learning
to take advantage of the potential speed up.In this paper, we develop a
broadly applicable parallel programming method, one that is easily applied to
many different learning algorithms. Our work is in distinct contrast to the
tradition in machine learning of designing (often ingenious) ways to speed up
a single algorithm at a time.Specifically, we show that algorithms that fit
the Statistical Query model can be written in a certain “summation form,”
which allows them to be easily parallelized on multicore computers. We adapt
Google’s map-reduce paradigm to demonstrate this parallel speed up technique
on a variety of learning algorithms including locally weighted linear
regression (LWLR), k-means, logistic regression (LR), naive Bayes (NB), SVM,
ICA, PCA, g</p><p>2 0.1336063 <a title="581-tfidf-2" href="../high_scalability-2009/high_scalability-2009-05-31-Parallel_Programming_for_real-world.html">612 high scalability-2009-05-31-Parallel Programming for real-world</a></p>
<p>Introduction: Multicore computers shift the burden of software performance from chip
designers and architects to software developers.What is the parallel Computing
? and what the different between Multi-Threading and Concurrency and
Parallelism ? and what is differences between task and data parallel ? and how
we can use it ?Fundamental article into Parallel Programming...</p><p>3 0.10444107 <a title="581-tfidf-3" href="../high_scalability-2010/high_scalability-2010-06-30-Paper%3A_GraphLab%3A_A_New_Framework_For_Parallel_Machine_Learning.html">850 high scalability-2010-06-30-Paper: GraphLab: A New Framework For Parallel Machine Learning</a></p>
<p>Introduction: In the never ending quest to figure out how to do something useful with never
ending streams of data, GraphLab: A New Framework For Parallel Machine
Learning wants to go beyond low-level programming, MapReduce, and dataflow
languages with a new parallel framework for ML (machine learning) which
exploits the sparse structure and common computational patterns of ML
algorithms. GraphLab enables ML experts to easily design and implement
efﬁcient scalable parallel algorithms by composing problem speciﬁc
computation, data-dependencies, and scheduling.  Our main contributions
include: A graph-based data model which simultaneously represents data and
computational dependencies. A set of concurrent access models which provide a
range of sequential-consistency guarantees. A sophisticated modular scheduling
mechanism. An aggregation framework to manage global state. From the
abstract:Designing and implementing efﬁcient, provably correct parallel
machine learning (ML) algorithms is challenging. Ex</p><p>4 0.10432486 <a title="581-tfidf-4" href="../high_scalability-2009/high_scalability-2009-03-12-Google_TechTalk%3A_Amdahl%27s_Law_in_the_Multicore_Era.html">534 high scalability-2009-03-12-Google TechTalk: Amdahl's Law in the Multicore Era</a></p>
<p>Introduction: Over the last several decades computer architects have been phenomenally
successful turning the transistor bounty provided by Moore's Law into chips
with ever increasing single-threaded performance. During many of these
successful years, however, many researchers paid scant attention to
multiprocessor work. Now as vendors turn to multicore chips, researchers are
reacting with more papers on multi-threaded systems. While this is good, we
are concerned that further work on single-thread performance will be
squashed.To help understand future high-level trade-offs, we develop a
corollary to Amdahl's Law for multicore chips [Hill & Marty, IEEE Computer
2008]. It models fixed chip resources for alternative designs that use
symmetric cores, asymmetric cores, or dynamic techniques that allow cores to
work together on sequential execution. Our results encourage multicore
designers to view performance of the entire chip rather than focus on core
efficiencies. Moreover, we observe that obtaining</p><p>5 0.095715918 <a title="581-tfidf-5" href="../high_scalability-2009/high_scalability-2009-06-23-Learn_How_to_Exploit_Multiple_Cores_for_Better_Performance_and_Scalability.html">636 high scalability-2009-06-23-Learn How to Exploit Multiple Cores for Better Performance and Scalability</a></p>
<p>Introduction: InfoQueue has thisexcellent talk by Brian Goetzon the new features being added
to Java SE 7 that will allow programmers to fully exploit our massively multi-
processor future. While the talk is about Java it's really more general than
that and there's a lot to learn here for everyone.Brian starts with a short,
coherent, and compelling explanation of why programmers can't expect to be
saved by ever faster CPUs and why we must learn to exploit the strengths of
multiple core computers to make our software go faster.Some techniques for
exploiting multiple cores are given in an equally short, coherent, and
compelling explanation of why divide and conquer as the secret to multi-core
bliss, fork-join, how the Java approach differs from map-reduce, and lots of
other juicy topics.The multi-core "problem" is only going to get worse. Tilera
founder Anant Agarwalestimates by 2017embedded processors could have 4,096
cores, server CPUs might have 512 cores and desktop chips could use 128 cores.
Some</p><p>6 0.095517509 <a title="581-tfidf-6" href="../high_scalability-2009/high_scalability-2009-09-19-Space_Based_Programming_in_.NET.html">709 high scalability-2009-09-19-Space Based Programming in .NET</a></p>
<p>7 0.08635857 <a title="581-tfidf-7" href="../high_scalability-2014/high_scalability-2014-02-21-Stuff_The_Internet_Says_On_Scalability_For_February_21st%2C_2014.html">1600 high scalability-2014-02-21-Stuff The Internet Says On Scalability For February 21st, 2014</a></p>
<p>8 0.081837624 <a title="581-tfidf-8" href="../high_scalability-2012/high_scalability-2012-03-06-Ask_For_Forgiveness_Programming_-_Or_How_We%27ll_Program_1000_Cores.html">1204 high scalability-2012-03-06-Ask For Forgiveness Programming - Or How We'll Program 1000 Cores</a></p>
<p>9 0.080365539 <a title="581-tfidf-9" href="../high_scalability-2013/high_scalability-2013-08-16-Stuff_The_Internet_Says_On_Scalability_For_August_16%2C_2013.html">1502 high scalability-2013-08-16-Stuff The Internet Says On Scalability For August 16, 2013</a></p>
<p>10 0.078961812 <a title="581-tfidf-10" href="../high_scalability-2014/high_scalability-2014-02-19-Planetary-Scale_Computing_Architectures_for_Electronic_Trading_and_How_Algorithms_Shape_Our_World.html">1599 high scalability-2014-02-19-Planetary-Scale Computing Architectures for Electronic Trading and How Algorithms Shape Our World</a></p>
<p>11 0.078540079 <a title="581-tfidf-11" href="../high_scalability-2012/high_scalability-2012-07-30-Prismatic_Architecture_-_Using_Machine_Learning_on_Social_Networks_to_Figure_Out_What_You_Should_Read_on_the_Web_.html">1293 high scalability-2012-07-30-Prismatic Architecture - Using Machine Learning on Social Networks to Figure Out What You Should Read on the Web </a></p>
<p>12 0.076403961 <a title="581-tfidf-12" href="../high_scalability-2009/high_scalability-2009-04-26-Scale-up_vs._Scale-out%3A_A_Case_Study_by_IBM_using_Nutch-Lucene.html">583 high scalability-2009-04-26-Scale-up vs. Scale-out: A Case Study by IBM using Nutch-Lucene</a></p>
<p>13 0.074852809 <a title="581-tfidf-13" href="../high_scalability-2013/high_scalability-2013-08-30-Stuff_The_Internet_Says_On_Scalability_For_August_30%2C_2013.html">1509 high scalability-2013-08-30-Stuff The Internet Says On Scalability For August 30, 2013</a></p>
<p>14 0.073205829 <a title="581-tfidf-14" href="../high_scalability-2009/high_scalability-2009-04-13-High_Performance_Web_Pages_%E2%80%93_Real_World_Examples%3A_Netflix_Case_Study.html">566 high scalability-2009-04-13-High Performance Web Pages – Real World Examples: Netflix Case Study</a></p>
<p>15 0.071460031 <a title="581-tfidf-15" href="../high_scalability-2012/high_scalability-2012-06-22-Stuff_The_Internet_Says_On_Scalability_For_June_22%2C_2012.html">1270 high scalability-2012-06-22-Stuff The Internet Says On Scalability For June 22, 2012</a></p>
<p>16 0.070388846 <a title="581-tfidf-16" href="../high_scalability-2010/high_scalability-2010-03-30-Running_Large_Graph_Algorithms_-_Evaluation_of_Current_State-of-the-Art_and_Lessons_Learned.html">801 high scalability-2010-03-30-Running Large Graph Algorithms - Evaluation of Current State-of-the-Art and Lessons Learned</a></p>
<p>17 0.070216589 <a title="581-tfidf-17" href="../high_scalability-2012/high_scalability-2012-03-02-Stuff_The_Internet_Says_On_Scalability_For_March_2%2C_2012.html">1203 high scalability-2012-03-02-Stuff The Internet Says On Scalability For March 2, 2012</a></p>
<p>18 0.067950174 <a title="581-tfidf-18" href="../high_scalability-2014/high_scalability-2014-01-10-Stuff_The_Internet_Says_On_Scalability_For_January_10th%2C_2014.html">1576 high scalability-2014-01-10-Stuff The Internet Says On Scalability For January 10th, 2014</a></p>
<p>19 0.067183651 <a title="581-tfidf-19" href="../high_scalability-2013/high_scalability-2013-12-06-Stuff_The_Internet_Says_On_Scalability_For_December_6th%2C_2013.html">1559 high scalability-2013-12-06-Stuff The Internet Says On Scalability For December 6th, 2013</a></p>
<p>20 0.066837579 <a title="581-tfidf-20" href="../high_scalability-2013/high_scalability-2013-09-20-Stuff_The_Internet_Says_On_Scalability_For_September_20%2C_2013.html">1520 high scalability-2013-09-20-Stuff The Internet Says On Scalability For September 20, 2013</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.098), (1, 0.053), (2, 0.017), (3, 0.071), (4, -0.028), (5, 0.047), (6, 0.012), (7, 0.06), (8, -0.052), (9, 0.061), (10, 0.015), (11, -0.031), (12, 0.003), (13, 0.005), (14, 0.016), (15, -0.063), (16, 0.009), (17, -0.018), (18, 0.035), (19, 0.005), (20, 0.01), (21, -0.012), (22, -0.068), (23, 0.001), (24, -0.015), (25, 0.007), (26, -0.045), (27, -0.028), (28, 0.038), (29, 0.022), (30, 0.023), (31, 0.044), (32, -0.036), (33, 0.013), (34, -0.014), (35, -0.096), (36, 0.092), (37, -0.019), (38, 0.03), (39, -0.006), (40, -0.045), (41, 0.025), (42, -0.03), (43, -0.053), (44, -0.003), (45, 0.016), (46, -0.018), (47, -0.014), (48, 0.03), (49, -0.017)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98107851 <a title="581-lsi-1" href="../high_scalability-2009/high_scalability-2009-04-26-Map-Reduce_for_Machine_Learning_on_Multicore.html">581 high scalability-2009-04-26-Map-Reduce for Machine Learning on Multicore</a></p>
<p>Introduction: We are at the beginning of the multicore era. Computers will have increasingly
many cores (processors), but there is still no good programming framework for
these architectures, and thus no simple and unified way for machine learning
to take advantage of the potential speed up.In this paper, we develop a
broadly applicable parallel programming method, one that is easily applied to
many different learning algorithms. Our work is in distinct contrast to the
tradition in machine learning of designing (often ingenious) ways to speed up
a single algorithm at a time.Specifically, we show that algorithms that fit
the Statistical Query model can be written in a certain “summation form,”
which allows them to be easily parallelized on multicore computers. We adapt
Google’s map-reduce paradigm to demonstrate this parallel speed up technique
on a variety of learning algorithms including locally weighted linear
regression (LWLR), k-means, logistic regression (LR), naive Bayes (NB), SVM,
ICA, PCA, g</p><p>2 0.80329317 <a title="581-lsi-2" href="../high_scalability-2009/high_scalability-2009-03-12-Google_TechTalk%3A_Amdahl%27s_Law_in_the_Multicore_Era.html">534 high scalability-2009-03-12-Google TechTalk: Amdahl's Law in the Multicore Era</a></p>
<p>Introduction: Over the last several decades computer architects have been phenomenally
successful turning the transistor bounty provided by Moore's Law into chips
with ever increasing single-threaded performance. During many of these
successful years, however, many researchers paid scant attention to
multiprocessor work. Now as vendors turn to multicore chips, researchers are
reacting with more papers on multi-threaded systems. While this is good, we
are concerned that further work on single-thread performance will be
squashed.To help understand future high-level trade-offs, we develop a
corollary to Amdahl's Law for multicore chips [Hill & Marty, IEEE Computer
2008]. It models fixed chip resources for alternative designs that use
symmetric cores, asymmetric cores, or dynamic techniques that allow cores to
work together on sequential execution. Our results encourage multicore
designers to view performance of the entire chip rather than focus on core
efficiencies. Moreover, we observe that obtaining</p><p>3 0.75330806 <a title="581-lsi-3" href="../high_scalability-2010/high_scalability-2010-06-30-Paper%3A_GraphLab%3A_A_New_Framework_For_Parallel_Machine_Learning.html">850 high scalability-2010-06-30-Paper: GraphLab: A New Framework For Parallel Machine Learning</a></p>
<p>Introduction: In the never ending quest to figure out how to do something useful with never
ending streams of data, GraphLab: A New Framework For Parallel Machine
Learning wants to go beyond low-level programming, MapReduce, and dataflow
languages with a new parallel framework for ML (machine learning) which
exploits the sparse structure and common computational patterns of ML
algorithms. GraphLab enables ML experts to easily design and implement
efﬁcient scalable parallel algorithms by composing problem speciﬁc
computation, data-dependencies, and scheduling.  Our main contributions
include: A graph-based data model which simultaneously represents data and
computational dependencies. A set of concurrent access models which provide a
range of sequential-consistency guarantees. A sophisticated modular scheduling
mechanism. An aggregation framework to manage global state. From the
abstract:Designing and implementing efﬁcient, provably correct parallel
machine learning (ML) algorithms is challenging. Ex</p><p>4 0.74668038 <a title="581-lsi-4" href="../high_scalability-2009/high_scalability-2009-05-31-Parallel_Programming_for_real-world.html">612 high scalability-2009-05-31-Parallel Programming for real-world</a></p>
<p>Introduction: Multicore computers shift the burden of software performance from chip
designers and architects to software developers.What is the parallel Computing
? and what the different between Multi-Threading and Concurrency and
Parallelism ? and what is differences between task and data parallel ? and how
we can use it ?Fundamental article into Parallel Programming...</p><p>5 0.74078345 <a title="581-lsi-5" href="../high_scalability-2009/high_scalability-2009-05-27-The_Future_of_the_Parallelism_and_its_Challenges.html">608 high scalability-2009-05-27-The Future of the Parallelism and its Challenges</a></p>
<p>Introduction: The Future of the Parallelism and its ChallengesResearch and education in
Parallel computing technologies is more important than ever. Here I present a
perspective on the past contributions, current status, and future direction of
the parallelism technologies.While machine power will grow impressively,
increased parallelism, rather than clock rate, will be driving force in
computing in the foreseeable future. This ongoing shift toward parallel
architectural paradigms is one of the greatest challenges for the
microprocessor and software industries. In 2005, Justin Ratter, chief
technology officer of Intel Corporation, said ‘We are at the cusp of a
transition to multicore, multithreaded architectures, and we still have not
demonstrated the ease of programming the move will require…’Key points:A
Little historyParallelism ChallengesUnder the hood, Parallelism
ChallengesSynchronization problemsCAS problemsThe future of the parallelism</p><p>6 0.71711296 <a title="581-lsi-6" href="../high_scalability-2014/high_scalability-2014-05-01-Paper%3A_Can_Programming_Be_Liberated_From_The_Von_Neumann_Style%3F_.html">1641 high scalability-2014-05-01-Paper: Can Programming Be Liberated From The Von Neumann Style? </a></p>
<p>7 0.69338727 <a title="581-lsi-7" href="../high_scalability-2009/high_scalability-2009-07-21-Paper%3A_Parallelizing_the_Web_Browser.html">660 high scalability-2009-07-21-Paper: Parallelizing the Web Browser</a></p>
<p>8 0.67055851 <a title="581-lsi-8" href="../high_scalability-2009/high_scalability-2009-05-06-Dyrad.html">591 high scalability-2009-05-06-Dyrad</a></p>
<p>9 0.66520947 <a title="581-lsi-9" href="../high_scalability-2010/high_scalability-2010-06-18-Paper%3A_The_Declarative_Imperative%3A_Experiences_and_Conjectures_in_Distributed_Logic.html">844 high scalability-2010-06-18-Paper: The Declarative Imperative: Experiences and Conjectures in Distributed Logic</a></p>
<p>10 0.65811646 <a title="581-lsi-10" href="../high_scalability-2010/high_scalability-2010-05-12-The_Rise_of_the_Virtual_Cellular_Machines.html">826 high scalability-2010-05-12-The Rise of the Virtual Cellular Machines</a></p>
<p>11 0.65772814 <a title="581-lsi-11" href="../high_scalability-2011/high_scalability-2011-09-28-Pursue_robust_indefinite_scalability_with_the_Movable_Feast_Machine.html">1127 high scalability-2011-09-28-Pursue robust indefinite scalability with the Movable Feast Machine</a></p>
<p>12 0.65670717 <a title="581-lsi-12" href="../high_scalability-2012/high_scalability-2012-08-16-Paper%3A_A_Provably_Correct_Scalable_Concurrent_Skip_List.html">1305 high scalability-2012-08-16-Paper: A Provably Correct Scalable Concurrent Skip List</a></p>
<p>13 0.64455968 <a title="581-lsi-13" href="../high_scalability-2009/high_scalability-2009-05-20-Paper%3A_Flux%3A_An_Adaptive_Partitioning_Operator_for_Continuous_Query_Systems.html">604 high scalability-2009-05-20-Paper: Flux: An Adaptive Partitioning Operator for Continuous Query Systems</a></p>
<p>14 0.64030927 <a title="581-lsi-14" href="../high_scalability-2009/high_scalability-2009-03-12-Paper%3A_Understanding_and_Designing_New_Server_Architectures_for_Emerging_Warehouse-Computing_Environments.html">535 high scalability-2009-03-12-Paper: Understanding and Designing New Server Architectures for Emerging Warehouse-Computing Environments</a></p>
<p>15 0.63498998 <a title="581-lsi-15" href="../high_scalability-2010/high_scalability-2010-06-09-Paper%3A_Propagation_Networks%3A_A_Flexible_and_Expressive_Substrate_for_Computation_.html">839 high scalability-2010-06-09-Paper: Propagation Networks: A Flexible and Expressive Substrate for Computation </a></p>
<p>16 0.63156998 <a title="581-lsi-16" href="../high_scalability-2014/high_scalability-2014-04-03-Leslie_Lamport_to_Programmers%3A_You%27re_Doing_it_Wrong.html">1625 high scalability-2014-04-03-Leslie Lamport to Programmers: You're Doing it Wrong</a></p>
<p>17 0.63148689 <a title="581-lsi-17" href="../high_scalability-2009/high_scalability-2009-06-23-Learn_How_to_Exploit_Multiple_Cores_for_Better_Performance_and_Scalability.html">636 high scalability-2009-06-23-Learn How to Exploit Multiple Cores for Better Performance and Scalability</a></p>
<p>18 0.63086468 <a title="581-lsi-18" href="../high_scalability-2009/high_scalability-2009-11-01-Squeeze_more_performance_from_Parallelism.html">735 high scalability-2009-11-01-Squeeze more performance from Parallelism</a></p>
<p>19 0.6107775 <a title="581-lsi-19" href="../high_scalability-2008/high_scalability-2008-10-04-Is_MapReduce_going_mainstream%3F.html">401 high scalability-2008-10-04-Is MapReduce going mainstream?</a></p>
<p>20 0.60978025 <a title="581-lsi-20" href="../high_scalability-2011/high_scalability-2011-02-02-Piccolo_-_Building_Distributed_Programs_that_are_11x_Faster_than_Hadoop.html">983 high scalability-2011-02-02-Piccolo - Building Distributed Programs that are 11x Faster than Hadoop</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.09), (2, 0.121), (10, 0.028), (40, 0.034), (61, 0.062), (74, 0.231), (79, 0.262), (94, 0.068)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.90641057 <a title="581-lda-1" href="../high_scalability-2012/high_scalability-2012-06-13-Why_My_Soap_Film_is_Better_than_Your_Hadoop_Cluster.html">1263 high scalability-2012-06-13-Why My Soap Film is Better than Your Hadoop Cluster</a></p>
<p>Introduction: The ever amazingslime moldis not the only way to solve complex compute
problems without performing calculations. There is another:soap film.
Unfortunately for soap film it isn't nearly as photogenic as slime mold, all
we get are boring looking pictures, but the underlying idea is still
fascinating and ten times less spooky.As a quick introduction we'll lean on
Long Ouyang, who has really straightforward  explanation of how soap film
works inApproaching P=NP: Can Soap Bubbles Solve The Steiner Tree Problem In
Polynomial.It's computers, so playing the role of the motivating graph problem
we have theSteiner tree problem, which Ouyang explains as:Find the minimum
spanning tree for a bunch of vertices, given that you can add additional
points.Soap helps solve this problem because:Soap, in water, acts a
surfactant, which decreases the surface tension in water.This acts to minimize
the surface energy of the liquid.This should minimize surface area (graph
weight), and solve the problem.Dip two</p><p>same-blog 2 0.90163106 <a title="581-lda-2" href="../high_scalability-2009/high_scalability-2009-04-26-Map-Reduce_for_Machine_Learning_on_Multicore.html">581 high scalability-2009-04-26-Map-Reduce for Machine Learning on Multicore</a></p>
<p>Introduction: We are at the beginning of the multicore era. Computers will have increasingly
many cores (processors), but there is still no good programming framework for
these architectures, and thus no simple and unified way for machine learning
to take advantage of the potential speed up.In this paper, we develop a
broadly applicable parallel programming method, one that is easily applied to
many different learning algorithms. Our work is in distinct contrast to the
tradition in machine learning of designing (often ingenious) ways to speed up
a single algorithm at a time.Specifically, we show that algorithms that fit
the Statistical Query model can be written in a certain “summation form,”
which allows them to be easily parallelized on multicore computers. We adapt
Google’s map-reduce paradigm to demonstrate this parallel speed up technique
on a variety of learning algorithms including locally weighted linear
regression (LWLR), k-means, logistic regression (LR), naive Bayes (NB), SVM,
ICA, PCA, g</p><p>3 0.81320769 <a title="581-lda-3" href="../high_scalability-2010/high_scalability-2010-09-08-4_General_Core_Scalability_Patterns.html">897 high scalability-2010-09-08-4 General Core Scalability Patterns</a></p>
<p>Introduction: Jesper Soderlund put together an excellent list of four general scalability
patterns and four subpatterns in his postScalability patterns and an
interesting story:Load distribution- Spread the system load across multiple
processing unitsLoad balancing / load sharing- Spreading the load across many
components with equal properties for handling the requestPartitioning-
Spreading the load across many components by routing an individual request to
a component that owns that data specificVertical partitioning- Spreading the
load across the functional boundaries of a problem space, separate functions
being handled by different processing unitsHorizontal partitioning- Spreading
a single type of data element across many instances, according to some
partitioning key, e.g. hashing the player id and doing a modulus operation,
etc. Quite often referred to as sharding.Queuing and batch \- Achieve
efficiencies of scale by processing batches of data, usually because the
overhead of an operation is am</p><p>4 0.7905429 <a title="581-lda-4" href="../high_scalability-2010/high_scalability-2010-08-04-Dremel%3A_Interactive_Analysis_of_Web-Scale_Datasets_-_Data_as_a_Programming_Paradigm.html">871 high scalability-2010-08-04-Dremel: Interactive Analysis of Web-Scale Datasets - Data as a Programming Paradigm</a></p>
<p>Introduction: If Google was a boxer then MapReduce would be a probing right hand that sets
up the massive left hook that is Dremel, Google's--scalable (thousands of
CPUs, petabytes of data, trillions of rows), SQL based, columnar, interactive
(results returned in seconds), ad-hoc--analytics system. If Google was a
magician then MapReduce would be the shiny thing that distracts the mind while
the trick goes unnoticed. I say that because even though Dremel has been
around internally at Google since 2006, we have not heard a whisper about it.
All we've heard about is MapReduce, clones of which have inspired entire new
industries.Tricky.Dremel, according to Brian Bershad, Director of Engineering
at Google, is targeted at solvingBigData class problems:While we all know that
systems are huge and will get even huger, the implications of this size on
programmability, manageability, power, etc. is hard to comprehend. Alfred
noted that the Internet is predicted to be carrying a zetta-byte (1021bytes)
per year</p><p>5 0.78873199 <a title="581-lda-5" href="../high_scalability-2013/high_scalability-2013-03-08-Stuff_The_Internet_Says_On_Scalability_For_March_8%2C_2013.html">1420 high scalability-2013-03-08-Stuff The Internet Says On Scalability For March 8, 2013</a></p>
<p>Introduction: Hey, it's HighScalability time: Quotable Quotes:@ibogost: Disabling features
of SimCity due to ineffective central infrastructure is probably the most
realistic simulation of the modern city.antirez: The point is simply to show
how SSDs can't be considered, currently, as a bit slower version of memory.
Their performance characteristics are a lot more about, simply, "faster
disks".@jessenoller: I only use JavaScript so I can gain maximum scalability
across multiple cores. Also unicorns. Paint thinner gingerbread@liammclennan:
high-scalability ruby. Why bother?@scomma: Problem with BitCoin is not
scalability, not even usability. It's whether someone will crack the algorithm
and render BTC entirely useless.@webclimber: Amazing how often I find myself
explaining that scalability is not magical@mvmsan: Flash as Primary Storage -
Highest Cost, Lack of HA, scalability and management features #flash #SSD
#CIO@pneuman42: Game servers are the *worst* scalability problem. Most
services start smal</p><p>6 0.7874797 <a title="581-lda-6" href="../high_scalability-2013/high_scalability-2013-02-08-Stuff_The_Internet_Says_On_Scalability_For_February_8%2C_2013.html">1403 high scalability-2013-02-08-Stuff The Internet Says On Scalability For February 8, 2013</a></p>
<p>7 0.78508079 <a title="581-lda-7" href="../high_scalability-2010/high_scalability-2010-02-25-Paper%3A_High_Performance_Scalable_Data_Stores_.html">784 high scalability-2010-02-25-Paper: High Performance Scalable Data Stores </a></p>
<p>8 0.78172666 <a title="581-lda-8" href="../high_scalability-2009/high_scalability-2009-08-13-Reconnoiter_-_Large-Scale_Trending_and_Fault-Detection.html">680 high scalability-2009-08-13-Reconnoiter - Large-Scale Trending and Fault-Detection</a></p>
<p>9 0.78123629 <a title="581-lda-9" href="../high_scalability-2009/high_scalability-2009-04-13-High_Performance_Web_Pages_%E2%80%93_Real_World_Examples%3A_Netflix_Case_Study.html">566 high scalability-2009-04-13-High Performance Web Pages – Real World Examples: Netflix Case Study</a></p>
<p>10 0.78023875 <a title="581-lda-10" href="../high_scalability-2013/high_scalability-2013-07-19-Stuff_The_Internet_Says_On_Scalability_For_July_19%2C_2013.html">1494 high scalability-2013-07-19-Stuff The Internet Says On Scalability For July 19, 2013</a></p>
<p>11 0.77952033 <a title="581-lda-11" href="../high_scalability-2012/high_scalability-2012-01-05-Shutterfly_Saw_a_Speedup_of_500%25_With_Flashcache.html">1169 high scalability-2012-01-05-Shutterfly Saw a Speedup of 500% With Flashcache</a></p>
<p>12 0.77771473 <a title="581-lda-12" href="../high_scalability-2012/high_scalability-2012-07-05-10_Golden_Principles_For_Building_Successful_Mobile-Web_Applications.html">1277 high scalability-2012-07-05-10 Golden Principles For Building Successful Mobile-Web Applications</a></p>
<p>13 0.77663767 <a title="581-lda-13" href="../high_scalability-2012/high_scalability-2012-01-25-Google_Goes_MoreSQL_with_Tenzing_-__SQL_Over_MapReduce_.html">1181 high scalability-2012-01-25-Google Goes MoreSQL with Tenzing -  SQL Over MapReduce </a></p>
<p>14 0.77578771 <a title="581-lda-14" href="../high_scalability-2008/high_scalability-2008-11-22-Google_Architecture.html">448 high scalability-2008-11-22-Google Architecture</a></p>
<p>15 0.77562666 <a title="581-lda-15" href="../high_scalability-2011/high_scalability-2011-08-18-Paper%3A_The_Akamai_Network_-_61%2C000_servers%2C_1%2C000__networks%2C_70_countries_.html">1100 high scalability-2011-08-18-Paper: The Akamai Network - 61,000 servers, 1,000  networks, 70 countries </a></p>
<p>16 0.7738632 <a title="581-lda-16" href="../high_scalability-2010/high_scalability-2010-03-02-Using_the_Ambient_Cloud_as_an_Application_Runtime.html">786 high scalability-2010-03-02-Using the Ambient Cloud as an Application Runtime</a></p>
<p>17 0.77282673 <a title="581-lda-17" href="../high_scalability-2009/high_scalability-2009-07-02-Product%3A_Hbase.html">650 high scalability-2009-07-02-Product: Hbase</a></p>
<p>18 0.76537538 <a title="581-lda-18" href="../high_scalability-2011/high_scalability-2011-05-27-Stuff_The_Internet_Says_On_Scalability_For_May_27%2C_2011.html">1048 high scalability-2011-05-27-Stuff The Internet Says On Scalability For May 27, 2011</a></p>
<p>19 0.76474917 <a title="581-lda-19" href="../high_scalability-2008/high_scalability-2008-05-19-Twitter_as_a_scalability_case_study.html">323 high scalability-2008-05-19-Twitter as a scalability case study</a></p>
<p>20 0.76260322 <a title="581-lda-20" href="../high_scalability-2013/high_scalability-2013-01-23-Building_Redundant_Datacenter_Networks_is_Not_For_Sissies_-_Use_an_Outside_WAN_Backbone.html">1392 high scalability-2013-01-23-Building Redundant Datacenter Networks is Not For Sissies - Use an Outside WAN Backbone</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
