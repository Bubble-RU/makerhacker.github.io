<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>513 high scalability-2009-02-16-Handle 1 Billion Events Per Day Using a Memory Grid</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2009" href="../home/high_scalability-2009_home.html">high_scalability-2009</a> <a title="high_scalability-2009-513" href="#">high_scalability-2009-513</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>513 high scalability-2009-02-16-Handle 1 Billion Events Per Day Using a Memory Grid</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2009-513-html" href="http://highscalability.com//blog/2009/2/16/handle-1-billion-events-per-day-using-a-memory-grid.html">html</a></p><p>Introduction: Moshe Kaplan of RockeTier  shows the life cycle of an affiliate marketing system  that starts off as a cub handling one million events per day and ends up a lion handling 200 million to even one billion events per day. The resulting system uses ten commodity servers at a cost of $35,000.   Mr. Kaplan's paper is especially interesting because it documents a system architecture evolution we may see a lot more of in the future:  database centric --> cache centric --> memory grid .   As scaling and performance requirements for complicated operations increase, leaving the entire system in memory starts to make a great deal of sense. Why use cache at all? Why shouldn't your system be all in memory from the start?
  General Approach to Evolving the System to Scale    Analyze the system architecture and the main business processes. Detect the main hardware bottlenecks and the related business process causing them. Focus efforts on points of greatest return.   Rate the bottlenecks by importance</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Moshe Kaplan of RockeTier  shows the life cycle of an affiliate marketing system  that starts off as a cub handling one million events per day and ends up a lion handling 200 million to even one billion events per day. [sent-1, score-1.97]
</p><p>2 Kaplan's paper is especially interesting because it documents a system architecture evolution we may see a lot more of in the future:  database centric --> cache centric --> memory grid . [sent-4, score-0.936]
</p><p>3 General Approach to Evolving the System to Scale    Analyze the system architecture and the main business processes. [sent-8, score-0.319]
</p><p>4 Detect the main hardware bottlenecks and the related business process causing them. [sent-9, score-0.288]
</p><p>5 Rate the bottlenecks by importance and provide immediate and practical recommendation to improve performance. [sent-11, score-0.342]
</p><p>6 Implement the recommendations to provide immediate relief to problems. [sent-12, score-0.297]
</p><p>7 One Million Event Per Day System     The events are common advertising system operations like: ad impressions, clicks, and sales. [sent-16, score-0.391]
</p><p>8 Impressions and banner sales are written directly to the database. [sent-18, score-0.215]
</p><p>9 5 million events per day so something needed to be done. [sent-20, score-0.836]
</p><p>10 The next goal was to handle 20 million events per day. [sent-26, score-0.913]
</p><p>11 20 Million Event Per Day System     To make this scaling leap a rethinking of how the system worked was in order. [sent-27, score-0.295]
</p><p>12 The main load of the system was validating inputs in order to prevent forgery. [sent-28, score-0.409]
</p><p>13 An in-memory database was used to accumulate transactions over time (impression counting, clicks, sales recording). [sent-31, score-0.383]
</p><p>14 A periodic process was used to write transactions from the in-memory database to the database server. [sent-32, score-0.448]
</p><p>15 This architecture could handle 20 million events using existing hardware. [sent-33, score-0.66]
</p><p>16 Business projections required a system that could handle 200 million events. [sent-34, score-0.544]
</p><p>17 200 Million Event Per Day System     The next architectural evolution was to a scale out grid product. [sent-35, score-0.262]
</p><p>18 Data is still stored in the database as the data is used for statistics, reports, billing, fraud detection and so on. [sent-39, score-0.259]
</p><p>19 Latency was slashed because logic was separated out of the HTTP request/response loop into a separate process and database persistence is done offline. [sent-40, score-0.263]
</p><p>20 At this point architecture supports near-linear scaling and it's projected that it can easily scale to a billion events per day. [sent-41, score-0.673]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('events', 0.264), ('kaplan', 0.25), ('million', 0.245), ('systemthe', 0.227), ('immediate', 0.184), ('per', 0.174), ('day', 0.153), ('centric', 0.151), ('impressions', 0.136), ('system', 0.127), ('event', 0.126), ('main', 0.115), ('articlesgridgain', 0.113), ('relief', 0.113), ('sales', 0.108), ('banner', 0.107), ('evolution', 0.106), ('clicks', 0.103), ('projections', 0.098), ('accumulate', 0.098), ('affiliate', 0.098), ('validating', 0.098), ('database', 0.097), ('rethinking', 0.095), ('systemthat', 0.095), ('process', 0.092), ('redesign', 0.09), ('supports', 0.082), ('fortune', 0.082), ('fraud', 0.082), ('periodic', 0.082), ('bottlenecks', 0.081), ('used', 0.08), ('grid', 0.079), ('goal', 0.079), ('cache', 0.078), ('architecture', 0.077), ('recommendation', 0.077), ('next', 0.077), ('starts', 0.076), ('projected', 0.076), ('recording', 0.076), ('handling', 0.075), ('separated', 0.074), ('handle', 0.074), ('leap', 0.073), ('counting', 0.071), ('paper', 0.07), ('impression', 0.069), ('inputs', 0.069)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9999997 <a title="513-tfidf-1" href="../high_scalability-2009/high_scalability-2009-02-16-Handle_1_Billion_Events_Per_Day_Using_a_Memory_Grid.html">513 high scalability-2009-02-16-Handle 1 Billion Events Per Day Using a Memory Grid</a></p>
<p>Introduction: Moshe Kaplan of RockeTier  shows the life cycle of an affiliate marketing system  that starts off as a cub handling one million events per day and ends up a lion handling 200 million to even one billion events per day. The resulting system uses ten commodity servers at a cost of $35,000.   Mr. Kaplan's paper is especially interesting because it documents a system architecture evolution we may see a lot more of in the future:  database centric --> cache centric --> memory grid .   As scaling and performance requirements for complicated operations increase, leaving the entire system in memory starts to make a great deal of sense. Why use cache at all? Why shouldn't your system be all in memory from the start?
  General Approach to Evolving the System to Scale    Analyze the system architecture and the main business processes. Detect the main hardware bottlenecks and the related business process causing them. Focus efforts on points of greatest return.   Rate the bottlenecks by importance</p><p>2 0.15863642 <a title="513-tfidf-2" href="../high_scalability-2011/high_scalability-2011-03-08-Medialets_Architecture_-__Defeating_the_Daunting_Mobile_Device_Data_Deluge.html">1000 high scalability-2011-03-08-Medialets Architecture -  Defeating the Daunting Mobile Device Data Deluge</a></p>
<p>Introduction: Mobile developers have a huge scaling problem ahead: doing something useful with massive continuous streams of telemetry data from millions and millions of devices. This is a really good problem to have. It means smartphone sales are finally fulfilling their destiny:  slaughtering PCs  in the sales arena. And it also means mobile devices aren't just containers for simple standalone apps anymore, they are becoming the dominant interface to giant backend systems.
    
While developers are now rocking mobile development on the client side, their next challenge is how to code those tricky backend bits. A company facing those same exact problems right now is  Medialets , a mobile rich media ad platform. What they do is help publishers create high quality interactive ads, though for our purposes their ad stuff isn't that interesting. What I did find really interesting about their system is how they are tackling the problem of defeating the mobile device data deluge.
 
Each day Medialets munc</p><p>3 0.15493692 <a title="513-tfidf-3" href="../high_scalability-2009/high_scalability-2009-06-26-PlentyOfFish_Architecture.html">638 high scalability-2009-06-26-PlentyOfFish Architecture</a></p>
<p>Introduction: Update 5 :  PlentyOfFish Update - 6 Billion Pageviews And 32 Billion Images A Month     Update 4 :  Jeff Atwood  costs out Markus' scale up approach against a scale out approach and finds scale up wanting. The discussion in the comments is as interesting as the article. My guess is Markus doesn't want to rewrite his software to work across a scale out cluster so even if it's more expensive scale up works better for his needs.   Update 3 :  POF now has 200 million images  and serves 10,000 images served per second. They'll be moving to a 250,000 IOPS RamSan to handle the load. Also upgraded to a core database machine with 512 GB of RAM, 32 CPU’s, SQLServer  2008 and Windows 2008.   Update 2 : This seems to be a  POF Peer1 love fest infomercial . It's pretty content free, but the production values are high. Lots of quirky sounds and fish swimming on the screen.  Update : by Facebook standards Read/WriteWeb says POF is worth a cool  one billion dollars . It helps to talk like Dr. Evil whe</p><p>4 0.15322828 <a title="513-tfidf-4" href="../high_scalability-2012/high_scalability-2012-11-22-Gone_Fishin%27%3A_PlentyOfFish_Architecture.html">1361 high scalability-2012-11-22-Gone Fishin': PlentyOfFish Architecture</a></p>
<p>Introduction: Other than  StackOverflow , PlentyOfFish is perhaps the most spectacular example of scale-up architectures working for what your average sane person would consider a large system. It doesn't hurt that it's also a sexy story. 
 
 Update 5 :  PlentyOfFish Update - 6 Billion Pageviews And 32 Billion Images A Month     Update 4 :  Jeff Atwood  costs out Markus' scale up approach against a scale out approach and finds scale up wanting. The discussion in the comments is as interesting as the article. My guess is Markus doesn't want to rewrite his software to work across a scale out cluster so even if it's more expensive scale up works better for his needs.   Update 3 :  POF now has 200 million images  and serves 10,000 images served per second. They'll be moving to a 250,000 IOPS RamSan to handle the load. Also upgraded to a core database machine with 512 GB of RAM, 32 CPU’s, SQLServer  2008 and Windows 2008.   Update 2 : This seems to be a  POF Peer1 love fest infomercial . It's pretty cont</p><p>5 0.1490068 <a title="513-tfidf-5" href="../high_scalability-2011/high_scalability-2011-03-22-Facebook%27s_New_Realtime_Analytics_System%3A_HBase_to_Process_20_Billion_Events_Per_Day.html">1008 high scalability-2011-03-22-Facebook's New Realtime Analytics System: HBase to Process 20 Billion Events Per Day</a></p>
<p>Introduction: Facebook did it again. They've built another system capable of doing something useful with ginormous streams of realtime data. Last time we saw Facebook release their  New Real-Time Messaging System: HBase To Store 135+ Billion Messages A Month . This time it's a realtime analytics system handling  over 20 billion events per day (200,000 events per second) with a lag of less than 30 seconds . 
 
Alex Himel, Engineering Manager at Facebook,  explains what they've built  ( video ) and the scale required:
  

Social plugins have become an important and growing source of traffic for millions of websites over the past year. We released a new version of Insights for Websites last week to give site owners better analytics on how people interact with their content and to help them optimize their websites in real time. To accomplish this, we had to engineer a system that could process over 20 billion events per day (200,000 events per second) with a lag of less than 30 seconds. 

  
Alex does a</p><p>6 0.14430054 <a title="513-tfidf-6" href="../high_scalability-2013/high_scalability-2013-11-19-We_Finally_Cracked_the_10K_Problem_-_This_Time_for_Managing_Servers_with_2000x_Servers_Managed_Per_Sysadmin.html">1550 high scalability-2013-11-19-We Finally Cracked the 10K Problem - This Time for Managing Servers with 2000x Servers Managed Per Sysadmin</a></p>
<p>7 0.13915555 <a title="513-tfidf-7" href="../high_scalability-2009/high_scalability-2009-02-12-MySpace_Architecture.html">511 high scalability-2009-02-12-MySpace Architecture</a></p>
<p>8 0.13902003 <a title="513-tfidf-8" href="../high_scalability-2013/high_scalability-2013-05-13-The_Secret_to_10_Million_Concurrent_Connections_-The_Kernel_is_the_Problem%2C_Not_the_Solution.html">1456 high scalability-2013-05-13-The Secret to 10 Million Concurrent Connections -The Kernel is the Problem, Not the Solution</a></p>
<p>9 0.13708837 <a title="513-tfidf-9" href="../high_scalability-2009/high_scalability-2009-03-16-Are_Cloud_Based_Memory_Architectures_the_Next_Big_Thing%3F.html">538 high scalability-2009-03-16-Are Cloud Based Memory Architectures the Next Big Thing?</a></p>
<p>10 0.13077939 <a title="513-tfidf-10" href="../high_scalability-2012/high_scalability-2012-06-05-Sponsored_Post%3A_Digital_Ocean%2C_NetDNA%2C_Torbit%2C_Velocity%2C_Reality_Check_Network%2C_Gigaspaces%2C_AiCache%2C_Logic_Monitor%2C_Attribution_Modeling%2C_AppDynamics%2C_CloudSigma%2C_ManageEnine%2C_Site24x7.html">1257 high scalability-2012-06-05-Sponsored Post: Digital Ocean, NetDNA, Torbit, Velocity, Reality Check Network, Gigaspaces, AiCache, Logic Monitor, Attribution Modeling, AppDynamics, CloudSigma, ManageEnine, Site24x7</a></p>
<p>11 0.12968759 <a title="513-tfidf-11" href="../high_scalability-2008/high_scalability-2008-05-02-Friends_for_Sale_Architecture_-_A_300_Million_Page_View-Month_Facebook_RoR_App.html">313 high scalability-2008-05-02-Friends for Sale Architecture - A 300 Million Page View-Month Facebook RoR App</a></p>
<p>12 0.12695794 <a title="513-tfidf-12" href="../high_scalability-2014/high_scalability-2014-05-06-The_Quest_for_Database_Scale%3A_the_1_M_TPS_challenge_-_Three_Design_Points_and_Five_common_Bottlenecks_to_avoid.html">1643 high scalability-2014-05-06-The Quest for Database Scale: the 1 M TPS challenge - Three Design Points and Five common Bottlenecks to avoid</a></p>
<p>13 0.12043542 <a title="513-tfidf-13" href="../high_scalability-2008/high_scalability-2008-03-12-YouTube_Architecture.html">274 high scalability-2008-03-12-YouTube Architecture</a></p>
<p>14 0.11844616 <a title="513-tfidf-14" href="../high_scalability-2012/high_scalability-2012-04-10-Sponsored_Post%3A_Infragistics%2C_Reality_Check_Network%2C_Gigaspaces%2C_AiCache%2C_ElasticHosts%2C_Logic_Monitor%2C_Attribution_Modeling%2C_New_Relic%2C_AppDynamics%2C_CloudSigma%2C_ManageEnine%2C_Site24x7.html">1226 high scalability-2012-04-10-Sponsored Post: Infragistics, Reality Check Network, Gigaspaces, AiCache, ElasticHosts, Logic Monitor, Attribution Modeling, New Relic, AppDynamics, CloudSigma, ManageEnine, Site24x7</a></p>
<p>15 0.11844616 <a title="513-tfidf-15" href="../high_scalability-2012/high_scalability-2012-04-24-Sponsored_Post%3A_Reality_Check_Network%2C_Infragistics%2C_Gigaspaces%2C_AiCache%2C_ElasticHosts%2C_Logic_Monitor%2C_Attribution_Modeling%2C_New_Relic%2C_AppDynamics%2C_CloudSigma%2C_ManageEnine%2C_Site24x7.html">1232 high scalability-2012-04-24-Sponsored Post: Reality Check Network, Infragistics, Gigaspaces, AiCache, ElasticHosts, Logic Monitor, Attribution Modeling, New Relic, AppDynamics, CloudSigma, ManageEnine, Site24x7</a></p>
<p>16 0.11802343 <a title="513-tfidf-16" href="../high_scalability-2014/high_scalability-2014-02-26-The_WhatsApp_Architecture_Facebook_Bought_For_%2419_Billion.html">1602 high scalability-2014-02-26-The WhatsApp Architecture Facebook Bought For $19 Billion</a></p>
<p>17 0.11739699 <a title="513-tfidf-17" href="../high_scalability-2013/high_scalability-2013-08-13-In_Memoriam%3A_Lavabit_Architecture_-_Creating_a_Scalable_Email_Service.html">1501 high scalability-2013-08-13-In Memoriam: Lavabit Architecture - Creating a Scalable Email Service</a></p>
<p>18 0.11587234 <a title="513-tfidf-18" href="../high_scalability-2009/high_scalability-2009-04-04-Digg_Architecture.html">554 high scalability-2009-04-04-Digg Architecture</a></p>
<p>19 0.1157259 <a title="513-tfidf-19" href="../high_scalability-2010/high_scalability-2010-05-03-MocoSpace_Architecture_-_3_Billion_Mobile_Page_Views_a_Month.html">821 high scalability-2010-05-03-MocoSpace Architecture - 3 Billion Mobile Page Views a Month</a></p>
<p>20 0.11350489 <a title="513-tfidf-20" href="../high_scalability-2012/high_scalability-2012-06-26-Sponsored_Post%3A_New_Relic%2C_Digital_Ocean%2C_NetDNA%2C_Torbit%2C_Reality_Check_Network%2C_Gigaspaces%2C_AiCache%2C_Logic_Monitor%2C_AppDynamics%2C_CloudSigma%2C_ManageEnine%2C_Site24x7.html">1272 high scalability-2012-06-26-Sponsored Post: New Relic, Digital Ocean, NetDNA, Torbit, Reality Check Network, Gigaspaces, AiCache, Logic Monitor, AppDynamics, CloudSigma, ManageEnine, Site24x7</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.225), (1, 0.091), (2, -0.019), (3, -0.101), (4, -0.029), (5, 0.041), (6, 0.014), (7, 0.021), (8, -0.02), (9, 0.042), (10, 0.014), (11, 0.01), (12, 0.04), (13, 0.056), (14, -0.062), (15, 0.054), (16, -0.023), (17, -0.004), (18, 0.01), (19, 0.075), (20, 0.026), (21, 0.006), (22, 0.012), (23, -0.056), (24, 0.064), (25, -0.073), (26, -0.067), (27, 0.04), (28, 0.035), (29, 0.05), (30, 0.038), (31, 0.014), (32, -0.078), (33, -0.017), (34, 0.013), (35, 0.029), (36, -0.044), (37, -0.031), (38, 0.057), (39, 0.074), (40, -0.027), (41, -0.035), (42, 0.037), (43, 0.023), (44, 0.025), (45, 0.001), (46, 0.008), (47, 0.019), (48, 0.002), (49, 0.001)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97975981 <a title="513-lsi-1" href="../high_scalability-2009/high_scalability-2009-02-16-Handle_1_Billion_Events_Per_Day_Using_a_Memory_Grid.html">513 high scalability-2009-02-16-Handle 1 Billion Events Per Day Using a Memory Grid</a></p>
<p>Introduction: Moshe Kaplan of RockeTier  shows the life cycle of an affiliate marketing system  that starts off as a cub handling one million events per day and ends up a lion handling 200 million to even one billion events per day. The resulting system uses ten commodity servers at a cost of $35,000.   Mr. Kaplan's paper is especially interesting because it documents a system architecture evolution we may see a lot more of in the future:  database centric --> cache centric --> memory grid .   As scaling and performance requirements for complicated operations increase, leaving the entire system in memory starts to make a great deal of sense. Why use cache at all? Why shouldn't your system be all in memory from the start?
  General Approach to Evolving the System to Scale    Analyze the system architecture and the main business processes. Detect the main hardware bottlenecks and the related business process causing them. Focus efforts on points of greatest return.   Rate the bottlenecks by importance</p><p>2 0.81190991 <a title="513-lsi-2" href="../high_scalability-2014/high_scalability-2014-02-13-Snabb_Switch_-_Skip_the_OS_and_Get_40_million_Requests_Per_Second_in_Lua.html">1595 high scalability-2014-02-13-Snabb Switch - Skip the OS and Get 40 million Requests Per Second in Lua</a></p>
<p>Introduction: Snabb Switch  - a toolkit for solving novel problems in networking. If you are building a new packet-processing network appliance then you can use Snabb Switch to get the job done more quickly.
 
Here's a great impassioned overview from  erichocean :
  Or, you could just avoid the OS altogether:  https://github.com/SnabbCo/snabbswitch    

Our current engineering target is 1 million writes/sec and > 10 million reads/sec on top of an architecture similar to that, on a single box, to our fully transactional, MVCC database (write do not block reads, and vice versa) that runs in the same process (a la SQLite), which we've also merged with our application code and our caching tier, so we're down to—literally—a single process for what would have been at least three separate tiers in a traditional setup.


The result is that we had to move to measuring request latency in microseconds exclusively. The architecture (without additional application-specific processing) supports a wire-to-wire mes</p><p>3 0.80597186 <a title="513-lsi-3" href="../high_scalability-2009/high_scalability-2009-02-12-MySpace_Architecture.html">511 high scalability-2009-02-12-MySpace Architecture</a></p>
<p>Introduction: Update:  Presentation: Behind the Scenes at MySpace.com . Dan Farino, Chief Systems Architect at MySpace shares details of some of MySpace's cool internal operations tools.   MySpace.com  is one of the fastest growing site on the Internet with 65 million subscribers and 260,000 new users registering each day. Often criticized for poor performance, MySpace has had to tackle scalability issues few other sites have faced. How did they do it?
 
Site: http://myspace.com
  Information Sources    Presentation: Behind the Scenes at MySpace.com      Inside MySpace.com  
 Platform 
    ASP.NET 2.0     Windows    IIS    SQL Server 
 What's Inside? 
   300 million users.   Pushes 100 gigabits/second to the internet. 10Gb/sec is HTML content.   4,500+ web servers windows 2003/IIS 6.0/APS.NET.   1,200+ cache servers running 64-bit Windows 2003. 16GB of objects cached in RAM.   500+ database servers running 64-bit Windows and SQL Server 2005.      MySpace processes 1.5 Billion page views per day and</p><p>4 0.77231681 <a title="513-lsi-4" href="../high_scalability-2013/high_scalability-2013-05-13-The_Secret_to_10_Million_Concurrent_Connections_-The_Kernel_is_the_Problem%2C_Not_the_Solution.html">1456 high scalability-2013-05-13-The Secret to 10 Million Concurrent Connections -The Kernel is the Problem, Not the Solution</a></p>
<p>Introduction: Now that we have the    C10K concurrent connection problem    licked, how do we level up and support 10 million concurrent connections? Impossible you say. Nope, systems right now are delivering 10 million concurrent connections using techniques that are as radical as they may be unfamiliar. 
   To learn how it’s done we turn to    Robert Graham   , CEO of Errata Security, and his absolutely fantastic talk at    Shmoocon 2013    called    C10M Defending The Internet At Scale   . 
  Robert has a brilliant way of framing the problem that I’ve never heard of before. He starts with a little bit of history, relating how Unix wasn’t originally designed to be a general server OS, it was designed to be a control system for a telephone network. It was the telephone network that actually transported the data so there was a clean separation between the control plane and the data plane. The  problem is we now use Unix servers as part of the data plane , which we shouldn’t do at all. If we were des</p><p>5 0.77151841 <a title="513-lsi-5" href="../high_scalability-2014/high_scalability-2014-05-06-The_Quest_for_Database_Scale%3A_the_1_M_TPS_challenge_-_Three_Design_Points_and_Five_common_Bottlenecks_to_avoid.html">1643 high scalability-2014-05-06-The Quest for Database Scale: the 1 M TPS challenge - Three Design Points and Five common Bottlenecks to avoid</a></p>
<p>Introduction: This a guest post by  Rajkumar Iyer , a Member of Technical Staff at Aerospike. 
 
About a year ago, Aerospike embarked upon a quest to increase in-memory database performance - 1 Million TPS on a single inexpensive commodity server. NoSQL has the reputation of speed, and we saw great benefit from improving latency and throughput of cacheless architectures. At that time, we took a version of Aerospike delivering about 200K TPS, improved a few things - performance went to 500k TPS - and published the Aerospike 2.0 Community Edition. We then used kernel tuning techniques and published the  recipe  for how we achieved 1 M TPS on $5k of hardware.
  This year we continued the quest. Our goal was to achieve 1 Million database transactions per second per server; more than doubling previous performance. This compares to Cassandra’s boast of 1M TPS on over 300 servers in Google Compute Engine - at a cost of $2 million dollars per year. We  achieved this  without kernel tuning. 
  This article d</p><p>6 0.76398379 <a title="513-lsi-6" href="../high_scalability-2012/high_scalability-2012-11-22-Gone_Fishin%27%3A_PlentyOfFish_Architecture.html">1361 high scalability-2012-11-22-Gone Fishin': PlentyOfFish Architecture</a></p>
<p>7 0.76397139 <a title="513-lsi-7" href="../high_scalability-2009/high_scalability-2009-06-26-PlentyOfFish_Architecture.html">638 high scalability-2009-06-26-PlentyOfFish Architecture</a></p>
<p>8 0.75429648 <a title="513-lsi-8" href="../high_scalability-2010/high_scalability-2010-09-17-Hot_Scalability_Links_For_Sep_17%2C_2010.html">903 high scalability-2010-09-17-Hot Scalability Links For Sep 17, 2010</a></p>
<p>9 0.75382668 <a title="513-lsi-9" href="../high_scalability-2008/high_scalability-2008-12-20-Second_Life_Architecture_-_The_Grid.html">473 high scalability-2008-12-20-Second Life Architecture - The Grid</a></p>
<p>10 0.74436057 <a title="513-lsi-10" href="../high_scalability-2010/high_scalability-2010-03-04-How_MySpace_Tested_Their_Live_Site_with_1_Million_Concurrent_Users.html">788 high scalability-2010-03-04-How MySpace Tested Their Live Site with 1 Million Concurrent Users</a></p>
<p>11 0.73739982 <a title="513-lsi-11" href="../high_scalability-2010/high_scalability-2010-10-26-Scaling_DISQUS_to_75_Million_Comments_and_17%2C000_RPS.html">928 high scalability-2010-10-26-Scaling DISQUS to 75 Million Comments and 17,000 RPS</a></p>
<p>12 0.73184234 <a title="513-lsi-12" href="../high_scalability-2007/high_scalability-2007-08-22-Wikimedia_architecture.html">72 high scalability-2007-08-22-Wikimedia architecture</a></p>
<p>13 0.72646296 <a title="513-lsi-13" href="../high_scalability-2008/high_scalability-2008-05-27-eBay_Architecture.html">331 high scalability-2008-05-27-eBay Architecture</a></p>
<p>14 0.72341764 <a title="513-lsi-14" href="../high_scalability-2010/high_scalability-2010-05-03-MocoSpace_Architecture_-_3_Billion_Mobile_Page_Views_a_Month.html">821 high scalability-2010-05-03-MocoSpace Architecture - 3 Billion Mobile Page Views a Month</a></p>
<p>15 0.7206409 <a title="513-lsi-15" href="../high_scalability-2013/high_scalability-2013-11-19-We_Finally_Cracked_the_10K_Problem_-_This_Time_for_Managing_Servers_with_2000x_Servers_Managed_Per_Sysadmin.html">1550 high scalability-2013-11-19-We Finally Cracked the 10K Problem - This Time for Managing Servers with 2000x Servers Managed Per Sysadmin</a></p>
<p>16 0.71992975 <a title="513-lsi-16" href="../high_scalability-2007/high_scalability-2007-08-01-Product%3A_Memcached.html">52 high scalability-2007-08-01-Product: Memcached</a></p>
<p>17 0.71921104 <a title="513-lsi-17" href="../high_scalability-2014/high_scalability-2014-03-31-How_WhatsApp_Grew_to_Nearly_500_Million_Users%2C_11%2C000_cores%2C_and_70_Million_Messages_a_Second.html">1622 high scalability-2014-03-31-How WhatsApp Grew to Nearly 500 Million Users, 11,000 cores, and 70 Million Messages a Second</a></p>
<p>18 0.7158345 <a title="513-lsi-18" href="../high_scalability-2009/high_scalability-2009-05-15-Wolfram%7CAlpha_Architecture.html">600 high scalability-2009-05-15-Wolfram|Alpha Architecture</a></p>
<p>19 0.71071571 <a title="513-lsi-19" href="../high_scalability-2007/high_scalability-2007-07-12-FeedBurner_Architecture.html">7 high scalability-2007-07-12-FeedBurner Architecture</a></p>
<p>20 0.70022476 <a title="513-lsi-20" href="../high_scalability-2007/high_scalability-2007-10-26-How_Gravatar_scales_on_WordPress.com_hardware.html">133 high scalability-2007-10-26-How Gravatar scales on WordPress.com hardware</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.168), (2, 0.24), (10, 0.023), (30, 0.039), (58, 0.121), (61, 0.069), (77, 0.027), (79, 0.115), (85, 0.085), (94, 0.029)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94329911 <a title="513-lda-1" href="../high_scalability-2009/high_scalability-2009-02-16-Handle_1_Billion_Events_Per_Day_Using_a_Memory_Grid.html">513 high scalability-2009-02-16-Handle 1 Billion Events Per Day Using a Memory Grid</a></p>
<p>Introduction: Moshe Kaplan of RockeTier  shows the life cycle of an affiliate marketing system  that starts off as a cub handling one million events per day and ends up a lion handling 200 million to even one billion events per day. The resulting system uses ten commodity servers at a cost of $35,000.   Mr. Kaplan's paper is especially interesting because it documents a system architecture evolution we may see a lot more of in the future:  database centric --> cache centric --> memory grid .   As scaling and performance requirements for complicated operations increase, leaving the entire system in memory starts to make a great deal of sense. Why use cache at all? Why shouldn't your system be all in memory from the start?
  General Approach to Evolving the System to Scale    Analyze the system architecture and the main business processes. Detect the main hardware bottlenecks and the related business process causing them. Focus efforts on points of greatest return.   Rate the bottlenecks by importance</p><p>2 0.94094414 <a title="513-lda-2" href="../high_scalability-2010/high_scalability-2010-11-29-Stuff_the_Internet_Says_on_Scalability_For_November_29th%2C_2010.html">949 high scalability-2010-11-29-Stuff the Internet Says on Scalability For November 29th, 2010</a></p>
<p>Introduction: Eating turkey all weekend and wondering what you might have missed?
  
 James Hamilton on why “all you have learned about disks so far is probably wrong" in  Availability in Globally Distributed Storage . It turns out for the same reason our financial systems melt down:  black swans . The world is predictably unpredictable. Murat Demirbas also has a  good post  on the same  Google research paper . 
  Stack Overflow  Hits  10M Uniques  
 Vroom...Formula One racecar  streams 27 gigabytes of telemetry  data during a race weekend! 200 sensors “measuring anything and everything that moves or gets warm.  
 Quotable Quotes:          
 
  @dmalenko  :  It is cool to sit by the ocean, oversee the sunset and think about scalability models for a web app 
  @detroitpro : I have to admit; sometimes I think "This would be easier with a SQL DB" #NoSQL #NotOften #ComplextRelationships #FindingRootObjects 
 
 
 You may have missed the Google App Engine  cage match . First GAE  sucks  and then it's  gre</p><p>3 0.9368878 <a title="513-lda-3" href="../high_scalability-2013/high_scalability-2013-01-14-MongoDB_and_GridFS_for_Inter_and_Intra_Datacenter_Data_Replication_.html">1386 high scalability-2013-01-14-MongoDB and GridFS for Inter and Intra Datacenter Data Replication </a></p>
<p>Introduction: This is a guest post by  Jeff Behl , VP Ops @ LogicMonitor.  Jeff  has been a bit herder for the last 20 years, architecting and overseeing the infrastructure for a number of SaaS based companies.   
  Data Replication for Disaster Recovery  
An inevitable part of disaster recovery planning is making sure customer data exists in multiple locations.  In the case of LogicMonitor, a SaaS-based monitoring solution for physical, virtual, and cloud environments, we wanted copies of customer data files both within a data center and outside of it.  The former was to protect against the loss of individual servers within a facility, and the latter for recovery in the event of the complete loss of a data center.
  Where we were:  Rsync  
Like most everyone who starts off in a Linux environment, we used our trusty friend rsync to copy data around.
 
 
 
     
  Rsync is tried, true and tested, and works well when the number of servers, the amount of data, and the number of files is not horrendous.</p><p>4 0.92886603 <a title="513-lda-4" href="../high_scalability-2011/high_scalability-2011-03-08-Medialets_Architecture_-__Defeating_the_Daunting_Mobile_Device_Data_Deluge.html">1000 high scalability-2011-03-08-Medialets Architecture -  Defeating the Daunting Mobile Device Data Deluge</a></p>
<p>Introduction: Mobile developers have a huge scaling problem ahead: doing something useful with massive continuous streams of telemetry data from millions and millions of devices. This is a really good problem to have. It means smartphone sales are finally fulfilling their destiny:  slaughtering PCs  in the sales arena. And it also means mobile devices aren't just containers for simple standalone apps anymore, they are becoming the dominant interface to giant backend systems.
    
While developers are now rocking mobile development on the client side, their next challenge is how to code those tricky backend bits. A company facing those same exact problems right now is  Medialets , a mobile rich media ad platform. What they do is help publishers create high quality interactive ads, though for our purposes their ad stuff isn't that interesting. What I did find really interesting about their system is how they are tackling the problem of defeating the mobile device data deluge.
 
Each day Medialets munc</p><p>5 0.92812788 <a title="513-lda-5" href="../high_scalability-2009/high_scalability-2009-08-05-Stack_Overflow_Architecture.html">671 high scalability-2009-08-05-Stack Overflow Architecture</a></p>
<p>Introduction: Update 2 :  Stack Overflow Architecture Update - Now At 95 Million Page Views A Month    Update:  Startup – ASP.NET   MVC, Cloud Scale & Deployment  shows an interesting alternative approach for a Windows stack using ServerPath/GoGrid for a dedicated database machine, elastic VMs for the front end, and a free load balancer.    Stack Overflow  is a much loved programmer question and answer site written by two guys nobody has ever heard of before. Well, not exactly. The site was created by top programmer and blog stars  Jeff Atwood  and  Joel Spolsky . In that sense Stack Overflow is like a celebrity owned restaurant, only it should be around for a while. Joel estimates 1/3 of all the programmers in the world have used the site so they must be serving up something good.
 
I fell in deep like with Stack Overflow for purely selfish reasons, it helped me solve a few difficult problems that were jabbing my eyes out with pain. I also appreciate their no-apologies anthropologically based desig</p><p>6 0.92544466 <a title="513-lda-6" href="../high_scalability-2014/high_scalability-2014-02-21-Stuff_The_Internet_Says_On_Scalability_For_February_21st%2C_2014.html">1600 high scalability-2014-02-21-Stuff The Internet Says On Scalability For February 21st, 2014</a></p>
<p>7 0.92455626 <a title="513-lda-7" href="../high_scalability-2009/high_scalability-2009-08-07-The_Canonical_Cloud_Architecture_.html">674 high scalability-2009-08-07-The Canonical Cloud Architecture </a></p>
<p>8 0.92418033 <a title="513-lda-8" href="../high_scalability-2013/high_scalability-2013-04-26-Stuff_The_Internet_Says_On_Scalability_For_April_26%2C_2013.html">1447 high scalability-2013-04-26-Stuff The Internet Says On Scalability For April 26, 2013</a></p>
<p>9 0.92383218 <a title="513-lda-9" href="../high_scalability-2014/high_scalability-2014-01-28-How_Next_Big_Sound_Tracks_Over_a_Trillion_Song_Plays%2C_Likes%2C_and_More_Using_a_Version_Control_System_for_Hadoop_Data.html">1586 high scalability-2014-01-28-How Next Big Sound Tracks Over a Trillion Song Plays, Likes, and More Using a Version Control System for Hadoop Data</a></p>
<p>10 0.92281616 <a title="513-lda-10" href="../high_scalability-2007/high_scalability-2007-11-19-Tailrank_Architecture_-_Learn_How_to_Track_Memes_Across_the_Entire_Blogosphere.html">160 high scalability-2007-11-19-Tailrank Architecture - Learn How to Track Memes Across the Entire Blogosphere</a></p>
<p>11 0.92254215 <a title="513-lda-11" href="../high_scalability-2011/high_scalability-2011-03-22-Facebook%27s_New_Realtime_Analytics_System%3A_HBase_to_Process_20_Billion_Events_Per_Day.html">1008 high scalability-2011-03-22-Facebook's New Realtime Analytics System: HBase to Process 20 Billion Events Per Day</a></p>
<p>12 0.92227912 <a title="513-lda-12" href="../high_scalability-2011/high_scalability-2011-03-03-Stack_Overflow_Architecture_Update_-_Now_at_95_Million_Page_Views_a_Month.html">998 high scalability-2011-03-03-Stack Overflow Architecture Update - Now at 95 Million Page Views a Month</a></p>
<p>13 0.92163813 <a title="513-lda-13" href="../high_scalability-2013/high_scalability-2013-09-18-If_You%27re_Programming_a_Cell_Phone_Like_a_Server_You%27re_Doing_it_Wrong.html">1519 high scalability-2013-09-18-If You're Programming a Cell Phone Like a Server You're Doing it Wrong</a></p>
<p>14 0.92161304 <a title="513-lda-14" href="../high_scalability-2013/high_scalability-2013-08-16-Stuff_The_Internet_Says_On_Scalability_For_August_16%2C_2013.html">1502 high scalability-2013-08-16-Stuff The Internet Says On Scalability For August 16, 2013</a></p>
<p>15 0.92105484 <a title="513-lda-15" href="../high_scalability-2009/high_scalability-2009-05-01-FastBit%3A_An_Efficient_Compressed_Bitmap_Index_Technology.html">587 high scalability-2009-05-01-FastBit: An Efficient Compressed Bitmap Index Technology</a></p>
<p>16 0.92104346 <a title="513-lda-16" href="../high_scalability-2013/high_scalability-2013-03-29-Stuff_The_Internet_Says_On_Scalability_For_March_29%2C_2013.html">1431 high scalability-2013-03-29-Stuff The Internet Says On Scalability For March 29, 2013</a></p>
<p>17 0.92075521 <a title="513-lda-17" href="../high_scalability-2007/high_scalability-2007-09-15-The_Role_of_Memory_within_Web_2.0_Architectures_and_Deployments.html">92 high scalability-2007-09-15-The Role of Memory within Web 2.0 Architectures and Deployments</a></p>
<p>18 0.91990459 <a title="513-lda-18" href="../high_scalability-2013/high_scalability-2013-03-27-The_Changing_Face_of_Scale_-_The_Downside_of_Scaling_in_the_Contextual_Age_.html">1430 high scalability-2013-03-27-The Changing Face of Scale - The Downside of Scaling in the Contextual Age </a></p>
<p>19 0.91952926 <a title="513-lda-19" href="../high_scalability-2010/high_scalability-2010-04-12-Poppen.de_Architecture.html">808 high scalability-2010-04-12-Poppen.de Architecture</a></p>
<p>20 0.91952592 <a title="513-lda-20" href="../high_scalability-2014/high_scalability-2014-04-25-Stuff_The_Internet_Says_On_Scalability_For_April_25th%2C_2014.html">1637 high scalability-2014-04-25-Stuff The Internet Says On Scalability For April 25th, 2014</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
