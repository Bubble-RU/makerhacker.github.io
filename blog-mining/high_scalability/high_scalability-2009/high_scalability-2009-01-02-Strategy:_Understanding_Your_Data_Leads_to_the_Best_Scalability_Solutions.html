<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>481 high scalability-2009-01-02-Strategy: Understanding Your Data Leads to the Best Scalability Solutions</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2009" href="../home/high_scalability-2009_home.html">high_scalability-2009</a> <a title="high_scalability-2009-481" href="#">high_scalability-2009-481</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>481 high scalability-2009-01-02-Strategy: Understanding Your Data Leads to the Best Scalability Solutions</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2009-481-html" href="http://highscalability.com//blog/2009/1/2/strategy-understanding-your-data-leads-to-the-best-scalabili.html">html</a></p><p>Introduction: In article   Building Super-Scalable Web Systems with REST   Udi Dahan tells an interesting story of how they made a weather reporting system scale for over 10 million users.  So many users hitting their weather database didn't scale. Caching in a straightforward way wouldn't work because weather is obviously local. Caching all local reports would bring  the entire database into memory, which would work for some companies, but wasn't cost efficient for them.   So in typical REST fashion they turned locations into URIs. For example: http://weather.myclient.com/UK/London. This allows the weather information to be cached by intermediaries instead of hitting their servers.  Hopefully for each location their servers will be hit a few times and then the caches will be hit until expiry.
   
 In order to send users directly to the correct location an IP location check is performed on login and stored in a cookie. The lookup is done once and from then on out a GET is performed directly on the r</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In article   Building Super-Scalable Web Systems with REST   Udi Dahan tells an interesting story of how they made a weather reporting system scale for over 10 million users. [sent-1, score-0.862]
</p><p>2 So many users hitting their weather database didn't scale. [sent-2, score-0.863]
</p><p>3 Caching in a straightforward way wouldn't work because weather is obviously local. [sent-3, score-0.8]
</p><p>4 Caching all local reports would bring  the entire database into memory, which would work for some companies, but wasn't cost efficient for them. [sent-4, score-0.401]
</p><p>5 So in typical REST fashion they turned locations into URIs. [sent-5, score-0.334]
</p><p>6 This allows the weather information to be cached by intermediaries instead of hitting their servers. [sent-9, score-1.041]
</p><p>7 Hopefully for each location their servers will be hit a few times and then the caches will be hit until expiry. [sent-10, score-0.677]
</p><p>8 In order to send users directly to the correct location an IP location check is performed on login and stored in a cookie. [sent-11, score-1.114]
</p><p>9 The lookup is done once and from then on out a GET is performed directly on the resource. [sent-12, score-0.463]
</p><p>10 There's no need to hit their servers and do a lookup on the user to get the location. [sent-13, score-0.371]
</p><p>11 I like Udi's summary of the approach and is why I think this is a good strategy :  This isn’t a “cheap trick”. [sent-15, score-0.126]
</p><p>12 While being straight forward for something like weather, understanding the nature of your data and intelligently mapping that to a URI space is critical to building a scalable system, and reaping the benefits of REST. [sent-16, score-0.895]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('weather', 0.607), ('location', 0.229), ('hitting', 0.196), ('lookup', 0.187), ('hit', 0.184), ('reaping', 0.178), ('udi', 0.178), ('performed', 0.166), ('intermediaries', 0.16), ('articlebuilding', 0.141), ('uri', 0.133), ('rest', 0.12), ('login', 0.11), ('directly', 0.11), ('trick', 0.107), ('fashion', 0.106), ('intelligently', 0.106), ('straightforward', 0.102), ('straight', 0.101), ('mapping', 0.097), ('reporting', 0.095), ('tells', 0.091), ('obviously', 0.091), ('hopefully', 0.089), ('locations', 0.089), ('correct', 0.087), ('reports', 0.085), ('caching', 0.083), ('forward', 0.081), ('caches', 0.08), ('cached', 0.078), ('turned', 0.077), ('ip', 0.076), ('benefits', 0.072), ('summary', 0.071), ('nature', 0.07), ('would', 0.069), ('story', 0.069), ('understanding', 0.069), ('critical', 0.067), ('cheap', 0.067), ('bring', 0.064), ('typical', 0.062), ('check', 0.062), ('send', 0.061), ('users', 0.06), ('efficient', 0.057), ('local', 0.057), ('strategy', 0.055), ('space', 0.054)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999994 <a title="481-tfidf-1" href="../high_scalability-2009/high_scalability-2009-01-02-Strategy%3A_Understanding_Your_Data_Leads_to_the_Best_Scalability_Solutions.html">481 high scalability-2009-01-02-Strategy: Understanding Your Data Leads to the Best Scalability Solutions</a></p>
<p>Introduction: In article   Building Super-Scalable Web Systems with REST   Udi Dahan tells an interesting story of how they made a weather reporting system scale for over 10 million users.  So many users hitting their weather database didn't scale. Caching in a straightforward way wouldn't work because weather is obviously local. Caching all local reports would bring  the entire database into memory, which would work for some companies, but wasn't cost efficient for them.   So in typical REST fashion they turned locations into URIs. For example: http://weather.myclient.com/UK/London. This allows the weather information to be cached by intermediaries instead of hitting their servers.  Hopefully for each location their servers will be hit a few times and then the caches will be hit until expiry.
   
 In order to send users directly to the correct location an IP location check is performed on login and stored in a cookie. The lookup is done once and from then on out a GET is performed directly on the r</p><p>2 0.13432497 <a title="481-tfidf-2" href="../high_scalability-2007/high_scalability-2007-07-27-Product%3A_Munin_Monitoriting_Tool.html">34 high scalability-2007-07-27-Product: Munin Monitoriting Tool</a></p>
<p>Introduction: Munin  the monitoring tool surveys all your computers and remembers what it saw. It presents all the information in graphs through a web interface. Its emphasis is on plug and play capabilities. After completing a installation a high number of monitoring plugins will be playing with no more effort.  Using Munin you can easily monitor the performance of your computers, networks, SANs, applications, weather measurements and whatever comes to mind. It makes it easy to determine "what's different today" when a performance problem crops up. It makes it easy to see how you're doing capacity-wise on any resources.</p><p>3 0.10567628 <a title="481-tfidf-3" href="../high_scalability-2010/high_scalability-2010-01-11-Have_We_Reached_the_End_of_Scaling%3F.html">758 high scalability-2010-01-11-Have We Reached the End of Scaling?</a></p>
<p>Introduction: This is an excerpt from my article  Building Super Scalable Systems: Blade Runner Meets Autonomic Computing in the Ambient Cloud.  
 
Have we reached the end of scaling? That's what I asked myself one day after noticing a bunch of "The End of" headlines. We've reached  The End of History  because the Western liberal democracy is the "end point of humanity's sociocultural evolution and the final form of human government."  We've reached  The End of Science  because of the "fact that there aren't going to be any obvious, cataclysmic revolutions." We've even reached  The End of Theory  because all answers can be found in the continuous stream of data we're collecting. And doesn't always seem like we're at  The End of the World ?
 
Motivated by the prospect of everything ending, I began to wonder: have we really reached The End of Scaling?
 
For a while I thought this might be true. The reason I thought the End of Scaling might be near is because of the slow down of potential articles at m</p><p>4 0.087699614 <a title="481-tfidf-4" href="../high_scalability-2009/high_scalability-2009-09-12-How_Google_Taught_Me_to_Cache_and_Cash-In.html">703 high scalability-2009-09-12-How Google Taught Me to Cache and Cash-In</a></p>
<p>Introduction: A user named Apathy   on how Reddit scales some of their features, shares some advice he learned while working at Google and other major companies.   To be fair, I [Apathy] was working at Google at the time, and every job I held between 1995 and 2005 involved at least one of the largest websites on the planet. I didn't come up with any of these ideas, just watched other smart people I worked with who knew what they were doing and found (or wrote) tools that did the same things. But the theme is always the same: 
  
 Cache everything you can and store the rest in some sort of database (not necessarily relational and not necessarily centralized).  
 Cache everything that doesn't change rapidly. Most of the time you don't have to hit the database for anything other than checking whether the users' new message count has transitioned from 0 to (1 or more). 
 Cache everything--templates, user message status, the front page components--and hit the database once a minute or so to update the fr</p><p>5 0.085476816 <a title="481-tfidf-5" href="../high_scalability-2008/high_scalability-2008-03-28-How_to_Get_DNS_Names_of_a_Web_Server.html">290 high scalability-2008-03-28-How to Get DNS Names of a Web Server</a></p>
<p>Introduction: For some special reason, I'm trying to make a web server able to get all the DNS names mapped to its IP. Let me explain more, I'm creating a website that will run in a web farm, every web server in the farm will have some subdomains mapped to its ip, what I want is that whenever my application starts on a web server is to be able to get all the subdomains mapped/assigned to that server, e.g. sub1.mydomain.com, sub2.mydomain.com. I understand that I have to use reverse dns lookup (i.e. give the IP get the domain name), but I also want to get all the subdomains not just the first one that maps to that IP. I've been reading about DNS on the internet but I don't seem to find any information on how to achieve what I want, normally you use dns to get the ip of a domain but I'm not sure that all servers enable reverse lookup.     The problem is that I'm still not sure whether I'll host my own DNS server or use the services of some company (many companies offer DNS hosting services), so, my qu</p><p>6 0.084306121 <a title="481-tfidf-6" href="../high_scalability-2011/high_scalability-2011-07-29-Stuff_The_Internet_Says_On_Scalability_For_July_29%2C_2011.html">1089 high scalability-2011-07-29-Stuff The Internet Says On Scalability For July 29, 2011</a></p>
<p>7 0.074839532 <a title="481-tfidf-7" href="../high_scalability-2009/high_scalability-2009-02-12-MySpace_Architecture.html">511 high scalability-2009-02-12-MySpace Architecture</a></p>
<p>8 0.074171409 <a title="481-tfidf-8" href="../high_scalability-2007/high_scalability-2007-07-16-Blog%3A_MySQL_Performance_Blog_-_Everything_about_MySQL_Performance._.html">15 high scalability-2007-07-16-Blog: MySQL Performance Blog - Everything about MySQL Performance. </a></p>
<p>9 0.071680985 <a title="481-tfidf-9" href="../high_scalability-2011/high_scalability-2011-06-06-Apple_iCloud%3A_Syncing_and_Distributed_Storage_Over_Streaming_and_Centralized_Storage.html">1053 high scalability-2011-06-06-Apple iCloud: Syncing and Distributed Storage Over Streaming and Centralized Storage</a></p>
<p>10 0.071351998 <a title="481-tfidf-10" href="../high_scalability-2009/high_scalability-2009-04-16-Serving_250M_quotes-day_at_CNBC.com_with_aiCache.html">573 high scalability-2009-04-16-Serving 250M quotes-day at CNBC.com with aiCache</a></p>
<p>11 0.071037605 <a title="481-tfidf-11" href="../high_scalability-2009/high_scalability-2009-01-04-Alternative_Memcache_Usage%3A_A_Highly_Scalable%2C_Highly_Available%2C_In-Memory_Shard_Index.html">482 high scalability-2009-01-04-Alternative Memcache Usage: A Highly Scalable, Highly Available, In-Memory Shard Index</a></p>
<p>12 0.06986694 <a title="481-tfidf-12" href="../high_scalability-2011/high_scalability-2011-02-28-A_Practical_Guide_to_Varnish_-_Why_Varnish_Matters.html">996 high scalability-2011-02-28-A Practical Guide to Varnish - Why Varnish Matters</a></p>
<p>13 0.069299869 <a title="481-tfidf-13" href="../high_scalability-2013/high_scalability-2013-01-30-Better_Browser_Caching_is_More_Important_than_No_Javascript_or_Fast_Networks_for_HTTP_Performance.html">1396 high scalability-2013-01-30-Better Browser Caching is More Important than No Javascript or Fast Networks for HTTP Performance</a></p>
<p>14 0.066437438 <a title="481-tfidf-14" href="../high_scalability-2009/high_scalability-2009-05-08-Eight_Best_Practices_for_Building_Scalable_Systems.html">594 high scalability-2009-05-08-Eight Best Practices for Building Scalable Systems</a></p>
<p>15 0.066145055 <a title="481-tfidf-15" href="../high_scalability-2007/high_scalability-2007-08-22-How_many_machines_do_you_need_to_run_your_site%3F.html">70 high scalability-2007-08-22-How many machines do you need to run your site?</a></p>
<p>16 0.065858305 <a title="481-tfidf-16" href="../high_scalability-2010/high_scalability-2010-12-06-What_the_heck_are_you_actually_using_NoSQL_for%3F.html">954 high scalability-2010-12-06-What the heck are you actually using NoSQL for?</a></p>
<p>17 0.065836735 <a title="481-tfidf-17" href="../high_scalability-2009/high_scalability-2009-10-13-Why_are_Facebook%2C_Digg%2C_and_Twitter_so_hard_to_scale%3F.html">721 high scalability-2009-10-13-Why are Facebook, Digg, and Twitter so hard to scale?</a></p>
<p>18 0.065775581 <a title="481-tfidf-18" href="../high_scalability-2007/high_scalability-2007-12-07-Synchronizing_databases_in_different_geographic_locations.html">176 high scalability-2007-12-07-Synchronizing databases in different geographic locations</a></p>
<p>19 0.065123156 <a title="481-tfidf-19" href="../high_scalability-2010/high_scalability-2010-02-06-GEO-aware_traffic_load_balancing_and_caching_at_CNBC.com.html">773 high scalability-2010-02-06-GEO-aware traffic load balancing and caching at CNBC.com</a></p>
<p>20 0.062049747 <a title="481-tfidf-20" href="../high_scalability-2012/high_scalability-2012-04-17-YouTube_Strategy%3A_Adding_Jitter_isn%27t_a_Bug.html">1229 high scalability-2012-04-17-YouTube Strategy: Adding Jitter isn't a Bug</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.116), (1, 0.061), (2, -0.014), (3, -0.066), (4, 0.004), (5, -0.008), (6, -0.01), (7, 0.005), (8, -0.019), (9, 0.002), (10, -0.002), (11, -0.006), (12, -0.022), (13, 0.031), (14, 0.01), (15, 0.005), (16, -0.006), (17, -0.027), (18, 0.008), (19, -0.001), (20, -0.024), (21, 0.01), (22, 0.003), (23, 0.024), (24, 0.002), (25, -0.006), (26, 0.007), (27, 0.027), (28, 0.001), (29, 0.006), (30, -0.011), (31, -0.004), (32, -0.019), (33, 0.024), (34, -0.004), (35, 0.01), (36, -0.023), (37, -0.005), (38, 0.006), (39, -0.005), (40, 0.048), (41, -0.012), (42, 0.01), (43, -0.031), (44, 0.034), (45, 0.018), (46, -0.037), (47, -0.043), (48, -0.034), (49, 0.037)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.94502175 <a title="481-lsi-1" href="../high_scalability-2009/high_scalability-2009-01-02-Strategy%3A_Understanding_Your_Data_Leads_to_the_Best_Scalability_Solutions.html">481 high scalability-2009-01-02-Strategy: Understanding Your Data Leads to the Best Scalability Solutions</a></p>
<p>Introduction: In article   Building Super-Scalable Web Systems with REST   Udi Dahan tells an interesting story of how they made a weather reporting system scale for over 10 million users.  So many users hitting their weather database didn't scale. Caching in a straightforward way wouldn't work because weather is obviously local. Caching all local reports would bring  the entire database into memory, which would work for some companies, but wasn't cost efficient for them.   So in typical REST fashion they turned locations into URIs. For example: http://weather.myclient.com/UK/London. This allows the weather information to be cached by intermediaries instead of hitting their servers.  Hopefully for each location their servers will be hit a few times and then the caches will be hit until expiry.
   
 In order to send users directly to the correct location an IP location check is performed on login and stored in a cookie. The lookup is done once and from then on out a GET is performed directly on the r</p><p>2 0.76356608 <a title="481-lsi-2" href="../high_scalability-2013/high_scalability-2013-11-04-ESPN%27s_Architecture_at_Scale_-_Operating_at_100%2C000_Duh_Nuh_Nuhs_Per_Second.html">1542 high scalability-2013-11-04-ESPN's Architecture at Scale - Operating at 100,000 Duh Nuh Nuhs Per Second</a></p>
<p>Introduction: ESPN went on airin 1978. In those 30+ years think of the wonders we've seen!
When I think of ESPN I think of a world wide brand that is the very definition
of prime time. And it shows in their stats. ESPN.com peaks at 100,000 requests
per second. Their peak event is, not surprisingly, the World Cup. But would
you be surprised to learn ESPN is powered by only a few hundred servers and a
couple of dozen engineers? I was.And would you be surprised to learn ESPN is
undergoing a fundamental transition from an Enterprise architecture to one
capable of handling web scale loads driven by increasing mobile usage,
personalization, and a service orientation? Again, thinking ESPN was just
about watching sports on TV, I was surprised. ESPN is becoming much more than
that. ESPN is becoming a sports platform. How does ESPN handle all of this
complexity, responsibility, change, and load? Unlike most every other profile
on HighScalability. The fascinating story of ESPN's architecture is told
byManny Pe</p><p>3 0.7625069 <a title="481-lsi-3" href="../high_scalability-2007/high_scalability-2007-10-28-Scaling_Early_Stage_Startups.html">136 high scalability-2007-10-28-Scaling Early Stage Startups</a></p>
<p>Introduction: Mark Maunder of  No VC Required --who advocates not taking VC money lest you be turned into a frog instead of the prince (or princess) you were dreaming of--has an excellent  slide deck  on how to scale an early stage startup.  His blog also has some good SEO tips and a very spooky widget showing the geographical location of his readers. Perfect for Halloween! What is Mark's other worldly scaling strategies for startups?
 
Site: http://novcrequired.com/
  Information Sources     Slides from Seattle Tech Startup Talk .     Scaling Early Stage Startups  blog post by Mark Maunder. 
 The Platform 
    Linxux    An ISAM type data store.    Perl     Httperf  is used for benchmarking.      Websitepulse.com  is used for perf monitoring. 
 The Architecture 
    Performance matters because being slow could cost you 20% of your revenue. The UIE guys disagree saying this ain't necessarily so. They explain their reasoning in   Usability Tools Podcast: The Truth About Page Download Time . The idea i</p><p>4 0.7312429 <a title="481-lsi-4" href="../high_scalability-2010/high_scalability-2010-03-26-Strategy%3A_Caching_404s_Saved_the_Onion_66%25_on_Server_Time.html">800 high scalability-2010-03-26-Strategy: Caching 404s Saved the Onion 66% on Server Time</a></p>
<p>Introduction: In the article  The  Onion Uses Django, And Why It Matters To Us , a lot of interesting points are made about their ambitious infrastructure move from Drupal/PHP to Django/Python: the move wasn't that hard, it just took time and work because of their previous experience moving the A.V. Club website; churn in core framework APIs make it more attractive to move than stay; supporting the structure of older versions of the site is an unsolved problem; the built-in Django admin saved a lot of work; group development is easier with "fewer specialized or hacked together pieces"; they use IRC for distributed development; sphinx for full-text search; nginx is the media server and reverse proxy; haproxy made the launch process a 5  second procedure; capistrano for deployment; clean component separation makes moving easier; Git for version control; ORM with complicated querysets is a performance problem; memcached for caching rendered pages; the CDN checks for updates every 10 minutes; videos, ar</p><p>5 0.72454947 <a title="481-lsi-5" href="../high_scalability-2007/high_scalability-2007-07-10-mixi.jp__Architecture.html">5 high scalability-2007-07-10-mixi.jp  Architecture</a></p>
<p>Introduction: Mixi is a fast growing social networking site in Japan. They provide services like: diary, community, message, review, and photo album. Having a lot in common with LiveJournal they also developed many of the same approaches. Their write up on how they scaled their system is easily one of the best out there.
 
Site: http://mixi.jp
  Information Sources     mixi.jp  - scaling out with open source 
 Platform 
    Linux    Apache    MySQL    Perl    Memcached    Squid    Shard 
 What's Inside? 
   They grew to approximately 4 million users in two years and add over 15,000 new users/day.     Ranks 35th on Alexa and 3rd in Japan.    More than 100 MySQL servers    Add more than 10 servers/month    Use non-persistent connections.    Diary traffic is 85% read and 15% write.    Message traffic is is 75% read and 25% write.    Ran into replication performance problems so they had to split the database.    Considered splitting vertically by user or splitting horizontally by table type.    The ende</p><p>6 0.72119719 <a title="481-lsi-6" href="../high_scalability-2010/high_scalability-2010-05-17-7_Lessons_Learned_While_Building_Reddit_to_270_Million_Page_Views_a_Month.html">828 high scalability-2010-05-17-7 Lessons Learned While Building Reddit to 270 Million Page Views a Month</a></p>
<p>7 0.71569991 <a title="481-lsi-7" href="../high_scalability-2013/high_scalability-2013-01-30-Better_Browser_Caching_is_More_Important_than_No_Javascript_or_Fast_Networks_for_HTTP_Performance.html">1396 high scalability-2013-01-30-Better Browser Caching is More Important than No Javascript or Fast Networks for HTTP Performance</a></p>
<p>8 0.71521658 <a title="481-lsi-8" href="../high_scalability-2009/high_scalability-2009-05-17-Scaling_Django_Web_Apps_by_Mike_Malone.html">602 high scalability-2009-05-17-Scaling Django Web Apps by Mike Malone</a></p>
<p>9 0.71254963 <a title="481-lsi-9" href="../high_scalability-2010/high_scalability-2010-05-03-MocoSpace_Architecture_-_3_Billion_Mobile_Page_Views_a_Month.html">821 high scalability-2010-05-03-MocoSpace Architecture - 3 Billion Mobile Page Views a Month</a></p>
<p>10 0.70968342 <a title="481-lsi-10" href="../high_scalability-2009/high_scalability-2009-09-12-How_Google_Taught_Me_to_Cache_and_Cash-In.html">703 high scalability-2009-09-12-How Google Taught Me to Cache and Cash-In</a></p>
<p>11 0.70913333 <a title="481-lsi-11" href="../high_scalability-2013/high_scalability-2013-08-26-Reddit%3A_Lessons_Learned_from_Mistakes_Made_Scaling_to_1_Billion_Pageviews_a_Month.html">1507 high scalability-2013-08-26-Reddit: Lessons Learned from Mistakes Made Scaling to 1 Billion Pageviews a Month</a></p>
<p>12 0.70877117 <a title="481-lsi-12" href="../high_scalability-2010/high_scalability-2010-06-04-Strategy%3A_Cache_Larger_Chunks_-_Cache_Hit_Rate_is_a_Bad_Indicator.html">836 high scalability-2010-06-04-Strategy: Cache Larger Chunks - Cache Hit Rate is a Bad Indicator</a></p>
<p>13 0.69971126 <a title="481-lsi-13" href="../high_scalability-2009/high_scalability-2009-05-08-Eight_Best_Practices_for_Building_Scalable_Systems.html">594 high scalability-2009-05-08-Eight Best Practices for Building Scalable Systems</a></p>
<p>14 0.6941334 <a title="481-lsi-14" href="../high_scalability-2010/high_scalability-2010-05-10-Sify.com_Architecture_-_A_Portal_at_3900_Requests_Per_Second.html">825 high scalability-2010-05-10-Sify.com Architecture - A Portal at 3900 Requests Per Second</a></p>
<p>15 0.6912896 <a title="481-lsi-15" href="../high_scalability-2011/high_scalability-2011-06-03-Stuff_The_Internet_Says_On_Scalability_For_June_3%2C_2011.html">1052 high scalability-2011-06-03-Stuff The Internet Says On Scalability For June 3, 2011</a></p>
<p>16 0.69013155 <a title="481-lsi-16" href="../high_scalability-2014/high_scalability-2014-02-17-How_the_AOL.com_Architecture_Evolved_to_99.999%25_Availability%2C_8_Million_Visitors_Per_Day%2C_and_200%2C000_Requests_Per_Second.html">1597 high scalability-2014-02-17-How the AOL.com Architecture Evolved to 99.999% Availability, 8 Million Visitors Per Day, and 200,000 Requests Per Second</a></p>
<p>17 0.68629295 <a title="481-lsi-17" href="../high_scalability-2007/high_scalability-2007-11-12-Slashdot_Architecture_-_How_the_Old_Man_of_the_Internet_Learned_to_Scale.html">150 high scalability-2007-11-12-Slashdot Architecture - How the Old Man of the Internet Learned to Scale</a></p>
<p>18 0.68473554 <a title="481-lsi-18" href="../high_scalability-2009/high_scalability-2009-02-12-MySpace_Architecture.html">511 high scalability-2009-02-12-MySpace Architecture</a></p>
<p>19 0.68443489 <a title="481-lsi-19" href="../high_scalability-2009/high_scalability-2009-08-18-Real_World_Web%3A_Performance_%26_Scalability.html">684 high scalability-2009-08-18-Real World Web: Performance & Scalability</a></p>
<p>20 0.6838522 <a title="481-lsi-20" href="../high_scalability-2014/high_scalability-2014-05-19-A_Short_On_How_the_Wayback_Machine_Stores_More_Pages_than_Stars_in_the_Milky_Way.html">1650 high scalability-2014-05-19-A Short On How the Wayback Machine Stores More Pages than Stars in the Milky Way</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.142), (2, 0.242), (51, 0.315), (61, 0.068), (79, 0.032), (94, 0.081)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.90084326 <a title="481-lda-1" href="../high_scalability-2012/high_scalability-2012-01-04-How_Facebook_Handled_the_New_Year%27s_Eve_Onslaught.html">1168 high scalability-2012-01-04-How Facebook Handled the New Year's Eve Onslaught</a></p>
<p>Introduction: How does Facebook handle the massive New Year's Eve traffic spike? Thanks to Mike Swift, in  Facebook gets ready for New Year's Eve , we get a little insight as to their method for the madness, nothing really detailed, but still interesting.
  Problem Setup   
 Facebook expects tha one billion+ photos will be shared on New Year's eve. 
 Facebook's 800 million users are scattered around the world. Three quarters live outside the US. Each user is linked to an average of 130 friends. 
 Photos and posts must appear in less than a second. Opening a homepage requires executing requests on a 100 different servers, and those requests have to be ranked, sorted, and privacy-checked, and then rendered. 
 Different events put different stresses on different parts of Facebook.       
 
 Photo and Video Uploads - Holidays require hundreds of terabytes of capacity  
 News Feed - News events like big sports events and the death of Steve Jobs drive user status updates 
 
 
   Coping Strategies   
  Try</p><p>2 0.89407462 <a title="481-lda-2" href="../high_scalability-2010/high_scalability-2010-04-30-Behind_the_scenes_of_an_online_marketplace.html">818 high scalability-2010-04-30-Behind the scenes of an online marketplace</a></p>
<p>Introduction: In a presentation originally held at the 4. O2 Hosting Event in Hamburg, I spoke about the technology at a large online marketplace in Germany called  Hitmeister .
              
Â Some of the topics discussed include:
  
 what makes up a marketplace? technically 
 system principles 
 development patterns 
 tools philosophy 
 data model 
 hardware 
  
I am looking forward to comments and suggestions for both the presentation and our work.</p><p>3 0.87635756 <a title="481-lda-3" href="../high_scalability-2007/high_scalability-2007-09-18-Sync_data_on_all_servers.html">98 high scalability-2007-09-18-Sync data on all servers</a></p>
<p>Introduction: I have a few apache servers ( arround 11 atm ) serving a small amount of data ( arround 44 gigs right now ).     For some time I have been using rsync to keep all the content equal on all servers, but the amount of data has been growing, and rsync takes a few too much time to "compare" all data from source to destination, and create a lot of I/O.     I have been taking a look at MogileFS, it seems a good and reliable option, but as the fuse module is not finished, we should have to rewrite all our apps, and its not an option atm.     Any ideas?     I just want a "real time, non resource-hungry" solution alternative for rsync. If I get more features on the way, then they are welcome :)     Why I prefer to use a Distributed File System instead of using NAS + NFS?     - I need 2 NAS, if I dont want a point of failure, and NAS hard is expensive.   - Non-shared hardware, all server has their own local disks.   - As files are replicated, I can save a lot of money, RAID is not a MUST.     Thn</p><p>same-blog 4 0.84730899 <a title="481-lda-4" href="../high_scalability-2009/high_scalability-2009-01-02-Strategy%3A_Understanding_Your_Data_Leads_to_the_Best_Scalability_Solutions.html">481 high scalability-2009-01-02-Strategy: Understanding Your Data Leads to the Best Scalability Solutions</a></p>
<p>Introduction: In article   Building Super-Scalable Web Systems with REST   Udi Dahan tells an interesting story of how they made a weather reporting system scale for over 10 million users.  So many users hitting their weather database didn't scale. Caching in a straightforward way wouldn't work because weather is obviously local. Caching all local reports would bring  the entire database into memory, which would work for some companies, but wasn't cost efficient for them.   So in typical REST fashion they turned locations into URIs. For example: http://weather.myclient.com/UK/London. This allows the weather information to be cached by intermediaries instead of hitting their servers.  Hopefully for each location their servers will be hit a few times and then the caches will be hit until expiry.
   
 In order to send users directly to the correct location an IP location check is performed on login and stored in a cookie. The lookup is done once and from then on out a GET is performed directly on the r</p><p>5 0.81351197 <a title="481-lda-5" href="../high_scalability-2011/high_scalability-2011-10-28-Stuff_The_Internet_Says_On_Scalability_For_October_28%2C_2011.html">1134 high scalability-2011-10-28-Stuff The Internet Says On Scalability For October 28, 2011</a></p>
<p>Introduction: You deserve a  HighScalability today :
  
  S3: 566 Billion Objects, 370K requests/sec ;  Titan: 38,400-processor, 20-petaflop  
  1,000,000 daily users and no cache . Wooga flash game with 50K DB updates/second, Ruby backend. They hit an IO wall with MySQL at 1000 DB updates/sec. They needed more so they went with Redis. Not quite honest to say no cache was used as everything is RAM, but maybe that's the point. Use a lot of automation. Inactive users are archived. Moved away from EBS.  
  Making dynamic sitesscale like static sites  by Wim Godden. Use Varnish, Nginx, and memcached.  
  The Lifecycle of a Web Page on StumbleUpon  infographic. 2.2 mllion web pages are added to StumbleUpon each month. Nice discussion of  bounce rate .  
 James Hamilton with an excellent overview of the  Storage Infrastructure Behind Facebook Messages . That's 6B+ messages a day. 
  Scaling Twilio . Twilio has scaled traffic by more 100x over the past year, and expanded their server infrastructure from a</p><p>6 0.81347919 <a title="481-lda-6" href="../high_scalability-2009/high_scalability-2009-02-09-Paper%3A_Consensus_Protocols%3A_Two-Phase_Commit__.html">510 high scalability-2009-02-09-Paper: Consensus Protocols: Two-Phase Commit  </a></p>
<p>7 0.80808294 <a title="481-lda-7" href="../high_scalability-2014/high_scalability-2014-05-07-Update_on_Disqus%3A_It%27s_Still_About_Realtime%2C_But_Go_Demolishes_Python.html">1644 high scalability-2014-05-07-Update on Disqus: It's Still About Realtime, But Go Demolishes Python</a></p>
<p>8 0.80563354 <a title="481-lda-8" href="../high_scalability-2009/high_scalability-2009-11-16-Building_Scalable_Systems_Using_Data_as_a_Composite_Material.html">741 high scalability-2009-11-16-Building Scalable Systems Using Data as a Composite Material</a></p>
<p>9 0.8014918 <a title="481-lda-9" href="../high_scalability-2012/high_scalability-2012-06-25-StubHub_Architecture%3A_The_Surprising_Complexity_Behind_the_World%E2%80%99s_Largest_Ticket_Marketplace.html">1271 high scalability-2012-06-25-StubHub Architecture: The Surprising Complexity Behind the World’s Largest Ticket Marketplace</a></p>
<p>10 0.80049914 <a title="481-lda-10" href="../high_scalability-2010/high_scalability-2010-06-08-Sponsored_Post%3A__Jobs%3A_Digg%2C_Huffington_Post_Events%3A__Velocity_Conference%2C_Social_Developer_Summit.html">838 high scalability-2010-06-08-Sponsored Post:  Jobs: Digg, Huffington Post Events:  Velocity Conference, Social Developer Summit</a></p>
<p>11 0.78459239 <a title="481-lda-11" href="../high_scalability-2014/high_scalability-2014-04-10-Paper%3A_Scalable_Atomic_Visibility_with_RAMP_Transactions_-_Scale_Linearly_to_100_Servers.html">1629 high scalability-2014-04-10-Paper: Scalable Atomic Visibility with RAMP Transactions - Scale Linearly to 100 Servers</a></p>
<p>12 0.76633936 <a title="481-lda-12" href="../high_scalability-2007/high_scalability-2007-10-30-Feedblendr_Architecture_-_Using_EC2_to_Scale.html">138 high scalability-2007-10-30-Feedblendr Architecture - Using EC2 to Scale</a></p>
<p>13 0.75966287 <a title="481-lda-13" href="../high_scalability-2010/high_scalability-2010-12-03-GPU_vs_CPU_Smackdown_%3A_The_Rise_of_Throughput-Oriented_Architectures.html">953 high scalability-2010-12-03-GPU vs CPU Smackdown : The Rise of Throughput-Oriented Architectures</a></p>
<p>14 0.75888354 <a title="481-lda-14" href="../high_scalability-2008/high_scalability-2008-04-07-Lazy_web_sites_run_faster.html">298 high scalability-2008-04-07-Lazy web sites run faster</a></p>
<p>15 0.73219204 <a title="481-lda-15" href="../high_scalability-2014/high_scalability-2014-04-28-How_Disqus_Went_Realtime_with_165K_Messages_Per_Second_and_Less_than_.2_Seconds_Latency.html">1638 high scalability-2014-04-28-How Disqus Went Realtime with 165K Messages Per Second and Less than .2 Seconds Latency</a></p>
<p>16 0.71645302 <a title="481-lda-16" href="../high_scalability-2010/high_scalability-2010-06-22-Sponsored_Post%3A__Jobs%3A_Etsy%2C_Digg%2C_Huffington_Post_Event%3A_Velocity_Conference.html">846 high scalability-2010-06-22-Sponsored Post:  Jobs: Etsy, Digg, Huffington Post Event: Velocity Conference</a></p>
<p>17 0.70411474 <a title="481-lda-17" href="../high_scalability-2009/high_scalability-2009-06-10-Paper%3A_Graph_Databases_and_the_Future_of_Large-Scale_Knowledge_Management.html">626 high scalability-2009-06-10-Paper: Graph Databases and the Future of Large-Scale Knowledge Management</a></p>
<p>18 0.69859225 <a title="481-lda-18" href="../high_scalability-2007/high_scalability-2007-08-09-Lots_of_questions_for_high_scalability_-_high_availability.html">63 high scalability-2007-08-09-Lots of questions for high scalability - high availability</a></p>
<p>19 0.69770271 <a title="481-lda-19" href="../high_scalability-2009/high_scalability-2009-05-28-Scaling_PostgreSQL_using_CUDA.html">609 high scalability-2009-05-28-Scaling PostgreSQL using CUDA</a></p>
<p>20 0.69327986 <a title="481-lda-20" href="../high_scalability-2008/high_scalability-2008-07-26-Google%27s_Paxos_Made_Live_%E2%80%93_An_Engineering_Perspective.html">357 high scalability-2008-07-26-Google's Paxos Made Live – An Engineering Perspective</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
