<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>516 high scalability-2009-02-19-Heavy upload server scalability</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2009" href="../home/high_scalability-2009_home.html">high_scalability-2009</a> <a title="high_scalability-2009-516" href="#">high_scalability-2009-516</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>516 high scalability-2009-02-19-Heavy upload server scalability</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2009-516-html" href="http://highscalability.com//blog/2009/2/19/heavy-upload-server-scalability.html">html</a></p><p>Introduction: Hi,We are running a backup solution that uploads every night the files our
clients worked on during the day (Cabonite-like).We have currently about 10GB
of data per night, via http PUT requests (1 per file), and the files are
written as-is on a NAS.Our architecture is basically compound of a load
balancer (hardware, sticky sessions), 5 servers (Tomcat under RHEL4/5, ) and a
NAS (nfs 3).Since our number of clients is rising, (as is our system load) how
would you recommend we could scale our infrastructure? hardware and software?
Should we go towards NAS sharding, more servers, NIO on tomcat...?Thanks for
your inputs!</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Hi,We are running a backup solution that uploads every night the files our clients worked on during the day (Cabonite-like). [sent-1, score-1.344]
</p><p>2 We have currently about 10GB of data per night, via http PUT requests (1 per file), and the files are written as-is on a NAS. [sent-2, score-0.805]
</p><p>3 Our architecture is basically compound of a load balancer (hardware, sticky sessions), 5 servers (Tomcat under RHEL4/5, ) and a NAS (nfs 3). [sent-3, score-1.009]
</p><p>4 Since our number of clients is rising, (as is our system load) how would you recommend we could scale our infrastructure? [sent-4, score-0.606]
</p><p>5 Should we go towards NAS sharding, more servers, NIO on tomcat. [sent-6, score-0.184]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('nas', 0.432), ('compound', 0.294), ('night', 0.294), ('nio', 0.239), ('clients', 0.22), ('rising', 0.212), ('sticky', 0.209), ('files', 0.191), ('uploads', 0.185), ('inputs', 0.179), ('tomcat', 0.179), ('nfs', 0.177), ('recommend', 0.176), ('balancer', 0.14), ('sessions', 0.137), ('thanks', 0.137), ('basically', 0.136), ('towards', 0.132), ('hardware', 0.128), ('sharding', 0.114), ('backup', 0.112), ('worked', 0.108), ('per', 0.1), ('load', 0.096), ('currently', 0.093), ('servers', 0.084), ('via', 0.079), ('written', 0.079), ('requests', 0.075), ('file', 0.072), ('put', 0.072), ('http', 0.066), ('day', 0.066), ('solution', 0.065), ('infrastructure', 0.06), ('running', 0.055), ('number', 0.054), ('go', 0.052), ('architecture', 0.05), ('every', 0.048), ('software', 0.048), ('could', 0.047), ('scale', 0.041), ('would', 0.038), ('system', 0.03), ('data', 0.022)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="516-tfidf-1" href="../high_scalability-2009/high_scalability-2009-02-19-Heavy_upload_server_scalability.html">516 high scalability-2009-02-19-Heavy upload server scalability</a></p>
<p>Introduction: Hi,We are running a backup solution that uploads every night the files our
clients worked on during the day (Cabonite-like).We have currently about 10GB
of data per night, via http PUT requests (1 per file), and the files are
written as-is on a NAS.Our architecture is basically compound of a load
balancer (hardware, sticky sessions), 5 servers (Tomcat under RHEL4/5, ) and a
NAS (nfs 3).Since our number of clients is rising, (as is our system load) how
would you recommend we could scale our infrastructure? hardware and software?
Should we go towards NAS sharding, more servers, NIO on tomcat...?Thanks for
your inputs!</p><p>2 0.24438563 <a title="516-tfidf-2" href="../high_scalability-2007/high_scalability-2007-09-18-Sync_data_on_all_servers.html">98 high scalability-2007-09-18-Sync data on all servers</a></p>
<p>Introduction: I have a few apache servers ( arround 11 atm ) serving a small amount of data
( arround 44 gigs right now ).For some time I have been using rsync to keep
all the content equal on all servers, but the amount of data has been growing,
and rsync takes a few too much time to "compare" all data from source to
destination, and create a lot of I/O.I have been taking a look at MogileFS, it
seems a good and reliable option, but as the fuse module is not finished, we
should have to rewrite all our apps, and its not an option atm.Any ideas?I
just want a "real time, non resource-hungry" solution alternative for rsync.
If I get more features on the way, then they are welcome :)Why I prefer to use
a Distributed File System instead of using NAS + NFS?- I need 2 NAS, if I dont
want a point of failure, and NAS hard is expensive.- Non-shared hardware, all
server has their own local disks.- As files are replicated, I can save a lot
of money, RAID is not a MUST.Thnx in advance for your help and sorry for</p><p>3 0.13358539 <a title="516-tfidf-3" href="../high_scalability-2008/high_scalability-2008-04-22-Simple_NFS_failover_solution_with_symbolic_link%3F.html">308 high scalability-2008-04-22-Simple NFS failover solution with symbolic link?</a></p>
<p>Introduction: I've been trying to find a high availability file storage solution without
success. I tried GlusterFS which looks very promising but experienced problems
with stability and don't want something I can't easily control and rely on.
Other solutions are too complicated or have a SPOF.So I'm thinking of the
following setup:Two NFS servers, a primary and a warm backup. The primary
server will be rsynced with the warm backup every minute or two. I can do it
so frequently as a PHP script will know which directories have changed
recently from a database and only rsync those. Both servers will be NFS
mounted on a cluster of web servers as /mnt/nfs-primary (sym linked as
/home/websites) and /mnt/nfs-backup.I'll then use Ucarp
(http://www.ucarp.org/project/ucarp) to monitor both NFS servers availability
every couple of seconds and when one goes down, the Ucarp up script will be
set to change the symbolic link on all web servers for the /home/websites dir
from /mnt/nfs-primary to /mnt/nfs-backupThe</p><p>4 0.13301542 <a title="516-tfidf-4" href="../high_scalability-2007/high_scalability-2007-10-04-You_Can_Now_Store_All_Your_Stuff_on_Your_Own_Google_Like_File_System.html">112 high scalability-2007-10-04-You Can Now Store All Your Stuff on Your Own Google Like File System</a></p>
<p>Introduction: Update: Parascale's CTO on what's different about Parascale.Let's say you have
gigglebytes of data to store and you aren't sure you want to use aCDN.
Amazon'sS3doesn't excite you. And you aren't quite ready to join
thegridnation. You want to keep it all in house. Wouldn't it be nice to have
something like theGoogle File Systemyou could use to create a unified file
system out of all your disks sitting on all your nodes?According to Robin
Harris, a.k.aStorageMojo(a great blog BTW), you can now have your own
GFS:Parascale launches Google-like storage software.Parascalecalls their
softwate a Virtual Storage Network (VSN). It "aggregates disks across
commodity Linux x86 servers to deliver petabyte-scale file storage. With
features such as automated, transparent file replication and file migration,
Parascale eliminates storage hotspots and delivers massive read/write
bandwidth." Why should you care?I don't know about you, but the "storage
problem" is one the most frustrating parts of buildin</p><p>5 0.13215801 <a title="516-tfidf-5" href="../high_scalability-2008/high_scalability-2008-04-29-High_performance_file_server.html">310 high scalability-2008-04-29-High performance file server</a></p>
<p>Introduction: What have bunch of applications which run on Debian servers, which processes
huge amount of data stored in a shared NFS drive.we have 3 applications
working as a pipeline, which process data stored in the NFS drive. The first
application processes the data and store the output in some folder in the NFS
drive, the second app in the pipeline process the data from the previous step
and so on.The data load to the pipeline is like 1 GBytes per minute. I think
the NFS drive is the bottleneck here.Would buying a specialized file server
improve the performance of data read write from the disk ?</p><p>6 0.12572663 <a title="516-tfidf-6" href="../high_scalability-2013/high_scalability-2013-08-13-In_Memoriam%3A_Lavabit_Architecture_-_Creating_a_Scalable_Email_Service.html">1501 high scalability-2013-08-13-In Memoriam: Lavabit Architecture - Creating a Scalable Email Service</a></p>
<p>7 0.12535483 <a title="516-tfidf-7" href="../high_scalability-2009/high_scalability-2009-08-18-Hardware_Architecture_Example_%28geographical_level_mapping_of_servers%29.html">683 high scalability-2009-08-18-Hardware Architecture Example (geographical level mapping of servers)</a></p>
<p>8 0.11938266 <a title="516-tfidf-8" href="../high_scalability-2008/high_scalability-2008-02-26-Architecture_to_Allow_High_Availability_File_Upload.html">262 high scalability-2008-02-26-Architecture to Allow High Availability File Upload</a></p>
<p>9 0.11880652 <a title="516-tfidf-9" href="../high_scalability-2008/high_scalability-2008-03-18-Shared_filesystem_on_EC2.html">283 high scalability-2008-03-18-Shared filesystem on EC2</a></p>
<p>10 0.11180025 <a title="516-tfidf-10" href="../high_scalability-2008/high_scalability-2008-10-01-Joyent_-_Cloud_Computing_Built_on_Accelerators.html">399 high scalability-2008-10-01-Joyent - Cloud Computing Built on Accelerators</a></p>
<p>11 0.11041577 <a title="516-tfidf-11" href="../high_scalability-2009/high_scalability-2009-06-05-HotPads_Shows_the_True_Cost_of_Hosting_on_Amazon.html">619 high scalability-2009-06-05-HotPads Shows the True Cost of Hosting on Amazon</a></p>
<p>12 0.10588711 <a title="516-tfidf-12" href="../high_scalability-2009/high_scalability-2009-06-05-SSL_RPC_API_Scalability.html">620 high scalability-2009-06-05-SSL RPC API Scalability</a></p>
<p>13 0.10409953 <a title="516-tfidf-13" href="../high_scalability-2007/high_scalability-2007-07-30-Product%3A_GridLayer._Utility_computing_for_online_application.html">42 high scalability-2007-07-30-Product: GridLayer. Utility computing for online application</a></p>
<p>14 0.09856084 <a title="516-tfidf-14" href="../high_scalability-2013/high_scalability-2013-01-14-MongoDB_and_GridFS_for_Inter_and_Intra_Datacenter_Data_Replication_.html">1386 high scalability-2013-01-14-MongoDB and GridFS for Inter and Intra Datacenter Data Replication </a></p>
<p>15 0.096746318 <a title="516-tfidf-15" href="../high_scalability-2008/high_scalability-2008-01-29-Building_scalable_storage_into_application_-_Instead_of_MogileFS_OpenAFS_etc..html">229 high scalability-2008-01-29-Building scalable storage into application - Instead of MogileFS OpenAFS etc.</a></p>
<p>16 0.093517467 <a title="516-tfidf-16" href="../high_scalability-2007/high_scalability-2007-08-09-Lots_of_questions_for_high_scalability_-_high_availability.html">63 high scalability-2007-08-09-Lots of questions for high scalability - high availability</a></p>
<p>17 0.092411645 <a title="516-tfidf-17" href="../high_scalability-2008/high_scalability-2008-05-02-Friends_for_Sale_Architecture_-_A_300_Million_Page_View-Month_Facebook_RoR_App.html">313 high scalability-2008-05-02-Friends for Sale Architecture - A 300 Million Page View-Month Facebook RoR App</a></p>
<p>18 0.091846168 <a title="516-tfidf-18" href="../high_scalability-2013/high_scalability-2013-02-07-Ask_HighScalability%3A_Web_asset_server_concept_-_3rd_party_software_available%3F.html">1402 high scalability-2013-02-07-Ask HighScalability: Web asset server concept - 3rd party software available?</a></p>
<p>19 0.091013581 <a title="516-tfidf-19" href="../high_scalability-2007/high_scalability-2007-10-21-Paper%3A_Standardizing_Storage_Clusters_%28with_pNFS%29.html">128 high scalability-2007-10-21-Paper: Standardizing Storage Clusters (with pNFS)</a></p>
<p>20 0.089463219 <a title="516-tfidf-20" href="../high_scalability-2011/high_scalability-2011-09-27-Use_Instance_Caches_to_Save_Money%3A_Latency_%3D%3D_%24%24%24.html">1126 high scalability-2011-09-27-Use Instance Caches to Save Money: Latency == $$$</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.109), (1, 0.05), (2, -0.036), (3, -0.114), (4, -0.01), (5, -0.033), (6, 0.074), (7, -0.064), (8, 0.006), (9, 0.043), (10, -0.009), (11, -0.03), (12, 0.034), (13, -0.043), (14, 0.027), (15, 0.046), (16, -0.002), (17, 0.046), (18, -0.044), (19, 0.059), (20, 0.018), (21, 0.027), (22, -0.031), (23, -0.024), (24, 0.055), (25, -0.018), (26, 0.052), (27, -0.033), (28, -0.069), (29, 0.016), (30, 0.017), (31, -0.03), (32, -0.021), (33, 0.017), (34, -0.045), (35, 0.047), (36, 0.052), (37, -0.057), (38, 0.013), (39, -0.052), (40, -0.027), (41, -0.03), (42, -0.026), (43, -0.013), (44, -0.017), (45, 0.03), (46, 0.038), (47, 0.011), (48, 0.048), (49, -0.019)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95826846 <a title="516-lsi-1" href="../high_scalability-2009/high_scalability-2009-02-19-Heavy_upload_server_scalability.html">516 high scalability-2009-02-19-Heavy upload server scalability</a></p>
<p>Introduction: Hi,We are running a backup solution that uploads every night the files our
clients worked on during the day (Cabonite-like).We have currently about 10GB
of data per night, via http PUT requests (1 per file), and the files are
written as-is on a NAS.Our architecture is basically compound of a load
balancer (hardware, sticky sessions), 5 servers (Tomcat under RHEL4/5, ) and a
NAS (nfs 3).Since our number of clients is rising, (as is our system load) how
would you recommend we could scale our infrastructure? hardware and software?
Should we go towards NAS sharding, more servers, NIO on tomcat...?Thanks for
your inputs!</p><p>2 0.78663868 <a title="516-lsi-2" href="../high_scalability-2008/high_scalability-2008-01-29-Building_scalable_storage_into_application_-_Instead_of_MogileFS_OpenAFS_etc..html">229 high scalability-2008-01-29-Building scalable storage into application - Instead of MogileFS OpenAFS etc.</a></p>
<p>Introduction: I am planning the scaling of a hosted service, similar to typepad etc. and
would appreciate feedback on my plan so far.Looking into scaling storage, I
have come accross MogileFS and OpenAFS. My concern with these is I am not at
all experienced with them and as the sole tech guy I don't want to build
something into this hosting service that proves complex to update and
adminster.So, I'm thinking of building replication and scalability right into
the application, in a similar but simplified way to how MogileFS works (I
think).So, for our database table of uploaded files, here's how it currently
looks (simplified):fileid (pkey)filenameowneridFor adding the replication and
scalability, I would add a few more
columns:serveroneidservertwoidserverthreeids3At the time the user uploads a
file, it will go to a specific server (managed by the application) and the id
of that server will be placed in the "serverone" column. Then hourly or so, a
cron job will run through the "files" table, and copy</p><p>3 0.7741729 <a title="516-lsi-3" href="../high_scalability-2008/high_scalability-2008-03-18-Shared_filesystem_on_EC2.html">283 high scalability-2008-03-18-Shared filesystem on EC2</a></p>
<p>Introduction: Hi. I'm looking for a way to share files between EC2 nodes. Currently we are
using glusterfs to do this. It has been reliable recently, but in the past it
has crashed under high load and we've had trouble starting it up again. We've
only been able to restart it by removing the files, restarting the cluster,
and filing it up again with our files from backup. This takes ages, and will
take even longer the more files we get.What worries me is that it seems to
make each node a point of failure for the entire system. One node crashes and
soon the entire cluster has crashed. The other problem is adding another node.
It seems like you have to take down the whole thing, reconfigure to include
the new node, and restart. This kind of defeats the horizontal scaling
strategy.We are using 2 EC2 instances as web servers, 1 as a DB master, and 1
as a slave. GlusterFS is installed on the web server machines as well as the
DB slave machine (we backup files to s3 from this machine). The files are
mostly</p><p>4 0.71437895 <a title="516-lsi-4" href="../high_scalability-2008/high_scalability-2008-04-22-Simple_NFS_failover_solution_with_symbolic_link%3F.html">308 high scalability-2008-04-22-Simple NFS failover solution with symbolic link?</a></p>
<p>Introduction: I've been trying to find a high availability file storage solution without
success. I tried GlusterFS which looks very promising but experienced problems
with stability and don't want something I can't easily control and rely on.
Other solutions are too complicated or have a SPOF.So I'm thinking of the
following setup:Two NFS servers, a primary and a warm backup. The primary
server will be rsynced with the warm backup every minute or two. I can do it
so frequently as a PHP script will know which directories have changed
recently from a database and only rsync those. Both servers will be NFS
mounted on a cluster of web servers as /mnt/nfs-primary (sym linked as
/home/websites) and /mnt/nfs-backup.I'll then use Ucarp
(http://www.ucarp.org/project/ucarp) to monitor both NFS servers availability
every couple of seconds and when one goes down, the Ucarp up script will be
set to change the symbolic link on all web servers for the /home/websites dir
from /mnt/nfs-primary to /mnt/nfs-backupThe</p><p>5 0.71055514 <a title="516-lsi-5" href="../high_scalability-2009/high_scalability-2009-05-22-Distributed_content_system_with_bandwidth_balancing.html">605 high scalability-2009-05-22-Distributed content system with bandwidth balancing</a></p>
<p>Introduction: I am looking for a way to distribute files over servers in different physical
locations. My main concern is that I have bandwidth limitations on each
location, and wish to spread the bandwidth load evenly. Atm. I just have 1:1
copies of the files on all servers, and have the application pick a random
server to serve the file as a temp fix...It's a small video streaming service.
I want to spoonfeed the stream to the client with a max bandwidth output, and
support seek. At present I use php to limit the network stream, and read the
file at a given offset sendt as a get parameter from the player for seek. It's
psuedo streaming, but it works.I have been looking at MogileFS, which would
solve the storage part. With MogileFS I can make use of my current php
solution as it supports lighttpd and apache (with mod_rewrite or similar).
However I don't see how I can apply MogileFS to check for bandwidth %
usage?Any reccomendations for how I can solve this?</p><p>6 0.70205384 <a title="516-lsi-6" href="../high_scalability-2009/high_scalability-2009-06-05-SSL_RPC_API_Scalability.html">620 high scalability-2009-06-05-SSL RPC API Scalability</a></p>
<p>7 0.68858814 <a title="516-lsi-7" href="../high_scalability-2007/high_scalability-2007-08-22-Wikimedia_architecture.html">72 high scalability-2007-08-22-Wikimedia architecture</a></p>
<p>8 0.68708611 <a title="516-lsi-8" href="../high_scalability-2007/high_scalability-2007-09-18-Sync_data_on_all_servers.html">98 high scalability-2007-09-18-Sync data on all servers</a></p>
<p>9 0.67212236 <a title="516-lsi-9" href="../high_scalability-2007/high_scalability-2007-10-09-High_Load_on_production_Webservers_after_Sourcecode_sync.html">118 high scalability-2007-10-09-High Load on production Webservers after Sourcecode sync</a></p>
<p>10 0.67080516 <a title="516-lsi-10" href="../high_scalability-2009/high_scalability-2009-06-05-HotPads_Shows_the_True_Cost_of_Hosting_on_Amazon.html">619 high scalability-2009-06-05-HotPads Shows the True Cost of Hosting on Amazon</a></p>
<p>11 0.66626567 <a title="516-lsi-11" href="../high_scalability-2007/high_scalability-2007-11-02-How_WordPress.com_Tracks_300_Servers_Handling_10_Million_Pageviews.html">140 high scalability-2007-11-02-How WordPress.com Tracks 300 Servers Handling 10 Million Pageviews</a></p>
<p>12 0.65746802 <a title="516-lsi-12" href="../high_scalability-2007/high_scalability-2007-09-06-Scaling_IMAP_and_POP3.html">81 high scalability-2007-09-06-Scaling IMAP and POP3</a></p>
<p>13 0.65492278 <a title="516-lsi-13" href="../high_scalability-2007/high_scalability-2007-10-04-Number_of_load_balanced_servers.html">111 high scalability-2007-10-04-Number of load balanced servers</a></p>
<p>14 0.65177041 <a title="516-lsi-14" href="../high_scalability-2007/high_scalability-2007-11-16-Product%3A_lbpool_-_Load_Balancing_JDBC_Pool.html">157 high scalability-2007-11-16-Product: lbpool - Load Balancing JDBC Pool</a></p>
<p>15 0.64884943 <a title="516-lsi-15" href="../high_scalability-2009/high_scalability-2009-04-13-High_Performance_Web_Pages_%E2%80%93_Real_World_Examples%3A_Netflix_Case_Study.html">566 high scalability-2009-04-13-High Performance Web Pages – Real World Examples: Netflix Case Study</a></p>
<p>16 0.64685231 <a title="516-lsi-16" href="../high_scalability-2014/high_scalability-2014-02-10-13_Simple_Tricks_for_Scaling_Python_and_Django_with_Apache_from_HackerEarth.html">1593 high scalability-2014-02-10-13 Simple Tricks for Scaling Python and Django with Apache from HackerEarth</a></p>
<p>17 0.64170921 <a title="516-lsi-17" href="../high_scalability-2012/high_scalability-2012-06-07-Case_Study_on_Scaling_PaaS_infrastructure_.html">1260 high scalability-2012-06-07-Case Study on Scaling PaaS infrastructure </a></p>
<p>18 0.63940287 <a title="516-lsi-18" href="../high_scalability-2008/high_scalability-2008-02-18-How_to_deal_with_an_I-O_bottleneck_to_disk%3F.html">251 high scalability-2008-02-18-How to deal with an I-O bottleneck to disk?</a></p>
<p>19 0.63789934 <a title="516-lsi-19" href="../high_scalability-2010/high_scalability-2010-08-23-6_Ways_to_Kill_Your_Servers_-__Learning_How_to_Scale_the_Hard_Way.html">884 high scalability-2010-08-23-6 Ways to Kill Your Servers -  Learning How to Scale the Hard Way</a></p>
<p>20 0.63723725 <a title="516-lsi-20" href="../high_scalability-2013/high_scalability-2013-09-23-Salesforce_Architecture_-_How_they_Handle_1.3_Billion_Transactions_a_Day.html">1521 high scalability-2013-09-23-Salesforce Architecture - How they Handle 1.3 Billion Transactions a Day</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.049), (2, 0.303), (20, 0.207), (30, 0.086), (61, 0.173), (79, 0.04)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.92137355 <a title="516-lda-1" href="../high_scalability-2009/high_scalability-2009-02-19-Heavy_upload_server_scalability.html">516 high scalability-2009-02-19-Heavy upload server scalability</a></p>
<p>Introduction: Hi,We are running a backup solution that uploads every night the files our
clients worked on during the day (Cabonite-like).We have currently about 10GB
of data per night, via http PUT requests (1 per file), and the files are
written as-is on a NAS.Our architecture is basically compound of a load
balancer (hardware, sticky sessions), 5 servers (Tomcat under RHEL4/5, ) and a
NAS (nfs 3).Since our number of clients is rising, (as is our system load) how
would you recommend we could scale our infrastructure? hardware and software?
Should we go towards NAS sharding, more servers, NIO on tomcat...?Thanks for
your inputs!</p><p>2 0.85271966 <a title="516-lda-2" href="../high_scalability-2013/high_scalability-2013-12-18-How_to_get_started_with_sizing_and_capacity_planning%2C_assuming_you_don%27t_know_the_software_behavior%3F.html">1566 high scalability-2013-12-18-How to get started with sizing and capacity planning, assuming you don't know the software behavior?</a></p>
<p>Introduction: Here's a common situation and question from themechanical-sympathyGoogle group
by Avinash Agrawal on the black art of capacity planning:How to get started
with sizing and capacity planning, assuming we don't know the software
behavior and its completely new product to deal with?Gil Tene, Vice President
of Technology and CTO & Co-Founder, wrote a very understandable and useful
answer that is worth highlighting:Start with requirements. I see way too many
"capacity planning" exercises that go off spending weeks measuring some
irrelevant metrics about a system (like how many widgets per hour can this
thing do) without knowing what they actually need it to do.There are two key
sets of metrics to state here: the "how much" set and the "how bad" set:In the
"How Much" part, you need to establish, based on expected business needs,
Numbers for things (like connections, users, streams, transactions or messages
per second) that you expect to interact with at the peak time of normal
operations, and</p><p>3 0.83963615 <a title="516-lda-3" href="../high_scalability-2009/high_scalability-2009-10-02-HighScalability_has_Moved_to_Squarespace.com%21_.html">714 high scalability-2009-10-02-HighScalability has Moved to Squarespace.com! </a></p>
<p>Introduction: You may have noticed something is a little a different when visiting
HighScalability today: We've Moved! HighScalability.com has switched hosting
services to Squarespace.com. House warming gifts are completely unnecessary.
Thanks for the thought though.It's been a long long long process. Importing a
largish Drupal site to Wordpress and then into Squarespace is a bit like
dental work without the happy juice, but the results are worth it. While the
site is missing a few features I think it looks nicer, feels faster, and I'm
betting it will be more scalable and more reliable. All good things.I'll
explain more about the move later in this post, but there's some admistrivia
that needs to be handled to make the move complete:If you have a user account
and have posted on HighScalability before then you have a user account, but
since I don't know your passwords I had to make new passwords up for you. So
pleasecontact me and I'll give you your password so you can login and change
it. Then you c</p><p>4 0.83605975 <a title="516-lda-4" href="../high_scalability-2009/high_scalability-2009-08-20-Dependency_Injection_and_AOP_frameworks_for_.NET_.html">685 high scalability-2009-08-20-Dependency Injection and AOP frameworks for .NET </a></p>
<p>Introduction: We're looking to implement a framework to do Dependency Injection and AOP for
a new solution we're working on. It will likely get hit pretty hard, so we'd
like to chose a framework that's proven to scale well, and operates well under
pressure.Right now, we're looking closely at Spring.NET, Castle Project's
Windsor framework, and Unity. Does anyone have any feedback on implementing
any of these in large, high traffic environments?</p><p>5 0.82181865 <a title="516-lda-5" href="../high_scalability-2010/high_scalability-2010-02-05-High_Availability_Principle_%3A_Concurrency_Control.html">772 high scalability-2010-02-05-High Availability Principle : Concurrency Control</a></p>
<p>Introduction: One important high availability principle is concurrency control.  The idea is
to allow only that much traffic through to your system which your system can
handle successfully.  For example: if your system is certified to handle a
concurrency of 100 then the 101st request should either timeout, be asked to
try later  or wait until one of the previous 100 requests finish.  The 101st
request should not be allowed to negatively impact the experience of the other
100 users.  Only the 101st request should be impacted.Read more here...</p><p>6 0.82038379 <a title="516-lda-6" href="../high_scalability-2011/high_scalability-2011-10-31-15_Ways_to_Make_Your_Application_Feel_More_Responsive_under_Google_App_Engine.html">1135 high scalability-2011-10-31-15 Ways to Make Your Application Feel More Responsive under Google App Engine</a></p>
<p>7 0.81985962 <a title="516-lda-7" href="../high_scalability-2008/high_scalability-2008-06-08-Search_fast_in_million_rows.html">342 high scalability-2008-06-08-Search fast in million rows</a></p>
<p>8 0.81941634 <a title="516-lda-8" href="../high_scalability-2007/high_scalability-2007-07-24-Major_Websites_Down%3A_Or_Why_You_Want_to_Run_in_Two_or_More_Data_Centers..html">23 high scalability-2007-07-24-Major Websites Down: Or Why You Want to Run in Two or More Data Centers.</a></p>
<p>9 0.81915671 <a title="516-lda-9" href="../high_scalability-2009/high_scalability-2009-11-25-Brian_Aker%27s_Hilarious_NoSQL_Stand_Up_Routine.html">745 high scalability-2009-11-25-Brian Aker's Hilarious NoSQL Stand Up Routine</a></p>
<p>10 0.81752372 <a title="516-lda-10" href="../high_scalability-2010/high_scalability-2010-12-21-SQL_%2B_NoSQL_%3D_Yes_%21.html">961 high scalability-2010-12-21-SQL + NoSQL = Yes !</a></p>
<p>11 0.81590688 <a title="516-lda-11" href="../high_scalability-2010/high_scalability-2010-05-10-Sify.com_Architecture_-_A_Portal_at_3900_Requests_Per_Second.html">825 high scalability-2010-05-10-Sify.com Architecture - A Portal at 3900 Requests Per Second</a></p>
<p>12 0.81486923 <a title="516-lda-12" href="../high_scalability-2010/high_scalability-2010-10-08-4_Scalability_Themes_from_Surgecon.html">917 high scalability-2010-10-08-4 Scalability Themes from Surgecon</a></p>
<p>13 0.81328362 <a title="516-lda-13" href="../high_scalability-2008/high_scalability-2008-02-18-limit_on_the_number_of_databases_open.html">252 high scalability-2008-02-18-limit on the number of databases open</a></p>
<p>14 0.81219655 <a title="516-lda-14" href="../high_scalability-2009/high_scalability-2009-09-12-How_Google_Taught_Me_to_Cache_and_Cash-In.html">703 high scalability-2009-09-12-How Google Taught Me to Cache and Cash-In</a></p>
<p>15 0.80978072 <a title="516-lda-15" href="../high_scalability-2011/high_scalability-2011-07-06-11_Common_Web_Use_Cases_Solved_in_Redis.html">1074 high scalability-2011-07-06-11 Common Web Use Cases Solved in Redis</a></p>
<p>16 0.80716431 <a title="516-lda-16" href="../high_scalability-2011/high_scalability-2011-11-14-Using_Gossip_Protocols_for_Failure_Detection%2C_Monitoring%2C_Messaging_and_Other_Good_Things.html">1142 high scalability-2011-11-14-Using Gossip Protocols for Failure Detection, Monitoring, Messaging and Other Good Things</a></p>
<p>17 0.80660462 <a title="516-lda-17" href="../high_scalability-2012/high_scalability-2012-10-10-Antirez%3A_You_Need_to_Think_in_Terms_of_Organizing_Your_Data_for_Fetching.html">1337 high scalability-2012-10-10-Antirez: You Need to Think in Terms of Organizing Your Data for Fetching</a></p>
<p>18 0.80651134 <a title="516-lda-18" href="../high_scalability-2011/high_scalability-2011-11-07-10_Core_Architecture_Pattern_Variations_for_Achieving_Scalability.html">1138 high scalability-2011-11-07-10 Core Architecture Pattern Variations for Achieving Scalability</a></p>
<p>19 0.80638188 <a title="516-lda-19" href="../high_scalability-2010/high_scalability-2010-11-16-Facebook%27s_New_Real-time_Messaging_System%3A_HBase_to_Store_135%2B_Billion_Messages_a_Month.html">943 high scalability-2010-11-16-Facebook's New Real-time Messaging System: HBase to Store 135+ Billion Messages a Month</a></p>
<p>20 0.80443454 <a title="516-lda-20" href="../high_scalability-2014/high_scalability-2014-03-19-Strategy%3A_Three_Techniques_to_Survive_Traffic_Surges_by_Quickly_Scaling_Your_Site.html">1615 high scalability-2014-03-19-Strategy: Three Techniques to Survive Traffic Surges by Quickly Scaling Your Site</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
