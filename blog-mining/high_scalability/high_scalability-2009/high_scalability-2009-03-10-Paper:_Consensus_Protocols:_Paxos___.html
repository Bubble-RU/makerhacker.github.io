<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>529 high scalability-2009-03-10-Paper: Consensus Protocols: Paxos   </title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2009" href="../home/high_scalability-2009_home.html">high_scalability-2009</a> <a title="high_scalability-2009-529" href="#">high_scalability-2009-529</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>529 high scalability-2009-03-10-Paper: Consensus Protocols: Paxos   </h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2009-529-html" href="http://highscalability.com//blog/2009/3/10/paper-consensus-protocols-paxos.html">html</a></p><p>Introduction: Update:   Barbara Liskov’s Turing Award, and Byzantine Fault Tolerance .  Henry Robinson has created an excellent series of articles on consensus protocols. We already covered his  2 Phase Commit  article and he also has a  3 Phase Commit  article showing how to handle 2PC under single node failures.  But that is not enough!  3PC works well under node failures, but fails for network failures. So another consensus mechanism is needed that handles both network and node failures. And that's  Paxos .  Paxos correctly handles both types of failures, but it does this by becoming inaccessible if too many components fail. This is the "liveness" property of protocols. Paxos waits until the faults are fixed. Read queries can be handled, but updates will be blocked until the protocol thinks it can make forward progress.   The liveness of Paxos is primarily dependent on network stability. In a distributed heterogeneous environment you are at risk of losing the ability to make updates. Users hate t</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Henry Robinson has created an excellent series of articles on consensus protocols. [sent-2, score-0.367]
</p><p>2 We already covered his  2 Phase Commit  article and he also has a  3 Phase Commit  article showing how to handle 2PC under single node failures. [sent-3, score-0.296]
</p><p>3 3PC works well under node failures, but fails for network failures. [sent-5, score-0.276]
</p><p>4 So another consensus mechanism is needed that handles both network and node failures. [sent-6, score-0.745]
</p><p>5 Paxos correctly handles both types of failures, but it does this by becoming inaccessible if too many components fail. [sent-8, score-0.193]
</p><p>6 Read queries can be handled, but updates will be blocked until the protocol thinks it can make forward progress. [sent-11, score-0.239]
</p><p>7 The liveness of Paxos is primarily dependent on network stability. [sent-12, score-0.481]
</p><p>8 In a distributed heterogeneous environment you are at risk of losing the ability to make updates. [sent-13, score-0.309]
</p><p>9 So when companies like Amazon do the seemingly insane thing of creating  eventually consistent databases , it should be a little easier to understand now. [sent-15, score-0.201]
</p><p>10 Not being able to write under partition failures is unacceptable. [sent-18, score-0.17]
</p><p>11 Therefor create a system that can always write and work on consistency when all the downed partitions/networks are repaired. [sent-19, score-0.126]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('consensus', 0.367), ('commitarticle', 0.322), ('paxos', 0.27), ('liveness', 0.261), ('failures', 0.17), ('liskov', 0.146), ('paxosby', 0.146), ('phase', 0.14), ('therefor', 0.137), ('lynch', 0.137), ('amir', 0.137), ('byzantine', 0.137), ('impossibility', 0.137), ('node', 0.133), ('robinson', 0.131), ('downed', 0.126), ('articlesgoogle', 0.119), ('turing', 0.119), ('barbara', 0.119), ('partitioning', 0.116), ('ken', 0.116), ('faulty', 0.116), ('waits', 0.111), ('handles', 0.11), ('nasty', 0.109), ('et', 0.102), ('seemingly', 0.102), ('blocked', 0.101), ('award', 0.099), ('insane', 0.099), ('hate', 0.096), ('jonathan', 0.096), ('heterogeneous', 0.093), ('covered', 0.092), ('faults', 0.089), ('correctly', 0.083), ('losing', 0.082), ('coordination', 0.082), ('dependent', 0.082), ('property', 0.075), ('fails', 0.073), ('thinks', 0.072), ('showing', 0.071), ('network', 0.07), ('brings', 0.068), ('primarily', 0.068), ('risk', 0.067), ('distributed', 0.067), ('forward', 0.066), ('mechanism', 0.065)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="529-tfidf-1" href="../high_scalability-2009/high_scalability-2009-03-10-Paper%3A_Consensus_Protocols%3A_Paxos___.html">529 high scalability-2009-03-10-Paper: Consensus Protocols: Paxos   </a></p>
<p>Introduction: Update:   Barbara Liskov’s Turing Award, and Byzantine Fault Tolerance .  Henry Robinson has created an excellent series of articles on consensus protocols. We already covered his  2 Phase Commit  article and he also has a  3 Phase Commit  article showing how to handle 2PC under single node failures.  But that is not enough!  3PC works well under node failures, but fails for network failures. So another consensus mechanism is needed that handles both network and node failures. And that's  Paxos .  Paxos correctly handles both types of failures, but it does this by becoming inaccessible if too many components fail. This is the "liveness" property of protocols. Paxos waits until the faults are fixed. Read queries can be handled, but updates will be blocked until the protocol thinks it can make forward progress.   The liveness of Paxos is primarily dependent on network stability. In a distributed heterogeneous environment you are at risk of losing the ability to make updates. Users hate t</p><p>2 0.28863993 <a title="529-tfidf-2" href="../high_scalability-2009/high_scalability-2009-02-09-Paper%3A_Consensus_Protocols%3A_Two-Phase_Commit__.html">510 high scalability-2009-02-09-Paper: Consensus Protocols: Two-Phase Commit  </a></p>
<p>Introduction: Henry Robinson has created an excellent series of articles on consensus protocols. Henry starts with a very useful discussion of what all this talk about consensus really means:  The consensus problem is the problem of getting a set of nodes in a distributed system to agree on something - it might be a value, a course of action or a decision. Achieving consensus allows a distributed system to act as a single entity, with every individual node aware of and in agreement with the actions of the whole of the network.   In this article Henry tackles Two-Phase Commit, the protocol most databases use to arrive at a consensus for database writes. The article is very well written with lots of pretty and informative pictures. He did a really good job.  In conclusion we learn 2PC is very efficient, a minimal number of messages are exchanged and latency is low. The problem is when a co-ordinator fails availability is dramatically reduced. This is why 2PC isn't generally used on highly distributed</p><p>3 0.21139561 <a title="529-tfidf-3" href="../high_scalability-2008/high_scalability-2008-07-26-Google%27s_Paxos_Made_Live_%E2%80%93_An_Engineering_Perspective.html">357 high scalability-2008-07-26-Google's Paxos Made Live – An Engineering Perspective</a></p>
<p>Introduction: This is an unusually well written and  useful paper . It talks in detail about experiences implementing a complex project, something we don't see very often. They shockingly even admit that creating a working implementation of Paxos was more difficult than just translating the pseudo code. Imagine that, programmers aren't merely typists! I particularly like the explanation of the Paxos algorithm and why anyone would care about it, working with disk corruption, using leases to support simultaneous reads, using epoch numbers to indicate a new master election, using snapshots to prevent unbounded logs, using MultiOp to implement database transactions, how they tested the system, and their openness with the various problems they had. A lot to learn here.  From the paper:  We describe our experience building a fault-tolerant data-base using the Paxos consensus algorithm. Despite the existing literature in the field, building such a database proved to be non-trivial. We describe selected alg</p><p>4 0.19635284 <a title="529-tfidf-4" href="../high_scalability-2012/high_scalability-2012-05-10-Paper%3A_Paxos_Made_Moderately_Complex.html">1243 high scalability-2012-05-10-Paper: Paxos Made Moderately Complex</a></p>
<p>Introduction: If you are a normal human being and find the  Paxos protocol  confusing, then this paper,  Paxos Made Moderately Complex , is a great find. Robbert van Renesse from Cornell University has written a clear and well written paper with excellent explanations.
 
The Abstract:
  For anybody who has ever tried to implement it, Paxos is by no means a simple protocol, even though it is based on relatively simple invariants. This paper provides imperative pseudo-code for the full Paxos (or Multi-Paxos) protocol without shying away from discussing various implementation details. The initial description avoids optimizations that complicate comprehension. Next we discuss liveness, and list various optimizations that make the protocol practical.   Related Articles   
  Paxos on HighScalability.com</p><p>5 0.14283493 <a title="529-tfidf-5" href="../high_scalability-2012/high_scalability-2012-12-18-Georeplication%3A_When_Bad_Things_Happen_to_Good_Systems.html">1374 high scalability-2012-12-18-Georeplication: When Bad Things Happen to Good Systems</a></p>
<p>Introduction: Georeplication is one of the standard techniques for dealing when bad things--failure and latency--happen to good systems. The problem is always: how do you do that?  Murat Demirbas , Associate Professor at SUNY Buffalo, has a couple of really good posts that can help:  MDCC: Multi-Data Center Consistency  and  Making Geo-Replicated Systems Fast as Possible, Consistent when Necessary . 
 
In  MDCC: Multi-Data Center Consistency  Murat discusses a paper that says synchronous wide-area replication can be feasible. There's a quick and clear explanation of Paxos and various optimizations that is worth the price of admission. We find that strong consistency doesn't have to be lost across a WAN:
  

The good thing about using Paxos over the WAN is you /almost/ get the full CAP  (all three properties: consistency, availability, and partition-freedom). As we discussed earlier (Paxos taught), Paxos is CP, that is, in the presence of a partition, Paxos keeps consistency over availability. But, P</p><p>6 0.13794927 <a title="529-tfidf-6" href="../high_scalability-2009/high_scalability-2009-08-24-How_Google_Serves_Data_from_Multiple_Datacenters.html">687 high scalability-2009-08-24-How Google Serves Data from Multiple Datacenters</a></p>
<p>7 0.1333138 <a title="529-tfidf-7" href="../high_scalability-2011/high_scalability-2011-11-14-Using_Gossip_Protocols_for_Failure_Detection%2C_Monitoring%2C_Messaging_and_Other_Good_Things.html">1142 high scalability-2011-11-14-Using Gossip Protocols for Failure Detection, Monitoring, Messaging and Other Good Things</a></p>
<p>8 0.10873055 <a title="529-tfidf-8" href="../high_scalability-2012/high_scalability-2012-10-22-Spanner_-_It%27s_About_Programmers_Building_Apps_Using_SQL_Semantics_at_NoSQL_Scale.html">1345 high scalability-2012-10-22-Spanner - It's About Programmers Building Apps Using SQL Semantics at NoSQL Scale</a></p>
<p>9 0.10402787 <a title="529-tfidf-9" href="../high_scalability-2014/high_scalability-2014-03-03-The_%E2%80%9CFour_Hamiltons%E2%80%9D_Framework_for_Mitigating_Faults_in_the_Cloud%3A_Avoid_it%2C_Mask_it%2C_Bound_it%2C_Fix_it_Fast.html">1604 high scalability-2014-03-03-The “Four Hamiltons” Framework for Mitigating Faults in the Cloud: Avoid it, Mask it, Bound it, Fix it Fast</a></p>
<p>10 0.094537005 <a title="529-tfidf-10" href="../high_scalability-2013/high_scalability-2013-05-03-Stuff_The_Internet_Says_On_Scalability_For_May_3%2C_2013.html">1451 high scalability-2013-05-03-Stuff The Internet Says On Scalability For May 3, 2013</a></p>
<p>11 0.088677421 <a title="529-tfidf-11" href="../high_scalability-2013/high_scalability-2013-03-08-Stuff_The_Internet_Says_On_Scalability_For_March_8%2C_2013.html">1420 high scalability-2013-03-08-Stuff The Internet Says On Scalability For March 8, 2013</a></p>
<p>12 0.086119659 <a title="529-tfidf-12" href="../high_scalability-2009/high_scalability-2009-09-20-PaxosLease%3A_Diskless_Paxos_for_Leases.html">710 high scalability-2009-09-20-PaxosLease: Diskless Paxos for Leases</a></p>
<p>13 0.077046864 <a title="529-tfidf-13" href="../high_scalability-2009/high_scalability-2009-05-05-Drop_ACID_and_Think_About_Data.html">589 high scalability-2009-05-05-Drop ACID and Think About Data</a></p>
<p>14 0.072703473 <a title="529-tfidf-14" href="../high_scalability-2013/high_scalability-2013-10-08-F1_and_Spanner_Holistically_Compared.html">1529 high scalability-2013-10-08-F1 and Spanner Holistically Compared</a></p>
<p>15 0.071013644 <a title="529-tfidf-15" href="../high_scalability-2013/high_scalability-2013-10-04-Stuff_The_Internet_Says_On_Scalability_For_October_4th%2C_2013.html">1527 high scalability-2013-10-04-Stuff The Internet Says On Scalability For October 4th, 2013</a></p>
<p>16 0.070672087 <a title="529-tfidf-16" href="../high_scalability-2013/high_scalability-2013-05-16-Paper%3A_Warp%3A_Multi-Key_Transactions_for_Key-Value_Stores.html">1459 high scalability-2013-05-16-Paper: Warp: Multi-Key Transactions for Key-Value Stores</a></p>
<p>17 0.069570892 <a title="529-tfidf-17" href="../high_scalability-2010/high_scalability-2010-10-15-Troubles_with_Sharding_-_What_can_we_learn_from_the_Foursquare_Incident%3F.html">920 high scalability-2010-10-15-Troubles with Sharding - What can we learn from the Foursquare Incident?</a></p>
<p>18 0.067503311 <a title="529-tfidf-18" href="../high_scalability-2013/high_scalability-2013-08-07-RAFT_-_In_Search_of_an_Understandable_Consensus_Algorithm.html">1498 high scalability-2013-08-07-RAFT - In Search of an Understandable Consensus Algorithm</a></p>
<p>19 0.062498122 <a title="529-tfidf-19" href="../high_scalability-2011/high_scalability-2011-05-27-Stuff_The_Internet_Says_On_Scalability_For_May_27%2C_2011.html">1048 high scalability-2011-05-27-Stuff The Internet Says On Scalability For May 27, 2011</a></p>
<p>20 0.061393488 <a title="529-tfidf-20" href="../high_scalability-2011/high_scalability-2011-12-30-Stuff_The_Internet_Says_On_Scalability_For_December_30%2C_2011.html">1166 high scalability-2011-12-30-Stuff The Internet Says On Scalability For December 30, 2011</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.092), (1, 0.061), (2, -0.004), (3, 0.038), (4, 0.011), (5, 0.055), (6, 0.01), (7, -0.02), (8, -0.052), (9, -0.032), (10, 0.009), (11, 0.031), (12, -0.061), (13, -0.038), (14, 0.053), (15, 0.036), (16, 0.036), (17, -0.022), (18, -0.0), (19, -0.023), (20, 0.071), (21, 0.053), (22, -0.029), (23, 0.024), (24, -0.092), (25, -0.007), (26, 0.069), (27, 0.028), (28, 0.003), (29, -0.007), (30, 0.003), (31, -0.024), (32, -0.065), (33, -0.01), (34, 0.013), (35, -0.061), (36, -0.019), (37, -0.005), (38, 0.01), (39, -0.008), (40, -0.035), (41, -0.028), (42, 0.008), (43, -0.012), (44, -0.034), (45, 0.019), (46, 0.039), (47, 0.048), (48, -0.045), (49, -0.039)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95958668 <a title="529-lsi-1" href="../high_scalability-2009/high_scalability-2009-03-10-Paper%3A_Consensus_Protocols%3A_Paxos___.html">529 high scalability-2009-03-10-Paper: Consensus Protocols: Paxos   </a></p>
<p>Introduction: Update:   Barbara Liskov’s Turing Award, and Byzantine Fault Tolerance .  Henry Robinson has created an excellent series of articles on consensus protocols. We already covered his  2 Phase Commit  article and he also has a  3 Phase Commit  article showing how to handle 2PC under single node failures.  But that is not enough!  3PC works well under node failures, but fails for network failures. So another consensus mechanism is needed that handles both network and node failures. And that's  Paxos .  Paxos correctly handles both types of failures, but it does this by becoming inaccessible if too many components fail. This is the "liveness" property of protocols. Paxos waits until the faults are fixed. Read queries can be handled, but updates will be blocked until the protocol thinks it can make forward progress.   The liveness of Paxos is primarily dependent on network stability. In a distributed heterogeneous environment you are at risk of losing the ability to make updates. Users hate t</p><p>2 0.82736075 <a title="529-lsi-2" href="../high_scalability-2009/high_scalability-2009-02-09-Paper%3A_Consensus_Protocols%3A_Two-Phase_Commit__.html">510 high scalability-2009-02-09-Paper: Consensus Protocols: Two-Phase Commit  </a></p>
<p>Introduction: Henry Robinson has created an excellent series of articles on consensus protocols. Henry starts with a very useful discussion of what all this talk about consensus really means:  The consensus problem is the problem of getting a set of nodes in a distributed system to agree on something - it might be a value, a course of action or a decision. Achieving consensus allows a distributed system to act as a single entity, with every individual node aware of and in agreement with the actions of the whole of the network.   In this article Henry tackles Two-Phase Commit, the protocol most databases use to arrive at a consensus for database writes. The article is very well written with lots of pretty and informative pictures. He did a really good job.  In conclusion we learn 2PC is very efficient, a minimal number of messages are exchanged and latency is low. The problem is when a co-ordinator fails availability is dramatically reduced. This is why 2PC isn't generally used on highly distributed</p><p>3 0.78059775 <a title="529-lsi-3" href="../high_scalability-2010/high_scalability-2010-09-01-Paper%3A_The_Case_for_Determinism_in_Database_Systems__.html">890 high scalability-2010-09-01-Paper: The Case for Determinism in Database Systems  </a></p>
<p>Introduction: Can you have your ACID cake and eat your distributed database too? Yes explains Daniel Abadi, Assistant Professor of Computer Science at Yale University, in an epic post,  The problems with ACID, and how to fix them without going NoSQL , coauthored with  Alexander Thomson , on their paper  The Case for Determinism in Database Systems . We've already seen  VoltDB  offer the best of both worlds, this sounds like a completely different approach.
 
The solution, they propose, is: 
  

  ...an architecture and execution model that avoids deadlock, copes with failures without aborting transactions, and achieves high concurrency. The paper contains full details, but the basic idea is to use ordered locking coupled with optimistic lock location prediction, while exploiting deterministic systems' nice replication properties in the case of failures.  

  
  The problem they are trying to solve is:  
  

    In our opinion, the NoSQL decision to give up on ACID is the lazy solution to these scala</p><p>4 0.77364677 <a title="529-lsi-4" href="../high_scalability-2012/high_scalability-2012-12-18-Georeplication%3A_When_Bad_Things_Happen_to_Good_Systems.html">1374 high scalability-2012-12-18-Georeplication: When Bad Things Happen to Good Systems</a></p>
<p>Introduction: Georeplication is one of the standard techniques for dealing when bad things--failure and latency--happen to good systems. The problem is always: how do you do that?  Murat Demirbas , Associate Professor at SUNY Buffalo, has a couple of really good posts that can help:  MDCC: Multi-Data Center Consistency  and  Making Geo-Replicated Systems Fast as Possible, Consistent when Necessary . 
 
In  MDCC: Multi-Data Center Consistency  Murat discusses a paper that says synchronous wide-area replication can be feasible. There's a quick and clear explanation of Paxos and various optimizations that is worth the price of admission. We find that strong consistency doesn't have to be lost across a WAN:
  

The good thing about using Paxos over the WAN is you /almost/ get the full CAP  (all three properties: consistency, availability, and partition-freedom). As we discussed earlier (Paxos taught), Paxos is CP, that is, in the presence of a partition, Paxos keeps consistency over availability. But, P</p><p>5 0.74510258 <a title="529-lsi-5" href="../high_scalability-2011/high_scalability-2011-11-23-Paper%3A_Don%E2%80%99t_Settle_for_Eventual%3A_Scalable_Causal_Consistency_for_Wide-Area_Storage_with_COPS.html">1146 high scalability-2011-11-23-Paper: Don’t Settle for Eventual: Scalable Causal Consistency for Wide-Area Storage with COPS</a></p>
<p>Introduction: Teams from  Princeton  and CMU are  working together  to solve one of the most difficult problems in the repertoire: scalable geo-distributed data stores. Major companies like Google and Facebook have been working on multiple datacenter database functionality for some time, but there's still a general lack of available systems that work for complex data scenarios.
 
The ideas in this paper-- Don’t Settle for Eventual: Scalable Causal Consistency for Wide-Area Storage with COPS --are different. It's not another eventually consistent system, or a traditional transaction oriented system, or a replication based system, or a system that punts on the issue. It's something new, a causally consistent system that achieves  ALPS  system properties. Move over CAP, NoSQL, etc, we have another acronym: ALPS - Available (operations always complete successfully), Low-latency (operations complete quickly (single digit milliseconds)), Partition-tolerant (operates with a partition), and Scalable (just a</p><p>6 0.74131292 <a title="529-lsi-6" href="../high_scalability-2013/high_scalability-2013-05-23-Paper%3A_Calvin%3A_Fast_Distributed_Transactions_for_Partitioned_Database_Systems.html">1463 high scalability-2013-05-23-Paper: Calvin: Fast Distributed Transactions for Partitioned Database Systems</a></p>
<p>7 0.73994458 <a title="529-lsi-7" href="../high_scalability-2009/high_scalability-2009-06-10-Managing_cross_partition_transactions_in_a_distributed_KV_system.html">625 high scalability-2009-06-10-Managing cross partition transactions in a distributed KV system</a></p>
<p>8 0.73673189 <a title="529-lsi-8" href="../high_scalability-2013/high_scalability-2013-05-01-Myth%3A_Eric_Brewer_on_Why_Banks_are_BASE_Not_ACID_-_Availability_Is_Revenue_.html">1450 high scalability-2013-05-01-Myth: Eric Brewer on Why Banks are BASE Not ACID - Availability Is Revenue </a></p>
<p>9 0.73072743 <a title="529-lsi-9" href="../high_scalability-2009/high_scalability-2009-09-16-Paper%3A_A_practical_scalable_distributed_B-tree.html">705 high scalability-2009-09-16-Paper: A practical scalable distributed B-tree</a></p>
<p>10 0.72245979 <a title="529-lsi-10" href="../high_scalability-2013/high_scalability-2013-05-16-Paper%3A_Warp%3A_Multi-Key_Transactions_for_Key-Value_Stores.html">1459 high scalability-2013-05-16-Paper: Warp: Multi-Key Transactions for Key-Value Stores</a></p>
<p>11 0.71465015 <a title="529-lsi-11" href="../high_scalability-2011/high_scalability-2011-11-14-Using_Gossip_Protocols_for_Failure_Detection%2C_Monitoring%2C_Messaging_and_Other_Good_Things.html">1142 high scalability-2011-11-14-Using Gossip Protocols for Failure Detection, Monitoring, Messaging and Other Good Things</a></p>
<p>12 0.71076691 <a title="529-lsi-12" href="../high_scalability-2009/high_scalability-2009-08-08-Yahoo%21%27s_PNUTS_Database%3A_Too_Hot%2C_Too_Cold_or_Just_Right%3F.html">676 high scalability-2009-08-08-Yahoo!'s PNUTS Database: Too Hot, Too Cold or Just Right?</a></p>
<p>13 0.69418585 <a title="529-lsi-13" href="../high_scalability-2011/high_scalability-2011-12-08-Update_on_Scalable_Causal_Consistency_For_Wide-Area_Storage_With_COPS.html">1153 high scalability-2011-12-08-Update on Scalable Causal Consistency For Wide-Area Storage With COPS</a></p>
<p>14 0.68781883 <a title="529-lsi-14" href="../high_scalability-2012/high_scalability-2012-06-27-Paper%3A_Logic_and_Lattices_for_Distributed_Programming.html">1273 high scalability-2012-06-27-Paper: Logic and Lattices for Distributed Programming</a></p>
<p>15 0.68643302 <a title="529-lsi-15" href="../high_scalability-2012/high_scalability-2012-05-10-Paper%3A_Paxos_Made_Moderately_Complex.html">1243 high scalability-2012-05-10-Paper: Paxos Made Moderately Complex</a></p>
<p>16 0.68439311 <a title="529-lsi-16" href="../high_scalability-2014/high_scalability-2014-04-10-Paper%3A_Scalable_Atomic_Visibility_with_RAMP_Transactions_-_Scale_Linearly_to_100_Servers.html">1629 high scalability-2014-04-10-Paper: Scalable Atomic Visibility with RAMP Transactions - Scale Linearly to 100 Servers</a></p>
<p>17 0.67017555 <a title="529-lsi-17" href="../high_scalability-2009/high_scalability-2009-02-03-Paper%3A_Optimistic_Replication.html">507 high scalability-2009-02-03-Paper: Optimistic Replication</a></p>
<p>18 0.66258413 <a title="529-lsi-18" href="../high_scalability-2010/high_scalability-2010-12-23-Paper%3A_CRDTs%3A_Consistency_without_concurrency_control.html">963 high scalability-2010-12-23-Paper: CRDTs: Consistency without concurrency control</a></p>
<p>19 0.6600371 <a title="529-lsi-19" href="../high_scalability-2008/high_scalability-2008-07-26-Google%27s_Paxos_Made_Live_%E2%80%93_An_Engineering_Perspective.html">357 high scalability-2008-07-26-Google's Paxos Made Live – An Engineering Perspective</a></p>
<p>20 0.65851295 <a title="529-lsi-20" href="../high_scalability-2008/high_scalability-2008-03-17-Paper%3A_Consistent_Hashing_and_Random_Trees%3A_Distributed_Caching_Protocols_for_Relieving_Hot_Spots_on_the_World_Wide_Web.html">280 high scalability-2008-03-17-Paper: Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.084), (2, 0.182), (6, 0.32), (10, 0.023), (47, 0.038), (51, 0.072), (61, 0.072), (79, 0.108)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.84672034 <a title="529-lda-1" href="../high_scalability-2007/high_scalability-2007-10-01-SmugMug_Found_their_Perfect_Storage_Array.html">104 high scalability-2007-10-01-SmugMug Found their Perfect Storage Array</a></p>
<p>Introduction: SmugMug's CEO & Chief Geek Don MacAskill smugly (hard to resist) gushes over finally finding, after a long and arduous quest, their "best bang-for-the-buck storage array."  It's the   Dell MD300  . His in-depth explanation of why he prefers the MD3000 should help anyone with their own painful storage deliberations. His key points are:        The price is right; DAS via SAS, 15 spindles at 15K rpm each, 512MB of mirrored battery-backed write cache; You can disable read caching; You can disable read-ahead prefetching; The stripe sizes are configurable up to 512KB; The controller ignores host-based flush commands by default; They support an ‘Enhanced JBOD’ mode.     His reasoning for the desirability each option is astute and he even gives you the configuration options for carrying out the configuration. This is not your average CEO.     Don also speculates that a three tier system using flash (system RAM + flash storage + RAID disks) is a possible future direction.  Unfortunately, flash</p><p>same-blog 2 0.8397454 <a title="529-lda-2" href="../high_scalability-2009/high_scalability-2009-03-10-Paper%3A_Consensus_Protocols%3A_Paxos___.html">529 high scalability-2009-03-10-Paper: Consensus Protocols: Paxos   </a></p>
<p>Introduction: Update:   Barbara Liskov’s Turing Award, and Byzantine Fault Tolerance .  Henry Robinson has created an excellent series of articles on consensus protocols. We already covered his  2 Phase Commit  article and he also has a  3 Phase Commit  article showing how to handle 2PC under single node failures.  But that is not enough!  3PC works well under node failures, but fails for network failures. So another consensus mechanism is needed that handles both network and node failures. And that's  Paxos .  Paxos correctly handles both types of failures, but it does this by becoming inaccessible if too many components fail. This is the "liveness" property of protocols. Paxos waits until the faults are fixed. Read queries can be handled, but updates will be blocked until the protocol thinks it can make forward progress.   The liveness of Paxos is primarily dependent on network stability. In a distributed heterogeneous environment you are at risk of losing the ability to make updates. Users hate t</p><p>3 0.82075554 <a title="529-lda-3" href="../high_scalability-2010/high_scalability-2010-05-31-Scalable_federated_security_with_Kerberos_.html">832 high scalability-2010-05-31-Scalable federated security with Kerberos </a></p>
<p>Introduction: In my last  post , I outlined considerations that need to be taken into account when choosing between a centralized and federated security model. So, how do we implement the chosen model?   Based on a real-world case study, I will outline a Kerberos architecture that enables cutting-edge collaborative research through federated sharing of resources. 
 
 Read more on  BigDataMatters.com</p><p>4 0.79870445 <a title="529-lda-4" href="../high_scalability-2009/high_scalability-2009-09-20-PaxosLease%3A_Diskless_Paxos_for_Leases.html">710 high scalability-2009-09-20-PaxosLease: Diskless Paxos for Leases</a></p>
<p>Introduction: PaxosLease  is a distributed algorithm for lease negotiation. It is based on Paxos, but does not require disk writes or clock synchrony. PaxosLease is used for master lease negotation in the open-source Keyspace replicated key-value store.</p><p>5 0.78434068 <a title="529-lda-5" href="../high_scalability-2007/high_scalability-2007-09-16-What_software_runs_on_this_site%3F.html">93 high scalability-2007-09-16-What software runs on this site?</a></p>
<p>Introduction: It's pretty slick! olla</p><p>6 0.76003146 <a title="529-lda-6" href="../high_scalability-2010/high_scalability-2010-03-11-What_would_you_like_to_ask_Justin.tv%3F.html">794 high scalability-2010-03-11-What would you like to ask Justin.tv?</a></p>
<p>7 0.73024601 <a title="529-lda-7" href="../high_scalability-2008/high_scalability-2008-01-15-Does_Sun_Buying_MySQL_Change_Your_Scaling_Strategy%3F.html">213 high scalability-2008-01-15-Does Sun Buying MySQL Change Your Scaling Strategy?</a></p>
<p>8 0.66277564 <a title="529-lda-8" href="../high_scalability-2013/high_scalability-2013-03-13-Iron.io_Moved_From_Ruby_to_Go%3A_28_Servers_Cut_and_Colossal_Clusterf%2A%2Aks_Prevented.html">1423 high scalability-2013-03-13-Iron.io Moved From Ruby to Go: 28 Servers Cut and Colossal Clusterf**ks Prevented</a></p>
<p>9 0.65218747 <a title="529-lda-9" href="../high_scalability-2011/high_scalability-2011-05-06-Stuff_The_Internet_Says_On_Scalability_For_May_6th%2C_2011.html">1036 high scalability-2011-05-06-Stuff The Internet Says On Scalability For May 6th, 2011</a></p>
<p>10 0.63649344 <a title="529-lda-10" href="../high_scalability-2008/high_scalability-2008-02-07-clusteradmin.blogspot.com_-_blog_about_building_and_administering_clusters.html">243 high scalability-2008-02-07-clusteradmin.blogspot.com - blog about building and administering clusters</a></p>
<p>11 0.63575894 <a title="529-lda-11" href="../high_scalability-2013/high_scalability-2013-01-18-Stuff_The_Internet_Says_On_Scalability_For_January_18%2C_2013.html">1389 high scalability-2013-01-18-Stuff The Internet Says On Scalability For January 18, 2013</a></p>
<p>12 0.6116398 <a title="529-lda-12" href="../high_scalability-2013/high_scalability-2013-11-25-How_To_Make_an_Infinitely_Scalable_Relational_Database_Management_System_%28RDBMS%29.html">1553 high scalability-2013-11-25-How To Make an Infinitely Scalable Relational Database Management System (RDBMS)</a></p>
<p>13 0.60278761 <a title="529-lda-13" href="../high_scalability-2009/high_scalability-2009-11-16-Building_Scalable_Systems_Using_Data_as_a_Composite_Material.html">741 high scalability-2009-11-16-Building Scalable Systems Using Data as a Composite Material</a></p>
<p>14 0.59071332 <a title="529-lda-14" href="../high_scalability-2009/high_scalability-2009-08-24-How_Google_Serves_Data_from_Multiple_Datacenters.html">687 high scalability-2009-08-24-How Google Serves Data from Multiple Datacenters</a></p>
<p>15 0.5900296 <a title="529-lda-15" href="../high_scalability-2011/high_scalability-2011-10-28-Stuff_The_Internet_Says_On_Scalability_For_October_28%2C_2011.html">1134 high scalability-2011-10-28-Stuff The Internet Says On Scalability For October 28, 2011</a></p>
<p>16 0.58787447 <a title="529-lda-16" href="../high_scalability-2007/high_scalability-2007-10-30-Feedblendr_Architecture_-_Using_EC2_to_Scale.html">138 high scalability-2007-10-30-Feedblendr Architecture - Using EC2 to Scale</a></p>
<p>17 0.58769548 <a title="529-lda-17" href="../high_scalability-2012/high_scalability-2012-06-25-StubHub_Architecture%3A_The_Surprising_Complexity_Behind_the_World%E2%80%99s_Largest_Ticket_Marketplace.html">1271 high scalability-2012-06-25-StubHub Architecture: The Surprising Complexity Behind the World’s Largest Ticket Marketplace</a></p>
<p>18 0.58472365 <a title="529-lda-18" href="../high_scalability-2008/high_scalability-2008-07-26-Sharding_the_Hibernate_Way.html">358 high scalability-2008-07-26-Sharding the Hibernate Way</a></p>
<p>19 0.58461696 <a title="529-lda-19" href="../high_scalability-2009/high_scalability-2009-11-04-Damn%2C_Which_Database_do_I_Use_Now%3F.html">736 high scalability-2009-11-04-Damn, Which Database do I Use Now?</a></p>
<p>20 0.58458364 <a title="529-lda-20" href="../high_scalability-2009/high_scalability-2009-08-06-An_Unorthodox_Approach_to_Database_Design_%3A_The_Coming_of_the_Shard.html">672 high scalability-2009-08-06-An Unorthodox Approach to Database Design : The Coming of the Shard</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
