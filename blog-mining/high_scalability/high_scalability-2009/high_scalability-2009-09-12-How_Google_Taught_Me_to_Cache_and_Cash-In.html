<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>703 high scalability-2009-09-12-How Google Taught Me to Cache and Cash-In</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2009" href="../home/high_scalability-2009_home.html">high_scalability-2009</a> <a title="high_scalability-2009-703" href="#">high_scalability-2009-703</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>703 high scalability-2009-09-12-How Google Taught Me to Cache and Cash-In</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2009-703-html" href="http://highscalability.com//blog/2009/9/12/how-google-taught-me-to-cache-and-cash-in.html">html</a></p><p>Introduction: A user named Apathy   on how Reddit scales some of their features, shares some advice he learned while working at Google and other major companies.   To be fair, I [Apathy] was working at Google at the time, and every job I held between 1995 and 2005 involved at least one of the largest websites on the planet. I didn't come up with any of these ideas, just watched other smart people I worked with who knew what they were doing and found (or wrote) tools that did the same things. But the theme is always the same: 
  
 Cache everything you can and store the rest in some sort of database (not necessarily relational and not necessarily centralized).  
 Cache everything that doesn't change rapidly. Most of the time you don't have to hit the database for anything other than checking whether the users' new message count has transitioned from 0 to (1 or more). 
 Cache everything--templates, user message status, the front page components--and hit the database once a minute or so to update the fr</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 I didn't come up with any of these ideas, just watched other smart people I worked with who knew what they were doing and found (or wrote) tools that did the same things. [sent-3, score-0.196]
</p><p>2 But the theme is always the same:      Cache everything you can and store the rest in some sort of database (not necessarily relational and not necessarily centralized). [sent-4, score-0.32]
</p><p>3 Most of the time you don't have to hit the database for anything other than checking whether the users' new message count has transitioned from 0 to (1 or more). [sent-6, score-0.435]
</p><p>4 Cache everything--templates, user message status, the front page components--and hit the database once a minute or so to update the front page, forums, etc. [sent-7, score-1.015]
</p><p>5 Combine the previous two steps to generate a menu from cached blocks. [sent-14, score-0.363]
</p><p>6 The golden rule of website engineering is that you don't try to enforce partial ordering simultaneously with your updates. [sent-26, score-0.172]
</p><p>7 When running a search engine operate  the crawler separately from the indexer. [sent-27, score-0.176]
</p><p>8 Ranking scores are used as necessary from the index, usually cached for popular queries. [sent-28, score-0.43]
</p><p>9 Re-rank popular subreddits or the front page once a minute. [sent-29, score-0.63]
</p><p>10 Then cache numbers 100-200 when someone bothers to visit the 5th page of a subreddit, etc. [sent-32, score-0.463]
</p><p>11 For less-popular subreddits, you cache the results until an update comes in. [sent-33, score-0.267]
</p><p>12 With enough horsepower and common sense, almost any volume of data can be managed, just not in realtime. [sent-34, score-0.221]
</p><p>13 Merge all the normalized rankings and cache the output every minute or so. [sent-36, score-0.502]
</p><p>14 It's a lot cheaper to merge cached lists than build them from scratch. [sent-38, score-0.264]
</p><p>15 This delays the crushing read/write bottleneck at the database. [sent-39, score-0.167]
</p><p>16 If that's not found, look for the components and build an exact match. [sent-43, score-0.204]
</p><p>17 The majority of traffic on almost all websites comes from the default, un-logged-in front page or from random forum/comment/result pages. [sent-44, score-0.592]
</p><p>18 If one or more of the components aren't found, regenerate those from the DB (now it's cached! [sent-47, score-0.196]
</p><p>19 You (almost) always have to hit the database on writes. [sent-50, score-0.241]
</p><p>20 The key is to avoid hitting it for reads until you're forced to do so. [sent-51, score-0.152]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('blow', 0.327), ('cached', 0.264), ('subreddits', 0.228), ('cache', 0.194), ('hit', 0.167), ('front', 0.164), ('page', 0.16), ('exact', 0.129), ('necessarily', 0.123), ('regenerate', 0.121), ('rankings', 0.121), ('almost', 0.119), ('bothers', 0.109), ('minute', 0.105), ('votes', 0.105), ('transitioned', 0.102), ('formatting', 0.102), ('conditional', 0.102), ('horsepower', 0.102), ('watched', 0.099), ('menu', 0.099), ('found', 0.097), ('pump', 0.096), ('crawler', 0.094), ('hooks', 0.092), ('characters', 0.092), ('message', 0.092), ('crushing', 0.091), ('user', 0.089), ('enforce', 0.088), ('scores', 0.088), ('append', 0.086), ('rank', 0.084), ('golden', 0.084), ('forums', 0.082), ('separately', 0.082), ('normalized', 0.082), ('reads', 0.08), ('popular', 0.078), ('google', 0.077), ('lucene', 0.076), ('websites', 0.076), ('delays', 0.076), ('components', 0.075), ('database', 0.074), ('avoids', 0.073), ('comes', 0.073), ('sold', 0.072), ('forced', 0.072), ('fair', 0.072)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="703-tfidf-1" href="../high_scalability-2009/high_scalability-2009-09-12-How_Google_Taught_Me_to_Cache_and_Cash-In.html">703 high scalability-2009-09-12-How Google Taught Me to Cache and Cash-In</a></p>
<p>Introduction: A user named Apathy   on how Reddit scales some of their features, shares some advice he learned while working at Google and other major companies.   To be fair, I [Apathy] was working at Google at the time, and every job I held between 1995 and 2005 involved at least one of the largest websites on the planet. I didn't come up with any of these ideas, just watched other smart people I worked with who knew what they were doing and found (or wrote) tools that did the same things. But the theme is always the same: 
  
 Cache everything you can and store the rest in some sort of database (not necessarily relational and not necessarily centralized).  
 Cache everything that doesn't change rapidly. Most of the time you don't have to hit the database for anything other than checking whether the users' new message count has transitioned from 0 to (1 or more). 
 Cache everything--templates, user message status, the front page components--and hit the database once a minute or so to update the fr</p><p>2 0.19418178 <a title="703-tfidf-2" href="../high_scalability-2008/high_scalability-2008-08-04-A_Bunch_of_Great_Strategies_for_Using_Memcached_and_MySQL_Better_Together.html">360 high scalability-2008-08-04-A Bunch of Great Strategies for Using Memcached and MySQL Better Together</a></p>
<p>Introduction: The primero recommendation for speeding up a website is almost always to add cache and more cache. And after that add a little more cache just in case. Memcached is almost always given as the recommended cache to use. What we don't often hear is how to effectively use a cache in our own products. MySQL hosted two excellent webinars (referenced below) on the subject of how to deploy and use memcached. The star of the show, other than MySQL of course, is Farhan Mashraqi of Fotolog. You may recall we did an earlier article on Fotolog in  Secrets to Fotolog's Scaling Success , which was one of my personal favorites.  Fotolog, as they themselves point out, is probably the largest site nobody has ever heard of, pulling in more page views than even Flickr. Fotolog has 51 instances of memcached on 21 servers with 175G in use and 254G available. As a large successful photo-blogging site they have very demanding performance and scaling requirements. To meet those requirements they've developed a</p><p>3 0.18657048 <a title="703-tfidf-3" href="../high_scalability-2010/high_scalability-2010-05-17-7_Lessons_Learned_While_Building_Reddit_to_270_Million_Page_Views_a_Month.html">828 high scalability-2010-05-17-7 Lessons Learned While Building Reddit to 270 Million Page Views a Month</a></p>
<p>Introduction: Steve Huffman , co-founder of social news site  Reddit , gave an excellent  presentation  ( slides ,  transcript ) on the lessons he learned while building and growing Reddit to 7.5 million users per month, 270 million page views per month, and 20+ database servers.
 
Steve says a lot of the lessons were really obvious, so you may not find a lot of completely new ideas in the presentation. But Steve has an earnestness and genuineness about him that is so obviously grounded in experience that you can't help but think deeply about what you could be doing different. And if Steve didn't know about these lessons, I'm betting others don't either.
 
There are seven lessons, each has their own summary section: Lesson one: Crash Often; Lesson 2: Separation of Services; Lesson 3: Open Schema; Lesson 4: Keep it Stateless; Lesson 5: Memcache; Lesson 6: Store Redundant Data; Lesson 7: Work Offline.
 
By far the most surprising feature of their architecture is in Lesson Six, whose essential idea is:</p><p>4 0.15241177 <a title="703-tfidf-4" href="../high_scalability-2010/high_scalability-2010-09-28-6_Strategies_for_Scaling_BBC_iPlayer.html">908 high scalability-2010-09-28-6 Strategies for Scaling BBC iPlayer</a></p>
<p>Introduction: The BBC's iPlayer site averages 8 million page views a day for 1.3 million users. Technical Architect Simon Frost describes how they scaled their site in   Scaling the BBC iPlayer to handle demand :
  
  Use frameworks . Frameworks support component based development which makes it convenient for team development, but can introduce delays that have to be minimized. Zend/PHP is used because it supports components and is easy to recruit for.Â  MySQL is used for program metadata. CouchDB is used for key-value access for fast read/write of user-focused data. 
  Prove architecture before building it . Eliminate guesswork by coming up with alternate architectures and create prototypes to determine which option works best. Balance performance with factors like ease of development.  
  Cache a lot . Data is cached in memcached for a few seconds to minutes. Short cache invalidation periods keep the data up to date for the users, but even these short periods make a huge difference in performance.</p><p>5 0.1473321 <a title="703-tfidf-5" href="../high_scalability-2013/high_scalability-2013-08-26-Reddit%3A_Lessons_Learned_from_Mistakes_Made_Scaling_to_1_Billion_Pageviews_a_Month.html">1507 high scalability-2013-08-26-Reddit: Lessons Learned from Mistakes Made Scaling to 1 Billion Pageviews a Month</a></p>
<p>Introduction: Jeremy Edberg   , the first paid employee at reddit, teaches us a lot about how to create a successful social site in a really good talk he gave at the RAMP conference. Watch it here at  Scaling Reddit from 1 Million to 1 Billion–Pitfalls and Lessons .  
 
 Jeremy uses a virtue and sin approach. Examples of the mistakes made in scaling reddit are shared and it turns out they did a lot of good stuff too. Somewhat of a shocker is that   Jeremy is now a Reliability Architect at Netflix, so we get a little Netflix perspective thrown in for free. 
 
 Some of the lessons that stood out most for me:  
  
  Think of SSDs as cheap RAM, not expensive disk . When reddit moved from spinning disks to SSDs for the database the number of servers was reduced from 12 to 1 with a ton of headroom. SSDs are 4x more expensive but you get 16x the performance. Worth the cost.  
  Give users a little bit of power, see what they do with it, and turn the good stuff into features . One of the biggest revelations</p><p>6 0.13985318 <a title="703-tfidf-6" href="../high_scalability-2014/high_scalability-2014-02-17-How_the_AOL.com_Architecture_Evolved_to_99.999%25_Availability%2C_8_Million_Visitors_Per_Day%2C_and_200%2C000_Requests_Per_Second.html">1597 high scalability-2014-02-17-How the AOL.com Architecture Evolved to 99.999% Availability, 8 Million Visitors Per Day, and 200,000 Requests Per Second</a></p>
<p>7 0.13435002 <a title="703-tfidf-7" href="../high_scalability-2009/high_scalability-2009-08-07-Strategy%3A_Break_Up_the_Memcache_Dog_Pile_.html">673 high scalability-2009-08-07-Strategy: Break Up the Memcache Dog Pile </a></p>
<p>8 0.13284215 <a title="703-tfidf-8" href="../high_scalability-2013/high_scalability-2013-01-30-Better_Browser_Caching_is_More_Important_than_No_Javascript_or_Fast_Networks_for_HTTP_Performance.html">1396 high scalability-2013-01-30-Better Browser Caching is More Important than No Javascript or Fast Networks for HTTP Performance</a></p>
<p>9 0.13091213 <a title="703-tfidf-9" href="../high_scalability-2008/high_scalability-2008-12-16-%5BANN%5D_New_Open_Source_Cache_System.html">467 high scalability-2008-12-16-[ANN] New Open Source Cache System</a></p>
<p>10 0.12201595 <a title="703-tfidf-10" href="../high_scalability-2012/high_scalability-2012-10-02-An_Epic_TripAdvisor_Update%3A_Why_Not_Run_on_the_Cloud%3F_The_Grand_Experiment..html">1331 high scalability-2012-10-02-An Epic TripAdvisor Update: Why Not Run on the Cloud? The Grand Experiment.</a></p>
<p>11 0.12164319 <a title="703-tfidf-11" href="../high_scalability-2008/high_scalability-2008-05-02-Friends_for_Sale_Architecture_-_A_300_Million_Page_View-Month_Facebook_RoR_App.html">313 high scalability-2008-05-02-Friends for Sale Architecture - A 300 Million Page View-Month Facebook RoR App</a></p>
<p>12 0.12107079 <a title="703-tfidf-12" href="../high_scalability-2008/high_scalability-2008-03-12-YouTube_Architecture.html">274 high scalability-2008-03-12-YouTube Architecture</a></p>
<p>13 0.1203457 <a title="703-tfidf-13" href="../high_scalability-2010/high_scalability-2010-04-12-Poppen.de_Architecture.html">808 high scalability-2010-04-12-Poppen.de Architecture</a></p>
<p>14 0.12033863 <a title="703-tfidf-14" href="../high_scalability-2009/high_scalability-2009-02-12-MySpace_Architecture.html">511 high scalability-2009-02-12-MySpace Architecture</a></p>
<p>15 0.1186786 <a title="703-tfidf-15" href="../high_scalability-2009/high_scalability-2009-10-13-Why_are_Facebook%2C_Digg%2C_and_Twitter_so_hard_to_scale%3F.html">721 high scalability-2009-10-13-Why are Facebook, Digg, and Twitter so hard to scale?</a></p>
<p>16 0.11857113 <a title="703-tfidf-16" href="../high_scalability-2009/high_scalability-2009-06-26-PlentyOfFish_Architecture.html">638 high scalability-2009-06-26-PlentyOfFish Architecture</a></p>
<p>17 0.11781562 <a title="703-tfidf-17" href="../high_scalability-2008/high_scalability-2008-11-02-Strategy%3A_How_to_Manage_Sessions_Using_Memcached.html">436 high scalability-2008-11-02-Strategy: How to Manage Sessions Using Memcached</a></p>
<p>18 0.11720395 <a title="703-tfidf-18" href="../high_scalability-2012/high_scalability-2012-11-22-Gone_Fishin%27%3A_PlentyOfFish_Architecture.html">1361 high scalability-2012-11-22-Gone Fishin': PlentyOfFish Architecture</a></p>
<p>19 0.11548965 <a title="703-tfidf-19" href="../high_scalability-2009/high_scalability-2009-06-27-Scaling_Twitter%3A_Making_Twitter_10000_Percent_Faster.html">639 high scalability-2009-06-27-Scaling Twitter: Making Twitter 10000 Percent Faster</a></p>
<p>20 0.11547408 <a title="703-tfidf-20" href="../high_scalability-2010/high_scalability-2010-06-04-Strategy%3A_Cache_Larger_Chunks_-_Cache_Hit_Rate_is_a_Bad_Indicator.html">836 high scalability-2010-06-04-Strategy: Cache Larger Chunks - Cache Hit Rate is a Bad Indicator</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.183), (1, 0.114), (2, -0.062), (3, -0.143), (4, 0.04), (5, 0.005), (6, -0.04), (7, 0.023), (8, 0.014), (9, -0.017), (10, -0.019), (11, -0.051), (12, -0.01), (13, 0.088), (14, -0.036), (15, -0.043), (16, -0.089), (17, -0.035), (18, 0.061), (19, -0.043), (20, -0.035), (21, 0.018), (22, 0.071), (23, 0.015), (24, -0.059), (25, -0.023), (26, -0.012), (27, 0.091), (28, -0.057), (29, 0.008), (30, -0.042), (31, 0.01), (32, -0.061), (33, 0.009), (34, -0.005), (35, 0.059), (36, -0.0), (37, -0.007), (38, 0.057), (39, 0.022), (40, 0.027), (41, -0.002), (42, -0.091), (43, 0.001), (44, -0.031), (45, -0.009), (46, -0.021), (47, -0.02), (48, -0.051), (49, 0.039)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.98763609 <a title="703-lsi-1" href="../high_scalability-2009/high_scalability-2009-09-12-How_Google_Taught_Me_to_Cache_and_Cash-In.html">703 high scalability-2009-09-12-How Google Taught Me to Cache and Cash-In</a></p>
<p>Introduction: A user named Apathy   on how Reddit scales some of their features, shares some advice he learned while working at Google and other major companies.   To be fair, I [Apathy] was working at Google at the time, and every job I held between 1995 and 2005 involved at least one of the largest websites on the planet. I didn't come up with any of these ideas, just watched other smart people I worked with who knew what they were doing and found (or wrote) tools that did the same things. But the theme is always the same: 
  
 Cache everything you can and store the rest in some sort of database (not necessarily relational and not necessarily centralized).  
 Cache everything that doesn't change rapidly. Most of the time you don't have to hit the database for anything other than checking whether the users' new message count has transitioned from 0 to (1 or more). 
 Cache everything--templates, user message status, the front page components--and hit the database once a minute or so to update the fr</p><p>2 0.86063743 <a title="703-lsi-2" href="../high_scalability-2010/high_scalability-2010-09-28-6_Strategies_for_Scaling_BBC_iPlayer.html">908 high scalability-2010-09-28-6 Strategies for Scaling BBC iPlayer</a></p>
<p>Introduction: The BBC's iPlayer site averages 8 million page views a day for 1.3 million users. Technical Architect Simon Frost describes how they scaled their site in   Scaling the BBC iPlayer to handle demand :
  
  Use frameworks . Frameworks support component based development which makes it convenient for team development, but can introduce delays that have to be minimized. Zend/PHP is used because it supports components and is easy to recruit for.Â  MySQL is used for program metadata. CouchDB is used for key-value access for fast read/write of user-focused data. 
  Prove architecture before building it . Eliminate guesswork by coming up with alternate architectures and create prototypes to determine which option works best. Balance performance with factors like ease of development.  
  Cache a lot . Data is cached in memcached for a few seconds to minutes. Short cache invalidation periods keep the data up to date for the users, but even these short periods make a huge difference in performance.</p><p>3 0.80851501 <a title="703-lsi-3" href="../high_scalability-2009/high_scalability-2009-08-07-Strategy%3A_Break_Up_the_Memcache_Dog_Pile_.html">673 high scalability-2009-08-07-Strategy: Break Up the Memcache Dog Pile </a></p>
<p>Introduction: Update:  Asynchronous  HTTP cache validations . A proposed HTTP caching extension:  if your application can afford to show slightly out of date content, then stale-while-revalidate can guarantee that the user will always be served directly from the cache, hence guaranteeing a consistent response-time user-experience.   Caching is like aspirin for headaches. Head hurts: pop a 'sprin. Slow site: add caching.  Facebook  must have a lot of headaches because they popped 805 memcached servers between 10,000 web servers and 1,800 MySQL servers and they reportedly have a 99% cache hit rate. But what's the best way for you to cache for your application? It's a remarkably complex and rich topic. Alexey Kovyrin talks about one common caching problem called the   Dog Pile  Effect  in  Dog-pile Effect and How to Avoid it with Ruby on Rails . Glenn Franxman also has a Django solution in  MintCache .
 
Data is usually cached because it's too expensive to calculate for every hit. Maybe it's a gnarly S</p><p>4 0.80851471 <a title="703-lsi-4" href="../high_scalability-2010/high_scalability-2010-06-04-Strategy%3A_Cache_Larger_Chunks_-_Cache_Hit_Rate_is_a_Bad_Indicator.html">836 high scalability-2010-06-04-Strategy: Cache Larger Chunks - Cache Hit Rate is a Bad Indicator</a></p>
<p>Introduction: Isn't the secret to fast, scalable websites to  cache everything ? Caching, if not the  secret sauce  of many a website, is it at least a popular condiment. But not so fast says Peter Zaitsev in  Beyond great cache hit ratio . The point Peter makes is that we read about websites like  Amazon  and  Facebook  that can literally make hundreds of calls to satisfy a user request. Even if you have an awesome cache hit ratio, pages can still be slow because making and processing all those requests takes time. The solution is to  remove requests all together . You do this by  caching larger blocks  so you have to make fewer requests. 
 
The post has a lot of good advice worth reading: 1) Make non cacheable blocks as small as possible, 2) Maximize amount of uses of the cache item, 3) Control invalidation, 4) Multi-Get.</p><p>5 0.79969954 <a title="703-lsi-5" href="../high_scalability-2010/high_scalability-2010-05-17-7_Lessons_Learned_While_Building_Reddit_to_270_Million_Page_Views_a_Month.html">828 high scalability-2010-05-17-7 Lessons Learned While Building Reddit to 270 Million Page Views a Month</a></p>
<p>Introduction: Steve Huffman , co-founder of social news site  Reddit , gave an excellent  presentation  ( slides ,  transcript ) on the lessons he learned while building and growing Reddit to 7.5 million users per month, 270 million page views per month, and 20+ database servers.
 
Steve says a lot of the lessons were really obvious, so you may not find a lot of completely new ideas in the presentation. But Steve has an earnestness and genuineness about him that is so obviously grounded in experience that you can't help but think deeply about what you could be doing different. And if Steve didn't know about these lessons, I'm betting others don't either.
 
There are seven lessons, each has their own summary section: Lesson one: Crash Often; Lesson 2: Separation of Services; Lesson 3: Open Schema; Lesson 4: Keep it Stateless; Lesson 5: Memcache; Lesson 6: Store Redundant Data; Lesson 7: Work Offline.
 
By far the most surprising feature of their architecture is in Lesson Six, whose essential idea is:</p><p>6 0.7947697 <a title="703-lsi-6" href="../high_scalability-2008/high_scalability-2008-11-02-Strategy%3A_How_to_Manage_Sessions_Using_Memcached.html">436 high scalability-2008-11-02-Strategy: How to Manage Sessions Using Memcached</a></p>
<p>7 0.79077858 <a title="703-lsi-7" href="../high_scalability-2008/high_scalability-2008-08-04-A_Bunch_of_Great_Strategies_for_Using_Memcached_and_MySQL_Better_Together.html">360 high scalability-2008-08-04-A Bunch of Great Strategies for Using Memcached and MySQL Better Together</a></p>
<p>8 0.76742005 <a title="703-lsi-8" href="../high_scalability-2013/high_scalability-2013-01-30-Better_Browser_Caching_is_More_Important_than_No_Javascript_or_Fast_Networks_for_HTTP_Performance.html">1396 high scalability-2013-01-30-Better Browser Caching is More Important than No Javascript or Fast Networks for HTTP Performance</a></p>
<p>9 0.75965506 <a title="703-lsi-9" href="../high_scalability-2008/high_scalability-2008-02-12-We_want_to_cache_a_lot_%3A%29_How_do_we_go_about_it_%3F.html">247 high scalability-2008-02-12-We want to cache a lot :) How do we go about it ?</a></p>
<p>10 0.75878119 <a title="703-lsi-10" href="../high_scalability-2014/high_scalability-2014-03-27-Strategy%3A_Cache_Stored_Procedure_Results.html">1620 high scalability-2014-03-27-Strategy: Cache Stored Procedure Results</a></p>
<p>11 0.75275093 <a title="703-lsi-11" href="../high_scalability-2014/high_scalability-2014-04-16-Six_Lessons_Learned_the_Hard_Way_About_Scaling_a_Million_User_System_.html">1633 high scalability-2014-04-16-Six Lessons Learned the Hard Way About Scaling a Million User System </a></p>
<p>12 0.73600632 <a title="703-lsi-12" href="../high_scalability-2012/high_scalability-2012-10-24-Saving_Cash_Using_Less_Cache_-__90%25_Savings_in_the_Caching_Tier.html">1346 high scalability-2012-10-24-Saving Cash Using Less Cache -  90% Savings in the Caching Tier</a></p>
<p>13 0.72097379 <a title="703-lsi-13" href="../high_scalability-2010/high_scalability-2010-03-26-Strategy%3A_Caching_404s_Saved_the_Onion_66%25_on_Server_Time.html">800 high scalability-2010-03-26-Strategy: Caching 404s Saved the Onion 66% on Server Time</a></p>
<p>14 0.72089428 <a title="703-lsi-14" href="../high_scalability-2008/high_scalability-2008-07-29-Ehcache_-_A_Java_Distributed_Cache_.html">359 high scalability-2008-07-29-Ehcache - A Java Distributed Cache </a></p>
<p>15 0.71818054 <a title="703-lsi-15" href="../high_scalability-2007/high_scalability-2007-12-05-Product%3A_Tugela_Cache.html">174 high scalability-2007-12-05-Product: Tugela Cache</a></p>
<p>16 0.71722388 <a title="703-lsi-16" href="../high_scalability-2010/high_scalability-2010-09-30-More_Troubles_with_Caching.html">911 high scalability-2010-09-30-More Troubles with Caching</a></p>
<p>17 0.71625978 <a title="703-lsi-17" href="../high_scalability-2008/high_scalability-2008-12-16-%5BANN%5D_New_Open_Source_Cache_System.html">467 high scalability-2008-12-16-[ANN] New Open Source Cache System</a></p>
<p>18 0.71533263 <a title="703-lsi-18" href="../high_scalability-2009/high_scalability-2009-01-17-Intro_to_Caching%2CCaching_algorithms_and_caching_frameworks_part_1.html">495 high scalability-2009-01-17-Intro to Caching,Caching algorithms and caching frameworks part 1</a></p>
<p>19 0.70570695 <a title="703-lsi-19" href="../high_scalability-2009/high_scalability-2009-01-02-Strategy%3A_Understanding_Your_Data_Leads_to_the_Best_Scalability_Solutions.html">481 high scalability-2009-01-02-Strategy: Understanding Your Data Leads to the Best Scalability Solutions</a></p>
<p>20 0.70409817 <a title="703-lsi-20" href="../high_scalability-2011/high_scalability-2011-02-28-A_Practical_Guide_to_Varnish_-_Why_Varnish_Matters.html">996 high scalability-2011-02-28-A Practical Guide to Varnish - Why Varnish Matters</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.07), (2, 0.23), (3, 0.011), (10, 0.078), (30, 0.063), (40, 0.029), (46, 0.122), (61, 0.143), (79, 0.026), (85, 0.051), (94, 0.082), (96, 0.022)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.93422389 <a title="703-lda-1" href="../high_scalability-2009/high_scalability-2009-09-12-How_Google_Taught_Me_to_Cache_and_Cash-In.html">703 high scalability-2009-09-12-How Google Taught Me to Cache and Cash-In</a></p>
<p>Introduction: A user named Apathy   on how Reddit scales some of their features, shares some advice he learned while working at Google and other major companies.   To be fair, I [Apathy] was working at Google at the time, and every job I held between 1995 and 2005 involved at least one of the largest websites on the planet. I didn't come up with any of these ideas, just watched other smart people I worked with who knew what they were doing and found (or wrote) tools that did the same things. But the theme is always the same: 
  
 Cache everything you can and store the rest in some sort of database (not necessarily relational and not necessarily centralized).  
 Cache everything that doesn't change rapidly. Most of the time you don't have to hit the database for anything other than checking whether the users' new message count has transitioned from 0 to (1 or more). 
 Cache everything--templates, user message status, the front page components--and hit the database once a minute or so to update the fr</p><p>2 0.8920368 <a title="703-lda-2" href="../high_scalability-2007/high_scalability-2007-11-08-ID_generator.html">145 high scalability-2007-11-08-ID generator</a></p>
<p>Introduction: Hi,     I would like feed back on a ID generator I just made. What positive and negative effects do you see with this. It's programmed in Java, but could just as easily be programmed in any other typical language. It's thread safe and does not use any synchronization. When testing it on my laptop, I was able to generate 10 million IDs within about 15 seconds, so it should be more than fast enough.     Take a look at the attachment.. (had to rename it from IdGen.java to IdGen.txt to attach it)      IdGen.java</p><p>3 0.8872202 <a title="703-lda-3" href="../high_scalability-2010/high_scalability-2010-11-16-Facebook%27s_New_Real-time_Messaging_System%3A_HBase_to_Store_135%2B_Billion_Messages_a_Month.html">943 high scalability-2010-11-16-Facebook's New Real-time Messaging System: HBase to Store 135+ Billion Messages a Month</a></p>
<p>Introduction: You may have read somewhere that Facebook has introduced a new  Social Inbox  integrating email, IM, SMS,  text messages, on-site Facebook messages. All-in-all they need to store over 135 billion messages a month. Where do they store all that stuff? Facebook's Kannan Muthukkaruppan gives the surprise answer in  The Underlying Technology of Messages :  HBase . HBase beat out MySQL, Cassandra, and a few others.
 
Why a surprise? Facebook created Cassandra and it was purpose built for an inbox type application, but they found Cassandra's eventual consistency model wasn't a good match for their new real-time Messages product. Facebook also has an extensive  MySQL infrastructure , but they found performance suffered as data set and indexes grew larger. And they could have built their own, but they chose HBase.
 
HBase is a   scaleout table store supporting very high rates of row-level updates over massive amounts of data  . Exactly what is needed for a Messaging system. HBase is also a colu</p><p>4 0.87982243 <a title="703-lda-4" href="../high_scalability-2008/high_scalability-2008-03-08-Audiogalaxy.com_Architecture.html">269 high scalability-2008-03-08-Audiogalaxy.com Architecture</a></p>
<p>Introduction: Update 3:  Always Refer to Your V1 As a Prototype . You really do have to plan to throw one away.     Update 2:  Lessons Learned Scaling the Audiogalaxy Search Engine . Things he should have done and fun things he couldn’t justify doing.     Update:  Design details of Audiogalaxy.com’s high performance MySQL search engine . At peak times, the search engine needed to handle 1500-2000 searches every second against a MySQL database with about 200 million rows.      Search was one of most interesting problems at Audiogalaxy. It was one of the core functions of the site, and somewhere between 50 to 70 million searches were performed every day. At peak times, the search engine needed to handle 1500-2000 searches every second against a MySQL database with about 200 million rows.</p><p>5 0.87731653 <a title="703-lda-5" href="../high_scalability-2008/high_scalability-2008-08-08-Separation_into_read-write_only_databases.html">361 high scalability-2008-08-08-Separation into read-write only databases</a></p>
<p>Introduction: At least in the articles on Plenty of Fish and Slashdot it was mentioned that one can achieve higher performance by creating read-only and write-only databases where possible.    I have read the comments and tried unsuccessfully to find more information on the net about this. I still do not understand the concept. Can someone explain it in more detail, as well as recommend resources for further investigation? (Are there books written specifically about this technique?) I think it is a very important issue, because databases are oftentimes the bottleneck.</p><p>6 0.87318295 <a title="703-lda-6" href="../high_scalability-2007/high_scalability-2007-07-11-Friendster_Architecture.html">6 high scalability-2007-07-11-Friendster Architecture</a></p>
<p>7 0.8712312 <a title="703-lda-7" href="../high_scalability-2011/high_scalability-2011-12-09-Stuff_The_Internet_Says_On_Scalability_For_December_9%2C_2011.html">1154 high scalability-2011-12-09-Stuff The Internet Says On Scalability For December 9, 2011</a></p>
<p>8 0.87072569 <a title="703-lda-8" href="../high_scalability-2012/high_scalability-2012-05-11-Stuff_The_Internet_Says_On_Scalability_For_May_11%2C_2012.html">1244 high scalability-2012-05-11-Stuff The Internet Says On Scalability For May 11, 2012</a></p>
<p>9 0.86956745 <a title="703-lda-9" href="../high_scalability-2011/high_scalability-2011-10-31-15_Ways_to_Make_Your_Application_Feel_More_Responsive_under_Google_App_Engine.html">1135 high scalability-2011-10-31-15 Ways to Make Your Application Feel More Responsive under Google App Engine</a></p>
<p>10 0.86941725 <a title="703-lda-10" href="../high_scalability-2013/high_scalability-2013-10-28-Design_Decisions_for_Scaling_Your_High_Traffic_Feeds.html">1538 high scalability-2013-10-28-Design Decisions for Scaling Your High Traffic Feeds</a></p>
<p>11 0.86901075 <a title="703-lda-11" href="../high_scalability-2013/high_scalability-2013-05-24-Stuff_The_Internet_Says_On_Scalability_For_May_24%2C_2013.html">1464 high scalability-2013-05-24-Stuff The Internet Says On Scalability For May 24, 2013</a></p>
<p>12 0.8687526 <a title="703-lda-12" href="../high_scalability-2011/high_scalability-2011-07-06-11_Common_Web_Use_Cases_Solved_in_Redis.html">1074 high scalability-2011-07-06-11 Common Web Use Cases Solved in Redis</a></p>
<p>13 0.8665055 <a title="703-lda-13" href="../high_scalability-2009/high_scalability-2009-03-19-Product%3A_Redis_-_Not_Just_Another_Key-Value_Store.html">545 high scalability-2009-03-19-Product: Redis - Not Just Another Key-Value Store</a></p>
<p>14 0.8631891 <a title="703-lda-14" href="../high_scalability-2012/high_scalability-2012-08-02-Strategy%3A_Use_Spare_Region_Capacity_to_Survive_Availability_Zone_Failures.html">1296 high scalability-2012-08-02-Strategy: Use Spare Region Capacity to Survive Availability Zone Failures</a></p>
<p>15 0.86295033 <a title="703-lda-15" href="../high_scalability-2013/high_scalability-2013-07-08-The_Architecture_Twitter_Uses_to_Deal_with_150M_Active_Users%2C_300K_QPS%2C_a_22_MB-S_Firehose%2C_and_Send_Tweets_in_Under_5_Seconds.html">1488 high scalability-2013-07-08-The Architecture Twitter Uses to Deal with 150M Active Users, 300K QPS, a 22 MB-S Firehose, and Send Tweets in Under 5 Seconds</a></p>
<p>16 0.86027384 <a title="703-lda-16" href="../high_scalability-2010/high_scalability-2010-03-26-Strategy%3A_Caching_404s_Saved_the_Onion_66%25_on_Server_Time.html">800 high scalability-2010-03-26-Strategy: Caching 404s Saved the Onion 66% on Server Time</a></p>
<p>17 0.85853344 <a title="703-lda-17" href="../high_scalability-2012/high_scalability-2012-06-18-The_Clever_Ways_Chrome_Hides_Latency_by_Anticipating_Your_Every_Need.html">1267 high scalability-2012-06-18-The Clever Ways Chrome Hides Latency by Anticipating Your Every Need</a></p>
<p>18 0.85828507 <a title="703-lda-18" href="../high_scalability-2012/high_scalability-2012-06-20-iDoneThis_-_Scaling_an_Email-based_App_from_Scratch.html">1269 high scalability-2012-06-20-iDoneThis - Scaling an Email-based App from Scratch</a></p>
<p>19 0.85795194 <a title="703-lda-19" href="../high_scalability-2012/high_scalability-2012-06-01-Stuff_The_Internet_Says_On_Scalability_For_June_1%2C_2012.html">1255 high scalability-2012-06-01-Stuff The Internet Says On Scalability For June 1, 2012</a></p>
<p>20 0.85791397 <a title="703-lda-20" href="../high_scalability-2013/high_scalability-2013-02-15-Stuff_The_Internet_Says_On_Scalability_For_February_15%2C_2013.html">1407 high scalability-2013-02-15-Stuff The Internet Says On Scalability For February 15, 2013</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
