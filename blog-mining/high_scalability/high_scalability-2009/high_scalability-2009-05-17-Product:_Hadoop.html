<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>601 high scalability-2009-05-17-Product: Hadoop</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2009" href="../home/high_scalability-2009_home.html">high_scalability-2009</a> <a title="high_scalability-2009-601" href="#">high_scalability-2009-601</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>601 high scalability-2009-05-17-Product: Hadoop</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2009-601-html" href="http://highscalability.com//blog/2009/5/17/product-hadoop.html">html</a></p><p>Introduction: Update 5:Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62
Secondsand has itsgreen cred questionedbecause it took 40 times the number of
machines Greenplum used to do the same work.Update 4:Introduction to Pig. Pig
allows you to skip programming Hadoop at the low map-reduce level. You don't
have to know Java. Using the Pig Latin language, which is a scripting data
flow language, you can think about your problem as a data flow program. 10
lines of Pig Latin = 200 lines of Java.Update 3: Scaling Hadoop to4000 nodes
at Yahoo!. 30,000 cores with nearly 16PB of raw disk; sorted 6TB of data
completed in 37 minutes; 14,000 map tasks writes (reads) 360 MB (about 3
blocks) of data into a single file with a total of 5.04 TB for the whole
job.Update 2: HadoopSummit and Data-Intensive Computing Symposium Videos and
Slides. Topics include: Pig, JAQL, Hbase, Hive, Data-Intensive Scalable
Computing, Clouds and ManyCore: The Revolution, Simplicity and Complexity in
Data Systems at Scale, Han</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Using the Pig Latin language, which is a scripting data flow language, you can think about your problem as a data flow program. [sent-6, score-0.354]
</p><p>2 30,000 cores with nearly 16PB of raw disk; sorted 6TB of data completed in 37 minutes; 14,000 map tasks writes (reads) 360 MB (about 3 blocks) of data into a single file with a total of 5. [sent-10, score-0.374]
</p><p>3 Hadoopis a framework for running applications on large clusters of commodity hardware using a computational paradigm named map/reduce, where the application is divided into many small fragments of work, each of which may be executed on any node in the cluster. [sent-17, score-0.733]
</p><p>4 Jeremy Zawodnyhas a wonderful overview of why Hadoop is important for large website builders:breakbreakFor the last several years, every company involved in building large web-scale systems has faced some of the same fundamental challenges. [sent-19, score-0.256]
</p><p>5 While nearly everyone agrees that the "divide-and- conquer using lots of cheap hardware" approach to breaking down large problems is the only way to scale, doing so is not easy. [sent-20, score-0.513]
</p><p>6 Even if you use somebody else's commodity hardware, you still have to develop the software that'll do the divide-and-conquer work to keep them all busyIt's hard work. [sent-23, score-0.163]
</p><p>7 And it needs to be commoditized, just like the hardware has been. [sent-24, score-0.096]
</p><p>8 Hadoop also provides a distributed file system that stores data on the compute nodes, providing very high aggregate bandwidth across the cluster. [sent-25, score-0.2]
</p><p>9 Both map/reduce and the distributed file system are designed so that node failures are automatically handled by the framework. [sent-26, score-0.21]
</p><p>10 Hadoop has been demonstrated on clusters with 2000 nodes. [sent-27, score-0.156]
</p><p>11 The obvious question of the day is: should you build your website around Hadoop? [sent-29, score-0.086]
</p><p>12 There seems to be a few types of things you do with lots of data: process, transform, and serve. [sent-31, score-0.094]
</p><p>13 Yahoo literally has petabytes of log files, web pages, and other data they process. [sent-32, score-0.273]
</p><p>14 If you are YouTube and you have petabytes of media to serve, do you really need map/reduce? [sent-37, score-0.2]
</p><p>15 Maybe not, but the clustered file system is great. [sent-38, score-0.109]
</p><p>16 Perfect for when you have lots of stuff to store. [sent-40, score-0.094]
</p><p>17 With that you could create thumbnails, previews, transcode media files, and so on. [sent-42, score-0.182]
</p><p>18 Everyone needs to store structured data in a scalable, reliable, highly performing data store. [sent-44, score-0.182]
</p><p>19 I can't wait for experience reports about "normal" people, familiar with a completely different paradigm, adopting this infrastructure. [sent-46, score-0.076]
</p><p>20 I wonder what animal O'Reilly will use on their Hadoop cover? [sent-47, score-0.104]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hadoop', 0.408), ('pig', 0.314), ('latin', 0.191), ('paradigm', 0.119), ('petabytes', 0.113), ('sherpa', 0.111), ('file', 0.109), ('animal', 0.104), ('greenplum', 0.104), ('hadoopsummit', 0.104), ('previews', 0.104), ('node', 0.101), ('hadoopby', 0.099), ('manycore', 0.099), ('yahoo', 0.099), ('hardware', 0.096), ('transcode', 0.095), ('lots', 0.094), ('commodity', 0.093), ('symposium', 0.092), ('agrees', 0.092), ('lines', 0.092), ('data', 0.091), ('conquer', 0.088), ('media', 0.087), ('thumbnails', 0.086), ('website', 0.086), ('flow', 0.086), ('large', 0.085), ('commoditized', 0.084), ('fragments', 0.084), ('clusters', 0.083), ('nearly', 0.083), ('builders', 0.08), ('computing', 0.078), ('directions', 0.077), ('adopting', 0.076), ('affinity', 0.075), ('terabyte', 0.074), ('prospect', 0.073), ('sense', 0.073), ('demonstrated', 0.073), ('replicates', 0.073), ('divided', 0.072), ('bet', 0.072), ('files', 0.072), ('everyone', 0.071), ('language', 0.07), ('somebody', 0.07), ('literally', 0.069)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000004 <a title="601-tfidf-1" href="../high_scalability-2009/high_scalability-2009-05-17-Product%3A_Hadoop.html">601 high scalability-2009-05-17-Product: Hadoop</a></p>
<p>Introduction: Update 5:Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62
Secondsand has itsgreen cred questionedbecause it took 40 times the number of
machines Greenplum used to do the same work.Update 4:Introduction to Pig. Pig
allows you to skip programming Hadoop at the low map-reduce level. You don't
have to know Java. Using the Pig Latin language, which is a scripting data
flow language, you can think about your problem as a data flow program. 10
lines of Pig Latin = 200 lines of Java.Update 3: Scaling Hadoop to4000 nodes
at Yahoo!. 30,000 cores with nearly 16PB of raw disk; sorted 6TB of data
completed in 37 minutes; 14,000 map tasks writes (reads) 360 MB (about 3
blocks) of data into a single file with a total of 5.04 TB for the whole
job.Update 2: HadoopSummit and Data-Intensive Computing Symposium Videos and
Slides. Topics include: Pig, JAQL, Hbase, Hive, Data-Intensive Scalable
Computing, Clouds and ManyCore: The Revolution, Simplicity and Complexity in
Data Systems at Scale, Han</p><p>2 0.38837776 <a title="601-tfidf-2" href="../high_scalability-2008/high_scalability-2008-11-14-Paper%3A_Pig_Latin%3A_A_Not-So-Foreign_Language_for_Data_Processing.html">443 high scalability-2008-11-14-Paper: Pig Latin: A Not-So-Foreign Language for Data Processing</a></p>
<p>Introduction: Yahoo has developed a new language called Pig Latin that fit in a sweet spot
between high-level declarative querying in the spirit of SQL, and low-level,
procedural programming `a la map-reduce and combines best of both worlds.The
accompanying system, Pig, is fully implemented, and compiles Pig Latin into
physical plans that are executed over Hadoop, an open-source, map-reduce
implementation. Pig has just graduated from the Apache Incubator and joined
Hadoop as a subproject.The paper has a few examples of how engineers at Yahoo!
are using Pig to dramatically reduce the time required for the development and
execution of their data analysis tasks, compared tousing Hadoop
directly.References:Apache Pig Wiki</p><p>3 0.25971073 <a title="601-tfidf-3" href="../high_scalability-2009/high_scalability-2009-06-11-Yahoo%21_Distribution_of_Hadoop.html">627 high scalability-2009-06-11-Yahoo! Distribution of Hadoop</a></p>
<p>Introduction: Many people in the Apache Hadoop community have asked Yahoo! to publish the
version of Apache Hadoop they test and deploy across their large Hadoop
clusters. As a service to the Hadoop community, Yahoo is releasing the Yahoo!
Distribution of Hadoop -- a source code distribution that is based entirely on
code found in the Apache Hadoop project.This source distribution includes code
patches that they have added to improve the stability and performance of their
clusters. In all cases, these patches have already been contributed back to
Apache, but they may not yet be available in an Apache release of Hadoop.Read
more and get the Hadoop distribution from Yahoo</p><p>4 0.25217238 <a title="601-tfidf-4" href="../high_scalability-2008/high_scalability-2008-10-15-Hadoop_-_A_Primer.html">414 high scalability-2008-10-15-Hadoop - A Primer</a></p>
<p>Introduction: Hadoop is a distributed computing platform written in Java. It incorporates
features similar to those of theGoogle File System and of MapReduce to process
vast amounts of data"Hadoop is a Free Java software framework that supports
data intensive distributed applications running on large clusters of commodity
computers. It enables applications to easily scale out to thousands of nodes
and petabytes of data" (Wikipedia)* What platform does Hadoop run on?* Java
1.5.x or higher, preferably from Sun* Linux* Windows for development* Solaris</p><p>5 0.21179111 <a title="601-tfidf-5" href="../high_scalability-2008/high_scalability-2008-09-28-Product%3A_Happy_%3D_Hadoop_%2B_Python.html">397 high scalability-2008-09-28-Product: Happy = Hadoop + Python</a></p>
<p>Introduction: Has a Java only Hadoop been getting you down? Now you can beHappy. Happy is
aframework for writing map-reduce programs for Hadoop using Jython. It files
off the sharp edges on Hadoop and makes writing map-reduce programs a breeze.
There's really no history yet on Happy, but I'm delighted at the idea of being
able to map-reduce inother languages. The more ways the better.From the
website:Happy is a framework that allows Hadoop jobs to be written and run in
Python 2.2 using Jython. It is aneasy way to write map-reduce programs for
Hadoop, and includes some new useful features as well.The current release
supports Hadoop 0.17.2.Map-reduce jobs in Happy are defined by sub-classing
happy.HappyJob and implementing amap(records, task) and reduce(key, values,
task) function. Then you create an instance of theclass, set the job
parameters (such as inputs and outputs) and call run().When you call run(),
Happy serializes your job instance and copies it and all accompanyinglibraries
out to the Hado</p><p>6 0.20001175 <a title="601-tfidf-6" href="../high_scalability-2011/high_scalability-2011-07-27-Making_Hadoop_1000x_Faster_for_Graph_Problems.html">1088 high scalability-2011-07-27-Making Hadoop 1000x Faster for Graph Problems</a></p>
<p>7 0.19821298 <a title="601-tfidf-7" href="../high_scalability-2011/high_scalability-2011-01-04-Map-Reduce_With_Ruby_Using_Hadoop.html">968 high scalability-2011-01-04-Map-Reduce With Ruby Using Hadoop</a></p>
<p>8 0.19379076 <a title="601-tfidf-8" href="../high_scalability-2012/high_scalability-2012-06-15-Stuff_The_Internet_Says_On_Scalability_For_June_15%2C_2012.html">1265 high scalability-2012-06-15-Stuff The Internet Says On Scalability For June 15, 2012</a></p>
<p>9 0.18616119 <a title="601-tfidf-9" href="../high_scalability-2008/high_scalability-2008-11-22-Google_Architecture.html">448 high scalability-2008-11-22-Google Architecture</a></p>
<p>10 0.18331662 <a title="601-tfidf-10" href="../high_scalability-2010/high_scalability-2010-02-19-Twitter%E2%80%99s_Plan_to_Analyze_100_Billion_Tweets.html">780 high scalability-2010-02-19-Twitter’s Plan to Analyze 100 Billion Tweets</a></p>
<p>11 0.18286459 <a title="601-tfidf-11" href="../high_scalability-2010/high_scalability-2010-07-02-Hot_Scalability_Links_for_July_2%2C_2010.html">851 high scalability-2010-07-02-Hot Scalability Links for July 2, 2010</a></p>
<p>12 0.1817843 <a title="601-tfidf-12" href="../high_scalability-2010/high_scalability-2010-07-20-Strategy%3A_Consider_When_a_Service_Starts_Billing_in_Your_Algorithm_Cost.html">862 high scalability-2010-07-20-Strategy: Consider When a Service Starts Billing in Your Algorithm Cost</a></p>
<p>13 0.17657292 <a title="601-tfidf-13" href="../high_scalability-2009/high_scalability-2009-07-02-Product%3A_Hbase.html">650 high scalability-2009-07-02-Product: Hbase</a></p>
<p>14 0.1618737 <a title="601-tfidf-14" href="../high_scalability-2014/high_scalability-2014-01-28-How_Next_Big_Sound_Tracks_Over_a_Trillion_Song_Plays%2C_Likes%2C_and_More_Using_a_Version_Control_System_for_Hadoop_Data.html">1586 high scalability-2014-01-28-How Next Big Sound Tracks Over a Trillion Song Plays, Likes, and More Using a Version Control System for Hadoop Data</a></p>
<p>15 0.16114375 <a title="601-tfidf-15" href="../high_scalability-2012/high_scalability-2012-08-28-Making_Hadoop_Run_Faster.html">1313 high scalability-2012-08-28-Making Hadoop Run Faster</a></p>
<p>16 0.15613618 <a title="601-tfidf-16" href="../high_scalability-2008/high_scalability-2008-01-30-How_Rackspace_Now_Uses_MapReduce_and_Hadoop_to_Query_Terabytes_of_Data.html">233 high scalability-2008-01-30-How Rackspace Now Uses MapReduce and Hadoop to Query Terabytes of Data</a></p>
<p>17 0.15454005 <a title="601-tfidf-17" href="../high_scalability-2011/high_scalability-2011-03-08-Medialets_Architecture_-__Defeating_the_Daunting_Mobile_Device_Data_Deluge.html">1000 high scalability-2011-03-08-Medialets Architecture -  Defeating the Daunting Mobile Device Data Deluge</a></p>
<p>18 0.15366337 <a title="601-tfidf-18" href="../high_scalability-2009/high_scalability-2009-07-30-Learn_How_to_Think_at_Scale.html">666 high scalability-2009-07-30-Learn How to Think at Scale</a></p>
<p>19 0.14971599 <a title="601-tfidf-19" href="../high_scalability-2011/high_scalability-2011-07-08-Stuff_The_Internet_Says_On_Scalability_For_July_8%2C_2011.html">1076 high scalability-2011-07-08-Stuff The Internet Says On Scalability For July 8, 2011</a></p>
<p>20 0.14356641 <a title="601-tfidf-20" href="../high_scalability-2012/high_scalability-2012-11-05-Gone_Fishin%27%3A_Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_In_The_Ambient_Cloud.html">1355 high scalability-2012-11-05-Gone Fishin': Building Super Scalable Systems: Blade Runner Meets Autonomic Computing In The Ambient Cloud</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.236), (1, 0.095), (2, 0.039), (3, 0.073), (4, -0.008), (5, 0.071), (6, 0.093), (7, 0.009), (8, 0.141), (9, 0.187), (10, 0.068), (11, -0.056), (12, 0.121), (13, -0.137), (14, 0.148), (15, -0.053), (16, -0.063), (17, -0.01), (18, -0.065), (19, 0.064), (20, -0.006), (21, 0.108), (22, 0.053), (23, 0.068), (24, 0.014), (25, 0.01), (26, 0.151), (27, 0.015), (28, -0.011), (29, 0.083), (30, 0.089), (31, 0.126), (32, -0.011), (33, -0.009), (34, -0.012), (35, 0.029), (36, -0.089), (37, 0.06), (38, -0.019), (39, -0.073), (40, -0.005), (41, 0.027), (42, -0.066), (43, -0.051), (44, 0.023), (45, 0.016), (46, -0.001), (47, 0.037), (48, -0.031), (49, 0.023)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95929748 <a title="601-lsi-1" href="../high_scalability-2009/high_scalability-2009-05-17-Product%3A_Hadoop.html">601 high scalability-2009-05-17-Product: Hadoop</a></p>
<p>Introduction: Update 5:Hadoop Sorts a Petabyte in 16.25 Hours and a Terabyte in 62
Secondsand has itsgreen cred questionedbecause it took 40 times the number of
machines Greenplum used to do the same work.Update 4:Introduction to Pig. Pig
allows you to skip programming Hadoop at the low map-reduce level. You don't
have to know Java. Using the Pig Latin language, which is a scripting data
flow language, you can think about your problem as a data flow program. 10
lines of Pig Latin = 200 lines of Java.Update 3: Scaling Hadoop to4000 nodes
at Yahoo!. 30,000 cores with nearly 16PB of raw disk; sorted 6TB of data
completed in 37 minutes; 14,000 map tasks writes (reads) 360 MB (about 3
blocks) of data into a single file with a total of 5.04 TB for the whole
job.Update 2: HadoopSummit and Data-Intensive Computing Symposium Videos and
Slides. Topics include: Pig, JAQL, Hbase, Hive, Data-Intensive Scalable
Computing, Clouds and ManyCore: The Revolution, Simplicity and Complexity in
Data Systems at Scale, Han</p><p>2 0.91862696 <a title="601-lsi-2" href="../high_scalability-2011/high_scalability-2011-01-04-Map-Reduce_With_Ruby_Using_Hadoop.html">968 high scalability-2011-01-04-Map-Reduce With Ruby Using Hadoop</a></p>
<p>Introduction: A demonstration, with repeatable steps, of how to quickly fire-up a Hadoop
cluster on Amazon EC2, load data onto the HDFS (Hadoop Distributed File-
System), write map-reduce scripts in Ruby and use them to run a map-reduce job
on your Hadoop cluster. You willnotneed to ssh into the cluster, as all tasks
are run from your local machine. Below I am using my MacBook Pro as my local
machine, but the steps I have provided should be reproducible on other
platforms running bash and Java.Fire-Up Your Hadoop ClusterI choose
theCloudera distribution of Hadoopwhich is still 100% Apache licensed, but has
some additional benefits. One of these benefits is that it is released byDoug
Cutting, who started Hadoop and drove it’s development at Yahoo! He also
startedLucene, which is another of my favourite Apache Projects, so I have
good faith that he knows what he is doing. Another benefit, as you will see,
is that it is simple to fire-up a Hadoop cluster.I am going to use
Cloudera’sWhirr script, which</p><p>3 0.86821401 <a title="601-lsi-3" href="../high_scalability-2008/high_scalability-2008-11-14-Paper%3A_Pig_Latin%3A_A_Not-So-Foreign_Language_for_Data_Processing.html">443 high scalability-2008-11-14-Paper: Pig Latin: A Not-So-Foreign Language for Data Processing</a></p>
<p>Introduction: Yahoo has developed a new language called Pig Latin that fit in a sweet spot
between high-level declarative querying in the spirit of SQL, and low-level,
procedural programming `a la map-reduce and combines best of both worlds.The
accompanying system, Pig, is fully implemented, and compiles Pig Latin into
physical plans that are executed over Hadoop, an open-source, map-reduce
implementation. Pig has just graduated from the Apache Incubator and joined
Hadoop as a subproject.The paper has a few examples of how engineers at Yahoo!
are using Pig to dramatically reduce the time required for the development and
execution of their data analysis tasks, compared tousing Hadoop
directly.References:Apache Pig Wiki</p><p>4 0.83739948 <a title="601-lsi-4" href="../high_scalability-2008/high_scalability-2008-09-28-Product%3A_Happy_%3D_Hadoop_%2B_Python.html">397 high scalability-2008-09-28-Product: Happy = Hadoop + Python</a></p>
<p>Introduction: Has a Java only Hadoop been getting you down? Now you can beHappy. Happy is
aframework for writing map-reduce programs for Hadoop using Jython. It files
off the sharp edges on Hadoop and makes writing map-reduce programs a breeze.
There's really no history yet on Happy, but I'm delighted at the idea of being
able to map-reduce inother languages. The more ways the better.From the
website:Happy is a framework that allows Hadoop jobs to be written and run in
Python 2.2 using Jython. It is aneasy way to write map-reduce programs for
Hadoop, and includes some new useful features as well.The current release
supports Hadoop 0.17.2.Map-reduce jobs in Happy are defined by sub-classing
happy.HappyJob and implementing amap(records, task) and reduce(key, values,
task) function. Then you create an instance of theclass, set the job
parameters (such as inputs and outputs) and call run().When you call run(),
Happy serializes your job instance and copies it and all accompanyinglibraries
out to the Hado</p><p>5 0.83311063 <a title="601-lsi-5" href="../high_scalability-2009/high_scalability-2009-06-11-Yahoo%21_Distribution_of_Hadoop.html">627 high scalability-2009-06-11-Yahoo! Distribution of Hadoop</a></p>
<p>Introduction: Many people in the Apache Hadoop community have asked Yahoo! to publish the
version of Apache Hadoop they test and deploy across their large Hadoop
clusters. As a service to the Hadoop community, Yahoo is releasing the Yahoo!
Distribution of Hadoop -- a source code distribution that is based entirely on
code found in the Apache Hadoop project.This source distribution includes code
patches that they have added to improve the stability and performance of their
clusters. In all cases, these patches have already been contributed back to
Apache, but they may not yet be available in an Apache release of Hadoop.Read
more and get the Hadoop distribution from Yahoo</p><p>6 0.82844621 <a title="601-lsi-6" href="../high_scalability-2008/high_scalability-2008-10-15-Hadoop_-_A_Primer.html">414 high scalability-2008-10-15-Hadoop - A Primer</a></p>
<p>7 0.8097918 <a title="601-lsi-7" href="../high_scalability-2009/high_scalability-2009-08-03-Building_a_Data_Intensive_Web_Application_with_Cloudera%2C_Hadoop%2C_Hive%2C_Pig%2C_and_EC2.html">669 high scalability-2009-08-03-Building a Data Intensive Web Application with Cloudera, Hadoop, Hive, Pig, and EC2</a></p>
<p>8 0.79654783 <a title="601-lsi-8" href="../high_scalability-2010/high_scalability-2010-07-20-Strategy%3A_Consider_When_a_Service_Starts_Billing_in_Your_Algorithm_Cost.html">862 high scalability-2010-07-20-Strategy: Consider When a Service Starts Billing in Your Algorithm Cost</a></p>
<p>9 0.78662473 <a title="601-lsi-9" href="../high_scalability-2013/high_scalability-2013-04-24-Strategy%3A_Using_Lots_of_RAM_Often_Cheaper_than_Using_a_Hadoop_Cluster.html">1445 high scalability-2013-04-24-Strategy: Using Lots of RAM Often Cheaper than Using a Hadoop Cluster</a></p>
<p>10 0.78105348 <a title="601-lsi-10" href="../high_scalability-2012/high_scalability-2012-06-15-Stuff_The_Internet_Says_On_Scalability_For_June_15%2C_2012.html">1265 high scalability-2012-06-15-Stuff The Internet Says On Scalability For June 15, 2012</a></p>
<p>11 0.77692831 <a title="601-lsi-11" href="../high_scalability-2012/high_scalability-2012-08-28-Making_Hadoop_Run_Faster.html">1313 high scalability-2012-08-28-Making Hadoop Run Faster</a></p>
<p>12 0.77340639 <a title="601-lsi-12" href="../high_scalability-2008/high_scalability-2008-02-19-Hadoop_Getting_Closer_to_1.0_Release.html">254 high scalability-2008-02-19-Hadoop Getting Closer to 1.0 Release</a></p>
<p>13 0.7704463 <a title="601-lsi-13" href="../high_scalability-2012/high_scalability-2012-01-12-Peregrine_-_A_Map_Reduce_Framework_for_Iterative_and_Pipelined_Jobs.html">1173 high scalability-2012-01-12-Peregrine - A Map Reduce Framework for Iterative and Pipelined Jobs</a></p>
<p>14 0.7691738 <a title="601-lsi-14" href="../high_scalability-2009/high_scalability-2009-07-02-Product%3A_Hbase.html">650 high scalability-2009-07-02-Product: Hbase</a></p>
<p>15 0.73188061 <a title="601-lsi-15" href="../high_scalability-2010/high_scalability-2010-07-02-Hot_Scalability_Links_for_July_2%2C_2010.html">851 high scalability-2010-07-02-Hot Scalability Links for July 2, 2010</a></p>
<p>16 0.72283089 <a title="601-lsi-16" href="../high_scalability-2013/high_scalability-2013-01-07-Analyzing_billions_of_credit_card_transactions_and_serving_low-latency_insights_in_the_cloud.html">1382 high scalability-2013-01-07-Analyzing billions of credit card transactions and serving low-latency insights in the cloud</a></p>
<p>17 0.70037162 <a title="601-lsi-17" href="../high_scalability-2009/high_scalability-2009-07-30-Learn_How_to_Think_at_Scale.html">666 high scalability-2009-07-30-Learn How to Think at Scale</a></p>
<p>18 0.6973666 <a title="601-lsi-18" href="../high_scalability-2009/high_scalability-2009-09-17-Hot_Links_for_2009-9-17_.html">707 high scalability-2009-09-17-Hot Links for 2009-9-17 </a></p>
<p>19 0.68399012 <a title="601-lsi-19" href="../high_scalability-2011/high_scalability-2011-07-08-Stuff_The_Internet_Says_On_Scalability_For_July_8%2C_2011.html">1076 high scalability-2011-07-08-Stuff The Internet Says On Scalability For July 8, 2011</a></p>
<p>20 0.67059356 <a title="601-lsi-20" href="../high_scalability-2011/high_scalability-2011-07-27-Making_Hadoop_1000x_Faster_for_Graph_Problems.html">1088 high scalability-2011-07-27-Making Hadoop 1000x Faster for Graph Problems</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.103), (2, 0.225), (4, 0.013), (10, 0.05), (30, 0.023), (52, 0.091), (56, 0.018), (61, 0.071), (79, 0.269), (85, 0.027), (94, 0.033)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.96448809 <a title="601-lda-1" href="../high_scalability-2011/high_scalability-2011-05-27-Stuff_The_Internet_Says_On_Scalability_For_May_27%2C_2011.html">1048 high scalability-2011-05-27-Stuff The Internet Says On Scalability For May 27, 2011</a></p>
<p>Introduction: Submitted for your scaling pleasure: Good idea:Open The Index And Speed Up The
Internet.SmugMug estimates 50%of their CPU is spent serving crawler robots.
Having a common meta-data repository wouldn't prevent search engines from
having their own special sauce. Then the problem becomes one of syncing data
between repositories and processing change events. A generous soul could even
offer a shared MapReduce service over the data. Now that wouldspeed up the
internet.Scaling Achievements:YouTube Sees 3 Billion Views per Day;Twitter
produces a sustained feed of 35 Mb per second; companies processing billions
of APIs calls (Twitter, Netflix, Amazon, NPR, Google, Facebook, eBay, Bing);
Astronomers Identify the Farthest Object Ever Observed, 13.14 Billion Light
Years AwayQuotes that are Quotably Quotable:eekygeeky: When cloud computing
news is slow? Switch to "big data"-100% of the vaguery, none of the used-up,
mushy marketing feel!singhns: an API is like diamonds, a huge range of value
based</p><p>2 0.96365923 <a title="601-lda-2" href="../high_scalability-2013/high_scalability-2013-07-19-Stuff_The_Internet_Says_On_Scalability_For_July_19%2C_2013.html">1494 high scalability-2013-07-19-Stuff The Internet Says On Scalability For July 19, 2013</a></p>
<p>Introduction: Hey, it's HighScalability time:(Still not a transporter:Looping at 685 mph)898
exabytes: US storage, 1/3 global total; 1 Kb/s: data transmit rate from
harvestable energy from human motionCreate your own trust nobody point-to-
point private cloud. Dan Brown shows how step-by-step in How I Created My Own
Personal Cloud Using BitTorrent Sync, Owncloud, and Raspberry Pi. BitTorrent
Sync is used to copy large files around. Raspberry Pi is a cheap low power
always on device with BitTorrent Sync installed. Owncloud is an open source
cloud that provides a web interface for file access files from anywhere.This
is different. Funding astartup using Airbnb as a source of start-up capital.
It beats getting a part-time job and one of your guests might even be a
VC.This is not different. Old industries clawing and digging in, using the
tools of power to beat back competition. Steve Blank details a familiar story
in Strangling Innovation: Tesla versus "Rent Seekers". The thing is nobody
really wants t</p><p>3 0.96317995 <a title="601-lda-3" href="../high_scalability-2013/high_scalability-2013-03-08-Stuff_The_Internet_Says_On_Scalability_For_March_8%2C_2013.html">1420 high scalability-2013-03-08-Stuff The Internet Says On Scalability For March 8, 2013</a></p>
<p>Introduction: Hey, it's HighScalability time: Quotable Quotes:@ibogost: Disabling features
of SimCity due to ineffective central infrastructure is probably the most
realistic simulation of the modern city.antirez: The point is simply to show
how SSDs can't be considered, currently, as a bit slower version of memory.
Their performance characteristics are a lot more about, simply, "faster
disks".@jessenoller: I only use JavaScript so I can gain maximum scalability
across multiple cores. Also unicorns. Paint thinner gingerbread@liammclennan:
high-scalability ruby. Why bother?@scomma: Problem with BitCoin is not
scalability, not even usability. It's whether someone will crack the algorithm
and render BTC entirely useless.@webclimber: Amazing how often I find myself
explaining that scalability is not magical@mvmsan: Flash as Primary Storage -
Highest Cost, Lack of HA, scalability and management features #flash #SSD
#CIO@pneuman42: Game servers are the *worst* scalability problem. Most
services start smal</p><p>4 0.96090311 <a title="601-lda-4" href="../high_scalability-2010/high_scalability-2010-03-02-Using_the_Ambient_Cloud_as_an_Application_Runtime.html">786 high scalability-2010-03-02-Using the Ambient Cloud as an Application Runtime</a></p>
<p>Introduction: This is an excerpt from my articleBuilding Super Scalable Systems: Blade
Runner Meets Autonomic Computing in the Ambient Cloud.The future looks many,
big, complex, and adaptive:Many clouds.Many servers.Many operating
systems.Many languages.Many storage services.Many database services.Many
software services.Many adjunct human networks (like Mechanical Turk).Many fast
interconnects.Many CDNs.Many cache memory pools.Many application profiles
(simple request-response, live streaming, computationally complex, sensor
driven, memory intensive, storage intensive, monolithic, decomposable,
etc).Many legal jurisdictions. Don't want to perform a function on Patriot Act
"protected" systems then move the function elsewhere.Many SLAs.Many data
driven pricing policies that like airplane pricing algorithms will price
"seats" to maximize profit using multi-variate time sensitive pricing
models.Many competitive products. The need to defend your territory never
seems to go away. Though what will map to s</p><p>5 0.95974785 <a title="601-lda-5" href="../high_scalability-2008/high_scalability-2008-02-11-Yahoo_Live%27s_Scaling_Problems_Prove%3A_Release_Early_and_Often_-_Just_Don%27t_Screw_Up.html">244 high scalability-2008-02-11-Yahoo Live's Scaling Problems Prove: Release Early and Often - Just Don't Screw Up</a></p>
<p>Introduction: Tech Crunch chomped down on some initial scaling problems with Yahoo's new
live video streaming serviceYahoo Live. After a bit of chewing on Yahoo's old
bones, TC spat out:If Yahoo cant scale something like this (no matter how much
they claim it’s an experiment, it’s still a live service), it shows how far
the once brightest star of the online world has fallen.This kind of thinking
kills innovation. When there's no room for a few hiccups or a little failure
you have to cover your ass so completely nothing new will ever see the light
of day.breakI thought we were supposed to beagile. We are supposed to release
early and often. Not every 'i' has to be dotted and not every last router has
to be installed before we take the first step of a grand new journey.Get it
out there. Let users help you make it better. Listen to customers, make
changes, push the new code out, listen some more, and fix problems as they
come up. Following this process we'll make something the customer wants and
needs</p><p>6 0.95867616 <a title="601-lda-6" href="../high_scalability-2013/high_scalability-2013-07-01-PRISM%3A_The_Amazingly_Low_Cost_of_%C2%ADUsing_BigData_to_Know_More_About_You_in_Under_a_Minute.html">1485 high scalability-2013-07-01-PRISM: The Amazingly Low Cost of ­Using BigData to Know More About You in Under a Minute</a></p>
<p>7 0.95744413 <a title="601-lda-7" href="../high_scalability-2013/high_scalability-2013-02-08-Stuff_The_Internet_Says_On_Scalability_For_February_8%2C_2013.html">1403 high scalability-2013-02-08-Stuff The Internet Says On Scalability For February 8, 2013</a></p>
<p>same-blog 8 0.95422256 <a title="601-lda-8" href="../high_scalability-2009/high_scalability-2009-05-17-Product%3A_Hadoop.html">601 high scalability-2009-05-17-Product: Hadoop</a></p>
<p>9 0.95407987 <a title="601-lda-9" href="../high_scalability-2008/high_scalability-2008-11-22-Google_Architecture.html">448 high scalability-2008-11-22-Google Architecture</a></p>
<p>10 0.9527486 <a title="601-lda-10" href="../high_scalability-2010/high_scalability-2010-07-27-YeSQL%3A_An_Overview_of_the_Various_Query_Semantics_in_the_Post_Only-SQL_World.html">867 high scalability-2010-07-27-YeSQL: An Overview of the Various Query Semantics in the Post Only-SQL World</a></p>
<p>11 0.95141846 <a title="601-lda-11" href="../high_scalability-2010/high_scalability-2010-08-18-Misco%3A_A_MapReduce_Framework_for_Mobile_Systems_-_Start_of_the_Ambient_Cloud%3F.html">882 high scalability-2010-08-18-Misco: A MapReduce Framework for Mobile Systems - Start of the Ambient Cloud?</a></p>
<p>12 0.94881541 <a title="601-lda-12" href="../high_scalability-2010/high_scalability-2010-08-04-Dremel%3A_Interactive_Analysis_of_Web-Scale_Datasets_-_Data_as_a_Programming_Paradigm.html">871 high scalability-2010-08-04-Dremel: Interactive Analysis of Web-Scale Datasets - Data as a Programming Paradigm</a></p>
<p>13 0.94543636 <a title="601-lda-13" href="../high_scalability-2012/high_scalability-2012-09-24-Google_Spanner%27s_Most_Surprising_Revelation%3A_NoSQL_is_Out_and_NewSQL_is_In.html">1328 high scalability-2012-09-24-Google Spanner's Most Surprising Revelation: NoSQL is Out and NewSQL is In</a></p>
<p>14 0.94541508 <a title="601-lda-14" href="../high_scalability-2013/high_scalability-2013-01-23-Building_Redundant_Datacenter_Networks_is_Not_For_Sissies_-_Use_an_Outside_WAN_Backbone.html">1392 high scalability-2013-01-23-Building Redundant Datacenter Networks is Not For Sissies - Use an Outside WAN Backbone</a></p>
<p>15 0.94498569 <a title="601-lda-15" href="../high_scalability-2009/high_scalability-2009-08-13-Reconnoiter_-_Large-Scale_Trending_and_Fault-Detection.html">680 high scalability-2009-08-13-Reconnoiter - Large-Scale Trending and Fault-Detection</a></p>
<p>16 0.94099176 <a title="601-lda-16" href="../high_scalability-2009/high_scalability-2009-06-11-Yahoo%21_Distribution_of_Hadoop.html">627 high scalability-2009-06-11-Yahoo! Distribution of Hadoop</a></p>
<p>17 0.93935794 <a title="601-lda-17" href="../high_scalability-2007/high_scalability-2007-09-10-Is_there_a_difference_between_partitioning_and_federation_and_sharding%3F.html">89 high scalability-2007-09-10-Is there a difference between partitioning and federation and sharding?</a></p>
<p>18 0.9392857 <a title="601-lda-18" href="../high_scalability-2007/high_scalability-2007-07-30-Build_an_Infinitely_Scalable_Infrastructure_for_%24100_Using_Amazon_Services.html">38 high scalability-2007-07-30-Build an Infinitely Scalable Infrastructure for $100 Using Amazon Services</a></p>
<p>19 0.93766218 <a title="601-lda-19" href="../high_scalability-2011/high_scalability-2011-08-15-Should_any_cloud_be_considered_one_availability_zone%3F_The_Amazon_experience_says_yes..html">1098 high scalability-2011-08-15-Should any cloud be considered one availability zone? The Amazon experience says yes.</a></p>
<p>20 0.93679565 <a title="601-lda-20" href="../high_scalability-2008/high_scalability-2008-03-27-Amazon_Announces_Static_IP_Addresses_and_Multiple_Datacenter_Operation.html">289 high scalability-2008-03-27-Amazon Announces Static IP Addresses and Multiple Datacenter Operation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
