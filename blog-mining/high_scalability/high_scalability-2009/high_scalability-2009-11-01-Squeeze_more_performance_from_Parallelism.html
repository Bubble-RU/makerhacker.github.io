<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>735 high scalability-2009-11-01-Squeeze more performance from Parallelism</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2009" href="../home/high_scalability-2009_home.html">high_scalability-2009</a> <a title="high_scalability-2009-735" href="#">high_scalability-2009-735</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>735 high scalability-2009-11-01-Squeeze more performance from Parallelism</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2009-735-html" href="http://highscalability.com//blog/2009/11/1/squeeze-more-performance-from-parallelism.html">html</a></p><p>Introduction: In many posts, such as:  The Future of the Parallelism and its Challenges  I mentioned that synchronization the access to the shared resource is the major challenge to write parallel code.
 
The synchronization and coordination take long time from the overall execution time, which reduce the benefits of the parallelism; the synchronization and coordination also reduce the scalability.
 
There are many forms of synchronization and coordination, such as:
  
 Create Task object in frameworks such as: Microsoft TPL, Intel TDD, and Parallel Runtime Library. Create and enqueue task objects require synchronization that it’s takes long time especially if we create it into recursive work such as: Quick Sort algorithm. 
 Synchronization the access to shared data. 
  
But there are a few techniques to avoid these issues, such as: Shared-Nothing, Actor Model, and Hyper Object (A.K.A. Combinable Object). Simply if we reduce the shared data by re-architect our code this will gives us a huge benefits</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In many posts, such as:  The Future of the Parallelism and its Challenges  I mentioned that synchronization the access to the shared resource is the major challenge to write parallel code. [sent-1, score-1.34]
</p><p>2 The synchronization and coordination take long time from the overall execution time, which reduce the benefits of the parallelism; the synchronization and coordination also reduce the scalability. [sent-2, score-2.65]
</p><p>3 There are many forms of synchronization and coordination, such as:     Create Task object in frameworks such as: Microsoft TPL, Intel TDD, and Parallel Runtime Library. [sent-3, score-1.041]
</p><p>4 Create and enqueue task objects require synchronization that it’s takes long time especially if we create it into recursive work such as: Quick Sort algorithm. [sent-4, score-1.602]
</p><p>5 But there are a few techniques to avoid these issues, such as: Shared-Nothing, Actor Model, and Hyper Object (A. [sent-6, score-0.142]
</p><p>6 Simply if we reduce the shared data by re-architect our code this will gives us a huge benefits in performance and scalability. [sent-10, score-0.729]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('synchronization', 0.559), ('coordination', 0.342), ('object', 0.219), ('parallelism', 0.197), ('enqueue', 0.191), ('tdd', 0.191), ('reduce', 0.186), ('shared', 0.186), ('hyper', 0.175), ('task', 0.165), ('benefits', 0.163), ('recursive', 0.161), ('actor', 0.129), ('create', 0.126), ('forms', 0.116), ('parallel', 0.114), ('runtime', 0.112), ('mentioned', 0.102), ('intel', 0.1), ('frameworks', 0.096), ('long', 0.089), ('posts', 0.088), ('access', 0.087), ('execution', 0.084), ('quick', 0.078), ('overall', 0.078), ('challenge', 0.076), ('objects', 0.072), ('techniques', 0.071), ('avoid', 0.071), ('sort', 0.07), ('challenges', 0.069), ('microsoft', 0.065), ('require', 0.064), ('especially', 0.063), ('time', 0.062), ('simply', 0.061), ('major', 0.061), ('huge', 0.059), ('gives', 0.059), ('resource', 0.059), ('future', 0.055), ('many', 0.051), ('issues', 0.051), ('takes', 0.05), ('http', 0.046), ('model', 0.046), ('write', 0.045), ('us', 0.041), ('code', 0.035)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="735-tfidf-1" href="../high_scalability-2009/high_scalability-2009-11-01-Squeeze_more_performance_from_Parallelism.html">735 high scalability-2009-11-01-Squeeze more performance from Parallelism</a></p>
<p>Introduction: In many posts, such as:  The Future of the Parallelism and its Challenges  I mentioned that synchronization the access to the shared resource is the major challenge to write parallel code.
 
The synchronization and coordination take long time from the overall execution time, which reduce the benefits of the parallelism; the synchronization and coordination also reduce the scalability.
 
There are many forms of synchronization and coordination, such as:
  
 Create Task object in frameworks such as: Microsoft TPL, Intel TDD, and Parallel Runtime Library. Create and enqueue task objects require synchronization that it’s takes long time especially if we create it into recursive work such as: Quick Sort algorithm. 
 Synchronization the access to shared data. 
  
But there are a few techniques to avoid these issues, such as: Shared-Nothing, Actor Model, and Hyper Object (A.K.A. Combinable Object). Simply if we reduce the shared data by re-architect our code this will gives us a huge benefits</p><p>2 0.34726974 <a title="735-tfidf-2" href="../high_scalability-2013/high_scalability-2013-10-31-Paper%3A_Everything_You_Always_Wanted_to_Know_About_Synchronization_but_Were_Afraid_to_Ask.html">1541 high scalability-2013-10-31-Paper: Everything You Always Wanted to Know About Synchronization but Were Afraid to Ask</a></p>
<p>Introduction: Awesome paper on how particular synchronization mechanisms scale on multi-core architectures:  Everything You Always Wanted to Know About Synchronization but Were Afraid to Ask .
 
The goal is to pick a locking approach that doesn't degrade as the number of cores increase. Like everything else in life, that doesn't appear to be generically possible:
  
 None of the nine locking schemes we consider consistently outperforms any other one, on all target architectures or workloads. Strictly speaking, to seek optimality,  a lock algorithm should thus be selected based on the hardware platform and the expected workload .  
  
Abstract:
  

This paper presents the most exhaustive study of synchronization to date. We span multiple layers, from hardware cache-coherence protocols up to high-level concurrent software. We do so on different types architectures, from single-socket – uniform and nonuniform – to multi-socket – directory and broadcastbased – many-cores. We draw a set of observations t</p><p>3 0.14693722 <a title="735-tfidf-3" href="../high_scalability-2009/high_scalability-2009-05-27-The_Future_of_the_Parallelism_and_its_Challenges.html">608 high scalability-2009-05-27-The Future of the Parallelism and its Challenges</a></p>
<p>Introduction: The Future of the Parallelism and its Challenges    Research and education in Parallel computing technologies is more important than ever. Here I present a perspective on the past contributions, current status, and future direction of the parallelism technologies.     While machine power will grow impressively, increased parallelism, rather than clock rate, will be driving force in computing in the foreseeable future. This ongoing shift toward parallel architectural paradigms is one of the greatest challenges for the microprocessor and software industries. In 2005, Justin Ratter, chief technology officer of Intel Corporation, said ‘We are at the cusp of a transition to multicore, multithreaded architectures, and we still have not demonstrated the ease of programming the move will require…’      Key points:       A Little history   Parallelism Challenges   Under the hood, Parallelism Challenges    Synchronization problems   CAS problems       The future of the parallelism</p><p>4 0.14549688 <a title="735-tfidf-4" href="../high_scalability-2013/high_scalability-2013-05-16-Paper%3A_Warp%3A_Multi-Key_Transactions_for_Key-Value_Stores.html">1459 high scalability-2013-05-16-Paper: Warp: Multi-Key Transactions for Key-Value Stores</a></p>
<p>Introduction: Looks like an interesting take on "a completely asynchronous, low-latency transaction management protocol, in line with the fully distributed NoSQL architecture."
 
 Warp: Multi-Key Transactions for Key-Value Stores Â overview:
  

Implementing ACID transactions has been a longstanding challenge for NoSQL systems. Because these systems are based on a sharded architecture, transactions necessarily require coordination across multiple servers. Past work in this space has relied either on heavyweight protocols such as Paxos or clock synchronization for this coordination.


This paper presents a novel protocol for coordinating distributed transactions with ACID semantics on top of a sharded data store. Called linear transactions, this protocol achieves scalability by distributing the coordination task to only those servers that hold relevant data for each transaction. It achieves high performance by serializing only those transactions whose concurrent execution could potentially yield a vio</p><p>5 0.13052151 <a title="735-tfidf-5" href="../high_scalability-2009/high_scalability-2009-05-31-Parallel_Programming_for_real-world.html">612 high scalability-2009-05-31-Parallel Programming for real-world</a></p>
<p>Introduction: Multicore computers shift the burden of software performance from chip designers and architects to software developers.   
What is the parallel Computing ? and what the different between Multi-Threading and Concurrency and Parallelism ? and what is differences between task and data parallel ?  and how we can use it ?    Fundamental article into Parallel Programming...</p><p>6 0.1286096 <a title="735-tfidf-6" href="../high_scalability-2012/high_scalability-2012-03-06-Ask_For_Forgiveness_Programming_-_Or_How_We%27ll_Program_1000_Cores.html">1204 high scalability-2012-03-06-Ask For Forgiveness Programming - Or How We'll Program 1000 Cores</a></p>
<p>7 0.12072867 <a title="735-tfidf-7" href="../high_scalability-2009/high_scalability-2009-06-23-Learn_How_to_Exploit_Multiple_Cores_for_Better_Performance_and_Scalability.html">636 high scalability-2009-06-23-Learn How to Exploit Multiple Cores for Better Performance and Scalability</a></p>
<p>8 0.11671625 <a title="735-tfidf-8" href="../high_scalability-2008/high_scalability-2008-08-29-Product%3A_ScaleOut_StateServer_is_Memcached_on_Steroids.html">373 high scalability-2008-08-29-Product: ScaleOut StateServer is Memcached on Steroids</a></p>
<p>9 0.11250311 <a title="735-tfidf-9" href="../high_scalability-2009/high_scalability-2009-07-08-Art_of_Parallelism_presentation.html">652 high scalability-2009-07-08-Art of Parallelism presentation</a></p>
<p>10 0.11215173 <a title="735-tfidf-10" href="../high_scalability-2013/high_scalability-2013-10-25-Stuff_The_Internet_Says_On_Scalability_For_October_25th%2C_2013.html">1537 high scalability-2013-10-25-Stuff The Internet Says On Scalability For October 25th, 2013</a></p>
<p>11 0.10521705 <a title="735-tfidf-11" href="../high_scalability-2013/high_scalability-2013-03-18-Beyond_Threads_and_Callbacks_-_Application_Architecture_Pros_and_Cons.html">1425 high scalability-2013-03-18-Beyond Threads and Callbacks - Application Architecture Pros and Cons</a></p>
<p>12 0.097164139 <a title="735-tfidf-12" href="../high_scalability-2013/high_scalability-2013-02-27-42_Monster_Problems_that_Attack_as_Loads_Increase.html">1413 high scalability-2013-02-27-42 Monster Problems that Attack as Loads Increase</a></p>
<p>13 0.095035225 <a title="735-tfidf-13" href="../high_scalability-2013/high_scalability-2013-05-08-Typesafe_Interview%3A_Scala_%2B_Akka_is_an_IaaS_for_Your_Process_Architecture.html">1454 high scalability-2013-05-08-Typesafe Interview: Scala + Akka is an IaaS for Your Process Architecture</a></p>
<p>14 0.093136288 <a title="735-tfidf-14" href="../high_scalability-2008/high_scalability-2008-07-15-ZooKeeper_-_A_Reliable%2C_Scalable_Distributed_Coordination_System_.html">350 high scalability-2008-07-15-ZooKeeper - A Reliable, Scalable Distributed Coordination System </a></p>
<p>15 0.087678209 <a title="735-tfidf-15" href="../high_scalability-2011/high_scalability-2011-06-06-NoSQL_Pain%3F_Learn_How_to_Read-write_Scale_Without_a_Complete_Re-write.html">1054 high scalability-2011-06-06-NoSQL Pain? Learn How to Read-write Scale Without a Complete Re-write</a></p>
<p>16 0.087344639 <a title="735-tfidf-16" href="../high_scalability-2013/high_scalability-2013-03-25-AppBackplane_-_A_Framework_for_Supporting_Multiple_Application_Architectures.html">1429 high scalability-2013-03-25-AppBackplane - A Framework for Supporting Multiple Application Architectures</a></p>
<p>17 0.079122841 <a title="735-tfidf-17" href="../high_scalability-2009/high_scalability-2009-07-21-Paper%3A_Parallelizing_the_Web_Browser.html">660 high scalability-2009-07-21-Paper: Parallelizing the Web Browser</a></p>
<p>18 0.079071976 <a title="735-tfidf-18" href="../high_scalability-2008/high_scalability-2008-09-28-Product%3A_Happy_%3D_Hadoop_%2B_Python.html">397 high scalability-2008-09-28-Product: Happy = Hadoop + Python</a></p>
<p>19 0.077665292 <a title="735-tfidf-19" href="../high_scalability-2009/high_scalability-2009-04-21-Thread_Pool_Engine_in_MS_CLR_4%2C_and_Work-Stealing_scheduling_algorithm.html">575 high scalability-2009-04-21-Thread Pool Engine in MS CLR 4, and Work-Stealing scheduling algorithm</a></p>
<p>20 0.07517492 <a title="735-tfidf-20" href="../high_scalability-2008/high_scalability-2008-12-06-Paper%3A_Real-world_Concurrency.html">462 high scalability-2008-12-06-Paper: Real-world Concurrency</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.095), (1, 0.059), (2, 0.008), (3, 0.025), (4, -0.013), (5, 0.052), (6, 0.034), (7, 0.039), (8, -0.095), (9, -0.009), (10, 0.002), (11, 0.018), (12, -0.004), (13, -0.029), (14, -0.031), (15, -0.064), (16, -0.008), (17, -0.003), (18, 0.023), (19, -0.017), (20, -0.004), (21, -0.02), (22, -0.041), (23, 0.004), (24, -0.005), (25, -0.018), (26, 0.029), (27, 0.017), (28, 0.073), (29, 0.005), (30, 0.025), (31, 0.056), (32, -0.019), (33, 0.051), (34, -0.03), (35, 0.001), (36, 0.095), (37, -0.001), (38, 0.088), (39, 0.021), (40, -0.04), (41, 0.064), (42, -0.054), (43, -0.018), (44, -0.068), (45, 0.003), (46, 0.019), (47, -0.011), (48, 0.033), (49, 0.066)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96565241 <a title="735-lsi-1" href="../high_scalability-2009/high_scalability-2009-11-01-Squeeze_more_performance_from_Parallelism.html">735 high scalability-2009-11-01-Squeeze more performance from Parallelism</a></p>
<p>Introduction: In many posts, such as:  The Future of the Parallelism and its Challenges  I mentioned that synchronization the access to the shared resource is the major challenge to write parallel code.
 
The synchronization and coordination take long time from the overall execution time, which reduce the benefits of the parallelism; the synchronization and coordination also reduce the scalability.
 
There are many forms of synchronization and coordination, such as:
  
 Create Task object in frameworks such as: Microsoft TPL, Intel TDD, and Parallel Runtime Library. Create and enqueue task objects require synchronization that it’s takes long time especially if we create it into recursive work such as: Quick Sort algorithm. 
 Synchronization the access to shared data. 
  
But there are a few techniques to avoid these issues, such as: Shared-Nothing, Actor Model, and Hyper Object (A.K.A. Combinable Object). Simply if we reduce the shared data by re-architect our code this will gives us a huge benefits</p><p>2 0.81312597 <a title="735-lsi-2" href="../high_scalability-2013/high_scalability-2013-10-31-Paper%3A_Everything_You_Always_Wanted_to_Know_About_Synchronization_but_Were_Afraid_to_Ask.html">1541 high scalability-2013-10-31-Paper: Everything You Always Wanted to Know About Synchronization but Were Afraid to Ask</a></p>
<p>Introduction: Awesome paper on how particular synchronization mechanisms scale on multi-core architectures:  Everything You Always Wanted to Know About Synchronization but Were Afraid to Ask .
 
The goal is to pick a locking approach that doesn't degrade as the number of cores increase. Like everything else in life, that doesn't appear to be generically possible:
  
 None of the nine locking schemes we consider consistently outperforms any other one, on all target architectures or workloads. Strictly speaking, to seek optimality,  a lock algorithm should thus be selected based on the hardware platform and the expected workload .  
  
Abstract:
  

This paper presents the most exhaustive study of synchronization to date. We span multiple layers, from hardware cache-coherence protocols up to high-level concurrent software. We do so on different types architectures, from single-socket – uniform and nonuniform – to multi-socket – directory and broadcastbased – many-cores. We draw a set of observations t</p><p>3 0.69421315 <a title="735-lsi-3" href="../high_scalability-2009/high_scalability-2009-05-27-The_Future_of_the_Parallelism_and_its_Challenges.html">608 high scalability-2009-05-27-The Future of the Parallelism and its Challenges</a></p>
<p>Introduction: The Future of the Parallelism and its Challenges    Research and education in Parallel computing technologies is more important than ever. Here I present a perspective on the past contributions, current status, and future direction of the parallelism technologies.     While machine power will grow impressively, increased parallelism, rather than clock rate, will be driving force in computing in the foreseeable future. This ongoing shift toward parallel architectural paradigms is one of the greatest challenges for the microprocessor and software industries. In 2005, Justin Ratter, chief technology officer of Intel Corporation, said ‘We are at the cusp of a transition to multicore, multithreaded architectures, and we still have not demonstrated the ease of programming the move will require…’      Key points:       A Little history   Parallelism Challenges   Under the hood, Parallelism Challenges    Synchronization problems   CAS problems       The future of the parallelism</p><p>4 0.6785115 <a title="735-lsi-4" href="../high_scalability-2009/high_scalability-2009-04-21-Thread_Pool_Engine_in_MS_CLR_4%2C_and_Work-Stealing_scheduling_algorithm.html">575 high scalability-2009-04-21-Thread Pool Engine in MS CLR 4, and Work-Stealing scheduling algorithm</a></p>
<p>Introduction: I just saw this article in   HFadeel blog   that spaek about Parallelism in .NET Framework 4, and how Thread Pool work, and the most faoums scheduling algorithm : Work-stealing algorithm. With preisnation to see it in action.</p><p>5 0.67635262 <a title="735-lsi-5" href="../high_scalability-2009/high_scalability-2009-06-23-Learn_How_to_Exploit_Multiple_Cores_for_Better_Performance_and_Scalability.html">636 high scalability-2009-06-23-Learn How to Exploit Multiple Cores for Better Performance and Scalability</a></p>
<p>Introduction: InfoQueue has this  excellent talk by Brian Goetz  on the new features being added to Java SE 7 that will allow programmers to fully exploit our massively multi-processor future. While the talk is about Java it's really more general than that and there's a lot to learn here for everyone.  Brian starts with a short, coherent, and compelling explanation of why programmers can't expect to be saved by ever faster CPUs and why we must learn to exploit the strengths of multiple core computers to make our software go faster.   Some techniques for exploiting multiple cores are given in an equally  short, coherent, and compelling explanation of why divide and conquer as the secret to multi-core bliss, fork-join, how the Java approach differs from map-reduce, and lots of other juicy topics.   The multi-core "problem" is only going to get worse. Tilera founder Anant Agarwal  estimates by 2017  embedded processors could have 4,096 cores, server CPUs might have 512 cores and desktop chips could use</p><p>6 0.67449456 <a title="735-lsi-6" href="../high_scalability-2009/high_scalability-2009-05-31-Parallel_Programming_for_real-world.html">612 high scalability-2009-05-31-Parallel Programming for real-world</a></p>
<p>7 0.67164159 <a title="735-lsi-7" href="../high_scalability-2009/high_scalability-2009-04-26-Map-Reduce_for_Machine_Learning_on_Multicore.html">581 high scalability-2009-04-26-Map-Reduce for Machine Learning on Multicore</a></p>
<p>8 0.63700163 <a title="735-lsi-8" href="../high_scalability-2011/high_scalability-2011-02-02-Piccolo_-_Building_Distributed_Programs_that_are_11x_Faster_than_Hadoop.html">983 high scalability-2011-02-02-Piccolo - Building Distributed Programs that are 11x Faster than Hadoop</a></p>
<p>9 0.625723 <a title="735-lsi-9" href="../high_scalability-2012/high_scalability-2012-08-06-Paper%3A_High-Performance_Concurrency_Control_Mechanisms_for_Main-Memory_Databases.html">1299 high scalability-2012-08-06-Paper: High-Performance Concurrency Control Mechanisms for Main-Memory Databases</a></p>
<p>10 0.62181896 <a title="735-lsi-10" href="../high_scalability-2012/high_scalability-2012-03-06-Ask_For_Forgiveness_Programming_-_Or_How_We%27ll_Program_1000_Cores.html">1204 high scalability-2012-03-06-Ask For Forgiveness Programming - Or How We'll Program 1000 Cores</a></p>
<p>11 0.60985076 <a title="735-lsi-11" href="../high_scalability-2013/high_scalability-2013-05-08-Typesafe_Interview%3A_Scala_%2B_Akka_is_an_IaaS_for_Your_Process_Architecture.html">1454 high scalability-2013-05-08-Typesafe Interview: Scala + Akka is an IaaS for Your Process Architecture</a></p>
<p>12 0.59479296 <a title="735-lsi-12" href="../high_scalability-2010/high_scalability-2010-06-18-Paper%3A_The_Declarative_Imperative%3A_Experiences_and_Conjectures_in_Distributed_Logic.html">844 high scalability-2010-06-18-Paper: The Declarative Imperative: Experiences and Conjectures in Distributed Logic</a></p>
<p>13 0.59228605 <a title="735-lsi-13" href="../high_scalability-2012/high_scalability-2012-08-16-Paper%3A_A_Provably_Correct_Scalable_Concurrent_Skip_List.html">1305 high scalability-2012-08-16-Paper: A Provably Correct Scalable Concurrent Skip List</a></p>
<p>14 0.58059061 <a title="735-lsi-14" href="../high_scalability-2013/high_scalability-2013-03-18-Beyond_Threads_and_Callbacks_-_Application_Architecture_Pros_and_Cons.html">1425 high scalability-2013-03-18-Beyond Threads and Callbacks - Application Architecture Pros and Cons</a></p>
<p>15 0.56897742 <a title="735-lsi-15" href="../high_scalability-2009/high_scalability-2009-03-12-Google_TechTalk%3A_Amdahl%27s_Law_in_the_Multicore_Era.html">534 high scalability-2009-03-12-Google TechTalk: Amdahl's Law in the Multicore Era</a></p>
<p>16 0.56763285 <a title="735-lsi-16" href="../high_scalability-2009/high_scalability-2009-07-08-Art_of_Parallelism_presentation.html">652 high scalability-2009-07-08-Art of Parallelism presentation</a></p>
<p>17 0.56459397 <a title="735-lsi-17" href="../high_scalability-2011/high_scalability-2011-09-28-Pursue_robust_indefinite_scalability_with_the_Movable_Feast_Machine.html">1127 high scalability-2011-09-28-Pursue robust indefinite scalability with the Movable Feast Machine</a></p>
<p>18 0.55919081 <a title="735-lsi-18" href="../high_scalability-2010/high_scalability-2010-10-04-Paper%3A_An_Analysis_of_Linux_Scalability_to_Many_Cores__.html">914 high scalability-2010-10-04-Paper: An Analysis of Linux Scalability to Many Cores  </a></p>
<p>19 0.55709958 <a title="735-lsi-19" href="../high_scalability-2008/high_scalability-2008-05-10-Hitting_300_SimbleDB_Requests_Per_Second_on_a_Small_EC2_Instance.html">317 high scalability-2008-05-10-Hitting 300 SimbleDB Requests Per Second on a Small EC2 Instance</a></p>
<p>20 0.5567075 <a title="735-lsi-20" href="../high_scalability-2008/high_scalability-2008-12-06-Paper%3A_Real-world_Concurrency.html">462 high scalability-2008-12-06-Paper: Real-world Concurrency</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.093), (2, 0.234), (10, 0.022), (49, 0.258), (79, 0.137), (85, 0.115)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.91583991 <a title="735-lda-1" href="../high_scalability-2009/high_scalability-2009-11-01-Squeeze_more_performance_from_Parallelism.html">735 high scalability-2009-11-01-Squeeze more performance from Parallelism</a></p>
<p>Introduction: In many posts, such as:  The Future of the Parallelism and its Challenges  I mentioned that synchronization the access to the shared resource is the major challenge to write parallel code.
 
The synchronization and coordination take long time from the overall execution time, which reduce the benefits of the parallelism; the synchronization and coordination also reduce the scalability.
 
There are many forms of synchronization and coordination, such as:
  
 Create Task object in frameworks such as: Microsoft TPL, Intel TDD, and Parallel Runtime Library. Create and enqueue task objects require synchronization that it’s takes long time especially if we create it into recursive work such as: Quick Sort algorithm. 
 Synchronization the access to shared data. 
  
But there are a few techniques to avoid these issues, such as: Shared-Nothing, Actor Model, and Hyper Object (A.K.A. Combinable Object). Simply if we reduce the shared data by re-architect our code this will gives us a huge benefits</p><p>2 0.89374334 <a title="735-lda-2" href="../high_scalability-2008/high_scalability-2008-10-01-The_Pattern_Bible_for_Distributed_Computing.html">400 high scalability-2008-10-01-The Pattern Bible for Distributed Computing</a></p>
<p>Introduction: Software design patterns are an emerging tool for guiding and documenting system design. Patterns usually describe software abstractions used by advanced designers and programmers in their software. Patterns can provide guidance for designing highly scalable distributed systems. Let's see how!     Patterns are in essence solutions to problems. Most of them are expressed in a format called   Alexandrian form which draws on constructs used by Christopher Alexander. There are variants but most look like this:     The pattern name   The problem the pattern is trying to solve     Context   Solution   Examples   Design rationale: This tells where the pattern came from, why it works, and why experts use it       Patterns rarely stand alone. Each pattern works on a context, and transforms the system in that context to produce a new system in a new context. New problems arise in the new system and context, and the next ‘‘layer’’ of patterns can be applied. A pattern language is a structured col</p><p>3 0.85917866 <a title="735-lda-3" href="../high_scalability-2009/high_scalability-2009-11-05-A_Yes_for_a_NoSQL_Taxonomy.html">737 high scalability-2009-11-05-A Yes for a NoSQL Taxonomy</a></p>
<p>Introduction: NorthScale's Steven Yen in his highly entertaining  NoSQL is a Horseless Carriage  presentation has come up with a NoSQL taxonomy that thankfully focuses a little more on what NoSQL is, than  what it isn't :
  
 key‐value‐cache    
 
 memcached, repcached, coherence, infinispan, eXtreme scale, jboss cache, velocity, terracoqa 
 
 
  key‐value‐store    
 
 keyspace, flare, schema‐free, RAMCloud 
 
 
 eventually‐consistent key‐value‐store    
 
 dynamo, voldemort, Dynomite, SubRecord, Mo8onDb, Dovetaildb 
 
 
 ordered‐key‐value‐store    
 
 tokyo tyrant, lightcloud, NMDB, luxio, memcachedb, actord 
 
 
 data‐structures server    
 
  redis 
 
 
 tuple‐store    
 
 gigaspaces, coord, apache river 
 
 
 object database    
 
 ZopeDB, db4o, Shoal 
 
 
 document store    
 
  CouchDB, Mongo, Jackrabbit, XML Databases, ThruDB, CloudKit, Perservere, Riak Basho, Scalaris 
 
 
 wide columnar store    
 
 BigTable, Hbase, Cassandra, Hypertable, KAI, OpenNeptune, Qbase, KDI 
 
 
  
"Who will win?"</p><p>4 0.81990689 <a title="735-lda-4" href="../high_scalability-2011/high_scalability-2011-06-01-Why_is_your_network_so_slow%3F_Your_switch_should_tell_you..html">1051 high scalability-2011-06-01-Why is your network so slow? Your switch should tell you.</a></p>
<p>Introduction: Who hasn't cursed their network for being slow while waiting for that annoying little hour glass of pain to release all its grains of sand? But what's really going on? Is your network really slow? PacketPushers  Show 45 – Arista – EOS Network Software Architecture  has a good explanation of what may be really at fault (paraphrased):
  

Network operators get calls from application guys saying the network is slow, but the problem is usually dropped packets due to congestion. It's not usually latency, it's usually packet loss. Packet loss causes TCP to back off and retransmit, which causes applications to appear slow.


Packet loss can be caused by a flakey transceiver, but the problem is usually network congestion. Somewhere on the network there's fan-in, a bottleneck develops, queues build up to a certain point, and when a queue overflows it drops packets. Often the first sign of this happening is application slowness.


Queues get deeper and deeper because the network is getting more</p><p>5 0.81735301 <a title="735-lda-5" href="../high_scalability-2010/high_scalability-2010-06-16-WTF_is_Elastic_Data_Grid%3F_%28By_Example%29.html">843 high scalability-2010-06-16-WTF is Elastic Data Grid? (By Example)</a></p>
<p>Introduction: Forrester released their new wave report:  T he Forrester Wave™: Elastic Caching Platforms, Q2 2010  where they listed GigaSpaces, IBM, Oracle, and Terracotta as leading vendors in the field. In this post I'd like to take some time to explain what some of these terms mean, and why they’re important to you. I’ll start with a definition of Elastic Data Grid (Elastic Caching), how it is different then other caching and NoSQL alternatives, and more importantly -- I'll illustrate how it works through some real code examples.
 
You can read the full story  here .</p><p>6 0.79624021 <a title="735-lda-6" href="../high_scalability-2012/high_scalability-2012-08-24-Stuff_The_Internet_Says_On_Scalability_For_August_24%2C_2012.html">1311 high scalability-2012-08-24-Stuff The Internet Says On Scalability For August 24, 2012</a></p>
<p>7 0.78635657 <a title="735-lda-7" href="../high_scalability-2010/high_scalability-2010-05-05-How_will_memristors_change_everything%3F_.html">823 high scalability-2010-05-05-How will memristors change everything? </a></p>
<p>8 0.78048539 <a title="735-lda-8" href="../high_scalability-2007/high_scalability-2007-12-12-Report_from_OpenSocial_Meetup_at_Google.html">183 high scalability-2007-12-12-Report from OpenSocial Meetup at Google</a></p>
<p>9 0.77351451 <a title="735-lda-9" href="../high_scalability-2011/high_scalability-2011-07-25-Is_NoSQL_a_Premature_Optimization_that%27s_Worse_than_Death%3F_Or_the_Lady_Gaga_of_the_Database_World%3F.html">1085 high scalability-2011-07-25-Is NoSQL a Premature Optimization that's Worse than Death? Or the Lady Gaga of the Database World?</a></p>
<p>10 0.76090354 <a title="735-lda-10" href="../high_scalability-2012/high_scalability-2012-11-15-Gone_Fishin%27%3A_Justin.Tv%27s_Live_Video_Broadcasting_Architecture.html">1359 high scalability-2012-11-15-Gone Fishin': Justin.Tv's Live Video Broadcasting Architecture</a></p>
<p>11 0.75949198 <a title="735-lda-11" href="../high_scalability-2010/high_scalability-2010-03-16-Justin.tv%27s_Live_Video_Broadcasting_Architecture.html">796 high scalability-2010-03-16-Justin.tv's Live Video Broadcasting Architecture</a></p>
<p>12 0.7391609 <a title="735-lda-12" href="../high_scalability-2011/high_scalability-2011-05-02-Stack_Overflow_Makes_Slow_Pages_100x_Faster_by_Simple_SQL_Tuning.html">1032 high scalability-2011-05-02-Stack Overflow Makes Slow Pages 100x Faster by Simple SQL Tuning</a></p>
<p>13 0.73859036 <a title="735-lda-13" href="../high_scalability-2008/high_scalability-2008-05-10-Hitting_300_SimbleDB_Requests_Per_Second_on_a_Small_EC2_Instance.html">317 high scalability-2008-05-10-Hitting 300 SimbleDB Requests Per Second on a Small EC2 Instance</a></p>
<p>14 0.73480606 <a title="735-lda-14" href="../high_scalability-2008/high_scalability-2008-05-17-WebSphere_Commerce_High_Availability_and_Performance_Configurations.html">321 high scalability-2008-05-17-WebSphere Commerce High Availability and Performance Configurations</a></p>
<p>15 0.734492 <a title="735-lda-15" href="../high_scalability-2011/high_scalability-2011-07-15-Stuff_The_Internet_Says_On_Scalability_For_July_15%2C_2011.html">1080 high scalability-2011-07-15-Stuff The Internet Says On Scalability For July 15, 2011</a></p>
<p>16 0.73235643 <a title="735-lda-16" href="../high_scalability-2007/high_scalability-2007-10-09-High_Load_on_production_Webservers_after_Sourcecode_sync.html">118 high scalability-2007-10-09-High Load on production Webservers after Sourcecode sync</a></p>
<p>17 0.72813964 <a title="735-lda-17" href="../high_scalability-2012/high_scalability-2012-09-21-Stuff_The_Internet_Says_On_Scalability_For_September_21%2C_2012.html">1327 high scalability-2012-09-21-Stuff The Internet Says On Scalability For September 21, 2012</a></p>
<p>18 0.72732896 <a title="735-lda-18" href="../high_scalability-2012/high_scalability-2012-04-03-Hazelcast_2.0%3A_Big_Data_In-Memory.html">1221 high scalability-2012-04-03-Hazelcast 2.0: Big Data In-Memory</a></p>
<p>19 0.72549224 <a title="735-lda-19" href="../high_scalability-2014/high_scalability-2014-01-13-NYTimes_Architecture%3A_No_Head%2C_No_Master%2C_No_Single_Point_of_Failure.html">1577 high scalability-2014-01-13-NYTimes Architecture: No Head, No Master, No Single Point of Failure</a></p>
<p>20 0.72545654 <a title="735-lda-20" href="../high_scalability-2014/high_scalability-2014-02-07-Stuff_The_Internet_Says_On_Scalability_For_February_7th%2C_2014.html">1592 high scalability-2014-02-07-Stuff The Internet Says On Scalability For February 7th, 2014</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
