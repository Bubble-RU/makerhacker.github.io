<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>689 high scalability-2009-08-28-Strategy: Solve Only 80 Percent of the Problem</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2009" href="../home/high_scalability-2009_home.html">high_scalability-2009</a> <a title="high_scalability-2009-689" href="#">high_scalability-2009-689</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>689 high scalability-2009-08-28-Strategy: Solve Only 80 Percent of the Problem</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2009-689-html" href="http://highscalability.com//blog/2009/8/28/strategy-solve-only-80-percent-of-the-problem.html">html</a></p><p>Introduction: Solve only 80% of a problem. That's usually good enough and you'll not only
get done faster, you'll actually have a chance of getting done at all.This
strategy is given by Amix inHOW TWITTER (AND FACEBOOK) SOLVE PROBLEMS
PARTIALLY. The idea is solving 100% of a complex problem can be so hard and so
expensive that you'll end up wasting all your bullets on a problem that could
have been satisfactoraly solved in a much simpler way.The example given is for
Twitter's real-time search. Real-time search almost by definition is focussed
on recent events. So in the design should you be able to search historically
back from the beginning of time or should you just be able to search for
recent time periods? A complete historical search is the 100% solution. The
recent data only search is the 80% solution. Which should you choose?breakThe
100% solution is dramatically more difficult to solve. It requires searching
disk in real-time which is a killer. So it makes more sense to work on the 80%
probl</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The idea is solving 100% of a complex problem can be so hard and so expensive that you'll end up wasting all your bullets on a problem that could have been satisfactoraly solved in a much simpler way. [sent-4, score-0.419]
</p><p>2 Real-time search almost by definition is focussed on recent events. [sent-6, score-0.396]
</p><p>3 So in the design should you be able to search historically back from the beginning of time or should you just be able to search for recent time periods? [sent-7, score-0.631]
</p><p>4 By reducing the amount of data you need to search it's possible to make some simplifying design choices, like using fixed sized buffers that reside completely in memory. [sent-14, score-0.338]
</p><p>5 Sometimes as programmers we are blinded by the glory of the challenge of solving the 100% solution when there's a more reasonable, rational alternative that's almost as good. [sent-18, score-0.576]
</p><p>6 The lesson to be learned from this is that it is often undesirable to go for the right thing first. [sent-24, score-0.33]
</p><p>7 It is better to get half of the right thing available so that it spreads like a virus. [sent-25, score-0.31]
</p><p>8 Once people are hooked on it, take the time to improve it to 90% of the right thing. [sent-26, score-0.208]
</p><p>9 Unix, C, C++, Twitter and almost every product that has experienced wide adoption has followed this philosophy. [sent-27, score-0.211]
</p><p>10 Worse-is-Better solutions have the following characteristics:Simplicity- The design must be simple, both in implementation and interface. [sent-28, score-0.395]
</p><p>11 It is more important for the implementation to be simpler than the interface. [sent-29, score-0.285]
</p><p>12 Simplicity is the most important consideration in a design. [sent-30, score-0.15]
</p><p>13 Correctness- The design must be correct in all observable aspects. [sent-31, score-0.388]
</p><p>14 Consistency can be sacrificed for simplicity in some cases, but it is better to drop those parts of the design that deal with less common circumstances than to introduce either implementational complexity or inconsistency. [sent-34, score-0.757]
</p><p>15 Completeness- The design must cover as many important situations as is practical. [sent-35, score-0.369]
</p><p>16 Completeness can be sacrificed in favor of any other quality. [sent-37, score-0.365]
</p><p>17 In fact, completeness must be sacrificed whenever implementation simplicity is jeopardized. [sent-38, score-1.099]
</p><p>18 Consistency can be sacrificed to achieve completeness if simplicity is retained; especially worthless is consistency of interface. [sent-39, score-1.083]
</p><p>19 In my gut I think Worse-is-Better is different than "Solve Only 80 Percent of the Problem" primarily because Worse- is-Better is more about product adoption curves and 80% is more a design heuristic. [sent-40, score-0.439]
</p><p>20 After some cogitating this seems a false distinction so I have to concluded I'm wrong and have added Worse-is-Better to this post. [sent-41, score-0.167]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sacrificed', 0.365), ('completeness', 0.285), ('simplicity', 0.223), ('search', 0.169), ('design', 0.169), ('news', 0.144), ('twitter', 0.131), ('must', 0.124), ('recent', 0.124), ('thing', 0.118), ('right', 0.117), ('adoption', 0.108), ('simpler', 0.107), ('blinded', 0.106), ('glory', 0.106), ('worthless', 0.106), ('consistency', 0.104), ('almost', 0.103), ('implementation', 0.102), ('breakthe', 0.099), ('strategy', 0.097), ('solve', 0.096), ('retained', 0.095), ('observable', 0.095), ('undesirable', 0.095), ('solution', 0.093), ('blisteringly', 0.091), ('concluded', 0.091), ('hooked', 0.091), ('solving', 0.089), ('gabriel', 0.088), ('overly', 0.086), ('inhow', 0.086), ('hacker', 0.085), ('systemsby', 0.084), ('joseph', 0.084), ('praise', 0.084), ('gut', 0.082), ('clay', 0.082), ('curves', 0.08), ('rational', 0.079), ('distinction', 0.076), ('important', 0.076), ('spreads', 0.075), ('reasonably', 0.075), ('characteristic', 0.075), ('problem', 0.075), ('cases', 0.074), ('consideration', 0.074), ('wasting', 0.073)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999988 <a title="689-tfidf-1" href="../high_scalability-2009/high_scalability-2009-08-28-Strategy%3A_Solve_Only_80_Percent_of_the_Problem.html">689 high scalability-2009-08-28-Strategy: Solve Only 80 Percent of the Problem</a></p>
<p>Introduction: Solve only 80% of a problem. That's usually good enough and you'll not only
get done faster, you'll actually have a chance of getting done at all.This
strategy is given by Amix inHOW TWITTER (AND FACEBOOK) SOLVE PROBLEMS
PARTIALLY. The idea is solving 100% of a complex problem can be so hard and so
expensive that you'll end up wasting all your bullets on a problem that could
have been satisfactoraly solved in a much simpler way.The example given is for
Twitter's real-time search. Real-time search almost by definition is focussed
on recent events. So in the design should you be able to search historically
back from the beginning of time or should you just be able to search for
recent time periods? A complete historical search is the 100% solution. The
recent data only search is the 80% solution. Which should you choose?breakThe
100% solution is dramatically more difficult to solve. It requires searching
disk in real-time which is a killer. So it makes more sense to work on the 80%
probl</p><p>2 0.12341341 <a title="689-tfidf-2" href="../high_scalability-2010/high_scalability-2010-06-07-Six_Ways_Twitter_May_Reach_its_Big_Hairy_Audacious_Goal_of_One_Billion_Users.html">837 high scalability-2010-06-07-Six Ways Twitter May Reach its Big Hairy Audacious Goal of One Billion Users</a></p>
<p>Introduction: Twitter has a big hairy audacious goal of reaching one billion users by 2013.
Three forces stand against Twitter. The world will end in2012. But let's be
optimistic and assume we'll make it. Next is Facebook. Currently Facebook is
the user leader with over 400 million users. Will Facebook stumble or will
they rocket to one billion users before Twitter? And lastly, there's Twitter's
"low" starting point and "slow" growth rate. Twitter currently has106
millionregistered users and adds about 300,000 new users a day. That doesn't
add up to a billion in three years. Twitter needs to triple the number of
registered users they add per day. How will Twitter reach its goal of over one
billion users served?From recent infrastructure announcements and information
gleaned atChirp (videos) and other talks, it has become a little clearer how
they hope to reach their billion user goal: 1) Make a Big Hairy Audacious Goal
2) Hire Lots of Quality People 3) Hug Developers and Users 4) Drive the
Technolog</p><p>3 0.11614029 <a title="689-tfidf-3" href="../high_scalability-2010/high_scalability-2010-12-06-What_the_heck_are_you_actually_using_NoSQL_for%3F.html">954 high scalability-2010-12-06-What the heck are you actually using NoSQL for?</a></p>
<p>Introduction: It's a truism that we should choose theright tool for the job. Everyone says
that. And who can disagree? The problem is this is not helpful advice without
being able to answer more specific questions like: What jobs are the tools
good at? Will they work on jobs like mine? Is it worth the risk to try
something new when all my people know something else and we have a deadline to
meet? How can I make all the tools work together?In the NoSQL space this kind
of real-world data is still a bit vague. When asked, vendors tend to give very
general answers like NoSQL is good for BigData or key-value access. What does
that mean for for the developer in the trenches faced with the task of solving
a specific problem and there are a dozen confusing choices and no obvious
winner? Not a lot. It's often hard to take that next step and imagine how
their specific problems could be solved in a way that's worth taking the
trouble and risk.Let's change that. What problems are you using NoSQL to
solve? Which</p><p>4 0.1136869 <a title="689-tfidf-4" href="../high_scalability-2013/high_scalability-2013-01-28-DuckDuckGo_Architecture_-_1_Million_Deep_Searches_a_Day_and_Growing.html">1395 high scalability-2013-01-28-DuckDuckGo Architecture - 1 Million Deep Searches a Day and Growing</a></p>
<p>Introduction: This is an interview with Gabriel Weinberg, founder of Duck Duck Go and
general all around startup guru, on what DDG's architecture looks like in
2012.Innovative search engine upstart DuckDuckGo had30 million searchesin
February 2012 and averages over 1 million searches a day. It's being
positioned bysuper investor Fred Wilsonas a clean, private, impartial and fast
search engine. After talking with Gabriel I like what Fred Wilson said
earlier, it seems closer to the heart of the matter:We invested in DuckDuckGo
for the Reddit, Hacker News anarchists.                  Choosing DuckDuckGo
can be thought of as not just a technical choice, but a vote for revolution.
In an age when knowing your essence is not about about love or friendship, but
about more effectively selling you to advertisers, DDG is positioning
themselves as thedo not track alternative, keepers of theprivacy flame. You
will still be monetized of course, but in a more civilized and anonymous way.
Pushing privacy is a good</p><p>5 0.10933993 <a title="689-tfidf-5" href="../high_scalability-2010/high_scalability-2010-10-28-NoSQL_Took_Away_the_Relational_Model_and_Gave_Nothing_Back.html">930 high scalability-2010-10-28-NoSQL Took Away the Relational Model and Gave Nothing Back</a></p>
<p>Introduction: Update: Benjamin Black said he was the source of the quote and also said I was
wrong about what he meant. His real point:The meaning of the statement was
that NoSQL systems (really the various map-reduce systems) are lacking a
standard model for describing and querying and that developing one should be a
high priority task for them.At the A NoSQL Evening in Palo Alto, an audience
member, sorry, I couldn't tell who, said something I found really
interesting:NoSQL took away the relational model and gave nothing back.The
idea being that NoSQL has focussed on ease of use, scalability, performance,
etc, but it has lost the idea of how data relates to other data. True to its
name, the relational model is very good at capturing a managing relationships.
With NoSQL all relationships have been pushed back onto the poor programmer to
implement in code rather than the database managing it. We've sacrificed
usability. NoSQL is about concurrency, latency, and scalability, but it's not
about data.My</p><p>6 0.1091129 <a title="689-tfidf-6" href="../high_scalability-2008/high_scalability-2008-05-28-Job_queue_and_search_engine.html">332 high scalability-2008-05-28-Job queue and search engine</a></p>
<p>7 0.10660512 <a title="689-tfidf-7" href="../high_scalability-2008/high_scalability-2008-09-30-Scalability_Worst_Practices.html">398 high scalability-2008-09-30-Scalability Worst Practices</a></p>
<p>8 0.10369477 <a title="689-tfidf-8" href="../high_scalability-2008/high_scalability-2008-03-08-Audiogalaxy.com_Architecture.html">269 high scalability-2008-03-08-Audiogalaxy.com Architecture</a></p>
<p>9 0.10250484 <a title="689-tfidf-9" href="../high_scalability-2009/high_scalability-2009-06-27-Scaling_Twitter%3A_Making_Twitter_10000_Percent_Faster.html">639 high scalability-2009-06-27-Scaling Twitter: Making Twitter 10000 Percent Faster</a></p>
<p>10 0.096434705 <a title="689-tfidf-10" href="../high_scalability-2010/high_scalability-2010-02-19-Twitter%E2%80%99s_Plan_to_Analyze_100_Billion_Tweets.html">780 high scalability-2010-02-19-Twitter’s Plan to Analyze 100 Billion Tweets</a></p>
<p>11 0.093974203 <a title="689-tfidf-11" href="../high_scalability-2010/high_scalability-2010-06-10-The_Four_Meta_Secrets_of_Scaling_at_Facebook.html">840 high scalability-2010-06-10-The Four Meta Secrets of Scaling at Facebook</a></p>
<p>12 0.0868086 <a title="689-tfidf-12" href="../high_scalability-2009/high_scalability-2009-11-26-Kngine_Snippet_Search_New_Indexing_Technology.html">746 high scalability-2009-11-26-Kngine Snippet Search New Indexing Technology</a></p>
<p>13 0.084789276 <a title="689-tfidf-13" href="../high_scalability-2010/high_scalability-2010-07-11-So%2C_Why_is_Twitter_Really_Not_Using_Cassandra_to_Store_Tweets%3F.html">855 high scalability-2010-07-11-So, Why is Twitter Really Not Using Cassandra to Store Tweets?</a></p>
<p>14 0.0842783 <a title="689-tfidf-14" href="../high_scalability-2012/high_scalability-2012-11-05-Gone_Fishin%27%3A_Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_In_The_Ambient_Cloud.html">1355 high scalability-2012-11-05-Gone Fishin': Building Super Scalable Systems: Blade Runner Meets Autonomic Computing In The Ambient Cloud</a></p>
<p>15 0.084273838 <a title="689-tfidf-15" href="../high_scalability-2009/high_scalability-2009-12-16-Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_in_the_Ambient_Cloud.html">750 high scalability-2009-12-16-Building Super Scalable Systems: Blade Runner Meets Autonomic Computing in the Ambient Cloud</a></p>
<p>16 0.083316267 <a title="689-tfidf-16" href="../high_scalability-2007/high_scalability-2007-09-18-Amazon_Architecture.html">96 high scalability-2007-09-18-Amazon Architecture</a></p>
<p>17 0.081848852 <a title="689-tfidf-17" href="../high_scalability-2011/high_scalability-2011-04-06-Netflix%3A_Run_Consistency_Checkers_All_the_time_to_Fixup_Transactions.html">1017 high scalability-2011-04-06-Netflix: Run Consistency Checkers All the time to Fixup Transactions</a></p>
<p>18 0.081458345 <a title="689-tfidf-18" href="../high_scalability-2013/high_scalability-2013-09-09-Need_Help_with_Database_Scalability%3F_Understand_I-O.html">1514 high scalability-2013-09-09-Need Help with Database Scalability? Understand I-O</a></p>
<p>19 0.081073746 <a title="689-tfidf-19" href="../high_scalability-2010/high_scalability-2010-10-28-Notes_from_A_NOSQL_Evening_in_Palo_Alto_.html">931 high scalability-2010-10-28-Notes from A NOSQL Evening in Palo Alto </a></p>
<p>20 0.080572411 <a title="689-tfidf-20" href="../high_scalability-2009/high_scalability-2009-10-13-Why_are_Facebook%2C_Digg%2C_and_Twitter_so_hard_to_scale%3F.html">721 high scalability-2009-10-13-Why are Facebook, Digg, and Twitter so hard to scale?</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.157), (1, 0.093), (2, -0.016), (3, 0.047), (4, 0.039), (5, -0.008), (6, -0.06), (7, 0.046), (8, 0.008), (9, -0.031), (10, -0.003), (11, 0.058), (12, -0.05), (13, 0.005), (14, 0.022), (15, 0.001), (16, 0.044), (17, -0.031), (18, 0.011), (19, 0.004), (20, 0.044), (21, -0.053), (22, 0.048), (23, 0.034), (24, -0.044), (25, -0.028), (26, -0.002), (27, -0.016), (28, -0.009), (29, 0.08), (30, -0.027), (31, 0.023), (32, -0.057), (33, 0.024), (34, -0.012), (35, 0.033), (36, 0.007), (37, 0.031), (38, -0.074), (39, -0.025), (40, 0.086), (41, 0.015), (42, -0.044), (43, 0.032), (44, 0.013), (45, 0.006), (46, 0.001), (47, 0.017), (48, 0.029), (49, -0.059)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9868558 <a title="689-lsi-1" href="../high_scalability-2009/high_scalability-2009-08-28-Strategy%3A_Solve_Only_80_Percent_of_the_Problem.html">689 high scalability-2009-08-28-Strategy: Solve Only 80 Percent of the Problem</a></p>
<p>Introduction: Solve only 80% of a problem. That's usually good enough and you'll not only
get done faster, you'll actually have a chance of getting done at all.This
strategy is given by Amix inHOW TWITTER (AND FACEBOOK) SOLVE PROBLEMS
PARTIALLY. The idea is solving 100% of a complex problem can be so hard and so
expensive that you'll end up wasting all your bullets on a problem that could
have been satisfactoraly solved in a much simpler way.The example given is for
Twitter's real-time search. Real-time search almost by definition is focussed
on recent events. So in the design should you be able to search historically
back from the beginning of time or should you just be able to search for
recent time periods? A complete historical search is the 100% solution. The
recent data only search is the 80% solution. Which should you choose?breakThe
100% solution is dramatically more difficult to solve. It requires searching
disk in real-time which is a killer. So it makes more sense to work on the 80%
probl</p><p>2 0.7507894 <a title="689-lsi-2" href="../high_scalability-2014/high_scalability-2014-02-25-Peter_Norvig%27s_9_Master_Steps_to_Improving_a_Program.html">1601 high scalability-2014-02-25-Peter Norvig's 9 Master Steps to Improving a Program</a></p>
<p>Introduction: Inspired by axkcd comic,Peter Norvig,Director of Research at Google and all
around interesting and nice guy, has created an above par code kata involving
a regex program that demonstrates the core inner loop of many successful
systems profiled on HighScalability.The original code is atxkcd 1313: Regex
Golf, which comes up with an algorithm to find a short regex that matches the
winners and not the losers from two arbitrary lists. The Python code is
readable, the process is TDDish, and the problem, which sounds simple, but
soon explodes into regex weirdness, as does most regex code. If you find
regular expressions confusing you'll definitely benefit from Peter's
deliberate strategy for finding a regex.The post demonstrating the iterated
improvement of the program is atxkcd 1313: Regex Golf (Part 2: Infinite
Problems). As with most first solutions it wasn't optimal. To improve the
program Peter recommends the following steps: Profiling: Figure out where the
program spends its time. Speed</p><p>3 0.71970952 <a title="689-lsi-3" href="../high_scalability-2013/high_scalability-2013-01-28-DuckDuckGo_Architecture_-_1_Million_Deep_Searches_a_Day_and_Growing.html">1395 high scalability-2013-01-28-DuckDuckGo Architecture - 1 Million Deep Searches a Day and Growing</a></p>
<p>Introduction: This is an interview with Gabriel Weinberg, founder of Duck Duck Go and
general all around startup guru, on what DDG's architecture looks like in
2012.Innovative search engine upstart DuckDuckGo had30 million searchesin
February 2012 and averages over 1 million searches a day. It's being
positioned bysuper investor Fred Wilsonas a clean, private, impartial and fast
search engine. After talking with Gabriel I like what Fred Wilson said
earlier, it seems closer to the heart of the matter:We invested in DuckDuckGo
for the Reddit, Hacker News anarchists.                  Choosing DuckDuckGo
can be thought of as not just a technical choice, but a vote for revolution.
In an age when knowing your essence is not about about love or friendship, but
about more effectively selling you to advertisers, DDG is positioning
themselves as thedo not track alternative, keepers of theprivacy flame. You
will still be monetized of course, but in a more civilized and anonymous way.
Pushing privacy is a good</p><p>4 0.6919713 <a title="689-lsi-4" href="../high_scalability-2012/high_scalability-2012-05-04-Stuff_The_Internet_Says_On_Scalability_For_May_4%2C_2012.html">1239 high scalability-2012-05-04-Stuff The Internet Says On Scalability For May 4, 2012</a></p>
<p>Introduction: It's HighScalability Time:Quotable quotes:Richard Feynman: Suppose that little
things behave very differently than anything big@orgnet: "Data, data
everywhere, but not a thought to think" -- John Allen Paulos,
Mathematician@bcarlso: just throw out the word "scalability". That'll bring em
out@codypo:Here are the steps to the Scalability Shuffle. 1: log everything.
2: analyze logs. 3: profile. 4: refactor. 5: repeat.@FoggSeastack:If math had
been taught in a relevant way I might have been a #BigData person
today.@secboffin: I know a programming joke about 10,000 mutexes, but it's a
bit contentious.Twitter gets personal with  Improved personalization
algorithms and real-time indexing, a tale of a real-time tool chain. Earlybird
is Twitter's real-time search system. Every Tweet has its URLs extracted and
expanded. URL contents are fetched via SpiderDuck. Cassovary, a graph
processing library, is used to find important connections. Then Twitter's
search engine is used to find URLs shared in</p><p>5 0.67541277 <a title="689-lsi-5" href="../high_scalability-2011/high_scalability-2011-11-29-DataSift_Architecture%3A_Realtime_Datamining_at_120%2C000_Tweets_Per_Second.html">1148 high scalability-2011-11-29-DataSift Architecture: Realtime Datamining at 120,000 Tweets Per Second</a></p>
<p>Introduction: I remember the excitement of when Twitter first opened up their firehose. As
an early adopter of the Twitter API I could easily imagine some of the cool
things you could do with all that data. I also remember the disappointment of
learning that in the land of BigData, data has a price, and that price would
be too high for little fish like me. It was like learning for the first time
there would be no BigData Santa Clause.For a while though I had the pleasure
of pondering just how I would handle all that data. It's a fascinating
problem. You have to be able to reliably consume it, normalize it, merge it
with other data, apply functions on it, store it, query it, distribute it, and
oh yah, monetize it. Most of that in realish-time. And if you are trying to
create a platform for allowing the entire Internet do to the same thing to the
firehose, the challenge is exponentially harder.DataSift is in the exciting
position of creating just such a firehose eating, data chomping machine. You
see,</p><p>6 0.67082685 <a title="689-lsi-6" href="../high_scalability-2011/high_scalability-2011-11-11-Stuff_The_Internet_Says_On_Scalability_For_November_11%2C_2011.html">1141 high scalability-2011-11-11-Stuff The Internet Says On Scalability For November 11, 2011</a></p>
<p>7 0.66722143 <a title="689-lsi-7" href="../high_scalability-2013/high_scalability-2013-07-15-Ask_HS%3A_What%27s_Wrong_with_Twitter%2C_Why_Isn%27t_One_Machine_Enough%3F.html">1491 high scalability-2013-07-15-Ask HS: What's Wrong with Twitter, Why Isn't One Machine Enough?</a></p>
<p>8 0.66472423 <a title="689-lsi-8" href="../high_scalability-2013/high_scalability-2013-04-26-Stuff_The_Internet_Says_On_Scalability_For_April_26%2C_2013.html">1447 high scalability-2013-04-26-Stuff The Internet Says On Scalability For April 26, 2013</a></p>
<p>9 0.65394682 <a title="689-lsi-9" href="../high_scalability-2010/high_scalability-2010-02-19-Twitter%E2%80%99s_Plan_to_Analyze_100_Billion_Tweets.html">780 high scalability-2010-02-19-Twitter’s Plan to Analyze 100 Billion Tweets</a></p>
<p>10 0.65343988 <a title="689-lsi-10" href="../high_scalability-2013/high_scalability-2013-02-08-Stuff_The_Internet_Says_On_Scalability_For_February_8%2C_2013.html">1403 high scalability-2013-02-08-Stuff The Internet Says On Scalability For February 8, 2013</a></p>
<p>11 0.65270764 <a title="689-lsi-11" href="../high_scalability-2013/high_scalability-2013-03-29-Stuff_The_Internet_Says_On_Scalability_For_March_29%2C_2013.html">1431 high scalability-2013-03-29-Stuff The Internet Says On Scalability For March 29, 2013</a></p>
<p>12 0.65269709 <a title="689-lsi-12" href="../high_scalability-2013/high_scalability-2013-05-15-Lesson_from_Airbnb%3A_Give_Yourself_Permission_to_Experiment_with_Non-scalable_Changes.html">1458 high scalability-2013-05-15-Lesson from Airbnb: Give Yourself Permission to Experiment with Non-scalable Changes</a></p>
<p>13 0.65233999 <a title="689-lsi-13" href="../high_scalability-2013/high_scalability-2013-01-18-Stuff_The_Internet_Says_On_Scalability_For_January_18%2C_2013.html">1389 high scalability-2013-01-18-Stuff The Internet Says On Scalability For January 18, 2013</a></p>
<p>14 0.64941806 <a title="689-lsi-14" href="../high_scalability-2007/high_scalability-2007-11-27-Solving_the_Client_Side_API_Scalability_Problem_with_a_Little_Game_Theory.html">166 high scalability-2007-11-27-Solving the Client Side API Scalability Problem with a Little Game Theory</a></p>
<p>15 0.6491971 <a title="689-lsi-15" href="../high_scalability-2010/high_scalability-2010-07-11-So%2C_Why_is_Twitter_Really_Not_Using_Cassandra_to_Store_Tweets%3F.html">855 high scalability-2010-07-11-So, Why is Twitter Really Not Using Cassandra to Store Tweets?</a></p>
<p>16 0.64907557 <a title="689-lsi-16" href="../high_scalability-2012/high_scalability-2012-04-27-Stuff_The_Internet_Says_On_Scalability_For_April_27%2C_2012.html">1235 high scalability-2012-04-27-Stuff The Internet Says On Scalability For April 27, 2012</a></p>
<p>17 0.64716762 <a title="689-lsi-17" href="../high_scalability-2008/high_scalability-2008-07-07-Five_Ways_to_Stop_Framework_Fixation_from_Crashing_Your_Scaling_Strategy.html">347 high scalability-2008-07-07-Five Ways to Stop Framework Fixation from Crashing Your Scaling Strategy</a></p>
<p>18 0.64649868 <a title="689-lsi-18" href="../high_scalability-2008/high_scalability-2008-05-27-Should_Twitter_be_an_All-You-Can-Eat_Buffet_or_a_Vending_Machine%3F.html">330 high scalability-2008-05-27-Should Twitter be an All-You-Can-Eat Buffet or a Vending Machine?</a></p>
<p>19 0.64323139 <a title="689-lsi-19" href="../high_scalability-2009/high_scalability-2009-11-16-Building_Scalable_Systems_Using_Data_as_a_Composite_Material.html">741 high scalability-2009-11-16-Building Scalable Systems Using Data as a Composite Material</a></p>
<p>20 0.64317179 <a title="689-lsi-20" href="../high_scalability-2012/high_scalability-2012-05-28-The_Anatomy_of_Search_Technology%3A_Crawling_using_Combinators.html">1253 high scalability-2012-05-28-The Anatomy of Search Technology: Crawling using Combinators</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.073), (2, 0.206), (10, 0.44), (40, 0.014), (61, 0.114), (85, 0.016), (94, 0.054)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.9763093 <a title="689-lda-1" href="../high_scalability-2007/high_scalability-2007-12-10-1_Master%2C_N_Slaves.html">178 high scalability-2007-12-10-1 Master, N Slaves</a></p>
<p>Introduction: Hello all,Reading the site you can note that "1 Master for writes, N Slaves
for reads" scheme is used offen.How is this implemented? Who decides where
writes and reads go? Something in application level or specific database
proxies, like Slony-I?Thanks.</p><p>2 0.97282165 <a title="689-lda-2" href="../high_scalability-2010/high_scalability-2010-08-07-ArchCamp%3A_Scalable_Databases_%28NoSQL%29.html">874 high scalability-2010-08-07-ArchCamp: Scalable Databases (NoSQL)</a></p>
<p>Introduction: ArchCamp: Scalable Databasess (NoSQL)The ArchCamp unconference was held this
past Friday at HackerDojo in Mountain View, CA.  There was plenty of pizza,
beer, and great conversation.  This session started out free-form, but shaped
up pretty quickly into a discussion of the popular open source scalable NoSQL
databases and the architectural categories in which they belong.</p><p>3 0.9502539 <a title="689-lda-3" href="../high_scalability-2012/high_scalability-2012-08-06-Paper%3A_High-Performance_Concurrency_Control_Mechanisms_for_Main-Memory_Databases.html">1299 high scalability-2012-08-06-Paper: High-Performance Concurrency Control Mechanisms for Main-Memory Databases</a></p>
<p>Introduction: If you stayed up all night watching the life reaffirmingCuriosity landing on
Mars, then this paper,High-Performance Concurrency Control Mechanisms for
Main-Memory Databases, has nothing to do with that at all, but it is an
excellent look at how to use optimistic MVCC schemes to reduce lock overhead
on in-memory datastructures:A database system optimized for in-memory storage
can support much higher transaction rates than current systems. However,
standard concurrency control methods used today do not scale to the high
transaction rates achievable by such systems. In this paper we introduce two
efficient concurrency control methods specifically designed for main-memory
databases. Both use multiversioning to isolate read-only transactions from
updates but differ in how atomicity is ensured: one is optimistic and one is
pessimistic. To avoid expensive context switching, transactions never block
during normal processing but they may have to wait before commit to ensure
correct serializatio</p><p>4 0.9476456 <a title="689-lda-4" href="../high_scalability-2011/high_scalability-2011-06-22-It%27s_the_Fraking_IOPS_-_1_SSD_is_44%2C000_IOPS%2C_Hard_Drive_is_180.html">1066 high scalability-2011-06-22-It's the Fraking IOPS - 1 SSD is 44,000 IOPS, Hard Drive is 180</a></p>
<p>Introduction: Planning your next buildout and thinking SSDs are still far in the future?
Still too expensive, too low density. Hard disks are cheap, familiar, and
store lots of stuff. In this short and entertaining video Wikia's Artur
Bergmanwants to change your mind about SSDs. SSDs are for today, get with the
math already.Here's Artur's logic:Wikia is all SSD in production. The new
Wikia file servers have a theoretical read rate of ~10GB/sec sequential,
6GB/sec random and 1.2 million IOPs. If you can't do math or love the past,
you love spinning rust. If you are awesome you love SSDs.SSDs are cheaper than
drives using the most relevant metric: $/GB/IOPS. 1 SSD is 44,000 IOPS and one
hard drive is 180 IOPS. Need 1 SSD instead of 50 hard drives.With 8 million
files there's a 9 minute fsck. Full backup in 12 minutes (X-25M based).4
GB/sec random read average latency 1 msec.2.2 GB/sec random write average
latency 1 msec.50TBs of SSDs in one machine for $80,000. With the densities
most products can ski</p><p>5 0.92518908 <a title="689-lda-5" href="../high_scalability-2013/high_scalability-2013-06-24-Update_on_How_29_Cloud_Price_Drops_Changed_the_Bottom_Line_of_TripAdvisor_and_Pinterest_-_Results_Mixed.html">1480 high scalability-2013-06-24-Update on How 29 Cloud Price Drops Changed the Bottom Line of TripAdvisor and Pinterest - Results Mixed</a></p>
<p>Introduction: This is a guest post byAli Khajeh-Hosseini, Technical Lead atPlanForCloud. The
original article was publishedon their site. With 29 cloud price reductions I
thought it would be interesting to see how the bottom line would change
compared to an articlewe published last year. The result is surprisingly
little for TripAdvisor because prices for On Demand instances havenot dropped
as fastas for other other instances types.Over the last year and a half,we
counted 29 price reductionsin cloud services provided by AWS, Google Compute
Engine, Windows Azure, and Rackspace Cloud. Price reductions have a direct
effect on cloud users, but given the usual tiny reductions, how significant is
that effect on the bottom line?Last year I wrote aboutcloud cost forecasts for
TripAdvisor and Pinterest.TripAdvisor was experimenting with AWSand attempted
to process 700K HTTP requests per minute on a replica of its live site,
andPinterest was growing massively on AWS. In the wake of the cloud providers'
price</p><p>6 0.9205665 <a title="689-lda-6" href="../high_scalability-2008/high_scalability-2008-10-26-Should_you_use_a_SAN_to_scale_your_architecture%3F_.html">430 high scalability-2008-10-26-Should you use a SAN to scale your architecture? </a></p>
<p>7 0.91165489 <a title="689-lda-7" href="../high_scalability-2014/high_scalability-2014-04-21-This_is_why_Microsoft_won._And_why_they_lost..html">1635 high scalability-2014-04-21-This is why Microsoft won. And why they lost.</a></p>
<p>same-blog 8 0.90935272 <a title="689-lda-8" href="../high_scalability-2009/high_scalability-2009-08-28-Strategy%3A_Solve_Only_80_Percent_of_the_Problem.html">689 high scalability-2009-08-28-Strategy: Solve Only 80 Percent of the Problem</a></p>
<p>9 0.89195895 <a title="689-lda-9" href="../high_scalability-2009/high_scalability-2009-04-27-Some_Questions_from_a_newbie.html">584 high scalability-2009-04-27-Some Questions from a newbie</a></p>
<p>10 0.89169252 <a title="689-lda-10" href="../high_scalability-2007/high_scalability-2007-12-02-a8cjdbc_-_update_verision_1.3.html">171 high scalability-2007-12-02-a8cjdbc - update verision 1.3</a></p>
<p>11 0.89167261 <a title="689-lda-11" href="../high_scalability-2007/high_scalability-2007-12-02-Database-Clustering%3A_a8cjdbc_-_update%3A_version_1.3.html">170 high scalability-2007-12-02-Database-Clustering: a8cjdbc - update: version 1.3</a></p>
<p>12 0.8828904 <a title="689-lda-12" href="../high_scalability-2010/high_scalability-2010-03-10-How_FarmVille_Scales_-_The_Follow-up.html">792 high scalability-2010-03-10-How FarmVille Scales - The Follow-up</a></p>
<p>13 0.88088608 <a title="689-lda-13" href="../high_scalability-2011/high_scalability-2011-05-23-Evernote_Architecture_-_9_Million_Users_and_150_Million_Requests_a_Day.html">1046 high scalability-2011-05-23-Evernote Architecture - 9 Million Users and 150 Million Requests a Day</a></p>
<p>14 0.87730157 <a title="689-lda-14" href="../high_scalability-2014/high_scalability-2014-04-14-How_do_you_even_do_anything_without_using_EBS%3F.html">1631 high scalability-2014-04-14-How do you even do anything without using EBS?</a></p>
<p>15 0.86878604 <a title="689-lda-15" href="../high_scalability-2010/high_scalability-2010-01-27-Hot_Scalability_Links_for_January_28_2010.html">767 high scalability-2010-01-27-Hot Scalability Links for January 28 2010</a></p>
<p>16 0.85389841 <a title="689-lda-16" href="../high_scalability-2014/high_scalability-2014-01-24-Stuff_The_Internet_Says_On_Scalability_For_January_24th%2C_2014.html">1585 high scalability-2014-01-24-Stuff The Internet Says On Scalability For January 24th, 2014</a></p>
<p>17 0.81094742 <a title="689-lda-17" href="../high_scalability-2007/high_scalability-2007-11-05-Strategy%3A_Diagonal_Scaling_-_Don%27t_Forget_to_Scale_Out_AND_Up.html">142 high scalability-2007-11-05-Strategy: Diagonal Scaling - Don't Forget to Scale Out AND Up</a></p>
<p>18 0.80151296 <a title="689-lda-18" href="../high_scalability-2013/high_scalability-2013-07-08-The_Architecture_Twitter_Uses_to_Deal_with_150M_Active_Users%2C_300K_QPS%2C_a_22_MB-S_Firehose%2C_and_Send_Tweets_in_Under_5_Seconds.html">1488 high scalability-2013-07-08-The Architecture Twitter Uses to Deal with 150M Active Users, 300K QPS, a 22 MB-S Firehose, and Send Tweets in Under 5 Seconds</a></p>
<p>19 0.79621464 <a title="689-lda-19" href="../high_scalability-2012/high_scalability-2012-10-02-An_Epic_TripAdvisor_Update%3A_Why_Not_Run_on_the_Cloud%3F_The_Grand_Experiment..html">1331 high scalability-2012-10-02-An Epic TripAdvisor Update: Why Not Run on the Cloud? The Grand Experiment.</a></p>
<p>20 0.78811908 <a title="689-lda-20" href="../high_scalability-2011/high_scalability-2011-03-14-Twitter_by_the_Numbers_-_460%2C000_New_Accounts_and_140_Million_Tweets_Per_Day.html">1004 high scalability-2011-03-14-Twitter by the Numbers - 460,000 New Accounts and 140 Million Tweets Per Day</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
