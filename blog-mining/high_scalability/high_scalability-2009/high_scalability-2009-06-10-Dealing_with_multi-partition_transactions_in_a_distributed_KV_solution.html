<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>623 high scalability-2009-06-10-Dealing with multi-partition transactions in a distributed KV solution</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2009" href="../home/high_scalability-2009_home.html">high_scalability-2009</a> <a title="high_scalability-2009-623" href="#">high_scalability-2009-623</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>623 high scalability-2009-06-10-Dealing with multi-partition transactions in a distributed KV solution</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2009-623-html" href="http://highscalability.com//blog/2009/6/10/dealing-with-multi-partition-transactions-in-a-distributed-k.html">html</a></p><p>Introduction: I've been getting asked about this a lot lately so I figured I'd just blog
about it. Products like WebSphere eXtreme Scale work by taking a dataset,
partitioning it using a key and then assigning those partitions to a number of
JVMs. Each partition usually has a primary and a replica. These 'shards' are
assigned to JVMs. A transactional application typically interacts with the
data on a single partition at a time. This means the transaction is executed
in a single JVM. A server box will be able to do M of those transactions per
second and it scales because N boxes does MN (M multiplied by N) transactions
per second. Increase N, you get more transactions per second. Availability is
very good because a transaction only depends on 1 of the N servers that are
currently online. Any of the other (N-1) servers can go down or fail with no
impact on the transaction. So, single partition transactions can scale
indefinitely from a throughput point of view, offer very consistent response
times and</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 So, single partition transactions can scale indefinitely from a throughput point of view, offer very consistent response times and they are very available because they only point a small part of the grid at once. [sent-11, score-1.369]
</p><p>2 The value is an account object with the users online username and their password, address, portal profile, bank account information etc. [sent-15, score-1.072]
</p><p>3 Almost all access to the account is using the account number. [sent-16, score-0.766]
</p><p>4 The user doesn't login with their account number, they login with the username. [sent-18, score-0.762]
</p><p>5 We have not partitioned on user name, we partitioned on account and did so for good reason as every other transaction type is keyed on account number. [sent-19, score-1.091]
</p><p>6 Lets do a parallel search across all partitions to find account objects whose user name attribute is 'billy'. [sent-22, score-1.121]
</p><p>7 It will run a query within that partition to find any accounts in that partition with a username of 'billy'. [sent-25, score-0.997]
</p><p>8 One account object should match across the whole grid and the client which called the agent should receive the account number as the result. [sent-26, score-1.229]
</p><p>9 The grid is scaling in terms of account capacity and records searched/second but the throughput number is not scaling at all. [sent-44, score-0.931]
</p><p>10 The single partition transactions only used a single partition/server. [sent-46, score-0.75]
</p><p>11 The every partition transaction needs the whole grid to be up to complete. [sent-47, score-0.829]
</p><p>12 This lack of throughput scalability for every partition transactions is a problem as login is a operation whose throughput needs to go up as the web site becomes more popular. [sent-53, score-1.469]
</p><p>13 We could partition using user name instead of account but now we have the search problem for all the account number based transactions which are the bulk of all transactions and besides, users like being able to change the user name which would be a nightmare if everything was based on usernames. [sent-56, score-2.176]
</p><p>14 get("billy") and receive the account id with a single partition transaction and the throughput of those does scale with grid size. [sent-66, score-1.569]
</p><p>15 We have converted a parallel transaction to a single partition transaction and as a result, our login operation is now throughput scalable. [sent-74, score-1.511]
</p><p>16 Larger grids can store larger amounts of data but the throughput typically stays the same as the grid grows (assuming the data size grows linearly with grid size). [sent-77, score-0.885]
</p><p>17 This means using parallel operations for something whose throughput will grow as your application scales up is a mistake as the throughput of the grid has nothing to do with the grid size, it's limited to the throughput of the slowest box. [sent-78, score-1.651]
</p><p>18 You need to convert that parallel search operation to a single partition get if you want the system to scale from a throughput point of view. [sent-79, score-1.252]
</p><p>19 How can you make an every partition operation scale from a throughput point of view then if you can't use reverse indexes? [sent-81, score-1.025]
</p><p>20 If you need throughput scalable every partition transactions then this is probably the only way to make it scale from a throughput point of view. [sent-84, score-1.279]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('account', 0.383), ('partition', 0.377), ('throughput', 0.265), ('grid', 0.214), ('transactions', 0.201), ('parallel', 0.199), ('transaction', 0.184), ('username', 0.15), ('login', 0.146), ('reverse', 0.142), ('slowest', 0.138), ('search', 0.138), ('name', 0.125), ('websphere', 0.112), ('twice', 0.11), ('mapgridagent', 0.104), ('mn', 0.104), ('map', 0.102), ('partitions', 0.098), ('loader', 0.094), ('accounts', 0.093), ('whose', 0.091), ('lets', 0.09), ('index', 0.089), ('password', 0.089), ('billy', 0.089), ('bank', 0.088), ('user', 0.087), ('single', 0.086), ('second', 0.077), ('extreme', 0.074), ('double', 0.071), ('operation', 0.07), ('number', 0.069), ('size', 0.069), ('object', 0.068), ('cache', 0.065), ('persistent', 0.065), ('grids', 0.062), ('returning', 0.061), ('typically', 0.061), ('scale', 0.06), ('point', 0.057), ('agent', 0.057), ('per', 0.056), ('client', 0.055), ('delay', 0.054), ('every', 0.054), ('response', 0.052), ('searches', 0.052)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="623-tfidf-1" href="../high_scalability-2009/high_scalability-2009-06-10-Dealing_with_multi-partition_transactions_in_a_distributed_KV_solution.html">623 high scalability-2009-06-10-Dealing with multi-partition transactions in a distributed KV solution</a></p>
<p>Introduction: I've been getting asked about this a lot lately so I figured I'd just blog
about it. Products like WebSphere eXtreme Scale work by taking a dataset,
partitioning it using a key and then assigning those partitions to a number of
JVMs. Each partition usually has a primary and a replica. These 'shards' are
assigned to JVMs. A transactional application typically interacts with the
data on a single partition at a time. This means the transaction is executed
in a single JVM. A server box will be able to do M of those transactions per
second and it scales because N boxes does MN (M multiplied by N) transactions
per second. Increase N, you get more transactions per second. Availability is
very good because a transaction only depends on 1 of the N servers that are
currently online. Any of the other (N-1) servers can go down or fail with no
impact on the transaction. So, single partition transactions can scale
indefinitely from a throughput point of view, offer very consistent response
times and</p><p>2 0.28702372 <a title="623-tfidf-2" href="../high_scalability-2009/high_scalability-2009-06-10-Managing_cross_partition_transactions_in_a_distributed_KV_system.html">625 high scalability-2009-06-10-Managing cross partition transactions in a distributed KV system</a></p>
<p>Introduction: I spend ablog entrydiscussing single partition and every partition
transactions when using distributed KV systems and solutions for some common
problems</p><p>3 0.185504 <a title="623-tfidf-3" href="../high_scalability-2012/high_scalability-2012-10-22-Spanner_-_It%27s_About_Programmers_Building_Apps_Using_SQL_Semantics_at_NoSQL_Scale.html">1345 high scalability-2012-10-22-Spanner - It's About Programmers Building Apps Using SQL Semantics at NoSQL Scale</a></p>
<p>Introduction: A lot of people seem to passionately dislike the termNewSQL, or pretty much
any newly coined term for that matter, but after watching Alex Lloyd, Senior
Staff Software Engineer Google, give a great talkon Building Spanner, that's
the term that fits Spanner best.Spanner wraps the SQL + transaction model of
OldSQL around the reworked bones of a globally distributed NoSQL system. That
seems NewSQL to me.As Spanner is a not so distant cousin of BigTable, the
NoSQL component should be no surprise. Spanner is charged with spanning
millions of machines inside any number of geographically distributed
datacenters. What is surprising is how OldSQL has been embraced. In anearlier
2011 talkgiven by Alex at the HotStorage conference, the reason for embracing
OldSQL was the desire to make it easier and faster for programmers to build
applications. The main ideas will seem quite familiar:There's a false
dichotomy between little complicated databases and huge, scalable, simple
ones. We can have featur</p><p>4 0.1823242 <a title="623-tfidf-4" href="../high_scalability-2007/high_scalability-2007-07-30-Product%3A_Photobucket.html">44 high scalability-2007-07-30-Product: Photobucket</a></p>
<p>Introduction: Photobucket'sfree account has a storage limit and a download bandwidth limit
of 10 GB per month. There's no bandwidth limit on the $25 Pro account.</p><p>5 0.17117354 <a title="623-tfidf-5" href="../high_scalability-2009/high_scalability-2009-03-16-Are_Cloud_Based_Memory_Architectures_the_Next_Big_Thing%3F.html">538 high scalability-2009-03-16-Are Cloud Based Memory Architectures the Next Big Thing?</a></p>
<p>Introduction: We are on the edge of two potent technological changes: Clouds and Memory
Based Architectures. This evolution will rip open a chasm where new players
can enter and prosper. Google is the master of disk. You can't beat them at a
game they perfected. Disk based databases like SimpleDB and BigTable are
complicated beasts, typical last gasp products of any aging technology before
a change. The next era is the age of Memory and Cloud which will allow for new
players to succeed. The tipping point will be soon.Let's take a short trip
down web architecture lane:It's 1993: Yahoo runs on FreeBSD, Apache, Perl
scripts and a SQL databaseIt's 1995: Scale-up the database.It's 1998: LAMPIt's
1999: Stateless + Load Balanced + Database + SANIt's 2001: In-memory data-
grid.It's 2003: Add a caching layer.It's 2004: Add scale-out and
partitioning.It's 2005: Add asynchronous job scheduling and maybe a
distributed file system.It's 2007: Move it all into the cloud.It's 2008: Cloud
+ web scalable database.It'</p><p>6 0.1646736 <a title="623-tfidf-6" href="../high_scalability-2008/high_scalability-2008-04-05-Skype_Plans_for_PostgreSQL_to_Scale_to_1_Billion_Users.html">297 high scalability-2008-04-05-Skype Plans for PostgreSQL to Scale to 1 Billion Users</a></p>
<p>7 0.16290081 <a title="623-tfidf-7" href="../high_scalability-2011/high_scalability-2011-06-21-Running_TPC-C_on_MySQL-RDS.html">1065 high scalability-2011-06-21-Running TPC-C on MySQL-RDS</a></p>
<p>8 0.16005184 <a title="623-tfidf-8" href="../high_scalability-2012/high_scalability-2012-05-24-Build_your_own_twitter_like_real_time_analytics_-_a_step_by_step_guide.html">1251 high scalability-2012-05-24-Build your own twitter like real time analytics - a step by step guide</a></p>
<p>9 0.15681873 <a title="623-tfidf-9" href="../high_scalability-2012/high_scalability-2012-03-07-Scale_Indefinitely_on_S3_With_These_Secrets_of_the_S3_Masters.html">1205 high scalability-2012-03-07-Scale Indefinitely on S3 With These Secrets of the S3 Masters</a></p>
<p>10 0.14156158 <a title="623-tfidf-10" href="../high_scalability-2009/high_scalability-2009-03-17-IBM_WebSphere_eXtreme_Scale_%28IMDG%29.html">542 high scalability-2009-03-17-IBM WebSphere eXtreme Scale (IMDG)</a></p>
<p>11 0.14061099 <a title="623-tfidf-11" href="../high_scalability-2008/high_scalability-2008-05-05-HSCALE_-__Handling_200_Million_Transactions_Per_Month_Using_Transparent_Partitioning_With_MySQL_Proxy.html">315 high scalability-2008-05-05-HSCALE -  Handling 200 Million Transactions Per Month Using Transparent Partitioning With MySQL Proxy</a></p>
<p>12 0.13318519 <a title="623-tfidf-12" href="../high_scalability-2014/high_scalability-2014-03-31-How_WhatsApp_Grew_to_Nearly_500_Million_Users%2C_11%2C000_cores%2C_and_70_Million_Messages_a_Second.html">1622 high scalability-2014-03-31-How WhatsApp Grew to Nearly 500 Million Users, 11,000 cores, and 70 Million Messages a Second</a></p>
<p>13 0.12929726 <a title="623-tfidf-13" href="../high_scalability-2013/high_scalability-2013-05-01-Myth%3A_Eric_Brewer_on_Why_Banks_are_BASE_Not_ACID_-_Availability_Is_Revenue_.html">1450 high scalability-2013-05-01-Myth: Eric Brewer on Why Banks are BASE Not ACID - Availability Is Revenue </a></p>
<p>14 0.12875062 <a title="623-tfidf-14" href="../high_scalability-2010/high_scalability-2010-12-06-What_the_heck_are_you_actually_using_NoSQL_for%3F.html">954 high scalability-2010-12-06-What the heck are you actually using NoSQL for?</a></p>
<p>15 0.12525807 <a title="623-tfidf-15" href="../high_scalability-2013/high_scalability-2013-05-16-Paper%3A_Warp%3A_Multi-Key_Transactions_for_Key-Value_Stores.html">1459 high scalability-2013-05-16-Paper: Warp: Multi-Key Transactions for Key-Value Stores</a></p>
<p>16 0.11959483 <a title="623-tfidf-16" href="../high_scalability-2010/high_scalability-2010-06-28-VoltDB_Decapitates_Six_SQL_Urban_Myths_and_Delivers_Internet_Scale_OLTP_in_the_Process.html">849 high scalability-2010-06-28-VoltDB Decapitates Six SQL Urban Myths and Delivers Internet Scale OLTP in the Process</a></p>
<p>17 0.11896169 <a title="623-tfidf-17" href="../high_scalability-2008/high_scalability-2008-05-27-eBay_Architecture.html">331 high scalability-2008-05-27-eBay Architecture</a></p>
<p>18 0.11553929 <a title="623-tfidf-18" href="../high_scalability-2011/high_scalability-2011-08-05-Stuff_The_Internet_Says_On_Scalability_For_August_5%2C_2011.html">1093 high scalability-2011-08-05-Stuff The Internet Says On Scalability For August 5, 2011</a></p>
<p>19 0.11552618 <a title="623-tfidf-19" href="../high_scalability-2009/high_scalability-2009-10-13-Why_are_Facebook%2C_Digg%2C_and_Twitter_so_hard_to_scale%3F.html">721 high scalability-2009-10-13-Why are Facebook, Digg, and Twitter so hard to scale?</a></p>
<p>20 0.11455534 <a title="623-tfidf-20" href="../high_scalability-2009/high_scalability-2009-10-02-HighScalability_has_Moved_to_Squarespace.com%21_.html">714 high scalability-2009-10-02-HighScalability has Moved to Squarespace.com! </a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.198), (1, 0.103), (2, -0.015), (3, -0.067), (4, -0.022), (5, 0.12), (6, 0.037), (7, -0.007), (8, -0.039), (9, -0.005), (10, 0.03), (11, 0.03), (12, -0.041), (13, 0.061), (14, 0.061), (15, 0.024), (16, -0.063), (17, -0.012), (18, 0.05), (19, -0.023), (20, 0.059), (21, -0.015), (22, 0.038), (23, -0.007), (24, -0.059), (25, -0.097), (26, -0.046), (27, 0.043), (28, 0.06), (29, 0.04), (30, 0.047), (31, -0.051), (32, -0.136), (33, 0.045), (34, 0.054), (35, -0.043), (36, 0.06), (37, 0.006), (38, -0.037), (39, -0.009), (40, 0.063), (41, -0.046), (42, 0.078), (43, -0.107), (44, -0.017), (45, -0.016), (46, 0.085), (47, 0.09), (48, -0.031), (49, -0.052)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.97182053 <a title="623-lsi-1" href="../high_scalability-2009/high_scalability-2009-06-10-Dealing_with_multi-partition_transactions_in_a_distributed_KV_solution.html">623 high scalability-2009-06-10-Dealing with multi-partition transactions in a distributed KV solution</a></p>
<p>Introduction: I've been getting asked about this a lot lately so I figured I'd just blog
about it. Products like WebSphere eXtreme Scale work by taking a dataset,
partitioning it using a key and then assigning those partitions to a number of
JVMs. Each partition usually has a primary and a replica. These 'shards' are
assigned to JVMs. A transactional application typically interacts with the
data on a single partition at a time. This means the transaction is executed
in a single JVM. A server box will be able to do M of those transactions per
second and it scales because N boxes does MN (M multiplied by N) transactions
per second. Increase N, you get more transactions per second. Availability is
very good because a transaction only depends on 1 of the N servers that are
currently online. Any of the other (N-1) servers can go down or fail with no
impact on the transaction. So, single partition transactions can scale
indefinitely from a throughput point of view, offer very consistent response
times and</p><p>2 0.7160458 <a title="623-lsi-2" href="../high_scalability-2009/high_scalability-2009-06-10-Managing_cross_partition_transactions_in_a_distributed_KV_system.html">625 high scalability-2009-06-10-Managing cross partition transactions in a distributed KV system</a></p>
<p>Introduction: I spend ablog entrydiscussing single partition and every partition
transactions when using distributed KV systems and solutions for some common
problems</p><p>3 0.69217753 <a title="623-lsi-3" href="../high_scalability-2012/high_scalability-2012-03-07-Scale_Indefinitely_on_S3_With_These_Secrets_of_the_S3_Masters.html">1205 high scalability-2012-03-07-Scale Indefinitely on S3 With These Secrets of the S3 Masters</a></p>
<p>Introduction: In a great article, Amazon S3 Performance Tips & Tricks, Doug Grismore,
Director of Storage Operations for AWS, has outed the secret arcana normally
reserved for Premium Developer Support customers on how to really use S3:Size
matters. Workloads with less than 50-100 total requests per second don't
require any special effort. Customers that routinely perform thousands of
requests per second need a plan.Automated partitioning. Automated systems
scale S3 horizontally by continuously splitting data into partitions based on
high request rates and the number of keys in a partition (which leads to slow
lookups). Lessons you've learned with sharding may also apply to S3.   Avoid
hot spots. Like most sharding schemes, you want to avoid hot spots by the
smart selection of key names. S3 objects are stored inbuckets.  Each object is
identified using a key. Keys are kept in sorted order. Keys in S3 are
partitioned by prefix. Objects that sort together are stored together, so you
want to select key</p><p>4 0.66351938 <a title="623-lsi-4" href="../high_scalability-2011/high_scalability-2011-06-21-Running_TPC-C_on_MySQL-RDS.html">1065 high scalability-2011-06-21-Running TPC-C on MySQL-RDS</a></p>
<p>Introduction: I recently came across a TPC-C benchmark results held on MySQL based RDS
databases. You can see ithere. I think the results may bring light to many
questions concerning MySQL scalability in general and RDS scalability in
particular. (For disclosure, I'm working forScaleBasewhere we run an internal
scale out TPC-C benchmark these days, and will publish results soon).TPC-
CTPC-C is a standard database benchmark, used to measure databases. The
database vendors invest big bucks in running this test, and showing off which
database is faster, and can scale better.It is a write intensive test, so it
doesn't necessarily reflect the behavior of the database in your application.
But it does give some very important insights on what you can expect from your
database under heavy load.The Benchmark ProcessFirst of all, I have some
comments for the benchmark method itself.Generally - the benchmarks were held
in an orderly fashion and in a rather methodological way - which increases the
credibility o</p><p>5 0.66105127 <a title="623-lsi-5" href="../high_scalability-2009/high_scalability-2009-03-17-IBM_WebSphere_eXtreme_Scale_%28IMDG%29.html">542 high scalability-2009-03-17-IBM WebSphere eXtreme Scale (IMDG)</a></p>
<p>Introduction: IBM WebSphere eXtreme Scale is IBMs in memory data grid product (IMDG). It can
be used as a key-value store which partitions the keys (using a form of
consistent hashing) over a set of servers such that each server is responsible
for a subset of the keys. It automatically handles replication which can be
either synchronous of asynchronous and handles advanced placement so that
replicas can be placed in different physical zones when compared to the
placement of the primary. Think buildings, racks, floor, data centers.It is
fully elastic in that servers can be added and removed and it automatically
redistributes the partition primaries and backups. It can be scaled from one
server to hundreds if not thousands of JVMs in a single grid. Each additional
server provides more CPU, memory capacity and network and it scales linearly
with grid growth.It also has a key-graph mode where a graph of objects can be
associated with a single key and it allows fine grained modification of that
graph. Th</p><p>6 0.65100467 <a title="623-lsi-6" href="../high_scalability-2014/high_scalability-2014-04-10-Paper%3A_Scalable_Atomic_Visibility_with_RAMP_Transactions_-_Scale_Linearly_to_100_Servers.html">1629 high scalability-2014-04-10-Paper: Scalable Atomic Visibility with RAMP Transactions - Scale Linearly to 100 Servers</a></p>
<p>7 0.64378881 <a title="623-lsi-7" href="../high_scalability-2012/high_scalability-2012-10-22-Spanner_-_It%27s_About_Programmers_Building_Apps_Using_SQL_Semantics_at_NoSQL_Scale.html">1345 high scalability-2012-10-22-Spanner - It's About Programmers Building Apps Using SQL Semantics at NoSQL Scale</a></p>
<p>8 0.64003694 <a title="623-lsi-8" href="../high_scalability-2010/high_scalability-2010-02-10-ElasticSearch_-_Open_Source%2C_Distributed%2C_RESTful_Search_Engine.html">775 high scalability-2010-02-10-ElasticSearch - Open Source, Distributed, RESTful Search Engine</a></p>
<p>9 0.61881655 <a title="623-lsi-9" href="../high_scalability-2012/high_scalability-2012-08-14-MemSQL_Architecture_-_The_Fast_%28MVCC%2C_InMem%2C_LockFree%2C_CodeGen%29_and_Familiar_%28SQL%29.html">1304 high scalability-2012-08-14-MemSQL Architecture - The Fast (MVCC, InMem, LockFree, CodeGen) and Familiar (SQL)</a></p>
<p>10 0.60437137 <a title="623-lsi-10" href="../high_scalability-2008/high_scalability-2008-04-05-Skype_Plans_for_PostgreSQL_to_Scale_to_1_Billion_Users.html">297 high scalability-2008-04-05-Skype Plans for PostgreSQL to Scale to 1 Billion Users</a></p>
<p>11 0.59988457 <a title="623-lsi-11" href="../high_scalability-2007/high_scalability-2007-11-13-Flickr_Architecture.html">152 high scalability-2007-11-13-Flickr Architecture</a></p>
<p>12 0.59922647 <a title="623-lsi-12" href="../high_scalability-2011/high_scalability-2011-02-10-Database_Isolation_Levels_And_Their_Effects_on_Performance_and_Scalability.html">986 high scalability-2011-02-10-Database Isolation Levels And Their Effects on Performance and Scalability</a></p>
<p>13 0.59795624 <a title="623-lsi-13" href="../high_scalability-2012/high_scalability-2012-04-25-The_Anatomy_of_Search_Technology%3A_blekko%E2%80%99s_NoSQL_database.html">1233 high scalability-2012-04-25-The Anatomy of Search Technology: blekko’s NoSQL database</a></p>
<p>14 0.59159762 <a title="623-lsi-14" href="../high_scalability-2014/high_scalability-2014-03-31-How_WhatsApp_Grew_to_Nearly_500_Million_Users%2C_11%2C000_cores%2C_and_70_Million_Messages_a_Second.html">1622 high scalability-2014-03-31-How WhatsApp Grew to Nearly 500 Million Users, 11,000 cores, and 70 Million Messages a Second</a></p>
<p>15 0.59071738 <a title="623-lsi-15" href="../high_scalability-2013/high_scalability-2013-01-28-DuckDuckGo_Architecture_-_1_Million_Deep_Searches_a_Day_and_Growing.html">1395 high scalability-2013-01-28-DuckDuckGo Architecture - 1 Million Deep Searches a Day and Growing</a></p>
<p>16 0.58683407 <a title="623-lsi-16" href="../high_scalability-2008/high_scalability-2008-03-18-Database_Design_101.html">281 high scalability-2008-03-18-Database Design 101</a></p>
<p>17 0.58534127 <a title="623-lsi-17" href="../high_scalability-2009/high_scalability-2009-09-16-Paper%3A_A_practical_scalable_distributed_B-tree.html">705 high scalability-2009-09-16-Paper: A practical scalable distributed B-tree</a></p>
<p>18 0.58211374 <a title="623-lsi-18" href="../high_scalability-2011/high_scalability-2011-02-02-Piccolo_-_Building_Distributed_Programs_that_are_11x_Faster_than_Hadoop.html">983 high scalability-2011-02-02-Piccolo - Building Distributed Programs that are 11x Faster than Hadoop</a></p>
<p>19 0.58132058 <a title="623-lsi-19" href="../high_scalability-2013/high_scalability-2013-05-23-Paper%3A_Calvin%3A_Fast_Distributed_Transactions_for_Partitioned_Database_Systems.html">1463 high scalability-2013-05-23-Paper: Calvin: Fast Distributed Transactions for Partitioned Database Systems</a></p>
<p>20 0.58121437 <a title="623-lsi-20" href="../high_scalability-2010/high_scalability-2010-06-28-VoltDB_Decapitates_Six_SQL_Urban_Myths_and_Delivers_Internet_Scale_OLTP_in_the_Process.html">849 high scalability-2010-06-28-VoltDB Decapitates Six SQL Urban Myths and Delivers Internet Scale OLTP in the Process</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.245), (2, 0.194), (10, 0.045), (17, 0.102), (43, 0.012), (61, 0.114), (79, 0.097), (85, 0.039), (89, 0.016), (94, 0.048)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96011436 <a title="623-lda-1" href="../high_scalability-2009/high_scalability-2009-06-10-Dealing_with_multi-partition_transactions_in_a_distributed_KV_solution.html">623 high scalability-2009-06-10-Dealing with multi-partition transactions in a distributed KV solution</a></p>
<p>Introduction: I've been getting asked about this a lot lately so I figured I'd just blog
about it. Products like WebSphere eXtreme Scale work by taking a dataset,
partitioning it using a key and then assigning those partitions to a number of
JVMs. Each partition usually has a primary and a replica. These 'shards' are
assigned to JVMs. A transactional application typically interacts with the
data on a single partition at a time. This means the transaction is executed
in a single JVM. A server box will be able to do M of those transactions per
second and it scales because N boxes does MN (M multiplied by N) transactions
per second. Increase N, you get more transactions per second. Availability is
very good because a transaction only depends on 1 of the N servers that are
currently online. Any of the other (N-1) servers can go down or fail with no
impact on the transaction. So, single partition transactions can scale
indefinitely from a throughput point of view, offer very consistent response
times and</p><p>2 0.95331496 <a title="623-lda-2" href="../high_scalability-2008/high_scalability-2008-10-22-Server_load_balancing_architectures%2C_Part_2%3A_Application-level_load_balancing.html">427 high scalability-2008-10-22-Server load balancing architectures, Part 2: Application-level load balancing</a></p>
<p>Introduction: The transport-level server load balancing architectures described inthe first
half of this articleare more than adequate for many Web sites, but more
complex and dynamic sites can't depend on them. Applications that rely on
cache or session data must be able to handle a sequence of requests from the
same client accurately and efficiently, without failing. In this follow up to
his introduction to server load balancing, Gregor Roth discusses various
application-level load balancing architectures, helping you decide which one
will best meet the business requirements of your Web site.The first half of
this articledescribes transport-level server load balancing solutions, such as
TCP/IP-based load balancers, and analyzes their benefits and disadvantages.
Load balancing on the TCP/IP level spreads incoming TCP connections over the
real servers in a server farm. It is sufficient in most cases, especially for
static Web sites. However, support for dynamic Web sites often requires
higher-level</p><p>3 0.95276374 <a title="623-lda-3" href="../high_scalability-2010/high_scalability-2010-07-30-Hot_Scalability_Links_for_July_30%2C_2010.html">869 high scalability-2010-07-30-Hot Scalability Links for July 30, 2010</a></p>
<p>Introduction: Jeremy Zawodny, while performing data alchemy in the dungeons of Craigslist,
stored1,250,000,000 Key/Value Pairs in Redis on a 32GB Machine.Data sorting
world record: 1 terabyte, 1 minute. The system has 52 computer nodes, each
node is a commodity server with two quad-core processors, 24 gigabytes (GB)
memory and sixteen 500 GB disks. It's not just hardware though, they also
built a software that utilized all their CPU and RAM.Tweets of Gold:wm: I am
really getting the sense that none of you yokels waxing profound about
scalability actually has anything factual to sayjoestump: I think you can do
things to *mitigate* pain points up front. You don't need to over-engineer,
but it's not hard to look forward.danielcrenna: I love it when I check in
debug code accidentally and it turns into a three day hunt for a major
scalability problemjoestump: Your post also makes me think of another phrase I
say often: Scaling == Specialization. Bigger scale = More
specialization.Quora: What are the scal</p><p>4 0.9485966 <a title="623-lda-4" href="../high_scalability-2010/high_scalability-2010-06-10-The_Four_Meta_Secrets_of_Scaling_at_Facebook.html">840 high scalability-2010-06-10-The Four Meta Secrets of Scaling at Facebook</a></p>
<p>Introduction: Aditya Agarwal, Director of Engineering at Facebook, gave an excellent Scale
at Facebooktalk that covers their architecture, but the talk is really more
about how to scale an organization by preserving the best parts of its
culture. The key take home of the talk is: You can get the code right, you can
get the products right, but you need to get the culture right first. If you
don't get the culture right then your company won't scale.This leads into the
four meta secrets of scaling at Facebook:Scaling takes IterationDon't Over
DesignChoose the right tool for the job, but realize that your choice comes
with overhead.Get the culture right. Move Fast \- break things. Huge Impact \-
small teams. Be bold \- innovate.Some Background Facebook is big: 400 million
active users; users spend an average of 20 minutes a day; 5 billion pieces of
content (status updates, comments, likes, photo uploads, video uploads, chat
messages, inbox messages, group events, fan pages, friend connections) are
share</p><p>5 0.94859308 <a title="623-lda-5" href="../high_scalability-2007/high_scalability-2007-08-22-How_many_machines_do_you_need_to_run_your_site%3F.html">70 high scalability-2007-08-22-How many machines do you need to run your site?</a></p>
<p>Introduction: AmazinglyTechCrunchruns their website on one web server and one database
server, according to the fascinating surveyWhat the Webâ&euro;&trade;s most popular sites
are running onbyPingdom, a provider of uptime and response time
monitoring.Early we learnedPlentyOfFishcatches and releases many millions of
hits a day on just 1 web server and three database servers.Googleruns
aDalekarmy full of servers.YouSendIt, a company making it easy to send and
receive large files, has 24 web servers, 3 database servers, 170 storage
servers, and a few miscellaneous servers.Vimeo, a video sharing company, has
100 servers for streaming video, 4 web servers, and 2 database servers.Meebo,
an AJAX based instant messaging company, uses 40 servers to handle messaging,
over 40 web servers, and 10 servers for forums, jabber, testing, and so
on.FeedBurner, a news feed management company, has 70 web servers, 15 database
servers, and 10 miscellaneous servers. Now multiply FeedBurner's server count
by two because they maintain</p><p>6 0.94856995 <a title="623-lda-6" href="../high_scalability-2008/high_scalability-2008-12-16-%5BANN%5D_New_Open_Source_Cache_System.html">467 high scalability-2008-12-16-[ANN] New Open Source Cache System</a></p>
<p>7 0.94689709 <a title="623-lda-7" href="../high_scalability-2010/high_scalability-2010-10-12-The_CIO%E2%80%99s_Problem%3A_Cloud_%E2%80%9CMess%E2%80%9D_or_Cloud_%E2%80%9CMash%E2%80%9D.html">918 high scalability-2010-10-12-The CIO’s Problem: Cloud “Mess” or Cloud “Mash”</a></p>
<p>8 0.94643039 <a title="623-lda-8" href="../high_scalability-2013/high_scalability-2013-12-02-Evolution_of_Bazaarvoice%E2%80%99s_Architecture_to_500M_Unique_Users_Per_Month.html">1557 high scalability-2013-12-02-Evolution of Bazaarvoice’s Architecture to 500M Unique Users Per Month</a></p>
<p>9 0.94600725 <a title="623-lda-9" href="../high_scalability-2012/high_scalability-2012-02-07-Hypertable_Routs_HBase_in_Performance_Test_--_HBase_Overwhelmed_by_Garbage_Collection.html">1189 high scalability-2012-02-07-Hypertable Routs HBase in Performance Test -- HBase Overwhelmed by Garbage Collection</a></p>
<p>10 0.94449061 <a title="623-lda-10" href="../high_scalability-2009/high_scalability-2009-04-23-Which_Key_value_pair_database_to_be_used.html">578 high scalability-2009-04-23-Which Key value pair database to be used</a></p>
<p>11 0.94412738 <a title="623-lda-11" href="../high_scalability-2011/high_scalability-2011-08-05-Stuff_The_Internet_Says_On_Scalability_For_August_5%2C_2011.html">1093 high scalability-2011-08-05-Stuff The Internet Says On Scalability For August 5, 2011</a></p>
<p>12 0.94374657 <a title="623-lda-12" href="../high_scalability-2012/high_scalability-2012-02-21-Pixable_Architecture_-_Crawling%2C_Analyzing%2C_and_Ranking_20_Million_Photos_a_Day.html">1197 high scalability-2012-02-21-Pixable Architecture - Crawling, Analyzing, and Ranking 20 Million Photos a Day</a></p>
<p>13 0.9430747 <a title="623-lda-13" href="../high_scalability-2013/high_scalability-2013-01-21-Processing_100_Million_Pixels_a_Day_-_Small_Amounts_of_Contention_Cause_Big_Problems_at_Scale.html">1390 high scalability-2013-01-21-Processing 100 Million Pixels a Day - Small Amounts of Contention Cause Big Problems at Scale</a></p>
<p>14 0.94079101 <a title="623-lda-14" href="../high_scalability-2009/high_scalability-2009-08-31-Squarespace_Architecture_-_A_Grid_Handles_Hundreds_of_Millions_of_Requests_a_Month_.html">691 high scalability-2009-08-31-Squarespace Architecture - A Grid Handles Hundreds of Millions of Requests a Month </a></p>
<p>15 0.94018036 <a title="623-lda-15" href="../high_scalability-2009/high_scalability-2009-08-26-Hot_Links_for_2009-8-26.html">688 high scalability-2009-08-26-Hot Links for 2009-8-26</a></p>
<p>16 0.93919969 <a title="623-lda-16" href="../high_scalability-2012/high_scalability-2012-08-22-Cloud_Deployment%3A_It%E2%80%99s_All_About_Cloud_Automation.html">1309 high scalability-2012-08-22-Cloud Deployment: It’s All About Cloud Automation</a></p>
<p>17 0.93912745 <a title="623-lda-17" href="../high_scalability-2010/high_scalability-2010-06-14-How_scalable_could_be_a_cPanel_Hosting_service%3F.html">841 high scalability-2010-06-14-How scalable could be a cPanel Hosting service?</a></p>
<p>18 0.93905514 <a title="623-lda-18" href="../high_scalability-2009/high_scalability-2009-01-17-Scaling_in_Games_%26_Virtual_Worlds___.html">496 high scalability-2009-01-17-Scaling in Games & Virtual Worlds   </a></p>
<p>19 0.93845081 <a title="623-lda-19" href="../high_scalability-2007/high_scalability-2007-07-06-Start_Here.html">1 high scalability-2007-07-06-Start Here</a></p>
<p>20 0.93813151 <a title="623-lda-20" href="../high_scalability-2009/high_scalability-2009-10-12-High_Performance_at_Massive_Scale_%E2%80%93__Lessons_learned_at_Facebook.html">720 high scalability-2009-10-12-High Performance at Massive Scale –  Lessons learned at Facebook</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
