<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>534 high scalability-2009-03-12-Google TechTalk: Amdahl's Law in the Multicore Era</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2009" href="../home/high_scalability-2009_home.html">high_scalability-2009</a> <a title="high_scalability-2009-534" href="#">high_scalability-2009-534</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>534 high scalability-2009-03-12-Google TechTalk: Amdahl's Law in the Multicore Era</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2009-534-html" href="http://highscalability.com//blog/2009/3/12/google-techtalk-amdahls-law-in-the-multicore-era.html">html</a></p><p>Introduction: Over the last several decades computer architects have been phenomenally successful turning the transistor bounty provided by Moore's Law into chips with ever increasing single-threaded performance. During many of these successful years, however, many researchers paid scant attention to multiprocessor work. Now as vendors turn to multicore chips, researchers are reacting with more papers on multi-threaded systems. While this is good, we are concerned that further work on single-thread performance will be squashed.     To help understand future high-level trade-offs, we develop a corollary to Amdahl's Law for multicore chips [Hill & Marty, IEEE Computer 2008]. It models fixed chip resources for alternative designs that use symmetric cores, asymmetric cores, or dynamic techniques that allow cores to work together on sequential execution. Our results encourage multicore designers to view performance of the entire chip rather than focus on core efficiencies. Moreover, we observe that obtai</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Over the last several decades computer architects have been phenomenally successful turning the transistor bounty provided by Moore's Law into chips with ever increasing single-threaded performance. [sent-1, score-0.929]
</p><p>2 During many of these successful years, however, many researchers paid scant attention to multiprocessor work. [sent-2, score-0.591]
</p><p>3 Now as vendors turn to multicore chips, researchers are reacting with more papers on multi-threaded systems. [sent-3, score-0.661]
</p><p>4 While this is good, we are concerned that further work on single-thread performance will be squashed. [sent-4, score-0.079]
</p><p>5 To help understand future high-level trade-offs, we develop a corollary to Amdahl's Law for multicore chips [Hill & Marty, IEEE Computer 2008]. [sent-5, score-0.668]
</p><p>6 It models fixed chip resources for alternative designs that use symmetric cores, asymmetric cores, or dynamic techniques that allow cores to work together on sequential execution. [sent-6, score-0.789]
</p><p>7 Our results encourage multicore designers to view performance of the entire chip rather than focus on core efficiencies. [sent-7, score-0.543]
</p><p>8 Moreover, we observe that obtaining optimal multicore performance requires further research BOTH in extracting more parallelism and making sequential cores faster. [sent-8, score-1.088]
</p><p>9 This talk is based on an HPCA 2008 keynote address. [sent-9, score-0.085]
</p><p>10 edu/~markhill) is professor in both the computer sciences department and the electrical and computer engineering department at the University of Wisconsin--Madison, where he also co-leads the Wisconsin Multifacet (http://www. [sent-15, score-1.172]
</p><p>11 His research interests include parallel computer system design, memory system design, computer simulation, and recently transactional memory. [sent-19, score-0.716]
</p><p>12 He earned a PhD from University of California, Berkeley. [sent-20, score-0.102]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('multicore', 0.318), ('computer', 0.254), ('chips', 0.23), ('ieee', 0.218), ('hill', 0.213), ('cores', 0.204), ('fellow', 0.2), ('department', 0.175), ('researchers', 0.163), ('university', 0.153), ('chip', 0.145), ('sequential', 0.135), ('marty', 0.134), ('scant', 0.134), ('wisconsin', 0.134), ('law', 0.126), ('multiprocessor', 0.126), ('symmetric', 0.126), ('corollary', 0.12), ('obtaining', 0.116), ('sciences', 0.116), ('reacting', 0.112), ('asymmetric', 0.109), ('amdahl', 0.109), ('phd', 0.109), ('extracting', 0.107), ('transistor', 0.107), ('research', 0.104), ('interests', 0.104), ('observe', 0.104), ('earned', 0.102), ('professor', 0.1), ('successful', 0.099), ('acm', 0.098), ('electrical', 0.098), ('decades', 0.092), ('moreover', 0.088), ('simulation', 0.087), ('keynote', 0.085), ('encourage', 0.08), ('california', 0.08), ('moore', 0.08), ('concerned', 0.079), ('turning', 0.077), ('david', 0.073), ('mark', 0.072), ('architects', 0.07), ('designs', 0.07), ('paid', 0.069), ('papers', 0.068)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99999982 <a title="534-tfidf-1" href="../high_scalability-2009/high_scalability-2009-03-12-Google_TechTalk%3A_Amdahl%27s_Law_in_the_Multicore_Era.html">534 high scalability-2009-03-12-Google TechTalk: Amdahl's Law in the Multicore Era</a></p>
<p>Introduction: Over the last several decades computer architects have been phenomenally successful turning the transistor bounty provided by Moore's Law into chips with ever increasing single-threaded performance. During many of these successful years, however, many researchers paid scant attention to multiprocessor work. Now as vendors turn to multicore chips, researchers are reacting with more papers on multi-threaded systems. While this is good, we are concerned that further work on single-thread performance will be squashed.     To help understand future high-level trade-offs, we develop a corollary to Amdahl's Law for multicore chips [Hill & Marty, IEEE Computer 2008]. It models fixed chip resources for alternative designs that use symmetric cores, asymmetric cores, or dynamic techniques that allow cores to work together on sequential execution. Our results encourage multicore designers to view performance of the entire chip rather than focus on core efficiencies. Moreover, we observe that obtai</p><p>2 0.1997588 <a title="534-tfidf-2" href="../high_scalability-2009/high_scalability-2009-05-31-Parallel_Programming_for_real-world.html">612 high scalability-2009-05-31-Parallel Programming for real-world</a></p>
<p>Introduction: Multicore computers shift the burden of software performance from chip designers and architects to software developers.   
What is the parallel Computing ? and what the different between Multi-Threading and Concurrency and Parallelism ? and what is differences between task and data parallel ?  and how we can use it ?    Fundamental article into Parallel Programming...</p><p>3 0.18161356 <a title="534-tfidf-3" href="../high_scalability-2009/high_scalability-2009-06-23-Learn_How_to_Exploit_Multiple_Cores_for_Better_Performance_and_Scalability.html">636 high scalability-2009-06-23-Learn How to Exploit Multiple Cores for Better Performance and Scalability</a></p>
<p>Introduction: InfoQueue has this  excellent talk by Brian Goetz  on the new features being added to Java SE 7 that will allow programmers to fully exploit our massively multi-processor future. While the talk is about Java it's really more general than that and there's a lot to learn here for everyone.  Brian starts with a short, coherent, and compelling explanation of why programmers can't expect to be saved by ever faster CPUs and why we must learn to exploit the strengths of multiple core computers to make our software go faster.   Some techniques for exploiting multiple cores are given in an equally  short, coherent, and compelling explanation of why divide and conquer as the secret to multi-core bliss, fork-join, how the Java approach differs from map-reduce, and lots of other juicy topics.   The multi-core "problem" is only going to get worse. Tilera founder Anant Agarwal  estimates by 2017  embedded processors could have 4,096 cores, server CPUs might have 512 cores and desktop chips could use</p><p>4 0.13798487 <a title="534-tfidf-4" href="../high_scalability-2008/high_scalability-2008-10-13-Challenges_from_large_scale_computing_at_Google.html">409 high scalability-2008-10-13-Challenges from large scale computing at Google</a></p>
<p>Introduction: From Greg Linden on a talk Google Fellow Jeff Dean gave last week at University of Washington Computer Science titled "Research Challenges Inspired by Large-Scale Computing at Google" :   Coming away from the talk, the biggest points for me were the considerable interest in reducing costs (especially reducing power costs), the suggestion that the Google cluster may eventually contain 10M machines at 1k locations, and the call to action for researchers on distributed systems and databases to think orders of magnitude bigger than they often are, not about running on hundreds of machines in one location, but hundreds of thousands of machines across many locations.</p><p>5 0.13218465 <a title="534-tfidf-5" href="../high_scalability-2012/high_scalability-2012-03-06-Ask_For_Forgiveness_Programming_-_Or_How_We%27ll_Program_1000_Cores.html">1204 high scalability-2012-03-06-Ask For Forgiveness Programming - Or How We'll Program 1000 Cores</a></p>
<p>Introduction: The argument for a massively multicore future is now familiar: while clock speeds have leveled off, device density is increasing, so the future is cheap chips with hundreds and thousands of cores. That’s the inexorable logic behind our multicore future.
 
The unsolved question that lurks deep in the dark part of a programmer’s mind is: how on earth are we to program these things? For problems that aren’t   embarrassingly parallel   , we really have no idea. IBM Research’s    David Ungar    has an idea. And it’s radical in the extreme...     Grace Hopper    once advised “It's easier to ask for forgiveness than it is to get permission.” I wonder if she had any idea that her strategy for dealing with human bureaucracy would the same strategy David Ungar thinks will help us tame  the technological bureaucracy of 1000+ core systems?    You may recognize David as the co-creator of the    Self programming    language, inspiration for the HotSpot technology in the JVM and the prototype model u</p><p>6 0.11630762 <a title="534-tfidf-6" href="../high_scalability-2010/high_scalability-2010-07-22-How_can_we_spark_the_movement_of_research_out_of_the_Ivory_Tower_and_into_production%3F.html">863 high scalability-2010-07-22-How can we spark the movement of research out of the Ivory Tower and into production?</a></p>
<p>7 0.11091596 <a title="534-tfidf-7" href="../high_scalability-2010/high_scalability-2010-02-01-What_Will_Kill_the_Cloud%3F.html">768 high scalability-2010-02-01-What Will Kill the Cloud?</a></p>
<p>8 0.10432486 <a title="534-tfidf-8" href="../high_scalability-2009/high_scalability-2009-04-26-Map-Reduce_for_Machine_Learning_on_Multicore.html">581 high scalability-2009-04-26-Map-Reduce for Machine Learning on Multicore</a></p>
<p>9 0.10015105 <a title="534-tfidf-9" href="../high_scalability-2010/high_scalability-2010-11-09-The_Tera-Scale_Effect_.html">939 high scalability-2010-11-09-The Tera-Scale Effect </a></p>
<p>10 0.098215744 <a title="534-tfidf-10" href="../high_scalability-2011/high_scalability-2011-05-12-Paper%3A_Mind_the_Gap%3A_Reconnecting_Architecture_and_OS_Research.html">1039 high scalability-2011-05-12-Paper: Mind the Gap: Reconnecting Architecture and OS Research</a></p>
<p>11 0.09749189 <a title="534-tfidf-11" href="../high_scalability-2008/high_scalability-2008-09-08-Guerrilla_Capacity_Planning_and_the_Law_of_Universal_Scalability.html">381 high scalability-2008-09-08-Guerrilla Capacity Planning and the Law of Universal Scalability</a></p>
<p>12 0.09288767 <a title="534-tfidf-12" href="../high_scalability-2009/high_scalability-2009-02-01-More_Chips_Means_Less_Salsa.html">505 high scalability-2009-02-01-More Chips Means Less Salsa</a></p>
<p>13 0.088572234 <a title="534-tfidf-13" href="../high_scalability-2010/high_scalability-2010-02-15-The_Amazing_Collective_Compute_Power_of_the_Ambient_Cloud.html">778 high scalability-2010-02-15-The Amazing Collective Compute Power of the Ambient Cloud</a></p>
<p>14 0.088256449 <a title="534-tfidf-14" href="../high_scalability-2009/high_scalability-2009-05-27-The_Future_of_the_Parallelism_and_its_Challenges.html">608 high scalability-2009-05-27-The Future of the Parallelism and its Challenges</a></p>
<p>15 0.087476708 <a title="534-tfidf-15" href="../high_scalability-2009/high_scalability-2009-09-04-Hot_Links_for_2009-9-4_.html">694 high scalability-2009-09-04-Hot Links for 2009-9-4 </a></p>
<p>16 0.086524807 <a title="534-tfidf-16" href="../high_scalability-2013/high_scalability-2013-09-20-Stuff_The_Internet_Says_On_Scalability_For_September_20%2C_2013.html">1520 high scalability-2013-09-20-Stuff The Internet Says On Scalability For September 20, 2013</a></p>
<p>17 0.082819827 <a title="534-tfidf-17" href="../high_scalability-2014/high_scalability-2014-02-21-Stuff_The_Internet_Says_On_Scalability_For_February_21st%2C_2014.html">1600 high scalability-2014-02-21-Stuff The Internet Says On Scalability For February 21st, 2014</a></p>
<p>18 0.070760302 <a title="534-tfidf-18" href="../high_scalability-2012/high_scalability-2012-09-10-Russ%E2%80%99_10_Ingredient_Recipe_for_Making_1_Million_TPS_on_%245K_Hardware.html">1319 high scalability-2012-09-10-Russ’ 10 Ingredient Recipe for Making 1 Million TPS on $5K Hardware</a></p>
<p>19 0.070597678 <a title="534-tfidf-19" href="../high_scalability-2008/high_scalability-2008-04-23-Behind_The_Scenes_of_Google_Scalability.html">309 high scalability-2008-04-23-Behind The Scenes of Google Scalability</a></p>
<p>20 0.069789685 <a title="534-tfidf-20" href="../high_scalability-2012/high_scalability-2012-11-05-Gone_Fishin%27%3A_Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_In_The_Ambient_Cloud.html">1355 high scalability-2012-11-05-Gone Fishin': Building Super Scalable Systems: Blade Runner Meets Autonomic Computing In The Ambient Cloud</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.087), (1, 0.035), (2, 0.027), (3, 0.081), (4, -0.021), (5, 0.016), (6, 0.001), (7, 0.062), (8, -0.092), (9, 0.047), (10, -0.004), (11, -0.062), (12, 0.03), (13, 0.041), (14, -0.025), (15, -0.027), (16, 0.016), (17, -0.021), (18, -0.019), (19, 0.021), (20, 0.008), (21, 0.009), (22, -0.095), (23, 0.011), (24, -0.008), (25, -0.023), (26, -0.043), (27, -0.017), (28, 0.035), (29, 0.021), (30, 0.016), (31, 0.055), (32, -0.028), (33, 0.015), (34, 0.017), (35, -0.081), (36, 0.067), (37, -0.003), (38, 0.012), (39, -0.012), (40, -0.027), (41, 0.046), (42, 0.0), (43, -0.077), (44, 0.021), (45, 0.009), (46, -0.02), (47, -0.028), (48, 0.058), (49, 0.048)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96812087 <a title="534-lsi-1" href="../high_scalability-2009/high_scalability-2009-03-12-Google_TechTalk%3A_Amdahl%27s_Law_in_the_Multicore_Era.html">534 high scalability-2009-03-12-Google TechTalk: Amdahl's Law in the Multicore Era</a></p>
<p>Introduction: Over the last several decades computer architects have been phenomenally successful turning the transistor bounty provided by Moore's Law into chips with ever increasing single-threaded performance. During many of these successful years, however, many researchers paid scant attention to multiprocessor work. Now as vendors turn to multicore chips, researchers are reacting with more papers on multi-threaded systems. While this is good, we are concerned that further work on single-thread performance will be squashed.     To help understand future high-level trade-offs, we develop a corollary to Amdahl's Law for multicore chips [Hill & Marty, IEEE Computer 2008]. It models fixed chip resources for alternative designs that use symmetric cores, asymmetric cores, or dynamic techniques that allow cores to work together on sequential execution. Our results encourage multicore designers to view performance of the entire chip rather than focus on core efficiencies. Moreover, we observe that obtai</p><p>2 0.8057487 <a title="534-lsi-2" href="../high_scalability-2009/high_scalability-2009-04-26-Map-Reduce_for_Machine_Learning_on_Multicore.html">581 high scalability-2009-04-26-Map-Reduce for Machine Learning on Multicore</a></p>
<p>Introduction: We are at the beginning of the multicore era. Computers will have increasingly many cores (processors), but there is still no good programming framework for these architectures, and thus no simple and unified way for machine learning to take advantage of the potential speed up.      In this paper, we develop a broadly applicable parallel programming method, one that is easily applied to many different learning algorithms. Our work is in distinct contrast to the tradition in machine learning of designing (often ingenious) ways to speed up a single algorithm at a time.      Specifically, we show that algorithms that fit the Statistical Query model can be written in a certain “summation form,” which allows them to be easily parallelized on multicore computers. We adapt Google’s map-reduce  paradigm to demonstrate this parallel speed up technique on a variety of learning algorithms including locally weighted linear regression (LWLR), k-means, logistic regression (LR), naive Bayes (NB), SVM</p><p>3 0.77101868 <a title="534-lsi-3" href="../high_scalability-2009/high_scalability-2009-05-27-The_Future_of_the_Parallelism_and_its_Challenges.html">608 high scalability-2009-05-27-The Future of the Parallelism and its Challenges</a></p>
<p>Introduction: The Future of the Parallelism and its Challenges    Research and education in Parallel computing technologies is more important than ever. Here I present a perspective on the past contributions, current status, and future direction of the parallelism technologies.     While machine power will grow impressively, increased parallelism, rather than clock rate, will be driving force in computing in the foreseeable future. This ongoing shift toward parallel architectural paradigms is one of the greatest challenges for the microprocessor and software industries. In 2005, Justin Ratter, chief technology officer of Intel Corporation, said ‘We are at the cusp of a transition to multicore, multithreaded architectures, and we still have not demonstrated the ease of programming the move will require…’      Key points:       A Little history   Parallelism Challenges   Under the hood, Parallelism Challenges    Synchronization problems   CAS problems       The future of the parallelism</p><p>4 0.7190845 <a title="534-lsi-4" href="../high_scalability-2009/high_scalability-2009-05-31-Parallel_Programming_for_real-world.html">612 high scalability-2009-05-31-Parallel Programming for real-world</a></p>
<p>Introduction: Multicore computers shift the burden of software performance from chip designers and architects to software developers.   
What is the parallel Computing ? and what the different between Multi-Threading and Concurrency and Parallelism ? and what is differences between task and data parallel ?  and how we can use it ?    Fundamental article into Parallel Programming...</p><p>5 0.69277322 <a title="534-lsi-5" href="../high_scalability-2014/high_scalability-2014-05-01-Paper%3A_Can_Programming_Be_Liberated_From_The_Von_Neumann_Style%3F_.html">1641 high scalability-2014-05-01-Paper: Can Programming Be Liberated From The Von Neumann Style? </a></p>
<p>Introduction: Famous computer scientist  John Backus , he's the B in BNF(Backus-Naur form) and the creator of Fortran, gave a Turing Award Lecture titled  Can programming be liberated from the von Neumann style?: a functional style and its algebra of programs , that has layed out a division in programming that lives long after it was published in 1977. 
 
It's the now familiar argument for why functional programming is superior:
  

The assignment statement is the von Neumann bottleneck of programming languages and keeps us thinking in word-at-a-time terms in much the same way the computer's bottleneck does.


...


The second world of conventional programming languages is the world of statements. The primary statement in that world is the assignment statement itself. All the other statements of the language exist in order to make it possible to perform a computation that must be based on this primitive construct: the assignment statement.

  
Here's a response by Dijkstra  A review of the 1977 Turi</p><p>6 0.68197578 <a title="534-lsi-6" href="../high_scalability-2009/high_scalability-2009-03-12-Paper%3A_Understanding_and_Designing_New_Server_Architectures_for_Emerging_Warehouse-Computing_Environments.html">535 high scalability-2009-03-12-Paper: Understanding and Designing New Server Architectures for Emerging Warehouse-Computing Environments</a></p>
<p>7 0.66977888 <a title="534-lsi-7" href="../high_scalability-2010/high_scalability-2010-05-12-The_Rise_of_the_Virtual_Cellular_Machines.html">826 high scalability-2010-05-12-The Rise of the Virtual Cellular Machines</a></p>
<p>8 0.63888156 <a title="534-lsi-8" href="../high_scalability-2011/high_scalability-2011-09-28-Pursue_robust_indefinite_scalability_with_the_Movable_Feast_Machine.html">1127 high scalability-2011-09-28-Pursue robust indefinite scalability with the Movable Feast Machine</a></p>
<p>9 0.61561942 <a title="534-lsi-9" href="../high_scalability-2009/high_scalability-2009-06-23-Learn_How_to_Exploit_Multiple_Cores_for_Better_Performance_and_Scalability.html">636 high scalability-2009-06-23-Learn How to Exploit Multiple Cores for Better Performance and Scalability</a></p>
<p>10 0.61270058 <a title="534-lsi-10" href="../high_scalability-2011/high_scalability-2011-05-12-Paper%3A_Mind_the_Gap%3A_Reconnecting_Architecture_and_OS_Research.html">1039 high scalability-2011-05-12-Paper: Mind the Gap: Reconnecting Architecture and OS Research</a></p>
<p>11 0.60596234 <a title="534-lsi-11" href="../high_scalability-2009/high_scalability-2009-02-01-More_Chips_Means_Less_Salsa.html">505 high scalability-2009-02-01-More Chips Means Less Salsa</a></p>
<p>12 0.59551561 <a title="534-lsi-12" href="../high_scalability-2010/high_scalability-2010-06-09-Paper%3A_Propagation_Networks%3A_A_Flexible_and_Expressive_Substrate_for_Computation_.html">839 high scalability-2010-06-09-Paper: Propagation Networks: A Flexible and Expressive Substrate for Computation </a></p>
<p>13 0.59132493 <a title="534-lsi-13" href="../high_scalability-2009/high_scalability-2009-07-21-Paper%3A_Parallelizing_the_Web_Browser.html">660 high scalability-2009-07-21-Paper: Parallelizing the Web Browser</a></p>
<p>14 0.58772856 <a title="534-lsi-14" href="../high_scalability-2010/high_scalability-2010-12-03-GPU_vs_CPU_Smackdown_%3A_The_Rise_of_Throughput-Oriented_Architectures.html">953 high scalability-2010-12-03-GPU vs CPU Smackdown : The Rise of Throughput-Oriented Architectures</a></p>
<p>15 0.58158708 <a title="534-lsi-15" href="../high_scalability-2009/high_scalability-2009-05-20-Paper%3A_Flux%3A_An_Adaptive_Partitioning_Operator_for_Continuous_Query_Systems.html">604 high scalability-2009-05-20-Paper: Flux: An Adaptive Partitioning Operator for Continuous Query Systems</a></p>
<p>16 0.57411957 <a title="534-lsi-16" href="../high_scalability-2012/high_scalability-2012-03-29-Strategy%3A_Exploit_Processor_Affinity_for_High_and_Predictable_Performance.html">1218 high scalability-2012-03-29-Strategy: Exploit Processor Affinity for High and Predictable Performance</a></p>
<p>17 0.57244098 <a title="534-lsi-17" href="../high_scalability-2012/high_scalability-2012-04-26-Akaros_-_an_open_source_operating_system_for_manycore_architectures.html">1234 high scalability-2012-04-26-Akaros - an open source operating system for manycore architectures</a></p>
<p>18 0.56812316 <a title="534-lsi-18" href="../high_scalability-2012/high_scalability-2012-08-16-Paper%3A_A_Provably_Correct_Scalable_Concurrent_Skip_List.html">1305 high scalability-2012-08-16-Paper: A Provably Correct Scalable Concurrent Skip List</a></p>
<p>19 0.56225592 <a title="534-lsi-19" href="../high_scalability-2010/high_scalability-2010-10-04-Paper%3A_An_Analysis_of_Linux_Scalability_to_Many_Cores__.html">914 high scalability-2010-10-04-Paper: An Analysis of Linux Scalability to Many Cores  </a></p>
<p>20 0.55424088 <a title="534-lsi-20" href="../high_scalability-2009/high_scalability-2009-05-06-Dyrad.html">591 high scalability-2009-05-06-Dyrad</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.133), (2, 0.139), (5, 0.468), (10, 0.013), (40, 0.034), (61, 0.058), (79, 0.056)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.85546845 <a title="534-lda-1" href="../high_scalability-2009/high_scalability-2009-03-12-Google_TechTalk%3A_Amdahl%27s_Law_in_the_Multicore_Era.html">534 high scalability-2009-03-12-Google TechTalk: Amdahl's Law in the Multicore Era</a></p>
<p>Introduction: Over the last several decades computer architects have been phenomenally successful turning the transistor bounty provided by Moore's Law into chips with ever increasing single-threaded performance. During many of these successful years, however, many researchers paid scant attention to multiprocessor work. Now as vendors turn to multicore chips, researchers are reacting with more papers on multi-threaded systems. While this is good, we are concerned that further work on single-thread performance will be squashed.     To help understand future high-level trade-offs, we develop a corollary to Amdahl's Law for multicore chips [Hill & Marty, IEEE Computer 2008]. It models fixed chip resources for alternative designs that use symmetric cores, asymmetric cores, or dynamic techniques that allow cores to work together on sequential execution. Our results encourage multicore designers to view performance of the entire chip rather than focus on core efficiencies. Moreover, we observe that obtai</p><p>2 0.69186753 <a title="534-lda-2" href="../high_scalability-2008/high_scalability-2008-06-06-GigaOm_Structure_08_Conference_on_June_25th_in_San_Francisco.html">341 high scalability-2008-06-06-GigaOm Structure 08 Conference on June 25th in San Francisco</a></p>
<p>Introduction: If you just can't get enough high scalability talk you might want to take a look GigaOm's Structure 08 conference. The slate of speakers looks appropriately interesting and San Francisco is truly magical this time of year. High Scalability readers even get a price break is you use the HIGHSCALE discount code! I'll be on vacation so I won't see you there, but it looks like a good time. For a nice change of pace consider visiting   MoMA   next door.  Here's a blurb on the conference:   A reminder to our readers about Structure 08, GigaOm's upcoming conference dedicated to web infrastructure. In addition to keynotes from leaders like Jim Crowe, chairman and CEO of Level 3 Communications and Werner Vogels, CTO of Amazon, the event will feature workshops from Google App Engine, Microsoft and a special workshop from Fenwick and West who will cover how to raise money for an infrastructure start up. Learn from the guru's at Amazon, Google, Microsoft, Sun, VMWare and more about what the future</p><p>3 0.67242122 <a title="534-lda-3" href="../high_scalability-2012/high_scalability-2012-06-27-Paper%3A_Logic_and_Lattices_for_Distributed_Programming.html">1273 high scalability-2012-06-27-Paper: Logic and Lattices for Distributed Programming</a></p>
<p>Introduction: Neil Conway  from Berkeley CS is giving an advanced level talk at  a meetup today  in San Francisco on a new paper:  Logic and Lattices for Distributed Programming  - extending set logic to support CRDT-style lattices. 
 
The description of the meetup is probably the clearest introduction to the paper:
  Developers are increasingly choosing datastores that sacrifice strong consistency guarantees in exchange for improved performance and availability. Unfortunately, writing reliable distributed programs without the benefit of strong consistency can be very challenging.

 


In this talk, I'll discuss work from our group at UC Berkeley that aims to make it easier to write distributed programs without relying on strong consistency. Bloom is a declarative programming language for distributed computing, while CALM is an analysis technique that identifies programs that are guaranteed to be eventually consistent. I'll then discuss our recent work on extending CALM to support a broader range of</p><p>4 0.66185534 <a title="534-lda-4" href="../high_scalability-2007/high_scalability-2007-11-13-Friendster_Lost_Lead_Because_of_a_Failure_to_Scale.html">153 high scalability-2007-11-13-Friendster Lost Lead Because of a Failure to Scale</a></p>
<p>Introduction: Hey, this scaling stuff might just be important.  Jim Scheinman,  former Bebo and Friendster exec, puts the blame squarely on Friendster's inability to scale as why they lost the social networking race:   VB  : Can you tell me a bit about what you learned in your time at Friendster? 
 
     JS  :  For me, it basically came down to failed execution on the technology side — we had millions of Friendster members begging us to get the site working faster so they could log in and spend hours social networking with their friends. I remember coming in to the office for months reading thousands of customer service emails telling us that if we didn’t get our site working better soon, they’d be ‘forced to join’ a new social networking site that had just launched called MySpace…the rest is history. To be fair to Friendster’s technology team at the time, they were on the forefront of many new scaling and database issues that web sites simply hadn’t had to deal with prior to Friendster. As is often</p><p>5 0.63081139 <a title="534-lda-5" href="../high_scalability-2011/high_scalability-2011-09-09-Stuff_The_Internet_Says_On_Scalability_For_September_9%2C_2011.html">1113 high scalability-2011-09-09-Stuff The Internet Says On Scalability For September 9, 2011</a></p>
<p>Introduction: Scale the modern way   / No brush / No lather / No rub-in / Big tube 35 cents - Drug stores /  HighScalability : 
  
    GAE  Serves 1.5 Billion Pages a Day   
  Potent  quotables :  
 
   @ kendallmiller    : The code changes I'm most proud of are the ones few people will ever see - like I just tripled the scalability of our session analysis. 
  @ Kellblog  : Heard: "Cassandra is more a system on which you build a  DBMS  than a  DBMS  itself."  
  @DDevine_au   :  Ah  dammit . I'm thinking of using a    #    NoSQL     database. Down the rabbit hole I go. 
 
 
  A comprehensive guide to parallel video decoding  .  Emeric  Grange with a sweet explanation of the decoding process.   
   Node.js vs. Scala - "Scaling in the large" . tedsuo tldrs it:  in node, there is only one concurrency model.  A number of other platforms offer multiple concurrency models.  If you want access to one of those other models down the line, you will have to carve off that part of your application and rewrite i</p><p>6 0.58759254 <a title="534-lda-6" href="../high_scalability-2010/high_scalability-2010-09-09-How_did_Google_Instant_become_Faster_with_5-7X_More_Results_Pages%3F.html">899 high scalability-2010-09-09-How did Google Instant become Faster with 5-7X More Results Pages?</a></p>
<p>7 0.57354742 <a title="534-lda-7" href="../high_scalability-2008/high_scalability-2008-02-26-Architecture_to_Allow_High_Availability_File_Upload.html">262 high scalability-2008-02-26-Architecture to Allow High Availability File Upload</a></p>
<p>8 0.56871814 <a title="534-lda-8" href="../high_scalability-2009/high_scalability-2009-01-05-Messaging_is_not_just_for_investment_banks.html">485 high scalability-2009-01-05-Messaging is not just for investment banks</a></p>
<p>9 0.56762004 <a title="534-lda-9" href="../high_scalability-2013/high_scalability-2013-09-27-Stuff_The_Internet_Says_On_Scalability_For_September_27%2C_2013.html">1523 high scalability-2013-09-27-Stuff The Internet Says On Scalability For September 27, 2013</a></p>
<p>10 0.56055391 <a title="534-lda-10" href="../high_scalability-2007/high_scalability-2007-07-30-Product%3A_GridLayer._Utility_computing_for_online_application.html">42 high scalability-2007-07-30-Product: GridLayer. Utility computing for online application</a></p>
<p>11 0.55853027 <a title="534-lda-11" href="../high_scalability-2013/high_scalability-2013-07-05-Stuff_The_Internet_Says_On_Scalability_For_July_5%2C_2013.html">1487 high scalability-2013-07-05-Stuff The Internet Says On Scalability For July 5, 2013</a></p>
<p>12 0.55121273 <a title="534-lda-12" href="../high_scalability-2012/high_scalability-2012-05-18-Stuff_The_Internet_Says_On_Scalability_For_May_18%2C_2012.html">1247 high scalability-2012-05-18-Stuff The Internet Says On Scalability For May 18, 2012</a></p>
<p>13 0.53221792 <a title="534-lda-13" href="../high_scalability-2013/high_scalability-2013-03-15-Stuff_The_Internet_Says_On_Scalability_For_March_15%2C_2013.html">1424 high scalability-2013-03-15-Stuff The Internet Says On Scalability For March 15, 2013</a></p>
<p>14 0.51847345 <a title="534-lda-14" href="../high_scalability-2014/high_scalability-2014-05-15-Paper%3A_SwiftCloud%3A_Fault-Tolerant_Geo-Replication_Integrated_all_the_Way_to_the_Client_Machine.html">1648 high scalability-2014-05-15-Paper: SwiftCloud: Fault-Tolerant Geo-Replication Integrated all the Way to the Client Machine</a></p>
<p>15 0.50466502 <a title="534-lda-15" href="../high_scalability-2007/high_scalability-2007-12-05-Product%3A_Tugela_Cache.html">174 high scalability-2007-12-05-Product: Tugela Cache</a></p>
<p>16 0.4892357 <a title="534-lda-16" href="../high_scalability-2007/high_scalability-2007-10-20-Should_you_build_your_next_website_using_3tera%27s_grid_OS%3F.html">126 high scalability-2007-10-20-Should you build your next website using 3tera's grid OS?</a></p>
<p>17 0.45140395 <a title="534-lda-17" href="../high_scalability-2012/high_scalability-2012-12-10-Switch_your_databases_to_Flash_storage._Now._Or_you%27re_doing_it_wrong..html">1369 high scalability-2012-12-10-Switch your databases to Flash storage. Now. Or you're doing it wrong.</a></p>
<p>18 0.44922599 <a title="534-lda-18" href="../high_scalability-2009/high_scalability-2009-05-08-Publish-subscribe_model_does_not_scale%3F.html">595 high scalability-2009-05-08-Publish-subscribe model does not scale?</a></p>
<p>19 0.4377993 <a title="534-lda-19" href="../high_scalability-2010/high_scalability-2010-12-23-Paper%3A_CRDTs%3A_Consistency_without_concurrency_control.html">963 high scalability-2010-12-23-Paper: CRDTs: Consistency without concurrency control</a></p>
<p>20 0.42800286 <a title="534-lda-20" href="../high_scalability-2010/high_scalability-2010-08-18-Misco%3A_A_MapReduce_Framework_for_Mobile_Systems_-_Start_of_the_Ambient_Cloud%3F.html">882 high scalability-2010-08-18-Misco: A MapReduce Framework for Mobile Systems - Start of the Ambient Cloud?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
