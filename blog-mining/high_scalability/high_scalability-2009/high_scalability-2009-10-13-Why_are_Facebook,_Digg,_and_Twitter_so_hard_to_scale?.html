<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>721 high scalability-2009-10-13-Why are Facebook, Digg, and Twitter so hard to scale?</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2009" href="../home/high_scalability-2009_home.html">high_scalability-2009</a> <a title="high_scalability-2009-721" href="#">high_scalability-2009-721</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>721 high scalability-2009-10-13-Why are Facebook, Digg, and Twitter so hard to scale?</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2009-721-html" href="http://highscalability.com//blog/2009/10/13/why-are-facebook-digg-and-twitter-so-hard-to-scale.html">html</a></p><p>Introduction: Real-time social graphs (connectivity between people, places, and things). That's why scaling Facebook is hard  says Jeff Rothschild , Vice President of Technology at Facebook. Social networking sites like Facebook, Digg, and Twitter are simply harder than traditional websites to scale. Why is that? Why would social networking sites be any more difficult to scale than traditional web sites? Let's find out.
 
Traditional websites are easier to scale than social networking sites for two reasons:
  
 They usually access only their own data and common cached data. 
 Only 1-2% of users are active on the site at one time. 
  
Imagine a huge site like Yahoo. When you come to Yahoo they can get your profile record with one get and that's enough to build your view of the website for you. It's relatively straightforward to scale systems based around single records using  distributed hashing schemes . And since only a few percent of the people are on the site at once it takes comparatively little</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Social networking sites like Facebook, Digg, and Twitter are simply harder than traditional websites to scale. [sent-3, score-0.387]
</p><p>2 Why would social networking sites be any more difficult to scale than traditional web sites? [sent-5, score-0.579]
</p><p>3 Traditional websites are easier to scale than social networking sites for two reasons:     They usually access only their own data and common cached data. [sent-7, score-0.625]
</p><p>4 And since only a few percent of the people are on the site at once it takes comparatively little RAM cache to handle all the active users. [sent-12, score-0.352]
</p><p>5 When you hit your Facebook account it has to go  gather the status of all 200 of your friends at the same time  so you can see what's new for them. [sent-15, score-0.354]
</p><p>6 That means 200 requests need to go out simultaneously, the replies need to be merged together, other services need to be contacted to get more details, and all this needs to be munged together and sent through PHP and a web server so you see your Facebook page in a reasonable amount of time. [sent-16, score-0.355]
</p><p>7 There are several implications here, especially given that on social networking sites a high percentage of users are on the system at one time (that's the social part, people hang around):     All data is active all the time. [sent-18, score-1.007]
</p><p>8 Everything must be kept in RAM cache so that the data can be accessed as fast as possible. [sent-20, score-0.42]
</p><p>9 Partitioning means you would like to find some way to cluster commonly accessed data together so it can be accessed more efficiently. [sent-21, score-0.63]
</p><p>10 So instead of partitioning and denormalizing data Facebook  keeps data normalized and randomly distributes  data amongst thousands of databases. [sent-23, score-0.524]
</p><p>11 All data is kept in cache and they've made a lot of modifications to memcached to speed it up and to help it handle more requests (all contributed back to the community). [sent-26, score-0.428]
</p><p>12 They've developed a complicated system to keep data in the caching tier consistent with the database, even across multiple distributed data centers. [sent-30, score-0.458]
</p><p>13 Remember, they are caching user data here, not HTML pages or page fragments. [sent-31, score-0.455]
</p><p>14 Given how much their data changes it's would be hard to make page caching work. [sent-32, score-0.538]
</p><p>15 To recreate a page or a display fragment they run the complete query. [sent-37, score-0.268]
</p><p>16 To find out if one of your friends has added a new favorite band Facebook actually queries all your friends to find what's new. [sent-38, score-0.888]
</p><p>17 Another approach to find out what's new is the  Push on Change  model. [sent-42, score-0.263]
</p><p>18 There's no need to poll all their friends for changes. [sent-45, score-0.341]
</p><p>19 A lot of duplicate data (or references) is being stored, so this is a denormalized approach which can make for some consistency problems. [sent-49, score-0.323]
</p><p>20 Should permission be consulted when data is produced or consumed, for example? [sent-50, score-0.302]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('friends', 0.267), ('facebook', 0.26), ('social', 0.192), ('digg', 0.171), ('sites', 0.163), ('networking', 0.15), ('accessed', 0.144), ('find', 0.137), ('approach', 0.126), ('caching', 0.124), ('data', 0.12), ('active', 0.119), ('page', 0.113), ('timeso', 0.107), ('consulted', 0.107), ('duplications', 0.107), ('interconnectedness', 0.1), ('user', 0.098), ('hard', 0.096), ('permissions', 0.096), ('tier', 0.094), ('around', 0.092), ('denormalizing', 0.092), ('diggs', 0.092), ('rothschild', 0.092), ('comparatively', 0.089), ('account', 0.087), ('together', 0.085), ('changes', 0.085), ('contacted', 0.083), ('kept', 0.083), ('band', 0.08), ('fragment', 0.078), ('updates', 0.077), ('brave', 0.077), ('recreate', 0.077), ('denormalized', 0.077), ('rose', 0.077), ('contributed', 0.076), ('modifications', 0.076), ('permission', 0.075), ('poll', 0.074), ('copied', 0.074), ('replies', 0.074), ('traditional', 0.074), ('cache', 0.073), ('desire', 0.073), ('normalized', 0.072), ('people', 0.071), ('change', 0.071)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000006 <a title="721-tfidf-1" href="../high_scalability-2009/high_scalability-2009-10-13-Why_are_Facebook%2C_Digg%2C_and_Twitter_so_hard_to_scale%3F.html">721 high scalability-2009-10-13-Why are Facebook, Digg, and Twitter so hard to scale?</a></p>
<p>Introduction: Real-time social graphs (connectivity between people, places, and things). That's why scaling Facebook is hard  says Jeff Rothschild , Vice President of Technology at Facebook. Social networking sites like Facebook, Digg, and Twitter are simply harder than traditional websites to scale. Why is that? Why would social networking sites be any more difficult to scale than traditional web sites? Let's find out.
 
Traditional websites are easier to scale than social networking sites for two reasons:
  
 They usually access only their own data and common cached data. 
 Only 1-2% of users are active on the site at one time. 
  
Imagine a huge site like Yahoo. When you come to Yahoo they can get your profile record with one get and that's enough to build your view of the website for you. It's relatively straightforward to scale systems based around single records using  distributed hashing schemes . And since only a few percent of the people are on the site at once it takes comparatively little</p><p>2 0.25095019 <a title="721-tfidf-2" href="../high_scalability-2009/high_scalability-2009-10-12-High_Performance_at_Massive_Scale_%E2%80%93__Lessons_learned_at_Facebook.html">720 high scalability-2009-10-12-High Performance at Massive Scale –  Lessons learned at Facebook</a></p>
<p>Introduction: Jeff Rothschild, Vice President of Technology at Facebook gave a great presentation at UC San Diego on our favorite subject: " High Performance at Massive Scale –  Lessons learned at Facebook ". The abstract for the talk is:
  

Facebook has grown into one of the largest sites on the Internet today serving over 200 billion pages per month. The nature of social data makes engineering a site for this level of scale a particularly challenging proposition. In this presentation, I will discuss the aspects of social data that present challenges for scalability and will describe the the core architectural components and design principles that Facebook has used to address these challenges. In addition, I will discuss emerging technologies that offer new opportunities for building cost-effective high performance web architectures.

  
There's a lot of interesting about this talk that we'll get into  later, but I thought you might want a head start on learning how Facebook handles 30K+ machines,</p><p>3 0.23107356 <a title="721-tfidf-3" href="../high_scalability-2009/high_scalability-2009-02-14-Scaling_Digg_and_Other_Web_Applications.html">512 high scalability-2009-02-14-Scaling Digg and Other Web Applications</a></p>
<p>Introduction: Joe Stump, Lead Architect at Digg, gave  this presentation  at the Web 2.0 Expo. I couldn't find the actual presentation, but fortunately  Kris Jordan  took some great notes. That's how key moments in history are accidentally captured forever. Joe was also kind enough to respond to my email questions with a phone call.   In this first part of the post Joe shares some timeless wisdom that you may or may not have read before. I of course take some pains to extract all the wit from the original presentation in favor of simple rules. What really struck me however was how Joe thought MemcacheDB  Will be the biggest new kid on the block in scaling .  MemcacheDB has been around for a little while and I've never thought of it in that way. Well learn why Joe is so excited by MemcacheDB at the end of the post.
  Impressive Stats   80th-100th largest site in the world   26 million uniques a month   30 million users.   Uniques are only half that traffic. Traffic = unique web visitors + APIs + Digg</p><p>4 0.22629716 <a title="721-tfidf-4" href="../high_scalability-2011/high_scalability-2011-09-23-The_Real_News_is_Not_that_Facebook_Serves_Up_1_Trillion_Pages_a_Month%E2%80%A6.html">1123 high scalability-2011-09-23-The Real News is Not that Facebook Serves Up 1 Trillion Pages a Month…</a></p>
<p>Introduction: There’s some amount of debate whether Facebook  really  crossed over the one trillion page view per month threshold. While one report says it did,  another respected firm says it did not ; that its monthly page views are a mere 467 billion per month.


In the big scheme of things, the discrepancy is somewhat irrelevant, as neither show the  true  load on Facebook’s infrastructure – which is far more impressive a set of numbers than its externally measured “page view” metric.  Mashable reported in “ Facebook Surpasses 1 Trillion Pageviews per Month ” that the social networking giant saw “approximately 870 million unique visitors in June and 860 million in July” and followed up with some per visitor statistics, indicating “each visitor averaged approximately 1,160 page views in July and 40 per visit — enormous by any standard. Time spent on the site was around 25 minutes per user.”


From an architectural standpoint it’s not  just  about the page views. It’s about requests and responses,</p><p>5 0.20572007 <a title="721-tfidf-5" href="../high_scalability-2011/high_scalability-2011-03-22-Facebook%27s_New_Realtime_Analytics_System%3A_HBase_to_Process_20_Billion_Events_Per_Day.html">1008 high scalability-2011-03-22-Facebook's New Realtime Analytics System: HBase to Process 20 Billion Events Per Day</a></p>
<p>Introduction: Facebook did it again. They've built another system capable of doing something useful with ginormous streams of realtime data. Last time we saw Facebook release their  New Real-Time Messaging System: HBase To Store 135+ Billion Messages A Month . This time it's a realtime analytics system handling  over 20 billion events per day (200,000 events per second) with a lag of less than 30 seconds . 
 
Alex Himel, Engineering Manager at Facebook,  explains what they've built  ( video ) and the scale required:
  

Social plugins have become an important and growing source of traffic for millions of websites over the past year. We released a new version of Insights for Websites last week to give site owners better analytics on how people interact with their content and to help them optimize their websites in real time. To accomplish this, we had to engineer a system that could process over 20 billion events per day (200,000 events per second) with a lag of less than 30 seconds. 

  
Alex does a</p><p>6 0.20513494 <a title="721-tfidf-6" href="../high_scalability-2009/high_scalability-2009-06-27-Scaling_Twitter%3A_Making_Twitter_10000_Percent_Faster.html">639 high scalability-2009-06-27-Scaling Twitter: Making Twitter 10000 Percent Faster</a></p>
<p>7 0.20082484 <a title="721-tfidf-7" href="../high_scalability-2008/high_scalability-2008-05-02-Friends_for_Sale_Architecture_-_A_300_Million_Page_View-Month_Facebook_RoR_App.html">313 high scalability-2008-05-02-Friends for Sale Architecture - A 300 Million Page View-Month Facebook RoR App</a></p>
<p>8 0.1982508 <a title="721-tfidf-8" href="../high_scalability-2009/high_scalability-2009-04-04-Digg_Architecture.html">554 high scalability-2009-04-04-Digg Architecture</a></p>
<p>9 0.1949144 <a title="721-tfidf-9" href="../high_scalability-2010/high_scalability-2010-01-11-Have_We_Reached_the_End_of_Scaling%3F.html">758 high scalability-2010-01-11-Have We Reached the End of Scaling?</a></p>
<p>10 0.19316085 <a title="721-tfidf-10" href="../high_scalability-2009/high_scalability-2009-10-26-Facebook%27s_Memcached_Multiget_Hole%3A_More_machines_%21%3D_More_Capacity_.html">728 high scalability-2009-10-26-Facebook's Memcached Multiget Hole: More machines != More Capacity </a></p>
<p>11 0.18964301 <a title="721-tfidf-11" href="../high_scalability-2009/high_scalability-2009-08-06-An_Unorthodox_Approach_to_Database_Design_%3A_The_Coming_of_the_Shard.html">672 high scalability-2009-08-06-An Unorthodox Approach to Database Design : The Coming of the Shard</a></p>
<p>12 0.18793701 <a title="721-tfidf-12" href="../high_scalability-2012/high_scalability-2012-11-07-Gone_Fishin%27%3A_10_Ways_to_Take_your_Site_from_One_to_One_Million_Users_by_Kevin_Rose__.html">1356 high scalability-2012-11-07-Gone Fishin': 10 Ways to Take your Site from One to One Million Users by Kevin Rose  </a></p>
<p>13 0.18640317 <a title="721-tfidf-13" href="../high_scalability-2010/high_scalability-2010-06-10-The_Four_Meta_Secrets_of_Scaling_at_Facebook.html">840 high scalability-2010-06-10-The Four Meta Secrets of Scaling at Facebook</a></p>
<p>14 0.18503782 <a title="721-tfidf-14" href="../high_scalability-2007/high_scalability-2007-12-12-Report_from_OpenSocial_Meetup_at_Google.html">183 high scalability-2007-12-12-Report from OpenSocial Meetup at Google</a></p>
<p>15 0.18123859 <a title="721-tfidf-15" href="../high_scalability-2007/high_scalability-2007-07-10-mixi.jp__Architecture.html">5 high scalability-2007-07-10-mixi.jp  Architecture</a></p>
<p>16 0.18122 <a title="721-tfidf-16" href="../high_scalability-2009/high_scalability-2009-10-06-10_Ways_to_Take_your_Site_from_One_to_One_Million_Users_by_Kevin_Rose__.html">715 high scalability-2009-10-06-10 Ways to Take your Site from One to One Million Users by Kevin Rose  </a></p>
<p>17 0.17773306 <a title="721-tfidf-17" href="../high_scalability-2010/high_scalability-2010-08-02-7_Scaling_Strategies_Facebook_Used_to_Grow_to_500_Million_Users.html">870 high scalability-2010-08-02-7 Scaling Strategies Facebook Used to Grow to 500 Million Users</a></p>
<p>18 0.17051609 <a title="721-tfidf-18" href="../high_scalability-2008/high_scalability-2008-08-04-A_Bunch_of_Great_Strategies_for_Using_Memcached_and_MySQL_Better_Together.html">360 high scalability-2008-08-04-A Bunch of Great Strategies for Using Memcached and MySQL Better Together</a></p>
<p>19 0.16533221 <a title="721-tfidf-19" href="../high_scalability-2012/high_scalability-2012-11-05-Gone_Fishin%27%3A_Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_In_The_Ambient_Cloud.html">1355 high scalability-2012-11-05-Gone Fishin': Building Super Scalable Systems: Blade Runner Meets Autonomic Computing In The Ambient Cloud</a></p>
<p>20 0.16513178 <a title="721-tfidf-20" href="../high_scalability-2009/high_scalability-2009-12-16-Building_Super_Scalable_Systems%3A_Blade_Runner_Meets_Autonomic_Computing_in_the_Ambient_Cloud.html">750 high scalability-2009-12-16-Building Super Scalable Systems: Blade Runner Meets Autonomic Computing in the Ambient Cloud</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.29), (1, 0.167), (2, 0.006), (3, -0.137), (4, 0.137), (5, -0.041), (6, -0.116), (7, 0.056), (8, 0.034), (9, 0.05), (10, 0.062), (11, 0.153), (12, -0.029), (13, 0.09), (14, -0.037), (15, 0.02), (16, -0.013), (17, 0.003), (18, 0.083), (19, 0.095), (20, 0.0), (21, 0.161), (22, 0.173), (23, 0.069), (24, -0.015), (25, -0.062), (26, 0.016), (27, 0.001), (28, 0.02), (29, -0.127), (30, 0.016), (31, -0.051), (32, -0.003), (33, 0.035), (34, -0.01), (35, -0.016), (36, 0.049), (37, 0.032), (38, -0.027), (39, 0.009), (40, -0.001), (41, -0.028), (42, -0.057), (43, -0.039), (44, -0.031), (45, -0.042), (46, -0.032), (47, -0.057), (48, 0.015), (49, -0.022)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95971382 <a title="721-lsi-1" href="../high_scalability-2009/high_scalability-2009-10-13-Why_are_Facebook%2C_Digg%2C_and_Twitter_so_hard_to_scale%3F.html">721 high scalability-2009-10-13-Why are Facebook, Digg, and Twitter so hard to scale?</a></p>
<p>Introduction: Real-time social graphs (connectivity between people, places, and things). That's why scaling Facebook is hard  says Jeff Rothschild , Vice President of Technology at Facebook. Social networking sites like Facebook, Digg, and Twitter are simply harder than traditional websites to scale. Why is that? Why would social networking sites be any more difficult to scale than traditional web sites? Let's find out.
 
Traditional websites are easier to scale than social networking sites for two reasons:
  
 They usually access only their own data and common cached data. 
 Only 1-2% of users are active on the site at one time. 
  
Imagine a huge site like Yahoo. When you come to Yahoo they can get your profile record with one get and that's enough to build your view of the website for you. It's relatively straightforward to scale systems based around single records using  distributed hashing schemes . And since only a few percent of the people are on the site at once it takes comparatively little</p><p>2 0.76777112 <a title="721-lsi-2" href="../high_scalability-2009/high_scalability-2009-02-14-Scaling_Digg_and_Other_Web_Applications.html">512 high scalability-2009-02-14-Scaling Digg and Other Web Applications</a></p>
<p>Introduction: Joe Stump, Lead Architect at Digg, gave  this presentation  at the Web 2.0 Expo. I couldn't find the actual presentation, but fortunately  Kris Jordan  took some great notes. That's how key moments in history are accidentally captured forever. Joe was also kind enough to respond to my email questions with a phone call.   In this first part of the post Joe shares some timeless wisdom that you may or may not have read before. I of course take some pains to extract all the wit from the original presentation in favor of simple rules. What really struck me however was how Joe thought MemcacheDB  Will be the biggest new kid on the block in scaling .  MemcacheDB has been around for a little while and I've never thought of it in that way. Well learn why Joe is so excited by MemcacheDB at the end of the post.
  Impressive Stats   80th-100th largest site in the world   26 million uniques a month   30 million users.   Uniques are only half that traffic. Traffic = unique web visitors + APIs + Digg</p><p>3 0.75855112 <a title="721-lsi-3" href="../high_scalability-2010/high_scalability-2010-01-11-Have_We_Reached_the_End_of_Scaling%3F.html">758 high scalability-2010-01-11-Have We Reached the End of Scaling?</a></p>
<p>Introduction: This is an excerpt from my article  Building Super Scalable Systems: Blade Runner Meets Autonomic Computing in the Ambient Cloud.  
 
Have we reached the end of scaling? That's what I asked myself one day after noticing a bunch of "The End of" headlines. We've reached  The End of History  because the Western liberal democracy is the "end point of humanity's sociocultural evolution and the final form of human government."  We've reached  The End of Science  because of the "fact that there aren't going to be any obvious, cataclysmic revolutions." We've even reached  The End of Theory  because all answers can be found in the continuous stream of data we're collecting. And doesn't always seem like we're at  The End of the World ?
 
Motivated by the prospect of everything ending, I began to wonder: have we really reached The End of Scaling?
 
For a while I thought this might be true. The reason I thought the End of Scaling might be near is because of the slow down of potential articles at m</p><p>4 0.74620491 <a title="721-lsi-4" href="../high_scalability-2010/high_scalability-2010-06-10-The_Four_Meta_Secrets_of_Scaling_at_Facebook.html">840 high scalability-2010-06-10-The Four Meta Secrets of Scaling at Facebook</a></p>
<p>Introduction: Aditya Agarwal, Director of Engineering at Facebook, gave an excellent  Scale at Facebook  talk that covers their architecture, but the talk is really more about how to scale an organization by preserving the best parts of its culture. The key take home of the talk is: 
  

You can get the code right, you can get the products right, but you need to get the culture right first. If you don't get the culture right then your company won't scale.

  
This leads into the four meta secrets of scaling at Facebook:
  
 Scaling takes Iteration 
 Don't Over Design 
 Choose the right tool for the job, but realize that your choice comes with overhead. 
 Get the culture right. Move Fast - break things. Huge Impact - small teams. Be bold - innovate. 
      Some Background    
  Facebook is big : 400 million active users; users spend an average of 20 minutes a day; 5 billion pieces of content (status updates, comments, likes, photo uploads, video uploads, chat messages, inbox messages, group events, f</p><p>5 0.74501991 <a title="721-lsi-5" href="../high_scalability-2010/high_scalability-2010-08-02-7_Scaling_Strategies_Facebook_Used_to_Grow_to_500_Million_Users.html">870 high scalability-2010-08-02-7 Scaling Strategies Facebook Used to Grow to 500 Million Users</a></p>
<p>Introduction: Robert Johnson,   a director of engineering at Facebook, celebrated Facebook's monumental achievement of reaching 500 million users by sharing the  scaling principles that helped  reach that milestone. In case you weren't suitably impressed by the 500 million user number, Robert ratchets up the numbers game with these impressive figures:      
 
 1 million users per engineer 
 500 million active users 
 100 billion hits per day 
 50 billion photos 
 2 trillion objects cached, with hundreds of millions of requests per second 
 130TB of logs every day 
 
  
How did Facebook get to this point?
   
  People Matter Most . It's people who build and run systems. The best tools for scaling are an engineering and operations teams that can handle anything. 
  Scale Horizontally . Handling exponentially growing traffic requires spreading load arbitrarily across many machines. Using different databases for tables like accounts and profiles only doubles capacity. This approach hurts efficiency, but</p><p>6 0.73361635 <a title="721-lsi-6" href="../high_scalability-2010/high_scalability-2010-02-08-How_FarmVille_Scales_to_Harvest_75_Million_Players_a_Month.html">774 high scalability-2010-02-08-How FarmVille Scales to Harvest 75 Million Players a Month</a></p>
<p>7 0.72085869 <a title="721-lsi-7" href="../high_scalability-2011/high_scalability-2011-03-22-Facebook%27s_New_Realtime_Analytics_System%3A_HBase_to_Process_20_Billion_Events_Per_Day.html">1008 high scalability-2011-03-22-Facebook's New Realtime Analytics System: HBase to Process 20 Billion Events Per Day</a></p>
<p>8 0.70580357 <a title="721-lsi-8" href="../high_scalability-2009/high_scalability-2009-10-12-High_Performance_at_Massive_Scale_%E2%80%93__Lessons_learned_at_Facebook.html">720 high scalability-2009-10-12-High Performance at Massive Scale –  Lessons learned at Facebook</a></p>
<p>9 0.70460325 <a title="721-lsi-9" href="../high_scalability-2014/high_scalability-2014-03-26-Oculus_Causes_a_Rift%2C_but_the_Facebook_Deal_Will_Avoid_a_Scaling_Crisis_for_Virtual_Reality.html">1619 high scalability-2014-03-26-Oculus Causes a Rift, but the Facebook Deal Will Avoid a Scaling Crisis for Virtual Reality</a></p>
<p>10 0.70137447 <a title="721-lsi-10" href="../high_scalability-2011/high_scalability-2011-01-20-75%25_Chance_of_Scale_-_Leveraging_the_New_Scaleogenic_Environment_for_Growth.html">976 high scalability-2011-01-20-75% Chance of Scale - Leveraging the New Scaleogenic Environment for Growth</a></p>
<p>11 0.69850028 <a title="721-lsi-11" href="../high_scalability-2007/high_scalability-2007-10-08-Lessons_from_Pownce_-_The_Early_Years.html">116 high scalability-2007-10-08-Lessons from Pownce - The Early Years</a></p>
<p>12 0.69790566 <a title="721-lsi-12" href="../high_scalability-2007/high_scalability-2007-12-12-Report_from_OpenSocial_Meetup_at_Google.html">183 high scalability-2007-12-12-Report from OpenSocial Meetup at Google</a></p>
<p>13 0.697743 <a title="721-lsi-13" href="../high_scalability-2009/high_scalability-2009-10-06-10_Ways_to_Take_your_Site_from_One_to_One_Million_Users_by_Kevin_Rose__.html">715 high scalability-2009-10-06-10 Ways to Take your Site from One to One Million Users by Kevin Rose  </a></p>
<p>14 0.69741577 <a title="721-lsi-14" href="../high_scalability-2012/high_scalability-2012-11-07-Gone_Fishin%27%3A_10_Ways_to_Take_your_Site_from_One_to_One_Million_Users_by_Kevin_Rose__.html">1356 high scalability-2012-11-07-Gone Fishin': 10 Ways to Take your Site from One to One Million Users by Kevin Rose  </a></p>
<p>15 0.6958952 <a title="721-lsi-15" href="../high_scalability-2007/high_scalability-2007-11-13-Friendster_Lost_Lead_Because_of_a_Failure_to_Scale.html">153 high scalability-2007-11-13-Friendster Lost Lead Because of a Failure to Scale</a></p>
<p>16 0.69302112 <a title="721-lsi-16" href="../high_scalability-2012/high_scalability-2012-01-17-Paper%3A_Feeding_Frenzy%3A_Selectively_Materializing_Users%E2%80%99_Event_Feeds.html">1175 high scalability-2012-01-17-Paper: Feeding Frenzy: Selectively Materializing Users’ Event Feeds</a></p>
<p>17 0.68999463 <a title="721-lsi-17" href="../high_scalability-2008/high_scalability-2008-05-31-memcached_and_Storage_of_Friend_list.html">337 high scalability-2008-05-31-memcached and Storage of Friend list</a></p>
<p>18 0.68676388 <a title="721-lsi-18" href="../high_scalability-2012/high_scalability-2012-01-23-Facebook_Timeline%3A_Brought_to_You_by_the_Power_of_Denormalization.html">1179 high scalability-2012-01-23-Facebook Timeline: Brought to You by the Power of Denormalization</a></p>
<p>19 0.68322366 <a title="721-lsi-19" href="../high_scalability-2010/high_scalability-2010-03-10-How_FarmVille_Scales_-_The_Follow-up.html">792 high scalability-2010-03-10-How FarmVille Scales - The Follow-up</a></p>
<p>20 0.68050385 <a title="721-lsi-20" href="../high_scalability-2007/high_scalability-2007-10-23-Hire_Facebook%2C_Ning%2C_and_Salesforce_to_Scale_for_You.html">129 high scalability-2007-10-23-Hire Facebook, Ning, and Salesforce to Scale for You</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.088), (2, 0.354), (10, 0.022), (30, 0.223), (40, 0.013), (61, 0.042), (79, 0.12), (94, 0.048), (96, 0.012)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.97557402 <a title="721-lda-1" href="../high_scalability-2008/high_scalability-2008-12-13-Strategy%3A_Facebook_Tweaks_to_Handle_6_Time_as_Many_Memcached_Requests.html">464 high scalability-2008-12-13-Strategy: Facebook Tweaks to Handle 6 Time as Many Memcached Requests</a></p>
<p>Introduction: Our latest strategy is taken from a  great post by Paul Saab of Facebook , detailing how with changes Facebook has made to memcached they have:
  ...been able to scale memcached to handle 200,000 UDP requests per second with an average latency of 173 microseconds. The total throughput achieved is 300,000 UDP requests/s, but the latency at that request rate is too high to be useful in our system. This is an amazing increase from 50,000 UDP requests/s using the stock version of Linux and memcached.  
To scale Facebook has hundreds of thousands of TCP connections open to their memcached processes. First, this is still amazing. It's not so long ago you could have never done this. Optimizing connection use was always a priority because the OS simply couldn't handle large numbers of connections or large numbers of threads or large numbers of CPUs. To get to this point is a big accomplishment. Still, at that scale there are problems that are often solved.  Some of the problem Facebook faced a</p><p>2 0.95960295 <a title="721-lda-2" href="../high_scalability-2008/high_scalability-2008-02-25-Make_Your_Site_Run_10_Times_Faster.html">261 high scalability-2008-02-25-Make Your Site Run 10 Times Faster</a></p>
<p>Introduction: This is what  Mike Peters says he can do : make your site run 10 times faster. His test bed is "half a dozen servers parsing 200,000 pages per hour over 40 IP addresses, 24 hours a day." Before optimization CPU spiked to 90% with 50 concurrent connections. After optimization each machine "was effectively handling 500 concurrent connections per second with CPU at 8% and no degradation in performance."  Mike identifies six major bottlenecks:
   Database write access (read is cheaper)    Database read access    PHP, ASP, JSP and any other server side scripting    Client side JavaScript    Multiple/Fat Images, scripts or css files from different domains on your page    Slow keep-alive client connections, clogging your available sockets   Mike's solutions:      Switch all database writes to offline processing    Minimize number of database read access to the bare minimum. No more than two queries per page.    Denormalize your database and Optimize MySQL tables    Implement MemCached and cha</p><p>3 0.95678324 <a title="721-lda-3" href="../high_scalability-2010/high_scalability-2010-10-08-4_Scalability_Themes_from_Surgecon.html">917 high scalability-2010-10-08-4 Scalability Themes from Surgecon</a></p>
<p>Introduction: Robert Haas in his  SURGE Recap  of the  Surge  conference, reflected a bit, and came up with an interesting checklist of general themes from what he was seeing. I'm directly quoting his post, so please see the post for a full discussion. He uses this framework to think about the larger picture and where PostgreSQL stands in its progression.
  
  Make use of the academic literature .  Inventing your own way to do something is fine, but at least consider the possibility that someone smarter than you has thought about this problem before.  
  Failures are inevitable, so plan for them .   Try to minimize the possibility of cascading failures, and plan in advance how you can operate in degraded mode if disaster (or the Slashdot effect) strikes.  
  Disk technology matters .  Drive firmware bugs are common and nightmarish, and you can expect very limited help from the manufacturer, especially if the drive is billed as consumer-grade rather than enterprise-grade. SSDs can save you a lot of m</p><p>4 0.95423615 <a title="721-lda-4" href="../high_scalability-2010/high_scalability-2010-02-24-Hot_Scalability_Links_for_February_24%2C_2010.html">783 high scalability-2010-02-24-Hot Scalability Links for February 24, 2010</a></p>
<p>Introduction: Cassandra @ Twitter: An Interview with Ryan King . Great interview by Alex Popescu on Twitter's thought process for switching to Cassandra. Twitter chose Cassandra because it had more big system features out of the box. Is that Cassandra FTW?  
   I Had Downtime Today. Here’s What I’m Doing About It  by Patrick McKenzie. Awesome deep dive into went wrong with Bingo Card Creator. Sh*t happens. How do you design a process to help prevent it from happening and how do you deal with problems with integrity when they do?  
   High Availability Principle : Request Queueing  by Ashish Soni. Queue request to ride out traffic spikes: 1) Request Queuing allows your system to operate at optimal throughput. 2) Your users only experience linear degradation versus exponential degradation. 3) Your system experiences NO degradation.  
   pfffft twatter tweeter  by Knowbuddy.  The reason you should care [about NoSQL] is because now you have more options--you're not stuck trying to wedge your system into</p><p>same-blog 5 0.94857603 <a title="721-lda-5" href="../high_scalability-2009/high_scalability-2009-10-13-Why_are_Facebook%2C_Digg%2C_and_Twitter_so_hard_to_scale%3F.html">721 high scalability-2009-10-13-Why are Facebook, Digg, and Twitter so hard to scale?</a></p>
<p>Introduction: Real-time social graphs (connectivity between people, places, and things). That's why scaling Facebook is hard  says Jeff Rothschild , Vice President of Technology at Facebook. Social networking sites like Facebook, Digg, and Twitter are simply harder than traditional websites to scale. Why is that? Why would social networking sites be any more difficult to scale than traditional web sites? Let's find out.
 
Traditional websites are easier to scale than social networking sites for two reasons:
  
 They usually access only their own data and common cached data. 
 Only 1-2% of users are active on the site at one time. 
  
Imagine a huge site like Yahoo. When you come to Yahoo they can get your profile record with one get and that's enough to build your view of the website for you. It's relatively straightforward to scale systems based around single records using  distributed hashing schemes . And since only a few percent of the people are on the site at once it takes comparatively little</p><p>6 0.94695681 <a title="721-lda-6" href="../high_scalability-2008/high_scalability-2008-05-31-Biggest_Under_Reported_Story%3A_Google%27s_BigTable_Costs_10_Times_Less_than_Amazon%27s_SimpleDB.html">336 high scalability-2008-05-31-Biggest Under Reported Story: Google's BigTable Costs 10 Times Less than Amazon's SimpleDB</a></p>
<p>7 0.94529378 <a title="721-lda-7" href="../high_scalability-2008/high_scalability-2008-02-27-Product%3A_System_Imager_-_Automate_Deployment_and_Installs.html">263 high scalability-2008-02-27-Product: System Imager - Automate Deployment and Installs</a></p>
<p>8 0.9358843 <a title="721-lda-8" href="../high_scalability-2010/high_scalability-2010-05-26-End-To-End_Performance_Study_of_Cloud_Services.html">831 high scalability-2010-05-26-End-To-End Performance Study of Cloud Services</a></p>
<p>9 0.91681296 <a title="721-lda-9" href="../high_scalability-2008/high_scalability-2008-04-30-Rather_small_site_architecture..html">312 high scalability-2008-04-30-Rather small site architecture.</a></p>
<p>10 0.91250694 <a title="721-lda-10" href="../high_scalability-2008/high_scalability-2008-02-21-Tracking_usage_of_public_resources_-_throttling_accesses_per_hour.html">256 high scalability-2008-02-21-Tracking usage of public resources - throttling accesses per hour</a></p>
<p>11 0.91204095 <a title="721-lda-11" href="../high_scalability-2009/high_scalability-2009-06-06-Graph_server.html">621 high scalability-2009-06-06-Graph server</a></p>
<p>12 0.90889543 <a title="721-lda-12" href="../high_scalability-2009/high_scalability-2009-01-22-Heterogeneous_vs._Homogeneous_System_Architectures.html">500 high scalability-2009-01-22-Heterogeneous vs. Homogeneous System Architectures</a></p>
<p>13 0.90889394 <a title="721-lda-13" href="../high_scalability-2008/high_scalability-2008-02-18-limit_on_the_number_of_databases_open.html">252 high scalability-2008-02-18-limit on the number of databases open</a></p>
<p>14 0.90193844 <a title="721-lda-14" href="../high_scalability-2008/high_scalability-2008-06-08-Search_fast_in_million_rows.html">342 high scalability-2008-06-08-Search fast in million rows</a></p>
<p>15 0.90065235 <a title="721-lda-15" href="../high_scalability-2007/high_scalability-2007-07-10-mixi.jp__Architecture.html">5 high scalability-2007-07-10-mixi.jp  Architecture</a></p>
<p>16 0.89857173 <a title="721-lda-16" href="../high_scalability-2013/high_scalability-2013-01-30-Better_Browser_Caching_is_More_Important_than_No_Javascript_or_Fast_Networks_for_HTTP_Performance.html">1396 high scalability-2013-01-30-Better Browser Caching is More Important than No Javascript or Fast Networks for HTTP Performance</a></p>
<p>17 0.89670181 <a title="721-lda-17" href="../high_scalability-2013/high_scalability-2013-01-15-More_Numbers_Every_Awesome_Programmer_Must_Know.html">1387 high scalability-2013-01-15-More Numbers Every Awesome Programmer Must Know</a></p>
<p>18 0.89434671 <a title="721-lda-18" href="../high_scalability-2013/high_scalability-2013-05-16-Paper%3A_Warp%3A_Multi-Key_Transactions_for_Key-Value_Stores.html">1459 high scalability-2013-05-16-Paper: Warp: Multi-Key Transactions for Key-Value Stores</a></p>
<p>19 0.89199406 <a title="721-lda-19" href="../high_scalability-2008/high_scalability-2008-12-01-Deploying_MySQL_Database_in_Solaris_Cluster_Environments.html">454 high scalability-2008-12-01-Deploying MySQL Database in Solaris Cluster Environments</a></p>
<p>20 0.89158362 <a title="721-lda-20" href="../high_scalability-2013/high_scalability-2013-06-19-Paper%3A_MegaPipe%3A_A_New_Programming_Interface_for_Scalable_Network_I-O.html">1478 high scalability-2013-06-19-Paper: MegaPipe: A New Programming Interface for Scalable Network I-O</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
