<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>391 high scalability-2008-09-23-The 7 Stages of Scaling Web Apps</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2008" href="../home/high_scalability-2008_home.html">high_scalability-2008</a> <a title="high_scalability-2008-391" href="#">high_scalability-2008-391</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>391 high scalability-2008-09-23-The 7 Stages of Scaling Web Apps</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2008-391-html" href="http://highscalability.com//blog/2008/9/23/the-7-stages-of-scaling-web-apps.html">html</a></p><p>Introduction: By John Engales CTO, Rackspace. Good presentation of the stages a typical successful website goes through: 
 
 Stage 1 - The Beginning: Simple architecture, low complexity. no redundancy. Firewall, load balancer, a pair of web servers, database server, and internal storage. 
 Stage 2 - More of the same, just bigger. 
 Stage 3 - The Pain Begins: publicity hits. Use reverse proxy, cache static content, load balancers, more databases, re-coding. 
 Stage 4 - The Pain Intensifies: caching with memcached, writes overload and replication takes too long, start database partitioning, shared storage makes sense for content, significant re-architecting for DB. 
 Stage 5 - This Really Hurts!: rethink entire application, partition on geography user ID, etc, create user clusters, using hashing scheme for locating which user belongs to which cluster. 
 Stage 6 - Getting a little less painful: scalable application and database architecture, acceptable performance, starting to add ne features again, op</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Good presentation of the stages a typical successful website goes through:     Stage 1 - The Beginning: Simple architecture, low complexity. [sent-2, score-0.373]
</p><p>2 Firewall, load balancer, a pair of web servers, database server, and internal storage. [sent-4, score-0.422]
</p><p>3 Use reverse proxy, cache static content, load balancers, more databases, re-coding. [sent-7, score-0.292]
</p><p>4 Stage 4 - The Pain Intensifies: caching with memcached, writes overload and replication takes too long, start database partitioning, shared storage makes sense for content, significant re-architecting for DB. [sent-8, score-0.355]
</p><p>5 : rethink entire application, partition on geography user ID, etc, create user clusters, using hashing scheme for locating which user belongs to which cluster. [sent-10, score-1.378]
</p><p>6 Stage 6 - Getting a little less painful: scalable application and database architecture, acceptable performance, starting to add ne features again, optimizing some code, still growing but manageable. [sent-11, score-0.612]
</p><p>7 Stage 7 - Entering the unknown: where are the remaining bottlenecks (power, space, bandwidth, CDN, firewall, load balancer, storage, people, process, database), all eggs in one basked (single datacenter, single instance of data). [sent-12, score-0.504]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('firewall', 0.235), ('pain', 0.229), ('publicity', 0.216), ('balancer', 0.205), ('locating', 0.203), ('eggs', 0.194), ('ne', 0.194), ('belongs', 0.187), ('hurts', 0.187), ('geography', 0.168), ('overload', 0.159), ('begins', 0.147), ('painful', 0.144), ('rethink', 0.144), ('unknown', 0.142), ('entering', 0.136), ('pair', 0.13), ('stages', 0.129), ('remaining', 0.128), ('acceptable', 0.127), ('stage', 0.121), ('hashing', 0.12), ('user', 0.119), ('content', 0.114), ('beginning', 0.113), ('balancers', 0.113), ('reverse', 0.108), ('scheme', 0.108), ('cto', 0.106), ('database', 0.106), ('load', 0.105), ('proxy', 0.105), ('optimizing', 0.103), ('john', 0.103), ('partition', 0.091), ('significant', 0.09), ('presentation', 0.089), ('partitioning', 0.086), ('cdn', 0.084), ('starting', 0.082), ('datacenter', 0.082), ('clusters', 0.081), ('internal', 0.081), ('id', 0.08), ('successful', 0.08), ('static', 0.079), ('bottlenecks', 0.077), ('etc', 0.076), ('typical', 0.075), ('architecture', 0.074)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="391-tfidf-1" href="../high_scalability-2008/high_scalability-2008-09-23-The_7_Stages_of_Scaling_Web_Apps.html">391 high scalability-2008-09-23-The 7 Stages of Scaling Web Apps</a></p>
<p>Introduction: By John Engales CTO, Rackspace. Good presentation of the stages a typical successful website goes through: 
 
 Stage 1 - The Beginning: Simple architecture, low complexity. no redundancy. Firewall, load balancer, a pair of web servers, database server, and internal storage. 
 Stage 2 - More of the same, just bigger. 
 Stage 3 - The Pain Begins: publicity hits. Use reverse proxy, cache static content, load balancers, more databases, re-coding. 
 Stage 4 - The Pain Intensifies: caching with memcached, writes overload and replication takes too long, start database partitioning, shared storage makes sense for content, significant re-architecting for DB. 
 Stage 5 - This Really Hurts!: rethink entire application, partition on geography user ID, etc, create user clusters, using hashing scheme for locating which user belongs to which cluster. 
 Stage 6 - Getting a little less painful: scalable application and database architecture, acceptable performance, starting to add ne features again, op</p><p>2 0.12364571 <a title="391-tfidf-2" href="../high_scalability-2011/high_scalability-2011-12-12-Netflix%3A_Developing%2C_Deploying%2C_and_Supporting_Software_According_to_the_Way_of_the_Cloud.html">1155 high scalability-2011-12-12-Netflix: Developing, Deploying, and Supporting Software According to the Way of the Cloud</a></p>
<p>Introduction: At a  Cloud Computing Meetup , Siddharth "Sid" Anand of Netflix, backed by a merry band of Netflixians, gave an interesting talk:  Keeping Movies Running Amid Thunderstorms . While the talk gave a good overview of their move to the cloud, issues with capacity planning,  thundering herds , latency problems, and  simian armageddon , I found myself most taken with how they handle  software deployment in the cloud .
 
I've worked on half a dozen or more build and deployment systems, some small, some quite large, but never for a large organization like Netflix in the cloud. The cloud has this amazing capability that has never existed before that enables a novel approach to fault-tolerant software deployments:  the ability to spin up huge numbers of instances to completely run a new release while running the old release at the same time .
 
The process goes something like: 
  
 A  canary machine  is launched first with the new software load running real traffic to sanity test the load in a p</p><p>3 0.121306 <a title="391-tfidf-3" href="../high_scalability-2008/high_scalability-2008-04-05-Skype_Plans_for_PostgreSQL_to_Scale_to_1_Billion_Users.html">297 high scalability-2008-04-05-Skype Plans for PostgreSQL to Scale to 1 Billion Users</a></p>
<p>Introduction: Skype  uses PostgreSQL as their backend database . PostgreSQL doesn't get enough run in the database world so I was excited to see how PostgreSQL is used "as the main DB for most of [Skype's] business needs." Their approach is to use a traditional stored procedure interface for accessing data and on top of that layer proxy servers which hash SQL requests to a set of database servers that actually carry out queries. The result is a horizontally partitioned system that they think will scale to handle 1 billion users.
   Skype's goal is an architecture that can handle 1 billion plus users. This level of scale isn't practically solvable with one really big computer,  so our masked superhero horizontal scaling comes to the rescue.    Hardware is dual or quad Opterons with SCSI RAID.    Followed common database progression: Start with one DB. Add new databases partitioned by functionality. Replicate read-mostly data for better read access. Then horizontally partition data across multiple nod</p><p>4 0.1144317 <a title="391-tfidf-4" href="../high_scalability-2008/high_scalability-2008-08-04-A_Bunch_of_Great_Strategies_for_Using_Memcached_and_MySQL_Better_Together.html">360 high scalability-2008-08-04-A Bunch of Great Strategies for Using Memcached and MySQL Better Together</a></p>
<p>Introduction: The primero recommendation for speeding up a website is almost always to add cache and more cache. And after that add a little more cache just in case. Memcached is almost always given as the recommended cache to use. What we don't often hear is how to effectively use a cache in our own products. MySQL hosted two excellent webinars (referenced below) on the subject of how to deploy and use memcached. The star of the show, other than MySQL of course, is Farhan Mashraqi of Fotolog. You may recall we did an earlier article on Fotolog in  Secrets to Fotolog's Scaling Success , which was one of my personal favorites.  Fotolog, as they themselves point out, is probably the largest site nobody has ever heard of, pulling in more page views than even Flickr. Fotolog has 51 instances of memcached on 21 servers with 175G in use and 254G available. As a large successful photo-blogging site they have very demanding performance and scaling requirements. To meet those requirements they've developed a</p><p>5 0.11339939 <a title="391-tfidf-5" href="../high_scalability-2008/high_scalability-2008-05-05-HSCALE_-__Handling_200_Million_Transactions_Per_Month_Using_Transparent_Partitioning_With_MySQL_Proxy.html">315 high scalability-2008-05-05-HSCALE -  Handling 200 Million Transactions Per Month Using Transparent Partitioning With MySQL Proxy</a></p>
<p>Introduction: Update 2:  A  HSCALE benchmark  finds HSCALE "adds a maximum overhead of about 0.24 ms per query (against a partitioned table)."  Future releases  promise much improved results.  Update:  A new presentation at  An Introduction to HSCALE .   After writing  Skype Plans for PostgreSQL to Scale to 1 Billion Users , which shows how Skype smartly uses a proxy architecture for scaling, I'm now seeing MySQL Proxy articles all over the place. It's like those "get rich quick" books that say all you have to do is visualize a giraffe with a big yellow dot superimposed over it and by sympathetic magic giraffes will suddenly stampede into your life. Without realizing it I must have visualized transparent proxies smothered in yellow dots.  One of the brightest images is a wonderful series of articles by Peter Romianowski describing the evolution of their proxy architecture. Their application is an OLTP system executing 200 million transaction per month, tables with more than 1.5 billion rows, and a 6</p><p>6 0.11074214 <a title="391-tfidf-6" href="../high_scalability-2013/high_scalability-2013-09-23-Salesforce_Architecture_-_How_they_Handle_1.3_Billion_Transactions_a_Day.html">1521 high scalability-2013-09-23-Salesforce Architecture - How they Handle 1.3 Billion Transactions a Day</a></p>
<p>7 0.11056612 <a title="391-tfidf-7" href="../high_scalability-2008/high_scalability-2008-09-23-How_to_Scale_with_Ruby_on_Rails.html">389 high scalability-2008-09-23-How to Scale with Ruby on Rails</a></p>
<p>8 0.10972698 <a title="391-tfidf-8" href="../high_scalability-2014/high_scalability-2014-05-12-4_Architecture_Issues_When_Scaling_Web_Applications%3A_Bottlenecks%2C_Database%2C_CPU%2C_IO.html">1646 high scalability-2014-05-12-4 Architecture Issues When Scaling Web Applications: Bottlenecks, Database, CPU, IO</a></p>
<p>9 0.10606261 <a title="391-tfidf-9" href="../high_scalability-2010/high_scalability-2010-07-12-Creating_Scalable_Digital_Libraries.html">856 high scalability-2010-07-12-Creating Scalable Digital Libraries</a></p>
<p>10 0.10402319 <a title="391-tfidf-10" href="../high_scalability-2007/high_scalability-2007-10-28-Scaling_Early_Stage_Startups.html">136 high scalability-2007-10-28-Scaling Early Stage Startups</a></p>
<p>11 0.10351635 <a title="391-tfidf-11" href="../high_scalability-2012/high_scalability-2012-10-02-An_Epic_TripAdvisor_Update%3A_Why_Not_Run_on_the_Cloud%3F_The_Grand_Experiment..html">1331 high scalability-2012-10-02-An Epic TripAdvisor Update: Why Not Run on the Cloud? The Grand Experiment.</a></p>
<p>12 0.10264929 <a title="391-tfidf-12" href="../high_scalability-2011/high_scalability-2011-08-22-Strategy%3A_Run_a_Scalable%2C_Available%2C_and_Cheap_Static_Site_on_S3_or_GitHub.html">1102 high scalability-2011-08-22-Strategy: Run a Scalable, Available, and Cheap Static Site on S3 or GitHub</a></p>
<p>13 0.10156877 <a title="391-tfidf-13" href="../high_scalability-2008/high_scalability-2008-05-02-Friends_for_Sale_Architecture_-_A_300_Million_Page_View-Month_Facebook_RoR_App.html">313 high scalability-2008-05-02-Friends for Sale Architecture - A 300 Million Page View-Month Facebook RoR App</a></p>
<p>14 0.099186428 <a title="391-tfidf-14" href="../high_scalability-2009/high_scalability-2009-02-12-MySpace_Architecture.html">511 high scalability-2009-02-12-MySpace Architecture</a></p>
<p>15 0.098952666 <a title="391-tfidf-15" href="../high_scalability-2014/high_scalability-2014-02-17-How_the_AOL.com_Architecture_Evolved_to_99.999%25_Availability%2C_8_Million_Visitors_Per_Day%2C_and_200%2C000_Requests_Per_Second.html">1597 high scalability-2014-02-17-How the AOL.com Architecture Evolved to 99.999% Availability, 8 Million Visitors Per Day, and 200,000 Requests Per Second</a></p>
<p>16 0.097160585 <a title="391-tfidf-16" href="../high_scalability-2009/high_scalability-2009-03-16-Are_Cloud_Based_Memory_Architectures_the_Next_Big_Thing%3F.html">538 high scalability-2009-03-16-Are Cloud Based Memory Architectures the Next Big Thing?</a></p>
<p>17 0.095863685 <a title="391-tfidf-17" href="../high_scalability-2012/high_scalability-2012-06-26-Sponsored_Post%3A_New_Relic%2C_Digital_Ocean%2C_NetDNA%2C_Torbit%2C_Reality_Check_Network%2C_Gigaspaces%2C_AiCache%2C_Logic_Monitor%2C_AppDynamics%2C_CloudSigma%2C_ManageEnine%2C_Site24x7.html">1272 high scalability-2012-06-26-Sponsored Post: New Relic, Digital Ocean, NetDNA, Torbit, Reality Check Network, Gigaspaces, AiCache, Logic Monitor, AppDynamics, CloudSigma, ManageEnine, Site24x7</a></p>
<p>18 0.095170386 <a title="391-tfidf-18" href="../high_scalability-2008/high_scalability-2008-03-12-YouTube_Architecture.html">274 high scalability-2008-03-12-YouTube Architecture</a></p>
<p>19 0.092854567 <a title="391-tfidf-19" href="../high_scalability-2008/high_scalability-2008-07-22-Scaling_Bumper_Sticker%3A_A_1_Billion_Page_Per_Month_Facebook_RoR_App__.html">356 high scalability-2008-07-22-Scaling Bumper Sticker: A 1 Billion Page Per Month Facebook RoR App  </a></p>
<p>20 0.091551058 <a title="391-tfidf-20" href="../high_scalability-2013/high_scalability-2013-04-15-Scaling_Pinterest_-_From_0_to_10s_of_Billions_of_Page_Views_a_Month_in_Two_Years.html">1440 high scalability-2013-04-15-Scaling Pinterest - From 0 to 10s of Billions of Page Views a Month in Two Years</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.163), (1, 0.064), (2, -0.043), (3, -0.124), (4, -0.032), (5, -0.003), (6, 0.004), (7, -0.089), (8, -0.028), (9, 0.038), (10, -0.008), (11, -0.024), (12, -0.069), (13, 0.032), (14, -0.018), (15, 0.02), (16, 0.021), (17, 0.022), (18, 0.005), (19, -0.023), (20, -0.018), (21, 0.072), (22, 0.008), (23, -0.049), (24, -0.002), (25, 0.025), (26, 0.026), (27, 0.025), (28, -0.004), (29, 0.026), (30, 0.027), (31, -0.011), (32, -0.035), (33, 0.027), (34, -0.017), (35, -0.06), (36, -0.013), (37, 0.01), (38, 0.015), (39, -0.0), (40, -0.007), (41, 0.017), (42, -0.032), (43, 0.001), (44, -0.049), (45, 0.071), (46, 0.028), (47, -0.02), (48, -0.044), (49, 0.019)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.96048087 <a title="391-lsi-1" href="../high_scalability-2008/high_scalability-2008-09-23-The_7_Stages_of_Scaling_Web_Apps.html">391 high scalability-2008-09-23-The 7 Stages of Scaling Web Apps</a></p>
<p>Introduction: By John Engales CTO, Rackspace. Good presentation of the stages a typical successful website goes through: 
 
 Stage 1 - The Beginning: Simple architecture, low complexity. no redundancy. Firewall, load balancer, a pair of web servers, database server, and internal storage. 
 Stage 2 - More of the same, just bigger. 
 Stage 3 - The Pain Begins: publicity hits. Use reverse proxy, cache static content, load balancers, more databases, re-coding. 
 Stage 4 - The Pain Intensifies: caching with memcached, writes overload and replication takes too long, start database partitioning, shared storage makes sense for content, significant re-architecting for DB. 
 Stage 5 - This Really Hurts!: rethink entire application, partition on geography user ID, etc, create user clusters, using hashing scheme for locating which user belongs to which cluster. 
 Stage 6 - Getting a little less painful: scalable application and database architecture, acceptable performance, starting to add ne features again, op</p><p>2 0.76384592 <a title="391-lsi-2" href="../high_scalability-2009/high_scalability-2009-05-17-Scaling_Django_Web_Apps_by_Mike_Malone.html">602 high scalability-2009-05-17-Scaling Django Web Apps by Mike Malone</a></p>
<p>Introduction: Film buffs will recognize Django as a classic 1966 spaghetti western that spawned hundreds of imitators. Web heads will certainly first think of Django as the classic Python based Web framework that has also spawned hundreds of imitators and has become the gold standard framework for the web.     Mike Malone, who worked on Pownce, a blogging tool now owned by Six Apart, tells in this very informative EuroDjangoCon presentation how Pownce scaled using Django in the real world.      I was surprised to learn how large Pounce was:  hundreds of requests/sec, thousands of DB operations/sec, millions of user relationships, millions of notes,  and terabytes of static data. Django has a lot of functionality in the box to help you scale, but if you want to scale large it turns out Django has some limitations and Mike tells you what these are and also provides some code to get around them.      Mike's talk-although Django specific--will really help anyone creating applications on the web. There's</p><p>3 0.76341033 <a title="391-lsi-3" href="../high_scalability-2007/high_scalability-2007-08-22-Wikimedia_architecture.html">72 high scalability-2007-08-22-Wikimedia architecture</a></p>
<p>Introduction: Wikimedia is the platform on which Wikipedia, Wiktionary, and the other seven wiki dwarfs are built on. This document is just excellent for the student trying to scale the heights of giant websites. It is full of details and innovative ideas that have been proven on some of the most used websites on the internet.
 
Site: http://wikimedia.org/
  Information Sources     Wikimedia architecture     http://meta.wikimedia.org/wiki/Wikimedia_servers     scale-out vs scale-up  in the  from Oracle to MySQL  blog. 
 Platform 
    Apache    Linux    MySQL    PHP    Squid    LVS    Lucene for Search    Memcached for Distributed Object Cache    Lighttpd Image Server 
 The Stats 
    8 million articles spread over hundreds of language projects (english, dutch, ...)    10th busiest site in the world (source: Alexa)    Exponential growth: doubling every 4-6 months in terms of visitors / traffic / servers    30 000 HTTP requests/s during peak-time    3 Gbit/s of data traffic    3 data centers: Tampa, A</p><p>4 0.75151539 <a title="391-lsi-4" href="../high_scalability-2008/high_scalability-2008-09-23-How_to_Scale_with_Ruby_on_Rails.html">389 high scalability-2008-09-23-How to Scale with Ruby on Rails</a></p>
<p>Introduction: By George Palmer of 3dogsbark.com. Covers:   * How you start out: shared hosting,  web server DB on same machine. Move two 2 machines. Minimal code changes.   * Scaling the database. Add read slaves on their own machines. Then master-master setup. Still minimal code changes.   * Scaling the web server. Load balance against multiple application servers. Application servers scale but the database doesn't.   * User clusters. Partition and allocate users to their own dedicated cluster. Requires substantial code changes.   * Caching. A large percentage of hits are read only. Use reverse proxy, memcached, and language specific cache.   * Elastic architectures. Based on Amazon EC2. Start and stop instances on demand. For global applications keep a cache on each continent, assign users to clusters by location, maintain app servers on each continent, use transaction replication software if you must replicate your site globally.</p><p>5 0.73780274 <a title="391-lsi-5" href="../high_scalability-2007/high_scalability-2007-11-12-Slashdot_Architecture_-_How_the_Old_Man_of_the_Internet_Learned_to_Scale.html">150 high scalability-2007-11-12-Slashdot Architecture - How the Old Man of the Internet Learned to Scale</a></p>
<p>Introduction: Slashdot effect  : overwhelming unprepared sites with an avalanche of reader's clicks after being mentioned on Slashdot. Sure, we now have the "Digg effect" and other hot new stars, but Slashdot was the original. And like many stars from generations past, Slashdot plays the elder statesman's role with with class, dignity, and restraint. Yet with millions and millions of users Slashdot is still box office gold and more than keeps up with the young'ins. And with age comes the wisdom of learning how to handle all those users. Just how does Slashdot scale and what can you learn by going old school?       Site: http://slashdot.org       Information Sources          Slashdot's Setup, Part 1- Hardware     Slashdot's Setup, Part 2- Software     History of Slashdot Part 3- Going Corporate     The History of Slashdot Part 4 - Yesterday, Today, Tomorrow     The Platform      MySQL   Linux (CentOS/RHEL)   Pound   Apache   Perl   Memcached   LVS    The Stats      Started building the system in 1999</p><p>6 0.71844143 <a title="391-lsi-6" href="../high_scalability-2007/high_scalability-2007-07-10-mixi.jp__Architecture.html">5 high scalability-2007-07-10-mixi.jp  Architecture</a></p>
<p>7 0.7184388 <a title="391-lsi-7" href="../high_scalability-2012/high_scalability-2012-09-26-WordPress.com_Serves_70%2C000_req-sec_and_over_15_Gbit-sec_of_Traffic_using_NGINX.html">1329 high scalability-2012-09-26-WordPress.com Serves 70,000 req-sec and over 15 Gbit-sec of Traffic using NGINX</a></p>
<p>8 0.70138824 <a title="391-lsi-8" href="../high_scalability-2007/high_scalability-2007-12-07-Synchronizing_databases_in_different_geographic_locations.html">176 high scalability-2007-12-07-Synchronizing databases in different geographic locations</a></p>
<p>9 0.7013185 <a title="391-lsi-9" href="../high_scalability-2013/high_scalability-2013-02-06-Super_Bowl_Advertisers_Ready_for_the_Traffic%3F_Nope..It%27s_Lights_Out..html">1401 high scalability-2013-02-06-Super Bowl Advertisers Ready for the Traffic? Nope..It's Lights Out.</a></p>
<p>10 0.69949454 <a title="391-lsi-10" href="../high_scalability-2013/high_scalability-2013-09-23-Salesforce_Architecture_-_How_they_Handle_1.3_Billion_Transactions_a_Day.html">1521 high scalability-2013-09-23-Salesforce Architecture - How they Handle 1.3 Billion Transactions a Day</a></p>
<p>11 0.69311839 <a title="391-lsi-11" href="../high_scalability-2008/high_scalability-2008-07-22-Scaling_Bumper_Sticker%3A_A_1_Billion_Page_Per_Month_Facebook_RoR_App__.html">356 high scalability-2008-07-22-Scaling Bumper Sticker: A 1 Billion Page Per Month Facebook RoR App  </a></p>
<p>12 0.68526822 <a title="391-lsi-12" href="../high_scalability-2008/high_scalability-2008-11-03-How_Sites_are_Scaling_Up_for_the_Election_Night_Crush.html">437 high scalability-2008-11-03-How Sites are Scaling Up for the Election Night Crush</a></p>
<p>13 0.68424588 <a title="391-lsi-13" href="../high_scalability-2014/high_scalability-2014-02-17-How_the_AOL.com_Architecture_Evolved_to_99.999%25_Availability%2C_8_Million_Visitors_Per_Day%2C_and_200%2C000_Requests_Per_Second.html">1597 high scalability-2014-02-17-How the AOL.com Architecture Evolved to 99.999% Availability, 8 Million Visitors Per Day, and 200,000 Requests Per Second</a></p>
<p>14 0.6812495 <a title="391-lsi-14" href="../high_scalability-2013/high_scalability-2013-12-16-22_Recommendations_for_Building_Effective_High_Traffic_Web_Software.html">1565 high scalability-2013-12-16-22 Recommendations for Building Effective High Traffic Web Software</a></p>
<p>15 0.6773349 <a title="391-lsi-15" href="../high_scalability-2007/high_scalability-2007-07-12-FeedBurner_Architecture.html">7 high scalability-2007-07-12-FeedBurner Architecture</a></p>
<p>16 0.67674804 <a title="391-lsi-16" href="../high_scalability-2008/high_scalability-2008-04-05-Skype_Plans_for_PostgreSQL_to_Scale_to_1_Billion_Users.html">297 high scalability-2008-04-05-Skype Plans for PostgreSQL to Scale to 1 Billion Users</a></p>
<p>17 0.67668992 <a title="391-lsi-17" href="../high_scalability-2007/high_scalability-2007-10-28-Scaling_Early_Stage_Startups.html">136 high scalability-2007-10-28-Scaling Early Stage Startups</a></p>
<p>18 0.67610228 <a title="391-lsi-18" href="../high_scalability-2008/high_scalability-2008-05-02-Friends_for_Sale_Architecture_-_A_300_Million_Page_View-Month_Facebook_RoR_App.html">313 high scalability-2008-05-02-Friends for Sale Architecture - A 300 Million Page View-Month Facebook RoR App</a></p>
<p>19 0.67335469 <a title="391-lsi-19" href="../high_scalability-2009/high_scalability-2009-04-04-Digg_Architecture.html">554 high scalability-2009-04-04-Digg Architecture</a></p>
<p>20 0.67060947 <a title="391-lsi-20" href="../high_scalability-2010/high_scalability-2010-08-24-21_Quality_Screencasts_on_Scaling_Rails.html">886 high scalability-2010-08-24-21 Quality Screencasts on Scaling Rails</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.166), (2, 0.242), (10, 0.047), (13, 0.197), (40, 0.026), (61, 0.065), (79, 0.112), (94, 0.048)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.94340146 <a title="391-lda-1" href="../high_scalability-2012/high_scalability-2012-07-12-4_Strategies_for_Punching_Down_Traffic_Spikes.html">1282 high scalability-2012-07-12-4 Strategies for Punching Down Traffic Spikes</a></p>
<p>Introduction: Travis Reeder in  Spikability - An Application's Ability to Handle Unknown and/or Inconsistent Load  gives four good ways of handling spikey loads:
  
  Have more resources than you'll ever need . Estimate the maximum traffic you'll need and keep that many servers running. Downside is you are paying for capacity you aren't using. 
  Disable features during high loads . Reduce load by disabling features or substituting in lighter weight features. Downside is users to have access to features. 
  Auto scaling . Launch new servers in response to load. Downsides are it's complicated to setup and slow to respond. Random spikes will cause cycling of instances going up and down. 
  Use message queues . Queues soak up work requests during traffic spikes. More servers can be started to process work from the queue. Resources aren't wasted and features are disabled. Downside is increased latency.</p><p>same-blog 2 0.93534124 <a title="391-lda-2" href="../high_scalability-2008/high_scalability-2008-09-23-The_7_Stages_of_Scaling_Web_Apps.html">391 high scalability-2008-09-23-The 7 Stages of Scaling Web Apps</a></p>
<p>Introduction: By John Engales CTO, Rackspace. Good presentation of the stages a typical successful website goes through: 
 
 Stage 1 - The Beginning: Simple architecture, low complexity. no redundancy. Firewall, load balancer, a pair of web servers, database server, and internal storage. 
 Stage 2 - More of the same, just bigger. 
 Stage 3 - The Pain Begins: publicity hits. Use reverse proxy, cache static content, load balancers, more databases, re-coding. 
 Stage 4 - The Pain Intensifies: caching with memcached, writes overload and replication takes too long, start database partitioning, shared storage makes sense for content, significant re-architecting for DB. 
 Stage 5 - This Really Hurts!: rethink entire application, partition on geography user ID, etc, create user clusters, using hashing scheme for locating which user belongs to which cluster. 
 Stage 6 - Getting a little less painful: scalable application and database architecture, acceptable performance, starting to add ne features again, op</p><p>3 0.93338084 <a title="391-lda-3" href="../high_scalability-2010/high_scalability-2010-01-11-Have_We_Reached_the_End_of_Scaling%3F.html">758 high scalability-2010-01-11-Have We Reached the End of Scaling?</a></p>
<p>Introduction: This is an excerpt from my article  Building Super Scalable Systems: Blade Runner Meets Autonomic Computing in the Ambient Cloud.  
 
Have we reached the end of scaling? That's what I asked myself one day after noticing a bunch of "The End of" headlines. We've reached  The End of History  because the Western liberal democracy is the "end point of humanity's sociocultural evolution and the final form of human government."  We've reached  The End of Science  because of the "fact that there aren't going to be any obvious, cataclysmic revolutions." We've even reached  The End of Theory  because all answers can be found in the continuous stream of data we're collecting. And doesn't always seem like we're at  The End of the World ?
 
Motivated by the prospect of everything ending, I began to wonder: have we really reached The End of Scaling?
 
For a while I thought this might be true. The reason I thought the End of Scaling might be near is because of the slow down of potential articles at m</p><p>4 0.92412287 <a title="391-lda-4" href="../high_scalability-2008/high_scalability-2008-12-29-100%25_on_Amazon_Web_Services%3A_Soocial.com_-_a_lesson_of_porting_your_service_to_Amazon.html">477 high scalability-2008-12-29-100% on Amazon Web Services: Soocial.com - a lesson of porting your service to Amazon</a></p>
<p>Introduction: Simone Brunozzi, technology evangelist for Amazon Web Services in Europe, describes how Soocial.com was fully ported to Amazon web services.         ----------------   This period of the year I decided to dedicate some time to better understand how our customers use AWS, therefore I spent some online time with Stefan Fountain and the nice guys at Soocial.com, a "one address book solution to contact management", and I would like to share with you some details of their IT infrastructure, which now runs 100% on Amazon Web Services!    In the last few months, they've been working hard to cope with tens of thousands of users and to get ready to easily scale to millions. To make this possible, they decided to move ALL their architecture to Amazon Web Services. Despite the fact that they were quite happy with their previous hosting provider, Amazon proved to be the way to go.    -----------------           Read the rest of the article here .</p><p>5 0.92038465 <a title="391-lda-5" href="../high_scalability-2011/high_scalability-2011-06-29-Second_Hand_Seizure_%3A_A_New_Cause_of_Site_Death.html">1070 high scalability-2011-06-29-Second Hand Seizure : A New Cause of Site Death</a></p>
<p>Introduction: Like a digital SWAT team that implodes the wrong door on a raid, the FBI  seized multiple racks of computers  from DigitalOne, these  racks  host websites from many clients that just happened to be in the same racks as whomever they are investigating. Downed sites include Instapaper, Curbed Network, and  Pinboard . With the  density of servers  these days many 1000s of sites could easily have been effected.
 
Sites like Pinboard were victims by association, they did not inhale. This is an association sites have no control over. On a shared hosting service, you have no control over your fellow VM mates. In a cloud or a managed service, you have no control over which racks your servers are in. So like second hand smoke, you get the disease by random association. There's something inherently unfair about that.
 
A  comment by illumin8  shows just how Darth insidious this process can be:
  

A popular method used by hackers is to sign up for a virtual server with a stolen credit card. If t</p><p>6 0.9160077 <a title="391-lda-6" href="../high_scalability-2007/high_scalability-2007-10-04-You_Can_Now_Store_All_Your_Stuff_on_Your_Own_Google_Like_File_System.html">112 high scalability-2007-10-04-You Can Now Store All Your Stuff on Your Own Google Like File System</a></p>
<p>7 0.91424805 <a title="391-lda-7" href="../high_scalability-2014/high_scalability-2014-01-10-Stuff_The_Internet_Says_On_Scalability_For_January_10th%2C_2014.html">1576 high scalability-2014-01-10-Stuff The Internet Says On Scalability For January 10th, 2014</a></p>
<p>8 0.90566307 <a title="391-lda-8" href="../high_scalability-2009/high_scalability-2009-07-29-Strategy%3A_Devirtualize_for_More_Vroom.html">664 high scalability-2009-07-29-Strategy: Devirtualize for More Vroom</a></p>
<p>9 0.89630032 <a title="391-lda-9" href="../high_scalability-2007/high_scalability-2007-07-27-Product%3A_Munin_Monitoriting_Tool.html">34 high scalability-2007-07-27-Product: Munin Monitoriting Tool</a></p>
<p>10 0.8820067 <a title="391-lda-10" href="../high_scalability-2013/high_scalability-2013-03-29-Stuff_The_Internet_Says_On_Scalability_For_March_29%2C_2013.html">1431 high scalability-2013-03-29-Stuff The Internet Says On Scalability For March 29, 2013</a></p>
<p>11 0.86072934 <a title="391-lda-11" href="../high_scalability-2013/high_scalability-2013-04-25-Paper%3A_Making_reliable_distributed_systems_in_the_presence_of_software_errors.html">1446 high scalability-2013-04-25-Paper: Making reliable distributed systems in the presence of software errors</a></p>
<p>12 0.85214478 <a title="391-lda-12" href="../high_scalability-2008/high_scalability-2008-10-22-Server_load_balancing_architectures%2C_Part_1%3A_Transport-level_load_balancing.html">426 high scalability-2008-10-22-Server load balancing architectures, Part 1: Transport-level load balancing</a></p>
<p>13 0.85184318 <a title="391-lda-13" href="../high_scalability-2007/high_scalability-2007-08-30-Log_Everything_All_the_Time.html">77 high scalability-2007-08-30-Log Everything All the Time</a></p>
<p>14 0.84969002 <a title="391-lda-14" href="../high_scalability-2013/high_scalability-2013-06-05-A_Simple_6_Step_Transition_Guide_for_Moving_Away_from_X_to_AWS_.html">1470 high scalability-2013-06-05-A Simple 6 Step Transition Guide for Moving Away from X to AWS </a></p>
<p>15 0.8495366 <a title="391-lda-15" href="../high_scalability-2009/high_scalability-2009-06-29-How_to_Succeed_at_Capacity_Planning_Without_Really_Trying_%3A__An_Interview_with_Flickr%27s_John_Allspaw_on_His_New_Book.html">643 high scalability-2009-06-29-How to Succeed at Capacity Planning Without Really Trying :  An Interview with Flickr's John Allspaw on His New Book</a></p>
<p>16 0.84893608 <a title="391-lda-16" href="../high_scalability-2010/high_scalability-2010-06-07-Six_Ways_Twitter_May_Reach_its_Big_Hairy_Audacious_Goal_of_One_Billion_Users.html">837 high scalability-2010-06-07-Six Ways Twitter May Reach its Big Hairy Audacious Goal of One Billion Users</a></p>
<p>17 0.84892553 <a title="391-lda-17" href="../high_scalability-2010/high_scalability-2010-10-26-Marrying_memcached_and_NoSQL.html">927 high scalability-2010-10-26-Marrying memcached and NoSQL</a></p>
<p>18 0.84847558 <a title="391-lda-18" href="../high_scalability-2012/high_scalability-2012-04-20-Stuff_The_Internet_Says_On_Scalability_For_April_20%2C_2012.html">1231 high scalability-2012-04-20-Stuff The Internet Says On Scalability For April 20, 2012</a></p>
<p>19 0.84843522 <a title="391-lda-19" href="../high_scalability-2008/high_scalability-2008-12-03-Java_World_Interview_on_Scalability_and_Other_Java_Scalability_Secrets.html">459 high scalability-2008-12-03-Java World Interview on Scalability and Other Java Scalability Secrets</a></p>
<p>20 0.84842461 <a title="391-lda-20" href="../high_scalability-2011/high_scalability-2011-09-19-Big_Iron_Returns_with_BigMemory.html">1118 high scalability-2011-09-19-Big Iron Returns with BigMemory</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
