<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>205 high scalability-2008-01-10-Letting Clients Know What's Changed: Push Me or Pull Me?</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2008" href="../home/high_scalability-2008_home.html">high_scalability-2008</a> <a title="high_scalability-2008-205" href="#">high_scalability-2008-205</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>205 high scalability-2008-01-10-Letting Clients Know What's Changed: Push Me or Pull Me?</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2008-205-html" href="http://highscalability.com//blog/2008/1/10/letting-clients-know-whats-changed-push-me-or-pull-me.html">html</a></p><p>Introduction: I had a false belief I thought I came here to stay We're all just visiting All just breaking like waves The oceans made me, but who came up with me? Push me, pull me, push me, or pull me out .     So true Perl Jam   (Push me Pull me lyrics)  , so true. I too have wondered how web clients should be notified of model changes. Should servers push events to clients or should clients pull events from servers? A topic worthy of its own song if ever there was one.       To pull events the client simply starts a timer and makes a request to the server. This is polling. You can either pull a complete set of fresh data or get a list of changes. The server "knows" if anything you are interested in has changed and makes those changes available to you.  Knowing what has changed can be relatively simple with a publish-subscribe type backend or you can get very complex with fine grained bit maps of attributes and keeping per client state on what I client still needs to see.     Polling is heavy man.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 I too have wondered how web clients should be notified of model changes. [sent-4, score-0.374]
</p><p>2 Should servers push events to clients or should clients pull events from servers? [sent-5, score-1.086]
</p><p>3 To pull events the client simply starts a timer and makes a request to the server. [sent-7, score-0.908]
</p><p>4 You can either pull a complete set of fresh data or get a list of changes. [sent-9, score-0.259]
</p><p>5 Knowing what has changed can be relatively simple with a publish-subscribe type backend or you can get very complex with fine grained bit maps of attributes and keeping per client state on what I client still needs to see. [sent-11, score-1.022]
</p><p>6 Of course, caching can smooth out this jagged trip, but if you keep per client state you need more clever per client cache views. [sent-15, score-1.122]
</p><p>7 The overhead of polling can be mitigated somewhat by piggy backing updates on replies to client requests. [sent-16, score-0.995]
</p><p>8 So if polling has a high overhead then it makes sense to only send data when there's an update the client should see. [sent-17, score-0.919]
</p><p>9 The current push model favorite is   Comet  :    a World Wide Web application architecture in which a web server sends data to a client program (normally a web browser) asynchronously without any need for the client to explicitly request it. [sent-19, score-1.462]
</p><p>10 A connection has to be kept open between the client and server for the new data to pushed over. [sent-22, score-0.553]
</p><p>11 For every connection you also have to store the data to push to the client and you need a thread to send it. [sent-26, score-0.841]
</p><p>12 Architecturally I've always sided on polling for complete datasets rather than pushing or polling just for changes. [sent-28, score-1.013]
</p><p>13 Machines can go up and down at will and your client will always be correct and consistent. [sent-30, score-0.482]
</p><p>14 And you use client resources to do the polling and the update on the client side. [sent-35, score-1.147]
</p><p>15 All you have to do to scale polling is have enough machines, smart caching to handle the load, enough bandwidth to handle larger datasets, and a problem where low latency isn't required. [sent-36, score-0.603]
</p><p>16 That's all :-)     The   Comet Daily  , not affiliated with Super Man I hear,  is making a strong case for push in their articles   Comet is Always Better Than Polling   and   20,000 Reasons Why Comet Scales  . [sent-37, score-0.391]
</p><p>17 With Comet they found:    The key result is that sub-second latency is achievable even for 20,000 users. [sent-42, score-0.254]
</p><p>18 throughput tradeoff: for example, for 5,000 users, 100ms latency is achievable up to 2,000 messages per second, but increases to over 250ms for rates over 3,000 messages per second. [sent-44, score-0.548]
</p><p>19 Most people haven't deployed or even considered push based architectures. [sent-46, score-0.299]
</p><p>20 I can't resist adding this cute animation of a     llama push me pull me  . [sent-48, score-0.792]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('client', 0.404), ('polling', 0.339), ('push', 0.299), ('pull', 0.259), ('comet', 0.206), ('clients', 0.172), ('achievable', 0.15), ('overhead', 0.113), ('latency', 0.104), ('affiliated', 0.092), ('jam', 0.092), ('lyrics', 0.092), ('oceans', 0.092), ('events', 0.092), ('datasets', 0.088), ('jagged', 0.087), ('pushing', 0.086), ('request', 0.084), ('sided', 0.083), ('handle', 0.08), ('cute', 0.08), ('per', 0.079), ('always', 0.078), ('animation', 0.077), ('resist', 0.077), ('came', 0.076), ('changed', 0.076), ('mitigated', 0.075), ('connection', 0.075), ('server', 0.074), ('flurry', 0.072), ('notified', 0.072), ('smooth', 0.069), ('timer', 0.069), ('worthy', 0.069), ('web', 0.068), ('servlets', 0.068), ('song', 0.068), ('messages', 0.068), ('connections', 0.065), ('poll', 0.064), ('replies', 0.064), ('simplest', 0.063), ('send', 0.063), ('wondered', 0.062), ('explicitly', 0.061), ('fortunately', 0.061), ('naive', 0.06), ('tradeoff', 0.06), ('grained', 0.059)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0000001 <a title="205-tfidf-1" href="../high_scalability-2008/high_scalability-2008-01-10-Letting_Clients_Know_What%27s_Changed%3A_Push_Me_or_Pull_Me%3F.html">205 high scalability-2008-01-10-Letting Clients Know What's Changed: Push Me or Pull Me?</a></p>
<p>Introduction: I had a false belief I thought I came here to stay We're all just visiting All just breaking like waves The oceans made me, but who came up with me? Push me, pull me, push me, or pull me out .     So true Perl Jam   (Push me Pull me lyrics)  , so true. I too have wondered how web clients should be notified of model changes. Should servers push events to clients or should clients pull events from servers? A topic worthy of its own song if ever there was one.       To pull events the client simply starts a timer and makes a request to the server. This is polling. You can either pull a complete set of fresh data or get a list of changes. The server "knows" if anything you are interested in has changed and makes those changes available to you.  Knowing what has changed can be relatively simple with a publish-subscribe type backend or you can get very complex with fine grained bit maps of attributes and keeping per client state on what I client still needs to see.     Polling is heavy man.</p><p>2 0.25844723 <a title="205-tfidf-2" href="../high_scalability-2008/high_scalability-2008-05-14-New_Facebook_Chat_Feature_Scales_to_70_Million_Users_Using_Erlang.html">318 high scalability-2008-05-14-New Facebook Chat Feature Scales to 70 Million Users Using Erlang</a></p>
<p>Introduction: Update :  Erlang at Facebook  by Eugene Letuchy. How Facebook uses Erlang to implement Chat, AIM Presence, and Chat Jabber support.  
 
I've done some  XMPP  development so when I read  Facebook was making a Jabber chat client  I was really curious how they would make it work. While core XMPP is straightforward, a number of protocol extensions like discovery, forms, chat states, pubsub, multi user chat, and privacy lists really up the implementation complexity. Some real engineering challenges were involved to make this puppy scale and perform.  It's not clear what extensions they've  implemented, but a blog entry by Facebook's Eugene Letuchy hits some of the architectural challenges they faced and how they overcame them.
 
A web based Jabber client poses a few problems because XMPP, like most IM protocols, is an asynchronous event driven system that pretty much assumes you have a full time open connection. After logging in the server sends a client roster information and presence info</p><p>3 0.20659164 <a title="205-tfidf-3" href="../high_scalability-2008/high_scalability-2008-10-08-Strategy%3A_Flickr_-_Do_the_Essential_Work_Up-front_and_Queue_the_Rest_.html">406 high scalability-2008-10-08-Strategy: Flickr - Do the Essential Work Up-front and Queue the Rest </a></p>
<p>Introduction: This strategy is stated perfectly by Flickr's Myles Grant:   The Flickr engineering team is obsessed with making pages load as quickly as possible. To that end, weâ&euro;&trade;re refactoring large amounts of our code to do only the essential work up front, and rely on our queuing system to do the rest.   Flickr uses a queuing system to process 11 million tasks a day. Leslie Michael Orchard also does a great job explaining the queuing meme in his excellent post   Queue everything and delight everyone  . Asynchronous work queues are how you scalably solve problems that are too big to handle in real-time.     The process:     Identify the minimum feedback the client (UI, API) needs to know an operation succeeded . It's enough, for example, to update a client's view when a posting a message to a microblogging service. The client probably isn't aware of all the other steps that happen when a message is added and doesn't really care when they happen as long as the obvious cases happen in an appropariate</p><p>4 0.18683818 <a title="205-tfidf-4" href="../high_scalability-2011/high_scalability-2011-01-27-Comet_-_An_Example_of_the_New_Key-Code_Databases.html">979 high scalability-2011-01-27-Comet - An Example of the New Key-Code Databases</a></p>
<p>Introduction: Comet is an  active distributed key-value store  built at the University of Washington. The paper describing Comet is  Comet: An active distributed key-value store , there are also  slides , and a  MP3  of a presentation given at  OSDI '10 . Here's a succinct  overview of Comet :
  

Today's cloud storage services, such as Amazon S3 or peer-to-peer DHTs, are highly inflexible and impose a variety of constraints on their clients: specific replication and consistency schemes, fixed data timeouts, limited logging, etc. We witnessed such inflexibility first-hand as part of our Vanish work, where we used a DHT to store encryption keys temporarily. To address this issue, we built Comet, an extensible storage service that allows clients to inject snippets of code that control their data's behavior inside the storage service.

  
I found this paper quite interesting because it takes the initial steps of collocating code with a key-value store, which turns it into what might called a  key-code</p><p>5 0.18262731 <a title="205-tfidf-5" href="../high_scalability-2010/high_scalability-2010-01-11-Strategy%3A_Don%27t_Use_Polling_for_Real-time_Feeds.html">759 high scalability-2010-01-11-Strategy: Don't Use Polling for Real-time Feeds</a></p>
<p>Introduction: Ivan Zuzak wrote a fascinating article on  Real-time feed processing and filtering  using Google App Engine to build  Feed-buster , a service that  inserts MediaRSS tags into feeds that don't have them . He talks about using polling and  PubSubHubBub  (real-time) to process FriendFeed feeds. Ivan is trying to devise a separate filtering service where:   
  
  filtering services should be applied as close to the publisher  as possible so notifications that nobody wants don’t waste network resource.  
  processing services should be applied as close to the subscriber  so that the original update may be transported through the network as a single notification for as long as possible. 
  
Besides being a generally interesting article, Ivan makes an insightful observation on the nature of using polling services in combination with metered Infrastructure/Platform services:
  

Polling is bad because AppEngine applications have a  fixed free daily quota  for consumed resources, when the numbe</p><p>6 0.16419147 <a title="205-tfidf-6" href="../high_scalability-2011/high_scalability-2011-03-09-Google_and_Netflix_Strategy%3A_Use_Partial_Responses_to_Reduce_Request_Sizes.html">1001 high scalability-2011-03-09-Google and Netflix Strategy: Use Partial Responses to Reduce Request Sizes</a></p>
<p>7 0.15580201 <a title="205-tfidf-7" href="../high_scalability-2008/high_scalability-2008-06-09-Apple%27s_iPhone_to_Use_a_Centralized_Push_Based_Notification_Architecture.html">343 high scalability-2008-06-09-Apple's iPhone to Use a Centralized Push Based Notification Architecture</a></p>
<p>8 0.14678186 <a title="205-tfidf-8" href="../high_scalability-2009/high_scalability-2009-07-25-Latency_is_Everywhere_and_it_Costs_You_Sales_-_How_to_Crush_it.html">661 high scalability-2009-07-25-Latency is Everywhere and it Costs You Sales - How to Crush it</a></p>
<p>9 0.14677221 <a title="205-tfidf-9" href="../high_scalability-2008/high_scalability-2008-03-20-Paper%3A_Asynchronous_HTTP_and_Comet_architectures.html">286 high scalability-2008-03-20-Paper: Asynchronous HTTP and Comet architectures</a></p>
<p>10 0.13973573 <a title="205-tfidf-10" href="../high_scalability-2013/high_scalability-2013-03-11-Low_Level_Scalability_Solutions_-_The_Conditioning_Collection.html">1421 high scalability-2013-03-11-Low Level Scalability Solutions - The Conditioning Collection</a></p>
<p>11 0.13927373 <a title="205-tfidf-11" href="../high_scalability-2010/high_scalability-2010-03-10-How_FarmVille_Scales_-_The_Follow-up.html">792 high scalability-2010-03-10-How FarmVille Scales - The Follow-up</a></p>
<p>12 0.13451862 <a title="205-tfidf-12" href="../high_scalability-2008/high_scalability-2008-10-01-Joyent_-_Cloud_Computing_Built_on_Accelerators.html">399 high scalability-2008-10-01-Joyent - Cloud Computing Built on Accelerators</a></p>
<p>13 0.13117182 <a title="205-tfidf-13" href="../high_scalability-2010/high_scalability-2010-12-20-Netflix%3A_Use_Less_Chatty_Protocols_in_the_Cloud_-_Plus_26_Fixes.html">960 high scalability-2010-12-20-Netflix: Use Less Chatty Protocols in the Cloud - Plus 26 Fixes</a></p>
<p>14 0.12629746 <a title="205-tfidf-14" href="../high_scalability-2014/high_scalability-2014-05-15-Paper%3A_SwiftCloud%3A_Fault-Tolerant_Geo-Replication_Integrated_all_the_Way_to_the_Client_Machine.html">1648 high scalability-2014-05-15-Paper: SwiftCloud: Fault-Tolerant Geo-Replication Integrated all the Way to the Client Machine</a></p>
<p>15 0.12585913 <a title="205-tfidf-15" href="../high_scalability-2013/high_scalability-2013-08-13-In_Memoriam%3A_Lavabit_Architecture_-_Creating_a_Scalable_Email_Service.html">1501 high scalability-2013-08-13-In Memoriam: Lavabit Architecture - Creating a Scalable Email Service</a></p>
<p>16 0.12449747 <a title="205-tfidf-16" href="../high_scalability-2009/high_scalability-2009-09-10-How_to_handle_so_many_socket_connection.html">699 high scalability-2009-09-10-How to handle so many socket connection</a></p>
<p>17 0.12337827 <a title="205-tfidf-17" href="../high_scalability-2009/high_scalability-2009-03-16-Are_Cloud_Based_Memory_Architectures_the_Next_Big_Thing%3F.html">538 high scalability-2009-03-16-Are Cloud Based Memory Architectures the Next Big Thing?</a></p>
<p>18 0.12298049 <a title="205-tfidf-18" href="../high_scalability-2011/high_scalability-2011-06-13-Automation_on_AWS_with_Ruby_and_Puppet.html">1058 high scalability-2011-06-13-Automation on AWS with Ruby and Puppet</a></p>
<p>19 0.12117615 <a title="205-tfidf-19" href="../high_scalability-2013/high_scalability-2013-02-27-42_Monster_Problems_that_Attack_as_Loads_Increase.html">1413 high scalability-2013-02-27-42 Monster Problems that Attack as Loads Increase</a></p>
<p>20 0.12056934 <a title="205-tfidf-20" href="../high_scalability-2013/high_scalability-2013-12-16-22_Recommendations_for_Building_Effective_High_Traffic_Web_Software.html">1565 high scalability-2013-12-16-22 Recommendations for Building Effective High Traffic Web Software</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.206), (1, 0.099), (2, -0.062), (3, -0.07), (4, -0.011), (5, -0.018), (6, 0.081), (7, 0.069), (8, -0.089), (9, -0.013), (10, 0.038), (11, 0.095), (12, 0.01), (13, -0.044), (14, -0.017), (15, 0.018), (16, 0.032), (17, 0.018), (18, 0.055), (19, -0.052), (20, 0.012), (21, -0.026), (22, 0.053), (23, -0.043), (24, 0.071), (25, 0.0), (26, 0.076), (27, 0.037), (28, -0.004), (29, -0.063), (30, -0.003), (31, -0.059), (32, 0.016), (33, -0.021), (34, 0.041), (35, -0.013), (36, 0.068), (37, 0.039), (38, -0.072), (39, -0.062), (40, 0.053), (41, -0.029), (42, 0.047), (43, 0.0), (44, -0.031), (45, -0.052), (46, -0.043), (47, 0.017), (48, 0.003), (49, -0.032)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.95989782 <a title="205-lsi-1" href="../high_scalability-2008/high_scalability-2008-01-10-Letting_Clients_Know_What%27s_Changed%3A_Push_Me_or_Pull_Me%3F.html">205 high scalability-2008-01-10-Letting Clients Know What's Changed: Push Me or Pull Me?</a></p>
<p>Introduction: I had a false belief I thought I came here to stay We're all just visiting All just breaking like waves The oceans made me, but who came up with me? Push me, pull me, push me, or pull me out .     So true Perl Jam   (Push me Pull me lyrics)  , so true. I too have wondered how web clients should be notified of model changes. Should servers push events to clients or should clients pull events from servers? A topic worthy of its own song if ever there was one.       To pull events the client simply starts a timer and makes a request to the server. This is polling. You can either pull a complete set of fresh data or get a list of changes. The server "knows" if anything you are interested in has changed and makes those changes available to you.  Knowing what has changed can be relatively simple with a publish-subscribe type backend or you can get very complex with fine grained bit maps of attributes and keeping per client state on what I client still needs to see.     Polling is heavy man.</p><p>2 0.86032951 <a title="205-lsi-2" href="../high_scalability-2008/high_scalability-2008-05-14-New_Facebook_Chat_Feature_Scales_to_70_Million_Users_Using_Erlang.html">318 high scalability-2008-05-14-New Facebook Chat Feature Scales to 70 Million Users Using Erlang</a></p>
<p>Introduction: Update :  Erlang at Facebook  by Eugene Letuchy. How Facebook uses Erlang to implement Chat, AIM Presence, and Chat Jabber support.  
 
I've done some  XMPP  development so when I read  Facebook was making a Jabber chat client  I was really curious how they would make it work. While core XMPP is straightforward, a number of protocol extensions like discovery, forms, chat states, pubsub, multi user chat, and privacy lists really up the implementation complexity. Some real engineering challenges were involved to make this puppy scale and perform.  It's not clear what extensions they've  implemented, but a blog entry by Facebook's Eugene Letuchy hits some of the architectural challenges they faced and how they overcame them.
 
A web based Jabber client poses a few problems because XMPP, like most IM protocols, is an asynchronous event driven system that pretty much assumes you have a full time open connection. After logging in the server sends a client roster information and presence info</p><p>3 0.82416433 <a title="205-lsi-3" href="../high_scalability-2008/high_scalability-2008-10-08-Strategy%3A_Flickr_-_Do_the_Essential_Work_Up-front_and_Queue_the_Rest_.html">406 high scalability-2008-10-08-Strategy: Flickr - Do the Essential Work Up-front and Queue the Rest </a></p>
<p>Introduction: This strategy is stated perfectly by Flickr's Myles Grant:   The Flickr engineering team is obsessed with making pages load as quickly as possible. To that end, weâ&euro;&trade;re refactoring large amounts of our code to do only the essential work up front, and rely on our queuing system to do the rest.   Flickr uses a queuing system to process 11 million tasks a day. Leslie Michael Orchard also does a great job explaining the queuing meme in his excellent post   Queue everything and delight everyone  . Asynchronous work queues are how you scalably solve problems that are too big to handle in real-time.     The process:     Identify the minimum feedback the client (UI, API) needs to know an operation succeeded . It's enough, for example, to update a client's view when a posting a message to a microblogging service. The client probably isn't aware of all the other steps that happen when a message is added and doesn't really care when they happen as long as the obvious cases happen in an appropariate</p><p>4 0.81020373 <a title="205-lsi-4" href="../high_scalability-2011/high_scalability-2011-03-09-Google_and_Netflix_Strategy%3A_Use_Partial_Responses_to_Reduce_Request_Sizes.html">1001 high scalability-2011-03-09-Google and Netflix Strategy: Use Partial Responses to Reduce Request Sizes</a></p>
<p>Introduction: This strategy targets reducing the amount of protocol data in packets by sending only the attributes that are needed. Google calls this  Partial Response and Partial Update .
 
Netflix posted about adopting this strategy in their  recent Netflix API redesign . We've seen previously how Netflix improved performance by creating  less chatty protocols .
 
As a consequence packet sizes rise as more data is being stuffed into each packet in order to reduce the number of round trips. But we don't like large packets either (memory usage and packet processing overhead), so we have to think of creative ways to shrink them back down.
 
The change Netflx is making is to conceptualize their API as a database. What does this mean?
 
A  partial response  is like a SQL select statement where you can specify only the fields you want back. Only the attributes of interest are requested.  Previously all fields for objects were returned, even if the client didn't need them. So the goal is reduce payload s</p><p>5 0.7909252 <a title="205-lsi-5" href="../high_scalability-2013/high_scalability-2013-03-11-Low_Level_Scalability_Solutions_-_The_Conditioning_Collection.html">1421 high scalability-2013-03-11-Low Level Scalability Solutions - The Conditioning Collection</a></p>
<p>Introduction: We talked about  42 Monster Problems That Attack As Loads Increase . And in  The Aggregation Collection  we talked about the value of prioritizing work and making smart queues as a way of absorbing and not reflecting traffic spikes.
 
Now we move on to our next batch of strategies where the theme is  conditioning , which is the idea of shaping and controlling flows of work within your application...
  Use Resources Proportional To a Fixed Limit  
This is probably the most important rule for achieving scalability within an application. What it means:
  
 Find the resource that has a fixed limit that you know you can support. For example, a guarantee to handle a certain number of objects in memory. So if we always use resources proportional to the number of objects it is likely we can prevent resource exhaustion. 
 Devise ways of tying what you need to do to the individual resources. 
  
Some examples:
  
 Keep a list of purchase orders with line items over $20 (or whatever). Do not keep</p><p>6 0.76788414 <a title="205-lsi-6" href="../high_scalability-2009/high_scalability-2009-01-13-Product%3A_Gearman_-_Open_Source_Message_Queuing_System.html">491 high scalability-2009-01-13-Product: Gearman - Open Source Message Queuing System</a></p>
<p>7 0.76148331 <a title="205-lsi-7" href="../high_scalability-2010/high_scalability-2010-12-20-Netflix%3A_Use_Less_Chatty_Protocols_in_the_Cloud_-_Plus_26_Fixes.html">960 high scalability-2010-12-20-Netflix: Use Less Chatty Protocols in the Cloud - Plus 26 Fixes</a></p>
<p>8 0.7567116 <a title="205-lsi-8" href="../high_scalability-2012/high_scalability-2012-08-27-Zoosk_-_The_Engineering_behind_Real_Time_Communications.html">1312 high scalability-2012-08-27-Zoosk - The Engineering behind Real Time Communications</a></p>
<p>9 0.73509228 <a title="205-lsi-9" href="../high_scalability-2008/high_scalability-2008-10-27-Notify.me_Architecture_-_Synchronicity_Kills.html">431 high scalability-2008-10-27-Notify.me Architecture - Synchronicity Kills</a></p>
<p>10 0.73262429 <a title="205-lsi-10" href="../high_scalability-2014/high_scalability-2014-02-05-Little%E2%80%99s_Law%2C_Scalability_and_Fault_Tolerance%3A_The_OS_is_your_bottleneck._What_you_can_do%3F.html">1591 high scalability-2014-02-05-Little’s Law, Scalability and Fault Tolerance: The OS is your bottleneck. What you can do?</a></p>
<p>11 0.72550154 <a title="205-lsi-11" href="../high_scalability-2013/high_scalability-2013-02-27-42_Monster_Problems_that_Attack_as_Loads_Increase.html">1413 high scalability-2013-02-27-42 Monster Problems that Attack as Loads Increase</a></p>
<p>12 0.72242492 <a title="205-lsi-12" href="../high_scalability-2011/high_scalability-2011-02-08-Mollom_Architecture_-_Killing_Over_373_Million_Spams_at_100_Requests_Per_Second.html">985 high scalability-2011-02-08-Mollom Architecture - Killing Over 373 Million Spams at 100 Requests Per Second</a></p>
<p>13 0.71922213 <a title="205-lsi-13" href="../high_scalability-2009/high_scalability-2009-09-10-How_to_handle_so_many_socket_connection.html">699 high scalability-2009-09-10-How to handle so many socket connection</a></p>
<p>14 0.71265775 <a title="205-lsi-14" href="../high_scalability-2014/high_scalability-2014-05-15-Paper%3A_SwiftCloud%3A_Fault-Tolerant_Geo-Replication_Integrated_all_the_Way_to_the_Client_Machine.html">1648 high scalability-2014-05-15-Paper: SwiftCloud: Fault-Tolerant Geo-Replication Integrated all the Way to the Client Machine</a></p>
<p>15 0.71209967 <a title="205-lsi-15" href="../high_scalability-2009/high_scalability-2009-10-26-Facebook%27s_Memcached_Multiget_Hole%3A_More_machines_%21%3D_More_Capacity_.html">728 high scalability-2009-10-26-Facebook's Memcached Multiget Hole: More machines != More Capacity </a></p>
<p>16 0.70997864 <a title="205-lsi-16" href="../high_scalability-2010/high_scalability-2010-02-05-High_Availability_Principle_%3A_Concurrency_Control.html">772 high scalability-2010-02-05-High Availability Principle : Concurrency Control</a></p>
<p>17 0.70779228 <a title="205-lsi-17" href="../high_scalability-2014/high_scalability-2014-02-26-The_WhatsApp_Architecture_Facebook_Bought_For_%2419_Billion.html">1602 high scalability-2014-02-26-The WhatsApp Architecture Facebook Bought For $19 Billion</a></p>
<p>18 0.70584047 <a title="205-lsi-18" href="../high_scalability-2012/high_scalability-2012-06-18-Google_on_Latency_Tolerant_Systems%3A_Making_a_Predictable_Whole_Out_of_Unpredictable_Parts___.html">1266 high scalability-2012-06-18-Google on Latency Tolerant Systems: Making a Predictable Whole Out of Unpredictable Parts   </a></p>
<p>19 0.70433372 <a title="205-lsi-19" href="../high_scalability-2014/high_scalability-2014-03-31-How_WhatsApp_Grew_to_Nearly_500_Million_Users%2C_11%2C000_cores%2C_and_70_Million_Messages_a_Second.html">1622 high scalability-2014-03-31-How WhatsApp Grew to Nearly 500 Million Users, 11,000 cores, and 70 Million Messages a Second</a></p>
<p>20 0.70303929 <a title="205-lsi-20" href="../high_scalability-2009/high_scalability-2009-07-25-Latency_is_Everywhere_and_it_Costs_You_Sales_-_How_to_Crush_it.html">661 high scalability-2009-07-25-Latency is Everywhere and it Costs You Sales - How to Crush it</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.094), (2, 0.641), (10, 0.043), (30, 0.011), (40, 0.015), (61, 0.047), (79, 0.038), (94, 0.032)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.99878365 <a title="205-lda-1" href="../high_scalability-2008/high_scalability-2008-01-10-Letting_Clients_Know_What%27s_Changed%3A_Push_Me_or_Pull_Me%3F.html">205 high scalability-2008-01-10-Letting Clients Know What's Changed: Push Me or Pull Me?</a></p>
<p>Introduction: I had a false belief I thought I came here to stay We're all just visiting All just breaking like waves The oceans made me, but who came up with me? Push me, pull me, push me, or pull me out .     So true Perl Jam   (Push me Pull me lyrics)  , so true. I too have wondered how web clients should be notified of model changes. Should servers push events to clients or should clients pull events from servers? A topic worthy of its own song if ever there was one.       To pull events the client simply starts a timer and makes a request to the server. This is polling. You can either pull a complete set of fresh data or get a list of changes. The server "knows" if anything you are interested in has changed and makes those changes available to you.  Knowing what has changed can be relatively simple with a publish-subscribe type backend or you can get very complex with fine grained bit maps of attributes and keeping per client state on what I client still needs to see.     Polling is heavy man.</p><p>2 0.99716777 <a title="205-lda-2" href="../high_scalability-2009/high_scalability-2009-05-08-Eight_Best_Practices_for_Building_Scalable_Systems.html">594 high scalability-2009-05-08-Eight Best Practices for Building Scalable Systems</a></p>
<p>Introduction: Wille Faler has  created an excellent list of best practices  for building scalable and high performance systems. Here's a short  summary of his points:
   Offload the database  - Avoid hitting the database, and avoid opening transactions or connections unless you absolutely need to use them.    What a difference a cache makes  - For read heavy applications caching is the easiest way offload the database.    Cache as coarse-grained objects as possible  - Coarse-grained objects save CPU and time by requiring fewer reads to assemble objects.    Donâ&euro;&trade;t store transient state permanently  - Is it really necessary to store your transient data in the database?    Location, Location  - put things close to where they are supposed to be delivered.    Constrain concurrent access to limited resource  - it's quicker to let a single thread do work and finish rather than flooding finite resources with 200 client threads.    Staged, asynchronous processing  - separate a process using asynchronicity int</p><p>3 0.99475706 <a title="205-lda-3" href="../high_scalability-2011/high_scalability-2011-12-12-Netflix%3A_Developing%2C_Deploying%2C_and_Supporting_Software_According_to_the_Way_of_the_Cloud.html">1155 high scalability-2011-12-12-Netflix: Developing, Deploying, and Supporting Software According to the Way of the Cloud</a></p>
<p>Introduction: At a  Cloud Computing Meetup , Siddharth "Sid" Anand of Netflix, backed by a merry band of Netflixians, gave an interesting talk:  Keeping Movies Running Amid Thunderstorms . While the talk gave a good overview of their move to the cloud, issues with capacity planning,  thundering herds , latency problems, and  simian armageddon , I found myself most taken with how they handle  software deployment in the cloud .
 
I've worked on half a dozen or more build and deployment systems, some small, some quite large, but never for a large organization like Netflix in the cloud. The cloud has this amazing capability that has never existed before that enables a novel approach to fault-tolerant software deployments:  the ability to spin up huge numbers of instances to completely run a new release while running the old release at the same time .
 
The process goes something like: 
  
 A  canary machine  is launched first with the new software load running real traffic to sanity test the load in a p</p><p>4 0.99409175 <a title="205-lda-4" href="../high_scalability-2011/high_scalability-2011-01-03-Stuff_The_Internet_Says_On_Scalability_For_January_3%2C_2010.html">967 high scalability-2011-01-03-Stuff The Internet Says On Scalability For January 3, 2010</a></p>
<p>Introduction: Submitted for your reading pleasure...
  
 Quotable Quotes           
 
  @hofmanndavid : Performance and scalability anxiety makes developers want to catch the flying butterflies 
  @tivrfoa :  "Scalability solutions aren't magic. They involve partitioning, indexing and replication." Twitter engineer  
 Alan Perlis:  Fools ignore complexity; pragmatists suffer it; experts avoid it; geniuses remove it.  
 
 
  CIO update: Post-mortem on the Skype outage . Interesting tale of a cascading collapse in complex, distributed, interactive systems. For more background see the highly illuminating  Explaining Supernodes  by Dan York. 
  RethinkDB and SSD Databases. SSD was not a revolution  by Kevin Burton.  What’s really shocking to me, is that while SSD and flash storage is very exciting, it wasn’t as revolutionary in 2010 as I would have liked to have seen.  
  The case for Datastore-Side-Scripting . Russell Sullivan predicts real-time web applications are going in the direction of being enti</p><p>5 0.99320966 <a title="205-lda-5" href="../high_scalability-2010/high_scalability-2010-09-30-More_Troubles_with_Caching.html">911 high scalability-2010-09-30-More Troubles with Caching</a></p>
<p>Introduction: As a tasty pairing with  Facebook And Site Failures Caused By Complex, Weakly Interacting, Layered Systems , is another excellent tale of caching gone wrong by Peter Zaitsev, in an exciting twin billing:  Cache Miss Storm  and  More on dangers of the caches . This is fascinating case where the cause turned out to be software upgrade that ran long because it had to be rolled back. During the long recovery time many of the cache entries timed out. When the database came back, slam, all the clients queried the database to repopulate the cache and bad things happened to the database. The solution was equally interesting: 
  

So the immediate solution to bring the system up was surprisingly simple. We just had to get traffic on the system in stages allowing Memcache to be warmed up. There were no code which would allow to do it on application side so we did it on MySQL side instead. “SET GLOBAL max_connections=20” to limit number of connections to MySQL and so let application to err when i</p><p>6 0.99294144 <a title="205-lda-6" href="../high_scalability-2010/high_scalability-2010-08-12-Strategy%3A_Terminate_SSL_Connections_in_Hardware_and_Reduce_Server_Count_by_40%25.html">878 high scalability-2010-08-12-Strategy: Terminate SSL Connections in Hardware and Reduce Server Count by 40%</a></p>
<p>7 0.99179715 <a title="205-lda-7" href="../high_scalability-2008/high_scalability-2008-12-01-MySQL_Database_Scale-out_and_Replication_for_High_Growth_Businesses.html">455 high scalability-2008-12-01-MySQL Database Scale-out and Replication for High Growth Businesses</a></p>
<p>8 0.99067396 <a title="205-lda-8" href="../high_scalability-2010/high_scalability-2010-06-04-Strategy%3A_Cache_Larger_Chunks_-_Cache_Hit_Rate_is_a_Bad_Indicator.html">836 high scalability-2010-06-04-Strategy: Cache Larger Chunks - Cache Hit Rate is a Bad Indicator</a></p>
<p>9 0.99056244 <a title="205-lda-9" href="../high_scalability-2009/high_scalability-2009-10-16-Paper%3A_Scaling_Online_Social_Networks_without_Pains.html">723 high scalability-2009-10-16-Paper: Scaling Online Social Networks without Pains</a></p>
<p>10 0.98917902 <a title="205-lda-10" href="../high_scalability-2012/high_scalability-2012-02-27-Zen_and_the_Art_of_Scaling_-_A_Koan_and_Epigram_Approach.html">1199 high scalability-2012-02-27-Zen and the Art of Scaling - A Koan and Epigram Approach</a></p>
<p>11 0.98862219 <a title="205-lda-11" href="../high_scalability-2012/high_scalability-2012-02-10-Stuff_The_Internet_Says_On_Scalability_For_February_10%2C_2012.html">1190 high scalability-2012-02-10-Stuff The Internet Says On Scalability For February 10, 2012</a></p>
<p>12 0.98855823 <a title="205-lda-12" href="../high_scalability-2014/high_scalability-2014-02-05-Little%E2%80%99s_Law%2C_Scalability_and_Fault_Tolerance%3A_The_OS_is_your_bottleneck._What_you_can_do%3F.html">1591 high scalability-2014-02-05-Little’s Law, Scalability and Fault Tolerance: The OS is your bottleneck. What you can do?</a></p>
<p>13 0.98719889 <a title="205-lda-13" href="../high_scalability-2008/high_scalability-2008-01-25-Google%3A_Introduction_to_Distributed_System_Design.html">223 high scalability-2008-01-25-Google: Introduction to Distributed System Design</a></p>
<p>14 0.98649824 <a title="205-lda-14" href="../high_scalability-2007/high_scalability-2007-07-31-BerkeleyDB_%26_other_distributed_high_performance_key-value_databases.html">50 high scalability-2007-07-31-BerkeleyDB & other distributed high performance key-value databases</a></p>
<p>15 0.98634803 <a title="205-lda-15" href="../high_scalability-2010/high_scalability-2010-06-18-Paper%3A_The_Declarative_Imperative%3A_Experiences_and_Conjectures_in_Distributed_Logic.html">844 high scalability-2010-06-18-Paper: The Declarative Imperative: Experiences and Conjectures in Distributed Logic</a></p>
<p>16 0.98580086 <a title="205-lda-16" href="../high_scalability-2008/high_scalability-2008-11-02-Strategy%3A_How_to_Manage_Sessions_Using_Memcached.html">436 high scalability-2008-11-02-Strategy: How to Manage Sessions Using Memcached</a></p>
<p>17 0.98495114 <a title="205-lda-17" href="../high_scalability-2011/high_scalability-2011-03-17-Are_long_VM_instance_spin-up_times_in_the_cloud_costing_you_money%3F.html">1006 high scalability-2011-03-17-Are long VM instance spin-up times in the cloud costing you money?</a></p>
<p>18 0.98146737 <a title="205-lda-18" href="../high_scalability-2009/high_scalability-2009-06-27-Scaling_Twitter%3A_Making_Twitter_10000_Percent_Faster.html">639 high scalability-2009-06-27-Scaling Twitter: Making Twitter 10000 Percent Faster</a></p>
<p>19 0.98145765 <a title="205-lda-19" href="../high_scalability-2007/high_scalability-2007-08-03-Running_Hadoop_MapReduce_on_Amazon_EC2_and_Amazon_S3.html">56 high scalability-2007-08-03-Running Hadoop MapReduce on Amazon EC2 and Amazon S3</a></p>
<p>20 0.98145765 <a title="205-lda-20" href="../high_scalability-2009/high_scalability-2009-04-13-Benchmark_for_keeping_data_in_browser_in_AJAX_projects.html">565 high scalability-2009-04-13-Benchmark for keeping data in browser in AJAX projects</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
