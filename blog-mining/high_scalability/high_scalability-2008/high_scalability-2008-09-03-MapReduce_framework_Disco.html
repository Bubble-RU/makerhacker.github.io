<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>376 high scalability-2008-09-03-MapReduce framework Disco</title>
</head>

<body>
<p><a title="high_scalability" href="../high_scalability_home.html">high_scalability</a> <a title="high_scalability-2008" href="../home/high_scalability-2008_home.html">high_scalability-2008</a> <a title="high_scalability-2008-376" href="#">high_scalability-2008-376</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>376 high scalability-2008-09-03-MapReduce framework Disco</h1>
<br/><h2>meta infos for this blog</h2><p>Source: <a title="high_scalability-2008-376-html" href="http://highscalability.com//blog/2008/9/3/mapreduce-framework-disco.html">html</a></p><p>Introduction: Disco  is an open-source implementation of the MapReduce framework for distributed computing.  It was started at  Nokia Research Center  as a lightweight framework for rapid scripting of distributed data processing tasks.   The Disco core is written in Erlang. The MapReduce jobs in Disco are natively described as Python programs, which makes it possible to express complex algorithmic and data processing tasks often only in tens of lines of code.</p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Disco  is an open-source implementation of the MapReduce framework for distributed computing. [sent-1, score-0.38]
</p><p>2 It was started at  Nokia Research Center  as a lightweight framework for rapid scripting of distributed data processing tasks. [sent-2, score-1.035]
</p><p>3 The MapReduce jobs in Disco are natively described as Python programs, which makes it possible to express complex algorithmic and data processing tasks often only in tens of lines of code. [sent-4, score-1.692]
</p>
<br/>
<h2>similar blogs computed by tfidf model</h2><h3>tfidf for this blog:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('disco', 0.656), ('atnokia', 0.298), ('mapreduce', 0.22), ('algorithmic', 0.203), ('natively', 0.196), ('framework', 0.193), ('express', 0.179), ('scripting', 0.172), ('lightweight', 0.163), ('tens', 0.154), ('processing', 0.152), ('described', 0.138), ('programs', 0.127), ('lines', 0.124), ('rapid', 0.123), ('research', 0.115), ('python', 0.115), ('tasks', 0.112), ('jobs', 0.111), ('started', 0.096), ('implementation', 0.096), ('distributed', 0.091), ('written', 0.08), ('core', 0.08), ('often', 0.074), ('complex', 0.072), ('possible', 0.07), ('makes', 0.062), ('code', 0.051), ('data', 0.045)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 1.0 <a title="376-tfidf-1" href="../high_scalability-2008/high_scalability-2008-09-03-MapReduce_framework_Disco.html">376 high scalability-2008-09-03-MapReduce framework Disco</a></p>
<p>Introduction: Disco  is an open-source implementation of the MapReduce framework for distributed computing.  It was started at  Nokia Research Center  as a lightweight framework for rapid scripting of distributed data processing tasks.   The Disco core is written in Erlang. The MapReduce jobs in Disco are natively described as Python programs, which makes it possible to express complex algorithmic and data processing tasks often only in tens of lines of code.</p><p>2 0.13452943 <a title="376-tfidf-2" href="../high_scalability-2009/high_scalability-2009-01-04-Paper%3A_MapReduce%3A_Simplified_Data_Processing_on_Large_Clusters.html">483 high scalability-2009-01-04-Paper: MapReduce: Simplified Data Processing on Large Clusters</a></p>
<p>Introduction: Update:   MapReduce and PageRank Notes from Remzi Arpaci-Dusseau's Fall 2008 class  . Collects interesting facts about MapReduce and PageRank. For example, the history of the solution to searching for the term "flu" is traced through multiple generations of technology.   With Google entering the cloud space with  Google AppEngine  and a maturing  Hadoop  product, the MapReduce scaling approach might finally become a standard programmer practice. This is the best paper on the subject and is an excellent primer on a content-addressable memory future.  Some interesting stats from the paper: Google executes 100k MapReduce jobs each day; more than 20 petabytes of data are processed per day; more than 10k MapReduce programs have been implemented; machines are dual processor with gigabit ethernet and 4-8 GB of memory.  One common criticism ex-Googlers have is that it takes months to get up and be productive in the Google environment. Hopefully a way will be found to lower the learning curve a</p><p>3 0.10876185 <a title="376-tfidf-3" href="../high_scalability-2008/high_scalability-2008-12-17-Ringo_-_Distributed_key-value_storage_for_immutable_data.html">468 high scalability-2008-12-17-Ringo - Distributed key-value storage for immutable data</a></p>
<p>Introduction: Ringo is an experimental, distributed, replicating key-value store based on consistent hashing and immutable data. Unlike many general-purpose databases, Ringo is designed for a specific use case: For archiving small (less than 4KB) or medium-size data items (<100MB) in real-time so that the data can survive K - 1 disk breaks, where K is the desired number of replicas, without any downtime, in a manner that scales to terabytes of data. In addition to storing, Ringo should be able to retrieve individual or small sets of data items with low latencies (<10ms) and provide a convenient on-disk format for bulk data access.       Ringo is compatible with the   map-reduce framework Disco   and it was started at   Nokia Research Center   Palo Alto.</p><p>4 0.10015146 <a title="376-tfidf-4" href="../high_scalability-2012/high_scalability-2012-08-28-Making_Hadoop_Run_Faster.html">1313 high scalability-2012-08-28-Making Hadoop Run Faster</a></p>
<p>Introduction: Making Hadoop Run Faster   
 

One of the challenges in processing data is that the speed at which we can input data is quite often much faster than the speed at which we can process it. This problem becomes even more pronounced in the context of Big Data, where the volume of data keeps on growing, along with a corresponding need for more insights, and thus the need for more complex processing also increases.


 Batch Processing to the Rescue 


Hadoop was designed to deal with this challenge in the following ways:


1. Use a distributed file system: This enables us to spread the load and grow our system as needed.


2. Optimize for write speed: To enable fast writes the Hadoop architecture was designed so that writes are first logged, and then processed. This enables fairly fast write speeds.


3. Use batch processing (Map/Reduce) to balance the speed for the data feeds with the processing speed.


 Batch Processing Challenges 


The challenge with batch-processing is that it assumes</p><p>5 0.097883999 <a title="376-tfidf-5" href="../high_scalability-2009/high_scalability-2009-05-06-Art_of_Distributed.html">590 high scalability-2009-05-06-Art of Distributed</a></p>
<p>Introduction: Art of Distributed   Part 1: Rethinking about distributed computing models  
I â&euro;&tilde;m getting a lot of questions lately about the distributed computing, especially distributed computing model, and MapReduce, such as: What is MapReduce? Can MapReduce fit in all situations? How we can compares it with other technologies such as Grid Computing? And what is the best solution to our situation? So I decide to write about the distributed computing article in two parts. First one about the distributed computing model and what is the difference between them. In the second part I will discuss the reliability, and distributed storage systems.   Download the article in PDF format.   Download the article in MS Word format.   I wait for your comments, and questions, and I will answer it in part two.</p><p>6 0.092391267 <a title="376-tfidf-6" href="../high_scalability-2009/high_scalability-2009-07-30-Learn_How_to_Think_at_Scale.html">666 high scalability-2009-07-30-Learn How to Think at Scale</a></p>
<p>7 0.089997128 <a title="376-tfidf-7" href="../high_scalability-2010/high_scalability-2010-08-18-Misco%3A_A_MapReduce_Framework_for_Mobile_Systems_-_Start_of_the_Ambient_Cloud%3F.html">882 high scalability-2010-08-18-Misco: A MapReduce Framework for Mobile Systems - Start of the Ambient Cloud?</a></p>
<p>8 0.087502807 <a title="376-tfidf-8" href="../high_scalability-2008/high_scalability-2008-11-22-Google_Architecture.html">448 high scalability-2008-11-22-Google Architecture</a></p>
<p>9 0.08544752 <a title="376-tfidf-9" href="../high_scalability-2008/high_scalability-2008-10-04-Is_MapReduce_going_mainstream%3F.html">401 high scalability-2008-10-04-Is MapReduce going mainstream?</a></p>
<p>10 0.085374303 <a title="376-tfidf-10" href="../high_scalability-2008/high_scalability-2008-08-11-Distributed__Computing_%26_Google_Infrastructure.html">362 high scalability-2008-08-11-Distributed  Computing & Google Infrastructure</a></p>
<p>11 0.080960073 <a title="376-tfidf-11" href="../high_scalability-2008/high_scalability-2008-09-28-Product%3A_Happy_%3D_Hadoop_%2B_Python.html">397 high scalability-2008-09-28-Product: Happy = Hadoop + Python</a></p>
<p>12 0.079100348 <a title="376-tfidf-12" href="../high_scalability-2009/high_scalability-2009-08-20-Dependency_Injection_and_AOP_frameworks_for_.NET_.html">685 high scalability-2009-08-20-Dependency Injection and AOP frameworks for .NET </a></p>
<p>13 0.079041928 <a title="376-tfidf-13" href="../high_scalability-2010/high_scalability-2010-08-04-Dremel%3A_Interactive_Analysis_of_Web-Scale_Datasets_-_Data_as_a_Programming_Paradigm.html">871 high scalability-2010-08-04-Dremel: Interactive Analysis of Web-Scale Datasets - Data as a Programming Paradigm</a></p>
<p>14 0.078634731 <a title="376-tfidf-14" href="../high_scalability-2007/high_scalability-2007-11-20-Product%3A_SmartFrog_a_Distributed_Configuration_and_Deployment_Framework.html">161 high scalability-2007-11-20-Product: SmartFrog a Distributed Configuration and Deployment Framework</a></p>
<p>15 0.077580534 <a title="376-tfidf-15" href="../high_scalability-2010/high_scalability-2010-10-01-Google_Paper%3A_Large-scale_Incremental_Processing_Using_Distributed_Transactions_and_Notifications.html">912 high scalability-2010-10-01-Google Paper: Large-scale Incremental Processing Using Distributed Transactions and Notifications</a></p>
<p>16 0.075175099 <a title="376-tfidf-16" href="../high_scalability-2008/high_scalability-2008-07-26-Google%27s_Paxos_Made_Live_%E2%80%93_An_Engineering_Perspective.html">357 high scalability-2008-07-26-Google's Paxos Made Live – An Engineering Perspective</a></p>
<p>17 0.071605332 <a title="376-tfidf-17" href="../high_scalability-2008/high_scalability-2008-10-15-Hadoop_-_A_Primer.html">414 high scalability-2008-10-15-Hadoop - A Primer</a></p>
<p>18 0.068113744 <a title="376-tfidf-18" href="../high_scalability-2012/high_scalability-2012-03-26-7_Years_of_YouTube_Scalability_Lessons_in_30_Minutes.html">1215 high scalability-2012-03-26-7 Years of YouTube Scalability Lessons in 30 Minutes</a></p>
<p>19 0.064155594 <a title="376-tfidf-19" href="../high_scalability-2009/high_scalability-2009-04-04-Performance_Anti-Pattern.html">555 high scalability-2009-04-04-Performance Anti-Pattern</a></p>
<p>20 0.064151049 <a title="376-tfidf-20" href="../high_scalability-2011/high_scalability-2011-03-14-6_Lessons_from_Dropbox_-_One_Million_Files_Saved_Every_15_minutes.html">1003 high scalability-2011-03-14-6 Lessons from Dropbox - One Million Files Saved Every 15 minutes</a></p>
<br/>
<h2>similar blogs computed by <a title="lsi-model" href="../home/high_scalability_lsi.html">lsi model</a></h2><h3>lsi for this blog:</h3><p>topicId topicWeight</p>
<p>[(0, 0.063), (1, 0.036), (2, 0.011), (3, 0.05), (4, 0.029), (5, 0.044), (6, 0.052), (7, 0.021), (8, 0.007), (9, 0.086), (10, 0.033), (11, 0.001), (12, 0.038), (13, -0.082), (14, 0.021), (15, -0.035), (16, -0.03), (17, -0.027), (18, 0.024), (19, 0.008), (20, 0.014), (21, -0.006), (22, 0.003), (23, 0.011), (24, 0.018), (25, 0.027), (26, 0.032), (27, 0.041), (28, -0.017), (29, 0.054), (30, 0.017), (31, 0.077), (32, -0.049), (33, 0.013), (34, -0.008), (35, -0.057), (36, -0.01), (37, -0.013), (38, 0.019), (39, 0.092), (40, -0.007), (41, 0.009), (42, -0.051), (43, 0.026), (44, 0.049), (45, -0.024), (46, -0.019), (47, -0.013), (48, -0.024), (49, -0.059)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>same-blog 1 0.9651075 <a title="376-lsi-1" href="../high_scalability-2008/high_scalability-2008-09-03-MapReduce_framework_Disco.html">376 high scalability-2008-09-03-MapReduce framework Disco</a></p>
<p>Introduction: Disco  is an open-source implementation of the MapReduce framework for distributed computing.  It was started at  Nokia Research Center  as a lightweight framework for rapid scripting of distributed data processing tasks.   The Disco core is written in Erlang. The MapReduce jobs in Disco are natively described as Python programs, which makes it possible to express complex algorithmic and data processing tasks often only in tens of lines of code.</p><p>2 0.74170697 <a title="376-lsi-2" href="../high_scalability-2009/high_scalability-2009-01-04-Paper%3A_MapReduce%3A_Simplified_Data_Processing_on_Large_Clusters.html">483 high scalability-2009-01-04-Paper: MapReduce: Simplified Data Processing on Large Clusters</a></p>
<p>Introduction: Update:   MapReduce and PageRank Notes from Remzi Arpaci-Dusseau's Fall 2008 class  . Collects interesting facts about MapReduce and PageRank. For example, the history of the solution to searching for the term "flu" is traced through multiple generations of technology.   With Google entering the cloud space with  Google AppEngine  and a maturing  Hadoop  product, the MapReduce scaling approach might finally become a standard programmer practice. This is the best paper on the subject and is an excellent primer on a content-addressable memory future.  Some interesting stats from the paper: Google executes 100k MapReduce jobs each day; more than 20 petabytes of data are processed per day; more than 10k MapReduce programs have been implemented; machines are dual processor with gigabit ethernet and 4-8 GB of memory.  One common criticism ex-Googlers have is that it takes months to get up and be productive in the Google environment. Hopefully a way will be found to lower the learning curve a</p><p>3 0.73816854 <a title="376-lsi-3" href="../high_scalability-2008/high_scalability-2008-10-04-Is_MapReduce_going_mainstream%3F.html">401 high scalability-2008-10-04-Is MapReduce going mainstream?</a></p>
<p>Introduction: Compares MapReduce to other parallel processing approaches and suggests new paradigm for clouds and grids</p><p>4 0.72058094 <a title="376-lsi-4" href="../high_scalability-2013/high_scalability-2013-09-05-Paper%3A_MillWheel%3A_Fault-Tolerant_Stream_Processing_at_Internet_Scale.html">1512 high scalability-2013-09-05-Paper: MillWheel: Fault-Tolerant Stream Processing at Internet Scale</a></p>
<p>Introduction: Ever wonder what powers Google's world spirit sensing  Zeitgeist service ? No, it's not a homunculus of  Georg Wilhelm Friedrich Hegel  sitting in each browser. It's actually a stream processing (think streaming MapReduce on steroids) system called MillWheel, described in this very well written paper:  MillWheel: Fault-Tolerant Stream Processing at Internet Scale . MillWheel isn't just used for Zeitgeist at Google, it's also used for streaming joins for a variety of Ads customers, generalized anomaly-detection service, and network switch and cluster health monitoring.
 
Abstract:
  MillWheel is a framework for building low-latency data-processing applications that is widely used at Google. Users specify a directed computation graph and application code for individual nodes, and the system manages persistent state and the continuous ﬂow of records, all within the envelope of the framework’s fault-tolerance guarantees.

 


This paper describes MillWheel’s programming model as well as it</p><p>5 0.69386727 <a title="376-lsi-5" href="../high_scalability-2010/high_scalability-2010-06-30-Paper%3A_GraphLab%3A_A_New_Framework_For_Parallel_Machine_Learning.html">850 high scalability-2010-06-30-Paper: GraphLab: A New Framework For Parallel Machine Learning</a></p>
<p>Introduction: In the never ending quest to figure out how to do something useful with never ending streams of data,  GraphLab: A New Framework For Parallel Machine Learning  wants to go beyond low-level programming, MapReduce, and dataflow languages with  a new parallel framework for ML (machine learning) which exploits the sparse structure and common computational patterns of ML algorithms. GraphLab enables ML experts to easily design and implement efﬁcient scalable parallel algorithms by composing problem speciﬁc computation, data-dependencies, and scheduling .   Our main contributions include:  
  
  A graph-based data model which simultaneously represents data and computational dependencies.   
  A set of concurrent access models which provide a range of sequential-consistency guarantees.   
  A sophisticated modular scheduling mechanism.   
  An aggregation framework to manage global state.   
   From the abstract:
  
  Designing and implementing efﬁcient, provably correct parallel machine lear</p><p>6 0.63890302 <a title="376-lsi-6" href="../high_scalability-2008/high_scalability-2008-10-15-Hadoop_-_A_Primer.html">414 high scalability-2008-10-15-Hadoop - A Primer</a></p>
<p>7 0.63334084 <a title="376-lsi-7" href="../high_scalability-2012/high_scalability-2012-08-28-Making_Hadoop_Run_Faster.html">1313 high scalability-2012-08-28-Making Hadoop Run Faster</a></p>
<p>8 0.62914592 <a title="376-lsi-8" href="../high_scalability-2008/high_scalability-2008-08-11-Distributed__Computing_%26_Google_Infrastructure.html">362 high scalability-2008-08-11-Distributed  Computing & Google Infrastructure</a></p>
<p>9 0.61058062 <a title="376-lsi-9" href="../high_scalability-2008/high_scalability-2008-09-28-Product%3A_Happy_%3D_Hadoop_%2B_Python.html">397 high scalability-2008-09-28-Product: Happy = Hadoop + Python</a></p>
<p>10 0.60943151 <a title="376-lsi-10" href="../high_scalability-2009/high_scalability-2009-06-20-Building_a_data_cycle_at_LinkedIn_with_Hadoop_and_Project_Voldemort.html">634 high scalability-2009-06-20-Building a data cycle at LinkedIn with Hadoop and Project Voldemort</a></p>
<p>11 0.58949971 <a title="376-lsi-11" href="../high_scalability-2008/high_scalability-2008-10-29-CTL_-_Distributed_Control_Dispatching_Framework_.html">433 high scalability-2008-10-29-CTL - Distributed Control Dispatching Framework </a></p>
<p>12 0.58811915 <a title="376-lsi-12" href="../high_scalability-2009/high_scalability-2009-05-17-Product%3A_Hadoop.html">601 high scalability-2009-05-17-Product: Hadoop</a></p>
<p>13 0.57914698 <a title="376-lsi-13" href="../high_scalability-2008/high_scalability-2008-10-15-Need_help_with_your_Hadoop_deployment%3F_This_company_may_help%21.html">415 high scalability-2008-10-15-Need help with your Hadoop deployment? This company may help!</a></p>
<p>14 0.57771999 <a title="376-lsi-14" href="../high_scalability-2010/high_scalability-2010-10-01-Google_Paper%3A_Large-scale_Incremental_Processing_Using_Distributed_Transactions_and_Notifications.html">912 high scalability-2010-10-01-Google Paper: Large-scale Incremental Processing Using Distributed Transactions and Notifications</a></p>
<p>15 0.57570672 <a title="376-lsi-15" href="../high_scalability-2009/high_scalability-2009-07-02-Product%3A_Hbase.html">650 high scalability-2009-07-02-Product: Hbase</a></p>
<p>16 0.57390058 <a title="376-lsi-16" href="../high_scalability-2009/high_scalability-2009-08-03-Building_a_Data_Intensive_Web_Application_with_Cloudera%2C_Hadoop%2C_Hive%2C_Pig%2C_and_EC2.html">669 high scalability-2009-08-03-Building a Data Intensive Web Application with Cloudera, Hadoop, Hive, Pig, and EC2</a></p>
<p>17 0.5662775 <a title="376-lsi-17" href="../high_scalability-2009/high_scalability-2009-05-06-Dyrad.html">591 high scalability-2009-05-06-Dyrad</a></p>
<p>18 0.55677134 <a title="376-lsi-18" href="../high_scalability-2010/high_scalability-2010-08-04-Dremel%3A_Interactive_Analysis_of_Web-Scale_Datasets_-_Data_as_a_Programming_Paradigm.html">871 high scalability-2010-08-04-Dremel: Interactive Analysis of Web-Scale Datasets - Data as a Programming Paradigm</a></p>
<p>19 0.55325836 <a title="376-lsi-19" href="../high_scalability-2013/high_scalability-2013-01-07-Analyzing_billions_of_credit_card_transactions_and_serving_low-latency_insights_in_the_cloud.html">1382 high scalability-2013-01-07-Analyzing billions of credit card transactions and serving low-latency insights in the cloud</a></p>
<p>20 0.53881329 <a title="376-lsi-20" href="../high_scalability-2008/high_scalability-2008-11-14-Paper%3A_Pig_Latin%3A_A_Not-So-Foreign_Language_for_Data_Processing.html">443 high scalability-2008-11-14-Paper: Pig Latin: A Not-So-Foreign Language for Data Processing</a></p>
<br/>
<h2>similar blogs computed by <a title="lda-model" href="../home/high_scalability_lda.html">lda model</a></h2><h3>lda for this blog:</h3><p>topicId topicWeight</p>
<p>[(1, 0.072), (2, 0.313), (10, 0.031), (61, 0.056), (67, 0.245), (94, 0.107)]</p>
<h3>similar blogs list:</h3><p>simIndex simValue blogId blogTitle</p>
<p>1 0.90895057 <a title="376-lda-1" href="../high_scalability-2013/high_scalability-2013-03-12-If_Your_System_was_a_Symphony_it_Might_Sound_Like_This....html">1422 high scalability-2013-03-12-If Your System was a Symphony it Might Sound Like This...</a></p>
<p>Introduction: I am in no way a music expert, but when I listen to  Symphony No. 4  by  Charles Ives , I imagine it's what a complex software/hardware system might sound like if we could hear its inner workings. Ives uses a lot of riotously competing rhythms in this work. It can sound discordant, yet the effect is deeply layered and eventually harmonious, just like the systems we use, create, and become part of.
 
I was pointed to this piece by someone who said there were two conductors. I'd never heard of such a thing! So I was intrigued. The first version of the performance sounds and looks great, but it unfortunately does not use two conductors. The second version uses two conductors, but is unfortunately just a snippet.
 
It's strikingly odd to see two conductors, but I imagine different parts of our systems using different conductors too, running at different rhythms, sometimes slow, sometimes fast, sometimes there are outbursts, sometimes in vicious conflict. Yet conceptually it all stills seem</p><p>2 0.90258348 <a title="376-lda-2" href="../high_scalability-2010/high_scalability-2010-09-09-6_Scalability_Lessons.html">898 high scalability-2010-09-09-6 Scalability Lessons</a></p>
<p>Introduction: Jesper Söderlund not only put together a few interesting  scalability patterns , he also came up with a few interesting  scalability lessons :
  
  Lesson #1 . Put Smarty compile and template caches on an active-active DRBD cluster with high load and your servers will DIE! 
  Lesson #2 . Don't use out-of-the-box configurations. 
  Lesson #3 . Single points of contention will eventually become a bottleneck. 
  Lesson #4 . Plan in advance.  
  Lesson #5 . Offload your databases as much as possible. 
  Lesson #6 . File systems matter and can run out of space / inodes. 
  
For more details and explanations see the original post.</p><p>same-blog 3 0.88532478 <a title="376-lda-3" href="../high_scalability-2008/high_scalability-2008-09-03-MapReduce_framework_Disco.html">376 high scalability-2008-09-03-MapReduce framework Disco</a></p>
<p>Introduction: Disco  is an open-source implementation of the MapReduce framework for distributed computing.  It was started at  Nokia Research Center  as a lightweight framework for rapid scripting of distributed data processing tasks.   The Disco core is written in Erlang. The MapReduce jobs in Disco are natively described as Python programs, which makes it possible to express complex algorithmic and data processing tasks often only in tens of lines of code.</p><p>4 0.80201817 <a title="376-lda-4" href="../high_scalability-2012/high_scalability-2012-01-10-A_Perfect_Fifth_of_Notes_on_Scalability.html">1172 high scalability-2012-01-10-A Perfect Fifth of Notes on Scalability</a></p>
<p>Introduction: Jeremiah Peschka with a great a set of  Notes on Scalability , just in case you do reach your wildest expectations of success:
  
   Build it to Break . Plan for the fact that everything you make is going to break. Design in layers that are independent and redundant. 
  Everything is a Feature . Your application is a set of features created by a series of conscious choices made by considering trade-offs.  
  Scale Out, Not Up . Purchasing more hardware is easier than coding and managing horizontal resources.  
  Buy More Storage . Large numbers of smaller, faster drives have more IOPS than fewer, larger drives. 
  You’re Going to Do It Wrong . Be prepared to iterate on your ideas. You will make mistakes.  Be prepared to re-write code and to quickly move on to the next idea. 
  
Please read the  original article  for a much more expansive treatment.</p><p>5 0.77775246 <a title="376-lda-5" href="../high_scalability-2009/high_scalability-2009-10-16-Paper%3A_Scaling_Online_Social_Networks_without_Pains.html">723 high scalability-2009-10-16-Paper: Scaling Online Social Networks without Pains</a></p>
<p>Introduction: We saw in  Why are Facebook, Digg, and Twitter so hard to scale?  scaling social networks is a lot harder than you might think. This paper,  Scaling Online Social Networks without Pains , from a team at Telefonica Research in Spain hopes to meet the challenge of status distribution, user generated content distribution, and managing the social graph through a technique they call  One-Hop Replication  (OHR). OHR  abstracts and delegates the complexity of scaling up from the social network application . The abstract:   
 Online Social Networks (OSN) face serious scalability challenges due to their rapid growth and popularity. To address this issue we present a novel approach to scale up OSN called One Hop Replication (OHR). Our system combines partitioning and replication in a middleware to transparently scale up a centralized OSN design, and therefore, avoid the OSN application to undergo the costly transition to a fully distributed system to meet its scalability needs. OHR exploits some</p><p>6 0.77721083 <a title="376-lda-6" href="../high_scalability-2011/high_scalability-2011-01-03-Stuff_The_Internet_Says_On_Scalability_For_January_3%2C_2010.html">967 high scalability-2011-01-03-Stuff The Internet Says On Scalability For January 3, 2010</a></p>
<p>7 0.77661508 <a title="376-lda-7" href="../high_scalability-2014/high_scalability-2014-02-05-Little%E2%80%99s_Law%2C_Scalability_and_Fault_Tolerance%3A_The_OS_is_your_bottleneck._What_you_can_do%3F.html">1591 high scalability-2014-02-05-Little’s Law, Scalability and Fault Tolerance: The OS is your bottleneck. What you can do?</a></p>
<p>8 0.77575761 <a title="376-lda-8" href="../high_scalability-2012/high_scalability-2012-02-27-Zen_and_the_Art_of_Scaling_-_A_Koan_and_Epigram_Approach.html">1199 high scalability-2012-02-27-Zen and the Art of Scaling - A Koan and Epigram Approach</a></p>
<p>9 0.77487576 <a title="376-lda-9" href="../high_scalability-2008/high_scalability-2008-01-10-Letting_Clients_Know_What%27s_Changed%3A_Push_Me_or_Pull_Me%3F.html">205 high scalability-2008-01-10-Letting Clients Know What's Changed: Push Me or Pull Me?</a></p>
<p>10 0.77306885 <a title="376-lda-10" href="../high_scalability-2009/high_scalability-2009-03-30-Lavabit_Architecture_-_Creating_a_Scalable_Email_Service.html">551 high scalability-2009-03-30-Lavabit Architecture - Creating a Scalable Email Service</a></p>
<p>11 0.77292645 <a title="376-lda-11" href="../high_scalability-2012/high_scalability-2012-02-10-Stuff_The_Internet_Says_On_Scalability_For_February_10%2C_2012.html">1190 high scalability-2012-02-10-Stuff The Internet Says On Scalability For February 10, 2012</a></p>
<p>12 0.77137685 <a title="376-lda-12" href="../high_scalability-2009/high_scalability-2009-12-17-Oracle_and_IBM_databases%3A_Disk-based_vs_In-memory_databases_.html">752 high scalability-2009-12-17-Oracle and IBM databases: Disk-based vs In-memory databases </a></p>
<p>13 0.77114433 <a title="376-lda-13" href="../high_scalability-2012/high_scalability-2012-12-17-11_Uses_For_the_Humble_Presents_Queue%2C_er%2C_Message_Queue.html">1373 high scalability-2012-12-17-11 Uses For the Humble Presents Queue, er, Message Queue</a></p>
<p>14 0.77066427 <a title="376-lda-14" href="../high_scalability-2011/high_scalability-2011-09-27-Use_Instance_Caches_to_Save_Money%3A_Latency_%3D%3D_%24%24%24.html">1126 high scalability-2011-09-27-Use Instance Caches to Save Money: Latency == $$$</a></p>
<p>15 0.76949805 <a title="376-lda-15" href="../high_scalability-2008/high_scalability-2008-01-24-Mailinator_Architecture.html">221 high scalability-2008-01-24-Mailinator Architecture</a></p>
<p>16 0.76856476 <a title="376-lda-16" href="../high_scalability-2009/high_scalability-2009-06-27-Scaling_Twitter%3A_Making_Twitter_10000_Percent_Faster.html">639 high scalability-2009-06-27-Scaling Twitter: Making Twitter 10000 Percent Faster</a></p>
<p>17 0.76693898 <a title="376-lda-17" href="../high_scalability-2011/high_scalability-2011-03-17-Are_long_VM_instance_spin-up_times_in_the_cloud_costing_you_money%3F.html">1006 high scalability-2011-03-17-Are long VM instance spin-up times in the cloud costing you money?</a></p>
<p>18 0.76660275 <a title="376-lda-18" href="../high_scalability-2009/high_scalability-2009-05-08-Eight_Best_Practices_for_Building_Scalable_Systems.html">594 high scalability-2009-05-08-Eight Best Practices for Building Scalable Systems</a></p>
<p>19 0.76611948 <a title="376-lda-19" href="../high_scalability-2013/high_scalability-2013-04-24-Strategy%3A_Using_Lots_of_RAM_Often_Cheaper_than_Using_a_Hadoop_Cluster.html">1445 high scalability-2013-04-24-Strategy: Using Lots of RAM Often Cheaper than Using a Hadoop Cluster</a></p>
<p>20 0.76498216 <a title="376-lda-20" href="../high_scalability-2012/high_scalability-2012-09-19-The_4_Building_Blocks_of_Architecting_Systems_for_Scale.html">1325 high scalability-2012-09-19-The 4 Building Blocks of Architecting Systems for Scale</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
